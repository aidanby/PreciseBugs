{"buggy_code": ["[![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Title.png)](https://github.com/man-group/dtale)\n\n* [Live Demo](http://alphatechadmin.pythonanywhere.com)\n* [Animated US COVID-19 Deaths By State](http://alphatechadmin.pythonanywhere.com/dtale/charts/3?chart_type=maps&query=date+%3E+%2720200301%27&agg=raw&map_type=choropleth&loc_mode=USA-states&loc=state_code&map_val=deaths&colorscale=Reds&cpg=false&animate_by=date)\n* [3D Scatter Chart](http://alphatechadmin.pythonanywhere.com/dtale/charts/4?chart_type=3d_scatter&query=&x=date&z=Col0&agg=raw&cpg=false&y=%5B%22security_id%22%5D)\n* [Surface Chart](http://alphatechadmin.pythonanywhere.com/dtale/charts/4?chart_type=surface&query=&x=date&z=Col0&agg=raw&cpg=false&y=%5B%22security_id%22%5D)\n* [Network Analysis](http://alphatechadmin.pythonanywhere.com/dtale/network/5?to=to&from=from&group=route_id&weight=)\n\n-----------------\n\n[![CircleCI](https://circleci.com/gh/man-group/dtale.svg?style=shield)](https://circleci.com/gh/man-group/dtale)\n[![PyPI Python Versions](https://img.shields.io/pypi/pyversions/dtale.svg)](https://pypi.python.org/pypi/dtale/)\n[![PyPI](https://img.shields.io/pypi/v/dtale)](https://pypi.org/project/dtale/)\n[![Conda](https://img.shields.io/conda/v/conda-forge/dtale)](https://anaconda.org/conda-forge/dtale)\n[![ReadTheDocs](https://readthedocs.org/projects/dtale/badge)](https://dtale.readthedocs.io)\n[![codecov](https://codecov.io/gh/man-group/dtale/branch/master/graph/badge.svg)](https://codecov.io/gh/man-group/dtale)\n[![Downloads](https://static.pepy.tech/badge/dtale)](https://pepy.tech/project/dtale)\n[![Open in VS Code](https://img.shields.io/badge/Visual_Studio_Code-0078D4?style=flat&logo=visual%20studio%20code&logoColor=white)](https://open.vscode.dev/man-group/dtale)\n\n## What is it?\n\nD-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures.  It integrates seamlessly with ipython notebooks & python/ipython terminals.  Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex.\n\n## Origins\n\nD-Tale was the product of a SAS to Python conversion.  What was originally a perl script wrapper on top of SAS's `insight` function is now a lightweight web client on top of Pandas data structures.\n\n## In The News\n\n - [4 Libraries that can perform EDA in one line of python code](https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae)\n - [React Status](https://react.statuscode.com/issues/204)\n - [KDNuggets](https://www.kdnuggets.com/2020/08/bring-pandas-dataframes-life-d-tale.html)\n - [Man Institute](https://www.man.com/maninstitute/d-tale) (warning: contains deprecated functionality)\n - [Python Bytes](https://pythonbytes.fm/episodes/show/169/jupyter-notebooks-natively-on-your-ipad)\n - [FlaskCon 2020](https://www.youtube.com/watch?v=BNgolmUWBp4&t=33s)\n - [San Diego Python](https://www.youtube.com/watch?v=fLsGur5YqeE&t=29s)\n - [Medium: towards data science](https://towardsdatascience.com/introduction-to-d-tale-5eddd81abe3f)\n - [Medium: Exploratory Data Analysis \u2013 Using D-Tale](https://medium.com/da-tum/exploratory-data-analysis-1-4-using-d-tale-99a2c267db79)\n - [EOD Notes: Using python and dtale to analyze correlations](https://www.google.com/amp/s/eod-notes.com/2020/05/07/using-python-and-dtale-to-analyze-correlations/amp/)\n - [Data Exploration is Now Super Easy w/ D-Tale](https://dibyendudeb.com/d-tale-data-exploration-tool/)\n - [Practical Business Python](https://pbpython.com/dataframe-gui-overview.html)\n\n## Tutorials\n\n - [Pip Install Python YouTube Channel](https://m.youtube.com/watch?v=0RihZNdQc7k&feature=youtu.be)\n - [machine_learning_2019](https://www.youtube.com/watch?v=-egtEUVBy9c)\n - [D-Tale The Best Library To Perform Exploratory Data Analysis Using Single Line Of Code\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25](https://www.youtube.com/watch?v=xSXGcuiEzUc)\n - [Explore and Analyze Pandas Data Structures w/ D-Tale](https://m.youtube.com/watch?v=JUu5IYVGqCg)\n - [Data Preprocessing simplest method \ud83d\udd25](https://www.youtube.com/watch?v=Q2kMNPKgN4g)\n\n ## Related Resources\n\n - [Adventures In Flask While Developing D-Tale](https://github.com/man-group/dtale/blob/master/docs/FlaskCon/FlaskAdventures.md)\n - [Adding Range Selection to react-virtualized](https://github.com/man-group/dtale/blob/master/docs/RANGE_SELECTION.md)\n - [Building Draggable/Resizable Modals](https://github.com/man-group/dtale/blob/master/docs/DRAGGABLE_RESIZABLE_MODALS.md)\n - [Embedding Flask Apps within Streamlit](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_STREAMLIT.md)\n\n## Contents\n\n- [Where To Get It](#where-to-get-it)\n- [Getting Started](#getting-started)\n  - [Python Terminal](#python-terminal)\n  - [As A Script](#as-a-script)\n  - [Jupyter Notebook](#jupyter-notebook)\n  - [Jupyterhub w/ Jupyter Server Proxy](#jupyterhub-w-jupyter-server-proxy)\n  - [Jupyterhub w/ Kubernetes](https://github.com/man-group/dtale/blob/master/docs/JUPYTERHUB_KUBERNETES.md)\n  - [Docker Container](#docker-container)\n  - [Google Colab](#google-colab)\n  - [Kaggle](#kaggle)\n  - [Binder](#binder)\n  - [R with Reticulate](#r-with-reticulate)\n  - [Startup with No Data](#startup-with-no-data)\n  - [Command-line](#command-line)\n  - [Custom Command-line Loaders](#custom-command-line-loaders)\n  - [Embedding Within Your Own Flask App](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_FLASK.md)\n  - [Embedding Within Your Own Django App](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_DJANGO.md)\n  - [Embedding Within Streamlit](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_DTALE_STREAMLIT.md)\n  - [Running D-Tale On Gunicorn w/ Redis](https://github.com/man-group/dtale/blob/master/docs/GUNICORN_REDIS.md)\n  - [Configuration](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md)\n  - [Authentication](#authentication)\n  - [Predefined Filters](#predefined-filters)\n  - [Using Swifter](#using-swifter)\n  - [Behavior for Wide Dataframes](#behavior-for-wide-dataframes)\n- [UI](#ui)\n  - [Dimensions/Ribbon Menu/Main Menu](#dimensionsribbon-menumain-menu)\n  - [Header](#header)\n  - [Resize Columns](#resize-columns)\n  - [Editing Cells](#editing-cells)\n  - [Copy Cells Into Clipboard](#copy-cells-into-clipboard)\n  - [Main Menu Functions](#main-menu-functions)\n    - [XArray Operations](#xarray-operations), [Describe](#describe), [Outlier Detection](#outlier-detection), [Custom Filter](#custom-filter), [Dataframe Functions](#dataframe-functions), [Merge & Stack](#merge--stack), [Summarize Data](#summarize-data), [Duplicates](#duplicates), [Missing Analysis](#missing-analysis), [Correlations](#correlations), [Predictive Power Score](#predictive-power-score), [Heat Map](#heat-map), [Highlight Dtypes](#highlight-dtypes), [Highlight Missing](#highlight-missing), [Highlight Outliers](#highlight-outliers), [Highlight Range](#highlight-range), [Low Variance Flag](#low-variance-flag), [Instances](#instances), [Code Exports](#code-exports), [Export CSV](#export-csv), [Load Data & Sample Datasets](#load-data--sample-datasets), [Refresh Widths](#refresh-widths), [About](#about), [Theme](#theme), [Reload Data](#reload-data), [Unpin/Pin Menu](#unpinpin-menu), [Language](#language), [Shutdown](#shutdown)\n  - [Column Menu Functions](#column-menu-functions)\n    - [Filtering](#filtering), [Moving Columns](#moving-columns), [Hiding Columns](#hiding-columns), [Delete](#delete), [Rename](#rename), [Replacements](#replacements), [Lock](#lock), [Unlock](#unlock), [Sorting](#sorting), [Formats](#formats), [Describe (Column Analysis)](#describe-column-analysis)\n  - [Charts](#charts)\n  - [Network Viewer](#network-viewer)\n  - [Hotkeys](#hotkeys)\n  - [Menu Functions Depending on Browser Dimensions](#menu-functions-depending-on-browser-dimensions)\n- [For Developers](#for-developers)\n  - [Cloning](#cloning)\n  - [Running Tests](#running-tests)\n  - [Linting](#linting)\n  - [Formatting JS](#formatting-js)\n  - [Docker Development](#docker-development)\n  - [Adding Language Support](#adding-language-support)\n- [Global State/Data Storage](https://github.com/man-group/dtale/blob/master/docs/GLOBAL_STATE.md)\n- [Startup Behavior](#startup-behavior)\n- [Documentation](#documentation)\n- [Dependencies](#dependencies)\n- [Acknowledgements](#acknowledgements)\n- [License](#license)\n\n## Where To get It\nThe source code is currently hosted on GitHub at:\nhttps://github.com/man-group/dtale\n\nBinary installers for the latest released version are available at the [Python\npackage index](https://pypi.org/project/dtale) and on conda using [conda-forge](https://github.com/conda-forge/dtale-feedstock).\n\n```sh\n# conda\nconda install dtale -c conda-forge\n# if you want to also use \"Export to PNG\" for charts\nconda install -c plotly python-kaleido\n```\n\n```sh\n# or PyPI\npip install dtale\n```\n\n## Getting Started\n\n|PyCharm|jupyter|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/dtale_demo_mini.gif)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/dtale_ipython.gif)|\n\n### Python Terminal\nThis comes courtesy of PyCharm\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Python_Terminal.png)\nFeel free to invoke `python` or `ipython` directly and use the commands in the screenshot above and it should work\n\n#### Issues With Windows Firewall\n\nIf you run into issues with viewing D-Tale in your browser on Windows please try making Python public under \"Allowed Apps\" in your Firewall configuration.  Here is a nice article:\n[How to Allow Apps to Communicate Through the Windows Firewall](https://www.howtogeek.com/howto/uncategorized/how-to-create-exceptions-in-windows-vista-firewall/)\n\n#### Additional functions available programmatically\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame([dict(a=1,b=2,c=3)])\n\n# Assigning a reference to a running D-Tale process.\nd = dtale.show(df)\n\n# Accessing data associated with D-Tale process.\ntmp = d.data.copy()\ntmp['d'] = 4\n\n# Altering data associated with D-Tale process\n# FYI: this will clear any front-end settings you have at the time for this process (filter, sorts, formatting)\nd.data = tmp\n\n# Shutting down D-Tale process\nd.kill()\n\n# Using Python's `webbrowser` package it will try and open your server's default browser to this process.\nd.open_browser()\n\n# There is also some helpful metadata about the process.\nd._data_id  # The process's data identifier.\nd._url  # The url to access the process.\n\nd2 = dtale.get_instance(d._data_id)  # Returns a new reference to the instance running at that data_id.\n\ndtale.instances()  # Prints a list of all ids & urls of running D-Tale sessions.\n\n```\n\n#### Duplicate data check\nTo help guard against users loading the same data to D-Tale multiple times and thus eating up precious memory, we have a loose check for duplicate input data.  The check runs the following:\n * Are row & column count the same as a previously loaded piece of data?\n * Are the names and order of columns the same as a previously loaded piece of data?\n\nIf both these conditions are true then you will be presented with an error and a link to the previously loaded data.  Here is an example of how the interaction looks:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Duplicate_data.png)\n\n\n### As A Script\n\nD-Tale can be run as script by adding `subprocess=False` to your `dtale.show` command.  Here is an example script:\n```python\nimport dtale\nimport pandas as pd\n\nif __name__ == '__main__':\n      dtale.show(pd.DataFrame([1,2,3,4,5]), subprocess=False)\n```\n\n### Jupyter Notebook\nWithin any jupyter (ipython) notebook executing a cell like this will display a small instance of D-Tale in the output cell.  Here are some examples:\n\n|`dtale.show`|assignment|instance|\n|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython3.png)|\n\nIf you are running ipython<=5.0 then you also have the ability to adjust the size of your output cell for the most recent instance displayed:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython_adjust.png)\n\nOne thing of note is that a lot of the modal popups you see in the standard browser version will now open separate browser windows for spacial convienence:\n\n|Column Menus|Correlations|Describe|Column Analysis|Instances|\n|:------:|:------:|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Column_menu.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/correlations_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/describe_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/histogram_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/instances_popup.png)|\n\n### JupyterHub w/ Jupyter Server Proxy\n\nJupyterHub has an extension that allows to proxy port for user, [JupyterHub Server Proxy](https://github.com/jupyterhub/jupyter-server-proxy)\n\nTo me it seems like this extension might be the best solution to getting D-Tale running within kubernetes.  Here's how to use it:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.JUPYTER_SERVER_PROXY = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nNotice the command `dtale_app.JUPYTER_SERVER_PROXY = True` this will make sure that any D-Tale instance will be served with the jupyter server proxy application root prefix:\n\n`/user/{jupyter username}/proxy/{dtale instance port}/`\n\nOne thing to note is that if you try to look at the `_main_url` of your D-Tale instance in your notebook it will not include the hostname or port:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.JUPYTER_SERVER_PROXY = True\n\nd = dtale.show(pd.DataFrame([1,2,3]))\nd._main_url # /user/johndoe/proxy/40000/dtale/main/1\n```\n\n This is because it's very hard to promgramatically figure out the host/port that your notebook is running on.  So if you want to look at `_main_url` please be sure to preface it with:\n \n `http[s]://[jupyterhub host]:[jupyterhub port]`\n\nIf for some reason jupyterhub changes their API so that the application root changes you can also override D-Tale's application root by using the `app_root` parameter to the `show()` function:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale.show(pd.DataFrame([1,2,3]), app_root='/user/johndoe/proxy/40000/`)\n```\n\nUsing this parameter will only apply the application root to that specific instance so you would have to include it on every call to `show()`.\n\n### JupyterHub w/ Kubernetes\n\nPlease read this [post](https://github.com/man-group/dtale/blob/master/docs/JUPYTERHUB_KUBERNETES.md)\n\n### Docker Container\n\nIf you have D-Tale installed within your docker container please add the following parameters to your `docker run` command.\n\n**On a Mac**:\n\n```sh\ndocker run -h `hostname` -p 40000:40000\n```\n\n* `-h` this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs\n* `-p` access to port 40000 which is the default port for running D-Tale\n\n**On Windows**:\n\n```sh\ndocker run -p 40000:40000\n```\n\n* `-p` access to port 40000 which is the default port for running D-Tale\n* D-Tale URL will be http://127.0.0.1:40000/\n\n**Everything Else**:\n\n```sh\ndocker run -h `hostname` --network host\n```\n\n* `-h` this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs\n* `--network host` this will allow access to as many ports as needed for running D-Tale processes\n\n### Google Colab\n\nThis is a hosted notebook site and thanks to Colab's internal function `google.colab.output.eval_js` & the JS function `google.colab.kernel.proexyPort` users can run D-Tale within their notebooks.\n\n**DISCLAIMER:** It is important that you set `USE_COLAB` to true when using D-Tale within this service.  Here is an example:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.USE_COLAB = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nIf this does not work for you try using `USE_NGROK` which is described in the next section.\n\n### Kaggle\n\nThis is yet another hosted notebook site and thanks to the work of [flask_ngrok](https://github.com/gstaff/flask-ngrok) users can run D-Tale within their notebooks.\n\n**DISCLAIMER:** It is import that you set `USE_NGROK` to true when using D-Tale within this service.  Here is an example:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.USE_NGROK = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nHere are some video tutorials of each:\n\n|Service|Tutorial|Addtl Notes|\n|:------:|:------:|:------:|\n|Google Colab|[![](http://img.youtube.com/vi/pOYl2M1clIw/0.jpg)](http://www.youtube.com/watch?v=pOYl2M1clIw \"Google Colab\")||\n|Kaggle|[![](http://img.youtube.com/vi/8Il-2HHs2Mg/0.jpg)](http://www.youtube.com/watch?v=8Il-2HHs2Mg \"Kaggle\")|make sure you switch the \"Internet\" toggle to \"On\" under settings of your notebook so you can install the egg from pip|\n\nIt is important to note that using NGROK will limit you to 20 connections per mintue so if you see this error:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ngrok_limit.png)\n\nWait a little while and it should allow you to do work again.  I am actively working on finding a more sustainable solution similar to what I did for google colab. :pray:\n\n### Binder\n\nI have built a repo which shows an example of how to run D-Tale within Binder [here](https://github.com/aschonfeld/dtale-binder).\n\nThe important take-aways are:\n* you must have `jupyter-server-proxy` installed\n* look at the `environment.yml` file to see how to add it to your environment\n* look at the `postBuild` file for how to activate it on startup\n\n\n### R with Reticulate\n\nI was able to get D-Tale running in R using reticulate. Here is an example:\n\n```\nlibrary('reticulate')\ndtale <- import('dtale')\ndf <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/boot/acme.csv')\ndtale$show(df, subprocess=FALSE, open_browser=TRUE)\n```\n\nNow the problem with doing this is that D-Tale is not running as a subprocess so it will block your R console and you'll lose out the following functions:\n - manipulating the state of your data from your R console\n - adding more data to D-Tale\n\n`open_browser=TRUE` isn't required and won't work if you don't have a default browser installed on your machine. If you don't use that parameter simply copy & paste the URL that gets printed to your console in the browser of your choice.\n\nI'm going to do some more digging on why R doesn't seem to like using python subprocesses (not sure if it something with how reticulate manages the state of python) and post any findings to this thread.\n\nHere's some helpful links for getting setup:\n\nreticulate\n\ninstalling python packages\n\n### Startup with No Data\n\nIt is now possible to run D-Tale with no data loaded up front. So simply call `dtale.show()` and this will start the application for you and when you go to view it you will be presented with a screen where you can upload either a CSV or TSV file for data.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/no_data.png)\n\nOnce you've loaded a file it will take you directly to the standard data grid comprised of the data from the file you loaded.  This might make it easier to use this as an on demand application within a container management system like kubernetes. You start and stop these on demand and you'll be presented with a new instance to load any CSV or TSV file to!\n\n### Command-line\nBase CLI options (run `dtale --help` to see all options available)\n\n|Prop     |Description|\n|:--------|:-----------|\n|`--host` |the name of the host you would like to use (most likely not needed since `socket.gethostname()` should figure this out)|\n|`--port` |the port you would like to assign to your D-Tale instance|\n|`--name` |an optional name you can assign to your D-Tale instance (this will be displayed in the `<title>` & Instances popup)|\n|`--debug`|turn on Flask's \"debug\" mode for your D-Tale instance|\n|`--no-reaper`|flag to turn off auto-reaping subprocess (kill D-Tale instances after an hour of inactivity), good for long-running displays |\n|`--open-browser`|flag to automatically open up your server's default browser to your D-Tale instance|\n|`--force`|flag to force D-Tale to try an kill any pre-existing process at the port you've specified so it can use it|\n\nLoading data from [**ArcticDB**(high performance, serverless DataFrame database)](https://github.com/man-group/ArcticDB) (this requires either installing **arcticdb** or **dtale[arcticdb]**)\n```bash\ndtale --arcticdb-uri lmdb:///<path> --arcticdb-library jdoe.my_lib --arcticdb-symbol my_symbol\n```\nIf you would like to change your storage mechanism to ArcticDB then add the `--arcticdb-use_store` flag\n```bash\ndtale --arcticdb-uri lmdb:///<path> --arcticdb-library my_lib --arcticdb-symbol my_symbol --arcticdb-use_store\n```\nLoading data from [**arctic**(high performance datastore for pandas dataframes)](https://github.com/man-group/arctic) (this requires either installing **arctic** or **dtale[arctic]**)\n```bash\ndtale --arctic-uri mongodb://localhost:27027 --arctic-library my_lib --arctic-symbol my_symbol --arctic-start 20130101 --arctic-end 20161231\n```\nLoading data from **CSV**\n```bash\ndtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date\n```\nLoading data from **EXCEL**\n```bash\ndtale --excel-path /home/jdoe/my_csv.xlsx --excel-parse_dates date\ndtale --excel-path /home/jdoe/my_csv.xls --excel-parse_dates date\n```\nLoading data from **JSON**\n```bash\ndtale --json-path /home/jdoe/my_json.json --json-parse_dates date\n```\nor\n```bash\ndtale --json-path http://json-endpoint --json-parse_dates date\n```\nLoading data from **R Datasets**\n```bash\ndtale --r-path /home/jdoe/my_dataset.rda\n```\nLoading data from **SQLite DB Files**\n```bash\ndtale --sqlite-path /home/jdoe/test.sqlite3 --sqlite-table test_table\n```\n\n### Custom Command-line Loaders\n\nLoading data from a **Custom** loader\n- Using the DTALE_CLI_LOADERS environment variable, specify a path to a location containing some python modules\n- Any python module containing the global variables LOADER_KEY & LOADER_PROPS will be picked up as a custom loader\n  - LOADER_KEY: the key that will be associated with your loader.  By default you are given **arctic** & **csv** (if you use one of these are your key it will override these)\n  - LOADER_PROPS: the individual props available to be specified.\n    - For example, with arctic we have host, library, symbol, start & end.\n    - If you leave this property as an empty list your loader will be treated as a flag.  For example, instead of using all the arctic properties we would simply specify `--arctic` (this wouldn't work well in arctic's case since it depends on all those properties)\n- You will also need to specify a function with the following signature `def find_loader(kwargs)` which returns a function that returns a dataframe or `None`\n- Here is an example of a custom loader:\n```python\nfrom dtale.cli.clickutils import get_loader_options\n\n'''\n  IMPORTANT!!! This global variable is required for building any customized CLI loader.\n  When find loaders on startup it will search for any modules containing the global variable LOADER_KEY.\n'''\nLOADER_KEY = 'testdata'\nLOADER_PROPS = ['rows', 'columns']\n\n\ndef test_data(rows, columns):\n    import pandas as pd\n    import numpy as np\n    import random\n    from past.utils import old_div\n    from pandas.tseries.offsets import Day\n    from dtale.utils import dict_merge\n    import string\n\n    now = pd.Timestamp(pd.Timestamp('now').date())\n    dates = pd.date_range(now - Day(364), now)\n    num_of_securities = max(old_div(rows, len(dates)), 1)  # always have at least one security\n    securities = [\n        dict(security_id=100000 + sec_id, int_val=random.randint(1, 100000000000),\n             str_val=random.choice(string.ascii_letters) * 5)\n        for sec_id in range(num_of_securities)\n    ]\n    data = pd.concat([\n        pd.DataFrame([dict_merge(dict(date=date), sd) for sd in securities])\n        for date in dates\n    ], ignore_index=True)[['date', 'security_id', 'int_val', 'str_val']]\n\n    col_names = ['Col{}'.format(c) for c in range(columns)]\n    return pd.concat([data, pd.DataFrame(np.random.randn(len(data), columns), columns=col_names)], axis=1)\n\n\n# IMPORTANT!!! This function is required for building any customized CLI loader.\ndef find_loader(kwargs):\n    test_data_opts = get_loader_options(LOADER_KEY, LOADER_PROPS, kwargs)\n    if len([f for f in test_data_opts.values() if f]):\n        def _testdata_loader():\n            return test_data(int(test_data_opts.get('rows', 1000500)), int(test_data_opts.get('columns', 96)))\n\n        return _testdata_loader\n    return None\n```\nIn this example we simplying building a dataframe with some dummy data based on dimensions specified on the command-line:\n- `--testdata-rows`\n- `--testdata-columns`\n\nHere's how you would use this loader:\n```bash\nDTALE_CLI_LOADERS=./path_to_loaders bash -c 'dtale --testdata-rows 10 --testdata-columns 5'\n```\n\n### Authentication\n\nYou can choose to use optional authentication by adding the following to your D-Tale `.ini` file ([directions here](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md)):\n\n```ini\n[auth]\nactive = True\nusername = johndoe\npassword = 1337h4xOr\n```\n\nOr you can call the following:\n\n```python\nimport dtale.global_state as global_state\n\nglobal_state.set_auth_settings({'active': True, 'username': 'johndoe', 'password': '1337h4x0r'})\n```\n\nIf you have done this before initially starting D-Tale it will have authentication applied.  If you are adding this after starting D-Tale you will have to kill your service and start it over.\n\nWhen opening your D-Tale session you will be presented with a screen like this:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/login.png)\n\nFrom there you can enter the credentials you either set in your `.ini` file or in your call to `dtale.global_state.set_auth_settings` and you will be brought to the main grid as normal.  You will now have an additional option in your main menu to logout:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/logout.png)\n\n### Instance Settings\n\nUsers can set front-end properties on their instances programmatically in the `dtale.show` function or by calling the `update_settings` function on their instance.  For example:\n\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame(dict(\n  a=[1,2,3,4,5],\n  b=[6,7,8,9,10],\n  c=['a','b','c','d','e']\n))\ndtale.show(\n  df,\n  locked=['c'],\n  column_formats={'a': {'fmt': '0.0000'}},\n  nan_display='...',\n  background_mode='heatmap-col',\n  sort=[('a','DESC')],\n  vertical_headers=True,\n)\n```\n\nor\n\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame(dict(\n  a=[1,2,3,4,5],\n  b=[6,7,8,9,10],\n  c=['a','b','c','d','e']\n))\nd = dtale.show(\n  df\n)\nd.update_settings(\n  locked=['c'],\n  column_formats={'a': {'fmt': '0.0000'}},\n  nan_display='...',\n  background_mode='heatmap-col',\n  sort=[('a','DESC')],\n  vertical_headers=True,\n)\nd\n```\n\nHere's a short description of each instance setting available:\n\n#### show_columns\nA list of column names you would like displayed in your grid. Anything else will be hidden.\n\n#### hide_columns\nA list of column names you would like initially hidden from the grid display.\n\n#### column_formats\nA dictionary of column name keys and their front-end display configuration. Here are examples of the different format configurations:\n* Numeric: `{'fmt': '0.00000'}`\n* String:\n  * `{'fmt': {'truncate': 10}}` truncate string values to no more than 10 characters followed by an ellipses \n  * `{'fmt': {'link': True}}` if your strings are URLs convert them to clickable links\n  * `{'fmt': {'html': True}}` if your strings are HTML fragments render them as HTML\n* Date: `{'fmt': 'MMMM Do YYYY, h:mm:ss a'}` uses [Moment.js formatting](https://momentjs.com/docs/#/displaying/format/)\n\n#### nan_display\nConverts any `nan` values in your dataframe to this when it is sent to the browser (doesn't actually change the state of your dataframe)\n\n#### sort\nList of tuples which sort your dataframe (EX: `[('a', 'ASC'), ('b', 'DESC')]`)\n\n#### locked\nList of column names which will be locked to the right side of your grid while you scroll to the left.\n\n#### background_mode\nA string denoting one of the many background displays available in D-Tale. Options are:\n* heatmap-all: turn on heatmap for all numeric columns where the colors are determined by the range of values over all numeric columns combined\n* heatmap-col: turn on heatmap for all numeric columns where the colors are determined by the range of values in the column\n* heatmap-col-[column name]: turn on heatmap highlighting for a specific column\n* dtypes: highlight columns based on it's data type\n* missing: highlight any missing values (np.nan, empty strings, strings of all spaces)\n* outliers: highlight any outliers\n* range: highlight values for any matchers entered in the \"range_highlights\" option\n* lowVariance: highlight values with a low variance\n\n#### range_highlights\nDictionary of column name keys and range configurations which if the value for that column exists then it will be shaded that color.  Here is an example input:\n```\n'a': {\n  'active': True,\n  'equals': {'active': True, 'value': 3, 'color': {'r': 255, 'g': 245, 'b': 157, 'a': 1}}, # light yellow\n  'greaterThan': {'active': True, 'value': 3, 'color': {'r': 80, 'g': 227, 'b': 194, 'a': 1}}, # mint green\n  'lessThan': {'active': True, 'value': 3, 'color': {'r': 245, 'g': 166, 'b': 35, 'a': 1}}, # orange\n}\n```\n\n#### vertical_headers\nIf set to `True` then the headers in your grid will be rotated 90 degrees vertically to conserve width.\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/vertical_headers.png)\n\n\n### Predefined Filters\n\nUsers can build their own custom filters which can be used from the front-end using the following code snippet:\n```python\nimport pandas as pd\nimport dtale\nimport dtale.predefined_filters as predefined_filters\nimport dtale.global_state as global_state\n\nglobal_state.set_app_settings(dict(open_predefined_filters_on_startup=True))\n\npredefined_filters.set_filters([\n    {\n        \"name\": \"A and B > 2\",\n        \"column\": \"A\",\n        \"description\": \"Filter A with B greater than 2\",\n        \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] > 2)],\n        \"input_type\": \"input\",\n        \"default\": 1,\n        \"active\": False,\n    },\n    {\n        \"name\": \"A and (B % 2) == 0\",\n        \"column\": \"A\",\n        \"description\": \"Filter A with B mod 2 equals zero (is even)\",\n        \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] % 2 == 0)],\n        \"input_type\": \"select\",\n        \"default\": 1,\n        \"active\": False,\n    },\n    {\n        \"name\": \"A in values and (B % 2) == 0\",\n        \"column\": \"A\",\n        \"description\": \"A is within a group of values and B mod 2 equals zero (is even)\",\n        \"handler\": lambda df, val: df[df[\"A\"].isin(val) & (df[\"B\"] % 2 == 0)],\n        \"input_type\": \"multiselect\",\n        \"default\": [1],\n        \"active\": True,\n    }\n])\n\ndf = pd.DataFrame(\n    ([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]]),\n    columns=['A', 'B', 'C', 'D', 'E', 'F']\n)\ndtale.show(df)\n```\n\nThis code illustrates the types of inputs you can have on the front end:\n* __input__: just a simple text input box which users can enter any value they want (if the value specified for `\"column\"` is an int or float it will try to convert the string to that data type) and it will be passed to the handler\n* __select__: this creates a dropdown populated with the unique values of `\"column\"` (an asynchronous dropdown if the column has a large amount of unique values)\n* __multiselect__: same as \"select\" but it will allow you to choose multiple values (handy if you want to perform an `isin` operation in your filter)\n\nHere is a demo of the functionality:\n[![](http://img.youtube.com/vi/8mryVxpxjM4/0.jpg)](http://www.youtube.com/watch?v=8mryVxpxjM4 \"Predefined Filters\")\n\nIf there are any new types of inputs you would like available please don't hesitate to submit a request on the \"Issues\" page of the repo.\n\n### Using Swifter\n\nSwifter is a package which will increase performance on any `apply()` function on a pandas series or dataframe.  If install the package in your virtual environment\n```sh\npip install swifter\n# or\npip install dtale[swifter]\n```\n\nIt will be used for the following operations:\n- Standard dataframe formatting in the main grid & chart display\n- Column Builders\n  - Type Conversions\n    - string hex -> int or float\n    - int or float -> hex\n    - mixed -> boolean\n    - int -> timestamp\n    - date -> int\n  - Similarity Distance Calculation\n- Handling of empty strings when calculating missing counts\n- Building unique values by data type in \"Describe\" popup\n\n\n### Behavior for Wide Dataframes\n\nThere is currently a performance bottleneck on the front-end when loading \"wide dataframes\" (dataframes with many columns). The current solution to this problem is that upon initial load of these dataframes to D-Tale any column with an index greater than 100 (going from left to right) will be hidden on the front-end.  You can still unhide these columns the same way you would any other and you still have the option to show all columns using the \"Describe\" popup. Here's a sample of this behavior:\n\nSay you loaded this dataframe into D-Tale.\n\n```python\nimport pandas as pd\nimport dtale\n\ndtale.show(pd.DataFrame(\n    {'col{}'.format(i): list(range(1000)) for i in range(105)}\n))\n```\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/initial_load.png)\n\nYou will now have access to a new \"Jump To Column\" menu item.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/jump_to_col_menu_item.png)\n\nIt would be too hard to scroll to the column you're looking for. So now you'll be able to type in the name of the column you're looking for and select it.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/jump_to_col_popup.png)\n\nAnd now you'll see only the columns you've had locked (we've locked no columns in this example) and the column you chose to jump to.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/jump_to_col_done.png)\n\n### Accessing CLI Loaders in Notebook or Console\nI am pleased to announce that all CLI loaders will be available within notebooks & consoles.  Here are some examples (the last working if you've installed `dtale[arctic]`):\n- `dtale.show_csv(path='test.csv', parse_dates=['date'])`\n- `dtale.show_csv(path='http://csv-endpoint', index_col=0)`\n- `dtale.show_excel(path='test.xlsx', parse_dates=['date'])`\n- `dtale.show_excel(path='test.xls', sheet=)`\n- `dtale.show_excel(path='http://excel-endpoint', index_col=0)`\n- `dtale.show_json(path='http://json-endpoint', parse_dates=['date'])`\n- `dtale.show_json(path='test.json', parse_dates=['date'])`\n- `dtale.show_r(path='text.rda')`\n- `dtale.show_arctic(uri='uri', library='library', symbol='symbol', start='20200101', end='20200101')`\n- `dtale.show_arcticdb(uri='uri', library='library', symbol='symbol', start='20200101', end='20200101', use_store=True)`\n\n### Using ArcticDB as your data store for D-Tale\nSo one of the major drawbacks of using D-Tale is that it stores a copy of your dataframe in memory (unless you specify the `inplace=True` when calling `dtale.show`). One way around this is to switch your storage mechanism to ArcticDB. This will use ArcticDB's [QueryBuilder](https://docs.arcticdb.io/api/query_builder) to perform all data loading and filtering.  This will significantly drop your memory footprint, but it will remove a lot of the original D-Tale functionality:\n- Custom Filtering\n- Range filtering in Numeric Column Filters\n- Regex filtering on String Column Filters\n- Editing Cells\n- Data Reshaping\n- Dataframe Functions\n- Drop Filtered Rows\n- Sorting\n\nIf the symbol you're loading from ArcticDB contains more than 1,000,000 rows then you will also lose the following:\n- Column Filtering using dropdowns of unique values (you'll have to manually type your values)\n- Outlier Highlighting\n- Most of the details in the \"Describe\" screen\n\nIn order to update your storage mechanism there are a few options, the first being `use_arcticdb_store`:\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>')\ndtale.show_arcticdb(library='my_lib', symbol='my_symbol')\n```\n\nOr you can set your library ahead of time so you can use `dtale.show`:\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>', library='my_lib')\ndtale.show('my_symbol')\n```\n\nOr you can set your library using `dtale.show` with a pipe-delimited identifier:\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>')\ndtale.show('my_lib|my_symbol')\n```\n\nYou can also do everything using `dtale.show_arcticdb`:\n```python\nimport dtale\n\ndtale.show_arcticdb(uri='lmdb:///<path>', library='my_lib', symbol='my_symbol', use_store=True)\n```\n\n### Navigating to different libraries/symbols in your ArcticDB database\n\nWhen starting D-Tale with no data\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>')\ndtale.show()\n```\n\nyou'll be presented with this screen on startup\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/arcticdb/select_library_and_symbol.png)\n\nOnce you choose a library and a symbol you can click \"Load\" and it will bring you to the main grid comprised of the data for that symbol.\n\nYou can also view information about the symbol you've selected before loading it by clicking the \"Info\" button\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/arcticdb/description.png)\n\n## UI\nOnce you have kicked off your D-Tale session please copy & paste the link on the last line of output in your browser\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Browser1.png)\n\n### Dimensions/Ribbon Menu/Main Menu\nThe information in the upper right-hand corner gives grid dimensions ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Info_cell.png)\n- lower-left => row count\n- upper-right => column count\n\nRibbon Menu\n- hovering around to top of your browser will display a menu items (similar to the ones in the main menu) across the top of the screen\n- to close the menu simply click outside the menu and/or dropdowns from the menu\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Ribbon_menu.png)\n\nMain Menu\n- clicking the triangle displays the menu of standard functions (click outside menu to close it)\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Info_menu_small.png)\n\n### Header\n\nThe header gives users an idea of what operations have taken place on your data (sorts, filters, hidden columns).  These values will be persisted across broswer instances.  So if you perform one of these operations and then send a link to one of your colleagues they will see the same thing :)\n\nNotice the \"X\" icon on the right of each display.  Clicking this will remove those operations.\n\nWhen performing multiple of the same operation the description will become too large to display so the display will truncate the description and if users click it they will be presented with a tooltip where you can crop individual operations.  Here are some examples:\n\n|Sorts|Filters|Hidden Columns|\n|-----|-------|--------------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/sorts.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/filters.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/hidden.PNG)|\n\n### Resize Columns\n\nCurrently there are two ways which you can resize columns.\n* Dragging the right border of the column's header cell.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/resize_columns_w_drag.gif)\n\n* Altering the \"Maximum Column Width\" property from the ribbon menu.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/resize_columns_max_width.gif)\n\n* __Side Note:__ You can also set the `max_column_width` property ahead of time in your [global configuration](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md) or programmatically using:\n\n```python\nimport dtale.global_state as global_state\n\nglobal_state.set_app_settings(dict(max_column_width=100))\n```\n\n### Editing Cells\n\nYou may edit any cells in your grid (with the exception of the row indexes or headers, the ladder can be edited using the [Rename](#rename) column menu function).\n\nIn order to edit a cell simply double-click on it.  This will convert it into a text-input field and you should see a blinking cursor.  In addition to turning that cell into an input it will also display an input at the top of the screen for better viewing of long strings. It is assumed that the value you type in will match the data type of the column you editing.  For example:\n\n* integers -> should be a valid positive or negative integer\n* float -> should be a valid positive or negative float\n* string -> any valid string will do\n* category -> either a pre-existing category or this will create a new category for (so beware!)\n* date, timestamp, timedelta -> should be valid string versions of each\n* boolean -> any string you input will be converted to lowercase and if it equals \"true\" then it will make the cell `True`, otherwise `False`\n\nUsers can make use of two protected values as well:\n\n* \"nan\" -> `numpy.nan`\n* \"inf\" -> `numpy.inf`\n\nTo save your change simply press \"Enter\" or to cancel your changes press \"Esc\".\n\nIf there is a conversion issue with the value you have entered it will display a popup with the specific exception in question.\n\nHere's a quick demo:\n\n[![](http://img.youtube.com/vi/MY5w0m_4IAc/0.jpg)](http://www.youtube.com/watch?v=MY5w0m_4IAc \"Editing Long String Cells\")\n\nHere's a demo of editing cells with long strings:\n\n[![](http://img.youtube.com/vi/3p9ltzdBaDQ/0.jpg)](http://www.youtube.com/watch?v=3p9ltzdBaDQ \"Editing Cells\")\n\n\n### Copy Cells Into Clipboard\n\n|Select|Copy|Paste|\n|:-----:|:-----:|:-----:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/select_range1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/select_range2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/select_range3.png)|\n\nOne request that I have heard time and time again while working on D-Tale is \"it would be great to be able to copy a range of cells into excel\".  Well here is how that is accomplished:\n1) Shift + Click on a cell\n2) Shift + Click on another cell (this will trigger a popup)\n3) Choose whether you want to include headers in your copy by clicking the checkbox\n4) Click Yes\n5) Go to your excel workbook and execute Ctrl + V or manually choose \"Paste\"\n  * You can also paste this into a standard text editor and what you're left with is tab-delimited data\n\n\n### Main Menu Functions\n\n#### XArray Operations\n\n* **Convert To XArray**: If you have are currently viewing a pandas dataframe in D-Tale you will be given this option to convert your data to an `xarray.Dataset`.  It is as simple as selecting one or many columns as an index and then your dataframe will be converted to a dataset (`df.set_index([...]).to_xarray()`) which makes toggling between indexes slices much easier.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/xarray_indexes.png)\n\n* **XArray Dimensions**: If you are currently viewing data associated with an `xarray.Dataset` you will be given the ability to toggle which dimension coordinates you're viewing by clicking this button.  You can select values for all, some or none (all data - no filter) of your coordinates and the data displayed in your grid will match your selections.  Under the hood the code being executed is as follows: `ds.sel(dim1=coord1,...).to_dataframe()`\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/xarray_dimensions.png)\n\n#### Describe\nView all the columns & their data types as well as individual details of each column\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe.png)\n\n|Data Type|Display|Notes|\n|--------|:------:|:------:|\n|date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_date.png)||\n|string|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_string.png)|If you have less than or equal to 100 unique values they will be displayed at the bottom of your popup|\n|int|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_int.png)|Anything with standard numeric classifications (min, max, 25%, 50%, 75%) will have a nice boxplot with the mean (if it exists) displayed as an outlier if you look closely.|\n|float|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_float.png)||\n\n#### Outlier Detection\nWhen viewing integer & float columns in the [\"Describe\" popup](#describe) you will see in the lower right-hand corner a toggle for Uniques & Outliers.\n\n|Outliers|Filter|\n|--------|------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/outliers.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/outlier_filter.png)|\n\nIf you click the \"Outliers\" toggle this will load the top 100 outliers in your column based on the following code snippet:\n```python\ns = df[column]\nq1 = s.quantile(0.25)\nq3 = s.quantile(0.75)\niqr = q3 - q1\niqr_lower = q1 - 1.5 * iqr\niqr_upper = q3 + 1.5 * iqr\noutliers = s[(s < iqr_lower) | (s > iqr_upper)]\n```\nIf you click on the \"Apply outlier filter\" link this will add an addtional \"outlier\" filter for this column which can be removed from the [header](#header) or the [custom filter](#custom-filter) shown in picture above to the right.\n\n#### Custom Filter\nApply a custom pandas `query` to your data (link to pandas documentation included in popup)  \n\n|Editing|Result|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Filter_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_filter.png)|\n\nYou can also see any outlier or column filters you've applied (which will be included in addition to your custom query) and remove them if you'd like.\n\nContext Variables are user-defined values passed in via the `context_variables` argument to dtale.show(); they can be referenced in filters by prefixing the variable name with '@'.\n\nFor example, here is how you can use context variables in a pandas query:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame([\n  dict(name='Joe', age=7),\n  dict(name='Bob', age=23),\n  dict(name='Ann', age=45),\n  dict(name='Cat', age=88),\n])\ntwo_oldest_ages = df['age'].nlargest(2)\ndf.query('age in @two_oldest_ages')\n```\nAnd here is how you would pass that context variable to D-Tale: `dtale.show(df, context_variables=dict(two_oldest_ages=two_oldest_ages))`\n\nHere's some nice documentation on the performance of [pandas queries](https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#pandas-eval-performance)\n\n#### Dataframe Functions\n\n[![](http://img.youtube.com/vi/G6wNS9-lG04/0.jpg)](http://www.youtube.com/watch?v=G6wNS9-lG04 \"Dataframe Functions in D-Tale\")\n\nThis video shows you how to build the following:\n - Numeric: adding/subtracting two columns or columns with static values\n - Bins: bucketing values using pandas cut & qcut as well as assigning custom labels\n - Dates: retrieving date properties (hour, weekday, month...) as well as conversions (month end)\n - Random: columns of data type (int, float, string & date) populated with random uniformly distributed values.\n  - Type Conversion: switch columns from one data type to another, fun. :smile:\n\n\n#### Merge & Stack\n\n[![](http://img.youtube.com/vi/ignDS6OaGVQ/0.jpg)](http://www.youtube.com/watch?v=ignDS6OaGVQ \"Merge & Stack\")\n\nThis feature allows users to merge or stack (vertically concatenate) dataframes they have loaded into D-Tale.  They can also upload additional data to D-Tale while wihin this feature.  The demo shown above goes over the following actions:\n- Editing of parameters to either a pandas merge or stack (vertical concatenation) of dataframes\n  - Viewing direct examples of each from the pandas documentation\n- Selection of dataframes\n- Uploading of additional dataframes from an excel file\n- Viewing code & resulting data from a merge or stack\n\n#### Summarize Data\n\nThis is very powerful functionality which allows users to create a new data from currently loaded data.  The operations currently available are:\n- **Aggregation**: consolidate data by running different aggregations on columns by a specific index\n- **Pivot**: this is simple wrapper around [pandas.Dataframe.pivot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html) and [pandas.pivot_table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html)\n- **Transpose**: transpose your data on a index (be careful dataframes can get very wide if your index has many unique values)\n\n|Function|Data|\n|:------:|:------:|\n|No Reshaping|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/original_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_col.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_col_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_func.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_func_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/pivot.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/pivot_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/transpose.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/transpose_data.png)|\n\n[![](http://img.youtube.com/vi/fYsxogXKZ2c/0.jpg)](http://www.youtube.com/watch?v=fYsxogXKZ2c \"Reshaping Tutorial\")\n\n#### Duplicates\nRemove duplicate columns/values from your data as well as extract duplicates out into separate instances.\n\nThe folowing screen shots are for a dataframe with the following data:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/data.png)\n\n|Function|Description|Preview|\n|:------:|:---------:|:-----:|\n|**Remove Duplicate Columns**|Remove any columns that contain the same data as another and you can either keep the first, last or none of these columns that match this criteria.  You can test which columns will be removed by clicking the \"View Duplicates\" button.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/columns.png)|\n|**Remove Duplicate Column Names**|Remove any columns with the same name (name comparison is case-insensitive) and you can either keep the first, last or none of these columns that match this criteria. You can test which columns will be removed by clicking the \"View Duplicates\" button.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/column_names.png)|\n|**Remove Duplicate Rows**|Remove any rows from your dataframe where the values of a subset of columns are considered duplicates. You can choose to keep the first, last or none of the rows considered duplicated.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/rows.png)|\n|**Show Duplicates**|Break any duplicate rows (based on a subset of columns) out into another dataframe viewable in your D-Tale session. You can choose to view all duplicates or select specific groups based on the duplicated value.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/show.png)|\n\n#### Missing Analysis\nDisplay charts analyzing the presence of missing (NaN) data in your dataset using the [missingno](https://github.com/ResidentMario/missingno) pacakage.  You can also open them in a tab by themselves or export them to a static PNG using the links in the upper right corner. You can also close the side panel using the ESC key.\n\n\n| Chart        | Sample |\n|--------------|--------|\n| Matrix     | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/matrix.png)|\n| Bar        | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/bar.png)|\n| Heatmap    | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/heatmap.png)|\n| Dendrogram | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/dendrogram.png)|\n\n#### Charts\nBuild custom charts based off your data(powered by [plotly/dash](https://github.com/plotly/dash)).\n \n - The Charts will open in a tab because of the fact there is so much functionality offered there you'll probably want to be able to reference the main grid data in the original tab\n - To build a chart you must pick a value for X & Y inputs which effectively drive what data is along the X & Y axes\n   - If you are working with a 3-Dimensional chart (heatmap, 3D Scatter, Surface) you'll need to enter a value for the Z axis as well\n - Once you have entered all the required axes a chart will be built\n - If your data along the x-axis (or combination of x & y in the case of 3D charts) has duplicates you have three options:\n   - Specify a group, which will create series for each group\n   - Specify an aggregation, you can choose from one of the following: Count, First, Last, Mean, Median, Minimum, Maximum, Standard Deviation, Variance, Mean Absolute Deviation, Product of All Items, Sum, Rolling\n     - Specifying a \"Rolling\" aggregation will also require a Window & a Computation (Correlation, Count, Covariance, Kurtosis, Maximum, Mean, Median, Minimum, Skew, Standard Deviation, Sum or Variance)\n     - For heatmaps you will also have access to the \"Correlation\" aggregation since viewing correlation matrices in heatmaps is very useful.  This aggregation is not supported elsewhere\n   - Specify both a group & an aggregation\n - You now have the ability to toggle between different chart types: line, bar, pie, wordcloud, heatmap, 3D scatter & surface\n - If you have specified a group then you have the ability between showing all series in one chart and breaking each series out into its own chart \"Chart per Group\"\n\nHere are some examples:\n\n|Chart Type|Chart|Chart per Group|\n|:------:|:------:|:------:|\n|line|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/line.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/line_pg.png)|\n|bar|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_pg.png)|\n|stacked|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/stacked.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/stacked_pg.png)|\n|pie|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/pie.png)||\n|wordcloud|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/wordcloud.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/wordcloud_pg.png)|\n|heatmap|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/heatmap.png)||\n|3D scatter|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/3d_scatter.png)||\n|surface|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/surface.png)||\n|Maps (Scatter GEO)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/scattergeo.png)||\n|Maps (Choropleth)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/choropleth.png)||\n\nY-Axis Toggling\n\nUsers now have the ability to toggle between 3 different behaviors for their y-axis display:\n- *Default*: selecting this option will use the default behavior that comes with plotly for your chart's y-axis\n- *Single*: this allows users to set the range of all series in your chart to be on the same basis as well as making that basis (min/max) editable\n- *Multi*: this allows users to give each series its own y-axis and making that axis' range editable\n\nHere's a quick tutorial: [![](http://img.youtube.com/vi/asblF-rAACY/0.jpg)](http://www.youtube.com/watch?v=asblF-rAACY \"Y-Axis Toggling\")\n\nAnd some screenshots:\n\n|Default|Single|Multi|\n|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/default.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/single.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/multi.PNG)|\n\nWith a bar chart that only has a single Y-Axis you have the ability to sort the bars based on the values for the Y-Axis\n\n|Pre-sort|Post-sort|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_presort.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_postsort.png)|\n\n**Popup Charts**\n\nViewing multiple charts at once and want to separate one out into its own window or simply move one off to the side so you can work on building another for comparison?  Well now you can by clicking the \"Popup\" button :smile:\n\n**Copy Link**\n\nWant to send what you're looking at to someone else?  Simply click the \"Copy Link\" button and it will save a pre-populated chart URL into your clipboard. As long as your D-Tale process is still running when that link is opened you will see your original chart.\n\n**Exporting Charts**\n\nYou can now export your dash charts (with the exception of Wordclouds) to static HTML files which can be emailed to others or saved down to be viewed at a later time.  The best part is that all of the javascript for plotly is embedded in these files so the nice zooming, panning, etc is still available! :boom:\n\n**Exporting CSV**\n\nI've been asked about being able to export the data that is contained within your chart to a CSV for further analysis in tools like Excel.  This button makes that possible.\n\n### OFFLINE CHARTS\n\nWant to run D-Tale in a jupyter notebook and build a chart that will still be displayed even after your D-Tale process has shutdown?  Now you can!  Here's an example code snippet show how to use it:\n\n```\nimport dtale\n\ndef test_data():\n    import random\n    import pandas as pd\n    import numpy as np\n\n    df = pd.DataFrame([\n        dict(x=i, y=i % 2)\n        for i in range(30)\n    ])\n    rand_data = pd.DataFrame(np.random.randn(len(df), 5), columns=['z{}'.format(j) for j in range(5)])\n    return pd.concat([df, rand_data], axis=1)\n\nd = dtale.show(test_data())\nd.offline_chart(chart_type='bar', x='x', y='z3', agg='sum')\n```\n[![](http://img.youtube.com/vi/DseSmc3fZvc/0.jpg)](http://www.youtube.com/watch?v=DseSmc3fZvc \"Offline Charts Tutorial\")\n\n**Pro Tip: If generating offline charts in jupyter notebooks and you run out of memory please add the following to your command-line when starting jupyter**\n\n`--NotebookApp.iopub_data_rate_limit=1.0e10`\n\n\n**Disclaimer: Long Running Chart Requests**\n\nIf you choose to build a chart that requires a lot of computational resources then it will take some time to run.  Based on the way Flask & plotly/dash interact this will block you from performing any other request until it completes.  There are two courses of action in this situation:\n\n1) Restart your jupyter notebook kernel or python console\n2) Open a new D-Tale session on a different port than the current session.  You can do that with the following command: `dtale.show(df, port=[any open port], force=True)`\n\nIf you miss the legacy (non-plotly/dash) charts, not to worry!  They are still available from the link in the upper-right corner, but on for a limited time...\nHere is the documentation for those: [Legacy Charts](https://github.com/man-group/dtale/blob/master/docs/LEGACY_CHARTS.md)\n\n**Your Feedback is Valuable**\n\nThis is a very powerful feature with many more features that could be offered (linked subplots, different statistical aggregations, etc...) so please submit issues :)\n\n#### Network Viewer\n\nThis tool gives users the ability to visualize directed graphs.  For the screenshots I'll beshowing for this functionality we'll be working off a dataframe with the following data:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/data.png)\n\nStart by selecting columns containing the \"To\" and \"From\" values for the nodes in you network and then click \"Load\":\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_to_from.png)\n\nYou can also see instructions on to interact with the network by expanding the directions section by clicking on the header \"Network Viewer\" at the top.  You can also view details about the network provided by the package [networkx](https://github.com/networkx) by clicking the header \"Network Analysis\".\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_expanded.png)\n\nSelect a column containing weighting for the edges of the nodes in the \"Weight\" column and click \"Load\":\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_weight.png)\n\nSelect a column containing group information for each node in the \"From\" column by populating \"Group\" and then clicking \"Load\":\n ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_group.png)\n\n Perform shortest path analysis by doing a Shift+Click on two nodes:\n ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_shortest_path.png)\n\nView direct descendants of each node by clicking on it:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_descendants.png)\n\nYou can zoom in on nodes by double-clicking and zoom back out by pressing \"Esc\".\n\n#### Correlations\nShows a pearson correlation matrix of all numeric columns against all other numeric columns\n  - By default, it will show a grid of pearson correlations (filtering available by using drop-down see 2nd table of screenshots)\n  - If you have a date-type column, you can click an individual cell and see a timeseries of pearson correlations for that column combination\n    - Currently if you have multiple date-type columns you will have the ability to toggle between them by way of a drop-down\n  - Furthermore, you can click on individual points in the timeseries to view the scatter plot of the points going into that correlation\n    - Within the scatter plot section you can also view the details of the PPS for those data points in the chart by hovering over the number next to \"PPS\"\n\n|Matrix|PPS|Timeseries|Scatter|\n|------|---|----------|-------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_pps.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_ts.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_scatter.png)|\n\n|Col1 Filtered|Col2 Filtered|Col1 & Col2 Filtered|\n|------|----------|-------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_col1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_col2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_both.png)|\n\nWhen the data being viewed in D-Tale has date or timestamp columns but for each date/timestamp vlaue there is only one row of data the behavior of the Correlations popup is a little different\n  - Instead of a timeseries correlation chart the user is given a rolling correlation chart which can have the window (default: 10) altered\n  - The scatter chart will be created when a user clicks on a point in the rollign correlation chart.  The data displayed in the scatter will be for the ranges of dates involved in the rolling correlation for that date.\n\n|Data|Correlations|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr_data.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr.png)|\n\n#### Predictive Power Score\nPredictive Power Score (using the package [ppscore](https://github.com/8080labs/ppscore)) is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). WARNING: This could take a while to load.\n\nThis page works similar to the [Correlations](#correlations) page but uses the PPS calcuation to populate the grid and by clicking on cells you can view the details of the PPS for those two columns in question.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr_data.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/pps.png)\n\n#### Heat Map\nThis will hide any non-float or non-int columns (with the exception of the index on the right) and apply a color to the background of each cell.\n\n  - Each float is renormalized to be a value between 0 and 1.0\n  - You have two options for the renormalization\n    - **By Col**: each value is calculated based on the min/max of its column\n    - **Overall**: each value is caluclated by the overall min/max of all the non-hidden float/int columns in the dataset\n  - Each renormalized value is passed to a color scale of red(0) - yellow(0.5) - green(1.0)\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Heatmap.png)\n\nTurn off Heat Map by clicking menu option you previously selected one more time\n\n#### Highlight Dtypes\nThis is a quick way to check and see if your data has been categorized correctly.  By clicking this menu option it will assign a specific background color to each column of a specific data type.\n\n|category|timedelta|float|int|date|string|bool|\n|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n|purple|orange|green|light blue|pink|white|yellow\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_dtypes.png)\n\n#### Highlight Missing\n\n* Any cells which contain `nan` values will be highlighted in yellow.\n* Any string column cells which are empty strings or strings consisting only of spaces will be highlighted in orange.\n*  \u2757will be prepended to any column header which contains missing values.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_missing.png)\n\n#### Highlight Outliers\nHighlight any cells for numeric columns which surpass the upper or lower bounds of a [custom outlier computation](#outlier-detection). \n* Lower bounds outliers will be on a red scale, where the darker reds will be near the maximum value for the column.\n* Upper bounds outliers will be on a blue scale, where the darker blues will be closer to the minimum value for the column.\n* \u2b50 will be prepended to any column header which contains outliers.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_outliers.png)\n\n#### Highlight Range\nHighlight any range of numeric cells based on three different criteria:\n* equals\n* greater than\n* less than\n\nYou can activate as many of these criteria as you'd like nad they will be treated as an \"or\" expression.  For example, `(x == 0) or (x < -1) or (x > 1)`\n\n|Selections|Output|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_range_selections.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_range_output.png)|\n\n#### Low Variance Flag\nShow flags on column headers where both these conditions are true:\n* Count of unique values / column size < 10%\n* Count of most common value / Count of second most common value > 20\n\nHere's an example of what this will look like when you apply it:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_range_selections.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/low_variance.png)\n\n#### Code Exports\n*Code Exports* are small snippets of code representing the current state of the grid you're viewing including things like:\n - columns built\n - filtering\n - sorting\n\nOther code exports available are:\n - Describe (Column Analysis)\n - Correlations (grid, timeseries chart & scatter chart)\n - Charts built using the Chart Builder\n\n [![](http://img.youtube.com/vi/6CkKgpv3d6I/0.jpg)](http://www.youtube.com/watch?v=6CkKgpv3d6I \"Code Export Tutorial\")\n\n|Type|Code Export|\n|:------:|:------:|\n|Main Grid|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/main.png)|\n|Histogram|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/histogram.png)|\n|Describe|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/describe.png)|\n|Correlation Grid|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/main.png)|\n|Correlation Timeseries|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/corr_ts.png)|\n|Correlation Scatter|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/corr_scatter.png)|\n|Charts|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/charts.png)|\n\n#### Export CSV\n\nExport your current data to either a CSV or TSV file:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/export_csv.png)\n\n#### Load Data & Sample Datasets\n\nSo either when starting D-Tale with no pre-loaded data or after you've already loaded some data you now have the ability to load data or choose from some sample datasets directly from the GUI:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/load_data.png)\n\nHere's the options at you disposal:\n* Load a CSV/TSV file by dragging a file to the dropzone in the top or select a file by clicking the dropzone\n* Load a CSV/TSV or JSON directly from the web by entering a URL (also throw in a proxy if you are using one)\n* Choose from one of our sample datasets:\n  * US COVID-19 data from NY Times (updated daily)\n  * Script breakdowns of popular shows Seinfeld & The Simpsons\n  * Movie dataset containing release date, director, actors, box office, reviews...\n  * Video games and their sales\n  * pandas.util.testing.makeTimeDataFrame\n\n\n#### Instances\nThis will give you information about other D-Tale instances are running under your current Python process.\n\nFor example, if you ran the following script:\n```python\nimport pandas as pd\nimport dtale\n\ndtale.show(pd.DataFrame([dict(foo=1, bar=2, biz=3, baz=4, snoopy_D_O_double_gizzle=5)]))\ndtale.show(pd.DataFrame([\n    dict(a=1, b=2, c=3, d=4),\n    dict(a=2, b=3, c=4, d=5),\n    dict(a=3, b=4, c=5, d=6),\n    dict(a=4, b=5, c=6, d=7)\n]))\ndtale.show(pd.DataFrame([range(6), range(6), range(6), range(6), range(6), range(6)]), name=\"foo\")\n```\nThis will make the **Instances** button available in all 3 of these D-Tale instances. Clicking that button while in the first instance invoked above will give you this popup:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Instances.png)\n\nThe grid above contains the following information:\n  - Process: timestamp when the process was started along with the name (if specified in `dtale.show()`)\n  - Rows: number of rows\n  - Columns: number of columns\n  - Column Names: comma-separated string of column names (only first 30 characters, hover for full listing)\n  - Preview: this button is available any of the non-current instances.  Clicking this will bring up left-most 5X5 grid information for that instance\n  - The row highlighted in green signifys the current D-Tale instance\n  - Any other row can be clicked to switch to that D-Tale instance\n\nHere is an example of clicking the \"Preview\" button:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Instances_preview.png)\n\n#### About\nThis will give you information about what version of D-Tale you're running as well as if its out of date to whats on PyPi.\n\n|Up To Date|Out Of Date|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/About-up-to-date.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/About-out-of-date.png)|\n\n#### Refresh Widths\nMostly a fail-safe in the event that your columns are no longer lining up. Click this and should fix that\n\n#### Theme\nToggle between light & dark themes for your viewing pleasure (only affects grid, not popups or charts).\n\n|Light|Dark|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/theme/light.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/theme/dark.png)|\n\n#### Reload Data\nForce a reload of the data from the server for the current rows being viewing in the grid by clicking this button. This can be helpful when viewing the grid from within another application like jupyter or nested within another website.\n\n#### Unpin/Pin Menu\nIf you would like to keep your menu pinned to the side of your grid all times rather than always having to click the triaangle in the upper left-hand corner simply click this button.  It is persisted back to the server so that it can be applied to all piece of data you've loaded into your session and beyond refreshes.\n\n#### Language\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/chinese_dtale.png)\n\nI am happy to announce that D-Tale now supports both English & Chinese (there is still more of the translation to be completed but the infrastructure is there).  And we are happy to add support for any other languages.  Please see instruction on how, [here](#adding-language-support).\n\n#### Shutdown\nPretty self-explanatory, kills your D-Tale session (there is also an auto-kill process that will kill your D-Tale after an hour of inactivity)\n\n### Column Menu Functions\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Col_menu.png)\n\n#### Filtering\n\n[![](http://img.youtube.com/vi/8zo5ZiI1Yzo/0.jpg)](http://www.youtube.com/watch?v=8zo5ZiI1Yzo \"Column Filtering\")\n\nThese interactive filters come in 3 different types: String, Numeric & Date.  Note that you will still have the ability to apply custom filters from the \"Filter\" popup on the main menu, but it will get applied in addition to any column filters.\n\n|Type|Filter|Data Types|Features|\n|----|------|----------|--------|\n|String|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/string.PNG)|strings & booleans|The ability to select multiple values based on what exists in the column. Notice the \"Show Missing Only\" toggle, this will only show up if your column has nan values|\n|Date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/dates.PNG)|dates|Specify a range of dates to filter on based on start & end inputs|\n|Numeric|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/numeric.PNG)|ints & floats|For integers the \"=\" will be similar to strings where you can select multiple values based on what exists in the column.  You also have access to other operands: <,>,<=,>=,() - \"Range exclusve\", [] - \"Range inclusive\".|\n\n#### Moving Columns\n\n[![](http://img.youtube.com/vi/We4TH477rRs/0.jpg)](http://www.youtube.com/watch?v=We4TH477rRs \"Moving Columns in D-Tale\")\n\nAll column movements are saved on the server so refreshing your browser won't lose them :ok_hand:\n\n#### Hiding Columns\n\n[![](http://img.youtube.com/vi/ryZT2Lk_YaA/0.jpg)](http://www.youtube.com/watch?v=ryZT2Lk_YaA \"Hide/Unhide Columns in D-Tale\")\n\nAll column movements are saved on the server so refreshing your browser won't lose them :ok_hand:\n\n#### Delete\n\nAs simple as it sounds, click this button to delete this column from your dataframe.\n\n#### Rename\n\nUpdate the name of any column in your dataframe to a name that is not currently in use by your dataframe.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rename.png)\n\n#### Replacements\n\nThis feature allows users to replace content on their column directly or for safer purposes in a brand new column.  Here are the options you have:\n\n|Type        |Data Types   |Description|Menu    |\n|------------|-------------|-----------|--------|\n|Value(s)    |all          |Replace specific values in a column with raw values, output from another column or an aggregation on your column|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_value.png)|\n|Spaces Only |strings      |Replace string values consisting only of spaces with raw values|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_spaces.png)|\n|Contains Char/Substring|strings      |Replace string values containing a specific character or substring|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_strings.png)|\n|Scikit-Learn Imputer|numeric      |Replace missing values with the output of using different Scikit-Learn imputers like iterative, knn & simple|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_sklearn.png)|\n\nHere's a quick demo: [![](http://img.youtube.com/vi/GiNFRtcpIt8/0.jpg)](http://www.youtube.com/watch?v=GiNFRtcpIt8 \"Column Replacements\")\n\n#### Lock\nAdds your column to \"locked\" columns\n  - \"locked\" means that if you scroll horizontally these columns will stay pinned to the right-hand side\n  - this is handy when you want to keep track of which date or security_id you're looking at\n  - by default, any index columns on the data passed to D-Tale will be locked\n\n#### Unlock\nRemoved column from \"locked\" columns\n\n#### Sorting\nApplies/removes sorting (Ascending/Descending/Clear) to the column selected\n  \n*Important*: as you add sorts they sort added will be added to the end of the multi-sort.  For example:\n\n| Action        | Sort           |\n| ------------- |:--------------:|\n| click \"a\"     |                |\n| sort asc      | a (asc)        |\n| click \"b\"     | a (asc)        |\n| sort desc     | a (asc), b(desc)|\n| click \"a\"     | a (asc), b(desc)|\n| sort None     | b(desc)|\n| sort desc     | b(desc), a(desc)|\n| click \"X\" on sort display | |\n\n#### Formats\nApply simple formats to numeric values in your grid\n\n|Type|Editing|Result|\n|--------|:------:|:------:|\n|Numeric|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_formatting.png)|\n|Date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_date_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_date_formatting.png)|\n|String|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_string_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_string_formatting.png)|\n\nFor all data types you have the ability to change what string is ued for display.\n\n\nFor numbers here's a grid of all the formats available with -123456.789 as input:\n  \n| Format        | Output         |\n| ------------- |:--------------:|\n| Precision (6) | -123456.789000 |\n| Thousands Sep | -123,456.789   |\n| Abbreviate    | -123k          |\n| Exponent      | -1e+5          |\n| BPS           | -1234567890BPS |\n| Red Negatives | <span style=\"color: red;\">-123457</span>|\n\nFor strings you can apply the follwoing formats:\n* **Truncation:** truncate long strings to a certain number of characters and replace with an allipses \"...\" and see the whole value on hover.\n* **Hyperlinks:** If your column is comprised of URL strings you can make them hyperlinks which will open a new tab\n\n#### Describe (Column Analysis)\nBased on the data type of a column different charts will be shown.  This side panel can be closed using the 'X' button in the upper right or by pressing the ESC key.\n\n| Chart         | Data Types     | Sample |\n|---------------|----------------|--------|\n| Box Plot      | Float, Int, Date | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/boxplot.png)|\n| Histogram     | Float, Int |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/histogram.PNG)|\n| Value Counts  | Int, String, Bool, Date, Category|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/value_counts.PNG)|\n| Word Value Counts | String | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/word_value_counts.png)|\n| Category      | Float   |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/category.PNG)|\n| Geolocation*  | Int, Float     |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/geolocation.PNG)|\n| Q-Q Plot      | Int, Float, Date |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/qq.png)|\n\n\n**Histogram** can be displayed in any number of bins (default: 20), simply type a new integer value in the bins input\n\n**Value Count** by default, show the top 100 values ranked by frequency.  If you would like to show the least frequent values simply make your number negative (-10 => 10 least frequent value)\n\n**Value Count w/ Ordinal** you can also apply an ordinal to your **Value Count** chart by selecting a column (of type int or float) and applying an aggregation (default: sum) to it (sum, mean, etc...) this column will be grouped by the column you're analyzing and the value produced by the aggregation will be used to sort your bars and also displayed in a line.  Here's an example:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/value_counts_ordinal.PNG\n)\n\n**Word Value Count** you can analyze string data by splitting each record by spaces to see the counts of each word.  This chart has all the same functions available as \"Value Counts\".  In addition, you can select multiple \"Cleaner\" functions to be applied to your column before building word values. These functions will perform operations like removing punctuation, removing numeric character & normalizing accent characters.\n\n**Category (Category Breakdown)** when viewing float columns you can also see them broken down by a categorical column (string, date, int, etc...).  This means that when you select a category column this will then display the frequency of each category in a line as well as bars based on the float column you're analyzing grouped by that category and computed by your aggregation (default: mean).\n\n**Geolocation** when your data contains latitude & longitude then you can view the coverage in a plotly scattergeo map.  In order to have access this chart your dataframe must have at least one of each of these types of columns:\n* \"lat\" must be contained within the lower-cased version of the column name and values be between -90 & 90\n* \"lon\" must be contained within the lower-cased version of the column name and values be between -180 & 180\n\n### Hotkeys\n\nThese are key combinations you can use in place of clicking actual buttons to save a little time:\n\n| Keymap      | Action         |\n|-------------|----------------|\n|`shift+m`    | Opens main menu*|\n|`shift+d`    | Opens \"Describe\" page*|\n|`shift+f`    | Opens \"Custom Filter\"*|\n|`shift+b`    | Opens \"Build Column\"*|\n|`shift+c`    | Opens \"Charts\" page*|\n|`shift+x`    | Opens \"Code Export\"*|\n|`esc`        | Closes any open modal window or side panel & exits cell editing|\n\n`*` Does not fire if user is actively editing a cell.\n\n### Menu Functions Depending on Browser Dimensions\nDepending on the dimensions of your browser window the following buttons will not open modals, but rather separate browser windows:  Correlations, Describe & Instances (see images from [Jupyter Notebook](#jupyter-notebook), also Charts will always open in a separate browser window)\n\n## For Developers\n\n### Cloning\n\nClone the code (git clone ssh://git@github.com:manahl/dtale.git), then start the backend server:\n\n```bash\n$ git clone ssh://git@github.com:manahl/dtale.git\n# install the dependencies\n$ python setup.py develop\n# start the server\n$ dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date\n```\n\nYou can also run dtale from PyDev directly.\n\nYou will also want to import javascript dependencies and build the source (all javascript code resides in the `frontend` folder):\n\n``` bash\n$ cd frontend\n$ npm install\n# 1) a persistent server that serves the latest JS:\n$ npm run watch\n# 2) or one-off build:\n$ npm run build\n```\n\n### Running tests\n\nThe usual npm test command works:\n\n```\n$ npm test\n```\n\nYou can run individual test files:\n\n```\n$ npm run test -- static/__tests__/dtale/DataViewer-base-test.jsx\n```\n\n### Linting\n\nYou can lint all the JS and CSS to confirm there's nothing obviously wrong with\nit:\n\n``` bash\n$ npm run lint\n```\n\nYou can also lint individual JS files:\n\n``` bash\n$ npm run lint-js-file -s -- static/dtale/DataViewer.jsx\n```\n\n### Formatting JS\n\nYou can auto-format code as follows:\n\n``` bash\n$ npm run format\n```\n\n### Docker Development\n\nYou can build python 27-3 & run D-Tale as follows:\n```bash\n$ yarn run build\n$ docker-compose build dtale_2_7\n$ docker run -it --network host dtale_2_7:latest\n$ python\n>>> import pandas as pd\n>>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n>>> import dtale\n>>> dtale.show(df)\n```\nThen view your D-Tale instance in your browser using the link that gets printed\n\nYou can build python 36-1 & run D-Tale as follows:\n```bash\n$ yarn run build\n$ docker-compose build dtale_3_6\n$ docker run -it --network host dtale_3_6:latest\n$ python\n>>> import pandas as pd\n>>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n>>> import dtale\n>>> dtale.show(df)\n```\nThen view your D-Tale instance in your browser using the link that gets printed\n\n\n### Adding Language Support\n\nCurrently D-Tale support both english & chinese but other languages will gladly be supported.  To add another language simply open a pull request with the following:\n- cake a copy & translate the values in the following JSON english JSON files and save them to the same locations as each file\n - [Back-End](https://github.com/man-group/dtale/blob/master/dtale/translations/en.json)\n - [Front-End](https://github.com/man-group/dtale/blob/master/static/translations/en.json)\n- please make the name of these files the name of the language you are adding (currently english -> en, chinese -> cn) \n- be sure to keep the keys in english, that is important\n\nLooking forward to what languages come next! :smile:\n\n\n## Global State/Data Storage\n\nIf D-Tale is running in an environment with multiple python processes (ex: on a web server running [gunicorn](https://github.com/benoitc/gunicorn)) it will most likely encounter issues with inconsistent state.  Developers can fix this by configuring the system D-Tale uses for storing data.  Detailed documentation is available here: [Data Storage and managing Global State](https://github.com/man-group/dtale/blob/master/docs/GLOBAL_STATE.md)\n\n\n## Startup Behavior\n\nHere's a little background on how the `dtale.show()` function works:\n - by default it will look for ports between 40000 & 49000, but you can change that range by specifying the environment variables DTALE_MIN_PORT & DTALE_MAX_PORT\n - think of sessions as python consoles or jupyter notebooks\n\n1) Session 1 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1|http://localhost:40000/dtale/main/1|\n\n2) Session 1 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1,2|http://localhost:40000/dtale/main/[1,2]|\n\n2) Session 2 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1,2|http://localhost:40000/dtale/main/[1,2]|\n|2|40001|1|http://localhost:40001/dtale/main/1|\n\n3) Session 1 executes `dtale.show(df, port=40001, force=True)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n\n4) Session 3 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n\n5) Session 2 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n\n6) Session 4 executes `dtale.show(df, port=8080)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n\n7) Session 1 executes `dtale.get_instance(1).kill()` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|2,3|http://localhost:40001/dtale/main/[2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n\n7) Session 5 sets DTALE_MIN_RANGE to 30000 and DTALE_MAX_RANGE 39000 and executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data ID(s)|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|2,3|http://localhost:40001/dtale/main/[2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n|5|30000|1|http://localhost:30000/dtale/main/1|\n\n## Documentation\n\nHave a look at the [detailed documentation](https://dtale.readthedocs.io).\n\n## Dependencies\n\n* Back-end\n  * dash\n  * dash_daq\n  * Flask\n  * Flask-Compress\n  * flask-ngrok\n  * Pandas\n  * plotly\n  * scikit-learn\n  * scipy\n  * xarray\n  * arctic [extra]\n  * redis [extra]\n  * rpy2 [extra]\n* Front-end\n  * react-virtualized\n  * chart.js\n\n## Acknowledgements\n\nD-Tale has been under active development at [Man Numeric](http://www.numeric.com/) since 2019.\n\nOriginal concept and implementation: [Andrew Schonfeld](https://github.com/aschonfeld)\n\nContributors:\n\n * [Phillip Dupuis](https://github.com/phillipdupuis)\n * [Fernando Saravia Rajal](https://github.com/fersarr)\n * [Dominik Christ](https://github.com/DominikMChrist)\n * [Reza Moshksar](https://github.com/reza1615)\n * [Bertrand Nouvel](https://github.com/bnouvelbmll)\n * [Chris Boddy](https://github.com/cboddy)\n * [Jason Holden](https://github.com/jasonkholden)\n * [Tom Taylor](https://github.com/TomTaylorLondon)\n * [Wilfred Hughes](https://github.com/Wilfred)\n * Mike Kelly\n * [Vincent Riemer](https://github.com/vincentriemer)\n * [Youssef Habchi](http://youssef-habchi.com/) - title font\n * ... and many others ...\n\nContributions welcome!\n\n## License\n\nD-Tale is licensed under the GNU LGPL v2.1.  A copy of which is included in [LICENSE](LICENSE)\n", "# D-Tale Configuration\n\nThere are a lot of parameters that can be passed to the `dtale.show()` function and if you're calling this function often which similar parameters it can become quite cumbersome.  Not to fear, there is a way around this and it is via an `.ini` file.\n\nThere are two options for specifying the `.ini` file (in order of precedence):\n1) create an environment variable `DTALE_CONFIG` and set it to the path to your `.ini` file\n2) save your `.ini` file to `$HOME/.config/dtale.ini`\n\nIf you do make use of the `.ini` file then the `[app]` section properties will be applied to the application overall and the `[show]` section will be applied every time you call `dtale.show`.\n\nHere is an example of the `.ini` file with all the properties available and what their defaults are (if they have one):\n```ini\n[app]\ntheme = light\ngithub_fork = False\nhide_shutdown = False\npin_menu = False\nlanguage = en\nmax_column_width = 100 # the default value is None\nmain_title = My App # only use this if you don't want to see the D-Tale logo\nmain_title_font = Arial # this font is applied to your custom title\nquery_engine = python\nhide_header_editor = False\nlock_header_menu = False\nhide_header_menu = False\nhide_main_menu = False\nhide_column_menus = False\n\n[charts] # this controls how many points can be contained within scatter & 3D charts\nscatter_points = 15000\n3d_points = 4000\n\n[auth]\nactive = True\nusername = johndoe\npassword = 1337h4xOr\n\n[show]\nhost = localhost\nport = 8080\ndebug = False\nreaper_on = True\nopen_browser = False\nignore_duplicate = True\nallow_cell_edits = True\ninplace = False\ndrop_index = False\napp_root = additional_path\nprecision = 6 # how many digits to show on floats\nshow_columns = a,b\nhide_columns = c\ncolumn_formats = {\"a\": {\"fmt\": {\"html\": true}}}\nsort = a|ASC,b|DESC\nlocked = a,b\ncolumn_edit_options = {\"a\": [\"yes\", \"no\", \"maybe\"]}\nauto_hide_empty_columns = False\n```\n\nSome notes on these properties:\n* *theme:* available values are 'light' & 'dark'\n* *host/port/app_root:* have no default\n\nHere is the hierarchy of how parameters are passed to `dtale.show` (in order of most important to least):\n1) Passing parameters directly into `dtale.show` or passing a dictionary of settings to `dtale.global_state.set_app_settings` or `dtale.global_state.set_auth_settings`\n2) Calling `dtale.config.set_config(path_to_file)` which is probably only useful if you have a long-running process like a jupyter notebook\n3) Specifying an `.ini` file via `DTALE_CONFIG` environment variable\n4) Specifying an `.ini` file in `$HOME/.config/dtale.ini`\n", "import warnings\n\nfrom flask import Blueprint\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\n        \"ignore\",\n        (\n            \"\\nThe dash_html_components package is deprecated. Please replace\"\n            \"\\n`import dash_html_components as html` with `from dash import html`\"\n        ),\n    )\n    warnings.filterwarnings(\"ignore\", module=\"matplotlib\\\\..*\")  # noqa: W605\n\n    dtale = Blueprint(\"dtale\", __name__, url_prefix=\"/dtale\")\n\n    ALLOW_CELL_EDITS = True\n    HIDE_SHUTDOWN = False\n    GITHUB_FORK = False\n    HIDE_HEADER_EDITOR = False\n    LOCK_HEADER_MENU = False\n    HIDE_HEADER_MENU = False\n    HIDE_MAIN_MENU = False\n    HIDE_COLUMN_MENUS = False\n\n    # flake8: NOQA\n    from dtale.app import show, get_instance, instances, offline_chart  # isort:skip\n    from dtale.cli.loaders import LOADERS  # isort:skip\n    from dtale.cli.clickutils import retrieve_meta_info_and_version\n    from dtale.global_state import update_id  # isort:skip\n\n    for loader_name, loader in LOADERS.items():\n        if hasattr(loader, \"show_loader\"):\n            globals()[\"show_{}\".format(loader_name)] = loader.show_loader\n\n    __version__ = retrieve_meta_info_and_version(\"dtale\")[1]\n", "from __future__ import absolute_import, print_function\n\nimport getpass\nimport jinja2\nimport logging\nimport os\nimport pandas as pd\nimport random\nimport socket\nimport sys\nimport time\nimport traceback\nfrom builtins import map, str\nfrom contextlib import closing\nfrom logging import ERROR as LOG_ERROR\nfrom threading import Timer\nfrom werkzeug.routing import Map\n\nfrom flask import (\n    Flask,\n    jsonify,\n    redirect,\n    render_template,\n    request,\n    url_for as flask_url_for,\n)\nfrom flask.testing import FlaskClient\n\nimport requests\nfrom flask_compress import Compress\nfrom six import PY3\n\nimport dtale.auth as auth\nimport dtale.global_state as global_state\nimport dtale.config as dtale_config\nfrom dtale import dtale\nfrom dtale.cli.clickutils import retrieve_meta_info_and_version, setup_logging\nfrom dtale.dash_application import views as dash_views\nfrom dtale.utils import (\n    DuplicateDataError,\n    build_shutdown_url,\n    build_url,\n    dict_merge,\n    fix_url_path,\n    get_host,\n    get_url_unquote,\n    is_app_root_defined,\n    make_list,\n    running_with_flask_debug,\n)\nfrom dtale.views import DtaleData, head_endpoint, is_up, kill, startup\n\nif PY3:\n    import _thread\nelse:\n    import thread as _thread\n\nlogger = logging.getLogger(__name__)\n\nUSE_NGROK = False\nUSE_COLAB = False\nJUPYTER_SERVER_PROXY = False\nACTIVE_HOST = None\nACTIVE_PORT = None\nSSL_CONTEXT = None\n\n_basepath = os.path.dirname(__file__)\n_filepath = os.path.abspath(os.path.join(_basepath, \"static\"))\n\nSHORT_LIFE_PATHS = [\n    \"dist\",\n    os.path.join(_filepath, \"dist\"),\n    \"dash\",\n    os.path.join(_filepath, \"dash\"),\n]\nSHORT_LIFE_TIMEOUT = 60\n\nREAPER_TIMEOUT = 60.0 * 60.0  # one-hour\n\n\nclass DtaleFlaskTesting(FlaskClient):\n    \"\"\"\n    Overriding Flask's implementation of flask.FlaskClient so we\n    can control the port associated with tests.\n\n    This class is required for setting the port on your test so that\n    we won't have SETTING keys colliding with other tests since the default\n    for every test would be 80.\n\n    :param args: Optional arguments to be passed to :class:`flask:flask.FlaskClient`\n    :param kwargs: Optional keyword arguments to be passed to :class:`flask:flask.FlaskClient`\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Constructor method\n        \"\"\"\n        self.host = kwargs.pop(\"hostname\", \"localhost\")\n        self.port = kwargs.pop(\"port\", random.randint(1025, 65535)) or random.randint(\n            1025, 65535\n        )\n        super(DtaleFlaskTesting, self).__init__(*args, **kwargs)\n        self.application.config[\"SERVER_NAME\"] = \"{host}:{port}\".format(\n            host=self.host, port=self.port\n        )\n        self.application.config[\"SESSION_COOKIE_DOMAIN\"] = \"localhost.localdomain\"\n\n    def get(self, *args, **kwargs):\n        \"\"\"\n        :param args: Optional arguments to be passed to :meth:`flask:flask.FlaskClient.get`\n        :param kwargs: Optional keyword arguments to be passed to :meth:`flask:flask.FlaskClient.get`\n        \"\"\"\n        return super(DtaleFlaskTesting, self).get(url_scheme=\"http\", *args, **kwargs)\n\n\ndef contains_route(routes, route):\n    return any((r.rule == route.rule and r.endpoint == route.endpoint) for r in routes)\n\n\nclass DtaleFlask(Flask):\n    \"\"\"\n    Overriding Flask's implementation of\n    get_send_file_max_age, test_client & run\n\n    :param import_name: the name of the application package\n    :param reaper_on: whether to run auto-reaper subprocess\n    :type reaper_on: bool\n    :param args: Optional arguments to be passed to :class:`flask:flask.Flask`\n    :param kwargs: Optional keyword arguments to be passed to :class:`flask:flask.Flask`\n    \"\"\"\n\n    def __init__(\n        self, import_name, reaper_on=True, url=None, app_root=None, *args, **kwargs\n    ):\n        \"\"\"\n        Constructor method\n        :param reaper_on: whether to run auto-reaper subprocess\n        :type reaper_on: bool\n        \"\"\"\n        self.reaper_on = reaper_on\n        self.reaper = None\n        self._setup_url_props(url)\n        self.port = None\n        self.app_root = app_root\n        super(DtaleFlask, self).__init__(import_name, *args, **kwargs)\n\n    def _setup_url_props(self, url):\n        self.base_url = url\n        self.shutdown_url = build_shutdown_url(url)\n\n    def update_template_context(self, context):\n        super(DtaleFlask, self).update_template_context(context)\n        if self.app_root is not None:\n            context[\"url_for\"] = self.url_for\n\n    def url_for(self, endpoint, *args, **kwargs):\n        if self.app_root is not None and endpoint == \"static\":\n            if \"filename\" in kwargs:\n                return fix_url_path(\n                    \"{}/{}/{}\".format(\n                        self.app_root, self.static_url_path, kwargs[\"filename\"]\n                    )\n                )\n            return fix_url_path(\"{}/{}\".format(self.app_root, args[0]))\n        if hasattr(Flask, \"url_for\"):\n            return Flask.url_for(self, endpoint, *args, **kwargs)\n        return flask_url_for(endpoint, *args, **kwargs)\n\n    def _override_routes(self, rule):\n        try:\n            routes_to_remove = [r for r in self.url_map._rules if r.rule == rule]\n            if routes_to_remove:\n                updated_rules = [\n                    r.empty()\n                    for r in self.url_map._rules\n                    if not contains_route(routes_to_remove, r)\n                ]\n                url_map_keys = [\n                    \"default_subdomains\",\n                    \"charset\",\n                    \"strict_slashes\",\n                    \"merge_slashes\",\n                    \"redirect_defaults\",\n                    \"converters\",\n                    \"sort_parameters\",\n                    \"sort_key\",\n                    \"encoding_errors\",\n                    \"host_matching\",\n                ]\n                url_map_data = {\n                    k: getattr(self.url_map, k)\n                    for k in url_map_keys\n                    if hasattr(self.url_map, k)\n                }\n                self.url_map = Map(rules=updated_rules, **url_map_data)\n\n            self.url_map._remap = True\n            self.url_map.update()\n        except BaseException:\n            logger.exception(\n                \"Could not override routes, if you're trying to specify a route for '/' then it will be ignored...\"\n            )\n\n    def route(self, rule, **options):\n        self._override_routes(rule)\n        return super(DtaleFlask, self).route(rule, **options)\n\n    def run(self, *args, **kwargs):\n        \"\"\"\n        :param args: Optional arguments to be passed to :meth:`flask:flask.run`\n        :param kwargs: Optional keyword arguments to be passed to :meth:`flask:flask.run`\n        \"\"\"\n        port_num = kwargs.get(\"port\")\n        self.port = str(port_num or \"\")\n        if not self.base_url:\n            host = kwargs.get(\"host\")\n            initialize_process_props(host, port_num)\n            app_url = build_url(self.port, host)\n            self._setup_url_props(app_url)\n        if kwargs.get(\"debug\", False):\n            self.reaper_on = False\n        self.build_reaper()\n        super(DtaleFlask, self).run(\n            use_reloader=kwargs.get(\"debug\", False), *args, **kwargs\n        )\n\n    def test_client(self, reaper_on=False, port=None, app_root=None, *args, **kwargs):\n        \"\"\"\n        Overriding Flask's implementation of test_client so we can specify ports for testing and\n        whether auto-reaper should be running\n\n        :param reaper_on: whether to run auto-reaper subprocess\n        :type reaper_on: bool\n        :param port: port number of flask application\n        :type port: int\n        :param args: Optional arguments to be passed to :meth:`flask:flask.Flask.test_client`\n        :param kwargs: Optional keyword arguments to be passed to :meth:`flask:flask.Flask.test_client`\n        :return: Flask's test client\n        :rtype: :class:`dtale.app.DtaleFlaskTesting`\n        \"\"\"\n        self.reaper_on = reaper_on\n        self.app_root = app_root\n        if app_root is not None:\n            self.config[\"APPLICATION_ROOT\"] = app_root\n            self.jinja_env.globals[\"url_for\"] = self.url_for\n        self.test_client_class = DtaleFlaskTesting\n        return super(DtaleFlask, self).test_client(\n            *args, **dict_merge(kwargs, dict(port=port))\n        )\n\n    def clear_reaper(self):\n        \"\"\"\n        Restarts auto-reaper countdown\n        \"\"\"\n        if self.reaper:\n            self.reaper.cancel()\n\n    def build_reaper(self, timeout=REAPER_TIMEOUT):\n        \"\"\"\n        Builds D-Tale's auto-reaping process to cleanup process after an hour of inactivity\n\n        :param timeout: time in seconds before D-Tale is shutdown for inactivity, defaults to one hour\n        :type timeout: float\n        \"\"\"\n        if not self.reaper_on:\n            return\n        self.clear_reaper()\n\n        def _func():\n            logger.info(\"Executing shutdown due to inactivity...\")\n            if is_up(self.base_url):  # make sure the Flask process is still running\n                requests.get(self.shutdown_url)\n            sys.exit()  # kill off the reaper thread\n\n        self.reaper = Timer(timeout, _func)\n        self.reaper.start()\n\n    def get_send_file_max_age(self, name):\n        \"\"\"\n        Overriding Flask's implementation of\n        get_send_file_max_age so we can lower the\n        timeout for javascript and css files which\n        are changed more often\n\n        :param name: filename\n        :return: Flask's default behavior for get_send_max_age if filename is not in SHORT_LIFE_PATHS\n                 otherwise SHORT_LIFE_TIMEOUT\n\n        \"\"\"\n        if name and any([str(name).startswith(path) for path in SHORT_LIFE_PATHS]):\n            return SHORT_LIFE_TIMEOUT\n        return super(DtaleFlask, self).get_send_file_max_age(name)\n\n\ndef build_app(\n    url=None, reaper_on=True, app_root=None, additional_templates=None, **kwargs\n):\n    \"\"\"\n    Builds :class:`flask:flask.Flask` application encapsulating endpoints for D-Tale's front-end\n\n    :param url: optional parameter which sets the host & root for any internal endpoints (ex: pinging shutdown)\n    :type url: str, optional\n    :param reaper_on: whether to run auto-reaper subprocess\n    :type reaper_on: bool\n    :param app_root: Optional path to prepend to the routes of D-Tale. This is used when making use of\n                     Jupyterhub server proxy\n    :type app_root: str, optional\n    :param additional_templates: path(s) to any other jinja templates you would like to load.  This comes into play if\n                                 you're embedding D-Tale into your own Flask app\n    :type: str, list, optional\n    :return: :class:`flask:flask.Flask` application\n    :rtype: :class:`dtale.app.DtaleFlask`\n    \"\"\"\n\n    app = DtaleFlask(\n        \"dtale\",\n        reaper_on=reaper_on,\n        static_url_path=\"/dtale/static\",\n        url=url,\n        instance_relative_config=False,\n        app_root=app_root,\n    )\n    app.config[\"SECRET_KEY\"] = \"Dtale\"\n\n    app.jinja_env.trim_blocks = True\n    app.jinja_env.lstrip_blocks = True\n\n    if app_root is not None:\n        app.config[\"APPLICATION_ROOT\"] = app_root\n        app.jinja_env.globals[\"url_for\"] = app.url_for\n    app.jinja_env.globals[\"is_app_root_defined\"] = is_app_root_defined\n\n    if additional_templates:\n        loaders = [app.jinja_loader]\n        loaders += [\n            jinja2.FileSystemLoader(loc) for loc in make_list(additional_templates)\n        ]\n        my_loader = jinja2.ChoiceLoader(loaders)\n        app.jinja_loader = my_loader\n\n    app.register_blueprint(dtale)\n\n    compress = Compress()\n    compress.init_app(app)\n\n    def _root():\n        return redirect(\"/dtale/{}\".format(head_endpoint()))\n\n    @app.route(\"/\")\n    def root():\n        return _root()\n\n    @app.route(\"/dtale\")\n    def dtale_base():\n        \"\"\"\n        :class:`flask:flask.Flask` routes which redirect to dtale/main\n\n        :return: 302 - flask.redirect('/dtale/main')\n        \"\"\"\n        return _root()\n\n    @app.route(\"/favicon.ico\")\n    def favicon():\n        \"\"\"\n        :class:`flask:flask.Flask` routes which returns favicon\n\n        :return: image/png\n        \"\"\"\n        return redirect(app.url_for(\"static\", filename=\"images/favicon.ico\"))\n\n    @app.route(\"/missing-js\")\n    def missing_js():\n        missing_js_commands = (\n            \">> cd [location of your local dtale repo]\\n\"\n            \">> yarn install\\n\"\n            \">> yarn run build  # or 'yarn run watch' if you're trying to develop\"\n        )\n        return render_template(\n            \"dtale/errors/missing_js.html\", missing_js_commands=missing_js_commands\n        )\n\n    @app.errorhandler(404)\n    def page_not_found(e=None):\n        \"\"\"\n        :class:`flask:flask.Flask` routes which returns favicon\n\n        :param e: exception\n        :return: text/html with exception information\n        \"\"\"\n        return (\n            render_template(\n                \"dtale/errors/404.html\",\n                page=\"\",\n                error=e,\n                stacktrace=str(traceback.format_exc()),\n            ),\n            404,\n        )\n\n    @app.errorhandler(500)\n    def internal_server_error(e=None):\n        \"\"\"\n        :class:`flask:flask.Flask` route which returns favicon\n\n        :param e: exception\n        :return: text/html with exception information\n        \"\"\"\n        return (\n            render_template(\n                \"dtale/errors/500.html\",\n                page=\"\",\n                error=e,\n                stacktrace=str(traceback.format_exc()),\n            ),\n            500,\n        )\n\n    def shutdown_server():\n        global ACTIVE_HOST, ACTIVE_PORT\n        \"\"\"\n        This function that checks if flask.request.environ['werkzeug.server.shutdown'] exists and\n        if so, executes that function\n        \"\"\"\n        logger.info(\"Executing shutdown...\")\n        func = request.environ.get(\"werkzeug.server.shutdown\")\n        if func is None:\n            logger.info(\n                \"Not running with the Werkzeug Server, exiting by searching gc for BaseWSGIServer\"\n            )\n            import gc\n            from werkzeug.serving import BaseWSGIServer\n\n            for obj in gc.get_objects():\n                try:\n                    if isinstance(obj, BaseWSGIServer):\n                        obj.shutdown()\n                        break\n                except Exception as e:\n                    logger.error(e)\n        else:\n            func()\n        global_state.cleanup()\n        ACTIVE_PORT = None\n        ACTIVE_HOST = None\n\n    @app.route(\"/shutdown\")\n    def shutdown():\n        \"\"\"\n        :class:`flask:flask.Flask` route for initiating server shutdown\n\n        :return: text/html with server shutdown message\n        \"\"\"\n        app.clear_reaper()\n        shutdown_server()\n        return \"Server shutting down...\"\n\n    @app.before_request\n    @auth.requires_auth\n    def before_request():\n        \"\"\"\n        Logic executed before each :attr:`flask:flask.request`\n\n        :return: text/html with server shutdown message\n        \"\"\"\n        app.build_reaper()\n\n    @app.route(\"/site-map\")\n    def site_map():\n        \"\"\"\n        :class:`flask:flask.Flask` route listing all available flask endpoints\n\n        :return: JSON of all flask enpoints [\n            [endpoint1, function path1],\n            ...,\n            [endpointN, function pathN]\n        ]\n        \"\"\"\n\n        def has_no_empty_params(rule):\n            defaults = rule.defaults or ()\n            arguments = rule.arguments or ()\n            return len(defaults) >= len(arguments)\n\n        links = []\n        for rule in app.url_map.iter_rules():\n            # Filter out rules we can't navigate to in a browser\n            # and rules that require parameters\n            if \"GET\" in rule.methods and has_no_empty_params(rule):\n                url = app.url_for(rule.endpoint, **(rule.defaults or {}))\n                links.append((url, rule.endpoint))\n        return jsonify(links)\n\n    @app.route(\"/version-info\")\n    def version_info():\n        \"\"\"\n        :class:`flask:flask.Flask` route for retrieving version information about D-Tale\n\n        :return: text/html version information\n        \"\"\"\n        _, version = retrieve_meta_info_and_version(\"dtale\")\n        return str(version)\n\n    @app.route(\"/health\")\n    def health_check():\n        \"\"\"\n        :class:`flask:flask.Flask` route for checking if D-Tale is up and running\n\n        :return: text/html 'ok'\n        \"\"\"\n        return \"ok\"\n\n    @app.url_value_preprocessor\n    def handle_data_id(_endpoint, values):\n        if values and \"data_id\" in values:\n            # https://github.com/man-group/dtale/commit/536691d365b69a580df836e617978eb563402ac5\n            values[\"data_id\"] = get_url_unquote()(\n                values[\"data_id\"]\n            )  # for handling back-slashes in arcticDB symbols\n            data_id_from_name = global_state.get_data_id_by_name(values[\"data_id\"])\n            values[\"data_id\"] = data_id_from_name or values[\"data_id\"]\n\n    auth.setup_auth(app)\n\n    with app.app_context():\n        app = dash_views.add_dash(app)\n        return app\n\n\ndef initialize_process_props(host=None, port=None, force=False):\n    \"\"\"\n    Helper function to initalize global state corresponding to the host & port being used for your\n    :class:`flask:flask.Flask` process\n\n    :param host: hostname to use otherwise it will default to the output of :func:`python:socket.gethostname`\n    :type host: str, optional\n    :param port: port to use otherwise default to the output of :meth:`dtale.app.find_free_port`\n    :type port: str, optional\n    :param force: boolean flag to determine whether to ignore the :meth:`dtale.app.find_free_port` function\n    :type force: bool\n    :return:\n    \"\"\"\n    global ACTIVE_HOST, ACTIVE_PORT\n\n    if force:\n        active_host = get_host(ACTIVE_HOST)\n        curr_base = build_url(ACTIVE_PORT, active_host)\n        final_host = get_host(host)\n        new_base = build_url(port, final_host)\n        if curr_base != new_base:\n            if is_up(new_base):\n                try:\n                    kill(new_base)  # kill the original process\n                except BaseException:\n                    raise IOError(\n                        (\n                            \"Could not kill process at {}, possibly something else is running at port {}. Please try \"\n                            \"another port.\"\n                        ).format(new_base, port)\n                    )\n                while is_up(new_base):\n                    time.sleep(0.01)\n            ACTIVE_HOST = final_host\n            ACTIVE_PORT = port\n            return\n\n    if ACTIVE_HOST is None:\n        ACTIVE_HOST = get_host(host)\n\n    if ACTIVE_PORT is None:\n        ACTIVE_PORT = int(port or find_free_port())\n\n\ndef is_port_in_use(port):\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        try:\n            s.bind((\"localhost\", port))\n            return False\n        except BaseException:\n            return True\n\n\ndef find_free_port():\n    \"\"\"\n    Searches for free port on executing server to run the :class:`flask:flask.Flask` process. Checks ports in range\n    specified using environment variables:\n\n    DTALE_MIN_PORT (default: 40000)\n    DTALE_MAX_PORT (default: 49000)\n\n    The range limitation is required for usage in tools such as jupyterhub.  Will raise an exception if an open\n    port cannot be found.\n\n    :return: port number\n    :rtype: int\n    \"\"\"\n\n    min_port = int(os.environ.get(\"DTALE_MIN_PORT\") or 40000)\n    max_port = int(os.environ.get(\"DTALE_MAX_PORT\") or 49000)\n    base = min_port\n    while is_port_in_use(base):\n        base += 1\n        if base > max_port:\n            msg = (\n                \"D-Tale could not find an open port from {} to {}, please increase your range by altering the \"\n                \"environment variables DTALE_MIN_PORT & DTALE_MAX_PORT.\"\n            ).format(min_port, max_port)\n            raise IOError(msg)\n    return base\n\n\ndef build_startup_url_and_app_root(app_root=None):\n    global ACTIVE_HOST, ACTIVE_PORT, SSL_CONTEXT, JUPYTER_SERVER_PROXY, USE_COLAB\n\n    if USE_COLAB:\n        colab_host = use_colab(ACTIVE_PORT)\n        if colab_host:\n            return colab_host, None\n    url = build_url(ACTIVE_PORT, ACTIVE_HOST, SSL_CONTEXT is not None)\n    final_app_root = app_root\n    if final_app_root is None and JUPYTER_SERVER_PROXY:\n        final_app_root = os.environ.get(\"JUPYTERHUB_SERVICE_PREFIX\")\n        if final_app_root is None:\n            final_app_root = \"/user/{}\".format(getpass.getuser())\n        final_app_root = (\n            \"{}/proxy\".format(final_app_root)\n            if not final_app_root.endswith(\"/proxy\")\n            else final_app_root\n        )\n    if final_app_root is not None:\n        if JUPYTER_SERVER_PROXY:\n            final_app_root = fix_url_path(\"{}/{}\".format(final_app_root, ACTIVE_PORT))\n            return final_app_root, final_app_root\n        else:\n            return fix_url_path(\"{}/{}\".format(url, final_app_root)), final_app_root\n    return url, final_app_root\n\n\ndef use_colab(port):\n    try:\n        from google.colab.output import eval_js\n\n        colab_host = eval_js(\n            'google.colab.kernel.proxyPort(%d, {\"cache\": false})' % port\n        )\n        return colab_host[:-1] if colab_host.endswith(\"/\") else colab_host\n    except BaseException:\n        return None\n\n\ndef show(data=None, data_loader=None, name=None, context_vars=None, **options):\n    \"\"\"\n    Entry point for kicking off D-Tale :class:`flask:flask.Flask` process from python process\n\n    :param data: data which D-Tale will display\n    :type data: :class:`pandas:pandas.DataFrame` or :class:`pandas:pandas.Series`\n                or :class:`pandas:pandas.DatetimeIndex` or :class:`pandas:pandas.MultiIndex`, optional\n    :param host: hostname of D-Tale, defaults to 0.0.0.0\n    :type host: str, optional\n    :param port: port number of D-Tale process, defaults to any open port on server\n    :type port: str, optional\n    :param name: optional label to assign a D-Tale process\n    :type name: str, optional\n    :param debug: will turn on :class:`flask:flask.Flask` debug functionality, defaults to False\n    :type debug: bool, optional\n    :param subprocess: run D-Tale as a subprocess of your current process, defaults to True\n    :type subprocess: bool, optional\n    :param data_loader: function to load your data\n    :type data_loader: func, optional\n    :param reaper_on: turn on subprocess which will terminate D-Tale after 1 hour of inactivity\n    :type reaper_on: bool, optional\n    :param open_browser: if true, this will try using the :mod:`python:webbrowser` package to automatically open\n                         your default browser to your D-Tale process\n    :type open_browser: bool, optional\n    :param notebook: if true, this will try displaying an :class:`ipython:IPython.display.IFrame`\n    :type notebook: bool, optional\n    :param force: if true, this will force the D-Tale instance to run on the specified host/port by killing any\n                  other process running at that location\n    :type force: bool, optional\n    :param context_vars: a dictionary of the variables that will be available for use in user-defined expressions,\n                         such as filters\n    :type context_vars: dict, optional\n    :param ignore_duplicate: if true, this will not check if this data matches any other data previously loaded to\n                             D-Tale\n    :type ignore_duplicate: bool, optional\n    :param app_root: Optional path to prepend to the routes of D-Tale. This is used when making use of\n                     Jupyterhub server proxy\n    :type app_root: str, optional\n    :param allow_cell_edits: If false, this will not allow users to edit cells directly in their D-Tale grid\n    :type allow_cell_edits: bool, optional\n    :param inplace: If true, this will call `reset_index(inplace=True)` on the dataframe used as a way to save memory.\n                    Otherwise this will create a brand new dataframe, thus doubling memory but leaving the dataframe\n                    input unchanged.\n    :type inplace: bool, optional\n    :param drop_index: If true, this will drop any pre-existing index on the dataframe input.\n    :type drop_index: bool, optional\n    :param hide_shutdown: If true, this will hide the \"Shutdown\" buton from users\n    :type hide_shutdown: bool, optional\n    :param github_fork: If true, this will display a \"Fork me on GitHub\" ribbon in the upper right-hand corner of the\n                        app\n    :type github_fork: bool, optional\n    :param hide_drop_rows: If true, this will hide the \"Drop Rows\" button from users\n    :type hide_drop_rows: bool, optional\n    :param hide_header_editor: If true, this will hide the header editor when editing cells\n    :type hide_header_editor: bool, optional\n    :param lock_header_menu: if true, this will always the display the header menu which usually only displays when you\n                             hover over the top\n    :type lock_header_menu: bool, optional\n    :param hide_header_menu: If true, this will hide the header menu from the screen\n    :type hide_header_menu: bool, optional\n    :param hide_main_menu: If true, this will hide the main menu from the screen\n    :type hide_main_menu: bool, optional\n    :param hide_column_menus: If true, this will hide the column menus from the screen\n    :type hide_column_menus: bool, optional\n    :param column_edit_options: The options to allow on the front-end when editing a cell for the columns specified\n    :type column_edit_options: dict, optional\n    :param auto_hide_empty_columns: if True, then auto-hide any columns on the front-end that are comprised entirely of\n                                    NaN values\n    :type auto_hide_empty_columns: boolean, optional\n    :param highlight_filter: if True, then highlight rows on the frontend which will be filtered when applying a filter\n                             rather than hiding them from the dataframe\n    :type highlight_filter: boolean, optional\n\n    :Example:\n\n        >>> import dtale\n        >>> import pandas as pd\n        >>> df = pandas.DataFrame([dict(a=1,b=2,c=3)])\n        >>> dtale.show(df)\n        D-Tale started at: http://hostname:port\n\n        ..link displayed in logging can be copied and pasted into any browser\n    \"\"\"\n    global ACTIVE_HOST, ACTIVE_PORT, SSL_CONTEXT, USE_NGROK\n\n    if name:\n        if global_state.get_data_id_by_name(name):\n            print(\n                \"Data has already been loaded to D-Tale with the name '{}', please try another one.\".format(\n                    name\n                )\n            )\n            return\n        if any(not c.isalnum() and not c.isspace() for c in name):\n            print(\n                \"'name' property cannot contain any special characters only letters, numbers or spaces.\"\n            )\n            return\n\n    try:\n        final_options = dtale_config.build_show_options(options)\n        logfile, log_level, verbose = map(\n            final_options.get, [\"logfile\", \"log_level\", \"verbose\"]\n        )\n        setup_logging(logfile, log_level or \"info\", verbose)\n\n        if USE_NGROK:\n            if not PY3:\n                raise Exception(\n                    \"In order to use ngrok you must be using Python 3 or higher!\"\n                )\n\n            from flask_ngrok import _run_ngrok\n\n            ACTIVE_HOST = _run_ngrok()\n            ACTIVE_PORT = None\n        else:\n            initialize_process_props(\n                final_options[\"host\"], final_options[\"port\"], final_options[\"force\"]\n            )\n\n        SSL_CONTEXT = options.get(\"ssl_context\")\n        app_url = build_url(ACTIVE_PORT, ACTIVE_HOST)\n        startup_url, final_app_root = build_startup_url_and_app_root(\n            final_options[\"app_root\"]\n        )\n        instance = startup(\n            startup_url,\n            data=data,\n            data_loader=data_loader,\n            name=name,\n            context_vars=context_vars,\n            ignore_duplicate=final_options[\"ignore_duplicate\"],\n            allow_cell_edits=final_options[\"allow_cell_edits\"],\n            inplace=final_options[\"inplace\"],\n            drop_index=final_options[\"drop_index\"],\n            precision=final_options[\"precision\"],\n            show_columns=final_options[\"show_columns\"],\n            hide_columns=final_options[\"hide_columns\"],\n            column_formats=final_options[\"column_formats\"],\n            nan_display=final_options[\"nan_display\"],\n            sort=final_options[\"sort\"],\n            locked=final_options[\"locked\"],\n            background_mode=final_options[\"background_mode\"],\n            range_highlights=final_options[\"range_highlights\"],\n            vertical_headers=final_options[\"vertical_headers\"],\n            is_proxy=JUPYTER_SERVER_PROXY,\n            app_root=final_app_root,\n            hide_shutdown=final_options.get(\"hide_shutdown\"),\n            column_edit_options=final_options.get(\"column_edit_options\"),\n            auto_hide_empty_columns=final_options.get(\"auto_hide_empty_columns\"),\n            highlight_filter=final_options.get(\"highlight_filter\"),\n            hide_header_editor=final_options.get(\"hide_header_editor\"),\n            lock_header_menu=final_options.get(\"lock_header_menu\"),\n            hide_header_menu=final_options.get(\"hide_header_menu\"),\n            hide_main_menu=final_options.get(\"hide_main_menu\"),\n            hide_column_menus=final_options.get(\"hide_column_menus\"),\n        )\n        instance.started_with_open_browser = final_options[\"open_browser\"]\n        is_active = not running_with_flask_debug() and is_up(app_url)\n        if is_active:\n\n            def _start():\n                if final_options[\"open_browser\"]:\n                    instance.open_browser()\n\n        else:\n            if USE_NGROK:\n                thread = Timer(1, _run_ngrok)\n                thread.setDaemon(True)\n                thread.start()\n\n            def _start():\n                try:\n                    app = build_app(\n                        app_url,\n                        reaper_on=final_options[\"reaper_on\"],\n                        host=ACTIVE_HOST,\n                        app_root=final_app_root,\n                    )\n                    if final_options[\"debug\"] and not USE_NGROK:\n                        app.jinja_env.auto_reload = True\n                        app.config[\"TEMPLATES_AUTO_RELOAD\"] = True\n                    else:\n                        logging.getLogger(\"werkzeug\").setLevel(LOG_ERROR)\n\n                    if final_options[\"open_browser\"]:\n                        instance.open_browser()\n\n                    # hide banner message in production environments\n                    cli = sys.modules.get(\"flask.cli\")\n                    if cli is not None:\n                        cli.show_server_banner = lambda *x: None\n\n                    run_kwargs = {}\n                    if options.get(\"ssl_context\"):\n                        run_kwargs[\"ssl_context\"] = options.get(\"ssl_context\")\n\n                    if USE_NGROK:\n                        app.run(threaded=True, **run_kwargs)\n                    else:\n                        app.run(\n                            host=\"0.0.0.0\",\n                            port=ACTIVE_PORT,\n                            debug=final_options[\"debug\"],\n                            threaded=True,\n                            **run_kwargs\n                        )\n                except BaseException as ex:\n                    logger.exception(ex)\n\n        if final_options[\"subprocess\"]:\n            if is_active:\n                _start()\n            else:\n                _thread.start_new_thread(_start, ())\n\n            if final_options[\"notebook\"]:\n                instance.notebook()\n        else:\n            # Need to use logging.info() because for some reason other libraries like arctic seem to break logging\n            logging.info(\"D-Tale started at: {}\".format(app_url))\n            _start()\n\n        return instance\n    except DuplicateDataError as ex:\n        print(\n            \"It looks like this data may have already been loaded to D-Tale based on shape and column names. Here is \"\n            \"URL of the data that seems to match it:\\n\\n{}\\n\\nIf you still want to load this data please use the \"\n            \"following command:\\n\\ndtale.show(df, ignore_duplicate=True)\".format(\n                DtaleData(ex.data_id, build_url(ACTIVE_PORT, ACTIVE_HOST)).main_url()\n            )\n        )\n    return None\n\n\ndef instances():\n    \"\"\"\n    Prints all urls to the current pieces of data being viewed\n    \"\"\"\n    if global_state.size() > 0:\n\n        def _instance_msgs():\n            for data_id in global_state.keys():\n                startup_url, final_app_root = build_startup_url_and_app_root()\n                instance = DtaleData(\n                    data_id,\n                    startup_url,\n                    is_proxy=JUPYTER_SERVER_PROXY,\n                    app_root=final_app_root,\n                )\n                name = global_state.get_name(data_id)\n                yield [data_id, name or \"\", instance.build_main_url()]\n                if name is not None:\n                    yield [\n                        global_state.convert_name_to_url_path(name),\n                        name,\n                        instance.build_main_url(\n                            global_state.convert_name_to_url_path(name)\n                        ),\n                    ]\n\n        data = pd.DataFrame(\n            list(_instance_msgs()), columns=[\"ID\", \"Name\", \"URL\"]\n        ).to_string(index=False)\n        print(\n            (\n                \"To gain access to an instance object simply pass the value from 'ID' to dtale.get_instance(ID)\\n\\n{}\"\n            ).format(data)\n        )\n    else:\n        print(\"currently no running instances...\")\n\n\ndef get_instance(data_id):\n    \"\"\"\n    Returns a :class:`dtale.views.DtaleData` object for the data_id passed as input, will return None if the data_id\n    does not exist\n\n    :param data_id: integer identifier for a D-Tale process's data\n    :type data_id: int\n    :return: :class:`dtale.views.DtaleData`\n    \"\"\"\n    final_data_id = global_state.get_data_id_by_name(data_id) or data_id\n    if not global_state.contains(final_data_id):\n        return None\n\n    if data_id is not None:\n        startup_url, final_app_root = build_startup_url_and_app_root()\n        return DtaleData(\n            final_data_id,\n            startup_url,\n            is_proxy=JUPYTER_SERVER_PROXY,\n            app_root=final_app_root,\n        )\n    return None\n\n\ndef offline_chart(\n    df,\n    chart_type=None,\n    query=None,\n    x=None,\n    y=None,\n    z=None,\n    group=None,\n    agg=None,\n    window=None,\n    rolling_comp=None,\n    barmode=None,\n    barsort=None,\n    yaxis=None,\n    filepath=None,\n    title=None,\n    **kwargs\n):\n    \"\"\"\n    Builds the HTML for a plotly chart figure to saved to a file or output to a jupyter notebook\n\n    :param df: integer string identifier for a D-Tale process's data\n    :type df: :class:`pandas:pandas.DataFrame`\n    :param chart_type: type of chart, possible options are line|bar|pie|scatter|3d_scatter|surface|heatmap\n    :type chart_type: str\n    :param query: pandas dataframe query string\n    :type query: str, optional\n    :param x: column to use for the X-Axis\n    :type x: str\n    :param y: columns to use for the Y-Axes\n    :type y: list of str\n    :param z: column to use for the Z-Axis\n    :type z: str, optional\n    :param group: column(s) to use for grouping\n    :type group: list of str or str, optional\n    :param agg: specific aggregation that can be applied to y or z axes.  Possible values are: count, first, last mean,\n                median, min, max, std, var, mad, prod, sum.  This is included in label of axis it is being applied to.\n    :type agg: str, optional\n    :param window: number of days to include in rolling aggregations\n    :type window: int, optional\n    :param rolling_comp: computation to use in rolling aggregations\n    :type rolling_comp: str, optional\n    :param barmode: mode to use for bar chart display. possible values are stack|group(default)|overlay|relative\n    :type barmode: str, optional\n    :param barsort: axis name to sort the bars in a bar chart by (default is the 'x', but other options are any of\n                    columns names used in the 'y' parameter\n    :type barsort: str, optional\n    :param filepath: location to save HTML output\n    :type filepath: str, optional\n    :param title: Title of your chart\n    :type title: str, optional\n    :param kwargs: optional keyword arguments, here in case invalid arguments are passed to this function\n    :type kwargs: dict\n    :return: possible outcomes are:\n             - if run within a jupyter notebook and no 'filepath' is specified it will print the resulting HTML\n               within a cell in your notebook\n             - if 'filepath' is specified it will save the chart to the path specified\n             - otherwise it will return the HTML output as a string\n    \"\"\"\n    instance = startup(url=None, data=df, data_id=999, is_proxy=JUPYTER_SERVER_PROXY)\n    output = instance.offline_chart(\n        chart_type=chart_type,\n        query=query,\n        x=x,\n        y=y,\n        z=z,\n        group=group,\n        agg=agg,\n        window=window,\n        rolling_comp=rolling_comp,\n        barmode=barmode,\n        barsort=barsort,\n        yaxis=yaxis,\n        filepath=filepath,\n        title=title,\n        **kwargs\n    )\n    global_state.cleanup()\n    return output\n", "import json\nimport os\n\nfrom six import string_types\nfrom six.moves.configparser import ConfigParser\n\nimport dtale.global_state as global_state\nfrom dtale.utils import dict_merge\n\nLOADED_CONFIG = None\n\n\ndef load_config_state(path):\n    if not path:\n        return None\n    # load .ini file with properties specific to D-Tale\n    config = ConfigParser()\n    config.read(path)\n    return config\n\n\ndef get_config():\n    global LOADED_CONFIG\n\n    if LOADED_CONFIG:\n        return LOADED_CONFIG\n    ini_path = os.path.expandvars(\n        os.environ.get(\"DTALE_CONFIG\") or \"$HOME/.config/dtale.ini\"\n    )\n    if os.path.isfile(ini_path):\n        return load_config_state(ini_path)\n    return None\n\n\ndef set_config(path):\n    global LOADED_CONFIG\n\n    LOADED_CONFIG = load_config_state(path)\n\n\ndef get_config_val(config, defaults, prop, getter=\"get\", section=\"show\"):\n    if config.has_option(section, prop):\n        return getattr(config, getter)(section, prop)\n    return defaults.get(prop)\n\n\ndef load_app_settings(config):\n    if config is None:\n        return\n    curr_app_settings = global_state.get_app_settings()\n    theme = get_config_val(config, curr_app_settings, \"theme\", section=\"app\")\n    pin_menu = get_config_val(\n        config, curr_app_settings, \"pin_menu\", section=\"app\", getter=\"getboolean\"\n    )\n    language = get_config_val(config, curr_app_settings, \"language\", section=\"app\")\n    github_fork = get_config_val(\n        config, curr_app_settings, \"github_fork\", section=\"app\", getter=\"getboolean\"\n    )\n    hide_shutdown = get_config_val(\n        config, curr_app_settings, \"hide_shutdown\", section=\"app\", getter=\"getboolean\"\n    )\n    max_column_width = get_config_val(\n        config, curr_app_settings, \"max_column_width\", section=\"app\", getter=\"getint\"\n    )\n    max_row_height = get_config_val(\n        config, curr_app_settings, \"max_row_height\", section=\"app\", getter=\"getint\"\n    )\n    main_title = get_config_val(config, curr_app_settings, \"main_title\", section=\"app\")\n    main_title_font = get_config_val(\n        config, curr_app_settings, \"main_title_font\", section=\"app\"\n    )\n    query_engine = get_config_val(\n        config, curr_app_settings, \"query_engine\", section=\"app\"\n    )\n    hide_header_editor = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_header_editor\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_header_menu = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_header_menu\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_main_menu = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_main_menu\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_column_menus = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_column_menus\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    lock_header_menu = get_config_val(\n        config,\n        curr_app_settings,\n        \"lock_header_menu\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    open_custom_filter_on_startup = get_config_val(\n        config,\n        curr_app_settings,\n        \"open_custom_filter_on_startup\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    open_predefined_filters_on_startup = get_config_val(\n        config,\n        curr_app_settings,\n        \"open_predefined_filters_on_startup\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_drop_rows = get_config_val(\n        config, curr_app_settings, \"hide_drop_rows\", section=\"app\", getter=\"getboolean\"\n    )\n\n    global_state.set_app_settings(\n        dict(\n            theme=theme,\n            pin_menu=pin_menu,\n            language=language,\n            github_fork=github_fork,\n            hide_shutdown=hide_shutdown,\n            max_column_width=max_column_width,\n            max_row_height=max_row_height,\n            main_title=main_title,\n            main_title_font=main_title_font,\n            query_engine=query_engine,\n            open_custom_filter_on_startup=open_custom_filter_on_startup,\n            open_predefined_filters_on_startup=open_predefined_filters_on_startup,\n            hide_drop_rows=hide_drop_rows,\n            hide_header_editor=hide_header_editor,\n            lock_header_menu=lock_header_menu,\n            hide_header_menu=hide_header_menu,\n            hide_main_menu=hide_main_menu,\n            hide_column_menus=hide_column_menus,\n        )\n    )\n\n\ndef load_chart_settings(config):\n    if config is None:\n        return\n    curr_chart_settings = global_state.get_chart_settings()\n\n    scatter_points = get_config_val(\n        config, curr_chart_settings, \"scatter_points\", section=\"charts\", getter=\"getint\"\n    )\n    three_dimensional_points = get_config_val(\n        config, curr_chart_settings, \"3d_points\", section=\"charts\", getter=\"getint\"\n    )\n    global_state.set_chart_settings(\n        {\"scatter_points\": scatter_points, \"3d_points\": three_dimensional_points}\n    )\n\n\ndef load_auth_settings(config):\n    if config is None:\n        return\n    curr_auth_settings = global_state.get_auth_settings()\n    active = get_config_val(\n        config, curr_auth_settings, \"active\", section=\"auth\", getter=\"getboolean\"\n    )\n    username = get_config_val(config, curr_auth_settings, \"username\", section=\"auth\")\n    password = get_config_val(config, curr_auth_settings, \"password\", section=\"auth\")\n\n    global_state.set_auth_settings(\n        dict(active=active, username=username, password=password)\n    )\n\n\ndef build_show_options(options=None):\n    defaults = dict(\n        host=None,\n        port=None,\n        debug=False,\n        subprocess=True,\n        reaper_on=True,\n        open_browser=False,\n        notebook=False,\n        force=False,\n        ignore_duplicate=True,\n        app_root=None,\n        allow_cell_edits=True,\n        inplace=False,\n        drop_index=False,\n        precision=2,\n        show_columns=None,\n        hide_columns=None,\n        column_formats=None,\n        nan_display=None,\n        sort=None,\n        locked=None,\n        background_mode=None,\n        range_highlights=None,\n        vertical_headers=False,\n        hide_shutdown=None,\n        column_edit_options=None,\n        auto_hide_empty_columns=False,\n        highlight_filter=False,\n        hide_header_editor=None,\n        lock_header_menu=None,\n        hide_header_menu=None,\n        hide_main_menu=None,\n        hide_column_menus=None,\n    )\n    config_options = {}\n    config = get_config()\n    if config and config.has_section(\"show\"):\n        config_options[\"host\"] = get_config_val(config, defaults, \"host\")\n        config_options[\"port\"] = get_config_val(config, defaults, \"port\")\n        config_options[\"debug\"] = get_config_val(\n            config, defaults, \"debug\", \"getboolean\"\n        )\n        config_options[\"subprocess\"] = get_config_val(\n            config, defaults, \"subprocess\", \"getboolean\"\n        )\n        config_options[\"reaper_on\"] = get_config_val(\n            config, defaults, \"reaper_on\", \"getboolean\"\n        )\n        config_options[\"open_browser\"] = get_config_val(\n            config, defaults, \"open_browser\", \"getboolean\"\n        )\n        config_options[\"notebook\"] = get_config_val(\n            config, defaults, \"notebook\", \"getboolean\"\n        )\n        config_options[\"force\"] = get_config_val(\n            config, defaults, \"force\", \"getboolean\"\n        )\n        config_options[\"ignore_duplicate\"] = get_config_val(\n            config, defaults, \"ignore_duplicate\", \"getboolean\"\n        )\n        config_options[\"app_root\"] = get_config_val(config, defaults, \"app_root\")\n        config_options[\"allow_cell_edits\"] = get_config_val(\n            config, defaults, \"allow_cell_edits\"\n        )\n        if isinstance(config_options[\"allow_cell_edits\"], string_types):\n            if config_options[\"allow_cell_edits\"] == \"True\":\n                config_options[\"allow_cell_edits\"] = True\n            elif config_options[\"allow_cell_edits\"] == \"False\":\n                config_options[\"allow_cell_edits\"] = False\n            else:\n                config_options[\"allow_cell_edits\"] = config_options[\n                    \"allow_cell_edits\"\n                ].split(\",\")\n\n        config_options[\"inplace\"] = get_config_val(\n            config, defaults, \"inplace\", \"getboolean\"\n        )\n        config_options[\"drop_index\"] = get_config_val(\n            config, defaults, \"drop_index\", \"getboolean\"\n        )\n        config_options[\"precision\"] = get_config_val(\n            config, defaults, \"precision\", \"getint\"\n        )\n        config_options[\"show_columns\"] = get_config_val(\n            config, defaults, \"show_columns\"\n        )\n        if config_options[\"show_columns\"]:\n            config_options[\"show_columns\"] = config_options[\"show_columns\"].split(\",\")\n        config_options[\"hide_columns\"] = get_config_val(\n            config, defaults, \"hide_columns\"\n        )\n        if config_options[\"hide_columns\"]:\n            config_options[\"hide_columns\"] = config_options[\"hide_columns\"].split(\",\")\n        config_options[\"column_formats\"] = get_config_val(\n            config, defaults, \"column_formats\"\n        )\n        if config_options[\"column_formats\"]:\n            config_options[\"column_formats\"] = json.loads(\n                config_options[\"column_formats\"]\n            )\n        config_options[\"nan_display\"] = get_config_val(config, defaults, \"nan_display\")\n        config_options[\"sort\"] = get_config_val(config, defaults, \"sort\")\n        if config_options[\"sort\"]:\n            config_options[\"sort\"] = [\n                tuple(sort.split(\"|\")) for sort in config_options[\"sort\"].split(\",\")\n            ]\n        config_options[\"locked\"] = get_config_val(config, defaults, \"locked\")\n        if config_options[\"locked\"]:\n            config_options[\"locked\"] = config_options[\"locked\"].split(\",\")\n        config_options[\"background_mode\"] = get_config_val(\n            config, defaults, \"background_mode\"\n        )\n        config_options[\"range_highlights\"] = get_config_val(\n            config, defaults, \"range_highlights\"\n        )\n        if config_options[\"range_highlights\"]:\n            config_options[\"range_highlights\"] = json.loads(\n                config_options[\"range_highlights\"]\n            )\n        config_options[\"vertical_headers\"] = get_config_val(\n            config, defaults, \"vertical_headers\", \"getboolean\"\n        )\n        config_options[\"column_edit_options\"] = get_config_val(\n            config, defaults, \"column_edit_options\"\n        )\n        if config_options[\"column_edit_options\"]:\n            config_options[\"column_edit_options\"] = json.loads(\n                config_options[\"column_edit_options\"]\n            )\n        config_options[\"auto_hide_empty_columns\"] = get_config_val(\n            config, defaults, \"auto_hide_empty_columns\", \"getboolean\"\n        )\n        config_options[\"highlight_filter\"] = get_config_val(\n            config, defaults, \"highlight_filter\", \"getboolean\"\n        )\n\n    return dict_merge(defaults, config_options, options)\n\n\nLOADED_CONFIG = get_config()\nload_app_settings(LOADED_CONFIG)\nload_auth_settings(LOADED_CONFIG)\nload_chart_settings(LOADED_CONFIG)\n", "import string\nimport inspect\n\n\nfrom six import PY3\n\nfrom dtale.utils import dict_merge, format_data\n\ntry:\n    from collections.abc import MutableMapping\nexcept ImportError:\n    from collections import MutableMapping\n\nAPP_SETTINGS = {\n    \"theme\": \"light\",\n    \"pin_menu\": False,\n    \"language\": \"en\",\n    \"github_fork\": False,\n    \"hide_shutdown\": False,\n    \"max_column_width\": None,\n    \"max_row_height\": None,\n    \"main_title\": None,\n    \"main_title_font\": None,\n    \"query_engine\": \"python\",\n    \"open_custom_filter_on_startup\": False,\n    \"open_predefined_filters_on_startup\": False,\n    \"hide_drop_rows\": False,\n    \"hide_header_editor\": False,\n    \"lock_header_menu\": False,\n    \"hide_header_menu\": False,\n    \"hide_main_menu\": False,\n    \"hide_column_menus\": False,\n}\n\nAUTH_SETTINGS = {\"active\": False, \"username\": None, \"password\": None}\n\nCHART_SETTINGS = {\"scatter_points\": 15000, \"3d_points\": 40000}\n\n\nclass DtaleInstance(object):\n    _dataset = None\n    _dataset_dim = None\n    _dtypes = None\n    _metadata = None\n    _context_variables = None\n    _history = None\n    _settings = None\n    _name = \"\"\n    _rows = 0\n\n    def __init__(self, data):\n        self._data = data\n        self._rows = 0 if self._data is None else len(data)\n\n    def load_data(self):\n        return self._data\n\n    def rows(self, **kwargs):\n        return self._rows\n\n    @property\n    def is_large(self):\n        return False\n\n    @property\n    def data(self):\n        return self.load_data()\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def dataset(self):\n        return self._dataset\n\n    @property\n    def dataset_dim(self):\n        return self._dataset_dim\n\n    @property\n    def dtypes(self):\n        return self._dtypes\n\n    @property\n    def metadata(self):\n        return self._metadata\n\n    @property\n    def context_variables(self):\n        return self._context_variables\n\n    @property\n    def history(self):\n        return self._history\n\n    @property\n    def settings(self):\n        return self._settings\n\n    @property\n    def is_xarray_dataset(self):\n        if self._dataset is not None:\n            return True\n        return False\n\n    @data.setter\n    def data(self, data):\n        self._data = data\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    @dataset.setter\n    def dataset(self, dataset):\n        self._dataset = dataset\n\n    @dataset_dim.setter\n    def dataset_dim(self, dataset_dim):\n        self._dataset_dim = dataset_dim\n\n    @dtypes.setter\n    def dtypes(self, dtypes):\n        self._dtypes = dtypes\n\n    @context_variables.setter\n    def context_variables(self, context_variables):\n        self._context_variables = context_variables\n\n    @metadata.setter\n    def metadata(self, metadata):\n        self._metadata = metadata\n\n    @history.setter\n    def history(self, history):\n        self._history = history\n\n    @settings.setter\n    def settings(self, settings):\n        self._settings = settings\n\n\nLARGE_ARCTICDB = 1000000\n\n\ndef get_num_rows(lib, symbol):\n    try:\n        return lib._nvs.get_num_rows(symbol)\n    except BaseException:\n        read_options = lib._nvs._get_read_options()\n        version_query = lib._nvs._get_version_query(None)\n        dit = lib._nvs.version_store.read_descriptor(\n            symbol, version_query, read_options\n        )\n        return dit.timeseries_descriptor.total_rows\n\n\nclass DtaleArcticDBInstance(DtaleInstance):\n    def __init__(self, data, data_id, parent):\n        super(DtaleArcticDBInstance, self).__init__(data)\n        self.parent = parent\n        data_id_segs = (data_id or \"\").split(\"|\")\n        symbol = data_id_segs[-1]\n        if len(data_id_segs) > 1:\n            self.lib_name = data_id_segs[0]\n            if not parent.lib or self.lib_name != parent.lib.name:\n                parent.update_library(self.lib_name)\n        else:\n            self.lib_name = self.parent.lib.name if self.parent.lib else None\n\n        lib = self.parent.get_library(self.lib_name) if self.lib_name else None\n        self.symbol = symbol\n        self._rows = 0\n        self._cols = 0\n        self._base_df = None\n        if lib and self.symbol and self.symbol in self.parent._symbols[self.lib_name]:\n            self._rows = get_num_rows(lib, self.symbol)\n            self._base_df = self.load_data(row_range=[0, 1])\n            self._cols = len(format_data(self._base_df)[0].columns)\n        elif (\n            lib\n            and self.symbol\n            and self.symbol not in self.parent._symbols[self.lib_name]\n        ):\n            raise ValueError(\n                \"Symbol ({}) not in library, {}! Please select another symbol.\".format(\n                    symbol, self.lib_name\n                )\n            )\n\n    def load_data(self, **kwargs):\n        from arcticdb.version_store._store import VersionedItem\n\n        if self.symbol not in self.parent._symbols[self.lib_name]:\n            raise ValueError(\n                \"{} does not exist in {}!\".format(self.symbol, self.lib_name)\n            )\n\n        data = self.parent.get_library(self.lib_name)._nvs.read(self.symbol, **kwargs)\n        if isinstance(data, VersionedItem):\n            return data.data\n        return data\n\n    def rows(self, **kwargs):\n        if kwargs.get(\"query_builder\"):\n            version_query, read_options, read_query = self.parent.get_library(\n                self.lib_name\n            )._nvs._get_queries(\n                None, None, None, None, query_builder=kwargs[\"query_builder\"]\n            )\n            read_result = self.parent.get_library(self.lib_name)._nvs._read_dataframe(\n                self.symbol, version_query, read_query, read_options\n            )\n            return len(read_result.frame_data.value.data[0])\n        return self._rows\n\n    @property\n    def base_df(self):\n        return self._base_df\n\n    @property\n    def is_large(self):\n        if self.rows() > LARGE_ARCTICDB:\n            return True\n        if self._cols > 50:\n            return True\n        return False\n\n    @property\n    def data(self):\n        return self.load_data()\n\n    @data.setter\n    def data(self, data):\n        try:\n            self.parent.get_library(self.lib_name).write(self.symbol, data)\n        except BaseException:\n            pass\n\n\nclass DtaleBaseStore(dict):\n    def build_instance(self, data_id, data=None):\n        return DtaleInstance(data)\n\n\nclass DtaleArcticDB(DtaleBaseStore):\n    \"\"\"Interface allowing dtale to use 'arcticdb' databases for global data storage.\"\"\"\n\n    def __init__(self, uri=None, library=None, **kwargs):\n        from arcticdb import Arctic\n\n        self._db = dict()\n        self.uri = uri\n        self.conn = Arctic(self.uri)\n        self.lib = None\n        self._libraries = []\n        self._symbols = {}\n        self.load_libraries()\n        self.update_library(library)\n\n    def get_library(self, library_name):\n        return self.conn[library_name]\n\n    def update_library(self, library=None):\n        if self.lib and library == self.lib.name:  # library already selected\n            return\n        if library in self._libraries:\n            self.lib = self.get_library(library)\n            if library not in self._symbols:\n                self.load_symbols()\n        elif library is not None:\n            raise ValueError(\"Library '{}' does not exist!\".format(library))\n\n    def load_libraries(self):\n        self._libraries = sorted(self.conn.list_libraries())\n\n    @property\n    def libraries(self):\n        return self._libraries\n\n    def load_symbols(self, library=None):\n        self._symbols[library or self.lib.name] = sorted(\n            (self.conn[library] if library else self.lib).list_symbols()\n        )\n\n    @property\n    def symbols(self):\n        return self._symbols[self.lib.name]\n\n    def build_instance(self, data_id, data=None):\n        if data_id is None:\n            return DtaleInstance(data)\n        return DtaleArcticDBInstance(data, data_id, self)\n\n    def get(self, key, **kwargs):\n        if key is None:\n            return self.build_instance(key)\n        key = str(key)\n        if key not in self._db:\n            self._db[key] = self.build_instance(key)\n        return self._db[key]\n\n    def __setitem__(self, key, value):\n        if key is None:\n            return\n        key = str(key)\n        self._db[key] = value\n\n    def __delitem__(self, key):\n        if key is None:\n            return\n        key = str(key)\n        # TODO: should we actually delete from ArcticDB???\n        del self._db[key]\n\n    def __contains__(self, key):\n        key = str(key)\n        return key in self._db\n\n    def clear(self):\n        pass\n\n    def to_dict(self):\n        return dict(self._db)\n\n    def items(self):\n        return self.to_dict().items()\n\n    def keys(self):\n        return self.to_dict().keys()\n\n    def __len__(self):\n        return len(self.keys())\n\n    def save_db(self):\n        raise NotImplementedError\n\n\nclass DefaultStore(object):\n    def __init__(self):\n        self._data_store = DtaleBaseStore()\n        self._data_names = dict()\n\n    # Use int for data_id for easier sorting\n    def build_data_id(self):\n        if len(self._data_store) == 0:\n            return \"1\"\n\n        def parse_int(x):\n            try:\n                return int(x)\n            except ValueError:\n                return None\n\n        ids = list(filter(None, map(parse_int, self._data_store.keys())))\n        if not len(ids):\n            return \"1\"\n        return str(max(ids) + 1)\n\n    @property\n    def is_arcticdb(self):\n        return isinstance(self._data_store, DtaleArcticDB)\n\n    # exposing  _data_store for custom data store plugins.\n    @property\n    def store(self):\n        return self._data_store\n\n    @store.setter\n    def store(self, new_store):\n        self._data_store = new_store\n\n    def keys(self):\n        return list(self._data_store.keys())\n\n    def items(self):\n        return self._data_store.items()\n\n    def contains(self, key):\n        if key is None:\n            return False\n        return str(key) in self._data_store\n\n    # this should be a property but somehow it stays 0 no matter what.\n    def size(self):\n        return len(self._data_store)\n\n    def get_data_inst(self, data_id):\n        # handle non-exist data_id\n        if data_id is None:\n            return self._data_store.build_instance(data_id)\n\n        if str(data_id) not in self._data_store:\n            self._data_store[str(data_id)] = self._data_store.build_instance(data_id)\n\n        return self._data_store.get(str(data_id))\n\n    def new_data_inst(self, data_id=None, instance=None):\n        if data_id is None:\n            data_id = self.build_data_id()\n        data_id = str(data_id)\n        new_data = instance or self._data_store.build_instance(data_id)\n        self._data_store[data_id] = new_data\n        return data_id\n\n    def get_data(self, data_id, **kwargs):\n        return self.get_data_inst(data_id).load_data(**kwargs)\n\n    def get_data_id_by_name(self, data_name):\n        data_id = next(\n            (\n                value\n                for key, value in self._data_names.items()\n                if convert_name_to_url_path(key) == data_name or key == data_name\n            ),\n            None,\n        )\n        return data_id\n\n    def get_dataset(self, data_id):\n        return self.get_data_inst(data_id).dataset\n\n    def get_dataset_dim(self, data_id):\n        return self.get_data_inst(data_id).dataset_dim\n\n    def get_dtypes(self, data_id):\n        return self.get_data_inst(data_id).dtypes\n\n    def get_context_variables(self, data_id):\n        return self.get_data_inst(data_id).context_variables\n\n    def get_history(self, data_id):\n        return self.get_data_inst(data_id).history\n\n    def get_name(self, data_id):\n        return self.get_data_inst(data_id).name\n\n    def get_settings(self, data_id):\n        return self.get_data_inst(data_id).settings\n\n    def get_metadata(self, data_id):\n        return self.get_data_inst(data_id).metadata\n\n    def set_data(self, data_id=None, val=None):\n        if data_id is None:\n            data_id = self.new_data_inst()\n        data_id = str(data_id)\n        if data_id not in self._data_store.keys():\n            data_id = self.new_data_inst(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.data = val\n        self._data_store[data_id] = data_inst\n\n    def set_dataset(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.dataset = val\n        self._data_store[data_id] = data_inst\n\n    def set_dataset_dim(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.dataset_dim = val\n        self._data_store[data_id] = data_inst\n\n    def set_dtypes(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.dtypes = val\n        self._data_store[data_id] = data_inst\n\n    def set_name(self, data_id, val):\n        if val in [None, \"\"]:\n            return\n        if val in self._data_names:\n            raise Exception(\"Name {} already exists!\".format(val))\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        self._data_names[val] = data_id\n        data_inst.name = val\n        self._data_store[data_id] = data_inst\n\n    def set_context_variables(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.context_variables = val\n        self._data_store[data_id] = data_inst\n\n    def set_settings(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.settings = val\n        self._data_store[data_id] = data_inst\n\n    def set_metadata(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.metadata = val\n        self._data_store[data_id] = data_inst\n\n    def set_history(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.history = val\n        self._data_store[data_id] = data_inst\n\n    def delete_instance(self, data_id):\n        data_id = str(data_id)\n        instance = self._data_store.get(data_id)\n        if instance:\n            if instance.name:\n                try:\n                    del self._data_names[instance.name]\n                except KeyError:\n                    pass\n            try:\n                del self._data_store[data_id]\n            except KeyError:\n                pass\n\n    def clear_store(self):\n        self._data_store.clear()\n        self._data_names.clear()\n\n\n\"\"\"\nThis block dynamically exports functions from DefaultStore class.\nIt's here for backward compatibility reasons.\nIt may trigger linter errors in other py files because functions are not statically exported.\n\"\"\"\n_default_store = DefaultStore()\nfn_list = list(\n    filter(\n        lambda x: not x.startswith(\"_\"),\n        [x[0] for x in inspect.getmembers(DefaultStore)],\n    )\n)\n\nfor fn_name in fn_list:\n    globals()[fn_name] = getattr(_default_store, fn_name)\n\n\n# for tests. default_store is always initialized.\ndef use_default_store():\n    new_store = DtaleBaseStore()\n    for k, v in _as_dict(_default_store.store).items():\n        new_store[str(k)] = v\n    _default_store.store.clear()\n    _default_store.store = new_store\n    globals()[\"is_arcticdb\"] = getattr(_default_store, \"is_arcticdb\")\n    globals()[\"store\"] = getattr(_default_store, \"store\")\n    pass\n\n\ndef drop_punctuation(val):\n    if PY3:\n        return val.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n    return val.translate(string.maketrans(\"\", \"\"), string.punctuation)\n\n\ndef convert_name_to_url_path(name):\n    if name is None:\n        return None\n    url_name = drop_punctuation(\"{}\".format(name))\n    url_name = url_name.lower()\n    return \"_\".join(url_name.split(\" \"))\n\n\ndef get_dtype_info(data_id, col):\n    dtypes = get_dtypes(data_id)  # noqa: F821\n    return next((c for c in dtypes or [] if c[\"name\"] == col), None)\n\n\ndef update_settings(data_id, settings):\n    curr_settings = _default_store.get_settings(data_id) or {}\n    updated_settings = dict_merge(curr_settings, settings)\n    _default_store.set_settings(data_id, updated_settings)\n\n\ndef get_app_settings():\n    global APP_SETTINGS\n    return APP_SETTINGS\n\n\ndef set_app_settings(settings):\n    global APP_SETTINGS\n\n    for prop, val in settings.items():\n        APP_SETTINGS[prop] = val\n\n    instance_updates = {}\n    if settings.get(\"hide_shutdown\") is not None:\n        instance_updates[\"hide_shutdown\"] = settings.get(\"hide_shutdown\")\n    if settings.get(\"hide_header_editor\") is not None:\n        instance_updates[\"hide_header_editor\"] = settings.get(\"hide_header_editor\")\n    if settings.get(\"lock_header_menu\") is not None:\n        instance_updates[\"lock_header_menu\"] = settings.get(\"lock_header_menu\")\n    if settings.get(\"hide_header_menu\") is not None:\n        instance_updates[\"hide_header_menu\"] = settings.get(\"hide_header_menu\")\n    if settings.get(\"hide_main_menu\") is not None:\n        instance_updates[\"hide_main_menu\"] = settings.get(\"hide_main_menu\")\n    if settings.get(\"hide_column_menus\") is not None:\n        instance_updates[\"hide_column_menus\"] = settings.get(\"hide_column_menus\")\n\n    if _default_store.size() > 0 and len(instance_updates):\n        for data_id in _default_store.keys():\n            update_settings(data_id, instance_updates)\n\n\ndef get_auth_settings():\n    global AUTH_SETTINGS\n    return AUTH_SETTINGS\n\n\ndef set_auth_settings(settings):\n    global AUTH_SETTINGS\n\n    for prop, val in settings.items():\n        AUTH_SETTINGS[prop] = val\n\n\ndef get_chart_settings():\n    global CHART_SETTINGS\n    return CHART_SETTINGS\n\n\ndef set_chart_settings(settings):\n    global CHART_SETTINGS\n\n    for prop, val in settings.items():\n        CHART_SETTINGS[prop] = val\n\n\ndef cleanup(data_id=None):\n    if data_id is None:\n        _default_store.clear_store()\n    else:\n        _default_store.delete_instance(data_id)\n\n\ndef update_id(old_data_id, new_data_id):\n    if _default_store.contains(new_data_id):\n        raise Exception(\"Data already exists for id ({})\".format(new_data_id))\n    curr_data = _default_store.get_data_inst(old_data_id)\n    _default_store.delete_instance(old_data_id)\n    data_id = str(new_data_id)\n    _default_store.new_data_inst(data_id, curr_data)\n    return new_data_id\n\n\ndef load_flag(data_id, flag_name, default):\n    import dtale\n\n    app_settings = get_app_settings()\n    curr_settings = get_settings(data_id) or {}  # noqa: F821\n    global_flag = getattr(dtale, flag_name.upper())\n    if global_flag != default:\n        return global_flag\n    if flag_name in app_settings and app_settings[flag_name] != default:\n        return app_settings[flag_name]\n    return curr_settings.get(flag_name, app_settings.get(flag_name, default))\n\n\ndef _as_dict(store):\n    \"\"\"Return the dict representation of a data store.\n    Stores must either be an instance of MutableMapping OR have a to_dict method.\n\n    :param store: data store (dict, redis connection, etc.)\n    :return: dict\n    \"\"\"\n    return dict(store.items()) if isinstance(store, MutableMapping) else store.to_dict()\n\n\ndef use_store(store_class, create_store):\n    \"\"\"\n    Customize how dtale stores and retrieves global data.\n    By default it uses global dictionaries, but this can be problematic if\n    there are memory limitations or multiple python processes are running.\n    Ex: a web server with multiple workers (processes) for processing requests.\n\n    :param store_class: Class providing an interface to the data store. To be valid, it must:\n                        1. Implement get, keys, items clear, __setitem__, __delitem__, __iter__, __len__, __contains__.\n                        2. Either be a subclass of MutableMapping or implement the 'to_dict' method.\n    :param create_store: Factory function for producing instances of <store_class>.\n                         Must take 'name' as the only parameter.\n    :return: None\n    \"\"\"\n\n    assert inspect.isclass(store_class), \"Must be a class\"\n    assert all(\n        hasattr(store_class, a)\n        for a in (\n            \"get\",\n            \"clear\",\n            \"keys\",\n            \"items\",\n            \"__setitem__\",\n            \"__delitem__\",\n            \"__len__\",\n            \"__contains__\",\n        )\n    ), \"Missing required methods\"\n    assert issubclass(store_class, MutableMapping) or hasattr(\n        store_class, \"to_dict\"\n    ), 'Must subclass MutableMapping or implement \"to_dict\"'\n\n    assert inspect.isfunction(create_store), \"Must be a function\"\n    if PY3:\n        assert list(inspect.signature(create_store).parameters) == [\n            \"name\"\n        ], 'Must take \"name\" as the only parameter'\n    else:\n        assert inspect.getargspec(create_store).args == [\n            \"name\"\n        ], 'Must take \"name\" as the only parameter'\n\n    def convert(old_store, name):\n        \"\"\"Convert a data store to the new type\n        :param old_store: old data store\n        :param name: name associated with this data store\n        :return: new data store\n        \"\"\"\n        new_store = create_store(name)\n        assert isinstance(new_store, store_class)\n        new_store.clear()\n        for k, v in _as_dict(old_store).items():\n            new_store[k] = v\n        old_store.clear()\n        return new_store\n\n    _default_store.store = convert(_default_store.store, \"default_store\")\n    globals()[\"is_arcticdb\"] = getattr(_default_store, \"is_arcticdb\")\n    globals()[\"store\"] = getattr(_default_store, \"store\")\n\n\ndef use_shelve_store(directory):\n    \"\"\"\n    Configure dtale to use python's standard 'shelve' library for a persistent global data store.\n\n    :param directory: directory that the shelve db files will be stored in\n    :type directory: str\n    :return: None\n    \"\"\"\n    import shelve\n    import time\n    from os.path import join\n    from threading import Thread\n\n    class DtaleShelf(DtaleBaseStore):\n        \"\"\"Interface allowing dtale to use 'shelf' databases for global data storage.\"\"\"\n\n        def __init__(self, filename):\n            self.filename = filename\n            self.db = shelve.open(self.filename, flag=\"c\", writeback=True)\n            # super hacky autosave\n            t = Thread(target=self.save_db)\n            t.daemon = True\n            t.start()\n\n        def get(self, key):\n            # using str here because shelve doesn't support int keys\n            key = str(key)\n            return self.db.get(key)\n\n        def __setitem__(self, key, value):\n            key = str(key)\n            self.db[key] = value\n            self.db.sync()\n\n        def __delitem__(self, key):\n            key = str(key)\n            del self.db[key]\n            self.db.sync()\n\n        def __contains__(self, key):\n            key = str(key)\n            return key in self.db\n\n        def clear(self):\n            self.db.clear()\n            self.db.sync()\n\n        def to_dict(self):\n            return dict(self.db)\n\n        def items(self):\n            return self.to_dict().items()\n\n        def keys(self):\n            return self.to_dict().keys()\n\n        def __len__(self):\n            return len(self.db)\n\n        def save_db(self):\n            while True:\n                self.db.sync()\n                time.sleep(5)\n\n    def create_shelf(name):\n        file_path = join(directory, name)\n        return DtaleShelf(file_path)\n\n    use_store(DtaleShelf, create_shelf)\n\n\ndef use_redis_store(directory, *args, **kwargs):\n    \"\"\"Configure dtale to use redis for the global data store. Useful for web servers.\n\n    :param db_folder: folder that db files will be stored in\n    :type db_folder: str\n    :param args: All other arguments supported by the redislite.Redis() class\n    :param kwargs: All other keyword arguments supported by the redislite.Redis() class\n    :return: None\n    \"\"\"\n    import pickle\n    from os.path import join\n\n    try:\n        from redislite import Redis\n    except ImportError:\n        raise Exception(\"redislite must be installed\")\n\n    class DtaleRedis(DtaleBaseStore, Redis):\n        \"\"\"Wrapper class around Redis() to make it work as a global data store in dtale.\"\"\"\n\n        def __init__(self, file_path, *args, **kwargs):\n            super(Redis, self).__init__(file_path, *args, **kwargs)\n\n        def __setitem__(self, key, value):\n            key = str(key)\n            self.set(key, value)\n\n        def __delitem__(self, key):\n            key = str(key)\n            super(Redis, self).__delitem__(str(key))\n\n        def __contains__(self, key):\n            key = str(key)\n            return super(Redis, self).__contains__(str(key))\n\n        def get(self, name, *args, **kwargs):\n            value = super(Redis, self).get(name, *args, **kwargs)\n            if value is not None:\n                return pickle.loads(value)\n\n        def keys(self):\n            return [str(k) for k in super(Redis, self).keys()]\n\n        def set(self, name, value, *args, **kwargs):\n            value = pickle.dumps(value)\n            return super(Redis, self).set(name, value, *args, **kwargs)\n\n        def clear(self):\n            self.flushdb()\n\n        def to_dict(self):\n            return {k.decode(\"utf-8\"): self.get(k) for k in super(Redis, self).keys()}\n\n        def items(self):\n            return self.to_dict().items()\n\n        def __len__(self):\n            return len(self.keys())\n\n    def create_redis(name):\n        file_path = join(directory, name + \".db\")\n        return DtaleRedis(file_path, *args, **kwargs)\n\n    use_store(DtaleRedis, create_redis)\n\n\ndef use_arcticdb_store(*args, **kwargs):\n    \"\"\"\n    Configure dtale to use arcticdb for a persistent global data store.\n\n    :param uri: URI that arcticdb will connect to (local file storage: lmdb:///<path>)\n    :type uri: str\n    :param library: default library to load from arcticdb URI\n    :type library: str\n    :param symbol: defualt symbol to load from library\n    :type symbol: str\n    :return: None\n    \"\"\"\n\n    def create_arcticdb(name):\n        return DtaleArcticDB(**kwargs)\n\n    use_store(DtaleArcticDB, create_arcticdb)\n", "<!doctype html>\n<html style=\"{{ 'height: 100%' if network else '' }}\">\n    <head>\n\t<meta charset=\"utf-8\">\n\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=yes\">\n\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n    {% if is_app_root_defined(config.APPLICATION_ROOT) %}\n        <script type=\"text/javascript\">\n            window.resourceBaseUrl = '{{config.APPLICATION_ROOT}}';\n        </script>\n    {% endif %}\n\t<link rel=\"shortcut icon\" href=\"{{ url_for('static', filename='images/favicon.png') }}\">\n    <link rel=\"preload\" href=\"{{ url_for('static', filename='fonts/istok.woff') }}\" as=\"font\" type=\"font/woff\" crossorigin>\n    <link rel=\"preload\" href=\"{{ url_for('static', filename='fonts/istok-bold.woff') }}\" as=\"font\" type=\"font/woff\" crossorigin>\n    <link rel=\"preload\" href=\"{{ url_for('static', filename='fonts/Road_Rage.woff') }}\" as=\"font\" type=\"font/woff\" crossorigin>\n    <title>{{ title }}</title>\n    {% if missing_js is not defined %}\n        <script type=\"text/javascript\" src=\"{{ url_for('static', filename='dist/base_styles_bundle.js') }}\"></script>\n    {% endif %}\n    {% block css %}{% endblock %}\n    {#\n        Despite the fact we reload these again later on, in order for the header to rendered correctly\n        before the rest of the page is built we need to load them here as well.\n    #}\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/main.css') }}\" />\n    </head>\n    <body class=\"{{ theme }}-mode\" style=\"{{ 'height: 100%' if network else '' }}\">\n        {% if github_fork %}\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/github_fork.css') }}\" />\n        <span id=\"forkongithub\">\n            <a href=\"https://github.com/man-group/dtale\">Fork me on GitHub</a>\n        </span>\n        {% endif %}\n        <input type=\"hidden\" id=\"data_id\" value=\"{{data_id}}\" />\n        <input type=\"hidden\" id=\"xarray\" value=\"{{xarray}}\" />\n        <input type=\"hidden\" id=\"xarray_dim\" value=\"{{xarray_dim}}\" />\n        <input type=\"hidden\" id=\"app_settings\" value=\"{{app_settings}}\" />\n        <input type=\"hidden\" id=\"settings\" value=\"{{settings}}\" />\n        <input type=\"hidden\" id=\"version\" value=\"{{version}}\" />\n        <input type=\"hidden\" id=\"hide_shutdown\" value=\"{{hide_shutdown}}\" />\n        <input type=\"hidden\" id=\"hide_header_editor\" value=\"{{hide_header_editor}}\" />\n        <input type=\"hidden\" id=\"lock_header_menu\" value=\"{{lock_header_menu}}\" />\n        <input type=\"hidden\" id=\"hide_header_menu\" value=\"{{hide_header_menu}}\" />\n        <input type=\"hidden\" id=\"hide_main_menu\" value=\"{{hide_main_menu}}\" />\n        <input type=\"hidden\" id=\"hide_column_menus\" value=\"{{hide_column_menus}}\" />\n        <input type=\"hidden\" id=\"allow_cell_edits\" value=\"{{allow_cell_edits}}\" />\n        <input type=\"hidden\" id=\"hide_drop_rows\" value=\"{{hide_drop_rows}}\" />\n        <input type=\"hidden\" id=\"is_vscode\" value=\"{{is_vscode}}\" />\n        <input type=\"hidden\" id=\"is_arcticdb\" value=\"{{is_arcticdb}}\" />\n        <input type=\"hidden\" id=\"arctic_conn\" value=\"{{arctic_conn}}\" />\n        <input type=\"hidden\" id=\"column_count\" value=\"{{column_count}}\" />\n        <input type=\"hidden\" id=\"processes\" value={{processes}} />\n        <input type=\"hidden\" id=\"theme\" value=\"{{theme}}\" />\n        <input type=\"hidden\" id=\"pin_menu\" value=\"{{pin_menu}}\" />\n        <input type=\"hidden\" id=\"language\" value=\"{{language}}\" />\n        <input type=\"hidden\" id=\"python_version\" value=\"{{python_version}}\" />\n        <input type=\"hidden\" id=\"auth\" value=\"{{session.get('logged_in', False)}}\" />\n        <input type=\"hidden\" id=\"username\" value=\"{{session.get('username')}}\" />\n        <input type=\"hidden\" id=\"predefined_filters\" value=\"{{predefined_filters}}\" />\n        <input type=\"hidden\" id=\"max_column_width\" value={{max_column_width}} />\n        <input type=\"hidden\" id=\"max_row_height\" value={{max_row_height}} />\n        <input type=\"hidden\" id=\"main_title\" value=\"{{main_title}}\" />\n        <input type=\"hidden\" id=\"main_title_font\" value=\"{{main_title_font}}\" />\n        <input type=\"hidden\" id=\"query_engine\" value=\"{{query_engine}}\" />\n        <input type=\"hidden\" id=\"open_custom_filter_on_startup\" value=\"{{open_custom_filter_on_startup}}\" />\n        <input type=\"hidden\" id=\"open_predefined_filters_on_startup\" value=\"{{open_predefined_filters_on_startup}}\" />\n        {% block full_content %}{% endblock %}\n        {% if missing_js is not defined %}\n        <script type=\"text/javascript\" src=\"{{ url_for('static', filename='dist/polyfills_bundle.js') }}\"></script>\n        {% endif %}\n        {% block js %}{% endblock %}\n        {#\n          In order to get styles to load correctly, we need to reload these files.\n          In CSS, the last style declared takes precedence.\n        #}\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/main.css') }}\" />\n        {#\n           When loaded in an iframe, the code below will allow the parent\n           window to register and listen for the data id.\n        #}\n        <script type=\"text/javascript\" >\n            const msg = {\n                data_id: '{{data_id}}',\n                xarray: '{{xarray}}',\n                xarray_dim: '{{xarray_dim}}',\n                settings: '{{settings}}',\n                version: '{{version}}',\n                hide_shutdown: '{{hide_shutdown}}',\n                processes: '{{processes}}',\n            };\n            window.addEventListener(\n                'load',\n                function() { window.parent.postMessage(msg, '*'); },\n                false\n            );\n        </script>\n    </body>\n</html>\n", "# coding=utf-8\nfrom __future__ import absolute_import, division\n\nimport os\nimport time\nfrom builtins import map, range, str, zip\nfrom functools import wraps\nfrom logging import getLogger\n\nfrom flask import (\n    current_app,\n    json,\n    make_response,\n    redirect,\n    render_template,\n    request,\n    Response,\n)\n\nimport itertools\nimport missingno as msno\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport platform\nimport requests\nimport scipy.stats as sts\nimport seaborn as sns\nimport xarray as xr\nfrom six import BytesIO, PY3, string_types, StringIO\n\nimport dtale.correlations as correlations\nimport dtale.datasets as datasets\nimport dtale.env_util as env_util\nimport dtale.gage_rnr as gage_rnr\nimport dtale.global_state as global_state\nimport dtale.pandas_util as pandas_util\nimport dtale.predefined_filters as predefined_filters\nfrom dtale import dtale\nfrom dtale.charts.utils import build_base_chart, CHART_POINTS_LIMIT\nfrom dtale.cli.clickutils import retrieve_meta_info_and_version\nfrom dtale.column_analysis import ColumnAnalysis\nfrom dtale.column_builders import ColumnBuilder, printable\nfrom dtale.column_filters import ColumnFilter\nfrom dtale.column_replacements import ColumnReplacement\nfrom dtale.combine_data import CombineData\nfrom dtale.describe import load_describe\nfrom dtale.duplicate_checks import DuplicateCheck\nfrom dtale.dash_application.charts import (\n    build_raw_chart,\n    chart_url_params,\n    chart_url_querystring,\n    export_chart,\n    export_png,\n    export_chart_data,\n    url_encode_func,\n)\nfrom dtale.data_reshapers import DataReshaper\nfrom dtale.code_export import build_code_export\nfrom dtale.query import (\n    build_col_key,\n    build_query,\n    build_query_builder,\n    handle_predefined,\n    load_filterable_data,\n    load_index_filter,\n    run_query,\n)\nfrom dtale.timeseries_analysis import TimeseriesAnalysis\nfrom dtale.utils import (\n    DuplicateDataError,\n    apply,\n    build_formatters,\n    build_shutdown_url,\n    build_url,\n    classify_type,\n    coord_type,\n    dict_merge,\n    divide_chunks,\n    export_to_csv_buffer,\n    find_dtype,\n    find_dtype_formatter,\n    format_data,\n    format_grid,\n    get_bool_arg,\n    get_dtypes,\n    get_int_arg,\n    get_json_arg,\n    get_str_arg,\n    get_url_quote,\n    grid_columns,\n    grid_formatter,\n    json_date,\n    json_float,\n    json_int,\n    json_timestamp,\n    jsonify,\n    jsonify_error,\n    read_file,\n    make_list,\n    optimize_df,\n    option,\n    retrieve_grid_params,\n    running_with_flask_debug,\n    running_with_pytest,\n    sort_df_for_grid,\n    unique_count,\n)\nfrom dtale.translations import text\n\nlogger = getLogger(__name__)\nIDX_COL = str(\"dtale_index\")\n\n\ndef exception_decorator(func):\n    @wraps(func)\n    def _handle_exceptions(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except BaseException as e:\n            return jsonify_error(e)\n\n    return _handle_exceptions\n\n\ndef matplotlib_decorator(func):\n    @wraps(func)\n    def _handle_matplotlib(*args, **kwargs):\n        try:\n            import matplotlib\n\n            current_backend = matplotlib.get_backend()\n\n            matplotlib.use(\"agg\")  # noqa: E261\n\n            import matplotlib.pyplot as plt\n\n            plt.rcParams[\"font.sans-serif\"] = [\n                \"SimHei\"\n            ]  # Or any other Chinese characters\n            matplotlib.rcParams[\"font.family\"] = [\"Heiti TC\"]\n\n            return func(*args, **kwargs)\n        except BaseException as e:\n            raise e\n        finally:\n            try:\n                matplotlib.pyplot.switch_backend(current_backend)\n            except BaseException:\n                pass\n\n    return _handle_matplotlib\n\n\nclass NoDataLoadedException(Exception):\n    \"\"\"Container class for any scenario where no data has been loaded into D-Tale.\n\n    This will usually force the user to load data using the CSV/TSV loader UI.\n    \"\"\"\n\n\ndef head_endpoint(popup_type=None):\n    data_keys = global_state.keys()\n    if not len(data_keys):\n        return \"popup/{}\".format(\"arcticdb\" if global_state.is_arcticdb else \"upload\")\n    head_id = sorted(data_keys)[0]\n    head_id = get_url_quote()(get_url_quote()(head_id, safe=\"\"))\n    if popup_type:\n        return \"popup/{}/{}\".format(popup_type, head_id)\n    return \"main/{}\".format(head_id)\n\n\ndef in_ipython_frontend():\n    \"\"\"\n    Helper function which is variation of :meth:`pandas:pandas.io.formats.console.in_ipython_frontend` which\n    checks to see if we are inside an IPython zmq frontend\n\n    :return: `True` if D-Tale is being invoked within ipython notebook, `False` otherwise\n    \"\"\"\n    try:\n        from IPython import get_ipython\n\n        ip = get_ipython()\n        return \"zmq\" in str(type(ip)).lower()\n    except BaseException:\n        pass\n    return False\n\n\ndef kill(base):\n    \"\"\"\n    This function fires a request to this instance's 'shutdown' route to kill it\n\n    \"\"\"\n    try:\n        requests.get(build_shutdown_url(base))\n    except BaseException:\n        logger.info(\"Shutdown complete\")\n\n\ndef is_up(base):\n    \"\"\"\n    This function checks to see if instance's :mod:`flask:flask.Flask` process is up by hitting 'health' route.\n\n    Using `verify=False` will allow us to validate instances being served up over SSL\n\n    :return: `True` if :mod:`flask:flask.Flask` process is up and running, `False` otherwise\n    \"\"\"\n    try:\n        return requests.get(\"{}/health\".format(base), verify=False, timeout=10).ok\n    except BaseException:\n        return False\n\n\nclass DtaleData(object):\n    \"\"\"\n    Wrapper class to abstract the global state of a D-Tale process while allowing\n    a user to programatically interact with a running D-Tale instance\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param url: endpoint for instances :class:`flask:flask.Flask` process\n    :type url: str\n\n    Attributes:\n        _data_id            data identifier\n        _url                :class:`flask:flask.Flask` endpoint\n        _notebook_handle    reference to the most recent :class:`ipython:IPython.display.DisplayHandle` created\n\n    :Example:\n\n        >>> import dtale\n        >>> import pandas as pd\n        >>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n        >>> d = dtale.show(df)\n        >>> tmp = d.data.copy()\n        >>> tmp['d'] = 4\n        >>> d.data = tmp\n        >>> d.kill()\n    \"\"\"\n\n    def __init__(self, data_id, url, is_proxy=False, app_root=None):\n        self._data_id = data_id\n        self._url = url\n        self._notebook_handle = None\n        self.started_with_open_browser = False\n        self.is_proxy = is_proxy\n        self.app_root = app_root\n\n    def build_main_url(self, name=None):\n        if name or self._data_id:\n            quoted_data_id = get_url_quote()(\n                get_url_quote()(name or self._data_id, safe=\"\")\n            )\n            return \"{}/dtale/main/{}\".format(\n                self.app_root if self.is_proxy else self._url, quoted_data_id\n            )\n        return (\n            self.app_root if self.is_proxy else self._url\n        )  # \"load data\" or \"library/symbol selection\" screens\n\n    @property\n    def _main_url(self):\n        suffix = self._data_id\n        name = global_state.get_name(suffix)\n        if name:\n            suffix = global_state.convert_name_to_url_path(name)\n        return self.build_main_url(name=suffix)\n\n    @property\n    def data(self):\n        \"\"\"\n        Property which is a reference to the globally stored data associated with this instance\n\n        \"\"\"\n        return global_state.get_data(self._data_id)\n\n    @data.setter\n    def data(self, data):\n        \"\"\"\n        Setter which will go through all standard formatting to make sure changes will be handled correctly by D-Tale\n\n        \"\"\"\n        startup(self._url, data=data, data_id=self._data_id)\n\n    def get_corr_matrix(self, encode_strings=False, as_df=False):\n        \"\"\"Helper function to build correlation matrix from data (can be an image or dataframe).\"\"\"\n        matrix_data = build_correlations_matrix(\n            self._data_id, is_pps=False, encode_strings=encode_strings, image=not as_df\n        )\n        _, _, _, _, _, _, df_or_image = matrix_data\n        return df_or_image\n\n    def get_pps_matrix(self, encode_strings=False, as_df=False):\n        \"\"\"Helper function to build correlation matrix from data (can be an image or dataframe).\"\"\"\n        matrix_data = build_correlations_matrix(\n            self._data_id, is_pps=True, encode_strings=encode_strings, image=not as_df\n        )\n        _, _, _, _, _, _, df_or_image = matrix_data\n        return df_or_image\n\n    def update_id(self, new_data_id):\n        \"\"\"\n        Update current data_id to new data_id\n\n        :param new_data_id: the data_id to update to\n        \"\"\"\n        self._data_id = global_state.update_id(self._data_id, new_data_id)\n\n    def main_url(self):\n        \"\"\"\n        Helper function creating main :class:`flask:flask.Flask` route using instance's url & data_id\n        :return: str\n        \"\"\"\n        if in_ipython_frontend():\n            print(self._main_url)\n            return None\n        return self._main_url\n\n    def kill(self):\n        \"\"\"Helper function to pass instance's endpoint to :meth:`dtale.views.kill`\"\"\"\n        kill_url = self._url\n        if in_ipython_frontend() and not kill_url.startswith(\"http\"):\n            from dtale.app import ACTIVE_PORT, ACTIVE_HOST\n\n            kill_url = build_url(ACTIVE_PORT, ACTIVE_HOST)\n        kill(kill_url)\n\n    def cleanup(self):\n        \"\"\"Helper function to clean up data associated with this instance from global state.\"\"\"\n        global_state.cleanup(self._data_id)\n\n    def update_settings(self, **updates):\n        \"\"\"\n        Helper function for updating instance-specific settings. For example:\n        * allow_cell_edits - whether cells can be edited\n        * locked - which columns are locked to the left of the grid\n        * sort - The sort to apply to the data on startup (EX: [(\"col1\", \"ASC\"), (\"col2\", \"DESC\"),...])\n        * custom_formats - display formatting for specific columns\n        * background_mode - different background displays in grid\n        * range_highlights - specify background colors for ranges of values in the grid\n        * vertical_headers - if True, then rotate column headers vertically\n        * column_edit_options - the options to allow on the front-end when editing a cell for the columns specified\n        * highlight_filter - if True, then highlight rows on the frontend which will be filtered when applying a filter\n                             rather than hiding them from the dataframe\n        * hide_shutdown - if true, this will hide the \"Shutdown\" button from users\n        * nan_display - if value in dataframe is :attr:`numpy:numpy.nan` then return this value on the frontend\n        * hide_header_editor - if true, this will hide header editor when editing cells on the frontend\n        * lock_header_menu - if true, this will always the display the header menu which usually only displays when you\n                             hover over the top\n        * hide_header_menu - if true, this will hide the header menu from the screen\n        * hide_main_menu - if true, this will hide the main menu from the screen\n        * hide_column_menus - if true, this will hide the column menus from the screen\n\n        After applying please refresh any open browsers!\n        \"\"\"\n        name_updates = dict(\n            range_highlights=\"rangeHighlight\",\n            column_formats=\"columnFormats\",\n            background_mode=\"backgroundMode\",\n            vertical_headers=\"verticalHeaders\",\n            highlight_filter=\"highlightFilter\",\n        )\n        settings = {name_updates.get(k, k): v for k, v in updates.items()}\n        global_state.update_settings(self._data_id, settings)\n\n    def get_settings(self):\n        \"\"\"Helper function for retrieving any instance-specific settings.\"\"\"\n        return global_state.get_settings(self._data_id) or {}\n\n    def open_browser(self):\n        \"\"\"\n        This function uses the :mod:`python:webbrowser` library to try and automatically open server's default browser\n        to this D-Tale instance\n        \"\"\"\n        env_util.open_browser(self._main_url)\n\n    def is_up(self):\n        \"\"\"\n        Helper function to pass instance's endpoint to :meth:`dtale.views.is_up`\n        \"\"\"\n        return is_up(self._url)\n\n    def __str__(self):\n        \"\"\"\n        Will try to create an :class:`ipython:IPython.display.IFrame` if being invoked from within ipython notebook\n        otherwise simply returns the output of running :meth:`pandas:pandas.DataFrame.__str__` on the data associated\n        with this instance\n\n        \"\"\"\n        if in_ipython_frontend():\n            if self.started_with_open_browser:\n                self.started_with_open_browser = False\n                return \"\"\n            self.notebook()\n            return \"\"\n\n        if global_state.is_arcticdb and global_state.store.get(self._data_id).is_large:\n            return self.main_url()\n\n        return self.data.__str__()\n\n    def __repr__(self):\n        \"\"\"\n        Will try to create an :class:`ipython:IPython.display.IFrame` if being invoked from within ipython notebook\n        otherwise simply returns the output of running :meth:`pandas:pandas.DataFrame.__repr__` on the data for\n        this instance\n\n        \"\"\"\n        if in_ipython_frontend():\n            if self.started_with_open_browser:\n                self.started_with_open_browser = False\n                return \"\"\n            self.notebook()\n            if self._notebook_handle is not None:\n                return \"\"\n        return self.main_url()\n\n    def _build_iframe(\n        self, route=\"/dtale/iframe/\", params=None, width=\"100%\", height=475\n    ):\n        \"\"\"\n        Helper function to build an :class:`ipython:IPython.display.IFrame` if that module exists within\n        your environment\n\n        :param route: the :class:`flask:flask.Flask` route to hit on D-Tale\n        :type route: str, optional\n        :param params: properties & values passed as query parameters to the route\n        :type params: dict, optional\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        :return: :class:`ipython:IPython.display.IFrame`\n        \"\"\"\n        try:\n            from IPython.display import IFrame\n        except ImportError:\n            logger.info(\"in order to use this function, please install IPython\")\n            return None\n        iframe_url = \"{}{}{}\".format(\n            self.app_root if self.is_proxy else self._url, route, self._data_id\n        )\n        if params is not None:\n            if isinstance(params, string_types):  # has this already been encoded?\n                iframe_url = \"{}?{}\".format(iframe_url, params)\n            else:\n                iframe_url = \"{}?{}\".format(iframe_url, url_encode_func()(params))\n\n        return IFrame(iframe_url, width=width, height=height)\n\n    def notebook(self, route=\"/dtale/iframe/\", params=None, width=\"100%\", height=475):\n        \"\"\"\n        Helper function which checks to see if :mod:`flask:flask.Flask` process is up and running and then tries to\n        build an :class:`ipython:IPython.display.IFrame` and run :meth:`ipython:IPython.display.display` on it so\n        it will be displayed in the ipython notebook which invoked it.\n\n        A reference to the :class:`ipython:IPython.display.DisplayHandle` is stored in _notebook_handle for\n        updating if you are running ipython>=5.0\n\n        :param route: the :class:`flask:flask.Flask` route to hit on D-Tale\n        :type route: str, optional\n        :param params: properties & values passed as query parameters to the route\n        :type params: dict, optional\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        \"\"\"\n        try:\n            from IPython.display import display\n        except ImportError:\n            logger.info(\"in order to use this function, please install IPython\")\n            return self.data.__repr__()\n\n        retries = 0\n        while not self.is_up() and retries < 10:\n            time.sleep(0.01)\n            retries += 1\n\n        self._notebook_handle = display(\n            self._build_iframe(route=route, params=params, width=width, height=height),\n            display_id=True,\n        )\n        if self._notebook_handle is None:\n            self._notebook_handle = True\n\n    def notebook_correlations(self, col1, col2, width=\"100%\", height=475):\n        \"\"\"\n        Helper function to build an `ipython:IPython.display.IFrame` pointing at the correlations popup\n\n        :param col1: column on left side of correlation\n        :type col1: str\n        :param col2: column on right side of correlation\n        :type col2: str\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        :return: :class:`ipython:IPython.display.IFrame`\n        \"\"\"\n        self.notebook(\n            \"/dtale/popup/correlations/\",\n            params=dict(col1=col1, col2=col2),\n            width=width,\n            height=height,\n        )\n\n    def notebook_charts(\n        self,\n        chart_type=\"line\",\n        query=None,\n        x=None,\n        y=None,\n        z=None,\n        group=None,\n        agg=None,\n        window=None,\n        rolling_comp=None,\n        barmode=None,\n        barsort=None,\n        width=\"100%\",\n        height=800,\n    ):\n        \"\"\"\n        Helper function to build an `ipython:IPython.display.IFrame` pointing at the charts popup\n\n        :param chart_type: type of chart, possible options are line|bar|pie|scatter|3d_scatter|surface|heatmap\n        :type chart_type: str\n        :param query: pandas dataframe query string\n        :type query: str, optional\n        :param x: column to use for the X-Axis\n        :type x: str\n        :param y: columns to use for the Y-Axes\n        :type y: list of str\n        :param z: column to use for the Z-Axis\n        :type z: str, optional\n        :param group: column(s) to use for grouping\n        :type group: list of str or str, optional\n        :param agg: specific aggregation that can be applied to y or z axes.  Possible values are: count, first, last,\n                    mean, median, min, max, std, var, mad, prod, sum.  This is included in label of axis it is being\n                    applied to.\n        :type agg: str, optional\n        :param window: number of days to include in rolling aggregations\n        :type window: int, optional\n        :param rolling_comp: computation to use in rolling aggregations\n        :type rolling_comp: str, optional\n        :param barmode: mode to use for bar chart display. possible values are stack|group(default)|overlay|relative\n        :type barmode: str, optional\n        :param barsort: axis name to sort the bars in a bar chart by (default is the 'x', but other options are any of\n                        columns names used in the 'y' parameter\n        :type barsort: str, optional\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        :return: :class:`ipython:IPython.display.IFrame`\n        \"\"\"\n        params = dict(\n            chart_type=chart_type,\n            query=query,\n            x=x,\n            y=make_list(y),\n            z=z,\n            group=make_list(group),\n            agg=agg,\n            window=window,\n            rolling_comp=rolling_comp,\n            barmode=barmode,\n            barsort=barsort,\n        )\n        self.notebook(\n            route=\"/dtale/charts/\",\n            params=chart_url_querystring(params),\n            width=width,\n            height=height,\n        )\n\n    def offline_chart(\n        self,\n        chart_type=None,\n        query=None,\n        x=None,\n        y=None,\n        z=None,\n        group=None,\n        agg=None,\n        window=None,\n        rolling_comp=None,\n        barmode=None,\n        barsort=None,\n        yaxis=None,\n        filepath=None,\n        title=None,\n        # fmt: off\n        **kwargs\n        # fmt: on\n    ):\n        \"\"\"\n        Builds the HTML for a plotly chart figure to saved to a file or output to a jupyter notebook\n\n        :param chart_type: type of chart, possible options are line|bar|pie|scatter|3d_scatter|surface|heatmap\n        :type chart_type: str\n        :param query: pandas dataframe query string\n        :type query: str, optional\n        :param x: column to use for the X-Axis\n        :type x: str\n        :param y: columns to use for the Y-Axes\n        :type y: list of str\n        :param z: column to use for the Z-Axis\n        :type z: str, optional\n        :param group: column(s) to use for grouping\n        :type group: list of str or str, optional\n        :param agg: specific aggregation that can be applied to y or z axes.  Possible values are: count, first, last,\n                    mean, median, min, max, std, var, mad, prod, sum.  This is included in label of axis it is being\n                    applied to.\n        :type agg: str, optional\n        :param window: number of days to include in rolling aggregations\n        :type window: int, optional\n        :param rolling_comp: computation to use in rolling aggregations\n        :type rolling_comp: str, optional\n        :param barmode: mode to use for bar chart display. possible values are stack|group(default)|overlay|relative\n        :type barmode: str, optional\n        :param barsort: axis name to sort the bars in a bar chart by (default is the 'x', but other options are any of\n                        columns names used in the 'y' parameter\n        :type barsort: str, optional\n        :param yaxis: dictionary specifying the min/max for each y-axis in your chart\n        :type yaxis: dict, optional\n        :param filepath: location to save HTML output\n        :type filepath: str, optional\n        :param title: Title of your chart\n        :type title: str, optional\n        :param kwargs: optional keyword arguments, here in case invalid arguments are passed to this function\n        :type kwargs: dict\n        :return: possible outcomes are:\n                 - if run within a jupyter notebook and no 'filepath' is specified it will print the resulting HTML\n                   within a cell in your notebook\n                 - if 'filepath' is specified it will save the chart to the path specified\n                 - otherwise it will return the HTML output as a string\n        \"\"\"\n        params = dict(\n            chart_type=chart_type,\n            query=query,\n            x=x,\n            y=make_list(y),\n            z=z,\n            group=make_list(group),\n            agg=agg,\n            window=window,\n            rolling_comp=rolling_comp,\n            barmode=barmode,\n            barsort=barsort,\n            yaxis=yaxis,\n            title=title,\n        )\n        params = dict_merge(params, kwargs)\n\n        if filepath is None and in_ipython_frontend():\n            from plotly.offline import iplot, init_notebook_mode\n\n            init_notebook_mode(connected=True)\n            chart = build_raw_chart(self._data_id, export=True, **params)\n            chart.pop(\n                \"id\", None\n            )  # for some reason iplot does not like when the 'id' property is populated\n            iplot(chart)\n            return\n\n        html_str = export_chart(self._data_id, params)\n        if filepath is None:\n            return html_str\n\n        if not filepath.endswith(\".html\"):\n            filepath = \"{}.html\".format(filepath)\n\n        with open(filepath, \"w\") as f:\n            f.write(html_str)\n\n    def adjust_cell_dimensions(self, width=\"100%\", height=350):\n        \"\"\"\n        If you are running ipython>=5.0 then this will update the most recent notebook cell you displayed D-Tale in\n        for this instance with the height/width properties you have passed in as input\n\n        :param width: width of the ipython cell\n        :param height: height of the ipython cell\n        \"\"\"\n        if self._notebook_handle is not None and hasattr(\n            self._notebook_handle, \"update\"\n        ):\n            self._notebook_handle.update(self._build_iframe(width=width, height=height))\n        else:\n            logger.debug(\"You must ipython>=5.0 installed to use this functionality\")\n\n\ndef dtype_formatter(data, dtypes, data_ranges, prev_dtypes=None):\n    \"\"\"\n    Helper function to build formatter for the descriptive information about each column in the dataframe you\n    are viewing in D-Tale.  This data is later returned to the browser to help with controlling inputs to functions\n    which are heavily tied to specific data types.\n\n    :param data: dataframe\n    :type data: :class:`pandas:pandas.DataFrame`\n    :param dtypes: column data type\n    :type dtypes: dict\n    :param data_ranges: dictionary containing minimum and maximum value for column (if applicable)\n    :type data_ranges: dict, optional\n    :param prev_dtypes: previous column information for syncing updates to pre-existing columns\n    :type prev_dtypes: dict, optional\n    :return: formatter function which takes column indexes and names\n    :rtype: func\n    \"\"\"\n\n    def _formatter(col_index, col):\n        visible = True\n        dtype = dtypes.get(col)\n        if prev_dtypes and col in prev_dtypes:\n            visible = prev_dtypes[col].get(\"visible\", True)\n        s = data[col]\n        dtype_data = dict(\n            name=col,\n            dtype=dtype,\n            index=col_index,\n            visible=visible,\n            hasOutliers=0,\n            hasMissing=1,\n        )\n        if global_state.is_arcticdb:\n            return dtype_data\n\n        dtype_data[\"unique_ct\"] = unique_count(s)\n        dtype_data[\"hasMissing\"] = int(s.isnull().sum())\n        classification = classify_type(dtype)\n        if (\n            classification in [\"F\", \"I\"] and not s.isnull().all() and col in data_ranges\n        ):  # floats/ints\n            col_ranges = data_ranges[col]\n            if not any((np.isnan(v) or np.isinf(v) for v in col_ranges.values())):\n                dtype_data = dict_merge(col_ranges, dtype_data)\n\n            # load outlier information\n            o_s, o_e = calc_outlier_range(s)\n            if not any((np.isnan(v) or np.isinf(v) for v in [o_s, o_e])):\n                dtype_data[\"hasOutliers\"] += int(((s < o_s) | (s > o_e)).sum())\n                dtype_data[\"outlierRange\"] = dict(lower=o_s, upper=o_e)\n            dtype_data[\"skew\"] = json_float(s.skew())\n            dtype_data[\"kurt\"] = json_float(s.kurt())\n\n        if classification in [\"F\", \"I\"] and not s.isnull().all():\n            # build variance flag\n            unique_ct = dtype_data[\"unique_ct\"]\n            check1 = (unique_ct / len(data[col])) < 0.1\n            check2 = False\n            if check1 and unique_ct >= 2:\n                val_counts = s.value_counts()\n                check2 = (val_counts.values[0] / val_counts.values[1]) > 20\n            dtype_data[\"lowVariance\"] = bool(check1 and check2)\n            dtype_data[\"coord\"] = coord_type(s)\n\n        if classification in [\"D\"] and not s.isnull().all():\n            timestamps = apply(s, lambda x: json_timestamp(x, np.nan))\n            dtype_data[\"skew\"] = json_float(timestamps.skew())\n            dtype_data[\"kurt\"] = json_float(timestamps.kurt())\n\n        if classification == \"S\" and not dtype_data[\"hasMissing\"]:\n            if (\n                dtype.startswith(\"category\")\n                and classify_type(s.dtype.categories.dtype.name) == \"S\"\n            ):\n                dtype_data[\"hasMissing\"] += int(\n                    (apply(s, lambda x: str(x).strip()) == \"\").sum()\n                )\n            else:\n                dtype_data[\"hasMissing\"] += int(\n                    (s.astype(\"str\").str.strip() == \"\").sum()\n                )\n\n        return dtype_data\n\n    return _formatter\n\n\ndef calc_data_ranges(data, dtypes={}):\n    try:\n        return data.agg([\"min\", \"max\"]).to_dict()\n    except ValueError:\n        # I've seen when transposing data and data types get combined into one column this exception emerges\n        # when calling 'agg' on the new data\n        return {}\n    except TypeError:\n        non_str_cols = [\n            c for c in data.columns if classify_type(dtypes.get(c, \"\")) != \"S\"\n        ]\n        if not len(non_str_cols):\n            return {}\n        try:\n            return data[non_str_cols].agg([\"min\", \"max\"]).to_dict()\n        except BaseException:\n            return {}\n\n\ndef build_dtypes_state(data, prev_state=None, ranges=None):\n    \"\"\"\n    Helper function to build globally managed state pertaining to a D-Tale instances columns & data types\n\n    :param data: dataframe to build data type information for\n    :type data: :class:`pandas:pandas.DataFrame`\n    :return: a list of dictionaries containing column names, indexes and data types\n    \"\"\"\n    prev_dtypes = {c[\"name\"]: c for c in prev_state or []}\n    dtypes = get_dtypes(data)\n    loaded_ranges = ranges or calc_data_ranges(data, dtypes)\n    dtype_f = dtype_formatter(data, dtypes, loaded_ranges, prev_dtypes)\n    return [dtype_f(i, c) for i, c in enumerate(data.columns)]\n\n\ndef check_duplicate_data(data):\n    \"\"\"\n    This function will do a rough check to see if a user has already loaded this piece of data to D-Tale to avoid\n    duplicated state.  The checks that take place are:\n     - shape (# of rows & # of columns\n     - column names and ordering of columns (eventually might add dtype checking as well...)\n\n    :param data: dataframe to validate\n    :type data: :class:`pandas:pandas.DataFrame`\n    :raises :class:`dtale.utils.DuplicateDataError`: if duplicate data exists\n    \"\"\"\n    cols = [str(col) for col in data.columns]\n\n    for d_id, datainst in global_state.items():\n        d_cols = [str(col) for col in datainst.data.columns]\n        if datainst.data.shape == data.shape and cols == d_cols:\n            raise DuplicateDataError(d_id)\n\n\ndef convert_xarray_to_dataset(dataset, **indexers):\n    def _convert_zero_dim_dataset(dataset):\n        ds_dict = dataset.to_dict()\n        data = {}\n        for coord, coord_data in ds_dict[\"coords\"].items():\n            data[coord] = coord_data[\"data\"]\n        for col, col_data in ds_dict[\"data_vars\"].items():\n            data[col] = col_data[\"data\"]\n        return pd.DataFrame([data]).set_index(list(ds_dict[\"coords\"].keys()))\n\n    ds_sel = dataset.sel(**indexers)\n    try:\n        df = ds_sel.to_dataframe()\n        df = df.reset_index().drop(\"index\", axis=1, errors=\"ignore\")\n        return df.set_index(list(dataset.dims.keys()))\n    except ValueError:\n        return _convert_zero_dim_dataset(ds_sel)\n\n\ndef handle_koalas(data):\n    \"\"\"\n    Helper function to check if koalas is installed and also if incoming data is a koalas dataframe, if so convert it\n    to :class:`pandas:pandas.DataFrame`, otherwise simply return the original data structure.\n\n    :param data: data we want to check if its a koalas dataframe and if so convert to :class:`pandas:pandas.DataFrame`\n    :return: :class:`pandas:pandas.DataFrame`\n    \"\"\"\n    if is_koalas(data):\n        return data.to_pandas()\n    return data\n\n\ndef is_koalas(data):\n    try:\n        from databricks.koalas import frame\n\n        return isinstance(data, frame.DataFrame)\n    except BaseException:\n        return False\n\n\ndef startup(\n    url=\"\",\n    data=None,\n    data_loader=None,\n    name=None,\n    data_id=None,\n    context_vars=None,\n    ignore_duplicate=True,\n    allow_cell_edits=True,\n    inplace=False,\n    drop_index=False,\n    precision=2,\n    show_columns=None,\n    hide_columns=None,\n    optimize_dataframe=False,\n    column_formats=None,\n    nan_display=None,\n    sort=None,\n    locked=None,\n    background_mode=None,\n    range_highlights=None,\n    app_root=None,\n    is_proxy=None,\n    vertical_headers=False,\n    hide_shutdown=None,\n    column_edit_options=None,\n    auto_hide_empty_columns=False,\n    highlight_filter=False,\n    hide_header_editor=None,\n    lock_header_menu=None,\n    hide_header_menu=None,\n    hide_main_menu=None,\n    hide_column_menus=None,\n    force_save=True,\n):\n    \"\"\"\n    Loads and stores data globally\n     - If data has indexes then it will lock save those columns as locked on the front-end\n     - If data has column named index it will be dropped so that it won't collide with row numbering (dtale_index)\n     - Create location in memory for storing settings which can be manipulated from the front-end (sorts, filter, ...)\n\n    :param url: the base URL that D-Tale is running from to be referenced in redirects to shutdown\n    :param data: :class:`pandas:pandas.DataFrame` or :class:`pandas:pandas.Series`\n    :param data_loader: function which returns :class:`pandas:pandas.DataFrame`\n    :param name: string label to apply to your session\n    :param data_id: integer id assigned to a piece of data viewable in D-Tale, if this is populated then it will\n                    override the data at that id\n    :param context_vars: a dictionary of the variables that will be available for use in user-defined expressions,\n                         such as filters\n    :type context_vars: dict, optional\n    :param ignore_duplicate: if set to True this will not test whether this data matches any previously loaded to D-Tale\n    :param allow_cell_edits: If false, this will not allow users to edit cells directly in their D-Tale grid\n    :type allow_cell_edits: bool, optional\n    :param inplace: If true, this will call `reset_index(inplace=True)` on the dataframe used as a way to save memory.\n                    Otherwise this will create a brand new dataframe, thus doubling memory but leaving the dataframe\n                    input unchanged.\n    :type inplace: bool, optional\n    :param drop_index: If true, this will drop any pre-existing index on the dataframe input.\n    :type drop_index: bool, optional\n    :param precision: The default precision to display for float data in D-Tale grid\n    :type precision: int, optional\n    :param show_columns: Columns to show on load, hide all others\n    :type show_columns: list, optional\n    :param hide_columns: Columns to hide on load\n    :type hide_columns: list, optional\n    :param optimize_dataframe: this will convert string columns with less certain then a certain number of distinct\n                              values into categories\n    :type optimize_dataframe: boolean\n    :param column_formats: The formatting to apply to certain columns on the front-end\n    :type column_formats: dict, optional\n    :param sort: The sort to apply to the data on startup (EX: [(\"col1\", \"ASC\"), (\"col2\", \"DESC\"),...])\n    :type sort: list[tuple], optional\n    :param locked: Columns to lock to the left of your grid on load\n    :type locked: list, optional\n    :param background_mode: Different background highlighting modes available on the frontend. Possible values are:\n                            - heatmap-all: turn on heatmap for all numeric columns where the colors are determined by\n                                           the range of values over all numeric columns combined\n                            - heatmap-col: turn on heatmap for all numeric columns where the colors are determined by\n                                           the range of values in the column\n                            - heatmap-col-[column name]: turn on heatmap highlighting for a specific column\n                            - dtypes: highlight columns based on it's data type\n                            - missing: highlight any missing values (np.nan, empty strings, strings of all spaces)\n                            - outliers: highlight any outliers\n                            - range: highlight values for any matchers entered in the \"range_highlights\" option\n                            - lowVariance: highlight values with a low variance\n    :type background_mode: string, optional\n    :param range_highlights: Definitions for equals, less-than or greater-than ranges for individual (or all) columns\n                             which apply different background colors to cells which fall in those ranges.\n    :type range_highlights: dict, optional\n    :param vertical_headers: if True, then rotate column headers vertically\n    :type vertical_headers: boolean, optional\n    :param column_edit_options: The options to allow on the front-end when editing a cell for the columns specified\n    :type column_edit_options: dict, optional\n    :param auto_hide_empty_columns: if True, then auto-hide any columns on the front-end that are comprised entirely of\n                                    NaN values\n    :type auto_hide_empty_columns: boolean, optional\n    :param highlight_filter: if True, then highlight rows on the frontend which will be filtered when applying a filter\n                             rather than hiding them from the dataframe\n    :type highlight_filter: boolean, optional\n    \"\"\"\n\n    if (\n        data_loader is None and data is None\n    ):  # scenario where we'll force users to upload a CSV/TSV\n        return DtaleData(\"1\", url, is_proxy=is_proxy, app_root=app_root)\n\n    if data_loader is not None:\n        data = data_loader()\n        if isinstance(data, string_types) and global_state.contains(data):\n            return DtaleData(data, url, is_proxy=is_proxy, app_root=app_root)\n        elif (\n            data is None and global_state.is_arcticdb\n        ):  # send user to the library/symbol selection screen\n            return DtaleData(None, url, is_proxy=is_proxy, app_root=app_root)\n\n    if global_state.is_arcticdb and isinstance(data, string_types):\n        data_id = data\n        data_id_segs = data_id.split(\"|\")\n        if len(data_id_segs) < 2:\n            if not global_state.store.lib:\n                raise ValueError(\n                    (\n                        \"When specifying a data identifier for ArcticDB it must be comprised of a library and a symbol.\"\n                        \"Use the following format: [library]|[symbol]\"\n                    )\n                )\n            data_id = \"{}|{}\".format(global_state.store.lib.name, data_id)\n        global_state.new_data_inst(data_id)\n        instance = global_state.store.get(data_id)\n        data = instance.base_df\n        ret_data = startup(\n            url=url,\n            data=data,\n            data_id=data_id,\n            force_save=False,\n            name=name,\n            context_vars=context_vars,\n            ignore_duplicate=ignore_duplicate,\n            allow_cell_edits=allow_cell_edits,\n            precision=precision,\n            show_columns=show_columns,\n            hide_columns=hide_columns,\n            column_formats=column_formats,\n            nan_display=nan_display,\n            sort=sort,\n            locked=locked,\n            background_mode=background_mode,\n            range_highlights=range_highlights,\n            app_root=app_root,\n            is_proxy=is_proxy,\n            vertical_headers=vertical_headers,\n            hide_shutdown=hide_shutdown,\n            column_edit_options=column_edit_options,\n            auto_hide_empty_columns=auto_hide_empty_columns,\n            highlight_filter=highlight_filter,\n            hide_header_editor=hide_header_editor,\n            lock_header_menu=lock_header_menu,\n            hide_header_menu=hide_header_menu,\n            hide_main_menu=hide_main_menu,\n            hide_column_menus=hide_column_menus,\n        )\n        startup_code = (\n            \"from arcticdb import Arctic\\n\"\n            \"from arcticdb.version_store._store import VersionedItem\\n\\n\"\n            \"conn = Arctic('{uri}')\\n\"\n            \"lib = conn.get_library('{library}')\\n\"\n            \"df = lib.read('{symbol}')\\n\"\n            \"if isinstance(data, VersionedItem):\\n\"\n            \"\\tdf = df.data\\n\"\n        ).format(\n            uri=global_state.store.uri,\n            library=global_state.store.lib.name,\n            symbol=data_id,\n        )\n        curr_settings = global_state.get_settings(data_id)\n        global_state.set_settings(\n            data_id, dict_merge(curr_settings, dict(startup_code=startup_code))\n        )\n        return ret_data\n\n    if data is not None:\n        data = handle_koalas(data)\n        valid_types = (\n            pd.DataFrame,\n            pd.Series,\n            pd.DatetimeIndex,\n            pd.MultiIndex,\n            xr.Dataset,\n            np.ndarray,\n            list,\n            dict,\n        )\n        if not isinstance(data, valid_types):\n            raise Exception(\n                (\n                    \"data loaded must be one of the following types: pandas.DataFrame, pandas.Series, \"\n                    \"pandas.DatetimeIndex, pandas.MultiIndex, xarray.Dataset, numpy.array, numpy.ndarray, list, dict\"\n                )\n            )\n\n        if isinstance(data, xr.Dataset):\n            df = convert_xarray_to_dataset(data)\n            instance = startup(\n                url,\n                df,\n                name=name,\n                data_id=data_id,\n                context_vars=context_vars,\n                ignore_duplicate=ignore_duplicate,\n                allow_cell_edits=allow_cell_edits,\n                precision=precision,\n                show_columns=show_columns,\n                hide_columns=hide_columns,\n                column_formats=column_formats,\n                nan_display=nan_display,\n                sort=sort,\n                locked=locked,\n                background_mode=background_mode,\n                range_highlights=range_highlights,\n                app_root=app_root,\n                is_proxy=is_proxy,\n                vertical_headers=vertical_headers,\n                hide_shutdown=hide_shutdown,\n                column_edit_options=column_edit_options,\n                auto_hide_empty_columns=auto_hide_empty_columns,\n                highlight_filter=highlight_filter,\n                hide_header_editor=hide_header_editor,\n                lock_header_menu=lock_header_menu,\n                hide_header_menu=hide_header_menu,\n                hide_main_menu=hide_main_menu,\n                hide_column_menus=hide_column_menus,\n            )\n\n            global_state.set_dataset(instance._data_id, data)\n            global_state.set_dataset_dim(instance._data_id, {})\n            return instance\n\n        data, curr_index = format_data(data, inplace=inplace, drop_index=drop_index)\n        # check to see if this dataframe has already been loaded to D-Tale\n        if data_id is None and not ignore_duplicate and not global_state.is_arcticdb:\n            check_duplicate_data(data)\n\n        logger.debug(\n            \"pytest: {}, flask-debug: {}\".format(\n                running_with_pytest(), running_with_flask_debug()\n            )\n        )\n\n        if data_id is None:\n            data_id = global_state.new_data_inst()\n        if global_state.get_settings(data_id) is not None:\n            curr_settings = global_state.get_settings(data_id)\n            curr_locked = curr_settings.get(\"locked\", [])\n            # filter out previous locked columns that don't exist\n            curr_locked = [c for c in curr_locked if c in data.columns]\n            # add any new columns in index\n            curr_locked += [c for c in curr_index if c not in curr_locked]\n        else:\n            logger.debug(\n                \"pre-locking index columns ({}) to settings[{}]\".format(\n                    curr_index, data_id\n                )\n            )\n            curr_locked = locked or curr_index\n            global_state.set_metadata(data_id, dict(start=pd.Timestamp(\"now\")))\n        global_state.set_name(data_id, name)\n        # in the case that data has been updated we will drop any sorts or filter for ease of use\n        base_settings = dict(\n            indexes=curr_index,\n            locked=curr_locked,\n            allow_cell_edits=True if allow_cell_edits is None else allow_cell_edits,\n            precision=precision,\n            columnFormats=column_formats or {},\n            backgroundMode=background_mode,\n            rangeHighlight=range_highlights,\n            verticalHeaders=vertical_headers,\n            highlightFilter=highlight_filter,\n        )\n        base_predefined = predefined_filters.init_filters()\n        if base_predefined:\n            base_settings[\"predefinedFilters\"] = base_predefined\n        if sort:\n            base_settings[\"sortInfo\"] = sort\n            data = sort_df_for_grid(data, dict(sort=sort))\n        if nan_display is not None:\n            base_settings[\"nanDisplay\"] = nan_display\n        if hide_shutdown is not None:\n            base_settings[\"hide_shutdown\"] = hide_shutdown\n        if hide_header_editor is not None:\n            base_settings[\"hide_header_editor\"] = hide_header_editor\n        if lock_header_menu is not None:\n            base_settings[\"lock_header_menu\"] = lock_header_menu\n        if hide_header_menu is not None:\n            base_settings[\"hide_header_menu\"] = hide_header_menu\n        if hide_main_menu is not None:\n            base_settings[\"hide_main_menu\"] = hide_main_menu\n        if hide_column_menus is not None:\n            base_settings[\"hide_column_menus\"] = hide_column_menus\n        if column_edit_options is not None:\n            base_settings[\"column_edit_options\"] = column_edit_options\n        global_state.set_settings(data_id, base_settings)\n        if optimize_dataframe and not global_state.is_arcticdb:\n            data = optimize_df(data)\n        if force_save or (\n            global_state.is_arcticdb and not global_state.contains(data_id)\n        ):\n            data = data[curr_locked + [c for c in data.columns if c not in curr_locked]]\n            global_state.set_data(data_id, data)\n        dtypes_data = data\n        ranges = None\n        if global_state.is_arcticdb:\n            instance = global_state.store.get(data_id)\n            if not instance.is_large:\n                dtypes_data = instance.load_data()\n                dtypes_data, _ = format_data(\n                    dtypes_data, inplace=inplace, drop_index=drop_index\n                )\n                ranges = calc_data_ranges(dtypes_data)\n                dtypes_data = dtypes_data[\n                    curr_locked\n                    + [c for c in dtypes_data.columns if c not in curr_locked]\n                ]\n        dtypes_state = build_dtypes_state(\n            dtypes_data, global_state.get_dtypes(data_id) or [], ranges=ranges\n        )\n\n        for col in dtypes_state:\n            if show_columns and col[\"name\"] not in show_columns:\n                col[\"visible\"] = False\n                continue\n            if hide_columns and col[\"name\"] in hide_columns:\n                col[\"visible\"] = False\n                continue\n            if col[\"index\"] >= 100:\n                col[\"visible\"] = False\n        if auto_hide_empty_columns and not global_state.is_arcticdb:\n            is_empty = data.isnull().all()\n            is_empty = list(is_empty[is_empty].index.values)\n            for col in dtypes_state:\n                if col[\"name\"] in is_empty:\n                    col[\"visible\"] = False\n        global_state.set_dtypes(data_id, dtypes_state)\n        global_state.set_context_variables(\n            data_id, build_context_variables(data_id, context_vars)\n        )\n        return DtaleData(data_id, url, is_proxy=is_proxy, app_root=app_root)\n    else:\n        raise NoDataLoadedException(\"No data has been loaded into this D-Tale session!\")\n\n\ndef is_vscode():\n    if os.environ.get(\"VSCODE_PID\") is not None:\n        return True\n    if \"1\" == os.environ.get(\"VSCODE_INJECTION\"):\n        return True\n    return False\n\n\ndef base_render_template(template, data_id, **kwargs):\n    \"\"\"\n    Overriden version of Flask.render_template which will also include vital instance information\n     - settings\n     - version\n     - processes\n    \"\"\"\n    if not len(os.listdir(\"{}/static/dist\".format(os.path.dirname(__file__)))):\n        return redirect(current_app.url_for(\"missing_js\"))\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_app_settings = global_state.get_app_settings()\n    _, version = retrieve_meta_info_and_version(\"dtale\")\n    hide_shutdown = global_state.load_flag(data_id, \"hide_shutdown\", False)\n    allow_cell_edits = global_state.load_flag(data_id, \"allow_cell_edits\", True)\n    github_fork = global_state.load_flag(data_id, \"github_fork\", False)\n    hide_header_editor = global_state.load_flag(data_id, \"hide_header_editor\", False)\n    lock_header_menu = global_state.load_flag(data_id, \"lock_header_menu\", False)\n    hide_header_menu = global_state.load_flag(data_id, \"hide_header_menu\", False)\n    hide_main_menu = global_state.load_flag(data_id, \"hide_main_menu\", False)\n    hide_column_menus = global_state.load_flag(data_id, \"hide_column_menus\", False)\n    app_overrides = dict(\n        allow_cell_edits=json.dumps(allow_cell_edits),\n        hide_shutdown=hide_shutdown,\n        hide_header_editor=hide_header_editor,\n        lock_header_menu=lock_header_menu,\n        hide_header_menu=hide_header_menu,\n        hide_main_menu=hide_main_menu,\n        hide_column_menus=hide_column_menus,\n        github_fork=github_fork,\n    )\n    is_arcticdb = 0\n    arctic_conn = \"\"\n    if global_state.is_arcticdb:\n        instance = global_state.store.get(data_id)\n        is_arcticdb = instance.rows()\n        arctic_conn = global_state.store.uri\n    return render_template(\n        template,\n        data_id=get_url_quote()(get_url_quote()(data_id, safe=\"\"))\n        if data_id is not None\n        else \"\",\n        xarray=global_state.get_data_inst(data_id).is_xarray_dataset,\n        xarray_dim=json.dumps(global_state.get_dataset_dim(data_id)),\n        settings=json.dumps(curr_settings),\n        version=str(version),\n        processes=global_state.size(),\n        python_version=platform.python_version(),\n        predefined_filters=json.dumps(\n            [f.asdict() for f in predefined_filters.get_filters()]\n        ),\n        is_vscode=is_vscode(),\n        is_arcticdb=is_arcticdb,\n        arctic_conn=arctic_conn,\n        column_count=len(global_state.get_dtypes(data_id) or []),\n        # fmt: off\n        **dict_merge(kwargs, curr_app_settings, app_overrides)\n        # fmt: on\n    )\n\n\ndef _view_main(data_id, iframe=False):\n    \"\"\"\n    Helper function rendering main HTML which will also build title and store whether we are viewing from an <iframe>\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param iframe: boolean flag indicating whether this is being viewed from an <iframe> (usually means ipython)\n    :type iframe: bool, optional\n    :return: HTML\n    \"\"\"\n    title = \"D-Tale\"\n    name = global_state.get_name(data_id)\n    if name:\n        title = \"{} ({})\".format(title, name)\n    return base_render_template(\"dtale/main.html\", data_id, title=title, iframe=iframe)\n\n\n@dtale.route(\"/main\")\n@dtale.route(\"/main/<data_id>\")\ndef view_main(data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if not global_state.contains(data_id):\n        if global_state.is_arcticdb:\n            try:\n                startup(data=data_id)\n                return _view_main(data_id)\n            except BaseException as ex:\n                logger.exception(ex)\n        return redirect(\"/dtale/{}\".format(head_endpoint()))\n    return _view_main(data_id)\n\n\n@dtale.route(\"/main/name/<data_name>\")\ndef view_main_by_name(data_name=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_name: integer string identifier for a D-Tale process's data\n    :type data_name: str\n    :return: HTML\n    \"\"\"\n    data_id = global_state.get_data_id_by_name(data_name)\n    return view_main(data_id)\n\n\n@dtale.route(\"/iframe\")\n@dtale.route(\"/iframe/<data_id>\")\ndef view_iframe(data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if data_id is None:\n        return redirect(\"/dtale/iframe/{}\".format(head_endpoint()))\n    return _view_main(data_id, iframe=True)\n\n\n@dtale.route(\"/iframe/popup/<popup_type>\")\n@dtale.route(\"/iframe/popup/<popup_type>/<data_id>\")\ndef iframe_popup(popup_type, data_id=None):\n    route = \"/dtale/popup/{}\".format(popup_type)\n    if data_id:\n        return redirect(\"{}/{}\".format(route, data_id))\n    return redirect(route)\n\n\nPOPUP_TITLES = {\n    \"reshape\": \"Summarize Data\",\n    \"filter\": \"Custom Filter\",\n    \"upload\": \"Load Data\",\n    \"pps\": \"Predictive Power Score\",\n    \"merge\": \"Merge & Stack\",\n    \"arcticdb\": \"Load ArcticDB Data\",\n}\n\n\n@dtale.route(\"/popup/<popup_type>\")\n@dtale.route(\"/popup/<popup_type>/<data_id>\")\ndef view_popup(popup_type, data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up a base jinja template for any popup, additionally forwards any\n    request parameters as input to template.\n\n    :param popup_type: type of popup to be opened. Possible values: charts, correlations, describe, histogram, instances\n    :type popup_type: str\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if data_id is None and popup_type not in [\"upload\", \"merge\", \"arcticdb\"]:\n        return redirect(\"/dtale/{}\".format(head_endpoint(popup_type)))\n    main_title = global_state.get_app_settings().get(\"main_title\")\n    title = main_title or \"D-Tale\"\n    name = global_state.get_name(data_id)\n    if name:\n        title = \"{} ({})\".format(title, name)\n    popup_title = text(\n        POPUP_TITLES.get(popup_type)\n        or \" \".join([pt.capitalize() for pt in popup_type.split(\"-\")])\n    )\n    title = \"{} - {}\".format(title, popup_title)\n    params = request.args.to_dict()\n    if len(params):\n\n        def pretty_print(obj):\n            return \", \".join([\"{}: {}\".format(k, str(v)) for k, v in obj.items()])\n\n        title = \"{} ({})\".format(title, pretty_print(params))\n\n    grid_links = []\n    for id in global_state.keys():\n        label = global_state.get_name(id)\n        if label:\n            label = \" ({})\".format(label)\n        else:\n            label = \"\"\n        grid_links.append((id, \"{}{}\".format(id, label)))\n    return base_render_template(\n        \"dtale/popup.html\",\n        data_id,\n        title=title,\n        popup_title=popup_title,\n        js_prefix=popup_type,\n        grid_links=grid_links,\n        back_to_data=text(\"Back To Data\"),\n    )\n\n\n@dtale.route(\"/calculation/<calc_type>\")\ndef view_calculation(calc_type=\"skew\"):\n    return render_template(\"dtale/{}.html\".format(calc_type))\n\n\n@dtale.route(\"/network\")\n@dtale.route(\"/network/<data_id>\")\ndef view_network(data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if not global_state.contains(data_id):\n        return redirect(\"/dtale/network/{}\".format(head_endpoint()))\n    return base_render_template(\n        \"dtale/network.html\", data_id, title=\"Network Viewer\", iframe=False\n    )\n\n\n@dtale.route(\"/code-popup\")\ndef view_code_popup():\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up a base jinja template for code snippets\n\n    :return: HTML\n    \"\"\"\n    return render_template(\"dtale/code_popup.html\", **global_state.get_app_settings())\n\n\n@dtale.route(\"/processes\")\n@exception_decorator\ndef get_processes():\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns list of running D-Tale processes within current python process\n\n    :return: JSON {\n        data: [\n            {\n                port: 1, name: 'name1', rows: 5, columns: 5, names: 'col1,...,col5', start: '2018-04-30 12:36:44',\n                ts: 1525106204000\n            },\n            ...,\n            {\n                port: N, name: 'nameN', rows: 5, columns: 5, names: 'col1,...,col5', start: '2018-04-30 12:36:44',\n                ts: 1525106204000\n            }\n        ],\n        success: True/False\n    }\n    \"\"\"\n\n    load_dtypes = get_bool_arg(request, \"dtypes\")\n\n    def _load_process(data_id):\n        data = global_state.get_data(data_id)\n        dtypes = global_state.get_dtypes(data_id)\n        mdata = global_state.get_metadata(data_id)\n        return dict(\n            data_id=str(data_id),\n            rows=len(data),\n            columns=len(dtypes),\n            names=dtypes if load_dtypes else \",\".join([c[\"name\"] for c in dtypes]),\n            start=json_date(mdata[\"start\"], fmt=\"%-I:%M:%S %p\"),\n            ts=json_timestamp(mdata[\"start\"]),\n            name=global_state.get_name(data_id),\n            # mem usage in MB\n            mem_usage=int(\n                global_state.get_data(data_id)\n                .memory_usage(index=False, deep=True)\n                .sum()\n            ),\n        )\n\n    processes = sorted(\n        [_load_process(data_id) for data_id in global_state.keys()],\n        key=lambda p: p[\"ts\"],\n    )\n    return jsonify(dict(data=processes, success=True))\n\n\n@dtale.route(\"/process-keys\")\n@exception_decorator\ndef process_keys():\n    return jsonify(\n        dict(\n            data=[\n                dict(id=str(data_id), name=global_state.get_name(data_id))\n                for data_id in global_state.keys()\n            ],\n            success=True,\n        )\n    )\n\n\n@dtale.route(\"/update-settings/<data_id>\")\n@exception_decorator\ndef update_settings(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which updates global SETTINGS for current port\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param settings: JSON string from flask.request.args['settings'] which gets decoded and stored in SETTINGS variable\n    :return: JSON\n    \"\"\"\n\n    global_state.update_settings(data_id, get_json_arg(request, \"settings\", {}))\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-theme\")\n@exception_decorator\ndef update_theme():\n    theme = get_str_arg(request, \"theme\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"theme\"] = theme\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-pin-menu\")\n@exception_decorator\ndef update_pin_menu():\n    pinned = get_bool_arg(request, \"pinned\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"pin_menu\"] = pinned\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-language\")\n@exception_decorator\ndef update_language():\n    language = get_str_arg(request, \"language\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"language\"] = language\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-maximum-column-width\")\n@exception_decorator\ndef update_maximum_column_width():\n    width = get_str_arg(request, \"width\")\n    if width:\n        width = int(width)\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"max_column_width\"] = width\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-maximum-row-height\")\n@exception_decorator\ndef update_maximum_row_height():\n    height = get_str_arg(request, \"height\")\n    if height:\n        height = int(height)\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"max_row_height\"] = height\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-query-engine\")\n@exception_decorator\ndef update_query_engine():\n    engine = get_str_arg(request, \"engine\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"query_engine\"] = engine\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-formats/<data_id>\")\n@exception_decorator\ndef update_formats(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which updates the \"columnFormats\" property for global SETTINGS associated w/ the\n    current port\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param all: boolean flag which, if true, tells us we should apply this formatting to all columns with the same\n                data type as our selected column\n    :param col: selected column\n    :param format: JSON string for the formatting configuration we want applied to either the selected column of all\n                   columns with the selected column's data type\n    :return: JSON\n    \"\"\"\n    update_all_dtype = get_bool_arg(request, \"all\")\n    nan_display = get_str_arg(request, \"nanDisplay\")\n    if nan_display is None:\n        nan_display = \"nan\"\n    col = get_str_arg(request, \"col\")\n    col_format = get_json_arg(request, \"format\")\n    curr_settings = global_state.get_settings(data_id) or {}\n    updated_formats = {col: col_format}\n    if update_all_dtype:\n        col_dtype = global_state.get_dtype_info(data_id, col)[\"dtype\"]\n        updated_formats = {\n            c[\"name\"]: col_format\n            for c in global_state.get_dtypes(data_id)\n            if c[\"dtype\"] == col_dtype\n        }\n    updated_formats = dict_merge(\n        curr_settings.get(\"columnFormats\") or {}, updated_formats\n    )\n    updated_settings = dict_merge(\n        curr_settings, dict(columnFormats=updated_formats, nanDisplay=nan_display)\n    )\n    global_state.set_settings(data_id, updated_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/save-range-highlights/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef save_range_highlights(data_id):\n    ranges = json.loads(request.json.get(\"ranges\", \"{}\"))\n    curr_settings = global_state.get_settings(data_id) or {}\n    updated_settings = dict_merge(curr_settings, dict(rangeHighlight=ranges))\n    global_state.set_settings(data_id, updated_settings)\n    return jsonify(dict(success=True))\n\n\ndef refresh_col_indexes(data_id):\n    \"\"\"\n    Helper function to sync column indexes to current state of dataframe for data_id.\n    \"\"\"\n    curr_dtypes = {c[\"name\"]: c for c in global_state.get_dtypes(data_id)}\n    curr_data = global_state.get_data(data_id)\n    global_state.set_dtypes(\n        data_id,\n        [\n            dict_merge(curr_dtypes[c], dict(index=idx))\n            for idx, c in enumerate(curr_data.columns)\n        ],\n    )\n\n\n@dtale.route(\"/update-column-position/<data_id>\")\n@exception_decorator\ndef update_column_position(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle moving of columns within a :class:`pandas:pandas.DataFrame`. Columns can\n    be moved in one of these 4 directions: front, back, left, right\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param action: string from flask.request.args['action'] of direction to move column\n    :param col: string from flask.request.args['col'] of column name to move\n    :return: JSON {success: True/False}\n    \"\"\"\n    action = get_str_arg(request, \"action\")\n    col = get_str_arg(request, \"col\")\n\n    curr_cols = global_state.get_data(data_id).columns.tolist()\n    if action == \"front\":\n        curr_cols = [col] + [c for c in curr_cols if c != col]\n    elif action == \"back\":\n        curr_cols = [c for c in curr_cols if c != col] + [col]\n    elif action == \"left\":\n        if curr_cols[0] != col:\n            col_idx = next((idx for idx, c in enumerate(curr_cols) if c == col), None)\n            col_to_shift = curr_cols[col_idx - 1]\n            curr_cols[col_idx - 1] = col\n            curr_cols[col_idx] = col_to_shift\n    elif action == \"right\":\n        if curr_cols[-1] != col:\n            col_idx = next((idx for idx, c in enumerate(curr_cols) if c == col), None)\n            col_to_shift = curr_cols[col_idx + 1]\n            curr_cols[col_idx + 1] = col\n            curr_cols[col_idx] = col_to_shift\n\n    global_state.set_data(data_id, global_state.get_data(data_id)[curr_cols])\n    refresh_col_indexes(data_id)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/update-locked/<data_id>\")\n@exception_decorator\ndef update_locked(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle saving state associated with locking and unlocking columns\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param action: string from flask.request.args['action'] of action to perform (lock or unlock)\n    :param col: string from flask.request.args['col'] of column name to lock/unlock\n    :return: JSON {success: True/False}\n    \"\"\"\n    action = get_str_arg(request, \"action\")\n    col = get_str_arg(request, \"col\")\n    curr_settings = global_state.get_settings(data_id)\n    curr_data = global_state.get_data(data_id)\n    curr_settings[\"locked\"] = curr_settings.get(\"locked\") or []\n    if action == \"lock\" and col not in curr_settings[\"locked\"]:\n        curr_settings[\"locked\"] = curr_settings[\"locked\"] + [col]\n    elif action == \"unlock\":\n        curr_settings[\"locked\"] = [c for c in curr_settings[\"locked\"] if c != col]\n\n    final_cols = curr_settings[\"locked\"] + [\n        c for c in curr_data.columns if c not in curr_settings[\"locked\"]\n    ]\n    global_state.set_data(data_id, curr_data[final_cols])\n    global_state.set_settings(data_id, curr_settings)\n    refresh_col_indexes(data_id)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/update-visibility/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef update_visibility(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle saving state associated visiblity of columns on the front-end\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param visibility: string from flask.request.args['action'] of dictionary of visibility of all columns in a\n                       dataframe\n    :type visibility: dict, optional\n    :param toggle: string from flask.request.args['col'] of column name whose visibility should be toggled\n    :type toggle: str, optional\n    :return: JSON {success: True/False}\n    \"\"\"\n    curr_dtypes = global_state.get_dtypes(data_id)\n    if request.json.get(\"visibility\"):\n        visibility = json.loads(request.json.get(\"visibility\", \"{}\"))\n        global_state.set_dtypes(\n            data_id,\n            [dict_merge(d, dict(visible=visibility[d[\"name\"]])) for d in curr_dtypes],\n        )\n    elif request.json.get(\"toggle\"):\n        toggle_col = request.json.get(\"toggle\")\n        toggle_idx = next(\n            (idx for idx, d in enumerate(curr_dtypes) if d[\"name\"] == toggle_col), None\n        )\n        toggle_cfg = curr_dtypes[toggle_idx]\n        curr_dtypes[toggle_idx] = dict_merge(\n            toggle_cfg, dict(visible=not toggle_cfg[\"visible\"])\n        )\n        global_state.set_dtypes(data_id, curr_dtypes)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/build-column/<data_id>\")\n@exception_decorator\ndef build_column(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle the building of new columns in a dataframe. Some of the operations the\n    are available are:\n     - numeric: sum/difference/multiply/divide any combination of two columns or static values\n     - datetime: retrieving date properties (hour, minute, month, year...) or conversions of dates (month start, month\n                 end, quarter start...)\n     - bins: bucketing numeric data into bins using :meth:`pandas:pandas.cut` & :meth:`pandas:pandas.qcut`\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param name: string from flask.request.args['name'] of new column to create\n    :param type: string from flask.request.args['type'] of the type of column to build (numeric/datetime/bins)\n    :param cfg: dict from flask.request.args['cfg'] of how to calculate the new column\n    :return: JSON {success: True/False}\n    \"\"\"\n    data = global_state.get_data(data_id)\n    col_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    save_as = get_str_arg(request, \"saveAs\", \"new\")\n    if save_as == \"inplace\":\n        name = cfg[\"col\"]\n    else:\n        name = get_str_arg(request, \"name\")\n        if not name and col_type != \"type_conversion\":\n            raise Exception(\"'name' is required for new column!\")\n        # non-type conversions cannot be done inplace and thus need a name and the name needs to be checked that it\n        # won't overwrite something else\n        name = str(name)\n        data = global_state.get_data(data_id)\n        if name in data.columns:\n            raise Exception(\"A column named '{}' already exists!\".format(name))\n\n    def _build_column():\n        builder = ColumnBuilder(data_id, col_type, name, cfg)\n        new_col_data = builder.build_column()\n        new_cols = []\n        if isinstance(new_col_data, pd.Series):\n            if pandas_util.is_pandas2():\n                data[name] = new_col_data\n            else:\n                data.loc[:, name] = new_col_data\n            new_cols.append(name)\n        else:\n            for i in range(len(new_col_data.columns)):\n                new_col = new_col_data.iloc[:, i]\n                if pandas_util.is_pandas2():\n                    data[str(new_col.name)] = new_col\n                else:\n                    data.loc[:, str(new_col.name)] = new_col\n\n        new_types = {}\n        data_ranges = {}\n        for new_col in new_cols:\n            dtype = find_dtype(data[new_col])\n            if classify_type(dtype) == \"F\" and not data[new_col].isnull().all():\n                new_ranges = calc_data_ranges(data[[new_col]])\n                data_ranges[new_col] = new_ranges.get(new_col, data_ranges.get(new_col))\n            new_types[new_col] = dtype\n        dtype_f = dtype_formatter(data, new_types, data_ranges)\n        global_state.set_data(data_id, data)\n        curr_dtypes = global_state.get_dtypes(data_id)\n        if next((cdt for cdt in curr_dtypes if cdt[\"name\"] in new_cols), None):\n            curr_dtypes = [\n                dtype_f(len(curr_dtypes), cdt[\"name\"])\n                if cdt[\"name\"] in new_cols\n                else cdt\n                for cdt in curr_dtypes\n            ]\n        else:\n            curr_dtypes += [dtype_f(len(curr_dtypes), new_col) for new_col in new_cols]\n        global_state.set_dtypes(data_id, curr_dtypes)\n        curr_history = global_state.get_history(data_id) or []\n        curr_history += make_list(builder.build_code())\n        global_state.set_history(data_id, curr_history)\n\n    if cfg.get(\"applyAllType\", False):\n        cols = [\n            dtype[\"name\"]\n            for dtype in global_state.get_dtypes(data_id)\n            if dtype[\"dtype\"] == cfg[\"from\"]\n        ]\n        for col in cols:\n            cfg = dict_merge(cfg, dict(col=col))\n            name = col\n            _build_column()\n    else:\n        _build_column()\n    return jsonify(success=True)\n\n\n@dtale.route(\"/bins-tester/<data_id>\")\n@exception_decorator\ndef build_column_bins_tester(data_id):\n    col_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    builder = ColumnBuilder(data_id, col_type, cfg[\"col\"], cfg)\n    data = global_state.get_data(data_id)\n    data, labels = builder.builder.build_test(data)\n    return jsonify(dict(data=data, labels=labels, timestamp=round(time.time() * 1000)))\n\n\n@dtale.route(\"/duplicates/<data_id>\")\n@exception_decorator\ndef get_duplicates(data_id):\n    dupe_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    action = get_str_arg(request, \"action\")\n    duplicate_check = DuplicateCheck(data_id, dupe_type, cfg)\n    if action == \"test\":\n        return jsonify(results=duplicate_check.test())\n    updated_data_id = duplicate_check.execute()\n    return jsonify(success=True, data_id=updated_data_id)\n\n\n@dtale.route(\"/reshape/<data_id>\")\n@exception_decorator\ndef reshape_data(data_id):\n    output = get_str_arg(request, \"output\")\n    shape_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    builder = DataReshaper(data_id, shape_type, cfg)\n    if output == \"new\":\n        instance = startup(data=builder.reshape(), ignore_duplicate=True)\n    else:\n        instance = startup(\n            data=builder.reshape(), data_id=data_id, ignore_duplicate=True\n        )\n    curr_settings = global_state.get_settings(instance._data_id)\n    global_state.set_settings(\n        instance._data_id,\n        dict_merge(curr_settings, dict(startup_code=builder.build_code())),\n    )\n    return jsonify(success=True, data_id=instance._data_id)\n\n\n@dtale.route(\"/build-replacement/<data_id>\")\n@exception_decorator\ndef build_replacement(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle the replacement of specific values within a column in a dataframe. Some\n    of the operations the are available are:\n     - spaces: replace values consisting of only spaces with a specific value\n     - value: replace specific values with a specific value or aggregation\n     - strings: replace values which contain a specific character or string (case-insensitive or not) with a\n                       specific value\n     - imputer: replace nan values using sklearn imputers iterative, knn or simple\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param col: string from flask.request.args['col'] of the column to perform replacements upon\n    :param type: string from flask.request.args['type'] of the type of replacement to perform\n                 (spaces/fillna/strings/imputer)\n    :param cfg: dict from flask.request.args['cfg'] of how to calculate the replacements\n    :return: JSON {success: True/False}\n    \"\"\"\n\n    def _build_data_ranges(data, col, dtype):\n        data_ranges = {}\n        if classify_type(dtype) == \"F\" and not data[col].isnull().all():\n            col_ranges = calc_data_ranges(data[[col]])\n            if col_ranges:\n                data_ranges[col] = col_ranges[col]\n        return data_ranges\n\n    data = global_state.get_data(data_id)\n    name = get_str_arg(request, \"name\")\n    if name is not None:\n        name = str(name)\n        if name in data.columns:\n            raise Exception(\"A column named '{}' already exists!\".format(name))\n    col = get_str_arg(request, \"col\")\n    replacement_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n\n    builder = ColumnReplacement(data_id, col, replacement_type, cfg, name)\n    output = builder.build_replacements()\n    dtype = find_dtype(output)\n    curr_dtypes = global_state.get_dtypes(data_id)\n\n    if name is not None:\n        data.loc[:, name] = output\n        dtype_f = dtype_formatter(\n            data, {name: dtype}, _build_data_ranges(data, name, dtype)\n        )\n        curr_dtypes.append(dtype_f(len(curr_dtypes), name))\n    else:\n        data.loc[:, col] = output\n        dtype_f = dtype_formatter(\n            data, {col: dtype}, _build_data_ranges(data, col, dtype)\n        )\n        col_index = next(\n            (i for i, d in enumerate(curr_dtypes) if d[\"name\"] == col), None\n        )\n        curr_col_dtype = dtype_f(col_index, col)\n        curr_dtypes = [curr_col_dtype if d[\"name\"] == col else d for d in curr_dtypes]\n\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, curr_dtypes)\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [builder.build_code()]\n    global_state.set_history(data_id, curr_history)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/test-filter/<data_id>\")\n@exception_decorator\ndef test_filter(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which will test out pandas query before it gets applied to DATA and return\n    exception information to the screen if there is any\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :return: JSON {success: True/False}\n    \"\"\"\n    query = get_str_arg(request, \"query\")\n    run_query(\n        handle_predefined(data_id),\n        build_query(data_id, query),\n        global_state.get_context_variables(data_id),\n    )\n    if get_str_arg(request, \"save\"):\n        curr_settings = global_state.get_settings(data_id) or {}\n        if query is not None:\n            curr_settings = dict_merge(curr_settings, dict(query=query))\n        else:\n            curr_settings = {k: v for k, v in curr_settings.items() if k != \"query\"}\n        global_state.set_settings(data_id, curr_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/dtypes/<data_id>\")\n@exception_decorator\ndef dtypes(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns a list of column names and dtypes to the front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n\n    :return: JSON {\n        dtypes: [\n            {index: 1, name: col1, dtype: int64},\n            ...,\n            {index: N, name: colN, dtype: float64}\n        ],\n        success: True/False\n    }\n    \"\"\"\n    return jsonify(dtypes=global_state.get_dtypes(data_id), success=True)\n\n\ndef build_sequential_diffs(s, col, sort=None):\n    if sort is not None:\n        s = s.sort_values(ascending=sort == \"ASC\")\n    diff = s.diff()\n    diff = diff[diff == diff]  # remove nan or nat values\n    min_diff = diff.min()\n    max_diff = diff.max()\n    avg_diff = diff.mean()\n    diff_vals = diff.value_counts().sort_values(ascending=False)\n    diff_vals.index.name = \"value\"\n    diff_vals.name = \"count\"\n    diff_vals = diff_vals.reset_index()\n\n    diff_vals_f = grid_formatter(grid_columns(diff_vals), as_string=True)\n    diff_fmt = next((f[2] for f in diff_vals_f.fmts if f[1] == \"value\"), None)\n    diff_ct = len(diff_vals)\n\n    code = (\n        \"sequential_diffs = data['{}'].diff()\\n\"\n        \"diff = diff[diff == diff]\\n\"\n        \"min_diff = sequential_diffs.min()\\n\"\n        \"max_diff = sequential_diffs.max()\\n\"\n        \"avg_diff = sequential_diffs.mean()\\n\"\n        \"diff_vals = sequential_diffs.value_counts().sort_values(ascending=False)\"\n    ).format(col)\n\n    metrics = {\n        \"diffs\": {\n            \"data\": diff_vals_f.format_dicts(diff_vals.head(100).itertuples()),\n            \"top\": diff_ct > 100,\n            \"total\": diff_ct,\n        },\n        \"min\": diff_fmt(min_diff, \"N/A\"),\n        \"max\": diff_fmt(max_diff, \"N/A\"),\n        \"avg\": diff_fmt(avg_diff, \"N/A\"),\n    }\n    return metrics, code\n\n\ndef build_string_metrics(s, col):\n    char_len = s.len()\n\n    def calc_len(x):\n        try:\n            return len(x)\n        except BaseException:\n            return 0\n\n    word_len = apply(s.replace(r\"[\\s]+\", \" \").str.split(\" \"), calc_len)\n\n    def txt_count(r):\n        return s.count(r).astype(bool).sum()\n\n    string_metrics = dict(\n        char_min=int(char_len.min()),\n        char_max=int(char_len.max()),\n        char_mean=json_float(char_len.mean()),\n        char_std=json_float(char_len.std()),\n        with_space=int(txt_count(r\"\\s\")),\n        with_accent=int(txt_count(r\"[\u00c0-\u00d6\u00d9-\u00f6\u00f9-\u00ff\u0100-\u017e\u1e00-\u1eff]\")),\n        with_num=int(txt_count(r\"[\\d]\")),\n        with_upper=int(txt_count(r\"[A-Z]\")),\n        with_lower=int(txt_count(r\"[a-z]\")),\n        with_punc=int(\n            txt_count(\n                r'(\\!|\"|\\#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|\\-|\\.|/|\\:|\\;|\\<|\\=|\\>|\\?|@|\\[|\\\\|\\]|\\^|_|\\`|\\{|\\||\\}|\\~)'\n            )\n        ),\n        space_at_the_first=int(txt_count(r\"^ \")),\n        space_at_the_end=int(txt_count(r\" $\")),\n        multi_space_after_each_other=int(txt_count(r\"\\s{2,}\")),\n        with_hidden=int(txt_count(r\"[^{}]+\".format(printable))),\n        word_min=int(word_len.min()),\n        word_max=int(word_len.max()),\n        word_mean=json_float(word_len.mean()),\n        word_std=json_float(word_len.std()),\n    )\n\n    punc_reg = (\n        \"\"\"\\tr'(\\\\!|\"|\\\\#|\\\\$|%|&|\\\\'|\\\\(|\\\\)|\\\\*|\\\\+|,|\\\\-|\\\\.|/|\\\\:|\\\\;|\\\\<|\\\\=|\"\"\"\n        \"\"\"\\\\>|\\\\?|@|\\\\[|\\\\\\\\|\\\\]|\\\\^|_|\\\\`|\\\\{|\\\\||\\\\}|\\\\~)'\"\"\"\n    )\n    code = [\n        \"s = data['{}']\".format(col),\n        \"s = s[~s.isnull()].str\",\n        \"char_len = s.len()\\n\",\n        \"def calc_len(x):\",\n        \"\\ttry:\",\n        \"\\t\\treturn len(x)\",\n        \"\\texcept:\",\n        \"\\t\\treturn 0\\n\",\n        \"word_len = s.replace(r'[\\\\s]+', ' ').str.split(' ').apply(calc_len)\\n\",\n        \"def txt_count(r):\",\n        \"\\treturn s.count(r).astype(bool).sum()\\n\",\n        \"char_min=char_len.min()\",\n        \"char_max = char_len.max()\",\n        \"char_mean = char_len.mean()\",\n        \"char_std = char_len.std()\",\n        \"with_space = txt_count(r'\\\\s')\",\n        \"with_accent = txt_count(r'[\u00c0-\u00d6\u00d9-\u00f6\u00f9-\u00ff\u0100-\u017e\u1e00-\u1eff]')\",\n        \"with_num = txt_count(r'[\\\\d]')\",\n        \"with_upper = txt_count(r'[A-Z]')\",\n        \"with_lower = txt_count(r'[a-z]')\",\n        \"with_punc = txt_count(\",\n        \"\\t{}\".format(punc_reg),\n        \")\",\n        \"space_at_the_first = txt_count(r'^ ')\",\n        \"space_at_the_end = txt_count(r' $')\",\n        \"multi_space_after_each_other = txt_count(r'\\\\s{2,}')\",\n        \"printable = r'\\\\w \\\\!\\\"#\\\\$%&'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<\u00bb\u00ab\u061b\u060c\u0640\\\\=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}~'\",\n        \"with_hidden = txt_count(r'[^{}]+'.format(printable))\",\n        \"word_min = word_len.min()\",\n        \"word_max = word_len.max()\",\n        \"word_mean = word_len.mean()\",\n        \"word_std = word_len.std()\",\n    ]\n\n    return string_metrics, code\n\n\n@dtale.route(\"/describe/<data_id>\")\n@exception_decorator\ndef describe(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns standard details about column data using\n    :meth:`pandas:pandas.DataFrame.describe` to the front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param column: required dash separated string \"START-END\" stating a range of row indexes to be returned\n                   to the screen\n    :return: JSON {\n        describe: object representing output from :meth:`pandas:pandas.Series.describe`,\n        unique_data: array of unique values when data has <= 100 unique values\n        success: True/False\n    }\n\n    \"\"\"\n    column = get_str_arg(request, \"col\")\n    curr_settings = global_state.get_settings(data_id) or {}\n    columns_to_load = [column]\n    indexes = curr_settings.get(\"indexes\", [])\n    # TODO: update this to use arcticdb's index function once it becomes available\n    if global_state.is_arcticdb and column in indexes:\n        column_to_load = next(\n            (\n                c\n                for c in global_state.get_dtypes(data_id) or []\n                if c[\"name\"] not in indexes\n            ),\n            None,\n        )\n        columns_to_load = [column_to_load[\"name\"]] if column_to_load else None\n    data = load_filterable_data(data_id, request, columns=columns_to_load)\n    data = data[[column]]\n    additional_aggs = None\n    dtype = global_state.get_dtype_info(data_id, column)\n    classification = classify_type(dtype[\"dtype\"])\n    if classification == \"I\":\n        additional_aggs = [\"sum\", \"median\", \"mode\", \"var\", \"sem\"]\n    elif classification == \"F\":\n        additional_aggs = [\"sum\", \"median\", \"var\", \"sem\"]\n    code = build_code_export(data_id)\n    desc, desc_code = load_describe(data[column], additional_aggs=additional_aggs)\n    code += desc_code\n    return_data = dict(describe=desc, success=True)\n    if \"unique\" not in return_data[\"describe\"] and \"unique_ct\" in dtype:\n        return_data[\"describe\"][\"unique\"] = json_int(dtype[\"unique_ct\"], as_string=True)\n    for p in [\"skew\", \"kurt\"]:\n        if p in dtype:\n            return_data[\"describe\"][p] = dtype[p]\n\n    if classification != \"F\" and not global_state.store.get(data_id).is_large:\n        uniq_vals = data[column].value_counts().sort_values(ascending=False)\n        uniq_vals.index.name = \"value\"\n        uniq_vals.name = \"count\"\n        uniq_vals = uniq_vals.reset_index()\n\n        # build top\n        top_freq = uniq_vals[\"count\"].values[0]\n        top_freq_pct = (top_freq / uniq_vals[\"count\"].sum()) * 100\n        top_vals = (\n            uniq_vals[uniq_vals[\"count\"] == top_freq].sort_values(\"value\").head(5)\n        )\n        top_vals_f = grid_formatter(grid_columns(top_vals), as_string=True)\n        top_vals = top_vals_f.format_lists(top_vals)\n        return_data[\"describe\"][\"top\"] = \"{} ({}%)\".format(\n            \", \".join(top_vals[\"value\"]), json_float(top_freq_pct, as_string=True)\n        )\n        return_data[\"describe\"][\"freq\"] = int(top_freq)\n\n        code.append(\n            (\n                \"uniq_vals = data['{}'].value_counts().sort_values(ascending=False)\\n\"\n                \"uniq_vals.index.name = 'value'\\n\"\n                \"uniq_vals.name = 'count'\\n\"\n                \"uniq_vals = uniq_vals.reset_index()\"\n            ).format(column)\n        )\n\n        if dtype[\"dtype\"].startswith(\"mixed\"):\n            uniq_vals[\"type\"] = apply(uniq_vals[\"value\"], lambda i: type(i).__name__)\n            dtype_counts = uniq_vals.groupby(\"type\")[\"count\"].sum().reset_index()\n            dtype_counts.columns = [\"dtype\", \"count\"]\n            return_data[\"dtype_counts\"] = dtype_counts.to_dict(orient=\"records\")\n            code.append(\n                (\n                    \"uniq_vals['type'] = uniq_vals['value'].apply( lambda i: type(i).__name__)\\n\"\n                    \"dtype_counts = uniq_vals.groupby('type')['count'].sum().reset_index()\\n\"\n                    \"dtype_counts.columns = ['dtype', 'count']\"\n                )\n            )\n        else:\n            uniq_vals.loc[:, \"type\"] = find_dtype(uniq_vals[\"value\"])\n            code.append(\n                \"uniq_vals.loc[:, 'type'] = '{}'\".format(uniq_vals[\"type\"].values[0])\n            )\n\n        return_data[\"uniques\"] = {}\n        for uniq_type, uniq_grp in uniq_vals.groupby(\"type\"):\n            total = len(uniq_grp)\n            top = total > 100\n            uniq_grp = (\n                uniq_grp[[\"value\", \"count\"]]\n                .sort_values([\"count\", \"value\"], ascending=[False, True])\n                .head(100)\n            )\n            # pandas started supporting string dtypes in 1.1.0\n            conversion_type = (\n                uniq_type\n                if pandas_util.check_pandas_version(\"1.1.0\") and uniq_type == \"string\"\n                else \"object\"\n            )\n            uniq_grp[\"value\"] = uniq_grp[\"value\"].astype(conversion_type)\n            uniq_f, _ = build_formatters(uniq_grp)\n            return_data[\"uniques\"][uniq_type] = dict(\n                data=uniq_f.format_dicts(uniq_grp.itertuples()), total=total, top=top\n            )\n\n    if (\n        classification in [\"I\", \"F\", \"D\"]\n        and not global_state.store.get(data_id).is_large\n    ):\n        sd_metrics, sd_code = build_sequential_diffs(data[column], column)\n        return_data[\"sequential_diffs\"] = sd_metrics\n        code.append(sd_code)\n\n    if classification == \"S\":\n        str_col = data[column]\n        sm_metrics, sm_code = build_string_metrics(\n            str_col[~str_col.isnull()].astype(\"str\").str, column\n        )\n        return_data[\"string_metrics\"] = sm_metrics\n        code += sm_code\n\n    return_data[\"code\"] = \"\\n\".join(code)\n    return jsonify(return_data)\n\n\n@dtale.route(\"/variance/<data_id>\")\n@exception_decorator\ndef variance(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns standard details about column data using\n    :meth:`pandas:pandas.DataFrame.describe` to the front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param column: required dash separated string \"START-END\" stating a range of row indexes to be returned\n                   to the screen\n    :return: JSON {\n        describe: object representing output from :meth:`pandas:pandas.Series.describe`,\n        unique_data: array of unique values when data has <= 100 unique values\n        success: True/False\n    }\n\n    \"\"\"\n    column = get_str_arg(request, \"col\")\n    data = load_filterable_data(data_id, request)\n    s = data[column]\n    code = [\"s = df['{}']\".format(column)]\n    unique_ct = unique_count(s)\n    code.append(\"unique_ct = s.unique().size\")\n    s_size = len(s)\n    code.append(\"s_size = len(s)\")\n    check1 = bool((unique_ct / s_size) < 0.1)\n    code.append(\"check1 = (unique_ct / s_size) < 0.1\")\n    return_data = dict(check1=dict(unique=unique_ct, size=s_size, result=check1))\n    dtype = global_state.get_dtype_info(data_id, column)\n    if unique_ct >= 2:\n        val_counts = s.value_counts()\n        check2 = bool((val_counts.values[0] / val_counts.values[1]) > 20)\n        fmt = find_dtype_formatter(dtype[\"dtype\"])\n        return_data[\"check2\"] = dict(\n            val1=dict(val=fmt(val_counts.index[0]), ct=int(val_counts.values[0])),\n            val2=dict(val=fmt(val_counts.index[1]), ct=int(val_counts.values[1])),\n            result=check2,\n        )\n    code += [\n        \"check2 = False\",\n        \"if unique_ct > 1:\",\n        \"\\tval_counts = s.value_counts()\",\n        \"\\tcheck2 = (val_counts.values[0] / val_counts.values[1]) > 20\",\n        \"low_variance = check1 and check2\",\n    ]\n\n    return_data[\"size\"] = len(s)\n    return_data[\"outlierCt\"] = dtype[\"hasOutliers\"]\n    return_data[\"missingCt\"] = int(s.isnull().sum())\n\n    jb_stat, jb_p = sts.jarque_bera(s)\n    return_data[\"jarqueBera\"] = dict(statistic=float(jb_stat), pvalue=float(jb_p))\n    sw_stat, sw_p = sts.shapiro(s)\n    return_data[\"shapiroWilk\"] = dict(statistic=float(sw_stat), pvalue=float(sw_p))\n    code += [\n        \"\\nimport scipy.stats as sts\\n\",\n        \"jb_stat, jb_p = sts.jarque_bera(s)\",\n        \"sw_stat, sw_p = sts.shapiro(s)\",\n    ]\n    return_data[\"code\"] = \"\\n\".join(code)\n    return jsonify(return_data)\n\n\ndef calc_outlier_range(s):\n    try:\n        q1 = s.quantile(0.25)\n        q3 = s.quantile(0.75)\n    except BaseException:  # this covers the case when a series contains pd.NA\n        return np.nan, np.nan\n    iqr = q3 - q1\n    iqr_lower = q1 - 1.5 * iqr\n    iqr_upper = q3 + 1.5 * iqr\n    return iqr_lower, iqr_upper\n\n\ndef build_outlier_query(iqr_lower, iqr_upper, min_val, max_val, column):\n    queries = []\n    if iqr_lower > min_val:\n        queries.append(\n            \"{column} < {lower}\".format(\n                column=build_col_key(column), lower=json_float(iqr_lower)\n            )\n        )\n    if iqr_upper < max_val:\n        queries.append(\n            \"{column} > {upper}\".format(\n                column=build_col_key(column), upper=json_float(iqr_upper)\n            )\n        )\n    return \"(({}))\".format(\") or (\".join(queries)) if len(queries) > 1 else queries[0]\n\n\n@dtale.route(\"/outliers/<data_id>\")\n@exception_decorator\ndef outliers(data_id):\n    column = get_str_arg(request, \"col\")\n    df = global_state.get_data(data_id)\n    s = df[column]\n    iqr_lower, iqr_upper = calc_outlier_range(s)\n    formatter = find_dtype_formatter(find_dtype(s))\n\n    df = load_filterable_data(data_id, request)\n    s = df[column]\n    outliers = sorted(s[(s < iqr_lower) | (s > iqr_upper)].unique())\n    if not len(outliers):\n        return jsonify(outliers=[])\n\n    top = len(outliers) > 100\n    outliers = [formatter(v) for v in outliers[:100]]\n    query = build_outlier_query(iqr_lower, iqr_upper, s.min(), s.max(), column)\n    code = (\n        \"s = df['{column}']\\n\"\n        \"q1 = s.quantile(0.25)\\n\"\n        \"q3 = s.quantile(0.75)\\n\"\n        \"iqr = q3 - q1\\n\"\n        \"iqr_lower = q1 - 1.5 * iqr\\n\"\n        \"iqr_upper = q3 + 1.5 * iqr\\n\"\n        \"outliers = dict(s[(s < iqr_lower) | (s > iqr_upper)])\"\n    ).format(column=column)\n    queryApplied = column in (\n        (global_state.get_settings(data_id) or {}).get(\"outlierFilters\") or {}\n    )\n    return jsonify(\n        outliers=outliers, query=query, code=code, queryApplied=queryApplied, top=top\n    )\n\n\n@dtale.route(\"/toggle-outlier-filter/<data_id>\")\n@exception_decorator\ndef toggle_outlier_filter(data_id):\n    column = get_str_arg(request, \"col\")\n    settings = global_state.get_settings(data_id) or {}\n    outlierFilters = settings.get(\"outlierFilters\") or {}\n    if column in outlierFilters:\n        settings[\"outlierFilters\"] = {\n            k: v for k, v in outlierFilters.items() if k != column\n        }\n    else:\n        dtype_info = global_state.get_dtype_info(data_id, column)\n        outlier_range, min_val, max_val = (\n            dtype_info.get(p) for p in [\"outlierRange\", \"min\", \"max\"]\n        )\n        iqr_lower, iqr_upper = (outlier_range.get(p) for p in [\"lower\", \"upper\"])\n        query = build_outlier_query(iqr_lower, iqr_upper, min_val, max_val, column)\n        settings[\"outlierFilters\"] = dict_merge(\n            outlierFilters, {column: {\"query\": query}}\n        )\n    global_state.set_settings(data_id, settings)\n    return jsonify(dict(success=True, outlierFilters=settings[\"outlierFilters\"]))\n\n\n@dtale.route(\"/delete-col/<data_id>\")\n@exception_decorator\ndef delete_col(data_id):\n    columns = get_json_arg(request, \"cols\")\n    data = global_state.get_data(data_id)\n    data = data[[c for c in data.columns if c not in columns]]\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\"df = df.drop(columns=['{}'])\".format(\"','\".join(columns))]\n    global_state.set_history(data_id, curr_history)\n    dtypes = global_state.get_dtypes(data_id)\n    dtypes = [dt for dt in dtypes if dt[\"name\"] not in columns]\n    curr_settings = global_state.get_settings(data_id)\n    curr_settings[\"locked\"] = [\n        c for c in curr_settings.get(\"locked\", []) if c not in columns\n    ]\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, dtypes)\n    global_state.set_settings(data_id, curr_settings)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/rename-col/<data_id>\")\n@exception_decorator\ndef rename_col(data_id):\n    column = get_str_arg(request, \"col\")\n    rename = get_str_arg(request, \"rename\")\n    data = global_state.get_data(data_id)\n    if column != rename and rename in data.columns:\n        return jsonify(error='Column name \"{}\" already exists!')\n\n    data = data.rename(columns={column: rename})\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\"df = df.rename(columns={'%s': '%s'})\" % (column, rename)]\n    global_state.set_history(data_id, curr_history)\n    dtypes = global_state.get_dtypes(data_id)\n    dtypes = [\n        dict_merge(dt, {\"name\": rename}) if dt[\"name\"] == column else dt\n        for dt in dtypes\n    ]\n    curr_settings = global_state.get_settings(data_id)\n    curr_settings[\"locked\"] = [\n        rename if c == column else c for c in curr_settings.get(\"locked\", [])\n    ]\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, dtypes)\n    global_state.set_settings(data_id, curr_settings)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/duplicate-col/<data_id>\")\n@exception_decorator\ndef duplicate_col(data_id):\n    column = get_str_arg(request, \"col\")\n    data = global_state.get_data(data_id)\n    dupe_idx = 2\n    new_col = \"{}_{}\".format(column, dupe_idx)\n    while new_col in data.columns:\n        dupe_idx += 1\n        new_col = \"{}_{}\".format(column, dupe_idx)\n\n    data.loc[:, new_col] = data[column]\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\"df.loc[:, '%s'] = df['%s']\" % (new_col, column)]\n    global_state.set_history(data_id, curr_history)\n    dtypes = []\n    cols = []\n    idx = 0\n    for dt in global_state.get_dtypes(data_id):\n        dt[\"index\"] = idx\n        dtypes.append(dt)\n        cols.append(dt[\"name\"])\n        idx += 1\n        if dt[\"name\"] == column:\n            dtypes.append(dict_merge(dt, dict(name=new_col, index=idx)))\n            cols.append(new_col)\n            idx += 1\n\n    global_state.set_data(data_id, data[cols])\n    global_state.set_dtypes(data_id, dtypes)\n    return jsonify(success=True, col=new_col)\n\n\n@dtale.route(\"/edit-cell/<data_id>\")\n@exception_decorator\ndef edit_cell(data_id):\n    column = get_str_arg(request, \"col\")\n    row_index = get_int_arg(request, \"rowIndex\")\n    updated = get_str_arg(request, \"updated\")\n    updated_str = updated\n    curr_settings = global_state.get_settings(data_id)\n\n    # make sure to load filtered data in order to get correct row index\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    row_index_val = data.iloc[[row_index]].index[0]\n    data = global_state.get_data(data_id)\n    dtype = find_dtype(data[column])\n\n    code = []\n    if updated in [\"nan\", \"inf\"]:\n        updated_str = \"np.{}\".format(updated)\n        updated = getattr(np, updated)\n        data.loc[row_index_val, column] = updated\n        code.append(\n            \"df.loc[{row_index}, '{column}'] = {updated}\".format(\n                row_index=row_index_val, column=column, updated=updated_str\n            )\n        )\n    else:\n        classification = classify_type(dtype)\n        if classification == \"B\":\n            updated = updated.lower() == \"true\"\n            updated_str = str(updated)\n        elif classification == \"I\":\n            updated = int(updated)\n        elif classification == \"F\":\n            updated = float(updated)\n        elif classification == \"D\":\n            updated_str = \"pd.Timestamp({})\".format(updated)\n            updated = pd.Timestamp(updated)\n        elif classification == \"TD\":\n            updated_str = \"pd.Timedelta({})\".format(updated)\n            updated = pd.Timedelta(updated)\n        else:\n            if dtype.startswith(\"category\") and updated not in data[column].unique():\n                if pandas_util.is_pandas2():\n                    data.loc[:, column] = pd.Categorical(\n                        data[column],\n                        categories=data[column].cat.add_categories(updated),\n                        ordered=True,\n                    )\n                    code.append(\n                        (\n                            \"data.loc[:, '{column}'] = pd.Categorical(\\n\"\n                            \"\\tdata['{column}'],\\n\"\n                            \"\\tcategories=data['{column}'].cat.add_categories('{updated}'),\\n\"\n                            \"\\tordered=True\\n\"\n                            \")\"\n                        ).format(column=column, updated=updated)\n                    )\n                else:\n                    data[column].cat.add_categories(updated, inplace=True)\n                    code.append(\n                        \"data['{column}'].cat.add_categories('{updated}', inplace=True)\".format(\n                            column=column, updated=updated\n                        )\n                    )\n            updated_str = \"'{}'\".format(updated)\n        data.at[row_index_val, column] = updated\n        code.append(\n            \"df.at[{row_index}, '{column}'] = {updated}\".format(\n                row_index=row_index_val, column=column, updated=updated_str\n            )\n        )\n    global_state.set_data(data_id, data)\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += code\n    global_state.set_history(data_id, curr_history)\n\n    data = global_state.get_data(data_id)\n    dtypes = global_state.get_dtypes(data_id)\n    ranges = calc_data_ranges(data[[column]])\n    dtype_f = dtype_formatter(data, {column: dtype}, ranges)\n    dtypes = [\n        dtype_f(dt[\"index\"], column) if dt[\"name\"] == column else dt for dt in dtypes\n    ]\n    global_state.set_dtypes(data_id, dtypes)\n    return jsonify(success=True)\n\n\ndef build_filter_vals(series, data_id, column, fmt):\n    dtype_info = global_state.get_dtype_info(data_id, column)\n    vals = list(series.dropna().unique())\n    try:\n        vals = sorted(vals)\n    except BaseException:\n        pass  # if there are mixed values (EX: strings with ints) this fails\n    if dtype_info.get(\"unique_ct\", 0) > 500:\n        # columns with too many unique values will need to use asynchronous loading, so for now we'll give the\n        # first 5 values\n        vals = vals[:5]\n    vals = [fmt(v) for v in vals]\n    return vals\n\n\n@dtale.route(\"/column-filter-data/<data_id>\")\n@exception_decorator\ndef get_column_filter_data(data_id):\n    if global_state.is_arcticdb and global_state.store.get(data_id).is_large:\n        return jsonify(dict(success=True, hasMissing=True))\n    column = get_str_arg(request, \"col\")\n    s = global_state.get_data(data_id)[column]\n    dtype = find_dtype(s)\n    fmt = find_dtype_formatter(dtype)\n    classification = classify_type(dtype)\n    ret = dict(success=True, hasMissing=bool(s.isnull().any()))\n    if classification not in [\"S\", \"B\"]:\n        data_range = s.agg([\"min\", \"max\"]).to_dict()\n        data_range = {k: fmt(v) for k, v in data_range.items()}\n        ret = dict_merge(ret, data_range)\n    if classification in [\"S\", \"I\", \"B\"]:\n        ret[\"uniques\"] = build_filter_vals(s, data_id, column, fmt)\n    return jsonify(ret)\n\n\n@dtale.route(\"/async-column-filter-data/<data_id>\")\n@exception_decorator\ndef get_async_column_filter_data(data_id):\n    column = get_str_arg(request, \"col\")\n    input = get_str_arg(request, \"input\")\n    s = global_state.get_data(data_id)[column]\n    dtype = find_dtype(s)\n    fmt = find_dtype_formatter(dtype)\n    vals = s[s.astype(\"str\").str.startswith(input)]\n    vals = [option(fmt(v)) for v in sorted(vals.unique())[:5]]\n    return jsonify(vals)\n\n\n@dtale.route(\"/save-column-filter/<data_id>\")\n@exception_decorator\ndef save_column_filter(data_id):\n    column = get_str_arg(request, \"col\")\n    curr_filters = ColumnFilter(\n        data_id, column, get_str_arg(request, \"cfg\")\n    ).save_filter()\n    return jsonify(success=True, currFilters=curr_filters)\n\n\n@dtale.route(\"/data/<data_id>\")\n@exception_decorator\ndef get_data(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns current rows from DATA (based on scrollbar specs and saved settings)\n    to front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param ids: required dash separated string \"START-END\" stating a range of row indexes to be returned to the screen\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :param sort: JSON string from flask.request.args['sort'] which is applied to DATA using the sort_values() or\n                 sort_index() function.  Here is the JSON structure: [col1,dir1],[col2,dir2],....[coln,dirn]\n    :return: JSON {\n        results: [\n            {dtale_index: 1, col1: val1_1, ...,colN: valN_1},\n            ...,\n            {dtale_index: N2, col1: val1_N2, ...,colN: valN_N2}\n        ],\n        columns: [{name: col1, dtype: 'int64'},...,{name: colN, dtype: 'datetime'}],\n        total: N2,\n        success: True/False\n    }\n    \"\"\"\n\n    # handling for gunicorn-hosted instances w/ ArcticDB\n    if global_state.is_arcticdb and not len(global_state.get_dtypes(data_id) or []):\n        global_state.store.build_instance(data_id)\n        startup(data=data_id)\n\n    params = retrieve_grid_params(request)\n    export = get_bool_arg(request, \"export\")\n    ids = get_json_arg(request, \"ids\")\n    if not export and ids is None:\n        return jsonify({})\n\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_locked = curr_settings.get(\"locked\", [])\n    final_query = build_query(data_id, curr_settings.get(\"query\"))\n    highlight_filter = curr_settings.get(\"highlightFilter\") or False\n\n    if global_state.is_arcticdb:\n        col_types = global_state.get_dtypes(data_id) or []\n        columns_to_load = [c[\"name\"] for c in col_types if c[\"visible\"]]\n        f = grid_formatter(\n            [c for c in col_types if c[\"visible\"]],\n            nan_display=curr_settings.get(\"nanDisplay\", \"nan\"),\n        )\n\n        query_builder = build_query_builder(data_id)\n        date_range = load_index_filter(data_id)\n        instance = global_state.store.get(data_id)\n        total = instance.rows()\n        results = {}\n        if total:\n            if export:\n                export_rows = get_int_arg(request, \"export_rows\")\n                if export_rows:\n                    if query_builder:\n                        data = instance.load_data(\n                            query_builder=query_builder, **date_range\n                        )\n                        data = data.head(export_rows)\n                    elif len(date_range):\n                        data = instance.load_data(**date_range)\n                        data = data.head(export_rows)\n                    else:\n                        data = instance.load_data(row_range=[0, export_rows])\n                data, _ = format_data(data)\n                data = data[\n                    curr_locked + [c for c in data.columns if c not in curr_locked]\n                ]\n                results = f.format_dicts(data.itertuples())\n                results = [dict_merge({IDX_COL: i}, r) for i, r in enumerate(results)]\n            elif query_builder:\n                df = instance.load_data(\n                    query_builder=query_builder,\n                    columns=columns_to_load,\n                    # fmt: off\n                    **date_range\n                    # fmt: on\n                )\n                total = len(df)\n                df, _ = format_data(df)\n                df = df[curr_locked + [c for c in df.columns if c not in curr_locked]]\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = df.iloc[sub_range[0] : sub_range[0] + 1]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                    else:\n                        [start, end] = sub_range\n                        sub_df = (\n                            df.iloc[start:]\n                            if end >= total - 1\n                            else df.iloc[start : end + 1]\n                        )\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n            elif len(date_range):\n                df = instance.load_data(columns=columns_to_load, **date_range)\n                total = len(df)\n                df, _ = format_data(df)\n                df = df[curr_locked + [c for c in df.columns if c not in curr_locked]]\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = df.iloc[sub_range[0] : sub_range[0] + 1]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                    else:\n                        [start, end] = sub_range\n                        sub_df = (\n                            df.iloc[start:]\n                            if end >= total - 1\n                            else df.iloc[start : end + 1]\n                        )\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n            else:\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = instance.load_data(\n                            row_range=[sub_range[0], sub_range[0] + 1],\n                            columns=columns_to_load,\n                        )\n                        sub_df, _ = format_data(sub_df)\n                        sub_df = sub_df[\n                            curr_locked\n                            + [c for c in sub_df.columns if c not in curr_locked]\n                        ]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                    else:\n                        [start, end] = sub_range\n                        sub_df = instance.load_data(\n                            row_range=[start, total if end >= total else end + 1],\n                            columns=columns_to_load,\n                        )\n                        sub_df, _ = format_data(sub_df)\n                        sub_df = sub_df[\n                            curr_locked\n                            + [c for c in sub_df.columns if c not in curr_locked]\n                        ]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n    else:\n        data = global_state.get_data(data_id)\n\n        # this will check for when someone instantiates D-Tale programmatically and directly alters the internal\n        # state of the dataframe (EX: d.data['new_col'] = 'foo')\n        curr_dtypes = [c[\"name\"] for c in global_state.get_dtypes(data_id)]\n        if any(c not in curr_dtypes for c in data.columns):\n            data, _ = format_data(data)\n            data = data[curr_locked + [c for c in data.columns if c not in curr_locked]]\n            global_state.set_data(data_id, data)\n            global_state.set_dtypes(\n                data_id,\n                build_dtypes_state(data, global_state.get_dtypes(data_id) or []),\n            )\n\n        col_types = global_state.get_dtypes(data_id)\n        f = grid_formatter(\n            col_types, nan_display=curr_settings.get(\"nanDisplay\", \"nan\")\n        )\n        if curr_settings.get(\"sortInfo\") != params.get(\"sort\"):\n            data = sort_df_for_grid(data, params)\n            global_state.set_data(data_id, data)\n        if params.get(\"sort\") is not None:\n            curr_settings = dict_merge(curr_settings, dict(sortInfo=params[\"sort\"]))\n        else:\n            curr_settings = {k: v for k, v in curr_settings.items() if k != \"sortInfo\"}\n        filtered_indexes = []\n        data = run_query(\n            handle_predefined(data_id),\n            final_query,\n            global_state.get_context_variables(data_id),\n            ignore_empty=True,\n            highlight_filter=highlight_filter,\n        )\n        if highlight_filter:\n            data, filtered_indexes = data\n        global_state.set_settings(data_id, curr_settings)\n\n        total = len(data)\n        results = {}\n        if total:\n            if export:\n                export_rows = get_int_arg(request, \"export_rows\")\n                if export_rows:\n                    data = data.head(export_rows)\n                results = f.format_dicts(data.itertuples())\n                results = [dict_merge({IDX_COL: i}, r) for i, r in enumerate(results)]\n            else:\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = data.iloc[sub_range[0] : sub_range[0] + 1]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                        if highlight_filter and sub_range[0] in filtered_indexes:\n                            results[sub_range[0]][\"__filtered\"] = True\n                    else:\n                        [start, end] = sub_range\n                        sub_df = (\n                            data.iloc[start:]\n                            if end >= total - 1\n                            else data.iloc[start : end + 1]\n                        )\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n                            if highlight_filter and i in filtered_indexes:\n                                results[i][\"__filtered\"] = True\n    columns = [\n        dict(name=IDX_COL, dtype=\"int64\", visible=True)\n    ] + global_state.get_dtypes(data_id)\n    return_data = dict(\n        results=results,\n        columns=columns,\n        total=total,\n        final_query=None if highlight_filter else final_query,\n    )\n\n    if export:\n        return export_html(data_id, return_data)\n\n    return jsonify(return_data)\n\n\ndef export_html(data_id, return_data):\n    def load_file(fpath, encoding=\"utf-8\"):\n        return read_file(\n            os.path.join(os.path.dirname(__file__), \"static/{}\".format(fpath)),\n            encoding=encoding,\n        )\n\n    istok_woff = load_file(\"fonts/istok_woff64.txt\", encoding=None)\n    istok_bold_woff = load_file(\"fonts/istok-bold_woff64.txt\", encoding=None)\n\n    font_styles = (\n        \"\"\"\n        @font-face {\n          font-family: \"istok\";\n          font-weight: 400;\n          font-style: normal;\n          src: url(data:font/truetype;charset=utf-8;base64,\"\"\"\n        + istok_woff\n        + \"\"\") format(\"woff\");\n        }\n\n        @font-face {\n          font-family: \"istok\";\n          font-weight: 700;\n          font-style: normal;\n          src: url(data:font/truetype;charset=utf-8;base64,\"\"\"\n        + istok_bold_woff\n        + \"\"\") format(\"woff\");\n        }\n        \"\"\"\n    )\n\n    main_styles = load_file(\"css/main.css\", encoding=\"utf-8\" if PY3 else None).split(\n        \"\\n\"\n    )\n    main_styles = \"\\n\".join(main_styles[28:])\n    main_styles = \"{}\\n{}\\n\".format(font_styles, main_styles)\n\n    if not PY3:\n        main_styles = main_styles.decode(\"utf-8\")\n\n    return_data[\"results\"] = {r[IDX_COL]: r for r in return_data[\"results\"]}\n\n    polyfills_js = load_file(\"dist/polyfills_bundle.js\")\n    export_js = load_file(\"dist/export_bundle.js\")\n\n    return send_file(\n        base_render_template(\n            \"dtale/html_export.html\",\n            data_id,\n            main_styles=main_styles,\n            polyfills_js=polyfills_js,\n            export_js=export_js,\n            response=return_data,\n        ),\n        \"dtale_html_export_{}.html\".format(json_timestamp(pd.Timestamp(\"now\"))),\n        \"text/html\",\n    )\n\n\n@dtale.route(\"/load-filtered-ranges/<data_id>\")\n@exception_decorator\ndef load_filtered_ranges(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    final_query = build_query(data_id, curr_settings.get(\"query\"))\n    if not final_query:\n        return {}\n    curr_filtered_ranges = curr_settings.get(\"filteredRanges\", {})\n    if final_query == curr_filtered_ranges.get(\"query\"):\n        return jsonify(curr_filtered_ranges)\n    data = run_query(\n        handle_predefined(data_id),\n        final_query,\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n\n    def _filter_numeric(col):\n        s = data[col]\n        dtype = find_dtype(s)\n        return classify_type(dtype) in [\"F\", \"I\"] and not s.isnull().all()\n\n    numeric_cols = [col for col in data.columns if _filter_numeric(col)]\n    filtered_ranges = calc_data_ranges(data[numeric_cols])\n    updated_dtypes = build_dtypes_state(\n        data, global_state.get_dtypes(data_id) or [], filtered_ranges\n    )\n    updated_dtypes = {col[\"name\"]: col for col in updated_dtypes}\n    overall_min, overall_max = None, None\n    if len(filtered_ranges):\n        overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n        overall_max = max([v[\"max\"] for v in filtered_ranges.values()])\n    curr_settings[\"filteredRanges\"] = dict(\n        query=final_query,\n        ranges=filtered_ranges,\n        dtypes=updated_dtypes,\n        overall=dict(min=overall_min, max=overall_max),\n    )\n    global_state.set_settings(data_id, curr_settings)\n    return jsonify(curr_settings[\"filteredRanges\"])\n\n\n@dtale.route(\"/data-export/<data_id>\")\n@exception_decorator\ndef data_export(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_dtypes = global_state.get_dtypes(data_id) or []\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    data = data[\n        [\n            c[\"name\"]\n            for c in sorted(curr_dtypes, key=lambda c: c[\"index\"])\n            if c[\"visible\"]\n        ]\n    ]\n    file_type = get_str_arg(request, \"type\", \"csv\")\n    if file_type in [\"csv\", \"tsv\"]:\n        tsv = file_type == \"tsv\"\n        csv_buffer = export_to_csv_buffer(data, tsv=tsv)\n        filename = build_chart_filename(\"data\", ext=file_type)\n        return send_file(csv_buffer.getvalue(), filename, \"text/{}\".format(file_type))\n    elif file_type == \"parquet\":\n        from dtale.utils import export_to_parquet_buffer\n\n        parquet_buffer = export_to_parquet_buffer(data)\n        filename = build_chart_filename(\"data\", ext=\"parquet.gzip\")\n        return send_file(\n            parquet_buffer.getvalue(), filename, \"application/octet-stream\"\n        )\n    return jsonify(success=False)\n\n\n@dtale.route(\"/column-analysis/<data_id>\")\n@exception_decorator\ndef get_column_analysis(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns output from numpy.histogram/pd.value_counts to front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param col: string from flask.request.args['col'] containing name of a column in your dataframe\n    :param type: string from flask.request.args['type'] to signify either a histogram or value counts\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :param bins: the number of bins to display in your histogram, options on the front-end are 5, 10, 20, 50\n    :param top: the number of top values to display in your value counts, default is 100\n    :returns: JSON {results: DATA, desc: output from pd.DataFrame[col].describe(), success: True/False}\n    \"\"\"\n\n    analysis = ColumnAnalysis(data_id, request)\n    return jsonify(**analysis.build())\n\n\n@matplotlib_decorator\ndef build_correlations_matrix_image(\n    data,\n    is_pps,\n    valid_corr_cols,\n    valid_str_corr_cols,\n    valid_date_cols,\n    dummy_col_mappings,\n    pps_data,\n    code,\n):\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\n    plt.figure(figsize=(20, 12))\n    ax0 = plt.gca()\n    cmap = \"Blues\" if is_pps else \"RdYlGn\"\n    vmin = 0.0 if is_pps else -1.0\n    sns.heatmap(\n        data.mask(data.apply(lambda x: x.name == x.index)),\n        ax=ax0,\n        vmin=vmin,\n        vmax=1.0,\n        xticklabels=True,\n        yticklabels=True,\n        cmap=cmap,\n    )\n    output = BytesIO()\n    FigureCanvas(ax0.get_figure()).print_png(output)\n    return (\n        valid_corr_cols,\n        valid_str_corr_cols,\n        valid_date_cols,\n        dummy_col_mappings,\n        pps_data,\n        code,\n        output.getvalue(),\n    )\n\n\ndef build_correlations_matrix(data_id, is_pps=False, encode_strings=False, image=False):\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    valid_corr_cols, valid_str_corr_cols, valid_date_cols = correlations.get_col_groups(\n        data_id, data\n    )\n\n    str_encodings_code = \"\"\n    dummy_col_mappings = {}\n    if encode_strings and valid_str_corr_cols:\n        data = data[valid_corr_cols + valid_str_corr_cols]\n        dummy_kwargs = {}\n        if pandas_util.is_pandas2():\n            dummy_kwargs[\"dtype\"] = \"int\"\n        for str_col in valid_str_corr_cols:\n            dummies = pd.get_dummies(data[[str_col]], columns=[str_col], **dummy_kwargs)\n            dummy_cols = list(dummies.columns)\n            dummy_col_mappings[str_col] = dummy_cols\n            data[dummy_cols] = dummies\n            valid_corr_cols += dummy_cols\n        str_encodings_code = (\n            \"str_corr_cols = [\\n\\t'{valid_str_corr_cols}'\\n]\\n\"\n            \"dummies = pd.get_dummies(corr_data, str_corr_cols)\\n\"\n            \"corr_data.loc[:, dummies.columns] = dummies\\n\"\n        ).format(valid_str_corr_cols=\"', '\".join(valid_str_corr_cols))\n    else:\n        data = data[valid_corr_cols]\n\n    corr_cols_str = \"'\\n\\t'\".join(\n        [\"', '\".join(chunk) for chunk in divide_chunks(valid_corr_cols, 8)]\n    )\n\n    pps_data = None\n    if is_pps:\n        code = build_code_export(data_id, imports=\"import ppscore\\n\")\n        code.append(\n            (\n                \"corr_cols = [\\n\"\n                \"\\t'{corr_cols}'\\n\"\n                \"]\\n\"\n                \"corr_data = df[corr_cols]\\n\"\n                \"{str_encodings}\"\n                \"corr_data = ppscore.matrix(corr_data)\\n\"\n            ).format(corr_cols=corr_cols_str, str_encodings=str_encodings_code)\n        )\n\n        data, pps_data = get_ppscore_matrix(data[valid_corr_cols])\n    else:\n        data, matrix_code = correlations.build_matrix(\n            data_id,\n            data,\n            valid_corr_cols,\n            {\"corr_cols\": corr_cols_str, \"str_encodings\": str_encodings_code},\n        )\n        code = [matrix_code]\n\n    code.append(\n        \"corr_data.index.name = str('column')\\ncorr_data = corr_data.reset_index()\"\n    )\n    code = \"\\n\".join(code)\n    data.index.name = str(\"column\")\n    if image:\n        return build_correlations_matrix_image(\n            data,\n            is_pps,\n            valid_corr_cols,\n            valid_str_corr_cols,\n            valid_date_cols,\n            dummy_col_mappings,\n            pps_data,\n            code,\n        )\n    return (\n        valid_corr_cols,\n        valid_str_corr_cols,\n        valid_date_cols,\n        dummy_col_mappings,\n        pps_data,\n        code,\n        data,\n    )\n\n\n@dtale.route(\"/correlations/<data_id>\")\n@exception_decorator\ndef get_correlations(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which gathers Pearson correlations against all combinations of columns with\n    numeric data using :meth:`pandas:pandas.DataFrame.corr`\n\n    On large datasets with no :attr:`numpy:numpy.nan` data this code will use :meth:`numpy:numpy.corrcoef`\n    for speed purposes\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :returns: JSON {\n        data: [{column: col1, col1: 1.0, col2: 0.99, colN: 0.45},...,{column: colN, col1: 0.34, col2: 0.88, colN: 1.0}],\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    is_pps = get_bool_arg(request, \"pps\")\n    image = get_bool_arg(request, \"image\")\n    matrix_data = build_correlations_matrix(\n        data_id,\n        is_pps=is_pps,\n        encode_strings=get_bool_arg(request, \"encodeStrings\"),\n        image=image,\n    )\n    (\n        valid_corr_cols,\n        valid_str_corr_cols,\n        valid_date_cols,\n        dummy_col_mappings,\n        pps_data,\n        code,\n        df_or_image,\n    ) = matrix_data\n    if image:\n        fname = \"{}.png\".format(\"predictive_power_score\" if is_pps else \"correlations\")\n        return send_file(df_or_image, fname, \"image/png\")\n\n    data = df_or_image.reset_index()\n    col_types = grid_columns(data)\n    f = grid_formatter(col_types, nan_display=None)\n    return jsonify(\n        data=f.format_dicts(data.itertuples()),\n        dates=valid_date_cols,\n        strings=valid_str_corr_cols,\n        dummyColMappings=dummy_col_mappings,\n        code=code,\n        pps=pps_data,\n    )\n\n\n@dtale.route(\"/corr-analysis/<data_id>\")\n@exception_decorator\ndef get_corr_analysis(data_id):\n    column_name, max_score, corrs, ranks = correlations.get_analysis(data_id)\n    return jsonify(\n        column_name=column_name, max_score=max_score, corrs=corrs, ranks=ranks\n    )\n\n\n@dtale.route(\"/chart-data/<data_id>\")\n@exception_decorator\ndef get_chart_data(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which builds data associated with a chart.js chart\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :param x: string from flask.request.args['x'] column to be used as x-axis of chart\n    :param y: string from flask.request.args['y'] column to be used as y-axis of chart\n    :param group: string from flask.request.args['group'] comma-separated string of columns to group chart data by\n    :param agg: string from flask.request.args['agg'] points to a specific function that can be applied to\n                :func: pandas.core.groupby.DataFrameGroupBy.  Possible values are: count, first, last mean,\n                median, min, max, std, var, mad, prod, sum\n    :returns: JSON {\n        data: {\n            series1: { x: [x1, x2, ..., xN], y: [y1, y2, ..., yN] },\n            series2: { x: [x1, x2, ..., xN], y: [y1, y2, ..., yN] },\n            ...,\n            seriesN: { x: [x1, x2, ..., xN], y: [y1, y2, ..., yN] },\n        },\n        min: minY,\n        max: maxY,\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, get_str_arg(request, \"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    x = get_str_arg(request, \"x\")\n    y = get_json_arg(request, \"y\")\n    group_col = get_json_arg(request, \"group\")\n    agg = get_str_arg(request, \"agg\")\n    allow_duplicates = get_bool_arg(request, \"allowDupes\")\n    window = get_int_arg(request, \"rollingWin\")\n    comp = get_str_arg(request, \"rollingComp\")\n    data, code = build_base_chart(\n        data,\n        x,\n        y,\n        group_col=group_col,\n        agg=agg,\n        allow_duplicates=allow_duplicates,\n        rolling_win=window,\n        rolling_comp=comp,\n    )\n    data[\"success\"] = True\n    return jsonify(data)\n\n\ndef get_ppscore(df, col1, col2):\n    if not PY3:\n        return None\n    try:\n        import dtale.ppscore as ppscore\n\n        pps = ppscore.score(df, col1, col2)\n        pps[\"model\"] = pps[\"model\"].__str__()\n        pps[\"ppscore\"] = float(pps[\"ppscore\"])\n        pps[\"baseline_score\"] = float(pps[\"baseline_score\"])\n        pps[\"model_score\"] = float(pps[\"model_score\"])\n        return pps\n    except BaseException:\n        return None\n\n\ndef get_ppscore_matrix(df):\n    if not PY3:\n        return [], None\n    try:\n        import dtale.ppscore as ppscore\n\n        pps_data = ppscore.matrix(df)\n        data = (\n            pps_data[[\"x\", \"y\", \"ppscore\"]].set_index([\"x\", \"y\"]).unstack()[\"ppscore\"]\n        )\n\n        # additional PPS display\n        pps_data.loc[:, \"model\"] = pps_data[\"model\"].astype(\"str\")\n        pps_data = format_grid(pps_data)\n        pps_data = pps_data[\"results\"]\n        return data, pps_data\n    except BaseException:\n        return [], None\n\n\ndef update_df_for_encoded_strings(df, dummy_cols, cols, code):\n    if not dummy_cols:\n        return df\n    dummy_kwargs = {}\n    if pandas_util.is_pandas2():\n        dummy_kwargs[\"dtype\"] = \"int\"\n    dummies = pd.get_dummies(df[dummy_cols], columns=dummy_cols, **dummy_kwargs)\n    dummies = dummies[[c for c in dummies.columns if c in cols]]\n    df[dummies.columns] = dummies\n\n    code.append(\n        (\n            \"dummy_cols = ['{}']\\n\"\n            \"dummies = pd.get_dummies(df[dummy_cols], columns=dummy_cols)\\n\"\n            \"final_cols = ['{}']\\n\"\n            \"dummies = dummies[[c for c in dummies.columns if c in final_cols]]\\n\"\n            \"df.loc[:, dummies.columns] = dummies\"\n        ).format(\"', '\".join(dummy_cols), \"', '\".join(cols))\n    )\n    return df\n\n\n@dtale.route(\"/correlations-ts/<data_id>\")\n@exception_decorator\ndef get_correlations_ts(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns timeseries of Pearson correlations of two columns with numeric data\n    using :meth:`pandas:pandas.DataFrame.corr`\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param cols: comma-separated string from flask.request.args['cols'] containing names of two columns in dataframe\n    :param dateCol: string from flask.request.args['dateCol'] with name of date-type column in dateframe for timeseries\n    :returns: JSON {\n        data: {:col1:col2: {data: [{corr: 0.99, date: 'YYYY-MM-DD'},...], max: 0.99, min: 0.99}\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    cols = get_json_arg(request, \"cols\")\n    [col1, col2] = cols\n    date_col = get_str_arg(request, \"dateCol\")\n    rolling = get_bool_arg(request, \"rolling\")\n    rolling_window = get_int_arg(request, \"rollingWindow\")\n    min_periods = get_int_arg(request, \"minPeriods\")\n    dummy_cols = get_json_arg(request, \"dummyCols\", [])\n\n    code = build_code_export(data_id)\n    pps = get_ppscore(data, col1, col2)\n\n    if rolling:\n        data = data[\n            [c for c in data.columns if c in [date_col, col1, col2] + dummy_cols]\n        ]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        data = data.set_index(date_col)\n        rolling_kwargs = {}\n        if min_periods is not None:\n            rolling_kwargs[\"min_periods\"] = min_periods\n        data = (\n            data[[col1, col2]]\n            .rolling(rolling_window, **rolling_kwargs)\n            .corr()\n            .reset_index()\n        )\n        data = data.dropna()\n        data = data[data[\"level_1\"] == col1][[date_col, col2]]\n        code.append(\n            (\n                \"corr_ts = df[['{date_col}', '{col1}', '{col2}']].set_index('{date_col}')\\n\"\n                \"corr_ts = corr_ts[['{col1}', '{col2}']].rolling({rolling_window}, min_periods=min_periods).corr()\\n\"\n                \"corr_ts = corr_ts.reset_index().dropna()\\n\"\n                \"corr_ts = corr_ts[corr_ts['level_1'] == '{col1}'][['{date_col}', '{col2}']]\"\n            ).format(\n                col1=col1, col2=col2, date_col=date_col, rolling_window=rolling_window\n            )\n        )\n    else:\n        data = data[[c for c in data.columns if c in [date_col] + cols + dummy_cols]]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        data = data.groupby(date_col)[cols].corr(method=\"pearson\")\n        data.index.names = [\"date\", \"column\"]\n        data = data.reset_index()\n        data = data[data.column == col1][[\"date\", col2]]\n        code.append(\n            (\n                \"corr_ts = df.groupby('{date_col}')['{cols}'].corr(method='pearson')\\n\"\n                \"corr_ts.index.names = ['date', 'column']\\n\"\n                \"corr_ts = corr_ts[corr_ts.column == '{col1}'][['date', '{col2}']]\\n\"\n            ).format(col1=col1, col2=col2, date_col=date_col, cols=\"', '\".join(cols))\n        )\n        if rolling_window:\n            data = data.set_index(\"date\")\n            rolling_kwargs = {}\n            if min_periods is not None:\n                rolling_kwargs[\"min_periods\"] = min_periods\n            data = (\n                data[[col2]]\n                .rolling(rolling_window, **rolling_kwargs)\n                .mean()\n                .reset_index()\n            )\n            data = data.dropna()\n            code.append(\n                (\n                    \"corr_ts = corr_ts.set_index('date')\\n\"\n                    \"corr_ts = corr_ts[['{col}']].rolling({rolling_window}, min_periods={min_periods}).mean()\\n\"\n                    \"corr_ts = corr_ts.reset_index().dropna()\"\n                ).format(\n                    col=col2, rolling_window=rolling_window, min_periods=min_periods\n                )\n            )\n\n    data.columns = [\"date\", \"corr\"]\n    code.append(\"corr_ts.columns = ['date', 'corr']\")\n    return_data, _code = build_base_chart(data.fillna(0), \"date\", \"corr\", agg=\"raw\")\n    return_data[\"success\"] = True\n    return_data[\"code\"] = \"\\n\".join(code)\n    return_data[\"pps\"] = pps\n    return jsonify(return_data)\n\n\n@dtale.route(\"/scatter/<data_id>\")\n@exception_decorator\ndef get_scatter(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns data used in correlation of two columns for scatter chart\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param cols: comma-separated string from flask.request.args['cols'] containing names of two columns in dataframe\n    :param dateCol: string from flask.request.args['dateCol'] with name of date-type column in dateframe for timeseries\n    :param date: string from flask.request.args['date'] date value in dateCol to filter dataframe to\n    :returns: JSON {\n        data: [{col1: 0.123, col2: 0.123, index: 1},...,{col1: 0.123, col2: 0.123, index: N}],\n        stats: {\n        stats: {\n            correlated: 50,\n            only_in_s0: 1,\n            only_in_s1: 2,\n            pearson: 0.987,\n            spearman: 0.879,\n        }\n        x: col1,\n        y: col2\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    cols = get_json_arg(request, \"cols\")\n    dummy_cols = get_json_arg(request, \"dummyCols\", [])\n    date_index = get_int_arg(request, \"index\")\n    date_col = get_str_arg(request, \"dateCol\")\n    rolling = get_bool_arg(request, \"rolling\")\n\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    idx_col = str(\"_corr_index\")\n    y_cols = [cols[1], idx_col]\n    code = build_code_export(data_id)\n    selected_date = None\n    if rolling:\n        data = data[[c for c in data.columns if c in [date_col] + cols + dummy_cols]]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        window = get_int_arg(request, \"window\")\n        min_periods = get_int_arg(request, \"minPeriods\", default=0)\n        dates = data[date_col].sort_values().unique()\n        date_index = min(date_index + max(min_periods - 1, 1), len(dates) - 1)\n        selected_date = dates[date_index]\n        idx = min(data[data[date_col] == selected_date].index) + 1\n        selected_date = json_date(selected_date, nan_display=None)\n        selected_date = \"{} thru {}\".format(\n            json_date(dates[max(date_index - (window - 1), 0)]), selected_date\n        )\n        data = data.iloc[max(idx - window, 0) : idx]\n        data = data[cols + [date_col]].dropna(how=\"any\")\n        y_cols.append(date_col)\n        code.append(\n            (\n                \"idx = min(df[df['{date_col}'] == '{date}'].index) + 1\\n\"\n                \"scatter_data = scatter_data.iloc[max(idx - {window}, 0):idx]\\n\"\n                \"scatter_data = scatter_data['{cols}'].dropna(how='any')\"\n            ).format(\n                date_col=date_col,\n                date=selected_date,\n                window=window,\n                cols=\"', '\".join(sorted(list(set(cols)) + [date_col])),\n            )\n        )\n    else:\n        selected_cols = (\n            ([date_col] if date_index is not None else []) + cols + dummy_cols\n        )\n        data = data[[c for c in data.columns if c in selected_cols]]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        if date_index is not None:\n            selected_date = data[date_col].sort_values().unique()[date_index]\n            data = data[data[date_col] == selected_date]\n            selected_date = json_date(selected_date, nan_display=None)\n        data = data[cols].dropna(how=\"any\")\n        code.append(\n            (\n                \"scatter_data = df[df['{date_col}'] == '{date}']\"\n                if date_index is not None\n                else \"scatter_data = df\"\n            ).format(date_col=date_col, date=selected_date)\n        )\n        code.append(\n            \"scatter_data = scatter_data['{cols}'].dropna(how='any')\".format(\n                cols=\"', '\".join(cols)\n            )\n        )\n\n    data[idx_col] = data.index\n    [col1, col2] = cols\n    s0 = data[col1]\n    s1 = data[col2]\n    pearson = s0.corr(s1, method=\"pearson\")\n    spearman = s0.corr(s1, method=\"spearman\")\n    pps = get_ppscore(data, col1, col2)\n    stats = dict(\n        pearson=\"N/A\" if pd.isnull(pearson) else pearson,\n        spearman=\"N/A\" if pd.isnull(spearman) else spearman,\n        pps=pps,\n        correlated=len(data),\n        only_in_s0=len(data[data[col1].isnull()]),\n        only_in_s1=len(data[data[col2].isnull()]),\n    )\n    code.append(\n        (\n            \"scatter_data['{idx_col}'] = scatter_data.index\\n\"\n            \"s0 = scatter_data['{col1}']\\n\"\n            \"s1 = scatter_data['{col2}']\\n\"\n            \"pearson = s0.corr(s1, method='pearson')\\n\"\n            \"spearman = s0.corr(s1, method='spearman')\\n\"\n            \"\\nimport ppscore\\n\\n\"\n            \"pps = ppscore.score(data, '{col1}', '{col2}')\\n\"\n            \"only_in_s0 = len(scatter_data[scatter_data['{col1}'].isnull()])\\n\"\n            \"only_in_s1 = len(scatter_data[scatter_data['{col2}'].isnull()])\"\n        ).format(col1=col1, col2=col2, idx_col=idx_col)\n    )\n\n    max_points = global_state.get_chart_settings()[\"scatter_points\"]\n    if len(data) > max_points:\n        return jsonify(\n            stats=stats,\n            code=\"\\n\".join(code),\n            error=\"Dataset exceeds {:,} records, cannot render scatter. Please apply filter...\".format(\n                max_points\n            ),\n            traceback=CHART_POINTS_LIMIT,\n        )\n    data, _code = build_base_chart(data, cols[0], y_cols, allow_duplicates=True)\n    data[\"x\"] = cols[0]\n    data[\"y\"] = cols[1]\n    data[\"stats\"] = stats\n    data[\"code\"] = \"\\n\".join(code)\n    data[\"date\"] = \" for {}\".format(selected_date) if selected_date else \"\"\n    return jsonify(data)\n\n\ndef build_context_variables(data_id, new_context_vars=None):\n    \"\"\"\n    Build and return the dictionary of context variables associated with a process.\n    If the names of any new variables are not formatted properly, an exception will be raised.\n    New variables will overwrite the values of existing variables if they share the same name.\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param new_context_vars: dictionary of name, value pairs for new context variables\n    :type new_context_vars: dict, optional\n    :returns: dict of the context variables for this process\n    :rtype: dict\n    \"\"\"\n    if new_context_vars:\n        for name, value in new_context_vars.items():\n            if not isinstance(name, string_types):\n                raise SyntaxError(\n                    \"{}, context variables must be a valid string\".format(name)\n                )\n            elif not name.replace(\"_\", \"\").isalnum():\n                raise SyntaxError(\n                    \"{}, context variables can only contain letters, digits, or underscores\".format(\n                        name\n                    )\n                )\n            elif name.startswith(\"_\"):\n                raise SyntaxError(\n                    \"{}, context variables can not start with an underscore\".format(\n                        name\n                    )\n                )\n\n    return dict_merge(global_state.get_context_variables(data_id), new_context_vars)\n\n\n@dtale.route(\"/filter-info/<data_id>\")\n@exception_decorator\ndef get_filter_info(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns a view-only version of the query, column filters & context variables\n    to the front end.\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: JSON\n    \"\"\"\n\n    def value_as_str(value):\n        \"\"\"Convert values into a string representation that can be shown to the user in the front-end.\"\"\"\n        return str(value)[:1000]\n\n    ctxt_vars = global_state.get_context_variables(data_id) or {}\n    ctxt_vars = [dict(name=k, value=value_as_str(v)) for k, v in ctxt_vars.items()]\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_settings = {\n        k: v\n        for k, v in curr_settings.items()\n        if k\n        in [\n            \"query\",\n            \"columnFilters\",\n            \"outlierFilters\",\n            \"predefinedFilters\",\n            \"invertFilter\",\n            \"highlightFilter\",\n        ]\n    }\n    return jsonify(contextVars=ctxt_vars, success=True, **curr_settings)\n\n\n@dtale.route(\"/xarray-coordinates/<data_id>\")\n@exception_decorator\ndef get_xarray_coords(data_id):\n    ds = global_state.get_dataset(data_id)\n\n    def _format_dim(coord, count):\n        return dict(name=coord, count=count, dtype=ds.coords[coord].dtype.name)\n\n    coord_data = [_format_dim(coord, count) for coord, count in ds.coords.dims.items()]\n    return jsonify(data=coord_data)\n\n\n@dtale.route(\"/xarray-dimension-values/<data_id>/<dim>\")\n@exception_decorator\ndef get_xarray_dimension_values(data_id, dim):\n    ds = global_state.get_dataset(data_id)\n    dim_entries = ds.coords[dim].data\n    dim = pd.DataFrame({\"value\": dim_entries})\n    dim_f, _ = build_formatters(dim)\n    return jsonify(data=dim_f.format_dicts(dim.itertuples()))\n\n\n@dtale.route(\"/update-xarray-selection/<data_id>\")\n@exception_decorator\ndef update_xarray_selection(data_id):\n    ds = global_state.get_dataset(data_id)\n    selection = get_json_arg(request, \"selection\") or {}\n    df = convert_xarray_to_dataset(ds, **selection)\n    startup(data=df, data_id=data_id, ignore_duplicate=True)\n    global_state.set_dataset_dim(data_id, selection)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/to-xarray/<data_id>\")\n@exception_decorator\ndef to_xarray(data_id):\n    df = global_state.get_data(data_id)\n    index_cols = get_json_arg(request, \"index\")\n    ds = df.set_index(index_cols).to_xarray()\n    startup(data=ds, data_id=data_id, ignore_duplicate=True)\n    curr_settings = global_state.get_settings(data_id)\n    startup_code = \"df = df.set_index(['{index}']).to_xarray()\".format(\n        index=\"', '\".join(index_cols)\n    )\n    global_state.set_settings(\n        data_id, dict_merge(curr_settings, dict(startup_code=startup_code))\n    )\n    return jsonify(success=True)\n\n\n@dtale.route(\"/code-export/<data_id>\")\n@exception_decorator\ndef get_code_export(data_id):\n    code = build_code_export(data_id)\n    return jsonify(code=\"\\n\".join(code), success=True)\n\n\ndef build_chart_filename(chart_type, ext=\"html\"):\n    return \"{}_export_{}.{}\".format(\n        chart_type, json_timestamp(pd.Timestamp(\"now\")), ext\n    )\n\n\ndef send_file(output, filename, content_type):\n    resp = make_response(output)\n    resp.headers[\"Content-Disposition\"] = \"attachment; filename=%s\" % filename\n    resp.headers[\"Content-Type\"] = content_type\n    return resp\n\n\n@dtale.route(\"/chart-export/<data_id>\")\n@exception_decorator\ndef chart_export(data_id):\n    export_type = get_str_arg(request, \"export_type\")\n    params = chart_url_params(request.args.to_dict())\n    if export_type == \"png\":\n        output = export_png(data_id, params)\n        filename = build_chart_filename(params[\"chart_type\"], ext=\"png\")\n        content_type = \"image/png\"\n    else:\n        output = export_chart(data_id, params)\n        filename = build_chart_filename(params[\"chart_type\"])\n        content_type = \"text/html\"\n    return send_file(output, filename, content_type)\n\n\n@dtale.route(\"/chart-export-all/<data_id>\")\n@exception_decorator\ndef chart_export_all(data_id):\n    params = chart_url_params(request.args.to_dict())\n    params[\"export_all\"] = True\n    output = export_chart(data_id, params)\n    filename = build_chart_filename(params[\"chart_type\"])\n    content_type = \"text/html\"\n    return send_file(output, filename, content_type)\n\n\n@dtale.route(\"/chart-csv-export/<data_id>\")\n@exception_decorator\ndef chart_csv_export(data_id):\n    params = chart_url_params(request.args.to_dict())\n    csv_buffer = export_chart_data(data_id, params)\n    filename = build_chart_filename(params[\"chart_type\"], ext=\"csv\")\n    return send_file(csv_buffer.getvalue(), filename, \"text/csv\")\n\n\n@dtale.route(\"/cleanup-datasets\")\n@exception_decorator\ndef cleanup_datasets():\n    data_ids = get_str_arg(request, \"dataIds\")\n    data_ids = (data_ids or \"\").split(\",\")\n    for data_id in data_ids:\n        global_state.cleanup(data_id)\n    return jsonify(success=True)\n\n\ndef load_new_data(df, startup_code, name=None, return_id=False, settings=None):\n    instance = startup(data=df, name=name, ignore_duplicate=True)\n    curr_settings = global_state.get_settings(instance._data_id)\n    global_state.set_settings(\n        instance._data_id,\n        dict_merge(curr_settings, dict(startup_code=startup_code, **(settings or {}))),\n    )\n    if return_id:\n        return instance._data_id\n    return jsonify(success=True, data_id=instance._data_id)\n\n\ndef handle_excel_upload(dfs):\n    sheet_names = list(dfs.keys())\n    data_ids = []\n    for sheet_name in sheet_names:\n        df, code = dfs[sheet_name]\n        if len(sheet_names) == 1:\n            return load_new_data(df, code, name=sheet_name)\n        data_id = load_new_data(df, code, name=sheet_name, return_id=True)\n        data_ids.append(dict(name=sheet_name, dataId=data_id))\n    return jsonify(dict(sheets=data_ids, success=True))\n\n\nUPLOAD_SEPARATORS = {\"comma\": \",\", \"tab\": \"\\t\", \"colon\": \":\", \"pipe\": \"|\"}\n\n\ndef build_csv_kwargs(request):\n    # Set engine to python to auto detect delimiter...\n    kwargs = {\"sep\": None}\n    sep_type = request.form.get(\"separatorType\")\n\n    if sep_type in UPLOAD_SEPARATORS:\n        kwargs[\"sep\"] = UPLOAD_SEPARATORS[sep_type]\n    elif sep_type == \"custom\" and request.form.get(\"separator\"):\n        kwargs[\"sep\"] = request.form[\"separator\"]\n        kwargs[\"sep\"] = str(kwargs[\"sep\"]) if PY3 else kwargs[\"sep\"].encode(\"utf8\")\n\n    if \"header\" in request.form:\n        kwargs[\"header\"] = 0 if request.form[\"header\"] == \"true\" else None\n\n    return kwargs\n\n\n@dtale.route(\"/upload\", methods=[\"POST\"])\n@exception_decorator\ndef upload():\n    if not request.files:\n        raise Exception(\"No file data loaded!\")\n    for filename in request.files:\n        contents = request.files[filename]\n        _, ext = os.path.splitext(filename)\n        if ext in [\".csv\", \".tsv\"]:\n            kwargs = build_csv_kwargs(request)\n            df = pd.read_csv(\n                StringIO(contents.read().decode()), engine=\"python\", **kwargs\n            )\n            return load_new_data(\n                df, \"df = pd.read_csv('{}', engine='python', sep=None)\".format(filename)\n            )\n        if ext in [\".xls\", \".xlsx\"]:\n            engine = \"xlrd\" if ext == \".xls\" else \"openpyxl\"\n            dfs = pd.read_excel(contents, sheet_name=None, engine=engine)\n\n            def build_xls_code(sheet_name):\n                return \"df = pd.read_excel('{}', sheet_name='{}', engine='{}')\".format(\n                    filename, sheet_name, engine\n                )\n\n            dfs = {\n                sheet_name: (df, build_xls_code(sheet_name))\n                for sheet_name, df in dfs.items()\n            }\n            return handle_excel_upload(dfs)\n        if \"parquet\" in filename:\n            df = pd.read_parquet(contents)\n            return load_new_data(df, \"df = pd.read_parquet('{}')\".format(filename))\n        raise Exception(\"File type of {} is not supported!\".format(ext))\n\n\n@dtale.route(\"/web-upload\")\n@exception_decorator\ndef web_upload():\n    from dtale.cli.loaders.csv_loader import loader_func as load_csv\n    from dtale.cli.loaders.json_loader import loader_func as load_json\n    from dtale.cli.loaders.excel_loader import load_file as load_excel\n    from dtale.cli.loaders.parquet_loader import loader_func as load_parquet\n\n    data_type = get_str_arg(request, \"type\")\n    url = get_str_arg(request, \"url\")\n    proxy = get_str_arg(request, \"proxy\")\n    if data_type == \"csv\":\n        df = load_csv(path=url, proxy=proxy)\n        startup_code = (\n            \"from dtale.cli.loaders.csv_loader import loader_func as load_csv\\n\\n\"\n            \"df = load_csv(path='{url}'{proxy})\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    elif data_type == \"tsv\":\n        df = load_csv(path=url, proxy=proxy, delimiter=\"\\t\")\n        startup_code = (\n            \"from dtale.cli.loaders.csv_loader import loader_func as load_csv\\n\\n\"\n            \"df = load_csv(path='{url}'{proxy}, delimiter='\\t')\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    elif data_type == \"json\":\n        df = load_json(path=url, proxy=proxy)\n        startup_code = (\n            \"from dtale.cli.loaders.json_loader import loader_func as load_json\\n\\n\"\n            \"df = load_json(path='{url}'{proxy})\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    elif data_type == \"excel\":\n        dfs = load_excel(path=url, proxy=proxy)\n\n        def build_xls_code(sheet_name):\n            return (\n                \"from dtale.cli.loaders.excel_loader import load_file as load_excel\\n\\n\"\n                \"df = load_excel(sheet_name='{sheet_name}', path='{url}'{proxy})\"\n            ).format(\n                sheet_name=sheet_name,\n                url=url,\n                proxy=\", '{}'\".format(proxy) if proxy else \"\",\n            )\n\n        dfs = {\n            sheet_name: (df, build_xls_code(sheet_name))\n            for sheet_name, df in dfs.items()\n        }\n        return handle_excel_upload(dfs)\n    elif data_type == \"parquet\":\n        df = load_parquet(path=url)\n        startup_code = (\n            \"from dtale.cli.loaders.parquet_loader import loader_func as load_parquet\\n\\n\"\n            \"df = load_parquet(path='{url}'{proxy})\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    return load_new_data(df, startup_code)\n\n\n@dtale.route(\"/datasets\")\n@exception_decorator\ndef dataset_upload():\n    dataset = get_str_arg(request, \"dataset\")\n    startup_code = \"from dtale.datasets import {dataset}\\n\\n\" \"df = {dataset}()\".format(\n        dataset=dataset\n    )\n    df, settings = getattr(datasets, dataset)()\n    return load_new_data(df, startup_code, settings=settings)\n\n\n@dtale.route(\"/build-column-copy/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef build_column_text(data_id):\n    columns = request.json.get(\"columns\")\n    columns = json.loads(columns)\n\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    return data[columns].to_csv(index=False, sep=\"\\t\", header=False)\n\n\n@dtale.route(\"/build-row-copy/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef build_row_text(data_id):\n    start, end, rows, columns = (\n        request.json.get(p) for p in [\"start\", \"end\", \"rows\", \"columns\"]\n    )\n    columns = json.loads(columns)\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    if rows:\n        rows = json.loads(rows)\n        data = data.iloc[rows, :]\n    else:\n        start = int(start)\n        end = int(end)\n        data = data.iloc[(start - 1) : end, :]\n    return data[columns].to_csv(index=False, sep=\"\\t\", header=False)\n\n\n@dtale.route(\"/network-data/<data_id>\")\n@exception_decorator\ndef network_data(data_id):\n    df = global_state.get_data(data_id)\n    to_col = get_str_arg(request, \"to\")\n    from_col = get_str_arg(request, \"from\")\n    group = get_str_arg(request, \"group\", \"\")\n    color = get_str_arg(request, \"color\", \"\")\n    weight = get_str_arg(request, \"weight\")\n\n    nodes = list(df[to_col].unique())\n    nodes += list(df[~df[from_col].isin(nodes)][from_col].unique())\n    nodes = sorted(nodes)\n    nodes = {node: node_id for node_id, node in enumerate(nodes, 1)}\n\n    edge_cols = [to_col, from_col]\n    if weight:\n        edge_cols.append(weight)\n\n    edges = df[[to_col, from_col]].applymap(nodes.get)\n    edges.columns = [\"to\", \"from\"]\n    if weight:\n        edges.loc[:, \"value\"] = df[weight]\n    edge_f = grid_formatter(grid_columns(edges), nan_display=\"nan\")\n    edges = edge_f.format_dicts(edges.itertuples())\n\n    def build_mapping(col):\n        if col:\n            return df[[from_col, col]].set_index(from_col)[col].astype(\"str\").to_dict()\n        return {}\n\n    group = build_mapping(group)\n    color = build_mapping(color)\n    groups = {}\n\n    def build_group(node, node_id):\n        group_val = group.get(node, \"N/A\")\n        groups[group_val] = node_id\n        return group_val\n\n    nodes = [\n        dict(\n            id=node_id,\n            label=node,\n            group=build_group(node, node_id),\n            color=color.get(node),\n        )\n        for node, node_id in nodes.items()\n    ]\n    return jsonify(dict(nodes=nodes, edges=edges, groups=groups, success=True))\n\n\n@dtale.route(\"/network-analysis/<data_id>\")\n@exception_decorator\ndef network_analysis(data_id):\n    df = global_state.get_data(data_id)\n    to_col = get_str_arg(request, \"to\")\n    from_col = get_str_arg(request, \"from\")\n    weight = get_str_arg(request, \"weight\")\n\n    G = nx.Graph()\n    max_edge, min_edge, avg_weight = (None, None, None)\n    if weight:\n        G.add_weighted_edges_from(\n            [tuple(x) for x in df[[to_col, from_col, weight]].values]\n        )\n        sorted_edges = sorted(\n            G.edges(data=True), key=lambda x: x[2][\"weight\"], reverse=True\n        )\n        max_edge = sorted_edges[0]\n        min_edge = sorted_edges[-1]\n        avg_weight = df[weight].mean()\n    else:\n        G.add_edges_from([tuple(x) for x in df[[to_col, from_col]].values])\n\n    most_connected_node = max(dict(G.degree()).items(), key=lambda x: x[1])\n    return_data = {\n        \"node_ct\": len(G),\n        \"triangle_ct\": int(sum(nx.triangles(G).values()) / 3),\n        \"most_connected_node\": \"{} (Connections: {})\".format(*most_connected_node),\n        \"leaf_ct\": sum((1 for edge, degree in dict(G.degree()).items() if degree == 1)),\n        \"edge_ct\": sum(dict(G.degree()).values()),\n        \"max_edge\": None\n        if max_edge is None\n        else \"{} (source: {}, target: {})\".format(\n            max_edge[-1][\"weight\"], max_edge[0], max_edge[1]\n        ),\n        \"min_edge\": None\n        if min_edge is None\n        else \"{} (source: {}, target: {})\".format(\n            min_edge[-1][\"weight\"], min_edge[0], min_edge[1]\n        ),\n        \"avg_weight\": json_float(avg_weight),\n    }\n    return jsonify(dict(data=return_data, success=True))\n\n\n@dtale.route(\"/shortest-path/<data_id>\")\n@exception_decorator\ndef shortest_path(data_id):\n    df = global_state.get_data(data_id)\n    to_col = get_str_arg(request, \"to\")\n    from_col = get_str_arg(request, \"from\")\n    start_val = get_str_arg(request, \"start\")\n    end_val = get_str_arg(request, \"end\")\n\n    G = nx.Graph()\n    G.add_edges_from([tuple(x) for x in df[[to_col, from_col]].values])\n    shortest_path = nx.shortest_path(G, source=start_val, target=end_val)\n    return jsonify(dict(data=shortest_path, success=True))\n\n\n@dtale.route(\"/sorted-sequential-diffs/<data_id>\")\n@exception_decorator\ndef get_sorted_sequential_diffs(data_id):\n    column = get_str_arg(request, \"col\")\n    sort = get_str_arg(request, \"sort\")\n    df = global_state.get_data(data_id)\n    metrics, _ = build_sequential_diffs(df[column], column, sort=sort)\n    return jsonify(metrics)\n\n\n@dtale.route(\"/merge\", methods=[\"POST\"])\n@exception_decorator\ndef build_merge():\n    cfg = request.json\n    name = cfg.get(\"name\")\n    builder = CombineData(cfg)\n    data = builder.build_data()\n    code = builder.build_code()\n    return load_new_data(data, code, name)\n\n\n@dtale.route(\"/missingno/<chart_type>/<data_id>\")\n@matplotlib_decorator\n@exception_decorator\ndef build_missingno_chart(chart_type, data_id):\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\n    df = global_state.get_data(data_id)\n    if chart_type == \"matrix\":\n        date_index = get_str_arg(request, \"date_index\")\n        freq = get_str_arg(request, \"freq\")\n        if date_index:\n            figure = msno.matrix(df.set_index(date_index), freq=freq)\n        else:\n            figure = msno.matrix(df)\n    elif chart_type == \"bar\":\n        figure = msno.bar(df)\n    elif chart_type == \"heatmap\":\n        figure = msno.heatmap(df)\n    elif chart_type == \"dendrogram\":\n        figure = msno.dendrogram(df)\n\n    output = BytesIO()\n    FigureCanvas(figure.get_figure()).print_png(output)\n    if get_bool_arg(request, \"file\"):\n        fname = \"missingno_{}.png\".format(chart_type)\n        return send_file(output.getvalue(), fname, \"image/png\")\n    return Response(output.getvalue(), mimetype=\"image/png\")\n\n\n@dtale.route(\"/drop-filtered-rows/<data_id>\")\n@exception_decorator\ndef drop_filtered_rows(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    final_query = build_query(data_id, curr_settings.get(\"query\"))\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\n        (\n            \"# drop filtered rows\\n\"\n            'df = df.query(\"{}\")'.format(final_query.replace(\"`\", \"\"))\n        )\n    ]\n    global_state.set_history(data_id, curr_history)\n    data = run_query(\n        handle_predefined(data_id),\n        final_query,\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, build_dtypes_state(data, []))\n    curr_predefined = curr_settings.get(\"predefinedFilters\", {})\n    global_state.update_settings(\n        data_id,\n        dict(\n            query=\"\",\n            columnFilters={},\n            outlierFilters={},\n            predefinedFilters={\n                k: dict_merge(v, {\"active\": False}) for k, v in curr_predefined.items()\n            },\n            invertFilter=False,\n        ),\n    )\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/move-filters-to-custom/<data_id>\")\n@exception_decorator\ndef move_filters_to_custom(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    query = build_query(data_id, curr_settings.get(\"query\"))\n    global_state.update_settings(\n        data_id,\n        {\n            \"columnFilters\": {},\n            \"outlierFilters\": {},\n            \"invertFilter\": False,\n            \"query\": query,\n        },\n    )\n    return jsonify(\n        dict(success=True, settings=global_state.get_settings(data_id) or {})\n    )\n\n\n@dtale.route(\"/gage-rnr/<data_id>\")\n@exception_decorator\ndef build_gage_rnr(data_id):\n    data = load_filterable_data(data_id, request)\n    operator_cols = get_json_arg(request, \"operator\")\n    operators = data.groupby(operator_cols)\n    measurements = get_json_arg(request, \"measurements\") or [\n        col for col in data.columns if col not in operator_cols\n    ]\n    sizes = set((len(operator) for _, operator in operators))\n    if len(sizes) > 1:\n        return jsonify(\n            dict(\n                success=False,\n                error=(\n                    \"Operators do not have the same amount of parts! Please select a new operator with equal rows in \"\n                    \"each group.\"\n                ),\n            )\n        )\n\n    res = gage_rnr.GageRnR(\n        np.array([operator[measurements].values for _, operator in operators])\n    ).calculate()\n    df = pd.DataFrame({name: res[name] for name in gage_rnr.ResultNames})\n    df.index = df.index.map(lambda idx: gage_rnr.ComponentNames.get(idx, idx))\n    df.index.name = \"Sources of Variance\"\n    df.columns = [gage_rnr.ResultNames.get(col, col) for col in df.columns]\n    df = df[df.index.isin(df.index.dropna())]\n    df = df.reset_index()\n    overrides = {\"F\": lambda f, i, c: f.add_float(i, c, precision=3)}\n    return jsonify(dict_merge(dict(success=True), format_grid(df, overrides)))\n\n\n@dtale.route(\"/timeseries-analysis/<data_id>\")\n@exception_decorator\ndef get_timeseries_analysis(data_id):\n    report_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    ts_rpt = TimeseriesAnalysis(data_id, report_type, cfg)\n    data = ts_rpt.run()\n    return jsonify(dict_merge(dict(success=True), data))\n\n\n@dtale.route(\"/arcticdb/libraries\")\n@exception_decorator\ndef get_arcticdb_libraries():\n    if get_bool_arg(request, \"refresh\"):\n        global_state.store.load_libraries()\n\n    libraries = global_state.store.libraries\n    is_async = False\n    if len(libraries) > 500:\n        is_async = True\n        libraries = libraries[:5]\n    ret_data = {\"success\": True, \"libraries\": libraries, \"async\": is_async}\n    if global_state.store.lib is not None:\n        ret_data[\"library\"] = global_state.store.lib.name\n    return jsonify(ret_data)\n\n\n@dtale.route(\"/arcticdb/async-libraries\")\n@exception_decorator\ndef get_async_arcticdb_libraries():\n    libraries = global_state.store.libraries\n    input = get_str_arg(request, \"input\")\n    vals = list(\n        itertools.islice((option(lib) for lib in libraries if lib.startswith(input)), 5)\n    )\n    return jsonify(vals)\n\n\n@dtale.route(\"/arcticdb/<library>/symbols\")\n@exception_decorator\ndef get_arcticdb_symbols(library):\n    if get_bool_arg(request, \"refresh\") or library not in global_state.store._symbols:\n        global_state.store.load_symbols(library)\n\n    symbols = global_state.store._symbols[library]\n    is_async = False\n    if len(symbols) > 500:\n        is_async = True\n        symbols = symbols[:5]\n    return jsonify({\"success\": True, \"symbols\": symbols, \"async\": is_async})\n\n\n@dtale.route(\"/arcticdb/<library>/async-symbols\")\n@exception_decorator\ndef get_async_arcticdb_symbols(library):\n    symbols = global_state.store._symbols[library]\n    input = get_str_arg(request, \"input\")\n    vals = list(\n        itertools.islice((option(sym) for sym in symbols if sym.startswith(input)), 5)\n    )\n    return jsonify(vals)\n\n\n@dtale.route(\"/arcticdb/load-description\")\n@exception_decorator\ndef load_arcticdb_description():\n    from arcticc.pb2.descriptors_pb2 import _TYPEDESCRIPTOR_VALUETYPE\n\n    library = get_str_arg(request, \"library\")\n    symbol = get_str_arg(request, \"symbol\")\n\n    lib = global_state.store.conn[library]\n    description = lib.get_description(symbol)\n\n    columns = list(\n        map(\n            lambda c: \"{} ({})\".format(\n                c.name, _TYPEDESCRIPTOR_VALUETYPE.values[c.dtype.value_type].name\n            ),\n            sorted(description.columns, key=lambda c: c.name),\n        )\n    )\n    index = list(\n        map(\n            lambda i: \"{} ({})\".format(\n                i[0], _TYPEDESCRIPTOR_VALUETYPE.values[i[1].value_type].name\n            ),\n            zip(description.index.name, description.index.dtype),\n        )\n    )\n    rows = description.row_count\n\n    description_str = (\n        \"ROWS: {rows:,.0f}\\n\"\n        \"INDEX:\\n\"\n        \"\\t- {index}\\n\"\n        \"COLUMNS ({col_count}):\\n\"\n        \"\\t- {columns}\\n\"\n    ).format(\n        rows=rows,\n        index=\"\\n\\t- \".join(index),\n        col_count=len(columns),\n        columns=\"\\n\\t- \".join(columns),\n    )\n    return jsonify(\n        dict(success=True, library=library, symbol=symbol, description=description_str)\n    )\n\n\n@dtale.route(\"/arcticdb/load-symbol\")\n@exception_decorator\ndef load_arcticdb_symbol():\n    library = get_str_arg(request, \"library\")\n    symbol = get_str_arg(request, \"symbol\")\n    data_id = \"{}|{}\".format(library, symbol)\n\n    if not global_state.store.lib or global_state.store.lib.name != library:\n        global_state.store.update_library(library)\n\n    startup(data=data_id)\n    startup_code = (\n        \"from arcticdb import Arctic\\n\"\n        \"from arcticdb.version_store._store import VersionedItem\\n\\n\"\n        \"conn = Arctic('{uri}')\\n\"\n        \"lib = conn.get_library('{library}')\\n\"\n        \"df = lib.read('{symbol}')\\n\"\n        \"if isinstance(data, VersionedItem):\\n\"\n        \"\\tdf = df.data\\n\"\n    ).format(uri=global_state.store.uri, library=library, symbol=symbol)\n    curr_settings = global_state.get_settings(data_id)\n    global_state.set_settings(\n        data_id, dict_merge(curr_settings, dict(startup_code=startup_code))\n    )\n    return dict(success=True, data_id=data_id)\n", "import { act, fireEvent, render, screen } from '@testing-library/react';\nimport axios from 'axios';\nimport * as React from 'react';\nimport { Provider } from 'react-redux';\nimport { Store } from 'redux';\n\nimport { DataViewer } from '../../dtale/DataViewer';\nimport DimensionsHelper from '../DimensionsHelper';\nimport reduxUtils from '../redux-test-utils';\nimport { buildInnerHTML, clickMainMenuButton, mockChartJS } from '../test-utils';\n\ndescribe('FilterPanel', () => {\n  let container: Element;\n  let store: Store;\n  const { open } = window;\n  const dimensions = new DimensionsHelper({\n    offsetWidth: 500,\n    offsetHeight: 500,\n    innerWidth: 1205,\n    innerHeight: 775,\n  });\n  const openFn = jest.fn();\n\n  beforeAll(() => {\n    dimensions.beforeAll();\n\n    delete (window as any).open;\n    window.open = openFn;\n\n    mockChartJS();\n  });\n\n  beforeEach(async () => {\n    (axios.get as any).mockImplementation((url: string) => Promise.resolve({ data: reduxUtils.urlFetcher(url) }));\n  });\n\n  afterEach(jest.resetAllMocks);\n\n  afterAll(() => {\n    dimensions.afterAll();\n    window.open = open;\n    jest.restoreAllMocks();\n  });\n\n  const toggleFilterMenu = async (): Promise<void> => {\n    await clickMainMenuButton('Custom Filter');\n  };\n\n  const buildResult = async (dataId = '1'): Promise<void> => {\n    store = reduxUtils.createDtaleStore();\n    buildInnerHTML({ settings: '', dataId }, store);\n    await act(() => {\n      const result = render(\n        <Provider store={store}>\n          <DataViewer />\n        </Provider>,\n        {\n          container: document.getElementById('content') ?? undefined,\n        },\n      );\n      container = result.container;\n    });\n    await toggleFilterMenu();\n  };\n\n  const clickFilterBtn = async (text: string): Promise<void> => {\n    await act(async () => {\n      const buttons = screen.getByTestId('filter-panel').getElementsByTagName('button');\n      fireEvent.click([...buttons].find((b) => b.textContent === text)!);\n    });\n  };\n\n  it('DataViewer: filtering', async () => {\n    await buildResult();\n    expect(screen.getByTestId('filter-panel')).toBeDefined();\n    await clickFilterBtn('Close');\n    expect(screen.queryByTestId('filter-panel')).toBeNull();\n    await toggleFilterMenu();\n    await clickFilterBtn('Clear');\n    expect(screen.queryByTestId('filter-panel')).toBeNull();\n    await toggleFilterMenu();\n    await clickFilterBtn('numexpr');\n    expect(store.getState().queryEngine).toBe('numexpr');\n    await act(async () => {\n      await fireEvent.click(screen.getByText('Highlight Filtered Rows').parentElement?.getElementsByTagName('i')[0]!);\n    });\n    expect(store.getState().settings.highlightFilter).toBe(true);\n    await act(async () => {\n      const textarea = screen.getByTestId('filter-panel').getElementsByTagName('textarea')[0];\n      fireEvent.change(textarea, { target: { value: 'test' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(container.getElementsByClassName('data-viewer-info')[0].textContent).toBe('Filter:test');\n    await act(async () => {\n      const cancelIcons = container.getElementsByClassName('data-viewer-info')[0].getElementsByClassName('ico-cancel');\n      fireEvent.click(cancelIcons[cancelIcons.length - 1]);\n    });\n    expect(\n      container.getElementsByClassName('data-viewer-info')[0].querySelectorAll('div.data-viewer-info.is-expanded')\n        .length,\n    ).toBe(0);\n  });\n\n  it('DataViewer: filtering with errors & documentation', async () => {\n    await buildResult();\n    await act(async () => {\n      const textarea = screen.getByTestId('filter-panel').getElementsByTagName('textarea')[0];\n      fireEvent.change(textarea, { target: { value: 'error' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(container.getElementsByClassName('dtale-alert')[0].textContent).toBe('No data found');\n    await act(async () => {\n      const filterPanel = screen.getByTestId('filter-panel');\n      const error = filterPanel.getElementsByClassName('dtale-alert')[0];\n      fireEvent.click(error.getElementsByClassName('ico-cancel')[0]);\n    });\n    expect(screen.getByTestId('filter-panel').getElementsByClassName('dtale-alert').length).toBe(0);\n    await clickFilterBtn('Help');\n    const pandasURL = 'https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-query';\n    expect(openFn.mock.calls[openFn.mock.calls.length - 1][0]).toBe(pandasURL);\n  });\n\n  it('DataViewer: filtering, context variables error', async () => {\n    await buildResult('error');\n    expect(screen.getByTestId('filter-panel').getElementsByClassName('dtale-alert')[0].textContent).toBe(\n      'Error loading context variables',\n    );\n  });\n\n  it('DataViewer: column filters', async () => {\n    await buildResult();\n    expect(screen.getByTestId('structured-filters').textContent).toBe('Active Column Filters:foo == 1 and');\n    await act(async () => {\n      fireEvent.click(screen.getByTestId('structured-filters').getElementsByClassName('ico-cancel')[0]);\n    });\n    expect(screen.queryByTestId('structured-filters')).toBeNull();\n  });\n});\n", "import * as reduxUtils from '../../dtale/reduxGridUtils';\nimport { DataViewerUpdateType } from '../../redux/state/AppState';\nimport { mockColumnDef } from '../mocks/MockColumnDef';\n\ndescribe('reduxGridUtils', () => {\n  const propagateState = jest.fn();\n\n  afterEach(jest.resetAllMocks);\n\n  afterAll(jest.restoreAllMocks);\n\n  it('handles drop-columns', () => {\n    const clearDataViewerUpdate = jest.fn();\n    const columns = [mockColumnDef({ name: 'foo' }), mockColumnDef({ name: 'bar' })];\n    const settings = {\n      allow_cell_edits: true,\n      hide_shutdown: false,\n      precision: 2,\n      verticalHeaders: false,\n      predefinedFilters: {},\n      hide_header_editor: false,\n      lock_header_menu: false,\n      hide_header_menu: false,\n      hide_main_menu: false,\n      hide_column_menus: false,\n    };\n    reduxUtils.handleReduxState(\n      columns,\n      {},\n      1,\n      { type: DataViewerUpdateType.DROP_COLUMNS, columns: ['foo'] },\n      clearDataViewerUpdate,\n      propagateState,\n      settings,\n    );\n    expect(propagateState).toHaveBeenCalledWith({ columns: [columns[1]], triggerResize: true }, clearDataViewerUpdate);\n  });\n});\n", "import { act, fireEvent, render, screen } from '@testing-library/react';\nimport * as React from 'react';\nimport { Provider } from 'react-redux';\nimport { Store } from 'redux';\n\nimport * as serverState from '../../../dtale/serverStateManagement';\nimport FilterPopup from '../../../popups/filter/FilterPopup';\nimport { ActionType } from '../../../redux/actions/AppActions';\nimport * as chartActions from '../../../redux/actions/charts';\nimport { InstanceSettings, QueryEngine } from '../../../redux/state/AppState';\nimport * as CustomFilterRepository from '../../../repository/CustomFilterRepository';\nimport * as GenericRepository from '../../../repository/GenericRepository';\nimport reduxUtils from '../../redux-test-utils';\nimport { buildInnerHTML } from '../../test-utils';\n\ndescribe('FilterPopup', () => {\n  let wrapper: Element;\n  let store: Store;\n  let loadInfoSpy: jest.SpyInstance<Promise<CustomFilterRepository.LoadInfoResponse | undefined>, [string]>;\n  let saveFilterSpy: jest.SpyInstance<Promise<GenericRepository.BaseResponse | undefined>, [string, string]>;\n  let updateSettingsSpy: jest.SpyInstance<serverState.BaseReturn, [Partial<InstanceSettings>, string]>;\n  let updateQueryEngineSpy: jest.SpyInstance<serverState.BaseReturn, [QueryEngine]>;\n\n  beforeEach(async () => {\n    loadInfoSpy = jest.spyOn(CustomFilterRepository, 'loadInfo');\n    loadInfoSpy.mockResolvedValue({\n      query: 'foo == foo',\n      columnFilters: { 0: { query: 'bar == bar', type: 'col' } },\n      outlierFilters: { 0: { query: 'baz == baz' } },\n      predefinedFilters: {},\n      contextVars: [],\n      invertFilter: false,\n      highlightFilter: false,\n      success: true,\n    });\n    saveFilterSpy = jest.spyOn(CustomFilterRepository, 'save');\n    saveFilterSpy.mockResolvedValue({ success: true });\n    updateSettingsSpy = jest.spyOn(serverState, 'updateSettings');\n    updateSettingsSpy.mockResolvedValue(Promise.resolve({ success: true }));\n    updateQueryEngineSpy = jest.spyOn(serverState, 'updateQueryEngine');\n    updateQueryEngineSpy.mockResolvedValue(Promise.resolve({ success: true }));\n    store = reduxUtils.createDtaleStore();\n    buildInnerHTML({ settings: '', dataId: '1', queryEngine: 'python' }, store);\n    store.dispatch({ type: ActionType.OPEN_CHART, chartData: { visible: true } });\n    wrapper = await act(\n      async () =>\n        await render(\n          <Provider store={store}>\n            <FilterPopup />\n          </Provider>,\n          {\n            container: document.getElementById('content') ?? undefined,\n          },\n        ).container,\n    );\n  });\n\n  afterEach(jest.resetAllMocks);\n  afterAll(jest.restoreAllMocks);\n\n  const clickFilterBtn = async (text: string): Promise<void> => {\n    await act(async () => {\n      await fireEvent.click(screen.getByText(text));\n    });\n  };\n\n  it('renders successfully', () => {\n    expect(wrapper.innerHTML).not.toBe('');\n  });\n\n  it('drops filter', async () => {\n    await act(async () => {\n      await fireEvent.click(screen.queryAllByTestId('structured-filters')[0].querySelector('.ico-cancel')!);\n    });\n    expect(updateSettingsSpy).toHaveBeenCalledTimes(1);\n    expect(screen.queryAllByTestId('structured-filters')).toHaveLength(1);\n  });\n\n  it('saves filter', async () => {\n    const onCloseSpy = jest.spyOn(chartActions, 'closeChart');\n    await act(async () => {\n      await fireEvent.change(wrapper.getElementsByTagName('textarea')[0], { target: { value: 'foo == foo' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(saveFilterSpy).toHaveBeenCalledWith('1', 'foo == foo');\n    expect(store.getState().settings.query).toBe('foo == foo');\n    expect(onCloseSpy).toHaveBeenCalledTimes(1);\n    onCloseSpy.mockRestore();\n  });\n\n  it('save failure', async () => {\n    saveFilterSpy.mockResolvedValue({ error: 'error', success: false });\n    await act(async () => {\n      await fireEvent.change(wrapper.getElementsByTagName('textarea')[0], { target: { value: 'foo == foo' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(screen.getByRole('alert').textContent).toBe('error');\n    await act(async () => {\n      await fireEvent.click(screen.getByRole('alert').querySelector('.ico-cancel')!);\n    });\n    expect(screen.queryAllByRole('alert')).toHaveLength(0);\n  });\n\n  it('clears filter', async () => {\n    const onCloseSpy = jest.spyOn(chartActions, 'closeChart');\n    await clickFilterBtn('Clear');\n    expect(updateSettingsSpy).toHaveBeenCalledWith({ query: '' }, '1');\n    expect(onCloseSpy).toHaveBeenCalledTimes(1);\n    onCloseSpy.mockRestore();\n  });\n\n  it('updates query engine', async () => {\n    await clickFilterBtn('numexpr');\n    expect(updateQueryEngineSpy).toHaveBeenCalledTimes(1);\n    expect(updateQueryEngineSpy.mock.calls[0][0]).toBe('numexpr');\n  });\n\n  it('toggle highlight filter', async () => {\n    await act(async () => {\n      await fireEvent.click(screen.getByText('Highlight Filtered Rows').parentElement?.getElementsByTagName('i')[0]!);\n    });\n    expect(updateSettingsSpy).toHaveBeenLastCalledWith({ highlightFilter: true }, '1');\n    expect(store.getState().settings.highlightFilter).toBe(true);\n  });\n\n  describe('new window', () => {\n    const { location, close, opener } = window;\n\n    beforeAll(() => {\n      delete (window as any).location;\n      delete (window as any).close;\n      delete window.opener;\n      (window as any).location = { pathname: '/dtale/popup/filter' };\n      window.close = jest.fn();\n      window.opener = {\n        location: {\n          reload: jest.fn(),\n        },\n      };\n    });\n\n    afterEach(jest.resetAllMocks);\n\n    afterAll(() => {\n      window.location = location;\n      window.close = close;\n      window.opener = opener;\n    });\n\n    it('drops filter', async () => {\n      await act(async () => {\n        await fireEvent.click(screen.queryAllByTestId('structured-filters')[0].querySelector('.ico-cancel')!);\n      });\n      expect(updateSettingsSpy).toHaveBeenCalledTimes(1);\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n    });\n\n    it('saves filter', async () => {\n      await act(async () => {\n        await fireEvent.change(wrapper.getElementsByTagName('textarea')[0], { target: { value: 'foo == foo' } });\n      });\n      await clickFilterBtn('Apply');\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n      expect(window.close).toHaveBeenCalledTimes(1);\n    });\n\n    it('clears filter', async () => {\n      await clickFilterBtn('Clear');\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n      expect(window.close).toHaveBeenCalledTimes(1);\n    });\n\n    it('toggle highlight filter', async () => {\n      await act(async () => {\n        await fireEvent.click(screen.getByText('Highlight Filtered Rows').parentElement?.getElementsByTagName('i')[0]!);\n      });\n      expect(updateSettingsSpy).toHaveBeenLastCalledWith({ highlightFilter: true }, '1');\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n    });\n  });\n});\n", "import { render } from '@testing-library/react';\nimport * as React from 'react';\nimport { Provider } from 'react-redux';\n\nimport { buildRangeState } from '../../dtale/rangeSelectUtils';\nimport { PopupType } from '../../redux/state/AppState';\nimport reduxUtils from '../redux-test-utils';\n\ndescribe('reducer tests', () => {\n  it('dtale: missing hidden input', () => {\n    const store = reduxUtils.createDtaleStore();\n    const body = document.getElementsByTagName('body')[0];\n    body.innerHTML = `<div id=\"content\" style=\"height: 1000px;width: 1000px;\"></div>`;\n\n    render(\n      <Provider store={store}>\n        <div />\n      </Provider>,\n      {\n        container: document.getElementById('content') ?? undefined,\n      },\n    );\n    const state = {\n      chartData: { visible: false, type: PopupType.HIDDEN },\n      hideShutdown: false,\n      hideHeaderEditor: false,\n      lockHeaderMenu: false,\n      hideHeaderMenu: false,\n      hideMainMenu: false,\n      hideColumnMenus: false,\n      hideDropRows: false,\n      iframe: false,\n      columnMenuOpen: false,\n      selectedCol: null,\n      selectedColRef: null,\n      dataId: '',\n      editedCell: null,\n      xarray: false,\n      xarrayDim: {},\n      allowCellEdits: true,\n      theme: 'light',\n      language: 'en',\n      filteredRanges: {},\n      settings: {\n        allow_cell_edits: true,\n        hide_shutdown: false,\n        precision: 2,\n        predefinedFilters: {},\n        verticalHeaders: false,\n        hide_header_editor: false,\n        lock_header_menu: false,\n        hide_header_menu: false,\n        hide_main_menu: false,\n        hide_column_menus: false,\n      },\n      pythonVersion: null,\n      isPreview: false,\n      menuPinned: false,\n      menuTooltip: {\n        visible: false,\n      },\n      ribbonDropdown: {\n        visible: false,\n      },\n      ribbonMenuOpen: false,\n      sidePanel: {\n        visible: false,\n      },\n      dataViewerUpdate: null,\n      auth: false,\n      username: null,\n      predefinedFilters: [],\n      maxColumnWidth: null,\n      maxRowHeight: null,\n      dragResize: null,\n      editedTextAreaHeight: 0,\n      mainTitle: null,\n      mainTitleFont: null,\n      showAllHeatmapColumns: false,\n      isVSCode: false,\n      isArcticDB: 0.0,\n      arcticConn: '',\n      columnCount: 0,\n      queryEngine: 'python',\n      openCustomFilterOnStartup: false,\n      openPredefinedFiltersOnStartup: false,\n      menuOpen: false,\n      formattingOpen: null,\n      ...buildRangeState(),\n    };\n    expect(state).toEqual(store.getState());\n  });\n});\n", "/* eslint max-classes-per-file: \"off\" */\nimport { act, fireEvent, Matcher, screen } from '@testing-library/react';\nimport {\n  ChartConfiguration,\n  ChartData,\n  ChartEvent,\n  ChartMeta,\n  ChartOptions,\n  ChartType,\n  DefaultDataPoint,\n  InteractionItem,\n  Scale,\n} from 'chart.js';\nimport { TFunction } from 'i18next';\nimport * as React from 'react';\nimport selectEvent from 'react-select-event';\nimport { Store } from 'redux';\n\nimport * as chartUtils from '../chartUtils';\n\nexport const parseUrlParams = (url?: string): Record<string, string> =>\n  JSON.parse(\n    '{\"' +\n      decodeURI((url ?? '').split('?')[1])\n        .replace(/\"/g, '\\\\\"')\n        .replace(/&/g, '\",\"')\n        .replace(/=/g, '\":\"') +\n      '\"}',\n  );\n\nexport const replaceNBSP = (text: string): string => text.replace(/\\s/g, ' ');\n\nexport const logException = (e: Error): void => {\n  console.error(`${e.name}: ${e.message} (${(e as any).fileName}:${(e as any).lineNumber})`); // eslint-disable-line no-console\n  console.error(e.stack); // eslint-disable-line no-console\n};\n\nconst BASE_SETTINGS = '{&quot;sortInfo&quot;:[[&quot;col1&quot;,&quot;ASC&quot;]],&quot;precision&quot;:2}';\nconst HIDE_SHUTDOWN = 'False';\nconst PROCESSES = 1;\nconst IFRAME = 'False';\nconst DATA_ID = 1;\n\nexport const PREDEFINED_FILTERS = [\n  '[{',\n  '&quot;name&quot;:&quot;custom_foo&quot;,',\n  '&quot;column&quot;:&quot;foo&quot;,',\n  '&quot;description&quot;:&quot;foo&quot;,',\n  '&quot;inputType&quot;: &quot;input&quot;',\n  '}]',\n].join('');\n\nconst buildHidden = (id: string, value: string | number): string =>\n  `<input type=\"hidden\" id=\"${id}\" value=\"${value}\" />`;\n\nconst BASE_HTML = `\n<div id=\"content\" style=\"height: 1000px;width: 1000px;\" />\n<div id=\"popup-content\" />\n<span id=\"code-title\" />\n`;\n\nexport const buildInnerHTML = (props: Record<string, string | undefined> = {}, store?: Store): void => {\n  const actions = require('../redux/actions/dtale');\n  const pjson = require('../../package.json');\n  const body = document.getElementsByTagName('body')[0];\n  body.innerHTML = [\n    buildHidden('settings', props.settings ?? BASE_SETTINGS),\n    buildHidden('version', pjson.version),\n    buildHidden('python_version', props.pythonVersion ?? '3.8.0'),\n    buildHidden('hide_shutdown', props.hideShutdown ?? HIDE_SHUTDOWN),\n    buildHidden('hide_drop_rows', props.hideDropRows ?? 'False'),\n    buildHidden('processes', props.processes ?? PROCESSES),\n    buildHidden('iframe', props.iframe ?? IFRAME),\n    buildHidden('data_id', props.dataId ?? DATA_ID),\n    buildHidden('xarray', props.xarray ?? 'False'),\n    buildHidden('xarray_dim', props.xarrayDim ?? '{}'),\n    buildHidden('allow_cell_edits', props.allowCellEdits ?? 'true'),\n    buildHidden('is_vscode', props.isVSCode ?? 'False'),\n    buildHidden('arctic_conn', props.arcticConn ?? ''),\n    buildHidden('is_arcticdb', props.isArcticDB ?? '0'),\n    buildHidden('column_count', props.columnCount ?? '0'),\n    buildHidden('theme', props.theme ?? 'light'),\n    buildHidden('language', props.language ?? 'en'),\n    buildHidden('pin_menu', props.pinMenu ?? 'False'),\n    buildHidden('filtered_ranges', props.filteredRanges ?? JSON.stringify({})),\n    buildHidden('auth', props.auth ?? 'False'),\n    buildHidden('username', props.username ?? ''),\n    buildHidden('predefined_filters', props.predefinedFilters ?? '[]'),\n    buildHidden('max_column_width', props.maxColumnWidth ?? 'None'),\n    buildHidden('main_title', props.mainTitle ?? ''),\n    buildHidden('main_title_font', props.mainTitleFont ?? ''),\n    buildHidden('query_engine', props.queryEngine ?? 'python'),\n    buildHidden('hide_header_editor', props.hideHeaderEditor ?? HIDE_SHUTDOWN),\n    buildHidden('lock_header_menu', props.lockHeaderMenu ?? HIDE_SHUTDOWN),\n    buildHidden('hide_header_menu', props.hideHeaderMenu ?? HIDE_SHUTDOWN),\n    buildHidden('hide_main_menu', props.hideMainMenu ?? HIDE_SHUTDOWN),\n    buildHidden('hide_column_menus', props.hideColumnMenus ?? HIDE_SHUTDOWN),\n    BASE_HTML,\n  ].join('');\n  store?.dispatch(actions.init());\n};\n\nexport const findMainMenuButton = (name: string, btnTag = 'button'): Element | undefined => {\n  const menu = screen.getByTestId('data-viewer-menu');\n  const buttons = menu.querySelectorAll(`ul li ${btnTag}`);\n  return [...buttons].find((b) => b?.textContent?.includes(name));\n};\n\nexport const clickMainMenuButton = async (name: string, btnTag = 'button'): Promise<void> => {\n  await act(async () => {\n    fireEvent.click(findMainMenuButton(name, btnTag)!);\n  });\n};\n\nexport const tick = (timeout = 0): Promise<void> => {\n  return new Promise((resolve) => {\n    setTimeout(resolve, timeout);\n  });\n};\n\n/** Mocked version of chart.js Chart object */\nexport class MockChart {\n  /** @override */\n  public static register = (): void => undefined;\n\n  public ctx: HTMLCanvasElement;\n  public cfg: ChartConfiguration;\n  public config: { _config: ChartConfiguration };\n  public data: ChartData;\n  public destroyed: boolean;\n  public options: ChartOptions;\n\n  /** @override */\n  constructor(ctx: HTMLCanvasElement, cfg: ChartConfiguration) {\n    this.ctx = ctx;\n    this.cfg = cfg;\n    this.config = { _config: cfg };\n    this.data = cfg.data;\n    this.destroyed = false;\n    this.options = cfg.options ?? { scales: { x: {}, y: {} } };\n  }\n\n  /** @override */\n  public destroy(): void {\n    this.destroyed = true;\n  }\n\n  /** @override */\n  public getElementsAtEventForMode(_evt: ChartEvent): InteractionItem[] {\n    return [\n      {\n        datasetIndex: 0,\n        index: 0,\n        _chart: { config: { _config: this.cfg }, data: this.cfg.data },\n      } as any as InteractionItem,\n    ];\n  }\n\n  /** @override */\n  public getDatasetMeta(idx: number): ChartMeta {\n    return { controller: { _config: { selectedPoint: 0 } } } as any as ChartMeta;\n  }\n\n  /** @override */\n  public update(): void {} // eslint-disable-line @typescript-eslint/no-empty-function\n}\n\nexport const mockChartJS = (): void => {\n  jest.mock('chart.js', () => ({ Chart: MockChart }));\n};\n\nexport const mockD3Cloud = (): void => {\n  jest.mock('d3-cloud', () => () => {\n    const cloudCfg: any = {};\n    const propUpdate =\n      (prop: string): ((val: string) => any) =>\n      (val: string): any => {\n        cloudCfg[prop] = val;\n        return cloudCfg;\n      };\n    cloudCfg.size = propUpdate('size');\n    cloudCfg.padding = propUpdate('padding');\n    cloudCfg.words = propUpdate('words');\n    cloudCfg.rotate = propUpdate('rotate');\n    cloudCfg.spiral = propUpdate('spiral');\n    cloudCfg.random = propUpdate('random');\n    cloudCfg.text = propUpdate('text');\n    cloudCfg.font = propUpdate('font');\n    cloudCfg.fontStyle = propUpdate('fontStyle');\n    cloudCfg.fontWeight = propUpdate('fontWeight');\n    cloudCfg.fontSize = () => ({\n      on: () => ({ start: () => undefined }),\n    });\n    return cloudCfg;\n  });\n};\n\nexport const mockWordcloud = (): void => {\n  jest.mock('react-wordcloud', () => {\n    const { createMockComponent } = require('./mocks/createMockComponent');\n    return {\n      __esModule: true,\n      default: createMockComponent('MockWordcloud', () => <div data-testid=\"mock-wordcloud\" />),\n    };\n  });\n};\n\nexport const mockT = ((key: string): string => {\n  const keySegs = key.split(':');\n  if (keySegs.length > 2) {\n    keySegs.shift();\n    return keySegs.join(':');\n  } else if (keySegs.length === 2) {\n    return keySegs[keySegs.length - 1];\n  }\n  return key;\n}) as any as TFunction;\n\n/** Type definition for chartUtils.createChart spies */\nexport type CreateChartSpy = jest.SpyInstance<\n  chartUtils.ChartObj,\n  [ctx: HTMLCanvasElement, cfg: ChartConfiguration<ChartType, DefaultDataPoint<ChartType>, unknown>]\n>;\n\nexport const getLastChart = (\n  spy: CreateChartSpy,\n  chartType?: string,\n): ChartConfiguration<ChartType, DefaultDataPoint<ChartType>, unknown> => {\n  if (chartType) {\n    const typeCalls = spy.mock.calls.filter((call) => call[1].type === chartType);\n    return typeCalls[typeCalls.length - 1][1];\n  }\n  return spy.mock.calls[spy.mock.calls.length - 1][1];\n};\n\nexport const buildChartContext = (): Partial<CanvasRenderingContext2D> => ({\n  createLinearGradient: (_px1: number, _px2: number, _px3: number, _px4: number): CanvasGradient => ({\n    addColorStop: (_px5: number, _color: string): void => undefined,\n  }),\n  save: jest.fn(),\n  beginPath: jest.fn(),\n  moveTo: jest.fn(),\n  lineTo: jest.fn(),\n  lineWidth: 0,\n  strokeStyle: undefined,\n  stroke: jest.fn(),\n  restore: jest.fn(),\n});\n\nexport const SCALE: Partial<Scale> = { getPixelForValue: (px: number): number => px };\n\n/** FakeMouseEvent properties */\ninterface MouseEventWithOffsets extends MouseEventInit {\n  pageX?: number;\n  pageY?: number;\n  offsetX?: number;\n  offsetY?: number;\n  x?: number;\n  y?: number;\n}\n\n/** Fake mouse event class */\nexport class FakeMouseEvent extends MouseEvent {\n  /** @override */\n  constructor(type: string, values: MouseEventWithOffsets) {\n    const { pageX, pageY, offsetX, offsetY, x, y, ...mouseValues } = values;\n    super(type, mouseValues);\n\n    Object.assign(this, {\n      offsetX: offsetX || 0,\n      offsetY: offsetY || 0,\n      pageX: pageX || 0,\n      pageY: pageY || 0,\n      x: x || 0,\n      y: y || 0,\n    });\n  }\n}\n\nexport const selectOption = async (selectElement: HTMLElement, option: Matcher | Matcher[]): Promise<void> => {\n  await act(async () => {\n    await selectEvent.openMenu(selectElement);\n  });\n  await act(async () => {\n    await selectEvent.select(selectElement, option, { container: document.body });\n  });\n};\n", "import * as React from 'react';\nimport * as ReactDOMClient from 'react-dom/client';\nimport { Provider } from 'react-redux';\n\nimport '../../i18n';\nimport * as actions from '../../redux/actions/dtale';\nimport appReducers from '../../redux/reducers/app';\nimport { AppState } from '../../redux/state/AppState';\nimport { createAppStore } from '../../redux/store';\nimport { DataResponseContent } from '../../repository/DataRepository';\n\nimport { ServerlessDataViewer } from './ServerlessDataViewer';\n\nrequire('../../publicPath');\n\nconst store = createAppStore<AppState>(appReducers);\nstore.dispatch(actions.init());\nactions.loadBackgroundMode(store);\nactions.loadHideShutdown(store);\nactions.loadAllowCellEdits(store);\nactions.loadHideHeaderEditor(store);\nactions.loadLockHeaderMenu(store);\nactions.loadHideHeaderMenu(store);\nactions.loadHideMainMenu(store);\nactions.loadHideColumnMenus(store);\nconst root = ReactDOMClient.createRoot(document.getElementById('content')!);\nroot.render(\n  <Provider store={store}>\n    <ServerlessDataViewer response={(global as any).RESPONSE! as DataResponseContent} />\n  </Provider>,\n);\n", "import * as React from 'react';\nimport * as ReactDOMClient from 'react-dom/client';\nimport { Provider } from 'react-redux';\nimport { Store } from 'redux';\n\nimport { DataViewer } from './dtale/DataViewer';\nimport './i18n';\nimport ColumnAnalysis from './popups/analysis/ColumnAnalysis';\nimport LibrarySymbolSelector from './popups/arcticdb/LibrarySymbolSelector';\nimport { CodeExport } from './popups/CodeExport';\nimport CodePopup from './popups/CodePopup';\nimport { Correlations } from './popups/correlations/Correlations';\nimport CreateColumn from './popups/create/CreateColumn';\nimport { CreateColumnType, PrepopulateCreateColumn, SaveAs } from './popups/create/CreateColumnState';\nimport Describe from './popups/describe/Describe';\nimport Duplicates from './popups/duplicates/Duplicates';\nimport FilterPopup from './popups/filter/FilterPopup';\nimport Instances from './popups/instances/Instances';\nimport MergeDatasets from './popups/merge/MergeDatasets';\nimport PredictivePowerScore from './popups/pps/PredictivePowerScore';\nimport CreateReplacement from './popups/replacement/CreateReplacement';\nimport Reshape from './popups/reshape/Reshape';\nimport Upload from './popups/upload/Upload';\nimport Variance from './popups/variance/Variance';\nimport * as actions from './redux/actions/dtale';\nimport * as mergeActions from './redux/actions/merge';\nimport appReducers from './redux/reducers/app';\nimport mergeReducers from './redux/reducers/merge';\nimport { getHiddenValue, toJson } from './redux/reducers/utils';\nimport { AppState, InstanceSettings } from './redux/state/AppState';\nimport { MergeState } from './redux/state/MergeState';\nimport { createAppStore } from './redux/store';\n\nrequire('./publicPath');\n\nlet pathname = window.location.pathname;\nif ((window as any).resourceBaseUrl) {\n  pathname = pathname.replace((window as any).resourceBaseUrl, '');\n}\nlet storeBuilder: () => Store = () => {\n  const store = createAppStore<AppState>(appReducers);\n  store.dispatch(actions.init());\n  actions.loadBackgroundMode(store);\n  actions.loadHideShutdown(store);\n  actions.loadAllowCellEdits(store);\n  actions.loadHideHeaderEditor(store);\n  actions.loadLockHeaderMenu(store);\n  actions.loadHideHeaderMenu(store);\n  actions.loadHideMainMenu(store);\n  actions.loadHideColumnMenus(store);\n  return store;\n};\nif (pathname.indexOf('/dtale/popup') === 0) {\n  require('./dtale/DataViewer.css');\n\n  let rootNode = null;\n  const settings = toJson<InstanceSettings>(getHiddenValue('settings'));\n  const dataId = getHiddenValue('data_id');\n  const pathSegs = pathname.split('/');\n  const popupType = pathSegs[pathSegs.length - 1] === 'code-popup' ? 'code-popup' : pathSegs[3];\n  const chartData: Record<string, any> = {\n    ...actions.getParams(),\n    ...(settings.query ? { query: settings.query } : {}),\n  };\n  switch (popupType) {\n    case 'filter':\n      rootNode = <FilterPopup />;\n      break;\n    case 'correlations':\n      rootNode = <Correlations />;\n      break;\n    case 'merge':\n      storeBuilder = () => {\n        const store = createAppStore<MergeState>(mergeReducers);\n        mergeActions.init(store.dispatch);\n        return store;\n      };\n      rootNode = <MergeDatasets />;\n      break;\n    case 'pps':\n      rootNode = <PredictivePowerScore />;\n      break;\n    case 'describe':\n      rootNode = <Describe />;\n      break;\n    case 'variance':\n      rootNode = <Variance />;\n      break;\n    case 'build':\n      rootNode = <CreateColumn />;\n      break;\n    case 'duplicates':\n      rootNode = <Duplicates />;\n      break;\n    case 'type-conversion': {\n      const prePopulated: PrepopulateCreateColumn = {\n        type: CreateColumnType.TYPE_CONVERSION,\n        saveAs: SaveAs.INPLACE,\n        cfg: { col: chartData.selectedCol, applyAllType: false },\n      };\n      rootNode = <CreateColumn prePopulated={prePopulated} />;\n      break;\n    }\n    case 'cleaners': {\n      const prePopulated: PrepopulateCreateColumn = {\n        type: CreateColumnType.CLEANING,\n        cfg: { col: chartData.selectedCol, cleaners: [] },\n      };\n      rootNode = <CreateColumn prePopulated={prePopulated} />;\n      break;\n    }\n    case 'replacement':\n      rootNode = <CreateReplacement />;\n      break;\n    case 'reshape':\n      rootNode = <Reshape />;\n      break;\n    case 'column-analysis':\n      rootNode = <ColumnAnalysis {...{ dataId, chartData }} height={250} />;\n      break;\n    case 'instances':\n      rootNode = <Instances />;\n      break;\n    case 'code-export':\n      rootNode = <CodeExport />;\n      break;\n    case 'arcticdb':\n      rootNode = <LibrarySymbolSelector />;\n      break;\n    case 'upload':\n    default:\n      rootNode = <Upload />;\n      break;\n  }\n  const store = storeBuilder();\n  store.getState().chartData = chartData;\n  const root = ReactDOMClient.createRoot(document.getElementById('popup-content')!);\n  root.render(<Provider store={store}>{rootNode}</Provider>);\n} else if (pathname.startsWith('/dtale/code-popup')) {\n  require('./dtale/DataViewer.css');\n  let title: string;\n  let body: JSX.Element;\n  if (window.opener) {\n    title = `${window.opener.code_popup.title} Code Export`;\n    body = <CodePopup code={window.opener.code_popup.code} />;\n  } else {\n    title = 'Code Missing';\n    body = <h1>No parent window containing code detected!</h1>;\n  }\n  const titleElement: HTMLElement | null = document.getElementById('code-title');\n  if (titleElement) {\n    titleElement.innerHTML = title;\n  }\n  const root = ReactDOMClient.createRoot(document.getElementById('popup-content')!);\n  root.render(body);\n} else {\n  const store = storeBuilder();\n  if (store.getState().openPredefinedFiltersOnStartup) {\n    store.dispatch(actions.openPredefinedFilters());\n  } else if (store.getState().openCustomFilterOnStartup) {\n    store.dispatch(actions.openCustomFilter());\n  }\n  const root = ReactDOMClient.createRoot(document.getElementById('content')!);\n  root.render(\n    <Provider store={store}>\n      <DataViewer />\n    </Provider>,\n  );\n}\n", "import { createSelector } from '@reduxjs/toolkit';\nimport * as React from 'react';\nimport { WithTranslation, withTranslation } from 'react-i18next';\nimport { useDispatch, useSelector } from 'react-redux';\nimport { AnyAction } from 'redux';\n\nimport ButtonToggle from '../../ButtonToggle';\nimport { ColumnFilter, OutlierFilter } from '../../dtale/DataViewerState';\nimport * as serverState from '../../dtale/serverStateManagement';\nimport SidePanelButtons from '../../dtale/side/SidePanelButtons';\nimport { ActionType, HideSidePanelAction, SetQueryEngineAction } from '../../redux/actions/AppActions';\nimport * as dtaleActions from '../../redux/actions/dtale';\nimport * as settingsActions from '../../redux/actions/settings';\nimport { selectDataId, selectQueryEngine, selectSettings } from '../../redux/selectors';\nimport { InstanceSettings, QueryEngine } from '../../redux/state/AppState';\nimport { RemovableError } from '../../RemovableError';\nimport * as CustomFilterRepository from '../../repository/CustomFilterRepository';\nimport { Checkbox } from '../create/LabeledCheckbox';\n\nimport ContextVariables from './ContextVariables';\nimport PandasQueryHelp from './PandasQueryHelp';\nimport QueryExamples from './QueryExamples';\nimport StructuredFilters from './StructuredFilters';\n\nconst selectResult = createSelector(\n  [selectDataId, selectQueryEngine, selectSettings],\n  (dataId, queryEngine, settings) => ({ dataId, queryEngine, settings }),\n);\n\nconst FilterPanel: React.FC<WithTranslation> = ({ t }) => {\n  const { dataId, queryEngine, settings } = useSelector(selectResult);\n  const dispatch = useDispatch();\n  const hideSidePanel = (): HideSidePanelAction => dispatch({ type: ActionType.HIDE_SIDE_PANEL });\n  const updateSettings = (updatedSettings: Partial<InstanceSettings>, callback?: () => void): AnyAction =>\n    dispatch(settingsActions.updateSettings(updatedSettings, callback) as any as AnyAction);\n  const setEngine = (engine: QueryEngine): SetQueryEngineAction => dispatch(dtaleActions.setQueryEngine(engine));\n\n  const [query, setQuery] = React.useState('');\n  const [highlightFilter, setHighlightFilter] = React.useState(settings.highlightFilter ?? false);\n  const [contextVars, setContextVars] = React.useState<Array<{ name: string; value: string }>>([]);\n  const [columnFilters, setColumnFilters] = React.useState<Record<string, ColumnFilter>>({});\n  const [outlierFilters, setOutlierFilters] = React.useState<Record<string, OutlierFilter>>({});\n  const [error, setError] = React.useState<JSX.Element>();\n\n  React.useEffect(() => {\n    CustomFilterRepository.loadInfo(dataId).then((response) => {\n      if (response?.error) {\n        setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n        return;\n      }\n      if (response) {\n        setQuery(response.query);\n        setContextVars(response.contextVars);\n        setColumnFilters(response.columnFilters);\n        setOutlierFilters(response.outlierFilters);\n      }\n    });\n  }, []);\n\n  const save = async (): Promise<void> => {\n    const response = await CustomFilterRepository.save(dataId, query);\n    if (response?.error) {\n      setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n      return;\n    }\n    updateSettings({ query }, hideSidePanel);\n  };\n\n  const saveHighlightFilter = async (updatedHighlightFilter: boolean): Promise<void> => {\n    await serverState.updateSettings({ highlightFilter: updatedHighlightFilter }, dataId);\n    updateSettings({ highlightFilter: updatedHighlightFilter }, () => setHighlightFilter(updatedHighlightFilter));\n  };\n\n  const dropFilter = async <T,>(\n    prop: string,\n    filters: Record<string, T>,\n    setter: (value: Record<string, T>) => void,\n    col: string,\n  ): Promise<void> => {\n    const updatedFilters = Object.keys(filters).reduce(\n      (res, key) => (key !== col ? { ...res, [key]: filters[key] } : res),\n      {},\n    );\n    const updatedSettings = { [prop]: updatedFilters };\n    await serverState.updateSettings(updatedSettings, dataId);\n    updateSettings(updatedSettings, () => setter(updatedFilters));\n  };\n\n  const clear = async (): Promise<void> => {\n    await serverState.updateSettings({ query: '' }, dataId);\n    updateSettings({ query: '' }, hideSidePanel);\n  };\n\n  const updateEngine = async (engine: QueryEngine): Promise<void> => {\n    await serverState.updateQueryEngine(engine);\n    setEngine(engine);\n  };\n\n  return (\n    <div data-testid=\"filter-panel\">\n      {error}\n      <div className=\"row\">\n        <div className=\"col-md-12\">\n          <div className=\"row m-0\">\n            <h1 className=\"mb-0\">{t('Filters', { ns: 'filter' })}</h1>\n            <div className=\"col\" />\n            <SidePanelButtons />\n          </div>\n          <div className=\"row m-0 pb-3\">\n            <div className=\"col p-0 font-weight-bold mt-auto\">{t('Custom Filter', { ns: 'filter' })}</div>\n            <PandasQueryHelp />\n            <button className=\"btn btn-primary col-auto pt-2 pb-2\" onClick={clear}>\n              <span>{t('Clear', { ns: 'filter' })}</span>\n            </button>\n            <button className=\"btn btn-primary col-auto pt-2 pb-2\" onClick={save}>\n              <span>{t('Apply', { ns: 'filter' })}</span>\n            </button>\n          </div>\n          <textarea\n            style={{ width: '100%', height: 150 }}\n            value={query || ''}\n            onChange={(event) => setQuery(event.target.value)}\n          />\n        </div>\n      </div>\n      <div className=\"row pt-3 pb-3\">\n        <span className=\"font-weight-bold col-auto pr-3\">{t('Highlight Filtered Rows', { ns: 'main' })}</span>\n        <Checkbox value={highlightFilter} setter={saveHighlightFilter} className=\"pt-1\" />\n      </div>\n      <div className=\"row pt-3 pb-3\">\n        <span className=\"font-weight-bold col-auto pr-0\">{t('Query Engine', { ns: 'filter' })}</span>\n        <ButtonToggle\n          className=\"ml-auto mr-3 font-weight-bold col\"\n          options={Object.values(QueryEngine).map((value) => ({ value }))}\n          update={updateEngine}\n          defaultValue={queryEngine}\n        />\n      </div>\n      {(!!Object.keys(columnFilters ?? {}).length || !!Object.keys(outlierFilters ?? {}).length) && (\n        <div className=\"row pb-5\">\n          <div className=\"col-md-6\">\n            <StructuredFilters\n              label={t('Column Filters', { ns: 'filter' })}\n              filters={columnFilters}\n              dropFilter={(col: string) => dropFilter('columnFilters', columnFilters, setColumnFilters, col)}\n            />\n          </div>\n          <div className=\"col-md-6\">\n            <StructuredFilters\n              label={t('Outlier Filters', { ns: 'filter' })}\n              filters={outlierFilters}\n              dropFilter={(col: string) => dropFilter('outlierFilters', outlierFilters, setOutlierFilters, col)}\n            />\n          </div>\n        </div>\n      )}\n      <div className=\"row\">\n        <div className=\"col-md-12\">\n          <QueryExamples />\n        </div>\n      </div>\n      {contextVars.length > 0 && <ContextVariables contextVars={contextVars} />}\n    </div>\n  );\n};\n\nexport default withTranslation(['filter', 'main'])(FilterPanel);\n", "import { createSelector } from '@reduxjs/toolkit';\nimport * as React from 'react';\nimport { WithTranslation, withTranslation } from 'react-i18next';\nimport { useDispatch, useSelector } from 'react-redux';\nimport { AnyAction } from 'redux';\n\nimport ButtonToggle from '../../ButtonToggle';\nimport { ColumnFilter, OutlierFilter } from '../../dtale/DataViewerState';\nimport * as serverState from '../../dtale/serverStateManagement';\nimport { CloseChartAction, SetQueryEngineAction } from '../../redux/actions/AppActions';\nimport { closeChart } from '../../redux/actions/charts';\nimport * as dtaleActions from '../../redux/actions/dtale';\nimport * as settingsActions from '../../redux/actions/settings';\nimport { selectDataId, selectQueryEngine, selectSettings } from '../../redux/selectors';\nimport { InstanceSettings, QueryEngine } from '../../redux/state/AppState';\nimport { RemovableError } from '../../RemovableError';\nimport * as CustomFilterRepository from '../../repository/CustomFilterRepository';\nimport { Checkbox } from '../create/LabeledCheckbox';\n\nimport ContextVariables from './ContextVariables';\nimport PandasQueryHelp from './PandasQueryHelp';\nimport QueryExamples from './QueryExamples';\nimport StructuredFilters from './StructuredFilters';\n\nconst selectResult = createSelector(\n  [selectDataId, selectQueryEngine, selectSettings],\n  (dataId, queryEngine, settings) => ({ dataId, queryEngine, settings }),\n);\n\nconst FilterPopup: React.FC<WithTranslation> = ({ t }) => {\n  const { dataId, queryEngine, settings } = useSelector(selectResult);\n  const dispatch = useDispatch();\n  const onClose = (): CloseChartAction => dispatch(closeChart());\n  const updateSettings = (updatedSettings: Partial<InstanceSettings>, callback?: () => void): AnyAction =>\n    dispatch(settingsActions.updateSettings(updatedSettings, callback) as any as AnyAction);\n  const setEngine = (engine: QueryEngine): SetQueryEngineAction => dispatch(dtaleActions.setQueryEngine(engine));\n\n  const [query, setQuery] = React.useState('');\n  const [highlightFilter, setHighlightFilter] = React.useState(settings.highlightFilter ?? false);\n  const [contextVars, setContextVars] = React.useState<Array<{ name: string; value: string }>>([]);\n  const [columnFilters, setColumnFilters] = React.useState<Record<string, ColumnFilter>>({});\n  const [outlierFilters, setOutlierFilters] = React.useState<Record<string, OutlierFilter>>({});\n  const [error, setError] = React.useState<JSX.Element>();\n\n  React.useEffect(() => {\n    CustomFilterRepository.loadInfo(dataId).then((response) => {\n      if (response?.error) {\n        setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n        return;\n      }\n      if (response) {\n        setQuery(response.query);\n        setHighlightFilter(response.highlightFilter);\n        setContextVars(response.contextVars);\n        setColumnFilters(response.columnFilters);\n        setOutlierFilters(response.outlierFilters);\n      }\n    });\n  }, []);\n\n  const save = async (): Promise<void> => {\n    const response = await CustomFilterRepository.save(dataId, query);\n    if (response?.error) {\n      setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n      return;\n    }\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n      window.close();\n    } else {\n      updateSettings({ query }, onClose);\n    }\n  };\n\n  const saveHighlightFilter = async (updatedHighlightFilter: boolean): Promise<void> => {\n    await serverState.updateSettings({ highlightFilter: updatedHighlightFilter }, dataId);\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n    } else {\n      updateSettings({ highlightFilter: updatedHighlightFilter }, () => setHighlightFilter(updatedHighlightFilter));\n    }\n  };\n\n  const dropFilter = async <T,>(\n    prop: string,\n    filters: Record<string, T>,\n    setter: (value: Record<string, T>) => void,\n    col: string,\n  ): Promise<void> => {\n    const updatedFilters = Object.keys(filters).reduce(\n      (res, key) => (key !== col ? { ...res, [key]: filters[key] } : res),\n      {},\n    );\n    const updatedSettings = { [prop]: updatedFilters };\n    await serverState.updateSettings(updatedSettings, dataId);\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n    } else {\n      updateSettings(updatedSettings, () => setter(updatedFilters));\n    }\n  };\n\n  const clear = async (): Promise<void> => {\n    await serverState.updateSettings({ query: '' }, dataId);\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n      window.close();\n    } else {\n      updateSettings({ query: '' }, onClose);\n    }\n  };\n\n  const updateEngine = async (engine: QueryEngine): Promise<void> => {\n    await serverState.updateQueryEngine(engine);\n    setEngine(engine);\n  };\n\n  return (\n    <React.Fragment>\n      <div className=\"modal-body filter-modal\">\n        {error}\n        <div className=\"row pt-3 pb-3\">\n          <span className=\"font-weight-bold col-auto pr-3\">{t('Highlight Filtered Rows', { ns: 'main' })}</span>\n          <Checkbox value={highlightFilter} setter={saveHighlightFilter} className=\"pt-1\" />\n        </div>\n        <div className=\"row\">\n          <div className=\"col-md-7\">\n            <div className=\"row h-100\">\n              <div className=\"col-md-12 h-100\">\n                <StructuredFilters\n                  label={t('Column Filters', { ns: 'filter' })}\n                  filters={columnFilters}\n                  dropFilter={(col: string) => dropFilter('columnFilters', columnFilters, setColumnFilters, col)}\n                />\n                <StructuredFilters\n                  label={t('Outlier Filters', { ns: 'filter' })}\n                  filters={outlierFilters}\n                  dropFilter={(col: string) => dropFilter('outlierFilters', outlierFilters, setOutlierFilters, col)}\n                />\n                <div className=\"font-weight-bold pt-3 pb-3\">{t('Custom Filter', { ns: 'filter' })}</div>\n                <textarea\n                  style={{ width: '100%', height: 150 }}\n                  value={query || ''}\n                  onChange={(event) => setQuery(event.target.value)}\n                />\n              </div>\n            </div>\n          </div>\n          <div className=\"col-md-5\">\n            <QueryExamples />\n          </div>\n        </div>\n        <div className=\"row pb-0\">\n          <span className=\"font-weight-bold col-auto pr-0\">{t('Query Engine', { ns: 'filter' })}</span>\n          <ButtonToggle\n            className=\"ml-auto mr-3 font-weight-bold col\"\n            options={Object.values(QueryEngine).map((value) => ({ value }))}\n            update={updateEngine}\n            defaultValue={queryEngine}\n          />\n        </div>\n        <div className=\"row\">\n          <div className=\"col-md-12\">{contextVars.length > 0 && <ContextVariables contextVars={contextVars} />}</div>\n        </div>\n      </div>\n      <div className=\"modal-footer\">\n        <PandasQueryHelp />\n        <button className=\"btn btn-primary\" onClick={clear}>\n          <span>{t('Clear', { ns: 'filter' })}</span>\n        </button>\n        <button className=\"btn btn-primary\" onClick={save}>\n          <span>{t('Apply', { ns: 'filter' })}</span>\n        </button>\n      </div>\n    </React.Fragment>\n  );\n};\n\nexport default withTranslation(['filter', 'main'])(FilterPopup);\n", "import { Action, AnyAction } from 'redux';\nimport { ThunkAction } from 'redux-thunk';\n\nimport {\n  AppState,\n  DataViewerUpdate,\n  FilteredRanges,\n  InstanceSettings,\n  Popups,\n  QueryEngine,\n  RangeState,\n  RibbonDropdownType,\n  SidePanelType,\n  ThemeType,\n} from '../state/AppState';\n\n/** Different application events */\nexport enum ActionType {\n  INIT_PARAMS = 'init-params',\n  LOAD_PREVIEW = 'load-preview',\n  EDIT_CELL = 'edit-cell',\n  TOGGLE_COLUMN_MENU = 'toggle-column-menu',\n  OPEN_MENU = 'open-menu',\n  CLOSE_MENU = 'close-menu',\n  OPEN_FORMATTING = 'open-formatting',\n  CLOSE_FORMATTING = 'close-formatting',\n  TOGGLE_MENU_PINNED = 'toggle-menu-pinned',\n  HIDE_COLUMN_MENU = 'hide-column-menu',\n  CLEAR_EDIT = 'clear-edit',\n  EDITED_CELL_TEXTAREA_HEIGHT = 'edited-cell-textarea-height',\n  CONVERT_TO_XARRAY = 'convert-to-xarray',\n  UPDATE_XARRAY_DIM = 'update-xarray-dim',\n  UPDATE_FILTERED_RANGES = 'update-filtered-ranges',\n  UPDATE_SETTINGS = 'update-settings',\n  SHOW_MENU_TOOLTIP = 'show-menu-tooltip',\n  HIDE_MENU_TOOLTIP = 'hide-menu-tooltip',\n  SHOW_RIBBON_MENU = 'show-ribbon-menu',\n  HIDE_RIBBON_MENU = 'hide-ribbon-menu',\n  OPEN_RIBBON_DROPDOWN = 'open-ribbon-dropdown',\n  SHOW_SIDE_PANEL = 'show-side-panel',\n  HIDE_SIDE_PANEL = 'hide-side-panel',\n  UPDATE_SIDE_PANEL_WIDTH = 'update-side-panel-width',\n  DATA_VIEWER_UPDATE = 'data-viewer-update',\n  CLEAR_DATA_VIEWER_UPDATE = 'clear-data-viewer-update',\n  DRAG_RESIZE = 'drag-resize',\n  STOP_RESIZE = 'stop-resize',\n  OPEN_CHART = 'open-chart',\n  CLOSE_CHART = 'close-chart',\n  LOADING_DATASETS = 'loading-datasets',\n  SET_THEME = 'set-theme',\n  SET_LANGUAGE = 'set-language',\n  UPDATE_MAX_WIDTH = 'update-max-width',\n  CLEAR_MAX_WIDTH = 'clear-max-width',\n  UPDATE_MAX_HEIGHT = 'update-max-height',\n  CLEAR_MAX_HEIGHT = 'clear-max-height',\n  SET_QUERY_ENGINE = 'set-query-engine',\n  UPDATE_SHOW_ALL_HEATMAP_COLUMNS = 'update-show-all-heatmap-columns',\n  SET_RANGE_STATE = 'set-range-state',\n  UPDATE_HIDE_SHUTDOWN = 'update-hide-shutdown',\n  UPDATE_ALLOW_CELL_EDITS = 'update-allow-cell-edits',\n  UPDATE_HIDE_HEADER_EDITOR = 'update-hide-header-editor',\n  UPDATE_LOCK_HEADER_MENU = 'update-lock-header-menu',\n  UPDATE_HIDE_HEADER_MENU = 'update-hide-header-menu',\n  UPDATE_HIDE_MAIN_MENU = 'update-hide-main-menu',\n  UPDATE_HIDE_COLUMN_MENUS = 'update-hide-column-menus',\n}\n\n/** Action fired when a range is selected */\nexport type SetRangeStateAction = Action<typeof ActionType.SET_RANGE_STATE> & RangeState;\n\n/** Action fired when application initially loads */\nexport type InitAction = Action<typeof ActionType.INIT_PARAMS>;\n\n/** Action fired when user cancels a cell edit */\nexport type ClearEditAction = Action<typeof ActionType.CLEAR_EDIT>;\n\n/** Action fired when user wants to convert their data to XArray */\nexport type ConvertToXarrayAction = Action<typeof ActionType.CONVERT_TO_XARRAY>;\n\n/** Action fired to hide menu item tooltips */\nexport type HideMenuTooltipAction = Action<typeof ActionType.HIDE_MENU_TOOLTIP>;\n\n/** Action fired to show the ribbon menu */\nexport type ShowRibbonMenuAction = Action<typeof ActionType.SHOW_RIBBON_MENU>;\n\n/** Action fired to hide the ribbon menu */\nexport type HideRibbonMenuAction = Action<typeof ActionType.HIDE_RIBBON_MENU>;\n\n/** Action fired to hide the side panel */\nexport type HideSidePanelAction = Action<typeof ActionType.HIDE_SIDE_PANEL>;\n\n/** Action fired to clear any executed data viewer updates */\nexport type ClearDataViewerUpdateAction = Action<typeof ActionType.CLEAR_DATA_VIEWER_UPDATE>;\n\n/** Action fired to stop resizing columns */\nexport type StopResizeAction = Action<typeof ActionType.STOP_RESIZE>;\n\n/** Action fired to close a popup */\nexport type CloseChartAction = Action<typeof ActionType.CLOSE_CHART>;\n\n/** Action fired when a dataset is being loaded */\nexport type LoadingDatasetsAction = Action<typeof ActionType.LOADING_DATASETS>;\n\n/** Action fired when clearing max width setting */\nexport type ClearMaxWidthAction = Action<typeof ActionType.CLEAR_MAX_WIDTH>;\n\n/** Action fired when clearing max height setting */\nexport type ClearMaxHeightAction = Action<typeof ActionType.CLEAR_MAX_HEIGHT>;\n\n/** Action fired when toggling the state of the main menu being pinned or not */\nexport type ToggleMenuPinnedAction = Action<typeof ActionType.TOGGLE_MENU_PINNED>;\n\n/** Action fired when D-Tale is loaded in \"preview\" mode */\nexport interface LoadPreviewAction extends Action<typeof ActionType.LOAD_PREVIEW> {\n  dataId: string;\n}\n\n/** Action fired when a user edits a cell */\nexport interface EditedCellAction extends Action<typeof ActionType.EDIT_CELL> {\n  editedCell?: string;\n}\n\n/** Action fired for sizing the height of textarea for cell editing */\nexport interface EditedTextAreaHeightAction extends Action<typeof ActionType.EDITED_CELL_TEXTAREA_HEIGHT> {\n  height: number;\n}\n\n/** Action fired for toggling the display of a column menu */\nexport interface ToggleColumnAction extends Action<typeof ActionType.TOGGLE_COLUMN_MENU | ActionType.HIDE_COLUMN_MENU> {\n  colName?: string;\n  headerRef?: HTMLDivElement;\n}\n\n/** Action fired for toggling the display of the main menu */\nexport type ToggleMenuAction = Action<typeof ActionType.OPEN_MENU | ActionType.CLOSE_MENU>;\n\n/** Action fired for opening the formatting menu */\nexport interface OpenFormattingAction extends Action<typeof ActionType.OPEN_FORMATTING> {\n  selectedCol: string;\n}\n\n/** Action fired for closing the formatting menu */\nexport type CloseFormattingAction = Action<typeof ActionType.CLOSE_FORMATTING>;\n\n/** Action fired when updating xarray dimensions */\nexport interface UpdateXarrayDimAction extends Action<typeof ActionType.UPDATE_XARRAY_DIM> {\n  xarrayDim: Record<string, boolean>;\n}\n\n/** Action fired when updating filtered ranges */\nexport interface UpdateFilteredRangesAction extends Action<typeof ActionType.UPDATE_FILTERED_RANGES> {\n  ranges: FilteredRanges;\n}\n\n/** Action fired when updating instance settings */\nexport interface UpdateSettingsAction extends Action<typeof ActionType.UPDATE_SETTINGS> {\n  settings: Partial<InstanceSettings>;\n}\n\n/** Action fired when showing a main menu tooltip */\nexport interface ShowMenuTooltipAction extends Action<typeof ActionType.SHOW_MENU_TOOLTIP> {\n  element: HTMLElement;\n  content: React.ReactNode;\n}\n\n/** Action fired when opening a ribbon dropdown */\nexport interface OpenRibbonDropdownAction extends Action<typeof ActionType.OPEN_RIBBON_DROPDOWN> {\n  element: HTMLDivElement;\n  name: RibbonDropdownType;\n}\n\n/** Action fired when showing or updating the width of the side panel */\nexport interface SidePanelAction\n  extends Action<typeof ActionType.SHOW_SIDE_PANEL | ActionType.UPDATE_SIDE_PANEL_WIDTH> {\n  view?: SidePanelType;\n  column?: string;\n  offset?: number;\n}\n\n/** Action fired when executing a data viewer update */\nexport interface DataViewerUpdateAction extends Action<typeof ActionType.DATA_VIEWER_UPDATE> {\n  update: DataViewerUpdate;\n}\n\n/** Action fired when dragging/resizing a column */\nexport interface DragResizeAction extends Action<typeof ActionType.DRAG_RESIZE> {\n  x: number;\n}\n\n/** Action fired when setting a theme */\nexport interface SetThemeAction extends Action<typeof ActionType.SET_THEME> {\n  theme: ThemeType;\n}\n\n/** Action fired when setting a language */\nexport interface SetLanguageAction extends Action<typeof ActionType.SET_LANGUAGE> {\n  language: string;\n}\n\n/** Action fired when updating the maximum column width */\nexport interface UpdateMaxColumnWidthAction extends Action<typeof ActionType.UPDATE_MAX_WIDTH> {\n  width: number;\n}\n\n/** Action fired when updating the maximum row height */\nexport interface UpdateMaxRowHeightAction extends Action<typeof ActionType.UPDATE_MAX_HEIGHT> {\n  height: number;\n}\n\n/** Action fired when setting the query engine for custom filters */\nexport interface SetQueryEngineAction extends Action<typeof ActionType.SET_QUERY_ENGINE> {\n  engine: QueryEngine;\n}\n\n/** Action fired when updating whether to show all columns when in \"heatmap\" mode */\nexport interface UpdateShowAllHeatmapColumnsAction extends Action<typeof ActionType.UPDATE_SHOW_ALL_HEATMAP_COLUMNS> {\n  showAllHeatmapColumns: boolean;\n}\n\n/** Action fired when opening a chart popup */\nexport interface OpenChartAction extends Action<typeof ActionType.OPEN_CHART> {\n  chartData: Popups;\n}\n\n/** Action fired when updating the hide_shutdown flag */\nexport interface UpdateHideShutdown extends Action<typeof ActionType.UPDATE_HIDE_SHUTDOWN> {\n  value: boolean;\n}\n\n/** Action fired when updating the allow_cell_edits flag */\nexport interface UpdateAllowCellEdits extends Action<typeof ActionType.UPDATE_ALLOW_CELL_EDITS> {\n  value: boolean | string[];\n}\n\n/** Action fired when updating the hide_header_editor flag */\nexport interface UpdateHideHeaderEditor extends Action<typeof ActionType.UPDATE_HIDE_HEADER_EDITOR> {\n  value: boolean;\n}\n\n/** Action fired when updating the lock_header_menu flag */\nexport interface UpdateLockHeaderMenu extends Action<typeof ActionType.UPDATE_LOCK_HEADER_MENU> {\n  value: boolean;\n}\n\n/** Action fired when updating the hide_header_menu flag */\nexport interface UpdateHideHeaderMenu extends Action<typeof ActionType.UPDATE_HIDE_HEADER_MENU> {\n  value: boolean;\n}\n\n/** Action fired when updating the hide_main_menu flag */\nexport interface UpdateHideMainMenu extends Action<typeof ActionType.UPDATE_HIDE_MAIN_MENU> {\n  value: boolean;\n}\n\n/** Action fired when updating the hide_column_menus flag */\nexport interface UpdateHideColumnMenus extends Action<typeof ActionType.UPDATE_HIDE_COLUMN_MENUS> {\n  value: boolean;\n}\n\n/** Type definition encompassing all application actions */\nexport type AppActionTypes =\n  | InitAction\n  | ClearEditAction\n  | ConvertToXarrayAction\n  | HideMenuTooltipAction\n  | ShowRibbonMenuAction\n  | HideRibbonMenuAction\n  | HideSidePanelAction\n  | ClearDataViewerUpdateAction\n  | StopResizeAction\n  | CloseChartAction\n  | LoadingDatasetsAction\n  | ClearMaxWidthAction\n  | ClearMaxHeightAction\n  | ToggleMenuPinnedAction\n  | LoadPreviewAction\n  | EditedCellAction\n  | EditedTextAreaHeightAction\n  | ToggleColumnAction\n  | ToggleMenuAction\n  | OpenFormattingAction\n  | CloseFormattingAction\n  | UpdateXarrayDimAction\n  | UpdateFilteredRangesAction\n  | UpdateSettingsAction\n  | ShowMenuTooltipAction\n  | OpenRibbonDropdownAction\n  | SidePanelAction\n  | DataViewerUpdateAction\n  | DragResizeAction\n  | SetThemeAction\n  | SetLanguageAction\n  | UpdateMaxColumnWidthAction\n  | UpdateMaxRowHeightAction\n  | SetQueryEngineAction\n  | UpdateShowAllHeatmapColumnsAction\n  | OpenChartAction\n  | SetRangeStateAction\n  | UpdateHideShutdown\n  | UpdateAllowCellEdits\n  | UpdateHideHeaderEditor\n  | UpdateLockHeaderMenu\n  | UpdateHideHeaderMenu\n  | UpdateHideMainMenu\n  | UpdateHideColumnMenus;\n\n/** Type definition for redux application actions */\nexport type AppActions<R> = ThunkAction<R, AppState, Record<string, unknown>, AnyAction>;\n", "import { AnyAction, Store } from 'redux';\n\nimport * as serverState from '../../dtale/serverStateManagement';\nimport { AppState, QueryEngine, SidePanelType } from '../state/AppState';\n\nimport {\n  ActionType,\n  AppActions,\n  InitAction,\n  SetQueryEngineAction,\n  SidePanelAction,\n  ToggleColumnAction,\n  UpdateShowAllHeatmapColumnsAction,\n  UpdateXarrayDimAction,\n} from './AppActions';\n\nexport const init = (): InitAction => ({ type: ActionType.INIT_PARAMS });\n\nexport const loadBackgroundMode = (store: Store<AppState, AnyAction>): void => {\n  const { settings } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_SETTINGS,\n    settings: { backgroundMode: settings.backgroundMode ?? (!!settings.rangeHighlight?.length ? 'range' : undefined) },\n  });\n};\n\nexport const loadHideShutdown = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideShutdown } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_SHUTDOWN,\n    value: hideShutdown ?? settings.hide_shutdown ?? hideShutdown,\n  });\n};\n\nexport const loadAllowCellEdits = (store: Store<AppState, AnyAction>): void => {\n  const { settings, allowCellEdits } = store.getState();\n  store.dispatch({ type: ActionType.UPDATE_ALLOW_CELL_EDITS, value: settings.allow_cell_edits ?? allowCellEdits });\n};\n\nexport const loadHideHeaderEditor = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideHeaderEditor } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_HEADER_EDITOR,\n    value: hideHeaderEditor ?? settings.hide_header_editor ?? hideHeaderEditor,\n  });\n};\n\nexport const loadLockHeaderMenu = (store: Store<AppState, AnyAction>): void => {\n  const { settings, lockHeaderMenu } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_LOCK_HEADER_MENU,\n    value: lockHeaderMenu ?? settings.lock_header_menu ?? lockHeaderMenu,\n  });\n};\n\nexport const loadHideHeaderMenu = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideHeaderMenu } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_HEADER_MENU,\n    value: hideHeaderMenu ?? settings.hide_header_menu ?? hideHeaderMenu,\n  });\n};\n\nexport const loadHideMainMenu = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideMainMenu } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_MAIN_MENU,\n    value: hideMainMenu ?? settings.hide_main_menu ?? hideMainMenu,\n  });\n};\n\nexport const loadHideColumnMenus = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideColumnMenus } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_COLUMN_MENUS,\n    value: hideColumnMenus ?? settings.hide_column_menus ?? hideColumnMenus,\n  });\n};\n\nexport const openCustomFilter = (): SidePanelAction => ({\n  type: ActionType.SHOW_SIDE_PANEL,\n  view: SidePanelType.FILTER,\n});\n\nexport const openPredefinedFilters = (): SidePanelAction => ({\n  type: ActionType.SHOW_SIDE_PANEL,\n  view: SidePanelType.PREDEFINED_FILTERS,\n});\n\nexport const toggleColumnMenu = (colName: string, headerRef: HTMLDivElement): ToggleColumnAction => ({\n  type: ActionType.TOGGLE_COLUMN_MENU,\n  colName,\n  headerRef,\n});\n\nexport const hideColumnMenu =\n  (colName: string): AppActions<void> =>\n  (dispatch, getState) => {\n    const { selectedCol } = getState();\n    // when clicking another header cell it calls this after the fact and thus causes the user to click again to show it\n    if (selectedCol === colName) {\n      dispatch({ type: ActionType.HIDE_COLUMN_MENU, colName });\n    }\n  };\n\nexport const closeColumnMenu = (): AppActions<void> => (dispatch, getState) =>\n  dispatch({ type: ActionType.HIDE_COLUMN_MENU, colName: getState().selectedCol });\n\nexport const updateXArrayDimAction = (xarrayDim: Record<string, boolean>): UpdateXarrayDimAction => ({\n  type: ActionType.UPDATE_XARRAY_DIM,\n  xarrayDim,\n});\n\nexport const updateXArrayDim =\n  (xarrayDim: Record<string, boolean>, callback: () => void): AppActions<void> =>\n  (dispatch) => {\n    dispatch(updateXArrayDimAction(xarrayDim));\n    callback();\n  };\n\nexport const convertToXArray =\n  (callback: () => void): AppActions<void> =>\n  (dispatch) => {\n    dispatch({ type: ActionType.CONVERT_TO_XARRAY });\n    callback();\n  };\n\nexport const setQueryEngine = (engine: QueryEngine): SetQueryEngineAction => ({\n  type: ActionType.SET_QUERY_ENGINE,\n  engine,\n});\n\nexport const isPopup = (): boolean => !!window.location.pathname?.startsWith('/dtale/popup');\n\nexport const isJSON = (str: string): boolean => {\n  try {\n    JSON.parse(str);\n  } catch (e) {\n    return false;\n  }\n  return true;\n};\n\nexport const getParams = (): Record<string, string | string[]> => {\n  const search = location.search.substring(1);\n  if (!search) {\n    return {};\n  }\n  const params = JSON.parse(\n    '{\"' + decodeURI(search).replace(/\"/g, '\\\\\"').replace(/&/g, '\",\"').replace(/=/g, '\":\"') + '\"}',\n  );\n  return Object.keys(params).reduce((res: Record<string, string | string[]>, key: string) => {\n    const value = `${params[key]}`;\n    if (value) {\n      if (value.includes(',') && !isJSON(value)) {\n        return { ...res, [key]: value.split(',') };\n      }\n      return { ...res, [key]: value };\n    }\n    return res;\n  }, {});\n};\n\nexport const updateFilteredRanges =\n  (query: string): AppActions<Promise<void>> =>\n  async (dispatch, getState) => {\n    const { dataId, filteredRanges, isArcticDB, columnCount } = getState();\n    if (!!isArcticDB && (isArcticDB >= 1_000_000 || columnCount > 100)) {\n      return;\n    }\n    const currQuery = filteredRanges?.query ?? '';\n    if (currQuery !== query) {\n      const ranges = await serverState.loadFilteredRanges(dataId!);\n      dispatch({ type: ActionType.UPDATE_FILTERED_RANGES, ranges });\n    }\n  };\n\nexport const updateMaxWidth =\n  (width: number): AppActions<void> =>\n  (dispatch) => {\n    dispatch({ type: ActionType.UPDATE_MAX_WIDTH, width });\n    dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-width', width } });\n  };\n\nexport const clearMaxWidth = (): AppActions<void> => (dispatch) => {\n  dispatch({ type: ActionType.CLEAR_MAX_WIDTH });\n  dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-width', width: null } });\n};\n\nexport const updateMaxHeight =\n  (height: number): AppActions<void> =>\n  (dispatch) => {\n    dispatch({ type: ActionType.UPDATE_MAX_HEIGHT, height });\n    dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-height', height } });\n  };\n\nexport const clearMaxHeight = (): AppActions<void> => (dispatch) => {\n  dispatch({ type: ActionType.CLEAR_MAX_HEIGHT });\n  dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-height', height: null } });\n};\n\nexport const updateShowAllHeatmapColumns = (showAllHeatmapColumns: boolean): UpdateShowAllHeatmapColumnsAction => ({\n  type: ActionType.UPDATE_SHOW_ALL_HEATMAP_COLUMNS,\n  showAllHeatmapColumns,\n});\n", "import { ActionType, AppActionTypes } from '../../actions/AppActions';\nimport { QueryEngine, ThemeType, Version } from '../../state/AppState';\nimport { getHiddenValue, toBool, toFloat, toJson } from '../utils';\n\nexport const hideShutdown = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_shutdown'));\n    case ActionType.UPDATE_HIDE_SHUTDOWN:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideHeaderEditor = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_header_editor'));\n    case ActionType.UPDATE_HIDE_HEADER_EDITOR:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const lockHeaderMenu = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('lock_header_menu'));\n    case ActionType.UPDATE_LOCK_HEADER_MENU:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideHeaderMenu = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_header_menu'));\n    case ActionType.UPDATE_HIDE_HEADER_MENU:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideMainMenu = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_main_menu'));\n    case ActionType.UPDATE_HIDE_MAIN_MENU:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideColumnMenus = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_column_menus'));\n    case ActionType.UPDATE_HIDE_COLUMN_MENUS:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const openCustomFilterOnStartup = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('open_custom_filter_on_startup'));\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const openPredefinedFiltersOnStartup = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return (\n        toBool(getHiddenValue('open_predefined_filters_on_startup')) &&\n        getHiddenValue('predefined_filters') !== undefined\n      );\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const hideDropRows = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_drop_rows'));\n    default:\n      return state;\n  }\n};\n\nexport const allowCellEdits = (state: boolean | string[] = true, action: AppActionTypes): boolean | string[] => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS: {\n      return toJson(getHiddenValue('allow_cell_edits'));\n    }\n    case ActionType.UPDATE_ALLOW_CELL_EDITS:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const theme = (state = ThemeType.LIGHT, action: AppActionTypes): ThemeType => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS: {\n      const themeStr = getHiddenValue('theme');\n      const themeVal = Object.values(ThemeType).find((t) => t.valueOf() === themeStr);\n      return themeVal ?? state;\n    }\n    case ActionType.SET_THEME: {\n      const body = document.getElementsByTagName('body')[0];\n      body.classList.remove(`${state}-mode`);\n      body.classList.add(`${action.theme}-mode`);\n      return action.theme;\n    }\n    default:\n      return state;\n  }\n};\n\nexport const language = (state = 'en', action: AppActionTypes): string => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('language') ?? state;\n    case ActionType.SET_LANGUAGE:\n      return action.language;\n    default:\n      return state;\n  }\n};\n\nexport const pythonVersion = (state: Version | null = null, action: AppActionTypes): Version | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n    case ActionType.LOAD_PREVIEW: {\n      const version = getHiddenValue('python_version');\n      if (version) {\n        return version.split('.').map((subVersion: string) => parseInt(subVersion, 10)) as Version;\n      }\n      return state;\n    }\n    default:\n      return state;\n  }\n};\n\nexport const isVSCode = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('is_vscode')) && global.top !== global.self;\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const isArcticDB = (state = 0, action: AppActionTypes): number => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('is_arcticdb')) as number;\n    case ActionType.LOAD_PREVIEW:\n      return 0;\n    default:\n      return state;\n  }\n};\n\nexport const arcticConn = (state = '', action: AppActionTypes): string => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('arctic_conn') ?? '';\n    case ActionType.LOAD_PREVIEW:\n      return '';\n    default:\n      return state;\n  }\n};\n\nexport const columnCount = (state = 0, action: AppActionTypes): number => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('column_count')) as number;\n    case ActionType.LOAD_PREVIEW:\n      return 0;\n    default:\n      return state;\n  }\n};\n\nexport const maxColumnWidth = (state: number | null = null, action: AppActionTypes): number | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('max_column_width'), true) ?? null;\n    case ActionType.UPDATE_MAX_WIDTH:\n      return action.width;\n    case ActionType.CLEAR_MAX_WIDTH:\n      return null;\n    default:\n      return state;\n  }\n};\n\nexport const maxRowHeight = (state: number | null = null, action: AppActionTypes): number | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('max_row_height'), true) ?? null;\n    case ActionType.UPDATE_MAX_HEIGHT:\n      return action.height;\n    case ActionType.CLEAR_MAX_HEIGHT:\n      return null;\n    default:\n      return state;\n  }\n};\n\nexport const mainTitle = (state: string | null = null, action: AppActionTypes): string | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('main_title') ?? null;\n    default:\n      return state;\n  }\n};\n\nexport const mainTitleFont = (state: string | null = null, action: AppActionTypes): string | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('main_title_font') ?? null;\n    default:\n      return state;\n  }\n};\n\nexport const queryEngine = (state = QueryEngine.PYTHON, action: AppActionTypes): QueryEngine => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS: {\n      const engineStr = getHiddenValue('query_engine');\n      const queryEngineVal = Object.values(QueryEngine).find((key) => key.valueOf() === engineStr);\n      return queryEngineVal ?? state;\n    }\n    case ActionType.SET_QUERY_ENGINE:\n      return action.engine;\n    default:\n      return state;\n  }\n};\n\nexport const showAllHeatmapColumns = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.UPDATE_SHOW_ALL_HEATMAP_COLUMNS:\n      return action.showAllHeatmapColumns;\n    default:\n      return state;\n  }\n};\n", "import { createSelector } from '@reduxjs/toolkit';\n\nimport { RangeSelection } from '../dtale/DataViewerState';\n\nimport {\n  AppState,\n  DataViewerUpdate,\n  FilteredRanges,\n  InstanceSettings,\n  Popups,\n  PredefinedFilter,\n  QueryEngine,\n  RibbonDropdownProps,\n  SidePanelProps,\n  ThemeType,\n  Version,\n} from './state/AppState';\n\nexport const selectDataId = (state: AppState): string => state.dataId;\nexport const selectAuth = (state: AppState): boolean => state.auth;\nexport const selectUsername = (state: AppState): string | null => state.username;\nexport const selectEditedCell = (state: AppState): string | null => state.editedCell;\nexport const selectIsVSCode = (state: AppState): boolean => state.isVSCode;\nexport const selectCtrlRows = (state: AppState): number[] | null => state.ctrlRows;\nexport const selectCtrlCols = (state: AppState): number[] | null => state.ctrlCols;\nexport const selectIsArcticDB = (state: AppState): number => state.isArcticDB;\nexport const selectColumnCount = (state: AppState): number => state.columnCount;\nexport const selectColumnMenuOpen = (state: AppState): boolean => state.columnMenuOpen;\nexport const selectSelectedCol = (state: AppState): string | null => state.selectedCol;\nexport const selectSelectedColRef = (state: AppState): HTMLDivElement | null => state.selectedColRef;\nexport const selectIsPreview = (state: AppState): boolean => state.isPreview;\nexport const selectRibbonDropdown = (state: AppState): RibbonDropdownProps => state.ribbonDropdown;\nexport const selectRibbonDropdownVisible = createSelector(\n  [selectRibbonDropdown],\n  (ribbonDropdown) => ribbonDropdown.visible,\n);\nexport const selectRibbonDropdownElement = createSelector(\n  [selectRibbonDropdown],\n  (ribbonDropdown) => ribbonDropdown.element,\n);\nexport const selectRibbonDropdownName = createSelector([selectRibbonDropdown], (ribbonDropdown) => ribbonDropdown.name);\nexport const selectSettings = (state: AppState): InstanceSettings => state.settings;\nexport const selectColumnFilters = createSelector([selectSettings], (settings) => settings?.columnFilters);\nexport const selectOutlierFilters = createSelector([selectSettings], (settings) => settings?.outlierFilters);\nexport const selectInvertFilter = createSelector([selectSettings], (settings) => settings?.invertFilter);\nexport const selectQuery = createSelector([selectSettings], (settings) => settings?.query);\nexport const selectPredefinedFilters = createSelector([selectSettings], (settings) => settings?.predefinedFilters);\nexport const selectSortInfo = createSelector([selectSettings], (settings) => settings?.sortInfo);\nexport const selectHighlightFilter = createSelector([selectSettings], (settings) => settings?.highlightFilter);\nexport const selectVerticalHeaders = createSelector([selectSettings], (settings) => settings?.verticalHeaders);\nexport const selectSettingsHideHeaderEditor = createSelector(\n  [selectSettings],\n  (settings) => settings?.hide_header_editor,\n);\nexport const selectBackgroundMode = createSelector([selectSettings], (settings) => settings?.backgroundMode);\nexport const selectRangeHighlight = createSelector([selectSettings], (settings) => settings?.rangeHighlight);\nexport const selectBaseLockHeaderMenu = (state: AppState): boolean => state.lockHeaderMenu;\nexport const selectBaseHideHeaderMenu = (state: AppState): boolean => state.hideHeaderMenu;\nexport const selectBaseHideMainMenu = (state: AppState): boolean => state.hideMainMenu;\nexport const selectBaseHideColumnMenus = (state: AppState): boolean => state.hideColumnMenus;\nexport const selectFilteredRanges = (state: AppState): FilteredRanges => state.filteredRanges;\nexport const selectShowAllHeatmapColumns = (state: AppState): boolean => state.showAllHeatmapColumns;\nexport const selectChartData = (state: AppState): Popups => state.chartData;\nexport const selectSidePanel = (state: AppState): SidePanelProps => state.sidePanel;\nexport const selectSidePanelVisible = createSelector([selectSidePanel], (sidePanel) => sidePanel.visible);\nexport const selectSidePanelView = createSelector([selectSidePanel], (sidePanel) => sidePanel.view);\nexport const selectSidePanelColumn = createSelector([selectSidePanel], (sidePanel) => sidePanel.column);\nexport const selectSidePanelOffset = createSelector([selectSidePanel], (sidePanel) => sidePanel.offset);\nexport const selectTheme = (state: AppState): ThemeType => state.theme;\nexport const selectPythonVersion = (state: AppState): Version | null => state.pythonVersion;\nexport const selectMaxColumnWidth = (state: AppState): number | null => state.maxColumnWidth;\nexport const selectAllowCellEdits = (state: AppState): boolean | string[] => state.allowCellEdits;\nexport const selectRowRange = (state: AppState): RangeSelection<number> | null => state.rowRange;\nexport const selectColumnRange = (state: AppState): RangeSelection<number> | null => state.columnRange;\nexport const selectRangeSelect = (state: AppState): RangeSelection<string> | null => state.rangeSelect;\nexport const selectSelectedRow = (state: AppState): number | null => state.selectedRow;\nexport const selectFormattingOpen = (state: AppState): string | null => state.formattingOpen;\nexport const selectMenuPinned = (state: AppState): boolean => state.menuPinned;\nexport const selectMenuOpen = (state: AppState): boolean => state.menuOpen;\nexport const selectBaseHideHeaderEditor = (state: AppState): boolean => state.hideHeaderEditor;\nexport const selectHideHeaderEditor = createSelector(\n  [selectSettingsHideHeaderEditor, selectBaseHideHeaderEditor],\n  (settingsHideHeaderEditor, hideHeaderEditor) => settingsHideHeaderEditor ?? hideHeaderEditor,\n);\nexport const selectPredefinedFilterConfigs = (state: AppState): PredefinedFilter[] => state.predefinedFilters;\nexport const selectHideDropRows = (state: AppState): boolean => state.hideDropRows;\nexport const selectArcticConn = (state: AppState): string => state.arcticConn;\nexport const selectBaseRibbonMenuOpen = (state: AppState): boolean => state.ribbonMenuOpen;\nconst selectSettingsLockHeaderMenu = createSelector([selectSettings], (settings) => settings?.lock_header_menu);\nexport const selectLockHeaderMenu = createSelector(\n  [selectSettingsLockHeaderMenu, selectBaseLockHeaderMenu],\n  (settingsLockHeaderMenu, lockHeaderMenu) => settingsLockHeaderMenu ?? lockHeaderMenu,\n);\nconst selectSettingsHideHeaderMenu = createSelector([selectSettings], (settings) => settings?.hide_header_menu);\nexport const selectHideHeaderMenu = createSelector(\n  [selectSettingsHideHeaderMenu, selectBaseHideHeaderMenu],\n  (settingsHideHeaderMenu, hideHeaderMenu) => settingsHideHeaderMenu ?? hideHeaderMenu,\n);\nconst selectSettingsHideMainMenu = createSelector([selectSettings], (settings) => settings?.hide_main_menu);\nexport const selectHideMainMenu = createSelector(\n  [selectSettingsHideMainMenu, selectBaseHideMainMenu],\n  (settingsHideMainMenu, hideMainMenu) => settingsHideMainMenu ?? hideMainMenu,\n);\nconst selectSettingsHideColumnMenus = createSelector([selectSettings], (settings) => settings?.hide_column_menus);\nexport const selectHideColumnMenus = createSelector(\n  [selectSettingsHideColumnMenus, selectBaseHideColumnMenus],\n  (settingsHideColumnMenus, hideColumnMenus) => settingsHideColumnMenus ?? hideColumnMenus,\n);\nexport const selectRibbonMenuOpen = createSelector(\n  [selectBaseRibbonMenuOpen, selectLockHeaderMenu, selectHideHeaderMenu],\n  (ribbonMenuOpen, lockHeaderMenu, hideHeaderMenu) => (ribbonMenuOpen || lockHeaderMenu) && !hideHeaderMenu,\n);\nexport const selectMainTitle = (state: AppState): string | null => state.mainTitle;\nexport const selectMainTitleFont = (state: AppState): string | null => state.mainTitleFont;\nexport const selectDataViewerUpdate = (state: AppState): DataViewerUpdate | null => state.dataViewerUpdate;\nexport const selectMaxRowHeight = (state: AppState): number | null => state.maxRowHeight;\nexport const selectEditedTextAreaHeight = (state: AppState): number => state.editedTextAreaHeight;\nexport const selectDragResize = (state: AppState): number | null => state.dragResize;\nexport const selectXArray = (state: AppState): boolean => state.xarray;\nexport const selectXArrayDim = (state: AppState): Record<string, any> => state.xarrayDim;\nexport const selectIFrame = (state: AppState): boolean => state.iframe;\nexport const selectLanguage = (state: AppState): string => state.language;\nexport const selectHideShutdown = (state: AppState): boolean => state.hideShutdown;\nexport const selectQueryEngine = (state: AppState): QueryEngine => state.queryEngine;\n", "import { RGBColor } from 'react-color';\n\nimport {\n  Bounds,\n  ColumnDef,\n  ColumnFilter,\n  ColumnFormat,\n  DataViewerPropagateState,\n  OutlierFilter,\n  RangeSelection,\n} from '../../dtale/DataViewerState';\n\n/** Base properties for react-select dropdown options */\nexport interface BaseOption<T> {\n  value: T;\n  label?: string | null;\n}\n\n/** Base properties for ButtonToggle options */\nexport interface ButtonOption<T> {\n  value: T;\n  label?: string | React.ReactNode;\n}\n\n/** Object which can be turned on/off */\nexport interface HasActivation {\n  active: boolean;\n}\n\n/** Object which has a visiblity flag */\nexport interface HasVisibility {\n  visible: boolean;\n}\n\nexport const initialVisibility: HasVisibility = { visible: false };\n\n/** Properties of a main menu tooltip */\nexport interface MenuTooltipProps extends HasVisibility {\n  element?: HTMLElement;\n  content?: React.ReactNode;\n}\n\n/** Ribbon dropdown types */\nexport enum RibbonDropdownType {\n  MAIN = 'main',\n  ACTIONS = 'actions',\n  VISUALIZE = 'visualize',\n  HIGHLIGHT = 'highlight',\n  SETTINGS = 'settings',\n}\n\n/** Properties of a ribbon menu dropdown */\nexport interface RibbonDropdownProps extends HasVisibility {\n  element?: HTMLDivElement;\n  name?: RibbonDropdownType;\n}\n\n/** Side panel types */\nexport enum SidePanelType {\n  SHOW_HIDE = 'show_hide',\n  DESCRIBE = 'describe',\n  MISSINGNO = 'missingno',\n  CORR_ANALYSIS = 'corr_analysis',\n  CORRELATIONS = 'correlations',\n  PPS = 'pps',\n  FILTER = 'filter',\n  PREDEFINED_FILTERS = 'predefined_filters',\n  GAGE_RNR = 'gage_rnr',\n  TIMESERIES_ANALYSIS = 'timeseries_analysis',\n}\n\n/** Properties of the current side panel */\nexport interface SidePanelProps extends HasVisibility {\n  view?: SidePanelType;\n  column?: string;\n  offset?: number;\n}\n\n/** Different types of data viewer updates */\nexport enum DataViewerUpdateType {\n  TOGGLE_COLUMNS = 'toggle-columns',\n  UPDATE_MAX_WIDTH = 'update-max-width',\n  UPDATE_MAX_HEIGHT = 'update-max-height',\n  DROP_COLUMNS = 'drop-columns',\n}\n\n/** Base properties for a DataViewer update */\ninterface BaseDataViewerUpdateProps<T extends DataViewerUpdateType> {\n  type: T;\n}\n\n/** Toggle columns DataViewer update */\nexport interface ToggleColumnsDataViewerUpdate extends BaseDataViewerUpdateProps<DataViewerUpdateType.TOGGLE_COLUMNS> {\n  columns: Record<string, boolean>;\n}\n\n/** Update maximum width DataViewer update */\nexport interface UpdateMaxWidthDataViewerUpdate\n  extends BaseDataViewerUpdateProps<DataViewerUpdateType.UPDATE_MAX_WIDTH> {\n  width: number;\n}\n\n/** Update maximum row height DataViewer update */\nexport type UpdateMaxHeightDataViewerUpdate = BaseDataViewerUpdateProps<DataViewerUpdateType.UPDATE_MAX_HEIGHT>;\n\n/** Drop columns DataViewer update */\nexport interface DropColumnsDataViewerUpdate extends BaseDataViewerUpdateProps<DataViewerUpdateType.DROP_COLUMNS> {\n  columns: string[];\n}\n\n/** DataViewer updates */\nexport type DataViewerUpdate =\n  | ToggleColumnsDataViewerUpdate\n  | UpdateMaxWidthDataViewerUpdate\n  | UpdateMaxHeightDataViewerUpdate\n  | DropColumnsDataViewerUpdate;\n\n/** Popup type names */\nexport enum PopupType {\n  HIDDEN = 'hidden',\n  FILTER = 'filter',\n  COLUMN_ANALYSIS = 'column-analysis',\n  CORRELATIONS = 'correlations',\n  PPS = 'pps',\n  BUILD = 'build',\n  TYPE_CONVERSION = 'type-conversion',\n  CLEANERS = 'cleaners',\n  RESHAPE = 'reshape',\n  ABOUT = 'about',\n  CONFIRM = 'confirm',\n  COPY_RANGE = 'copy-range',\n  COPY_COLUMN_RANGE = 'copy-column-range',\n  COPY_ROW_RANGE = 'copy-row-range',\n  RANGE = 'range',\n  XARRAY_DIMENSIONS = 'xarray-dimensions',\n  XARRAY_INDEXES = 'xarray-indexes',\n  RENAME = 'rename',\n  REPLACEMENT = 'replacement',\n  ERROR = 'error',\n  INSTANCES = 'instances',\n  VARIANCE = 'variance',\n  UPLOAD = 'upload',\n  DUPLICATES = 'duplicates',\n  CHARTS = 'charts',\n  DESCRIBE = 'describe',\n  EXPORT = 'export',\n  ARCTICDB = 'arcticdb',\n  JUMP_TO_COLUMN = 'jump_to_column',\n}\n\n/** Configuration for any data for a popup */\nexport interface PopupData<T extends PopupType> extends HasVisibility {\n  type: T;\n  title?: string;\n  size?: 'sm' | 'lg' | 'xl';\n  backdrop?: true | false | 'static';\n}\n\n/** Object which has a selected column */\ninterface HasColumnSelection {\n  selectedCol: string;\n}\n\n/** Popup configuration for About popup */\nexport type HiddenPopupData = PopupData<typeof PopupType.HIDDEN>;\n\nexport const initialPopup: HiddenPopupData = { ...initialVisibility, type: PopupType.HIDDEN };\n\n/** Popup configuration for About popup */\nexport type AboutPopupData = PopupData<typeof PopupType.ABOUT>;\n\n/** Popup configuration for Confirmation popup */\nexport interface ConfirmationPopupData extends PopupData<typeof PopupType.CONFIRM> {\n  msg: string;\n  yesAction?: () => void;\n}\n\n/** Base popup configuration for copying ranges */\ninterface BaseCopyRangeToClipboardData {\n  text: string;\n  headers: string[];\n}\n\n/** Popup configuration for CopyRangeToClipbard popup */\nexport type CopyRangeToClipboardPopupData = PopupData<typeof PopupType.COPY_RANGE> & BaseCopyRangeToClipboardData;\n\n/** Popup configuration for CopyRangeToClipbard popup */\nexport type CopyColumnRangeToClipboardPopupData = PopupData<typeof PopupType.COPY_COLUMN_RANGE> &\n  BaseCopyRangeToClipboardData;\n\n/** Popup configuration for CopyRangeToClipbard popup */\nexport type CopyRowRangeToClipboardPopupData = PopupData<typeof PopupType.COPY_ROW_RANGE> &\n  BaseCopyRangeToClipboardData;\n\n/** Popup configuration for Error popup */\nexport interface ErrorPopupData extends PopupData<typeof PopupType.ERROR> {\n  error: string;\n  traceback?: string;\n}\n\n/** Popup configuration for Error popup */\nexport interface ExportPopupData extends PopupData<typeof PopupType.EXPORT> {\n  rows: number;\n}\n\n/** Popup configuration for Error popup */\nexport interface RenamePopupData extends PopupData<typeof PopupType.RENAME>, HasColumnSelection {\n  columns: ColumnDef[];\n}\n\n/** Popup configuration for JumpToColumn popup */\nexport interface JumpToColumnPopupData extends PopupData<typeof PopupType.JUMP_TO_COLUMN> {\n  columns: ColumnDef[];\n}\n\n/** Popup configuration for RangeHighlight popup */\nexport interface RangeHighlightPopupData extends PopupData<typeof PopupType.RANGE> {\n  rangeHighlight?: RangeHighlightConfig;\n  backgroundMode?: string;\n  columns: ColumnDef[];\n}\n\n/** Popup configuration for XArrayDimensions popup */\nexport type XArrayDimensionsPopupData = PopupData<typeof PopupType.XARRAY_DIMENSIONS>;\n\n/** Popup configuration for XArrayIndexes popup */\nexport interface XArrayIndexesPopupData extends PopupData<typeof PopupType.XARRAY_INDEXES> {\n  columns: ColumnDef[];\n}\n\n/** Base properties for any column analysis popup */\nexport interface BaseColumnAnalysisPopupData extends HasColumnSelection {\n  query?: string;\n}\n\n/** Popup configuration for ColumnAnalysis popup */\nexport type ColumnAnalysisPopupData = PopupData<typeof PopupType.COLUMN_ANALYSIS> & BaseColumnAnalysisPopupData;\n\n/** Base properties for Correlation popups */\nexport interface BaseCorrelationsPopupData {\n  col1?: string;\n  col2?: string;\n}\n\n/** Popup configuration for Correlations popup */\nexport interface CorrelationsPopupData extends PopupData<typeof PopupType.CORRELATIONS>, BaseCorrelationsPopupData {\n  query?: string;\n}\n\n/** Popup configuration for Predictive Power Score popup */\nexport type PPSPopupData = PopupData<typeof PopupType.PPS> & BaseCorrelationsPopupData;\n\n/** Base popup configuration for column creation */\ninterface BaseCreateColumnPopupData {\n  selectedCol?: string;\n}\n\n/** Popup configuration for Create Column popup */\nexport type CreateColumnPopupData = PopupData<typeof PopupType.BUILD> & BaseCreateColumnPopupData;\n\n/** Popup configuration for Create Column - Type Conversion popup */\nexport type CreateTypeConversionPopupData = PopupData<typeof PopupType.TYPE_CONVERSION> & BaseCreateColumnPopupData;\n\n/** Popup configuration for Create Column - Cleaners popup */\nexport type CreateCleanersPopupData = PopupData<typeof PopupType.CLEANERS> & BaseCreateColumnPopupData;\n\n/** Popup configuration for Create Column popup */\nexport type ReshapePopupData = PopupData<typeof PopupType.RESHAPE>;\n\n/** Popup configuration for Charts popup */\nexport interface ChartsPopupData extends PopupData<typeof PopupType.CHARTS> {\n  query?: string;\n  x?: string;\n  y?: string[];\n  group?: string[];\n  aggregation?: string;\n  chartType?: string;\n  chartPerGroup?: boolean;\n}\n\n/** Popup configuration for Describe popup */\nexport interface DescribePopupData extends PopupData<typeof PopupType.DESCRIBE> {\n  selectedCol?: string;\n}\n\n/** Popup configuration for Duplicates popup */\nexport interface DuplicatesPopupData extends PopupData<typeof PopupType.DUPLICATES> {\n  selectedCol?: string;\n}\n\n/** Popup configuration for Filter popup */\nexport type CustomFilterPopupData = PopupData<typeof PopupType.FILTER>;\n\n/** Popup configuration for Upload popup */\nexport type UploadPopupData = PopupData<typeof PopupType.UPLOAD>;\n\n/** Popup configuration for ArcticDB popup */\nexport type ArcticDBPopupData = PopupData<typeof PopupType.ARCTICDB>;\n\n/** Popup configuration for Replacement popup */\nexport interface ReplacementPopupData extends PopupData<typeof PopupType.REPLACEMENT>, HasColumnSelection {\n  propagateState: DataViewerPropagateState;\n}\n\n/** Popup configuration for Variance popup */\nexport type VariancePopupData = PopupData<typeof PopupType.VARIANCE> & BaseColumnAnalysisPopupData;\n\n/** Popup configuration for Instances popup */\nexport type InstancesPopupData = PopupData<typeof PopupType.INSTANCES>;\n\n/** Popup configurations */\nexport type Popups =\n  | HiddenPopupData\n  | AboutPopupData\n  | ConfirmationPopupData\n  | CopyRangeToClipboardPopupData\n  | CopyColumnRangeToClipboardPopupData\n  | CopyRowRangeToClipboardPopupData\n  | ErrorPopupData\n  | RenamePopupData\n  | RangeHighlightPopupData\n  | XArrayDimensionsPopupData\n  | XArrayIndexesPopupData\n  | ColumnAnalysisPopupData\n  | ChartsPopupData\n  | CorrelationsPopupData\n  | PPSPopupData\n  | CreateColumnPopupData\n  | ReshapePopupData\n  | ChartsPopupData\n  | DescribePopupData\n  | DuplicatesPopupData\n  | CustomFilterPopupData\n  | UploadPopupData\n  | ReplacementPopupData\n  | VariancePopupData\n  | CreateTypeConversionPopupData\n  | CreateCleanersPopupData\n  | InstancesPopupData\n  | ExportPopupData\n  | ArcticDBPopupData\n  | JumpToColumnPopupData;\n\n/** Sort directions */\nexport enum SortDir {\n  ASC = 'ASC',\n  DESC = 'DESC',\n}\n\n/** Type definition for column being sorted and it's direction. */\nexport type SortDef = [string, SortDir];\n\n/** Value holder for predefined filters */\nexport interface PredefinedFilterValue extends HasActivation {\n  value?: any | any[];\n}\n\n/** Settings available to each instance (piece of data) of D-Tale */\nexport interface InstanceSettings {\n  locked?: string[];\n  allow_cell_edits: boolean | string[];\n  precision: number;\n  columnFormats?: Record<string, ColumnFormat>;\n  backgroundMode?: string;\n  rangeHighlight?: RangeHighlightConfig;\n  verticalHeaders: boolean;\n  predefinedFilters: Record<string, PredefinedFilterValue>;\n  sortInfo?: SortDef[];\n  nanDisplay?: string;\n  startup_code?: string;\n  query?: string;\n  highlightFilter?: boolean;\n  outlierFilters?: Record<string, OutlierFilter>;\n  filteredRanges?: FilteredRanges;\n  columnFilters?: Record<string, ColumnFilter>;\n  invertFilter?: boolean;\n  hide_shutdown: boolean;\n  column_edit_options?: Record<string, string[]>;\n  hide_header_editor: boolean;\n  lock_header_menu: boolean;\n  hide_header_menu: boolean;\n  hide_main_menu: boolean;\n  hide_column_menus: boolean;\n  isArcticDB?: number;\n}\n\nexport const BASE_INSTANCE_SETTINGS: InstanceSettings = Object.freeze({\n  allow_cell_edits: true,\n  hide_shutdown: false,\n  precision: 2,\n  verticalHeaders: false,\n  predefinedFilters: {},\n  hide_header_editor: false,\n  lock_header_menu: false,\n  hide_header_menu: false,\n  hide_main_menu: false,\n  hide_column_menus: false,\n});\n\n/** Type definition for semantic versioning of python */\nexport type Version = [number, number, number];\n\n/** Different themes available for D-Tale */\nexport enum ThemeType {\n  LIGHT = 'light',\n  DARK = 'dark',\n}\n\n/** Python query engines for executing custom queries */\nexport enum QueryEngine {\n  PYTHON = 'python',\n  NUMEXPR = 'numexpr',\n}\n\n/** Application-level settings */\nexport interface AppSettings {\n  hideShutdown: boolean;\n  openCustomFilterOnStartup: boolean;\n  openPredefinedFiltersOnStartup: boolean;\n  hideDropRows: boolean;\n  allowCellEdits: boolean | string[];\n  theme: ThemeType;\n  language: string;\n  pythonVersion: Version | null;\n  isVSCode: boolean;\n  isArcticDB: number;\n  arcticConn: string;\n  columnCount: number;\n  maxColumnWidth: number | null;\n  maxRowHeight: number | null;\n  mainTitle: string | null;\n  mainTitleFont: string | null;\n  queryEngine: QueryEngine;\n  showAllHeatmapColumns: boolean;\n  hideHeaderEditor: boolean;\n  lockHeaderMenu: boolean;\n  hideHeaderMenu: boolean;\n  hideMainMenu: boolean;\n  hideColumnMenus: boolean;\n}\n\n/** Properties for specifying filtered ranges */\nexport interface FilteredRanges {\n  query?: string;\n  dtypes?: Record<string, ColumnDef>;\n  ranges?: Record<string, Bounds>;\n  overall?: Bounds;\n}\n\n/** Predefined filter types */\nexport enum PredfinedFilterInputType {\n  INPUT = 'input',\n  SELECT = 'select',\n  MULTISELECT = 'multiselect',\n}\n\n/** Predefined filter properties */\nexport interface PredefinedFilter extends HasActivation {\n  column: string;\n  default?: string | number;\n  description?: string;\n  inputType: PredfinedFilterInputType;\n  name: string;\n}\n\n/** Range highlight configuration properties */\nexport interface RangeHighlightModeCfg extends HasActivation {\n  value?: number;\n  color: RGBColor;\n}\n\n/** Different types of range highlighting */\nexport interface RangeHighlightModes {\n  equals: RangeHighlightModeCfg;\n  greaterThan: RangeHighlightModeCfg;\n  lessThan: RangeHighlightModeCfg;\n}\n\n/** Range highlighting for individual columns or \"all\" columns */\nexport interface RangeHighlightConfig {\n  [key: string | 'all']: RangeHighlightModes & HasActivation;\n}\n\n/** Range selection properties */\nexport interface RangeState {\n  rowRange: RangeSelection<number> | null;\n  columnRange: RangeSelection<number> | null;\n  rangeSelect: RangeSelection<string> | null;\n  ctrlRows: number[] | null;\n  ctrlCols: number[] | null;\n  selectedRow: number | null;\n}\n\n/** Properties of application state */\nexport interface AppState extends AppSettings, RangeState {\n  chartData: Popups;\n  dataId: string;\n  editedCell: string | null;\n  editedTextAreaHeight: number;\n  iframe: boolean;\n  columnMenuOpen: boolean;\n  selectedCol: string | null;\n  selectedColRef: HTMLDivElement | null;\n  xarray: boolean;\n  xarrayDim: Record<string, any>;\n  filteredRanges: FilteredRanges;\n  settings: InstanceSettings;\n  isPreview: boolean;\n  menuPinned: boolean;\n  menuTooltip: MenuTooltipProps;\n  ribbonMenuOpen: boolean;\n  ribbonDropdown: RibbonDropdownProps;\n  sidePanel: SidePanelProps;\n  dataViewerUpdate: DataViewerUpdate | null;\n  predefinedFilters: PredefinedFilter[];\n  dragResize: number | null;\n  auth: boolean;\n  username: string | null;\n  menuOpen: boolean;\n  formattingOpen: string | null;\n}\n", "[app]\ntheme = light\ngithub_fork = False\nhide_shutdown = False\npin_menu = False\nlanguage = en\nmax_column_width = 100\nmain_title = My App\nmain_title_font = Arial\nquery_engine = python\nhide_header_editor = False\nlock_header_menu = False\nhide_header_menu = False\nhide_main_menu = False\nhide_column_menus = False\n\n[charts]\nscatter_points = 15000\n3d_points = 40000\n\n[show]\nhost = localhost\nport = 8080\nreaper_on = True\nopen_browser = False\nignore_duplicate = True\nallow_cell_edits = True\ninplace = False\ndrop_index = False\nprecision = 6\nshow_columns = a,b\nhide_columns = c\ncolumn_formats = {\"a\": {\"fmt\": {\"html\": true}}}\nsort = a|ASC\nlocked = a,b\ncolumn_edit_options = {\"a\": [\"foo\", \"bar\", \"baz\"]}\nauto_hide_empty_columns = False\nhighlight_filter = False\n\n[auth]\nactive = False\nusername = admin\npassword = admin\n", "[app]\nhide_shutdown = False\nhide_header_editor = False\nlock_header_menu = False\nhide_header_menu = False\nhide_main_menu = False\nhide_column_menus = False\n", "import mock\nimport os\nimport pytest\n\nfrom dtale.config import (\n    load_app_settings,\n    load_auth_settings,\n    load_config_state,\n    build_show_options,\n    set_config,\n)\nfrom tests import ExitStack\n\n\n@pytest.mark.unit\ndef test_load_app_settings():\n    settings = {\n        \"theme\": \"dark\",\n        \"github_fork\": False,\n        \"hide_shutdown\": True,\n        \"pin_menu\": True,\n        \"language\": \"cn\",\n        \"max_column_width\": 50,\n        \"query_engine\": \"numexpr\",\n        \"hide_header_editor\": True,\n        \"lock_header_menu\": True,\n        \"hide_header_menu\": True,\n        \"hide_main_menu\": True,\n        \"hide_column_menus\": True,\n    }\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"dtale.global_state.APP_SETTINGS\", settings))\n\n        load_app_settings(None)\n        assert settings[\"hide_shutdown\"]\n        assert settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"cn\"\n        assert settings[\"theme\"] == \"dark\"\n        assert settings[\"max_column_width\"] == 50\n        assert settings[\"query_engine\"] == \"numexpr\"\n        assert settings[\"hide_header_editor\"]\n        assert settings[\"lock_header_menu\"]\n        assert settings[\"hide_header_menu\"]\n        assert settings[\"hide_main_menu\"]\n        assert settings[\"hide_column_menus\"]\n\n        load_app_settings(\n            load_config_state(os.path.join(os.path.dirname(__file__), \"dtale.ini\"))\n        )\n        assert not settings[\"hide_shutdown\"]\n        assert not settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"en\"\n        assert settings[\"theme\"] == \"light\"\n        assert settings[\"max_column_width\"] == 100\n        assert settings[\"main_title\"] == \"My App\"\n        assert settings[\"main_title_font\"] == \"Arial\"\n        assert settings[\"query_engine\"] == \"python\"\n        assert not settings[\"hide_header_editor\"]\n        assert not settings[\"lock_header_menu\"]\n        assert not settings[\"hide_header_menu\"]\n        assert not settings[\"hide_main_menu\"]\n        assert not settings[\"hide_column_menus\"]\n\n\n@pytest.mark.unit\ndef test_load_app_settings_w_missing_props():\n    settings = {\n        \"theme\": \"light\",\n        \"github_fork\": False,\n        \"hide_shutdown\": True,\n        \"pin_menu\": True,\n        \"language\": \"cn\",\n        \"max_column_width\": None,\n        \"query_engine\": \"python\",\n        \"hide_header_editor\": True,\n        \"lock_header_menu\": True,\n        \"hide_header_menu\": True,\n        \"hide_main_menu\": True,\n        \"hide_column_menus\": True,\n    }\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"dtale.global_state.APP_SETTINGS\", settings))\n\n        load_app_settings(None)\n        assert settings[\"hide_shutdown\"]\n        assert settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"cn\"\n        assert settings[\"max_column_width\"] is None\n        assert settings[\"hide_header_editor\"]\n        assert settings[\"lock_header_menu\"]\n        assert settings[\"hide_header_menu\"]\n        assert settings[\"hide_main_menu\"]\n        assert settings[\"hide_column_menus\"]\n\n        load_app_settings(\n            load_config_state(\n                os.path.join(os.path.dirname(__file__), \"dtale_missing_props.ini\")\n            )\n        )\n        assert not settings[\"hide_shutdown\"]\n        assert settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"cn\"\n        assert settings[\"max_column_width\"] is None\n        assert not settings[\"hide_header_editor\"]\n        assert not settings[\"lock_header_menu\"]\n        assert not settings[\"hide_header_menu\"]\n        assert not settings[\"hide_main_menu\"]\n        assert not settings[\"hide_column_menus\"]\n\n\n@pytest.mark.unit\ndef test_load_auth_settings():\n    settings = {\"active\": True, \"username\": \"foo\", \"password\": \"foo\"}\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"dtale.global_state.AUTH_SETTINGS\", settings))\n\n        load_auth_settings(None)\n        assert settings[\"active\"]\n        assert settings[\"username\"] == \"foo\"\n        assert settings[\"password\"] == \"foo\"\n\n        load_auth_settings(\n            load_config_state(os.path.join(os.path.dirname(__file__), \"dtale.ini\"))\n        )\n        assert not settings[\"active\"]\n        assert settings[\"username\"] == \"admin\"\n        assert settings[\"password\"] == \"admin\"\n\n\n@pytest.mark.unit\ndef test_build_show_options(unittest):\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    options = dict(allow_cell_edits=False)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    ini_path = os.path.join(os.path.dirname(__file__), \"dtale.ini\")\n    os.environ[\"DTALE_CONFIG\"] = ini_path\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n    assert final_options[\"precision\"] == 6\n    unittest.assertEqual(final_options[\"show_columns\"], [\"a\", \"b\"])\n    unittest.assertEqual(final_options[\"hide_columns\"], [\"c\"])\n    unittest.assertEqual(\n        final_options[\"column_formats\"], {\"a\": {\"fmt\": {\"html\": True}}}\n    )\n    unittest.assertEqual(final_options[\"sort\"], [(\"a\", \"ASC\")])\n    unittest.assertEqual(final_options[\"locked\"], [\"a\", \"b\"])\n    unittest.assertEqual(\n        final_options[\"column_edit_options\"], {\"a\": [\"foo\", \"bar\", \"baz\"]}\n    )\n    assert not final_options[\"auto_hide_empty_columns\"]\n    assert not final_options[\"highlight_filter\"]\n\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    del os.environ[\"DTALE_CONFIG\"]\n\n    set_config(ini_path)\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    set_config(None)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n\n@pytest.mark.unit\ndef test_build_show_options_w_missing_ini_props(unittest):\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    options = dict(allow_cell_edits=False)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    ini_path = os.path.join(os.path.dirname(__file__), \"dtale_missing_props.ini\")\n    os.environ[\"DTALE_CONFIG\"] = ini_path\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    del os.environ[\"DTALE_CONFIG\"]\n\n    set_config(ini_path)\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    set_config(None)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    ini_path = os.path.join(\n        os.path.dirname(__file__), \"dtale_allow_cell_edits_list.ini\"\n    )\n    os.environ[\"DTALE_CONFIG\"] = ini_path\n    final_options = build_show_options()\n    unittest.assertEqual(final_options[\"allow_cell_edits\"], [\"a\", \"b\"])\n", "import json\nfrom builtins import str\n\nimport mock\nimport numpy as np\nimport os\nimport dtale.global_state as global_state\nimport pandas as pd\nimport pytest\nfrom pandas.tseries.offsets import Day\nfrom pkg_resources import parse_version\nfrom six import PY3\n\nimport dtale.pandas_util as pandas_util\n\nfrom dtale.app import build_app\nfrom dtale.pandas_util import check_pandas_version\nfrom dtale.utils import DuplicateDataError\nfrom tests import ExitStack, pdt\nfrom tests.dtale import build_data_inst, build_settings, build_dtypes\nfrom tests.dtale.test_charts import build_col_def\n\n\nURL = \"http://localhost:40000\"\napp = build_app(url=URL)\n\n\n@pytest.mark.unit\ndef test_head_endpoint():\n    import dtale.views as views\n\n    build_data_inst({\"1\": None, \"2\": None})\n    assert views.head_endpoint() == \"main/1\"\n    assert views.head_endpoint(\"test_popup\") == \"popup/test_popup/1\"\n\n    global_state.clear_store()\n    assert views.head_endpoint() == \"popup/upload\"\n\n\n@pytest.mark.unit\ndef test_startup(unittest):\n    import dtale.views as views\n    import dtale.global_state as global_state\n\n    global_state.clear_store()\n\n    instance = views.startup(URL)\n    assert instance._data_id == \"1\"\n\n    with pytest.raises(views.NoDataLoadedException) as error:\n        views.startup(URL, data_loader=lambda: None)\n    assert \"No data has been loaded into this D-Tale session!\" in str(\n        error.value.args[0]\n    )\n\n    with pytest.raises(BaseException) as error:\n        views.startup(URL, \"bad type\")\n    assert (\n        \"data loaded must be one of the following types: pandas.DataFrame, pandas.Series, pandas.DatetimeIndex\"\n        in str(error.value)\n    )\n\n    test_data = pd.DataFrame([dict(date=pd.Timestamp(\"now\"), security_id=1, foo=1.5)])\n    test_data = test_data.set_index([\"date\", \"security_id\"])\n    instance = views.startup(\n        URL,\n        data_loader=lambda: test_data,\n        sort=[(\"security_id\", \"ASC\")],\n        hide_header_editor=True,\n        hide_shutdown=True,\n        lock_header_menu=True,\n        hide_header_menu=True,\n        hide_main_menu=True,\n        hide_column_menus=True,\n    )\n\n    pdt.assert_frame_equal(instance.data, test_data.reset_index())\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            columnFormats={},\n            hide_shutdown=True,\n            hide_header_editor=True,\n            lock_header_menu=True,\n            hide_header_menu=True,\n            hide_main_menu=True,\n            hide_column_menus=True,\n            locked=[\"date\", \"security_id\"],\n            indexes=[\"date\", \"security_id\"],\n            precision=2,\n            sortInfo=[(\"security_id\", \"ASC\")],\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should lock index columns\",\n    )\n\n    global_state.set_app_settings(dict(hide_header_editor=False))\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            columnFormats={},\n            hide_shutdown=True,\n            hide_header_editor=False,\n            lock_header_menu=True,\n            hide_header_menu=True,\n            hide_main_menu=True,\n            hide_column_menus=True,\n            locked=[\"date\", \"security_id\"],\n            indexes=[\"date\", \"security_id\"],\n            precision=2,\n            sortInfo=[(\"security_id\", \"ASC\")],\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should hide header editor\",\n    )\n\n    test_data = test_data.reset_index()\n    with pytest.raises(DuplicateDataError):\n        views.startup(URL, data=test_data, ignore_duplicate=False)\n\n    range_highlights = {\n        \"foo\": {\n            \"active\": True,\n            \"equals\": {\n                \"active\": True,\n                \"value\": 3,\n                \"color\": {\"r\": 255, \"g\": 245, \"b\": 157, \"a\": 1},\n            },  # light yellow\n            \"greaterThan\": {\n                \"active\": True,\n                \"value\": 3,\n                \"color\": {\"r\": 80, \"g\": 227, \"b\": 194, \"a\": 1},\n            },  # mint green\n            \"lessThan\": {\n                \"active\": True,\n                \"value\": 3,\n                \"color\": {\"r\": 245, \"g\": 166, \"b\": 35, \"a\": 1},\n            },  # orange\n        }\n    }\n    instance = views.startup(\n        URL,\n        data=test_data,\n        ignore_duplicate=True,\n        allow_cell_edits=False,\n        precision=6,\n        range_highlights=range_highlights,\n    )\n    pdt.assert_frame_equal(instance.data, test_data)\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=False,\n            columnFormats={},\n            locked=[],\n            indexes=[],\n            precision=6,\n            rangeHighlight=range_highlights,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"no index = nothing locked\",\n    )\n\n    test_data = pd.DataFrame([dict(date=pd.Timestamp(\"now\"), security_id=1)])\n    test_data = test_data.set_index(\"security_id\").date\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    pdt.assert_frame_equal(instance.data, test_data.reset_index())\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            columnFormats={},\n            locked=[\"security_id\"],\n            indexes=[\"security_id\"],\n            precision=2,\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should lock index columns\",\n    )\n\n    test_data = pd.DatetimeIndex([pd.Timestamp(\"now\")], name=\"date\")\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    pdt.assert_frame_equal(instance.data, test_data.to_frame(index=False))\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            locked=[],\n            indexes=[],\n            precision=2,\n            columnFormats={},\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should not lock index columns\",\n    )\n\n    test_data = pd.MultiIndex.from_arrays([[1, 2], [3, 4]], names=(\"a\", \"b\"))\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    pdt.assert_frame_equal(instance.data, test_data.to_frame(index=False))\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            locked=[],\n            indexes=[],\n            precision=2,\n            columnFormats={},\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should not lock index columns\",\n    )\n\n    test_data = pd.DataFrame(\n        [\n            dict(date=pd.Timestamp(\"now\"), security_id=1, foo=1.0, bar=2.0, baz=np.nan),\n            dict(\n                date=pd.Timestamp(\"now\"), security_id=1, foo=2.0, bar=np.inf, baz=np.nan\n            ),\n        ],\n        columns=[\"date\", \"security_id\", \"foo\", \"bar\", \"baz\"],\n    )\n    instance = views.startup(\n        URL, data_loader=lambda: test_data, auto_hide_empty_columns=True\n    )\n    unittest.assertEqual(\n        {\n            \"name\": \"bar\",\n            \"dtype\": \"float64\",\n            \"index\": 3,\n            \"visible\": True,\n            \"hasMissing\": 0,\n            \"hasOutliers\": 0,\n            \"lowVariance\": False,\n            \"unique_ct\": 2,\n            \"kurt\": \"nan\",\n            \"skew\": \"nan\",\n            \"coord\": None,\n        },\n        next(\n            (\n                dt\n                for dt in global_state.get_dtypes(instance._data_id)\n                if dt[\"name\"] == \"bar\"\n            ),\n            None,\n        ),\n    )\n\n    non_visible = [\n        dt[\"name\"]\n        for dt in global_state.get_dtypes(instance._data_id)\n        if not dt[\"visible\"]\n    ]\n    unittest.assertEqual(non_visible, [\"baz\"])\n\n    test_data = pd.DataFrame([dict(a=1, b=2)])\n    test_data = test_data.rename(columns={\"b\": \"a\"})\n    with pytest.raises(Exception) as error:\n        views.startup(URL, data_loader=lambda: test_data)\n    assert \"data contains duplicated column names: a\" in str(error)\n\n    test_data = pd.DataFrame([dict(a=1, b=2)])\n    test_data = test_data.set_index(\"a\")\n    views.startup(URL, data=test_data, inplace=True, drop_index=True)\n    assert \"a\" not in test_data.columns\n\n    test_data = np.array([1, 2, 3])\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    unittest.assertEqual(list(instance.data.iloc[:, 0].tolist()), test_data.tolist())\n\n    test_data = np.ndarray(shape=(2, 2), dtype=float, order=\"F\")\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    np.testing.assert_almost_equal(instance.data.values, test_data)\n\n    test_data = [1, 2, 3]\n    instance = views.startup(URL, data_loader=lambda: test_data, ignore_duplicate=True)\n    unittest.assertEqual(instance.data.iloc[:, 0].tolist(), test_data)\n\n    test_data = dict(a=[1, 2, 3], b=[4, 5, 6])\n    instance = views.startup(URL, data_loader=lambda: test_data, ignore_duplicate=True)\n    unittest.assertEqual(instance.data[\"a\"].values.tolist(), test_data[\"a\"])\n    unittest.assertEqual(instance.data[\"b\"].values.tolist(), test_data[\"b\"])\n\n    test_data = dict(a=1, b=2, c=3)\n    instance = views.startup(URL, data_loader=lambda: test_data, ignore_duplicate=True)\n    unittest.assertEqual(\n        sorted(instance.data[\"index\"].values.tolist()), sorted(test_data.keys())\n    )\n    unittest.assertEqual(\n        sorted(instance.data[\"0\"].values.tolist()), sorted(test_data.values())\n    )\n\n    test_data = pd.DataFrame(\n        dict(\n            a=[\"{}\".format(i) for i in range(10)],\n            b=[\"{}\".format(i % 2) for i in range(10)],\n        )\n    )\n    instance = views.startup(\n        URL,\n        data_loader=lambda: test_data,\n        ignore_duplicate=True,\n        optimize_dataframe=True,\n    )\n    unittest.assertEqual(\n        list(instance.data.dtypes.apply(lambda x: x.name).values),\n        [\"object\", \"category\"],\n    )\n\n    many_cols = pd.DataFrame({\"sec{}\".format(v): [1] for v in range(500)})\n    instance = views.startup(URL, data=many_cols)\n    unittest.assertEqual(\n        len([v for v in global_state.get_dtypes(instance._data_id) if v[\"visible\"]]),\n        100,\n    )\n\n    if PY3 and check_pandas_version(\"0.25.0\"):\n        s_int = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"), dtype=pd.Int64Dtype())\n        s2_int = s_int.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"])\n        ints = pd.Series([1, 2, 3, 4, 5], index=list(\"abcfu\"))\n        test_data = pd.DataFrame(dict(na=s2_int, int=ints))\n        test_data.loc[:, \"unsigned_int\"] = pd.to_numeric(\n            test_data[\"int\"], downcast=\"unsigned\"\n        )\n        instance = views.startup(\n            URL, data_loader=lambda: test_data, ignore_duplicate=True\n        )\n\n        unittest.assertEqual(\n            {\n                \"coord\": None,\n                \"dtype\": \"Int64\",\n                \"hasMissing\": 2,\n                \"hasOutliers\": 0,\n                \"index\": 1,\n                \"kurt\": \"nan\",\n                \"lowVariance\": False,\n                \"max\": 3,\n                \"min\": 1,\n                \"name\": \"na\",\n                \"skew\": 0.0,\n                \"unique_ct\": 3,\n                \"visible\": True,\n                \"outlierRange\": {\"lower\": 0.0, \"upper\": 4.0},\n            },\n            global_state.get_dtypes(instance._data_id)[1],\n        )\n\n        unittest.assertEqual(\n            {\n                \"dtype\": \"uint8\",\n                \"hasMissing\": 0,\n                \"hasOutliers\": 0,\n                \"index\": 3,\n                \"name\": \"unsigned_int\",\n                \"unique_ct\": 5,\n                \"visible\": True,\n            },\n            global_state.get_dtypes(instance._data_id)[-1],\n        )\n\n\n@pytest.mark.unit\ndef test_formatting_complex_data(unittest):\n    from dtale.views import format_data\n\n    data = [[1, 2, 3], {1: \"a\", 2: \"b\", 3: \"c\"}, [1]]\n    df, _ = format_data(pd.DataFrame({\"foo\": data}))\n    unittest.assertEqual(\n        list(df[\"foo\"].values), [\"[1, 2, 3]\", \"{1: 'a', 2: 'b', 3: 'c'}\", \"[1]\"]\n    )\n\n    data = [1, 2, [3]]\n    df, _ = format_data(pd.DataFrame({\"foo\": data}))\n    unittest.assertEqual(list(df[\"foo\"].values), [\"1\", \"2\", \"[3]\"])\n\n    index_vals = [\n        pd.Timestamp(\"20230101\"),\n        pd.Timestamp(\"20230102\"),\n        pd.Timestamp(\"20230103\"),\n    ]\n    base_df = pd.DataFrame([1, 2, 3], index=index_vals)\n    df, index = format_data(base_df)\n    unittest.assertEqual(index, [\"index\"])\n    assert len(df.columns) > len(base_df.columns)\n\n\n@pytest.mark.unit\ndef test_in_ipython_frontend(builtin_pkg):\n    import dtale.views as views\n\n    orig_import = __import__\n\n    mock_ipython = mock.Mock()\n\n    class zmq(object):\n        __name__ = \"zmq\"\n\n        def __init__(self):\n            pass\n\n    mock_ipython.get_ipython = lambda: zmq()\n\n    def import_mock(name, *args, **kwargs):\n        if name == \"IPython\":\n            return mock_ipython\n        return orig_import(name, *args, **kwargs)\n\n    with ExitStack() as stack:\n        stack.enter_context(\n            mock.patch(\"{}.__import__\".format(builtin_pkg), side_effect=import_mock)\n        )\n        assert views.in_ipython_frontend()\n\n    def import_mock(name, *args, **kwargs):\n        if name == \"IPython\":\n            raise ImportError()\n        return orig_import(name, *args, **kwargs)\n\n    with ExitStack() as stack:\n        stack.enter_context(\n            mock.patch(\"{}.__import__\".format(builtin_pkg), side_effect=import_mock)\n        )\n        assert not views.in_ipython_frontend()\n\n\n@pytest.mark.unit\ndef test_shutdown(unittest):\n    import werkzeug\n\n    with app.test_client() as c:\n        try:\n            with ExitStack() as stack:\n                from werkzeug.serving import BaseWSGIServer\n\n                base_server = mock.Mock(spec=BaseWSGIServer)\n                base_server.shutdown = mock.Mock()\n                gc_objects = [base_server]\n                mock_gc = stack.enter_context(\n                    mock.patch(\"gc.get_objects\", mock.Mock(return_value=gc_objects))\n                )\n                resp = c.get(\"/shutdown\").data\n                assert \"Server shutting down...\" in str(resp)\n                mock_gc.assert_called()\n                base_server.shutdown.assert_called()\n            unittest.fail()\n        except:  # noqa\n            pass\n        if parse_version(werkzeug.__version__) < parse_version(\"2.1.0\"):\n            mock_shutdown = mock.Mock()\n            resp = c.get(\n                \"/shutdown\", environ_base={\"werkzeug.server.shutdown\": mock_shutdown}\n            ).data\n            assert \"Server shutting down...\" in str(resp)\n            mock_shutdown.assert_called()\n\n\n@pytest.mark.unit\ndef test_get_send_file_max_age():\n    with app.app_context():\n        assert app.get_send_file_max_age(\"test\") in [43200, None]\n        assert 60 == app.get_send_file_max_age(\"dist/test.js\")\n\n\n@pytest.mark.unit\ndef test_processes(test_data, unittest):\n    global_state.clear_store()\n    from dtale.views import build_dtypes_state\n\n    now = pd.Timestamp(\"20180430 12:36:44\").tz_localize(\"US/Eastern\")\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: build_dtypes_state(test_data)})\n        global_state.set_metadata(c.port, dict(start=now))\n        global_state.set_name(c.port, \"foo\")\n\n        response = c.get(\"/dtale/processes\")\n        response_data = response.get_json()\n        unittest.assertDictContainsSubset(\n            {\n                \"rows\": 50,\n                \"name\": \"foo\",\n                \"ts\": 1525106204000,\n                \"start\": \"12:36:44 PM\",\n                \"names\": \"date,security_id,foo,bar,baz\",\n                \"data_id\": str(c.port),\n                \"columns\": 5,\n                # \"mem_usage\": 4600 if PY3 else 4000,\n            },\n            response_data[\"data\"][0],\n        )\n\n        response = c.get(\"/dtale/process-keys\")\n        response_data = response.get_json()\n        assert response_data[\"data\"][0][\"id\"] == str(c.port)\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: build_dtypes_state(test_data)})\n        global_state.set_metadata(c.port, {})\n        response = c.get(\"/dtale/processes\")\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n\n@pytest.mark.unit\ndef test_update_settings(test_data, unittest):\n    settings = json.dumps(dict(locked=[\"a\", \"b\"]))\n\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            global_state.set_data(c.port, test_data)\n            mock_render_template = stack.enter_context(\n                mock.patch(\n                    \"dtale.views.render_template\",\n                    mock.Mock(return_value=json.dumps(dict(success=True))),\n                )\n            )\n            response = c.get(\n                \"/dtale/update-settings/{}\".format(c.port),\n                query_string=dict(settings=settings),\n            )\n            assert response.status_code == 200, \"should return 200 response\"\n\n            c.get(\"/dtale/main/{}\".format(c.port))\n            _, kwargs = mock_render_template.call_args\n            unittest.assertEqual(\n                kwargs[\"settings\"], settings, \"settings should be retrieved\"\n            )\n\n    settings = \"a\"\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            global_state.set_data(c.port, None)\n            response = c.get(\n                \"/dtale/update-settings/{}\".format(c.port),\n                query_string=dict(settings=settings),\n            )\n            assert response.status_code == 200, \"should return 200 response\"\n            response_data = response.get_json()\n            assert \"error\" in response_data\n\n\n@pytest.mark.unit\ndef test_update_formats():\n    from dtale.views import build_dtypes_state\n\n    settings = dict()\n    df = pd.DataFrame([dict(a=1, b=2)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        build_settings(settings)\n        response = c.get(\n            \"/dtale/update-formats/{}\".format(c.port),\n            query_string=dict(\n                all=False, col=\"a\", format=json.dumps(dict(fmt=\"\", style={}))\n            ),\n        )\n        assert response.status_code == 200, \"should return 200 response\"\n        assert \"a\" in global_state.get_settings(c.port)[\"columnFormats\"]\n\n        c.get(\n            \"/dtale/update-formats/{}\".format(c.port),\n            query_string=dict(\n                all=True,\n                col=\"a\",\n                format=json.dumps(dict(fmt=\"\", style={})),\n                nanDisplay=None,\n            ),\n        )\n        assert \"b\" in global_state.get_settings(c.port)[\"columnFormats\"]\n        assert \"nan\" in global_state.get_settings(c.port)[\"nanDisplay\"]\n\n    with app.test_client() as c:\n        global_state.set_dtypes(c.port, None)\n        response = c.get(\n            \"/dtale/update-formats/{}\".format(c.port),\n            query_string=dict(\n                all=True, col=\"a\", format=json.dumps(dict(fmt=\"\", style={}))\n            ),\n        )\n        assert \"error\" in response.json\n\n\n@pytest.mark.unit\ndef test_update_column_position():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    tests = [\n        (\"front\", 0),\n        (\"front\", 0),\n        (\"left\", 0),\n        (\"back\", -1),\n        (\"back\", -1),\n        (\"left\", -2),\n        (\"right\", -1),\n        (\"right\", -1),\n    ]\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        for action, col_idx in tests:\n            c.get(\n                \"/dtale/update-column-position/{}\".format(c.port),\n                query_string=dict(action=action, col=\"c\"),\n            )\n            assert global_state.get_data(c.port).columns[col_idx] == \"c\"\n            assert global_state.get_dtypes(c.port)[col_idx][\"name\"] == \"c\"\n\n        resp = c.get(\"/dtale/update-column-position/-1\")\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_update_locked(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        settings = {c.port: dict(locked=[])}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        build_settings(settings)\n\n        c.get(\n            \"/dtale/update-locked/{}\".format(c.port),\n            query_string=dict(action=\"lock\", col=\"c\"),\n        )\n        unittest.assertEqual([\"c\"], global_state.get_settings(c.port)[\"locked\"])\n        assert global_state.get_data(c.port).columns[0] == \"c\"\n        assert global_state.get_dtypes(c.port)[0][\"name\"] == \"c\"\n\n        c.get(\n            \"/dtale/update-locked/{}\".format(c.port),\n            query_string=dict(action=\"lock\", col=\"c\"),\n        )\n        unittest.assertEqual([\"c\"], global_state.get_settings(c.port)[\"locked\"])\n        assert global_state.get_data(c.port).columns[0] == \"c\"\n        assert global_state.get_dtypes(c.port)[0][\"name\"] == \"c\"\n\n        c.get(\n            \"/dtale/update-locked/{}\".format(c.port),\n            query_string=dict(action=\"unlock\", col=\"c\"),\n        )\n        unittest.assertEqual([], global_state.get_settings(c.port)[\"locked\"])\n        assert global_state.get_data(c.port).columns[0] == \"c\"\n        assert global_state.get_dtypes(c.port)[0][\"name\"] == \"c\"\n\n        resp = c.get(\"/dtale/update-locked/-1\")\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_delete_cols():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        delete_cols = [\"a\", \"b\"]\n        c.get(\n            \"/dtale/delete-col/{}\".format(c.port),\n            query_string=dict(cols=json.dumps(delete_cols)),\n        )\n        assert (\n            len(\n                [\n                    col\n                    for col in global_state.get_data(c.port).columns\n                    if col in delete_cols\n                ]\n            )\n            == 0\n        )\n        assert (\n            next(\n                (\n                    dt\n                    for dt in global_state.get_dtypes(c.port)\n                    if dt[\"name\"] in delete_cols\n                ),\n                None,\n            )\n            is None\n        )\n        assert len(global_state.get_settings(c.port)[\"locked\"]) == 0\n\n        resp = c.get(\"/dtale/delete-col/-1\", query_string=dict(cols=json.dumps([\"d\"])))\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_duplicate_cols(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n\n        resp = c.get(\n            \"/dtale/duplicate-col/{}\".format(c.port),\n            query_string=dict(col=\"b\"),\n        )\n        unittest.assertEquals(\n            list(global_state.get_data(c.port).columns), [\"a\", \"b\", \"b_2\", \"c\"]\n        )\n        assert resp.json[\"col\"] == \"b_2\"\n\n\n@pytest.mark.unit\ndef test_rename_col():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        c.get(\n            \"/dtale/rename-col/{}\".format(c.port),\n            query_string=dict(col=\"a\", rename=\"d\"),\n        )\n        assert \"a\" not in global_state.get_data(c.port).columns\n        assert (\n            next(\n                (dt for dt in global_state.get_dtypes(c.port) if dt[\"name\"] == \"a\"),\n                None,\n            )\n            is None\n        )\n        assert len(global_state.get_settings(c.port)[\"locked\"]) == 1\n\n        resp = c.get(\n            \"/dtale/rename-col/{}\".format(c.port),\n            query_string=dict(col=\"d\", rename=\"b\"),\n        )\n        assert \"error\" in json.loads(resp.data)\n\n        resp = c.get(\"/dtale/rename-col/-1\", query_string=dict(col=\"d\", rename=\"b\"))\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_outliers(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame.from_dict(\n        {\n            \"a\": [1, 2, 3, 4, 1000, 5, 3, 5, 55, 12, 13, 10000, 221, 12, 2000],\n            \"b\": list(range(15)),\n            \"c\": [\"a\"] * 15,\n        }\n    )\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"a\"))\n        resp = json.loads(resp.data)\n        unittest.assertEqual(resp[\"outliers\"], [1000, 2000, 10000])\n        c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"a\", cfg=json.dumps(dict(type=\"outliers\", query=resp[\"query\"]))\n            ),\n        )\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"a\"))\n        resp = json.loads(resp.data)\n        assert resp[\"queryApplied\"]\n        c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(col=\"a\", cfg=json.dumps(dict(type=\"outliers\"))),\n        )\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"a\"))\n        resp = json.loads(resp.data)\n        assert not resp[\"queryApplied\"]\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"b\"))\n        resp = json.loads(resp.data)\n        unittest.assertEqual(resp[\"outliers\"], [])\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"c\"))\n        resp = json.loads(resp.data)\n        assert resp[\"outliers\"] == []\n\n\n@pytest.mark.unit\ndef test_toggle_outlier_filter(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame.from_dict(\n        {\n            \"a\": [1, 2, 3, 4, 1000, 5, 3, 5, 55, 12, 13, 10000, 221, 12, 2000],\n            \"b\": list(range(15)),\n            \"c\": [\"a\"] * 15,\n        }\n    )\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        resp = c.get(\n            \"/dtale/toggle-outlier-filter/{}\".format(c.port), query_string=dict(col=\"a\")\n        )\n        resp = resp.get_json()\n        assert resp[\"outlierFilters\"][\"a\"][\"query\"] == \"`a` > 339.75\"\n        assert (\n            global_state.get_settings(c.port)[\"outlierFilters\"][\"a\"][\"query\"]\n            == \"`a` > 339.75\"\n        )\n        resp = c.get(\n            \"/dtale/toggle-outlier-filter/{}\".format(c.port), query_string=dict(col=\"a\")\n        )\n        resp = resp.get_json()\n        assert \"a\" not in resp[\"outlierFilters\"]\n        assert \"a\" not in global_state.get_settings(c.port)[\"outlierFilters\"]\n\n\n@pytest.mark.unit\ndef test_update_visibility(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst({c.port: df})\n        build_dtypes(dtypes)\n        c.post(\n            \"/dtale/update-visibility/{}\".format(c.port),\n            data=json.dumps(\n                dict(visibility=json.dumps({\"a\": True, \"b\": True, \"c\": False}))\n            ),\n            content_type=\"application/json\",\n        )\n        unittest.assertEqual(\n            [True, True, False],\n            [col[\"visible\"] for col in global_state.get_dtypes(c.port)],\n        )\n        c.post(\n            \"/dtale/update-visibility/{}\".format(c.port),\n            data=json.dumps(dict(toggle=\"c\")),\n            content_type=\"application/json\",\n        )\n        unittest.assertEqual(\n            [True, True, True],\n            [col[\"visible\"] for col in global_state.get_dtypes(c.port)],\n        )\n\n        resp = c.post(\n            \"/dtale/update-visibility/-1\",\n            data=json.dumps(dict(toggle=\"foo\")),\n            content_type=\"application/json\",\n        )\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_build_column():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3, d=pd.Timestamp(\"20200101\"))])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        resp = c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"not_implemented\", name=\"test\", cfg=json.dumps({})),\n        )\n        response_data = json.loads(resp.data)\n        assert (\n            response_data[\"error\"]\n            == \"'not_implemented' column builder not implemented yet!\"\n        )\n\n        resp = c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", cfg=json.dumps({})),\n        )\n        response_data = json.loads(resp.data)\n        assert response_data[\"error\"] == \"'name' is required for new column!\"\n\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"sum\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"sum\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"sum\"].values[0] == 3\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"sum\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int64\"\n\n        resp = c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"sum\", cfg=json.dumps(cfg)),\n        )\n        response_data = json.loads(resp.data)\n        assert response_data[\"error\"] == \"A column named 'sum' already exists!\"\n\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"difference\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"diff\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"diff\"].values[0] == -1\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"diff\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int64\"\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"multiply\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"mult\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"mult\"].values[0] == 2\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"mult\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int64\"\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"divide\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"div\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"div\"].values[0] == 0.5\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"div\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"float64\"\n        cfg = dict(left=dict(col=\"a\"), right=dict(val=100), operation=\"divide\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"div2\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"div2\"].values[0] == 0.01\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"div2\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"float64\"\n        cfg = dict(left=dict(val=100), right=dict(col=\"b\"), operation=\"divide\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"div3\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"div3\"].values[0] == 50\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"div3\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"float64\"\n\n        cfg = dict(col=\"d\", property=\"weekday\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"datetime\", name=\"datep\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"datep\"].values[0] == 2\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"datep\"\n        assert (\n            global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int32\"\n            if pandas_util.is_pandas2()\n            else \"int64\"\n        )\n\n        for p in [\"minute\", \"hour\", \"time\", \"date\", \"month\", \"quarter\", \"year\"]:\n            c.get(\n                \"/dtale/build-column/{}\".format(c.port),\n                query_string=dict(\n                    type=\"datetime\", name=p, cfg=json.dumps(dict(col=\"d\", property=p))\n                ),\n            )\n\n        cfg = dict(col=\"d\", conversion=\"month_end\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"datetime\", name=\"month_end\", cfg=json.dumps(cfg)),\n        )\n        assert (\n            pd.Timestamp(data[c.port][\"month_end\"].values[0]).strftime(\"%Y%m%d\")\n            == \"20200131\"\n        )\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"month_end\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"datetime64[ns]\"\n\n        for conv in [\n            \"month_start\",\n            \"quarter_start\",\n            \"quarter_end\",\n            \"year_start\",\n            \"year_end\",\n        ]:\n            c.get(\n                \"/dtale/build-column/{}\".format(c.port),\n                query_string=dict(\n                    type=\"datetime\",\n                    name=conv,\n                    cfg=json.dumps(dict(col=\"d\", conversion=conv)),\n                ),\n            )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        for dt in [\"float\", \"int\", \"string\", \"date\", \"bool\", \"choice\"]:\n            c.get(\n                \"/dtale/build-column/{}\".format(c.port),\n                query_string=dict(\n                    type=\"random\",\n                    name=\"random_{}\".format(dt),\n                    cfg=json.dumps(dict(type=dt)),\n                ),\n            )\n\n        cfg = dict(type=\"string\", chars=\"ABCD\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(\n                type=\"random\", name=\"random_string2\", cfg=json.dumps(cfg)\n            ),\n        )\n\n        cfg = dict(type=\"date\", timestamps=True, businessDay=True)\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"random\", name=\"random_date2\", cfg=json.dumps(cfg)),\n        )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n\n@pytest.mark.unit\ndef test_build_column_apply_all():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3, d=pd.Timestamp(\"20200101\"))])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        cfg = {\"col\": \"a\", \"to\": \"float\", \"from\": \"int64\", \"applyAllType\": True}\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"type_conversion\", name=\"test\", cfg=json.dumps(cfg)),\n        )\n        assert all(\n            data[c.port].dtypes[col].name == \"float64\" for col in [\"a\", \"b\", \"c\"]\n        )\n\n\n@pytest.mark.unit\ndef test_build_column_bins():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame(np.random.randn(100, 3), columns=[\"a\", \"b\", \"c\"])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        cfg = dict(col=\"a\", operation=\"cut\", bins=4)\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert len(resp[\"labels\"]) == 4\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"cut\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"cut\"].values[0] is not None\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"cut\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n        cfg = dict(col=\"a\", operation=\"cut\", bins=4, labels=\"foo,bar,biz,baz\")\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert resp[\"labels\"] == [\"foo\", \"bar\", \"biz\", \"baz\"]\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"cut2\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"cut2\"].values[0] in [\n            \"foo\",\n            \"bar\",\n            \"biz\",\n            \"baz\",\n        ]\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"cut2\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n        cfg = dict(col=\"a\", operation=\"qcut\", bins=4)\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert len(resp[\"labels\"]) == 4\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"qcut\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"qcut\"].values[0] is not None\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"qcut\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n        cfg = dict(col=\"a\", operation=\"qcut\", bins=4, labels=\"foo,bar,biz,baz\")\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert resp[\"labels\"] == [\"foo\", \"bar\", \"biz\", \"baz\"]\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"qcut2\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"qcut2\"].values[0] in [\n            \"foo\",\n            \"bar\",\n            \"biz\",\n            \"baz\",\n        ]\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"qcut2\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n\n@pytest.mark.unit\ndef test_cleanup_error(unittest):\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.cleanup\", mock.Mock(side_effect=Exception)\n                )\n            )\n            resp = c.get(\"/dtale/cleanup-datasets\", query_string=dict(dataIds=\"1\"))\n            assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_dtypes(test_data):\n    from dtale.views import build_dtypes_state, format_data\n\n    test_data = test_data.copy()\n    test_data.loc[:, \"mixed_col\"] = 1\n    test_data.loc[0, \"mixed_col\"] = \"x\"\n\n    with app.test_client() as c:\n        with ExitStack():\n            build_data_inst({c.port: test_data})\n            build_dtypes({c.port: build_dtypes_state(test_data)})\n            response = c.get(\"/dtale/dtypes/{}\".format(c.port))\n            response_data = response.get_json()\n            assert response_data[\"success\"]\n\n            for col in test_data.columns:\n                response = c.get(\n                    \"/dtale/describe/{}\".format(c.port), query_string=dict(col=col)\n                )\n                response_data = response.get_json()\n                assert response_data[\"success\"]\n\n    lots_of_groups = pd.DataFrame([dict(a=i, b=1) for i in range(150)])\n    with app.test_client() as c:\n        with ExitStack():\n            build_data_inst({c.port: lots_of_groups})\n            build_dtypes({c.port: build_dtypes_state(lots_of_groups)})\n            response = c.get(\"/dtale/dtypes/{}\".format(c.port))\n            response_data = response.get_json()\n            assert response_data[\"success\"]\n\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port), query_string=dict(col=\"a\")\n            )\n            response_data = response.get_json()\n            uniq_key = list(response_data[\"uniques\"].keys())[0]\n            assert response_data[\"uniques\"][uniq_key][\"top\"]\n            assert response_data[\"success\"]\n\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.get_dtypes\", side_effect=Exception)\n            )\n            response = c.get(\"/dtale/dtypes/{}\".format(c.port))\n            response_data = response.get_json()\n            assert \"error\" in response_data\n\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port), query_string=dict(col=\"foo\")\n            )\n            response_data = response.get_json()\n            assert \"error\" in response_data\n\n    df = pd.DataFrame(\n        [\n            dict(date=pd.Timestamp(\"now\"), security_id=1, foo=1.0, bar=2.0),\n            dict(date=pd.Timestamp(\"now\"), security_id=1, foo=2.0, bar=np.inf),\n        ],\n        columns=[\"date\", \"security_id\", \"foo\", \"bar\"],\n    )\n    df, _ = format_data(df)\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: build_dtypes_state(df)})\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port), query_string=dict(col=\"bar\")\n            )\n            response_data = response.get_json()\n            assert response_data[\"describe\"][\"min\"] == \"2\"\n            assert response_data[\"describe\"][\"max\"] == \"inf\"\n\n            global_state.set_settings(c.port, dict(query=\"security_id == 1\"))\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port),\n                query_string=dict(col=\"foo\", filtered=\"true\"),\n            )\n            response_data = response.get_json()\n            assert response_data[\"describe\"][\"min\"] == \"1\"\n            assert response_data[\"describe\"][\"max\"] == \"2\"\n\n\n@pytest.mark.unit\ndef test_variance(unittest):\n    from dtale.views import build_dtypes_state, format_data\n\n    global_state.clear_store()\n    with open(\n        os.path.join(os.path.dirname(__file__), \"..\", \"data/test_variance.json\"), \"r\"\n    ) as f:\n        expected = f.read()\n        expected = json.loads(expected)\n\n    def _df():\n        for i in range(2500):\n            yield dict(i=i, x=i % 5)\n\n    df = pd.DataFrame(list(_df()))\n    df.loc[:, \"low_var\"] = 2500\n    df.loc[0, \"low_var\"] = 1\n    df, _ = format_data(df)\n\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        dtypes = build_dtypes_state(df)\n        assert next((dt for dt in dtypes if dt[\"name\"] == \"low_var\"), None)[\n            \"lowVariance\"\n        ]\n        build_dtypes({c.port: dtypes})\n        response = c.get(\n            \"/dtale/variance/{}\".format(c.port), query_string=dict(col=\"x\")\n        )\n        if parse_version(pd.__version__) >= parse_version(\"1.3.0\"):\n            expected[\"x\"][\"check2\"][\"val1\"][\"val\"] = 0\n            expected[\"x\"][\"check2\"][\"val2\"][\"val\"] = 1\n        response_data = response.get_json()\n        del response_data[\"code\"]\n        response_data[\"jarqueBera\"][\"pvalue\"] = round(\n            response_data[\"jarqueBera\"][\"pvalue\"], 4\n        )\n        response_data[\"jarqueBera\"][\"statistic\"] = round(\n            response_data[\"jarqueBera\"][\"statistic\"], 4\n        )\n        unittest.assertEqual(response_data, expected[\"x\"])\n\n        response = c.get(\n            \"/dtale/variance/{}\".format(c.port), query_string=dict(col=\"low_var\")\n        )\n        response_data = response.get_json()\n        del response_data[\"code\"]\n        response_data[\"shapiroWilk\"][\"statistic\"] = round(\n            response_data[\"shapiroWilk\"][\"statistic\"], 4\n        )\n        response_data[\"jarqueBera\"][\"statistic\"] = round(\n            response_data[\"jarqueBera\"][\"statistic\"], 4\n        )\n        unittest.assertEqual(response_data, expected[\"low_var\"])\n\n\n@pytest.mark.unit\ndef test_test_filter(test_data):\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"date == date\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port), query_string=dict(query=\"foo == 1\")\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"date == '20000101'\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"baz == 'baz'\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port), query_string=dict(query=\"bar > 1.5\")\n        )\n        response_data = response.get_json()\n        assert not response_data[\"success\"]\n        assert response_data[\"error\"] == 'query \"bar > 1.5\" found no data, please alter'\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port), query_string=dict(query=\"foo2 == 1\")\n        )\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=None, save=\"true\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n    if PY3:\n        df = pd.DataFrame([dict(a=1)])\n        df[\"a.b\"] = 2\n        with app.test_client() as c:\n            build_data_inst({c.port: df})\n            response = c.get(\n                \"/dtale/test-filter/{}\".format(c.port),\n                query_string=dict(query=\"a.b == 2\"),\n            )\n            response_data = response.get_json()\n            assert not response_data[\"success\"]\n\n\n@pytest.mark.unit\ndef test_get_data(unittest, test_data):\n    import dtale.views as views\n    import dtale.global_state as global_state\n\n    with app.test_client() as c:\n        test_data, _ = views.format_data(test_data)\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        response = c.get(\"/dtale/data/{}\".format(c.port))\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data, {}, 'if no \"ids\" parameter an empty dict should be returned'\n        )\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1\"]))\n        )\n        response_data = response.get_json()\n        expected = dict(\n            total=50,\n            final_query=\"\",\n            results={\n                \"1\": dict(\n                    date=\"2000-01-01\",\n                    security_id=1,\n                    dtale_index=1,\n                    foo=1,\n                    bar=1.5,\n                    baz=\"baz\",\n                )\n            },\n            columns=[\n                dict(dtype=\"int64\", name=\"dtale_index\", visible=True),\n                dict(\n                    dtype=\"datetime64[ns]\",\n                    name=\"date\",\n                    index=0,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    unique_ct=1,\n                    kurt=0,\n                    skew=0,\n                ),\n                dict(\n                    dtype=\"int64\",\n                    name=\"security_id\",\n                    max=49,\n                    min=0,\n                    index=1,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    lowVariance=False,\n                    outlierRange={\"lower\": -24.5, \"upper\": 73.5},\n                    unique_ct=50,\n                    kurt=-1.2,\n                    skew=0,\n                    coord=None,\n                ),\n                dict(\n                    dtype=\"int64\",\n                    name=\"foo\",\n                    min=1,\n                    max=1,\n                    index=2,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    lowVariance=False,\n                    outlierRange={\"lower\": 1.0, \"upper\": 1.0},\n                    unique_ct=1,\n                    kurt=0,\n                    skew=0,\n                    coord=None,\n                ),\n                dict(\n                    dtype=\"float64\",\n                    name=\"bar\",\n                    min=1.5,\n                    max=1.5,\n                    index=3,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    lowVariance=False,\n                    outlierRange={\"lower\": 1.5, \"upper\": 1.5},\n                    unique_ct=1,\n                    kurt=0,\n                    skew=0,\n                    coord=None,\n                ),\n                dict(\n                    dtype=\"string\",\n                    name=\"baz\",\n                    index=4,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    unique_ct=1,\n                ),\n            ],\n        )\n        unittest.assertEqual(response_data, expected, \"should return data at index 1\")\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1-2\"]))\n        )\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            ),\n            \"2\": dict(\n                date=\"2000-01-01\",\n                security_id=2,\n                dtale_index=2,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            ),\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at indexes 1-2\"\n        )\n\n        params = dict(ids=json.dumps([\"1\"]), sort=json.dumps([[\"security_id\", \"DESC\"]]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=48,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at index 1 w/ sort\"\n        )\n        unittest.assertEqual(\n            global_state.get_settings(c.port),\n            {\"sortInfo\": [[\"security_id\", \"DESC\"]]},\n            \"should update settings\",\n        )\n\n        params = dict(ids=json.dumps([\"1\"]), sort=json.dumps([[\"security_id\", \"ASC\"]]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at index 1 w/ sort\"\n        )\n        unittest.assertEqual(\n            global_state.get_settings(c.port),\n            {\"sortInfo\": [[\"security_id\", \"ASC\"]]},\n            \"should update settings\",\n        )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"security_id == 1\", save=\"true\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n        unittest.assertEqual(\n            global_state.get_settings(c.port)[\"query\"],\n            \"security_id == 1\",\n            \"should update settings\",\n        )\n\n        params = dict(ids=json.dumps([\"0\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"0\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=0,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at index 1 w/ sort\"\n        )\n\n        global_state.get_settings(c.port)[\"highlightFilter\"] = True\n        params = dict(ids=json.dumps([\"1\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n                __filtered=True,\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"],\n            expected,\n            \"should return data at index 1 w/ filtered flag for highlighting\",\n        )\n\n        params = dict(ids=json.dumps([\"1-2\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n                __filtered=True,\n            ),\n            \"2\": dict(\n                date=\"2000-01-01\",\n                security_id=2,\n                dtale_index=2,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            ),\n        }\n        unittest.assertEqual(\n            response_data[\"results\"],\n            expected,\n            \"should return data at indexes 1-2 w/ filtered flag for highlighting\",\n        )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        global_state.get_settings(c.port)[\"query\"] = \"security_id == 50\"\n        global_state.get_settings(c.port)[\"highlightFilter\"] = False\n        params = dict(ids=json.dumps([\"0\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert len(response_data[\"results\"]) == 0\n\n        global_state.get_settings(c.port)[\"invertFilter\"] = True\n        params = dict(ids=json.dumps([\"0\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert len(response_data[\"results\"]) == 1\n\n        global_state.get_settings(c.port)[\"invertFilter\"] = False\n\n        response = c.get(\"/dtale/data-export/{}\".format(c.port))\n        assert response.content_type == \"text/csv\"\n\n        response = c.get(\n            \"/dtale/data-export/{}\".format(c.port), query_string=dict(type=\"tsv\")\n        )\n        assert response.content_type == \"text/tsv\"\n\n        response = c.get(\"/dtale/data-export/a\", query_string=dict(type=\"tsv\"))\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        build_settings({c.port: dict(query=\"missing_col == 'blah'\")})\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"0\"]))\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            \"name 'missing_col' is not defined\",\n            \"should handle data exception\",\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1\"]))\n        )\n        assert response.status_code == 200\n\n        tmp = global_state.get_data(c.port).copy()\n        tmp[\"biz\"] = 2.5\n        global_state.set_data(c.port, tmp)\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1\"]))\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"results\"],\n            {\n                \"1\": dict(\n                    date=\"2000-01-01\",\n                    security_id=1,\n                    dtale_index=1,\n                    foo=1,\n                    bar=1.5,\n                    baz=\"baz\",\n                    biz=2.5,\n                )\n            },\n            \"should handle data updates\",\n        )\n        unittest.assertEqual(\n            global_state.get_dtypes(c.port)[-1],\n            dict(\n                index=5,\n                name=\"biz\",\n                dtype=\"float64\",\n                min=2.5,\n                max=2.5,\n                visible=True,\n                hasMissing=0,\n                hasOutliers=0,\n                lowVariance=False,\n                outlierRange={\"lower\": 2.5, \"upper\": 2.5},\n                unique_ct=1,\n                kurt=0,\n                skew=0,\n                coord=None,\n            ),\n            \"should update dtypes on data structure change\",\n        )\n\n\n@pytest.mark.unit\ndef test_export_html(unittest, test_data):\n    import dtale.views as views\n\n    with app.test_client() as c:\n        test_data, _ = views.format_data(test_data)\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        response = c.get(\"/dtale/data/{}\".format(c.port))\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data, {}, 'if no \"ids\" parameter an empty dict should be returned'\n        )\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(export=True)\n        )\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port),\n            query_string=dict(export=True, export_rows=5),\n        )\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\ndef test_export_parquet(test_data):\n    pytest.importorskip(\"pyarrow\")\n\n    import dtale.views as views\n\n    with app.test_client() as c:\n        test_data, _ = views.format_data(test_data)\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n\n        response = c.get(\n            \"/dtale/data-export/{}\".format(c.port), query_string=dict(type=\"parquet\")\n        )\n        assert response.content_type == \"application/octet-stream\"\n\n\ndef build_ts_data(size=5, days=5):\n    start = pd.Timestamp(\"20000101\")\n    for d in pd.date_range(start, start + Day(days - 1)):\n        for i in range(size):\n            yield dict(date=d, security_id=i, foo=i, bar=i)\n\n\n@pytest.mark.unit\ndef test_get_chart_data(unittest, rolling_data):\n    import dtale.views as views\n\n    test_data = pd.DataFrame(\n        build_ts_data(size=50), columns=[\"date\", \"security_id\", \"foo\", \"bar\"]\n    )\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(x=\"date\", y=json.dumps([\"security_id\"]), agg=\"count\")\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"data\": {\n                \"all\": {\n                    \"x\": [\n                        \"2000-01-01\",\n                        \"2000-01-02\",\n                        \"2000-01-03\",\n                        \"2000-01-04\",\n                        \"2000-01-05\",\n                    ],\n                    build_col_def(\"count\", \"security_id\"): [50, 50, 50, 50, 50],\n                }\n            },\n            \"max\": {build_col_def(\"count\", \"security_id\"): 50, \"x\": \"2000-01-05\"},\n            \"min\": {build_col_def(\"count\", \"security_id\"): 50, \"x\": \"2000-01-01\"},\n            \"success\": True,\n        }\n        unittest.assertEqual(\n            {k: v for k, v in response_data.items() if k != \"code\"},\n            expected,\n            \"should return chart data\",\n        )\n\n    test_data.loc[:, \"baz\"] = \"baz\"\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(\n            x=\"date\",\n            y=json.dumps([\"security_id\"]),\n            group=json.dumps([\"baz\"]),\n            agg=\"mean\",\n        )\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert response_data[\"min\"][build_col_def(\"mean\", \"security_id\")] == 24.5\n        assert response_data[\"max\"][build_col_def(\"mean\", \"security_id\")] == 24.5\n        series_key = \"(baz: baz)\"\n        assert response_data[\"data\"][series_key][\"x\"][-1] == \"2000-01-05\"\n        assert (\n            len(response_data[\"data\"][series_key][build_col_def(\"mean\", \"security_id\")])\n            == 5\n        )\n        assert (\n            sum(response_data[\"data\"][series_key][build_col_def(\"mean\", \"security_id\")])\n            == 122.5\n        )\n\n    df, _ = views.format_data(rolling_data)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(\n            x=\"date\",\n            y=json.dumps([\"0\"]),\n            agg=\"rolling\",\n            rollingWin=10,\n            rollingComp=\"count\",\n        )\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(x=\"baz\", y=json.dumps([\"foo\"]))\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert response_data[\"error\"] == (\n            \"The grouping [baz] contains duplicates, please specify group or additional filtering or select \"\n            \"'No Aggregation' from Aggregation drop-down.\"\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(\n            x=\"date\", y=json.dumps([\"foo\"]), group=json.dumps([\"security_id\"])\n        )\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert \"Group (security_id) contains more than 30 unique values\" in str(\n            response_data[\"error\"]\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        response = c.get(\n            \"/dtale/chart-data/{}\".format(c.port),\n            query_string=dict(query=\"missing_col == 'blah'\"),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            \"name 'missing_col' is not defined\",\n            \"should handle data exception\",\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        response = c.get(\n            \"/dtale/chart-data/{}\".format(c.port),\n            query_string=dict(query=\"security_id == 51\"),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            'query \"security_id == 51\" found no data, please alter',\n        )\n\n    df = pd.DataFrame([dict(a=i, b=np.nan) for i in range(100)])\n    df, _ = views.format_data(df)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(x=\"a\", y=json.dumps([\"b\"]), allowDupes=True)\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        unittest.assertEqual(response_data[\"error\"], 'All data for column \"b\" is NaN!')\n\n    df = pd.DataFrame([dict(a=i, b=i) for i in range(15500)])\n    df, _ = views.format_data(df)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(x=\"a\", y=json.dumps([\"b\"]), allowDupes=True)\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            \"Dataset exceeds 15000 records, cannot render. Please apply filter...\",\n        )\n\n\n@pytest.mark.unit\ndef test_code_export():\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.views.build_code_export\", mock.Mock(side_effect=Exception())\n                )\n            )\n            response = c.get(\"/dtale/code-export/{}\".format(c.port))\n            assert \"error\" in response.get_json()\n\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_data\",\n                    mock.Mock(\n                        return_value={c.port: pd.DataFrame([dict(a=1), dict(a=2)])}\n                    ),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_settings\",\n                    mock.Mock(return_value={c.port: {\"query\": \"a in @a\"}}),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_context_variables\",\n                    mock.Mock(return_value={c.port: {\"a\": [1, 2]}}),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.views.build_code_export\", mock.Mock(side_effect=Exception())\n                )\n            )\n            response = c.get(\"/dtale/code-export/{}\".format(c.port))\n            assert \"error\" in response.get_json()\n\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_data\",\n                    mock.Mock(\n                        return_value={c.port: pd.DataFrame([dict(a=1), dict(a=2)])}\n                    ),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_settings\",\n                    mock.Mock(return_value={c.port: {\"query\": \"a == 1\"}}),\n                )\n            )\n            response = c.get(\"/dtale/code-export/{}\".format(c.port))\n            assert response.get_json()[\"success\"]\n\n\n@pytest.mark.unit\ndef test_version_info():\n    with app.test_client() as c:\n        with mock.patch(\n            \"dtale.cli.clickutils.pkg_resources.get_distribution\",\n            mock.Mock(side_effect=Exception(\"blah\")),\n        ):\n            response = c.get(\"version-info\")\n            assert \"unknown\" in str(response.data)\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_chart_exports(custom_data, state_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: custom_data})\n        build_dtypes({c.port: views.build_dtypes_state(custom_data)})\n        params = dict(chart_type=\"invalid\")\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"application/json\"\n\n        params = dict(\n            chart_type=\"line\",\n            x=\"date\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"Col5 == 50\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"application/json\"\n\n        params = dict(chart_type=\"bar\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(chart_type=\"line\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(chart_type=\"scatter\", x=\"Col0\", y=json.dumps([\"Col1\"]))\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        params[\"trendline\"] = \"ols\"\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"3d_scatter\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"surface\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"pie\",\n            x=\"security_id\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"security_id >= 100000 and security_id <= 100010\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"heatmap\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        del params[\"x\"]\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"application/json\"\n\n    with app.test_client() as c:\n        build_data_inst({c.port: state_data})\n        build_dtypes({c.port: views.build_dtypes_state(state_data)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"choropleth\",\n            loc_mode=\"USA-states\",\n            loc=\"Code\",\n            map_val=\"val\",\n            agg=\"raw\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n    df = pd.DataFrame(\n        {\n            \"lat\": np.random.uniform(-40, 40, 50),\n            \"lon\": np.random.uniform(-40, 40, 50),\n            \"val\": np.random.randint(0, high=100, size=50),\n        }\n    )\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"scattergeo\",\n            lat=\"lat\",\n            lon=\"lon\",\n            map_val=\"val\",\n            scope=\"world\",\n            agg=\"raw\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n\n@pytest.mark.skipif(not PY3, reason=\"requires python 3 or higher\")\ndef test_chart_exports_funnel(treemap_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: treemap_data})\n        build_dtypes({c.port: views.build_dtypes_state(treemap_data)})\n\n        params = dict(\n            chart_type=\"funnel\",\n            agg=\"mean\",\n            funnel_value=\"volume\",\n            funnel_label=\"label\",\n            funnel_group=json.dumps([\"group\"]),\n            funnel_stacked=False,\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.skipif(not PY3, reason=\"requires python 3 or higher\")\ndef test_chart_exports_clustergram(clustergram_data):\n    pytest.importorskip(\"dash_bio\")\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        df, _ = views.format_data(clustergram_data)\n        build_data_inst({c.port: df})\n        global_state.set_dtypes(c.port, views.build_dtypes_state(df))\n\n        params = dict(\n            chart_type=\"clustergram\",\n            clustergram_value=json.dumps([\"_all_columns_\"]),\n            clustergram_label=\"model\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\ndef test_chart_exports_pareto(pareto_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        df, _ = views.format_data(pareto_data)\n        build_data_inst({c.port: df})\n        global_state.set_dtypes(c.port, views.build_dtypes_state(df))\n\n        params = dict(\n            chart_type=\"pareto\",\n            pareto_x=\"desc\",\n            pareto_bars=\"count\",\n            pareto_line=\"cum_pct\",\n            pareto_dir=\"DESC\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\ndef test_chart_exports_histogram(test_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        df, _ = views.format_data(test_data)\n        build_data_inst({c.port: df})\n        global_state.set_dtypes(c.port, views.build_dtypes_state(df))\n\n        params = dict(\n            chart_type=\"histogram\",\n            histogram_col=\"foo\",\n            histogram_type=\"bins\",\n            histogram_bins=\"5\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_export_all_charts(custom_data, state_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: custom_data})\n        build_dtypes({c.port: views.build_dtypes_state(custom_data)})\n        params = dict(chart_type=\"invalid\")\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"application/json\"\n\n        params = dict(\n            chart_type=\"line\",\n            x=\"date\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"Col5 == 50\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"application/json\"\n\n        params = dict(chart_type=\"bar\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params[\"group\"] = json.dumps([\"bool_val\"])\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(chart_type=\"line\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(chart_type=\"scatter\", x=\"Col0\", y=json.dumps([\"Col1\"]))\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params[\"trendline\"] = \"ols\"\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"3d_scatter\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"surface\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"pie\",\n            x=\"security_id\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"security_id >= 100000 and security_id <= 100010\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"heatmap\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n    with app.test_client() as c:\n        build_data_inst({c.port: state_data})\n        build_dtypes({c.port: views.build_dtypes_state(state_data)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"choropleth\",\n            loc_mode=\"USA-states\",\n            loc=\"Code\",\n            map_val=\"val\",\n            agg=\"raw\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n    df = pd.DataFrame(\n        {\n            \"lat\": np.random.uniform(-40, 40, 50),\n            \"lon\": np.random.uniform(-40, 40, 50),\n            \"val\": np.random.randint(0, high=100, size=50),\n        }\n    )\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"scattergeo\",\n            lat=\"lat\",\n            lon=\"lon\",\n            map_val=\"val\",\n            scope=\"world\",\n            agg=\"raw\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_chart_png_export(custom_data):\n    import dtale.views as views\n\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: custom_data})\n            build_dtypes({c.port: views.build_dtypes_state(custom_data)})\n            write_image_mock = stack.enter_context(\n                mock.patch(\n                    \"dtale.dash_application.charts.write_image\", mock.MagicMock()\n                )\n            )\n\n            params = dict(\n                chart_type=\"heatmap\",\n                x=\"date\",\n                y=json.dumps([\"security_id\"]),\n                z=\"Col0\",\n                export_type=\"png\",\n            )\n            c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n            assert write_image_mock.called\n\n\n@pytest.mark.unit\ndef test_main():\n    import dtale.views as views\n\n    global_state.clear_store()\n    test_data = pd.DataFrame(\n        build_ts_data(), columns=[\"date\", \"security_id\", \"foo\", \"bar\"]\n    )\n    test_data, _ = views.format_data(test_data)\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_name(c.port, \"test_name\")\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/{}\".format(c.port))\n        assert \"<title>D-Tale (test_name)</title>\" in str(response.data)\n        response = c.get(\"/dtale/iframe/{}\".format(c.port))\n        assert \"<title>D-Tale (test_name)</title>\" in str(response.data)\n        response = c.get(\n            \"/dtale/popup/test/{}\".format(c.port), query_string=dict(col=\"foo\")\n        )\n        assert \"<title>D-Tale (test_name) - Test (col: foo)</title>\" in str(\n            response.data\n        )\n        response = c.get(\n            \"/dtale/popup/reshape/{}\".format(c.port), query_string=dict(col=\"foo\")\n        )\n        assert \"<title>D-Tale (test_name) - Summarize Data (col: foo)</title>\" in str(\n            response.data\n        )\n        response = c.get(\n            \"/dtale/popup/filter/{}\".format(c.port), query_string=dict(col=\"foo\")\n        )\n        assert \"<title>D-Tale (test_name) - Custom Filter (col: foo)</title>\" in str(\n            response.data\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/{}\".format(c.port))\n        assert \"<title>D-Tale</title>\" in str(response.data)\n\n\n@pytest.mark.unit\ndef test_view_by_name():\n    import dtale.views as views\n\n    global_state.clear_store()\n    test_data = pd.DataFrame(\n        build_ts_data(), columns=[\"date\", \"security_id\", \"foo\", \"bar\"]\n    )\n    test_data, _ = views.format_data(test_data)\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_name(c.port, \"test_name\")\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/name/{}\".format(\"test_name\"))\n        assert \"<title>D-Tale (test_name)</title>\" in str(response.data)\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_name(c.port, \"test_name2\")\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/name/{}\".format(\"test_name2\"))\n        assert \"<title>D-Tale (test_name2)</title>\" in str(response.data)\n\n\n@pytest.mark.unit\ndef test_200():\n    paths = [\n        \"/dtale/main/{port}\",\n        \"/dtale/iframe/{port}\",\n        \"/dtale/popup/test/{port}\",\n        \"site-map\",\n        \"version-info\",\n        \"health\",\n        \"/dtale/charts/{port}\",\n        \"/dtale/charts/popup/{port}\",\n        \"/dtale/code-popup\",\n        \"/dtale/popup/upload\",\n        \"/missing-js\",\n        \"/dtale/static/images/fire.jpg\",\n        \"/dtale/static/images/projections/miller.png\",\n        \"/dtale/static/images/map_type/choropleth.png\",\n        \"/dtale/static/maps/usa_110m.json\",\n        \"/dtale/network/{port}\",\n        \"/dtale/calculation/skew\",\n        \"/dtale/calculation/kurtosis\",\n    ]\n    with app.test_client() as c:\n        build_data_inst({c.port: None})\n        for path in paths:\n            final_path = path.format(port=c.port)\n            response = c.get(final_path)\n            assert response.status_code == 200, \"{} should return 200 response\".format(\n                final_path\n            )\n    with app.test_client(app_root=\"/test_route\") as c:\n        build_data_inst({c.port: None})\n        for path in paths:\n            final_path = path.format(port=c.port)\n            response = c.get(final_path)\n            assert response.status_code == 200, \"{} should return 200 response\".format(\n                final_path\n            )\n\n\n@pytest.mark.unit\ndef test_302():\n    import dtale.views as views\n\n    df = pd.DataFrame([1, 2, 3])\n    df, _ = views.format_data(df)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        for path in [\n            \"/\",\n            \"/dtale\",\n            \"/dtale/main\",\n            \"/dtale/iframe\",\n            \"/dtale/popup/test\",\n            \"/favicon.ico\",\n            \"/dtale/iframe/popup/upload\",\n            \"/dtale/iframe/popup/describe/{}\".format(c.port),\n        ]:\n            response = c.get(path)\n            assert response.status_code == 302, \"{} should return 302 response\".format(\n                path\n            )\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        for path in [\"/\"]:\n            response = c.get(path)\n            assert response.status_code == 302, \"{} should return 302 response\".format(\n                path\n            )\n\n\n@pytest.mark.unit\ndef test_missing_js():\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: pd.DataFrame([1, 2, 3])})\n            stack.enter_context(mock.patch(\"os.listdir\", mock.Mock(return_value=[])))\n            response = c.get(\"/\")\n            assert response.status_code == 302\n\n\n@pytest.mark.unit\ndef test_404():\n    response = app.test_client().get(\"/dtale/blah\")\n    assert response.status_code == 404\n    # make sure custom 404 page is returned\n    assert (\n        \"The page you were looking for <code>/dtale/blah</code> does not exist.\"\n        in str(response.data)\n    )\n\n\n@pytest.mark.unit\ndef test_500():\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({1: pd.DataFrame([1, 2, 3, 4, 5])})\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.views.render_template\",\n                    mock.Mock(side_effect=Exception(\"Test\")),\n                )\n            )\n            for path in [\"/dtale/main/1\"]:\n                response = c.get(path)\n                assert response.status_code == 500\n                assert \"<h1>Internal Server Error</h1>\" in str(response.data)\n\n\n@pytest.mark.unit\ndef test_jinja_output():\n    import dtale.views as views\n\n    df = pd.DataFrame([1, 2, 3])\n    df, _ = views.format_data(df)\n    url = \"http://localhost.localdomain:40000\"\n    with build_app(url=url).test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n            response = c.get(\"/dtale/main/{}\".format(c.port))\n            assert 'span id=\"forkongithub\"' not in str(response.data)\n            response = c.get(\"/dtale/charts/{}\".format(c.port))\n            assert 'span id=\"forkongithub\"' not in str(response.data)\n\n    with build_app(url=url).test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n            build_settings({c.port: {}})\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", {\"github_fork\": True})\n            )\n            response = c.get(\"/dtale/main/{}\".format(c.port))\n            assert 'span id=\"forkongithub\"' in str(response.data)\n\n\n@pytest.mark.unit\ndef test_build_context_variables():\n    import dtale.views as views\n    import dtale.global_state as global_state\n\n    data_id = \"1\"\n    global_state.new_data_inst(data_id)\n    with pytest.raises(SyntaxError) as error:\n        views.build_context_variables(data_id, {1: \"foo\"})\n    assert \"context variables must be a valid string\" in str(error.value)\n\n    with pytest.raises(SyntaxError) as error:\n        views.build_context_variables(data_id, {\"#!f_o#o\": \"bar\"})\n    assert \"context variables can only contain letters, digits, or underscores\" in str(\n        error.value\n    )\n\n    with pytest.raises(SyntaxError) as error:\n        views.build_context_variables(data_id, {\"_foo\": \"bar\"})\n    assert \"context variables can not start with an underscore\" in str(error.value)\n\n    # verify that pre-existing variables are not dropped when new ones are added\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"1\": \"cat\"})\n    )\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"2\": \"dog\"})\n    )\n    assert (global_state.get_context_variables(data_id)[\"1\"] == \"cat\") & (\n        global_state.get_context_variables(data_id)[\"2\"] == \"dog\"\n    )\n    # verify that new values will replace old ones if they share the same name\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"1\": \"cat\"})\n    )\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"1\": \"dog\"})\n    )\n    assert global_state.get_context_variables(data_id)[\"1\"] == \"dog\"\n\n\n@pytest.mark.unit\ndef test_get_filter_info(unittest):\n    with app.test_client() as c:\n        data_id = 1\n        context_vars = {\n            \"1\": [\"cat\", \"dog\"],\n            \"2\": 420346,\n            \"3\": pd.Series(range(1000)),\n            \"4\": \"A\" * 2000,\n        }\n        expected_return_value = [\n            dict(name=k, value=str(v)[:1000]) for k, v in context_vars.items()\n        ]\n        build_data_inst({data_id: None})\n        global_state.set_context_variables(data_id, context_vars)\n        response = c.get(\"/dtale/filter-info/{}\".format(data_id))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n        unittest.assertEqual(\n            response_data[\"contextVars\"], expected_return_value, \"should match expected\"\n        )\n\n    with app.test_client() as c:\n        global_state.set_context_variables(data_id, None)\n        response = c.get(\"/dtale/filter-info/{}\".format(data_id))\n        response_data = response.get_json()\n        unittest.assertEqual(len(response_data[\"contextVars\"]), 0)\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_get_column_filter_data(unittest, custom_data):\n    import dtale.views as views\n\n    df, _ = views.format_data(custom_data)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"bool_val\"),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data,\n            {\"hasMissing\": False, \"uniques\": [\"False\", \"True\"], \"success\": True},\n        )\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"str_val\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"hasMissing\"]\n        assert all(k in response_data for k in [\"hasMissing\", \"uniques\", \"success\"])\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"int_val\"),\n        )\n        response_data = response.get_json()\n        assert not response_data[\"hasMissing\"]\n        assert all(\n            k in response_data\n            for k in [\"max\", \"hasMissing\", \"uniques\", \"success\", \"min\"]\n        )\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port), query_string=dict(col=\"Col0\")\n        )\n        response_data = response.get_json()\n        assert not response_data[\"hasMissing\"]\n        assert all(k in response_data for k in [\"max\", \"hasMissing\", \"success\", \"min\"])\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port), query_string=dict(col=\"date\")\n        )\n        response_data = response.get_json()\n        assert not response_data[\"hasMissing\"]\n        assert all(k in response_data for k in [\"max\", \"hasMissing\", \"success\", \"min\"])\n\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"missing_col\"),\n        )\n        response_data = response.get_json()\n        assert not response_data[\"success\"]\n\n    # mixed data test\n    df = pd.DataFrame.from_dict(\n        {\n            \"a\": [\"a\", \"UNknown\", \"b\"],\n            \"b\": [\"\", \" \", \" - \"],\n            \"c\": [1, \"\", 3],\n            \"d\": [1.1, np.nan, 3],\n            \"e\": [\"a\", np.nan, \"b\"],\n        }\n    )\n    df, _ = views.format_data(df)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port), query_string=dict(col=\"c\")\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(sorted(response_data[\"uniques\"]), [\"\", \"1\", \"3\"])\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_get_async_column_filter_data(unittest, custom_data):\n    import dtale.views as views\n\n    df, _ = views.format_data(custom_data)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        str_val = df.str_val.values[0]\n        response = c.get(\n            \"/dtale/async-column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"str_val\", input=df.str_val.values[0]),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(response_data, [dict(value=str_val, label=str_val)])\n\n        int_val = df.int_val.values[0]\n        response = c.get(\n            \"/dtale/async-column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"int_val\", input=str(df.int_val.values[0])),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data, [dict(value=int_val, label=\"{}\".format(int_val))]\n        )\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_save_column_filter(unittest, custom_data):\n    import dtale.views as views\n\n    df, _ = views.format_data(custom_data)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        settings = {c.port: {}}\n        build_settings(settings)\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"bool_val\", cfg=json.dumps({\"type\": \"string\", \"value\": [\"False\"]})\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"bool_val\"],\n            {\n                \"query\": \"`bool_val` == False\",\n                \"value\": [\"False\"],\n                \"action\": \"equals\",\n                \"caseSensitive\": False,\n                \"operand\": \"=\",\n                \"raw\": None,\n                \"meta\": {\"classification\": \"B\", \"column\": \"bool_val\", \"type\": \"string\"},\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"str_val\", cfg=json.dumps({\"type\": \"string\", \"value\": [\"a\", \"b\"]})\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"str_val\"],\n            {\n                \"query\": \"`str_val` in ('a', 'b')\"\n                if PY3\n                else \"`str_val` in (u'a', u'b')\",\n                \"value\": [\"a\", \"b\"],\n                \"action\": \"equals\",\n                \"caseSensitive\": False,\n                \"operand\": \"=\",\n                \"raw\": None,\n                \"meta\": {\"classification\": \"S\", \"column\": \"str_val\", \"type\": \"string\"},\n            },\n        )\n        for col, f_type in [\n            (\"bool_val\", \"string\"),\n            (\"int_val\", \"int\"),\n            (\"date\", \"date\"),\n        ]:\n            response = c.get(\n                \"/dtale/save-column-filter/{}\".format(c.port),\n                query_string=dict(\n                    col=col, cfg=json.dumps({\"type\": f_type, \"missing\": True})\n                ),\n            )\n            col_cfg = response.get_json()[\"currFilters\"][col]\n            assert col_cfg[\"query\"] == \"`{col}`.isnull()\".format(col=col)\n            assert col_cfg[\"missing\"]\n\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(col=\"bool_val\", cfg=None),\n        )\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n        meta = {\"classification\": \"I\", \"column\": \"int_val\", \"type\": \"int\"}\n        for operand in [\"=\", \"<\", \">\", \"<=\", \">=\"]:\n            response = c.get(\n                \"/dtale/save-column-filter/{}\".format(c.port),\n                query_string=dict(\n                    col=\"int_val\",\n                    cfg=json.dumps({\"type\": \"int\", \"operand\": operand, \"value\": \"5\"}),\n                ),\n            )\n            query = \"`int_val` {} 5\".format(\"==\" if operand == \"=\" else operand)\n            unittest.assertEqual(\n                response.get_json()[\"currFilters\"][\"int_val\"],\n                {\"query\": query, \"value\": \"5\", \"operand\": operand, \"meta\": meta},\n            )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps({\"type\": \"int\", \"operand\": \"=\", \"value\": [\"5\", \"4\"]}),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` in (5, 4)\",\n                \"value\": [\"5\", \"4\"],\n                \"operand\": \"=\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"[]\", \"min\": \"4\", \"max\": \"5\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` >= 4 and `int_val` <= 5\",\n                \"min\": \"4\",\n                \"max\": \"5\",\n                \"operand\": \"[]\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": \"4\", \"max\": \"5\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` > 4 and `int_val` < 5\",\n                \"min\": \"4\",\n                \"max\": \"5\",\n                \"operand\": \"()\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": \"4\", \"max\": None}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\"query\": \"`int_val` > 4\", \"min\": \"4\", \"operand\": \"()\", \"meta\": meta},\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": None, \"max\": \"5\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\"query\": \"`int_val` < 5\", \"max\": \"5\", \"operand\": \"()\", \"meta\": meta},\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": \"4\", \"max\": \"4\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` == 4\",\n                \"min\": \"4\",\n                \"max\": \"4\",\n                \"operand\": \"()\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"date\",\n                cfg=json.dumps(\n                    {\"type\": \"date\", \"start\": \"20000101\", \"end\": \"20000101\"}\n                ),\n            ),\n        )\n        meta = {\"classification\": \"D\", \"column\": \"date\", \"type\": \"date\"}\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"date\"],\n            {\n                \"query\": \"`date` == '20000101'\",\n                \"start\": \"20000101\",\n                \"end\": \"20000101\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"date\",\n                cfg=json.dumps(\n                    {\"type\": \"date\", \"start\": \"20000101\", \"end\": \"20000102\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"date\"],\n            {\n                \"query\": \"`date` >= '20000101' and `date` <= '20000102'\",\n                \"start\": \"20000101\",\n                \"end\": \"20000102\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"date\", cfg=json.dumps({\"type\": \"date\", \"missing\": False})\n            ),\n        )\n        assert \"date\" not in response.get_json()[\"currFilters\"]\n\n        assert settings[c.port].get(\"query\") is None\n\n        response = c.get(\"/dtale/move-filters-to-custom/{}\".format(c.port))\n        assert response.json[\"success\"]\n        assert response.json[\"settings\"].get(\"query\") is not None\n\n\n@pytest.mark.unit\ndef test_build_dtypes_state(test_data):\n    import dtale.views as views\n\n    state = views.build_dtypes_state(test_data.set_index(\"security_id\").T)\n    assert all(\"min\" not in r and \"max\" not in r for r in state)\n\n\n@pytest.mark.unit\ndef test_update_theme():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"theme\": \"light\"}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-theme\", query_string={\"theme\": \"dark\"})\n            assert app_settings[\"theme\"] == \"dark\"\n            response = c.get(\"/dtale/main/{}\".format(c.port))\n            assert '<body class=\"dark-mode\"' in str(response.data)\n\n\n@pytest.mark.unit\ndef test_update_query_engine():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"query_engine\": \"python\"}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-query-engine\", query_string={\"engine\": \"numexpr\"})\n            assert app_settings[\"query_engine\"] == \"numexpr\"\n\n\n@pytest.mark.unit\ndef test_update_pin_menu():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"pin_menu\": False}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-pin-menu\", query_string={\"pinned\": True})\n            assert app_settings[\"pin_menu\"]\n\n\n@pytest.mark.unit\ndef test_update_language():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"language\": \"en\"}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-language\", query_string={\"language\": \"cn\"})\n            assert app_settings[\"language\"] == \"cn\"\n\n\n@pytest.mark.unit\ndef test_update_max_column_width():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-maximum-column-width\", query_string={\"width\": 100})\n            assert app_settings[\"max_column_width\"] == 100\n\n\n@pytest.mark.unit\ndef test_update_max_row_height():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-maximum-row-height\", query_string={\"height\": 100})\n            assert app_settings[\"max_row_height\"] == 100\n\n\n@pytest.mark.unit\ndef test_load_filtered_ranges(unittest):\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        settings = {c.port: dict(query=\"a > 1\")}\n        build_settings(settings)\n        response = c.get(\"/dtale/load-filtered-ranges/{}\".format(c.port))\n        ranges = response.json\n        assert ranges[\"ranges\"][\"a\"][\"max\"] == 3 and ranges[\"overall\"][\"min\"] == 2\n        assert ranges[\"query\"] == settings[c.port][\"filteredRanges\"][\"query\"]\n        assert ranges[\"dtypes\"][\"a\"][\"max\"] == 3 and ranges[\"dtypes\"][\"a\"][\"min\"] == 2\n\n        response = c.get(\"/dtale/load-filtered-ranges/{}\".format(c.port))\n        unittest.assertEqual(ranges, response.json)\n\n        del settings[c.port][\"query\"]\n        response = c.get(\"/dtale/load-filtered-ranges/{}\".format(c.port))\n        assert not len(response.json)\n\n\n@pytest.mark.unit\ndef test_build_column_text():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n\n        resp = c.post(\n            \"/dtale/build-column-copy/{}\".format(c.port),\n            data=json.dumps({\"columns\": json.dumps([\"a\"])}),\n            content_type=\"application/json\",\n        )\n        assert resp.data == b\"1\\n2\\n3\\n\"\n\n\n@pytest.mark.unit\ndef test_build_row_text():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n\n        resp = c.post(\n            \"/dtale/build-row-copy/{}\".format(c.port),\n            data=json.dumps({\"start\": 1, \"end\": 2, \"columns\": json.dumps([\"a\"])}),\n            content_type=\"application/json\",\n        )\n        assert resp.data == b\"1\\n2\\n\"\n        resp = c.post(\n            \"/dtale/build-row-copy/{}\".format(c.port),\n            data=json.dumps({\"rows\": json.dumps([1]), \"columns\": json.dumps([\"a\"])}),\n            content_type=\"application/json\",\n        )\n        assert resp.data == b\"2\\n\"\n\n\n@pytest.mark.unit\ndef test_sorted_sequential_diffs():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3, 4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n\n        resp = c.get(\n            \"/dtale/sorted-sequential-diffs/{}\".format(c.port),\n            query_string=dict(col=\"a\", sort=\"ASC\"),\n        )\n        data = resp.json\n        assert data[\"min\"] == \"1\"\n        assert data[\"max\"] == \"1\"\n\n        resp = c.get(\n            \"/dtale/sorted-sequential-diffs/{}\".format(c.port),\n            query_string=dict(col=\"a\", sort=\"DESC\"),\n        )\n        data = resp.json\n        assert data[\"min\"] == \"-1\"\n        assert data[\"max\"] == \"-1\"\n\n\n@pytest.mark.unit\ndef test_drop_filtered_rows():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3, 4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        settings = {c.port: {\"query\": \"a == 1\"}}\n        build_settings(settings)\n        c.get(\"/dtale/drop-filtered-rows/{}\".format(c.port))\n        assert len(global_state.get_data(c.port)) == 1\n        assert global_state.get_settings(c.port)[\"query\"] == \"\"\n\n\n@pytest.mark.unit\ndef test_save_range_highlights():\n    with build_app(url=URL).test_client() as c:\n        settings = {c.port: {}}\n        build_settings(settings)\n        range_highlights = {\n            \"foo\": {\n                \"active\": True,\n                \"equals\": {\n                    \"active\": True,\n                    \"value\": 3,\n                    \"color\": {\"r\": 255, \"g\": 245, \"b\": 157, \"a\": 1},\n                },  # light yellow\n                \"greaterThan\": {\n                    \"active\": True,\n                    \"value\": 3,\n                    \"color\": {\"r\": 80, \"g\": 227, \"b\": 194, \"a\": 1},\n                },\n                # mint green\n                \"lessThan\": {\n                    \"active\": True,\n                    \"value\": 3,\n                    \"color\": {\"r\": 245, \"g\": 166, \"b\": 35, \"a\": 1},\n                },  # orange\n            }\n        }\n        c.post(\n            \"/dtale/save-range-highlights/{}\".format(c.port),\n            data=json.dumps(dict(ranges=json.dumps(range_highlights))),\n            content_type=\"application/json\",\n        )\n        assert global_state.get_settings(c.port)[\"rangeHighlight\"] is not None\n"], "fixing_code": ["[![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Title.png)](https://github.com/man-group/dtale)\n\n* [Live Demo](http://alphatechadmin.pythonanywhere.com)\n* [Animated US COVID-19 Deaths By State](http://alphatechadmin.pythonanywhere.com/dtale/charts/3?chart_type=maps&query=date+%3E+%2720200301%27&agg=raw&map_type=choropleth&loc_mode=USA-states&loc=state_code&map_val=deaths&colorscale=Reds&cpg=false&animate_by=date)\n* [3D Scatter Chart](http://alphatechadmin.pythonanywhere.com/dtale/charts/4?chart_type=3d_scatter&query=&x=date&z=Col0&agg=raw&cpg=false&y=%5B%22security_id%22%5D)\n* [Surface Chart](http://alphatechadmin.pythonanywhere.com/dtale/charts/4?chart_type=surface&query=&x=date&z=Col0&agg=raw&cpg=false&y=%5B%22security_id%22%5D)\n* [Network Analysis](http://alphatechadmin.pythonanywhere.com/dtale/network/5?to=to&from=from&group=route_id&weight=)\n\n-----------------\n\n[![CircleCI](https://circleci.com/gh/man-group/dtale.svg?style=shield)](https://circleci.com/gh/man-group/dtale)\n[![PyPI Python Versions](https://img.shields.io/pypi/pyversions/dtale.svg)](https://pypi.python.org/pypi/dtale/)\n[![PyPI](https://img.shields.io/pypi/v/dtale)](https://pypi.org/project/dtale/)\n[![Conda](https://img.shields.io/conda/v/conda-forge/dtale)](https://anaconda.org/conda-forge/dtale)\n[![ReadTheDocs](https://readthedocs.org/projects/dtale/badge)](https://dtale.readthedocs.io)\n[![codecov](https://codecov.io/gh/man-group/dtale/branch/master/graph/badge.svg)](https://codecov.io/gh/man-group/dtale)\n[![Downloads](https://static.pepy.tech/badge/dtale)](https://pepy.tech/project/dtale)\n[![Open in VS Code](https://img.shields.io/badge/Visual_Studio_Code-0078D4?style=flat&logo=visual%20studio%20code&logoColor=white)](https://open.vscode.dev/man-group/dtale)\n\n## What is it?\n\nD-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures.  It integrates seamlessly with ipython notebooks & python/ipython terminals.  Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex.\n\n## Origins\n\nD-Tale was the product of a SAS to Python conversion.  What was originally a perl script wrapper on top of SAS's `insight` function is now a lightweight web client on top of Pandas data structures.\n\n## In The News\n\n - [4 Libraries that can perform EDA in one line of python code](https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae)\n - [React Status](https://react.statuscode.com/issues/204)\n - [KDNuggets](https://www.kdnuggets.com/2020/08/bring-pandas-dataframes-life-d-tale.html)\n - [Man Institute](https://www.man.com/maninstitute/d-tale) (warning: contains deprecated functionality)\n - [Python Bytes](https://pythonbytes.fm/episodes/show/169/jupyter-notebooks-natively-on-your-ipad)\n - [FlaskCon 2020](https://www.youtube.com/watch?v=BNgolmUWBp4&t=33s)\n - [San Diego Python](https://www.youtube.com/watch?v=fLsGur5YqeE&t=29s)\n - [Medium: towards data science](https://towardsdatascience.com/introduction-to-d-tale-5eddd81abe3f)\n - [Medium: Exploratory Data Analysis \u2013 Using D-Tale](https://medium.com/da-tum/exploratory-data-analysis-1-4-using-d-tale-99a2c267db79)\n - [EOD Notes: Using python and dtale to analyze correlations](https://www.google.com/amp/s/eod-notes.com/2020/05/07/using-python-and-dtale-to-analyze-correlations/amp/)\n - [Data Exploration is Now Super Easy w/ D-Tale](https://dibyendudeb.com/d-tale-data-exploration-tool/)\n - [Practical Business Python](https://pbpython.com/dataframe-gui-overview.html)\n\n## Tutorials\n\n - [Pip Install Python YouTube Channel](https://m.youtube.com/watch?v=0RihZNdQc7k&feature=youtu.be)\n - [machine_learning_2019](https://www.youtube.com/watch?v=-egtEUVBy9c)\n - [D-Tale The Best Library To Perform Exploratory Data Analysis Using Single Line Of Code\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25](https://www.youtube.com/watch?v=xSXGcuiEzUc)\n - [Explore and Analyze Pandas Data Structures w/ D-Tale](https://m.youtube.com/watch?v=JUu5IYVGqCg)\n - [Data Preprocessing simplest method \ud83d\udd25](https://www.youtube.com/watch?v=Q2kMNPKgN4g)\n\n ## Related Resources\n\n - [Adventures In Flask While Developing D-Tale](https://github.com/man-group/dtale/blob/master/docs/FlaskCon/FlaskAdventures.md)\n - [Adding Range Selection to react-virtualized](https://github.com/man-group/dtale/blob/master/docs/RANGE_SELECTION.md)\n - [Building Draggable/Resizable Modals](https://github.com/man-group/dtale/blob/master/docs/DRAGGABLE_RESIZABLE_MODALS.md)\n - [Embedding Flask Apps within Streamlit](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_STREAMLIT.md)\n\n## Contents\n\n- [Where To Get It](#where-to-get-it)\n- [Getting Started](#getting-started)\n  - [Python Terminal](#python-terminal)\n  - [As A Script](#as-a-script)\n  - [Jupyter Notebook](#jupyter-notebook)\n  - [Jupyterhub w/ Jupyter Server Proxy](#jupyterhub-w-jupyter-server-proxy)\n  - [Jupyterhub w/ Kubernetes](https://github.com/man-group/dtale/blob/master/docs/JUPYTERHUB_KUBERNETES.md)\n  - [Docker Container](#docker-container)\n  - [Google Colab](#google-colab)\n  - [Kaggle](#kaggle)\n  - [Binder](#binder)\n  - [R with Reticulate](#r-with-reticulate)\n  - [Startup with No Data](#startup-with-no-data)\n  - [Command-line](#command-line)\n  - [Custom Command-line Loaders](#custom-command-line-loaders)\n  - [Embedding Within Your Own Flask App](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_FLASK.md)\n  - [Embedding Within Your Own Django App](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_DJANGO.md)\n  - [Embedding Within Streamlit](https://github.com/man-group/dtale/blob/master/docs/EMBEDDED_DTALE_STREAMLIT.md)\n  - [Running D-Tale On Gunicorn w/ Redis](https://github.com/man-group/dtale/blob/master/docs/GUNICORN_REDIS.md)\n  - [Configuration](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md)\n  - [Authentication](#authentication)\n  - [Predefined Filters](#predefined-filters)\n  - [Using Swifter](#using-swifter)\n  - [Behavior for Wide Dataframes](#behavior-for-wide-dataframes)\n- [UI](#ui)\n  - [Dimensions/Ribbon Menu/Main Menu](#dimensionsribbon-menumain-menu)\n  - [Header](#header)\n  - [Resize Columns](#resize-columns)\n  - [Editing Cells](#editing-cells)\n  - [Copy Cells Into Clipboard](#copy-cells-into-clipboard)\n  - [Main Menu Functions](#main-menu-functions)\n    - [XArray Operations](#xarray-operations), [Describe](#describe), [Outlier Detection](#outlier-detection), [Custom Filter](#custom-filter), [Dataframe Functions](#dataframe-functions), [Merge & Stack](#merge--stack), [Summarize Data](#summarize-data), [Duplicates](#duplicates), [Missing Analysis](#missing-analysis), [Correlations](#correlations), [Predictive Power Score](#predictive-power-score), [Heat Map](#heat-map), [Highlight Dtypes](#highlight-dtypes), [Highlight Missing](#highlight-missing), [Highlight Outliers](#highlight-outliers), [Highlight Range](#highlight-range), [Low Variance Flag](#low-variance-flag), [Instances](#instances), [Code Exports](#code-exports), [Export CSV](#export-csv), [Load Data & Sample Datasets](#load-data--sample-datasets), [Refresh Widths](#refresh-widths), [About](#about), [Theme](#theme), [Reload Data](#reload-data), [Unpin/Pin Menu](#unpinpin-menu), [Language](#language), [Shutdown](#shutdown)\n  - [Column Menu Functions](#column-menu-functions)\n    - [Filtering](#filtering), [Moving Columns](#moving-columns), [Hiding Columns](#hiding-columns), [Delete](#delete), [Rename](#rename), [Replacements](#replacements), [Lock](#lock), [Unlock](#unlock), [Sorting](#sorting), [Formats](#formats), [Describe (Column Analysis)](#describe-column-analysis)\n  - [Charts](#charts)\n  - [Network Viewer](#network-viewer)\n  - [Hotkeys](#hotkeys)\n  - [Menu Functions Depending on Browser Dimensions](#menu-functions-depending-on-browser-dimensions)\n- [For Developers](#for-developers)\n  - [Cloning](#cloning)\n  - [Running Tests](#running-tests)\n  - [Linting](#linting)\n  - [Formatting JS](#formatting-js)\n  - [Docker Development](#docker-development)\n  - [Adding Language Support](#adding-language-support)\n- [Global State/Data Storage](https://github.com/man-group/dtale/blob/master/docs/GLOBAL_STATE.md)\n- [Startup Behavior](#startup-behavior)\n- [Documentation](#documentation)\n- [Dependencies](#dependencies)\n- [Acknowledgements](#acknowledgements)\n- [License](#license)\n\n## Where To get It\nThe source code is currently hosted on GitHub at:\nhttps://github.com/man-group/dtale\n\nBinary installers for the latest released version are available at the [Python\npackage index](https://pypi.org/project/dtale) and on conda using [conda-forge](https://github.com/conda-forge/dtale-feedstock).\n\n```sh\n# conda\nconda install dtale -c conda-forge\n# if you want to also use \"Export to PNG\" for charts\nconda install -c plotly python-kaleido\n```\n\n```sh\n# or PyPI\npip install dtale\n```\n\n## Getting Started\n\n|PyCharm|jupyter|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/dtale_demo_mini.gif)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/dtale_ipython.gif)|\n\n### Python Terminal\nThis comes courtesy of PyCharm\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Python_Terminal.png)\nFeel free to invoke `python` or `ipython` directly and use the commands in the screenshot above and it should work\n\n#### Issues With Windows Firewall\n\nIf you run into issues with viewing D-Tale in your browser on Windows please try making Python public under \"Allowed Apps\" in your Firewall configuration.  Here is a nice article:\n[How to Allow Apps to Communicate Through the Windows Firewall](https://www.howtogeek.com/howto/uncategorized/how-to-create-exceptions-in-windows-vista-firewall/)\n\n#### Additional functions available programmatically\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame([dict(a=1,b=2,c=3)])\n\n# Assigning a reference to a running D-Tale process.\nd = dtale.show(df)\n\n# Accessing data associated with D-Tale process.\ntmp = d.data.copy()\ntmp['d'] = 4\n\n# Altering data associated with D-Tale process\n# FYI: this will clear any front-end settings you have at the time for this process (filter, sorts, formatting)\nd.data = tmp\n\n# Shutting down D-Tale process\nd.kill()\n\n# Using Python's `webbrowser` package it will try and open your server's default browser to this process.\nd.open_browser()\n\n# There is also some helpful metadata about the process.\nd._data_id  # The process's data identifier.\nd._url  # The url to access the process.\n\nd2 = dtale.get_instance(d._data_id)  # Returns a new reference to the instance running at that data_id.\n\ndtale.instances()  # Prints a list of all ids & urls of running D-Tale sessions.\n\n```\n\n#### Duplicate data check\nTo help guard against users loading the same data to D-Tale multiple times and thus eating up precious memory, we have a loose check for duplicate input data.  The check runs the following:\n * Are row & column count the same as a previously loaded piece of data?\n * Are the names and order of columns the same as a previously loaded piece of data?\n\nIf both these conditions are true then you will be presented with an error and a link to the previously loaded data.  Here is an example of how the interaction looks:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Duplicate_data.png)\n\n\n### As A Script\n\nD-Tale can be run as script by adding `subprocess=False` to your `dtale.show` command.  Here is an example script:\n```python\nimport dtale\nimport pandas as pd\n\nif __name__ == '__main__':\n      dtale.show(pd.DataFrame([1,2,3,4,5]), subprocess=False)\n```\n\n### Jupyter Notebook\nWithin any jupyter (ipython) notebook executing a cell like this will display a small instance of D-Tale in the output cell.  Here are some examples:\n\n|`dtale.show`|assignment|instance|\n|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython3.png)|\n\nIf you are running ipython<=5.0 then you also have the ability to adjust the size of your output cell for the most recent instance displayed:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ipython_adjust.png)\n\nOne thing of note is that a lot of the modal popups you see in the standard browser version will now open separate browser windows for spacial convienence:\n\n|Column Menus|Correlations|Describe|Column Analysis|Instances|\n|:------:|:------:|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Column_menu.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/correlations_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/describe_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/histogram_popup.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/instances_popup.png)|\n\n### JupyterHub w/ Jupyter Server Proxy\n\nJupyterHub has an extension that allows to proxy port for user, [JupyterHub Server Proxy](https://github.com/jupyterhub/jupyter-server-proxy)\n\nTo me it seems like this extension might be the best solution to getting D-Tale running within kubernetes.  Here's how to use it:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.JUPYTER_SERVER_PROXY = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nNotice the command `dtale_app.JUPYTER_SERVER_PROXY = True` this will make sure that any D-Tale instance will be served with the jupyter server proxy application root prefix:\n\n`/user/{jupyter username}/proxy/{dtale instance port}/`\n\nOne thing to note is that if you try to look at the `_main_url` of your D-Tale instance in your notebook it will not include the hostname or port:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.JUPYTER_SERVER_PROXY = True\n\nd = dtale.show(pd.DataFrame([1,2,3]))\nd._main_url # /user/johndoe/proxy/40000/dtale/main/1\n```\n\n This is because it's very hard to promgramatically figure out the host/port that your notebook is running on.  So if you want to look at `_main_url` please be sure to preface it with:\n \n `http[s]://[jupyterhub host]:[jupyterhub port]`\n\nIf for some reason jupyterhub changes their API so that the application root changes you can also override D-Tale's application root by using the `app_root` parameter to the `show()` function:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale.show(pd.DataFrame([1,2,3]), app_root='/user/johndoe/proxy/40000/`)\n```\n\nUsing this parameter will only apply the application root to that specific instance so you would have to include it on every call to `show()`.\n\n### JupyterHub w/ Kubernetes\n\nPlease read this [post](https://github.com/man-group/dtale/blob/master/docs/JUPYTERHUB_KUBERNETES.md)\n\n### Docker Container\n\nIf you have D-Tale installed within your docker container please add the following parameters to your `docker run` command.\n\n**On a Mac**:\n\n```sh\ndocker run -h `hostname` -p 40000:40000\n```\n\n* `-h` this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs\n* `-p` access to port 40000 which is the default port for running D-Tale\n\n**On Windows**:\n\n```sh\ndocker run -p 40000:40000\n```\n\n* `-p` access to port 40000 which is the default port for running D-Tale\n* D-Tale URL will be http://127.0.0.1:40000/\n\n**Everything Else**:\n\n```sh\ndocker run -h `hostname` --network host\n```\n\n* `-h` this will allow the hostname (and not the PID of the docker container) to be available when building D-Tale URLs\n* `--network host` this will allow access to as many ports as needed for running D-Tale processes\n\n### Google Colab\n\nThis is a hosted notebook site and thanks to Colab's internal function `google.colab.output.eval_js` & the JS function `google.colab.kernel.proexyPort` users can run D-Tale within their notebooks.\n\n**DISCLAIMER:** It is important that you set `USE_COLAB` to true when using D-Tale within this service.  Here is an example:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.USE_COLAB = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nIf this does not work for you try using `USE_NGROK` which is described in the next section.\n\n### Kaggle\n\nThis is yet another hosted notebook site and thanks to the work of [flask_ngrok](https://github.com/gstaff/flask-ngrok) users can run D-Tale within their notebooks.\n\n**DISCLAIMER:** It is import that you set `USE_NGROK` to true when using D-Tale within this service.  Here is an example:\n\n```python\nimport pandas as pd\n\nimport dtale\nimport dtale.app as dtale_app\n\ndtale_app.USE_NGROK = True\n\ndtale.show(pd.DataFrame([1,2,3]))\n```\n\nHere are some video tutorials of each:\n\n|Service|Tutorial|Addtl Notes|\n|:------:|:------:|:------:|\n|Google Colab|[![](http://img.youtube.com/vi/pOYl2M1clIw/0.jpg)](http://www.youtube.com/watch?v=pOYl2M1clIw \"Google Colab\")||\n|Kaggle|[![](http://img.youtube.com/vi/8Il-2HHs2Mg/0.jpg)](http://www.youtube.com/watch?v=8Il-2HHs2Mg \"Kaggle\")|make sure you switch the \"Internet\" toggle to \"On\" under settings of your notebook so you can install the egg from pip|\n\nIt is important to note that using NGROK will limit you to 20 connections per mintue so if you see this error:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/ngrok_limit.png)\n\nWait a little while and it should allow you to do work again.  I am actively working on finding a more sustainable solution similar to what I did for google colab. :pray:\n\n### Binder\n\nI have built a repo which shows an example of how to run D-Tale within Binder [here](https://github.com/aschonfeld/dtale-binder).\n\nThe important take-aways are:\n* you must have `jupyter-server-proxy` installed\n* look at the `environment.yml` file to see how to add it to your environment\n* look at the `postBuild` file for how to activate it on startup\n\n\n### R with Reticulate\n\nI was able to get D-Tale running in R using reticulate. Here is an example:\n\n```\nlibrary('reticulate')\ndtale <- import('dtale')\ndf <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/boot/acme.csv')\ndtale$show(df, subprocess=FALSE, open_browser=TRUE)\n```\n\nNow the problem with doing this is that D-Tale is not running as a subprocess so it will block your R console and you'll lose out the following functions:\n - manipulating the state of your data from your R console\n - adding more data to D-Tale\n\n`open_browser=TRUE` isn't required and won't work if you don't have a default browser installed on your machine. If you don't use that parameter simply copy & paste the URL that gets printed to your console in the browser of your choice.\n\nI'm going to do some more digging on why R doesn't seem to like using python subprocesses (not sure if it something with how reticulate manages the state of python) and post any findings to this thread.\n\nHere's some helpful links for getting setup:\n\nreticulate\n\ninstalling python packages\n\n### Startup with No Data\n\nIt is now possible to run D-Tale with no data loaded up front. So simply call `dtale.show()` and this will start the application for you and when you go to view it you will be presented with a screen where you can upload either a CSV or TSV file for data.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/no_data.png)\n\nOnce you've loaded a file it will take you directly to the standard data grid comprised of the data from the file you loaded.  This might make it easier to use this as an on demand application within a container management system like kubernetes. You start and stop these on demand and you'll be presented with a new instance to load any CSV or TSV file to!\n\n### Command-line\nBase CLI options (run `dtale --help` to see all options available)\n\n|Prop     |Description|\n|:--------|:-----------|\n|`--host` |the name of the host you would like to use (most likely not needed since `socket.gethostname()` should figure this out)|\n|`--port` |the port you would like to assign to your D-Tale instance|\n|`--name` |an optional name you can assign to your D-Tale instance (this will be displayed in the `<title>` & Instances popup)|\n|`--debug`|turn on Flask's \"debug\" mode for your D-Tale instance|\n|`--no-reaper`|flag to turn off auto-reaping subprocess (kill D-Tale instances after an hour of inactivity), good for long-running displays |\n|`--open-browser`|flag to automatically open up your server's default browser to your D-Tale instance|\n|`--force`|flag to force D-Tale to try an kill any pre-existing process at the port you've specified so it can use it|\n\nLoading data from [**ArcticDB**(high performance, serverless DataFrame database)](https://github.com/man-group/ArcticDB) (this requires either installing **arcticdb** or **dtale[arcticdb]**)\n```bash\ndtale --arcticdb-uri lmdb:///<path> --arcticdb-library jdoe.my_lib --arcticdb-symbol my_symbol\n```\nIf you would like to change your storage mechanism to ArcticDB then add the `--arcticdb-use_store` flag\n```bash\ndtale --arcticdb-uri lmdb:///<path> --arcticdb-library my_lib --arcticdb-symbol my_symbol --arcticdb-use_store\n```\nLoading data from [**arctic**(high performance datastore for pandas dataframes)](https://github.com/man-group/arctic) (this requires either installing **arctic** or **dtale[arctic]**)\n```bash\ndtale --arctic-uri mongodb://localhost:27027 --arctic-library my_lib --arctic-symbol my_symbol --arctic-start 20130101 --arctic-end 20161231\n```\nLoading data from **CSV**\n```bash\ndtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date\n```\nLoading data from **EXCEL**\n```bash\ndtale --excel-path /home/jdoe/my_csv.xlsx --excel-parse_dates date\ndtale --excel-path /home/jdoe/my_csv.xls --excel-parse_dates date\n```\nLoading data from **JSON**\n```bash\ndtale --json-path /home/jdoe/my_json.json --json-parse_dates date\n```\nor\n```bash\ndtale --json-path http://json-endpoint --json-parse_dates date\n```\nLoading data from **R Datasets**\n```bash\ndtale --r-path /home/jdoe/my_dataset.rda\n```\nLoading data from **SQLite DB Files**\n```bash\ndtale --sqlite-path /home/jdoe/test.sqlite3 --sqlite-table test_table\n```\n\n### Custom Command-line Loaders\n\nLoading data from a **Custom** loader\n- Using the DTALE_CLI_LOADERS environment variable, specify a path to a location containing some python modules\n- Any python module containing the global variables LOADER_KEY & LOADER_PROPS will be picked up as a custom loader\n  - LOADER_KEY: the key that will be associated with your loader.  By default you are given **arctic** & **csv** (if you use one of these are your key it will override these)\n  - LOADER_PROPS: the individual props available to be specified.\n    - For example, with arctic we have host, library, symbol, start & end.\n    - If you leave this property as an empty list your loader will be treated as a flag.  For example, instead of using all the arctic properties we would simply specify `--arctic` (this wouldn't work well in arctic's case since it depends on all those properties)\n- You will also need to specify a function with the following signature `def find_loader(kwargs)` which returns a function that returns a dataframe or `None`\n- Here is an example of a custom loader:\n```python\nfrom dtale.cli.clickutils import get_loader_options\n\n'''\n  IMPORTANT!!! This global variable is required for building any customized CLI loader.\n  When find loaders on startup it will search for any modules containing the global variable LOADER_KEY.\n'''\nLOADER_KEY = 'testdata'\nLOADER_PROPS = ['rows', 'columns']\n\n\ndef test_data(rows, columns):\n    import pandas as pd\n    import numpy as np\n    import random\n    from past.utils import old_div\n    from pandas.tseries.offsets import Day\n    from dtale.utils import dict_merge\n    import string\n\n    now = pd.Timestamp(pd.Timestamp('now').date())\n    dates = pd.date_range(now - Day(364), now)\n    num_of_securities = max(old_div(rows, len(dates)), 1)  # always have at least one security\n    securities = [\n        dict(security_id=100000 + sec_id, int_val=random.randint(1, 100000000000),\n             str_val=random.choice(string.ascii_letters) * 5)\n        for sec_id in range(num_of_securities)\n    ]\n    data = pd.concat([\n        pd.DataFrame([dict_merge(dict(date=date), sd) for sd in securities])\n        for date in dates\n    ], ignore_index=True)[['date', 'security_id', 'int_val', 'str_val']]\n\n    col_names = ['Col{}'.format(c) for c in range(columns)]\n    return pd.concat([data, pd.DataFrame(np.random.randn(len(data), columns), columns=col_names)], axis=1)\n\n\n# IMPORTANT!!! This function is required for building any customized CLI loader.\ndef find_loader(kwargs):\n    test_data_opts = get_loader_options(LOADER_KEY, LOADER_PROPS, kwargs)\n    if len([f for f in test_data_opts.values() if f]):\n        def _testdata_loader():\n            return test_data(int(test_data_opts.get('rows', 1000500)), int(test_data_opts.get('columns', 96)))\n\n        return _testdata_loader\n    return None\n```\nIn this example we simplying building a dataframe with some dummy data based on dimensions specified on the command-line:\n- `--testdata-rows`\n- `--testdata-columns`\n\nHere's how you would use this loader:\n```bash\nDTALE_CLI_LOADERS=./path_to_loaders bash -c 'dtale --testdata-rows 10 --testdata-columns 5'\n```\n\n### Authentication\n\nYou can choose to use optional authentication by adding the following to your D-Tale `.ini` file ([directions here](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md)):\n\n```ini\n[auth]\nactive = True\nusername = johndoe\npassword = 1337h4xOr\n```\n\nOr you can call the following:\n\n```python\nimport dtale.global_state as global_state\n\nglobal_state.set_auth_settings({'active': True, 'username': 'johndoe', 'password': '1337h4x0r'})\n```\n\nIf you have done this before initially starting D-Tale it will have authentication applied.  If you are adding this after starting D-Tale you will have to kill your service and start it over.\n\nWhen opening your D-Tale session you will be presented with a screen like this:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/login.png)\n\nFrom there you can enter the credentials you either set in your `.ini` file or in your call to `dtale.global_state.set_auth_settings` and you will be brought to the main grid as normal.  You will now have an additional option in your main menu to logout:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/logout.png)\n\n### Instance Settings\n\nUsers can set front-end properties on their instances programmatically in the `dtale.show` function or by calling the `update_settings` function on their instance.  For example:\n\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame(dict(\n  a=[1,2,3,4,5],\n  b=[6,7,8,9,10],\n  c=['a','b','c','d','e']\n))\ndtale.show(\n  df,\n  locked=['c'],\n  column_formats={'a': {'fmt': '0.0000'}},\n  nan_display='...',\n  background_mode='heatmap-col',\n  sort=[('a','DESC')],\n  vertical_headers=True,\n)\n```\n\nor\n\n```python\nimport dtale\nimport pandas as pd\n\ndf = pd.DataFrame(dict(\n  a=[1,2,3,4,5],\n  b=[6,7,8,9,10],\n  c=['a','b','c','d','e']\n))\nd = dtale.show(\n  df\n)\nd.update_settings(\n  locked=['c'],\n  column_formats={'a': {'fmt': '0.0000'}},\n  nan_display='...',\n  background_mode='heatmap-col',\n  sort=[('a','DESC')],\n  vertical_headers=True,\n)\nd\n```\n\nHere's a short description of each instance setting available:\n\n#### show_columns\nA list of column names you would like displayed in your grid. Anything else will be hidden.\n\n#### hide_columns\nA list of column names you would like initially hidden from the grid display.\n\n#### column_formats\nA dictionary of column name keys and their front-end display configuration. Here are examples of the different format configurations:\n* Numeric: `{'fmt': '0.00000'}`\n* String:\n  * `{'fmt': {'truncate': 10}}` truncate string values to no more than 10 characters followed by an ellipses \n  * `{'fmt': {'link': True}}` if your strings are URLs convert them to clickable links\n  * `{'fmt': {'html': True}}` if your strings are HTML fragments render them as HTML\n* Date: `{'fmt': 'MMMM Do YYYY, h:mm:ss a'}` uses [Moment.js formatting](https://momentjs.com/docs/#/displaying/format/)\n\n#### nan_display\nConverts any `nan` values in your dataframe to this when it is sent to the browser (doesn't actually change the state of your dataframe)\n\n#### sort\nList of tuples which sort your dataframe (EX: `[('a', 'ASC'), ('b', 'DESC')]`)\n\n#### locked\nList of column names which will be locked to the right side of your grid while you scroll to the left.\n\n#### background_mode\nA string denoting one of the many background displays available in D-Tale. Options are:\n* heatmap-all: turn on heatmap for all numeric columns where the colors are determined by the range of values over all numeric columns combined\n* heatmap-col: turn on heatmap for all numeric columns where the colors are determined by the range of values in the column\n* heatmap-col-[column name]: turn on heatmap highlighting for a specific column\n* dtypes: highlight columns based on it's data type\n* missing: highlight any missing values (np.nan, empty strings, strings of all spaces)\n* outliers: highlight any outliers\n* range: highlight values for any matchers entered in the \"range_highlights\" option\n* lowVariance: highlight values with a low variance\n\n#### range_highlights\nDictionary of column name keys and range configurations which if the value for that column exists then it will be shaded that color.  Here is an example input:\n```\n'a': {\n  'active': True,\n  'equals': {'active': True, 'value': 3, 'color': {'r': 255, 'g': 245, 'b': 157, 'a': 1}}, # light yellow\n  'greaterThan': {'active': True, 'value': 3, 'color': {'r': 80, 'g': 227, 'b': 194, 'a': 1}}, # mint green\n  'lessThan': {'active': True, 'value': 3, 'color': {'r': 245, 'g': 166, 'b': 35, 'a': 1}}, # orange\n}\n```\n\n#### vertical_headers\nIf set to `True` then the headers in your grid will be rotated 90 degrees vertically to conserve width.\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/vertical_headers.png)\n\n\n### Predefined Filters\n\nUsers can build their own custom filters which can be used from the front-end using the following code snippet:\n```python\nimport pandas as pd\nimport dtale\nimport dtale.predefined_filters as predefined_filters\nimport dtale.global_state as global_state\n\nglobal_state.set_app_settings(dict(open_predefined_filters_on_startup=True))\n\npredefined_filters.set_filters([\n    {\n        \"name\": \"A and B > 2\",\n        \"column\": \"A\",\n        \"description\": \"Filter A with B greater than 2\",\n        \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] > 2)],\n        \"input_type\": \"input\",\n        \"default\": 1,\n        \"active\": False,\n    },\n    {\n        \"name\": \"A and (B % 2) == 0\",\n        \"column\": \"A\",\n        \"description\": \"Filter A with B mod 2 equals zero (is even)\",\n        \"handler\": lambda df, val: df[(df[\"A\"] == val) & (df[\"B\"] % 2 == 0)],\n        \"input_type\": \"select\",\n        \"default\": 1,\n        \"active\": False,\n    },\n    {\n        \"name\": \"A in values and (B % 2) == 0\",\n        \"column\": \"A\",\n        \"description\": \"A is within a group of values and B mod 2 equals zero (is even)\",\n        \"handler\": lambda df, val: df[df[\"A\"].isin(val) & (df[\"B\"] % 2 == 0)],\n        \"input_type\": \"multiselect\",\n        \"default\": [1],\n        \"active\": True,\n    }\n])\n\ndf = pd.DataFrame(\n    ([[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]]),\n    columns=['A', 'B', 'C', 'D', 'E', 'F']\n)\ndtale.show(df)\n```\n\nThis code illustrates the types of inputs you can have on the front end:\n* __input__: just a simple text input box which users can enter any value they want (if the value specified for `\"column\"` is an int or float it will try to convert the string to that data type) and it will be passed to the handler\n* __select__: this creates a dropdown populated with the unique values of `\"column\"` (an asynchronous dropdown if the column has a large amount of unique values)\n* __multiselect__: same as \"select\" but it will allow you to choose multiple values (handy if you want to perform an `isin` operation in your filter)\n\nHere is a demo of the functionality:\n[![](http://img.youtube.com/vi/8mryVxpxjM4/0.jpg)](http://www.youtube.com/watch?v=8mryVxpxjM4 \"Predefined Filters\")\n\nIf there are any new types of inputs you would like available please don't hesitate to submit a request on the \"Issues\" page of the repo.\n\n### Using Swifter\n\nSwifter is a package which will increase performance on any `apply()` function on a pandas series or dataframe.  If install the package in your virtual environment\n```sh\npip install swifter\n# or\npip install dtale[swifter]\n```\n\nIt will be used for the following operations:\n- Standard dataframe formatting in the main grid & chart display\n- Column Builders\n  - Type Conversions\n    - string hex -> int or float\n    - int or float -> hex\n    - mixed -> boolean\n    - int -> timestamp\n    - date -> int\n  - Similarity Distance Calculation\n- Handling of empty strings when calculating missing counts\n- Building unique values by data type in \"Describe\" popup\n\n\n### Behavior for Wide Dataframes\n\nThere is currently a performance bottleneck on the front-end when loading \"wide dataframes\" (dataframes with many columns). The current solution to this problem is that upon initial load of these dataframes to D-Tale any column with an index greater than 100 (going from left to right) will be hidden on the front-end.  You can still unhide these columns the same way you would any other and you still have the option to show all columns using the \"Describe\" popup. Here's a sample of this behavior:\n\nSay you loaded this dataframe into D-Tale.\n\n```python\nimport pandas as pd\nimport dtale\n\ndtale.show(pd.DataFrame(\n    {'col{}'.format(i): list(range(1000)) for i in range(105)}\n))\n```\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/initial_load.png)\n\nYou will now have access to a new \"Jump To Column\" menu item.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/jump_to_col_menu_item.png)\n\nIt would be too hard to scroll to the column you're looking for. So now you'll be able to type in the name of the column you're looking for and select it.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/jump_to_col_popup.png)\n\nAnd now you'll see only the columns you've had locked (we've locked no columns in this example) and the column you chose to jump to.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/wide_dataframes/jump_to_col_done.png)\n\n### Accessing CLI Loaders in Notebook or Console\nI am pleased to announce that all CLI loaders will be available within notebooks & consoles.  Here are some examples (the last working if you've installed `dtale[arctic]`):\n- `dtale.show_csv(path='test.csv', parse_dates=['date'])`\n- `dtale.show_csv(path='http://csv-endpoint', index_col=0)`\n- `dtale.show_excel(path='test.xlsx', parse_dates=['date'])`\n- `dtale.show_excel(path='test.xls', sheet=)`\n- `dtale.show_excel(path='http://excel-endpoint', index_col=0)`\n- `dtale.show_json(path='http://json-endpoint', parse_dates=['date'])`\n- `dtale.show_json(path='test.json', parse_dates=['date'])`\n- `dtale.show_r(path='text.rda')`\n- `dtale.show_arctic(uri='uri', library='library', symbol='symbol', start='20200101', end='20200101')`\n- `dtale.show_arcticdb(uri='uri', library='library', symbol='symbol', start='20200101', end='20200101', use_store=True)`\n\n### Using ArcticDB as your data store for D-Tale\nSo one of the major drawbacks of using D-Tale is that it stores a copy of your dataframe in memory (unless you specify the `inplace=True` when calling `dtale.show`). One way around this is to switch your storage mechanism to ArcticDB. This will use ArcticDB's [QueryBuilder](https://docs.arcticdb.io/api/query_builder) to perform all data loading and filtering.  This will significantly drop your memory footprint, but it will remove a lot of the original D-Tale functionality:\n- Custom Filtering\n- Range filtering in Numeric Column Filters\n- Regex filtering on String Column Filters\n- Editing Cells\n- Data Reshaping\n- Dataframe Functions\n- Drop Filtered Rows\n- Sorting\n\nIf the symbol you're loading from ArcticDB contains more than 1,000,000 rows then you will also lose the following:\n- Column Filtering using dropdowns of unique values (you'll have to manually type your values)\n- Outlier Highlighting\n- Most of the details in the \"Describe\" screen\n\nIn order to update your storage mechanism there are a few options, the first being `use_arcticdb_store`:\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>')\ndtale.show_arcticdb(library='my_lib', symbol='my_symbol')\n```\n\nOr you can set your library ahead of time so you can use `dtale.show`:\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>', library='my_lib')\ndtale.show('my_symbol')\n```\n\nOr you can set your library using `dtale.show` with a pipe-delimited identifier:\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>')\ndtale.show('my_lib|my_symbol')\n```\n\nYou can also do everything using `dtale.show_arcticdb`:\n```python\nimport dtale\n\ndtale.show_arcticdb(uri='lmdb:///<path>', library='my_lib', symbol='my_symbol', use_store=True)\n```\n\n### Navigating to different libraries/symbols in your ArcticDB database\n\nWhen starting D-Tale with no data\n```python\nimport dtale.global_state as global_state\nimport dtale\n\nglobal_state.use_arcticdb_store(uri='lmdb:///<path>')\ndtale.show()\n```\n\nyou'll be presented with this screen on startup\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/arcticdb/select_library_and_symbol.png)\n\nOnce you choose a library and a symbol you can click \"Load\" and it will bring you to the main grid comprised of the data for that symbol.\n\nYou can also view information about the symbol you've selected before loading it by clicking the \"Info\" button\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/arcticdb/description.png)\n\n## UI\nOnce you have kicked off your D-Tale session please copy & paste the link on the last line of output in your browser\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Browser1.png)\n\n### Dimensions/Ribbon Menu/Main Menu\nThe information in the upper right-hand corner gives grid dimensions ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Info_cell.png)\n- lower-left => row count\n- upper-right => column count\n\nRibbon Menu\n- hovering around to top of your browser will display a menu items (similar to the ones in the main menu) across the top of the screen\n- to close the menu simply click outside the menu and/or dropdowns from the menu\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Ribbon_menu.png)\n\nMain Menu\n- clicking the triangle displays the menu of standard functions (click outside menu to close it)\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Info_menu_small.png)\n\n### Header\n\nThe header gives users an idea of what operations have taken place on your data (sorts, filters, hidden columns).  These values will be persisted across broswer instances.  So if you perform one of these operations and then send a link to one of your colleagues they will see the same thing :)\n\nNotice the \"X\" icon on the right of each display.  Clicking this will remove those operations.\n\nWhen performing multiple of the same operation the description will become too large to display so the display will truncate the description and if users click it they will be presented with a tooltip where you can crop individual operations.  Here are some examples:\n\n|Sorts|Filters|Hidden Columns|\n|-----|-------|--------------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/sorts.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/filters.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/header/hidden.PNG)|\n\n### Resize Columns\n\nCurrently there are two ways which you can resize columns.\n* Dragging the right border of the column's header cell.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/resize_columns_w_drag.gif)\n\n* Altering the \"Maximum Column Width\" property from the ribbon menu.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/gifs/resize_columns_max_width.gif)\n\n* __Side Note:__ You can also set the `max_column_width` property ahead of time in your [global configuration](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md) or programmatically using:\n\n```python\nimport dtale.global_state as global_state\n\nglobal_state.set_app_settings(dict(max_column_width=100))\n```\n\n### Editing Cells\n\nYou may edit any cells in your grid (with the exception of the row indexes or headers, the ladder can be edited using the [Rename](#rename) column menu function).\n\nIn order to edit a cell simply double-click on it.  This will convert it into a text-input field and you should see a blinking cursor.  In addition to turning that cell into an input it will also display an input at the top of the screen for better viewing of long strings. It is assumed that the value you type in will match the data type of the column you editing.  For example:\n\n* integers -> should be a valid positive or negative integer\n* float -> should be a valid positive or negative float\n* string -> any valid string will do\n* category -> either a pre-existing category or this will create a new category for (so beware!)\n* date, timestamp, timedelta -> should be valid string versions of each\n* boolean -> any string you input will be converted to lowercase and if it equals \"true\" then it will make the cell `True`, otherwise `False`\n\nUsers can make use of two protected values as well:\n\n* \"nan\" -> `numpy.nan`\n* \"inf\" -> `numpy.inf`\n\nTo save your change simply press \"Enter\" or to cancel your changes press \"Esc\".\n\nIf there is a conversion issue with the value you have entered it will display a popup with the specific exception in question.\n\nHere's a quick demo:\n\n[![](http://img.youtube.com/vi/MY5w0m_4IAc/0.jpg)](http://www.youtube.com/watch?v=MY5w0m_4IAc \"Editing Long String Cells\")\n\nHere's a demo of editing cells with long strings:\n\n[![](http://img.youtube.com/vi/3p9ltzdBaDQ/0.jpg)](http://www.youtube.com/watch?v=3p9ltzdBaDQ \"Editing Cells\")\n\n\n### Copy Cells Into Clipboard\n\n|Select|Copy|Paste|\n|:-----:|:-----:|:-----:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/select_range1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/select_range2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/select_range3.png)|\n\nOne request that I have heard time and time again while working on D-Tale is \"it would be great to be able to copy a range of cells into excel\".  Well here is how that is accomplished:\n1) Shift + Click on a cell\n2) Shift + Click on another cell (this will trigger a popup)\n3) Choose whether you want to include headers in your copy by clicking the checkbox\n4) Click Yes\n5) Go to your excel workbook and execute Ctrl + V or manually choose \"Paste\"\n  * You can also paste this into a standard text editor and what you're left with is tab-delimited data\n\n\n### Main Menu Functions\n\n#### XArray Operations\n\n* **Convert To XArray**: If you have are currently viewing a pandas dataframe in D-Tale you will be given this option to convert your data to an `xarray.Dataset`.  It is as simple as selecting one or many columns as an index and then your dataframe will be converted to a dataset (`df.set_index([...]).to_xarray()`) which makes toggling between indexes slices much easier.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/xarray_indexes.png)\n\n* **XArray Dimensions**: If you are currently viewing data associated with an `xarray.Dataset` you will be given the ability to toggle which dimension coordinates you're viewing by clicking this button.  You can select values for all, some or none (all data - no filter) of your coordinates and the data displayed in your grid will match your selections.  Under the hood the code being executed is as follows: `ds.sel(dim1=coord1,...).to_dataframe()`\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/xarray_dimensions.png)\n\n#### Describe\nView all the columns & their data types as well as individual details of each column\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe.png)\n\n|Data Type|Display|Notes|\n|--------|:------:|:------:|\n|date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_date.png)||\n|string|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_string.png)|If you have less than or equal to 100 unique values they will be displayed at the bottom of your popup|\n|int|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_int.png)|Anything with standard numeric classifications (min, max, 25%, 50%, 75%) will have a nice boxplot with the mean (if it exists) displayed as an outlier if you look closely.|\n|float|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Describe_float.png)||\n\n#### Outlier Detection\nWhen viewing integer & float columns in the [\"Describe\" popup](#describe) you will see in the lower right-hand corner a toggle for Uniques & Outliers.\n\n|Outliers|Filter|\n|--------|------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/outliers.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/outlier_filter.png)|\n\nIf you click the \"Outliers\" toggle this will load the top 100 outliers in your column based on the following code snippet:\n```python\ns = df[column]\nq1 = s.quantile(0.25)\nq3 = s.quantile(0.75)\niqr = q3 - q1\niqr_lower = q1 - 1.5 * iqr\niqr_upper = q3 + 1.5 * iqr\noutliers = s[(s < iqr_lower) | (s > iqr_upper)]\n```\nIf you click on the \"Apply outlier filter\" link this will add an addtional \"outlier\" filter for this column which can be removed from the [header](#header) or the [custom filter](#custom-filter) shown in picture above to the right.\n\n#### Custom Filter\n\n**Starting with version 3.7.0 this feature will be turned off by default.\nCustom filters are vulnerable to code injection attacks, please only use in trusted environments.**\n\n**You can turn this feature on by doing one of the following:**\n - **add `enable_custom_filters=True` to your `dtale.show` call**\n - **add `enable_custom_filters = False` to the [app] section of your dtale.ini config file ([more info](https://github.com/man-group/dtale/blob/master/docs/CONFIGURATION.md))**\n - **run this code before calling dtale.show:**\n```python\nimport dtale.global_state as global_state\nglobal_state.set_app_settings(dict(enable_custom_filters=True))\n```\n\n\nApply a custom pandas `query` to your data (link to pandas documentation included in popup)  \n\n|Editing|Result|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Filter_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_filter.png)|\n\nYou can also see any outlier or column filters you've applied (which will be included in addition to your custom query) and remove them if you'd like.\n\nContext Variables are user-defined values passed in via the `context_variables` argument to dtale.show(); they can be referenced in filters by prefixing the variable name with '@'.\n\nFor example, here is how you can use context variables in a pandas query:\n```python\nimport pandas as pd\n\ndf = pd.DataFrame([\n  dict(name='Joe', age=7),\n  dict(name='Bob', age=23),\n  dict(name='Ann', age=45),\n  dict(name='Cat', age=88),\n])\ntwo_oldest_ages = df['age'].nlargest(2)\ndf.query('age in @two_oldest_ages')\n```\nAnd here is how you would pass that context variable to D-Tale: `dtale.show(df, context_variables=dict(two_oldest_ages=two_oldest_ages))`\n\nHere's some nice documentation on the performance of [pandas queries](https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#pandas-eval-performance)\n\n#### Dataframe Functions\n\n[![](http://img.youtube.com/vi/G6wNS9-lG04/0.jpg)](http://www.youtube.com/watch?v=G6wNS9-lG04 \"Dataframe Functions in D-Tale\")\n\nThis video shows you how to build the following:\n - Numeric: adding/subtracting two columns or columns with static values\n - Bins: bucketing values using pandas cut & qcut as well as assigning custom labels\n - Dates: retrieving date properties (hour, weekday, month...) as well as conversions (month end)\n - Random: columns of data type (int, float, string & date) populated with random uniformly distributed values.\n  - Type Conversion: switch columns from one data type to another, fun. :smile:\n\n\n#### Merge & Stack\n\n[![](http://img.youtube.com/vi/ignDS6OaGVQ/0.jpg)](http://www.youtube.com/watch?v=ignDS6OaGVQ \"Merge & Stack\")\n\nThis feature allows users to merge or stack (vertically concatenate) dataframes they have loaded into D-Tale.  They can also upload additional data to D-Tale while wihin this feature.  The demo shown above goes over the following actions:\n- Editing of parameters to either a pandas merge or stack (vertical concatenation) of dataframes\n  - Viewing direct examples of each from the pandas documentation\n- Selection of dataframes\n- Uploading of additional dataframes from an excel file\n- Viewing code & resulting data from a merge or stack\n\n#### Summarize Data\n\nThis is very powerful functionality which allows users to create a new data from currently loaded data.  The operations currently available are:\n- **Aggregation**: consolidate data by running different aggregations on columns by a specific index\n- **Pivot**: this is simple wrapper around [pandas.Dataframe.pivot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pivot.html) and [pandas.pivot_table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html)\n- **Transpose**: transpose your data on a index (be careful dataframes can get very wide if your index has many unique values)\n\n|Function|Data|\n|:------:|:------:|\n|No Reshaping|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/original_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_col.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_col_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_func.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/agg_func_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/pivot.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/pivot_data.png)|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/transpose.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/reshape/transpose_data.png)|\n\n[![](http://img.youtube.com/vi/fYsxogXKZ2c/0.jpg)](http://www.youtube.com/watch?v=fYsxogXKZ2c \"Reshaping Tutorial\")\n\n#### Duplicates\nRemove duplicate columns/values from your data as well as extract duplicates out into separate instances.\n\nThe folowing screen shots are for a dataframe with the following data:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/data.png)\n\n|Function|Description|Preview|\n|:------:|:---------:|:-----:|\n|**Remove Duplicate Columns**|Remove any columns that contain the same data as another and you can either keep the first, last or none of these columns that match this criteria.  You can test which columns will be removed by clicking the \"View Duplicates\" button.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/columns.png)|\n|**Remove Duplicate Column Names**|Remove any columns with the same name (name comparison is case-insensitive) and you can either keep the first, last or none of these columns that match this criteria. You can test which columns will be removed by clicking the \"View Duplicates\" button.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/column_names.png)|\n|**Remove Duplicate Rows**|Remove any rows from your dataframe where the values of a subset of columns are considered duplicates. You can choose to keep the first, last or none of the rows considered duplicated.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/rows.png)|\n|**Show Duplicates**|Break any duplicate rows (based on a subset of columns) out into another dataframe viewable in your D-Tale session. You can choose to view all duplicates or select specific groups based on the duplicated value.|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/duplicates/show.png)|\n\n#### Missing Analysis\nDisplay charts analyzing the presence of missing (NaN) data in your dataset using the [missingno](https://github.com/ResidentMario/missingno) pacakage.  You can also open them in a tab by themselves or export them to a static PNG using the links in the upper right corner. You can also close the side panel using the ESC key.\n\n\n| Chart        | Sample |\n|--------------|--------|\n| Matrix     | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/matrix.png)|\n| Bar        | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/bar.png)|\n| Heatmap    | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/heatmap.png)|\n| Dendrogram | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/missingno/dendrogram.png)|\n\n#### Charts\nBuild custom charts based off your data(powered by [plotly/dash](https://github.com/plotly/dash)).\n \n - The Charts will open in a tab because of the fact there is so much functionality offered there you'll probably want to be able to reference the main grid data in the original tab\n - To build a chart you must pick a value for X & Y inputs which effectively drive what data is along the X & Y axes\n   - If you are working with a 3-Dimensional chart (heatmap, 3D Scatter, Surface) you'll need to enter a value for the Z axis as well\n - Once you have entered all the required axes a chart will be built\n - If your data along the x-axis (or combination of x & y in the case of 3D charts) has duplicates you have three options:\n   - Specify a group, which will create series for each group\n   - Specify an aggregation, you can choose from one of the following: Count, First, Last, Mean, Median, Minimum, Maximum, Standard Deviation, Variance, Mean Absolute Deviation, Product of All Items, Sum, Rolling\n     - Specifying a \"Rolling\" aggregation will also require a Window & a Computation (Correlation, Count, Covariance, Kurtosis, Maximum, Mean, Median, Minimum, Skew, Standard Deviation, Sum or Variance)\n     - For heatmaps you will also have access to the \"Correlation\" aggregation since viewing correlation matrices in heatmaps is very useful.  This aggregation is not supported elsewhere\n   - Specify both a group & an aggregation\n - You now have the ability to toggle between different chart types: line, bar, pie, wordcloud, heatmap, 3D scatter & surface\n - If you have specified a group then you have the ability between showing all series in one chart and breaking each series out into its own chart \"Chart per Group\"\n\nHere are some examples:\n\n|Chart Type|Chart|Chart per Group|\n|:------:|:------:|:------:|\n|line|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/line.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/line_pg.png)|\n|bar|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_pg.png)|\n|stacked|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/stacked.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/stacked_pg.png)|\n|pie|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/pie.png)||\n|wordcloud|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/wordcloud.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/wordcloud_pg.png)|\n|heatmap|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/heatmap.png)||\n|3D scatter|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/3d_scatter.png)||\n|surface|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/surface.png)||\n|Maps (Scatter GEO)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/scattergeo.png)||\n|Maps (Choropleth)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/choropleth.png)||\n\nY-Axis Toggling\n\nUsers now have the ability to toggle between 3 different behaviors for their y-axis display:\n- *Default*: selecting this option will use the default behavior that comes with plotly for your chart's y-axis\n- *Single*: this allows users to set the range of all series in your chart to be on the same basis as well as making that basis (min/max) editable\n- *Multi*: this allows users to give each series its own y-axis and making that axis' range editable\n\nHere's a quick tutorial: [![](http://img.youtube.com/vi/asblF-rAACY/0.jpg)](http://www.youtube.com/watch?v=asblF-rAACY \"Y-Axis Toggling\")\n\nAnd some screenshots:\n\n|Default|Single|Multi|\n|:------:|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/default.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/single.PNG)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/axis_toggle/multi.PNG)|\n\nWith a bar chart that only has a single Y-Axis you have the ability to sort the bars based on the values for the Y-Axis\n\n|Pre-sort|Post-sort|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_presort.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/charts/bar_postsort.png)|\n\n**Popup Charts**\n\nViewing multiple charts at once and want to separate one out into its own window or simply move one off to the side so you can work on building another for comparison?  Well now you can by clicking the \"Popup\" button :smile:\n\n**Copy Link**\n\nWant to send what you're looking at to someone else?  Simply click the \"Copy Link\" button and it will save a pre-populated chart URL into your clipboard. As long as your D-Tale process is still running when that link is opened you will see your original chart.\n\n**Exporting Charts**\n\nYou can now export your dash charts (with the exception of Wordclouds) to static HTML files which can be emailed to others or saved down to be viewed at a later time.  The best part is that all of the javascript for plotly is embedded in these files so the nice zooming, panning, etc is still available! :boom:\n\n**Exporting CSV**\n\nI've been asked about being able to export the data that is contained within your chart to a CSV for further analysis in tools like Excel.  This button makes that possible.\n\n### OFFLINE CHARTS\n\nWant to run D-Tale in a jupyter notebook and build a chart that will still be displayed even after your D-Tale process has shutdown?  Now you can!  Here's an example code snippet show how to use it:\n\n```\nimport dtale\n\ndef test_data():\n    import random\n    import pandas as pd\n    import numpy as np\n\n    df = pd.DataFrame([\n        dict(x=i, y=i % 2)\n        for i in range(30)\n    ])\n    rand_data = pd.DataFrame(np.random.randn(len(df), 5), columns=['z{}'.format(j) for j in range(5)])\n    return pd.concat([df, rand_data], axis=1)\n\nd = dtale.show(test_data())\nd.offline_chart(chart_type='bar', x='x', y='z3', agg='sum')\n```\n[![](http://img.youtube.com/vi/DseSmc3fZvc/0.jpg)](http://www.youtube.com/watch?v=DseSmc3fZvc \"Offline Charts Tutorial\")\n\n**Pro Tip: If generating offline charts in jupyter notebooks and you run out of memory please add the following to your command-line when starting jupyter**\n\n`--NotebookApp.iopub_data_rate_limit=1.0e10`\n\n\n**Disclaimer: Long Running Chart Requests**\n\nIf you choose to build a chart that requires a lot of computational resources then it will take some time to run.  Based on the way Flask & plotly/dash interact this will block you from performing any other request until it completes.  There are two courses of action in this situation:\n\n1) Restart your jupyter notebook kernel or python console\n2) Open a new D-Tale session on a different port than the current session.  You can do that with the following command: `dtale.show(df, port=[any open port], force=True)`\n\nIf you miss the legacy (non-plotly/dash) charts, not to worry!  They are still available from the link in the upper-right corner, but on for a limited time...\nHere is the documentation for those: [Legacy Charts](https://github.com/man-group/dtale/blob/master/docs/LEGACY_CHARTS.md)\n\n**Your Feedback is Valuable**\n\nThis is a very powerful feature with many more features that could be offered (linked subplots, different statistical aggregations, etc...) so please submit issues :)\n\n#### Network Viewer\n\nThis tool gives users the ability to visualize directed graphs.  For the screenshots I'll beshowing for this functionality we'll be working off a dataframe with the following data:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/data.png)\n\nStart by selecting columns containing the \"To\" and \"From\" values for the nodes in you network and then click \"Load\":\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_to_from.png)\n\nYou can also see instructions on to interact with the network by expanding the directions section by clicking on the header \"Network Viewer\" at the top.  You can also view details about the network provided by the package [networkx](https://github.com/networkx) by clicking the header \"Network Analysis\".\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_expanded.png)\n\nSelect a column containing weighting for the edges of the nodes in the \"Weight\" column and click \"Load\":\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_weight.png)\n\nSelect a column containing group information for each node in the \"From\" column by populating \"Group\" and then clicking \"Load\":\n ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_group.png)\n\n Perform shortest path analysis by doing a Shift+Click on two nodes:\n ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_shortest_path.png)\n\nView direct descendants of each node by clicking on it:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/network/network_descendants.png)\n\nYou can zoom in on nodes by double-clicking and zoom back out by pressing \"Esc\".\n\n#### Correlations\nShows a pearson correlation matrix of all numeric columns against all other numeric columns\n  - By default, it will show a grid of pearson correlations (filtering available by using drop-down see 2nd table of screenshots)\n  - If you have a date-type column, you can click an individual cell and see a timeseries of pearson correlations for that column combination\n    - Currently if you have multiple date-type columns you will have the ability to toggle between them by way of a drop-down\n  - Furthermore, you can click on individual points in the timeseries to view the scatter plot of the points going into that correlation\n    - Within the scatter plot section you can also view the details of the PPS for those data points in the chart by hovering over the number next to \"PPS\"\n\n|Matrix|PPS|Timeseries|Scatter|\n|------|---|----------|-------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_pps.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_ts.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_scatter.png)|\n\n|Col1 Filtered|Col2 Filtered|Col1 & Col2 Filtered|\n|------|----------|-------|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_col1.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_col2.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Correlations_both.png)|\n\nWhen the data being viewed in D-Tale has date or timestamp columns but for each date/timestamp vlaue there is only one row of data the behavior of the Correlations popup is a little different\n  - Instead of a timeseries correlation chart the user is given a rolling correlation chart which can have the window (default: 10) altered\n  - The scatter chart will be created when a user clicks on a point in the rollign correlation chart.  The data displayed in the scatter will be for the ranges of dates involved in the rolling correlation for that date.\n\n|Data|Correlations|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr_data.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr.png)|\n\n#### Predictive Power Score\nPredictive Power Score (using the package [ppscore](https://github.com/8080labs/ppscore)) is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix). WARNING: This could take a while to load.\n\nThis page works similar to the [Correlations](#correlations) page but uses the PPS calcuation to populate the grid and by clicking on cells you can view the details of the PPS for those two columns in question.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rolling_corr_data.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/pps.png)\n\n#### Heat Map\nThis will hide any non-float or non-int columns (with the exception of the index on the right) and apply a color to the background of each cell.\n\n  - Each float is renormalized to be a value between 0 and 1.0\n  - You have two options for the renormalization\n    - **By Col**: each value is calculated based on the min/max of its column\n    - **Overall**: each value is caluclated by the overall min/max of all the non-hidden float/int columns in the dataset\n  - Each renormalized value is passed to a color scale of red(0) - yellow(0.5) - green(1.0)\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Heatmap.png)\n\nTurn off Heat Map by clicking menu option you previously selected one more time\n\n#### Highlight Dtypes\nThis is a quick way to check and see if your data has been categorized correctly.  By clicking this menu option it will assign a specific background color to each column of a specific data type.\n\n|category|timedelta|float|int|date|string|bool|\n|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n|purple|orange|green|light blue|pink|white|yellow\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_dtypes.png)\n\n#### Highlight Missing\n\n* Any cells which contain `nan` values will be highlighted in yellow.\n* Any string column cells which are empty strings or strings consisting only of spaces will be highlighted in orange.\n*  \u2757will be prepended to any column header which contains missing values.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_missing.png)\n\n#### Highlight Outliers\nHighlight any cells for numeric columns which surpass the upper or lower bounds of a [custom outlier computation](#outlier-detection). \n* Lower bounds outliers will be on a red scale, where the darker reds will be near the maximum value for the column.\n* Upper bounds outliers will be on a blue scale, where the darker blues will be closer to the minimum value for the column.\n* \u2b50 will be prepended to any column header which contains outliers.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_outliers.png)\n\n#### Highlight Range\nHighlight any range of numeric cells based on three different criteria:\n* equals\n* greater than\n* less than\n\nYou can activate as many of these criteria as you'd like nad they will be treated as an \"or\" expression.  For example, `(x == 0) or (x < -1) or (x > 1)`\n\n|Selections|Output|\n|:------:|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_range_selections.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_range_output.png)|\n\n#### Low Variance Flag\nShow flags on column headers where both these conditions are true:\n* Count of unique values / column size < 10%\n* Count of most common value / Count of second most common value > 20\n\nHere's an example of what this will look like when you apply it:\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/highlight_range_selections.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/low_variance.png)\n\n#### Code Exports\n*Code Exports* are small snippets of code representing the current state of the grid you're viewing including things like:\n - columns built\n - filtering\n - sorting\n\nOther code exports available are:\n - Describe (Column Analysis)\n - Correlations (grid, timeseries chart & scatter chart)\n - Charts built using the Chart Builder\n\n [![](http://img.youtube.com/vi/6CkKgpv3d6I/0.jpg)](http://www.youtube.com/watch?v=6CkKgpv3d6I \"Code Export Tutorial\")\n\n|Type|Code Export|\n|:------:|:------:|\n|Main Grid|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/main.png)|\n|Histogram|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/histogram.png)|\n|Describe|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/describe.png)|\n|Correlation Grid|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/main.png)|\n|Correlation Timeseries|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/corr_ts.png)|\n|Correlation Scatter|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/corr_scatter.png)|\n|Charts|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/code_export/charts.png)|\n\n#### Export CSV\n\nExport your current data to either a CSV or TSV file:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/export_csv.png)\n\n#### Load Data & Sample Datasets\n\nSo either when starting D-Tale with no pre-loaded data or after you've already loaded some data you now have the ability to load data or choose from some sample datasets directly from the GUI:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/load_data.png)\n\nHere's the options at you disposal:\n* Load a CSV/TSV file by dragging a file to the dropzone in the top or select a file by clicking the dropzone\n* Load a CSV/TSV or JSON directly from the web by entering a URL (also throw in a proxy if you are using one)\n* Choose from one of our sample datasets:\n  * US COVID-19 data from NY Times (updated daily)\n  * Script breakdowns of popular shows Seinfeld & The Simpsons\n  * Movie dataset containing release date, director, actors, box office, reviews...\n  * Video games and their sales\n  * pandas.util.testing.makeTimeDataFrame\n\n\n#### Instances\nThis will give you information about other D-Tale instances are running under your current Python process.\n\nFor example, if you ran the following script:\n```python\nimport pandas as pd\nimport dtale\n\ndtale.show(pd.DataFrame([dict(foo=1, bar=2, biz=3, baz=4, snoopy_D_O_double_gizzle=5)]))\ndtale.show(pd.DataFrame([\n    dict(a=1, b=2, c=3, d=4),\n    dict(a=2, b=3, c=4, d=5),\n    dict(a=3, b=4, c=5, d=6),\n    dict(a=4, b=5, c=6, d=7)\n]))\ndtale.show(pd.DataFrame([range(6), range(6), range(6), range(6), range(6), range(6)]), name=\"foo\")\n```\nThis will make the **Instances** button available in all 3 of these D-Tale instances. Clicking that button while in the first instance invoked above will give you this popup:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Instances.png)\n\nThe grid above contains the following information:\n  - Process: timestamp when the process was started along with the name (if specified in `dtale.show()`)\n  - Rows: number of rows\n  - Columns: number of columns\n  - Column Names: comma-separated string of column names (only first 30 characters, hover for full listing)\n  - Preview: this button is available any of the non-current instances.  Clicking this will bring up left-most 5X5 grid information for that instance\n  - The row highlighted in green signifys the current D-Tale instance\n  - Any other row can be clicked to switch to that D-Tale instance\n\nHere is an example of clicking the \"Preview\" button:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Instances_preview.png)\n\n#### About\nThis will give you information about what version of D-Tale you're running as well as if its out of date to whats on PyPi.\n\n|Up To Date|Out Of Date|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/About-up-to-date.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/About-out-of-date.png)|\n\n#### Refresh Widths\nMostly a fail-safe in the event that your columns are no longer lining up. Click this and should fix that\n\n#### Theme\nToggle between light & dark themes for your viewing pleasure (only affects grid, not popups or charts).\n\n|Light|Dark|\n|--------|:------:|\n|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/theme/light.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/theme/dark.png)|\n\n#### Reload Data\nForce a reload of the data from the server for the current rows being viewing in the grid by clicking this button. This can be helpful when viewing the grid from within another application like jupyter or nested within another website.\n\n#### Unpin/Pin Menu\nIf you would like to keep your menu pinned to the side of your grid all times rather than always having to click the triaangle in the upper left-hand corner simply click this button.  It is persisted back to the server so that it can be applied to all piece of data you've loaded into your session and beyond refreshes.\n\n#### Language\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/chinese_dtale.png)\n\nI am happy to announce that D-Tale now supports both English & Chinese (there is still more of the translation to be completed but the infrastructure is there).  And we are happy to add support for any other languages.  Please see instruction on how, [here](#adding-language-support).\n\n#### Shutdown\nPretty self-explanatory, kills your D-Tale session (there is also an auto-kill process that will kill your D-Tale after an hour of inactivity)\n\n### Column Menu Functions\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Col_menu.png)\n\n#### Filtering\n\n[![](http://img.youtube.com/vi/8zo5ZiI1Yzo/0.jpg)](http://www.youtube.com/watch?v=8zo5ZiI1Yzo \"Column Filtering\")\n\nThese interactive filters come in 3 different types: String, Numeric & Date.  Note that you will still have the ability to apply custom filters from the \"Filter\" popup on the main menu, but it will get applied in addition to any column filters.\n\n|Type|Filter|Data Types|Features|\n|----|------|----------|--------|\n|String|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/string.PNG)|strings & booleans|The ability to select multiple values based on what exists in the column. Notice the \"Show Missing Only\" toggle, this will only show up if your column has nan values|\n|Date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/dates.PNG)|dates|Specify a range of dates to filter on based on start & end inputs|\n|Numeric|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/filters/numeric.PNG)|ints & floats|For integers the \"=\" will be similar to strings where you can select multiple values based on what exists in the column.  You also have access to other operands: <,>,<=,>=,() - \"Range exclusve\", [] - \"Range inclusive\".|\n\n#### Moving Columns\n\n[![](http://img.youtube.com/vi/We4TH477rRs/0.jpg)](http://www.youtube.com/watch?v=We4TH477rRs \"Moving Columns in D-Tale\")\n\nAll column movements are saved on the server so refreshing your browser won't lose them :ok_hand:\n\n#### Hiding Columns\n\n[![](http://img.youtube.com/vi/ryZT2Lk_YaA/0.jpg)](http://www.youtube.com/watch?v=ryZT2Lk_YaA \"Hide/Unhide Columns in D-Tale\")\n\nAll column movements are saved on the server so refreshing your browser won't lose them :ok_hand:\n\n#### Delete\n\nAs simple as it sounds, click this button to delete this column from your dataframe.\n\n#### Rename\n\nUpdate the name of any column in your dataframe to a name that is not currently in use by your dataframe.\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/rename.png)\n\n#### Replacements\n\nThis feature allows users to replace content on their column directly or for safer purposes in a brand new column.  Here are the options you have:\n\n|Type        |Data Types   |Description|Menu    |\n|------------|-------------|-----------|--------|\n|Value(s)    |all          |Replace specific values in a column with raw values, output from another column or an aggregation on your column|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_value.png)|\n|Spaces Only |strings      |Replace string values consisting only of spaces with raw values|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_spaces.png)|\n|Contains Char/Substring|strings      |Replace string values containing a specific character or substring|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_strings.png)|\n|Scikit-Learn Imputer|numeric      |Replace missing values with the output of using different Scikit-Learn imputers like iterative, knn & simple|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/replacements_sklearn.png)|\n\nHere's a quick demo: [![](http://img.youtube.com/vi/GiNFRtcpIt8/0.jpg)](http://www.youtube.com/watch?v=GiNFRtcpIt8 \"Column Replacements\")\n\n#### Lock\nAdds your column to \"locked\" columns\n  - \"locked\" means that if you scroll horizontally these columns will stay pinned to the right-hand side\n  - this is handy when you want to keep track of which date or security_id you're looking at\n  - by default, any index columns on the data passed to D-Tale will be locked\n\n#### Unlock\nRemoved column from \"locked\" columns\n\n#### Sorting\nApplies/removes sorting (Ascending/Descending/Clear) to the column selected\n  \n*Important*: as you add sorts they sort added will be added to the end of the multi-sort.  For example:\n\n| Action        | Sort           |\n| ------------- |:--------------:|\n| click \"a\"     |                |\n| sort asc      | a (asc)        |\n| click \"b\"     | a (asc)        |\n| sort desc     | a (asc), b(desc)|\n| click \"a\"     | a (asc), b(desc)|\n| sort None     | b(desc)|\n| sort desc     | b(desc), a(desc)|\n| click \"X\" on sort display | |\n\n#### Formats\nApply simple formats to numeric values in your grid\n\n|Type|Editing|Result|\n|--------|:------:|:------:|\n|Numeric|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_formatting.png)|\n|Date|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_date_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_date_formatting.png)|\n|String|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Formatting_string_apply.png)|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/Post_string_formatting.png)|\n\nFor all data types you have the ability to change what string is ued for display.\n\n\nFor numbers here's a grid of all the formats available with -123456.789 as input:\n  \n| Format        | Output         |\n| ------------- |:--------------:|\n| Precision (6) | -123456.789000 |\n| Thousands Sep | -123,456.789   |\n| Abbreviate    | -123k          |\n| Exponent      | -1e+5          |\n| BPS           | -1234567890BPS |\n| Red Negatives | <span style=\"color: red;\">-123457</span>|\n\nFor strings you can apply the follwoing formats:\n* **Truncation:** truncate long strings to a certain number of characters and replace with an allipses \"...\" and see the whole value on hover.\n* **Hyperlinks:** If your column is comprised of URL strings you can make them hyperlinks which will open a new tab\n\n#### Describe (Column Analysis)\nBased on the data type of a column different charts will be shown.  This side panel can be closed using the 'X' button in the upper right or by pressing the ESC key.\n\n| Chart         | Data Types     | Sample |\n|---------------|----------------|--------|\n| Box Plot      | Float, Int, Date | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/boxplot.png)|\n| Histogram     | Float, Int |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/histogram.PNG)|\n| Value Counts  | Int, String, Bool, Date, Category|![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/value_counts.PNG)|\n| Word Value Counts | String | ![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/word_value_counts.png)|\n| Category      | Float   |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/category.PNG)|\n| Geolocation*  | Int, Float     |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/geolocation.PNG)|\n| Q-Q Plot      | Int, Float, Date |![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/qq.png)|\n\n\n**Histogram** can be displayed in any number of bins (default: 20), simply type a new integer value in the bins input\n\n**Value Count** by default, show the top 100 values ranked by frequency.  If you would like to show the least frequent values simply make your number negative (-10 => 10 least frequent value)\n\n**Value Count w/ Ordinal** you can also apply an ordinal to your **Value Count** chart by selecting a column (of type int or float) and applying an aggregation (default: sum) to it (sum, mean, etc...) this column will be grouped by the column you're analyzing and the value produced by the aggregation will be used to sort your bars and also displayed in a line.  Here's an example:\n\n![](https://raw.githubusercontent.com/aschonfeld/dtale-media/master/images/analysis/value_counts_ordinal.PNG\n)\n\n**Word Value Count** you can analyze string data by splitting each record by spaces to see the counts of each word.  This chart has all the same functions available as \"Value Counts\".  In addition, you can select multiple \"Cleaner\" functions to be applied to your column before building word values. These functions will perform operations like removing punctuation, removing numeric character & normalizing accent characters.\n\n**Category (Category Breakdown)** when viewing float columns you can also see them broken down by a categorical column (string, date, int, etc...).  This means that when you select a category column this will then display the frequency of each category in a line as well as bars based on the float column you're analyzing grouped by that category and computed by your aggregation (default: mean).\n\n**Geolocation** when your data contains latitude & longitude then you can view the coverage in a plotly scattergeo map.  In order to have access this chart your dataframe must have at least one of each of these types of columns:\n* \"lat\" must be contained within the lower-cased version of the column name and values be between -90 & 90\n* \"lon\" must be contained within the lower-cased version of the column name and values be between -180 & 180\n\n### Hotkeys\n\nThese are key combinations you can use in place of clicking actual buttons to save a little time:\n\n| Keymap      | Action         |\n|-------------|----------------|\n|`shift+m`    | Opens main menu*|\n|`shift+d`    | Opens \"Describe\" page*|\n|`shift+f`    | Opens \"Custom Filter\"*|\n|`shift+b`    | Opens \"Build Column\"*|\n|`shift+c`    | Opens \"Charts\" page*|\n|`shift+x`    | Opens \"Code Export\"*|\n|`esc`        | Closes any open modal window or side panel & exits cell editing|\n\n`*` Does not fire if user is actively editing a cell.\n\n### Menu Functions Depending on Browser Dimensions\nDepending on the dimensions of your browser window the following buttons will not open modals, but rather separate browser windows:  Correlations, Describe & Instances (see images from [Jupyter Notebook](#jupyter-notebook), also Charts will always open in a separate browser window)\n\n## For Developers\n\n### Cloning\n\nClone the code (git clone ssh://git@github.com:manahl/dtale.git), then start the backend server:\n\n```bash\n$ git clone ssh://git@github.com:manahl/dtale.git\n# install the dependencies\n$ python setup.py develop\n# start the server\n$ dtale --csv-path /home/jdoe/my_csv.csv --csv-parse_dates date\n```\n\nYou can also run dtale from PyDev directly.\n\nYou will also want to import javascript dependencies and build the source (all javascript code resides in the `frontend` folder):\n\n``` bash\n$ cd frontend\n$ npm install\n# 1) a persistent server that serves the latest JS:\n$ npm run watch\n# 2) or one-off build:\n$ npm run build\n```\n\n### Running tests\n\nThe usual npm test command works:\n\n```\n$ npm test\n```\n\nYou can run individual test files:\n\n```\n$ npm run test -- static/__tests__/dtale/DataViewer-base-test.jsx\n```\n\n### Linting\n\nYou can lint all the JS and CSS to confirm there's nothing obviously wrong with\nit:\n\n``` bash\n$ npm run lint\n```\n\nYou can also lint individual JS files:\n\n``` bash\n$ npm run lint-js-file -s -- static/dtale/DataViewer.jsx\n```\n\n### Formatting JS\n\nYou can auto-format code as follows:\n\n``` bash\n$ npm run format\n```\n\n### Docker Development\n\nYou can build python 27-3 & run D-Tale as follows:\n```bash\n$ yarn run build\n$ docker-compose build dtale_2_7\n$ docker run -it --network host dtale_2_7:latest\n$ python\n>>> import pandas as pd\n>>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n>>> import dtale\n>>> dtale.show(df)\n```\nThen view your D-Tale instance in your browser using the link that gets printed\n\nYou can build python 36-1 & run D-Tale as follows:\n```bash\n$ yarn run build\n$ docker-compose build dtale_3_6\n$ docker run -it --network host dtale_3_6:latest\n$ python\n>>> import pandas as pd\n>>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n>>> import dtale\n>>> dtale.show(df)\n```\nThen view your D-Tale instance in your browser using the link that gets printed\n\n\n### Adding Language Support\n\nCurrently D-Tale support both english & chinese but other languages will gladly be supported.  To add another language simply open a pull request with the following:\n- cake a copy & translate the values in the following JSON english JSON files and save them to the same locations as each file\n - [Back-End](https://github.com/man-group/dtale/blob/master/dtale/translations/en.json)\n - [Front-End](https://github.com/man-group/dtale/blob/master/static/translations/en.json)\n- please make the name of these files the name of the language you are adding (currently english -> en, chinese -> cn) \n- be sure to keep the keys in english, that is important\n\nLooking forward to what languages come next! :smile:\n\n\n## Global State/Data Storage\n\nIf D-Tale is running in an environment with multiple python processes (ex: on a web server running [gunicorn](https://github.com/benoitc/gunicorn)) it will most likely encounter issues with inconsistent state.  Developers can fix this by configuring the system D-Tale uses for storing data.  Detailed documentation is available here: [Data Storage and managing Global State](https://github.com/man-group/dtale/blob/master/docs/GLOBAL_STATE.md)\n\n\n## Startup Behavior\n\nHere's a little background on how the `dtale.show()` function works:\n - by default it will look for ports between 40000 & 49000, but you can change that range by specifying the environment variables DTALE_MIN_PORT & DTALE_MAX_PORT\n - think of sessions as python consoles or jupyter notebooks\n\n1) Session 1 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1|http://localhost:40000/dtale/main/1|\n\n2) Session 1 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1,2|http://localhost:40000/dtale/main/[1,2]|\n\n2) Session 2 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40000|1,2|http://localhost:40000/dtale/main/[1,2]|\n|2|40001|1|http://localhost:40001/dtale/main/1|\n\n3) Session 1 executes `dtale.show(df, port=40001, force=True)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n\n4) Session 3 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n\n5) Session 2 executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n\n6) Session 4 executes `dtale.show(df, port=8080)` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|1,2,3|http://localhost:40001/dtale/main/[1,2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n\n7) Session 1 executes `dtale.get_instance(1).kill()` our state is:\n\n|Session|Port|Active Data IDs|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|2,3|http://localhost:40001/dtale/main/[2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n\n7) Session 5 sets DTALE_MIN_RANGE to 30000 and DTALE_MAX_RANGE 39000 and executes `dtale.show(df)` our state is:\n\n|Session|Port|Active Data ID(s)|URL(s)|\n|:-----:|:-----:|:-----:|:-----:|\n|1|40001|2,3|http://localhost:40001/dtale/main/[2,3]|\n|3|40000|1|http://localhost:40000/dtale/main/1|\n|2|40002|1|http://localhost:40002/dtale/main/1|\n|4|8080|1|http://localhost:8080/dtale/main/1|\n|5|30000|1|http://localhost:30000/dtale/main/1|\n\n## Documentation\n\nHave a look at the [detailed documentation](https://dtale.readthedocs.io).\n\n## Dependencies\n\n* Back-end\n  * dash\n  * dash_daq\n  * Flask\n  * Flask-Compress\n  * flask-ngrok\n  * Pandas\n  * plotly\n  * scikit-learn\n  * scipy\n  * xarray\n  * arctic [extra]\n  * redis [extra]\n  * rpy2 [extra]\n* Front-end\n  * react-virtualized\n  * chart.js\n\n## Acknowledgements\n\nD-Tale has been under active development at [Man Numeric](http://www.numeric.com/) since 2019.\n\nOriginal concept and implementation: [Andrew Schonfeld](https://github.com/aschonfeld)\n\nContributors:\n\n * [Phillip Dupuis](https://github.com/phillipdupuis)\n * [Fernando Saravia Rajal](https://github.com/fersarr)\n * [Dominik Christ](https://github.com/DominikMChrist)\n * [Reza Moshksar](https://github.com/reza1615)\n * [Bertrand Nouvel](https://github.com/bnouvelbmll)\n * [Chris Boddy](https://github.com/cboddy)\n * [Jason Holden](https://github.com/jasonkholden)\n * [Tom Taylor](https://github.com/TomTaylorLondon)\n * [Wilfred Hughes](https://github.com/Wilfred)\n * Mike Kelly\n * [Vincent Riemer](https://github.com/vincentriemer)\n * [Youssef Habchi](http://youssef-habchi.com/) - title font\n * ... and many others ...\n\nContributions welcome!\n\n## License\n\nD-Tale is licensed under the GNU LGPL v2.1.  A copy of which is included in [LICENSE](LICENSE)\n", "# D-Tale Configuration\n\nThere are a lot of parameters that can be passed to the `dtale.show()` function and if you're calling this function often which similar parameters it can become quite cumbersome.  Not to fear, there is a way around this and it is via an `.ini` file.\n\nThere are two options for specifying the `.ini` file (in order of precedence):\n1) create an environment variable `DTALE_CONFIG` and set it to the path to your `.ini` file\n2) save your `.ini` file to `$HOME/.config/dtale.ini`\n\nIf you do make use of the `.ini` file then the `[app]` section properties will be applied to the application overall and the `[show]` section will be applied every time you call `dtale.show`.\n\nHere is an example of the `.ini` file with all the properties available and what their defaults are (if they have one):\n```ini\n[app]\ntheme = light\ngithub_fork = False\nhide_shutdown = False\npin_menu = False\nlanguage = en\nmax_column_width = 100 # the default value is None\nmain_title = My App # only use this if you don't want to see the D-Tale logo\nmain_title_font = Arial # this font is applied to your custom title\nquery_engine = python\nhide_header_editor = False\nlock_header_menu = False\nhide_header_menu = False\nhide_main_menu = False\nhide_column_menus = False\nenable_custom_filters = False\n\n[charts] # this controls how many points can be contained within scatter & 3D charts\nscatter_points = 15000\n3d_points = 4000\n\n[auth]\nactive = True\nusername = johndoe\npassword = 1337h4xOr\n\n[show]\nhost = localhost\nport = 8080\ndebug = False\nreaper_on = True\nopen_browser = False\nignore_duplicate = True\nallow_cell_edits = True\ninplace = False\ndrop_index = False\napp_root = additional_path\nprecision = 6 # how many digits to show on floats\nshow_columns = a,b\nhide_columns = c\ncolumn_formats = {\"a\": {\"fmt\": {\"html\": true}}}\nsort = a|ASC,b|DESC\nlocked = a,b\ncolumn_edit_options = {\"a\": [\"yes\", \"no\", \"maybe\"]}\nauto_hide_empty_columns = False\n```\n\nSome notes on these properties:\n* *theme:* available values are 'light' & 'dark'\n* *host/port/app_root:* have no default\n\nHere is the hierarchy of how parameters are passed to `dtale.show` (in order of most important to least):\n1) Passing parameters directly into `dtale.show` or passing a dictionary of settings to `dtale.global_state.set_app_settings` or `dtale.global_state.set_auth_settings`\n2) Calling `dtale.config.set_config(path_to_file)` which is probably only useful if you have a long-running process like a jupyter notebook\n3) Specifying an `.ini` file via `DTALE_CONFIG` environment variable\n4) Specifying an `.ini` file in `$HOME/.config/dtale.ini`\n", "import warnings\n\nfrom flask import Blueprint\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\n        \"ignore\",\n        (\n            \"\\nThe dash_html_components package is deprecated. Please replace\"\n            \"\\n`import dash_html_components as html` with `from dash import html`\"\n        ),\n    )\n    warnings.filterwarnings(\"ignore\", module=\"matplotlib\\\\..*\")  # noqa: W605\n\n    dtale = Blueprint(\"dtale\", __name__, url_prefix=\"/dtale\")\n\n    ALLOW_CELL_EDITS = True\n    HIDE_SHUTDOWN = False\n    GITHUB_FORK = False\n    HIDE_HEADER_EDITOR = False\n    LOCK_HEADER_MENU = False\n    HIDE_HEADER_MENU = False\n    HIDE_MAIN_MENU = False\n    HIDE_COLUMN_MENUS = False\n    ENABLE_CUSTOM_FILTERS = False\n\n    # flake8: NOQA\n    from dtale.app import show, get_instance, instances, offline_chart  # isort:skip\n    from dtale.cli.loaders import LOADERS  # isort:skip\n    from dtale.cli.clickutils import retrieve_meta_info_and_version\n    from dtale.global_state import update_id  # isort:skip\n\n    for loader_name, loader in LOADERS.items():\n        if hasattr(loader, \"show_loader\"):\n            globals()[\"show_{}\".format(loader_name)] = loader.show_loader\n\n    __version__ = retrieve_meta_info_and_version(\"dtale\")[1]\n", "from __future__ import absolute_import, print_function\n\nimport getpass\nimport jinja2\nimport logging\nimport os\nimport pandas as pd\nimport random\nimport socket\nimport sys\nimport time\nimport traceback\nfrom builtins import map, str\nfrom contextlib import closing\nfrom logging import ERROR as LOG_ERROR\nfrom threading import Timer\nfrom werkzeug.routing import Map\n\nfrom flask import (\n    Flask,\n    jsonify,\n    redirect,\n    render_template,\n    request,\n    url_for as flask_url_for,\n)\nfrom flask.testing import FlaskClient\n\nimport requests\nfrom flask_compress import Compress\nfrom six import PY3\n\nimport dtale.auth as auth\nimport dtale.global_state as global_state\nimport dtale.config as dtale_config\nfrom dtale import dtale\nfrom dtale.cli.clickutils import retrieve_meta_info_and_version, setup_logging\nfrom dtale.dash_application import views as dash_views\nfrom dtale.utils import (\n    DuplicateDataError,\n    build_shutdown_url,\n    build_url,\n    dict_merge,\n    fix_url_path,\n    get_host,\n    get_url_unquote,\n    is_app_root_defined,\n    make_list,\n    running_with_flask_debug,\n)\nfrom dtale.views import DtaleData, head_endpoint, is_up, kill, startup\n\nif PY3:\n    import _thread\nelse:\n    import thread as _thread\n\nlogger = logging.getLogger(__name__)\n\nUSE_NGROK = False\nUSE_COLAB = False\nJUPYTER_SERVER_PROXY = False\nACTIVE_HOST = None\nACTIVE_PORT = None\nSSL_CONTEXT = None\n\n_basepath = os.path.dirname(__file__)\n_filepath = os.path.abspath(os.path.join(_basepath, \"static\"))\n\nSHORT_LIFE_PATHS = [\n    \"dist\",\n    os.path.join(_filepath, \"dist\"),\n    \"dash\",\n    os.path.join(_filepath, \"dash\"),\n]\nSHORT_LIFE_TIMEOUT = 60\n\nREAPER_TIMEOUT = 60.0 * 60.0  # one-hour\n\n\nclass DtaleFlaskTesting(FlaskClient):\n    \"\"\"\n    Overriding Flask's implementation of flask.FlaskClient so we\n    can control the port associated with tests.\n\n    This class is required for setting the port on your test so that\n    we won't have SETTING keys colliding with other tests since the default\n    for every test would be 80.\n\n    :param args: Optional arguments to be passed to :class:`flask:flask.FlaskClient`\n    :param kwargs: Optional keyword arguments to be passed to :class:`flask:flask.FlaskClient`\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Constructor method\n        \"\"\"\n        self.host = kwargs.pop(\"hostname\", \"localhost\")\n        self.port = kwargs.pop(\"port\", random.randint(1025, 65535)) or random.randint(\n            1025, 65535\n        )\n        super(DtaleFlaskTesting, self).__init__(*args, **kwargs)\n        self.application.config[\"SERVER_NAME\"] = \"{host}:{port}\".format(\n            host=self.host, port=self.port\n        )\n        self.application.config[\"SESSION_COOKIE_DOMAIN\"] = \"localhost.localdomain\"\n\n    def get(self, *args, **kwargs):\n        \"\"\"\n        :param args: Optional arguments to be passed to :meth:`flask:flask.FlaskClient.get`\n        :param kwargs: Optional keyword arguments to be passed to :meth:`flask:flask.FlaskClient.get`\n        \"\"\"\n        return super(DtaleFlaskTesting, self).get(url_scheme=\"http\", *args, **kwargs)\n\n\ndef contains_route(routes, route):\n    return any((r.rule == route.rule and r.endpoint == route.endpoint) for r in routes)\n\n\nclass DtaleFlask(Flask):\n    \"\"\"\n    Overriding Flask's implementation of\n    get_send_file_max_age, test_client & run\n\n    :param import_name: the name of the application package\n    :param reaper_on: whether to run auto-reaper subprocess\n    :type reaper_on: bool\n    :param args: Optional arguments to be passed to :class:`flask:flask.Flask`\n    :param kwargs: Optional keyword arguments to be passed to :class:`flask:flask.Flask`\n    \"\"\"\n\n    def __init__(\n        self, import_name, reaper_on=True, url=None, app_root=None, *args, **kwargs\n    ):\n        \"\"\"\n        Constructor method\n        :param reaper_on: whether to run auto-reaper subprocess\n        :type reaper_on: bool\n        \"\"\"\n        self.reaper_on = reaper_on\n        self.reaper = None\n        self._setup_url_props(url)\n        self.port = None\n        self.app_root = app_root\n        super(DtaleFlask, self).__init__(import_name, *args, **kwargs)\n\n    def _setup_url_props(self, url):\n        self.base_url = url\n        self.shutdown_url = build_shutdown_url(url)\n\n    def update_template_context(self, context):\n        super(DtaleFlask, self).update_template_context(context)\n        if self.app_root is not None:\n            context[\"url_for\"] = self.url_for\n\n    def url_for(self, endpoint, *args, **kwargs):\n        if self.app_root is not None and endpoint == \"static\":\n            if \"filename\" in kwargs:\n                return fix_url_path(\n                    \"{}/{}/{}\".format(\n                        self.app_root, self.static_url_path, kwargs[\"filename\"]\n                    )\n                )\n            return fix_url_path(\"{}/{}\".format(self.app_root, args[0]))\n        if hasattr(Flask, \"url_for\"):\n            return Flask.url_for(self, endpoint, *args, **kwargs)\n        return flask_url_for(endpoint, *args, **kwargs)\n\n    def _override_routes(self, rule):\n        try:\n            routes_to_remove = [r for r in self.url_map._rules if r.rule == rule]\n            if routes_to_remove:\n                updated_rules = [\n                    r.empty()\n                    for r in self.url_map._rules\n                    if not contains_route(routes_to_remove, r)\n                ]\n                url_map_keys = [\n                    \"default_subdomains\",\n                    \"charset\",\n                    \"strict_slashes\",\n                    \"merge_slashes\",\n                    \"redirect_defaults\",\n                    \"converters\",\n                    \"sort_parameters\",\n                    \"sort_key\",\n                    \"encoding_errors\",\n                    \"host_matching\",\n                ]\n                url_map_data = {\n                    k: getattr(self.url_map, k)\n                    for k in url_map_keys\n                    if hasattr(self.url_map, k)\n                }\n                self.url_map = Map(rules=updated_rules, **url_map_data)\n\n            self.url_map._remap = True\n            self.url_map.update()\n        except BaseException:\n            logger.exception(\n                \"Could not override routes, if you're trying to specify a route for '/' then it will be ignored...\"\n            )\n\n    def route(self, rule, **options):\n        self._override_routes(rule)\n        return super(DtaleFlask, self).route(rule, **options)\n\n    def run(self, *args, **kwargs):\n        \"\"\"\n        :param args: Optional arguments to be passed to :meth:`flask:flask.run`\n        :param kwargs: Optional keyword arguments to be passed to :meth:`flask:flask.run`\n        \"\"\"\n        port_num = kwargs.get(\"port\")\n        self.port = str(port_num or \"\")\n        if not self.base_url:\n            host = kwargs.get(\"host\")\n            initialize_process_props(host, port_num)\n            app_url = build_url(self.port, host)\n            self._setup_url_props(app_url)\n        if kwargs.get(\"debug\", False):\n            self.reaper_on = False\n        self.build_reaper()\n        super(DtaleFlask, self).run(\n            use_reloader=kwargs.get(\"debug\", False), *args, **kwargs\n        )\n\n    def test_client(self, reaper_on=False, port=None, app_root=None, *args, **kwargs):\n        \"\"\"\n        Overriding Flask's implementation of test_client so we can specify ports for testing and\n        whether auto-reaper should be running\n\n        :param reaper_on: whether to run auto-reaper subprocess\n        :type reaper_on: bool\n        :param port: port number of flask application\n        :type port: int\n        :param args: Optional arguments to be passed to :meth:`flask:flask.Flask.test_client`\n        :param kwargs: Optional keyword arguments to be passed to :meth:`flask:flask.Flask.test_client`\n        :return: Flask's test client\n        :rtype: :class:`dtale.app.DtaleFlaskTesting`\n        \"\"\"\n        self.reaper_on = reaper_on\n        self.app_root = app_root\n        if app_root is not None:\n            self.config[\"APPLICATION_ROOT\"] = app_root\n            self.jinja_env.globals[\"url_for\"] = self.url_for\n        self.test_client_class = DtaleFlaskTesting\n        return super(DtaleFlask, self).test_client(\n            *args, **dict_merge(kwargs, dict(port=port))\n        )\n\n    def clear_reaper(self):\n        \"\"\"\n        Restarts auto-reaper countdown\n        \"\"\"\n        if self.reaper:\n            self.reaper.cancel()\n\n    def build_reaper(self, timeout=REAPER_TIMEOUT):\n        \"\"\"\n        Builds D-Tale's auto-reaping process to cleanup process after an hour of inactivity\n\n        :param timeout: time in seconds before D-Tale is shutdown for inactivity, defaults to one hour\n        :type timeout: float\n        \"\"\"\n        if not self.reaper_on:\n            return\n        self.clear_reaper()\n\n        def _func():\n            logger.info(\"Executing shutdown due to inactivity...\")\n            if is_up(self.base_url):  # make sure the Flask process is still running\n                requests.get(self.shutdown_url)\n            sys.exit()  # kill off the reaper thread\n\n        self.reaper = Timer(timeout, _func)\n        self.reaper.start()\n\n    def get_send_file_max_age(self, name):\n        \"\"\"\n        Overriding Flask's implementation of\n        get_send_file_max_age so we can lower the\n        timeout for javascript and css files which\n        are changed more often\n\n        :param name: filename\n        :return: Flask's default behavior for get_send_max_age if filename is not in SHORT_LIFE_PATHS\n                 otherwise SHORT_LIFE_TIMEOUT\n\n        \"\"\"\n        if name and any([str(name).startswith(path) for path in SHORT_LIFE_PATHS]):\n            return SHORT_LIFE_TIMEOUT\n        return super(DtaleFlask, self).get_send_file_max_age(name)\n\n\ndef build_app(\n    url=None, reaper_on=True, app_root=None, additional_templates=None, **kwargs\n):\n    \"\"\"\n    Builds :class:`flask:flask.Flask` application encapsulating endpoints for D-Tale's front-end\n\n    :param url: optional parameter which sets the host & root for any internal endpoints (ex: pinging shutdown)\n    :type url: str, optional\n    :param reaper_on: whether to run auto-reaper subprocess\n    :type reaper_on: bool\n    :param app_root: Optional path to prepend to the routes of D-Tale. This is used when making use of\n                     Jupyterhub server proxy\n    :type app_root: str, optional\n    :param additional_templates: path(s) to any other jinja templates you would like to load.  This comes into play if\n                                 you're embedding D-Tale into your own Flask app\n    :type: str, list, optional\n    :return: :class:`flask:flask.Flask` application\n    :rtype: :class:`dtale.app.DtaleFlask`\n    \"\"\"\n\n    app = DtaleFlask(\n        \"dtale\",\n        reaper_on=reaper_on,\n        static_url_path=\"/dtale/static\",\n        url=url,\n        instance_relative_config=False,\n        app_root=app_root,\n    )\n    app.config[\"SECRET_KEY\"] = \"Dtale\"\n\n    app.jinja_env.trim_blocks = True\n    app.jinja_env.lstrip_blocks = True\n\n    if app_root is not None:\n        app.config[\"APPLICATION_ROOT\"] = app_root\n        app.jinja_env.globals[\"url_for\"] = app.url_for\n    app.jinja_env.globals[\"is_app_root_defined\"] = is_app_root_defined\n\n    if additional_templates:\n        loaders = [app.jinja_loader]\n        loaders += [\n            jinja2.FileSystemLoader(loc) for loc in make_list(additional_templates)\n        ]\n        my_loader = jinja2.ChoiceLoader(loaders)\n        app.jinja_loader = my_loader\n\n    app.register_blueprint(dtale)\n\n    compress = Compress()\n    compress.init_app(app)\n\n    def _root():\n        return redirect(\"/dtale/{}\".format(head_endpoint()))\n\n    @app.route(\"/\")\n    def root():\n        return _root()\n\n    @app.route(\"/dtale\")\n    def dtale_base():\n        \"\"\"\n        :class:`flask:flask.Flask` routes which redirect to dtale/main\n\n        :return: 302 - flask.redirect('/dtale/main')\n        \"\"\"\n        return _root()\n\n    @app.route(\"/favicon.ico\")\n    def favicon():\n        \"\"\"\n        :class:`flask:flask.Flask` routes which returns favicon\n\n        :return: image/png\n        \"\"\"\n        return redirect(app.url_for(\"static\", filename=\"images/favicon.ico\"))\n\n    @app.route(\"/missing-js\")\n    def missing_js():\n        missing_js_commands = (\n            \">> cd [location of your local dtale repo]\\n\"\n            \">> yarn install\\n\"\n            \">> yarn run build  # or 'yarn run watch' if you're trying to develop\"\n        )\n        return render_template(\n            \"dtale/errors/missing_js.html\", missing_js_commands=missing_js_commands\n        )\n\n    @app.errorhandler(404)\n    def page_not_found(e=None):\n        \"\"\"\n        :class:`flask:flask.Flask` routes which returns favicon\n\n        :param e: exception\n        :return: text/html with exception information\n        \"\"\"\n        return (\n            render_template(\n                \"dtale/errors/404.html\",\n                page=\"\",\n                error=e,\n                stacktrace=str(traceback.format_exc()),\n            ),\n            404,\n        )\n\n    @app.errorhandler(500)\n    def internal_server_error(e=None):\n        \"\"\"\n        :class:`flask:flask.Flask` route which returns favicon\n\n        :param e: exception\n        :return: text/html with exception information\n        \"\"\"\n        return (\n            render_template(\n                \"dtale/errors/500.html\",\n                page=\"\",\n                error=e,\n                stacktrace=str(traceback.format_exc()),\n            ),\n            500,\n        )\n\n    def shutdown_server():\n        global ACTIVE_HOST, ACTIVE_PORT\n        \"\"\"\n        This function that checks if flask.request.environ['werkzeug.server.shutdown'] exists and\n        if so, executes that function\n        \"\"\"\n        logger.info(\"Executing shutdown...\")\n        func = request.environ.get(\"werkzeug.server.shutdown\")\n        if func is None:\n            logger.info(\n                \"Not running with the Werkzeug Server, exiting by searching gc for BaseWSGIServer\"\n            )\n            import gc\n            from werkzeug.serving import BaseWSGIServer\n\n            for obj in gc.get_objects():\n                try:\n                    if isinstance(obj, BaseWSGIServer):\n                        obj.shutdown()\n                        break\n                except Exception as e:\n                    logger.error(e)\n        else:\n            func()\n        global_state.cleanup()\n        ACTIVE_PORT = None\n        ACTIVE_HOST = None\n\n    @app.route(\"/shutdown\")\n    def shutdown():\n        \"\"\"\n        :class:`flask:flask.Flask` route for initiating server shutdown\n\n        :return: text/html with server shutdown message\n        \"\"\"\n        app.clear_reaper()\n        shutdown_server()\n        return \"Server shutting down...\"\n\n    @app.before_request\n    @auth.requires_auth\n    def before_request():\n        \"\"\"\n        Logic executed before each :attr:`flask:flask.request`\n\n        :return: text/html with server shutdown message\n        \"\"\"\n        app.build_reaper()\n\n    @app.route(\"/site-map\")\n    def site_map():\n        \"\"\"\n        :class:`flask:flask.Flask` route listing all available flask endpoints\n\n        :return: JSON of all flask enpoints [\n            [endpoint1, function path1],\n            ...,\n            [endpointN, function pathN]\n        ]\n        \"\"\"\n\n        def has_no_empty_params(rule):\n            defaults = rule.defaults or ()\n            arguments = rule.arguments or ()\n            return len(defaults) >= len(arguments)\n\n        links = []\n        for rule in app.url_map.iter_rules():\n            # Filter out rules we can't navigate to in a browser\n            # and rules that require parameters\n            if \"GET\" in rule.methods and has_no_empty_params(rule):\n                url = app.url_for(rule.endpoint, **(rule.defaults or {}))\n                links.append((url, rule.endpoint))\n        return jsonify(links)\n\n    @app.route(\"/version-info\")\n    def version_info():\n        \"\"\"\n        :class:`flask:flask.Flask` route for retrieving version information about D-Tale\n\n        :return: text/html version information\n        \"\"\"\n        _, version = retrieve_meta_info_and_version(\"dtale\")\n        return str(version)\n\n    @app.route(\"/health\")\n    def health_check():\n        \"\"\"\n        :class:`flask:flask.Flask` route for checking if D-Tale is up and running\n\n        :return: text/html 'ok'\n        \"\"\"\n        return \"ok\"\n\n    @app.url_value_preprocessor\n    def handle_data_id(_endpoint, values):\n        if values and \"data_id\" in values:\n            # https://github.com/man-group/dtale/commit/536691d365b69a580df836e617978eb563402ac5\n            values[\"data_id\"] = get_url_unquote()(\n                values[\"data_id\"]\n            )  # for handling back-slashes in arcticDB symbols\n            data_id_from_name = global_state.get_data_id_by_name(values[\"data_id\"])\n            values[\"data_id\"] = data_id_from_name or values[\"data_id\"]\n\n    auth.setup_auth(app)\n\n    with app.app_context():\n        app = dash_views.add_dash(app)\n        return app\n\n\ndef initialize_process_props(host=None, port=None, force=False):\n    \"\"\"\n    Helper function to initalize global state corresponding to the host & port being used for your\n    :class:`flask:flask.Flask` process\n\n    :param host: hostname to use otherwise it will default to the output of :func:`python:socket.gethostname`\n    :type host: str, optional\n    :param port: port to use otherwise default to the output of :meth:`dtale.app.find_free_port`\n    :type port: str, optional\n    :param force: boolean flag to determine whether to ignore the :meth:`dtale.app.find_free_port` function\n    :type force: bool\n    :return:\n    \"\"\"\n    global ACTIVE_HOST, ACTIVE_PORT\n\n    if force:\n        active_host = get_host(ACTIVE_HOST)\n        curr_base = build_url(ACTIVE_PORT, active_host)\n        final_host = get_host(host)\n        new_base = build_url(port, final_host)\n        if curr_base != new_base:\n            if is_up(new_base):\n                try:\n                    kill(new_base)  # kill the original process\n                except BaseException:\n                    raise IOError(\n                        (\n                            \"Could not kill process at {}, possibly something else is running at port {}. Please try \"\n                            \"another port.\"\n                        ).format(new_base, port)\n                    )\n                while is_up(new_base):\n                    time.sleep(0.01)\n            ACTIVE_HOST = final_host\n            ACTIVE_PORT = port\n            return\n\n    if ACTIVE_HOST is None:\n        ACTIVE_HOST = get_host(host)\n\n    if ACTIVE_PORT is None:\n        ACTIVE_PORT = int(port or find_free_port())\n\n\ndef is_port_in_use(port):\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        try:\n            s.bind((\"localhost\", port))\n            return False\n        except BaseException:\n            return True\n\n\ndef find_free_port():\n    \"\"\"\n    Searches for free port on executing server to run the :class:`flask:flask.Flask` process. Checks ports in range\n    specified using environment variables:\n\n    DTALE_MIN_PORT (default: 40000)\n    DTALE_MAX_PORT (default: 49000)\n\n    The range limitation is required for usage in tools such as jupyterhub.  Will raise an exception if an open\n    port cannot be found.\n\n    :return: port number\n    :rtype: int\n    \"\"\"\n\n    min_port = int(os.environ.get(\"DTALE_MIN_PORT\") or 40000)\n    max_port = int(os.environ.get(\"DTALE_MAX_PORT\") or 49000)\n    base = min_port\n    while is_port_in_use(base):\n        base += 1\n        if base > max_port:\n            msg = (\n                \"D-Tale could not find an open port from {} to {}, please increase your range by altering the \"\n                \"environment variables DTALE_MIN_PORT & DTALE_MAX_PORT.\"\n            ).format(min_port, max_port)\n            raise IOError(msg)\n    return base\n\n\ndef build_startup_url_and_app_root(app_root=None):\n    global ACTIVE_HOST, ACTIVE_PORT, SSL_CONTEXT, JUPYTER_SERVER_PROXY, USE_COLAB\n\n    if USE_COLAB:\n        colab_host = use_colab(ACTIVE_PORT)\n        if colab_host:\n            return colab_host, None\n    url = build_url(ACTIVE_PORT, ACTIVE_HOST, SSL_CONTEXT is not None)\n    final_app_root = app_root\n    if final_app_root is None and JUPYTER_SERVER_PROXY:\n        final_app_root = os.environ.get(\"JUPYTERHUB_SERVICE_PREFIX\")\n        if final_app_root is None:\n            final_app_root = \"/user/{}\".format(getpass.getuser())\n        final_app_root = (\n            \"{}/proxy\".format(final_app_root)\n            if not final_app_root.endswith(\"/proxy\")\n            else final_app_root\n        )\n    if final_app_root is not None:\n        if JUPYTER_SERVER_PROXY:\n            final_app_root = fix_url_path(\"{}/{}\".format(final_app_root, ACTIVE_PORT))\n            return final_app_root, final_app_root\n        else:\n            return fix_url_path(\"{}/{}\".format(url, final_app_root)), final_app_root\n    return url, final_app_root\n\n\ndef use_colab(port):\n    try:\n        from google.colab.output import eval_js\n\n        colab_host = eval_js(\n            'google.colab.kernel.proxyPort(%d, {\"cache\": false})' % port\n        )\n        return colab_host[:-1] if colab_host.endswith(\"/\") else colab_host\n    except BaseException:\n        return None\n\n\ndef show(data=None, data_loader=None, name=None, context_vars=None, **options):\n    \"\"\"\n    Entry point for kicking off D-Tale :class:`flask:flask.Flask` process from python process\n\n    :param data: data which D-Tale will display\n    :type data: :class:`pandas:pandas.DataFrame` or :class:`pandas:pandas.Series`\n                or :class:`pandas:pandas.DatetimeIndex` or :class:`pandas:pandas.MultiIndex`, optional\n    :param host: hostname of D-Tale, defaults to 0.0.0.0\n    :type host: str, optional\n    :param port: port number of D-Tale process, defaults to any open port on server\n    :type port: str, optional\n    :param name: optional label to assign a D-Tale process\n    :type name: str, optional\n    :param debug: will turn on :class:`flask:flask.Flask` debug functionality, defaults to False\n    :type debug: bool, optional\n    :param subprocess: run D-Tale as a subprocess of your current process, defaults to True\n    :type subprocess: bool, optional\n    :param data_loader: function to load your data\n    :type data_loader: func, optional\n    :param reaper_on: turn on subprocess which will terminate D-Tale after 1 hour of inactivity\n    :type reaper_on: bool, optional\n    :param open_browser: if true, this will try using the :mod:`python:webbrowser` package to automatically open\n                         your default browser to your D-Tale process\n    :type open_browser: bool, optional\n    :param notebook: if true, this will try displaying an :class:`ipython:IPython.display.IFrame`\n    :type notebook: bool, optional\n    :param force: if true, this will force the D-Tale instance to run on the specified host/port by killing any\n                  other process running at that location\n    :type force: bool, optional\n    :param context_vars: a dictionary of the variables that will be available for use in user-defined expressions,\n                         such as filters\n    :type context_vars: dict, optional\n    :param ignore_duplicate: if true, this will not check if this data matches any other data previously loaded to\n                             D-Tale\n    :type ignore_duplicate: bool, optional\n    :param app_root: Optional path to prepend to the routes of D-Tale. This is used when making use of\n                     Jupyterhub server proxy\n    :type app_root: str, optional\n    :param allow_cell_edits: If false, this will not allow users to edit cells directly in their D-Tale grid\n    :type allow_cell_edits: bool, optional\n    :param inplace: If true, this will call `reset_index(inplace=True)` on the dataframe used as a way to save memory.\n                    Otherwise this will create a brand new dataframe, thus doubling memory but leaving the dataframe\n                    input unchanged.\n    :type inplace: bool, optional\n    :param drop_index: If true, this will drop any pre-existing index on the dataframe input.\n    :type drop_index: bool, optional\n    :param hide_shutdown: If true, this will hide the \"Shutdown\" buton from users\n    :type hide_shutdown: bool, optional\n    :param github_fork: If true, this will display a \"Fork me on GitHub\" ribbon in the upper right-hand corner of the\n                        app\n    :type github_fork: bool, optional\n    :param hide_drop_rows: If true, this will hide the \"Drop Rows\" button from users\n    :type hide_drop_rows: bool, optional\n    :param hide_header_editor: If true, this will hide the header editor when editing cells\n    :type hide_header_editor: bool, optional\n    :param lock_header_menu: if true, this will always the display the header menu which usually only displays when you\n                             hover over the top\n    :type lock_header_menu: bool, optional\n    :param hide_header_menu: If true, this will hide the header menu from the screen\n    :type hide_header_menu: bool, optional\n    :param hide_main_menu: If true, this will hide the main menu from the screen\n    :type hide_main_menu: bool, optional\n    :param hide_column_menus: If true, this will hide the column menus from the screen\n    :type hide_column_menus: bool, optional\n    :param column_edit_options: The options to allow on the front-end when editing a cell for the columns specified\n    :type column_edit_options: dict, optional\n    :param auto_hide_empty_columns: if True, then auto-hide any columns on the front-end that are comprised entirely of\n                                    NaN values\n    :type auto_hide_empty_columns: boolean, optional\n    :param highlight_filter: if True, then highlight rows on the frontend which will be filtered when applying a filter\n                             rather than hiding them from the dataframe\n    :type highlight_filter: boolean, optional\n    :param enable_custom_filters: If true, this will enable users to make custom filters from the UI\n    :type enable_custom_filters: bool, optional\n\n    :Example:\n\n        >>> import dtale\n        >>> import pandas as pd\n        >>> df = pandas.DataFrame([dict(a=1,b=2,c=3)])\n        >>> dtale.show(df)\n        D-Tale started at: http://hostname:port\n\n        ..link displayed in logging can be copied and pasted into any browser\n    \"\"\"\n    global ACTIVE_HOST, ACTIVE_PORT, SSL_CONTEXT, USE_NGROK\n\n    if name:\n        if global_state.get_data_id_by_name(name):\n            print(\n                \"Data has already been loaded to D-Tale with the name '{}', please try another one.\".format(\n                    name\n                )\n            )\n            return\n        if any(not c.isalnum() and not c.isspace() for c in name):\n            print(\n                \"'name' property cannot contain any special characters only letters, numbers or spaces.\"\n            )\n            return\n\n    try:\n        final_options = dtale_config.build_show_options(options)\n        logfile, log_level, verbose = map(\n            final_options.get, [\"logfile\", \"log_level\", \"verbose\"]\n        )\n        setup_logging(logfile, log_level or \"info\", verbose)\n\n        if USE_NGROK:\n            if not PY3:\n                raise Exception(\n                    \"In order to use ngrok you must be using Python 3 or higher!\"\n                )\n\n            from flask_ngrok import _run_ngrok\n\n            ACTIVE_HOST = _run_ngrok()\n            ACTIVE_PORT = None\n        else:\n            initialize_process_props(\n                final_options[\"host\"], final_options[\"port\"], final_options[\"force\"]\n            )\n\n        SSL_CONTEXT = options.get(\"ssl_context\")\n        app_url = build_url(ACTIVE_PORT, ACTIVE_HOST)\n        startup_url, final_app_root = build_startup_url_and_app_root(\n            final_options[\"app_root\"]\n        )\n        instance = startup(\n            startup_url,\n            data=data,\n            data_loader=data_loader,\n            name=name,\n            context_vars=context_vars,\n            ignore_duplicate=final_options[\"ignore_duplicate\"],\n            allow_cell_edits=final_options[\"allow_cell_edits\"],\n            inplace=final_options[\"inplace\"],\n            drop_index=final_options[\"drop_index\"],\n            precision=final_options[\"precision\"],\n            show_columns=final_options[\"show_columns\"],\n            hide_columns=final_options[\"hide_columns\"],\n            column_formats=final_options[\"column_formats\"],\n            nan_display=final_options[\"nan_display\"],\n            sort=final_options[\"sort\"],\n            locked=final_options[\"locked\"],\n            background_mode=final_options[\"background_mode\"],\n            range_highlights=final_options[\"range_highlights\"],\n            vertical_headers=final_options[\"vertical_headers\"],\n            is_proxy=JUPYTER_SERVER_PROXY,\n            app_root=final_app_root,\n            hide_shutdown=final_options.get(\"hide_shutdown\"),\n            column_edit_options=final_options.get(\"column_edit_options\"),\n            auto_hide_empty_columns=final_options.get(\"auto_hide_empty_columns\"),\n            highlight_filter=final_options.get(\"highlight_filter\"),\n            hide_header_editor=final_options.get(\"hide_header_editor\"),\n            lock_header_menu=final_options.get(\"lock_header_menu\"),\n            hide_header_menu=final_options.get(\"hide_header_menu\"),\n            hide_main_menu=final_options.get(\"hide_main_menu\"),\n            hide_column_menus=final_options.get(\"hide_column_menus\"),\n            enable_custom_filters=final_options.get(\"enable_custom_filters\"),\n        )\n        instance.started_with_open_browser = final_options[\"open_browser\"]\n        is_active = not running_with_flask_debug() and is_up(app_url)\n        if is_active:\n\n            def _start():\n                if final_options[\"open_browser\"]:\n                    instance.open_browser()\n\n        else:\n            if USE_NGROK:\n                thread = Timer(1, _run_ngrok)\n                thread.setDaemon(True)\n                thread.start()\n\n            def _start():\n                try:\n                    app = build_app(\n                        app_url,\n                        reaper_on=final_options[\"reaper_on\"],\n                        host=ACTIVE_HOST,\n                        app_root=final_app_root,\n                    )\n                    if final_options[\"debug\"] and not USE_NGROK:\n                        app.jinja_env.auto_reload = True\n                        app.config[\"TEMPLATES_AUTO_RELOAD\"] = True\n                    else:\n                        logging.getLogger(\"werkzeug\").setLevel(LOG_ERROR)\n\n                    if final_options[\"open_browser\"]:\n                        instance.open_browser()\n\n                    # hide banner message in production environments\n                    cli = sys.modules.get(\"flask.cli\")\n                    if cli is not None:\n                        cli.show_server_banner = lambda *x: None\n\n                    run_kwargs = {}\n                    if options.get(\"ssl_context\"):\n                        run_kwargs[\"ssl_context\"] = options.get(\"ssl_context\")\n\n                    if USE_NGROK:\n                        app.run(threaded=True, **run_kwargs)\n                    else:\n                        app.run(\n                            host=\"0.0.0.0\",\n                            port=ACTIVE_PORT,\n                            debug=final_options[\"debug\"],\n                            threaded=True,\n                            **run_kwargs\n                        )\n                except BaseException as ex:\n                    logger.exception(ex)\n\n        if final_options[\"subprocess\"]:\n            if is_active:\n                _start()\n            else:\n                _thread.start_new_thread(_start, ())\n\n            if final_options[\"notebook\"]:\n                instance.notebook()\n        else:\n            # Need to use logging.info() because for some reason other libraries like arctic seem to break logging\n            logging.info(\"D-Tale started at: {}\".format(app_url))\n            _start()\n\n        return instance\n    except DuplicateDataError as ex:\n        print(\n            \"It looks like this data may have already been loaded to D-Tale based on shape and column names. Here is \"\n            \"URL of the data that seems to match it:\\n\\n{}\\n\\nIf you still want to load this data please use the \"\n            \"following command:\\n\\ndtale.show(df, ignore_duplicate=True)\".format(\n                DtaleData(ex.data_id, build_url(ACTIVE_PORT, ACTIVE_HOST)).main_url()\n            )\n        )\n    return None\n\n\ndef instances():\n    \"\"\"\n    Prints all urls to the current pieces of data being viewed\n    \"\"\"\n    if global_state.size() > 0:\n\n        def _instance_msgs():\n            for data_id in global_state.keys():\n                startup_url, final_app_root = build_startup_url_and_app_root()\n                instance = DtaleData(\n                    data_id,\n                    startup_url,\n                    is_proxy=JUPYTER_SERVER_PROXY,\n                    app_root=final_app_root,\n                )\n                name = global_state.get_name(data_id)\n                yield [data_id, name or \"\", instance.build_main_url()]\n                if name is not None:\n                    yield [\n                        global_state.convert_name_to_url_path(name),\n                        name,\n                        instance.build_main_url(\n                            global_state.convert_name_to_url_path(name)\n                        ),\n                    ]\n\n        data = pd.DataFrame(\n            list(_instance_msgs()), columns=[\"ID\", \"Name\", \"URL\"]\n        ).to_string(index=False)\n        print(\n            (\n                \"To gain access to an instance object simply pass the value from 'ID' to dtale.get_instance(ID)\\n\\n{}\"\n            ).format(data)\n        )\n    else:\n        print(\"currently no running instances...\")\n\n\ndef get_instance(data_id):\n    \"\"\"\n    Returns a :class:`dtale.views.DtaleData` object for the data_id passed as input, will return None if the data_id\n    does not exist\n\n    :param data_id: integer identifier for a D-Tale process's data\n    :type data_id: int\n    :return: :class:`dtale.views.DtaleData`\n    \"\"\"\n    final_data_id = global_state.get_data_id_by_name(data_id) or data_id\n    if not global_state.contains(final_data_id):\n        return None\n\n    if data_id is not None:\n        startup_url, final_app_root = build_startup_url_and_app_root()\n        return DtaleData(\n            final_data_id,\n            startup_url,\n            is_proxy=JUPYTER_SERVER_PROXY,\n            app_root=final_app_root,\n        )\n    return None\n\n\ndef offline_chart(\n    df,\n    chart_type=None,\n    query=None,\n    x=None,\n    y=None,\n    z=None,\n    group=None,\n    agg=None,\n    window=None,\n    rolling_comp=None,\n    barmode=None,\n    barsort=None,\n    yaxis=None,\n    filepath=None,\n    title=None,\n    **kwargs\n):\n    \"\"\"\n    Builds the HTML for a plotly chart figure to saved to a file or output to a jupyter notebook\n\n    :param df: integer string identifier for a D-Tale process's data\n    :type df: :class:`pandas:pandas.DataFrame`\n    :param chart_type: type of chart, possible options are line|bar|pie|scatter|3d_scatter|surface|heatmap\n    :type chart_type: str\n    :param query: pandas dataframe query string\n    :type query: str, optional\n    :param x: column to use for the X-Axis\n    :type x: str\n    :param y: columns to use for the Y-Axes\n    :type y: list of str\n    :param z: column to use for the Z-Axis\n    :type z: str, optional\n    :param group: column(s) to use for grouping\n    :type group: list of str or str, optional\n    :param agg: specific aggregation that can be applied to y or z axes.  Possible values are: count, first, last mean,\n                median, min, max, std, var, mad, prod, sum.  This is included in label of axis it is being applied to.\n    :type agg: str, optional\n    :param window: number of days to include in rolling aggregations\n    :type window: int, optional\n    :param rolling_comp: computation to use in rolling aggregations\n    :type rolling_comp: str, optional\n    :param barmode: mode to use for bar chart display. possible values are stack|group(default)|overlay|relative\n    :type barmode: str, optional\n    :param barsort: axis name to sort the bars in a bar chart by (default is the 'x', but other options are any of\n                    columns names used in the 'y' parameter\n    :type barsort: str, optional\n    :param filepath: location to save HTML output\n    :type filepath: str, optional\n    :param title: Title of your chart\n    :type title: str, optional\n    :param kwargs: optional keyword arguments, here in case invalid arguments are passed to this function\n    :type kwargs: dict\n    :return: possible outcomes are:\n             - if run within a jupyter notebook and no 'filepath' is specified it will print the resulting HTML\n               within a cell in your notebook\n             - if 'filepath' is specified it will save the chart to the path specified\n             - otherwise it will return the HTML output as a string\n    \"\"\"\n    instance = startup(url=None, data=df, data_id=999, is_proxy=JUPYTER_SERVER_PROXY)\n    output = instance.offline_chart(\n        chart_type=chart_type,\n        query=query,\n        x=x,\n        y=y,\n        z=z,\n        group=group,\n        agg=agg,\n        window=window,\n        rolling_comp=rolling_comp,\n        barmode=barmode,\n        barsort=barsort,\n        yaxis=yaxis,\n        filepath=filepath,\n        title=title,\n        **kwargs\n    )\n    global_state.cleanup()\n    return output\n", "import json\nimport os\n\nfrom six import string_types\nfrom six.moves.configparser import ConfigParser\n\nimport dtale.global_state as global_state\nfrom dtale.utils import dict_merge\n\nLOADED_CONFIG = None\n\n\ndef load_config_state(path):\n    if not path:\n        return None\n    # load .ini file with properties specific to D-Tale\n    config = ConfigParser()\n    config.read(path)\n    return config\n\n\ndef get_config():\n    global LOADED_CONFIG\n\n    if LOADED_CONFIG:\n        return LOADED_CONFIG\n    ini_path = os.path.expandvars(\n        os.environ.get(\"DTALE_CONFIG\") or \"$HOME/.config/dtale.ini\"\n    )\n    if os.path.isfile(ini_path):\n        return load_config_state(ini_path)\n    return None\n\n\ndef set_config(path):\n    global LOADED_CONFIG\n\n    LOADED_CONFIG = load_config_state(path)\n\n\ndef get_config_val(config, defaults, prop, getter=\"get\", section=\"show\"):\n    if config.has_option(section, prop):\n        return getattr(config, getter)(section, prop)\n    return defaults.get(prop)\n\n\ndef load_app_settings(config):\n    if config is None:\n        return\n    curr_app_settings = global_state.get_app_settings()\n    theme = get_config_val(config, curr_app_settings, \"theme\", section=\"app\")\n    pin_menu = get_config_val(\n        config, curr_app_settings, \"pin_menu\", section=\"app\", getter=\"getboolean\"\n    )\n    language = get_config_val(config, curr_app_settings, \"language\", section=\"app\")\n    github_fork = get_config_val(\n        config, curr_app_settings, \"github_fork\", section=\"app\", getter=\"getboolean\"\n    )\n    hide_shutdown = get_config_val(\n        config, curr_app_settings, \"hide_shutdown\", section=\"app\", getter=\"getboolean\"\n    )\n    max_column_width = get_config_val(\n        config, curr_app_settings, \"max_column_width\", section=\"app\", getter=\"getint\"\n    )\n    max_row_height = get_config_val(\n        config, curr_app_settings, \"max_row_height\", section=\"app\", getter=\"getint\"\n    )\n    main_title = get_config_val(config, curr_app_settings, \"main_title\", section=\"app\")\n    main_title_font = get_config_val(\n        config, curr_app_settings, \"main_title_font\", section=\"app\"\n    )\n    query_engine = get_config_val(\n        config, curr_app_settings, \"query_engine\", section=\"app\"\n    )\n    hide_header_editor = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_header_editor\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_header_menu = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_header_menu\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_main_menu = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_main_menu\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_column_menus = get_config_val(\n        config,\n        curr_app_settings,\n        \"hide_column_menus\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    lock_header_menu = get_config_val(\n        config,\n        curr_app_settings,\n        \"lock_header_menu\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    enable_custom_filters = get_config_val(\n        config,\n        curr_app_settings,\n        \"enable_custom_filters\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    open_custom_filter_on_startup = get_config_val(\n        config,\n        curr_app_settings,\n        \"open_custom_filter_on_startup\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    open_predefined_filters_on_startup = get_config_val(\n        config,\n        curr_app_settings,\n        \"open_predefined_filters_on_startup\",\n        section=\"app\",\n        getter=\"getboolean\",\n    )\n    hide_drop_rows = get_config_val(\n        config, curr_app_settings, \"hide_drop_rows\", section=\"app\", getter=\"getboolean\"\n    )\n\n    global_state.set_app_settings(\n        dict(\n            theme=theme,\n            pin_menu=pin_menu,\n            language=language,\n            github_fork=github_fork,\n            hide_shutdown=hide_shutdown,\n            max_column_width=max_column_width,\n            max_row_height=max_row_height,\n            main_title=main_title,\n            main_title_font=main_title_font,\n            query_engine=query_engine,\n            open_custom_filter_on_startup=open_custom_filter_on_startup,\n            open_predefined_filters_on_startup=open_predefined_filters_on_startup,\n            hide_drop_rows=hide_drop_rows,\n            hide_header_editor=hide_header_editor,\n            lock_header_menu=lock_header_menu,\n            hide_header_menu=hide_header_menu,\n            hide_main_menu=hide_main_menu,\n            hide_column_menus=hide_column_menus,\n            enable_custom_filters=enable_custom_filters,\n        )\n    )\n\n\ndef load_chart_settings(config):\n    if config is None:\n        return\n    curr_chart_settings = global_state.get_chart_settings()\n\n    scatter_points = get_config_val(\n        config, curr_chart_settings, \"scatter_points\", section=\"charts\", getter=\"getint\"\n    )\n    three_dimensional_points = get_config_val(\n        config, curr_chart_settings, \"3d_points\", section=\"charts\", getter=\"getint\"\n    )\n    global_state.set_chart_settings(\n        {\"scatter_points\": scatter_points, \"3d_points\": three_dimensional_points}\n    )\n\n\ndef load_auth_settings(config):\n    if config is None:\n        return\n    curr_auth_settings = global_state.get_auth_settings()\n    active = get_config_val(\n        config, curr_auth_settings, \"active\", section=\"auth\", getter=\"getboolean\"\n    )\n    username = get_config_val(config, curr_auth_settings, \"username\", section=\"auth\")\n    password = get_config_val(config, curr_auth_settings, \"password\", section=\"auth\")\n\n    global_state.set_auth_settings(\n        dict(active=active, username=username, password=password)\n    )\n\n\ndef build_show_options(options=None):\n    defaults = dict(\n        host=None,\n        port=None,\n        debug=False,\n        subprocess=True,\n        reaper_on=True,\n        open_browser=False,\n        notebook=False,\n        force=False,\n        ignore_duplicate=True,\n        app_root=None,\n        allow_cell_edits=True,\n        inplace=False,\n        drop_index=False,\n        precision=2,\n        show_columns=None,\n        hide_columns=None,\n        column_formats=None,\n        nan_display=None,\n        sort=None,\n        locked=None,\n        background_mode=None,\n        range_highlights=None,\n        vertical_headers=False,\n        hide_shutdown=None,\n        column_edit_options=None,\n        auto_hide_empty_columns=False,\n        highlight_filter=False,\n        hide_header_editor=None,\n        lock_header_menu=None,\n        hide_header_menu=None,\n        hide_main_menu=None,\n        hide_column_menus=None,\n        enable_custom_filters=None,\n    )\n    config_options = {}\n    config = get_config()\n    if config and config.has_section(\"show\"):\n        config_options[\"host\"] = get_config_val(config, defaults, \"host\")\n        config_options[\"port\"] = get_config_val(config, defaults, \"port\")\n        config_options[\"debug\"] = get_config_val(\n            config, defaults, \"debug\", \"getboolean\"\n        )\n        config_options[\"subprocess\"] = get_config_val(\n            config, defaults, \"subprocess\", \"getboolean\"\n        )\n        config_options[\"reaper_on\"] = get_config_val(\n            config, defaults, \"reaper_on\", \"getboolean\"\n        )\n        config_options[\"open_browser\"] = get_config_val(\n            config, defaults, \"open_browser\", \"getboolean\"\n        )\n        config_options[\"notebook\"] = get_config_val(\n            config, defaults, \"notebook\", \"getboolean\"\n        )\n        config_options[\"force\"] = get_config_val(\n            config, defaults, \"force\", \"getboolean\"\n        )\n        config_options[\"ignore_duplicate\"] = get_config_val(\n            config, defaults, \"ignore_duplicate\", \"getboolean\"\n        )\n        config_options[\"app_root\"] = get_config_val(config, defaults, \"app_root\")\n        config_options[\"allow_cell_edits\"] = get_config_val(\n            config, defaults, \"allow_cell_edits\"\n        )\n        if isinstance(config_options[\"allow_cell_edits\"], string_types):\n            if config_options[\"allow_cell_edits\"] == \"True\":\n                config_options[\"allow_cell_edits\"] = True\n            elif config_options[\"allow_cell_edits\"] == \"False\":\n                config_options[\"allow_cell_edits\"] = False\n            else:\n                config_options[\"allow_cell_edits\"] = config_options[\n                    \"allow_cell_edits\"\n                ].split(\",\")\n\n        config_options[\"inplace\"] = get_config_val(\n            config, defaults, \"inplace\", \"getboolean\"\n        )\n        config_options[\"drop_index\"] = get_config_val(\n            config, defaults, \"drop_index\", \"getboolean\"\n        )\n        config_options[\"precision\"] = get_config_val(\n            config, defaults, \"precision\", \"getint\"\n        )\n        config_options[\"show_columns\"] = get_config_val(\n            config, defaults, \"show_columns\"\n        )\n        if config_options[\"show_columns\"]:\n            config_options[\"show_columns\"] = config_options[\"show_columns\"].split(\",\")\n        config_options[\"hide_columns\"] = get_config_val(\n            config, defaults, \"hide_columns\"\n        )\n        if config_options[\"hide_columns\"]:\n            config_options[\"hide_columns\"] = config_options[\"hide_columns\"].split(\",\")\n        config_options[\"column_formats\"] = get_config_val(\n            config, defaults, \"column_formats\"\n        )\n        if config_options[\"column_formats\"]:\n            config_options[\"column_formats\"] = json.loads(\n                config_options[\"column_formats\"]\n            )\n        config_options[\"nan_display\"] = get_config_val(config, defaults, \"nan_display\")\n        config_options[\"sort\"] = get_config_val(config, defaults, \"sort\")\n        if config_options[\"sort\"]:\n            config_options[\"sort\"] = [\n                tuple(sort.split(\"|\")) for sort in config_options[\"sort\"].split(\",\")\n            ]\n        config_options[\"locked\"] = get_config_val(config, defaults, \"locked\")\n        if config_options[\"locked\"]:\n            config_options[\"locked\"] = config_options[\"locked\"].split(\",\")\n        config_options[\"background_mode\"] = get_config_val(\n            config, defaults, \"background_mode\"\n        )\n        config_options[\"range_highlights\"] = get_config_val(\n            config, defaults, \"range_highlights\"\n        )\n        if config_options[\"range_highlights\"]:\n            config_options[\"range_highlights\"] = json.loads(\n                config_options[\"range_highlights\"]\n            )\n        config_options[\"vertical_headers\"] = get_config_val(\n            config, defaults, \"vertical_headers\", \"getboolean\"\n        )\n        config_options[\"column_edit_options\"] = get_config_val(\n            config, defaults, \"column_edit_options\"\n        )\n        if config_options[\"column_edit_options\"]:\n            config_options[\"column_edit_options\"] = json.loads(\n                config_options[\"column_edit_options\"]\n            )\n        config_options[\"auto_hide_empty_columns\"] = get_config_val(\n            config, defaults, \"auto_hide_empty_columns\", \"getboolean\"\n        )\n        config_options[\"highlight_filter\"] = get_config_val(\n            config, defaults, \"highlight_filter\", \"getboolean\"\n        )\n\n    return dict_merge(defaults, config_options, options)\n\n\nLOADED_CONFIG = get_config()\nload_app_settings(LOADED_CONFIG)\nload_auth_settings(LOADED_CONFIG)\nload_chart_settings(LOADED_CONFIG)\n", "import string\nimport inspect\n\nfrom logging import getLogger\nfrom six import PY3\n\nfrom dtale.utils import dict_merge, format_data\n\ntry:\n    from collections.abc import MutableMapping\nexcept ImportError:\n    from collections import MutableMapping\n\nlogger = getLogger(__name__)\n\nAPP_SETTINGS = {\n    \"theme\": \"light\",\n    \"pin_menu\": False,\n    \"language\": \"en\",\n    \"github_fork\": False,\n    \"hide_shutdown\": False,\n    \"max_column_width\": None,\n    \"max_row_height\": None,\n    \"main_title\": None,\n    \"main_title_font\": None,\n    \"query_engine\": \"python\",\n    \"open_custom_filter_on_startup\": False,\n    \"open_predefined_filters_on_startup\": False,\n    \"hide_drop_rows\": False,\n    \"hide_header_editor\": False,\n    \"lock_header_menu\": False,\n    \"hide_header_menu\": False,\n    \"hide_main_menu\": False,\n    \"hide_column_menus\": False,\n    \"enable_custom_filters\": False,\n}\n\nAUTH_SETTINGS = {\"active\": False, \"username\": None, \"password\": None}\n\nCHART_SETTINGS = {\"scatter_points\": 15000, \"3d_points\": 40000}\n\n\nclass DtaleInstance(object):\n    _dataset = None\n    _dataset_dim = None\n    _dtypes = None\n    _metadata = None\n    _context_variables = None\n    _history = None\n    _settings = None\n    _name = \"\"\n    _rows = 0\n\n    def __init__(self, data):\n        self._data = data\n        self._rows = 0 if self._data is None else len(data)\n\n    def load_data(self):\n        return self._data\n\n    def rows(self, **kwargs):\n        return self._rows\n\n    @property\n    def is_large(self):\n        return False\n\n    @property\n    def data(self):\n        return self.load_data()\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def dataset(self):\n        return self._dataset\n\n    @property\n    def dataset_dim(self):\n        return self._dataset_dim\n\n    @property\n    def dtypes(self):\n        return self._dtypes\n\n    @property\n    def metadata(self):\n        return self._metadata\n\n    @property\n    def context_variables(self):\n        return self._context_variables\n\n    @property\n    def history(self):\n        return self._history\n\n    @property\n    def settings(self):\n        return self._settings\n\n    @property\n    def is_xarray_dataset(self):\n        if self._dataset is not None:\n            return True\n        return False\n\n    @data.setter\n    def data(self, data):\n        self._data = data\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    @dataset.setter\n    def dataset(self, dataset):\n        self._dataset = dataset\n\n    @dataset_dim.setter\n    def dataset_dim(self, dataset_dim):\n        self._dataset_dim = dataset_dim\n\n    @dtypes.setter\n    def dtypes(self, dtypes):\n        self._dtypes = dtypes\n\n    @context_variables.setter\n    def context_variables(self, context_variables):\n        self._context_variables = context_variables\n\n    @metadata.setter\n    def metadata(self, metadata):\n        self._metadata = metadata\n\n    @history.setter\n    def history(self, history):\n        self._history = history\n\n    @settings.setter\n    def settings(self, settings):\n        self._settings = settings\n\n\nLARGE_ARCTICDB = 1000000\n\n\ndef get_num_rows(lib, symbol):\n    try:\n        return lib._nvs.get_num_rows(symbol)\n    except BaseException:\n        read_options = lib._nvs._get_read_options()\n        version_query = lib._nvs._get_version_query(None)\n        dit = lib._nvs.version_store.read_descriptor(\n            symbol, version_query, read_options\n        )\n        return dit.timeseries_descriptor.total_rows\n\n\nclass DtaleArcticDBInstance(DtaleInstance):\n    def __init__(self, data, data_id, parent):\n        super(DtaleArcticDBInstance, self).__init__(data)\n        self.parent = parent\n        data_id_segs = (data_id or \"\").split(\"|\")\n        symbol = data_id_segs[-1]\n        if len(data_id_segs) > 1:\n            self.lib_name = data_id_segs[0]\n            if not parent.lib or self.lib_name != parent.lib.name:\n                parent.update_library(self.lib_name)\n        else:\n            self.lib_name = self.parent.lib.name if self.parent.lib else None\n\n        lib = self.parent.get_library(self.lib_name) if self.lib_name else None\n        self.symbol = symbol\n        self._rows = 0\n        self._cols = 0\n        self._base_df = None\n        if lib and self.symbol and self.symbol in self.parent._symbols[self.lib_name]:\n            self._rows = get_num_rows(lib, self.symbol)\n            self._base_df = self.load_data(row_range=[0, 1])\n            self._cols = len(format_data(self._base_df)[0].columns)\n        elif (\n            lib\n            and self.symbol\n            and self.symbol not in self.parent._symbols[self.lib_name]\n        ):\n            raise ValueError(\n                \"Symbol ({}) not in library, {}! Please select another symbol.\".format(\n                    symbol, self.lib_name\n                )\n            )\n\n    def load_data(self, **kwargs):\n        from arcticdb.version_store._store import VersionedItem\n\n        if self.symbol not in self.parent._symbols[self.lib_name]:\n            raise ValueError(\n                \"{} does not exist in {}!\".format(self.symbol, self.lib_name)\n            )\n\n        data = self.parent.get_library(self.lib_name)._nvs.read(self.symbol, **kwargs)\n        if isinstance(data, VersionedItem):\n            return data.data\n        return data\n\n    def rows(self, **kwargs):\n        if kwargs.get(\"query_builder\"):\n            version_query, read_options, read_query = self.parent.get_library(\n                self.lib_name\n            )._nvs._get_queries(\n                None, None, None, None, query_builder=kwargs[\"query_builder\"]\n            )\n            read_result = self.parent.get_library(self.lib_name)._nvs._read_dataframe(\n                self.symbol, version_query, read_query, read_options\n            )\n            return len(read_result.frame_data.value.data[0])\n        return self._rows\n\n    @property\n    def base_df(self):\n        return self._base_df\n\n    @property\n    def is_large(self):\n        if self.rows() > LARGE_ARCTICDB:\n            return True\n        if self._cols > 50:\n            return True\n        return False\n\n    @property\n    def data(self):\n        return self.load_data()\n\n    @data.setter\n    def data(self, data):\n        try:\n            self.parent.get_library(self.lib_name).write(self.symbol, data)\n        except BaseException:\n            pass\n\n\nclass DtaleBaseStore(dict):\n    def build_instance(self, data_id, data=None):\n        return DtaleInstance(data)\n\n\nclass DtaleArcticDB(DtaleBaseStore):\n    \"\"\"Interface allowing dtale to use 'arcticdb' databases for global data storage.\"\"\"\n\n    def __init__(self, uri=None, library=None, **kwargs):\n        from arcticdb import Arctic\n\n        self._db = dict()\n        self.uri = uri\n        self.conn = Arctic(self.uri)\n        self.lib = None\n        self._libraries = []\n        self._symbols = {}\n        self.load_libraries()\n        self.update_library(library)\n\n    def get_library(self, library_name):\n        return self.conn[library_name]\n\n    def update_library(self, library=None):\n        if self.lib and library == self.lib.name:  # library already selected\n            return\n        if library in self._libraries:\n            self.lib = self.get_library(library)\n            if library not in self._symbols:\n                self.load_symbols()\n        elif library is not None:\n            raise ValueError(\"Library '{}' does not exist!\".format(library))\n\n    def load_libraries(self):\n        self._libraries = sorted(self.conn.list_libraries())\n\n    @property\n    def libraries(self):\n        return self._libraries\n\n    def load_symbols(self, library=None):\n        self._symbols[library or self.lib.name] = sorted(\n            (self.conn[library] if library else self.lib).list_symbols()\n        )\n\n    @property\n    def symbols(self):\n        return self._symbols[self.lib.name]\n\n    def build_instance(self, data_id, data=None):\n        if data_id is None:\n            return DtaleInstance(data)\n        return DtaleArcticDBInstance(data, data_id, self)\n\n    def get(self, key, **kwargs):\n        if key is None:\n            return self.build_instance(key)\n        key = str(key)\n        if key not in self._db:\n            self._db[key] = self.build_instance(key)\n        return self._db[key]\n\n    def __setitem__(self, key, value):\n        if key is None:\n            return\n        key = str(key)\n        self._db[key] = value\n\n    def __delitem__(self, key):\n        if key is None:\n            return\n        key = str(key)\n        # TODO: should we actually delete from ArcticDB???\n        del self._db[key]\n\n    def __contains__(self, key):\n        key = str(key)\n        return key in self._db\n\n    def clear(self):\n        pass\n\n    def to_dict(self):\n        return dict(self._db)\n\n    def items(self):\n        return self.to_dict().items()\n\n    def keys(self):\n        return self.to_dict().keys()\n\n    def __len__(self):\n        return len(self.keys())\n\n    def save_db(self):\n        raise NotImplementedError\n\n\nclass DefaultStore(object):\n    def __init__(self):\n        self._data_store = DtaleBaseStore()\n        self._data_names = dict()\n\n    # Use int for data_id for easier sorting\n    def build_data_id(self):\n        if len(self._data_store) == 0:\n            return \"1\"\n\n        def parse_int(x):\n            try:\n                return int(x)\n            except ValueError:\n                return None\n\n        ids = list(filter(None, map(parse_int, self._data_store.keys())))\n        if not len(ids):\n            return \"1\"\n        return str(max(ids) + 1)\n\n    @property\n    def is_arcticdb(self):\n        return isinstance(self._data_store, DtaleArcticDB)\n\n    # exposing  _data_store for custom data store plugins.\n    @property\n    def store(self):\n        return self._data_store\n\n    @store.setter\n    def store(self, new_store):\n        self._data_store = new_store\n\n    def keys(self):\n        return list(self._data_store.keys())\n\n    def items(self):\n        return self._data_store.items()\n\n    def contains(self, key):\n        if key is None:\n            return False\n        return str(key) in self._data_store\n\n    # this should be a property but somehow it stays 0 no matter what.\n    def size(self):\n        return len(self._data_store)\n\n    def get_data_inst(self, data_id):\n        # handle non-exist data_id\n        if data_id is None:\n            return self._data_store.build_instance(data_id)\n\n        if str(data_id) not in self._data_store:\n            self._data_store[str(data_id)] = self._data_store.build_instance(data_id)\n\n        return self._data_store.get(str(data_id))\n\n    def new_data_inst(self, data_id=None, instance=None):\n        if data_id is None:\n            data_id = self.build_data_id()\n        data_id = str(data_id)\n        new_data = instance or self._data_store.build_instance(data_id)\n        self._data_store[data_id] = new_data\n        return data_id\n\n    def get_data(self, data_id, **kwargs):\n        return self.get_data_inst(data_id).load_data(**kwargs)\n\n    def get_data_id_by_name(self, data_name):\n        data_id = next(\n            (\n                value\n                for key, value in self._data_names.items()\n                if convert_name_to_url_path(key) == data_name or key == data_name\n            ),\n            None,\n        )\n        return data_id\n\n    def get_dataset(self, data_id):\n        return self.get_data_inst(data_id).dataset\n\n    def get_dataset_dim(self, data_id):\n        return self.get_data_inst(data_id).dataset_dim\n\n    def get_dtypes(self, data_id):\n        return self.get_data_inst(data_id).dtypes\n\n    def get_context_variables(self, data_id):\n        return self.get_data_inst(data_id).context_variables\n\n    def get_history(self, data_id):\n        return self.get_data_inst(data_id).history\n\n    def get_name(self, data_id):\n        return self.get_data_inst(data_id).name\n\n    def get_settings(self, data_id):\n        return self.get_data_inst(data_id).settings\n\n    def get_metadata(self, data_id):\n        return self.get_data_inst(data_id).metadata\n\n    def set_data(self, data_id=None, val=None):\n        if data_id is None:\n            data_id = self.new_data_inst()\n        data_id = str(data_id)\n        if data_id not in self._data_store.keys():\n            data_id = self.new_data_inst(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.data = val\n        self._data_store[data_id] = data_inst\n\n    def set_dataset(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.dataset = val\n        self._data_store[data_id] = data_inst\n\n    def set_dataset_dim(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.dataset_dim = val\n        self._data_store[data_id] = data_inst\n\n    def set_dtypes(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.dtypes = val\n        self._data_store[data_id] = data_inst\n\n    def set_name(self, data_id, val):\n        if val in [None, \"\"]:\n            return\n        if val in self._data_names:\n            raise Exception(\"Name {} already exists!\".format(val))\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        self._data_names[val] = data_id\n        data_inst.name = val\n        self._data_store[data_id] = data_inst\n\n    def set_context_variables(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.context_variables = val\n        self._data_store[data_id] = data_inst\n\n    def set_settings(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.settings = val\n        self._data_store[data_id] = data_inst\n\n    def set_metadata(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.metadata = val\n        self._data_store[data_id] = data_inst\n\n    def set_history(self, data_id, val):\n        data_id = str(data_id)\n        data_inst = self.get_data_inst(data_id)\n        data_inst.history = val\n        self._data_store[data_id] = data_inst\n\n    def delete_instance(self, data_id):\n        data_id = str(data_id)\n        instance = self._data_store.get(data_id)\n        if instance:\n            if instance.name:\n                try:\n                    del self._data_names[instance.name]\n                except KeyError:\n                    pass\n            try:\n                del self._data_store[data_id]\n            except KeyError:\n                pass\n\n    def clear_store(self):\n        self._data_store.clear()\n        self._data_names.clear()\n\n\n\"\"\"\nThis block dynamically exports functions from DefaultStore class.\nIt's here for backward compatibility reasons.\nIt may trigger linter errors in other py files because functions are not statically exported.\n\"\"\"\n_default_store = DefaultStore()\nfn_list = list(\n    filter(\n        lambda x: not x.startswith(\"_\"),\n        [x[0] for x in inspect.getmembers(DefaultStore)],\n    )\n)\n\nfor fn_name in fn_list:\n    globals()[fn_name] = getattr(_default_store, fn_name)\n\n\n# for tests. default_store is always initialized.\ndef use_default_store():\n    new_store = DtaleBaseStore()\n    for k, v in _as_dict(_default_store.store).items():\n        new_store[str(k)] = v\n    _default_store.store.clear()\n    _default_store.store = new_store\n    globals()[\"is_arcticdb\"] = getattr(_default_store, \"is_arcticdb\")\n    globals()[\"store\"] = getattr(_default_store, \"store\")\n    pass\n\n\ndef drop_punctuation(val):\n    if PY3:\n        return val.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n    return val.translate(string.maketrans(\"\", \"\"), string.punctuation)\n\n\ndef convert_name_to_url_path(name):\n    if name is None:\n        return None\n    url_name = drop_punctuation(\"{}\".format(name))\n    url_name = url_name.lower()\n    return \"_\".join(url_name.split(\" \"))\n\n\ndef get_dtype_info(data_id, col):\n    dtypes = get_dtypes(data_id)  # noqa: F821\n    return next((c for c in dtypes or [] if c[\"name\"] == col), None)\n\n\ndef update_settings(data_id, settings):\n    curr_settings = _default_store.get_settings(data_id) or {}\n    updated_settings = dict_merge(curr_settings, settings)\n    _default_store.set_settings(data_id, updated_settings)\n\n\ndef get_app_settings():\n    global APP_SETTINGS\n    return APP_SETTINGS\n\n\ndef set_app_settings(settings):\n    global APP_SETTINGS\n\n    for prop, val in settings.items():\n        APP_SETTINGS[prop] = val\n\n    instance_updates = {}\n    if settings.get(\"hide_shutdown\") is not None:\n        instance_updates[\"hide_shutdown\"] = settings.get(\"hide_shutdown\")\n    if settings.get(\"hide_header_editor\") is not None:\n        instance_updates[\"hide_header_editor\"] = settings.get(\"hide_header_editor\")\n    if settings.get(\"lock_header_menu\") is not None:\n        instance_updates[\"lock_header_menu\"] = settings.get(\"lock_header_menu\")\n    if settings.get(\"hide_header_menu\") is not None:\n        instance_updates[\"hide_header_menu\"] = settings.get(\"hide_header_menu\")\n    if settings.get(\"hide_main_menu\") is not None:\n        instance_updates[\"hide_main_menu\"] = settings.get(\"hide_main_menu\")\n    if settings.get(\"hide_column_menus\") is not None:\n        instance_updates[\"hide_column_menus\"] = settings.get(\"hide_column_menus\")\n    if settings.get(\"enable_custom_filters\") is not None:\n        instance_updates[\"enable_custom_filters\"] = settings.get(\n            \"enable_custom_filters\"\n        )\n        if instance_updates[\"enable_custom_filters\"]:\n            logger.warning(\n                (\n                    \"Turning on custom filtering. Custom filters are vulnerable to code injection attacks, please only \"\n                    \"use in trusted environments.\"\n                )\n            )\n\n    if _default_store.size() > 0 and len(instance_updates):\n        for data_id in _default_store.keys():\n            update_settings(data_id, instance_updates)\n\n\ndef get_auth_settings():\n    global AUTH_SETTINGS\n    return AUTH_SETTINGS\n\n\ndef set_auth_settings(settings):\n    global AUTH_SETTINGS\n\n    for prop, val in settings.items():\n        AUTH_SETTINGS[prop] = val\n\n\ndef get_chart_settings():\n    global CHART_SETTINGS\n    return CHART_SETTINGS\n\n\ndef set_chart_settings(settings):\n    global CHART_SETTINGS\n\n    for prop, val in settings.items():\n        CHART_SETTINGS[prop] = val\n\n\ndef cleanup(data_id=None):\n    if data_id is None:\n        _default_store.clear_store()\n    else:\n        _default_store.delete_instance(data_id)\n\n\ndef update_id(old_data_id, new_data_id):\n    if _default_store.contains(new_data_id):\n        raise Exception(\"Data already exists for id ({})\".format(new_data_id))\n    curr_data = _default_store.get_data_inst(old_data_id)\n    _default_store.delete_instance(old_data_id)\n    data_id = str(new_data_id)\n    _default_store.new_data_inst(data_id, curr_data)\n    return new_data_id\n\n\ndef load_flag(data_id, flag_name, default):\n    import dtale\n\n    app_settings = get_app_settings()\n    curr_settings = get_settings(data_id) or {}  # noqa: F821\n    global_flag = getattr(dtale, flag_name.upper())\n    if global_flag != default:\n        return global_flag\n    if flag_name in app_settings and app_settings[flag_name] != default:\n        return app_settings[flag_name]\n    return curr_settings.get(flag_name, app_settings.get(flag_name, default))\n\n\ndef _as_dict(store):\n    \"\"\"Return the dict representation of a data store.\n    Stores must either be an instance of MutableMapping OR have a to_dict method.\n\n    :param store: data store (dict, redis connection, etc.)\n    :return: dict\n    \"\"\"\n    return dict(store.items()) if isinstance(store, MutableMapping) else store.to_dict()\n\n\ndef use_store(store_class, create_store):\n    \"\"\"\n    Customize how dtale stores and retrieves global data.\n    By default it uses global dictionaries, but this can be problematic if\n    there are memory limitations or multiple python processes are running.\n    Ex: a web server with multiple workers (processes) for processing requests.\n\n    :param store_class: Class providing an interface to the data store. To be valid, it must:\n                        1. Implement get, keys, items clear, __setitem__, __delitem__, __iter__, __len__, __contains__.\n                        2. Either be a subclass of MutableMapping or implement the 'to_dict' method.\n    :param create_store: Factory function for producing instances of <store_class>.\n                         Must take 'name' as the only parameter.\n    :return: None\n    \"\"\"\n\n    assert inspect.isclass(store_class), \"Must be a class\"\n    assert all(\n        hasattr(store_class, a)\n        for a in (\n            \"get\",\n            \"clear\",\n            \"keys\",\n            \"items\",\n            \"__setitem__\",\n            \"__delitem__\",\n            \"__len__\",\n            \"__contains__\",\n        )\n    ), \"Missing required methods\"\n    assert issubclass(store_class, MutableMapping) or hasattr(\n        store_class, \"to_dict\"\n    ), 'Must subclass MutableMapping or implement \"to_dict\"'\n\n    assert inspect.isfunction(create_store), \"Must be a function\"\n    if PY3:\n        assert list(inspect.signature(create_store).parameters) == [\n            \"name\"\n        ], 'Must take \"name\" as the only parameter'\n    else:\n        assert inspect.getargspec(create_store).args == [\n            \"name\"\n        ], 'Must take \"name\" as the only parameter'\n\n    def convert(old_store, name):\n        \"\"\"Convert a data store to the new type\n        :param old_store: old data store\n        :param name: name associated with this data store\n        :return: new data store\n        \"\"\"\n        new_store = create_store(name)\n        assert isinstance(new_store, store_class)\n        new_store.clear()\n        for k, v in _as_dict(old_store).items():\n            new_store[k] = v\n        old_store.clear()\n        return new_store\n\n    _default_store.store = convert(_default_store.store, \"default_store\")\n    globals()[\"is_arcticdb\"] = getattr(_default_store, \"is_arcticdb\")\n    globals()[\"store\"] = getattr(_default_store, \"store\")\n\n\ndef use_shelve_store(directory):\n    \"\"\"\n    Configure dtale to use python's standard 'shelve' library for a persistent global data store.\n\n    :param directory: directory that the shelve db files will be stored in\n    :type directory: str\n    :return: None\n    \"\"\"\n    import shelve\n    import time\n    from os.path import join\n    from threading import Thread\n\n    class DtaleShelf(DtaleBaseStore):\n        \"\"\"Interface allowing dtale to use 'shelf' databases for global data storage.\"\"\"\n\n        def __init__(self, filename):\n            self.filename = filename\n            self.db = shelve.open(self.filename, flag=\"c\", writeback=True)\n            # super hacky autosave\n            t = Thread(target=self.save_db)\n            t.daemon = True\n            t.start()\n\n        def get(self, key):\n            # using str here because shelve doesn't support int keys\n            key = str(key)\n            return self.db.get(key)\n\n        def __setitem__(self, key, value):\n            key = str(key)\n            self.db[key] = value\n            self.db.sync()\n\n        def __delitem__(self, key):\n            key = str(key)\n            del self.db[key]\n            self.db.sync()\n\n        def __contains__(self, key):\n            key = str(key)\n            return key in self.db\n\n        def clear(self):\n            self.db.clear()\n            self.db.sync()\n\n        def to_dict(self):\n            return dict(self.db)\n\n        def items(self):\n            return self.to_dict().items()\n\n        def keys(self):\n            return self.to_dict().keys()\n\n        def __len__(self):\n            return len(self.db)\n\n        def save_db(self):\n            while True:\n                self.db.sync()\n                time.sleep(5)\n\n    def create_shelf(name):\n        file_path = join(directory, name)\n        return DtaleShelf(file_path)\n\n    use_store(DtaleShelf, create_shelf)\n\n\ndef use_redis_store(directory, *args, **kwargs):\n    \"\"\"Configure dtale to use redis for the global data store. Useful for web servers.\n\n    :param db_folder: folder that db files will be stored in\n    :type db_folder: str\n    :param args: All other arguments supported by the redislite.Redis() class\n    :param kwargs: All other keyword arguments supported by the redislite.Redis() class\n    :return: None\n    \"\"\"\n    import pickle\n    from os.path import join\n\n    try:\n        from redislite import Redis\n    except ImportError:\n        raise Exception(\"redislite must be installed\")\n\n    class DtaleRedis(DtaleBaseStore, Redis):\n        \"\"\"Wrapper class around Redis() to make it work as a global data store in dtale.\"\"\"\n\n        def __init__(self, file_path, *args, **kwargs):\n            super(Redis, self).__init__(file_path, *args, **kwargs)\n\n        def __setitem__(self, key, value):\n            key = str(key)\n            self.set(key, value)\n\n        def __delitem__(self, key):\n            key = str(key)\n            super(Redis, self).__delitem__(str(key))\n\n        def __contains__(self, key):\n            key = str(key)\n            return super(Redis, self).__contains__(str(key))\n\n        def get(self, name, *args, **kwargs):\n            value = super(Redis, self).get(name, *args, **kwargs)\n            if value is not None:\n                return pickle.loads(value)\n\n        def keys(self):\n            return [str(k) for k in super(Redis, self).keys()]\n\n        def set(self, name, value, *args, **kwargs):\n            value = pickle.dumps(value)\n            return super(Redis, self).set(name, value, *args, **kwargs)\n\n        def clear(self):\n            self.flushdb()\n\n        def to_dict(self):\n            return {k.decode(\"utf-8\"): self.get(k) for k in super(Redis, self).keys()}\n\n        def items(self):\n            return self.to_dict().items()\n\n        def __len__(self):\n            return len(self.keys())\n\n    def create_redis(name):\n        file_path = join(directory, name + \".db\")\n        return DtaleRedis(file_path, *args, **kwargs)\n\n    use_store(DtaleRedis, create_redis)\n\n\ndef use_arcticdb_store(*args, **kwargs):\n    \"\"\"\n    Configure dtale to use arcticdb for a persistent global data store.\n\n    :param uri: URI that arcticdb will connect to (local file storage: lmdb:///<path>)\n    :type uri: str\n    :param library: default library to load from arcticdb URI\n    :type library: str\n    :param symbol: defualt symbol to load from library\n    :type symbol: str\n    :return: None\n    \"\"\"\n\n    def create_arcticdb(name):\n        return DtaleArcticDB(**kwargs)\n\n    use_store(DtaleArcticDB, create_arcticdb)\n", "<!doctype html>\n<html style=\"{{ 'height: 100%' if network else '' }}\">\n    <head>\n\t<meta charset=\"utf-8\">\n\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=yes\">\n\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n    {% if is_app_root_defined(config.APPLICATION_ROOT) %}\n        <script type=\"text/javascript\">\n            window.resourceBaseUrl = '{{config.APPLICATION_ROOT}}';\n        </script>\n    {% endif %}\n\t<link rel=\"shortcut icon\" href=\"{{ url_for('static', filename='images/favicon.png') }}\">\n    <link rel=\"preload\" href=\"{{ url_for('static', filename='fonts/istok.woff') }}\" as=\"font\" type=\"font/woff\" crossorigin>\n    <link rel=\"preload\" href=\"{{ url_for('static', filename='fonts/istok-bold.woff') }}\" as=\"font\" type=\"font/woff\" crossorigin>\n    <link rel=\"preload\" href=\"{{ url_for('static', filename='fonts/Road_Rage.woff') }}\" as=\"font\" type=\"font/woff\" crossorigin>\n    <title>{{ title }}</title>\n    {% if missing_js is not defined %}\n        <script type=\"text/javascript\" src=\"{{ url_for('static', filename='dist/base_styles_bundle.js') }}\"></script>\n    {% endif %}\n    {% block css %}{% endblock %}\n    {#\n        Despite the fact we reload these again later on, in order for the header to rendered correctly\n        before the rest of the page is built we need to load them here as well.\n    #}\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/main.css') }}\" />\n    </head>\n    <body class=\"{{ theme }}-mode\" style=\"{{ 'height: 100%' if network else '' }}\">\n        {% if github_fork %}\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/github_fork.css') }}\" />\n        <span id=\"forkongithub\">\n            <a href=\"https://github.com/man-group/dtale\">Fork me on GitHub</a>\n        </span>\n        {% endif %}\n        <input type=\"hidden\" id=\"data_id\" value=\"{{data_id}}\" />\n        <input type=\"hidden\" id=\"xarray\" value=\"{{xarray}}\" />\n        <input type=\"hidden\" id=\"xarray_dim\" value=\"{{xarray_dim}}\" />\n        <input type=\"hidden\" id=\"app_settings\" value=\"{{app_settings}}\" />\n        <input type=\"hidden\" id=\"settings\" value=\"{{settings}}\" />\n        <input type=\"hidden\" id=\"version\" value=\"{{version}}\" />\n        <input type=\"hidden\" id=\"hide_shutdown\" value=\"{{hide_shutdown}}\" />\n        <input type=\"hidden\" id=\"hide_header_editor\" value=\"{{hide_header_editor}}\" />\n        <input type=\"hidden\" id=\"lock_header_menu\" value=\"{{lock_header_menu}}\" />\n        <input type=\"hidden\" id=\"hide_header_menu\" value=\"{{hide_header_menu}}\" />\n        <input type=\"hidden\" id=\"hide_main_menu\" value=\"{{hide_main_menu}}\" />\n        <input type=\"hidden\" id=\"hide_column_menus\" value=\"{{hide_column_menus}}\" />\n        <input type=\"hidden\" id=\"enable_custom_filters\" value=\"{{enable_custom_filters}}\" />\n        <input type=\"hidden\" id=\"allow_cell_edits\" value=\"{{allow_cell_edits}}\" />\n        <input type=\"hidden\" id=\"hide_drop_rows\" value=\"{{hide_drop_rows}}\" />\n        <input type=\"hidden\" id=\"is_vscode\" value=\"{{is_vscode}}\" />\n        <input type=\"hidden\" id=\"is_arcticdb\" value=\"{{is_arcticdb}}\" />\n        <input type=\"hidden\" id=\"arctic_conn\" value=\"{{arctic_conn}}\" />\n        <input type=\"hidden\" id=\"column_count\" value=\"{{column_count}}\" />\n        <input type=\"hidden\" id=\"processes\" value={{processes}} />\n        <input type=\"hidden\" id=\"theme\" value=\"{{theme}}\" />\n        <input type=\"hidden\" id=\"pin_menu\" value=\"{{pin_menu}}\" />\n        <input type=\"hidden\" id=\"language\" value=\"{{language}}\" />\n        <input type=\"hidden\" id=\"python_version\" value=\"{{python_version}}\" />\n        <input type=\"hidden\" id=\"auth\" value=\"{{session.get('logged_in', False)}}\" />\n        <input type=\"hidden\" id=\"username\" value=\"{{session.get('username')}}\" />\n        <input type=\"hidden\" id=\"predefined_filters\" value=\"{{predefined_filters}}\" />\n        <input type=\"hidden\" id=\"max_column_width\" value={{max_column_width}} />\n        <input type=\"hidden\" id=\"max_row_height\" value={{max_row_height}} />\n        <input type=\"hidden\" id=\"main_title\" value=\"{{main_title}}\" />\n        <input type=\"hidden\" id=\"main_title_font\" value=\"{{main_title_font}}\" />\n        <input type=\"hidden\" id=\"query_engine\" value=\"{{query_engine}}\" />\n        <input type=\"hidden\" id=\"open_custom_filter_on_startup\" value=\"{{open_custom_filter_on_startup}}\" />\n        <input type=\"hidden\" id=\"open_predefined_filters_on_startup\" value=\"{{open_predefined_filters_on_startup}}\" />\n        {% block full_content %}{% endblock %}\n        {% if missing_js is not defined %}\n        <script type=\"text/javascript\" src=\"{{ url_for('static', filename='dist/polyfills_bundle.js') }}\"></script>\n        {% endif %}\n        {% block js %}{% endblock %}\n        {#\n          In order to get styles to load correctly, we need to reload these files.\n          In CSS, the last style declared takes precedence.\n        #}\n        <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/main.css') }}\" />\n        {#\n           When loaded in an iframe, the code below will allow the parent\n           window to register and listen for the data id.\n        #}\n        <script type=\"text/javascript\" >\n            const msg = {\n                data_id: '{{data_id}}',\n                xarray: '{{xarray}}',\n                xarray_dim: '{{xarray_dim}}',\n                settings: '{{settings}}',\n                version: '{{version}}',\n                hide_shutdown: '{{hide_shutdown}}',\n                processes: '{{processes}}',\n            };\n            window.addEventListener(\n                'load',\n                function() { window.parent.postMessage(msg, '*'); },\n                false\n            );\n        </script>\n    </body>\n</html>\n", "# coding=utf-8\nfrom __future__ import absolute_import, division\n\nimport os\nimport time\nfrom builtins import map, range, str, zip\nfrom functools import wraps\nfrom logging import getLogger\n\nfrom flask import (\n    current_app,\n    json,\n    make_response,\n    redirect,\n    render_template,\n    request,\n    Response,\n)\n\nimport itertools\nimport missingno as msno\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport platform\nimport requests\nimport scipy.stats as sts\nimport seaborn as sns\nimport xarray as xr\nfrom six import BytesIO, PY3, string_types, StringIO\n\nimport dtale.correlations as correlations\nimport dtale.datasets as datasets\nimport dtale.env_util as env_util\nimport dtale.gage_rnr as gage_rnr\nimport dtale.global_state as global_state\nimport dtale.pandas_util as pandas_util\nimport dtale.predefined_filters as predefined_filters\nfrom dtale import dtale\nfrom dtale.charts.utils import build_base_chart, CHART_POINTS_LIMIT\nfrom dtale.cli.clickutils import retrieve_meta_info_and_version\nfrom dtale.column_analysis import ColumnAnalysis\nfrom dtale.column_builders import ColumnBuilder, printable\nfrom dtale.column_filters import ColumnFilter\nfrom dtale.column_replacements import ColumnReplacement\nfrom dtale.combine_data import CombineData\nfrom dtale.describe import load_describe\nfrom dtale.duplicate_checks import DuplicateCheck\nfrom dtale.dash_application.charts import (\n    build_raw_chart,\n    chart_url_params,\n    chart_url_querystring,\n    export_chart,\n    export_png,\n    export_chart_data,\n    url_encode_func,\n)\nfrom dtale.data_reshapers import DataReshaper\nfrom dtale.code_export import build_code_export\nfrom dtale.query import (\n    build_col_key,\n    build_query,\n    build_query_builder,\n    handle_predefined,\n    load_filterable_data,\n    load_index_filter,\n    run_query,\n)\nfrom dtale.timeseries_analysis import TimeseriesAnalysis\nfrom dtale.utils import (\n    DuplicateDataError,\n    apply,\n    build_formatters,\n    build_shutdown_url,\n    build_url,\n    classify_type,\n    coord_type,\n    dict_merge,\n    divide_chunks,\n    export_to_csv_buffer,\n    find_dtype,\n    find_dtype_formatter,\n    format_data,\n    format_grid,\n    get_bool_arg,\n    get_dtypes,\n    get_int_arg,\n    get_json_arg,\n    get_str_arg,\n    get_url_quote,\n    grid_columns,\n    grid_formatter,\n    json_date,\n    json_float,\n    json_int,\n    json_timestamp,\n    jsonify,\n    jsonify_error,\n    read_file,\n    make_list,\n    optimize_df,\n    option,\n    retrieve_grid_params,\n    running_with_flask_debug,\n    running_with_pytest,\n    sort_df_for_grid,\n    unique_count,\n)\nfrom dtale.translations import text\n\nlogger = getLogger(__name__)\nIDX_COL = str(\"dtale_index\")\n\n\ndef exception_decorator(func):\n    @wraps(func)\n    def _handle_exceptions(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except BaseException as e:\n            return jsonify_error(e)\n\n    return _handle_exceptions\n\n\ndef matplotlib_decorator(func):\n    @wraps(func)\n    def _handle_matplotlib(*args, **kwargs):\n        try:\n            import matplotlib\n\n            current_backend = matplotlib.get_backend()\n\n            matplotlib.use(\"agg\")  # noqa: E261\n\n            import matplotlib.pyplot as plt\n\n            plt.rcParams[\"font.sans-serif\"] = [\n                \"SimHei\"\n            ]  # Or any other Chinese characters\n            matplotlib.rcParams[\"font.family\"] = [\"Heiti TC\"]\n\n            return func(*args, **kwargs)\n        except BaseException as e:\n            raise e\n        finally:\n            try:\n                matplotlib.pyplot.switch_backend(current_backend)\n            except BaseException:\n                pass\n\n    return _handle_matplotlib\n\n\nclass NoDataLoadedException(Exception):\n    \"\"\"Container class for any scenario where no data has been loaded into D-Tale.\n\n    This will usually force the user to load data using the CSV/TSV loader UI.\n    \"\"\"\n\n\ndef head_endpoint(popup_type=None):\n    data_keys = global_state.keys()\n    if not len(data_keys):\n        return \"popup/{}\".format(\"arcticdb\" if global_state.is_arcticdb else \"upload\")\n    head_id = sorted(data_keys)[0]\n    head_id = get_url_quote()(get_url_quote()(head_id, safe=\"\"))\n    if popup_type:\n        return \"popup/{}/{}\".format(popup_type, head_id)\n    return \"main/{}\".format(head_id)\n\n\ndef in_ipython_frontend():\n    \"\"\"\n    Helper function which is variation of :meth:`pandas:pandas.io.formats.console.in_ipython_frontend` which\n    checks to see if we are inside an IPython zmq frontend\n\n    :return: `True` if D-Tale is being invoked within ipython notebook, `False` otherwise\n    \"\"\"\n    try:\n        from IPython import get_ipython\n\n        ip = get_ipython()\n        return \"zmq\" in str(type(ip)).lower()\n    except BaseException:\n        pass\n    return False\n\n\ndef kill(base):\n    \"\"\"\n    This function fires a request to this instance's 'shutdown' route to kill it\n\n    \"\"\"\n    try:\n        requests.get(build_shutdown_url(base))\n    except BaseException:\n        logger.info(\"Shutdown complete\")\n\n\ndef is_up(base):\n    \"\"\"\n    This function checks to see if instance's :mod:`flask:flask.Flask` process is up by hitting 'health' route.\n\n    Using `verify=False` will allow us to validate instances being served up over SSL\n\n    :return: `True` if :mod:`flask:flask.Flask` process is up and running, `False` otherwise\n    \"\"\"\n    try:\n        return requests.get(\"{}/health\".format(base), verify=False, timeout=10).ok\n    except BaseException:\n        return False\n\n\nclass DtaleData(object):\n    \"\"\"\n    Wrapper class to abstract the global state of a D-Tale process while allowing\n    a user to programatically interact with a running D-Tale instance\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param url: endpoint for instances :class:`flask:flask.Flask` process\n    :type url: str\n\n    Attributes:\n        _data_id            data identifier\n        _url                :class:`flask:flask.Flask` endpoint\n        _notebook_handle    reference to the most recent :class:`ipython:IPython.display.DisplayHandle` created\n\n    :Example:\n\n        >>> import dtale\n        >>> import pandas as pd\n        >>> df = pd.DataFrame([dict(a=1,b=2,c=3)])\n        >>> d = dtale.show(df)\n        >>> tmp = d.data.copy()\n        >>> tmp['d'] = 4\n        >>> d.data = tmp\n        >>> d.kill()\n    \"\"\"\n\n    def __init__(self, data_id, url, is_proxy=False, app_root=None):\n        self._data_id = data_id\n        self._url = url\n        self._notebook_handle = None\n        self.started_with_open_browser = False\n        self.is_proxy = is_proxy\n        self.app_root = app_root\n\n    def build_main_url(self, name=None):\n        if name or self._data_id:\n            quoted_data_id = get_url_quote()(\n                get_url_quote()(name or self._data_id, safe=\"\")\n            )\n            return \"{}/dtale/main/{}\".format(\n                self.app_root if self.is_proxy else self._url, quoted_data_id\n            )\n        return (\n            self.app_root if self.is_proxy else self._url\n        )  # \"load data\" or \"library/symbol selection\" screens\n\n    @property\n    def _main_url(self):\n        suffix = self._data_id\n        name = global_state.get_name(suffix)\n        if name:\n            suffix = global_state.convert_name_to_url_path(name)\n        return self.build_main_url(name=suffix)\n\n    @property\n    def data(self):\n        \"\"\"\n        Property which is a reference to the globally stored data associated with this instance\n\n        \"\"\"\n        return global_state.get_data(self._data_id)\n\n    @data.setter\n    def data(self, data):\n        \"\"\"\n        Setter which will go through all standard formatting to make sure changes will be handled correctly by D-Tale\n\n        \"\"\"\n        startup(self._url, data=data, data_id=self._data_id)\n\n    def get_corr_matrix(self, encode_strings=False, as_df=False):\n        \"\"\"Helper function to build correlation matrix from data (can be an image or dataframe).\"\"\"\n        matrix_data = build_correlations_matrix(\n            self._data_id, is_pps=False, encode_strings=encode_strings, image=not as_df\n        )\n        _, _, _, _, _, _, df_or_image = matrix_data\n        return df_or_image\n\n    def get_pps_matrix(self, encode_strings=False, as_df=False):\n        \"\"\"Helper function to build correlation matrix from data (can be an image or dataframe).\"\"\"\n        matrix_data = build_correlations_matrix(\n            self._data_id, is_pps=True, encode_strings=encode_strings, image=not as_df\n        )\n        _, _, _, _, _, _, df_or_image = matrix_data\n        return df_or_image\n\n    def update_id(self, new_data_id):\n        \"\"\"\n        Update current data_id to new data_id\n\n        :param new_data_id: the data_id to update to\n        \"\"\"\n        self._data_id = global_state.update_id(self._data_id, new_data_id)\n\n    def main_url(self):\n        \"\"\"\n        Helper function creating main :class:`flask:flask.Flask` route using instance's url & data_id\n        :return: str\n        \"\"\"\n        if in_ipython_frontend():\n            print(self._main_url)\n            return None\n        return self._main_url\n\n    def kill(self):\n        \"\"\"Helper function to pass instance's endpoint to :meth:`dtale.views.kill`\"\"\"\n        kill_url = self._url\n        if in_ipython_frontend() and not kill_url.startswith(\"http\"):\n            from dtale.app import ACTIVE_PORT, ACTIVE_HOST\n\n            kill_url = build_url(ACTIVE_PORT, ACTIVE_HOST)\n        kill(kill_url)\n\n    def cleanup(self):\n        \"\"\"Helper function to clean up data associated with this instance from global state.\"\"\"\n        global_state.cleanup(self._data_id)\n\n    def update_settings(self, **updates):\n        \"\"\"\n        Helper function for updating instance-specific settings. For example:\n        * allow_cell_edits - whether cells can be edited\n        * locked - which columns are locked to the left of the grid\n        * sort - The sort to apply to the data on startup (EX: [(\"col1\", \"ASC\"), (\"col2\", \"DESC\"),...])\n        * custom_formats - display formatting for specific columns\n        * background_mode - different background displays in grid\n        * range_highlights - specify background colors for ranges of values in the grid\n        * vertical_headers - if True, then rotate column headers vertically\n        * column_edit_options - the options to allow on the front-end when editing a cell for the columns specified\n        * highlight_filter - if True, then highlight rows on the frontend which will be filtered when applying a filter\n                             rather than hiding them from the dataframe\n        * hide_shutdown - if true, this will hide the \"Shutdown\" button from users\n        * nan_display - if value in dataframe is :attr:`numpy:numpy.nan` then return this value on the frontend\n        * hide_header_editor - if true, this will hide header editor when editing cells on the frontend\n        * lock_header_menu - if true, this will always the display the header menu which usually only displays when you\n                             hover over the top\n        * hide_header_menu - if true, this will hide the header menu from the screen\n        * hide_main_menu - if true, this will hide the main menu from the screen\n        * hide_column_menus - if true, this will hide the column menus from the screen\n        * enable_custom_filters - if True, allow users to specify custom filters from the UI using pandas.query strings\n\n        After applying please refresh any open browsers!\n        \"\"\"\n        name_updates = dict(\n            range_highlights=\"rangeHighlight\",\n            column_formats=\"columnFormats\",\n            background_mode=\"backgroundMode\",\n            vertical_headers=\"verticalHeaders\",\n            highlight_filter=\"highlightFilter\",\n        )\n        settings = {name_updates.get(k, k): v for k, v in updates.items()}\n        global_state.update_settings(self._data_id, settings)\n\n    def get_settings(self):\n        \"\"\"Helper function for retrieving any instance-specific settings.\"\"\"\n        return global_state.get_settings(self._data_id) or {}\n\n    def open_browser(self):\n        \"\"\"\n        This function uses the :mod:`python:webbrowser` library to try and automatically open server's default browser\n        to this D-Tale instance\n        \"\"\"\n        env_util.open_browser(self._main_url)\n\n    def is_up(self):\n        \"\"\"\n        Helper function to pass instance's endpoint to :meth:`dtale.views.is_up`\n        \"\"\"\n        return is_up(self._url)\n\n    def __str__(self):\n        \"\"\"\n        Will try to create an :class:`ipython:IPython.display.IFrame` if being invoked from within ipython notebook\n        otherwise simply returns the output of running :meth:`pandas:pandas.DataFrame.__str__` on the data associated\n        with this instance\n\n        \"\"\"\n        if in_ipython_frontend():\n            if self.started_with_open_browser:\n                self.started_with_open_browser = False\n                return \"\"\n            self.notebook()\n            return \"\"\n\n        if global_state.is_arcticdb and global_state.store.get(self._data_id).is_large:\n            return self.main_url()\n\n        return self.data.__str__()\n\n    def __repr__(self):\n        \"\"\"\n        Will try to create an :class:`ipython:IPython.display.IFrame` if being invoked from within ipython notebook\n        otherwise simply returns the output of running :meth:`pandas:pandas.DataFrame.__repr__` on the data for\n        this instance\n\n        \"\"\"\n        if in_ipython_frontend():\n            if self.started_with_open_browser:\n                self.started_with_open_browser = False\n                return \"\"\n            self.notebook()\n            if self._notebook_handle is not None:\n                return \"\"\n        return self.main_url()\n\n    def _build_iframe(\n        self, route=\"/dtale/iframe/\", params=None, width=\"100%\", height=475\n    ):\n        \"\"\"\n        Helper function to build an :class:`ipython:IPython.display.IFrame` if that module exists within\n        your environment\n\n        :param route: the :class:`flask:flask.Flask` route to hit on D-Tale\n        :type route: str, optional\n        :param params: properties & values passed as query parameters to the route\n        :type params: dict, optional\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        :return: :class:`ipython:IPython.display.IFrame`\n        \"\"\"\n        try:\n            from IPython.display import IFrame\n        except ImportError:\n            logger.info(\"in order to use this function, please install IPython\")\n            return None\n        iframe_url = \"{}{}{}\".format(\n            self.app_root if self.is_proxy else self._url, route, self._data_id\n        )\n        if params is not None:\n            if isinstance(params, string_types):  # has this already been encoded?\n                iframe_url = \"{}?{}\".format(iframe_url, params)\n            else:\n                iframe_url = \"{}?{}\".format(iframe_url, url_encode_func()(params))\n\n        return IFrame(iframe_url, width=width, height=height)\n\n    def notebook(self, route=\"/dtale/iframe/\", params=None, width=\"100%\", height=475):\n        \"\"\"\n        Helper function which checks to see if :mod:`flask:flask.Flask` process is up and running and then tries to\n        build an :class:`ipython:IPython.display.IFrame` and run :meth:`ipython:IPython.display.display` on it so\n        it will be displayed in the ipython notebook which invoked it.\n\n        A reference to the :class:`ipython:IPython.display.DisplayHandle` is stored in _notebook_handle for\n        updating if you are running ipython>=5.0\n\n        :param route: the :class:`flask:flask.Flask` route to hit on D-Tale\n        :type route: str, optional\n        :param params: properties & values passed as query parameters to the route\n        :type params: dict, optional\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        \"\"\"\n        try:\n            from IPython.display import display\n        except ImportError:\n            logger.info(\"in order to use this function, please install IPython\")\n            return self.data.__repr__()\n\n        retries = 0\n        while not self.is_up() and retries < 10:\n            time.sleep(0.01)\n            retries += 1\n\n        self._notebook_handle = display(\n            self._build_iframe(route=route, params=params, width=width, height=height),\n            display_id=True,\n        )\n        if self._notebook_handle is None:\n            self._notebook_handle = True\n\n    def notebook_correlations(self, col1, col2, width=\"100%\", height=475):\n        \"\"\"\n        Helper function to build an `ipython:IPython.display.IFrame` pointing at the correlations popup\n\n        :param col1: column on left side of correlation\n        :type col1: str\n        :param col2: column on right side of correlation\n        :type col2: str\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        :return: :class:`ipython:IPython.display.IFrame`\n        \"\"\"\n        self.notebook(\n            \"/dtale/popup/correlations/\",\n            params=dict(col1=col1, col2=col2),\n            width=width,\n            height=height,\n        )\n\n    def notebook_charts(\n        self,\n        chart_type=\"line\",\n        query=None,\n        x=None,\n        y=None,\n        z=None,\n        group=None,\n        agg=None,\n        window=None,\n        rolling_comp=None,\n        barmode=None,\n        barsort=None,\n        width=\"100%\",\n        height=800,\n    ):\n        \"\"\"\n        Helper function to build an `ipython:IPython.display.IFrame` pointing at the charts popup\n\n        :param chart_type: type of chart, possible options are line|bar|pie|scatter|3d_scatter|surface|heatmap\n        :type chart_type: str\n        :param query: pandas dataframe query string\n        :type query: str, optional\n        :param x: column to use for the X-Axis\n        :type x: str\n        :param y: columns to use for the Y-Axes\n        :type y: list of str\n        :param z: column to use for the Z-Axis\n        :type z: str, optional\n        :param group: column(s) to use for grouping\n        :type group: list of str or str, optional\n        :param agg: specific aggregation that can be applied to y or z axes.  Possible values are: count, first, last,\n                    mean, median, min, max, std, var, mad, prod, sum.  This is included in label of axis it is being\n                    applied to.\n        :type agg: str, optional\n        :param window: number of days to include in rolling aggregations\n        :type window: int, optional\n        :param rolling_comp: computation to use in rolling aggregations\n        :type rolling_comp: str, optional\n        :param barmode: mode to use for bar chart display. possible values are stack|group(default)|overlay|relative\n        :type barmode: str, optional\n        :param barsort: axis name to sort the bars in a bar chart by (default is the 'x', but other options are any of\n                        columns names used in the 'y' parameter\n        :type barsort: str, optional\n        :param width: width of the ipython cell\n        :type width: str or int, optional\n        :param height: height of the ipython cell\n        :type height: str or int, optional\n        :return: :class:`ipython:IPython.display.IFrame`\n        \"\"\"\n        params = dict(\n            chart_type=chart_type,\n            query=query,\n            x=x,\n            y=make_list(y),\n            z=z,\n            group=make_list(group),\n            agg=agg,\n            window=window,\n            rolling_comp=rolling_comp,\n            barmode=barmode,\n            barsort=barsort,\n        )\n        self.notebook(\n            route=\"/dtale/charts/\",\n            params=chart_url_querystring(params),\n            width=width,\n            height=height,\n        )\n\n    def offline_chart(\n        self,\n        chart_type=None,\n        query=None,\n        x=None,\n        y=None,\n        z=None,\n        group=None,\n        agg=None,\n        window=None,\n        rolling_comp=None,\n        barmode=None,\n        barsort=None,\n        yaxis=None,\n        filepath=None,\n        title=None,\n        # fmt: off\n        **kwargs\n        # fmt: on\n    ):\n        \"\"\"\n        Builds the HTML for a plotly chart figure to saved to a file or output to a jupyter notebook\n\n        :param chart_type: type of chart, possible options are line|bar|pie|scatter|3d_scatter|surface|heatmap\n        :type chart_type: str\n        :param query: pandas dataframe query string\n        :type query: str, optional\n        :param x: column to use for the X-Axis\n        :type x: str\n        :param y: columns to use for the Y-Axes\n        :type y: list of str\n        :param z: column to use for the Z-Axis\n        :type z: str, optional\n        :param group: column(s) to use for grouping\n        :type group: list of str or str, optional\n        :param agg: specific aggregation that can be applied to y or z axes.  Possible values are: count, first, last,\n                    mean, median, min, max, std, var, mad, prod, sum.  This is included in label of axis it is being\n                    applied to.\n        :type agg: str, optional\n        :param window: number of days to include in rolling aggregations\n        :type window: int, optional\n        :param rolling_comp: computation to use in rolling aggregations\n        :type rolling_comp: str, optional\n        :param barmode: mode to use for bar chart display. possible values are stack|group(default)|overlay|relative\n        :type barmode: str, optional\n        :param barsort: axis name to sort the bars in a bar chart by (default is the 'x', but other options are any of\n                        columns names used in the 'y' parameter\n        :type barsort: str, optional\n        :param yaxis: dictionary specifying the min/max for each y-axis in your chart\n        :type yaxis: dict, optional\n        :param filepath: location to save HTML output\n        :type filepath: str, optional\n        :param title: Title of your chart\n        :type title: str, optional\n        :param kwargs: optional keyword arguments, here in case invalid arguments are passed to this function\n        :type kwargs: dict\n        :return: possible outcomes are:\n                 - if run within a jupyter notebook and no 'filepath' is specified it will print the resulting HTML\n                   within a cell in your notebook\n                 - if 'filepath' is specified it will save the chart to the path specified\n                 - otherwise it will return the HTML output as a string\n        \"\"\"\n        params = dict(\n            chart_type=chart_type,\n            query=query,\n            x=x,\n            y=make_list(y),\n            z=z,\n            group=make_list(group),\n            agg=agg,\n            window=window,\n            rolling_comp=rolling_comp,\n            barmode=barmode,\n            barsort=barsort,\n            yaxis=yaxis,\n            title=title,\n        )\n        params = dict_merge(params, kwargs)\n\n        if filepath is None and in_ipython_frontend():\n            from plotly.offline import iplot, init_notebook_mode\n\n            init_notebook_mode(connected=True)\n            chart = build_raw_chart(self._data_id, export=True, **params)\n            chart.pop(\n                \"id\", None\n            )  # for some reason iplot does not like when the 'id' property is populated\n            iplot(chart)\n            return\n\n        html_str = export_chart(self._data_id, params)\n        if filepath is None:\n            return html_str\n\n        if not filepath.endswith(\".html\"):\n            filepath = \"{}.html\".format(filepath)\n\n        with open(filepath, \"w\") as f:\n            f.write(html_str)\n\n    def adjust_cell_dimensions(self, width=\"100%\", height=350):\n        \"\"\"\n        If you are running ipython>=5.0 then this will update the most recent notebook cell you displayed D-Tale in\n        for this instance with the height/width properties you have passed in as input\n\n        :param width: width of the ipython cell\n        :param height: height of the ipython cell\n        \"\"\"\n        if self._notebook_handle is not None and hasattr(\n            self._notebook_handle, \"update\"\n        ):\n            self._notebook_handle.update(self._build_iframe(width=width, height=height))\n        else:\n            logger.debug(\"You must ipython>=5.0 installed to use this functionality\")\n\n\ndef dtype_formatter(data, dtypes, data_ranges, prev_dtypes=None):\n    \"\"\"\n    Helper function to build formatter for the descriptive information about each column in the dataframe you\n    are viewing in D-Tale.  This data is later returned to the browser to help with controlling inputs to functions\n    which are heavily tied to specific data types.\n\n    :param data: dataframe\n    :type data: :class:`pandas:pandas.DataFrame`\n    :param dtypes: column data type\n    :type dtypes: dict\n    :param data_ranges: dictionary containing minimum and maximum value for column (if applicable)\n    :type data_ranges: dict, optional\n    :param prev_dtypes: previous column information for syncing updates to pre-existing columns\n    :type prev_dtypes: dict, optional\n    :return: formatter function which takes column indexes and names\n    :rtype: func\n    \"\"\"\n\n    def _formatter(col_index, col):\n        visible = True\n        dtype = dtypes.get(col)\n        if prev_dtypes and col in prev_dtypes:\n            visible = prev_dtypes[col].get(\"visible\", True)\n        s = data[col]\n        dtype_data = dict(\n            name=col,\n            dtype=dtype,\n            index=col_index,\n            visible=visible,\n            hasOutliers=0,\n            hasMissing=1,\n        )\n        if global_state.is_arcticdb:\n            return dtype_data\n\n        dtype_data[\"unique_ct\"] = unique_count(s)\n        dtype_data[\"hasMissing\"] = int(s.isnull().sum())\n        classification = classify_type(dtype)\n        if (\n            classification in [\"F\", \"I\"] and not s.isnull().all() and col in data_ranges\n        ):  # floats/ints\n            col_ranges = data_ranges[col]\n            if not any((np.isnan(v) or np.isinf(v) for v in col_ranges.values())):\n                dtype_data = dict_merge(col_ranges, dtype_data)\n\n            # load outlier information\n            o_s, o_e = calc_outlier_range(s)\n            if not any((np.isnan(v) or np.isinf(v) for v in [o_s, o_e])):\n                dtype_data[\"hasOutliers\"] += int(((s < o_s) | (s > o_e)).sum())\n                dtype_data[\"outlierRange\"] = dict(lower=o_s, upper=o_e)\n            dtype_data[\"skew\"] = json_float(s.skew())\n            dtype_data[\"kurt\"] = json_float(s.kurt())\n\n        if classification in [\"F\", \"I\"] and not s.isnull().all():\n            # build variance flag\n            unique_ct = dtype_data[\"unique_ct\"]\n            check1 = (unique_ct / len(data[col])) < 0.1\n            check2 = False\n            if check1 and unique_ct >= 2:\n                val_counts = s.value_counts()\n                check2 = (val_counts.values[0] / val_counts.values[1]) > 20\n            dtype_data[\"lowVariance\"] = bool(check1 and check2)\n            dtype_data[\"coord\"] = coord_type(s)\n\n        if classification in [\"D\"] and not s.isnull().all():\n            timestamps = apply(s, lambda x: json_timestamp(x, np.nan))\n            dtype_data[\"skew\"] = json_float(timestamps.skew())\n            dtype_data[\"kurt\"] = json_float(timestamps.kurt())\n\n        if classification == \"S\" and not dtype_data[\"hasMissing\"]:\n            if (\n                dtype.startswith(\"category\")\n                and classify_type(s.dtype.categories.dtype.name) == \"S\"\n            ):\n                dtype_data[\"hasMissing\"] += int(\n                    (apply(s, lambda x: str(x).strip()) == \"\").sum()\n                )\n            else:\n                dtype_data[\"hasMissing\"] += int(\n                    (s.astype(\"str\").str.strip() == \"\").sum()\n                )\n\n        return dtype_data\n\n    return _formatter\n\n\ndef calc_data_ranges(data, dtypes={}):\n    try:\n        return data.agg([\"min\", \"max\"]).to_dict()\n    except ValueError:\n        # I've seen when transposing data and data types get combined into one column this exception emerges\n        # when calling 'agg' on the new data\n        return {}\n    except TypeError:\n        non_str_cols = [\n            c for c in data.columns if classify_type(dtypes.get(c, \"\")) != \"S\"\n        ]\n        if not len(non_str_cols):\n            return {}\n        try:\n            return data[non_str_cols].agg([\"min\", \"max\"]).to_dict()\n        except BaseException:\n            return {}\n\n\ndef build_dtypes_state(data, prev_state=None, ranges=None):\n    \"\"\"\n    Helper function to build globally managed state pertaining to a D-Tale instances columns & data types\n\n    :param data: dataframe to build data type information for\n    :type data: :class:`pandas:pandas.DataFrame`\n    :return: a list of dictionaries containing column names, indexes and data types\n    \"\"\"\n    prev_dtypes = {c[\"name\"]: c for c in prev_state or []}\n    dtypes = get_dtypes(data)\n    loaded_ranges = ranges or calc_data_ranges(data, dtypes)\n    dtype_f = dtype_formatter(data, dtypes, loaded_ranges, prev_dtypes)\n    return [dtype_f(i, c) for i, c in enumerate(data.columns)]\n\n\ndef check_duplicate_data(data):\n    \"\"\"\n    This function will do a rough check to see if a user has already loaded this piece of data to D-Tale to avoid\n    duplicated state.  The checks that take place are:\n     - shape (# of rows & # of columns\n     - column names and ordering of columns (eventually might add dtype checking as well...)\n\n    :param data: dataframe to validate\n    :type data: :class:`pandas:pandas.DataFrame`\n    :raises :class:`dtale.utils.DuplicateDataError`: if duplicate data exists\n    \"\"\"\n    cols = [str(col) for col in data.columns]\n\n    for d_id, datainst in global_state.items():\n        d_cols = [str(col) for col in datainst.data.columns]\n        if datainst.data.shape == data.shape and cols == d_cols:\n            raise DuplicateDataError(d_id)\n\n\ndef convert_xarray_to_dataset(dataset, **indexers):\n    def _convert_zero_dim_dataset(dataset):\n        ds_dict = dataset.to_dict()\n        data = {}\n        for coord, coord_data in ds_dict[\"coords\"].items():\n            data[coord] = coord_data[\"data\"]\n        for col, col_data in ds_dict[\"data_vars\"].items():\n            data[col] = col_data[\"data\"]\n        return pd.DataFrame([data]).set_index(list(ds_dict[\"coords\"].keys()))\n\n    ds_sel = dataset.sel(**indexers)\n    try:\n        df = ds_sel.to_dataframe()\n        df = df.reset_index().drop(\"index\", axis=1, errors=\"ignore\")\n        return df.set_index(list(dataset.dims.keys()))\n    except ValueError:\n        return _convert_zero_dim_dataset(ds_sel)\n\n\ndef handle_koalas(data):\n    \"\"\"\n    Helper function to check if koalas is installed and also if incoming data is a koalas dataframe, if so convert it\n    to :class:`pandas:pandas.DataFrame`, otherwise simply return the original data structure.\n\n    :param data: data we want to check if its a koalas dataframe and if so convert to :class:`pandas:pandas.DataFrame`\n    :return: :class:`pandas:pandas.DataFrame`\n    \"\"\"\n    if is_koalas(data):\n        return data.to_pandas()\n    return data\n\n\ndef is_koalas(data):\n    try:\n        from databricks.koalas import frame\n\n        return isinstance(data, frame.DataFrame)\n    except BaseException:\n        return False\n\n\ndef startup(\n    url=\"\",\n    data=None,\n    data_loader=None,\n    name=None,\n    data_id=None,\n    context_vars=None,\n    ignore_duplicate=True,\n    allow_cell_edits=True,\n    inplace=False,\n    drop_index=False,\n    precision=2,\n    show_columns=None,\n    hide_columns=None,\n    optimize_dataframe=False,\n    column_formats=None,\n    nan_display=None,\n    sort=None,\n    locked=None,\n    background_mode=None,\n    range_highlights=None,\n    app_root=None,\n    is_proxy=None,\n    vertical_headers=False,\n    hide_shutdown=None,\n    column_edit_options=None,\n    auto_hide_empty_columns=False,\n    highlight_filter=False,\n    hide_header_editor=None,\n    lock_header_menu=None,\n    hide_header_menu=None,\n    hide_main_menu=None,\n    hide_column_menus=None,\n    enable_custom_filters=None,\n    force_save=True,\n):\n    \"\"\"\n    Loads and stores data globally\n     - If data has indexes then it will lock save those columns as locked on the front-end\n     - If data has column named index it will be dropped so that it won't collide with row numbering (dtale_index)\n     - Create location in memory for storing settings which can be manipulated from the front-end (sorts, filter, ...)\n\n    :param url: the base URL that D-Tale is running from to be referenced in redirects to shutdown\n    :param data: :class:`pandas:pandas.DataFrame` or :class:`pandas:pandas.Series`\n    :param data_loader: function which returns :class:`pandas:pandas.DataFrame`\n    :param name: string label to apply to your session\n    :param data_id: integer id assigned to a piece of data viewable in D-Tale, if this is populated then it will\n                    override the data at that id\n    :param context_vars: a dictionary of the variables that will be available for use in user-defined expressions,\n                         such as filters\n    :type context_vars: dict, optional\n    :param ignore_duplicate: if set to True this will not test whether this data matches any previously loaded to D-Tale\n    :param allow_cell_edits: If false, this will not allow users to edit cells directly in their D-Tale grid\n    :type allow_cell_edits: bool, optional\n    :param inplace: If true, this will call `reset_index(inplace=True)` on the dataframe used as a way to save memory.\n                    Otherwise this will create a brand new dataframe, thus doubling memory but leaving the dataframe\n                    input unchanged.\n    :type inplace: bool, optional\n    :param drop_index: If true, this will drop any pre-existing index on the dataframe input.\n    :type drop_index: bool, optional\n    :param precision: The default precision to display for float data in D-Tale grid\n    :type precision: int, optional\n    :param show_columns: Columns to show on load, hide all others\n    :type show_columns: list, optional\n    :param hide_columns: Columns to hide on load\n    :type hide_columns: list, optional\n    :param optimize_dataframe: this will convert string columns with less certain then a certain number of distinct\n                              values into categories\n    :type optimize_dataframe: boolean\n    :param column_formats: The formatting to apply to certain columns on the front-end\n    :type column_formats: dict, optional\n    :param sort: The sort to apply to the data on startup (EX: [(\"col1\", \"ASC\"), (\"col2\", \"DESC\"),...])\n    :type sort: list[tuple], optional\n    :param locked: Columns to lock to the left of your grid on load\n    :type locked: list, optional\n    :param background_mode: Different background highlighting modes available on the frontend. Possible values are:\n                            - heatmap-all: turn on heatmap for all numeric columns where the colors are determined by\n                                           the range of values over all numeric columns combined\n                            - heatmap-col: turn on heatmap for all numeric columns where the colors are determined by\n                                           the range of values in the column\n                            - heatmap-col-[column name]: turn on heatmap highlighting for a specific column\n                            - dtypes: highlight columns based on it's data type\n                            - missing: highlight any missing values (np.nan, empty strings, strings of all spaces)\n                            - outliers: highlight any outliers\n                            - range: highlight values for any matchers entered in the \"range_highlights\" option\n                            - lowVariance: highlight values with a low variance\n    :type background_mode: string, optional\n    :param range_highlights: Definitions for equals, less-than or greater-than ranges for individual (or all) columns\n                             which apply different background colors to cells which fall in those ranges.\n    :type range_highlights: dict, optional\n    :param vertical_headers: if True, then rotate column headers vertically\n    :type vertical_headers: boolean, optional\n    :param column_edit_options: The options to allow on the front-end when editing a cell for the columns specified\n    :type column_edit_options: dict, optional\n    :param auto_hide_empty_columns: if True, then auto-hide any columns on the front-end that are comprised entirely of\n                                    NaN values\n    :type auto_hide_empty_columns: boolean, optional\n    :param highlight_filter: if True, then highlight rows on the frontend which will be filtered when applying a filter\n                             rather than hiding them from the dataframe\n    :type highlight_filter: boolean, optional\n    \"\"\"\n\n    if (\n        data_loader is None and data is None\n    ):  # scenario where we'll force users to upload a CSV/TSV\n        return DtaleData(\"1\", url, is_proxy=is_proxy, app_root=app_root)\n\n    if data_loader is not None:\n        data = data_loader()\n        if isinstance(data, string_types) and global_state.contains(data):\n            return DtaleData(data, url, is_proxy=is_proxy, app_root=app_root)\n        elif (\n            data is None and global_state.is_arcticdb\n        ):  # send user to the library/symbol selection screen\n            return DtaleData(None, url, is_proxy=is_proxy, app_root=app_root)\n\n    if global_state.is_arcticdb and isinstance(data, string_types):\n        data_id = data\n        data_id_segs = data_id.split(\"|\")\n        if len(data_id_segs) < 2:\n            if not global_state.store.lib:\n                raise ValueError(\n                    (\n                        \"When specifying a data identifier for ArcticDB it must be comprised of a library and a symbol.\"\n                        \"Use the following format: [library]|[symbol]\"\n                    )\n                )\n            data_id = \"{}|{}\".format(global_state.store.lib.name, data_id)\n        global_state.new_data_inst(data_id)\n        instance = global_state.store.get(data_id)\n        data = instance.base_df\n        ret_data = startup(\n            url=url,\n            data=data,\n            data_id=data_id,\n            force_save=False,\n            name=name,\n            context_vars=context_vars,\n            ignore_duplicate=ignore_duplicate,\n            allow_cell_edits=allow_cell_edits,\n            precision=precision,\n            show_columns=show_columns,\n            hide_columns=hide_columns,\n            column_formats=column_formats,\n            nan_display=nan_display,\n            sort=sort,\n            locked=locked,\n            background_mode=background_mode,\n            range_highlights=range_highlights,\n            app_root=app_root,\n            is_proxy=is_proxy,\n            vertical_headers=vertical_headers,\n            hide_shutdown=hide_shutdown,\n            column_edit_options=column_edit_options,\n            auto_hide_empty_columns=auto_hide_empty_columns,\n            highlight_filter=highlight_filter,\n            hide_header_editor=hide_header_editor,\n            lock_header_menu=lock_header_menu,\n            hide_header_menu=hide_header_menu,\n            hide_main_menu=hide_main_menu,\n            hide_column_menus=hide_column_menus,\n            enable_custom_filters=enable_custom_filters,\n        )\n        startup_code = (\n            \"from arcticdb import Arctic\\n\"\n            \"from arcticdb.version_store._store import VersionedItem\\n\\n\"\n            \"conn = Arctic('{uri}')\\n\"\n            \"lib = conn.get_library('{library}')\\n\"\n            \"df = lib.read('{symbol}')\\n\"\n            \"if isinstance(data, VersionedItem):\\n\"\n            \"\\tdf = df.data\\n\"\n        ).format(\n            uri=global_state.store.uri,\n            library=global_state.store.lib.name,\n            symbol=data_id,\n        )\n        curr_settings = global_state.get_settings(data_id)\n        global_state.set_settings(\n            data_id, dict_merge(curr_settings, dict(startup_code=startup_code))\n        )\n        return ret_data\n\n    if data is not None:\n        data = handle_koalas(data)\n        valid_types = (\n            pd.DataFrame,\n            pd.Series,\n            pd.DatetimeIndex,\n            pd.MultiIndex,\n            xr.Dataset,\n            np.ndarray,\n            list,\n            dict,\n        )\n        if not isinstance(data, valid_types):\n            raise Exception(\n                (\n                    \"data loaded must be one of the following types: pandas.DataFrame, pandas.Series, \"\n                    \"pandas.DatetimeIndex, pandas.MultiIndex, xarray.Dataset, numpy.array, numpy.ndarray, list, dict\"\n                )\n            )\n\n        if isinstance(data, xr.Dataset):\n            df = convert_xarray_to_dataset(data)\n            instance = startup(\n                url,\n                df,\n                name=name,\n                data_id=data_id,\n                context_vars=context_vars,\n                ignore_duplicate=ignore_duplicate,\n                allow_cell_edits=allow_cell_edits,\n                precision=precision,\n                show_columns=show_columns,\n                hide_columns=hide_columns,\n                column_formats=column_formats,\n                nan_display=nan_display,\n                sort=sort,\n                locked=locked,\n                background_mode=background_mode,\n                range_highlights=range_highlights,\n                app_root=app_root,\n                is_proxy=is_proxy,\n                vertical_headers=vertical_headers,\n                hide_shutdown=hide_shutdown,\n                column_edit_options=column_edit_options,\n                auto_hide_empty_columns=auto_hide_empty_columns,\n                highlight_filter=highlight_filter,\n                hide_header_editor=hide_header_editor,\n                lock_header_menu=lock_header_menu,\n                hide_header_menu=hide_header_menu,\n                hide_main_menu=hide_main_menu,\n                hide_column_menus=hide_column_menus,\n                enable_custom_filters=enable_custom_filters,\n            )\n\n            global_state.set_dataset(instance._data_id, data)\n            global_state.set_dataset_dim(instance._data_id, {})\n            return instance\n\n        data, curr_index = format_data(data, inplace=inplace, drop_index=drop_index)\n        # check to see if this dataframe has already been loaded to D-Tale\n        if data_id is None and not ignore_duplicate and not global_state.is_arcticdb:\n            check_duplicate_data(data)\n\n        logger.debug(\n            \"pytest: {}, flask-debug: {}\".format(\n                running_with_pytest(), running_with_flask_debug()\n            )\n        )\n\n        if data_id is None:\n            data_id = global_state.new_data_inst()\n        if global_state.get_settings(data_id) is not None:\n            curr_settings = global_state.get_settings(data_id)\n            curr_locked = curr_settings.get(\"locked\", [])\n            # filter out previous locked columns that don't exist\n            curr_locked = [c for c in curr_locked if c in data.columns]\n            # add any new columns in index\n            curr_locked += [c for c in curr_index if c not in curr_locked]\n        else:\n            logger.debug(\n                \"pre-locking index columns ({}) to settings[{}]\".format(\n                    curr_index, data_id\n                )\n            )\n            curr_locked = locked or curr_index\n            global_state.set_metadata(data_id, dict(start=pd.Timestamp(\"now\")))\n        global_state.set_name(data_id, name)\n        # in the case that data has been updated we will drop any sorts or filter for ease of use\n        base_settings = dict(\n            indexes=curr_index,\n            locked=curr_locked,\n            allow_cell_edits=True if allow_cell_edits is None else allow_cell_edits,\n            precision=precision,\n            columnFormats=column_formats or {},\n            backgroundMode=background_mode,\n            rangeHighlight=range_highlights,\n            verticalHeaders=vertical_headers,\n            highlightFilter=highlight_filter,\n        )\n        base_predefined = predefined_filters.init_filters()\n        if base_predefined:\n            base_settings[\"predefinedFilters\"] = base_predefined\n        if sort:\n            base_settings[\"sortInfo\"] = sort\n            data = sort_df_for_grid(data, dict(sort=sort))\n        if nan_display is not None:\n            base_settings[\"nanDisplay\"] = nan_display\n        if hide_shutdown is not None:\n            base_settings[\"hide_shutdown\"] = hide_shutdown\n        if hide_header_editor is not None:\n            base_settings[\"hide_header_editor\"] = hide_header_editor\n        if lock_header_menu is not None:\n            base_settings[\"lock_header_menu\"] = lock_header_menu\n        if hide_header_menu is not None:\n            base_settings[\"hide_header_menu\"] = hide_header_menu\n        if hide_main_menu is not None:\n            base_settings[\"hide_main_menu\"] = hide_main_menu\n        if hide_column_menus is not None:\n            base_settings[\"hide_column_menus\"] = hide_column_menus\n        if enable_custom_filters is not None:\n            base_settings[\"enable_custom_filters\"] = enable_custom_filters\n        if column_edit_options is not None:\n            base_settings[\"column_edit_options\"] = column_edit_options\n        global_state.set_settings(data_id, base_settings)\n        if optimize_dataframe and not global_state.is_arcticdb:\n            data = optimize_df(data)\n        if force_save or (\n            global_state.is_arcticdb and not global_state.contains(data_id)\n        ):\n            data = data[curr_locked + [c for c in data.columns if c not in curr_locked]]\n            global_state.set_data(data_id, data)\n        dtypes_data = data\n        ranges = None\n        if global_state.is_arcticdb:\n            instance = global_state.store.get(data_id)\n            if not instance.is_large:\n                dtypes_data = instance.load_data()\n                dtypes_data, _ = format_data(\n                    dtypes_data, inplace=inplace, drop_index=drop_index\n                )\n                ranges = calc_data_ranges(dtypes_data)\n                dtypes_data = dtypes_data[\n                    curr_locked\n                    + [c for c in dtypes_data.columns if c not in curr_locked]\n                ]\n        dtypes_state = build_dtypes_state(\n            dtypes_data, global_state.get_dtypes(data_id) or [], ranges=ranges\n        )\n\n        for col in dtypes_state:\n            if show_columns and col[\"name\"] not in show_columns:\n                col[\"visible\"] = False\n                continue\n            if hide_columns and col[\"name\"] in hide_columns:\n                col[\"visible\"] = False\n                continue\n            if col[\"index\"] >= 100:\n                col[\"visible\"] = False\n        if auto_hide_empty_columns and not global_state.is_arcticdb:\n            is_empty = data.isnull().all()\n            is_empty = list(is_empty[is_empty].index.values)\n            for col in dtypes_state:\n                if col[\"name\"] in is_empty:\n                    col[\"visible\"] = False\n        global_state.set_dtypes(data_id, dtypes_state)\n        global_state.set_context_variables(\n            data_id, build_context_variables(data_id, context_vars)\n        )\n        if global_state.load_flag(data_id, \"enable_custom_filters\", False):\n            logger.warning(\n                (\n                    \"Custom filtering enabled. Custom filters are vulnerable to code injection attacks, please only \"\n                    \"use in trusted environments.\"\n                )\n            )\n        return DtaleData(data_id, url, is_proxy=is_proxy, app_root=app_root)\n    else:\n        raise NoDataLoadedException(\"No data has been loaded into this D-Tale session!\")\n\n\ndef is_vscode():\n    if os.environ.get(\"VSCODE_PID\") is not None:\n        return True\n    if \"1\" == os.environ.get(\"VSCODE_INJECTION\"):\n        return True\n    return False\n\n\ndef base_render_template(template, data_id, **kwargs):\n    \"\"\"\n    Overriden version of Flask.render_template which will also include vital instance information\n     - settings\n     - version\n     - processes\n    \"\"\"\n    if not len(os.listdir(\"{}/static/dist\".format(os.path.dirname(__file__)))):\n        return redirect(current_app.url_for(\"missing_js\"))\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_app_settings = global_state.get_app_settings()\n    _, version = retrieve_meta_info_and_version(\"dtale\")\n    hide_shutdown = global_state.load_flag(data_id, \"hide_shutdown\", False)\n    allow_cell_edits = global_state.load_flag(data_id, \"allow_cell_edits\", True)\n    github_fork = global_state.load_flag(data_id, \"github_fork\", False)\n    hide_header_editor = global_state.load_flag(data_id, \"hide_header_editor\", False)\n    lock_header_menu = global_state.load_flag(data_id, \"lock_header_menu\", False)\n    hide_header_menu = global_state.load_flag(data_id, \"hide_header_menu\", False)\n    hide_main_menu = global_state.load_flag(data_id, \"hide_main_menu\", False)\n    hide_column_menus = global_state.load_flag(data_id, \"hide_column_menus\", False)\n    enable_custom_filters = global_state.load_flag(\n        data_id, \"enable_custom_filters\", False\n    )\n    app_overrides = dict(\n        allow_cell_edits=json.dumps(allow_cell_edits),\n        hide_shutdown=hide_shutdown,\n        hide_header_editor=hide_header_editor,\n        lock_header_menu=lock_header_menu,\n        hide_header_menu=hide_header_menu,\n        hide_main_menu=hide_main_menu,\n        hide_column_menus=hide_column_menus,\n        enable_custom_filters=enable_custom_filters,\n        github_fork=github_fork,\n    )\n    is_arcticdb = 0\n    arctic_conn = \"\"\n    if global_state.is_arcticdb:\n        instance = global_state.store.get(data_id)\n        is_arcticdb = instance.rows()\n        arctic_conn = global_state.store.uri\n    return render_template(\n        template,\n        data_id=get_url_quote()(get_url_quote()(data_id, safe=\"\"))\n        if data_id is not None\n        else \"\",\n        xarray=global_state.get_data_inst(data_id).is_xarray_dataset,\n        xarray_dim=json.dumps(global_state.get_dataset_dim(data_id)),\n        settings=json.dumps(curr_settings),\n        version=str(version),\n        processes=global_state.size(),\n        python_version=platform.python_version(),\n        predefined_filters=json.dumps(\n            [f.asdict() for f in predefined_filters.get_filters()]\n        ),\n        is_vscode=is_vscode(),\n        is_arcticdb=is_arcticdb,\n        arctic_conn=arctic_conn,\n        column_count=len(global_state.get_dtypes(data_id) or []),\n        # fmt: off\n        **dict_merge(kwargs, curr_app_settings, app_overrides)\n        # fmt: on\n    )\n\n\ndef _view_main(data_id, iframe=False):\n    \"\"\"\n    Helper function rendering main HTML which will also build title and store whether we are viewing from an <iframe>\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param iframe: boolean flag indicating whether this is being viewed from an <iframe> (usually means ipython)\n    :type iframe: bool, optional\n    :return: HTML\n    \"\"\"\n    title = \"D-Tale\"\n    name = global_state.get_name(data_id)\n    if name:\n        title = \"{} ({})\".format(title, name)\n    return base_render_template(\"dtale/main.html\", data_id, title=title, iframe=iframe)\n\n\n@dtale.route(\"/main\")\n@dtale.route(\"/main/<data_id>\")\ndef view_main(data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if not global_state.contains(data_id):\n        if global_state.is_arcticdb:\n            try:\n                startup(data=data_id)\n                return _view_main(data_id)\n            except BaseException as ex:\n                logger.exception(ex)\n        return redirect(\"/dtale/{}\".format(head_endpoint()))\n    return _view_main(data_id)\n\n\n@dtale.route(\"/main/name/<data_name>\")\ndef view_main_by_name(data_name=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_name: integer string identifier for a D-Tale process's data\n    :type data_name: str\n    :return: HTML\n    \"\"\"\n    data_id = global_state.get_data_id_by_name(data_name)\n    return view_main(data_id)\n\n\n@dtale.route(\"/iframe\")\n@dtale.route(\"/iframe/<data_id>\")\ndef view_iframe(data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if data_id is None:\n        return redirect(\"/dtale/iframe/{}\".format(head_endpoint()))\n    return _view_main(data_id, iframe=True)\n\n\n@dtale.route(\"/iframe/popup/<popup_type>\")\n@dtale.route(\"/iframe/popup/<popup_type>/<data_id>\")\ndef iframe_popup(popup_type, data_id=None):\n    route = \"/dtale/popup/{}\".format(popup_type)\n    if data_id:\n        return redirect(\"{}/{}\".format(route, data_id))\n    return redirect(route)\n\n\nPOPUP_TITLES = {\n    \"reshape\": \"Summarize Data\",\n    \"filter\": \"Custom Filter\",\n    \"upload\": \"Load Data\",\n    \"pps\": \"Predictive Power Score\",\n    \"merge\": \"Merge & Stack\",\n    \"arcticdb\": \"Load ArcticDB Data\",\n}\n\n\n@dtale.route(\"/popup/<popup_type>\")\n@dtale.route(\"/popup/<popup_type>/<data_id>\")\ndef view_popup(popup_type, data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up a base jinja template for any popup, additionally forwards any\n    request parameters as input to template.\n\n    :param popup_type: type of popup to be opened. Possible values: charts, correlations, describe, histogram, instances\n    :type popup_type: str\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if data_id is None and popup_type not in [\"upload\", \"merge\", \"arcticdb\"]:\n        return redirect(\"/dtale/{}\".format(head_endpoint(popup_type)))\n    main_title = global_state.get_app_settings().get(\"main_title\")\n    title = main_title or \"D-Tale\"\n    name = global_state.get_name(data_id)\n    if name:\n        title = \"{} ({})\".format(title, name)\n    popup_title = text(\n        POPUP_TITLES.get(popup_type)\n        or \" \".join([pt.capitalize() for pt in popup_type.split(\"-\")])\n    )\n    title = \"{} - {}\".format(title, popup_title)\n    params = request.args.to_dict()\n    if len(params):\n\n        def pretty_print(obj):\n            return \", \".join([\"{}: {}\".format(k, str(v)) for k, v in obj.items()])\n\n        title = \"{} ({})\".format(title, pretty_print(params))\n\n    grid_links = []\n    for id in global_state.keys():\n        label = global_state.get_name(id)\n        if label:\n            label = \" ({})\".format(label)\n        else:\n            label = \"\"\n        grid_links.append((id, \"{}{}\".format(id, label)))\n    return base_render_template(\n        \"dtale/popup.html\",\n        data_id,\n        title=title,\n        popup_title=popup_title,\n        js_prefix=popup_type,\n        grid_links=grid_links,\n        back_to_data=text(\"Back To Data\"),\n    )\n\n\n@dtale.route(\"/calculation/<calc_type>\")\ndef view_calculation(calc_type=\"skew\"):\n    return render_template(\"dtale/{}.html\".format(calc_type))\n\n\n@dtale.route(\"/network\")\n@dtale.route(\"/network/<data_id>\")\ndef view_network(data_id=None):\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up base jinja template housing JS files\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: HTML\n    \"\"\"\n    if not global_state.contains(data_id):\n        return redirect(\"/dtale/network/{}\".format(head_endpoint()))\n    return base_render_template(\n        \"dtale/network.html\", data_id, title=\"Network Viewer\", iframe=False\n    )\n\n\n@dtale.route(\"/code-popup\")\ndef view_code_popup():\n    \"\"\"\n    :class:`flask:flask.Flask` route which serves up a base jinja template for code snippets\n\n    :return: HTML\n    \"\"\"\n    return render_template(\"dtale/code_popup.html\", **global_state.get_app_settings())\n\n\n@dtale.route(\"/processes\")\n@exception_decorator\ndef get_processes():\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns list of running D-Tale processes within current python process\n\n    :return: JSON {\n        data: [\n            {\n                port: 1, name: 'name1', rows: 5, columns: 5, names: 'col1,...,col5', start: '2018-04-30 12:36:44',\n                ts: 1525106204000\n            },\n            ...,\n            {\n                port: N, name: 'nameN', rows: 5, columns: 5, names: 'col1,...,col5', start: '2018-04-30 12:36:44',\n                ts: 1525106204000\n            }\n        ],\n        success: True/False\n    }\n    \"\"\"\n\n    load_dtypes = get_bool_arg(request, \"dtypes\")\n\n    def _load_process(data_id):\n        data = global_state.get_data(data_id)\n        dtypes = global_state.get_dtypes(data_id)\n        mdata = global_state.get_metadata(data_id)\n        return dict(\n            data_id=str(data_id),\n            rows=len(data),\n            columns=len(dtypes),\n            names=dtypes if load_dtypes else \",\".join([c[\"name\"] for c in dtypes]),\n            start=json_date(mdata[\"start\"], fmt=\"%-I:%M:%S %p\"),\n            ts=json_timestamp(mdata[\"start\"]),\n            name=global_state.get_name(data_id),\n            # mem usage in MB\n            mem_usage=int(\n                global_state.get_data(data_id)\n                .memory_usage(index=False, deep=True)\n                .sum()\n            ),\n        )\n\n    processes = sorted(\n        [_load_process(data_id) for data_id in global_state.keys()],\n        key=lambda p: p[\"ts\"],\n    )\n    return jsonify(dict(data=processes, success=True))\n\n\n@dtale.route(\"/process-keys\")\n@exception_decorator\ndef process_keys():\n    return jsonify(\n        dict(\n            data=[\n                dict(id=str(data_id), name=global_state.get_name(data_id))\n                for data_id in global_state.keys()\n            ],\n            success=True,\n        )\n    )\n\n\n@dtale.route(\"/update-settings/<data_id>\")\n@exception_decorator\ndef update_settings(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which updates global SETTINGS for current port\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param settings: JSON string from flask.request.args['settings'] which gets decoded and stored in SETTINGS variable\n    :return: JSON\n    \"\"\"\n\n    global_state.update_settings(data_id, get_json_arg(request, \"settings\", {}))\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-theme\")\n@exception_decorator\ndef update_theme():\n    theme = get_str_arg(request, \"theme\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"theme\"] = theme\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-pin-menu\")\n@exception_decorator\ndef update_pin_menu():\n    pinned = get_bool_arg(request, \"pinned\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"pin_menu\"] = pinned\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-language\")\n@exception_decorator\ndef update_language():\n    language = get_str_arg(request, \"language\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"language\"] = language\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-maximum-column-width\")\n@exception_decorator\ndef update_maximum_column_width():\n    width = get_str_arg(request, \"width\")\n    if width:\n        width = int(width)\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"max_column_width\"] = width\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-maximum-row-height\")\n@exception_decorator\ndef update_maximum_row_height():\n    height = get_str_arg(request, \"height\")\n    if height:\n        height = int(height)\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"max_row_height\"] = height\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-query-engine\")\n@exception_decorator\ndef update_query_engine():\n    engine = get_str_arg(request, \"engine\")\n    curr_app_settings = global_state.get_app_settings()\n    curr_app_settings[\"query_engine\"] = engine\n    global_state.set_app_settings(curr_app_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/update-formats/<data_id>\")\n@exception_decorator\ndef update_formats(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which updates the \"columnFormats\" property for global SETTINGS associated w/ the\n    current port\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param all: boolean flag which, if true, tells us we should apply this formatting to all columns with the same\n                data type as our selected column\n    :param col: selected column\n    :param format: JSON string for the formatting configuration we want applied to either the selected column of all\n                   columns with the selected column's data type\n    :return: JSON\n    \"\"\"\n    update_all_dtype = get_bool_arg(request, \"all\")\n    nan_display = get_str_arg(request, \"nanDisplay\")\n    if nan_display is None:\n        nan_display = \"nan\"\n    col = get_str_arg(request, \"col\")\n    col_format = get_json_arg(request, \"format\")\n    curr_settings = global_state.get_settings(data_id) or {}\n    updated_formats = {col: col_format}\n    if update_all_dtype:\n        col_dtype = global_state.get_dtype_info(data_id, col)[\"dtype\"]\n        updated_formats = {\n            c[\"name\"]: col_format\n            for c in global_state.get_dtypes(data_id)\n            if c[\"dtype\"] == col_dtype\n        }\n    updated_formats = dict_merge(\n        curr_settings.get(\"columnFormats\") or {}, updated_formats\n    )\n    updated_settings = dict_merge(\n        curr_settings, dict(columnFormats=updated_formats, nanDisplay=nan_display)\n    )\n    global_state.set_settings(data_id, updated_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/save-range-highlights/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef save_range_highlights(data_id):\n    ranges = json.loads(request.json.get(\"ranges\", \"{}\"))\n    curr_settings = global_state.get_settings(data_id) or {}\n    updated_settings = dict_merge(curr_settings, dict(rangeHighlight=ranges))\n    global_state.set_settings(data_id, updated_settings)\n    return jsonify(dict(success=True))\n\n\ndef refresh_col_indexes(data_id):\n    \"\"\"\n    Helper function to sync column indexes to current state of dataframe for data_id.\n    \"\"\"\n    curr_dtypes = {c[\"name\"]: c for c in global_state.get_dtypes(data_id)}\n    curr_data = global_state.get_data(data_id)\n    global_state.set_dtypes(\n        data_id,\n        [\n            dict_merge(curr_dtypes[c], dict(index=idx))\n            for idx, c in enumerate(curr_data.columns)\n        ],\n    )\n\n\n@dtale.route(\"/update-column-position/<data_id>\")\n@exception_decorator\ndef update_column_position(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle moving of columns within a :class:`pandas:pandas.DataFrame`. Columns can\n    be moved in one of these 4 directions: front, back, left, right\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param action: string from flask.request.args['action'] of direction to move column\n    :param col: string from flask.request.args['col'] of column name to move\n    :return: JSON {success: True/False}\n    \"\"\"\n    action = get_str_arg(request, \"action\")\n    col = get_str_arg(request, \"col\")\n\n    curr_cols = global_state.get_data(data_id).columns.tolist()\n    if action == \"front\":\n        curr_cols = [col] + [c for c in curr_cols if c != col]\n    elif action == \"back\":\n        curr_cols = [c for c in curr_cols if c != col] + [col]\n    elif action == \"left\":\n        if curr_cols[0] != col:\n            col_idx = next((idx for idx, c in enumerate(curr_cols) if c == col), None)\n            col_to_shift = curr_cols[col_idx - 1]\n            curr_cols[col_idx - 1] = col\n            curr_cols[col_idx] = col_to_shift\n    elif action == \"right\":\n        if curr_cols[-1] != col:\n            col_idx = next((idx for idx, c in enumerate(curr_cols) if c == col), None)\n            col_to_shift = curr_cols[col_idx + 1]\n            curr_cols[col_idx + 1] = col\n            curr_cols[col_idx] = col_to_shift\n\n    global_state.set_data(data_id, global_state.get_data(data_id)[curr_cols])\n    refresh_col_indexes(data_id)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/update-locked/<data_id>\")\n@exception_decorator\ndef update_locked(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle saving state associated with locking and unlocking columns\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param action: string from flask.request.args['action'] of action to perform (lock or unlock)\n    :param col: string from flask.request.args['col'] of column name to lock/unlock\n    :return: JSON {success: True/False}\n    \"\"\"\n    action = get_str_arg(request, \"action\")\n    col = get_str_arg(request, \"col\")\n    curr_settings = global_state.get_settings(data_id)\n    curr_data = global_state.get_data(data_id)\n    curr_settings[\"locked\"] = curr_settings.get(\"locked\") or []\n    if action == \"lock\" and col not in curr_settings[\"locked\"]:\n        curr_settings[\"locked\"] = curr_settings[\"locked\"] + [col]\n    elif action == \"unlock\":\n        curr_settings[\"locked\"] = [c for c in curr_settings[\"locked\"] if c != col]\n\n    final_cols = curr_settings[\"locked\"] + [\n        c for c in curr_data.columns if c not in curr_settings[\"locked\"]\n    ]\n    global_state.set_data(data_id, curr_data[final_cols])\n    global_state.set_settings(data_id, curr_settings)\n    refresh_col_indexes(data_id)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/update-visibility/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef update_visibility(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle saving state associated visiblity of columns on the front-end\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param visibility: string from flask.request.args['action'] of dictionary of visibility of all columns in a\n                       dataframe\n    :type visibility: dict, optional\n    :param toggle: string from flask.request.args['col'] of column name whose visibility should be toggled\n    :type toggle: str, optional\n    :return: JSON {success: True/False}\n    \"\"\"\n    curr_dtypes = global_state.get_dtypes(data_id)\n    if request.json.get(\"visibility\"):\n        visibility = json.loads(request.json.get(\"visibility\", \"{}\"))\n        global_state.set_dtypes(\n            data_id,\n            [dict_merge(d, dict(visible=visibility[d[\"name\"]])) for d in curr_dtypes],\n        )\n    elif request.json.get(\"toggle\"):\n        toggle_col = request.json.get(\"toggle\")\n        toggle_idx = next(\n            (idx for idx, d in enumerate(curr_dtypes) if d[\"name\"] == toggle_col), None\n        )\n        toggle_cfg = curr_dtypes[toggle_idx]\n        curr_dtypes[toggle_idx] = dict_merge(\n            toggle_cfg, dict(visible=not toggle_cfg[\"visible\"])\n        )\n        global_state.set_dtypes(data_id, curr_dtypes)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/build-column/<data_id>\")\n@exception_decorator\ndef build_column(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle the building of new columns in a dataframe. Some of the operations the\n    are available are:\n     - numeric: sum/difference/multiply/divide any combination of two columns or static values\n     - datetime: retrieving date properties (hour, minute, month, year...) or conversions of dates (month start, month\n                 end, quarter start...)\n     - bins: bucketing numeric data into bins using :meth:`pandas:pandas.cut` & :meth:`pandas:pandas.qcut`\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param name: string from flask.request.args['name'] of new column to create\n    :param type: string from flask.request.args['type'] of the type of column to build (numeric/datetime/bins)\n    :param cfg: dict from flask.request.args['cfg'] of how to calculate the new column\n    :return: JSON {success: True/False}\n    \"\"\"\n    data = global_state.get_data(data_id)\n    col_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    save_as = get_str_arg(request, \"saveAs\", \"new\")\n    if save_as == \"inplace\":\n        name = cfg[\"col\"]\n    else:\n        name = get_str_arg(request, \"name\")\n        if not name and col_type != \"type_conversion\":\n            raise Exception(\"'name' is required for new column!\")\n        # non-type conversions cannot be done inplace and thus need a name and the name needs to be checked that it\n        # won't overwrite something else\n        name = str(name)\n        data = global_state.get_data(data_id)\n        if name in data.columns:\n            raise Exception(\"A column named '{}' already exists!\".format(name))\n\n    def _build_column():\n        builder = ColumnBuilder(data_id, col_type, name, cfg)\n        new_col_data = builder.build_column()\n        new_cols = []\n        if isinstance(new_col_data, pd.Series):\n            if pandas_util.is_pandas2():\n                data[name] = new_col_data\n            else:\n                data.loc[:, name] = new_col_data\n            new_cols.append(name)\n        else:\n            for i in range(len(new_col_data.columns)):\n                new_col = new_col_data.iloc[:, i]\n                if pandas_util.is_pandas2():\n                    data[str(new_col.name)] = new_col\n                else:\n                    data.loc[:, str(new_col.name)] = new_col\n\n        new_types = {}\n        data_ranges = {}\n        for new_col in new_cols:\n            dtype = find_dtype(data[new_col])\n            if classify_type(dtype) == \"F\" and not data[new_col].isnull().all():\n                new_ranges = calc_data_ranges(data[[new_col]])\n                data_ranges[new_col] = new_ranges.get(new_col, data_ranges.get(new_col))\n            new_types[new_col] = dtype\n        dtype_f = dtype_formatter(data, new_types, data_ranges)\n        global_state.set_data(data_id, data)\n        curr_dtypes = global_state.get_dtypes(data_id)\n        if next((cdt for cdt in curr_dtypes if cdt[\"name\"] in new_cols), None):\n            curr_dtypes = [\n                dtype_f(len(curr_dtypes), cdt[\"name\"])\n                if cdt[\"name\"] in new_cols\n                else cdt\n                for cdt in curr_dtypes\n            ]\n        else:\n            curr_dtypes += [dtype_f(len(curr_dtypes), new_col) for new_col in new_cols]\n        global_state.set_dtypes(data_id, curr_dtypes)\n        curr_history = global_state.get_history(data_id) or []\n        curr_history += make_list(builder.build_code())\n        global_state.set_history(data_id, curr_history)\n\n    if cfg.get(\"applyAllType\", False):\n        cols = [\n            dtype[\"name\"]\n            for dtype in global_state.get_dtypes(data_id)\n            if dtype[\"dtype\"] == cfg[\"from\"]\n        ]\n        for col in cols:\n            cfg = dict_merge(cfg, dict(col=col))\n            name = col\n            _build_column()\n    else:\n        _build_column()\n    return jsonify(success=True)\n\n\n@dtale.route(\"/bins-tester/<data_id>\")\n@exception_decorator\ndef build_column_bins_tester(data_id):\n    col_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    builder = ColumnBuilder(data_id, col_type, cfg[\"col\"], cfg)\n    data = global_state.get_data(data_id)\n    data, labels = builder.builder.build_test(data)\n    return jsonify(dict(data=data, labels=labels, timestamp=round(time.time() * 1000)))\n\n\n@dtale.route(\"/duplicates/<data_id>\")\n@exception_decorator\ndef get_duplicates(data_id):\n    dupe_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    action = get_str_arg(request, \"action\")\n    duplicate_check = DuplicateCheck(data_id, dupe_type, cfg)\n    if action == \"test\":\n        return jsonify(results=duplicate_check.test())\n    updated_data_id = duplicate_check.execute()\n    return jsonify(success=True, data_id=updated_data_id)\n\n\n@dtale.route(\"/reshape/<data_id>\")\n@exception_decorator\ndef reshape_data(data_id):\n    output = get_str_arg(request, \"output\")\n    shape_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    builder = DataReshaper(data_id, shape_type, cfg)\n    if output == \"new\":\n        instance = startup(data=builder.reshape(), ignore_duplicate=True)\n    else:\n        instance = startup(\n            data=builder.reshape(), data_id=data_id, ignore_duplicate=True\n        )\n    curr_settings = global_state.get_settings(instance._data_id)\n    global_state.set_settings(\n        instance._data_id,\n        dict_merge(curr_settings, dict(startup_code=builder.build_code())),\n    )\n    return jsonify(success=True, data_id=instance._data_id)\n\n\n@dtale.route(\"/build-replacement/<data_id>\")\n@exception_decorator\ndef build_replacement(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route to handle the replacement of specific values within a column in a dataframe. Some\n    of the operations the are available are:\n     - spaces: replace values consisting of only spaces with a specific value\n     - value: replace specific values with a specific value or aggregation\n     - strings: replace values which contain a specific character or string (case-insensitive or not) with a\n                       specific value\n     - imputer: replace nan values using sklearn imputers iterative, knn or simple\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param col: string from flask.request.args['col'] of the column to perform replacements upon\n    :param type: string from flask.request.args['type'] of the type of replacement to perform\n                 (spaces/fillna/strings/imputer)\n    :param cfg: dict from flask.request.args['cfg'] of how to calculate the replacements\n    :return: JSON {success: True/False}\n    \"\"\"\n\n    def _build_data_ranges(data, col, dtype):\n        data_ranges = {}\n        if classify_type(dtype) == \"F\" and not data[col].isnull().all():\n            col_ranges = calc_data_ranges(data[[col]])\n            if col_ranges:\n                data_ranges[col] = col_ranges[col]\n        return data_ranges\n\n    data = global_state.get_data(data_id)\n    name = get_str_arg(request, \"name\")\n    if name is not None:\n        name = str(name)\n        if name in data.columns:\n            raise Exception(\"A column named '{}' already exists!\".format(name))\n    col = get_str_arg(request, \"col\")\n    replacement_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n\n    builder = ColumnReplacement(data_id, col, replacement_type, cfg, name)\n    output = builder.build_replacements()\n    dtype = find_dtype(output)\n    curr_dtypes = global_state.get_dtypes(data_id)\n\n    if name is not None:\n        data.loc[:, name] = output\n        dtype_f = dtype_formatter(\n            data, {name: dtype}, _build_data_ranges(data, name, dtype)\n        )\n        curr_dtypes.append(dtype_f(len(curr_dtypes), name))\n    else:\n        data.loc[:, col] = output\n        dtype_f = dtype_formatter(\n            data, {col: dtype}, _build_data_ranges(data, col, dtype)\n        )\n        col_index = next(\n            (i for i, d in enumerate(curr_dtypes) if d[\"name\"] == col), None\n        )\n        curr_col_dtype = dtype_f(col_index, col)\n        curr_dtypes = [curr_col_dtype if d[\"name\"] == col else d for d in curr_dtypes]\n\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, curr_dtypes)\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [builder.build_code()]\n    global_state.set_history(data_id, curr_history)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/test-filter/<data_id>\")\n@exception_decorator\ndef test_filter(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which will test out pandas query before it gets applied to DATA and return\n    exception information to the screen if there is any\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :return: JSON {success: True/False}\n    \"\"\"\n    query = get_str_arg(request, \"query\")\n    if query and not global_state.load_flag(data_id, \"enable_custom_filters\", False):\n        return jsonify(\n            dict(\n                success=False,\n                error=(\n                    \"Custom Filters not enabled! Custom filters are vulnerable to code injection attacks, please only \"\n                    \"use in trusted environments.\"\n                ),\n            )\n        )\n    run_query(\n        handle_predefined(data_id),\n        build_query(data_id, query),\n        global_state.get_context_variables(data_id),\n    )\n    if get_str_arg(request, \"save\"):\n        curr_settings = global_state.get_settings(data_id) or {}\n        if query is not None:\n            curr_settings = dict_merge(curr_settings, dict(query=query))\n        else:\n            curr_settings = {k: v for k, v in curr_settings.items() if k != \"query\"}\n        global_state.set_settings(data_id, curr_settings)\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/dtypes/<data_id>\")\n@exception_decorator\ndef dtypes(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns a list of column names and dtypes to the front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n\n    :return: JSON {\n        dtypes: [\n            {index: 1, name: col1, dtype: int64},\n            ...,\n            {index: N, name: colN, dtype: float64}\n        ],\n        success: True/False\n    }\n    \"\"\"\n    return jsonify(dtypes=global_state.get_dtypes(data_id), success=True)\n\n\ndef build_sequential_diffs(s, col, sort=None):\n    if sort is not None:\n        s = s.sort_values(ascending=sort == \"ASC\")\n    diff = s.diff()\n    diff = diff[diff == diff]  # remove nan or nat values\n    min_diff = diff.min()\n    max_diff = diff.max()\n    avg_diff = diff.mean()\n    diff_vals = diff.value_counts().sort_values(ascending=False)\n    diff_vals.index.name = \"value\"\n    diff_vals.name = \"count\"\n    diff_vals = diff_vals.reset_index()\n\n    diff_vals_f = grid_formatter(grid_columns(diff_vals), as_string=True)\n    diff_fmt = next((f[2] for f in diff_vals_f.fmts if f[1] == \"value\"), None)\n    diff_ct = len(diff_vals)\n\n    code = (\n        \"sequential_diffs = data['{}'].diff()\\n\"\n        \"diff = diff[diff == diff]\\n\"\n        \"min_diff = sequential_diffs.min()\\n\"\n        \"max_diff = sequential_diffs.max()\\n\"\n        \"avg_diff = sequential_diffs.mean()\\n\"\n        \"diff_vals = sequential_diffs.value_counts().sort_values(ascending=False)\"\n    ).format(col)\n\n    metrics = {\n        \"diffs\": {\n            \"data\": diff_vals_f.format_dicts(diff_vals.head(100).itertuples()),\n            \"top\": diff_ct > 100,\n            \"total\": diff_ct,\n        },\n        \"min\": diff_fmt(min_diff, \"N/A\"),\n        \"max\": diff_fmt(max_diff, \"N/A\"),\n        \"avg\": diff_fmt(avg_diff, \"N/A\"),\n    }\n    return metrics, code\n\n\ndef build_string_metrics(s, col):\n    char_len = s.len()\n\n    def calc_len(x):\n        try:\n            return len(x)\n        except BaseException:\n            return 0\n\n    word_len = apply(s.replace(r\"[\\s]+\", \" \").str.split(\" \"), calc_len)\n\n    def txt_count(r):\n        return s.count(r).astype(bool).sum()\n\n    string_metrics = dict(\n        char_min=int(char_len.min()),\n        char_max=int(char_len.max()),\n        char_mean=json_float(char_len.mean()),\n        char_std=json_float(char_len.std()),\n        with_space=int(txt_count(r\"\\s\")),\n        with_accent=int(txt_count(r\"[\u00c0-\u00d6\u00d9-\u00f6\u00f9-\u00ff\u0100-\u017e\u1e00-\u1eff]\")),\n        with_num=int(txt_count(r\"[\\d]\")),\n        with_upper=int(txt_count(r\"[A-Z]\")),\n        with_lower=int(txt_count(r\"[a-z]\")),\n        with_punc=int(\n            txt_count(\n                r'(\\!|\"|\\#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|\\-|\\.|/|\\:|\\;|\\<|\\=|\\>|\\?|@|\\[|\\\\|\\]|\\^|_|\\`|\\{|\\||\\}|\\~)'\n            )\n        ),\n        space_at_the_first=int(txt_count(r\"^ \")),\n        space_at_the_end=int(txt_count(r\" $\")),\n        multi_space_after_each_other=int(txt_count(r\"\\s{2,}\")),\n        with_hidden=int(txt_count(r\"[^{}]+\".format(printable))),\n        word_min=int(word_len.min()),\n        word_max=int(word_len.max()),\n        word_mean=json_float(word_len.mean()),\n        word_std=json_float(word_len.std()),\n    )\n\n    punc_reg = (\n        \"\"\"\\tr'(\\\\!|\"|\\\\#|\\\\$|%|&|\\\\'|\\\\(|\\\\)|\\\\*|\\\\+|,|\\\\-|\\\\.|/|\\\\:|\\\\;|\\\\<|\\\\=|\"\"\"\n        \"\"\"\\\\>|\\\\?|@|\\\\[|\\\\\\\\|\\\\]|\\\\^|_|\\\\`|\\\\{|\\\\||\\\\}|\\\\~)'\"\"\"\n    )\n    code = [\n        \"s = data['{}']\".format(col),\n        \"s = s[~s.isnull()].str\",\n        \"char_len = s.len()\\n\",\n        \"def calc_len(x):\",\n        \"\\ttry:\",\n        \"\\t\\treturn len(x)\",\n        \"\\texcept:\",\n        \"\\t\\treturn 0\\n\",\n        \"word_len = s.replace(r'[\\\\s]+', ' ').str.split(' ').apply(calc_len)\\n\",\n        \"def txt_count(r):\",\n        \"\\treturn s.count(r).astype(bool).sum()\\n\",\n        \"char_min=char_len.min()\",\n        \"char_max = char_len.max()\",\n        \"char_mean = char_len.mean()\",\n        \"char_std = char_len.std()\",\n        \"with_space = txt_count(r'\\\\s')\",\n        \"with_accent = txt_count(r'[\u00c0-\u00d6\u00d9-\u00f6\u00f9-\u00ff\u0100-\u017e\u1e00-\u1eff]')\",\n        \"with_num = txt_count(r'[\\\\d]')\",\n        \"with_upper = txt_count(r'[A-Z]')\",\n        \"with_lower = txt_count(r'[a-z]')\",\n        \"with_punc = txt_count(\",\n        \"\\t{}\".format(punc_reg),\n        \")\",\n        \"space_at_the_first = txt_count(r'^ ')\",\n        \"space_at_the_end = txt_count(r' $')\",\n        \"multi_space_after_each_other = txt_count(r'\\\\s{2,}')\",\n        \"printable = r'\\\\w \\\\!\\\"#\\\\$%&'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<\u00bb\u00ab\u061b\u060c\u0640\\\\=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}~'\",\n        \"with_hidden = txt_count(r'[^{}]+'.format(printable))\",\n        \"word_min = word_len.min()\",\n        \"word_max = word_len.max()\",\n        \"word_mean = word_len.mean()\",\n        \"word_std = word_len.std()\",\n    ]\n\n    return string_metrics, code\n\n\n@dtale.route(\"/describe/<data_id>\")\n@exception_decorator\ndef describe(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns standard details about column data using\n    :meth:`pandas:pandas.DataFrame.describe` to the front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param column: required dash separated string \"START-END\" stating a range of row indexes to be returned\n                   to the screen\n    :return: JSON {\n        describe: object representing output from :meth:`pandas:pandas.Series.describe`,\n        unique_data: array of unique values when data has <= 100 unique values\n        success: True/False\n    }\n\n    \"\"\"\n    column = get_str_arg(request, \"col\")\n    curr_settings = global_state.get_settings(data_id) or {}\n    columns_to_load = [column]\n    indexes = curr_settings.get(\"indexes\", [])\n    # TODO: update this to use arcticdb's index function once it becomes available\n    if global_state.is_arcticdb and column in indexes:\n        column_to_load = next(\n            (\n                c\n                for c in global_state.get_dtypes(data_id) or []\n                if c[\"name\"] not in indexes\n            ),\n            None,\n        )\n        columns_to_load = [column_to_load[\"name\"]] if column_to_load else None\n    data = load_filterable_data(data_id, request, columns=columns_to_load)\n    data = data[[column]]\n    additional_aggs = None\n    dtype = global_state.get_dtype_info(data_id, column)\n    classification = classify_type(dtype[\"dtype\"])\n    if classification == \"I\":\n        additional_aggs = [\"sum\", \"median\", \"mode\", \"var\", \"sem\"]\n    elif classification == \"F\":\n        additional_aggs = [\"sum\", \"median\", \"var\", \"sem\"]\n    code = build_code_export(data_id)\n    desc, desc_code = load_describe(data[column], additional_aggs=additional_aggs)\n    code += desc_code\n    return_data = dict(describe=desc, success=True)\n    if \"unique\" not in return_data[\"describe\"] and \"unique_ct\" in dtype:\n        return_data[\"describe\"][\"unique\"] = json_int(dtype[\"unique_ct\"], as_string=True)\n    for p in [\"skew\", \"kurt\"]:\n        if p in dtype:\n            return_data[\"describe\"][p] = dtype[p]\n\n    if classification != \"F\" and not global_state.store.get(data_id).is_large:\n        uniq_vals = data[column].value_counts().sort_values(ascending=False)\n        uniq_vals.index.name = \"value\"\n        uniq_vals.name = \"count\"\n        uniq_vals = uniq_vals.reset_index()\n\n        # build top\n        top_freq = uniq_vals[\"count\"].values[0]\n        top_freq_pct = (top_freq / uniq_vals[\"count\"].sum()) * 100\n        top_vals = (\n            uniq_vals[uniq_vals[\"count\"] == top_freq].sort_values(\"value\").head(5)\n        )\n        top_vals_f = grid_formatter(grid_columns(top_vals), as_string=True)\n        top_vals = top_vals_f.format_lists(top_vals)\n        return_data[\"describe\"][\"top\"] = \"{} ({}%)\".format(\n            \", \".join(top_vals[\"value\"]), json_float(top_freq_pct, as_string=True)\n        )\n        return_data[\"describe\"][\"freq\"] = int(top_freq)\n\n        code.append(\n            (\n                \"uniq_vals = data['{}'].value_counts().sort_values(ascending=False)\\n\"\n                \"uniq_vals.index.name = 'value'\\n\"\n                \"uniq_vals.name = 'count'\\n\"\n                \"uniq_vals = uniq_vals.reset_index()\"\n            ).format(column)\n        )\n\n        if dtype[\"dtype\"].startswith(\"mixed\"):\n            uniq_vals[\"type\"] = apply(uniq_vals[\"value\"], lambda i: type(i).__name__)\n            dtype_counts = uniq_vals.groupby(\"type\")[\"count\"].sum().reset_index()\n            dtype_counts.columns = [\"dtype\", \"count\"]\n            return_data[\"dtype_counts\"] = dtype_counts.to_dict(orient=\"records\")\n            code.append(\n                (\n                    \"uniq_vals['type'] = uniq_vals['value'].apply( lambda i: type(i).__name__)\\n\"\n                    \"dtype_counts = uniq_vals.groupby('type')['count'].sum().reset_index()\\n\"\n                    \"dtype_counts.columns = ['dtype', 'count']\"\n                )\n            )\n        else:\n            uniq_vals.loc[:, \"type\"] = find_dtype(uniq_vals[\"value\"])\n            code.append(\n                \"uniq_vals.loc[:, 'type'] = '{}'\".format(uniq_vals[\"type\"].values[0])\n            )\n\n        return_data[\"uniques\"] = {}\n        for uniq_type, uniq_grp in uniq_vals.groupby(\"type\"):\n            total = len(uniq_grp)\n            top = total > 100\n            uniq_grp = (\n                uniq_grp[[\"value\", \"count\"]]\n                .sort_values([\"count\", \"value\"], ascending=[False, True])\n                .head(100)\n            )\n            # pandas started supporting string dtypes in 1.1.0\n            conversion_type = (\n                uniq_type\n                if pandas_util.check_pandas_version(\"1.1.0\") and uniq_type == \"string\"\n                else \"object\"\n            )\n            uniq_grp[\"value\"] = uniq_grp[\"value\"].astype(conversion_type)\n            uniq_f, _ = build_formatters(uniq_grp)\n            return_data[\"uniques\"][uniq_type] = dict(\n                data=uniq_f.format_dicts(uniq_grp.itertuples()), total=total, top=top\n            )\n\n    if (\n        classification in [\"I\", \"F\", \"D\"]\n        and not global_state.store.get(data_id).is_large\n    ):\n        sd_metrics, sd_code = build_sequential_diffs(data[column], column)\n        return_data[\"sequential_diffs\"] = sd_metrics\n        code.append(sd_code)\n\n    if classification == \"S\":\n        str_col = data[column]\n        sm_metrics, sm_code = build_string_metrics(\n            str_col[~str_col.isnull()].astype(\"str\").str, column\n        )\n        return_data[\"string_metrics\"] = sm_metrics\n        code += sm_code\n\n    return_data[\"code\"] = \"\\n\".join(code)\n    return jsonify(return_data)\n\n\n@dtale.route(\"/variance/<data_id>\")\n@exception_decorator\ndef variance(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns standard details about column data using\n    :meth:`pandas:pandas.DataFrame.describe` to the front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param column: required dash separated string \"START-END\" stating a range of row indexes to be returned\n                   to the screen\n    :return: JSON {\n        describe: object representing output from :meth:`pandas:pandas.Series.describe`,\n        unique_data: array of unique values when data has <= 100 unique values\n        success: True/False\n    }\n\n    \"\"\"\n    column = get_str_arg(request, \"col\")\n    data = load_filterable_data(data_id, request)\n    s = data[column]\n    code = [\"s = df['{}']\".format(column)]\n    unique_ct = unique_count(s)\n    code.append(\"unique_ct = s.unique().size\")\n    s_size = len(s)\n    code.append(\"s_size = len(s)\")\n    check1 = bool((unique_ct / s_size) < 0.1)\n    code.append(\"check1 = (unique_ct / s_size) < 0.1\")\n    return_data = dict(check1=dict(unique=unique_ct, size=s_size, result=check1))\n    dtype = global_state.get_dtype_info(data_id, column)\n    if unique_ct >= 2:\n        val_counts = s.value_counts()\n        check2 = bool((val_counts.values[0] / val_counts.values[1]) > 20)\n        fmt = find_dtype_formatter(dtype[\"dtype\"])\n        return_data[\"check2\"] = dict(\n            val1=dict(val=fmt(val_counts.index[0]), ct=int(val_counts.values[0])),\n            val2=dict(val=fmt(val_counts.index[1]), ct=int(val_counts.values[1])),\n            result=check2,\n        )\n    code += [\n        \"check2 = False\",\n        \"if unique_ct > 1:\",\n        \"\\tval_counts = s.value_counts()\",\n        \"\\tcheck2 = (val_counts.values[0] / val_counts.values[1]) > 20\",\n        \"low_variance = check1 and check2\",\n    ]\n\n    return_data[\"size\"] = len(s)\n    return_data[\"outlierCt\"] = dtype[\"hasOutliers\"]\n    return_data[\"missingCt\"] = int(s.isnull().sum())\n\n    jb_stat, jb_p = sts.jarque_bera(s)\n    return_data[\"jarqueBera\"] = dict(statistic=float(jb_stat), pvalue=float(jb_p))\n    sw_stat, sw_p = sts.shapiro(s)\n    return_data[\"shapiroWilk\"] = dict(statistic=float(sw_stat), pvalue=float(sw_p))\n    code += [\n        \"\\nimport scipy.stats as sts\\n\",\n        \"jb_stat, jb_p = sts.jarque_bera(s)\",\n        \"sw_stat, sw_p = sts.shapiro(s)\",\n    ]\n    return_data[\"code\"] = \"\\n\".join(code)\n    return jsonify(return_data)\n\n\ndef calc_outlier_range(s):\n    try:\n        q1 = s.quantile(0.25)\n        q3 = s.quantile(0.75)\n    except BaseException:  # this covers the case when a series contains pd.NA\n        return np.nan, np.nan\n    iqr = q3 - q1\n    iqr_lower = q1 - 1.5 * iqr\n    iqr_upper = q3 + 1.5 * iqr\n    return iqr_lower, iqr_upper\n\n\ndef build_outlier_query(iqr_lower, iqr_upper, min_val, max_val, column):\n    queries = []\n    if iqr_lower > min_val:\n        queries.append(\n            \"{column} < {lower}\".format(\n                column=build_col_key(column), lower=json_float(iqr_lower)\n            )\n        )\n    if iqr_upper < max_val:\n        queries.append(\n            \"{column} > {upper}\".format(\n                column=build_col_key(column), upper=json_float(iqr_upper)\n            )\n        )\n    return \"(({}))\".format(\") or (\".join(queries)) if len(queries) > 1 else queries[0]\n\n\n@dtale.route(\"/outliers/<data_id>\")\n@exception_decorator\ndef outliers(data_id):\n    column = get_str_arg(request, \"col\")\n    df = global_state.get_data(data_id)\n    s = df[column]\n    iqr_lower, iqr_upper = calc_outlier_range(s)\n    formatter = find_dtype_formatter(find_dtype(s))\n\n    df = load_filterable_data(data_id, request)\n    s = df[column]\n    outliers = sorted(s[(s < iqr_lower) | (s > iqr_upper)].unique())\n    if not len(outliers):\n        return jsonify(outliers=[])\n\n    top = len(outliers) > 100\n    outliers = [formatter(v) for v in outliers[:100]]\n    query = build_outlier_query(iqr_lower, iqr_upper, s.min(), s.max(), column)\n    code = (\n        \"s = df['{column}']\\n\"\n        \"q1 = s.quantile(0.25)\\n\"\n        \"q3 = s.quantile(0.75)\\n\"\n        \"iqr = q3 - q1\\n\"\n        \"iqr_lower = q1 - 1.5 * iqr\\n\"\n        \"iqr_upper = q3 + 1.5 * iqr\\n\"\n        \"outliers = dict(s[(s < iqr_lower) | (s > iqr_upper)])\"\n    ).format(column=column)\n    queryApplied = column in (\n        (global_state.get_settings(data_id) or {}).get(\"outlierFilters\") or {}\n    )\n    return jsonify(\n        outliers=outliers, query=query, code=code, queryApplied=queryApplied, top=top\n    )\n\n\n@dtale.route(\"/toggle-outlier-filter/<data_id>\")\n@exception_decorator\ndef toggle_outlier_filter(data_id):\n    column = get_str_arg(request, \"col\")\n    settings = global_state.get_settings(data_id) or {}\n    outlierFilters = settings.get(\"outlierFilters\") or {}\n    if column in outlierFilters:\n        settings[\"outlierFilters\"] = {\n            k: v for k, v in outlierFilters.items() if k != column\n        }\n    else:\n        dtype_info = global_state.get_dtype_info(data_id, column)\n        outlier_range, min_val, max_val = (\n            dtype_info.get(p) for p in [\"outlierRange\", \"min\", \"max\"]\n        )\n        iqr_lower, iqr_upper = (outlier_range.get(p) for p in [\"lower\", \"upper\"])\n        query = build_outlier_query(iqr_lower, iqr_upper, min_val, max_val, column)\n        settings[\"outlierFilters\"] = dict_merge(\n            outlierFilters, {column: {\"query\": query}}\n        )\n    global_state.set_settings(data_id, settings)\n    return jsonify(dict(success=True, outlierFilters=settings[\"outlierFilters\"]))\n\n\n@dtale.route(\"/delete-col/<data_id>\")\n@exception_decorator\ndef delete_col(data_id):\n    columns = get_json_arg(request, \"cols\")\n    data = global_state.get_data(data_id)\n    data = data[[c for c in data.columns if c not in columns]]\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\"df = df.drop(columns=['{}'])\".format(\"','\".join(columns))]\n    global_state.set_history(data_id, curr_history)\n    dtypes = global_state.get_dtypes(data_id)\n    dtypes = [dt for dt in dtypes if dt[\"name\"] not in columns]\n    curr_settings = global_state.get_settings(data_id)\n    curr_settings[\"locked\"] = [\n        c for c in curr_settings.get(\"locked\", []) if c not in columns\n    ]\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, dtypes)\n    global_state.set_settings(data_id, curr_settings)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/rename-col/<data_id>\")\n@exception_decorator\ndef rename_col(data_id):\n    column = get_str_arg(request, \"col\")\n    rename = get_str_arg(request, \"rename\")\n    data = global_state.get_data(data_id)\n    if column != rename and rename in data.columns:\n        return jsonify(error='Column name \"{}\" already exists!')\n\n    data = data.rename(columns={column: rename})\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\"df = df.rename(columns={'%s': '%s'})\" % (column, rename)]\n    global_state.set_history(data_id, curr_history)\n    dtypes = global_state.get_dtypes(data_id)\n    dtypes = [\n        dict_merge(dt, {\"name\": rename}) if dt[\"name\"] == column else dt\n        for dt in dtypes\n    ]\n    curr_settings = global_state.get_settings(data_id)\n    curr_settings[\"locked\"] = [\n        rename if c == column else c for c in curr_settings.get(\"locked\", [])\n    ]\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, dtypes)\n    global_state.set_settings(data_id, curr_settings)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/duplicate-col/<data_id>\")\n@exception_decorator\ndef duplicate_col(data_id):\n    column = get_str_arg(request, \"col\")\n    data = global_state.get_data(data_id)\n    dupe_idx = 2\n    new_col = \"{}_{}\".format(column, dupe_idx)\n    while new_col in data.columns:\n        dupe_idx += 1\n        new_col = \"{}_{}\".format(column, dupe_idx)\n\n    data.loc[:, new_col] = data[column]\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\"df.loc[:, '%s'] = df['%s']\" % (new_col, column)]\n    global_state.set_history(data_id, curr_history)\n    dtypes = []\n    cols = []\n    idx = 0\n    for dt in global_state.get_dtypes(data_id):\n        dt[\"index\"] = idx\n        dtypes.append(dt)\n        cols.append(dt[\"name\"])\n        idx += 1\n        if dt[\"name\"] == column:\n            dtypes.append(dict_merge(dt, dict(name=new_col, index=idx)))\n            cols.append(new_col)\n            idx += 1\n\n    global_state.set_data(data_id, data[cols])\n    global_state.set_dtypes(data_id, dtypes)\n    return jsonify(success=True, col=new_col)\n\n\n@dtale.route(\"/edit-cell/<data_id>\")\n@exception_decorator\ndef edit_cell(data_id):\n    column = get_str_arg(request, \"col\")\n    row_index = get_int_arg(request, \"rowIndex\")\n    updated = get_str_arg(request, \"updated\")\n    updated_str = updated\n    curr_settings = global_state.get_settings(data_id)\n\n    # make sure to load filtered data in order to get correct row index\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    row_index_val = data.iloc[[row_index]].index[0]\n    data = global_state.get_data(data_id)\n    dtype = find_dtype(data[column])\n\n    code = []\n    if updated in [\"nan\", \"inf\"]:\n        updated_str = \"np.{}\".format(updated)\n        updated = getattr(np, updated)\n        data.loc[row_index_val, column] = updated\n        code.append(\n            \"df.loc[{row_index}, '{column}'] = {updated}\".format(\n                row_index=row_index_val, column=column, updated=updated_str\n            )\n        )\n    else:\n        classification = classify_type(dtype)\n        if classification == \"B\":\n            updated = updated.lower() == \"true\"\n            updated_str = str(updated)\n        elif classification == \"I\":\n            updated = int(updated)\n        elif classification == \"F\":\n            updated = float(updated)\n        elif classification == \"D\":\n            updated_str = \"pd.Timestamp({})\".format(updated)\n            updated = pd.Timestamp(updated)\n        elif classification == \"TD\":\n            updated_str = \"pd.Timedelta({})\".format(updated)\n            updated = pd.Timedelta(updated)\n        else:\n            if dtype.startswith(\"category\") and updated not in data[column].unique():\n                if pandas_util.is_pandas2():\n                    data.loc[:, column] = pd.Categorical(\n                        data[column],\n                        categories=data[column].cat.add_categories(updated),\n                        ordered=True,\n                    )\n                    code.append(\n                        (\n                            \"data.loc[:, '{column}'] = pd.Categorical(\\n\"\n                            \"\\tdata['{column}'],\\n\"\n                            \"\\tcategories=data['{column}'].cat.add_categories('{updated}'),\\n\"\n                            \"\\tordered=True\\n\"\n                            \")\"\n                        ).format(column=column, updated=updated)\n                    )\n                else:\n                    data[column].cat.add_categories(updated, inplace=True)\n                    code.append(\n                        \"data['{column}'].cat.add_categories('{updated}', inplace=True)\".format(\n                            column=column, updated=updated\n                        )\n                    )\n            updated_str = \"'{}'\".format(updated)\n        data.at[row_index_val, column] = updated\n        code.append(\n            \"df.at[{row_index}, '{column}'] = {updated}\".format(\n                row_index=row_index_val, column=column, updated=updated_str\n            )\n        )\n    global_state.set_data(data_id, data)\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += code\n    global_state.set_history(data_id, curr_history)\n\n    data = global_state.get_data(data_id)\n    dtypes = global_state.get_dtypes(data_id)\n    ranges = calc_data_ranges(data[[column]])\n    dtype_f = dtype_formatter(data, {column: dtype}, ranges)\n    dtypes = [\n        dtype_f(dt[\"index\"], column) if dt[\"name\"] == column else dt for dt in dtypes\n    ]\n    global_state.set_dtypes(data_id, dtypes)\n    return jsonify(success=True)\n\n\ndef build_filter_vals(series, data_id, column, fmt):\n    dtype_info = global_state.get_dtype_info(data_id, column)\n    vals = list(series.dropna().unique())\n    try:\n        vals = sorted(vals)\n    except BaseException:\n        pass  # if there are mixed values (EX: strings with ints) this fails\n    if dtype_info.get(\"unique_ct\", 0) > 500:\n        # columns with too many unique values will need to use asynchronous loading, so for now we'll give the\n        # first 5 values\n        vals = vals[:5]\n    vals = [fmt(v) for v in vals]\n    return vals\n\n\n@dtale.route(\"/column-filter-data/<data_id>\")\n@exception_decorator\ndef get_column_filter_data(data_id):\n    if global_state.is_arcticdb and global_state.store.get(data_id).is_large:\n        return jsonify(dict(success=True, hasMissing=True))\n    column = get_str_arg(request, \"col\")\n    s = global_state.get_data(data_id)[column]\n    dtype = find_dtype(s)\n    fmt = find_dtype_formatter(dtype)\n    classification = classify_type(dtype)\n    ret = dict(success=True, hasMissing=bool(s.isnull().any()))\n    if classification not in [\"S\", \"B\"]:\n        data_range = s.agg([\"min\", \"max\"]).to_dict()\n        data_range = {k: fmt(v) for k, v in data_range.items()}\n        ret = dict_merge(ret, data_range)\n    if classification in [\"S\", \"I\", \"B\"]:\n        ret[\"uniques\"] = build_filter_vals(s, data_id, column, fmt)\n    return jsonify(ret)\n\n\n@dtale.route(\"/async-column-filter-data/<data_id>\")\n@exception_decorator\ndef get_async_column_filter_data(data_id):\n    column = get_str_arg(request, \"col\")\n    input = get_str_arg(request, \"input\")\n    s = global_state.get_data(data_id)[column]\n    dtype = find_dtype(s)\n    fmt = find_dtype_formatter(dtype)\n    vals = s[s.astype(\"str\").str.startswith(input)]\n    vals = [option(fmt(v)) for v in sorted(vals.unique())[:5]]\n    return jsonify(vals)\n\n\n@dtale.route(\"/save-column-filter/<data_id>\")\n@exception_decorator\ndef save_column_filter(data_id):\n    column = get_str_arg(request, \"col\")\n    curr_filters = ColumnFilter(\n        data_id, column, get_str_arg(request, \"cfg\")\n    ).save_filter()\n    return jsonify(success=True, currFilters=curr_filters)\n\n\n@dtale.route(\"/data/<data_id>\")\n@exception_decorator\ndef get_data(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns current rows from DATA (based on scrollbar specs and saved settings)\n    to front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param ids: required dash separated string \"START-END\" stating a range of row indexes to be returned to the screen\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :param sort: JSON string from flask.request.args['sort'] which is applied to DATA using the sort_values() or\n                 sort_index() function.  Here is the JSON structure: [col1,dir1],[col2,dir2],....[coln,dirn]\n    :return: JSON {\n        results: [\n            {dtale_index: 1, col1: val1_1, ...,colN: valN_1},\n            ...,\n            {dtale_index: N2, col1: val1_N2, ...,colN: valN_N2}\n        ],\n        columns: [{name: col1, dtype: 'int64'},...,{name: colN, dtype: 'datetime'}],\n        total: N2,\n        success: True/False\n    }\n    \"\"\"\n\n    # handling for gunicorn-hosted instances w/ ArcticDB\n    if global_state.is_arcticdb and not len(global_state.get_dtypes(data_id) or []):\n        global_state.store.build_instance(data_id)\n        startup(data=data_id)\n\n    params = retrieve_grid_params(request)\n    export = get_bool_arg(request, \"export\")\n    ids = get_json_arg(request, \"ids\")\n    if not export and ids is None:\n        return jsonify({})\n\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_locked = curr_settings.get(\"locked\", [])\n    final_query = build_query(data_id, curr_settings.get(\"query\"))\n    highlight_filter = curr_settings.get(\"highlightFilter\") or False\n\n    if global_state.is_arcticdb:\n        col_types = global_state.get_dtypes(data_id) or []\n        columns_to_load = [c[\"name\"] for c in col_types if c[\"visible\"]]\n        f = grid_formatter(\n            [c for c in col_types if c[\"visible\"]],\n            nan_display=curr_settings.get(\"nanDisplay\", \"nan\"),\n        )\n\n        query_builder = build_query_builder(data_id)\n        date_range = load_index_filter(data_id)\n        instance = global_state.store.get(data_id)\n        total = instance.rows()\n        results = {}\n        if total:\n            if export:\n                export_rows = get_int_arg(request, \"export_rows\")\n                if export_rows:\n                    if query_builder:\n                        data = instance.load_data(\n                            query_builder=query_builder, **date_range\n                        )\n                        data = data.head(export_rows)\n                    elif len(date_range):\n                        data = instance.load_data(**date_range)\n                        data = data.head(export_rows)\n                    else:\n                        data = instance.load_data(row_range=[0, export_rows])\n                data, _ = format_data(data)\n                data = data[\n                    curr_locked + [c for c in data.columns if c not in curr_locked]\n                ]\n                results = f.format_dicts(data.itertuples())\n                results = [dict_merge({IDX_COL: i}, r) for i, r in enumerate(results)]\n            elif query_builder:\n                df = instance.load_data(\n                    query_builder=query_builder,\n                    columns=columns_to_load,\n                    # fmt: off\n                    **date_range\n                    # fmt: on\n                )\n                total = len(df)\n                df, _ = format_data(df)\n                df = df[curr_locked + [c for c in df.columns if c not in curr_locked]]\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = df.iloc[sub_range[0] : sub_range[0] + 1]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                    else:\n                        [start, end] = sub_range\n                        sub_df = (\n                            df.iloc[start:]\n                            if end >= total - 1\n                            else df.iloc[start : end + 1]\n                        )\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n            elif len(date_range):\n                df = instance.load_data(columns=columns_to_load, **date_range)\n                total = len(df)\n                df, _ = format_data(df)\n                df = df[curr_locked + [c for c in df.columns if c not in curr_locked]]\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = df.iloc[sub_range[0] : sub_range[0] + 1]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                    else:\n                        [start, end] = sub_range\n                        sub_df = (\n                            df.iloc[start:]\n                            if end >= total - 1\n                            else df.iloc[start : end + 1]\n                        )\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n            else:\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = instance.load_data(\n                            row_range=[sub_range[0], sub_range[0] + 1],\n                            columns=columns_to_load,\n                        )\n                        sub_df, _ = format_data(sub_df)\n                        sub_df = sub_df[\n                            curr_locked\n                            + [c for c in sub_df.columns if c not in curr_locked]\n                        ]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                    else:\n                        [start, end] = sub_range\n                        sub_df = instance.load_data(\n                            row_range=[start, total if end >= total else end + 1],\n                            columns=columns_to_load,\n                        )\n                        sub_df, _ = format_data(sub_df)\n                        sub_df = sub_df[\n                            curr_locked\n                            + [c for c in sub_df.columns if c not in curr_locked]\n                        ]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n    else:\n        data = global_state.get_data(data_id)\n\n        # this will check for when someone instantiates D-Tale programmatically and directly alters the internal\n        # state of the dataframe (EX: d.data['new_col'] = 'foo')\n        curr_dtypes = [c[\"name\"] for c in global_state.get_dtypes(data_id)]\n        if any(c not in curr_dtypes for c in data.columns):\n            data, _ = format_data(data)\n            data = data[curr_locked + [c for c in data.columns if c not in curr_locked]]\n            global_state.set_data(data_id, data)\n            global_state.set_dtypes(\n                data_id,\n                build_dtypes_state(data, global_state.get_dtypes(data_id) or []),\n            )\n\n        col_types = global_state.get_dtypes(data_id)\n        f = grid_formatter(\n            col_types, nan_display=curr_settings.get(\"nanDisplay\", \"nan\")\n        )\n        if curr_settings.get(\"sortInfo\") != params.get(\"sort\"):\n            data = sort_df_for_grid(data, params)\n            global_state.set_data(data_id, data)\n        if params.get(\"sort\") is not None:\n            curr_settings = dict_merge(curr_settings, dict(sortInfo=params[\"sort\"]))\n        else:\n            curr_settings = {k: v for k, v in curr_settings.items() if k != \"sortInfo\"}\n        filtered_indexes = []\n        data = run_query(\n            handle_predefined(data_id),\n            final_query,\n            global_state.get_context_variables(data_id),\n            ignore_empty=True,\n            highlight_filter=highlight_filter,\n        )\n        if highlight_filter:\n            data, filtered_indexes = data\n        global_state.set_settings(data_id, curr_settings)\n\n        total = len(data)\n        results = {}\n        if total:\n            if export:\n                export_rows = get_int_arg(request, \"export_rows\")\n                if export_rows:\n                    data = data.head(export_rows)\n                results = f.format_dicts(data.itertuples())\n                results = [dict_merge({IDX_COL: i}, r) for i, r in enumerate(results)]\n            else:\n                for sub_range in ids:\n                    sub_range = list(map(int, sub_range.split(\"-\")))\n                    if len(sub_range) == 1:\n                        sub_df = data.iloc[sub_range[0] : sub_range[0] + 1]\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        results[sub_range[0]] = dict_merge(\n                            {IDX_COL: sub_range[0]}, sub_df[0]\n                        )\n                        if highlight_filter and sub_range[0] in filtered_indexes:\n                            results[sub_range[0]][\"__filtered\"] = True\n                    else:\n                        [start, end] = sub_range\n                        sub_df = (\n                            data.iloc[start:]\n                            if end >= total - 1\n                            else data.iloc[start : end + 1]\n                        )\n                        sub_df = f.format_dicts(sub_df.itertuples())\n                        for i, d in zip(range(start, end + 1), sub_df):\n                            results[i] = dict_merge({IDX_COL: i}, d)\n                            if highlight_filter and i in filtered_indexes:\n                                results[i][\"__filtered\"] = True\n    columns = [\n        dict(name=IDX_COL, dtype=\"int64\", visible=True)\n    ] + global_state.get_dtypes(data_id)\n    return_data = dict(\n        results=results,\n        columns=columns,\n        total=total,\n        final_query=None if highlight_filter else final_query,\n    )\n\n    if export:\n        return export_html(data_id, return_data)\n\n    return jsonify(return_data)\n\n\ndef export_html(data_id, return_data):\n    def load_file(fpath, encoding=\"utf-8\"):\n        return read_file(\n            os.path.join(os.path.dirname(__file__), \"static/{}\".format(fpath)),\n            encoding=encoding,\n        )\n\n    istok_woff = load_file(\"fonts/istok_woff64.txt\", encoding=None)\n    istok_bold_woff = load_file(\"fonts/istok-bold_woff64.txt\", encoding=None)\n\n    font_styles = (\n        \"\"\"\n        @font-face {\n          font-family: \"istok\";\n          font-weight: 400;\n          font-style: normal;\n          src: url(data:font/truetype;charset=utf-8;base64,\"\"\"\n        + istok_woff\n        + \"\"\") format(\"woff\");\n        }\n\n        @font-face {\n          font-family: \"istok\";\n          font-weight: 700;\n          font-style: normal;\n          src: url(data:font/truetype;charset=utf-8;base64,\"\"\"\n        + istok_bold_woff\n        + \"\"\") format(\"woff\");\n        }\n        \"\"\"\n    )\n\n    main_styles = load_file(\"css/main.css\", encoding=\"utf-8\" if PY3 else None).split(\n        \"\\n\"\n    )\n    main_styles = \"\\n\".join(main_styles[28:])\n    main_styles = \"{}\\n{}\\n\".format(font_styles, main_styles)\n\n    if not PY3:\n        main_styles = main_styles.decode(\"utf-8\")\n\n    return_data[\"results\"] = {r[IDX_COL]: r for r in return_data[\"results\"]}\n\n    polyfills_js = load_file(\"dist/polyfills_bundle.js\")\n    export_js = load_file(\"dist/export_bundle.js\")\n\n    return send_file(\n        base_render_template(\n            \"dtale/html_export.html\",\n            data_id,\n            main_styles=main_styles,\n            polyfills_js=polyfills_js,\n            export_js=export_js,\n            response=return_data,\n        ),\n        \"dtale_html_export_{}.html\".format(json_timestamp(pd.Timestamp(\"now\"))),\n        \"text/html\",\n    )\n\n\n@dtale.route(\"/load-filtered-ranges/<data_id>\")\n@exception_decorator\ndef load_filtered_ranges(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    final_query = build_query(data_id, curr_settings.get(\"query\"))\n    if not final_query:\n        return {}\n    curr_filtered_ranges = curr_settings.get(\"filteredRanges\", {})\n    if final_query == curr_filtered_ranges.get(\"query\"):\n        return jsonify(curr_filtered_ranges)\n    data = run_query(\n        handle_predefined(data_id),\n        final_query,\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n\n    def _filter_numeric(col):\n        s = data[col]\n        dtype = find_dtype(s)\n        return classify_type(dtype) in [\"F\", \"I\"] and not s.isnull().all()\n\n    numeric_cols = [col for col in data.columns if _filter_numeric(col)]\n    filtered_ranges = calc_data_ranges(data[numeric_cols])\n    updated_dtypes = build_dtypes_state(\n        data, global_state.get_dtypes(data_id) or [], filtered_ranges\n    )\n    updated_dtypes = {col[\"name\"]: col for col in updated_dtypes}\n    overall_min, overall_max = None, None\n    if len(filtered_ranges):\n        overall_min = min([v[\"min\"] for v in filtered_ranges.values()])\n        overall_max = max([v[\"max\"] for v in filtered_ranges.values()])\n    curr_settings[\"filteredRanges\"] = dict(\n        query=final_query,\n        ranges=filtered_ranges,\n        dtypes=updated_dtypes,\n        overall=dict(min=overall_min, max=overall_max),\n    )\n    global_state.set_settings(data_id, curr_settings)\n    return jsonify(curr_settings[\"filteredRanges\"])\n\n\n@dtale.route(\"/data-export/<data_id>\")\n@exception_decorator\ndef data_export(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_dtypes = global_state.get_dtypes(data_id) or []\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    data = data[\n        [\n            c[\"name\"]\n            for c in sorted(curr_dtypes, key=lambda c: c[\"index\"])\n            if c[\"visible\"]\n        ]\n    ]\n    file_type = get_str_arg(request, \"type\", \"csv\")\n    if file_type in [\"csv\", \"tsv\"]:\n        tsv = file_type == \"tsv\"\n        csv_buffer = export_to_csv_buffer(data, tsv=tsv)\n        filename = build_chart_filename(\"data\", ext=file_type)\n        return send_file(csv_buffer.getvalue(), filename, \"text/{}\".format(file_type))\n    elif file_type == \"parquet\":\n        from dtale.utils import export_to_parquet_buffer\n\n        parquet_buffer = export_to_parquet_buffer(data)\n        filename = build_chart_filename(\"data\", ext=\"parquet.gzip\")\n        return send_file(\n            parquet_buffer.getvalue(), filename, \"application/octet-stream\"\n        )\n    return jsonify(success=False)\n\n\n@dtale.route(\"/column-analysis/<data_id>\")\n@exception_decorator\ndef get_column_analysis(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns output from numpy.histogram/pd.value_counts to front-end as JSON\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param col: string from flask.request.args['col'] containing name of a column in your dataframe\n    :param type: string from flask.request.args['type'] to signify either a histogram or value counts\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :param bins: the number of bins to display in your histogram, options on the front-end are 5, 10, 20, 50\n    :param top: the number of top values to display in your value counts, default is 100\n    :returns: JSON {results: DATA, desc: output from pd.DataFrame[col].describe(), success: True/False}\n    \"\"\"\n\n    analysis = ColumnAnalysis(data_id, request)\n    return jsonify(**analysis.build())\n\n\n@matplotlib_decorator\ndef build_correlations_matrix_image(\n    data,\n    is_pps,\n    valid_corr_cols,\n    valid_str_corr_cols,\n    valid_date_cols,\n    dummy_col_mappings,\n    pps_data,\n    code,\n):\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\n    plt.figure(figsize=(20, 12))\n    ax0 = plt.gca()\n    cmap = \"Blues\" if is_pps else \"RdYlGn\"\n    vmin = 0.0 if is_pps else -1.0\n    sns.heatmap(\n        data.mask(data.apply(lambda x: x.name == x.index)),\n        ax=ax0,\n        vmin=vmin,\n        vmax=1.0,\n        xticklabels=True,\n        yticklabels=True,\n        cmap=cmap,\n    )\n    output = BytesIO()\n    FigureCanvas(ax0.get_figure()).print_png(output)\n    return (\n        valid_corr_cols,\n        valid_str_corr_cols,\n        valid_date_cols,\n        dummy_col_mappings,\n        pps_data,\n        code,\n        output.getvalue(),\n    )\n\n\ndef build_correlations_matrix(data_id, is_pps=False, encode_strings=False, image=False):\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    valid_corr_cols, valid_str_corr_cols, valid_date_cols = correlations.get_col_groups(\n        data_id, data\n    )\n\n    str_encodings_code = \"\"\n    dummy_col_mappings = {}\n    if encode_strings and valid_str_corr_cols:\n        data = data[valid_corr_cols + valid_str_corr_cols]\n        dummy_kwargs = {}\n        if pandas_util.is_pandas2():\n            dummy_kwargs[\"dtype\"] = \"int\"\n        for str_col in valid_str_corr_cols:\n            dummies = pd.get_dummies(data[[str_col]], columns=[str_col], **dummy_kwargs)\n            dummy_cols = list(dummies.columns)\n            dummy_col_mappings[str_col] = dummy_cols\n            data[dummy_cols] = dummies\n            valid_corr_cols += dummy_cols\n        str_encodings_code = (\n            \"str_corr_cols = [\\n\\t'{valid_str_corr_cols}'\\n]\\n\"\n            \"dummies = pd.get_dummies(corr_data, str_corr_cols)\\n\"\n            \"corr_data.loc[:, dummies.columns] = dummies\\n\"\n        ).format(valid_str_corr_cols=\"', '\".join(valid_str_corr_cols))\n    else:\n        data = data[valid_corr_cols]\n\n    corr_cols_str = \"'\\n\\t'\".join(\n        [\"', '\".join(chunk) for chunk in divide_chunks(valid_corr_cols, 8)]\n    )\n\n    pps_data = None\n    if is_pps:\n        code = build_code_export(data_id, imports=\"import ppscore\\n\")\n        code.append(\n            (\n                \"corr_cols = [\\n\"\n                \"\\t'{corr_cols}'\\n\"\n                \"]\\n\"\n                \"corr_data = df[corr_cols]\\n\"\n                \"{str_encodings}\"\n                \"corr_data = ppscore.matrix(corr_data)\\n\"\n            ).format(corr_cols=corr_cols_str, str_encodings=str_encodings_code)\n        )\n\n        data, pps_data = get_ppscore_matrix(data[valid_corr_cols])\n    else:\n        data, matrix_code = correlations.build_matrix(\n            data_id,\n            data,\n            valid_corr_cols,\n            {\"corr_cols\": corr_cols_str, \"str_encodings\": str_encodings_code},\n        )\n        code = [matrix_code]\n\n    code.append(\n        \"corr_data.index.name = str('column')\\ncorr_data = corr_data.reset_index()\"\n    )\n    code = \"\\n\".join(code)\n    data.index.name = str(\"column\")\n    if image:\n        return build_correlations_matrix_image(\n            data,\n            is_pps,\n            valid_corr_cols,\n            valid_str_corr_cols,\n            valid_date_cols,\n            dummy_col_mappings,\n            pps_data,\n            code,\n        )\n    return (\n        valid_corr_cols,\n        valid_str_corr_cols,\n        valid_date_cols,\n        dummy_col_mappings,\n        pps_data,\n        code,\n        data,\n    )\n\n\n@dtale.route(\"/correlations/<data_id>\")\n@exception_decorator\ndef get_correlations(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which gathers Pearson correlations against all combinations of columns with\n    numeric data using :meth:`pandas:pandas.DataFrame.corr`\n\n    On large datasets with no :attr:`numpy:numpy.nan` data this code will use :meth:`numpy:numpy.corrcoef`\n    for speed purposes\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :returns: JSON {\n        data: [{column: col1, col1: 1.0, col2: 0.99, colN: 0.45},...,{column: colN, col1: 0.34, col2: 0.88, colN: 1.0}],\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    is_pps = get_bool_arg(request, \"pps\")\n    image = get_bool_arg(request, \"image\")\n    matrix_data = build_correlations_matrix(\n        data_id,\n        is_pps=is_pps,\n        encode_strings=get_bool_arg(request, \"encodeStrings\"),\n        image=image,\n    )\n    (\n        valid_corr_cols,\n        valid_str_corr_cols,\n        valid_date_cols,\n        dummy_col_mappings,\n        pps_data,\n        code,\n        df_or_image,\n    ) = matrix_data\n    if image:\n        fname = \"{}.png\".format(\"predictive_power_score\" if is_pps else \"correlations\")\n        return send_file(df_or_image, fname, \"image/png\")\n\n    data = df_or_image.reset_index()\n    col_types = grid_columns(data)\n    f = grid_formatter(col_types, nan_display=None)\n    return jsonify(\n        data=f.format_dicts(data.itertuples()),\n        dates=valid_date_cols,\n        strings=valid_str_corr_cols,\n        dummyColMappings=dummy_col_mappings,\n        code=code,\n        pps=pps_data,\n    )\n\n\n@dtale.route(\"/corr-analysis/<data_id>\")\n@exception_decorator\ndef get_corr_analysis(data_id):\n    column_name, max_score, corrs, ranks = correlations.get_analysis(data_id)\n    return jsonify(\n        column_name=column_name, max_score=max_score, corrs=corrs, ranks=ranks\n    )\n\n\n@dtale.route(\"/chart-data/<data_id>\")\n@exception_decorator\ndef get_chart_data(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which builds data associated with a chart.js chart\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param query: string from flask.request.args['query'] which is applied to DATA using the query() function\n    :param x: string from flask.request.args['x'] column to be used as x-axis of chart\n    :param y: string from flask.request.args['y'] column to be used as y-axis of chart\n    :param group: string from flask.request.args['group'] comma-separated string of columns to group chart data by\n    :param agg: string from flask.request.args['agg'] points to a specific function that can be applied to\n                :func: pandas.core.groupby.DataFrameGroupBy.  Possible values are: count, first, last mean,\n                median, min, max, std, var, mad, prod, sum\n    :returns: JSON {\n        data: {\n            series1: { x: [x1, x2, ..., xN], y: [y1, y2, ..., yN] },\n            series2: { x: [x1, x2, ..., xN], y: [y1, y2, ..., yN] },\n            ...,\n            seriesN: { x: [x1, x2, ..., xN], y: [y1, y2, ..., yN] },\n        },\n        min: minY,\n        max: maxY,\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, get_str_arg(request, \"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    x = get_str_arg(request, \"x\")\n    y = get_json_arg(request, \"y\")\n    group_col = get_json_arg(request, \"group\")\n    agg = get_str_arg(request, \"agg\")\n    allow_duplicates = get_bool_arg(request, \"allowDupes\")\n    window = get_int_arg(request, \"rollingWin\")\n    comp = get_str_arg(request, \"rollingComp\")\n    data, code = build_base_chart(\n        data,\n        x,\n        y,\n        group_col=group_col,\n        agg=agg,\n        allow_duplicates=allow_duplicates,\n        rolling_win=window,\n        rolling_comp=comp,\n    )\n    data[\"success\"] = True\n    return jsonify(data)\n\n\ndef get_ppscore(df, col1, col2):\n    if not PY3:\n        return None\n    try:\n        import dtale.ppscore as ppscore\n\n        pps = ppscore.score(df, col1, col2)\n        pps[\"model\"] = pps[\"model\"].__str__()\n        pps[\"ppscore\"] = float(pps[\"ppscore\"])\n        pps[\"baseline_score\"] = float(pps[\"baseline_score\"])\n        pps[\"model_score\"] = float(pps[\"model_score\"])\n        return pps\n    except BaseException:\n        return None\n\n\ndef get_ppscore_matrix(df):\n    if not PY3:\n        return [], None\n    try:\n        import dtale.ppscore as ppscore\n\n        pps_data = ppscore.matrix(df)\n        data = (\n            pps_data[[\"x\", \"y\", \"ppscore\"]].set_index([\"x\", \"y\"]).unstack()[\"ppscore\"]\n        )\n\n        # additional PPS display\n        pps_data.loc[:, \"model\"] = pps_data[\"model\"].astype(\"str\")\n        pps_data = format_grid(pps_data)\n        pps_data = pps_data[\"results\"]\n        return data, pps_data\n    except BaseException:\n        return [], None\n\n\ndef update_df_for_encoded_strings(df, dummy_cols, cols, code):\n    if not dummy_cols:\n        return df\n    dummy_kwargs = {}\n    if pandas_util.is_pandas2():\n        dummy_kwargs[\"dtype\"] = \"int\"\n    dummies = pd.get_dummies(df[dummy_cols], columns=dummy_cols, **dummy_kwargs)\n    dummies = dummies[[c for c in dummies.columns if c in cols]]\n    df[dummies.columns] = dummies\n\n    code.append(\n        (\n            \"dummy_cols = ['{}']\\n\"\n            \"dummies = pd.get_dummies(df[dummy_cols], columns=dummy_cols)\\n\"\n            \"final_cols = ['{}']\\n\"\n            \"dummies = dummies[[c for c in dummies.columns if c in final_cols]]\\n\"\n            \"df.loc[:, dummies.columns] = dummies\"\n        ).format(\"', '\".join(dummy_cols), \"', '\".join(cols))\n    )\n    return df\n\n\n@dtale.route(\"/correlations-ts/<data_id>\")\n@exception_decorator\ndef get_correlations_ts(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns timeseries of Pearson correlations of two columns with numeric data\n    using :meth:`pandas:pandas.DataFrame.corr`\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param cols: comma-separated string from flask.request.args['cols'] containing names of two columns in dataframe\n    :param dateCol: string from flask.request.args['dateCol'] with name of date-type column in dateframe for timeseries\n    :returns: JSON {\n        data: {:col1:col2: {data: [{corr: 0.99, date: 'YYYY-MM-DD'},...], max: 0.99, min: 0.99}\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    cols = get_json_arg(request, \"cols\")\n    [col1, col2] = cols\n    date_col = get_str_arg(request, \"dateCol\")\n    rolling = get_bool_arg(request, \"rolling\")\n    rolling_window = get_int_arg(request, \"rollingWindow\")\n    min_periods = get_int_arg(request, \"minPeriods\")\n    dummy_cols = get_json_arg(request, \"dummyCols\", [])\n\n    code = build_code_export(data_id)\n    pps = get_ppscore(data, col1, col2)\n\n    if rolling:\n        data = data[\n            [c for c in data.columns if c in [date_col, col1, col2] + dummy_cols]\n        ]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        data = data.set_index(date_col)\n        rolling_kwargs = {}\n        if min_periods is not None:\n            rolling_kwargs[\"min_periods\"] = min_periods\n        data = (\n            data[[col1, col2]]\n            .rolling(rolling_window, **rolling_kwargs)\n            .corr()\n            .reset_index()\n        )\n        data = data.dropna()\n        data = data[data[\"level_1\"] == col1][[date_col, col2]]\n        code.append(\n            (\n                \"corr_ts = df[['{date_col}', '{col1}', '{col2}']].set_index('{date_col}')\\n\"\n                \"corr_ts = corr_ts[['{col1}', '{col2}']].rolling({rolling_window}, min_periods=min_periods).corr()\\n\"\n                \"corr_ts = corr_ts.reset_index().dropna()\\n\"\n                \"corr_ts = corr_ts[corr_ts['level_1'] == '{col1}'][['{date_col}', '{col2}']]\"\n            ).format(\n                col1=col1, col2=col2, date_col=date_col, rolling_window=rolling_window\n            )\n        )\n    else:\n        data = data[[c for c in data.columns if c in [date_col] + cols + dummy_cols]]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        data = data.groupby(date_col)[cols].corr(method=\"pearson\")\n        data.index.names = [\"date\", \"column\"]\n        data = data.reset_index()\n        data = data[data.column == col1][[\"date\", col2]]\n        code.append(\n            (\n                \"corr_ts = df.groupby('{date_col}')['{cols}'].corr(method='pearson')\\n\"\n                \"corr_ts.index.names = ['date', 'column']\\n\"\n                \"corr_ts = corr_ts[corr_ts.column == '{col1}'][['date', '{col2}']]\\n\"\n            ).format(col1=col1, col2=col2, date_col=date_col, cols=\"', '\".join(cols))\n        )\n        if rolling_window:\n            data = data.set_index(\"date\")\n            rolling_kwargs = {}\n            if min_periods is not None:\n                rolling_kwargs[\"min_periods\"] = min_periods\n            data = (\n                data[[col2]]\n                .rolling(rolling_window, **rolling_kwargs)\n                .mean()\n                .reset_index()\n            )\n            data = data.dropna()\n            code.append(\n                (\n                    \"corr_ts = corr_ts.set_index('date')\\n\"\n                    \"corr_ts = corr_ts[['{col}']].rolling({rolling_window}, min_periods={min_periods}).mean()\\n\"\n                    \"corr_ts = corr_ts.reset_index().dropna()\"\n                ).format(\n                    col=col2, rolling_window=rolling_window, min_periods=min_periods\n                )\n            )\n\n    data.columns = [\"date\", \"corr\"]\n    code.append(\"corr_ts.columns = ['date', 'corr']\")\n    return_data, _code = build_base_chart(data.fillna(0), \"date\", \"corr\", agg=\"raw\")\n    return_data[\"success\"] = True\n    return_data[\"code\"] = \"\\n\".join(code)\n    return_data[\"pps\"] = pps\n    return jsonify(return_data)\n\n\n@dtale.route(\"/scatter/<data_id>\")\n@exception_decorator\ndef get_scatter(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns data used in correlation of two columns for scatter chart\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param cols: comma-separated string from flask.request.args['cols'] containing names of two columns in dataframe\n    :param dateCol: string from flask.request.args['dateCol'] with name of date-type column in dateframe for timeseries\n    :param date: string from flask.request.args['date'] date value in dateCol to filter dataframe to\n    :returns: JSON {\n        data: [{col1: 0.123, col2: 0.123, index: 1},...,{col1: 0.123, col2: 0.123, index: N}],\n        stats: {\n        stats: {\n            correlated: 50,\n            only_in_s0: 1,\n            only_in_s1: 2,\n            pearson: 0.987,\n            spearman: 0.879,\n        }\n        x: col1,\n        y: col2\n    } or {error: 'Exception message', traceback: 'Exception stacktrace'}\n    \"\"\"\n    cols = get_json_arg(request, \"cols\")\n    dummy_cols = get_json_arg(request, \"dummyCols\", [])\n    date_index = get_int_arg(request, \"index\")\n    date_col = get_str_arg(request, \"dateCol\")\n    rolling = get_bool_arg(request, \"rolling\")\n\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n    )\n    idx_col = str(\"_corr_index\")\n    y_cols = [cols[1], idx_col]\n    code = build_code_export(data_id)\n    selected_date = None\n    if rolling:\n        data = data[[c for c in data.columns if c in [date_col] + cols + dummy_cols]]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        window = get_int_arg(request, \"window\")\n        min_periods = get_int_arg(request, \"minPeriods\", default=0)\n        dates = data[date_col].sort_values().unique()\n        date_index = min(date_index + max(min_periods - 1, 1), len(dates) - 1)\n        selected_date = dates[date_index]\n        idx = min(data[data[date_col] == selected_date].index) + 1\n        selected_date = json_date(selected_date, nan_display=None)\n        selected_date = \"{} thru {}\".format(\n            json_date(dates[max(date_index - (window - 1), 0)]), selected_date\n        )\n        data = data.iloc[max(idx - window, 0) : idx]\n        data = data[cols + [date_col]].dropna(how=\"any\")\n        y_cols.append(date_col)\n        code.append(\n            (\n                \"idx = min(df[df['{date_col}'] == '{date}'].index) + 1\\n\"\n                \"scatter_data = scatter_data.iloc[max(idx - {window}, 0):idx]\\n\"\n                \"scatter_data = scatter_data['{cols}'].dropna(how='any')\"\n            ).format(\n                date_col=date_col,\n                date=selected_date,\n                window=window,\n                cols=\"', '\".join(sorted(list(set(cols)) + [date_col])),\n            )\n        )\n    else:\n        selected_cols = (\n            ([date_col] if date_index is not None else []) + cols + dummy_cols\n        )\n        data = data[[c for c in data.columns if c in selected_cols]]\n        data = update_df_for_encoded_strings(data, dummy_cols, cols, code)\n        if date_index is not None:\n            selected_date = data[date_col].sort_values().unique()[date_index]\n            data = data[data[date_col] == selected_date]\n            selected_date = json_date(selected_date, nan_display=None)\n        data = data[cols].dropna(how=\"any\")\n        code.append(\n            (\n                \"scatter_data = df[df['{date_col}'] == '{date}']\"\n                if date_index is not None\n                else \"scatter_data = df\"\n            ).format(date_col=date_col, date=selected_date)\n        )\n        code.append(\n            \"scatter_data = scatter_data['{cols}'].dropna(how='any')\".format(\n                cols=\"', '\".join(cols)\n            )\n        )\n\n    data[idx_col] = data.index\n    [col1, col2] = cols\n    s0 = data[col1]\n    s1 = data[col2]\n    pearson = s0.corr(s1, method=\"pearson\")\n    spearman = s0.corr(s1, method=\"spearman\")\n    pps = get_ppscore(data, col1, col2)\n    stats = dict(\n        pearson=\"N/A\" if pd.isnull(pearson) else pearson,\n        spearman=\"N/A\" if pd.isnull(spearman) else spearman,\n        pps=pps,\n        correlated=len(data),\n        only_in_s0=len(data[data[col1].isnull()]),\n        only_in_s1=len(data[data[col2].isnull()]),\n    )\n    code.append(\n        (\n            \"scatter_data['{idx_col}'] = scatter_data.index\\n\"\n            \"s0 = scatter_data['{col1}']\\n\"\n            \"s1 = scatter_data['{col2}']\\n\"\n            \"pearson = s0.corr(s1, method='pearson')\\n\"\n            \"spearman = s0.corr(s1, method='spearman')\\n\"\n            \"\\nimport ppscore\\n\\n\"\n            \"pps = ppscore.score(data, '{col1}', '{col2}')\\n\"\n            \"only_in_s0 = len(scatter_data[scatter_data['{col1}'].isnull()])\\n\"\n            \"only_in_s1 = len(scatter_data[scatter_data['{col2}'].isnull()])\"\n        ).format(col1=col1, col2=col2, idx_col=idx_col)\n    )\n\n    max_points = global_state.get_chart_settings()[\"scatter_points\"]\n    if len(data) > max_points:\n        return jsonify(\n            stats=stats,\n            code=\"\\n\".join(code),\n            error=\"Dataset exceeds {:,} records, cannot render scatter. Please apply filter...\".format(\n                max_points\n            ),\n            traceback=CHART_POINTS_LIMIT,\n        )\n    data, _code = build_base_chart(data, cols[0], y_cols, allow_duplicates=True)\n    data[\"x\"] = cols[0]\n    data[\"y\"] = cols[1]\n    data[\"stats\"] = stats\n    data[\"code\"] = \"\\n\".join(code)\n    data[\"date\"] = \" for {}\".format(selected_date) if selected_date else \"\"\n    return jsonify(data)\n\n\ndef build_context_variables(data_id, new_context_vars=None):\n    \"\"\"\n    Build and return the dictionary of context variables associated with a process.\n    If the names of any new variables are not formatted properly, an exception will be raised.\n    New variables will overwrite the values of existing variables if they share the same name.\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :param new_context_vars: dictionary of name, value pairs for new context variables\n    :type new_context_vars: dict, optional\n    :returns: dict of the context variables for this process\n    :rtype: dict\n    \"\"\"\n    if new_context_vars:\n        for name, value in new_context_vars.items():\n            if not isinstance(name, string_types):\n                raise SyntaxError(\n                    \"{}, context variables must be a valid string\".format(name)\n                )\n            elif not name.replace(\"_\", \"\").isalnum():\n                raise SyntaxError(\n                    \"{}, context variables can only contain letters, digits, or underscores\".format(\n                        name\n                    )\n                )\n            elif name.startswith(\"_\"):\n                raise SyntaxError(\n                    \"{}, context variables can not start with an underscore\".format(\n                        name\n                    )\n                )\n\n    return dict_merge(global_state.get_context_variables(data_id), new_context_vars)\n\n\n@dtale.route(\"/filter-info/<data_id>\")\n@exception_decorator\ndef get_filter_info(data_id):\n    \"\"\"\n    :class:`flask:flask.Flask` route which returns a view-only version of the query, column filters & context variables\n    to the front end.\n\n    :param data_id: integer string identifier for a D-Tale process's data\n    :type data_id: str\n    :return: JSON\n    \"\"\"\n\n    def value_as_str(value):\n        \"\"\"Convert values into a string representation that can be shown to the user in the front-end.\"\"\"\n        return str(value)[:1000]\n\n    ctxt_vars = global_state.get_context_variables(data_id) or {}\n    ctxt_vars = [dict(name=k, value=value_as_str(v)) for k, v in ctxt_vars.items()]\n    curr_settings = global_state.get_settings(data_id) or {}\n    curr_settings = {\n        k: v\n        for k, v in curr_settings.items()\n        if k\n        in [\n            \"query\",\n            \"columnFilters\",\n            \"outlierFilters\",\n            \"predefinedFilters\",\n            \"invertFilter\",\n            \"highlightFilter\",\n        ]\n    }\n    return jsonify(contextVars=ctxt_vars, success=True, **curr_settings)\n\n\n@dtale.route(\"/xarray-coordinates/<data_id>\")\n@exception_decorator\ndef get_xarray_coords(data_id):\n    ds = global_state.get_dataset(data_id)\n\n    def _format_dim(coord, count):\n        return dict(name=coord, count=count, dtype=ds.coords[coord].dtype.name)\n\n    coord_data = [_format_dim(coord, count) for coord, count in ds.coords.dims.items()]\n    return jsonify(data=coord_data)\n\n\n@dtale.route(\"/xarray-dimension-values/<data_id>/<dim>\")\n@exception_decorator\ndef get_xarray_dimension_values(data_id, dim):\n    ds = global_state.get_dataset(data_id)\n    dim_entries = ds.coords[dim].data\n    dim = pd.DataFrame({\"value\": dim_entries})\n    dim_f, _ = build_formatters(dim)\n    return jsonify(data=dim_f.format_dicts(dim.itertuples()))\n\n\n@dtale.route(\"/update-xarray-selection/<data_id>\")\n@exception_decorator\ndef update_xarray_selection(data_id):\n    ds = global_state.get_dataset(data_id)\n    selection = get_json_arg(request, \"selection\") or {}\n    df = convert_xarray_to_dataset(ds, **selection)\n    startup(data=df, data_id=data_id, ignore_duplicate=True)\n    global_state.set_dataset_dim(data_id, selection)\n    return jsonify(success=True)\n\n\n@dtale.route(\"/to-xarray/<data_id>\")\n@exception_decorator\ndef to_xarray(data_id):\n    df = global_state.get_data(data_id)\n    index_cols = get_json_arg(request, \"index\")\n    ds = df.set_index(index_cols).to_xarray()\n    startup(data=ds, data_id=data_id, ignore_duplicate=True)\n    curr_settings = global_state.get_settings(data_id)\n    startup_code = \"df = df.set_index(['{index}']).to_xarray()\".format(\n        index=\"', '\".join(index_cols)\n    )\n    global_state.set_settings(\n        data_id, dict_merge(curr_settings, dict(startup_code=startup_code))\n    )\n    return jsonify(success=True)\n\n\n@dtale.route(\"/code-export/<data_id>\")\n@exception_decorator\ndef get_code_export(data_id):\n    code = build_code_export(data_id)\n    return jsonify(code=\"\\n\".join(code), success=True)\n\n\ndef build_chart_filename(chart_type, ext=\"html\"):\n    return \"{}_export_{}.{}\".format(\n        chart_type, json_timestamp(pd.Timestamp(\"now\")), ext\n    )\n\n\ndef send_file(output, filename, content_type):\n    resp = make_response(output)\n    resp.headers[\"Content-Disposition\"] = \"attachment; filename=%s\" % filename\n    resp.headers[\"Content-Type\"] = content_type\n    return resp\n\n\n@dtale.route(\"/chart-export/<data_id>\")\n@exception_decorator\ndef chart_export(data_id):\n    export_type = get_str_arg(request, \"export_type\")\n    params = chart_url_params(request.args.to_dict())\n    if export_type == \"png\":\n        output = export_png(data_id, params)\n        filename = build_chart_filename(params[\"chart_type\"], ext=\"png\")\n        content_type = \"image/png\"\n    else:\n        output = export_chart(data_id, params)\n        filename = build_chart_filename(params[\"chart_type\"])\n        content_type = \"text/html\"\n    return send_file(output, filename, content_type)\n\n\n@dtale.route(\"/chart-export-all/<data_id>\")\n@exception_decorator\ndef chart_export_all(data_id):\n    params = chart_url_params(request.args.to_dict())\n    params[\"export_all\"] = True\n    output = export_chart(data_id, params)\n    filename = build_chart_filename(params[\"chart_type\"])\n    content_type = \"text/html\"\n    return send_file(output, filename, content_type)\n\n\n@dtale.route(\"/chart-csv-export/<data_id>\")\n@exception_decorator\ndef chart_csv_export(data_id):\n    params = chart_url_params(request.args.to_dict())\n    csv_buffer = export_chart_data(data_id, params)\n    filename = build_chart_filename(params[\"chart_type\"], ext=\"csv\")\n    return send_file(csv_buffer.getvalue(), filename, \"text/csv\")\n\n\n@dtale.route(\"/cleanup-datasets\")\n@exception_decorator\ndef cleanup_datasets():\n    data_ids = get_str_arg(request, \"dataIds\")\n    data_ids = (data_ids or \"\").split(\",\")\n    for data_id in data_ids:\n        global_state.cleanup(data_id)\n    return jsonify(success=True)\n\n\ndef load_new_data(df, startup_code, name=None, return_id=False, settings=None):\n    instance = startup(data=df, name=name, ignore_duplicate=True)\n    curr_settings = global_state.get_settings(instance._data_id)\n    global_state.set_settings(\n        instance._data_id,\n        dict_merge(curr_settings, dict(startup_code=startup_code, **(settings or {}))),\n    )\n    if return_id:\n        return instance._data_id\n    return jsonify(success=True, data_id=instance._data_id)\n\n\ndef handle_excel_upload(dfs):\n    sheet_names = list(dfs.keys())\n    data_ids = []\n    for sheet_name in sheet_names:\n        df, code = dfs[sheet_name]\n        if len(sheet_names) == 1:\n            return load_new_data(df, code, name=sheet_name)\n        data_id = load_new_data(df, code, name=sheet_name, return_id=True)\n        data_ids.append(dict(name=sheet_name, dataId=data_id))\n    return jsonify(dict(sheets=data_ids, success=True))\n\n\nUPLOAD_SEPARATORS = {\"comma\": \",\", \"tab\": \"\\t\", \"colon\": \":\", \"pipe\": \"|\"}\n\n\ndef build_csv_kwargs(request):\n    # Set engine to python to auto detect delimiter...\n    kwargs = {\"sep\": None}\n    sep_type = request.form.get(\"separatorType\")\n\n    if sep_type in UPLOAD_SEPARATORS:\n        kwargs[\"sep\"] = UPLOAD_SEPARATORS[sep_type]\n    elif sep_type == \"custom\" and request.form.get(\"separator\"):\n        kwargs[\"sep\"] = request.form[\"separator\"]\n        kwargs[\"sep\"] = str(kwargs[\"sep\"]) if PY3 else kwargs[\"sep\"].encode(\"utf8\")\n\n    if \"header\" in request.form:\n        kwargs[\"header\"] = 0 if request.form[\"header\"] == \"true\" else None\n\n    return kwargs\n\n\n@dtale.route(\"/upload\", methods=[\"POST\"])\n@exception_decorator\ndef upload():\n    if not request.files:\n        raise Exception(\"No file data loaded!\")\n    for filename in request.files:\n        contents = request.files[filename]\n        _, ext = os.path.splitext(filename)\n        if ext in [\".csv\", \".tsv\"]:\n            kwargs = build_csv_kwargs(request)\n            df = pd.read_csv(\n                StringIO(contents.read().decode()), engine=\"python\", **kwargs\n            )\n            return load_new_data(\n                df, \"df = pd.read_csv('{}', engine='python', sep=None)\".format(filename)\n            )\n        if ext in [\".xls\", \".xlsx\"]:\n            engine = \"xlrd\" if ext == \".xls\" else \"openpyxl\"\n            dfs = pd.read_excel(contents, sheet_name=None, engine=engine)\n\n            def build_xls_code(sheet_name):\n                return \"df = pd.read_excel('{}', sheet_name='{}', engine='{}')\".format(\n                    filename, sheet_name, engine\n                )\n\n            dfs = {\n                sheet_name: (df, build_xls_code(sheet_name))\n                for sheet_name, df in dfs.items()\n            }\n            return handle_excel_upload(dfs)\n        if \"parquet\" in filename:\n            df = pd.read_parquet(contents)\n            return load_new_data(df, \"df = pd.read_parquet('{}')\".format(filename))\n        raise Exception(\"File type of {} is not supported!\".format(ext))\n\n\n@dtale.route(\"/web-upload\")\n@exception_decorator\ndef web_upload():\n    from dtale.cli.loaders.csv_loader import loader_func as load_csv\n    from dtale.cli.loaders.json_loader import loader_func as load_json\n    from dtale.cli.loaders.excel_loader import load_file as load_excel\n    from dtale.cli.loaders.parquet_loader import loader_func as load_parquet\n\n    data_type = get_str_arg(request, \"type\")\n    url = get_str_arg(request, \"url\")\n    proxy = get_str_arg(request, \"proxy\")\n    if data_type == \"csv\":\n        df = load_csv(path=url, proxy=proxy)\n        startup_code = (\n            \"from dtale.cli.loaders.csv_loader import loader_func as load_csv\\n\\n\"\n            \"df = load_csv(path='{url}'{proxy})\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    elif data_type == \"tsv\":\n        df = load_csv(path=url, proxy=proxy, delimiter=\"\\t\")\n        startup_code = (\n            \"from dtale.cli.loaders.csv_loader import loader_func as load_csv\\n\\n\"\n            \"df = load_csv(path='{url}'{proxy}, delimiter='\\t')\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    elif data_type == \"json\":\n        df = load_json(path=url, proxy=proxy)\n        startup_code = (\n            \"from dtale.cli.loaders.json_loader import loader_func as load_json\\n\\n\"\n            \"df = load_json(path='{url}'{proxy})\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    elif data_type == \"excel\":\n        dfs = load_excel(path=url, proxy=proxy)\n\n        def build_xls_code(sheet_name):\n            return (\n                \"from dtale.cli.loaders.excel_loader import load_file as load_excel\\n\\n\"\n                \"df = load_excel(sheet_name='{sheet_name}', path='{url}'{proxy})\"\n            ).format(\n                sheet_name=sheet_name,\n                url=url,\n                proxy=\", '{}'\".format(proxy) if proxy else \"\",\n            )\n\n        dfs = {\n            sheet_name: (df, build_xls_code(sheet_name))\n            for sheet_name, df in dfs.items()\n        }\n        return handle_excel_upload(dfs)\n    elif data_type == \"parquet\":\n        df = load_parquet(path=url)\n        startup_code = (\n            \"from dtale.cli.loaders.parquet_loader import loader_func as load_parquet\\n\\n\"\n            \"df = load_parquet(path='{url}'{proxy})\"\n        ).format(url=url, proxy=\", '{}'\".format(proxy) if proxy else \"\")\n    return load_new_data(df, startup_code)\n\n\n@dtale.route(\"/datasets\")\n@exception_decorator\ndef dataset_upload():\n    dataset = get_str_arg(request, \"dataset\")\n    startup_code = \"from dtale.datasets import {dataset}\\n\\n\" \"df = {dataset}()\".format(\n        dataset=dataset\n    )\n    df, settings = getattr(datasets, dataset)()\n    return load_new_data(df, startup_code, settings=settings)\n\n\n@dtale.route(\"/build-column-copy/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef build_column_text(data_id):\n    columns = request.json.get(\"columns\")\n    columns = json.loads(columns)\n\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    return data[columns].to_csv(index=False, sep=\"\\t\", header=False)\n\n\n@dtale.route(\"/build-row-copy/<data_id>\", methods=[\"POST\"])\n@exception_decorator\ndef build_row_text(data_id):\n    start, end, rows, columns = (\n        request.json.get(p) for p in [\"start\", \"end\", \"rows\", \"columns\"]\n    )\n    columns = json.loads(columns)\n    curr_settings = global_state.get_settings(data_id) or {}\n    data = run_query(\n        handle_predefined(data_id),\n        build_query(data_id, curr_settings.get(\"query\")),\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    if rows:\n        rows = json.loads(rows)\n        data = data.iloc[rows, :]\n    else:\n        start = int(start)\n        end = int(end)\n        data = data.iloc[(start - 1) : end, :]\n    return data[columns].to_csv(index=False, sep=\"\\t\", header=False)\n\n\n@dtale.route(\"/network-data/<data_id>\")\n@exception_decorator\ndef network_data(data_id):\n    df = global_state.get_data(data_id)\n    to_col = get_str_arg(request, \"to\")\n    from_col = get_str_arg(request, \"from\")\n    group = get_str_arg(request, \"group\", \"\")\n    color = get_str_arg(request, \"color\", \"\")\n    weight = get_str_arg(request, \"weight\")\n\n    nodes = list(df[to_col].unique())\n    nodes += list(df[~df[from_col].isin(nodes)][from_col].unique())\n    nodes = sorted(nodes)\n    nodes = {node: node_id for node_id, node in enumerate(nodes, 1)}\n\n    edge_cols = [to_col, from_col]\n    if weight:\n        edge_cols.append(weight)\n\n    edges = df[[to_col, from_col]].applymap(nodes.get)\n    edges.columns = [\"to\", \"from\"]\n    if weight:\n        edges.loc[:, \"value\"] = df[weight]\n    edge_f = grid_formatter(grid_columns(edges), nan_display=\"nan\")\n    edges = edge_f.format_dicts(edges.itertuples())\n\n    def build_mapping(col):\n        if col:\n            return df[[from_col, col]].set_index(from_col)[col].astype(\"str\").to_dict()\n        return {}\n\n    group = build_mapping(group)\n    color = build_mapping(color)\n    groups = {}\n\n    def build_group(node, node_id):\n        group_val = group.get(node, \"N/A\")\n        groups[group_val] = node_id\n        return group_val\n\n    nodes = [\n        dict(\n            id=node_id,\n            label=node,\n            group=build_group(node, node_id),\n            color=color.get(node),\n        )\n        for node, node_id in nodes.items()\n    ]\n    return jsonify(dict(nodes=nodes, edges=edges, groups=groups, success=True))\n\n\n@dtale.route(\"/network-analysis/<data_id>\")\n@exception_decorator\ndef network_analysis(data_id):\n    df = global_state.get_data(data_id)\n    to_col = get_str_arg(request, \"to\")\n    from_col = get_str_arg(request, \"from\")\n    weight = get_str_arg(request, \"weight\")\n\n    G = nx.Graph()\n    max_edge, min_edge, avg_weight = (None, None, None)\n    if weight:\n        G.add_weighted_edges_from(\n            [tuple(x) for x in df[[to_col, from_col, weight]].values]\n        )\n        sorted_edges = sorted(\n            G.edges(data=True), key=lambda x: x[2][\"weight\"], reverse=True\n        )\n        max_edge = sorted_edges[0]\n        min_edge = sorted_edges[-1]\n        avg_weight = df[weight].mean()\n    else:\n        G.add_edges_from([tuple(x) for x in df[[to_col, from_col]].values])\n\n    most_connected_node = max(dict(G.degree()).items(), key=lambda x: x[1])\n    return_data = {\n        \"node_ct\": len(G),\n        \"triangle_ct\": int(sum(nx.triangles(G).values()) / 3),\n        \"most_connected_node\": \"{} (Connections: {})\".format(*most_connected_node),\n        \"leaf_ct\": sum((1 for edge, degree in dict(G.degree()).items() if degree == 1)),\n        \"edge_ct\": sum(dict(G.degree()).values()),\n        \"max_edge\": None\n        if max_edge is None\n        else \"{} (source: {}, target: {})\".format(\n            max_edge[-1][\"weight\"], max_edge[0], max_edge[1]\n        ),\n        \"min_edge\": None\n        if min_edge is None\n        else \"{} (source: {}, target: {})\".format(\n            min_edge[-1][\"weight\"], min_edge[0], min_edge[1]\n        ),\n        \"avg_weight\": json_float(avg_weight),\n    }\n    return jsonify(dict(data=return_data, success=True))\n\n\n@dtale.route(\"/shortest-path/<data_id>\")\n@exception_decorator\ndef shortest_path(data_id):\n    df = global_state.get_data(data_id)\n    to_col = get_str_arg(request, \"to\")\n    from_col = get_str_arg(request, \"from\")\n    start_val = get_str_arg(request, \"start\")\n    end_val = get_str_arg(request, \"end\")\n\n    G = nx.Graph()\n    G.add_edges_from([tuple(x) for x in df[[to_col, from_col]].values])\n    shortest_path = nx.shortest_path(G, source=start_val, target=end_val)\n    return jsonify(dict(data=shortest_path, success=True))\n\n\n@dtale.route(\"/sorted-sequential-diffs/<data_id>\")\n@exception_decorator\ndef get_sorted_sequential_diffs(data_id):\n    column = get_str_arg(request, \"col\")\n    sort = get_str_arg(request, \"sort\")\n    df = global_state.get_data(data_id)\n    metrics, _ = build_sequential_diffs(df[column], column, sort=sort)\n    return jsonify(metrics)\n\n\n@dtale.route(\"/merge\", methods=[\"POST\"])\n@exception_decorator\ndef build_merge():\n    cfg = request.json\n    name = cfg.get(\"name\")\n    builder = CombineData(cfg)\n    data = builder.build_data()\n    code = builder.build_code()\n    return load_new_data(data, code, name)\n\n\n@dtale.route(\"/missingno/<chart_type>/<data_id>\")\n@matplotlib_decorator\n@exception_decorator\ndef build_missingno_chart(chart_type, data_id):\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\n    df = global_state.get_data(data_id)\n    if chart_type == \"matrix\":\n        date_index = get_str_arg(request, \"date_index\")\n        freq = get_str_arg(request, \"freq\")\n        if date_index:\n            figure = msno.matrix(df.set_index(date_index), freq=freq)\n        else:\n            figure = msno.matrix(df)\n    elif chart_type == \"bar\":\n        figure = msno.bar(df)\n    elif chart_type == \"heatmap\":\n        figure = msno.heatmap(df)\n    elif chart_type == \"dendrogram\":\n        figure = msno.dendrogram(df)\n\n    output = BytesIO()\n    FigureCanvas(figure.get_figure()).print_png(output)\n    if get_bool_arg(request, \"file\"):\n        fname = \"missingno_{}.png\".format(chart_type)\n        return send_file(output.getvalue(), fname, \"image/png\")\n    return Response(output.getvalue(), mimetype=\"image/png\")\n\n\n@dtale.route(\"/drop-filtered-rows/<data_id>\")\n@exception_decorator\ndef drop_filtered_rows(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    final_query = build_query(data_id, curr_settings.get(\"query\"))\n    curr_history = global_state.get_history(data_id) or []\n    curr_history += [\n        (\n            \"# drop filtered rows\\n\"\n            'df = df.query(\"{}\")'.format(final_query.replace(\"`\", \"\"))\n        )\n    ]\n    global_state.set_history(data_id, curr_history)\n    data = run_query(\n        handle_predefined(data_id),\n        final_query,\n        global_state.get_context_variables(data_id),\n        ignore_empty=True,\n    )\n    global_state.set_data(data_id, data)\n    global_state.set_dtypes(data_id, build_dtypes_state(data, []))\n    curr_predefined = curr_settings.get(\"predefinedFilters\", {})\n    global_state.update_settings(\n        data_id,\n        dict(\n            query=\"\",\n            columnFilters={},\n            outlierFilters={},\n            predefinedFilters={\n                k: dict_merge(v, {\"active\": False}) for k, v in curr_predefined.items()\n            },\n            invertFilter=False,\n        ),\n    )\n    return jsonify(dict(success=True))\n\n\n@dtale.route(\"/move-filters-to-custom/<data_id>\")\n@exception_decorator\ndef move_filters_to_custom(data_id):\n    curr_settings = global_state.get_settings(data_id) or {}\n    query = build_query(data_id, curr_settings.get(\"query\"))\n    global_state.update_settings(\n        data_id,\n        {\n            \"columnFilters\": {},\n            \"outlierFilters\": {},\n            \"invertFilter\": False,\n            \"query\": query,\n        },\n    )\n    return jsonify(\n        dict(success=True, settings=global_state.get_settings(data_id) or {})\n    )\n\n\n@dtale.route(\"/gage-rnr/<data_id>\")\n@exception_decorator\ndef build_gage_rnr(data_id):\n    data = load_filterable_data(data_id, request)\n    operator_cols = get_json_arg(request, \"operator\")\n    operators = data.groupby(operator_cols)\n    measurements = get_json_arg(request, \"measurements\") or [\n        col for col in data.columns if col not in operator_cols\n    ]\n    sizes = set((len(operator) for _, operator in operators))\n    if len(sizes) > 1:\n        return jsonify(\n            dict(\n                success=False,\n                error=(\n                    \"Operators do not have the same amount of parts! Please select a new operator with equal rows in \"\n                    \"each group.\"\n                ),\n            )\n        )\n\n    res = gage_rnr.GageRnR(\n        np.array([operator[measurements].values for _, operator in operators])\n    ).calculate()\n    df = pd.DataFrame({name: res[name] for name in gage_rnr.ResultNames})\n    df.index = df.index.map(lambda idx: gage_rnr.ComponentNames.get(idx, idx))\n    df.index.name = \"Sources of Variance\"\n    df.columns = [gage_rnr.ResultNames.get(col, col) for col in df.columns]\n    df = df[df.index.isin(df.index.dropna())]\n    df = df.reset_index()\n    overrides = {\"F\": lambda f, i, c: f.add_float(i, c, precision=3)}\n    return jsonify(dict_merge(dict(success=True), format_grid(df, overrides)))\n\n\n@dtale.route(\"/timeseries-analysis/<data_id>\")\n@exception_decorator\ndef get_timeseries_analysis(data_id):\n    report_type = get_str_arg(request, \"type\")\n    cfg = json.loads(get_str_arg(request, \"cfg\"))\n    ts_rpt = TimeseriesAnalysis(data_id, report_type, cfg)\n    data = ts_rpt.run()\n    return jsonify(dict_merge(dict(success=True), data))\n\n\n@dtale.route(\"/arcticdb/libraries\")\n@exception_decorator\ndef get_arcticdb_libraries():\n    if get_bool_arg(request, \"refresh\"):\n        global_state.store.load_libraries()\n\n    libraries = global_state.store.libraries\n    is_async = False\n    if len(libraries) > 500:\n        is_async = True\n        libraries = libraries[:5]\n    ret_data = {\"success\": True, \"libraries\": libraries, \"async\": is_async}\n    if global_state.store.lib is not None:\n        ret_data[\"library\"] = global_state.store.lib.name\n    return jsonify(ret_data)\n\n\n@dtale.route(\"/arcticdb/async-libraries\")\n@exception_decorator\ndef get_async_arcticdb_libraries():\n    libraries = global_state.store.libraries\n    input = get_str_arg(request, \"input\")\n    vals = list(\n        itertools.islice((option(lib) for lib in libraries if lib.startswith(input)), 5)\n    )\n    return jsonify(vals)\n\n\n@dtale.route(\"/arcticdb/<library>/symbols\")\n@exception_decorator\ndef get_arcticdb_symbols(library):\n    if get_bool_arg(request, \"refresh\") or library not in global_state.store._symbols:\n        global_state.store.load_symbols(library)\n\n    symbols = global_state.store._symbols[library]\n    is_async = False\n    if len(symbols) > 500:\n        is_async = True\n        symbols = symbols[:5]\n    return jsonify({\"success\": True, \"symbols\": symbols, \"async\": is_async})\n\n\n@dtale.route(\"/arcticdb/<library>/async-symbols\")\n@exception_decorator\ndef get_async_arcticdb_symbols(library):\n    symbols = global_state.store._symbols[library]\n    input = get_str_arg(request, \"input\")\n    vals = list(\n        itertools.islice((option(sym) for sym in symbols if sym.startswith(input)), 5)\n    )\n    return jsonify(vals)\n\n\n@dtale.route(\"/arcticdb/load-description\")\n@exception_decorator\ndef load_arcticdb_description():\n    from arcticc.pb2.descriptors_pb2 import _TYPEDESCRIPTOR_VALUETYPE\n\n    library = get_str_arg(request, \"library\")\n    symbol = get_str_arg(request, \"symbol\")\n\n    lib = global_state.store.conn[library]\n    description = lib.get_description(symbol)\n\n    columns = list(\n        map(\n            lambda c: \"{} ({})\".format(\n                c.name, _TYPEDESCRIPTOR_VALUETYPE.values[c.dtype.value_type].name\n            ),\n            sorted(description.columns, key=lambda c: c.name),\n        )\n    )\n    index = list(\n        map(\n            lambda i: \"{} ({})\".format(\n                i[0], _TYPEDESCRIPTOR_VALUETYPE.values[i[1].value_type].name\n            ),\n            zip(description.index.name, description.index.dtype),\n        )\n    )\n    rows = description.row_count\n\n    description_str = (\n        \"ROWS: {rows:,.0f}\\n\"\n        \"INDEX:\\n\"\n        \"\\t- {index}\\n\"\n        \"COLUMNS ({col_count}):\\n\"\n        \"\\t- {columns}\\n\"\n    ).format(\n        rows=rows,\n        index=\"\\n\\t- \".join(index),\n        col_count=len(columns),\n        columns=\"\\n\\t- \".join(columns),\n    )\n    return jsonify(\n        dict(success=True, library=library, symbol=symbol, description=description_str)\n    )\n\n\n@dtale.route(\"/arcticdb/load-symbol\")\n@exception_decorator\ndef load_arcticdb_symbol():\n    library = get_str_arg(request, \"library\")\n    symbol = get_str_arg(request, \"symbol\")\n    data_id = \"{}|{}\".format(library, symbol)\n\n    if not global_state.store.lib or global_state.store.lib.name != library:\n        global_state.store.update_library(library)\n\n    startup(data=data_id)\n    startup_code = (\n        \"from arcticdb import Arctic\\n\"\n        \"from arcticdb.version_store._store import VersionedItem\\n\\n\"\n        \"conn = Arctic('{uri}')\\n\"\n        \"lib = conn.get_library('{library}')\\n\"\n        \"df = lib.read('{symbol}')\\n\"\n        \"if isinstance(data, VersionedItem):\\n\"\n        \"\\tdf = df.data\\n\"\n    ).format(uri=global_state.store.uri, library=library, symbol=symbol)\n    curr_settings = global_state.get_settings(data_id)\n    global_state.set_settings(\n        data_id, dict_merge(curr_settings, dict(startup_code=startup_code))\n    )\n    return dict(success=True, data_id=data_id)\n", "import { act, fireEvent, render, screen } from '@testing-library/react';\nimport axios from 'axios';\nimport * as React from 'react';\nimport { Provider } from 'react-redux';\nimport { Store } from 'redux';\n\nimport { DataViewer } from '../../dtale/DataViewer';\nimport { DISABLED_CUSTOM_FILTERS_MSG } from '../../popups/filter/FilterPopup';\nimport DimensionsHelper from '../DimensionsHelper';\nimport reduxUtils from '../redux-test-utils';\nimport { buildInnerHTML, clickMainMenuButton, mockChartJS } from '../test-utils';\n\ndescribe('FilterPanel', () => {\n  let container: Element;\n  let store: Store;\n  const { open } = window;\n  const dimensions = new DimensionsHelper({\n    offsetWidth: 500,\n    offsetHeight: 500,\n    innerWidth: 1205,\n    innerHeight: 775,\n  });\n  const openFn = jest.fn();\n\n  beforeAll(() => {\n    dimensions.beforeAll();\n\n    delete (window as any).open;\n    window.open = openFn;\n\n    mockChartJS();\n  });\n\n  beforeEach(async () => {\n    (axios.get as any).mockImplementation((url: string) => Promise.resolve({ data: reduxUtils.urlFetcher(url) }));\n  });\n\n  afterEach(jest.resetAllMocks);\n\n  afterAll(() => {\n    dimensions.afterAll();\n    window.open = open;\n    jest.restoreAllMocks();\n  });\n\n  const toggleFilterMenu = async (): Promise<void> => {\n    await clickMainMenuButton('Custom Filter');\n  };\n\n  const buildResult = async (dataId = '1', overrides?: Record<string, string>): Promise<void> => {\n    store = reduxUtils.createDtaleStore();\n    buildInnerHTML({ settings: '', dataId, enableCustomFilters: 'True', ...overrides }, store);\n    await act(() => {\n      const result = render(\n        <Provider store={store}>\n          <DataViewer />\n        </Provider>,\n        {\n          container: document.getElementById('content') ?? undefined,\n        },\n      );\n      container = result.container;\n    });\n    await toggleFilterMenu();\n  };\n\n  const clickFilterBtn = async (text: string): Promise<void> => {\n    await act(async () => {\n      const buttons = screen.getByTestId('filter-panel').getElementsByTagName('button');\n      fireEvent.click([...buttons].find((b) => b.textContent === text)!);\n    });\n  };\n\n  it('DataViewer: filtering', async () => {\n    await buildResult();\n    expect(screen.getByTestId('filter-panel')).toBeDefined();\n    await clickFilterBtn('Close');\n    expect(screen.queryByTestId('filter-panel')).toBeNull();\n    await toggleFilterMenu();\n    await clickFilterBtn('Clear');\n    expect(screen.queryByTestId('filter-panel')).toBeNull();\n    await toggleFilterMenu();\n    await clickFilterBtn('numexpr');\n    expect(store.getState().queryEngine).toBe('numexpr');\n    await act(async () => {\n      await fireEvent.click(screen.getByText('Highlight Filtered Rows').parentElement?.getElementsByTagName('i')[0]!);\n    });\n    expect(store.getState().settings.highlightFilter).toBe(true);\n    await act(async () => {\n      const textarea = screen.getByTestId('filter-panel').getElementsByTagName('textarea')[0];\n      fireEvent.change(textarea, { target: { value: 'test' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(container.getElementsByClassName('data-viewer-info')[0].textContent).toBe('Filter:test');\n    await act(async () => {\n      const cancelIcons = container.getElementsByClassName('data-viewer-info')[0].getElementsByClassName('ico-cancel');\n      fireEvent.click(cancelIcons[cancelIcons.length - 1]);\n    });\n    expect(\n      container.getElementsByClassName('data-viewer-info')[0].querySelectorAll('div.data-viewer-info.is-expanded')\n        .length,\n    ).toBe(0);\n  });\n\n  it('DataViewer: filtering with errors & documentation', async () => {\n    await buildResult();\n    await act(async () => {\n      const textarea = screen.getByTestId('filter-panel').getElementsByTagName('textarea')[0];\n      fireEvent.change(textarea, { target: { value: 'error' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(container.getElementsByClassName('dtale-alert')[0].textContent).toBe('No data found');\n    await act(async () => {\n      const filterPanel = screen.getByTestId('filter-panel');\n      const error = filterPanel.getElementsByClassName('dtale-alert')[0];\n      fireEvent.click(error.getElementsByClassName('ico-cancel')[0]);\n    });\n    expect(screen.getByTestId('filter-panel').getElementsByClassName('dtale-alert').length).toBe(0);\n    await clickFilterBtn('Help');\n    const pandasURL = 'https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-query';\n    expect(openFn.mock.calls[openFn.mock.calls.length - 1][0]).toBe(pandasURL);\n  });\n\n  it('DataViewer: filtering, context variables error', async () => {\n    await buildResult('error');\n    expect(screen.getByTestId('filter-panel').getElementsByClassName('dtale-alert')[0].textContent).toBe(\n      'Error loading context variables',\n    );\n  });\n\n  it('DataViewer: column filters', async () => {\n    await buildResult();\n    expect(screen.getByTestId('structured-filters').textContent).toBe('Active Column Filters:foo == 1 and');\n    await act(async () => {\n      fireEvent.click(screen.getByTestId('structured-filters').getElementsByClassName('ico-cancel')[0]);\n    });\n    expect(screen.queryByTestId('structured-filters')).toBeNull();\n  });\n\n  it('DataViewer: filtering with custom filtering not enabled', async () => {\n    await buildResult('1', { enableCustomFilters: 'False' });\n    const textarea = screen.getByTestId('filter-panel').getElementsByTagName('textarea')[0];\n    expect(textarea.value).toBe(DISABLED_CUSTOM_FILTERS_MSG);\n    expect(textarea).toBeDisabled();\n    const buttons = [...screen.getByTestId('filter-panel').querySelectorAll('button')];\n    expect(buttons.filter((b) => b.textContent === 'Apply')).toHaveLength(0);\n    expect(buttons.filter((b) => b.textContent === 'Clear')).toHaveLength(0);\n  });\n});\n", "import * as reduxUtils from '../../dtale/reduxGridUtils';\nimport { DataViewerUpdateType } from '../../redux/state/AppState';\nimport { mockColumnDef } from '../mocks/MockColumnDef';\n\ndescribe('reduxGridUtils', () => {\n  const propagateState = jest.fn();\n\n  afterEach(jest.resetAllMocks);\n\n  afterAll(jest.restoreAllMocks);\n\n  it('handles drop-columns', () => {\n    const clearDataViewerUpdate = jest.fn();\n    const columns = [mockColumnDef({ name: 'foo' }), mockColumnDef({ name: 'bar' })];\n    const settings = {\n      allow_cell_edits: true,\n      hide_shutdown: false,\n      precision: 2,\n      verticalHeaders: false,\n      predefinedFilters: {},\n      hide_header_editor: false,\n      lock_header_menu: false,\n      hide_header_menu: false,\n      hide_main_menu: false,\n      hide_column_menus: false,\n      enable_custom_filters: false,\n    };\n    reduxUtils.handleReduxState(\n      columns,\n      {},\n      1,\n      { type: DataViewerUpdateType.DROP_COLUMNS, columns: ['foo'] },\n      clearDataViewerUpdate,\n      propagateState,\n      settings,\n    );\n    expect(propagateState).toHaveBeenCalledWith({ columns: [columns[1]], triggerResize: true }, clearDataViewerUpdate);\n  });\n});\n", "import { act, fireEvent, render, screen } from '@testing-library/react';\nimport * as React from 'react';\nimport { Provider } from 'react-redux';\nimport { Store } from 'redux';\n\nimport * as serverState from '../../../dtale/serverStateManagement';\nimport FilterPopup from '../../../popups/filter/FilterPopup';\nimport { ActionType } from '../../../redux/actions/AppActions';\nimport * as chartActions from '../../../redux/actions/charts';\nimport { InstanceSettings, QueryEngine } from '../../../redux/state/AppState';\nimport * as CustomFilterRepository from '../../../repository/CustomFilterRepository';\nimport * as GenericRepository from '../../../repository/GenericRepository';\nimport reduxUtils from '../../redux-test-utils';\nimport { buildInnerHTML } from '../../test-utils';\n\ndescribe('FilterPopup', () => {\n  let wrapper: Element;\n  let store: Store;\n  let loadInfoSpy: jest.SpyInstance<Promise<CustomFilterRepository.LoadInfoResponse | undefined>, [string]>;\n  let saveFilterSpy: jest.SpyInstance<Promise<GenericRepository.BaseResponse | undefined>, [string, string]>;\n  let updateSettingsSpy: jest.SpyInstance<serverState.BaseReturn, [Partial<InstanceSettings>, string]>;\n  let updateQueryEngineSpy: jest.SpyInstance<serverState.BaseReturn, [QueryEngine]>;\n\n  beforeEach(async () => {\n    loadInfoSpy = jest.spyOn(CustomFilterRepository, 'loadInfo');\n    loadInfoSpy.mockResolvedValue({\n      query: 'foo == foo',\n      columnFilters: { 0: { query: 'bar == bar', type: 'col' } },\n      outlierFilters: { 0: { query: 'baz == baz' } },\n      predefinedFilters: {},\n      contextVars: [],\n      invertFilter: false,\n      highlightFilter: false,\n      success: true,\n    });\n    saveFilterSpy = jest.spyOn(CustomFilterRepository, 'save');\n    saveFilterSpy.mockResolvedValue({ success: true });\n    updateSettingsSpy = jest.spyOn(serverState, 'updateSettings');\n    updateSettingsSpy.mockResolvedValue(Promise.resolve({ success: true }));\n    updateQueryEngineSpy = jest.spyOn(serverState, 'updateQueryEngine');\n    updateQueryEngineSpy.mockResolvedValue(Promise.resolve({ success: true }));\n    store = reduxUtils.createDtaleStore();\n    buildInnerHTML({ settings: '', dataId: '1', queryEngine: 'python', enableCustomFilters: 'True' }, store);\n    store.dispatch({ type: ActionType.OPEN_CHART, chartData: { visible: true } });\n    wrapper = await act(\n      async () =>\n        await render(\n          <Provider store={store}>\n            <FilterPopup />\n          </Provider>,\n          {\n            container: document.getElementById('content') ?? undefined,\n          },\n        ).container,\n    );\n  });\n\n  afterEach(jest.resetAllMocks);\n  afterAll(jest.restoreAllMocks);\n\n  const clickFilterBtn = async (text: string): Promise<void> => {\n    await act(async () => {\n      await fireEvent.click(screen.getByText(text));\n    });\n  };\n\n  it('renders successfully', () => {\n    expect(wrapper.innerHTML).not.toBe('');\n  });\n\n  it('drops filter', async () => {\n    await act(async () => {\n      await fireEvent.click(screen.queryAllByTestId('structured-filters')[0].querySelector('.ico-cancel')!);\n    });\n    expect(updateSettingsSpy).toHaveBeenCalledTimes(1);\n    expect(screen.queryAllByTestId('structured-filters')).toHaveLength(1);\n  });\n\n  it('saves filter', async () => {\n    const onCloseSpy = jest.spyOn(chartActions, 'closeChart');\n    await act(async () => {\n      await fireEvent.change(wrapper.getElementsByTagName('textarea')[0], { target: { value: 'foo == foo' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(saveFilterSpy).toHaveBeenCalledWith('1', 'foo == foo');\n    expect(store.getState().settings.query).toBe('foo == foo');\n    expect(onCloseSpy).toHaveBeenCalledTimes(1);\n    onCloseSpy.mockRestore();\n  });\n\n  it('save failure', async () => {\n    saveFilterSpy.mockResolvedValue({ error: 'error', success: false });\n    await act(async () => {\n      await fireEvent.change(wrapper.getElementsByTagName('textarea')[0], { target: { value: 'foo == foo' } });\n    });\n    await clickFilterBtn('Apply');\n    expect(screen.getByRole('alert').textContent).toBe('error');\n    await act(async () => {\n      await fireEvent.click(screen.getByRole('alert').querySelector('.ico-cancel')!);\n    });\n    expect(screen.queryAllByRole('alert')).toHaveLength(0);\n  });\n\n  it('clears filter', async () => {\n    const onCloseSpy = jest.spyOn(chartActions, 'closeChart');\n    await clickFilterBtn('Clear');\n    expect(updateSettingsSpy).toHaveBeenCalledWith({ query: '' }, '1');\n    expect(onCloseSpy).toHaveBeenCalledTimes(1);\n    onCloseSpy.mockRestore();\n  });\n\n  it('updates query engine', async () => {\n    await clickFilterBtn('numexpr');\n    expect(updateQueryEngineSpy).toHaveBeenCalledTimes(1);\n    expect(updateQueryEngineSpy.mock.calls[0][0]).toBe('numexpr');\n  });\n\n  it('toggle highlight filter', async () => {\n    await act(async () => {\n      await fireEvent.click(screen.getByText('Highlight Filtered Rows').parentElement?.getElementsByTagName('i')[0]!);\n    });\n    expect(updateSettingsSpy).toHaveBeenLastCalledWith({ highlightFilter: true }, '1');\n    expect(store.getState().settings.highlightFilter).toBe(true);\n  });\n\n  describe('new window', () => {\n    const { location, close, opener } = window;\n\n    beforeAll(() => {\n      delete (window as any).location;\n      delete (window as any).close;\n      delete window.opener;\n      (window as any).location = { pathname: '/dtale/popup/filter' };\n      window.close = jest.fn();\n      window.opener = {\n        location: {\n          reload: jest.fn(),\n        },\n      };\n    });\n\n    afterEach(jest.resetAllMocks);\n\n    afterAll(() => {\n      window.location = location;\n      window.close = close;\n      window.opener = opener;\n    });\n\n    it('drops filter', async () => {\n      await act(async () => {\n        await fireEvent.click(screen.queryAllByTestId('structured-filters')[0].querySelector('.ico-cancel')!);\n      });\n      expect(updateSettingsSpy).toHaveBeenCalledTimes(1);\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n    });\n\n    it('saves filter', async () => {\n      await act(async () => {\n        await fireEvent.change(wrapper.getElementsByTagName('textarea')[0], { target: { value: 'foo == foo' } });\n      });\n      await clickFilterBtn('Apply');\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n      expect(window.close).toHaveBeenCalledTimes(1);\n    });\n\n    it('clears filter', async () => {\n      await clickFilterBtn('Clear');\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n      expect(window.close).toHaveBeenCalledTimes(1);\n    });\n\n    it('toggle highlight filter', async () => {\n      await act(async () => {\n        await fireEvent.click(screen.getByText('Highlight Filtered Rows').parentElement?.getElementsByTagName('i')[0]!);\n      });\n      expect(updateSettingsSpy).toHaveBeenLastCalledWith({ highlightFilter: true }, '1');\n      expect(window.opener.location.reload).toHaveBeenCalledTimes(1);\n    });\n  });\n});\n", "import { render } from '@testing-library/react';\nimport * as React from 'react';\nimport { Provider } from 'react-redux';\n\nimport { buildRangeState } from '../../dtale/rangeSelectUtils';\nimport { PopupType } from '../../redux/state/AppState';\nimport reduxUtils from '../redux-test-utils';\n\ndescribe('reducer tests', () => {\n  it('dtale: missing hidden input', () => {\n    const store = reduxUtils.createDtaleStore();\n    const body = document.getElementsByTagName('body')[0];\n    body.innerHTML = `<div id=\"content\" style=\"height: 1000px;width: 1000px;\"></div>`;\n\n    render(\n      <Provider store={store}>\n        <div />\n      </Provider>,\n      {\n        container: document.getElementById('content') ?? undefined,\n      },\n    );\n    const state = {\n      chartData: { visible: false, type: PopupType.HIDDEN },\n      hideShutdown: false,\n      hideHeaderEditor: false,\n      lockHeaderMenu: false,\n      hideHeaderMenu: false,\n      hideMainMenu: false,\n      hideColumnMenus: false,\n      enableCustomFilters: false,\n      hideDropRows: false,\n      iframe: false,\n      columnMenuOpen: false,\n      selectedCol: null,\n      selectedColRef: null,\n      dataId: '',\n      editedCell: null,\n      xarray: false,\n      xarrayDim: {},\n      allowCellEdits: true,\n      theme: 'light',\n      language: 'en',\n      filteredRanges: {},\n      settings: {\n        allow_cell_edits: true,\n        hide_shutdown: false,\n        precision: 2,\n        predefinedFilters: {},\n        verticalHeaders: false,\n        hide_header_editor: false,\n        lock_header_menu: false,\n        hide_header_menu: false,\n        hide_main_menu: false,\n        hide_column_menus: false,\n        enable_custom_filters: false,\n      },\n      pythonVersion: null,\n      isPreview: false,\n      menuPinned: false,\n      menuTooltip: {\n        visible: false,\n      },\n      ribbonDropdown: {\n        visible: false,\n      },\n      ribbonMenuOpen: false,\n      sidePanel: {\n        visible: false,\n      },\n      dataViewerUpdate: null,\n      auth: false,\n      username: null,\n      predefinedFilters: [],\n      maxColumnWidth: null,\n      maxRowHeight: null,\n      dragResize: null,\n      editedTextAreaHeight: 0,\n      mainTitle: null,\n      mainTitleFont: null,\n      showAllHeatmapColumns: false,\n      isVSCode: false,\n      isArcticDB: 0.0,\n      arcticConn: '',\n      columnCount: 0,\n      queryEngine: 'python',\n      openCustomFilterOnStartup: false,\n      openPredefinedFiltersOnStartup: false,\n      menuOpen: false,\n      formattingOpen: null,\n      ...buildRangeState(),\n    };\n    expect(state).toEqual(store.getState());\n  });\n});\n", "/* eslint max-classes-per-file: \"off\" */\nimport { act, fireEvent, Matcher, screen } from '@testing-library/react';\nimport {\n  ChartConfiguration,\n  ChartData,\n  ChartEvent,\n  ChartMeta,\n  ChartOptions,\n  ChartType,\n  DefaultDataPoint,\n  InteractionItem,\n  Scale,\n} from 'chart.js';\nimport { TFunction } from 'i18next';\nimport * as React from 'react';\nimport selectEvent from 'react-select-event';\nimport { Store } from 'redux';\n\nimport * as chartUtils from '../chartUtils';\n\nexport const parseUrlParams = (url?: string): Record<string, string> =>\n  JSON.parse(\n    '{\"' +\n      decodeURI((url ?? '').split('?')[1])\n        .replace(/\"/g, '\\\\\"')\n        .replace(/&/g, '\",\"')\n        .replace(/=/g, '\":\"') +\n      '\"}',\n  );\n\nexport const replaceNBSP = (text: string): string => text.replace(/\\s/g, ' ');\n\nexport const logException = (e: Error): void => {\n  console.error(`${e.name}: ${e.message} (${(e as any).fileName}:${(e as any).lineNumber})`); // eslint-disable-line no-console\n  console.error(e.stack); // eslint-disable-line no-console\n};\n\nconst BASE_SETTINGS = '{&quot;sortInfo&quot;:[[&quot;col1&quot;,&quot;ASC&quot;]],&quot;precision&quot;:2}';\nconst HIDE_SHUTDOWN = 'False';\nconst PROCESSES = 1;\nconst IFRAME = 'False';\nconst DATA_ID = 1;\n\nexport const PREDEFINED_FILTERS = [\n  '[{',\n  '&quot;name&quot;:&quot;custom_foo&quot;,',\n  '&quot;column&quot;:&quot;foo&quot;,',\n  '&quot;description&quot;:&quot;foo&quot;,',\n  '&quot;inputType&quot;: &quot;input&quot;',\n  '}]',\n].join('');\n\nconst buildHidden = (id: string, value: string | number): string =>\n  `<input type=\"hidden\" id=\"${id}\" value=\"${value}\" />`;\n\nconst BASE_HTML = `\n<div id=\"content\" style=\"height: 1000px;width: 1000px;\" />\n<div id=\"popup-content\" />\n<span id=\"code-title\" />\n`;\n\nexport const buildInnerHTML = (props: Record<string, string | undefined> = {}, store?: Store): void => {\n  const actions = require('../redux/actions/dtale');\n  const pjson = require('../../package.json');\n  const body = document.getElementsByTagName('body')[0];\n  body.innerHTML = [\n    buildHidden('settings', props.settings ?? BASE_SETTINGS),\n    buildHidden('version', pjson.version),\n    buildHidden('python_version', props.pythonVersion ?? '3.8.0'),\n    buildHidden('hide_shutdown', props.hideShutdown ?? HIDE_SHUTDOWN),\n    buildHidden('hide_drop_rows', props.hideDropRows ?? 'False'),\n    buildHidden('processes', props.processes ?? PROCESSES),\n    buildHidden('iframe', props.iframe ?? IFRAME),\n    buildHidden('data_id', props.dataId ?? DATA_ID),\n    buildHidden('xarray', props.xarray ?? 'False'),\n    buildHidden('xarray_dim', props.xarrayDim ?? '{}'),\n    buildHidden('allow_cell_edits', props.allowCellEdits ?? 'true'),\n    buildHidden('is_vscode', props.isVSCode ?? 'False'),\n    buildHidden('arctic_conn', props.arcticConn ?? ''),\n    buildHidden('is_arcticdb', props.isArcticDB ?? '0'),\n    buildHidden('column_count', props.columnCount ?? '0'),\n    buildHidden('theme', props.theme ?? 'light'),\n    buildHidden('language', props.language ?? 'en'),\n    buildHidden('pin_menu', props.pinMenu ?? 'False'),\n    buildHidden('filtered_ranges', props.filteredRanges ?? JSON.stringify({})),\n    buildHidden('auth', props.auth ?? 'False'),\n    buildHidden('username', props.username ?? ''),\n    buildHidden('predefined_filters', props.predefinedFilters ?? '[]'),\n    buildHidden('max_column_width', props.maxColumnWidth ?? 'None'),\n    buildHidden('main_title', props.mainTitle ?? ''),\n    buildHidden('main_title_font', props.mainTitleFont ?? ''),\n    buildHidden('query_engine', props.queryEngine ?? 'python'),\n    buildHidden('hide_header_editor', props.hideHeaderEditor ?? HIDE_SHUTDOWN),\n    buildHidden('lock_header_menu', props.lockHeaderMenu ?? HIDE_SHUTDOWN),\n    buildHidden('hide_header_menu', props.hideHeaderMenu ?? HIDE_SHUTDOWN),\n    buildHidden('hide_main_menu', props.hideMainMenu ?? HIDE_SHUTDOWN),\n    buildHidden('hide_column_menus', props.hideColumnMenus ?? HIDE_SHUTDOWN),\n    buildHidden('enable_custom_filters', props.enableCustomFilters ?? HIDE_SHUTDOWN),\n    BASE_HTML,\n  ].join('');\n  store?.dispatch(actions.init());\n};\n\nexport const findMainMenuButton = (name: string, btnTag = 'button'): Element | undefined => {\n  const menu = screen.getByTestId('data-viewer-menu');\n  const buttons = menu.querySelectorAll(`ul li ${btnTag}`);\n  return [...buttons].find((b) => b?.textContent?.includes(name));\n};\n\nexport const clickMainMenuButton = async (name: string, btnTag = 'button'): Promise<void> => {\n  await act(async () => {\n    fireEvent.click(findMainMenuButton(name, btnTag)!);\n  });\n};\n\nexport const tick = (timeout = 0): Promise<void> => {\n  return new Promise((resolve) => {\n    setTimeout(resolve, timeout);\n  });\n};\n\n/** Mocked version of chart.js Chart object */\nexport class MockChart {\n  /** @override */\n  public static register = (): void => undefined;\n\n  public ctx: HTMLCanvasElement;\n  public cfg: ChartConfiguration;\n  public config: { _config: ChartConfiguration };\n  public data: ChartData;\n  public destroyed: boolean;\n  public options: ChartOptions;\n\n  /** @override */\n  constructor(ctx: HTMLCanvasElement, cfg: ChartConfiguration) {\n    this.ctx = ctx;\n    this.cfg = cfg;\n    this.config = { _config: cfg };\n    this.data = cfg.data;\n    this.destroyed = false;\n    this.options = cfg.options ?? { scales: { x: {}, y: {} } };\n  }\n\n  /** @override */\n  public destroy(): void {\n    this.destroyed = true;\n  }\n\n  /** @override */\n  public getElementsAtEventForMode(_evt: ChartEvent): InteractionItem[] {\n    return [\n      {\n        datasetIndex: 0,\n        index: 0,\n        _chart: { config: { _config: this.cfg }, data: this.cfg.data },\n      } as any as InteractionItem,\n    ];\n  }\n\n  /** @override */\n  public getDatasetMeta(idx: number): ChartMeta {\n    return { controller: { _config: { selectedPoint: 0 } } } as any as ChartMeta;\n  }\n\n  /** @override */\n  public update(): void {} // eslint-disable-line @typescript-eslint/no-empty-function\n}\n\nexport const mockChartJS = (): void => {\n  jest.mock('chart.js', () => ({ Chart: MockChart }));\n};\n\nexport const mockD3Cloud = (): void => {\n  jest.mock('d3-cloud', () => () => {\n    const cloudCfg: any = {};\n    const propUpdate =\n      (prop: string): ((val: string) => any) =>\n      (val: string): any => {\n        cloudCfg[prop] = val;\n        return cloudCfg;\n      };\n    cloudCfg.size = propUpdate('size');\n    cloudCfg.padding = propUpdate('padding');\n    cloudCfg.words = propUpdate('words');\n    cloudCfg.rotate = propUpdate('rotate');\n    cloudCfg.spiral = propUpdate('spiral');\n    cloudCfg.random = propUpdate('random');\n    cloudCfg.text = propUpdate('text');\n    cloudCfg.font = propUpdate('font');\n    cloudCfg.fontStyle = propUpdate('fontStyle');\n    cloudCfg.fontWeight = propUpdate('fontWeight');\n    cloudCfg.fontSize = () => ({\n      on: () => ({ start: () => undefined }),\n    });\n    return cloudCfg;\n  });\n};\n\nexport const mockWordcloud = (): void => {\n  jest.mock('react-wordcloud', () => {\n    const { createMockComponent } = require('./mocks/createMockComponent');\n    return {\n      __esModule: true,\n      default: createMockComponent('MockWordcloud', () => <div data-testid=\"mock-wordcloud\" />),\n    };\n  });\n};\n\nexport const mockT = ((key: string): string => {\n  const keySegs = key.split(':');\n  if (keySegs.length > 2) {\n    keySegs.shift();\n    return keySegs.join(':');\n  } else if (keySegs.length === 2) {\n    return keySegs[keySegs.length - 1];\n  }\n  return key;\n}) as any as TFunction;\n\n/** Type definition for chartUtils.createChart spies */\nexport type CreateChartSpy = jest.SpyInstance<\n  chartUtils.ChartObj,\n  [ctx: HTMLCanvasElement, cfg: ChartConfiguration<ChartType, DefaultDataPoint<ChartType>, unknown>]\n>;\n\nexport const getLastChart = (\n  spy: CreateChartSpy,\n  chartType?: string,\n): ChartConfiguration<ChartType, DefaultDataPoint<ChartType>, unknown> => {\n  if (chartType) {\n    const typeCalls = spy.mock.calls.filter((call) => call[1].type === chartType);\n    return typeCalls[typeCalls.length - 1][1];\n  }\n  return spy.mock.calls[spy.mock.calls.length - 1][1];\n};\n\nexport const buildChartContext = (): Partial<CanvasRenderingContext2D> => ({\n  createLinearGradient: (_px1: number, _px2: number, _px3: number, _px4: number): CanvasGradient => ({\n    addColorStop: (_px5: number, _color: string): void => undefined,\n  }),\n  save: jest.fn(),\n  beginPath: jest.fn(),\n  moveTo: jest.fn(),\n  lineTo: jest.fn(),\n  lineWidth: 0,\n  strokeStyle: undefined,\n  stroke: jest.fn(),\n  restore: jest.fn(),\n});\n\nexport const SCALE: Partial<Scale> = { getPixelForValue: (px: number): number => px };\n\n/** FakeMouseEvent properties */\ninterface MouseEventWithOffsets extends MouseEventInit {\n  pageX?: number;\n  pageY?: number;\n  offsetX?: number;\n  offsetY?: number;\n  x?: number;\n  y?: number;\n}\n\n/** Fake mouse event class */\nexport class FakeMouseEvent extends MouseEvent {\n  /** @override */\n  constructor(type: string, values: MouseEventWithOffsets) {\n    const { pageX, pageY, offsetX, offsetY, x, y, ...mouseValues } = values;\n    super(type, mouseValues);\n\n    Object.assign(this, {\n      offsetX: offsetX || 0,\n      offsetY: offsetY || 0,\n      pageX: pageX || 0,\n      pageY: pageY || 0,\n      x: x || 0,\n      y: y || 0,\n    });\n  }\n}\n\nexport const selectOption = async (selectElement: HTMLElement, option: Matcher | Matcher[]): Promise<void> => {\n  await act(async () => {\n    await selectEvent.openMenu(selectElement);\n  });\n  await act(async () => {\n    await selectEvent.select(selectElement, option, { container: document.body });\n  });\n};\n", "import * as React from 'react';\nimport * as ReactDOMClient from 'react-dom/client';\nimport { Provider } from 'react-redux';\n\nimport '../../i18n';\nimport * as actions from '../../redux/actions/dtale';\nimport appReducers from '../../redux/reducers/app';\nimport { AppState } from '../../redux/state/AppState';\nimport { createAppStore } from '../../redux/store';\nimport { DataResponseContent } from '../../repository/DataRepository';\n\nimport { ServerlessDataViewer } from './ServerlessDataViewer';\n\nrequire('../../publicPath');\n\nconst store = createAppStore<AppState>(appReducers);\nstore.dispatch(actions.init());\nactions.loadBackgroundMode(store);\nactions.loadHideShutdown(store);\nactions.loadAllowCellEdits(store);\nactions.loadHideHeaderEditor(store);\nactions.loadLockHeaderMenu(store);\nactions.loadHideHeaderMenu(store);\nactions.loadHideMainMenu(store);\nactions.loadHideColumnMenus(store);\nactions.loadEnableCustomFilters(store);\nconst root = ReactDOMClient.createRoot(document.getElementById('content')!);\nroot.render(\n  <Provider store={store}>\n    <ServerlessDataViewer response={(global as any).RESPONSE! as DataResponseContent} />\n  </Provider>,\n);\n", "import * as React from 'react';\nimport * as ReactDOMClient from 'react-dom/client';\nimport { Provider } from 'react-redux';\nimport { Store } from 'redux';\n\nimport { DataViewer } from './dtale/DataViewer';\nimport './i18n';\nimport ColumnAnalysis from './popups/analysis/ColumnAnalysis';\nimport LibrarySymbolSelector from './popups/arcticdb/LibrarySymbolSelector';\nimport { CodeExport } from './popups/CodeExport';\nimport CodePopup from './popups/CodePopup';\nimport { Correlations } from './popups/correlations/Correlations';\nimport CreateColumn from './popups/create/CreateColumn';\nimport { CreateColumnType, PrepopulateCreateColumn, SaveAs } from './popups/create/CreateColumnState';\nimport Describe from './popups/describe/Describe';\nimport Duplicates from './popups/duplicates/Duplicates';\nimport FilterPopup from './popups/filter/FilterPopup';\nimport Instances from './popups/instances/Instances';\nimport MergeDatasets from './popups/merge/MergeDatasets';\nimport PredictivePowerScore from './popups/pps/PredictivePowerScore';\nimport CreateReplacement from './popups/replacement/CreateReplacement';\nimport Reshape from './popups/reshape/Reshape';\nimport Upload from './popups/upload/Upload';\nimport Variance from './popups/variance/Variance';\nimport * as actions from './redux/actions/dtale';\nimport * as mergeActions from './redux/actions/merge';\nimport appReducers from './redux/reducers/app';\nimport mergeReducers from './redux/reducers/merge';\nimport { getHiddenValue, toJson } from './redux/reducers/utils';\nimport { AppState, InstanceSettings } from './redux/state/AppState';\nimport { MergeState } from './redux/state/MergeState';\nimport { createAppStore } from './redux/store';\n\nrequire('./publicPath');\n\nlet pathname = window.location.pathname;\nif ((window as any).resourceBaseUrl) {\n  pathname = pathname.replace((window as any).resourceBaseUrl, '');\n}\nlet storeBuilder: () => Store = () => {\n  const store = createAppStore<AppState>(appReducers);\n  store.dispatch(actions.init());\n  actions.loadBackgroundMode(store);\n  actions.loadHideShutdown(store);\n  actions.loadAllowCellEdits(store);\n  actions.loadHideHeaderEditor(store);\n  actions.loadLockHeaderMenu(store);\n  actions.loadHideHeaderMenu(store);\n  actions.loadHideMainMenu(store);\n  actions.loadHideColumnMenus(store);\n  actions.loadEnableCustomFilters(store);\n  return store;\n};\nif (pathname.indexOf('/dtale/popup') === 0) {\n  require('./dtale/DataViewer.css');\n\n  let rootNode = null;\n  const settings = toJson<InstanceSettings>(getHiddenValue('settings'));\n  const dataId = getHiddenValue('data_id');\n  const pathSegs = pathname.split('/');\n  const popupType = pathSegs[pathSegs.length - 1] === 'code-popup' ? 'code-popup' : pathSegs[3];\n  const chartData: Record<string, any> = {\n    ...actions.getParams(),\n    ...(settings.query ? { query: settings.query } : {}),\n  };\n  switch (popupType) {\n    case 'filter':\n      rootNode = <FilterPopup />;\n      break;\n    case 'correlations':\n      rootNode = <Correlations />;\n      break;\n    case 'merge':\n      storeBuilder = () => {\n        const store = createAppStore<MergeState>(mergeReducers);\n        mergeActions.init(store.dispatch);\n        return store;\n      };\n      rootNode = <MergeDatasets />;\n      break;\n    case 'pps':\n      rootNode = <PredictivePowerScore />;\n      break;\n    case 'describe':\n      rootNode = <Describe />;\n      break;\n    case 'variance':\n      rootNode = <Variance />;\n      break;\n    case 'build':\n      rootNode = <CreateColumn />;\n      break;\n    case 'duplicates':\n      rootNode = <Duplicates />;\n      break;\n    case 'type-conversion': {\n      const prePopulated: PrepopulateCreateColumn = {\n        type: CreateColumnType.TYPE_CONVERSION,\n        saveAs: SaveAs.INPLACE,\n        cfg: { col: chartData.selectedCol, applyAllType: false },\n      };\n      rootNode = <CreateColumn prePopulated={prePopulated} />;\n      break;\n    }\n    case 'cleaners': {\n      const prePopulated: PrepopulateCreateColumn = {\n        type: CreateColumnType.CLEANING,\n        cfg: { col: chartData.selectedCol, cleaners: [] },\n      };\n      rootNode = <CreateColumn prePopulated={prePopulated} />;\n      break;\n    }\n    case 'replacement':\n      rootNode = <CreateReplacement />;\n      break;\n    case 'reshape':\n      rootNode = <Reshape />;\n      break;\n    case 'column-analysis':\n      rootNode = <ColumnAnalysis {...{ dataId, chartData }} height={250} />;\n      break;\n    case 'instances':\n      rootNode = <Instances />;\n      break;\n    case 'code-export':\n      rootNode = <CodeExport />;\n      break;\n    case 'arcticdb':\n      rootNode = <LibrarySymbolSelector />;\n      break;\n    case 'upload':\n    default:\n      rootNode = <Upload />;\n      break;\n  }\n  const store = storeBuilder();\n  store.getState().chartData = chartData;\n  const root = ReactDOMClient.createRoot(document.getElementById('popup-content')!);\n  root.render(<Provider store={store}>{rootNode}</Provider>);\n} else if (pathname.startsWith('/dtale/code-popup')) {\n  require('./dtale/DataViewer.css');\n  let title: string;\n  let body: JSX.Element;\n  if (window.opener) {\n    title = `${window.opener.code_popup.title} Code Export`;\n    body = <CodePopup code={window.opener.code_popup.code} />;\n  } else {\n    title = 'Code Missing';\n    body = <h1>No parent window containing code detected!</h1>;\n  }\n  const titleElement: HTMLElement | null = document.getElementById('code-title');\n  if (titleElement) {\n    titleElement.innerHTML = title;\n  }\n  const root = ReactDOMClient.createRoot(document.getElementById('popup-content')!);\n  root.render(body);\n} else {\n  const store = storeBuilder();\n  if (store.getState().openPredefinedFiltersOnStartup) {\n    store.dispatch(actions.openPredefinedFilters());\n  } else if (store.getState().openCustomFilterOnStartup) {\n    store.dispatch(actions.openCustomFilter());\n  }\n  const root = ReactDOMClient.createRoot(document.getElementById('content')!);\n  root.render(\n    <Provider store={store}>\n      <DataViewer />\n    </Provider>,\n  );\n}\n", "import * as React from 'react';\nimport { WithTranslation, withTranslation } from 'react-i18next';\nimport { useDispatch, useSelector } from 'react-redux';\nimport { AnyAction } from 'redux';\n\nimport ButtonToggle from '../../ButtonToggle';\nimport { ColumnFilter, OutlierFilter } from '../../dtale/DataViewerState';\nimport * as serverState from '../../dtale/serverStateManagement';\nimport SidePanelButtons from '../../dtale/side/SidePanelButtons';\nimport { ActionType, HideSidePanelAction, SetQueryEngineAction } from '../../redux/actions/AppActions';\nimport * as dtaleActions from '../../redux/actions/dtale';\nimport * as settingsActions from '../../redux/actions/settings';\nimport { InstanceSettings, QueryEngine } from '../../redux/state/AppState';\nimport { RemovableError } from '../../RemovableError';\nimport * as CustomFilterRepository from '../../repository/CustomFilterRepository';\nimport { Checkbox } from '../create/LabeledCheckbox';\n\nimport ContextVariables from './ContextVariables';\nimport { DISABLED_CUSTOM_FILTERS_MSG, selectResult } from './FilterPopup';\nimport PandasQueryHelp from './PandasQueryHelp';\nimport QueryExamples from './QueryExamples';\nimport StructuredFilters from './StructuredFilters';\n\nconst FilterPanel: React.FC<WithTranslation> = ({ t }) => {\n  const { dataId, enableCustomFilters, queryEngine, settings } = useSelector(selectResult);\n  const dispatch = useDispatch();\n  const hideSidePanel = (): HideSidePanelAction => dispatch({ type: ActionType.HIDE_SIDE_PANEL });\n  const updateSettings = (updatedSettings: Partial<InstanceSettings>, callback?: () => void): AnyAction =>\n    dispatch(settingsActions.updateSettings(updatedSettings, callback) as any as AnyAction);\n  const setEngine = (engine: QueryEngine): SetQueryEngineAction => dispatch(dtaleActions.setQueryEngine(engine));\n\n  const [query, setQuery] = React.useState('');\n  const [highlightFilter, setHighlightFilter] = React.useState(settings.highlightFilter ?? false);\n  const [contextVars, setContextVars] = React.useState<Array<{ name: string; value: string }>>([]);\n  const [columnFilters, setColumnFilters] = React.useState<Record<string, ColumnFilter>>({});\n  const [outlierFilters, setOutlierFilters] = React.useState<Record<string, OutlierFilter>>({});\n  const [error, setError] = React.useState<JSX.Element>();\n\n  React.useEffect(() => {\n    CustomFilterRepository.loadInfo(dataId).then((response) => {\n      if (response?.error) {\n        setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n        return;\n      }\n      if (response) {\n        setQuery(response.query);\n        setContextVars(response.contextVars);\n        setColumnFilters(response.columnFilters);\n        setOutlierFilters(response.outlierFilters);\n      }\n    });\n  }, []);\n\n  const save = async (): Promise<void> => {\n    const response = await CustomFilterRepository.save(dataId, query);\n    if (response?.error) {\n      setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n      return;\n    }\n    updateSettings({ query }, hideSidePanel);\n  };\n\n  const saveHighlightFilter = async (updatedHighlightFilter: boolean): Promise<void> => {\n    await serverState.updateSettings({ highlightFilter: updatedHighlightFilter }, dataId);\n    updateSettings({ highlightFilter: updatedHighlightFilter }, () => setHighlightFilter(updatedHighlightFilter));\n  };\n\n  const dropFilter = async <T,>(\n    prop: string,\n    filters: Record<string, T>,\n    setter: (value: Record<string, T>) => void,\n    col: string,\n  ): Promise<void> => {\n    const updatedFilters = Object.keys(filters).reduce(\n      (res, key) => (key !== col ? { ...res, [key]: filters[key] } : res),\n      {},\n    );\n    const updatedSettings = { [prop]: updatedFilters };\n    await serverState.updateSettings(updatedSettings, dataId);\n    updateSettings(updatedSettings, () => setter(updatedFilters));\n  };\n\n  const clear = async (): Promise<void> => {\n    await serverState.updateSettings({ query: '' }, dataId);\n    updateSettings({ query: '' }, hideSidePanel);\n  };\n\n  const updateEngine = async (engine: QueryEngine): Promise<void> => {\n    await serverState.updateQueryEngine(engine);\n    setEngine(engine);\n  };\n\n  return (\n    <div data-testid=\"filter-panel\">\n      {error}\n      <div className=\"row\">\n        <div className=\"col-md-12\">\n          <div className=\"row m-0\">\n            <h1 className=\"mb-0\">{t('Filters', { ns: 'filter' })}</h1>\n            <div className=\"col\" />\n            <SidePanelButtons />\n          </div>\n          <div className=\"row m-0 pb-3\">\n            <div className=\"col p-0 font-weight-bold mt-auto\">{t('Custom Filter', { ns: 'filter' })}</div>\n            <PandasQueryHelp />\n            {enableCustomFilters && (\n              <>\n                <button className=\"btn btn-primary col-auto pt-2 pb-2\" onClick={clear}>\n                  <span>{t('Clear', { ns: 'filter' })}</span>\n                </button>\n                <button className=\"btn btn-primary col-auto pt-2 pb-2\" onClick={save}>\n                  <span>{t('Apply', { ns: 'filter' })}</span>\n                </button>\n              </>\n            )}\n          </div>\n          <textarea\n            style={{ width: '100%', height: 150 }}\n            value={enableCustomFilters ? query : DISABLED_CUSTOM_FILTERS_MSG}\n            onChange={(event) => setQuery(event.target.value)}\n            disabled={!enableCustomFilters}\n          />\n        </div>\n      </div>\n      <div className=\"row pt-3 pb-3\">\n        <span className=\"font-weight-bold col-auto pr-3\">{t('Highlight Filtered Rows', { ns: 'main' })}</span>\n        <Checkbox value={highlightFilter} setter={saveHighlightFilter} className=\"pt-1\" />\n      </div>\n      <div className=\"row pt-3 pb-3\">\n        <span className=\"font-weight-bold col-auto pr-0\">{t('Query Engine', { ns: 'filter' })}</span>\n        <ButtonToggle\n          className=\"ml-auto mr-3 font-weight-bold col\"\n          options={Object.values(QueryEngine).map((value) => ({ value }))}\n          update={updateEngine}\n          defaultValue={queryEngine}\n        />\n      </div>\n      {(!!Object.keys(columnFilters ?? {}).length || !!Object.keys(outlierFilters ?? {}).length) && (\n        <div className=\"row pb-5\">\n          <div className=\"col-md-6\">\n            <StructuredFilters\n              label={t('Column Filters', { ns: 'filter' })}\n              filters={columnFilters}\n              dropFilter={(col: string) => dropFilter('columnFilters', columnFilters, setColumnFilters, col)}\n            />\n          </div>\n          <div className=\"col-md-6\">\n            <StructuredFilters\n              label={t('Outlier Filters', { ns: 'filter' })}\n              filters={outlierFilters}\n              dropFilter={(col: string) => dropFilter('outlierFilters', outlierFilters, setOutlierFilters, col)}\n            />\n          </div>\n        </div>\n      )}\n      <div className=\"row\">\n        <div className=\"col-md-12\">\n          <QueryExamples />\n        </div>\n      </div>\n      {contextVars.length > 0 && <ContextVariables contextVars={contextVars} />}\n    </div>\n  );\n};\n\nexport default withTranslation(['filter', 'main'])(FilterPanel);\n", "import { createSelector } from '@reduxjs/toolkit';\nimport * as React from 'react';\nimport { WithTranslation, withTranslation } from 'react-i18next';\nimport { useDispatch, useSelector } from 'react-redux';\nimport { AnyAction } from 'redux';\n\nimport ButtonToggle from '../../ButtonToggle';\nimport { ColumnFilter, OutlierFilter } from '../../dtale/DataViewerState';\nimport * as serverState from '../../dtale/serverStateManagement';\nimport { CloseChartAction, SetQueryEngineAction } from '../../redux/actions/AppActions';\nimport { closeChart } from '../../redux/actions/charts';\nimport * as dtaleActions from '../../redux/actions/dtale';\nimport * as settingsActions from '../../redux/actions/settings';\nimport { selectDataId, selectEnableCustomFilters, selectQueryEngine, selectSettings } from '../../redux/selectors';\nimport { InstanceSettings, QueryEngine } from '../../redux/state/AppState';\nimport { RemovableError } from '../../RemovableError';\nimport * as CustomFilterRepository from '../../repository/CustomFilterRepository';\nimport { Checkbox } from '../create/LabeledCheckbox';\n\nimport ContextVariables from './ContextVariables';\nimport PandasQueryHelp from './PandasQueryHelp';\nimport QueryExamples from './QueryExamples';\nimport StructuredFilters from './StructuredFilters';\n\nexport const DISABLED_CUSTOM_FILTERS_MSG = [\n  'Custom Filtering is currently disabled.  This feature is only for trusted environments, in order to unlock this ',\n  'feature you must do one of the following:\\n\\n',\n  '- add \"enable_custom_filters=True\" to your dtale.show call\\n',\n  '- run this code before calling dtale.show\\n',\n  '\\timport dtale.global_state as global_state\\n\\tglobal_state.set_app_settings(dict(enable_custom_filters=True))\\n',\n  '- add \"enable_custom_filters = False\" to the [app] section of your dtale.ini config file',\n].join('');\n\nexport const selectResult = createSelector(\n  [selectDataId, selectQueryEngine, selectEnableCustomFilters, selectSettings],\n  (dataId, queryEngine, enableCustomFilters, settings) => ({ dataId, queryEngine, enableCustomFilters, settings }),\n);\n\nconst FilterPopup: React.FC<WithTranslation> = ({ t }) => {\n  const { dataId, queryEngine, enableCustomFilters, settings } = useSelector(selectResult);\n  const dispatch = useDispatch();\n  const onClose = (): CloseChartAction => dispatch(closeChart());\n  const updateSettings = (updatedSettings: Partial<InstanceSettings>, callback?: () => void): AnyAction =>\n    dispatch(settingsActions.updateSettings(updatedSettings, callback) as any as AnyAction);\n  const setEngine = (engine: QueryEngine): SetQueryEngineAction => dispatch(dtaleActions.setQueryEngine(engine));\n\n  const [query, setQuery] = React.useState('');\n  const [highlightFilter, setHighlightFilter] = React.useState(settings.highlightFilter ?? false);\n  const [contextVars, setContextVars] = React.useState<Array<{ name: string; value: string }>>([]);\n  const [columnFilters, setColumnFilters] = React.useState<Record<string, ColumnFilter>>({});\n  const [outlierFilters, setOutlierFilters] = React.useState<Record<string, OutlierFilter>>({});\n  const [error, setError] = React.useState<JSX.Element>();\n\n  React.useEffect(() => {\n    CustomFilterRepository.loadInfo(dataId).then((response) => {\n      if (response?.error) {\n        setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n        return;\n      }\n      if (response) {\n        setQuery(response.query);\n        setHighlightFilter(response.highlightFilter);\n        setContextVars(response.contextVars);\n        setColumnFilters(response.columnFilters);\n        setOutlierFilters(response.outlierFilters);\n      }\n    });\n  }, []);\n\n  const save = async (): Promise<void> => {\n    const response = await CustomFilterRepository.save(dataId, query);\n    if (response?.error) {\n      setError(<RemovableError {...response} onRemove={() => setError(undefined)} />);\n      return;\n    }\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n      window.close();\n    } else {\n      updateSettings({ query }, onClose);\n    }\n  };\n\n  const saveHighlightFilter = async (updatedHighlightFilter: boolean): Promise<void> => {\n    await serverState.updateSettings({ highlightFilter: updatedHighlightFilter }, dataId);\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n    } else {\n      updateSettings({ highlightFilter: updatedHighlightFilter }, () => setHighlightFilter(updatedHighlightFilter));\n    }\n  };\n\n  const dropFilter = async <T,>(\n    prop: string,\n    filters: Record<string, T>,\n    setter: (value: Record<string, T>) => void,\n    col: string,\n  ): Promise<void> => {\n    const updatedFilters = Object.keys(filters).reduce(\n      (res, key) => (key !== col ? { ...res, [key]: filters[key] } : res),\n      {},\n    );\n    const updatedSettings = { [prop]: updatedFilters };\n    await serverState.updateSettings(updatedSettings, dataId);\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n    } else {\n      updateSettings(updatedSettings, () => setter(updatedFilters));\n    }\n  };\n\n  const clear = async (): Promise<void> => {\n    await serverState.updateSettings({ query: '' }, dataId);\n    if (window.location.pathname.startsWith('/dtale/popup/filter')) {\n      window.opener.location.reload();\n      window.close();\n    } else {\n      updateSettings({ query: '' }, onClose);\n    }\n  };\n\n  const updateEngine = async (engine: QueryEngine): Promise<void> => {\n    await serverState.updateQueryEngine(engine);\n    setEngine(engine);\n  };\n\n  return (\n    <React.Fragment>\n      <div className=\"modal-body filter-modal\">\n        {error}\n        <div className=\"row pt-3 pb-3\">\n          <span className=\"font-weight-bold col-auto pr-3\">{t('Highlight Filtered Rows', { ns: 'main' })}</span>\n          <Checkbox value={highlightFilter} setter={saveHighlightFilter} className=\"pt-1\" />\n        </div>\n        <div className=\"row\">\n          <div className=\"col-md-7\">\n            <div className=\"row h-100\">\n              <div className=\"col-md-12 h-100\">\n                <StructuredFilters\n                  label={t('Column Filters', { ns: 'filter' })}\n                  filters={columnFilters}\n                  dropFilter={(col: string) => dropFilter('columnFilters', columnFilters, setColumnFilters, col)}\n                />\n                <StructuredFilters\n                  label={t('Outlier Filters', { ns: 'filter' })}\n                  filters={outlierFilters}\n                  dropFilter={(col: string) => dropFilter('outlierFilters', outlierFilters, setOutlierFilters, col)}\n                />\n                <div className=\"font-weight-bold pt-3 pb-3\">{t('Custom Filter', { ns: 'filter' })}</div>\n                <textarea\n                  style={{ width: '100%', height: 150 }}\n                  value={enableCustomFilters ? query : DISABLED_CUSTOM_FILTERS_MSG}\n                  onChange={(event) => setQuery(event.target.value)}\n                  disabled={!enableCustomFilters}\n                />\n              </div>\n            </div>\n          </div>\n          <div className=\"col-md-5\">\n            <QueryExamples />\n          </div>\n        </div>\n        <div className=\"row pb-0\">\n          <span className=\"font-weight-bold col-auto pr-0\">{t('Query Engine', { ns: 'filter' })}</span>\n          <ButtonToggle\n            className=\"ml-auto mr-3 font-weight-bold col\"\n            options={Object.values(QueryEngine).map((value) => ({ value }))}\n            update={updateEngine}\n            defaultValue={queryEngine}\n          />\n        </div>\n        <div className=\"row\">\n          <div className=\"col-md-12\">{contextVars.length > 0 && <ContextVariables contextVars={contextVars} />}</div>\n        </div>\n      </div>\n      <div className=\"modal-footer\">\n        <PandasQueryHelp />\n        {enableCustomFilters && (\n          <>\n            <button className=\"btn btn-primary\" onClick={clear}>\n              <span>{t('Clear', { ns: 'filter' })}</span>\n            </button>\n            <button className=\"btn btn-primary\" onClick={save}>\n              <span>{t('Apply', { ns: 'filter' })}</span>\n            </button>\n          </>\n        )}\n      </div>\n    </React.Fragment>\n  );\n};\n\nexport default withTranslation(['filter', 'main'])(FilterPopup);\n", "import { Action, AnyAction } from 'redux';\nimport { ThunkAction } from 'redux-thunk';\n\nimport {\n  AppState,\n  DataViewerUpdate,\n  FilteredRanges,\n  InstanceSettings,\n  Popups,\n  QueryEngine,\n  RangeState,\n  RibbonDropdownType,\n  SidePanelType,\n  ThemeType,\n} from '../state/AppState';\n\n/** Different application events */\nexport enum ActionType {\n  INIT_PARAMS = 'init-params',\n  LOAD_PREVIEW = 'load-preview',\n  EDIT_CELL = 'edit-cell',\n  TOGGLE_COLUMN_MENU = 'toggle-column-menu',\n  OPEN_MENU = 'open-menu',\n  CLOSE_MENU = 'close-menu',\n  OPEN_FORMATTING = 'open-formatting',\n  CLOSE_FORMATTING = 'close-formatting',\n  TOGGLE_MENU_PINNED = 'toggle-menu-pinned',\n  HIDE_COLUMN_MENU = 'hide-column-menu',\n  CLEAR_EDIT = 'clear-edit',\n  EDITED_CELL_TEXTAREA_HEIGHT = 'edited-cell-textarea-height',\n  CONVERT_TO_XARRAY = 'convert-to-xarray',\n  UPDATE_XARRAY_DIM = 'update-xarray-dim',\n  UPDATE_FILTERED_RANGES = 'update-filtered-ranges',\n  UPDATE_SETTINGS = 'update-settings',\n  SHOW_MENU_TOOLTIP = 'show-menu-tooltip',\n  HIDE_MENU_TOOLTIP = 'hide-menu-tooltip',\n  SHOW_RIBBON_MENU = 'show-ribbon-menu',\n  HIDE_RIBBON_MENU = 'hide-ribbon-menu',\n  OPEN_RIBBON_DROPDOWN = 'open-ribbon-dropdown',\n  SHOW_SIDE_PANEL = 'show-side-panel',\n  HIDE_SIDE_PANEL = 'hide-side-panel',\n  UPDATE_SIDE_PANEL_WIDTH = 'update-side-panel-width',\n  DATA_VIEWER_UPDATE = 'data-viewer-update',\n  CLEAR_DATA_VIEWER_UPDATE = 'clear-data-viewer-update',\n  DRAG_RESIZE = 'drag-resize',\n  STOP_RESIZE = 'stop-resize',\n  OPEN_CHART = 'open-chart',\n  CLOSE_CHART = 'close-chart',\n  LOADING_DATASETS = 'loading-datasets',\n  SET_THEME = 'set-theme',\n  SET_LANGUAGE = 'set-language',\n  UPDATE_MAX_WIDTH = 'update-max-width',\n  CLEAR_MAX_WIDTH = 'clear-max-width',\n  UPDATE_MAX_HEIGHT = 'update-max-height',\n  CLEAR_MAX_HEIGHT = 'clear-max-height',\n  SET_QUERY_ENGINE = 'set-query-engine',\n  UPDATE_SHOW_ALL_HEATMAP_COLUMNS = 'update-show-all-heatmap-columns',\n  SET_RANGE_STATE = 'set-range-state',\n  UPDATE_HIDE_SHUTDOWN = 'update-hide-shutdown',\n  UPDATE_ALLOW_CELL_EDITS = 'update-allow-cell-edits',\n  UPDATE_HIDE_HEADER_EDITOR = 'update-hide-header-editor',\n  UPDATE_LOCK_HEADER_MENU = 'update-lock-header-menu',\n  UPDATE_HIDE_HEADER_MENU = 'update-hide-header-menu',\n  UPDATE_HIDE_MAIN_MENU = 'update-hide-main-menu',\n  UPDATE_HIDE_COLUMN_MENUS = 'update-hide-column-menus',\n  UPDATE_ENABLE_CUSTOM_FILTERS = 'update-enable-custom-filters',\n}\n\n/** Action fired when a range is selected */\nexport type SetRangeStateAction = Action<typeof ActionType.SET_RANGE_STATE> & RangeState;\n\n/** Action fired when application initially loads */\nexport type InitAction = Action<typeof ActionType.INIT_PARAMS>;\n\n/** Action fired when user cancels a cell edit */\nexport type ClearEditAction = Action<typeof ActionType.CLEAR_EDIT>;\n\n/** Action fired when user wants to convert their data to XArray */\nexport type ConvertToXarrayAction = Action<typeof ActionType.CONVERT_TO_XARRAY>;\n\n/** Action fired to hide menu item tooltips */\nexport type HideMenuTooltipAction = Action<typeof ActionType.HIDE_MENU_TOOLTIP>;\n\n/** Action fired to show the ribbon menu */\nexport type ShowRibbonMenuAction = Action<typeof ActionType.SHOW_RIBBON_MENU>;\n\n/** Action fired to hide the ribbon menu */\nexport type HideRibbonMenuAction = Action<typeof ActionType.HIDE_RIBBON_MENU>;\n\n/** Action fired to hide the side panel */\nexport type HideSidePanelAction = Action<typeof ActionType.HIDE_SIDE_PANEL>;\n\n/** Action fired to clear any executed data viewer updates */\nexport type ClearDataViewerUpdateAction = Action<typeof ActionType.CLEAR_DATA_VIEWER_UPDATE>;\n\n/** Action fired to stop resizing columns */\nexport type StopResizeAction = Action<typeof ActionType.STOP_RESIZE>;\n\n/** Action fired to close a popup */\nexport type CloseChartAction = Action<typeof ActionType.CLOSE_CHART>;\n\n/** Action fired when a dataset is being loaded */\nexport type LoadingDatasetsAction = Action<typeof ActionType.LOADING_DATASETS>;\n\n/** Action fired when clearing max width setting */\nexport type ClearMaxWidthAction = Action<typeof ActionType.CLEAR_MAX_WIDTH>;\n\n/** Action fired when clearing max height setting */\nexport type ClearMaxHeightAction = Action<typeof ActionType.CLEAR_MAX_HEIGHT>;\n\n/** Action fired when toggling the state of the main menu being pinned or not */\nexport type ToggleMenuPinnedAction = Action<typeof ActionType.TOGGLE_MENU_PINNED>;\n\n/** Action fired when D-Tale is loaded in \"preview\" mode */\nexport interface LoadPreviewAction extends Action<typeof ActionType.LOAD_PREVIEW> {\n  dataId: string;\n}\n\n/** Action fired when a user edits a cell */\nexport interface EditedCellAction extends Action<typeof ActionType.EDIT_CELL> {\n  editedCell?: string;\n}\n\n/** Action fired for sizing the height of textarea for cell editing */\nexport interface EditedTextAreaHeightAction extends Action<typeof ActionType.EDITED_CELL_TEXTAREA_HEIGHT> {\n  height: number;\n}\n\n/** Action fired for toggling the display of a column menu */\nexport interface ToggleColumnAction extends Action<typeof ActionType.TOGGLE_COLUMN_MENU | ActionType.HIDE_COLUMN_MENU> {\n  colName?: string;\n  headerRef?: HTMLDivElement;\n}\n\n/** Action fired for toggling the display of the main menu */\nexport type ToggleMenuAction = Action<typeof ActionType.OPEN_MENU | ActionType.CLOSE_MENU>;\n\n/** Action fired for opening the formatting menu */\nexport interface OpenFormattingAction extends Action<typeof ActionType.OPEN_FORMATTING> {\n  selectedCol: string;\n}\n\n/** Action fired for closing the formatting menu */\nexport type CloseFormattingAction = Action<typeof ActionType.CLOSE_FORMATTING>;\n\n/** Action fired when updating xarray dimensions */\nexport interface UpdateXarrayDimAction extends Action<typeof ActionType.UPDATE_XARRAY_DIM> {\n  xarrayDim: Record<string, boolean>;\n}\n\n/** Action fired when updating filtered ranges */\nexport interface UpdateFilteredRangesAction extends Action<typeof ActionType.UPDATE_FILTERED_RANGES> {\n  ranges: FilteredRanges;\n}\n\n/** Action fired when updating instance settings */\nexport interface UpdateSettingsAction extends Action<typeof ActionType.UPDATE_SETTINGS> {\n  settings: Partial<InstanceSettings>;\n}\n\n/** Action fired when showing a main menu tooltip */\nexport interface ShowMenuTooltipAction extends Action<typeof ActionType.SHOW_MENU_TOOLTIP> {\n  element: HTMLElement;\n  content: React.ReactNode;\n}\n\n/** Action fired when opening a ribbon dropdown */\nexport interface OpenRibbonDropdownAction extends Action<typeof ActionType.OPEN_RIBBON_DROPDOWN> {\n  element: HTMLDivElement;\n  name: RibbonDropdownType;\n}\n\n/** Action fired when showing or updating the width of the side panel */\nexport interface SidePanelAction\n  extends Action<typeof ActionType.SHOW_SIDE_PANEL | ActionType.UPDATE_SIDE_PANEL_WIDTH> {\n  view?: SidePanelType;\n  column?: string;\n  offset?: number;\n}\n\n/** Action fired when executing a data viewer update */\nexport interface DataViewerUpdateAction extends Action<typeof ActionType.DATA_VIEWER_UPDATE> {\n  update: DataViewerUpdate;\n}\n\n/** Action fired when dragging/resizing a column */\nexport interface DragResizeAction extends Action<typeof ActionType.DRAG_RESIZE> {\n  x: number;\n}\n\n/** Action fired when setting a theme */\nexport interface SetThemeAction extends Action<typeof ActionType.SET_THEME> {\n  theme: ThemeType;\n}\n\n/** Action fired when setting a language */\nexport interface SetLanguageAction extends Action<typeof ActionType.SET_LANGUAGE> {\n  language: string;\n}\n\n/** Action fired when updating the maximum column width */\nexport interface UpdateMaxColumnWidthAction extends Action<typeof ActionType.UPDATE_MAX_WIDTH> {\n  width: number;\n}\n\n/** Action fired when updating the maximum row height */\nexport interface UpdateMaxRowHeightAction extends Action<typeof ActionType.UPDATE_MAX_HEIGHT> {\n  height: number;\n}\n\n/** Action fired when setting the query engine for custom filters */\nexport interface SetQueryEngineAction extends Action<typeof ActionType.SET_QUERY_ENGINE> {\n  engine: QueryEngine;\n}\n\n/** Action fired when updating whether to show all columns when in \"heatmap\" mode */\nexport interface UpdateShowAllHeatmapColumnsAction extends Action<typeof ActionType.UPDATE_SHOW_ALL_HEATMAP_COLUMNS> {\n  showAllHeatmapColumns: boolean;\n}\n\n/** Action fired when opening a chart popup */\nexport interface OpenChartAction extends Action<typeof ActionType.OPEN_CHART> {\n  chartData: Popups;\n}\n\n/** Action fired when updating the hide_shutdown flag */\nexport interface UpdateHideShutdown extends Action<typeof ActionType.UPDATE_HIDE_SHUTDOWN> {\n  value: boolean;\n}\n\n/** Action fired when updating the allow_cell_edits flag */\nexport interface UpdateAllowCellEdits extends Action<typeof ActionType.UPDATE_ALLOW_CELL_EDITS> {\n  value: boolean | string[];\n}\n\n/** Action fired when updating the hide_header_editor flag */\nexport interface UpdateHideHeaderEditor extends Action<typeof ActionType.UPDATE_HIDE_HEADER_EDITOR> {\n  value: boolean;\n}\n\n/** Action fired when updating the lock_header_menu flag */\nexport interface UpdateLockHeaderMenu extends Action<typeof ActionType.UPDATE_LOCK_HEADER_MENU> {\n  value: boolean;\n}\n\n/** Action fired when updating the hide_header_menu flag */\nexport interface UpdateHideHeaderMenu extends Action<typeof ActionType.UPDATE_HIDE_HEADER_MENU> {\n  value: boolean;\n}\n\n/** Action fired when updating the hide_main_menu flag */\nexport interface UpdateHideMainMenu extends Action<typeof ActionType.UPDATE_HIDE_MAIN_MENU> {\n  value: boolean;\n}\n\n/** Action fired when updating the hide_column_menus flag */\nexport interface UpdateHideColumnMenus extends Action<typeof ActionType.UPDATE_HIDE_COLUMN_MENUS> {\n  value: boolean;\n}\n\n/** Action fired when updating the enable_custom_filters flag */\nexport interface UpdateEnableCustomFilters extends Action<typeof ActionType.UPDATE_ENABLE_CUSTOM_FILTERS> {\n  value: boolean;\n}\n\n/** Type definition encompassing all application actions */\nexport type AppActionTypes =\n  | InitAction\n  | ClearEditAction\n  | ConvertToXarrayAction\n  | HideMenuTooltipAction\n  | ShowRibbonMenuAction\n  | HideRibbonMenuAction\n  | HideSidePanelAction\n  | ClearDataViewerUpdateAction\n  | StopResizeAction\n  | CloseChartAction\n  | LoadingDatasetsAction\n  | ClearMaxWidthAction\n  | ClearMaxHeightAction\n  | ToggleMenuPinnedAction\n  | LoadPreviewAction\n  | EditedCellAction\n  | EditedTextAreaHeightAction\n  | ToggleColumnAction\n  | ToggleMenuAction\n  | OpenFormattingAction\n  | CloseFormattingAction\n  | UpdateXarrayDimAction\n  | UpdateFilteredRangesAction\n  | UpdateSettingsAction\n  | ShowMenuTooltipAction\n  | OpenRibbonDropdownAction\n  | SidePanelAction\n  | DataViewerUpdateAction\n  | DragResizeAction\n  | SetThemeAction\n  | SetLanguageAction\n  | UpdateMaxColumnWidthAction\n  | UpdateMaxRowHeightAction\n  | SetQueryEngineAction\n  | UpdateShowAllHeatmapColumnsAction\n  | OpenChartAction\n  | SetRangeStateAction\n  | UpdateHideShutdown\n  | UpdateAllowCellEdits\n  | UpdateHideHeaderEditor\n  | UpdateLockHeaderMenu\n  | UpdateHideHeaderMenu\n  | UpdateHideMainMenu\n  | UpdateHideColumnMenus\n  | UpdateEnableCustomFilters;\n\n/** Type definition for redux application actions */\nexport type AppActions<R> = ThunkAction<R, AppState, Record<string, unknown>, AnyAction>;\n", "import { AnyAction, Store } from 'redux';\n\nimport * as serverState from '../../dtale/serverStateManagement';\nimport { AppState, QueryEngine, SidePanelType } from '../state/AppState';\n\nimport {\n  ActionType,\n  AppActions,\n  InitAction,\n  SetQueryEngineAction,\n  SidePanelAction,\n  ToggleColumnAction,\n  UpdateShowAllHeatmapColumnsAction,\n  UpdateXarrayDimAction,\n} from './AppActions';\n\nexport const init = (): InitAction => ({ type: ActionType.INIT_PARAMS });\n\nexport const loadBackgroundMode = (store: Store<AppState, AnyAction>): void => {\n  const { settings } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_SETTINGS,\n    settings: { backgroundMode: settings.backgroundMode ?? (!!settings.rangeHighlight?.length ? 'range' : undefined) },\n  });\n};\n\nexport const loadHideShutdown = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideShutdown } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_SHUTDOWN,\n    value: hideShutdown ?? settings.hide_shutdown ?? hideShutdown,\n  });\n};\n\nexport const loadAllowCellEdits = (store: Store<AppState, AnyAction>): void => {\n  const { settings, allowCellEdits } = store.getState();\n  store.dispatch({ type: ActionType.UPDATE_ALLOW_CELL_EDITS, value: settings.allow_cell_edits ?? allowCellEdits });\n};\n\nexport const loadHideHeaderEditor = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideHeaderEditor } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_HEADER_EDITOR,\n    value: hideHeaderEditor ?? settings.hide_header_editor ?? hideHeaderEditor,\n  });\n};\n\nexport const loadLockHeaderMenu = (store: Store<AppState, AnyAction>): void => {\n  const { settings, lockHeaderMenu } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_LOCK_HEADER_MENU,\n    value: lockHeaderMenu ?? settings.lock_header_menu ?? lockHeaderMenu,\n  });\n};\n\nexport const loadHideHeaderMenu = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideHeaderMenu } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_HEADER_MENU,\n    value: hideHeaderMenu ?? settings.hide_header_menu ?? hideHeaderMenu,\n  });\n};\n\nexport const loadHideMainMenu = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideMainMenu } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_MAIN_MENU,\n    value: hideMainMenu ?? settings.hide_main_menu ?? hideMainMenu,\n  });\n};\n\nexport const loadHideColumnMenus = (store: Store<AppState, AnyAction>): void => {\n  const { settings, hideColumnMenus } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_HIDE_COLUMN_MENUS,\n    value: hideColumnMenus ?? settings.hide_column_menus ?? hideColumnMenus,\n  });\n};\n\nexport const loadEnableCustomFilters = (store: Store<AppState, AnyAction>): void => {\n  const { settings, enableCustomFilters } = store.getState();\n  store.dispatch({\n    type: ActionType.UPDATE_ENABLE_CUSTOM_FILTERS,\n    value: enableCustomFilters ?? settings.enable_custom_filters ?? enableCustomFilters,\n  });\n};\n\nexport const openCustomFilter = (): SidePanelAction => ({\n  type: ActionType.SHOW_SIDE_PANEL,\n  view: SidePanelType.FILTER,\n});\n\nexport const openPredefinedFilters = (): SidePanelAction => ({\n  type: ActionType.SHOW_SIDE_PANEL,\n  view: SidePanelType.PREDEFINED_FILTERS,\n});\n\nexport const toggleColumnMenu = (colName: string, headerRef: HTMLDivElement): ToggleColumnAction => ({\n  type: ActionType.TOGGLE_COLUMN_MENU,\n  colName,\n  headerRef,\n});\n\nexport const hideColumnMenu =\n  (colName: string): AppActions<void> =>\n  (dispatch, getState) => {\n    const { selectedCol } = getState();\n    // when clicking another header cell it calls this after the fact and thus causes the user to click again to show it\n    if (selectedCol === colName) {\n      dispatch({ type: ActionType.HIDE_COLUMN_MENU, colName });\n    }\n  };\n\nexport const closeColumnMenu = (): AppActions<void> => (dispatch, getState) =>\n  dispatch({ type: ActionType.HIDE_COLUMN_MENU, colName: getState().selectedCol });\n\nexport const updateXArrayDimAction = (xarrayDim: Record<string, boolean>): UpdateXarrayDimAction => ({\n  type: ActionType.UPDATE_XARRAY_DIM,\n  xarrayDim,\n});\n\nexport const updateXArrayDim =\n  (xarrayDim: Record<string, boolean>, callback: () => void): AppActions<void> =>\n  (dispatch) => {\n    dispatch(updateXArrayDimAction(xarrayDim));\n    callback();\n  };\n\nexport const convertToXArray =\n  (callback: () => void): AppActions<void> =>\n  (dispatch) => {\n    dispatch({ type: ActionType.CONVERT_TO_XARRAY });\n    callback();\n  };\n\nexport const setQueryEngine = (engine: QueryEngine): SetQueryEngineAction => ({\n  type: ActionType.SET_QUERY_ENGINE,\n  engine,\n});\n\nexport const isPopup = (): boolean => !!window.location.pathname?.startsWith('/dtale/popup');\n\nexport const isJSON = (str: string): boolean => {\n  try {\n    JSON.parse(str);\n  } catch (e) {\n    return false;\n  }\n  return true;\n};\n\nexport const getParams = (): Record<string, string | string[]> => {\n  const search = location.search.substring(1);\n  if (!search) {\n    return {};\n  }\n  const params = JSON.parse(\n    '{\"' + decodeURI(search).replace(/\"/g, '\\\\\"').replace(/&/g, '\",\"').replace(/=/g, '\":\"') + '\"}',\n  );\n  return Object.keys(params).reduce((res: Record<string, string | string[]>, key: string) => {\n    const value = `${params[key]}`;\n    if (value) {\n      if (value.includes(',') && !isJSON(value)) {\n        return { ...res, [key]: value.split(',') };\n      }\n      return { ...res, [key]: value };\n    }\n    return res;\n  }, {});\n};\n\nexport const updateFilteredRanges =\n  (query: string): AppActions<Promise<void>> =>\n  async (dispatch, getState) => {\n    const { dataId, filteredRanges, isArcticDB, columnCount } = getState();\n    if (!!isArcticDB && (isArcticDB >= 1_000_000 || columnCount > 100)) {\n      return;\n    }\n    const currQuery = filteredRanges?.query ?? '';\n    if (currQuery !== query) {\n      const ranges = await serverState.loadFilteredRanges(dataId!);\n      dispatch({ type: ActionType.UPDATE_FILTERED_RANGES, ranges });\n    }\n  };\n\nexport const updateMaxWidth =\n  (width: number): AppActions<void> =>\n  (dispatch) => {\n    dispatch({ type: ActionType.UPDATE_MAX_WIDTH, width });\n    dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-width', width } });\n  };\n\nexport const clearMaxWidth = (): AppActions<void> => (dispatch) => {\n  dispatch({ type: ActionType.CLEAR_MAX_WIDTH });\n  dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-width', width: null } });\n};\n\nexport const updateMaxHeight =\n  (height: number): AppActions<void> =>\n  (dispatch) => {\n    dispatch({ type: ActionType.UPDATE_MAX_HEIGHT, height });\n    dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-height', height } });\n  };\n\nexport const clearMaxHeight = (): AppActions<void> => (dispatch) => {\n  dispatch({ type: ActionType.CLEAR_MAX_HEIGHT });\n  dispatch({ type: ActionType.DATA_VIEWER_UPDATE, update: { type: 'update-max-height', height: null } });\n};\n\nexport const updateShowAllHeatmapColumns = (showAllHeatmapColumns: boolean): UpdateShowAllHeatmapColumnsAction => ({\n  type: ActionType.UPDATE_SHOW_ALL_HEATMAP_COLUMNS,\n  showAllHeatmapColumns,\n});\n", "import { ActionType, AppActionTypes } from '../../actions/AppActions';\nimport { QueryEngine, ThemeType, Version } from '../../state/AppState';\nimport { getHiddenValue, toBool, toFloat, toJson } from '../utils';\n\nexport const hideShutdown = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_shutdown'));\n    case ActionType.UPDATE_HIDE_SHUTDOWN:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideHeaderEditor = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_header_editor'));\n    case ActionType.UPDATE_HIDE_HEADER_EDITOR:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const lockHeaderMenu = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('lock_header_menu'));\n    case ActionType.UPDATE_LOCK_HEADER_MENU:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideHeaderMenu = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_header_menu'));\n    case ActionType.UPDATE_HIDE_HEADER_MENU:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideMainMenu = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_main_menu'));\n    case ActionType.UPDATE_HIDE_MAIN_MENU:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const hideColumnMenus = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_column_menus'));\n    case ActionType.UPDATE_HIDE_COLUMN_MENUS:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return true;\n    default:\n      return state;\n  }\n};\n\nexport const enableCustomFilters = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('enable_custom_filters'));\n    case ActionType.UPDATE_ENABLE_CUSTOM_FILTERS:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const openCustomFilterOnStartup = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('open_custom_filter_on_startup'));\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const openPredefinedFiltersOnStartup = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return (\n        toBool(getHiddenValue('open_predefined_filters_on_startup')) &&\n        getHiddenValue('predefined_filters') !== undefined\n      );\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const hideDropRows = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('hide_drop_rows'));\n    default:\n      return state;\n  }\n};\n\nexport const allowCellEdits = (state: boolean | string[] = true, action: AppActionTypes): boolean | string[] => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS: {\n      return toJson(getHiddenValue('allow_cell_edits'));\n    }\n    case ActionType.UPDATE_ALLOW_CELL_EDITS:\n      return action.value;\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const theme = (state = ThemeType.LIGHT, action: AppActionTypes): ThemeType => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS: {\n      const themeStr = getHiddenValue('theme');\n      const themeVal = Object.values(ThemeType).find((t) => t.valueOf() === themeStr);\n      return themeVal ?? state;\n    }\n    case ActionType.SET_THEME: {\n      const body = document.getElementsByTagName('body')[0];\n      body.classList.remove(`${state}-mode`);\n      body.classList.add(`${action.theme}-mode`);\n      return action.theme;\n    }\n    default:\n      return state;\n  }\n};\n\nexport const language = (state = 'en', action: AppActionTypes): string => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('language') ?? state;\n    case ActionType.SET_LANGUAGE:\n      return action.language;\n    default:\n      return state;\n  }\n};\n\nexport const pythonVersion = (state: Version | null = null, action: AppActionTypes): Version | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n    case ActionType.LOAD_PREVIEW: {\n      const version = getHiddenValue('python_version');\n      if (version) {\n        return version.split('.').map((subVersion: string) => parseInt(subVersion, 10)) as Version;\n      }\n      return state;\n    }\n    default:\n      return state;\n  }\n};\n\nexport const isVSCode = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toBool(getHiddenValue('is_vscode')) && global.top !== global.self;\n    case ActionType.LOAD_PREVIEW:\n      return false;\n    default:\n      return state;\n  }\n};\n\nexport const isArcticDB = (state = 0, action: AppActionTypes): number => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('is_arcticdb')) as number;\n    case ActionType.LOAD_PREVIEW:\n      return 0;\n    default:\n      return state;\n  }\n};\n\nexport const arcticConn = (state = '', action: AppActionTypes): string => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('arctic_conn') ?? '';\n    case ActionType.LOAD_PREVIEW:\n      return '';\n    default:\n      return state;\n  }\n};\n\nexport const columnCount = (state = 0, action: AppActionTypes): number => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('column_count')) as number;\n    case ActionType.LOAD_PREVIEW:\n      return 0;\n    default:\n      return state;\n  }\n};\n\nexport const maxColumnWidth = (state: number | null = null, action: AppActionTypes): number | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('max_column_width'), true) ?? null;\n    case ActionType.UPDATE_MAX_WIDTH:\n      return action.width;\n    case ActionType.CLEAR_MAX_WIDTH:\n      return null;\n    default:\n      return state;\n  }\n};\n\nexport const maxRowHeight = (state: number | null = null, action: AppActionTypes): number | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return toFloat(getHiddenValue('max_row_height'), true) ?? null;\n    case ActionType.UPDATE_MAX_HEIGHT:\n      return action.height;\n    case ActionType.CLEAR_MAX_HEIGHT:\n      return null;\n    default:\n      return state;\n  }\n};\n\nexport const mainTitle = (state: string | null = null, action: AppActionTypes): string | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('main_title') ?? null;\n    default:\n      return state;\n  }\n};\n\nexport const mainTitleFont = (state: string | null = null, action: AppActionTypes): string | null => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS:\n      return getHiddenValue('main_title_font') ?? null;\n    default:\n      return state;\n  }\n};\n\nexport const queryEngine = (state = QueryEngine.PYTHON, action: AppActionTypes): QueryEngine => {\n  switch (action.type) {\n    case ActionType.INIT_PARAMS: {\n      const engineStr = getHiddenValue('query_engine');\n      const queryEngineVal = Object.values(QueryEngine).find((key) => key.valueOf() === engineStr);\n      return queryEngineVal ?? state;\n    }\n    case ActionType.SET_QUERY_ENGINE:\n      return action.engine;\n    default:\n      return state;\n  }\n};\n\nexport const showAllHeatmapColumns = (state = false, action: AppActionTypes): boolean => {\n  switch (action.type) {\n    case ActionType.UPDATE_SHOW_ALL_HEATMAP_COLUMNS:\n      return action.showAllHeatmapColumns;\n    default:\n      return state;\n  }\n};\n", "import { createSelector } from '@reduxjs/toolkit';\n\nimport { RangeSelection } from '../dtale/DataViewerState';\n\nimport {\n  AppState,\n  DataViewerUpdate,\n  FilteredRanges,\n  InstanceSettings,\n  Popups,\n  PredefinedFilter,\n  QueryEngine,\n  RibbonDropdownProps,\n  SidePanelProps,\n  ThemeType,\n  Version,\n} from './state/AppState';\n\nexport const selectDataId = (state: AppState): string => state.dataId;\nexport const selectAuth = (state: AppState): boolean => state.auth;\nexport const selectUsername = (state: AppState): string | null => state.username;\nexport const selectEditedCell = (state: AppState): string | null => state.editedCell;\nexport const selectIsVSCode = (state: AppState): boolean => state.isVSCode;\nexport const selectCtrlRows = (state: AppState): number[] | null => state.ctrlRows;\nexport const selectCtrlCols = (state: AppState): number[] | null => state.ctrlCols;\nexport const selectIsArcticDB = (state: AppState): number => state.isArcticDB;\nexport const selectColumnCount = (state: AppState): number => state.columnCount;\nexport const selectColumnMenuOpen = (state: AppState): boolean => state.columnMenuOpen;\nexport const selectSelectedCol = (state: AppState): string | null => state.selectedCol;\nexport const selectSelectedColRef = (state: AppState): HTMLDivElement | null => state.selectedColRef;\nexport const selectIsPreview = (state: AppState): boolean => state.isPreview;\nexport const selectRibbonDropdown = (state: AppState): RibbonDropdownProps => state.ribbonDropdown;\nexport const selectRibbonDropdownVisible = createSelector(\n  [selectRibbonDropdown],\n  (ribbonDropdown) => ribbonDropdown.visible,\n);\nexport const selectRibbonDropdownElement = createSelector(\n  [selectRibbonDropdown],\n  (ribbonDropdown) => ribbonDropdown.element,\n);\nexport const selectRibbonDropdownName = createSelector([selectRibbonDropdown], (ribbonDropdown) => ribbonDropdown.name);\nexport const selectSettings = (state: AppState): InstanceSettings => state.settings;\nexport const selectColumnFilters = createSelector([selectSettings], (settings) => settings?.columnFilters);\nexport const selectOutlierFilters = createSelector([selectSettings], (settings) => settings?.outlierFilters);\nexport const selectInvertFilter = createSelector([selectSettings], (settings) => settings?.invertFilter);\nexport const selectQuery = createSelector([selectSettings], (settings) => settings?.query);\nexport const selectPredefinedFilters = createSelector([selectSettings], (settings) => settings?.predefinedFilters);\nexport const selectSortInfo = createSelector([selectSettings], (settings) => settings?.sortInfo);\nexport const selectHighlightFilter = createSelector([selectSettings], (settings) => settings?.highlightFilter);\nexport const selectVerticalHeaders = createSelector([selectSettings], (settings) => settings?.verticalHeaders);\nexport const selectSettingsHideHeaderEditor = createSelector(\n  [selectSettings],\n  (settings) => settings?.hide_header_editor,\n);\nexport const selectBackgroundMode = createSelector([selectSettings], (settings) => settings?.backgroundMode);\nexport const selectRangeHighlight = createSelector([selectSettings], (settings) => settings?.rangeHighlight);\nexport const selectBaseLockHeaderMenu = (state: AppState): boolean => state.lockHeaderMenu;\nexport const selectBaseHideHeaderMenu = (state: AppState): boolean => state.hideHeaderMenu;\nexport const selectBaseHideMainMenu = (state: AppState): boolean => state.hideMainMenu;\nexport const selectBaseHideColumnMenus = (state: AppState): boolean => state.hideColumnMenus;\nexport const selectBaseEnableCustomFilters = (state: AppState): boolean => state.enableCustomFilters;\nexport const selectFilteredRanges = (state: AppState): FilteredRanges => state.filteredRanges;\nexport const selectShowAllHeatmapColumns = (state: AppState): boolean => state.showAllHeatmapColumns;\nexport const selectChartData = (state: AppState): Popups => state.chartData;\nexport const selectSidePanel = (state: AppState): SidePanelProps => state.sidePanel;\nexport const selectSidePanelVisible = createSelector([selectSidePanel], (sidePanel) => sidePanel.visible);\nexport const selectSidePanelView = createSelector([selectSidePanel], (sidePanel) => sidePanel.view);\nexport const selectSidePanelColumn = createSelector([selectSidePanel], (sidePanel) => sidePanel.column);\nexport const selectSidePanelOffset = createSelector([selectSidePanel], (sidePanel) => sidePanel.offset);\nexport const selectTheme = (state: AppState): ThemeType => state.theme;\nexport const selectPythonVersion = (state: AppState): Version | null => state.pythonVersion;\nexport const selectMaxColumnWidth = (state: AppState): number | null => state.maxColumnWidth;\nexport const selectAllowCellEdits = (state: AppState): boolean | string[] => state.allowCellEdits;\nexport const selectRowRange = (state: AppState): RangeSelection<number> | null => state.rowRange;\nexport const selectColumnRange = (state: AppState): RangeSelection<number> | null => state.columnRange;\nexport const selectRangeSelect = (state: AppState): RangeSelection<string> | null => state.rangeSelect;\nexport const selectSelectedRow = (state: AppState): number | null => state.selectedRow;\nexport const selectFormattingOpen = (state: AppState): string | null => state.formattingOpen;\nexport const selectMenuPinned = (state: AppState): boolean => state.menuPinned;\nexport const selectMenuOpen = (state: AppState): boolean => state.menuOpen;\nexport const selectBaseHideHeaderEditor = (state: AppState): boolean => state.hideHeaderEditor;\nexport const selectHideHeaderEditor = createSelector(\n  [selectSettingsHideHeaderEditor, selectBaseHideHeaderEditor],\n  (settingsHideHeaderEditor, hideHeaderEditor) => settingsHideHeaderEditor ?? hideHeaderEditor,\n);\nexport const selectPredefinedFilterConfigs = (state: AppState): PredefinedFilter[] => state.predefinedFilters;\nexport const selectHideDropRows = (state: AppState): boolean => state.hideDropRows;\nexport const selectArcticConn = (state: AppState): string => state.arcticConn;\nexport const selectBaseRibbonMenuOpen = (state: AppState): boolean => state.ribbonMenuOpen;\nconst selectSettingsLockHeaderMenu = createSelector([selectSettings], (settings) => settings?.lock_header_menu);\nexport const selectLockHeaderMenu = createSelector(\n  [selectSettingsLockHeaderMenu, selectBaseLockHeaderMenu],\n  (settingsLockHeaderMenu, lockHeaderMenu) => settingsLockHeaderMenu ?? lockHeaderMenu,\n);\nconst selectSettingsHideHeaderMenu = createSelector([selectSettings], (settings) => settings?.hide_header_menu);\nexport const selectHideHeaderMenu = createSelector(\n  [selectSettingsHideHeaderMenu, selectBaseHideHeaderMenu],\n  (settingsHideHeaderMenu, hideHeaderMenu) => settingsHideHeaderMenu ?? hideHeaderMenu,\n);\nconst selectSettingsHideMainMenu = createSelector([selectSettings], (settings) => settings?.hide_main_menu);\nexport const selectHideMainMenu = createSelector(\n  [selectSettingsHideMainMenu, selectBaseHideMainMenu],\n  (settingsHideMainMenu, hideMainMenu) => settingsHideMainMenu ?? hideMainMenu,\n);\nconst selectSettingsHideColumnMenus = createSelector([selectSettings], (settings) => settings?.hide_column_menus);\nexport const selectHideColumnMenus = createSelector(\n  [selectSettingsHideColumnMenus, selectBaseHideColumnMenus],\n  (settingsHideColumnMenus, hideColumnMenus) => settingsHideColumnMenus ?? hideColumnMenus,\n);\nconst selectSettingsEnableCustomFilters = createSelector(\n  [selectSettings],\n  (settings) => settings?.enable_custom_filters,\n);\nexport const selectEnableCustomFilters = createSelector(\n  [selectSettingsEnableCustomFilters, selectBaseEnableCustomFilters],\n  (settingsEnableCustomFilters, enableCustomFilters) => settingsEnableCustomFilters ?? enableCustomFilters,\n);\nexport const selectRibbonMenuOpen = createSelector(\n  [selectBaseRibbonMenuOpen, selectLockHeaderMenu, selectHideHeaderMenu],\n  (ribbonMenuOpen, lockHeaderMenu, hideHeaderMenu) => (ribbonMenuOpen || lockHeaderMenu) && !hideHeaderMenu,\n);\nexport const selectMainTitle = (state: AppState): string | null => state.mainTitle;\nexport const selectMainTitleFont = (state: AppState): string | null => state.mainTitleFont;\nexport const selectDataViewerUpdate = (state: AppState): DataViewerUpdate | null => state.dataViewerUpdate;\nexport const selectMaxRowHeight = (state: AppState): number | null => state.maxRowHeight;\nexport const selectEditedTextAreaHeight = (state: AppState): number => state.editedTextAreaHeight;\nexport const selectDragResize = (state: AppState): number | null => state.dragResize;\nexport const selectXArray = (state: AppState): boolean => state.xarray;\nexport const selectXArrayDim = (state: AppState): Record<string, any> => state.xarrayDim;\nexport const selectIFrame = (state: AppState): boolean => state.iframe;\nexport const selectLanguage = (state: AppState): string => state.language;\nexport const selectHideShutdown = (state: AppState): boolean => state.hideShutdown;\nexport const selectQueryEngine = (state: AppState): QueryEngine => state.queryEngine;\n", "import { RGBColor } from 'react-color';\n\nimport {\n  Bounds,\n  ColumnDef,\n  ColumnFilter,\n  ColumnFormat,\n  DataViewerPropagateState,\n  OutlierFilter,\n  RangeSelection,\n} from '../../dtale/DataViewerState';\n\n/** Base properties for react-select dropdown options */\nexport interface BaseOption<T> {\n  value: T;\n  label?: string | null;\n}\n\n/** Base properties for ButtonToggle options */\nexport interface ButtonOption<T> {\n  value: T;\n  label?: string | React.ReactNode;\n}\n\n/** Object which can be turned on/off */\nexport interface HasActivation {\n  active: boolean;\n}\n\n/** Object which has a visiblity flag */\nexport interface HasVisibility {\n  visible: boolean;\n}\n\nexport const initialVisibility: HasVisibility = { visible: false };\n\n/** Properties of a main menu tooltip */\nexport interface MenuTooltipProps extends HasVisibility {\n  element?: HTMLElement;\n  content?: React.ReactNode;\n}\n\n/** Ribbon dropdown types */\nexport enum RibbonDropdownType {\n  MAIN = 'main',\n  ACTIONS = 'actions',\n  VISUALIZE = 'visualize',\n  HIGHLIGHT = 'highlight',\n  SETTINGS = 'settings',\n}\n\n/** Properties of a ribbon menu dropdown */\nexport interface RibbonDropdownProps extends HasVisibility {\n  element?: HTMLDivElement;\n  name?: RibbonDropdownType;\n}\n\n/** Side panel types */\nexport enum SidePanelType {\n  SHOW_HIDE = 'show_hide',\n  DESCRIBE = 'describe',\n  MISSINGNO = 'missingno',\n  CORR_ANALYSIS = 'corr_analysis',\n  CORRELATIONS = 'correlations',\n  PPS = 'pps',\n  FILTER = 'filter',\n  PREDEFINED_FILTERS = 'predefined_filters',\n  GAGE_RNR = 'gage_rnr',\n  TIMESERIES_ANALYSIS = 'timeseries_analysis',\n}\n\n/** Properties of the current side panel */\nexport interface SidePanelProps extends HasVisibility {\n  view?: SidePanelType;\n  column?: string;\n  offset?: number;\n}\n\n/** Different types of data viewer updates */\nexport enum DataViewerUpdateType {\n  TOGGLE_COLUMNS = 'toggle-columns',\n  UPDATE_MAX_WIDTH = 'update-max-width',\n  UPDATE_MAX_HEIGHT = 'update-max-height',\n  DROP_COLUMNS = 'drop-columns',\n}\n\n/** Base properties for a DataViewer update */\ninterface BaseDataViewerUpdateProps<T extends DataViewerUpdateType> {\n  type: T;\n}\n\n/** Toggle columns DataViewer update */\nexport interface ToggleColumnsDataViewerUpdate extends BaseDataViewerUpdateProps<DataViewerUpdateType.TOGGLE_COLUMNS> {\n  columns: Record<string, boolean>;\n}\n\n/** Update maximum width DataViewer update */\nexport interface UpdateMaxWidthDataViewerUpdate\n  extends BaseDataViewerUpdateProps<DataViewerUpdateType.UPDATE_MAX_WIDTH> {\n  width: number;\n}\n\n/** Update maximum row height DataViewer update */\nexport type UpdateMaxHeightDataViewerUpdate = BaseDataViewerUpdateProps<DataViewerUpdateType.UPDATE_MAX_HEIGHT>;\n\n/** Drop columns DataViewer update */\nexport interface DropColumnsDataViewerUpdate extends BaseDataViewerUpdateProps<DataViewerUpdateType.DROP_COLUMNS> {\n  columns: string[];\n}\n\n/** DataViewer updates */\nexport type DataViewerUpdate =\n  | ToggleColumnsDataViewerUpdate\n  | UpdateMaxWidthDataViewerUpdate\n  | UpdateMaxHeightDataViewerUpdate\n  | DropColumnsDataViewerUpdate;\n\n/** Popup type names */\nexport enum PopupType {\n  HIDDEN = 'hidden',\n  FILTER = 'filter',\n  COLUMN_ANALYSIS = 'column-analysis',\n  CORRELATIONS = 'correlations',\n  PPS = 'pps',\n  BUILD = 'build',\n  TYPE_CONVERSION = 'type-conversion',\n  CLEANERS = 'cleaners',\n  RESHAPE = 'reshape',\n  ABOUT = 'about',\n  CONFIRM = 'confirm',\n  COPY_RANGE = 'copy-range',\n  COPY_COLUMN_RANGE = 'copy-column-range',\n  COPY_ROW_RANGE = 'copy-row-range',\n  RANGE = 'range',\n  XARRAY_DIMENSIONS = 'xarray-dimensions',\n  XARRAY_INDEXES = 'xarray-indexes',\n  RENAME = 'rename',\n  REPLACEMENT = 'replacement',\n  ERROR = 'error',\n  INSTANCES = 'instances',\n  VARIANCE = 'variance',\n  UPLOAD = 'upload',\n  DUPLICATES = 'duplicates',\n  CHARTS = 'charts',\n  DESCRIBE = 'describe',\n  EXPORT = 'export',\n  ARCTICDB = 'arcticdb',\n  JUMP_TO_COLUMN = 'jump_to_column',\n}\n\n/** Configuration for any data for a popup */\nexport interface PopupData<T extends PopupType> extends HasVisibility {\n  type: T;\n  title?: string;\n  size?: 'sm' | 'lg' | 'xl';\n  backdrop?: true | false | 'static';\n}\n\n/** Object which has a selected column */\ninterface HasColumnSelection {\n  selectedCol: string;\n}\n\n/** Popup configuration for About popup */\nexport type HiddenPopupData = PopupData<typeof PopupType.HIDDEN>;\n\nexport const initialPopup: HiddenPopupData = { ...initialVisibility, type: PopupType.HIDDEN };\n\n/** Popup configuration for About popup */\nexport type AboutPopupData = PopupData<typeof PopupType.ABOUT>;\n\n/** Popup configuration for Confirmation popup */\nexport interface ConfirmationPopupData extends PopupData<typeof PopupType.CONFIRM> {\n  msg: string;\n  yesAction?: () => void;\n}\n\n/** Base popup configuration for copying ranges */\ninterface BaseCopyRangeToClipboardData {\n  text: string;\n  headers: string[];\n}\n\n/** Popup configuration for CopyRangeToClipbard popup */\nexport type CopyRangeToClipboardPopupData = PopupData<typeof PopupType.COPY_RANGE> & BaseCopyRangeToClipboardData;\n\n/** Popup configuration for CopyRangeToClipbard popup */\nexport type CopyColumnRangeToClipboardPopupData = PopupData<typeof PopupType.COPY_COLUMN_RANGE> &\n  BaseCopyRangeToClipboardData;\n\n/** Popup configuration for CopyRangeToClipbard popup */\nexport type CopyRowRangeToClipboardPopupData = PopupData<typeof PopupType.COPY_ROW_RANGE> &\n  BaseCopyRangeToClipboardData;\n\n/** Popup configuration for Error popup */\nexport interface ErrorPopupData extends PopupData<typeof PopupType.ERROR> {\n  error: string;\n  traceback?: string;\n}\n\n/** Popup configuration for Error popup */\nexport interface ExportPopupData extends PopupData<typeof PopupType.EXPORT> {\n  rows: number;\n}\n\n/** Popup configuration for Error popup */\nexport interface RenamePopupData extends PopupData<typeof PopupType.RENAME>, HasColumnSelection {\n  columns: ColumnDef[];\n}\n\n/** Popup configuration for JumpToColumn popup */\nexport interface JumpToColumnPopupData extends PopupData<typeof PopupType.JUMP_TO_COLUMN> {\n  columns: ColumnDef[];\n}\n\n/** Popup configuration for RangeHighlight popup */\nexport interface RangeHighlightPopupData extends PopupData<typeof PopupType.RANGE> {\n  rangeHighlight?: RangeHighlightConfig;\n  backgroundMode?: string;\n  columns: ColumnDef[];\n}\n\n/** Popup configuration for XArrayDimensions popup */\nexport type XArrayDimensionsPopupData = PopupData<typeof PopupType.XARRAY_DIMENSIONS>;\n\n/** Popup configuration for XArrayIndexes popup */\nexport interface XArrayIndexesPopupData extends PopupData<typeof PopupType.XARRAY_INDEXES> {\n  columns: ColumnDef[];\n}\n\n/** Base properties for any column analysis popup */\nexport interface BaseColumnAnalysisPopupData extends HasColumnSelection {\n  query?: string;\n}\n\n/** Popup configuration for ColumnAnalysis popup */\nexport type ColumnAnalysisPopupData = PopupData<typeof PopupType.COLUMN_ANALYSIS> & BaseColumnAnalysisPopupData;\n\n/** Base properties for Correlation popups */\nexport interface BaseCorrelationsPopupData {\n  col1?: string;\n  col2?: string;\n}\n\n/** Popup configuration for Correlations popup */\nexport interface CorrelationsPopupData extends PopupData<typeof PopupType.CORRELATIONS>, BaseCorrelationsPopupData {\n  query?: string;\n}\n\n/** Popup configuration for Predictive Power Score popup */\nexport type PPSPopupData = PopupData<typeof PopupType.PPS> & BaseCorrelationsPopupData;\n\n/** Base popup configuration for column creation */\ninterface BaseCreateColumnPopupData {\n  selectedCol?: string;\n}\n\n/** Popup configuration for Create Column popup */\nexport type CreateColumnPopupData = PopupData<typeof PopupType.BUILD> & BaseCreateColumnPopupData;\n\n/** Popup configuration for Create Column - Type Conversion popup */\nexport type CreateTypeConversionPopupData = PopupData<typeof PopupType.TYPE_CONVERSION> & BaseCreateColumnPopupData;\n\n/** Popup configuration for Create Column - Cleaners popup */\nexport type CreateCleanersPopupData = PopupData<typeof PopupType.CLEANERS> & BaseCreateColumnPopupData;\n\n/** Popup configuration for Create Column popup */\nexport type ReshapePopupData = PopupData<typeof PopupType.RESHAPE>;\n\n/** Popup configuration for Charts popup */\nexport interface ChartsPopupData extends PopupData<typeof PopupType.CHARTS> {\n  query?: string;\n  x?: string;\n  y?: string[];\n  group?: string[];\n  aggregation?: string;\n  chartType?: string;\n  chartPerGroup?: boolean;\n}\n\n/** Popup configuration for Describe popup */\nexport interface DescribePopupData extends PopupData<typeof PopupType.DESCRIBE> {\n  selectedCol?: string;\n}\n\n/** Popup configuration for Duplicates popup */\nexport interface DuplicatesPopupData extends PopupData<typeof PopupType.DUPLICATES> {\n  selectedCol?: string;\n}\n\n/** Popup configuration for Filter popup */\nexport type CustomFilterPopupData = PopupData<typeof PopupType.FILTER>;\n\n/** Popup configuration for Upload popup */\nexport type UploadPopupData = PopupData<typeof PopupType.UPLOAD>;\n\n/** Popup configuration for ArcticDB popup */\nexport type ArcticDBPopupData = PopupData<typeof PopupType.ARCTICDB>;\n\n/** Popup configuration for Replacement popup */\nexport interface ReplacementPopupData extends PopupData<typeof PopupType.REPLACEMENT>, HasColumnSelection {\n  propagateState: DataViewerPropagateState;\n}\n\n/** Popup configuration for Variance popup */\nexport type VariancePopupData = PopupData<typeof PopupType.VARIANCE> & BaseColumnAnalysisPopupData;\n\n/** Popup configuration for Instances popup */\nexport type InstancesPopupData = PopupData<typeof PopupType.INSTANCES>;\n\n/** Popup configurations */\nexport type Popups =\n  | HiddenPopupData\n  | AboutPopupData\n  | ConfirmationPopupData\n  | CopyRangeToClipboardPopupData\n  | CopyColumnRangeToClipboardPopupData\n  | CopyRowRangeToClipboardPopupData\n  | ErrorPopupData\n  | RenamePopupData\n  | RangeHighlightPopupData\n  | XArrayDimensionsPopupData\n  | XArrayIndexesPopupData\n  | ColumnAnalysisPopupData\n  | ChartsPopupData\n  | CorrelationsPopupData\n  | PPSPopupData\n  | CreateColumnPopupData\n  | ReshapePopupData\n  | ChartsPopupData\n  | DescribePopupData\n  | DuplicatesPopupData\n  | CustomFilterPopupData\n  | UploadPopupData\n  | ReplacementPopupData\n  | VariancePopupData\n  | CreateTypeConversionPopupData\n  | CreateCleanersPopupData\n  | InstancesPopupData\n  | ExportPopupData\n  | ArcticDBPopupData\n  | JumpToColumnPopupData;\n\n/** Sort directions */\nexport enum SortDir {\n  ASC = 'ASC',\n  DESC = 'DESC',\n}\n\n/** Type definition for column being sorted and it's direction. */\nexport type SortDef = [string, SortDir];\n\n/** Value holder for predefined filters */\nexport interface PredefinedFilterValue extends HasActivation {\n  value?: any | any[];\n}\n\n/** Settings available to each instance (piece of data) of D-Tale */\nexport interface InstanceSettings {\n  locked?: string[];\n  allow_cell_edits: boolean | string[];\n  precision: number;\n  columnFormats?: Record<string, ColumnFormat>;\n  backgroundMode?: string;\n  rangeHighlight?: RangeHighlightConfig;\n  verticalHeaders: boolean;\n  predefinedFilters: Record<string, PredefinedFilterValue>;\n  sortInfo?: SortDef[];\n  nanDisplay?: string;\n  startup_code?: string;\n  query?: string;\n  highlightFilter?: boolean;\n  outlierFilters?: Record<string, OutlierFilter>;\n  filteredRanges?: FilteredRanges;\n  columnFilters?: Record<string, ColumnFilter>;\n  invertFilter?: boolean;\n  hide_shutdown: boolean;\n  column_edit_options?: Record<string, string[]>;\n  hide_header_editor: boolean;\n  lock_header_menu: boolean;\n  hide_header_menu: boolean;\n  hide_main_menu: boolean;\n  hide_column_menus: boolean;\n  isArcticDB?: number;\n  enable_custom_filters: boolean;\n}\n\nexport const BASE_INSTANCE_SETTINGS: InstanceSettings = Object.freeze({\n  allow_cell_edits: true,\n  hide_shutdown: false,\n  precision: 2,\n  verticalHeaders: false,\n  predefinedFilters: {},\n  hide_header_editor: false,\n  lock_header_menu: false,\n  hide_header_menu: false,\n  hide_main_menu: false,\n  hide_column_menus: false,\n  enable_custom_filters: false,\n});\n\n/** Type definition for semantic versioning of python */\nexport type Version = [number, number, number];\n\n/** Different themes available for D-Tale */\nexport enum ThemeType {\n  LIGHT = 'light',\n  DARK = 'dark',\n}\n\n/** Python query engines for executing custom queries */\nexport enum QueryEngine {\n  PYTHON = 'python',\n  NUMEXPR = 'numexpr',\n}\n\n/** Application-level settings */\nexport interface AppSettings {\n  hideShutdown: boolean;\n  openCustomFilterOnStartup: boolean;\n  openPredefinedFiltersOnStartup: boolean;\n  hideDropRows: boolean;\n  allowCellEdits: boolean | string[];\n  theme: ThemeType;\n  language: string;\n  pythonVersion: Version | null;\n  isVSCode: boolean;\n  isArcticDB: number;\n  arcticConn: string;\n  columnCount: number;\n  maxColumnWidth: number | null;\n  maxRowHeight: number | null;\n  mainTitle: string | null;\n  mainTitleFont: string | null;\n  queryEngine: QueryEngine;\n  showAllHeatmapColumns: boolean;\n  hideHeaderEditor: boolean;\n  lockHeaderMenu: boolean;\n  hideHeaderMenu: boolean;\n  hideMainMenu: boolean;\n  hideColumnMenus: boolean;\n  enableCustomFilters: boolean;\n}\n\n/** Properties for specifying filtered ranges */\nexport interface FilteredRanges {\n  query?: string;\n  dtypes?: Record<string, ColumnDef>;\n  ranges?: Record<string, Bounds>;\n  overall?: Bounds;\n}\n\n/** Predefined filter types */\nexport enum PredfinedFilterInputType {\n  INPUT = 'input',\n  SELECT = 'select',\n  MULTISELECT = 'multiselect',\n}\n\n/** Predefined filter properties */\nexport interface PredefinedFilter extends HasActivation {\n  column: string;\n  default?: string | number;\n  description?: string;\n  inputType: PredfinedFilterInputType;\n  name: string;\n}\n\n/** Range highlight configuration properties */\nexport interface RangeHighlightModeCfg extends HasActivation {\n  value?: number;\n  color: RGBColor;\n}\n\n/** Different types of range highlighting */\nexport interface RangeHighlightModes {\n  equals: RangeHighlightModeCfg;\n  greaterThan: RangeHighlightModeCfg;\n  lessThan: RangeHighlightModeCfg;\n}\n\n/** Range highlighting for individual columns or \"all\" columns */\nexport interface RangeHighlightConfig {\n  [key: string | 'all']: RangeHighlightModes & HasActivation;\n}\n\n/** Range selection properties */\nexport interface RangeState {\n  rowRange: RangeSelection<number> | null;\n  columnRange: RangeSelection<number> | null;\n  rangeSelect: RangeSelection<string> | null;\n  ctrlRows: number[] | null;\n  ctrlCols: number[] | null;\n  selectedRow: number | null;\n}\n\n/** Properties of application state */\nexport interface AppState extends AppSettings, RangeState {\n  chartData: Popups;\n  dataId: string;\n  editedCell: string | null;\n  editedTextAreaHeight: number;\n  iframe: boolean;\n  columnMenuOpen: boolean;\n  selectedCol: string | null;\n  selectedColRef: HTMLDivElement | null;\n  xarray: boolean;\n  xarrayDim: Record<string, any>;\n  filteredRanges: FilteredRanges;\n  settings: InstanceSettings;\n  isPreview: boolean;\n  menuPinned: boolean;\n  menuTooltip: MenuTooltipProps;\n  ribbonMenuOpen: boolean;\n  ribbonDropdown: RibbonDropdownProps;\n  sidePanel: SidePanelProps;\n  dataViewerUpdate: DataViewerUpdate | null;\n  predefinedFilters: PredefinedFilter[];\n  dragResize: number | null;\n  auth: boolean;\n  username: string | null;\n  menuOpen: boolean;\n  formattingOpen: string | null;\n}\n", "[app]\ntheme = light\ngithub_fork = False\nhide_shutdown = False\npin_menu = False\nlanguage = en\nmax_column_width = 100\nmain_title = My App\nmain_title_font = Arial\nquery_engine = python\nhide_header_editor = False\nlock_header_menu = False\nhide_header_menu = False\nhide_main_menu = False\nhide_column_menus = False\nenable_custom_filters = False\n\n[charts]\nscatter_points = 15000\n3d_points = 40000\n\n[show]\nhost = localhost\nport = 8080\nreaper_on = True\nopen_browser = False\nignore_duplicate = True\nallow_cell_edits = True\ninplace = False\ndrop_index = False\nprecision = 6\nshow_columns = a,b\nhide_columns = c\ncolumn_formats = {\"a\": {\"fmt\": {\"html\": true}}}\nsort = a|ASC\nlocked = a,b\ncolumn_edit_options = {\"a\": [\"foo\", \"bar\", \"baz\"]}\nauto_hide_empty_columns = False\nhighlight_filter = False\n\n[auth]\nactive = False\nusername = admin\npassword = admin\n", "[app]\nhide_shutdown = False\nhide_header_editor = False\nlock_header_menu = False\nhide_header_menu = False\nhide_main_menu = False\nhide_column_menus = False\nenable_custom_filters = False\n", "import mock\nimport os\nimport pytest\n\nfrom dtale.config import (\n    load_app_settings,\n    load_auth_settings,\n    load_config_state,\n    build_show_options,\n    set_config,\n)\nfrom tests import ExitStack\n\n\n@pytest.mark.unit\ndef test_load_app_settings():\n    settings = {\n        \"theme\": \"dark\",\n        \"github_fork\": False,\n        \"hide_shutdown\": True,\n        \"pin_menu\": True,\n        \"language\": \"cn\",\n        \"max_column_width\": 50,\n        \"query_engine\": \"numexpr\",\n        \"hide_header_editor\": True,\n        \"lock_header_menu\": True,\n        \"hide_header_menu\": True,\n        \"hide_main_menu\": True,\n        \"hide_column_menus\": True,\n        \"enable_custom_filters\": True,\n    }\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"dtale.global_state.APP_SETTINGS\", settings))\n\n        load_app_settings(None)\n        assert settings[\"hide_shutdown\"]\n        assert settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"cn\"\n        assert settings[\"theme\"] == \"dark\"\n        assert settings[\"max_column_width\"] == 50\n        assert settings[\"query_engine\"] == \"numexpr\"\n        assert settings[\"hide_header_editor\"]\n        assert settings[\"lock_header_menu\"]\n        assert settings[\"hide_header_menu\"]\n        assert settings[\"hide_main_menu\"]\n        assert settings[\"hide_column_menus\"]\n        assert settings[\"enable_custom_filters\"]\n\n        load_app_settings(\n            load_config_state(os.path.join(os.path.dirname(__file__), \"dtale.ini\"))\n        )\n        assert not settings[\"hide_shutdown\"]\n        assert not settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"en\"\n        assert settings[\"theme\"] == \"light\"\n        assert settings[\"max_column_width\"] == 100\n        assert settings[\"main_title\"] == \"My App\"\n        assert settings[\"main_title_font\"] == \"Arial\"\n        assert settings[\"query_engine\"] == \"python\"\n        assert not settings[\"hide_header_editor\"]\n        assert not settings[\"lock_header_menu\"]\n        assert not settings[\"hide_header_menu\"]\n        assert not settings[\"hide_main_menu\"]\n        assert not settings[\"hide_column_menus\"]\n        assert not settings[\"enable_custom_filters\"]\n\n\n@pytest.mark.unit\ndef test_load_app_settings_w_missing_props():\n    settings = {\n        \"theme\": \"light\",\n        \"github_fork\": False,\n        \"hide_shutdown\": True,\n        \"pin_menu\": True,\n        \"language\": \"cn\",\n        \"max_column_width\": None,\n        \"query_engine\": \"python\",\n        \"hide_header_editor\": True,\n        \"lock_header_menu\": True,\n        \"hide_header_menu\": True,\n        \"hide_main_menu\": True,\n        \"hide_column_menus\": True,\n        \"enable_custom_filters\": True,\n    }\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"dtale.global_state.APP_SETTINGS\", settings))\n\n        load_app_settings(None)\n        assert settings[\"hide_shutdown\"]\n        assert settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"cn\"\n        assert settings[\"max_column_width\"] is None\n        assert settings[\"hide_header_editor\"]\n        assert settings[\"lock_header_menu\"]\n        assert settings[\"hide_header_menu\"]\n        assert settings[\"hide_main_menu\"]\n        assert settings[\"hide_column_menus\"]\n        assert settings[\"enable_custom_filters\"]\n\n        load_app_settings(\n            load_config_state(\n                os.path.join(os.path.dirname(__file__), \"dtale_missing_props.ini\")\n            )\n        )\n        assert not settings[\"hide_shutdown\"]\n        assert settings[\"pin_menu\"]\n        assert settings[\"language\"] == \"cn\"\n        assert settings[\"max_column_width\"] is None\n        assert not settings[\"hide_header_editor\"]\n        assert not settings[\"lock_header_menu\"]\n        assert not settings[\"hide_header_menu\"]\n        assert not settings[\"hide_main_menu\"]\n        assert not settings[\"hide_column_menus\"]\n        assert not settings[\"enable_custom_filters\"]\n\n\n@pytest.mark.unit\ndef test_load_auth_settings():\n    settings = {\"active\": True, \"username\": \"foo\", \"password\": \"foo\"}\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"dtale.global_state.AUTH_SETTINGS\", settings))\n\n        load_auth_settings(None)\n        assert settings[\"active\"]\n        assert settings[\"username\"] == \"foo\"\n        assert settings[\"password\"] == \"foo\"\n\n        load_auth_settings(\n            load_config_state(os.path.join(os.path.dirname(__file__), \"dtale.ini\"))\n        )\n        assert not settings[\"active\"]\n        assert settings[\"username\"] == \"admin\"\n        assert settings[\"password\"] == \"admin\"\n\n\n@pytest.mark.unit\ndef test_build_show_options(unittest):\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    options = dict(allow_cell_edits=False)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    ini_path = os.path.join(os.path.dirname(__file__), \"dtale.ini\")\n    os.environ[\"DTALE_CONFIG\"] = ini_path\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n    assert final_options[\"precision\"] == 6\n    unittest.assertEqual(final_options[\"show_columns\"], [\"a\", \"b\"])\n    unittest.assertEqual(final_options[\"hide_columns\"], [\"c\"])\n    unittest.assertEqual(\n        final_options[\"column_formats\"], {\"a\": {\"fmt\": {\"html\": True}}}\n    )\n    unittest.assertEqual(final_options[\"sort\"], [(\"a\", \"ASC\")])\n    unittest.assertEqual(final_options[\"locked\"], [\"a\", \"b\"])\n    unittest.assertEqual(\n        final_options[\"column_edit_options\"], {\"a\": [\"foo\", \"bar\", \"baz\"]}\n    )\n    assert not final_options[\"auto_hide_empty_columns\"]\n    assert not final_options[\"highlight_filter\"]\n\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    del os.environ[\"DTALE_CONFIG\"]\n\n    set_config(ini_path)\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    set_config(None)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n\n@pytest.mark.unit\ndef test_build_show_options_w_missing_ini_props(unittest):\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    options = dict(allow_cell_edits=False)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    ini_path = os.path.join(os.path.dirname(__file__), \"dtale_missing_props.ini\")\n    os.environ[\"DTALE_CONFIG\"] = ini_path\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    del os.environ[\"DTALE_CONFIG\"]\n\n    set_config(ini_path)\n    final_options = build_show_options()\n    assert final_options[\"allow_cell_edits\"]\n\n    set_config(None)\n    final_options = build_show_options(options)\n    assert not final_options[\"allow_cell_edits\"]\n\n    ini_path = os.path.join(\n        os.path.dirname(__file__), \"dtale_allow_cell_edits_list.ini\"\n    )\n    os.environ[\"DTALE_CONFIG\"] = ini_path\n    final_options = build_show_options()\n    unittest.assertEqual(final_options[\"allow_cell_edits\"], [\"a\", \"b\"])\n", "import json\nfrom builtins import str\n\nimport mock\nimport numpy as np\nimport os\nimport dtale.global_state as global_state\nimport pandas as pd\nimport pytest\nfrom pandas.tseries.offsets import Day\nfrom pkg_resources import parse_version\nfrom six import PY3\n\nimport dtale.pandas_util as pandas_util\n\nfrom dtale.app import build_app\nfrom dtale.pandas_util import check_pandas_version\nfrom dtale.utils import DuplicateDataError\nfrom tests import ExitStack, pdt\nfrom tests.dtale import build_data_inst, build_settings, build_dtypes\nfrom tests.dtale.test_charts import build_col_def\n\n\nURL = \"http://localhost:40000\"\napp = build_app(url=URL)\n\n\ndef setup_function(function):\n    global_state.cleanup()\n\n\ndef teardown_function(function):\n    global_state.cleanup()\n\n\n@pytest.mark.unit\ndef test_head_endpoint():\n    import dtale.views as views\n\n    build_data_inst({\"1\": None, \"2\": None})\n    assert views.head_endpoint() == \"main/1\"\n    assert views.head_endpoint(\"test_popup\") == \"popup/test_popup/1\"\n\n    global_state.clear_store()\n    assert views.head_endpoint() == \"popup/upload\"\n\n\n@pytest.mark.unit\ndef test_startup(unittest):\n    import dtale.views as views\n    import dtale.global_state as global_state\n\n    global_state.clear_store()\n\n    instance = views.startup(URL)\n    assert instance._data_id == \"1\"\n\n    with pytest.raises(views.NoDataLoadedException) as error:\n        views.startup(URL, data_loader=lambda: None)\n    assert \"No data has been loaded into this D-Tale session!\" in str(\n        error.value.args[0]\n    )\n\n    with pytest.raises(BaseException) as error:\n        views.startup(URL, \"bad type\")\n    assert (\n        \"data loaded must be one of the following types: pandas.DataFrame, pandas.Series, pandas.DatetimeIndex\"\n        in str(error.value)\n    )\n\n    test_data = pd.DataFrame([dict(date=pd.Timestamp(\"now\"), security_id=1, foo=1.5)])\n    test_data = test_data.set_index([\"date\", \"security_id\"])\n    instance = views.startup(\n        URL,\n        data_loader=lambda: test_data,\n        sort=[(\"security_id\", \"ASC\")],\n        hide_header_editor=True,\n        hide_shutdown=True,\n        lock_header_menu=True,\n        hide_header_menu=True,\n        hide_main_menu=True,\n        hide_column_menus=True,\n        enable_custom_filters=True,\n    )\n\n    pdt.assert_frame_equal(instance.data, test_data.reset_index())\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            columnFormats={},\n            hide_shutdown=True,\n            hide_header_editor=True,\n            lock_header_menu=True,\n            hide_header_menu=True,\n            hide_main_menu=True,\n            hide_column_menus=True,\n            enable_custom_filters=True,\n            locked=[\"date\", \"security_id\"],\n            indexes=[\"date\", \"security_id\"],\n            precision=2,\n            sortInfo=[(\"security_id\", \"ASC\")],\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should lock index columns\",\n    )\n\n    global_state.set_app_settings(dict(hide_header_editor=False))\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            columnFormats={},\n            hide_shutdown=True,\n            hide_header_editor=False,\n            lock_header_menu=True,\n            hide_header_menu=True,\n            hide_main_menu=True,\n            hide_column_menus=True,\n            enable_custom_filters=True,\n            locked=[\"date\", \"security_id\"],\n            indexes=[\"date\", \"security_id\"],\n            precision=2,\n            sortInfo=[(\"security_id\", \"ASC\")],\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should hide header editor\",\n    )\n\n    test_data = test_data.reset_index()\n    with pytest.raises(DuplicateDataError):\n        views.startup(URL, data=test_data, ignore_duplicate=False)\n\n    range_highlights = {\n        \"foo\": {\n            \"active\": True,\n            \"equals\": {\n                \"active\": True,\n                \"value\": 3,\n                \"color\": {\"r\": 255, \"g\": 245, \"b\": 157, \"a\": 1},\n            },  # light yellow\n            \"greaterThan\": {\n                \"active\": True,\n                \"value\": 3,\n                \"color\": {\"r\": 80, \"g\": 227, \"b\": 194, \"a\": 1},\n            },  # mint green\n            \"lessThan\": {\n                \"active\": True,\n                \"value\": 3,\n                \"color\": {\"r\": 245, \"g\": 166, \"b\": 35, \"a\": 1},\n            },  # orange\n        }\n    }\n    instance = views.startup(\n        URL,\n        data=test_data,\n        ignore_duplicate=True,\n        allow_cell_edits=False,\n        precision=6,\n        range_highlights=range_highlights,\n    )\n    pdt.assert_frame_equal(instance.data, test_data)\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=False,\n            columnFormats={},\n            locked=[],\n            indexes=[],\n            precision=6,\n            rangeHighlight=range_highlights,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"no index = nothing locked\",\n    )\n\n    test_data = pd.DataFrame([dict(date=pd.Timestamp(\"now\"), security_id=1)])\n    test_data = test_data.set_index(\"security_id\").date\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    pdt.assert_frame_equal(instance.data, test_data.reset_index())\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            columnFormats={},\n            locked=[\"security_id\"],\n            indexes=[\"security_id\"],\n            precision=2,\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should lock index columns\",\n    )\n\n    test_data = pd.DatetimeIndex([pd.Timestamp(\"now\")], name=\"date\")\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    pdt.assert_frame_equal(instance.data, test_data.to_frame(index=False))\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            locked=[],\n            indexes=[],\n            precision=2,\n            columnFormats={},\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should not lock index columns\",\n    )\n\n    test_data = pd.MultiIndex.from_arrays([[1, 2], [3, 4]], names=(\"a\", \"b\"))\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    pdt.assert_frame_equal(instance.data, test_data.to_frame(index=False))\n    unittest.assertEqual(\n        global_state.get_settings(instance._data_id),\n        dict(\n            allow_cell_edits=True,\n            locked=[],\n            indexes=[],\n            precision=2,\n            columnFormats={},\n            rangeHighlight=None,\n            backgroundMode=None,\n            verticalHeaders=False,\n            highlightFilter=False,\n        ),\n        \"should not lock index columns\",\n    )\n\n    test_data = pd.DataFrame(\n        [\n            dict(date=pd.Timestamp(\"now\"), security_id=1, foo=1.0, bar=2.0, baz=np.nan),\n            dict(\n                date=pd.Timestamp(\"now\"), security_id=1, foo=2.0, bar=np.inf, baz=np.nan\n            ),\n        ],\n        columns=[\"date\", \"security_id\", \"foo\", \"bar\", \"baz\"],\n    )\n    instance = views.startup(\n        URL, data_loader=lambda: test_data, auto_hide_empty_columns=True\n    )\n    unittest.assertEqual(\n        {\n            \"name\": \"bar\",\n            \"dtype\": \"float64\",\n            \"index\": 3,\n            \"visible\": True,\n            \"hasMissing\": 0,\n            \"hasOutliers\": 0,\n            \"lowVariance\": False,\n            \"unique_ct\": 2,\n            \"kurt\": \"nan\",\n            \"skew\": \"nan\",\n            \"coord\": None,\n        },\n        next(\n            (\n                dt\n                for dt in global_state.get_dtypes(instance._data_id)\n                if dt[\"name\"] == \"bar\"\n            ),\n            None,\n        ),\n    )\n\n    non_visible = [\n        dt[\"name\"]\n        for dt in global_state.get_dtypes(instance._data_id)\n        if not dt[\"visible\"]\n    ]\n    unittest.assertEqual(non_visible, [\"baz\"])\n\n    test_data = pd.DataFrame([dict(a=1, b=2)])\n    test_data = test_data.rename(columns={\"b\": \"a\"})\n    with pytest.raises(Exception) as error:\n        views.startup(URL, data_loader=lambda: test_data)\n    assert \"data contains duplicated column names: a\" in str(error)\n\n    test_data = pd.DataFrame([dict(a=1, b=2)])\n    test_data = test_data.set_index(\"a\")\n    views.startup(URL, data=test_data, inplace=True, drop_index=True)\n    assert \"a\" not in test_data.columns\n\n    test_data = np.array([1, 2, 3])\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    unittest.assertEqual(list(instance.data.iloc[:, 0].tolist()), test_data.tolist())\n\n    test_data = np.ndarray(shape=(2, 2), dtype=float, order=\"F\")\n    instance = views.startup(URL, data_loader=lambda: test_data)\n    np.testing.assert_almost_equal(instance.data.values, test_data)\n\n    test_data = [1, 2, 3]\n    instance = views.startup(URL, data_loader=lambda: test_data, ignore_duplicate=True)\n    unittest.assertEqual(instance.data.iloc[:, 0].tolist(), test_data)\n\n    test_data = dict(a=[1, 2, 3], b=[4, 5, 6])\n    instance = views.startup(URL, data_loader=lambda: test_data, ignore_duplicate=True)\n    unittest.assertEqual(instance.data[\"a\"].values.tolist(), test_data[\"a\"])\n    unittest.assertEqual(instance.data[\"b\"].values.tolist(), test_data[\"b\"])\n\n    test_data = dict(a=1, b=2, c=3)\n    instance = views.startup(URL, data_loader=lambda: test_data, ignore_duplicate=True)\n    unittest.assertEqual(\n        sorted(instance.data[\"index\"].values.tolist()), sorted(test_data.keys())\n    )\n    unittest.assertEqual(\n        sorted(instance.data[\"0\"].values.tolist()), sorted(test_data.values())\n    )\n\n    test_data = pd.DataFrame(\n        dict(\n            a=[\"{}\".format(i) for i in range(10)],\n            b=[\"{}\".format(i % 2) for i in range(10)],\n        )\n    )\n    instance = views.startup(\n        URL,\n        data_loader=lambda: test_data,\n        ignore_duplicate=True,\n        optimize_dataframe=True,\n    )\n    unittest.assertEqual(\n        list(instance.data.dtypes.apply(lambda x: x.name).values),\n        [\"object\", \"category\"],\n    )\n\n    many_cols = pd.DataFrame({\"sec{}\".format(v): [1] for v in range(500)})\n    instance = views.startup(URL, data=many_cols)\n    unittest.assertEqual(\n        len([v for v in global_state.get_dtypes(instance._data_id) if v[\"visible\"]]),\n        100,\n    )\n\n    if PY3 and check_pandas_version(\"0.25.0\"):\n        s_int = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"), dtype=pd.Int64Dtype())\n        s2_int = s_int.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"])\n        ints = pd.Series([1, 2, 3, 4, 5], index=list(\"abcfu\"))\n        test_data = pd.DataFrame(dict(na=s2_int, int=ints))\n        test_data.loc[:, \"unsigned_int\"] = pd.to_numeric(\n            test_data[\"int\"], downcast=\"unsigned\"\n        )\n        instance = views.startup(\n            URL, data_loader=lambda: test_data, ignore_duplicate=True\n        )\n\n        unittest.assertEqual(\n            {\n                \"coord\": None,\n                \"dtype\": \"Int64\",\n                \"hasMissing\": 2,\n                \"hasOutliers\": 0,\n                \"index\": 1,\n                \"kurt\": \"nan\",\n                \"lowVariance\": False,\n                \"max\": 3,\n                \"min\": 1,\n                \"name\": \"na\",\n                \"skew\": 0.0,\n                \"unique_ct\": 3,\n                \"visible\": True,\n                \"outlierRange\": {\"lower\": 0.0, \"upper\": 4.0},\n            },\n            global_state.get_dtypes(instance._data_id)[1],\n        )\n\n        unittest.assertEqual(\n            {\n                \"dtype\": \"uint8\",\n                \"hasMissing\": 0,\n                \"hasOutliers\": 0,\n                \"index\": 3,\n                \"name\": \"unsigned_int\",\n                \"unique_ct\": 5,\n                \"visible\": True,\n            },\n            global_state.get_dtypes(instance._data_id)[-1],\n        )\n\n\n@pytest.mark.unit\ndef test_formatting_complex_data(unittest):\n    from dtale.views import format_data\n\n    data = [[1, 2, 3], {1: \"a\", 2: \"b\", 3: \"c\"}, [1]]\n    df, _ = format_data(pd.DataFrame({\"foo\": data}))\n    unittest.assertEqual(\n        list(df[\"foo\"].values), [\"[1, 2, 3]\", \"{1: 'a', 2: 'b', 3: 'c'}\", \"[1]\"]\n    )\n\n    data = [1, 2, [3]]\n    df, _ = format_data(pd.DataFrame({\"foo\": data}))\n    unittest.assertEqual(list(df[\"foo\"].values), [\"1\", \"2\", \"[3]\"])\n\n    index_vals = [\n        pd.Timestamp(\"20230101\"),\n        pd.Timestamp(\"20230102\"),\n        pd.Timestamp(\"20230103\"),\n    ]\n    base_df = pd.DataFrame([1, 2, 3], index=index_vals)\n    df, index = format_data(base_df)\n    unittest.assertEqual(index, [\"index\"])\n    assert len(df.columns) > len(base_df.columns)\n\n\n@pytest.mark.unit\ndef test_in_ipython_frontend(builtin_pkg):\n    import dtale.views as views\n\n    orig_import = __import__\n\n    mock_ipython = mock.Mock()\n\n    class zmq(object):\n        __name__ = \"zmq\"\n\n        def __init__(self):\n            pass\n\n    mock_ipython.get_ipython = lambda: zmq()\n\n    def import_mock(name, *args, **kwargs):\n        if name == \"IPython\":\n            return mock_ipython\n        return orig_import(name, *args, **kwargs)\n\n    with ExitStack() as stack:\n        stack.enter_context(\n            mock.patch(\"{}.__import__\".format(builtin_pkg), side_effect=import_mock)\n        )\n        assert views.in_ipython_frontend()\n\n    def import_mock(name, *args, **kwargs):\n        if name == \"IPython\":\n            raise ImportError()\n        return orig_import(name, *args, **kwargs)\n\n    with ExitStack() as stack:\n        stack.enter_context(\n            mock.patch(\"{}.__import__\".format(builtin_pkg), side_effect=import_mock)\n        )\n        assert not views.in_ipython_frontend()\n\n\n@pytest.mark.unit\ndef test_shutdown(unittest):\n    import werkzeug\n\n    with app.test_client() as c:\n        try:\n            with ExitStack() as stack:\n                from werkzeug.serving import BaseWSGIServer\n\n                base_server = mock.Mock(spec=BaseWSGIServer)\n                base_server.shutdown = mock.Mock()\n                gc_objects = [base_server]\n                mock_gc = stack.enter_context(\n                    mock.patch(\"gc.get_objects\", mock.Mock(return_value=gc_objects))\n                )\n                resp = c.get(\"/shutdown\").data\n                assert \"Server shutting down...\" in str(resp)\n                mock_gc.assert_called()\n                base_server.shutdown.assert_called()\n            unittest.fail()\n        except:  # noqa\n            pass\n        if parse_version(werkzeug.__version__) < parse_version(\"2.1.0\"):\n            mock_shutdown = mock.Mock()\n            resp = c.get(\n                \"/shutdown\", environ_base={\"werkzeug.server.shutdown\": mock_shutdown}\n            ).data\n            assert \"Server shutting down...\" in str(resp)\n            mock_shutdown.assert_called()\n\n\n@pytest.mark.unit\ndef test_get_send_file_max_age():\n    with app.app_context():\n        assert app.get_send_file_max_age(\"test\") in [43200, None]\n        assert 60 == app.get_send_file_max_age(\"dist/test.js\")\n\n\n@pytest.mark.unit\ndef test_processes(test_data, unittest):\n    global_state.clear_store()\n    from dtale.views import build_dtypes_state\n\n    now = pd.Timestamp(\"20180430 12:36:44\").tz_localize(\"US/Eastern\")\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: build_dtypes_state(test_data)})\n        global_state.set_metadata(c.port, dict(start=now))\n        global_state.set_name(c.port, \"foo\")\n\n        response = c.get(\"/dtale/processes\")\n        response_data = response.get_json()\n        unittest.assertDictContainsSubset(\n            {\n                \"rows\": 50,\n                \"name\": \"foo\",\n                \"ts\": 1525106204000,\n                \"start\": \"12:36:44 PM\",\n                \"names\": \"date,security_id,foo,bar,baz\",\n                \"data_id\": str(c.port),\n                \"columns\": 5,\n                # \"mem_usage\": 4600 if PY3 else 4000,\n            },\n            response_data[\"data\"][0],\n        )\n\n        response = c.get(\"/dtale/process-keys\")\n        response_data = response.get_json()\n        assert response_data[\"data\"][0][\"id\"] == str(c.port)\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: build_dtypes_state(test_data)})\n        global_state.set_metadata(c.port, {})\n        response = c.get(\"/dtale/processes\")\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n\n@pytest.mark.unit\ndef test_update_settings(test_data, unittest):\n    settings = json.dumps(dict(locked=[\"a\", \"b\"]))\n\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            global_state.set_data(c.port, test_data)\n            mock_render_template = stack.enter_context(\n                mock.patch(\n                    \"dtale.views.render_template\",\n                    mock.Mock(return_value=json.dumps(dict(success=True))),\n                )\n            )\n            response = c.get(\n                \"/dtale/update-settings/{}\".format(c.port),\n                query_string=dict(settings=settings),\n            )\n            assert response.status_code == 200, \"should return 200 response\"\n\n            c.get(\"/dtale/main/{}\".format(c.port))\n            _, kwargs = mock_render_template.call_args\n            unittest.assertEqual(\n                kwargs[\"settings\"], settings, \"settings should be retrieved\"\n            )\n\n    settings = \"a\"\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            global_state.set_data(c.port, None)\n            response = c.get(\n                \"/dtale/update-settings/{}\".format(c.port),\n                query_string=dict(settings=settings),\n            )\n            assert response.status_code == 200, \"should return 200 response\"\n            response_data = response.get_json()\n            assert \"error\" in response_data\n\n\n@pytest.mark.unit\ndef test_update_formats():\n    from dtale.views import build_dtypes_state\n\n    settings = dict()\n    df = pd.DataFrame([dict(a=1, b=2)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        build_settings(settings)\n        response = c.get(\n            \"/dtale/update-formats/{}\".format(c.port),\n            query_string=dict(\n                all=False, col=\"a\", format=json.dumps(dict(fmt=\"\", style={}))\n            ),\n        )\n        assert response.status_code == 200, \"should return 200 response\"\n        assert \"a\" in global_state.get_settings(c.port)[\"columnFormats\"]\n\n        c.get(\n            \"/dtale/update-formats/{}\".format(c.port),\n            query_string=dict(\n                all=True,\n                col=\"a\",\n                format=json.dumps(dict(fmt=\"\", style={})),\n                nanDisplay=None,\n            ),\n        )\n        assert \"b\" in global_state.get_settings(c.port)[\"columnFormats\"]\n        assert \"nan\" in global_state.get_settings(c.port)[\"nanDisplay\"]\n\n    with app.test_client() as c:\n        global_state.set_dtypes(c.port, None)\n        response = c.get(\n            \"/dtale/update-formats/{}\".format(c.port),\n            query_string=dict(\n                all=True, col=\"a\", format=json.dumps(dict(fmt=\"\", style={}))\n            ),\n        )\n        assert \"error\" in response.json\n\n\n@pytest.mark.unit\ndef test_update_column_position():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    tests = [\n        (\"front\", 0),\n        (\"front\", 0),\n        (\"left\", 0),\n        (\"back\", -1),\n        (\"back\", -1),\n        (\"left\", -2),\n        (\"right\", -1),\n        (\"right\", -1),\n    ]\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        for action, col_idx in tests:\n            c.get(\n                \"/dtale/update-column-position/{}\".format(c.port),\n                query_string=dict(action=action, col=\"c\"),\n            )\n            assert global_state.get_data(c.port).columns[col_idx] == \"c\"\n            assert global_state.get_dtypes(c.port)[col_idx][\"name\"] == \"c\"\n\n        resp = c.get(\"/dtale/update-column-position/-1\")\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_update_locked(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        settings = {c.port: dict(locked=[])}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        build_settings(settings)\n\n        c.get(\n            \"/dtale/update-locked/{}\".format(c.port),\n            query_string=dict(action=\"lock\", col=\"c\"),\n        )\n        unittest.assertEqual([\"c\"], global_state.get_settings(c.port)[\"locked\"])\n        assert global_state.get_data(c.port).columns[0] == \"c\"\n        assert global_state.get_dtypes(c.port)[0][\"name\"] == \"c\"\n\n        c.get(\n            \"/dtale/update-locked/{}\".format(c.port),\n            query_string=dict(action=\"lock\", col=\"c\"),\n        )\n        unittest.assertEqual([\"c\"], global_state.get_settings(c.port)[\"locked\"])\n        assert global_state.get_data(c.port).columns[0] == \"c\"\n        assert global_state.get_dtypes(c.port)[0][\"name\"] == \"c\"\n\n        c.get(\n            \"/dtale/update-locked/{}\".format(c.port),\n            query_string=dict(action=\"unlock\", col=\"c\"),\n        )\n        unittest.assertEqual([], global_state.get_settings(c.port)[\"locked\"])\n        assert global_state.get_data(c.port).columns[0] == \"c\"\n        assert global_state.get_dtypes(c.port)[0][\"name\"] == \"c\"\n\n        resp = c.get(\"/dtale/update-locked/-1\")\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_delete_cols():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        delete_cols = [\"a\", \"b\"]\n        c.get(\n            \"/dtale/delete-col/{}\".format(c.port),\n            query_string=dict(cols=json.dumps(delete_cols)),\n        )\n        assert (\n            len(\n                [\n                    col\n                    for col in global_state.get_data(c.port).columns\n                    if col in delete_cols\n                ]\n            )\n            == 0\n        )\n        assert (\n            next(\n                (\n                    dt\n                    for dt in global_state.get_dtypes(c.port)\n                    if dt[\"name\"] in delete_cols\n                ),\n                None,\n            )\n            is None\n        )\n        assert len(global_state.get_settings(c.port)[\"locked\"]) == 0\n\n        resp = c.get(\"/dtale/delete-col/-1\", query_string=dict(cols=json.dumps([\"d\"])))\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_duplicate_cols(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n\n        resp = c.get(\n            \"/dtale/duplicate-col/{}\".format(c.port),\n            query_string=dict(col=\"b\"),\n        )\n        unittest.assertEquals(\n            list(global_state.get_data(c.port).columns), [\"a\", \"b\", \"b_2\", \"c\"]\n        )\n        assert resp.json[\"col\"] == \"b_2\"\n\n\n@pytest.mark.unit\ndef test_rename_col():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        c.get(\n            \"/dtale/rename-col/{}\".format(c.port),\n            query_string=dict(col=\"a\", rename=\"d\"),\n        )\n        assert \"a\" not in global_state.get_data(c.port).columns\n        assert (\n            next(\n                (dt for dt in global_state.get_dtypes(c.port) if dt[\"name\"] == \"a\"),\n                None,\n            )\n            is None\n        )\n        assert len(global_state.get_settings(c.port)[\"locked\"]) == 1\n\n        resp = c.get(\n            \"/dtale/rename-col/{}\".format(c.port),\n            query_string=dict(col=\"d\", rename=\"b\"),\n        )\n        assert \"error\" in json.loads(resp.data)\n\n        resp = c.get(\"/dtale/rename-col/-1\", query_string=dict(col=\"d\", rename=\"b\"))\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_outliers(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame.from_dict(\n        {\n            \"a\": [1, 2, 3, 4, 1000, 5, 3, 5, 55, 12, 13, 10000, 221, 12, 2000],\n            \"b\": list(range(15)),\n            \"c\": [\"a\"] * 15,\n        }\n    )\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"a\"))\n        resp = json.loads(resp.data)\n        unittest.assertEqual(resp[\"outliers\"], [1000, 2000, 10000])\n        c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"a\", cfg=json.dumps(dict(type=\"outliers\", query=resp[\"query\"]))\n            ),\n        )\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"a\"))\n        resp = json.loads(resp.data)\n        assert resp[\"queryApplied\"]\n        c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(col=\"a\", cfg=json.dumps(dict(type=\"outliers\"))),\n        )\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"a\"))\n        resp = json.loads(resp.data)\n        assert not resp[\"queryApplied\"]\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"b\"))\n        resp = json.loads(resp.data)\n        unittest.assertEqual(resp[\"outliers\"], [])\n        resp = c.get(\"/dtale/outliers/{}\".format(c.port), query_string=dict(col=\"c\"))\n        resp = json.loads(resp.data)\n        assert resp[\"outliers\"] == []\n\n\n@pytest.mark.unit\ndef test_toggle_outlier_filter(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame.from_dict(\n        {\n            \"a\": [1, 2, 3, 4, 1000, 5, 3, 5, 55, 12, 13, 10000, 221, 12, 2000],\n            \"b\": list(range(15)),\n            \"c\": [\"a\"] * 15,\n        }\n    )\n    with app.test_client() as c:\n        data = {c.port: df}\n        build_data_inst(data)\n        settings = {c.port: {\"locked\": [\"a\"]}}\n        build_settings(settings)\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_dtypes(dtypes)\n        resp = c.get(\n            \"/dtale/toggle-outlier-filter/{}\".format(c.port), query_string=dict(col=\"a\")\n        )\n        resp = resp.get_json()\n        assert resp[\"outlierFilters\"][\"a\"][\"query\"] == \"`a` > 339.75\"\n        assert (\n            global_state.get_settings(c.port)[\"outlierFilters\"][\"a\"][\"query\"]\n            == \"`a` > 339.75\"\n        )\n        resp = c.get(\n            \"/dtale/toggle-outlier-filter/{}\".format(c.port), query_string=dict(col=\"a\")\n        )\n        resp = resp.get_json()\n        assert \"a\" not in resp[\"outlierFilters\"]\n        assert \"a\" not in global_state.get_settings(c.port)[\"outlierFilters\"]\n\n\n@pytest.mark.unit\ndef test_update_visibility(unittest):\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3)])\n    with app.test_client() as c:\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst({c.port: df})\n        build_dtypes(dtypes)\n        c.post(\n            \"/dtale/update-visibility/{}\".format(c.port),\n            data=json.dumps(\n                dict(visibility=json.dumps({\"a\": True, \"b\": True, \"c\": False}))\n            ),\n            content_type=\"application/json\",\n        )\n        unittest.assertEqual(\n            [True, True, False],\n            [col[\"visible\"] for col in global_state.get_dtypes(c.port)],\n        )\n        c.post(\n            \"/dtale/update-visibility/{}\".format(c.port),\n            data=json.dumps(dict(toggle=\"c\")),\n            content_type=\"application/json\",\n        )\n        unittest.assertEqual(\n            [True, True, True],\n            [col[\"visible\"] for col in global_state.get_dtypes(c.port)],\n        )\n\n        resp = c.post(\n            \"/dtale/update-visibility/-1\",\n            data=json.dumps(dict(toggle=\"foo\")),\n            content_type=\"application/json\",\n        )\n        assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_build_column():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3, d=pd.Timestamp(\"20200101\"))])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        resp = c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"not_implemented\", name=\"test\", cfg=json.dumps({})),\n        )\n        response_data = json.loads(resp.data)\n        assert (\n            response_data[\"error\"]\n            == \"'not_implemented' column builder not implemented yet!\"\n        )\n\n        resp = c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", cfg=json.dumps({})),\n        )\n        response_data = json.loads(resp.data)\n        assert response_data[\"error\"] == \"'name' is required for new column!\"\n\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"sum\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"sum\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"sum\"].values[0] == 3\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"sum\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int64\"\n\n        resp = c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"sum\", cfg=json.dumps(cfg)),\n        )\n        response_data = json.loads(resp.data)\n        assert response_data[\"error\"] == \"A column named 'sum' already exists!\"\n\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"difference\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"diff\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"diff\"].values[0] == -1\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"diff\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int64\"\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"multiply\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"mult\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"mult\"].values[0] == 2\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"mult\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int64\"\n        cfg = dict(left=dict(col=\"a\"), right=dict(col=\"b\"), operation=\"divide\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"div\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"div\"].values[0] == 0.5\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"div\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"float64\"\n        cfg = dict(left=dict(col=\"a\"), right=dict(val=100), operation=\"divide\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"div2\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"div2\"].values[0] == 0.01\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"div2\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"float64\"\n        cfg = dict(left=dict(val=100), right=dict(col=\"b\"), operation=\"divide\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"numeric\", name=\"div3\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"div3\"].values[0] == 50\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"div3\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"float64\"\n\n        cfg = dict(col=\"d\", property=\"weekday\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"datetime\", name=\"datep\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"datep\"].values[0] == 2\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"datep\"\n        assert (\n            global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"int32\"\n            if pandas_util.is_pandas2()\n            else \"int64\"\n        )\n\n        for p in [\"minute\", \"hour\", \"time\", \"date\", \"month\", \"quarter\", \"year\"]:\n            c.get(\n                \"/dtale/build-column/{}\".format(c.port),\n                query_string=dict(\n                    type=\"datetime\", name=p, cfg=json.dumps(dict(col=\"d\", property=p))\n                ),\n            )\n\n        cfg = dict(col=\"d\", conversion=\"month_end\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"datetime\", name=\"month_end\", cfg=json.dumps(cfg)),\n        )\n        assert (\n            pd.Timestamp(data[c.port][\"month_end\"].values[0]).strftime(\"%Y%m%d\")\n            == \"20200131\"\n        )\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"month_end\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"datetime64[ns]\"\n\n        for conv in [\n            \"month_start\",\n            \"quarter_start\",\n            \"quarter_end\",\n            \"year_start\",\n            \"year_end\",\n        ]:\n            c.get(\n                \"/dtale/build-column/{}\".format(c.port),\n                query_string=dict(\n                    type=\"datetime\",\n                    name=conv,\n                    cfg=json.dumps(dict(col=\"d\", conversion=conv)),\n                ),\n            )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        for dt in [\"float\", \"int\", \"string\", \"date\", \"bool\", \"choice\"]:\n            c.get(\n                \"/dtale/build-column/{}\".format(c.port),\n                query_string=dict(\n                    type=\"random\",\n                    name=\"random_{}\".format(dt),\n                    cfg=json.dumps(dict(type=dt)),\n                ),\n            )\n\n        cfg = dict(type=\"string\", chars=\"ABCD\")\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(\n                type=\"random\", name=\"random_string2\", cfg=json.dumps(cfg)\n            ),\n        )\n\n        cfg = dict(type=\"date\", timestamps=True, businessDay=True)\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"random\", name=\"random_date2\", cfg=json.dumps(cfg)),\n        )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n\n@pytest.mark.unit\ndef test_build_column_apply_all():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame([dict(a=1, b=2, c=3, d=pd.Timestamp(\"20200101\"))])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        cfg = {\"col\": \"a\", \"to\": \"float\", \"from\": \"int64\", \"applyAllType\": True}\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"type_conversion\", name=\"test\", cfg=json.dumps(cfg)),\n        )\n        assert all(\n            data[c.port].dtypes[col].name == \"float64\" for col in [\"a\", \"b\", \"c\"]\n        )\n\n\n@pytest.mark.unit\ndef test_build_column_bins():\n    from dtale.views import build_dtypes_state\n\n    df = pd.DataFrame(np.random.randn(100, 3), columns=[\"a\", \"b\", \"c\"])\n    with app.test_client() as c:\n        data = {c.port: df}\n        dtypes = {c.port: build_dtypes_state(df)}\n        build_data_inst(data)\n        build_dtypes(dtypes)\n        cfg = dict(col=\"a\", operation=\"cut\", bins=4)\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert len(resp[\"labels\"]) == 4\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"cut\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"cut\"].values[0] is not None\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"cut\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n        cfg = dict(col=\"a\", operation=\"cut\", bins=4, labels=\"foo,bar,biz,baz\")\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert resp[\"labels\"] == [\"foo\", \"bar\", \"biz\", \"baz\"]\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"cut2\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"cut2\"].values[0] in [\n            \"foo\",\n            \"bar\",\n            \"biz\",\n            \"baz\",\n        ]\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"cut2\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n        cfg = dict(col=\"a\", operation=\"qcut\", bins=4)\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert len(resp[\"labels\"]) == 4\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"qcut\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"qcut\"].values[0] is not None\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"qcut\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n        cfg = dict(col=\"a\", operation=\"qcut\", bins=4, labels=\"foo,bar,biz,baz\")\n        resp = c.get(\n            \"/dtale/bins-tester/{}\".format(c.port),\n            query_string=dict(type=\"bins\", cfg=json.dumps(cfg)),\n        )\n        resp = resp.get_json()\n        assert len(resp[\"data\"]) == 4\n        assert resp[\"labels\"] == [\"foo\", \"bar\", \"biz\", \"baz\"]\n        c.get(\n            \"/dtale/build-column/{}\".format(c.port),\n            query_string=dict(type=\"bins\", name=\"qcut2\", cfg=json.dumps(cfg)),\n        )\n        assert global_state.get_data(c.port)[\"qcut2\"].values[0] in [\n            \"foo\",\n            \"bar\",\n            \"biz\",\n            \"baz\",\n        ]\n        assert global_state.get_dtypes(c.port)[-1][\"name\"] == \"qcut2\"\n        assert global_state.get_dtypes(c.port)[-1][\"dtype\"] == \"string\"\n\n\n@pytest.mark.unit\ndef test_cleanup_error(unittest):\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.cleanup\", mock.Mock(side_effect=Exception)\n                )\n            )\n            resp = c.get(\"/dtale/cleanup-datasets\", query_string=dict(dataIds=\"1\"))\n            assert \"error\" in json.loads(resp.data)\n\n\n@pytest.mark.unit\ndef test_dtypes(test_data):\n    from dtale.views import build_dtypes_state, format_data\n\n    test_data = test_data.copy()\n    test_data.loc[:, \"mixed_col\"] = 1\n    test_data.loc[0, \"mixed_col\"] = \"x\"\n\n    with app.test_client() as c:\n        with ExitStack():\n            build_data_inst({c.port: test_data})\n            build_dtypes({c.port: build_dtypes_state(test_data)})\n            response = c.get(\"/dtale/dtypes/{}\".format(c.port))\n            response_data = response.get_json()\n            assert response_data[\"success\"]\n\n            for col in test_data.columns:\n                response = c.get(\n                    \"/dtale/describe/{}\".format(c.port), query_string=dict(col=col)\n                )\n                response_data = response.get_json()\n                assert response_data[\"success\"]\n\n    lots_of_groups = pd.DataFrame([dict(a=i, b=1) for i in range(150)])\n    with app.test_client() as c:\n        with ExitStack():\n            build_data_inst({c.port: lots_of_groups})\n            build_dtypes({c.port: build_dtypes_state(lots_of_groups)})\n            response = c.get(\"/dtale/dtypes/{}\".format(c.port))\n            response_data = response.get_json()\n            assert response_data[\"success\"]\n\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port), query_string=dict(col=\"a\")\n            )\n            response_data = response.get_json()\n            uniq_key = list(response_data[\"uniques\"].keys())[0]\n            assert response_data[\"uniques\"][uniq_key][\"top\"]\n            assert response_data[\"success\"]\n\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.get_dtypes\", side_effect=Exception)\n            )\n            response = c.get(\"/dtale/dtypes/{}\".format(c.port))\n            response_data = response.get_json()\n            assert \"error\" in response_data\n\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port), query_string=dict(col=\"foo\")\n            )\n            response_data = response.get_json()\n            assert \"error\" in response_data\n\n    df = pd.DataFrame(\n        [\n            dict(date=pd.Timestamp(\"now\"), security_id=1, foo=1.0, bar=2.0),\n            dict(date=pd.Timestamp(\"now\"), security_id=1, foo=2.0, bar=np.inf),\n        ],\n        columns=[\"date\", \"security_id\", \"foo\", \"bar\"],\n    )\n    df, _ = format_data(df)\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: build_dtypes_state(df)})\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port), query_string=dict(col=\"bar\")\n            )\n            response_data = response.get_json()\n            assert response_data[\"describe\"][\"min\"] == \"2\"\n            assert response_data[\"describe\"][\"max\"] == \"inf\"\n\n            global_state.set_settings(c.port, dict(query=\"security_id == 1\"))\n            response = c.get(\n                \"/dtale/describe/{}\".format(c.port),\n                query_string=dict(col=\"foo\", filtered=\"true\"),\n            )\n            response_data = response.get_json()\n            assert response_data[\"describe\"][\"min\"] == \"1\"\n            assert response_data[\"describe\"][\"max\"] == \"2\"\n\n\n@pytest.mark.unit\ndef test_variance(unittest):\n    from dtale.views import build_dtypes_state, format_data\n\n    global_state.clear_store()\n    with open(\n        os.path.join(os.path.dirname(__file__), \"..\", \"data/test_variance.json\"), \"r\"\n    ) as f:\n        expected = f.read()\n        expected = json.loads(expected)\n\n    def _df():\n        for i in range(2500):\n            yield dict(i=i, x=i % 5)\n\n    df = pd.DataFrame(list(_df()))\n    df.loc[:, \"low_var\"] = 2500\n    df.loc[0, \"low_var\"] = 1\n    df, _ = format_data(df)\n\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        dtypes = build_dtypes_state(df)\n        assert next((dt for dt in dtypes if dt[\"name\"] == \"low_var\"), None)[\n            \"lowVariance\"\n        ]\n        build_dtypes({c.port: dtypes})\n        response = c.get(\n            \"/dtale/variance/{}\".format(c.port), query_string=dict(col=\"x\")\n        )\n        if parse_version(pd.__version__) >= parse_version(\"1.3.0\"):\n            expected[\"x\"][\"check2\"][\"val1\"][\"val\"] = 0\n            expected[\"x\"][\"check2\"][\"val2\"][\"val\"] = 1\n        response_data = response.get_json()\n        del response_data[\"code\"]\n        response_data[\"jarqueBera\"][\"pvalue\"] = round(\n            response_data[\"jarqueBera\"][\"pvalue\"], 4\n        )\n        response_data[\"jarqueBera\"][\"statistic\"] = round(\n            response_data[\"jarqueBera\"][\"statistic\"], 4\n        )\n        unittest.assertEqual(response_data, expected[\"x\"])\n\n        response = c.get(\n            \"/dtale/variance/{}\".format(c.port), query_string=dict(col=\"low_var\")\n        )\n        response_data = response.get_json()\n        del response_data[\"code\"]\n        response_data[\"shapiroWilk\"][\"statistic\"] = round(\n            response_data[\"shapiroWilk\"][\"statistic\"], 4\n        )\n        response_data[\"jarqueBera\"][\"statistic\"] = round(\n            response_data[\"jarqueBera\"][\"statistic\"], 4\n        )\n        unittest.assertEqual(response_data, expected[\"low_var\"])\n\n\n@pytest.mark.unit\ndef test_test_filter(test_data):\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_app_settings(dict(enable_custom_filters=True))\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"date == date\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port), query_string=dict(query=\"foo == 1\")\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"date == '20000101'\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"baz == 'baz'\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port), query_string=dict(query=\"bar > 1.5\")\n        )\n        response_data = response.get_json()\n        assert not response_data[\"success\"]\n        assert response_data[\"error\"] == 'query \"bar > 1.5\" found no data, please alter'\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port), query_string=dict(query=\"foo2 == 1\")\n        )\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=None, save=\"true\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        global_state.set_app_settings(dict(enable_custom_filters=False))\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"foo2 == 1\", save=True),\n        )\n        response_data = response.get_json()\n        assert not response_data[\"success\"]\n        assert response_data[\"error\"] == (\n            \"Custom Filters not enabled! Custom filters are vulnerable to code injection attacks, please only \"\n            \"use in trusted environments.\"\n        )\n        global_state.set_app_settings(dict(enable_custom_filters=True))\n\n    if PY3:\n        df = pd.DataFrame([dict(a=1)])\n        df[\"a.b\"] = 2\n        with app.test_client() as c:\n            build_data_inst({c.port: df})\n            response = c.get(\n                \"/dtale/test-filter/{}\".format(c.port),\n                query_string=dict(query=\"a.b == 2\"),\n            )\n            response_data = response.get_json()\n            assert not response_data[\"success\"]\n\n\n@pytest.mark.unit\ndef test_get_data(unittest, test_data):\n    import dtale.views as views\n    import dtale.global_state as global_state\n\n    with app.test_client() as c:\n        test_data, _ = views.format_data(test_data)\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        response = c.get(\"/dtale/data/{}\".format(c.port))\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data, {}, 'if no \"ids\" parameter an empty dict should be returned'\n        )\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1\"]))\n        )\n        response_data = response.get_json()\n        expected = dict(\n            total=50,\n            final_query=\"\",\n            results={\n                \"1\": dict(\n                    date=\"2000-01-01\",\n                    security_id=1,\n                    dtale_index=1,\n                    foo=1,\n                    bar=1.5,\n                    baz=\"baz\",\n                )\n            },\n            columns=[\n                dict(dtype=\"int64\", name=\"dtale_index\", visible=True),\n                dict(\n                    dtype=\"datetime64[ns]\",\n                    name=\"date\",\n                    index=0,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    unique_ct=1,\n                    kurt=0,\n                    skew=0,\n                ),\n                dict(\n                    dtype=\"int64\",\n                    name=\"security_id\",\n                    max=49,\n                    min=0,\n                    index=1,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    lowVariance=False,\n                    outlierRange={\"lower\": -24.5, \"upper\": 73.5},\n                    unique_ct=50,\n                    kurt=-1.2,\n                    skew=0,\n                    coord=None,\n                ),\n                dict(\n                    dtype=\"int64\",\n                    name=\"foo\",\n                    min=1,\n                    max=1,\n                    index=2,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    lowVariance=False,\n                    outlierRange={\"lower\": 1.0, \"upper\": 1.0},\n                    unique_ct=1,\n                    kurt=0,\n                    skew=0,\n                    coord=None,\n                ),\n                dict(\n                    dtype=\"float64\",\n                    name=\"bar\",\n                    min=1.5,\n                    max=1.5,\n                    index=3,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    lowVariance=False,\n                    outlierRange={\"lower\": 1.5, \"upper\": 1.5},\n                    unique_ct=1,\n                    kurt=0,\n                    skew=0,\n                    coord=None,\n                ),\n                dict(\n                    dtype=\"string\",\n                    name=\"baz\",\n                    index=4,\n                    visible=True,\n                    hasMissing=0,\n                    hasOutliers=0,\n                    unique_ct=1,\n                ),\n            ],\n        )\n        unittest.assertEqual(response_data, expected, \"should return data at index 1\")\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1-2\"]))\n        )\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            ),\n            \"2\": dict(\n                date=\"2000-01-01\",\n                security_id=2,\n                dtale_index=2,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            ),\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at indexes 1-2\"\n        )\n\n        params = dict(ids=json.dumps([\"1\"]), sort=json.dumps([[\"security_id\", \"DESC\"]]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=48,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at index 1 w/ sort\"\n        )\n        unittest.assertEqual(\n            global_state.get_settings(c.port),\n            {\"sortInfo\": [[\"security_id\", \"DESC\"]]},\n            \"should update settings\",\n        )\n\n        params = dict(ids=json.dumps([\"1\"]), sort=json.dumps([[\"security_id\", \"ASC\"]]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at index 1 w/ sort\"\n        )\n        unittest.assertEqual(\n            global_state.get_settings(c.port),\n            {\"sortInfo\": [[\"security_id\", \"ASC\"]]},\n            \"should update settings\",\n        )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        response = c.get(\n            \"/dtale/test-filter/{}\".format(c.port),\n            query_string=dict(query=\"security_id == 1\", save=\"true\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n        unittest.assertEqual(\n            global_state.get_settings(c.port)[\"query\"],\n            \"security_id == 1\",\n            \"should update settings\",\n        )\n\n        params = dict(ids=json.dumps([\"0\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"0\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=0,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"], expected, \"should return data at index 1 w/ sort\"\n        )\n\n        global_state.get_settings(c.port)[\"highlightFilter\"] = True\n        params = dict(ids=json.dumps([\"1\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n                __filtered=True,\n            )\n        }\n        unittest.assertEqual(\n            response_data[\"results\"],\n            expected,\n            \"should return data at index 1 w/ filtered flag for highlighting\",\n        )\n\n        params = dict(ids=json.dumps([\"1-2\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"1\": dict(\n                date=\"2000-01-01\",\n                security_id=1,\n                dtale_index=1,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n                __filtered=True,\n            ),\n            \"2\": dict(\n                date=\"2000-01-01\",\n                security_id=2,\n                dtale_index=2,\n                foo=1,\n                bar=1.5,\n                baz=\"baz\",\n            ),\n        }\n        unittest.assertEqual(\n            response_data[\"results\"],\n            expected,\n            \"should return data at indexes 1-2 w/ filtered flag for highlighting\",\n        )\n\n        response = c.get(\"/dtale/code-export/{}\".format(c.port))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n        global_state.get_settings(c.port)[\"query\"] = \"security_id == 50\"\n        global_state.get_settings(c.port)[\"highlightFilter\"] = False\n        params = dict(ids=json.dumps([\"0\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert len(response_data[\"results\"]) == 0\n\n        global_state.get_settings(c.port)[\"invertFilter\"] = True\n        params = dict(ids=json.dumps([\"0\"]))\n        response = c.get(\"/dtale/data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert len(response_data[\"results\"]) == 1\n\n        global_state.get_settings(c.port)[\"invertFilter\"] = False\n\n        response = c.get(\"/dtale/data-export/{}\".format(c.port))\n        assert response.content_type == \"text/csv\"\n\n        response = c.get(\n            \"/dtale/data-export/{}\".format(c.port), query_string=dict(type=\"tsv\")\n        )\n        assert response.content_type == \"text/tsv\"\n\n        response = c.get(\"/dtale/data-export/a\", query_string=dict(type=\"tsv\"))\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        build_settings({c.port: dict(query=\"missing_col == 'blah'\")})\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"0\"]))\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            \"name 'missing_col' is not defined\",\n            \"should handle data exception\",\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1\"]))\n        )\n        assert response.status_code == 200\n\n        tmp = global_state.get_data(c.port).copy()\n        tmp[\"biz\"] = 2.5\n        global_state.set_data(c.port, tmp)\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(ids=json.dumps([\"1\"]))\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"results\"],\n            {\n                \"1\": dict(\n                    date=\"2000-01-01\",\n                    security_id=1,\n                    dtale_index=1,\n                    foo=1,\n                    bar=1.5,\n                    baz=\"baz\",\n                    biz=2.5,\n                )\n            },\n            \"should handle data updates\",\n        )\n        unittest.assertEqual(\n            global_state.get_dtypes(c.port)[-1],\n            dict(\n                index=5,\n                name=\"biz\",\n                dtype=\"float64\",\n                min=2.5,\n                max=2.5,\n                visible=True,\n                hasMissing=0,\n                hasOutliers=0,\n                lowVariance=False,\n                outlierRange={\"lower\": 2.5, \"upper\": 2.5},\n                unique_ct=1,\n                kurt=0,\n                skew=0,\n                coord=None,\n            ),\n            \"should update dtypes on data structure change\",\n        )\n\n\n@pytest.mark.unit\ndef test_export_html(unittest, test_data):\n    import dtale.views as views\n\n    with app.test_client() as c:\n        test_data, _ = views.format_data(test_data)\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        response = c.get(\"/dtale/data/{}\".format(c.port))\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data, {}, 'if no \"ids\" parameter an empty dict should be returned'\n        )\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port), query_string=dict(export=True)\n        )\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/data/{}\".format(c.port),\n            query_string=dict(export=True, export_rows=5),\n        )\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\ndef test_export_parquet(test_data):\n    pytest.importorskip(\"pyarrow\")\n\n    import dtale.views as views\n\n    with app.test_client() as c:\n        test_data, _ = views.format_data(test_data)\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n\n        response = c.get(\n            \"/dtale/data-export/{}\".format(c.port), query_string=dict(type=\"parquet\")\n        )\n        assert response.content_type == \"application/octet-stream\"\n\n\ndef build_ts_data(size=5, days=5):\n    start = pd.Timestamp(\"20000101\")\n    for d in pd.date_range(start, start + Day(days - 1)):\n        for i in range(size):\n            yield dict(date=d, security_id=i, foo=i, bar=i)\n\n\n@pytest.mark.unit\ndef test_get_chart_data(unittest, rolling_data):\n    import dtale.views as views\n\n    test_data = pd.DataFrame(\n        build_ts_data(size=50), columns=[\"date\", \"security_id\", \"foo\", \"bar\"]\n    )\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(x=\"date\", y=json.dumps([\"security_id\"]), agg=\"count\")\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        expected = {\n            \"data\": {\n                \"all\": {\n                    \"x\": [\n                        \"2000-01-01\",\n                        \"2000-01-02\",\n                        \"2000-01-03\",\n                        \"2000-01-04\",\n                        \"2000-01-05\",\n                    ],\n                    build_col_def(\"count\", \"security_id\"): [50, 50, 50, 50, 50],\n                }\n            },\n            \"max\": {build_col_def(\"count\", \"security_id\"): 50, \"x\": \"2000-01-05\"},\n            \"min\": {build_col_def(\"count\", \"security_id\"): 50, \"x\": \"2000-01-01\"},\n            \"success\": True,\n        }\n        unittest.assertEqual(\n            {k: v for k, v in response_data.items() if k != \"code\"},\n            expected,\n            \"should return chart data\",\n        )\n\n    test_data.loc[:, \"baz\"] = \"baz\"\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(\n            x=\"date\",\n            y=json.dumps([\"security_id\"]),\n            group=json.dumps([\"baz\"]),\n            agg=\"mean\",\n        )\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert response_data[\"min\"][build_col_def(\"mean\", \"security_id\")] == 24.5\n        assert response_data[\"max\"][build_col_def(\"mean\", \"security_id\")] == 24.5\n        series_key = \"(baz: baz)\"\n        assert response_data[\"data\"][series_key][\"x\"][-1] == \"2000-01-05\"\n        assert (\n            len(response_data[\"data\"][series_key][build_col_def(\"mean\", \"security_id\")])\n            == 5\n        )\n        assert (\n            sum(response_data[\"data\"][series_key][build_col_def(\"mean\", \"security_id\")])\n            == 122.5\n        )\n\n    df, _ = views.format_data(rolling_data)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(\n            x=\"date\",\n            y=json.dumps([\"0\"]),\n            agg=\"rolling\",\n            rollingWin=10,\n            rollingComp=\"count\",\n        )\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(x=\"baz\", y=json.dumps([\"foo\"]))\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert response_data[\"error\"] == (\n            \"The grouping [baz] contains duplicates, please specify group or additional filtering or select \"\n            \"'No Aggregation' from Aggregation drop-down.\"\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        params = dict(\n            x=\"date\", y=json.dumps([\"foo\"]), group=json.dumps([\"security_id\"])\n        )\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        assert \"Group (security_id) contains more than 30 unique values\" in str(\n            response_data[\"error\"]\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        response = c.get(\n            \"/dtale/chart-data/{}\".format(c.port),\n            query_string=dict(query=\"missing_col == 'blah'\"),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            \"name 'missing_col' is not defined\",\n            \"should handle data exception\",\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        response = c.get(\n            \"/dtale/chart-data/{}\".format(c.port),\n            query_string=dict(query=\"security_id == 51\"),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            'query \"security_id == 51\" found no data, please alter',\n        )\n\n    df = pd.DataFrame([dict(a=i, b=np.nan) for i in range(100)])\n    df, _ = views.format_data(df)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(x=\"a\", y=json.dumps([\"b\"]), allowDupes=True)\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        unittest.assertEqual(response_data[\"error\"], 'All data for column \"b\" is NaN!')\n\n    df = pd.DataFrame([dict(a=i, b=i) for i in range(15500)])\n    df, _ = views.format_data(df)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(x=\"a\", y=json.dumps([\"b\"]), allowDupes=True)\n        response = c.get(\"/dtale/chart-data/{}\".format(c.port), query_string=params)\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data[\"error\"],\n            \"Dataset exceeds 15000 records, cannot render. Please apply filter...\",\n        )\n\n\n@pytest.mark.unit\ndef test_code_export():\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.views.build_code_export\", mock.Mock(side_effect=Exception())\n                )\n            )\n            response = c.get(\"/dtale/code-export/{}\".format(c.port))\n            assert \"error\" in response.get_json()\n\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_data\",\n                    mock.Mock(\n                        return_value={c.port: pd.DataFrame([dict(a=1), dict(a=2)])}\n                    ),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_settings\",\n                    mock.Mock(return_value={c.port: {\"query\": \"a in @a\"}}),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_context_variables\",\n                    mock.Mock(return_value={c.port: {\"a\": [1, 2]}}),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.views.build_code_export\", mock.Mock(side_effect=Exception())\n                )\n            )\n            response = c.get(\"/dtale/code-export/{}\".format(c.port))\n            assert \"error\" in response.get_json()\n\n        with ExitStack() as stack:\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_data\",\n                    mock.Mock(\n                        return_value={c.port: pd.DataFrame([dict(a=1), dict(a=2)])}\n                    ),\n                )\n            )\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.global_state.get_settings\",\n                    mock.Mock(return_value={c.port: {\"query\": \"a == 1\"}}),\n                )\n            )\n            response = c.get(\"/dtale/code-export/{}\".format(c.port))\n            assert response.get_json()[\"success\"]\n\n\n@pytest.mark.unit\ndef test_version_info():\n    with app.test_client() as c:\n        with mock.patch(\n            \"dtale.cli.clickutils.pkg_resources.get_distribution\",\n            mock.Mock(side_effect=Exception(\"blah\")),\n        ):\n            response = c.get(\"version-info\")\n            assert \"unknown\" in str(response.data)\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_chart_exports(custom_data, state_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: custom_data})\n        build_dtypes({c.port: views.build_dtypes_state(custom_data)})\n        params = dict(chart_type=\"invalid\")\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"application/json\"\n\n        params = dict(\n            chart_type=\"line\",\n            x=\"date\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"Col5 == 50\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"application/json\"\n\n        params = dict(chart_type=\"bar\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(chart_type=\"line\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(chart_type=\"scatter\", x=\"Col0\", y=json.dumps([\"Col1\"]))\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        params[\"trendline\"] = \"ols\"\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"3d_scatter\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"surface\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"pie\",\n            x=\"security_id\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"security_id >= 100000 and security_id <= 100010\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        params = dict(\n            chart_type=\"heatmap\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n        del params[\"x\"]\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"application/json\"\n\n    with app.test_client() as c:\n        build_data_inst({c.port: state_data})\n        build_dtypes({c.port: views.build_dtypes_state(state_data)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"choropleth\",\n            loc_mode=\"USA-states\",\n            loc=\"Code\",\n            map_val=\"val\",\n            agg=\"raw\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n    df = pd.DataFrame(\n        {\n            \"lat\": np.random.uniform(-40, 40, 50),\n            \"lon\": np.random.uniform(-40, 40, 50),\n            \"val\": np.random.randint(0, high=100, size=50),\n        }\n    )\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"scattergeo\",\n            lat=\"lat\",\n            lon=\"lon\",\n            map_val=\"val\",\n            scope=\"world\",\n            agg=\"raw\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n        response = c.get(\n            \"/dtale/chart-csv-export/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/csv\"\n\n\n@pytest.mark.skipif(not PY3, reason=\"requires python 3 or higher\")\ndef test_chart_exports_funnel(treemap_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: treemap_data})\n        build_dtypes({c.port: views.build_dtypes_state(treemap_data)})\n\n        params = dict(\n            chart_type=\"funnel\",\n            agg=\"mean\",\n            funnel_value=\"volume\",\n            funnel_label=\"label\",\n            funnel_group=json.dumps([\"group\"]),\n            funnel_stacked=False,\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.skipif(not PY3, reason=\"requires python 3 or higher\")\ndef test_chart_exports_clustergram(clustergram_data):\n    pytest.importorskip(\"dash_bio\")\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        df, _ = views.format_data(clustergram_data)\n        build_data_inst({c.port: df})\n        global_state.set_dtypes(c.port, views.build_dtypes_state(df))\n\n        params = dict(\n            chart_type=\"clustergram\",\n            clustergram_value=json.dumps([\"_all_columns_\"]),\n            clustergram_label=\"model\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\ndef test_chart_exports_pareto(pareto_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        df, _ = views.format_data(pareto_data)\n        build_data_inst({c.port: df})\n        global_state.set_dtypes(c.port, views.build_dtypes_state(df))\n\n        params = dict(\n            chart_type=\"pareto\",\n            pareto_x=\"desc\",\n            pareto_bars=\"count\",\n            pareto_line=\"cum_pct\",\n            pareto_dir=\"DESC\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\ndef test_chart_exports_histogram(test_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        df, _ = views.format_data(test_data)\n        build_data_inst({c.port: df})\n        global_state.set_dtypes(c.port, views.build_dtypes_state(df))\n\n        params = dict(\n            chart_type=\"histogram\",\n            histogram_col=\"foo\",\n            histogram_type=\"bins\",\n            histogram_bins=\"5\",\n        )\n        response = c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_export_all_charts(custom_data, state_data):\n    import dtale.views as views\n\n    global_state.clear_store()\n    with app.test_client() as c:\n        build_data_inst({c.port: custom_data})\n        build_dtypes({c.port: views.build_dtypes_state(custom_data)})\n        params = dict(chart_type=\"invalid\")\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"application/json\"\n\n        params = dict(\n            chart_type=\"line\",\n            x=\"date\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"Col5 == 50\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"application/json\"\n\n        params = dict(chart_type=\"bar\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params[\"group\"] = json.dumps([\"bool_val\"])\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(chart_type=\"line\", x=\"date\", y=json.dumps([\"Col0\"]), agg=\"sum\")\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(chart_type=\"scatter\", x=\"Col0\", y=json.dumps([\"Col1\"]))\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params[\"trendline\"] = \"ols\"\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"3d_scatter\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"surface\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"pie\",\n            x=\"security_id\",\n            y=json.dumps([\"Col0\"]),\n            agg=\"sum\",\n            query=\"security_id >= 100000 and security_id <= 100010\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n        params = dict(\n            chart_type=\"heatmap\", x=\"date\", y=json.dumps([\"security_id\"]), z=\"Col0\"\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n    with app.test_client() as c:\n        build_data_inst({c.port: state_data})\n        build_dtypes({c.port: views.build_dtypes_state(state_data)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"choropleth\",\n            loc_mode=\"USA-states\",\n            loc=\"Code\",\n            map_val=\"val\",\n            agg=\"raw\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n    df = pd.DataFrame(\n        {\n            \"lat\": np.random.uniform(-40, 40, 50),\n            \"lon\": np.random.uniform(-40, 40, 50),\n            \"val\": np.random.randint(0, high=100, size=50),\n        }\n    )\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        params = dict(\n            chart_type=\"maps\",\n            map_type=\"scattergeo\",\n            lat=\"lat\",\n            lon=\"lon\",\n            map_val=\"val\",\n            scope=\"world\",\n            agg=\"raw\",\n        )\n        response = c.get(\n            \"/dtale/chart-export-all/{}\".format(c.port), query_string=params\n        )\n        assert response.content_type == \"text/html\"\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_chart_png_export(custom_data):\n    import dtale.views as views\n\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: custom_data})\n            build_dtypes({c.port: views.build_dtypes_state(custom_data)})\n            write_image_mock = stack.enter_context(\n                mock.patch(\n                    \"dtale.dash_application.charts.write_image\", mock.MagicMock()\n                )\n            )\n\n            params = dict(\n                chart_type=\"heatmap\",\n                x=\"date\",\n                y=json.dumps([\"security_id\"]),\n                z=\"Col0\",\n                export_type=\"png\",\n            )\n            c.get(\"/dtale/chart-export/{}\".format(c.port), query_string=params)\n            assert write_image_mock.called\n\n\n@pytest.mark.unit\ndef test_main():\n    import dtale.views as views\n\n    global_state.clear_store()\n    test_data = pd.DataFrame(\n        build_ts_data(), columns=[\"date\", \"security_id\", \"foo\", \"bar\"]\n    )\n    test_data, _ = views.format_data(test_data)\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_name(c.port, \"test_name\")\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/{}\".format(c.port))\n        assert \"<title>D-Tale (test_name)</title>\" in str(response.data)\n        response = c.get(\"/dtale/iframe/{}\".format(c.port))\n        assert \"<title>D-Tale (test_name)</title>\" in str(response.data)\n        response = c.get(\n            \"/dtale/popup/test/{}\".format(c.port), query_string=dict(col=\"foo\")\n        )\n        assert \"<title>D-Tale (test_name) - Test (col: foo)</title>\" in str(\n            response.data\n        )\n        response = c.get(\n            \"/dtale/popup/reshape/{}\".format(c.port), query_string=dict(col=\"foo\")\n        )\n        assert \"<title>D-Tale (test_name) - Summarize Data (col: foo)</title>\" in str(\n            response.data\n        )\n        response = c.get(\n            \"/dtale/popup/filter/{}\".format(c.port), query_string=dict(col=\"foo\")\n        )\n        assert \"<title>D-Tale (test_name) - Custom Filter (col: foo)</title>\" in str(\n            response.data\n        )\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        build_dtypes({c.port: views.build_dtypes_state(test_data)})\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/{}\".format(c.port))\n        assert \"<title>D-Tale</title>\" in str(response.data)\n\n\n@pytest.mark.unit\ndef test_view_by_name():\n    import dtale.views as views\n\n    global_state.clear_store()\n    test_data = pd.DataFrame(\n        build_ts_data(), columns=[\"date\", \"security_id\", \"foo\", \"bar\"]\n    )\n    test_data, _ = views.format_data(test_data)\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_name(c.port, \"test_name\")\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/name/{}\".format(\"test_name\"))\n        assert \"<title>D-Tale (test_name)</title>\" in str(response.data)\n\n    with app.test_client() as c:\n        build_data_inst({c.port: test_data})\n        global_state.set_name(c.port, \"test_name2\")\n        build_settings({c.port: dict(locked=[])})\n        response = c.get(\"/dtale/main/name/{}\".format(\"test_name2\"))\n        assert \"<title>D-Tale (test_name2)</title>\" in str(response.data)\n\n\n@pytest.mark.unit\ndef test_200():\n    paths = [\n        \"/dtale/main/{port}\",\n        \"/dtale/iframe/{port}\",\n        \"/dtale/popup/test/{port}\",\n        \"site-map\",\n        \"version-info\",\n        \"health\",\n        \"/dtale/charts/{port}\",\n        \"/dtale/charts/popup/{port}\",\n        \"/dtale/code-popup\",\n        \"/dtale/popup/upload\",\n        \"/missing-js\",\n        \"/dtale/static/images/fire.jpg\",\n        \"/dtale/static/images/projections/miller.png\",\n        \"/dtale/static/images/map_type/choropleth.png\",\n        \"/dtale/static/maps/usa_110m.json\",\n        \"/dtale/network/{port}\",\n        \"/dtale/calculation/skew\",\n        \"/dtale/calculation/kurtosis\",\n    ]\n    with app.test_client() as c:\n        build_data_inst({c.port: None})\n        for path in paths:\n            final_path = path.format(port=c.port)\n            response = c.get(final_path)\n            assert response.status_code == 200, \"{} should return 200 response\".format(\n                final_path\n            )\n    with app.test_client(app_root=\"/test_route\") as c:\n        build_data_inst({c.port: None})\n        for path in paths:\n            final_path = path.format(port=c.port)\n            response = c.get(final_path)\n            assert response.status_code == 200, \"{} should return 200 response\".format(\n                final_path\n            )\n\n\n@pytest.mark.unit\ndef test_302():\n    import dtale.views as views\n\n    df = pd.DataFrame([1, 2, 3])\n    df, _ = views.format_data(df)\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        for path in [\n            \"/\",\n            \"/dtale\",\n            \"/dtale/main\",\n            \"/dtale/iframe\",\n            \"/dtale/popup/test\",\n            \"/favicon.ico\",\n            \"/dtale/iframe/popup/upload\",\n            \"/dtale/iframe/popup/describe/{}\".format(c.port),\n        ]:\n            response = c.get(path)\n            assert response.status_code == 302, \"{} should return 302 response\".format(\n                path\n            )\n    with app.test_client() as c:\n        build_data_inst({c.port: df})\n        for path in [\"/\"]:\n            response = c.get(path)\n            assert response.status_code == 302, \"{} should return 302 response\".format(\n                path\n            )\n\n\n@pytest.mark.unit\ndef test_missing_js():\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: pd.DataFrame([1, 2, 3])})\n            stack.enter_context(mock.patch(\"os.listdir\", mock.Mock(return_value=[])))\n            response = c.get(\"/\")\n            assert response.status_code == 302\n\n\n@pytest.mark.unit\ndef test_404():\n    response = app.test_client().get(\"/dtale/blah\")\n    assert response.status_code == 404\n    # make sure custom 404 page is returned\n    assert (\n        \"The page you were looking for <code>/dtale/blah</code> does not exist.\"\n        in str(response.data)\n    )\n\n\n@pytest.mark.unit\ndef test_500():\n    with app.test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({1: pd.DataFrame([1, 2, 3, 4, 5])})\n            stack.enter_context(\n                mock.patch(\n                    \"dtale.views.render_template\",\n                    mock.Mock(side_effect=Exception(\"Test\")),\n                )\n            )\n            for path in [\"/dtale/main/1\"]:\n                response = c.get(path)\n                assert response.status_code == 500\n                assert \"<h1>Internal Server Error</h1>\" in str(response.data)\n\n\n@pytest.mark.unit\ndef test_jinja_output():\n    import dtale.views as views\n\n    df = pd.DataFrame([1, 2, 3])\n    df, _ = views.format_data(df)\n    url = \"http://localhost.localdomain:40000\"\n    with build_app(url=url).test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n            response = c.get(\"/dtale/main/{}\".format(c.port))\n            assert 'span id=\"forkongithub\"' not in str(response.data)\n            response = c.get(\"/dtale/charts/{}\".format(c.port))\n            assert 'span id=\"forkongithub\"' not in str(response.data)\n\n    with build_app(url=url).test_client() as c:\n        with ExitStack() as stack:\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n            build_settings({c.port: {}})\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", {\"github_fork\": True})\n            )\n            response = c.get(\"/dtale/main/{}\".format(c.port))\n            assert 'span id=\"forkongithub\"' in str(response.data)\n\n\n@pytest.mark.unit\ndef test_build_context_variables():\n    import dtale.views as views\n    import dtale.global_state as global_state\n\n    data_id = \"1\"\n    global_state.new_data_inst(data_id)\n    with pytest.raises(SyntaxError) as error:\n        views.build_context_variables(data_id, {1: \"foo\"})\n    assert \"context variables must be a valid string\" in str(error.value)\n\n    with pytest.raises(SyntaxError) as error:\n        views.build_context_variables(data_id, {\"#!f_o#o\": \"bar\"})\n    assert \"context variables can only contain letters, digits, or underscores\" in str(\n        error.value\n    )\n\n    with pytest.raises(SyntaxError) as error:\n        views.build_context_variables(data_id, {\"_foo\": \"bar\"})\n    assert \"context variables can not start with an underscore\" in str(error.value)\n\n    # verify that pre-existing variables are not dropped when new ones are added\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"1\": \"cat\"})\n    )\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"2\": \"dog\"})\n    )\n    assert (global_state.get_context_variables(data_id)[\"1\"] == \"cat\") & (\n        global_state.get_context_variables(data_id)[\"2\"] == \"dog\"\n    )\n    # verify that new values will replace old ones if they share the same name\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"1\": \"cat\"})\n    )\n    global_state.set_context_variables(\n        data_id, views.build_context_variables(data_id, {\"1\": \"dog\"})\n    )\n    assert global_state.get_context_variables(data_id)[\"1\"] == \"dog\"\n\n\n@pytest.mark.unit\ndef test_get_filter_info(unittest):\n    with app.test_client() as c:\n        data_id = 1\n        context_vars = {\n            \"1\": [\"cat\", \"dog\"],\n            \"2\": 420346,\n            \"3\": pd.Series(range(1000)),\n            \"4\": \"A\" * 2000,\n        }\n        expected_return_value = [\n            dict(name=k, value=str(v)[:1000]) for k, v in context_vars.items()\n        ]\n        build_data_inst({data_id: None})\n        global_state.set_context_variables(data_id, context_vars)\n        response = c.get(\"/dtale/filter-info/{}\".format(data_id))\n        response_data = response.get_json()\n        assert response_data[\"success\"]\n        unittest.assertEqual(\n            response_data[\"contextVars\"], expected_return_value, \"should match expected\"\n        )\n\n    with app.test_client() as c:\n        global_state.set_context_variables(data_id, None)\n        response = c.get(\"/dtale/filter-info/{}\".format(data_id))\n        response_data = response.get_json()\n        unittest.assertEqual(len(response_data[\"contextVars\"]), 0)\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_get_column_filter_data(unittest, custom_data):\n    import dtale.views as views\n\n    df, _ = views.format_data(custom_data)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"bool_val\"),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data,\n            {\"hasMissing\": False, \"uniques\": [\"False\", \"True\"], \"success\": True},\n        )\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"str_val\"),\n        )\n        response_data = response.get_json()\n        assert response_data[\"hasMissing\"]\n        assert all(k in response_data for k in [\"hasMissing\", \"uniques\", \"success\"])\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"int_val\"),\n        )\n        response_data = response.get_json()\n        assert not response_data[\"hasMissing\"]\n        assert all(\n            k in response_data\n            for k in [\"max\", \"hasMissing\", \"uniques\", \"success\", \"min\"]\n        )\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port), query_string=dict(col=\"Col0\")\n        )\n        response_data = response.get_json()\n        assert not response_data[\"hasMissing\"]\n        assert all(k in response_data for k in [\"max\", \"hasMissing\", \"success\", \"min\"])\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port), query_string=dict(col=\"date\")\n        )\n        response_data = response.get_json()\n        assert not response_data[\"hasMissing\"]\n        assert all(k in response_data for k in [\"max\", \"hasMissing\", \"success\", \"min\"])\n\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"missing_col\"),\n        )\n        response_data = response.get_json()\n        assert not response_data[\"success\"]\n\n    # mixed data test\n    df = pd.DataFrame.from_dict(\n        {\n            \"a\": [\"a\", \"UNknown\", \"b\"],\n            \"b\": [\"\", \" \", \" - \"],\n            \"c\": [1, \"\", 3],\n            \"d\": [1.1, np.nan, 3],\n            \"e\": [\"a\", np.nan, \"b\"],\n        }\n    )\n    df, _ = views.format_data(df)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        response = c.get(\n            \"/dtale/column-filter-data/{}\".format(c.port), query_string=dict(col=\"c\")\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(sorted(response_data[\"uniques\"]), [\"\", \"1\", \"3\"])\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_get_async_column_filter_data(unittest, custom_data):\n    import dtale.views as views\n\n    df, _ = views.format_data(custom_data)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        str_val = df.str_val.values[0]\n        response = c.get(\n            \"/dtale/async-column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"str_val\", input=df.str_val.values[0]),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(response_data, [dict(value=str_val, label=str_val)])\n\n        int_val = df.int_val.values[0]\n        response = c.get(\n            \"/dtale/async-column-filter-data/{}\".format(c.port),\n            query_string=dict(col=\"int_val\", input=str(df.int_val.values[0])),\n        )\n        response_data = response.get_json()\n        unittest.assertEqual(\n            response_data, [dict(value=int_val, label=\"{}\".format(int_val))]\n        )\n\n\n@pytest.mark.unit\n@pytest.mark.parametrize(\"custom_data\", [dict(rows=1000, cols=3)], indirect=True)\ndef test_save_column_filter(unittest, custom_data):\n    import dtale.views as views\n\n    df, _ = views.format_data(custom_data)\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        settings = {c.port: {}}\n        build_settings(settings)\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"bool_val\", cfg=json.dumps({\"type\": \"string\", \"value\": [\"False\"]})\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"bool_val\"],\n            {\n                \"query\": \"`bool_val` == False\",\n                \"value\": [\"False\"],\n                \"action\": \"equals\",\n                \"caseSensitive\": False,\n                \"operand\": \"=\",\n                \"raw\": None,\n                \"meta\": {\"classification\": \"B\", \"column\": \"bool_val\", \"type\": \"string\"},\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"str_val\", cfg=json.dumps({\"type\": \"string\", \"value\": [\"a\", \"b\"]})\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"str_val\"],\n            {\n                \"query\": \"`str_val` in ('a', 'b')\"\n                if PY3\n                else \"`str_val` in (u'a', u'b')\",\n                \"value\": [\"a\", \"b\"],\n                \"action\": \"equals\",\n                \"caseSensitive\": False,\n                \"operand\": \"=\",\n                \"raw\": None,\n                \"meta\": {\"classification\": \"S\", \"column\": \"str_val\", \"type\": \"string\"},\n            },\n        )\n        for col, f_type in [\n            (\"bool_val\", \"string\"),\n            (\"int_val\", \"int\"),\n            (\"date\", \"date\"),\n        ]:\n            response = c.get(\n                \"/dtale/save-column-filter/{}\".format(c.port),\n                query_string=dict(\n                    col=col, cfg=json.dumps({\"type\": f_type, \"missing\": True})\n                ),\n            )\n            col_cfg = response.get_json()[\"currFilters\"][col]\n            assert col_cfg[\"query\"] == \"`{col}`.isnull()\".format(col=col)\n            assert col_cfg[\"missing\"]\n\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(col=\"bool_val\", cfg=None),\n        )\n        response_data = response.get_json()\n        assert \"error\" in response_data\n\n        meta = {\"classification\": \"I\", \"column\": \"int_val\", \"type\": \"int\"}\n        for operand in [\"=\", \"<\", \">\", \"<=\", \">=\"]:\n            response = c.get(\n                \"/dtale/save-column-filter/{}\".format(c.port),\n                query_string=dict(\n                    col=\"int_val\",\n                    cfg=json.dumps({\"type\": \"int\", \"operand\": operand, \"value\": \"5\"}),\n                ),\n            )\n            query = \"`int_val` {} 5\".format(\"==\" if operand == \"=\" else operand)\n            unittest.assertEqual(\n                response.get_json()[\"currFilters\"][\"int_val\"],\n                {\"query\": query, \"value\": \"5\", \"operand\": operand, \"meta\": meta},\n            )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps({\"type\": \"int\", \"operand\": \"=\", \"value\": [\"5\", \"4\"]}),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` in (5, 4)\",\n                \"value\": [\"5\", \"4\"],\n                \"operand\": \"=\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"[]\", \"min\": \"4\", \"max\": \"5\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` >= 4 and `int_val` <= 5\",\n                \"min\": \"4\",\n                \"max\": \"5\",\n                \"operand\": \"[]\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": \"4\", \"max\": \"5\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` > 4 and `int_val` < 5\",\n                \"min\": \"4\",\n                \"max\": \"5\",\n                \"operand\": \"()\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": \"4\", \"max\": None}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\"query\": \"`int_val` > 4\", \"min\": \"4\", \"operand\": \"()\", \"meta\": meta},\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": None, \"max\": \"5\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\"query\": \"`int_val` < 5\", \"max\": \"5\", \"operand\": \"()\", \"meta\": meta},\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"int_val\",\n                cfg=json.dumps(\n                    {\"type\": \"int\", \"operand\": \"()\", \"min\": \"4\", \"max\": \"4\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"int_val\"],\n            {\n                \"query\": \"`int_val` == 4\",\n                \"min\": \"4\",\n                \"max\": \"4\",\n                \"operand\": \"()\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"date\",\n                cfg=json.dumps(\n                    {\"type\": \"date\", \"start\": \"20000101\", \"end\": \"20000101\"}\n                ),\n            ),\n        )\n        meta = {\"classification\": \"D\", \"column\": \"date\", \"type\": \"date\"}\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"date\"],\n            {\n                \"query\": \"`date` == '20000101'\",\n                \"start\": \"20000101\",\n                \"end\": \"20000101\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"date\",\n                cfg=json.dumps(\n                    {\"type\": \"date\", \"start\": \"20000101\", \"end\": \"20000102\"}\n                ),\n            ),\n        )\n        unittest.assertEqual(\n            response.get_json()[\"currFilters\"][\"date\"],\n            {\n                \"query\": \"`date` >= '20000101' and `date` <= '20000102'\",\n                \"start\": \"20000101\",\n                \"end\": \"20000102\",\n                \"meta\": meta,\n            },\n        )\n        response = c.get(\n            \"/dtale/save-column-filter/{}\".format(c.port),\n            query_string=dict(\n                col=\"date\", cfg=json.dumps({\"type\": \"date\", \"missing\": False})\n            ),\n        )\n        assert \"date\" not in response.get_json()[\"currFilters\"]\n\n        assert settings[c.port].get(\"query\") is None\n\n        response = c.get(\"/dtale/move-filters-to-custom/{}\".format(c.port))\n        assert response.json[\"success\"]\n        assert response.json[\"settings\"].get(\"query\") is not None\n\n\n@pytest.mark.unit\ndef test_build_dtypes_state(test_data):\n    import dtale.views as views\n\n    state = views.build_dtypes_state(test_data.set_index(\"security_id\").T)\n    assert all(\"min\" not in r and \"max\" not in r for r in state)\n\n\n@pytest.mark.unit\ndef test_update_theme():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"theme\": \"light\"}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-theme\", query_string={\"theme\": \"dark\"})\n            assert app_settings[\"theme\"] == \"dark\"\n            response = c.get(\"/dtale/main/{}\".format(c.port))\n            assert '<body class=\"dark-mode\"' in str(response.data)\n\n\n@pytest.mark.unit\ndef test_update_query_engine():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"query_engine\": \"python\"}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-query-engine\", query_string={\"engine\": \"numexpr\"})\n            assert app_settings[\"query_engine\"] == \"numexpr\"\n\n\n@pytest.mark.unit\ndef test_update_pin_menu():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"pin_menu\": False}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-pin-menu\", query_string={\"pinned\": True})\n            assert app_settings[\"pin_menu\"]\n\n\n@pytest.mark.unit\ndef test_update_language():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {\"language\": \"en\"}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-language\", query_string={\"language\": \"cn\"})\n            assert app_settings[\"language\"] == \"cn\"\n\n\n@pytest.mark.unit\ndef test_update_max_column_width():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-maximum-column-width\", query_string={\"width\": 100})\n            assert app_settings[\"max_column_width\"] == 100\n\n\n@pytest.mark.unit\ndef test_update_max_row_height():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame([1, 2, 3, 4, 5]))\n    with build_app(url=URL).test_client() as c:\n        with ExitStack() as stack:\n            app_settings = {}\n            stack.enter_context(\n                mock.patch(\"dtale.global_state.APP_SETTINGS\", app_settings)\n            )\n\n            build_data_inst({c.port: df})\n            build_dtypes({c.port: views.build_dtypes_state(df)})\n\n            c.get(\"/dtale/update-maximum-row-height\", query_string={\"height\": 100})\n            assert app_settings[\"max_row_height\"] == 100\n\n\n@pytest.mark.unit\ndef test_load_filtered_ranges(unittest):\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        build_dtypes({c.port: views.build_dtypes_state(df)})\n        settings = {c.port: dict(query=\"a > 1\")}\n        build_settings(settings)\n        response = c.get(\"/dtale/load-filtered-ranges/{}\".format(c.port))\n        ranges = response.json\n        assert ranges[\"ranges\"][\"a\"][\"max\"] == 3 and ranges[\"overall\"][\"min\"] == 2\n        assert ranges[\"query\"] == settings[c.port][\"filteredRanges\"][\"query\"]\n        assert ranges[\"dtypes\"][\"a\"][\"max\"] == 3 and ranges[\"dtypes\"][\"a\"][\"min\"] == 2\n\n        response = c.get(\"/dtale/load-filtered-ranges/{}\".format(c.port))\n        unittest.assertEqual(ranges, response.json)\n\n        del settings[c.port][\"query\"]\n        response = c.get(\"/dtale/load-filtered-ranges/{}\".format(c.port))\n        assert not len(response.json)\n\n\n@pytest.mark.unit\ndef test_build_column_text():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n\n        resp = c.post(\n            \"/dtale/build-column-copy/{}\".format(c.port),\n            data=json.dumps({\"columns\": json.dumps([\"a\"])}),\n            content_type=\"application/json\",\n        )\n        assert resp.data == b\"1\\n2\\n3\\n\"\n\n\n@pytest.mark.unit\ndef test_build_row_text():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n\n        resp = c.post(\n            \"/dtale/build-row-copy/{}\".format(c.port),\n            data=json.dumps({\"start\": 1, \"end\": 2, \"columns\": json.dumps([\"a\"])}),\n            content_type=\"application/json\",\n        )\n        assert resp.data == b\"1\\n2\\n\"\n        resp = c.post(\n            \"/dtale/build-row-copy/{}\".format(c.port),\n            data=json.dumps({\"rows\": json.dumps([1]), \"columns\": json.dumps([\"a\"])}),\n            content_type=\"application/json\",\n        )\n        assert resp.data == b\"2\\n\"\n\n\n@pytest.mark.unit\ndef test_sorted_sequential_diffs():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3, 4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n\n        resp = c.get(\n            \"/dtale/sorted-sequential-diffs/{}\".format(c.port),\n            query_string=dict(col=\"a\", sort=\"ASC\"),\n        )\n        data = resp.json\n        assert data[\"min\"] == \"1\"\n        assert data[\"max\"] == \"1\"\n\n        resp = c.get(\n            \"/dtale/sorted-sequential-diffs/{}\".format(c.port),\n            query_string=dict(col=\"a\", sort=\"DESC\"),\n        )\n        data = resp.json\n        assert data[\"min\"] == \"-1\"\n        assert data[\"max\"] == \"-1\"\n\n\n@pytest.mark.unit\ndef test_drop_filtered_rows():\n    import dtale.views as views\n\n    df, _ = views.format_data(pd.DataFrame(dict(a=[1, 2, 3, 4, 5, 6])))\n    with build_app(url=URL).test_client() as c:\n        build_data_inst({c.port: df})\n        settings = {c.port: {\"query\": \"a == 1\"}}\n        build_settings(settings)\n        c.get(\"/dtale/drop-filtered-rows/{}\".format(c.port))\n        assert len(global_state.get_data(c.port)) == 1\n        assert global_state.get_settings(c.port)[\"query\"] == \"\"\n\n\n@pytest.mark.unit\ndef test_save_range_highlights():\n    with build_app(url=URL).test_client() as c:\n        settings = {c.port: {}}\n        build_settings(settings)\n        range_highlights = {\n            \"foo\": {\n                \"active\": True,\n                \"equals\": {\n                    \"active\": True,\n                    \"value\": 3,\n                    \"color\": {\"r\": 255, \"g\": 245, \"b\": 157, \"a\": 1},\n                },  # light yellow\n                \"greaterThan\": {\n                    \"active\": True,\n                    \"value\": 3,\n                    \"color\": {\"r\": 80, \"g\": 227, \"b\": 194, \"a\": 1},\n                },\n                # mint green\n                \"lessThan\": {\n                    \"active\": True,\n                    \"value\": 3,\n                    \"color\": {\"r\": 245, \"g\": 166, \"b\": 35, \"a\": 1},\n                },  # orange\n            }\n        }\n        c.post(\n            \"/dtale/save-range-highlights/{}\".format(c.port),\n            data=json.dumps(dict(ranges=json.dumps(range_highlights))),\n            content_type=\"application/json\",\n        )\n        assert global_state.get_settings(c.port)[\"rangeHighlight\"] is not None\n"], "filenames": ["README.md", "docs/CONFIGURATION.md", "dtale/__init__.py", "dtale/app.py", "dtale/config.py", "dtale/global_state.py", "dtale/templates/dtale/base.html", "dtale/views.py", "frontend/static/__tests__/dtale/DataViewer-filter-test.tsx", "frontend/static/__tests__/dtale/reduxGridUtils-test.ts", "frontend/static/__tests__/popups/filter/FilterPopup-test.tsx", "frontend/static/__tests__/reducers/dtale-test.tsx", "frontend/static/__tests__/test-utils.tsx", "frontend/static/dtale/export/main.tsx", "frontend/static/main.tsx", "frontend/static/popups/filter/FilterPanel.tsx", "frontend/static/popups/filter/FilterPopup.tsx", "frontend/static/redux/actions/AppActions.ts", "frontend/static/redux/actions/dtale.ts", "frontend/static/redux/reducers/app/settings.ts", "frontend/static/redux/selectors.ts", "frontend/static/redux/state/AppState.ts", "tests/dtale/config/dtale.ini", "tests/dtale/config/dtale_missing_props.ini", "tests/dtale/config/test_config.py", "tests/dtale/test_views.py"], "buggy_code_start_loc": [978, 27, 24, 721, 106, 4, 46, 353, 7, 25, 43, 30, 97, 25, 50, 1, 14, 65, 76, 77, 60, 384, 15, 7, 29, 25], "buggy_code_end_loc": [978, 27, 24, 806, 216, 604, 46, 1998, 137, 25, 44, 54, 97, 25, 50, 122, 174, 306, 76, 77, 108, 439, 15, 7, 108, 1377], "fixing_code_start_loc": [979, 28, 25, 722, 107, 4, 47, 354, 8, 26, 43, 31, 98, 26, 51, 0, 14, 66, 77, 78, 61, 385, 16, 8, 30, 26], "fixing_code_end_loc": [993, 29, 26, 810, 226, 619, 48, 2026, 149, 27, 44, 57, 99, 27, 52, 122, 188, 313, 85, 91, 118, 443, 17, 9, 115, 1404], "type": "NVD-CWE-noinfo", "message": "D-Tale is the combination of a Flask back-end and a React front-end to view & analyze Pandas data structures. Prior to version 3.7.0, users hosting D-Tale publicly can be vulnerable to remote code execution, allowing attackers to run malicious code on the server. This issue has been patched in version 3.7.0 by turning off \"Custom Filter\" input by default. The only workaround for versions earlier than 3.7.0 is to only host D-Tale to trusted users.\n", "other": {"cve": {"id": "CVE-2023-46134", "sourceIdentifier": "security-advisories@github.com", "published": "2023-10-25T21:15:10.167", "lastModified": "2023-11-06T17:14:17.363", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "D-Tale is the combination of a Flask back-end and a React front-end to view & analyze Pandas data structures. Prior to version 3.7.0, users hosting D-Tale publicly can be vulnerable to remote code execution, allowing attackers to run malicious code on the server. This issue has been patched in version 3.7.0 by turning off \"Custom Filter\" input by default. The only workaround for versions earlier than 3.7.0 is to only host D-Tale to trusted users.\n"}, {"lang": "es", "value": "D-Tale es la combinaci\u00f3n de un back-end de Flask y un front-end de React para ver y analizar las estructuras de datos de Pandas. Antes de la versi\u00f3n 3.7.0, los usuarios que alojaban D-Tale p\u00fablicamente pod\u00edan ser vulnerables a la ejecuci\u00f3n remota de c\u00f3digo, lo que permit\u00eda a los atacantes ejecutar c\u00f3digo malicioso en el servidor. Este problema se solucion\u00f3 en la versi\u00f3n 3.7.0 desactivando la entrada \"Filtro personalizado\" de forma predeterminada. El \u00fanico workaround para versiones anteriores a la 3.7.0 es alojar D-Tale \u00fanicamente para usuarios confiables."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:C/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 6.1, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 2.7}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-79"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:man:d-tale:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.7.0", "matchCriteriaId": "68AC5647-6905-43A2-86CB-2F15885EC755"}]}]}], "references": [{"url": "https://github.com/man-group/dtale/commit/bf8c54ab2490803f45f0652a9a0e221a94d39668", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/man-group/dtale/security/advisories/GHSA-jq6c-r9xf-qxjm", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/man-group/dtale/commit/bf8c54ab2490803f45f0652a9a0e221a94d39668"}}