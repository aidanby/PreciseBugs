{"buggy_code": ["\"\"\"\nThis module defines environment variables used in MLflow.\n\"\"\"\nimport os\n\n\nclass _EnvironmentVariable:\n    \"\"\"\n    Represents an environment variable.\n    \"\"\"\n\n    def __init__(self, name, type_, default):\n        self.name = name\n        self.type = type_\n        self.default = default\n\n    @property\n    def is_defined(self):\n        return self.name in os.environ\n\n    def get(self):\n        \"\"\"\n        Reads the value of the environment variable if it exists and converts it to the desired\n        type. Otherwise, returns the default value.\n        \"\"\"\n        val = os.getenv(self.name)\n        if val:\n            try:\n                return self.type(val)\n            except Exception as e:\n                raise ValueError(f\"Failed to convert {val} to {self.type} for {self.name}: {e}\")\n        return self.default\n\n    def __str__(self):\n        return f\"{self.name} (default: {self.default}, type: {self.type.__name__})\"\n\n    def __repr__(self):\n        return repr(self.name)\n\n\nclass _BooleanEnvironmentVariable(_EnvironmentVariable):\n    \"\"\"\n    Represents a boolean environment variable.\n    \"\"\"\n\n    def __init__(self, name, default):\n        # `default not in [True, False, None]` doesn't work because `1 in [True]`\n        # (or `0 in [False]`) returns True.\n        if not (default is True or default is False or default is None):\n            raise ValueError(f\"{name} default value must be one of [True, False, None]\")\n        super().__init__(name, bool, default)\n\n    def get(self):\n        if not self.is_defined:\n            return self.default\n\n        val = os.getenv(self.name)\n        lowercased = val.lower()\n        if lowercased not in [\"true\", \"false\", \"1\", \"0\"]:\n            raise ValueError(\n                f\"{self.name} value must be one of ['true', 'false', '1', '0'] (case-insensitive), \"\n                f\"but got {val}\"\n            )\n        return lowercased in [\"true\", \"1\"]\n\n\n#: Specifies the ``dfs_tmpdir`` parameter to use for ``mlflow.spark.save_model``,\n#: ``mlflow.spark.log_model`` and ``mlflow.spark.load_model``. See\n#: https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.save_model\n#: for more information.\n#: (default: ``/tmp/mlflow``)\nMLFLOW_DFS_TMP = _EnvironmentVariable(\"MLFLOW_DFS_TMP\", str, \"/tmp/mlflow\")\n\n#: Specifies the maximum number of retries for MLflow HTTP requests\n#: (default: ``5``)\nMLFLOW_HTTP_REQUEST_MAX_RETRIES = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", int, 5)\n\n#: Specifies the backoff increase factor between MLflow HTTP request failures\n#: (default: ``2``)\nMLFLOW_HTTP_REQUEST_BACKOFF_FACTOR = _EnvironmentVariable(\n    \"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", int, 2\n)\n\n#: Specifies the timeout in seconds for MLflow HTTP requests\n#: (default: ``120``)\nMLFLOW_HTTP_REQUEST_TIMEOUT = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", int, 120)\n\n#: Specifies whether MLFlow HTTP requests should be signed using AWS signature V4. It will overwrite\n#: (default: ``False``). When set, it will overwrite the \"Authorization\" HTTP header.\n#: See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html for more information.\nMLFLOW_TRACKING_AWS_SIGV4 = _BooleanEnvironmentVariable(\"MLFLOW_TRACKING_AWS_SIGV4\", False)\n\n#: Specifies the chunk size to use when downloading a file from GCS\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_DOWNLOAD_CHUNK_SIZE\", int, None)\n\n#: Specifies the chunk size to use when uploading a file to GCS.\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_UPLOAD_CHUNK_SIZE\", int, None)\n\n#: (Deprecated, please use ``MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT``)\n#: Specifies the default timeout to use when downloading/uploading a file from/to GCS\n#: (default: ``None``). If None, ``google.cloud.storage.constants._DEFAULT_TIMEOUT`` is used.\nMLFLOW_GCS_DEFAULT_TIMEOUT = _EnvironmentVariable(\"MLFLOW_GCS_DEFAULT_TIMEOUT\", int, None)\n\n#: Specifies whether to disable model logging and loading via mlflowdbfs.\n#: (default: ``None``)\n_DISABLE_MLFLOWDBFS = _EnvironmentVariable(\"DISABLE_MLFLOWDBFS\", str, None)\n\n#: Specifies the S3 endpoint URL to use for S3 artifact operations.\n#: (default: ``None``)\nMLFLOW_S3_ENDPOINT_URL = _EnvironmentVariable(\"MLFLOW_S3_ENDPOINT_URL\", str, None)\n\n#: Specifies whether or not to skip TLS certificate verification for S3 artifact operations.\n#: (default: ``False``)\nMLFLOW_S3_IGNORE_TLS = _BooleanEnvironmentVariable(\"MLFLOW_S3_IGNORE_TLS\", False)\n\n#: Specifies extra arguments for S3 artifact uploads.\n#: (default: ``None``)\nMLFLOW_S3_UPLOAD_EXTRA_ARGS = _EnvironmentVariable(\"MLFLOW_S3_UPLOAD_EXTRA_ARGS\", str, None)\n\n#: Specifies the location of a Kerberos ticket cache to use for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_TICKET_CACHE = _EnvironmentVariable(\"MLFLOW_KERBEROS_TICKET_CACHE\", str, None)\n\n#: Specifies a Kerberos user for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_USER = _EnvironmentVariable(\"MLFLOW_KERBEROS_USER\", str, None)\n\n#: Specifies extra pyarrow configurations for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_PYARROW_EXTRA_CONF = _EnvironmentVariable(\"MLFLOW_PYARROW_EXTRA_CONF\", str, None)\n\n#: Specifies the ``pool_size`` parameter to use for ``sqlalchemy.create_engine`` in the SQLAlchemy\n#: tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_size\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_SIZE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_SIZE\", int, None\n)\n\n#: Specifies the ``pool_recycle`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE\", int, None\n)\n\n#: Specifies the ``max_overflow`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.max_overflow\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW\", int, None\n)\n\n#: Specifies the ``echo`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.echo\n#: for more information.\n#: (default: ``False``)\nMLFLOW_SQLALCHEMYSTORE_ECHO = _BooleanEnvironmentVariable(\"MLFLOW_SQLALCHEMYSTORE_ECHO\", False)\n\n#: Specifies whether or not to print a warning when `--env-manager=conda` is specified.\n#: (default: ``False``)\nMLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING = _BooleanEnvironmentVariable(\n    \"MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\", False\n)\n#: Specifies the ``poolclass`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.poolclass\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOLCLASS = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOLCLASS\", str, None\n)\n\n#: Specifies the ``timeout_seconds`` for MLflow Model dependency inference operations.\n#: (default: ``120``)\nMLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT\", int, 120\n)\n\n#: Specifies the MLflow Model Scoring server request timeout in seconds\n#: (default: ``60``)\nMLFLOW_SCORING_SERVER_REQUEST_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT\", int, 60\n)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the timeout to use when uploading or downloading a file\n#: (default: ``None``). If None, individual artifact stores will choose defaults.\nMLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT\", int, None\n)\n\n#: Specifies the device intended for use in the predict function - can be used\n#: to override behavior where the GPU is used by default when available by\n#: setting this environment variable to be ``cpu``. Currently, this\n#: variable is only supported for the MLflow PyTorch flavor.\nMLFLOW_DEFAULT_PREDICTION_DEVICE = _EnvironmentVariable(\n    \"MLFLOW_DEFAULT_PREDICTION_DEVICE\", str, None\n)\n", "# Define all the service endpoint handlers here.\nimport json\nimport os\nimport re\nimport tempfile\nimport posixpath\nimport urllib\nimport pathlib\n\nimport logging\nfrom functools import wraps\n\nfrom flask import Response, request, current_app, send_file\nfrom google.protobuf import descriptor\nfrom google.protobuf.json_format import ParseError\n\nfrom mlflow.entities import Metric, Param, RunTag, ViewType, ExperimentTag, FileInfo\nfrom mlflow.entities.model_registry import RegisteredModelTag, ModelVersionTag\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model\nfrom mlflow.protos import databricks_pb2\nfrom mlflow.protos.service_pb2 import (\n    CreateExperiment,\n    MlflowService,\n    GetExperiment,\n    GetRun,\n    SearchRuns,\n    ListArtifacts,\n    GetMetricHistory,\n    CreateRun,\n    UpdateRun,\n    LogMetric,\n    LogParam,\n    SetTag,\n    SearchExperiments,\n    DeleteExperiment,\n    RestoreExperiment,\n    RestoreRun,\n    DeleteRun,\n    UpdateExperiment,\n    LogBatch,\n    DeleteTag,\n    SetExperimentTag,\n    GetExperimentByName,\n    LogModel,\n)\nfrom mlflow.protos.model_registry_pb2 import (\n    ModelRegistryService,\n    CreateRegisteredModel,\n    UpdateRegisteredModel,\n    DeleteRegisteredModel,\n    GetRegisteredModel,\n    GetLatestVersions,\n    CreateModelVersion,\n    UpdateModelVersion,\n    DeleteModelVersion,\n    GetModelVersion,\n    GetModelVersionDownloadUri,\n    SearchModelVersions,\n    RenameRegisteredModel,\n    TransitionModelVersionStage,\n    SearchRegisteredModels,\n    SetRegisteredModelTag,\n    DeleteRegisteredModelTag,\n    SetModelVersionTag,\n    DeleteModelVersionTag,\n    SetRegisteredModelAlias,\n    DeleteRegisteredModelAlias,\n    GetModelVersionByAlias,\n)\nfrom mlflow.protos.mlflow_artifacts_pb2 import (\n    MlflowArtifactsService,\n    DownloadArtifact,\n    UploadArtifact,\n    ListArtifacts as ListArtifactsMlflowArtifacts,\n    DeleteArtifact,\n)\nfrom mlflow.protos.databricks_pb2 import RESOURCE_DOES_NOT_EXIST, INVALID_PARAMETER_VALUE\nfrom mlflow.store.artifact.artifact_repository_registry import get_artifact_repository\nfrom mlflow.store.db.db_types import DATABASE_ENGINES\nfrom mlflow.tracking._model_registry.registry import ModelRegistryStoreRegistry\nfrom mlflow.tracking._tracking_service.registry import TrackingStoreRegistry\nfrom mlflow.utils.mime_type_utils import _guess_mime_type\nfrom mlflow.utils.proto_json_utils import message_to_json, parse_dict\nfrom mlflow.utils.validation import _validate_batch_log_api_req\nfrom mlflow.utils.string_utils import is_string_type\nfrom mlflow.utils.uri import is_local_uri\nfrom mlflow.utils.file_utils import local_file_uri_to_path\nfrom mlflow.tracking.registry import UnsupportedModelRegistryStoreURIException\n\n_logger = logging.getLogger(__name__)\n_tracking_store = None\n_model_registry_store = None\n_artifact_repo = None\nSTATIC_PREFIX_ENV_VAR = \"_MLFLOW_STATIC_PREFIX\"\n\n\nclass TrackingStoreRegistryWrapper(TrackingStoreRegistry):\n    def __init__(self):\n        super().__init__()\n        self.register(\"\", self._get_file_store)\n        self.register(\"file\", self._get_file_store)\n        for scheme in DATABASE_ENGINES:\n            self.register(scheme, self._get_sqlalchemy_store)\n        self.register_entrypoints()\n\n    @classmethod\n    def _get_file_store(cls, store_uri, artifact_uri):\n        from mlflow.store.tracking.file_store import FileStore\n\n        return FileStore(store_uri, artifact_uri)\n\n    @classmethod\n    def _get_sqlalchemy_store(cls, store_uri, artifact_uri):\n        from mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore\n\n        return SqlAlchemyStore(store_uri, artifact_uri)\n\n\nclass ModelRegistryStoreRegistryWrapper(ModelRegistryStoreRegistry):\n    def __init__(self):\n        super().__init__()\n        self.register(\"\", self._get_file_store)\n        self.register(\"file\", self._get_file_store)\n        for scheme in DATABASE_ENGINES:\n            self.register(scheme, self._get_sqlalchemy_store)\n        self.register_entrypoints()\n\n    @classmethod\n    def _get_file_store(cls, store_uri):\n        from mlflow.store.model_registry.file_store import FileStore\n\n        return FileStore(store_uri)\n\n    @classmethod\n    def _get_sqlalchemy_store(cls, store_uri):\n        from mlflow.store.model_registry.sqlalchemy_store import SqlAlchemyStore\n\n        return SqlAlchemyStore(store_uri)\n\n\n_tracking_store_registry = TrackingStoreRegistryWrapper()\n_model_registry_store_registry = ModelRegistryStoreRegistryWrapper()\n\n\ndef _get_artifact_repo_mlflow_artifacts():\n    \"\"\"\n    Get an artifact repository specified by ``--artifacts-destination`` option for ``mlflow server``\n    command.\n    \"\"\"\n    from mlflow.server import ARTIFACTS_DESTINATION_ENV_VAR\n\n    global _artifact_repo\n    if _artifact_repo is None:\n        _artifact_repo = get_artifact_repository(os.environ[ARTIFACTS_DESTINATION_ENV_VAR])\n    return _artifact_repo\n\n\ndef _is_serving_proxied_artifacts():\n    \"\"\"\n    :return: ``True`` if the MLflow server is serving proxied artifacts (i.e. acting as a proxy for\n             artifact upload / download / list operations), as would be enabled by specifying the\n             ``--serve-artifacts`` configuration option. ``False`` otherwise.\n    \"\"\"\n    from mlflow.server import SERVE_ARTIFACTS_ENV_VAR\n\n    return os.environ.get(SERVE_ARTIFACTS_ENV_VAR, \"false\") == \"true\"\n\n\ndef _is_servable_proxied_run_artifact_root(run_artifact_root):\n    \"\"\"\n    Determines whether or not the following are true:\n\n    - The specified Run artifact root is a proxied artifact root (i.e. an artifact root with scheme\n      ``http``, ``https``, or ``mlflow-artifacts``).\n\n    - The MLflow server is capable of resolving and accessing the underlying storage location\n      corresponding to the proxied artifact root, allowing it to fulfill artifact list and\n      download requests by using this storage location directly.\n\n    :param run_artifact_root: The Run artifact root location (URI).\n    :return: ``True`` if the specified Run artifact root refers to proxied artifacts that can be\n             served by this MLflow server (i.e. the server has access to the destination and\n             can respond to list and download requests for the artifact). ``False`` otherwise.\n    \"\"\"\n    parsed_run_artifact_root = urllib.parse.urlparse(run_artifact_root)\n    # NB: If the run artifact root is a proxied artifact root (has scheme `http`, `https`, or\n    # `mlflow-artifacts`) *and* the MLflow server is configured to serve artifacts, the MLflow\n    # server always assumes that it has access to the underlying storage location for the proxied\n    # artifacts. This may not always be accurate. For example:\n    #\n    # An organization may initially use the MLflow server to serve Tracking API requests and proxy\n    # access to artifacts stored in Location A (via `mlflow server --serve-artifacts`). Then, for\n    # scalability and / or security purposes, the organization may decide to store artifacts in a\n    # new location B and set up a separate server (e.g. `mlflow server --artifacts-only`) to proxy\n    # access to artifacts stored in Location B.\n    #\n    # In this scenario, requests for artifacts stored in Location B that are sent to the original\n    # MLflow server will fail if the original MLflow server does not have access to Location B\n    # because it will assume that it can serve all proxied artifacts regardless of the underlying\n    # location. Such failures can be remediated by granting the original MLflow server access to\n    # Location B.\n    return (\n        parsed_run_artifact_root.scheme in [\"http\", \"https\", \"mlflow-artifacts\"]\n        and _is_serving_proxied_artifacts()\n    )\n\n\ndef _get_proxied_run_artifact_destination_path(proxied_artifact_root, relative_path=None):\n    \"\"\"\n    Resolves the specified proxied artifact location within a Run to a concrete storage location.\n\n    :param proxied_artifact_root: The Run artifact root location (URI) with scheme ``http``,\n                                  ``https``, or `mlflow-artifacts` that can be resolved by the\n                                  MLflow server to a concrete storage location.\n    :param relative_path: The relative path of the destination within the specified\n                          ``proxied_artifact_root``. If ``None``, the destination is assumed to be\n                          the resolved ``proxied_artifact_root``.\n    :return: The storage location of the specified artifact.\n    \"\"\"\n    parsed_proxied_artifact_root = urllib.parse.urlparse(proxied_artifact_root)\n    assert parsed_proxied_artifact_root.scheme in [\"http\", \"https\", \"mlflow-artifacts\"]\n\n    if parsed_proxied_artifact_root.scheme == \"mlflow-artifacts\":\n        # If the proxied artifact root is an `mlflow-artifacts` URI, the run artifact root path is\n        # simply the path component of the URI, since the fully-qualified format of an\n        # `mlflow-artifacts` URI is `mlflow-artifacts://<netloc>/path/to/artifact`\n        proxied_run_artifact_root_path = parsed_proxied_artifact_root.path.lstrip(\"/\")\n    else:\n        # In this case, the proxied artifact root is an HTTP(S) URL referring to an mlflow-artifacts\n        # API route that can be used to download the artifact. These routes are always anchored at\n        # `/api/2.0/mlflow-artifacts/artifacts`. Accordingly, we split the path on this route anchor\n        # and interpret the rest of the path (everything after the route anchor) as the run artifact\n        # root path\n        mlflow_artifacts_http_route_anchor = \"/api/2.0/mlflow-artifacts/artifacts/\"\n        assert mlflow_artifacts_http_route_anchor in parsed_proxied_artifact_root.path\n\n        proxied_run_artifact_root_path = parsed_proxied_artifact_root.path.split(\n            mlflow_artifacts_http_route_anchor\n        )[1].lstrip(\"/\")\n\n    return (\n        posixpath.join(proxied_run_artifact_root_path, relative_path)\n        if relative_path is not None\n        else proxied_run_artifact_root_path\n    )\n\n\ndef _get_tracking_store(backend_store_uri=None, default_artifact_root=None):\n    from mlflow.server import BACKEND_STORE_URI_ENV_VAR, ARTIFACT_ROOT_ENV_VAR\n\n    global _tracking_store\n    if _tracking_store is None:\n        store_uri = backend_store_uri or os.environ.get(BACKEND_STORE_URI_ENV_VAR, None)\n        artifact_root = default_artifact_root or os.environ.get(ARTIFACT_ROOT_ENV_VAR, None)\n        _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\n    return _tracking_store\n\n\ndef _get_model_registry_store(registry_store_uri=None):\n    from mlflow.server import REGISTRY_STORE_URI_ENV_VAR, BACKEND_STORE_URI_ENV_VAR\n\n    global _model_registry_store\n    if _model_registry_store is None:\n        store_uri = (\n            registry_store_uri\n            or os.environ.get(REGISTRY_STORE_URI_ENV_VAR, None)\n            or os.environ.get(BACKEND_STORE_URI_ENV_VAR, None)\n        )\n        _model_registry_store = _model_registry_store_registry.get_store(store_uri)\n    return _model_registry_store\n\n\ndef initialize_backend_stores(\n    backend_store_uri=None, registry_store_uri=None, default_artifact_root=None\n):\n    _get_tracking_store(backend_store_uri, default_artifact_root)\n    try:\n        _get_model_registry_store(registry_store_uri)\n    except UnsupportedModelRegistryStoreURIException:\n        pass\n\n\ndef _assert_string(x):\n    assert isinstance(x, str)\n\n\ndef _assert_intlike(x):\n    try:\n        x = int(x)\n    except ValueError:\n        pass\n\n    assert isinstance(x, int)\n\n\ndef _assert_bool(x):\n    assert isinstance(x, bool)\n\n\ndef _assert_floatlike(x):\n    try:\n        x = float(x)\n    except ValueError:\n        pass\n\n    assert isinstance(x, float)\n\n\ndef _assert_array(x):\n    assert isinstance(x, list)\n\n\ndef _assert_required(x):\n    assert x is not None\n    # When parsing JSON payloads via proto, absent string fields\n    # are expressed as empty strings\n    assert x != \"\"\n\n\ndef _assert_less_than_or_equal(x, max_value):\n    assert x <= max_value\n\n\ndef _assert_item_type_string(x):\n    assert all(isinstance(item, str) for item in x)\n\n\n_TYPE_VALIDATORS = {\n    _assert_intlike,\n    _assert_string,\n    _assert_bool,\n    _assert_floatlike,\n    _assert_array,\n    _assert_item_type_string,\n}\n\n\ndef _validate_param_against_schema(schema, param, value, proto_parsing_succeeded=False):\n    \"\"\"\n    Attempts to validate a single parameter against a specified schema.\n    Examples of the elements of the schema are type assertions and checks for required parameters.\n    Returns None on validation success. Otherwise, raises an MLFlowException if an assertion fails.\n    This method is intended to be called for side effects.\n\n            Parameters:\n    :param schema: A list of functions to validate the parameter against.\n    :param param: The string name of the parameter being validated.\n    :param value: The corresponding value of the `param` being validated.\n    :param proto_parsing_succeeded: A boolean value indicating whether proto parsing succeeded.\n                                    If the proto was successfully parsed, we assume all of the types\n                                    of the parameters in the request body were correctly specified,\n                                    and thus we skip validating types. If proto parsing failed,\n                                    then we validate types in addition to the rest of the schema.\n                                    For details, see https://github.com/mlflow/mlflow/pull/\n                                    5458#issuecomment-1080880870.\n    \"\"\"\n\n    for f in schema:\n        if f in _TYPE_VALIDATORS and proto_parsing_succeeded:\n            continue\n\n        try:\n            f(value)\n        except AssertionError:\n            if f == _assert_required:\n                message = f\"Missing value for required parameter '{param}'.\"\n            else:\n                message = (\n                    f\"Invalid value {value} for parameter '{param}' supplied.\"\n                    f\" Hint: Value was of type '{type(value).__name__}'.\"\n                )\n            raise MlflowException(\n                message=(\n                    message + \" See the API docs for more information about request parameters.\"\n                ),\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n    return None\n\n\ndef _get_request_json(flask_request=request):\n    return flask_request.get_json(force=True, silent=True)\n\n\ndef _get_request_message(request_message, flask_request=request, schema=None):\n    from querystring_parser import parser\n\n    if flask_request.method == \"GET\" and len(flask_request.query_string) > 0:\n        # This is a hack to make arrays of length 1 work with the parser.\n        # for example experiment_ids%5B%5D=0 should be parsed to {experiment_ids: [0]}\n        # but it gets parsed to {experiment_ids: 0}\n        # but it doesn't. However, experiment_ids%5B0%5D=0 will get parsed to the right\n        # result.\n        query_string = re.sub(\"%5B%5D\", \"%5B0%5D\", flask_request.query_string.decode(\"utf-8\"))\n        request_dict = parser.parse(query_string, normalized=True)\n        # Convert atomic values of repeated fields to lists before calling protobuf deserialization.\n        # Context: We parse the parameter string into a dictionary outside of protobuf since\n        # protobuf does not know how to read the query parameters directly. The query parser above\n        # has no type information and hence any parameter that occurs exactly once is parsed as an\n        # atomic value. Since protobuf requires that the values of repeated fields are lists,\n        # deserialization will fail unless we do the fix below.\n        for field in request_message.DESCRIPTOR.fields:\n            if (\n                field.label == descriptor.FieldDescriptor.LABEL_REPEATED\n                and field.name in request_dict\n            ):\n                if not isinstance(request_dict[field.name], list):\n                    request_dict[field.name] = [request_dict[field.name]]\n        parse_dict(request_dict, request_message)\n        return request_message\n\n    request_json = _get_request_json(flask_request)\n\n    # Older clients may post their JSON double-encoded as strings, so the get_json\n    # above actually converts it to a string. Therefore, we check this condition\n    # (which we can tell for sure because any proper request should be a dictionary),\n    # and decode it a second time.\n    if is_string_type(request_json):\n        request_json = json.loads(request_json)\n\n    # If request doesn't have json body then assume it's empty.\n    if request_json is None:\n        request_json = {}\n\n    proto_parsing_succeeded = True\n    try:\n        parse_dict(request_json, request_message)\n    except ParseError:\n        proto_parsing_succeeded = False\n\n    schema = schema or {}\n    for schema_key, schema_validation_fns in schema.items():\n        if schema_key in request_json or _assert_required in schema_validation_fns:\n            value = request_json.get(schema_key)\n            if schema_key == \"run_id\" and value is None and \"run_uuid\" in request_json:\n                value = request_json.get(\"run_uuid\")\n            _validate_param_against_schema(\n                schema=schema_validation_fns,\n                param=schema_key,\n                value=value,\n                proto_parsing_succeeded=proto_parsing_succeeded,\n            )\n\n    return request_message\n\n\ndef _response_with_file_attachment_headers(file_path, response):\n    mime_type = _guess_mime_type(file_path)\n    filename = pathlib.Path(file_path).name\n    response.mimetype = mime_type\n    content_disposition_header_name = \"Content-Disposition\"\n    if content_disposition_header_name not in response.headers:\n        response.headers[content_disposition_header_name] = f\"attachment; filename={filename}\"\n    response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n    response.headers[\"Content-Type\"] = mime_type\n    return response\n\n\ndef _send_artifact(artifact_repository, path):\n    file_path = os.path.abspath(artifact_repository.download_artifacts(path))\n    # Always send artifacts as attachments to prevent the browser from displaying them on our web\n    # server's domain, which might enable XSS.\n    mime_type = _guess_mime_type(file_path)\n    file_sender_response = send_file(file_path, mimetype=mime_type, as_attachment=True)\n    return _response_with_file_attachment_headers(file_path, file_sender_response)\n\n\ndef catch_mlflow_exception(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except MlflowException as e:\n            response = Response(mimetype=\"application/json\")\n            response.set_data(e.serialize_as_json())\n            response.status_code = e.get_http_status_code()\n            return response\n\n    return wrapper\n\n\ndef _disable_unless_serve_artifacts(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not _is_serving_proxied_artifacts():\n            return Response(\n                (\n                    f\"Endpoint: {request.url_rule} disabled due to the mlflow server running \"\n                    \"with `--no-serve-artifacts`. To enable artifacts server functionality, \"\n                    \"run `mlflow server` with `--serve-artifacts`\"\n                ),\n                503,\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef _disable_if_artifacts_only(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        from mlflow.server import ARTIFACTS_ONLY_ENV_VAR\n\n        if os.environ.get(ARTIFACTS_ONLY_ENV_VAR):\n            return Response(\n                (\n                    f\"Endpoint: {request.url_rule} disabled due to the mlflow server running \"\n                    \"in `--artifacts-only` mode. To enable tracking server functionality, run \"\n                    \"`mlflow server` without `--artifacts-only`\"\n                ),\n                503,\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\n_OS_ALT_SEPS = [sep for sep in [os.sep, os.path.altsep] if sep is not None and sep != \"/\"]\n\n\ndef validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    \"\"\"\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(posixpath.sep)\n        or posixpath.isabs(path)\n    ):\n        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n\n\n@catch_mlflow_exception\ndef get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n\n\ndef _not_implemented():\n    response = Response()\n    response.status_code = 404\n    return response\n\n\n# Tracking Server APIs\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_experiment():\n    request_message = _get_request_message(\n        CreateExperiment(),\n        schema={\n            \"name\": [_assert_required, _assert_string],\n            \"artifact_location\": [_assert_string],\n            \"tags\": [_assert_array],\n        },\n    )\n\n    tags = [ExperimentTag(tag.key, tag.value) for tag in request_message.tags]\n    experiment_id = _get_tracking_store().create_experiment(\n        request_message.name, request_message.artifact_location, tags\n    )\n    response_message = CreateExperiment.Response()\n    response_message.experiment_id = experiment_id\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_experiment():\n    request_message = _get_request_message(\n        GetExperiment(), schema={\"experiment_id\": [_assert_required, _assert_string]}\n    )\n    response_message = GetExperiment.Response()\n    experiment = _get_tracking_store().get_experiment(request_message.experiment_id).to_proto()\n    response_message.experiment.MergeFrom(experiment)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_experiment_by_name():\n    request_message = _get_request_message(\n        GetExperimentByName(), schema={\"experiment_name\": [_assert_required, _assert_string]}\n    )\n    response_message = GetExperimentByName.Response()\n    store_exp = _get_tracking_store().get_experiment_by_name(request_message.experiment_name)\n    if store_exp is None:\n        raise MlflowException(\n            \"Could not find experiment with name '%s'\" % request_message.experiment_name,\n            error_code=RESOURCE_DOES_NOT_EXIST,\n        )\n    experiment = store_exp.to_proto()\n    response_message.experiment.MergeFrom(experiment)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_experiment():\n    request_message = _get_request_message(\n        DeleteExperiment(), schema={\"experiment_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().delete_experiment(request_message.experiment_id)\n    response_message = DeleteExperiment.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _restore_experiment():\n    request_message = _get_request_message(\n        RestoreExperiment(), schema={\"experiment_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().restore_experiment(request_message.experiment_id)\n    response_message = RestoreExperiment.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_experiment():\n    request_message = _get_request_message(\n        UpdateExperiment(),\n        schema={\n            \"experiment_id\": [_assert_required, _assert_string],\n            \"new_name\": [_assert_string, _assert_required],\n        },\n    )\n    if request_message.new_name:\n        _get_tracking_store().rename_experiment(\n            request_message.experiment_id, request_message.new_name\n        )\n    response_message = UpdateExperiment.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_run():\n    request_message = _get_request_message(\n        CreateRun(),\n        schema={\n            \"experiment_id\": [_assert_string],\n            \"start_time\": [_assert_intlike],\n            \"run_name\": [_assert_string],\n        },\n    )\n\n    tags = [RunTag(tag.key, tag.value) for tag in request_message.tags]\n    run = _get_tracking_store().create_run(\n        experiment_id=request_message.experiment_id,\n        user_id=request_message.user_id,\n        start_time=request_message.start_time,\n        tags=tags,\n        run_name=request_message.run_name,\n    )\n\n    response_message = CreateRun.Response()\n    response_message.run.MergeFrom(run.to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_run():\n    request_message = _get_request_message(\n        UpdateRun(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"end_time\": [_assert_intlike],\n            \"status\": [_assert_string],\n            \"run_name\": [_assert_string],\n        },\n    )\n    run_id = request_message.run_id or request_message.run_uuid\n    updated_info = _get_tracking_store().update_run_info(\n        run_id, request_message.status, request_message.end_time, request_message.run_name\n    )\n    response_message = UpdateRun.Response(run_info=updated_info.to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_run():\n    request_message = _get_request_message(\n        DeleteRun(), schema={\"run_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().delete_run(request_message.run_id)\n    response_message = DeleteRun.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _restore_run():\n    request_message = _get_request_message(\n        RestoreRun(), schema={\"run_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().restore_run(request_message.run_id)\n    response_message = RestoreRun.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_metric():\n    request_message = _get_request_message(\n        LogMetric(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_required, _assert_floatlike],\n            \"timestamp\": [_assert_intlike, _assert_required],\n            \"step\": [_assert_intlike],\n        },\n    )\n    metric = Metric(\n        request_message.key, request_message.value, request_message.timestamp, request_message.step\n    )\n    run_id = request_message.run_id or request_message.run_uuid\n    _get_tracking_store().log_metric(run_id, metric)\n    response_message = LogMetric.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_param():\n    request_message = _get_request_message(\n        LogParam(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_string],\n        },\n    )\n    param = Param(request_message.key, request_message.value)\n    run_id = request_message.run_id or request_message.run_uuid\n    _get_tracking_store().log_param(run_id, param)\n    response_message = LogParam.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_experiment_tag():\n    request_message = _get_request_message(\n        SetExperimentTag(),\n        schema={\n            \"experiment_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = ExperimentTag(request_message.key, request_message.value)\n    _get_tracking_store().set_experiment_tag(request_message.experiment_id, tag)\n    response_message = SetExperimentTag.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_tag():\n    request_message = _get_request_message(\n        SetTag(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = RunTag(request_message.key, request_message.value)\n    run_id = request_message.run_id or request_message.run_uuid\n    _get_tracking_store().set_tag(run_id, tag)\n    response_message = SetTag.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_tag():\n    request_message = _get_request_message(\n        DeleteTag(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n        },\n    )\n    _get_tracking_store().delete_tag(request_message.run_id, request_message.key)\n    response_message = DeleteTag.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_run():\n    request_message = _get_request_message(\n        GetRun(), schema={\"run_id\": [_assert_required, _assert_string]}\n    )\n    response_message = GetRun.Response()\n    run_id = request_message.run_id or request_message.run_uuid\n    response_message.run.MergeFrom(_get_tracking_store().get_run(run_id).to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_runs():\n    request_message = _get_request_message(\n        SearchRuns(),\n        schema={\n            \"experiment_ids\": [_assert_array],\n            \"filter\": [_assert_string],\n            \"max_results\": [_assert_intlike, lambda x: _assert_less_than_or_equal(x, 50000)],\n            \"order_by\": [_assert_array, _assert_item_type_string],\n        },\n    )\n    response_message = SearchRuns.Response()\n    run_view_type = ViewType.ACTIVE_ONLY\n    if request_message.HasField(\"run_view_type\"):\n        run_view_type = ViewType.from_proto(request_message.run_view_type)\n    filter_string = request_message.filter\n    max_results = request_message.max_results\n    experiment_ids = request_message.experiment_ids\n    order_by = request_message.order_by\n    page_token = request_message.page_token\n    run_entities = _get_tracking_store().search_runs(\n        experiment_ids, filter_string, run_view_type, max_results, order_by, page_token\n    )\n    response_message.runs.extend([r.to_proto() for r in run_entities])\n    if run_entities.token:\n        response_message.next_page_token = run_entities.token\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _list_artifacts():\n    request_message = _get_request_message(\n        ListArtifacts(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"path\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    response_message = ListArtifacts.Response()\n    if request_message.HasField(\"path\"):\n        path = request_message.path\n        validate_path_is_safe(path)\n    else:\n        path = None\n    run_id = request_message.run_id or request_message.run_uuid\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_entities = _list_artifacts_for_proxied_run_artifact_root(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_entities = _get_artifact_repo(run).list_artifacts(path)\n\n    response_message.files.extend([a.to_proto() for a in artifact_entities])\n    response_message.root_uri = run.info.artifact_uri\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\ndef _list_artifacts_for_proxied_run_artifact_root(proxied_artifact_root, relative_path=None):\n    \"\"\"\n    Lists artifacts from the specified ``relative_path`` within the specified proxied Run artifact\n    root (i.e. a Run artifact root with scheme ``http``, ``https``, or ``mlflow-artifacts``).\n\n    :param proxied_artifact_root: The Run artifact root location (URI) with scheme ``http``,\n                                  ``https``, or ``mlflow-artifacts`` that can be resolved by the\n                                  MLflow server to a concrete storage location.\n    :param relative_path: The relative path within the specified ``proxied_artifact_root`` under\n                          which to list artifact contents. If ``None``, artifacts are listed from\n                          the ``proxied_artifact_root`` directory.\n    \"\"\"\n    parsed_proxied_artifact_root = urllib.parse.urlparse(proxied_artifact_root)\n    assert parsed_proxied_artifact_root.scheme in [\"http\", \"https\", \"mlflow-artifacts\"]\n\n    artifact_destination_repo = _get_artifact_repo_mlflow_artifacts()\n    artifact_destination_path = _get_proxied_run_artifact_destination_path(\n        proxied_artifact_root=proxied_artifact_root,\n        relative_path=relative_path,\n    )\n\n    artifact_entities = []\n    for file_info in artifact_destination_repo.list_artifacts(artifact_destination_path):\n        basename = posixpath.basename(file_info.path)\n        run_relative_artifact_path = (\n            posixpath.join(relative_path, basename) if relative_path else basename\n        )\n        artifact_entities.append(\n            FileInfo(run_relative_artifact_path, file_info.is_dir, file_info.file_size)\n        )\n\n    return artifact_entities\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_metric_history():\n    request_message = _get_request_message(\n        GetMetricHistory(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"metric_key\": [_assert_string, _assert_required],\n        },\n    )\n    response_message = GetMetricHistory.Response()\n    run_id = request_message.run_id or request_message.run_uuid\n    metric_entities = _get_tracking_store().get_metric_history(run_id, request_message.metric_key)\n    response_message.metrics.extend([m.to_proto() for m in metric_entities])\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef get_metric_history_bulk_handler():\n    MAX_HISTORY_RESULTS = 25000\n    MAX_RUN_IDS_PER_REQUEST = 20\n    run_ids = request.args.to_dict(flat=False).get(\"run_id\", [])\n    if not run_ids:\n        raise MlflowException(\n            message=\"GetMetricHistoryBulk request must specify at least one run_id.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    if len(run_ids) > MAX_RUN_IDS_PER_REQUEST:\n        raise MlflowException(\n            message=(\n                f\"GetMetricHistoryBulk request cannot specify more than {MAX_RUN_IDS_PER_REQUEST}\"\n                f\" run_ids. Received {len(run_ids)} run_ids.\"\n            ),\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    metric_key = request.args.get(\"metric_key\")\n    if metric_key is None:\n        raise MlflowException(\n            message=\"GetMetricHistoryBulk request must specify a metric_key.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    max_results = int(request.args.get(\"max_results\", MAX_HISTORY_RESULTS))\n    max_results = min(max_results, MAX_HISTORY_RESULTS)\n\n    store = _get_tracking_store()\n\n    def _default_history_bulk_impl():\n        metrics_with_run_ids = []\n        for run_id in sorted(run_ids):\n            metrics_for_run = sorted(\n                store.get_metric_history(\n                    run_id=run_id,\n                    metric_key=metric_key,\n                    max_results=max_results,\n                ),\n                key=lambda metric: (metric.timestamp, metric.step, metric.value),\n            )\n            metrics_with_run_ids.extend(\n                [\n                    {\n                        \"key\": metric.key,\n                        \"value\": metric.value,\n                        \"timestamp\": metric.timestamp,\n                        \"step\": metric.step,\n                        \"run_id\": run_id,\n                    }\n                    for metric in metrics_for_run\n                ]\n            )\n        return metrics_with_run_ids\n\n    if hasattr(store, \"get_metric_history_bulk\"):\n        metrics_with_run_ids = [\n            metric.to_dict()\n            for metric in store.get_metric_history_bulk(\n                run_ids=run_ids,\n                metric_key=metric_key,\n                max_results=max_results,\n            )\n        ]\n    else:\n        metrics_with_run_ids = _default_history_bulk_impl()\n\n    return {\n        \"metrics\": metrics_with_run_ids[:max_results],\n    }\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_experiments():\n    request_message = _get_request_message(\n        SearchExperiments(),\n        schema={\n            \"view_type\": [_assert_intlike],\n            \"max_results\": [_assert_intlike],\n            \"order_by\": [_assert_array],\n            \"filter\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    experiment_entities = _get_tracking_store().search_experiments(\n        view_type=request_message.view_type,\n        max_results=request_message.max_results,\n        order_by=request_message.order_by,\n        filter_string=request_message.filter,\n        page_token=request_message.page_token,\n    )\n    response_message = SearchExperiments.Response()\n    response_message.experiments.extend([e.to_proto() for e in experiment_entities])\n    if experiment_entities.token:\n        response_message.next_page_token = experiment_entities.token\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\ndef _get_artifact_repo(run):\n    return get_artifact_repository(run.info.artifact_uri)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_batch():\n    def _assert_metrics_fields_present(metrics):\n        for m in metrics:\n            _assert_required(m[\"key\"])\n            _assert_required(m[\"value\"])\n            _assert_required(m[\"timestamp\"])\n            _assert_required(m[\"step\"])\n\n    def _assert_params_tags_fields_present(params_or_tags):\n        for param_or_tag in params_or_tags:\n            _assert_required(param_or_tag[\"key\"])\n\n    _validate_batch_log_api_req(_get_request_json())\n    request_message = _get_request_message(\n        LogBatch(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"metrics\": [_assert_array, _assert_metrics_fields_present],\n            \"params\": [_assert_array, _assert_params_tags_fields_present],\n            \"tags\": [_assert_array, _assert_params_tags_fields_present],\n        },\n    )\n    metrics = [Metric.from_proto(proto_metric) for proto_metric in request_message.metrics]\n    params = [Param.from_proto(proto_param) for proto_param in request_message.params]\n    tags = [RunTag.from_proto(proto_tag) for proto_tag in request_message.tags]\n    _get_tracking_store().log_batch(\n        run_id=request_message.run_id, metrics=metrics, params=params, tags=tags\n    )\n    response_message = LogBatch.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_model():\n    request_message = _get_request_message(\n        LogModel(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"model_json\": [_assert_string, _assert_required],\n        },\n    )\n    try:\n        model = json.loads(request_message.model_json)\n    except Exception:\n        raise MlflowException(\n            \"Malformed model info. \\n {} \\n is not a valid JSON.\".format(\n                request_message.model_json\n            ),\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    missing_fields = {\"artifact_path\", \"flavors\", \"utc_time_created\", \"run_id\"} - set(model.keys())\n\n    if missing_fields:\n        raise MlflowException(\n            f\"Model json is missing mandatory fields: {missing_fields}\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    _get_tracking_store().record_logged_model(\n        run_id=request_message.run_id, mlflow_model=Model.from_dict(model)\n    )\n    response_message = LogModel.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\ndef _wrap_response(response_message):\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n# Model Registry APIs\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_registered_model():\n    request_message = _get_request_message(\n        CreateRegisteredModel(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"tags\": [_assert_array],\n            \"description\": [_assert_string],\n        },\n    )\n    registered_model = _get_model_registry_store().create_registered_model(\n        name=request_message.name,\n        tags=request_message.tags,\n        description=request_message.description,\n    )\n    response_message = CreateRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_registered_model():\n    request_message = _get_request_message(\n        GetRegisteredModel(), schema={\"name\": [_assert_string, _assert_required]}\n    )\n    registered_model = _get_model_registry_store().get_registered_model(name=request_message.name)\n    response_message = GetRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_registered_model():\n    request_message = _get_request_message(\n        UpdateRegisteredModel(),\n        schema={\"name\": [_assert_string, _assert_required], \"description\": [_assert_string]},\n    )\n    name = request_message.name\n    new_description = request_message.description\n    registered_model = _get_model_registry_store().update_registered_model(\n        name=name, description=new_description\n    )\n    response_message = UpdateRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _rename_registered_model():\n    request_message = _get_request_message(\n        RenameRegisteredModel(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"new_name\": [_assert_string, _assert_required],\n        },\n    )\n    name = request_message.name\n    new_name = request_message.new_name\n    registered_model = _get_model_registry_store().rename_registered_model(\n        name=name, new_name=new_name\n    )\n    response_message = RenameRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_registered_model():\n    request_message = _get_request_message(\n        DeleteRegisteredModel(), schema={\"name\": [_assert_string, _assert_required]}\n    )\n    _get_model_registry_store().delete_registered_model(name=request_message.name)\n    return _wrap_response(DeleteRegisteredModel.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_registered_models():\n    request_message = _get_request_message(\n        SearchRegisteredModels(),\n        schema={\n            \"filter\": [_assert_string],\n            \"max_results\": [_assert_intlike, lambda x: _assert_less_than_or_equal(x, 1000)],\n            \"order_by\": [_assert_array, _assert_item_type_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    store = _get_model_registry_store()\n    registered_models = store.search_registered_models(\n        filter_string=request_message.filter,\n        max_results=request_message.max_results,\n        order_by=request_message.order_by,\n        page_token=request_message.page_token,\n    )\n    response_message = SearchRegisteredModels.Response()\n    response_message.registered_models.extend([e.to_proto() for e in registered_models])\n    if registered_models.token:\n        response_message.next_page_token = registered_models.token\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_latest_versions():\n    request_message = _get_request_message(\n        GetLatestVersions(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"stages\": [_assert_array, _assert_item_type_string],\n        },\n    )\n    latest_versions = _get_model_registry_store().get_latest_versions(\n        name=request_message.name, stages=request_message.stages\n    )\n    response_message = GetLatestVersions.Response()\n    response_message.model_versions.extend([e.to_proto() for e in latest_versions])\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_registered_model_tag():\n    request_message = _get_request_message(\n        SetRegisteredModelTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = RegisteredModelTag(key=request_message.key, value=request_message.value)\n    _get_model_registry_store().set_registered_model_tag(name=request_message.name, tag=tag)\n    return _wrap_response(SetRegisteredModelTag.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_registered_model_tag():\n    request_message = _get_request_message(\n        DeleteRegisteredModelTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_registered_model_tag(\n        name=request_message.name, key=request_message.key\n    )\n    return _wrap_response(DeleteRegisteredModelTag.Response())\n\n\ndef _validate_source(source: str, run_id: str) -> None:\n    if not is_local_uri(source):\n        return\n\n    if run_id:\n        store = _get_tracking_store()\n        run = store.get_run(run_id)\n        source = pathlib.Path(local_file_uri_to_path(source)).resolve()\n        run_artifact_dir = pathlib.Path(local_file_uri_to_path(run.info.artifact_uri)).resolve()\n        if run_artifact_dir in [source, *source.parents]:\n            return\n\n    raise MlflowException(\n        f\"Invalid source: '{source}'. To use a local path as source, the run_id request parameter \"\n        \"has to be specified and the local path has to be contained within the artifact directory \"\n        \"of the run specified by the run_id.\",\n        INVALID_PARAMETER_VALUE,\n    )\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_model_version():\n    request_message = _get_request_message(\n        CreateModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"source\": [_assert_string, _assert_required],\n            \"run_id\": [_assert_string],\n            \"tags\": [_assert_array],\n            \"run_link\": [_assert_string],\n            \"description\": [_assert_string],\n        },\n    )\n\n    _validate_source(request_message.source, request_message.run_id)\n\n    model_version = _get_model_registry_store().create_model_version(\n        name=request_message.name,\n        source=request_message.source,\n        run_id=request_message.run_id,\n        run_link=request_message.run_link,\n        tags=request_message.tags,\n        description=request_message.description,\n    )\n    response_message = CreateModelVersion.Response(model_version=model_version.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_model_version():\n    request_message = _get_request_message(\n        GetModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n        },\n    )\n    model_version = _get_model_registry_store().get_model_version(\n        name=request_message.name, version=request_message.version\n    )\n    response_proto = model_version.to_proto()\n    response_message = GetModelVersion.Response(model_version=response_proto)\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_model_version():\n    request_message = _get_request_message(\n        UpdateModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"description\": [_assert_string],\n        },\n    )\n    new_description = None\n    if request_message.HasField(\"description\"):\n        new_description = request_message.description\n    model_version = _get_model_registry_store().update_model_version(\n        name=request_message.name, version=request_message.version, description=new_description\n    )\n    return _wrap_response(UpdateModelVersion.Response(model_version=model_version.to_proto()))\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _transition_stage():\n    request_message = _get_request_message(\n        TransitionModelVersionStage(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"stage\": [_assert_string, _assert_required],\n            \"archive_existing_versions\": [_assert_bool],\n        },\n    )\n    model_version = _get_model_registry_store().transition_model_version_stage(\n        name=request_message.name,\n        version=request_message.version,\n        stage=request_message.stage,\n        archive_existing_versions=request_message.archive_existing_versions,\n    )\n    return _wrap_response(\n        TransitionModelVersionStage.Response(model_version=model_version.to_proto())\n    )\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_model_version():\n    request_message = _get_request_message(\n        DeleteModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_model_version(\n        name=request_message.name, version=request_message.version\n    )\n    return _wrap_response(DeleteModelVersion.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_model_version_download_uri():\n    request_message = _get_request_message(GetModelVersionDownloadUri())\n    download_uri = _get_model_registry_store().get_model_version_download_uri(\n        name=request_message.name, version=request_message.version\n    )\n    response_message = GetModelVersionDownloadUri.Response(artifact_uri=download_uri)\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_model_versions():\n    request_message = _get_request_message(\n        SearchModelVersions(),\n        schema={\n            \"filter\": [_assert_string],\n            \"max_results\": [_assert_intlike, lambda x: _assert_less_than_or_equal(x, 200_000)],\n            \"order_by\": [_assert_array, _assert_item_type_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    store = _get_model_registry_store()\n    model_versions = store.search_model_versions(\n        filter_string=request_message.filter,\n        max_results=request_message.max_results,\n        order_by=request_message.order_by,\n        page_token=request_message.page_token,\n    )\n    response_message = SearchModelVersions.Response()\n    response_message.model_versions.extend([e.to_proto() for e in model_versions])\n    if model_versions.token:\n        response_message.next_page_token = model_versions.token\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_model_version_tag():\n    request_message = _get_request_message(\n        SetModelVersionTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = ModelVersionTag(key=request_message.key, value=request_message.value)\n    _get_model_registry_store().set_model_version_tag(\n        name=request_message.name, version=request_message.version, tag=tag\n    )\n    return _wrap_response(SetModelVersionTag.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_model_version_tag():\n    request_message = _get_request_message(\n        DeleteModelVersionTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_model_version_tag(\n        name=request_message.name, version=request_message.version, key=request_message.key\n    )\n    return _wrap_response(DeleteModelVersionTag.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_registered_model_alias():\n    request_message = _get_request_message(\n        SetRegisteredModelAlias(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"alias\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().set_registered_model_alias(\n        name=request_message.name, alias=request_message.alias, version=request_message.version\n    )\n    return _wrap_response(SetRegisteredModelAlias.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_registered_model_alias():\n    request_message = _get_request_message(\n        DeleteRegisteredModelAlias(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"alias\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_registered_model_alias(\n        name=request_message.name, alias=request_message.alias\n    )\n    return _wrap_response(DeleteRegisteredModelAlias.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_model_version_by_alias():\n    request_message = _get_request_message(\n        GetModelVersionByAlias(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"alias\": [_assert_string, _assert_required],\n        },\n    )\n    model_version = _get_model_registry_store().get_model_version_by_alias(\n        name=request_message.name, alias=request_message.alias\n    )\n    response_proto = model_version.to_proto()\n    response_message = GetModelVersionByAlias.Response(model_version=response_proto)\n    return _wrap_response(response_message)\n\n\n# MLflow Artifacts APIs\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _download_artifact(artifact_path):\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts/<artifact_path>` to download an artifact\n    from `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    tmp_dir = tempfile.TemporaryDirectory()\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    dst = artifact_repo.download_artifacts(artifact_path, tmp_dir.name)\n\n    # Ref: https://stackoverflow.com/a/24613980/6943581\n    file_handle = open(dst, \"rb\")\n\n    def stream_and_remove_file():\n        yield from file_handle\n        file_handle.close()\n        tmp_dir.cleanup()\n\n    file_sender_response = current_app.response_class(stream_and_remove_file())\n\n    return _response_with_file_attachment_headers(artifact_path, file_sender_response)\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `PUT /mlflow-artifacts/artifacts/<artifact_path>` to upload an artifact\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    head, tail = posixpath.split(artifact_path)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        tmp_path = os.path.join(tmp_dir, tail)\n        with open(tmp_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            while True:\n                chunk = request.stream.read(chunk_size)\n                if len(chunk) == 0:\n                    break\n                f.write(chunk)\n\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_repo.log_artifact(tmp_path, artifact_path=head or None)\n\n    return _wrap_response(UploadArtifact.Response())\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _list_artifacts_mlflow_artifacts():\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts?path=<value>` to list artifacts in `path`\n    (a relative path from the root artifact directory).\n    \"\"\"\n    request_message = _get_request_message(ListArtifactsMlflowArtifacts())\n    if request_message.HasField(\"path\"):\n        validate_path_is_safe(request_message.path)\n        path = request_message.path\n    else:\n        path = None\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    files = []\n    for file_info in artifact_repo.list_artifacts(path):\n        basename = posixpath.basename(file_info.path)\n        new_file_info = FileInfo(basename, file_info.is_dir, file_info.file_size)\n        files.append(new_file_info.to_proto())\n    response_message = ListArtifacts.Response()\n    response_message.files.extend(files)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _delete_artifact_mlflow_artifacts(artifact_path):\n    \"\"\"\n    A request handler for `DELETE /mlflow-artifacts/artifacts?path=<value>` to delete artifacts in\n    `path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    _get_request_message(DeleteArtifact())\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    artifact_repo.delete_artifacts(artifact_path)\n    response_message = DeleteArtifact.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\ndef _add_static_prefix(route):\n    prefix = os.environ.get(STATIC_PREFIX_ENV_VAR)\n    if prefix:\n        return prefix + route\n    return route\n\n\ndef _get_paths(base_path):\n    \"\"\"\n    A service endpoints base path is typically something like /mlflow/experiment.\n    We should register paths like /api/2.0/mlflow/experiment and\n    /ajax-api/2.0/mlflow/experiment in the Flask router.\n    \"\"\"\n    return [f\"/api/2.0{base_path}\", _add_static_prefix(f\"/ajax-api/2.0{base_path}\")]\n\n\ndef get_handler(request_class):\n    \"\"\"\n    :param request_class: The type of protobuf message\n    :return:\n    \"\"\"\n    return HANDLERS.get(request_class, _not_implemented)\n\n\ndef get_endpoints():\n    \"\"\"\n    :return: List of tuples (path, handler, methods)\n    \"\"\"\n\n    def get_service_endpoints(service):\n        ret = []\n        for service_method in service.DESCRIPTOR.methods:\n            endpoints = service_method.GetOptions().Extensions[databricks_pb2.rpc].endpoints\n            for endpoint in endpoints:\n                for http_path in _get_paths(endpoint.path):\n                    handler = get_handler(service().GetRequestClass(service_method))\n                    ret.append((http_path, handler, [endpoint.method]))\n        return ret\n\n    return (\n        get_service_endpoints(MlflowService)\n        + get_service_endpoints(ModelRegistryService)\n        + get_service_endpoints(MlflowArtifactsService)\n    )\n\n\nHANDLERS = {\n    # Tracking Server APIs\n    CreateExperiment: _create_experiment,\n    GetExperiment: _get_experiment,\n    GetExperimentByName: _get_experiment_by_name,\n    DeleteExperiment: _delete_experiment,\n    RestoreExperiment: _restore_experiment,\n    UpdateExperiment: _update_experiment,\n    CreateRun: _create_run,\n    UpdateRun: _update_run,\n    DeleteRun: _delete_run,\n    RestoreRun: _restore_run,\n    LogParam: _log_param,\n    LogMetric: _log_metric,\n    SetExperimentTag: _set_experiment_tag,\n    SetTag: _set_tag,\n    DeleteTag: _delete_tag,\n    LogBatch: _log_batch,\n    LogModel: _log_model,\n    GetRun: _get_run,\n    SearchRuns: _search_runs,\n    ListArtifacts: _list_artifacts,\n    GetMetricHistory: _get_metric_history,\n    SearchExperiments: _search_experiments,\n    # Model Registry APIs\n    CreateRegisteredModel: _create_registered_model,\n    GetRegisteredModel: _get_registered_model,\n    DeleteRegisteredModel: _delete_registered_model,\n    UpdateRegisteredModel: _update_registered_model,\n    RenameRegisteredModel: _rename_registered_model,\n    SearchRegisteredModels: _search_registered_models,\n    GetLatestVersions: _get_latest_versions,\n    CreateModelVersion: _create_model_version,\n    GetModelVersion: _get_model_version,\n    DeleteModelVersion: _delete_model_version,\n    UpdateModelVersion: _update_model_version,\n    TransitionModelVersionStage: _transition_stage,\n    GetModelVersionDownloadUri: _get_model_version_download_uri,\n    SearchModelVersions: _search_model_versions,\n    SetRegisteredModelTag: _set_registered_model_tag,\n    DeleteRegisteredModelTag: _delete_registered_model_tag,\n    SetModelVersionTag: _set_model_version_tag,\n    DeleteModelVersionTag: _delete_model_version_tag,\n    SetRegisteredModelAlias: _set_registered_model_alias,\n    DeleteRegisteredModelAlias: _delete_registered_model_alias,\n    GetModelVersionByAlias: _get_model_version_by_alias,\n    # MLflow Artifacts APIs\n    DownloadArtifact: _download_artifact,\n    UploadArtifact: _upload_artifact,\n    ListArtifactsMlflowArtifacts: _list_artifacts_mlflow_artifacts,\n    DeleteArtifact: _delete_artifact_mlflow_artifacts,\n}\n", "import pathlib\nimport posixpath\nimport urllib.parse\nimport uuid\n\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE\nfrom mlflow.store.db.db_types import DATABASE_ENGINES\nfrom mlflow.utils.validation import _validate_db_type_string\nfrom mlflow.utils.os import is_windows\n\n_INVALID_DB_URI_MSG = (\n    \"Please refer to https://mlflow.org/docs/latest/tracking.html#storage for \"\n    \"format specifications.\"\n)\n\n_DBFS_FUSE_PREFIX = \"/dbfs/\"\n_DBFS_HDFS_URI_PREFIX = \"dbfs:/\"\n_DATABRICKS_UNITY_CATALOG_SCHEME = \"databricks-uc\"\n\n\ndef is_local_uri(uri):\n    \"\"\"Returns true if this is a local file path (/foo or file:/foo).\"\"\"\n    if uri == \"databricks\":\n        return False\n\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        # windows network drive path looks like: \"\\\\<server name>\\path\\...\"\n        return False\n\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname:\n        return False\n\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n\n    return False\n\n\ndef is_http_uri(uri):\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"http\" or scheme == \"https\"\n\n\ndef is_databricks_uri(uri):\n    \"\"\"\n    Databricks URIs look like 'databricks' (default profile) or 'databricks://profile'\n    or 'databricks://secret_scope:secret_key_prefix'.\n    \"\"\"\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"databricks\" or uri == \"databricks\"\n\n\ndef is_databricks_unity_catalog_uri(uri):\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == _DATABRICKS_UNITY_CATALOG_SCHEME or uri == _DATABRICKS_UNITY_CATALOG_SCHEME\n\n\ndef construct_db_uri_from_profile(profile):\n    if profile:\n        return \"databricks://\" + profile\n\n\n# Both scope and key_prefix should not contain special chars for URIs, like '/'\n# and ':'.\ndef validate_db_scope_prefix_info(scope, prefix):\n    for c in [\"/\", \":\", \" \"]:\n        if c in scope:\n            raise MlflowException(\n                \"Unsupported Databricks profile name: %s.\" % scope\n                + \" Profile names cannot contain '%s'.\" % c\n            )\n        if prefix and c in prefix:\n            raise MlflowException(\n                \"Unsupported Databricks profile key prefix: %s.\" % prefix\n                + \" Key prefixes cannot contain '%s'.\" % c\n            )\n    if prefix is not None and prefix.strip() == \"\":\n        raise MlflowException(\n            \"Unsupported Databricks profile key prefix: '%s'.\" % prefix\n            + \" Key prefixes cannot be empty.\"\n        )\n\n\ndef get_db_info_from_uri(uri):\n    \"\"\"\n    Get the Databricks profile specified by the tracking URI (if any), otherwise\n    returns None.\n    \"\"\"\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.scheme == \"databricks\" or parsed_uri.scheme == _DATABRICKS_UNITY_CATALOG_SCHEME:\n        # netloc should not be an empty string unless URI is formatted incorrectly.\n        if parsed_uri.netloc == \"\":\n            raise MlflowException(\n                \"URI is formatted incorrectly: no netloc in URI '%s'.\" % uri\n                + \" This may be the case if there is only one slash in the URI.\"\n            )\n        profile_tokens = parsed_uri.netloc.split(\":\")\n        parsed_scope = profile_tokens[0]\n        if len(profile_tokens) == 1:\n            parsed_key_prefix = None\n        elif len(profile_tokens) == 2:\n            parsed_key_prefix = profile_tokens[1]\n        else:\n            # parse the content before the first colon as the profile.\n            parsed_key_prefix = \":\".join(profile_tokens[1:])\n        validate_db_scope_prefix_info(parsed_scope, parsed_key_prefix)\n        return parsed_scope, parsed_key_prefix\n    return None, None\n\n\ndef get_databricks_profile_uri_from_artifact_uri(uri, result_scheme=\"databricks\"):\n    \"\"\"\n    Retrieves the netloc portion of the URI as a ``databricks://`` or `databricks-uc://` URI,\n    if it is a proper Databricks profile specification, e.g.\n    ``profile@databricks`` or ``secret_scope:key_prefix@databricks``.\n    \"\"\"\n    parsed = urllib.parse.urlparse(uri)\n    if not parsed.netloc or parsed.hostname != result_scheme:\n        return None\n    if not parsed.username:  # no profile or scope:key\n        return result_scheme  # the default tracking/registry URI\n    validate_db_scope_prefix_info(parsed.username, parsed.password)\n    key_prefix = \":\" + parsed.password if parsed.password else \"\"\n    return f\"{result_scheme}://\" + parsed.username + key_prefix\n\n\ndef remove_databricks_profile_info_from_artifact_uri(artifact_uri):\n    \"\"\"\n    Only removes the netloc portion of the URI if it is a Databricks\n    profile specification, e.g.\n    ``profile@databricks`` or ``secret_scope:key_prefix@databricks``.\n    \"\"\"\n    parsed = urllib.parse.urlparse(artifact_uri)\n    if not parsed.netloc or parsed.hostname != \"databricks\":\n        return artifact_uri\n    return urllib.parse.urlunparse(parsed._replace(netloc=\"\"))\n\n\ndef add_databricks_profile_info_to_artifact_uri(artifact_uri, databricks_profile_uri):\n    \"\"\"\n    Throws an exception if ``databricks_profile_uri`` is not valid.\n    \"\"\"\n    if not databricks_profile_uri or not is_databricks_uri(databricks_profile_uri):\n        return artifact_uri\n    artifact_uri_parsed = urllib.parse.urlparse(artifact_uri)\n    # Do not overwrite the authority section if there is already one\n    if artifact_uri_parsed.netloc:\n        return artifact_uri\n\n    scheme = artifact_uri_parsed.scheme\n    if scheme == \"dbfs\" or scheme == \"runs\" or scheme == \"models\":\n        if databricks_profile_uri == \"databricks\":\n            netloc = \"databricks\"\n        else:\n            (profile, key_prefix) = get_db_info_from_uri(databricks_profile_uri)\n            prefix = \":\" + key_prefix if key_prefix else \"\"\n            netloc = profile + prefix + \"@databricks\"\n        new_parsed = artifact_uri_parsed._replace(netloc=netloc)\n        return urllib.parse.urlunparse(new_parsed)\n    else:\n        return artifact_uri\n\n\ndef extract_db_type_from_uri(db_uri):\n    \"\"\"\n    Parse the specified DB URI to extract the database type. Confirm the database type is\n    supported. If a driver is specified, confirm it passes a plausible regex.\n    \"\"\"\n    scheme = urllib.parse.urlparse(db_uri).scheme\n    scheme_plus_count = scheme.count(\"+\")\n\n    if scheme_plus_count == 0:\n        db_type = scheme\n    elif scheme_plus_count == 1:\n        db_type, _ = scheme.split(\"+\")\n    else:\n        error_msg = f\"Invalid database URI: '{db_uri}'. {_INVALID_DB_URI_MSG}\"\n        raise MlflowException(error_msg, INVALID_PARAMETER_VALUE)\n\n    _validate_db_type_string(db_type)\n\n    return db_type\n\n\ndef get_uri_scheme(uri_or_path):\n    scheme = urllib.parse.urlparse(uri_or_path).scheme\n    if any(scheme.lower().startswith(db) for db in DATABASE_ENGINES):\n        return extract_db_type_from_uri(uri_or_path)\n    return scheme\n\n\ndef extract_and_normalize_path(uri):\n    parsed_uri_path = urllib.parse.urlparse(uri).path\n    normalized_path = posixpath.normpath(parsed_uri_path)\n    return normalized_path.lstrip(\"/\")\n\n\ndef append_to_uri_path(uri, *paths):\n    \"\"\"\n    Appends the specified POSIX `paths` to the path component of the specified `uri`.\n\n    :param uri: The input URI, represented as a string.\n    :param paths: The POSIX paths to append to the specified `uri`'s path component.\n    :return: A new URI with a path component consisting of the specified `paths` appended to\n             the path component of the specified `uri`.\n\n    >>> uri1 = \"s3://root/base/path?param=value\"\n    >>> uri1 = append_to_uri_path(uri1, \"some/subpath\", \"/anotherpath\")\n    >>> assert uri1 == \"s3://root/base/path/some/subpath/anotherpath?param=value\"\n    >>> uri2 = \"a/posix/path\"\n    >>> uri2 = append_to_uri_path(uri2, \"/some\", \"subpath\")\n    >>> assert uri2 == \"a/posixpath/some/subpath\"\n    \"\"\"\n    path = \"\"\n    for subpath in paths:\n        path = _join_posixpaths_and_append_absolute_suffixes(path, subpath)\n\n    parsed_uri = urllib.parse.urlparse(uri)\n    if len(parsed_uri.scheme) == 0:\n        # If the input URI does not define a scheme, we assume that it is a POSIX path\n        # and join it with the specified input paths\n        return _join_posixpaths_and_append_absolute_suffixes(uri, path)\n\n    prefix = \"\"\n    if not parsed_uri.path.startswith(\"/\"):\n        # For certain URI schemes (e.g., \"file:\"), urllib's unparse routine does\n        # not preserve the relative URI path component properly. In certain cases,\n        # urlunparse converts relative paths to absolute paths. We introduce this logic\n        # to circumvent urlunparse's erroneous conversion\n        prefix = parsed_uri.scheme + \":\"\n        parsed_uri = parsed_uri._replace(scheme=\"\")\n\n    new_uri_path = _join_posixpaths_and_append_absolute_suffixes(parsed_uri.path, path)\n    new_parsed_uri = parsed_uri._replace(path=new_uri_path)\n    return prefix + urllib.parse.urlunparse(new_parsed_uri)\n\n\ndef _join_posixpaths_and_append_absolute_suffixes(prefix_path, suffix_path):\n    \"\"\"\n    Joins the POSIX path `prefix_path` with the POSIX path `suffix_path`. Unlike posixpath.join(),\n    if `suffix_path` is an absolute path, it is appended to prefix_path.\n\n    >>> result1 = _join_posixpaths_and_append_absolute_suffixes(\"relpath1\", \"relpath2\")\n    >>> assert result1 == \"relpath1/relpath2\"\n    >>> result2 = _join_posixpaths_and_append_absolute_suffixes(\"relpath\", \"/absolutepath\")\n    >>> assert result2 == \"relpath/absolutepath\"\n    >>> result3 = _join_posixpaths_and_append_absolute_suffixes(\"/absolutepath\", \"relpath\")\n    >>> assert result3 == \"/absolutepath/relpath\"\n    >>> result4 = _join_posixpaths_and_append_absolute_suffixes(\"/absolutepath1\", \"/absolutepath2\")\n    >>> assert result4 == \"/absolutepath1/absolutepath2\"\n    \"\"\"\n    if len(prefix_path) == 0:\n        return suffix_path\n\n    # If the specified prefix path is non-empty, we must relativize the suffix path by removing\n    # the leading slash, if present. Otherwise, posixpath.join() would omit the prefix from the\n    # joined path\n    suffix_path = suffix_path.lstrip(posixpath.sep)\n    return posixpath.join(prefix_path, suffix_path)\n\n\ndef is_databricks_acled_artifacts_uri(artifact_uri):\n    _ACLED_ARTIFACT_URI = \"databricks/mlflow-tracking/\"\n    artifact_uri_path = extract_and_normalize_path(artifact_uri)\n    return artifact_uri_path.startswith(_ACLED_ARTIFACT_URI)\n\n\ndef is_databricks_model_registry_artifacts_uri(artifact_uri):\n    _MODEL_REGISTRY_ARTIFACT_URI = \"databricks/mlflow-registry/\"\n    artifact_uri_path = extract_and_normalize_path(artifact_uri)\n    return artifact_uri_path.startswith(_MODEL_REGISTRY_ARTIFACT_URI)\n\n\ndef is_valid_dbfs_uri(uri):\n    parsed = urllib.parse.urlparse(uri)\n    if parsed.scheme != \"dbfs\":\n        return False\n    try:\n        db_profile_uri = get_databricks_profile_uri_from_artifact_uri(uri)\n    except MlflowException:\n        db_profile_uri = None\n    return not parsed.netloc or db_profile_uri is not None\n\n\ndef dbfs_hdfs_uri_to_fuse_path(dbfs_uri):\n    \"\"\"\n    Converts the provided DBFS URI into a DBFS FUSE path\n    :param dbfs_uri: A DBFS URI like \"dbfs:/my-directory\". Can also be a scheme-less URI like\n                     \"/my-directory\" if running in an environment where the default HDFS filesystem\n                     is \"dbfs:/\" (e.g. Databricks)\n    :return A DBFS FUSE-style path, e.g. \"/dbfs/my-directory\"\n    \"\"\"\n    if not is_valid_dbfs_uri(dbfs_uri) and dbfs_uri == posixpath.abspath(dbfs_uri):\n        # Convert posixpaths (e.g. \"/tmp/mlflow\") to DBFS URIs by adding \"dbfs:/\" as a prefix\n        dbfs_uri = \"dbfs:\" + dbfs_uri\n    if not dbfs_uri.startswith(_DBFS_HDFS_URI_PREFIX):\n        raise MlflowException(\n            \"Path '%s' did not start with expected DBFS URI prefix '%s'\"\n            % (dbfs_uri, _DBFS_HDFS_URI_PREFIX),\n        )\n\n    return _DBFS_FUSE_PREFIX + dbfs_uri[len(_DBFS_HDFS_URI_PREFIX) :]\n\n\ndef resolve_uri_if_local(local_uri):\n    \"\"\"\n    if `local_uri` is passed in as a relative local path, this function\n    resolves it to absolute path relative to current working directory.\n\n    :param local_uri: Relative or absolute path or local file uri\n\n    :return: a fully-formed absolute uri path or an absolute filesystem path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    if local_uri is not None and is_local_uri(local_uri):\n        scheme = get_uri_scheme(local_uri)\n        cwd = pathlib.Path.cwd()\n        local_path = local_file_uri_to_path(local_uri)\n        if not pathlib.Path(local_path).is_absolute():\n            if scheme == \"\":\n                if is_windows():\n                    return urllib.parse.urlunsplit(\n                        (\n                            \"file\",\n                            None,\n                            cwd.joinpath(local_path).as_posix(),\n                            None,\n                            None,\n                        )\n                    )\n                return cwd.joinpath(local_path).as_posix()\n            local_uri_split = urllib.parse.urlsplit(local_uri)\n            resolved_absolute_uri = urllib.parse.urlunsplit(\n                (\n                    local_uri_split.scheme,\n                    None,\n                    cwd.joinpath(local_path).as_posix(),\n                    local_uri_split.query,\n                    local_uri_split.fragment,\n                )\n            )\n            return resolved_absolute_uri\n    return local_uri\n\n\ndef generate_tmp_dfs_path(dfs_tmp):\n    return posixpath.join(dfs_tmp, str(uuid.uuid4()))\n", "from subprocess import Popen\n\nimport sys\nimport os\nimport logging\nimport socket\nimport time\n\nimport mlflow\nfrom mlflow.server import BACKEND_STORE_URI_ENV_VAR, ARTIFACT_ROOT_ENV_VAR\nfrom tests.helper_functions import LOCALHOST, get_safe_port\n\n_logger = logging.getLogger(__name__)\n\n\ndef _await_server_up_or_die(port, timeout=10):\n    \"\"\"Waits until the local flask server is listening on the given port.\"\"\"\n    _logger.info(f\"Awaiting server to be up on {LOCALHOST}:{port}\")\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(2)\n            if sock.connect_ex((LOCALHOST, port)) == 0:\n                _logger.info(f\"Server is up on {LOCALHOST}:{port}!\")\n                break\n        _logger.info(\"Server not yet up, waiting...\")\n        time.sleep(0.5)\n    else:\n        raise Exception(f\"Failed to connect on {LOCALHOST}:{port} within {timeout} seconds\")\n\n\n# NB: We explicitly wait and timeout on server shutdown in order to ensure that pytest output\n# reveals the cause in the event of a test hang due to the subprocess not exiting.\ndef _terminate_server(process, timeout=10):\n    \"\"\"Waits until the local flask server process is terminated.\"\"\"\n    _logger.info(\"Terminating server...\")\n    process.terminate()\n    process.wait(timeout=timeout)\n\n\ndef _init_server(backend_uri, root_artifact_uri):\n    \"\"\"\n    Launch a new REST server using the tracking store specified by backend_uri and root artifact\n    directory specified by root_artifact_uri.\n    :returns A tuple (url, process) containing the string URL of the server and a handle to the\n             server process (a multiprocessing.Process object).\n    \"\"\"\n    mlflow.set_tracking_uri(None)\n    server_port = get_safe_port()\n    process = Popen(\n        [\n            sys.executable,\n            \"-c\",\n            f'from mlflow.server import app; app.run(\"{LOCALHOST}\", {server_port})',\n        ],\n        env={\n            **os.environ,\n            BACKEND_STORE_URI_ENV_VAR: backend_uri,\n            ARTIFACT_ROOT_ENV_VAR: root_artifact_uri,\n        },\n    )\n\n    _await_server_up_or_die(server_port)\n    url = f\"http://{LOCALHOST}:{server_port}\"\n    _logger.info(f\"Launching tracking server against backend URI {backend_uri}. Server URL: {url}\")\n    return url, process\n\n\ndef _send_rest_tracking_post_request(tracking_server_uri, api_path, json_payload):\n    \"\"\"\n    Make a POST request to the specified MLflow Tracking API and retrieve the\n    corresponding `requests.Response` object\n    \"\"\"\n    import requests\n\n    url = tracking_server_uri + api_path\n    response = requests.post(url, json=json_payload)\n    return response\n", "\"\"\"\nIntegration test which starts a local Tracking Server on an ephemeral port,\nand ensures we can use the tracking API to communicate with it.\n\"\"\"\nimport pathlib\n\nimport flask\nimport json\nimport os\nimport sys\nimport posixpath\nimport logging\nimport tempfile\nimport time\nimport urllib.parse\nimport requests\nfrom unittest import mock\n\nimport pytest\n\nfrom mlflow import MlflowClient\nfrom mlflow.artifacts import download_artifacts\nimport mlflow.experiments\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.entities import Metric, Param, RunTag, ViewType\nfrom mlflow.models import Model\nimport mlflow.pyfunc\nfrom mlflow.server.handlers import validate_path_is_safe\nfrom mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore\nfrom mlflow.utils.file_utils import TempDir\nfrom mlflow.utils.mlflow_tags import (\n    MLFLOW_USER,\n    MLFLOW_PARENT_RUN_ID,\n    MLFLOW_SOURCE_TYPE,\n    MLFLOW_SOURCE_NAME,\n    MLFLOW_PROJECT_ENTRY_POINT,\n    MLFLOW_GIT_COMMIT,\n)\nfrom mlflow.utils.file_utils import path_to_local_file_uri\nfrom mlflow.utils.time_utils import get_current_time_millis\nfrom mlflow.utils.os import is_windows\n\nfrom tests.integration.utils import invoke_cli_runner\nfrom tests.tracking.integration_test_utils import (\n    _terminate_server,\n    _init_server,\n    _send_rest_tracking_post_request,\n)\n\n\n_logger = logging.getLogger(__name__)\n\n\n@pytest.fixture(params=[\"file\", \"sqlalchemy\"])\ndef mlflow_client(request, tmp_path):\n    \"\"\"Provides an MLflow Tracking API client pointed at the local tracking server.\"\"\"\n    if request.param == \"file\":\n        backend_uri = tmp_path.joinpath(\"file\").as_uri()\n    elif request.param == \"sqlalchemy\":\n        path = tmp_path.joinpath(\"sqlalchemy.db\").as_uri()\n        backend_uri = (\"sqlite://\" if sys.platform == \"win32\" else \"sqlite:////\") + path[\n            len(\"file://\") :\n        ]\n\n    url, process = _init_server(backend_uri, root_artifact_uri=tmp_path.as_uri())\n    yield MlflowClient(url)\n\n    _terminate_server(process)\n\n\n@pytest.fixture()\ndef cli_env(mlflow_client):\n    \"\"\"Provides an environment for the MLflow CLI pointed at the local tracking server.\"\"\"\n    cli_env = {\n        \"LC_ALL\": \"en_US.UTF-8\",\n        \"LANG\": \"en_US.UTF-8\",\n        \"MLFLOW_TRACKING_URI\": mlflow_client.tracking_uri,\n    }\n    return cli_env\n\n\ndef create_experiments(client, names):\n    return [client.create_experiment(n) for n in names]\n\n\ndef test_create_get_search_experiment(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        \"My Experiment\", artifact_location=\"my_location\", tags={\"key1\": \"val1\", \"key2\": \"val2\"}\n    )\n    exp = mlflow_client.get_experiment(experiment_id)\n    assert exp.name == \"My Experiment\"\n    if is_windows():\n        assert exp.artifact_location == pathlib.Path.cwd().joinpath(\"my_location\").as_uri()\n    else:\n        assert exp.artifact_location == str(pathlib.Path.cwd().joinpath(\"my_location\"))\n    assert len(exp.tags) == 2\n    assert exp.tags[\"key1\"] == \"val1\"\n    assert exp.tags[\"key2\"] == \"val2\"\n\n    experiments = mlflow_client.search_experiments()\n    assert {e.name for e in experiments} == {\"My Experiment\", \"Default\"}\n    mlflow_client.delete_experiment(experiment_id)\n    assert {e.name for e in mlflow_client.search_experiments()} == {\"Default\"}\n    assert {e.name for e in mlflow_client.search_experiments(view_type=ViewType.ACTIVE_ONLY)} == {\n        \"Default\"\n    }\n    assert {e.name for e in mlflow_client.search_experiments(view_type=ViewType.DELETED_ONLY)} == {\n        \"My Experiment\"\n    }\n    assert {e.name for e in mlflow_client.search_experiments(view_type=ViewType.ALL)} == {\n        \"My Experiment\",\n        \"Default\",\n    }\n    active_exps_paginated = mlflow_client.search_experiments(max_results=1)\n    assert {e.name for e in active_exps_paginated} == {\"Default\"}\n    assert active_exps_paginated.token is None\n\n    all_exps_paginated = mlflow_client.search_experiments(max_results=1, view_type=ViewType.ALL)\n    first_page_names = {e.name for e in all_exps_paginated}\n    all_exps_second_page = mlflow_client.search_experiments(\n        max_results=1, view_type=ViewType.ALL, page_token=all_exps_paginated.token\n    )\n    second_page_names = {e.name for e in all_exps_second_page}\n    assert len(first_page_names) == 1\n    assert len(second_page_names) == 1\n    assert first_page_names.union(second_page_names) == {\"Default\", \"My Experiment\"}\n\n\ndef test_create_experiment_validation(mlflow_client):\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/experiments/create\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"name\": 123,\n        },\n        \"Invalid value 123 for parameter 'name'\",\n    )\n    assert_bad_request({}, \"Missing value for required parameter 'name'\")\n    assert_bad_request(\n        {\n            \"name\": \"experiment name\",\n            \"artifact_location\": 9.0,\n            \"tags\": [{\"key\": \"key\", \"value\": \"value\"}],\n        },\n        \"Invalid value 9.0 for parameter 'artifact_location'\",\n    )\n    assert_bad_request(\n        {\n            \"name\": \"experiment name\",\n            \"artifact_location\": \"my_location\",\n            \"tags\": \"5\",\n        },\n        \"Invalid value 5 for parameter 'tags'\",\n    )\n\n\ndef test_delete_restore_experiment(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Deleterious\")\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n    mlflow_client.delete_experiment(experiment_id)\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"deleted\"\n    mlflow_client.restore_experiment(experiment_id)\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n\n\ndef test_delete_restore_experiment_cli(mlflow_client, cli_env):\n    experiment_name = \"DeleteriousCLI\"\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"create\", \"--experiment-name\", experiment_name], env=cli_env\n    )\n    experiment_id = mlflow_client.get_experiment_by_name(experiment_name).experiment_id\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"delete\", \"-x\", str(experiment_id)], env=cli_env\n    )\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"deleted\"\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"restore\", \"-x\", str(experiment_id)], env=cli_env\n    )\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n\n\ndef test_rename_experiment(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"BadName\")\n    assert mlflow_client.get_experiment(experiment_id).name == \"BadName\"\n    mlflow_client.rename_experiment(experiment_id, \"GoodName\")\n    assert mlflow_client.get_experiment(experiment_id).name == \"GoodName\"\n\n\ndef test_rename_experiment_cli(mlflow_client, cli_env):\n    bad_experiment_name = \"CLIBadName\"\n    good_experiment_name = \"CLIGoodName\"\n\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"create\", \"-n\", bad_experiment_name], env=cli_env\n    )\n    experiment_id = mlflow_client.get_experiment_by_name(bad_experiment_name).experiment_id\n    assert mlflow_client.get_experiment(experiment_id).name == bad_experiment_name\n    invoke_cli_runner(\n        mlflow.experiments.commands,\n        [\"rename\", \"--experiment-id\", str(experiment_id), \"--new-name\", good_experiment_name],\n        env=cli_env,\n    )\n    assert mlflow_client.get_experiment(experiment_id).name == good_experiment_name\n\n\n@pytest.mark.parametrize(\"parent_run_id_kwarg\", [None, \"my-parent-id\"])\ndef test_create_run_all_args(mlflow_client, parent_run_id_kwarg):\n    user = \"username\"\n    source_name = \"Hello\"\n    entry_point = \"entry\"\n    source_version = \"abc\"\n    create_run_kwargs = {\n        \"start_time\": 456,\n        \"run_name\": \"my name\",\n        \"tags\": {\n            MLFLOW_USER: user,\n            MLFLOW_SOURCE_TYPE: \"LOCAL\",\n            MLFLOW_SOURCE_NAME: source_name,\n            MLFLOW_PROJECT_ENTRY_POINT: entry_point,\n            MLFLOW_GIT_COMMIT: source_version,\n            MLFLOW_PARENT_RUN_ID: \"7\",\n            \"my\": \"tag\",\n            \"other\": \"tag\",\n        },\n    }\n    experiment_id = mlflow_client.create_experiment(\n        \"Run A Lot (parent_run_id=%s)\" % (parent_run_id_kwarg)\n    )\n    created_run = mlflow_client.create_run(experiment_id, **create_run_kwargs)\n    run_id = created_run.info.run_id\n    _logger.info(f\"Run id={run_id}\")\n    fetched_run = mlflow_client.get_run(run_id)\n    for run in [created_run, fetched_run]:\n        assert run.info.run_id == run_id\n        assert run.info.run_uuid == run_id\n        assert run.info.experiment_id == experiment_id\n        assert run.info.user_id == user\n        assert run.info.start_time == create_run_kwargs[\"start_time\"]\n        assert run.info.run_name == \"my name\"\n        for tag in create_run_kwargs[\"tags\"]:\n            assert tag in run.data.tags\n        assert run.data.tags.get(MLFLOW_USER) == user\n        assert run.data.tags.get(MLFLOW_PARENT_RUN_ID) == parent_run_id_kwarg or \"7\"\n        assert [run.info for run in mlflow_client.search_runs([experiment_id])] == [run.info]\n\n\ndef test_create_run_defaults(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Run A Little\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    run = mlflow_client.get_run(run_id)\n    assert run.info.run_id == run_id\n    assert run.info.experiment_id == experiment_id\n    assert run.info.user_id == \"unknown\"\n\n\ndef test_log_metrics_params_tags(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Oh My\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    mlflow_client.log_metric(run_id, key=\"metric\", value=123.456, timestamp=789, step=2)\n    mlflow_client.log_metric(run_id, key=\"nan_metric\", value=float(\"nan\"))\n    mlflow_client.log_metric(run_id, key=\"inf_metric\", value=float(\"inf\"))\n    mlflow_client.log_metric(run_id, key=\"-inf_metric\", value=-float(\"inf\"))\n    mlflow_client.log_metric(run_id, key=\"stepless-metric\", value=987.654, timestamp=321)\n    mlflow_client.log_param(run_id, \"param\", \"value\")\n    mlflow_client.set_tag(run_id, \"taggity\", \"do-dah\")\n    run = mlflow_client.get_run(run_id)\n    assert run.data.metrics.get(\"metric\") == 123.456\n    import math\n\n    assert math.isnan(run.data.metrics.get(\"nan_metric\"))\n    assert run.data.metrics.get(\"inf_metric\") >= 1.7976931348623157e308\n    assert run.data.metrics.get(\"-inf_metric\") <= -1.7976931348623157e308\n    assert run.data.metrics.get(\"stepless-metric\") == 987.654\n    assert run.data.params.get(\"param\") == \"value\"\n    assert run.data.tags.get(\"taggity\") == \"do-dah\"\n    metric_history0 = mlflow_client.get_metric_history(run_id, \"metric\")\n    assert len(metric_history0) == 1\n    metric0 = metric_history0[0]\n    assert metric0.key == \"metric\"\n    assert metric0.value == 123.456\n    assert metric0.timestamp == 789\n    assert metric0.step == 2\n    metric_history1 = mlflow_client.get_metric_history(run_id, \"stepless-metric\")\n    assert len(metric_history1) == 1\n    metric1 = metric_history1[0]\n    assert metric1.key == \"stepless-metric\"\n    assert metric1.value == 987.654\n    assert metric1.timestamp == 321\n    assert metric1.step == 0\n\n    metric_history = mlflow_client.get_metric_history(run_id, \"a_test_accuracy\")\n    assert metric_history == []\n\n\ndef test_log_metric_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"metrics validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/log-metric\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"run_id\": 31,\n            \"key\": \"metric\",\n            \"value\": 41,\n            \"timestamp\": 59,\n            \"step\": 26,\n        },\n        \"Invalid value 31 for parameter 'run_id' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": 31,\n            \"value\": 41,\n            \"timestamp\": 59,\n            \"step\": 26,\n        },\n        \"Invalid value 31 for parameter 'key' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": \"foo\",\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": \"foo\",\n        },\n        \"Invalid value foo for parameter 'step' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": \"foo\",\n            \"value\": 31,\n            \"timestamp\": \"foo\",\n            \"step\": 41,\n        },\n        \"Invalid value foo for parameter 'timestamp' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": None,\n            \"key\": \"foo\",\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": 41,\n        },\n        \"Missing value for required parameter 'run_id'\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            # Missing key\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": 41,\n        },\n        \"Missing value for required parameter 'key'\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": None,\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": 41,\n        },\n        \"Missing value for required parameter 'key'\",\n    )\n\n\ndef test_log_param_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"params validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/log-parameter\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"run_id\": 31,\n            \"key\": \"param\",\n            \"value\": 41,\n        },\n        \"Invalid value 31 for parameter 'run_id' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": 31,\n            \"value\": 41,\n        },\n        \"Invalid value 31 for parameter 'key' supplied\",\n    )\n\n\ndef test_log_param_with_empty_string_as_value(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_log_param_with_empty_string_as_value.__name__\n    )\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    mlflow_client.log_param(run_id, \"param_key\", \"\")\n    assert {\"param_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.params.items()\n\n\ndef test_set_tag_with_empty_string_as_value(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_set_tag_with_empty_string_as_value.__name__\n    )\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    mlflow_client.set_tag(run_id, \"tag_key\", \"\")\n    assert {\"tag_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.tags.items()\n\n\ndef test_log_batch_containing_params_and_tags_with_empty_string_values(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_log_batch_containing_params_and_tags_with_empty_string_values.__name__\n    )\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    mlflow_client.log_batch(\n        run_id=run_id,\n        params=[Param(\"param_key\", \"\")],\n        tags=[RunTag(\"tag_key\", \"\")],\n    )\n    assert {\"param_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.params.items()\n    assert {\"tag_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.tags.items()\n\n\ndef test_set_tag_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"tags validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/set-tag\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"run_id\": 31,\n            \"key\": \"tag\",\n            \"value\": 41,\n        },\n        \"Invalid value 31 for parameter 'run_id' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": \"param\",\n            \"value\": 41,\n        },\n        \"Invalid value 41 for parameter 'value' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            # Missing key\n            \"value\": \"value\",\n        },\n        \"Missing value for required parameter 'key'\",\n    )\n\n    response = _send_rest_tracking_post_request(\n        mlflow_client.tracking_uri,\n        \"/api/2.0/mlflow/runs/set-tag\",\n        {\n            \"run_uuid\": run_id,\n            \"key\": \"key\",\n            \"value\": \"value\",\n        },\n    )\n    assert response.status_code == 200\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"path\",\n        \"path/\",\n        \"path/to/file\",\n    ],\n)\ndef test_validate_path_is_safe_good(path):\n    validate_path_is_safe(path)\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"/path\",\n        \"../path\",\n        \"../../path\",\n        \"./../path\",\n        \"path/../to/file\",\n        \"path/../../to/file\",\n    ],\n)\ndef test_validate_path_is_safe_bad(path):\n    with pytest.raises(MlflowException, match=\"Invalid path\"):\n        validate_path_is_safe(path)\n\n\ndef test_path_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"tags validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    invalid_path = \"../path\"\n\n    def assert_response(resp):\n        assert resp.status_code == 400\n        assert response.json() == {\n            \"error_code\": \"INVALID_PARAMETER_VALUE\",\n            \"message\": f\"Invalid path: {invalid_path}\",\n        }\n\n    response = requests.get(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/artifacts/list\",\n        params={\"run_id\": run_id, \"path\": invalid_path},\n    )\n    assert_response(response)\n\n    response = requests.get(\n        f\"{mlflow_client.tracking_uri}/get-artifact\",\n        params={\"run_id\": run_id, \"path\": invalid_path},\n    )\n    assert_response(response)\n\n    response = requests.get(\n        f\"{mlflow_client.tracking_uri}//model-versions/get-artifact\",\n        params={\"name\": \"model\", \"version\": 1, \"path\": invalid_path},\n    )\n    assert_response(response)\n\n\ndef test_set_experiment_tag(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"SetExperimentTagTest\")\n    mlflow_client.set_experiment_tag(experiment_id, \"dataset\", \"imagenet1K\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    assert \"dataset\" in experiment.tags and experiment.tags[\"dataset\"] == \"imagenet1K\"\n    # test that updating a tag works\n    mlflow_client.set_experiment_tag(experiment_id, \"dataset\", \"birdbike\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    assert \"dataset\" in experiment.tags and experiment.tags[\"dataset\"] == \"birdbike\"\n    # test that setting a tag on 1 experiment does not impact another experiment.\n    experiment_id_2 = mlflow_client.create_experiment(\"SetExperimentTagTest2\")\n    experiment2 = mlflow_client.get_experiment(experiment_id_2)\n    assert len(experiment2.tags) == 0\n    # test that setting a tag on different experiments maintain different values across experiments\n    mlflow_client.set_experiment_tag(experiment_id_2, \"dataset\", \"birds200\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    experiment2 = mlflow_client.get_experiment(experiment_id_2)\n    assert \"dataset\" in experiment.tags and experiment.tags[\"dataset\"] == \"birdbike\"\n    assert \"dataset\" in experiment2.tags and experiment2.tags[\"dataset\"] == \"birds200\"\n    # test can set multi-line tags\n    mlflow_client.set_experiment_tag(experiment_id, \"multiline tag\", \"value2\\nvalue2\\nvalue2\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    assert (\n        \"multiline tag\" in experiment.tags\n        and experiment.tags[\"multiline tag\"] == \"value2\\nvalue2\\nvalue2\"\n    )\n\n\ndef test_set_experiment_tag_with_empty_string_as_value(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_set_experiment_tag_with_empty_string_as_value.__name__\n    )\n    mlflow_client.set_experiment_tag(experiment_id, \"tag_key\", \"\")\n    assert {\"tag_key\": \"\"}.items() <= mlflow_client.get_experiment(experiment_id).tags.items()\n\n\ndef test_delete_tag(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"DeleteTagExperiment\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    mlflow_client.log_metric(run_id, key=\"metric\", value=123.456, timestamp=789, step=2)\n    mlflow_client.log_metric(run_id, key=\"stepless-metric\", value=987.654, timestamp=321)\n    mlflow_client.log_param(run_id, \"param\", \"value\")\n    mlflow_client.set_tag(run_id, \"taggity\", \"do-dah\")\n    run = mlflow_client.get_run(run_id)\n    assert \"taggity\" in run.data.tags and run.data.tags[\"taggity\"] == \"do-dah\"\n    mlflow_client.delete_tag(run_id, \"taggity\")\n    run = mlflow_client.get_run(run_id)\n    assert \"taggity\" not in run.data.tags\n    with pytest.raises(MlflowException, match=r\"Run .+ not found\"):\n        mlflow_client.delete_tag(\"fake_run_id\", \"taggity\")\n    with pytest.raises(MlflowException, match=\"No tag with name: fakeTag\"):\n        mlflow_client.delete_tag(run_id, \"fakeTag\")\n    mlflow_client.delete_run(run_id)\n    with pytest.raises(MlflowException, match=f\"The run {run_id} must be in\"):\n        mlflow_client.delete_tag(run_id, \"taggity\")\n\n\ndef test_log_batch(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Batch em up\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    mlflow_client.log_batch(\n        run_id=run_id,\n        metrics=[Metric(\"metric\", 123.456, 789, 3)],\n        params=[Param(\"param\", \"value\")],\n        tags=[RunTag(\"taggity\", \"do-dah\")],\n    )\n    run = mlflow_client.get_run(run_id)\n    assert run.data.metrics.get(\"metric\") == 123.456\n    assert run.data.params.get(\"param\") == \"value\"\n    assert run.data.tags.get(\"taggity\") == \"do-dah\"\n    metric_history = mlflow_client.get_metric_history(run_id, \"metric\")\n    assert len(metric_history) == 1\n    metric = metric_history[0]\n    assert metric.key == \"metric\"\n    assert metric.value == 123.456\n    assert metric.timestamp == 789\n    assert metric.step == 3\n\n\ndef test_log_batch_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"log_batch validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/log-batch\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    for request_parameter in [\"metrics\", \"params\", \"tags\"]:\n        assert_bad_request(\n            {\n                \"run_id\": run_id,\n                request_parameter: \"foo\",\n            },\n            f\"Invalid value foo for parameter '{request_parameter}' supplied\",\n        )\n\n\n@pytest.mark.allow_infer_pip_requirements_fallback\ndef test_log_model(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Log models\")\n    with TempDir(chdr=True):\n        model_paths = [f\"model/path/{i}\" for i in range(3)]\n        mlflow.set_tracking_uri(mlflow_client.tracking_uri)\n        with mlflow.start_run(experiment_id=experiment_id) as run:\n            for i, m in enumerate(model_paths):\n                mlflow.pyfunc.log_model(m, loader_module=\"mlflow.pyfunc\")\n                mlflow.pyfunc.save_model(\n                    m,\n                    mlflow_model=Model(artifact_path=m, run_id=run.info.run_id),\n                    loader_module=\"mlflow.pyfunc\",\n                )\n                model = Model.load(os.path.join(m, \"MLmodel\"))\n                run = mlflow.get_run(run.info.run_id)\n                tag = run.data.tags[\"mlflow.log-model.history\"]\n                models = json.loads(tag)\n                model.utc_time_created = models[i][\"utc_time_created\"]\n\n                history_model_meta = models[i].copy()\n                original_model_uuid = history_model_meta.pop(\"model_uuid\")\n                model_meta = model.to_dict().copy()\n                new_model_uuid = model_meta.pop(\"model_uuid\")\n                assert history_model_meta == model_meta\n                assert original_model_uuid != new_model_uuid\n                assert len(models) == i + 1\n                for j in range(0, i + 1):\n                    assert models[j][\"artifact_path\"] == model_paths[j]\n\n\ndef test_set_terminated_defaults(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Terminator 1\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    assert mlflow_client.get_run(run_id).info.status == \"RUNNING\"\n    assert mlflow_client.get_run(run_id).info.end_time is None\n    mlflow_client.set_terminated(run_id)\n    assert mlflow_client.get_run(run_id).info.status == \"FINISHED\"\n    assert mlflow_client.get_run(run_id).info.end_time <= get_current_time_millis()\n\n\ndef test_set_terminated_status(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Terminator 2\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    assert mlflow_client.get_run(run_id).info.status == \"RUNNING\"\n    assert mlflow_client.get_run(run_id).info.end_time is None\n    mlflow_client.set_terminated(run_id, \"FAILED\")\n    assert mlflow_client.get_run(run_id).info.status == \"FAILED\"\n    assert mlflow_client.get_run(run_id).info.end_time <= get_current_time_millis()\n\n\ndef test_artifacts(mlflow_client, tmp_path):\n    experiment_id = mlflow_client.create_experiment(\"Art In Fact\")\n    experiment_info = mlflow_client.get_experiment(experiment_id)\n    assert experiment_info.artifact_location.startswith(path_to_local_file_uri(str(tmp_path)))\n    artifact_path = urllib.parse.urlparse(experiment_info.artifact_location).path\n    assert posixpath.split(artifact_path)[-1] == experiment_id\n\n    created_run = mlflow_client.create_run(experiment_id)\n    assert created_run.info.artifact_uri.startswith(experiment_info.artifact_location)\n    run_id = created_run.info.run_id\n    src_dir = tempfile.mkdtemp(\"test_artifacts_src\")\n    src_file = os.path.join(src_dir, \"my.file\")\n    with open(src_file, \"w\") as f:\n        f.write(\"Hello, World!\")\n    mlflow_client.log_artifact(run_id, src_file, None)\n    mlflow_client.log_artifacts(run_id, src_dir, \"dir\")\n\n    root_artifacts_list = mlflow_client.list_artifacts(run_id)\n    assert {a.path for a in root_artifacts_list} == {\"my.file\", \"dir\"}\n\n    dir_artifacts_list = mlflow_client.list_artifacts(run_id, \"dir\")\n    assert {a.path for a in dir_artifacts_list} == {\"dir/my.file\"}\n\n    all_artifacts = download_artifacts(\n        run_id=run_id, artifact_path=\".\", tracking_uri=mlflow_client.tracking_uri\n    )\n    assert open(\"%s/my.file\" % all_artifacts).read() == \"Hello, World!\"\n    assert open(\"%s/dir/my.file\" % all_artifacts).read() == \"Hello, World!\"\n\n    dir_artifacts = download_artifacts(\n        run_id=run_id, artifact_path=\"dir\", tracking_uri=mlflow_client.tracking_uri\n    )\n    assert open(\"%s/my.file\" % dir_artifacts).read() == \"Hello, World!\"\n\n\ndef test_search_pagination(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"search_pagination\")\n    runs = [mlflow_client.create_run(experiment_id, start_time=1).info.run_id for _ in range(0, 10)]\n    runs = sorted(runs)\n    result = mlflow_client.search_runs([experiment_id], max_results=4, page_token=None)\n    assert [r.info.run_id for r in result] == runs[0:4]\n    assert result.token is not None\n    result = mlflow_client.search_runs([experiment_id], max_results=4, page_token=result.token)\n    assert [r.info.run_id for r in result] == runs[4:8]\n    assert result.token is not None\n    result = mlflow_client.search_runs([experiment_id], max_results=4, page_token=result.token)\n    assert [r.info.run_id for r in result] == runs[8:]\n    assert result.token is None\n\n\ndef test_search_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"search_validation\")\n    with pytest.raises(\n        MlflowException, match=r\"Invalid value 123456789 for parameter 'max_results' supplied\"\n    ):\n        mlflow_client.search_runs([experiment_id], max_results=123456789)\n\n\ndef test_get_experiment_by_name(mlflow_client):\n    name = \"test_get_experiment_by_name\"\n    experiment_id = mlflow_client.create_experiment(name)\n    res = mlflow_client.get_experiment_by_name(name)\n    assert res.experiment_id == experiment_id\n    assert res.name == name\n    assert mlflow_client.get_experiment_by_name(\"idontexist\") is None\n\n\ndef test_get_experiment(mlflow_client):\n    name = \"test_get_experiment\"\n    experiment_id = mlflow_client.create_experiment(name)\n    res = mlflow_client.get_experiment(experiment_id)\n    assert res.experiment_id == experiment_id\n    assert res.name == name\n\n\ndef test_search_experiments(mlflow_client):\n    # To ensure the default experiment and non-default experiments have different creation_time\n    # for deterministic search results, send a request to the server and initialize the tracking\n    # store.\n    assert mlflow_client.search_experiments()[0].name == \"Default\"\n\n    experiments = [\n        (\"a\", {\"key\": \"value\"}),\n        (\"ab\", {\"key\": \"vaLue\"}),\n        (\"Abc\", None),\n    ]\n    experiment_ids = []\n    for name, tags in experiments:\n        # sleep for windows file system current_time precision in Python to enforce\n        # deterministic ordering based on last_update_time (creation_time due to no\n        # mutation of experiment state)\n        time.sleep(0.001)\n        experiment_ids.append(mlflow_client.create_experiment(name, tags=tags))\n\n    # filter_string\n    experiments = mlflow_client.search_experiments(filter_string=\"attribute.name = 'a'\")\n    assert [e.name for e in experiments] == [\"a\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"attribute.name != 'a'\")\n    assert [e.name for e in experiments] == [\"Abc\", \"ab\", \"Default\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"name LIKE 'a%'\")\n    assert [e.name for e in experiments] == [\"ab\", \"a\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"tag.key = 'value'\")\n    assert [e.name for e in experiments] == [\"a\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"tag.key != 'value'\")\n    assert [e.name for e in experiments] == [\"ab\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"tag.key ILIKE '%alu%'\")\n    assert [e.name for e in experiments] == [\"ab\", \"a\"]\n\n    # order_by\n    experiments = mlflow_client.search_experiments(order_by=[\"name DESC\"])\n    assert [e.name for e in experiments] == [\"ab\", \"a\", \"Default\", \"Abc\"]\n\n    # max_results\n    experiments = mlflow_client.search_experiments(max_results=2)\n    assert [e.name for e in experiments] == [\"Abc\", \"ab\"]\n    # page_token\n    experiments = mlflow_client.search_experiments(page_token=experiments.token)\n    assert [e.name for e in experiments] == [\"a\", \"Default\"]\n\n    # view_type\n    time.sleep(0.001)\n    mlflow_client.delete_experiment(experiment_ids[1])\n    experiments = mlflow_client.search_experiments(view_type=ViewType.ACTIVE_ONLY)\n    assert [e.name for e in experiments] == [\"Abc\", \"a\", \"Default\"]\n    experiments = mlflow_client.search_experiments(view_type=ViewType.DELETED_ONLY)\n    assert [e.name for e in experiments] == [\"ab\"]\n    experiments = mlflow_client.search_experiments(view_type=ViewType.ALL)\n    assert [e.name for e in experiments] == [\"Abc\", \"ab\", \"a\", \"Default\"]\n\n\ndef test_get_metric_history_bulk_rejects_invalid_requests(mlflow_client):\n    def assert_response(resp, message_part):\n        assert resp.status_code == 400\n        response_json = resp.json()\n        assert response_json.get(\"error_code\") == \"INVALID_PARAMETER_VALUE\"\n        assert message_part in response_json.get(\"message\", \"\")\n\n    response_no_run_ids_field = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"metric_key\": \"key\"},\n    )\n    assert_response(\n        response_no_run_ids_field,\n        \"GetMetricHistoryBulk request must specify at least one run_id\",\n    )\n\n    response_empty_run_ids = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [], \"metric_key\": \"key\"},\n    )\n    assert_response(\n        response_empty_run_ids,\n        \"GetMetricHistoryBulk request must specify at least one run_id\",\n    )\n\n    response_too_many_run_ids = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [f\"id_{i}\" for i in range(1000)], \"metric_key\": \"key\"},\n    )\n    assert_response(\n        response_too_many_run_ids,\n        \"GetMetricHistoryBulk request cannot specify more than\",\n    )\n\n    response_no_metric_key_field = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [\"123\"]},\n    )\n    assert_response(\n        response_no_metric_key_field,\n        \"GetMetricHistoryBulk request must specify a metric_key\",\n    )\n\n\ndef test_get_metric_history_bulk_returns_expected_metrics_in_expected_order(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"get metric history bulk\")\n    created_run1 = mlflow_client.create_run(experiment_id)\n    run_id1 = created_run1.info.run_id\n    created_run2 = mlflow_client.create_run(experiment_id)\n    run_id2 = created_run2.info.run_id\n    created_run3 = mlflow_client.create_run(experiment_id)\n    run_id3 = created_run3.info.run_id\n\n    metricA_history = [\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 2, \"value\": 10.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 11.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 12.0},\n        {\"key\": \"metricA\", \"timestamp\": 2, \"step\": 3, \"value\": 12.0},\n    ]\n    for metric in metricA_history:\n        mlflow_client.log_metric(run_id1, **metric)\n        metric_for_run2 = dict(metric)\n        metric_for_run2[\"value\"] += 1.0\n        mlflow_client.log_metric(run_id2, **metric_for_run2)\n\n    metricB_history = [\n        {\"key\": \"metricB\", \"timestamp\": 7, \"step\": -2, \"value\": -100.0},\n        {\"key\": \"metricB\", \"timestamp\": 8, \"step\": 0, \"value\": 0.0},\n        {\"key\": \"metricB\", \"timestamp\": 8, \"step\": 0, \"value\": 1.0},\n        {\"key\": \"metricB\", \"timestamp\": 9, \"step\": 1, \"value\": 12.0},\n    ]\n    for metric in metricB_history:\n        mlflow_client.log_metric(run_id1, **metric)\n        metric_for_run2 = dict(metric)\n        metric_for_run2[\"value\"] += 1.0\n        mlflow_client.log_metric(run_id2, **metric_for_run2)\n\n    response_run1_metricA = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id1], \"metric_key\": \"metricA\"},\n    )\n    assert response_run1_metricA.status_code == 200\n    assert response_run1_metricA.json().get(\"metrics\") == [\n        {**metric, \"run_id\": run_id1} for metric in metricA_history\n    ]\n\n    response_run2_metricB = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id2], \"metric_key\": \"metricB\"},\n    )\n    assert response_run2_metricB.status_code == 200\n    assert response_run2_metricB.json().get(\"metrics\") == [\n        {**metric, \"run_id\": run_id2, \"value\": metric[\"value\"] + 1.0} for metric in metricB_history\n    ]\n\n    response_run1_run2_metricA = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id1, run_id2], \"metric_key\": \"metricA\"},\n    )\n    assert response_run1_run2_metricA.status_code == 200\n    assert response_run1_run2_metricA.json().get(\"metrics\") == sorted(\n        [{**metric, \"run_id\": run_id1} for metric in metricA_history]\n        + [\n            {**metric, \"run_id\": run_id2, \"value\": metric[\"value\"] + 1.0}\n            for metric in metricA_history\n        ],\n        key=lambda metric: metric[\"run_id\"],\n    )\n\n    response_run1_run2_run_3_metricB = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id1, run_id2, run_id3], \"metric_key\": \"metricB\"},\n    )\n    assert response_run1_run2_run_3_metricB.status_code == 200\n    assert response_run1_run2_run_3_metricB.json().get(\"metrics\") == sorted(\n        [{**metric, \"run_id\": run_id1} for metric in metricB_history]\n        + [\n            {**metric, \"run_id\": run_id2, \"value\": metric[\"value\"] + 1.0}\n            for metric in metricB_history\n        ],\n        key=lambda metric: metric[\"run_id\"],\n    )\n\n\ndef test_get_metric_history_bulk_respects_max_results(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"get metric history bulk\")\n    run_id = mlflow_client.create_run(experiment_id).info.run_id\n    max_results = 2\n\n    metricA_history = [\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 2, \"value\": 10.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 11.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 12.0},\n        {\"key\": \"metricA\", \"timestamp\": 2, \"step\": 3, \"value\": 12.0},\n    ]\n    for metric in metricA_history:\n        mlflow_client.log_metric(run_id, **metric)\n\n    response_limited = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id], \"metric_key\": \"metricA\", \"max_results\": max_results},\n    )\n    assert response_limited.status_code == 200\n    assert response_limited.json().get(\"metrics\") == [\n        {**metric, \"run_id\": run_id} for metric in metricA_history[:max_results]\n    ]\n\n\ndef test_get_metric_history_bulk_calls_optimized_impl_when_expected(monkeypatch, tmp_path):\n    from mlflow.server.handlers import get_metric_history_bulk_handler\n\n    path = path_to_local_file_uri(str(tmp_path.joinpath(\"sqlalchemy.db\")))\n    uri = (\"sqlite://\" if sys.platform == \"win32\" else \"sqlite:////\") + path[len(\"file://\") :]\n    mock_store = mock.Mock(wraps=SqlAlchemyStore(uri, str(tmp_path)))\n\n    flask_app = flask.Flask(\"test_flask_app\")\n\n    class MockRequestArgs:\n        def __init__(self, args_dict):\n            self.args_dict = args_dict\n\n        def to_dict(self, flat):\n            return self.args_dict\n\n        def get(self, key, default=None):\n            return self.args_dict.get(key, default)\n\n    with mock.patch(\n        \"mlflow.server.handlers._get_tracking_store\", return_value=mock_store\n    ), flask_app.test_request_context() as mock_context:\n        run_ids = [str(i) for i in range(10)]\n        mock_context.request.args = MockRequestArgs(\n            {\n                \"run_id\": run_ids,\n                \"metric_key\": \"mock_key\",\n            }\n        )\n\n        get_metric_history_bulk_handler()\n\n        mock_store.get_metric_history_bulk.assert_called_once_with(\n            run_ids=run_ids,\n            metric_key=\"mock_key\",\n            max_results=25000,\n        )\n\n\ndef test_create_model_version_with_local_source(mlflow_client):\n    name = \"mode\"\n    mlflow_client.create_registered_model(name)\n    exp_id = mlflow_client.create_experiment(\"test\")\n    run = mlflow_client.create_run(experiment_id=exp_id)\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri,\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": f\"{run.info.artifact_uri}/model\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": f\"{run.info.artifact_uri}/.\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": f\"{run.info.artifact_uri}/model/..\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri[len(\"file://\") :],\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri,\n        },\n    )\n    assert response.status_code == 400\n    resp = response.json()\n    assert \"Invalid source\" in resp[\"message\"]\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": \"/tmp\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 400\n    resp = response.json()\n    assert \"Invalid source\" in resp[\"message\"]\n\n\ndef test_logging_model_with_local_artifact_uri(mlflow_client):\n    from sklearn.linear_model import LogisticRegression\n\n    mlflow.set_tracking_uri(mlflow_client.tracking_uri)\n    with mlflow.start_run() as run:\n        assert run.info.artifact_uri.startswith(\"file://\")\n        mlflow.sklearn.log_model(LogisticRegression(), \"model\", registered_model_name=\"rmn\")\n        mlflow.pyfunc.load_model(\"models:/rmn/1\")\n", "import pathlib\nimport posixpath\nimport pytest\n\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.store.db.db_types import DATABASE_ENGINES\nfrom mlflow.utils.uri import (\n    add_databricks_profile_info_to_artifact_uri,\n    append_to_uri_path,\n    extract_and_normalize_path,\n    extract_db_type_from_uri,\n    get_databricks_profile_uri_from_artifact_uri,\n    get_db_info_from_uri,\n    get_uri_scheme,\n    is_databricks_acled_artifacts_uri,\n    is_databricks_uri,\n    is_http_uri,\n    is_local_uri,\n    is_valid_dbfs_uri,\n    remove_databricks_profile_info_from_artifact_uri,\n    dbfs_hdfs_uri_to_fuse_path,\n    resolve_uri_if_local,\n)\nfrom mlflow.utils.os import is_windows\n\n\ndef test_extract_db_type_from_uri():\n    uri = \"{}://username:password@host:port/database\"\n    for legit_db in DATABASE_ENGINES:\n        assert legit_db == extract_db_type_from_uri(uri.format(legit_db))\n        assert legit_db == get_uri_scheme(uri.format(legit_db))\n\n        with_driver = legit_db + \"+driver-string\"\n        assert legit_db == extract_db_type_from_uri(uri.format(with_driver))\n        assert legit_db == get_uri_scheme(uri.format(with_driver))\n\n    for unsupported_db in [\"a\", \"aa\", \"sql\"]:\n        with pytest.raises(MlflowException, match=\"Invalid database engine\"):\n            extract_db_type_from_uri(unsupported_db)\n\n\n@pytest.mark.parametrize(\n    (\"server_uri\", \"result\"),\n    [\n        (\"databricks://aAbB\", (\"aAbB\", None)),\n        (\"databricks://aAbB/\", (\"aAbB\", None)),\n        (\"databricks://aAbB/path\", (\"aAbB\", None)),\n        (\"databricks://profile:prefix\", (\"profile\", \"prefix\")),\n        (\"databricks://profile:prefix/extra\", (\"profile\", \"prefix\")),\n        (\"nondatabricks://profile:prefix\", (None, None)),\n        (\"databricks://profile\", (\"profile\", None)),\n        (\"databricks://profile/\", (\"profile\", None)),\n        (\"databricks-uc://profile:prefix\", (\"profile\", \"prefix\")),\n        (\"databricks-uc://profile:prefix/extra\", (\"profile\", \"prefix\")),\n        (\"databricks-uc://profile\", (\"profile\", None)),\n        (\"databricks-uc://profile/\", (\"profile\", None)),\n    ],\n)\ndef test_get_db_info_from_uri(server_uri, result):\n    assert get_db_info_from_uri(server_uri) == result\n\n\n@pytest.mark.parametrize(\n    \"server_uri\",\n    [\"databricks:/profile:prefix\", \"databricks:/\", \"databricks://\"],\n)\ndef test_get_db_info_from_uri_errors_no_netloc(server_uri):\n    with pytest.raises(MlflowException, match=\"URI is formatted incorrectly\"):\n        get_db_info_from_uri(server_uri)\n\n\n@pytest.mark.parametrize(\n    \"server_uri\",\n    [\n        \"databricks://profile:prefix:extra\",\n        \"databricks://profile:prefix:extra  \",\n        \"databricks://profile:prefix extra\",\n        \"databricks://profile:prefix  \",\n        \"databricks://profile \",\n        \"databricks://profile:\",\n        \"databricks://profile: \",\n    ],\n)\ndef test_get_db_info_from_uri_errors_invalid_profile(server_uri):\n    with pytest.raises(MlflowException, match=\"Unsupported Databricks profile\"):\n        get_db_info_from_uri(server_uri)\n\n\ndef test_is_local_uri():\n    assert is_local_uri(\"mlruns\")\n    assert is_local_uri(\"./mlruns\")\n    assert is_local_uri(\"file:///foo/mlruns\")\n    assert is_local_uri(\"file:foo/mlruns\")\n\n    assert not is_local_uri(\"file://myhostname/path/to/file\")\n    assert not is_local_uri(\"https://whatever\")\n    assert not is_local_uri(\"http://whatever\")\n    assert not is_local_uri(\"databricks\")\n    assert not is_local_uri(\"databricks:whatever\")\n    assert not is_local_uri(\"databricks://whatever\")\n\n\n@pytest.mark.skipif(not is_windows(), reason=\"Windows-only test\")\ndef test_is_local_uri_windows():\n    assert is_local_uri(\"C:\\\\foo\\\\mlruns\")\n    assert is_local_uri(\"C:/foo/mlruns\")\n    assert is_local_uri(\"file:///C:\\\\foo\\\\mlruns\")\n    assert not is_local_uri(\"\\\\\\\\server\\\\aa\\\\bb\")\n\n\ndef test_is_databricks_uri():\n    assert is_databricks_uri(\"databricks\")\n    assert is_databricks_uri(\"databricks:whatever\")\n    assert is_databricks_uri(\"databricks://whatever\")\n    assert not is_databricks_uri(\"mlruns\")\n    assert not is_databricks_uri(\"http://whatever\")\n\n\ndef test_is_http_uri():\n    assert is_http_uri(\"http://whatever\")\n    assert is_http_uri(\"https://whatever\")\n    assert not is_http_uri(\"file://whatever\")\n    assert not is_http_uri(\"databricks://whatever\")\n    assert not is_http_uri(\"mlruns\")\n\n\ndef validate_append_to_uri_path_test_cases(cases):\n    for input_uri, input_path, expected_output_uri in cases:\n        assert append_to_uri_path(input_uri, input_path) == expected_output_uri\n        assert append_to_uri_path(input_uri, *posixpath.split(input_path)) == expected_output_uri\n\n\ndef test_append_to_uri_path_joins_uri_paths_and_posixpaths_correctly():\n    validate_append_to_uri_path_test_cases(\n        [\n            (\"\", \"path\", \"path\"),\n            (\"\", \"/path\", \"/path\"),\n            (\"path\", \"\", \"path/\"),\n            (\"path\", \"subpath\", \"path/subpath\"),\n            (\"path/\", \"subpath\", \"path/subpath\"),\n            (\"path/\", \"/subpath\", \"path/subpath\"),\n            (\"path\", \"/subpath\", \"path/subpath\"),\n            (\"/path\", \"/subpath\", \"/path/subpath\"),\n            (\"//path\", \"/subpath\", \"//path/subpath\"),\n            (\"///path\", \"/subpath\", \"///path/subpath\"),\n            (\"/path\", \"/subpath/subdir\", \"/path/subpath/subdir\"),\n            (\"file:path\", \"\", \"file:path/\"),\n            (\"file:path/\", \"\", \"file:path/\"),\n            (\"file:path\", \"subpath\", \"file:path/subpath\"),\n            (\"file:path\", \"/subpath\", \"file:path/subpath\"),\n            (\"file:/\", \"\", \"file:///\"),\n            (\"file:/path\", \"/subpath\", \"file:///path/subpath\"),\n            (\"file:///\", \"\", \"file:///\"),\n            (\"file:///\", \"subpath\", \"file:///subpath\"),\n            (\"file:///path\", \"/subpath\", \"file:///path/subpath\"),\n            (\"file:///path/\", \"subpath\", \"file:///path/subpath\"),\n            (\"file:///path\", \"subpath\", \"file:///path/subpath\"),\n            (\"s3://\", \"\", \"s3:\"),\n            (\"s3://\", \"subpath\", \"s3:subpath\"),\n            (\"s3://\", \"/subpath\", \"s3:/subpath\"),\n            (\"s3://host\", \"subpath\", \"s3://host/subpath\"),\n            (\"s3://host\", \"/subpath\", \"s3://host/subpath\"),\n            (\"s3://host/\", \"subpath\", \"s3://host/subpath\"),\n            (\"s3://host/\", \"/subpath\", \"s3://host/subpath\"),\n            (\"s3://host\", \"subpath/subdir\", \"s3://host/subpath/subdir\"),\n        ]\n    )\n\n\ndef test_append_to_uri_path_handles_special_uri_characters_in_posixpaths():\n    \"\"\"\n    Certain characters are treated specially when parsing and interpreting URIs. However, in the\n    case where a URI input for `append_to_uri_path` is simply a POSIX path, these characters should\n    not receive special treatment. This test case verifies that `append_to_uri_path` properly joins\n    POSIX paths containing these characters.\n    \"\"\"\n\n    def create_char_case(special_char):\n        def char_case(*case_args):\n            return tuple([item.format(c=special_char) for item in case_args])\n\n        return char_case\n\n    for special_char in [\n        \".\",\n        \"-\",\n        \"+\",\n        \":\",\n        \"?\",\n        \"@\",\n        \"&\",\n        \"$\",\n        \"%\",\n        \"/\",\n        \"[\",\n        \"]\",\n        \"(\",\n        \")\",\n        \"*\",\n        \"'\",\n        \",\",\n    ]:\n        char_case = create_char_case(special_char)\n        validate_append_to_uri_path_test_cases(\n            [\n                char_case(\"\", \"{c}subpath\", \"{c}subpath\"),\n                char_case(\"\", \"/{c}subpath\", \"/{c}subpath\"),\n                char_case(\"dirwith{c}{c}chars\", \"\", \"dirwith{c}{c}chars/\"),\n                char_case(\"dirwith{c}{c}chars\", \"subpath\", \"dirwith{c}{c}chars/subpath\"),\n                char_case(\"{c}{c}charsdir\", \"\", \"{c}{c}charsdir/\"),\n                char_case(\"/{c}{c}charsdir\", \"\", \"/{c}{c}charsdir/\"),\n                char_case(\"/{c}{c}charsdir\", \"subpath\", \"/{c}{c}charsdir/subpath\"),\n                char_case(\"/{c}{c}charsdir\", \"subpath\", \"/{c}{c}charsdir/subpath\"),\n            ]\n        )\n\n    validate_append_to_uri_path_test_cases(\n        [\n            (\"#?charsdir:\", \":?subpath#\", \"#?charsdir:/:?subpath#\"),\n            (\"/#--+charsdir.//:\", \"/../:?subpath#\", \"/#--+charsdir.//:/../:?subpath#\"),\n            (\"$@''(,\", \")]*%\", \"$@''(,/)]*%\"),\n        ]\n    )\n\n\ndef test_append_to_uri_path_preserves_uri_schemes_hosts_queries_and_fragments():\n    validate_append_to_uri_path_test_cases(\n        [\n            (\"dbscheme+dbdriver:\", \"\", \"dbscheme+dbdriver:\"),\n            (\"dbscheme+dbdriver:\", \"subpath\", \"dbscheme+dbdriver:subpath\"),\n            (\"dbscheme+dbdriver:path\", \"subpath\", \"dbscheme+dbdriver:path/subpath\"),\n            (\"dbscheme+dbdriver://host/path\", \"/subpath\", \"dbscheme+dbdriver://host/path/subpath\"),\n            (\"dbscheme+dbdriver:///path\", \"subpath\", \"dbscheme+dbdriver:/path/subpath\"),\n            (\"dbscheme+dbdriver:?somequery\", \"subpath\", \"dbscheme+dbdriver:subpath?somequery\"),\n            (\"dbscheme+dbdriver:?somequery\", \"/subpath\", \"dbscheme+dbdriver:/subpath?somequery\"),\n            (\"dbscheme+dbdriver:/?somequery\", \"subpath\", \"dbscheme+dbdriver:/subpath?somequery\"),\n            (\"dbscheme+dbdriver://?somequery\", \"subpath\", \"dbscheme+dbdriver:subpath?somequery\"),\n            (\"dbscheme+dbdriver:///?somequery\", \"/subpath\", \"dbscheme+dbdriver:/subpath?somequery\"),\n            (\"dbscheme+dbdriver:#somefrag\", \"subpath\", \"dbscheme+dbdriver:subpath#somefrag\"),\n            (\"dbscheme+dbdriver:#somefrag\", \"/subpath\", \"dbscheme+dbdriver:/subpath#somefrag\"),\n            (\"dbscheme+dbdriver:/#somefrag\", \"subpath\", \"dbscheme+dbdriver:/subpath#somefrag\"),\n            (\"dbscheme+dbdriver://#somefrag\", \"subpath\", \"dbscheme+dbdriver:subpath#somefrag\"),\n            (\"dbscheme+dbdriver:///#somefrag\", \"/subpath\", \"dbscheme+dbdriver:/subpath#somefrag\"),\n            (\n                \"dbscheme+dbdriver://root:password?creds=creds\",\n                \"subpath\",\n                \"dbscheme+dbdriver://root:password/subpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password/path/?creds=creds\",\n                \"/subpath/anotherpath\",\n                \"dbscheme+dbdriver://root:password/path/subpath/anotherpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password///path/?creds=creds\",\n                \"subpath/anotherpath\",\n                \"dbscheme+dbdriver://root:password///path/subpath/anotherpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password///path/?creds=creds\",\n                \"/subpath\",\n                \"dbscheme+dbdriver://root:password///path/subpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password#myfragment\",\n                \"/subpath\",\n                \"dbscheme+dbdriver://root:password/subpath#myfragment\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password//path/#fragmentwith$pecial@\",\n                \"subpath/anotherpath\",\n                \"dbscheme+dbdriver://root:password//path/subpath/anotherpath#fragmentwith$pecial@\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password@host?creds=creds#fragmentwith$pecial@\",\n                \"subpath\",\n                \"dbscheme+dbdriver://root:password@host/subpath?creds=creds#fragmentwith$pecial@\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password@host.com/path?creds=creds#*frag@*\",\n                \"subpath/dir\",\n                \"dbscheme+dbdriver://root:password@host.com/path/subpath/dir?creds=creds#*frag@*\",\n            ),\n            (\n                \"dbscheme-dbdriver://root:password@host.com/path?creds=creds#*frag@*\",\n                \"subpath/dir\",\n                \"dbscheme-dbdriver://root:password@host.com/path/subpath/dir?creds=creds#*frag@*\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password@host.com/path?creds=creds,param=value#*frag@*\",\n                \"subpath/dir\",\n                \"dbscheme+dbdriver://root:password@host.com/path/subpath/dir?\"\n                \"creds=creds,param=value#*frag@*\",\n            ),\n        ]\n    )\n\n\ndef test_extract_and_normalize_path():\n    base_uri = \"databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    assert (\n        extract_and_normalize_path(\"dbfs:databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\"dbfs:/databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\"dbfs:///databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\n            \"dbfs:/databricks///mlflow-tracking///EXP_ID///RUN_ID///artifacts/\"\n        )\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\n            \"dbfs:///databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n        )\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\n            \"dbfs:databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n        )\n        == base_uri\n    )\n\n\ndef test_is_databricks_acled_artifacts_uri():\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:/databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:///databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:/databricks///mlflow-tracking///EXP_ID///RUN_ID///artifacts/\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:///databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n    )\n    assert not is_databricks_acled_artifacts_uri(\n        \"dbfs:/databricks/mlflow//EXP_ID//RUN_ID///artifacts//\"\n    )\n\n\ndef _get_databricks_profile_uri_test_cases():\n    # Each test case is (uri, result, result_scheme)\n    test_case_groups = [\n        [\n            # URIs with no databricks profile info -> return None\n            (\"ftp://user:pass@realhost:port/path/to/nowhere\", None, result_scheme),\n            (\"dbfs:/path/to/nowhere\", None, result_scheme),\n            (\"dbfs://nondatabricks/path/to/nowhere\", None, result_scheme),\n            (\"dbfs://incorrect:netloc:format/path/to/nowhere\", None, result_scheme),\n            # URIs with legit databricks profile info\n            (f\"dbfs://{result_scheme}\", result_scheme, result_scheme),\n            (f\"dbfs://{result_scheme}/\", result_scheme, result_scheme),\n            (f\"dbfs://{result_scheme}/path/to/nowhere\", result_scheme, result_scheme),\n            (f\"dbfs://{result_scheme}:port/path/to/nowhere\", result_scheme, result_scheme),\n            (f\"dbfs://@{result_scheme}/path/to/nowhere\", result_scheme, result_scheme),\n            (f\"dbfs://@{result_scheme}:port/path/to/nowhere\", result_scheme, result_scheme),\n            (\n                f\"dbfs://profile@{result_scheme}/path/to/nowhere\",\n                f\"{result_scheme}://profile\",\n                result_scheme,\n            ),\n            (\n                f\"dbfs://profile@{result_scheme}:port/path/to/nowhere\",\n                f\"{result_scheme}://profile\",\n                result_scheme,\n            ),\n            (\n                f\"dbfs://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            (\n                f\"dbfs://scope:key_prefix@{result_scheme}:port/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            # Doesn't care about the scheme of the artifact URI\n            (\n                f\"runs://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            (\n                f\"models://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            (\n                f\"s3://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n        ]\n        for result_scheme in [\"databricks\", \"databricks-uc\"]\n    ]\n    return [test_case for test_case_group in test_case_groups for test_case in test_case_group]\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\", \"result_scheme\"), _get_databricks_profile_uri_test_cases()\n)\ndef test_get_databricks_profile_uri_from_artifact_uri(uri, result, result_scheme):\n    assert get_databricks_profile_uri_from_artifact_uri(uri, result_scheme=result_scheme) == result\n\n\n@pytest.mark.parametrize(\n    \"uri\",\n    [\n        # Treats secret key prefixes with \":\" to be invalid\n        \"dbfs://incorrect:netloc:format@databricks/path/a\",\n        \"dbfs://scope::key_prefix@databricks/path/abc\",\n        \"dbfs://scope:key_prefix:@databricks/path/abc\",\n    ],\n)\ndef test_get_databricks_profile_uri_from_artifact_uri_error_cases(uri):\n    with pytest.raises(MlflowException, match=\"Unsupported Databricks profile\"):\n        get_databricks_profile_uri_from_artifact_uri(uri)\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\"),\n    [\n        # URIs with no databricks profile info should stay the same\n        (\n            \"ftp://user:pass@realhost:port/path/nowhere\",\n            \"ftp://user:pass@realhost:port/path/nowhere\",\n        ),\n        (\"dbfs:/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://nondatabricks/path/to/nowhere\", \"dbfs://nondatabricks/path/to/nowhere\"),\n        (\"dbfs://incorrect:netloc:format/path/\", \"dbfs://incorrect:netloc:format/path/\"),\n        # URIs with legit databricks profile info\n        (\"dbfs://databricks\", \"dbfs:\"),\n        (\"dbfs://databricks/\", \"dbfs:/\"),\n        (\"dbfs://databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://databricks:port/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://@databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://@databricks:port/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://profile@databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://profile@databricks:port/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://scope:key_prefix@databricks/path/abc\", \"dbfs:/path/abc\"),\n        (\"dbfs://scope:key_prefix@databricks:port/path/abc\", \"dbfs:/path/abc\"),\n        # Treats secret key prefixes with \":\" to be valid\n        (\"dbfs://incorrect:netloc:format@databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        # Doesn't care about the scheme of the artifact URI\n        (\"runs://scope:key_prefix@databricks/path/abc\", \"runs:/path/abc\"),\n        (\"models://scope:key_prefix@databricks/path/abc\", \"models:/path/abc\"),\n        (\"s3://scope:key_prefix@databricks/path/abc\", \"s3:/path/abc\"),\n    ],\n)\ndef test_remove_databricks_profile_info_from_artifact_uri(uri, result):\n    assert remove_databricks_profile_info_from_artifact_uri(uri) == result\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"profile_uri\", \"result\"),\n    [\n        # test various profile URIs\n        (\"dbfs:/path/a/b\", \"databricks\", \"dbfs://databricks/path/a/b\"),\n        (\"dbfs:/path/a/b/\", \"databricks\", \"dbfs://databricks/path/a/b/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://Profile\", \"dbfs://Profile@databricks/path/a/b/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://profile/\", \"dbfs://profile@databricks/path/a/b/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope:key\", \"dbfs://scope:key@databricks/path/a/b/\"),\n        (\n            \"dbfs:/path/a/b/\",\n            \"databricks://scope:key/random_stuff\",\n            \"dbfs://scope:key@databricks/path/a/b/\",\n        ),\n        (\"dbfs:/path/a/b/\", \"nondatabricks://profile\", \"dbfs:/path/a/b/\"),\n        # test various artifact schemes\n        (\"runs:/path/a/b/\", \"databricks://Profile\", \"runs://Profile@databricks/path/a/b/\"),\n        (\"runs:/path/a/b/\", \"nondatabricks://profile\", \"runs:/path/a/b/\"),\n        (\"models:/path/a/b/\", \"databricks://profile\", \"models://profile@databricks/path/a/b/\"),\n        (\"models:/path/a/b/\", \"nondatabricks://Profile\", \"models:/path/a/b/\"),\n        (\"s3:/path/a/b/\", \"databricks://Profile\", \"s3:/path/a/b/\"),\n        (\"s3:/path/a/b/\", \"nondatabricks://profile\", \"s3:/path/a/b/\"),\n        (\"ftp:/path/a/b/\", \"databricks://profile\", \"ftp:/path/a/b/\"),\n        (\"ftp:/path/a/b/\", \"nondatabricks://Profile\", \"ftp:/path/a/b/\"),\n        # test artifact URIs already with authority\n        (\"ftp://user:pass@host:port/a/b\", \"databricks://Profile\", \"ftp://user:pass@host:port/a/b\"),\n        (\"ftp://user:pass@host:port/a/b\", \"nothing://Profile\", \"ftp://user:pass@host:port/a/b\"),\n        (\"dbfs://databricks\", \"databricks://OtherProfile\", \"dbfs://databricks\"),\n        (\"dbfs://databricks\", \"nondatabricks://Profile\", \"dbfs://databricks\"),\n        (\"dbfs://databricks/path/a/b\", \"databricks://OtherProfile\", \"dbfs://databricks/path/a/b\"),\n        (\"dbfs://databricks/path/a/b\", \"nondatabricks://Profile\", \"dbfs://databricks/path/a/b\"),\n        (\"dbfs://@databricks/path/a/b\", \"databricks://OtherProfile\", \"dbfs://@databricks/path/a/b\"),\n        (\"dbfs://@databricks/path/a/b\", \"nondatabricks://Profile\", \"dbfs://@databricks/path/a/b\"),\n        (\n            \"dbfs://profile@databricks/pp\",\n            \"databricks://OtherProfile\",\n            \"dbfs://profile@databricks/pp\",\n        ),\n        (\n            \"dbfs://profile@databricks/path\",\n            \"databricks://profile\",\n            \"dbfs://profile@databricks/path\",\n        ),\n        (\n            \"dbfs://profile@databricks/path\",\n            \"nondatabricks://Profile\",\n            \"dbfs://profile@databricks/path\",\n        ),\n    ],\n)\ndef test_add_databricks_profile_info_to_artifact_uri(artifact_uri, profile_uri, result):\n    assert add_databricks_profile_info_to_artifact_uri(artifact_uri, profile_uri) == result\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"profile_uri\"),\n    [\n        (\"dbfs:/path/a/b\", \"databricks://not:legit:auth\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope::key\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope:key:/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope:key \"),\n    ],\n)\ndef test_add_databricks_profile_info_to_artifact_uri_errors(artifact_uri, profile_uri):\n    with pytest.raises(MlflowException, match=\"Unsupported Databricks profile\"):\n        add_databricks_profile_info_to_artifact_uri(artifact_uri, profile_uri)\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\"),\n    [\n        (\"dbfs:/path/a/b\", True),\n        (\"dbfs://databricks/a/b\", True),\n        (\"dbfs://@databricks/a/b\", True),\n        (\"dbfs://profile@databricks/a/b\", True),\n        (\"dbfs://scope:key@databricks/a/b\", True),\n        (\"dbfs://scope:key:@databricks/a/b\", False),\n        (\"dbfs://scope::key@databricks/a/b\", False),\n        (\"dbfs://profile@notdatabricks/a/b\", False),\n        (\"dbfs://scope:key@notdatabricks/a/b\", False),\n        (\"dbfs://scope:key/a/b\", False),\n        (\"dbfs://notdatabricks/a/b\", False),\n        (\"s3:/path/a/b\", False),\n        (\"ftp://user:pass@host:port/path/a/b\", False),\n        (\"ftp://user:pass@databricks/path/a/b\", False),\n    ],\n)\ndef test_is_valid_dbfs_uri(uri, result):\n    assert is_valid_dbfs_uri(uri) == result\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\"),\n    [\n        (\"/tmp/path\", \"/dbfs/tmp/path\"),\n        (\"dbfs:/path\", \"/dbfs/path\"),\n        (\"dbfs:/path/a/b\", \"/dbfs/path/a/b\"),\n        (\"dbfs:/dbfs/123/abc\", \"/dbfs/dbfs/123/abc\"),\n    ],\n)\ndef test_dbfs_hdfs_uri_to_fuse_path(uri, result):\n    assert dbfs_hdfs_uri_to_fuse_path(uri) == result\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\"some/relative/local/path\", \"s3:/some/s3/path\", \"C:/cool/windows/path\"],\n)\ndef test_dbfs_hdfs_uri_to_fuse_path_raises(path):\n    with pytest.raises(MlflowException, match=\"did not start with expected DBFS URI prefix\"):\n        dbfs_hdfs_uri_to_fuse_path(path)\n\n\ndef _assert_resolve_uri_if_local(input_uri, expected_uri):\n    cwd = pathlib.Path.cwd().as_posix()\n    drive = pathlib.Path.cwd().drive\n    if is_windows():\n        cwd = f\"/{cwd}\"\n        drive = f\"{drive}/\"\n    assert resolve_uri_if_local(input_uri) == expected_uri.format(cwd=cwd, drive=drive)\n\n\n@pytest.mark.skipif(is_windows(), reason=\"This test fails on Windows\")\n@pytest.mark.parametrize(\n    (\"input_uri\", \"expected_uri\"),\n    [\n        (\"my/path\", \"{cwd}/my/path\"),\n        (\"#my/path?a=b\", \"{cwd}/#my/path?a=b\"),\n        (\"file://myhostname/my/path\", \"file://myhostname/my/path\"),\n        (\"file:///my/path\", \"file:///{drive}my/path\"),\n        (\"file:my/path\", \"file://{cwd}/my/path\"),\n        (\"/home/my/path\", \"/home/my/path\"),\n        (\"dbfs://databricks/a/b\", \"dbfs://databricks/a/b\"),\n        (\"s3://host/my/path\", \"s3://host/my/path\"),\n    ],\n)\ndef test_resolve_uri_if_local(input_uri, expected_uri):\n    _assert_resolve_uri_if_local(input_uri, expected_uri)\n\n\n@pytest.mark.skipif(not is_windows(), reason=\"This test only passes on Windows\")\n@pytest.mark.parametrize(\n    (\"input_uri\", \"expected_uri\"),\n    [\n        (\"my/path\", \"file://{cwd}/my/path\"),\n        (\"#my/path?a=b\", \"file://{cwd}/#my/path?a=b\"),\n        (\"file://myhostname/my/path\", \"file://myhostname/my/path\"),\n        (\"file:///my/path\", \"file:///{drive}my/path\"),\n        (\"file:my/path\", \"file://{cwd}/my/path\"),\n        (\"/home/my/path\", \"file:///{drive}home/my/path\"),\n        (\"dbfs://databricks/a/b\", \"dbfs://databricks/a/b\"),\n        (\"s3://host/my/path\", \"s3://host/my/path\"),\n    ],\n)\ndef test_resolve_uri_if_local_on_windows(input_uri, expected_uri):\n    _assert_resolve_uri_if_local(input_uri, expected_uri)\n"], "fixing_code": ["\"\"\"\nThis module defines environment variables used in MLflow.\n\"\"\"\nimport os\n\n\nclass _EnvironmentVariable:\n    \"\"\"\n    Represents an environment variable.\n    \"\"\"\n\n    def __init__(self, name, type_, default):\n        self.name = name\n        self.type = type_\n        self.default = default\n\n    @property\n    def is_defined(self):\n        return self.name in os.environ\n\n    def get(self):\n        \"\"\"\n        Reads the value of the environment variable if it exists and converts it to the desired\n        type. Otherwise, returns the default value.\n        \"\"\"\n        val = os.getenv(self.name)\n        if val:\n            try:\n                return self.type(val)\n            except Exception as e:\n                raise ValueError(f\"Failed to convert {val} to {self.type} for {self.name}: {e}\")\n        return self.default\n\n    def __str__(self):\n        return f\"{self.name} (default: {self.default}, type: {self.type.__name__})\"\n\n    def __repr__(self):\n        return repr(self.name)\n\n\nclass _BooleanEnvironmentVariable(_EnvironmentVariable):\n    \"\"\"\n    Represents a boolean environment variable.\n    \"\"\"\n\n    def __init__(self, name, default):\n        # `default not in [True, False, None]` doesn't work because `1 in [True]`\n        # (or `0 in [False]`) returns True.\n        if not (default is True or default is False or default is None):\n            raise ValueError(f\"{name} default value must be one of [True, False, None]\")\n        super().__init__(name, bool, default)\n\n    def get(self):\n        if not self.is_defined:\n            return self.default\n\n        val = os.getenv(self.name)\n        lowercased = val.lower()\n        if lowercased not in [\"true\", \"false\", \"1\", \"0\"]:\n            raise ValueError(\n                f\"{self.name} value must be one of ['true', 'false', '1', '0'] (case-insensitive), \"\n                f\"but got {val}\"\n            )\n        return lowercased in [\"true\", \"1\"]\n\n\n#: Specifies the ``dfs_tmpdir`` parameter to use for ``mlflow.spark.save_model``,\n#: ``mlflow.spark.log_model`` and ``mlflow.spark.load_model``. See\n#: https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.save_model\n#: for more information.\n#: (default: ``/tmp/mlflow``)\nMLFLOW_DFS_TMP = _EnvironmentVariable(\"MLFLOW_DFS_TMP\", str, \"/tmp/mlflow\")\n\n#: Specifies the maximum number of retries for MLflow HTTP requests\n#: (default: ``5``)\nMLFLOW_HTTP_REQUEST_MAX_RETRIES = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", int, 5)\n\n#: Specifies the backoff increase factor between MLflow HTTP request failures\n#: (default: ``2``)\nMLFLOW_HTTP_REQUEST_BACKOFF_FACTOR = _EnvironmentVariable(\n    \"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", int, 2\n)\n\n#: Specifies the timeout in seconds for MLflow HTTP requests\n#: (default: ``120``)\nMLFLOW_HTTP_REQUEST_TIMEOUT = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", int, 120)\n\n#: Specifies whether MLFlow HTTP requests should be signed using AWS signature V4. It will overwrite\n#: (default: ``False``). When set, it will overwrite the \"Authorization\" HTTP header.\n#: See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html for more information.\nMLFLOW_TRACKING_AWS_SIGV4 = _BooleanEnvironmentVariable(\"MLFLOW_TRACKING_AWS_SIGV4\", False)\n\n#: Specifies the chunk size to use when downloading a file from GCS\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_DOWNLOAD_CHUNK_SIZE\", int, None)\n\n#: Specifies the chunk size to use when uploading a file to GCS.\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_UPLOAD_CHUNK_SIZE\", int, None)\n\n#: (Deprecated, please use ``MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT``)\n#: Specifies the default timeout to use when downloading/uploading a file from/to GCS\n#: (default: ``None``). If None, ``google.cloud.storage.constants._DEFAULT_TIMEOUT`` is used.\nMLFLOW_GCS_DEFAULT_TIMEOUT = _EnvironmentVariable(\"MLFLOW_GCS_DEFAULT_TIMEOUT\", int, None)\n\n#: Specifies whether to disable model logging and loading via mlflowdbfs.\n#: (default: ``None``)\n_DISABLE_MLFLOWDBFS = _EnvironmentVariable(\"DISABLE_MLFLOWDBFS\", str, None)\n\n#: Specifies the S3 endpoint URL to use for S3 artifact operations.\n#: (default: ``None``)\nMLFLOW_S3_ENDPOINT_URL = _EnvironmentVariable(\"MLFLOW_S3_ENDPOINT_URL\", str, None)\n\n#: Specifies whether or not to skip TLS certificate verification for S3 artifact operations.\n#: (default: ``False``)\nMLFLOW_S3_IGNORE_TLS = _BooleanEnvironmentVariable(\"MLFLOW_S3_IGNORE_TLS\", False)\n\n#: Specifies extra arguments for S3 artifact uploads.\n#: (default: ``None``)\nMLFLOW_S3_UPLOAD_EXTRA_ARGS = _EnvironmentVariable(\"MLFLOW_S3_UPLOAD_EXTRA_ARGS\", str, None)\n\n#: Specifies the location of a Kerberos ticket cache to use for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_TICKET_CACHE = _EnvironmentVariable(\"MLFLOW_KERBEROS_TICKET_CACHE\", str, None)\n\n#: Specifies a Kerberos user for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_USER = _EnvironmentVariable(\"MLFLOW_KERBEROS_USER\", str, None)\n\n#: Specifies extra pyarrow configurations for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_PYARROW_EXTRA_CONF = _EnvironmentVariable(\"MLFLOW_PYARROW_EXTRA_CONF\", str, None)\n\n#: Specifies the ``pool_size`` parameter to use for ``sqlalchemy.create_engine`` in the SQLAlchemy\n#: tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_size\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_SIZE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_SIZE\", int, None\n)\n\n#: Specifies the ``pool_recycle`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE\", int, None\n)\n\n#: Specifies the ``max_overflow`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.max_overflow\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW\", int, None\n)\n\n#: Specifies the ``echo`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.echo\n#: for more information.\n#: (default: ``False``)\nMLFLOW_SQLALCHEMYSTORE_ECHO = _BooleanEnvironmentVariable(\"MLFLOW_SQLALCHEMYSTORE_ECHO\", False)\n\n#: Specifies whether or not to print a warning when `--env-manager=conda` is specified.\n#: (default: ``False``)\nMLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING = _BooleanEnvironmentVariable(\n    \"MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\", False\n)\n#: Specifies the ``poolclass`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.poolclass\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOLCLASS = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOLCLASS\", str, None\n)\n\n#: Specifies the ``timeout_seconds`` for MLflow Model dependency inference operations.\n#: (default: ``120``)\nMLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT\", int, 120\n)\n\n#: Specifies the MLflow Model Scoring server request timeout in seconds\n#: (default: ``60``)\nMLFLOW_SCORING_SERVER_REQUEST_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT\", int, 60\n)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the timeout to use when uploading or downloading a file\n#: (default: ``None``). If None, individual artifact stores will choose defaults.\nMLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT\", int, None\n)\n\n#: Specifies the device intended for use in the predict function - can be used\n#: to override behavior where the GPU is used by default when available by\n#: setting this environment variable to be ``cpu``. Currently, this\n#: variable is only supported for the MLflow PyTorch flavor.\nMLFLOW_DEFAULT_PREDICTION_DEVICE = _EnvironmentVariable(\n    \"MLFLOW_DEFAULT_PREDICTION_DEVICE\", str, None\n)\n\n#: Specifies whether or not to allow using a file URI as a model version source.\n#: Please be aware that setting this environment variable to True is potentially risky\n#: because it can allow access to arbitrary files on the specified filesystem\n#: (default: ``False``).\nMLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE = _BooleanEnvironmentVariable(\n    \"MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE\", False\n)\n", "# Define all the service endpoint handlers here.\nimport json\nimport os\nimport re\nimport tempfile\nimport posixpath\nimport urllib\nimport pathlib\n\nimport logging\nfrom functools import wraps\n\nfrom flask import Response, request, current_app, send_file\nfrom google.protobuf import descriptor\nfrom google.protobuf.json_format import ParseError\n\nfrom mlflow.entities import Metric, Param, RunTag, ViewType, ExperimentTag, FileInfo\nfrom mlflow.entities.model_registry import RegisteredModelTag, ModelVersionTag\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model\nfrom mlflow.protos import databricks_pb2\nfrom mlflow.protos.service_pb2 import (\n    CreateExperiment,\n    MlflowService,\n    GetExperiment,\n    GetRun,\n    SearchRuns,\n    ListArtifacts,\n    GetMetricHistory,\n    CreateRun,\n    UpdateRun,\n    LogMetric,\n    LogParam,\n    SetTag,\n    SearchExperiments,\n    DeleteExperiment,\n    RestoreExperiment,\n    RestoreRun,\n    DeleteRun,\n    UpdateExperiment,\n    LogBatch,\n    DeleteTag,\n    SetExperimentTag,\n    GetExperimentByName,\n    LogModel,\n)\nfrom mlflow.protos.model_registry_pb2 import (\n    ModelRegistryService,\n    CreateRegisteredModel,\n    UpdateRegisteredModel,\n    DeleteRegisteredModel,\n    GetRegisteredModel,\n    GetLatestVersions,\n    CreateModelVersion,\n    UpdateModelVersion,\n    DeleteModelVersion,\n    GetModelVersion,\n    GetModelVersionDownloadUri,\n    SearchModelVersions,\n    RenameRegisteredModel,\n    TransitionModelVersionStage,\n    SearchRegisteredModels,\n    SetRegisteredModelTag,\n    DeleteRegisteredModelTag,\n    SetModelVersionTag,\n    DeleteModelVersionTag,\n    SetRegisteredModelAlias,\n    DeleteRegisteredModelAlias,\n    GetModelVersionByAlias,\n)\nfrom mlflow.protos.mlflow_artifacts_pb2 import (\n    MlflowArtifactsService,\n    DownloadArtifact,\n    UploadArtifact,\n    ListArtifacts as ListArtifactsMlflowArtifacts,\n    DeleteArtifact,\n)\nfrom mlflow.protos.databricks_pb2 import RESOURCE_DOES_NOT_EXIST, INVALID_PARAMETER_VALUE\nfrom mlflow.store.artifact.artifact_repository_registry import get_artifact_repository\nfrom mlflow.store.db.db_types import DATABASE_ENGINES\nfrom mlflow.tracking._model_registry.registry import ModelRegistryStoreRegistry\nfrom mlflow.tracking._tracking_service.registry import TrackingStoreRegistry\nfrom mlflow.utils.mime_type_utils import _guess_mime_type\nfrom mlflow.utils.proto_json_utils import message_to_json, parse_dict\nfrom mlflow.utils.validation import _validate_batch_log_api_req\nfrom mlflow.utils.string_utils import is_string_type\nfrom mlflow.utils.uri import is_local_uri, is_file_uri\nfrom mlflow.utils.file_utils import local_file_uri_to_path\nfrom mlflow.tracking.registry import UnsupportedModelRegistryStoreURIException\nfrom mlflow.environment_variables import MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE\n\n_logger = logging.getLogger(__name__)\n_tracking_store = None\n_model_registry_store = None\n_artifact_repo = None\nSTATIC_PREFIX_ENV_VAR = \"_MLFLOW_STATIC_PREFIX\"\n\n\nclass TrackingStoreRegistryWrapper(TrackingStoreRegistry):\n    def __init__(self):\n        super().__init__()\n        self.register(\"\", self._get_file_store)\n        self.register(\"file\", self._get_file_store)\n        for scheme in DATABASE_ENGINES:\n            self.register(scheme, self._get_sqlalchemy_store)\n        self.register_entrypoints()\n\n    @classmethod\n    def _get_file_store(cls, store_uri, artifact_uri):\n        from mlflow.store.tracking.file_store import FileStore\n\n        return FileStore(store_uri, artifact_uri)\n\n    @classmethod\n    def _get_sqlalchemy_store(cls, store_uri, artifact_uri):\n        from mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore\n\n        return SqlAlchemyStore(store_uri, artifact_uri)\n\n\nclass ModelRegistryStoreRegistryWrapper(ModelRegistryStoreRegistry):\n    def __init__(self):\n        super().__init__()\n        self.register(\"\", self._get_file_store)\n        self.register(\"file\", self._get_file_store)\n        for scheme in DATABASE_ENGINES:\n            self.register(scheme, self._get_sqlalchemy_store)\n        self.register_entrypoints()\n\n    @classmethod\n    def _get_file_store(cls, store_uri):\n        from mlflow.store.model_registry.file_store import FileStore\n\n        return FileStore(store_uri)\n\n    @classmethod\n    def _get_sqlalchemy_store(cls, store_uri):\n        from mlflow.store.model_registry.sqlalchemy_store import SqlAlchemyStore\n\n        return SqlAlchemyStore(store_uri)\n\n\n_tracking_store_registry = TrackingStoreRegistryWrapper()\n_model_registry_store_registry = ModelRegistryStoreRegistryWrapper()\n\n\ndef _get_artifact_repo_mlflow_artifacts():\n    \"\"\"\n    Get an artifact repository specified by ``--artifacts-destination`` option for ``mlflow server``\n    command.\n    \"\"\"\n    from mlflow.server import ARTIFACTS_DESTINATION_ENV_VAR\n\n    global _artifact_repo\n    if _artifact_repo is None:\n        _artifact_repo = get_artifact_repository(os.environ[ARTIFACTS_DESTINATION_ENV_VAR])\n    return _artifact_repo\n\n\ndef _is_serving_proxied_artifacts():\n    \"\"\"\n    :return: ``True`` if the MLflow server is serving proxied artifacts (i.e. acting as a proxy for\n             artifact upload / download / list operations), as would be enabled by specifying the\n             ``--serve-artifacts`` configuration option. ``False`` otherwise.\n    \"\"\"\n    from mlflow.server import SERVE_ARTIFACTS_ENV_VAR\n\n    return os.environ.get(SERVE_ARTIFACTS_ENV_VAR, \"false\") == \"true\"\n\n\ndef _is_servable_proxied_run_artifact_root(run_artifact_root):\n    \"\"\"\n    Determines whether or not the following are true:\n\n    - The specified Run artifact root is a proxied artifact root (i.e. an artifact root with scheme\n      ``http``, ``https``, or ``mlflow-artifacts``).\n\n    - The MLflow server is capable of resolving and accessing the underlying storage location\n      corresponding to the proxied artifact root, allowing it to fulfill artifact list and\n      download requests by using this storage location directly.\n\n    :param run_artifact_root: The Run artifact root location (URI).\n    :return: ``True`` if the specified Run artifact root refers to proxied artifacts that can be\n             served by this MLflow server (i.e. the server has access to the destination and\n             can respond to list and download requests for the artifact). ``False`` otherwise.\n    \"\"\"\n    parsed_run_artifact_root = urllib.parse.urlparse(run_artifact_root)\n    # NB: If the run artifact root is a proxied artifact root (has scheme `http`, `https`, or\n    # `mlflow-artifacts`) *and* the MLflow server is configured to serve artifacts, the MLflow\n    # server always assumes that it has access to the underlying storage location for the proxied\n    # artifacts. This may not always be accurate. For example:\n    #\n    # An organization may initially use the MLflow server to serve Tracking API requests and proxy\n    # access to artifacts stored in Location A (via `mlflow server --serve-artifacts`). Then, for\n    # scalability and / or security purposes, the organization may decide to store artifacts in a\n    # new location B and set up a separate server (e.g. `mlflow server --artifacts-only`) to proxy\n    # access to artifacts stored in Location B.\n    #\n    # In this scenario, requests for artifacts stored in Location B that are sent to the original\n    # MLflow server will fail if the original MLflow server does not have access to Location B\n    # because it will assume that it can serve all proxied artifacts regardless of the underlying\n    # location. Such failures can be remediated by granting the original MLflow server access to\n    # Location B.\n    return (\n        parsed_run_artifact_root.scheme in [\"http\", \"https\", \"mlflow-artifacts\"]\n        and _is_serving_proxied_artifacts()\n    )\n\n\ndef _get_proxied_run_artifact_destination_path(proxied_artifact_root, relative_path=None):\n    \"\"\"\n    Resolves the specified proxied artifact location within a Run to a concrete storage location.\n\n    :param proxied_artifact_root: The Run artifact root location (URI) with scheme ``http``,\n                                  ``https``, or `mlflow-artifacts` that can be resolved by the\n                                  MLflow server to a concrete storage location.\n    :param relative_path: The relative path of the destination within the specified\n                          ``proxied_artifact_root``. If ``None``, the destination is assumed to be\n                          the resolved ``proxied_artifact_root``.\n    :return: The storage location of the specified artifact.\n    \"\"\"\n    parsed_proxied_artifact_root = urllib.parse.urlparse(proxied_artifact_root)\n    assert parsed_proxied_artifact_root.scheme in [\"http\", \"https\", \"mlflow-artifacts\"]\n\n    if parsed_proxied_artifact_root.scheme == \"mlflow-artifacts\":\n        # If the proxied artifact root is an `mlflow-artifacts` URI, the run artifact root path is\n        # simply the path component of the URI, since the fully-qualified format of an\n        # `mlflow-artifacts` URI is `mlflow-artifacts://<netloc>/path/to/artifact`\n        proxied_run_artifact_root_path = parsed_proxied_artifact_root.path.lstrip(\"/\")\n    else:\n        # In this case, the proxied artifact root is an HTTP(S) URL referring to an mlflow-artifacts\n        # API route that can be used to download the artifact. These routes are always anchored at\n        # `/api/2.0/mlflow-artifacts/artifacts`. Accordingly, we split the path on this route anchor\n        # and interpret the rest of the path (everything after the route anchor) as the run artifact\n        # root path\n        mlflow_artifacts_http_route_anchor = \"/api/2.0/mlflow-artifacts/artifacts/\"\n        assert mlflow_artifacts_http_route_anchor in parsed_proxied_artifact_root.path\n\n        proxied_run_artifact_root_path = parsed_proxied_artifact_root.path.split(\n            mlflow_artifacts_http_route_anchor\n        )[1].lstrip(\"/\")\n\n    return (\n        posixpath.join(proxied_run_artifact_root_path, relative_path)\n        if relative_path is not None\n        else proxied_run_artifact_root_path\n    )\n\n\ndef _get_tracking_store(backend_store_uri=None, default_artifact_root=None):\n    from mlflow.server import BACKEND_STORE_URI_ENV_VAR, ARTIFACT_ROOT_ENV_VAR\n\n    global _tracking_store\n    if _tracking_store is None:\n        store_uri = backend_store_uri or os.environ.get(BACKEND_STORE_URI_ENV_VAR, None)\n        artifact_root = default_artifact_root or os.environ.get(ARTIFACT_ROOT_ENV_VAR, None)\n        _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\n    return _tracking_store\n\n\ndef _get_model_registry_store(registry_store_uri=None):\n    from mlflow.server import REGISTRY_STORE_URI_ENV_VAR, BACKEND_STORE_URI_ENV_VAR\n\n    global _model_registry_store\n    if _model_registry_store is None:\n        store_uri = (\n            registry_store_uri\n            or os.environ.get(REGISTRY_STORE_URI_ENV_VAR, None)\n            or os.environ.get(BACKEND_STORE_URI_ENV_VAR, None)\n        )\n        _model_registry_store = _model_registry_store_registry.get_store(store_uri)\n    return _model_registry_store\n\n\ndef initialize_backend_stores(\n    backend_store_uri=None, registry_store_uri=None, default_artifact_root=None\n):\n    _get_tracking_store(backend_store_uri, default_artifact_root)\n    try:\n        _get_model_registry_store(registry_store_uri)\n    except UnsupportedModelRegistryStoreURIException:\n        pass\n\n\ndef _assert_string(x):\n    assert isinstance(x, str)\n\n\ndef _assert_intlike(x):\n    try:\n        x = int(x)\n    except ValueError:\n        pass\n\n    assert isinstance(x, int)\n\n\ndef _assert_bool(x):\n    assert isinstance(x, bool)\n\n\ndef _assert_floatlike(x):\n    try:\n        x = float(x)\n    except ValueError:\n        pass\n\n    assert isinstance(x, float)\n\n\ndef _assert_array(x):\n    assert isinstance(x, list)\n\n\ndef _assert_required(x):\n    assert x is not None\n    # When parsing JSON payloads via proto, absent string fields\n    # are expressed as empty strings\n    assert x != \"\"\n\n\ndef _assert_less_than_or_equal(x, max_value):\n    assert x <= max_value\n\n\ndef _assert_item_type_string(x):\n    assert all(isinstance(item, str) for item in x)\n\n\n_TYPE_VALIDATORS = {\n    _assert_intlike,\n    _assert_string,\n    _assert_bool,\n    _assert_floatlike,\n    _assert_array,\n    _assert_item_type_string,\n}\n\n\ndef _validate_param_against_schema(schema, param, value, proto_parsing_succeeded=False):\n    \"\"\"\n    Attempts to validate a single parameter against a specified schema.\n    Examples of the elements of the schema are type assertions and checks for required parameters.\n    Returns None on validation success. Otherwise, raises an MLFlowException if an assertion fails.\n    This method is intended to be called for side effects.\n\n            Parameters:\n    :param schema: A list of functions to validate the parameter against.\n    :param param: The string name of the parameter being validated.\n    :param value: The corresponding value of the `param` being validated.\n    :param proto_parsing_succeeded: A boolean value indicating whether proto parsing succeeded.\n                                    If the proto was successfully parsed, we assume all of the types\n                                    of the parameters in the request body were correctly specified,\n                                    and thus we skip validating types. If proto parsing failed,\n                                    then we validate types in addition to the rest of the schema.\n                                    For details, see https://github.com/mlflow/mlflow/pull/\n                                    5458#issuecomment-1080880870.\n    \"\"\"\n\n    for f in schema:\n        if f in _TYPE_VALIDATORS and proto_parsing_succeeded:\n            continue\n\n        try:\n            f(value)\n        except AssertionError:\n            if f == _assert_required:\n                message = f\"Missing value for required parameter '{param}'.\"\n            else:\n                message = (\n                    f\"Invalid value {value} for parameter '{param}' supplied.\"\n                    f\" Hint: Value was of type '{type(value).__name__}'.\"\n                )\n            raise MlflowException(\n                message=(\n                    message + \" See the API docs for more information about request parameters.\"\n                ),\n                error_code=INVALID_PARAMETER_VALUE,\n            )\n\n    return None\n\n\ndef _get_request_json(flask_request=request):\n    return flask_request.get_json(force=True, silent=True)\n\n\ndef _get_request_message(request_message, flask_request=request, schema=None):\n    from querystring_parser import parser\n\n    if flask_request.method == \"GET\" and len(flask_request.query_string) > 0:\n        # This is a hack to make arrays of length 1 work with the parser.\n        # for example experiment_ids%5B%5D=0 should be parsed to {experiment_ids: [0]}\n        # but it gets parsed to {experiment_ids: 0}\n        # but it doesn't. However, experiment_ids%5B0%5D=0 will get parsed to the right\n        # result.\n        query_string = re.sub(\"%5B%5D\", \"%5B0%5D\", flask_request.query_string.decode(\"utf-8\"))\n        request_dict = parser.parse(query_string, normalized=True)\n        # Convert atomic values of repeated fields to lists before calling protobuf deserialization.\n        # Context: We parse the parameter string into a dictionary outside of protobuf since\n        # protobuf does not know how to read the query parameters directly. The query parser above\n        # has no type information and hence any parameter that occurs exactly once is parsed as an\n        # atomic value. Since protobuf requires that the values of repeated fields are lists,\n        # deserialization will fail unless we do the fix below.\n        for field in request_message.DESCRIPTOR.fields:\n            if (\n                field.label == descriptor.FieldDescriptor.LABEL_REPEATED\n                and field.name in request_dict\n            ):\n                if not isinstance(request_dict[field.name], list):\n                    request_dict[field.name] = [request_dict[field.name]]\n        parse_dict(request_dict, request_message)\n        return request_message\n\n    request_json = _get_request_json(flask_request)\n\n    # Older clients may post their JSON double-encoded as strings, so the get_json\n    # above actually converts it to a string. Therefore, we check this condition\n    # (which we can tell for sure because any proper request should be a dictionary),\n    # and decode it a second time.\n    if is_string_type(request_json):\n        request_json = json.loads(request_json)\n\n    # If request doesn't have json body then assume it's empty.\n    if request_json is None:\n        request_json = {}\n\n    proto_parsing_succeeded = True\n    try:\n        parse_dict(request_json, request_message)\n    except ParseError:\n        proto_parsing_succeeded = False\n\n    schema = schema or {}\n    for schema_key, schema_validation_fns in schema.items():\n        if schema_key in request_json or _assert_required in schema_validation_fns:\n            value = request_json.get(schema_key)\n            if schema_key == \"run_id\" and value is None and \"run_uuid\" in request_json:\n                value = request_json.get(\"run_uuid\")\n            _validate_param_against_schema(\n                schema=schema_validation_fns,\n                param=schema_key,\n                value=value,\n                proto_parsing_succeeded=proto_parsing_succeeded,\n            )\n\n    return request_message\n\n\ndef _response_with_file_attachment_headers(file_path, response):\n    mime_type = _guess_mime_type(file_path)\n    filename = pathlib.Path(file_path).name\n    response.mimetype = mime_type\n    content_disposition_header_name = \"Content-Disposition\"\n    if content_disposition_header_name not in response.headers:\n        response.headers[content_disposition_header_name] = f\"attachment; filename={filename}\"\n    response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n    response.headers[\"Content-Type\"] = mime_type\n    return response\n\n\ndef _send_artifact(artifact_repository, path):\n    file_path = os.path.abspath(artifact_repository.download_artifacts(path))\n    # Always send artifacts as attachments to prevent the browser from displaying them on our web\n    # server's domain, which might enable XSS.\n    mime_type = _guess_mime_type(file_path)\n    file_sender_response = send_file(file_path, mimetype=mime_type, as_attachment=True)\n    return _response_with_file_attachment_headers(file_path, file_sender_response)\n\n\ndef catch_mlflow_exception(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except MlflowException as e:\n            response = Response(mimetype=\"application/json\")\n            response.set_data(e.serialize_as_json())\n            response.status_code = e.get_http_status_code()\n            return response\n\n    return wrapper\n\n\ndef _disable_unless_serve_artifacts(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not _is_serving_proxied_artifacts():\n            return Response(\n                (\n                    f\"Endpoint: {request.url_rule} disabled due to the mlflow server running \"\n                    \"with `--no-serve-artifacts`. To enable artifacts server functionality, \"\n                    \"run `mlflow server` with `--serve-artifacts`\"\n                ),\n                503,\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef _disable_if_artifacts_only(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        from mlflow.server import ARTIFACTS_ONLY_ENV_VAR\n\n        if os.environ.get(ARTIFACTS_ONLY_ENV_VAR):\n            return Response(\n                (\n                    f\"Endpoint: {request.url_rule} disabled due to the mlflow server running \"\n                    \"in `--artifacts-only` mode. To enable tracking server functionality, run \"\n                    \"`mlflow server` without `--artifacts-only`\"\n                ),\n                503,\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n\n\n_OS_ALT_SEPS = [sep for sep in [os.sep, os.path.altsep] if sep is not None and sep != \"/\"]\n\n\ndef validate_path_is_safe(path):\n    \"\"\"\n    Validates that the specified path is safe to join with a trusted prefix. This is a security\n    measure to prevent path traversal attacks.\n    \"\"\"\n    if (\n        any((s in path) for s in _OS_ALT_SEPS)\n        or \"..\" in path.split(posixpath.sep)\n        or posixpath.isabs(path)\n    ):\n        raise MlflowException(f\"Invalid path: {path}\", error_code=INVALID_PARAMETER_VALUE)\n\n\n@catch_mlflow_exception\ndef get_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = _get_artifact_repo(run)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n\n\ndef _not_implemented():\n    response = Response()\n    response.status_code = 404\n    return response\n\n\n# Tracking Server APIs\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_experiment():\n    request_message = _get_request_message(\n        CreateExperiment(),\n        schema={\n            \"name\": [_assert_required, _assert_string],\n            \"artifact_location\": [_assert_string],\n            \"tags\": [_assert_array],\n        },\n    )\n\n    tags = [ExperimentTag(tag.key, tag.value) for tag in request_message.tags]\n    experiment_id = _get_tracking_store().create_experiment(\n        request_message.name, request_message.artifact_location, tags\n    )\n    response_message = CreateExperiment.Response()\n    response_message.experiment_id = experiment_id\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_experiment():\n    request_message = _get_request_message(\n        GetExperiment(), schema={\"experiment_id\": [_assert_required, _assert_string]}\n    )\n    response_message = GetExperiment.Response()\n    experiment = _get_tracking_store().get_experiment(request_message.experiment_id).to_proto()\n    response_message.experiment.MergeFrom(experiment)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_experiment_by_name():\n    request_message = _get_request_message(\n        GetExperimentByName(), schema={\"experiment_name\": [_assert_required, _assert_string]}\n    )\n    response_message = GetExperimentByName.Response()\n    store_exp = _get_tracking_store().get_experiment_by_name(request_message.experiment_name)\n    if store_exp is None:\n        raise MlflowException(\n            \"Could not find experiment with name '%s'\" % request_message.experiment_name,\n            error_code=RESOURCE_DOES_NOT_EXIST,\n        )\n    experiment = store_exp.to_proto()\n    response_message.experiment.MergeFrom(experiment)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_experiment():\n    request_message = _get_request_message(\n        DeleteExperiment(), schema={\"experiment_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().delete_experiment(request_message.experiment_id)\n    response_message = DeleteExperiment.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _restore_experiment():\n    request_message = _get_request_message(\n        RestoreExperiment(), schema={\"experiment_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().restore_experiment(request_message.experiment_id)\n    response_message = RestoreExperiment.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_experiment():\n    request_message = _get_request_message(\n        UpdateExperiment(),\n        schema={\n            \"experiment_id\": [_assert_required, _assert_string],\n            \"new_name\": [_assert_string, _assert_required],\n        },\n    )\n    if request_message.new_name:\n        _get_tracking_store().rename_experiment(\n            request_message.experiment_id, request_message.new_name\n        )\n    response_message = UpdateExperiment.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_run():\n    request_message = _get_request_message(\n        CreateRun(),\n        schema={\n            \"experiment_id\": [_assert_string],\n            \"start_time\": [_assert_intlike],\n            \"run_name\": [_assert_string],\n        },\n    )\n\n    tags = [RunTag(tag.key, tag.value) for tag in request_message.tags]\n    run = _get_tracking_store().create_run(\n        experiment_id=request_message.experiment_id,\n        user_id=request_message.user_id,\n        start_time=request_message.start_time,\n        tags=tags,\n        run_name=request_message.run_name,\n    )\n\n    response_message = CreateRun.Response()\n    response_message.run.MergeFrom(run.to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_run():\n    request_message = _get_request_message(\n        UpdateRun(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"end_time\": [_assert_intlike],\n            \"status\": [_assert_string],\n            \"run_name\": [_assert_string],\n        },\n    )\n    run_id = request_message.run_id or request_message.run_uuid\n    updated_info = _get_tracking_store().update_run_info(\n        run_id, request_message.status, request_message.end_time, request_message.run_name\n    )\n    response_message = UpdateRun.Response(run_info=updated_info.to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_run():\n    request_message = _get_request_message(\n        DeleteRun(), schema={\"run_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().delete_run(request_message.run_id)\n    response_message = DeleteRun.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _restore_run():\n    request_message = _get_request_message(\n        RestoreRun(), schema={\"run_id\": [_assert_required, _assert_string]}\n    )\n    _get_tracking_store().restore_run(request_message.run_id)\n    response_message = RestoreRun.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_metric():\n    request_message = _get_request_message(\n        LogMetric(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_required, _assert_floatlike],\n            \"timestamp\": [_assert_intlike, _assert_required],\n            \"step\": [_assert_intlike],\n        },\n    )\n    metric = Metric(\n        request_message.key, request_message.value, request_message.timestamp, request_message.step\n    )\n    run_id = request_message.run_id or request_message.run_uuid\n    _get_tracking_store().log_metric(run_id, metric)\n    response_message = LogMetric.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_param():\n    request_message = _get_request_message(\n        LogParam(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_string],\n        },\n    )\n    param = Param(request_message.key, request_message.value)\n    run_id = request_message.run_id or request_message.run_uuid\n    _get_tracking_store().log_param(run_id, param)\n    response_message = LogParam.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_experiment_tag():\n    request_message = _get_request_message(\n        SetExperimentTag(),\n        schema={\n            \"experiment_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = ExperimentTag(request_message.key, request_message.value)\n    _get_tracking_store().set_experiment_tag(request_message.experiment_id, tag)\n    response_message = SetExperimentTag.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_tag():\n    request_message = _get_request_message(\n        SetTag(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = RunTag(request_message.key, request_message.value)\n    run_id = request_message.run_id or request_message.run_uuid\n    _get_tracking_store().set_tag(run_id, tag)\n    response_message = SetTag.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_tag():\n    request_message = _get_request_message(\n        DeleteTag(),\n        schema={\n            \"run_id\": [_assert_required, _assert_string],\n            \"key\": [_assert_required, _assert_string],\n        },\n    )\n    _get_tracking_store().delete_tag(request_message.run_id, request_message.key)\n    response_message = DeleteTag.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_run():\n    request_message = _get_request_message(\n        GetRun(), schema={\"run_id\": [_assert_required, _assert_string]}\n    )\n    response_message = GetRun.Response()\n    run_id = request_message.run_id or request_message.run_uuid\n    response_message.run.MergeFrom(_get_tracking_store().get_run(run_id).to_proto())\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_runs():\n    request_message = _get_request_message(\n        SearchRuns(),\n        schema={\n            \"experiment_ids\": [_assert_array],\n            \"filter\": [_assert_string],\n            \"max_results\": [_assert_intlike, lambda x: _assert_less_than_or_equal(x, 50000)],\n            \"order_by\": [_assert_array, _assert_item_type_string],\n        },\n    )\n    response_message = SearchRuns.Response()\n    run_view_type = ViewType.ACTIVE_ONLY\n    if request_message.HasField(\"run_view_type\"):\n        run_view_type = ViewType.from_proto(request_message.run_view_type)\n    filter_string = request_message.filter\n    max_results = request_message.max_results\n    experiment_ids = request_message.experiment_ids\n    order_by = request_message.order_by\n    page_token = request_message.page_token\n    run_entities = _get_tracking_store().search_runs(\n        experiment_ids, filter_string, run_view_type, max_results, order_by, page_token\n    )\n    response_message.runs.extend([r.to_proto() for r in run_entities])\n    if run_entities.token:\n        response_message.next_page_token = run_entities.token\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _list_artifacts():\n    request_message = _get_request_message(\n        ListArtifacts(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"path\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    response_message = ListArtifacts.Response()\n    if request_message.HasField(\"path\"):\n        path = request_message.path\n        validate_path_is_safe(path)\n    else:\n        path = None\n    run_id = request_message.run_id or request_message.run_uuid\n    run = _get_tracking_store().get_run(run_id)\n\n    if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n        artifact_entities = _list_artifacts_for_proxied_run_artifact_root(\n            proxied_artifact_root=run.info.artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_entities = _get_artifact_repo(run).list_artifacts(path)\n\n    response_message.files.extend([a.to_proto() for a in artifact_entities])\n    response_message.root_uri = run.info.artifact_uri\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\ndef _list_artifacts_for_proxied_run_artifact_root(proxied_artifact_root, relative_path=None):\n    \"\"\"\n    Lists artifacts from the specified ``relative_path`` within the specified proxied Run artifact\n    root (i.e. a Run artifact root with scheme ``http``, ``https``, or ``mlflow-artifacts``).\n\n    :param proxied_artifact_root: The Run artifact root location (URI) with scheme ``http``,\n                                  ``https``, or ``mlflow-artifacts`` that can be resolved by the\n                                  MLflow server to a concrete storage location.\n    :param relative_path: The relative path within the specified ``proxied_artifact_root`` under\n                          which to list artifact contents. If ``None``, artifacts are listed from\n                          the ``proxied_artifact_root`` directory.\n    \"\"\"\n    parsed_proxied_artifact_root = urllib.parse.urlparse(proxied_artifact_root)\n    assert parsed_proxied_artifact_root.scheme in [\"http\", \"https\", \"mlflow-artifacts\"]\n\n    artifact_destination_repo = _get_artifact_repo_mlflow_artifacts()\n    artifact_destination_path = _get_proxied_run_artifact_destination_path(\n        proxied_artifact_root=proxied_artifact_root,\n        relative_path=relative_path,\n    )\n\n    artifact_entities = []\n    for file_info in artifact_destination_repo.list_artifacts(artifact_destination_path):\n        basename = posixpath.basename(file_info.path)\n        run_relative_artifact_path = (\n            posixpath.join(relative_path, basename) if relative_path else basename\n        )\n        artifact_entities.append(\n            FileInfo(run_relative_artifact_path, file_info.is_dir, file_info.file_size)\n        )\n\n    return artifact_entities\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_metric_history():\n    request_message = _get_request_message(\n        GetMetricHistory(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"metric_key\": [_assert_string, _assert_required],\n        },\n    )\n    response_message = GetMetricHistory.Response()\n    run_id = request_message.run_id or request_message.run_uuid\n    metric_entities = _get_tracking_store().get_metric_history(run_id, request_message.metric_key)\n    response_message.metrics.extend([m.to_proto() for m in metric_entities])\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef get_metric_history_bulk_handler():\n    MAX_HISTORY_RESULTS = 25000\n    MAX_RUN_IDS_PER_REQUEST = 20\n    run_ids = request.args.to_dict(flat=False).get(\"run_id\", [])\n    if not run_ids:\n        raise MlflowException(\n            message=\"GetMetricHistoryBulk request must specify at least one run_id.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    if len(run_ids) > MAX_RUN_IDS_PER_REQUEST:\n        raise MlflowException(\n            message=(\n                f\"GetMetricHistoryBulk request cannot specify more than {MAX_RUN_IDS_PER_REQUEST}\"\n                f\" run_ids. Received {len(run_ids)} run_ids.\"\n            ),\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    metric_key = request.args.get(\"metric_key\")\n    if metric_key is None:\n        raise MlflowException(\n            message=\"GetMetricHistoryBulk request must specify a metric_key.\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    max_results = int(request.args.get(\"max_results\", MAX_HISTORY_RESULTS))\n    max_results = min(max_results, MAX_HISTORY_RESULTS)\n\n    store = _get_tracking_store()\n\n    def _default_history_bulk_impl():\n        metrics_with_run_ids = []\n        for run_id in sorted(run_ids):\n            metrics_for_run = sorted(\n                store.get_metric_history(\n                    run_id=run_id,\n                    metric_key=metric_key,\n                    max_results=max_results,\n                ),\n                key=lambda metric: (metric.timestamp, metric.step, metric.value),\n            )\n            metrics_with_run_ids.extend(\n                [\n                    {\n                        \"key\": metric.key,\n                        \"value\": metric.value,\n                        \"timestamp\": metric.timestamp,\n                        \"step\": metric.step,\n                        \"run_id\": run_id,\n                    }\n                    for metric in metrics_for_run\n                ]\n            )\n        return metrics_with_run_ids\n\n    if hasattr(store, \"get_metric_history_bulk\"):\n        metrics_with_run_ids = [\n            metric.to_dict()\n            for metric in store.get_metric_history_bulk(\n                run_ids=run_ids,\n                metric_key=metric_key,\n                max_results=max_results,\n            )\n        ]\n    else:\n        metrics_with_run_ids = _default_history_bulk_impl()\n\n    return {\n        \"metrics\": metrics_with_run_ids[:max_results],\n    }\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_experiments():\n    request_message = _get_request_message(\n        SearchExperiments(),\n        schema={\n            \"view_type\": [_assert_intlike],\n            \"max_results\": [_assert_intlike],\n            \"order_by\": [_assert_array],\n            \"filter\": [_assert_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    experiment_entities = _get_tracking_store().search_experiments(\n        view_type=request_message.view_type,\n        max_results=request_message.max_results,\n        order_by=request_message.order_by,\n        filter_string=request_message.filter,\n        page_token=request_message.page_token,\n    )\n    response_message = SearchExperiments.Response()\n    response_message.experiments.extend([e.to_proto() for e in experiment_entities])\n    if experiment_entities.token:\n        response_message.next_page_token = experiment_entities.token\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\ndef _get_artifact_repo(run):\n    return get_artifact_repository(run.info.artifact_uri)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_batch():\n    def _assert_metrics_fields_present(metrics):\n        for m in metrics:\n            _assert_required(m[\"key\"])\n            _assert_required(m[\"value\"])\n            _assert_required(m[\"timestamp\"])\n            _assert_required(m[\"step\"])\n\n    def _assert_params_tags_fields_present(params_or_tags):\n        for param_or_tag in params_or_tags:\n            _assert_required(param_or_tag[\"key\"])\n\n    _validate_batch_log_api_req(_get_request_json())\n    request_message = _get_request_message(\n        LogBatch(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"metrics\": [_assert_array, _assert_metrics_fields_present],\n            \"params\": [_assert_array, _assert_params_tags_fields_present],\n            \"tags\": [_assert_array, _assert_params_tags_fields_present],\n        },\n    )\n    metrics = [Metric.from_proto(proto_metric) for proto_metric in request_message.metrics]\n    params = [Param.from_proto(proto_param) for proto_param in request_message.params]\n    tags = [RunTag.from_proto(proto_tag) for proto_tag in request_message.tags]\n    _get_tracking_store().log_batch(\n        run_id=request_message.run_id, metrics=metrics, params=params, tags=tags\n    )\n    response_message = LogBatch.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _log_model():\n    request_message = _get_request_message(\n        LogModel(),\n        schema={\n            \"run_id\": [_assert_string, _assert_required],\n            \"model_json\": [_assert_string, _assert_required],\n        },\n    )\n    try:\n        model = json.loads(request_message.model_json)\n    except Exception:\n        raise MlflowException(\n            \"Malformed model info. \\n {} \\n is not a valid JSON.\".format(\n                request_message.model_json\n            ),\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n\n    missing_fields = {\"artifact_path\", \"flavors\", \"utc_time_created\", \"run_id\"} - set(model.keys())\n\n    if missing_fields:\n        raise MlflowException(\n            f\"Model json is missing mandatory fields: {missing_fields}\",\n            error_code=INVALID_PARAMETER_VALUE,\n        )\n    _get_tracking_store().record_logged_model(\n        run_id=request_message.run_id, mlflow_model=Model.from_dict(model)\n    )\n    response_message = LogModel.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\ndef _wrap_response(response_message):\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n# Model Registry APIs\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_registered_model():\n    request_message = _get_request_message(\n        CreateRegisteredModel(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"tags\": [_assert_array],\n            \"description\": [_assert_string],\n        },\n    )\n    registered_model = _get_model_registry_store().create_registered_model(\n        name=request_message.name,\n        tags=request_message.tags,\n        description=request_message.description,\n    )\n    response_message = CreateRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_registered_model():\n    request_message = _get_request_message(\n        GetRegisteredModel(), schema={\"name\": [_assert_string, _assert_required]}\n    )\n    registered_model = _get_model_registry_store().get_registered_model(name=request_message.name)\n    response_message = GetRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_registered_model():\n    request_message = _get_request_message(\n        UpdateRegisteredModel(),\n        schema={\"name\": [_assert_string, _assert_required], \"description\": [_assert_string]},\n    )\n    name = request_message.name\n    new_description = request_message.description\n    registered_model = _get_model_registry_store().update_registered_model(\n        name=name, description=new_description\n    )\n    response_message = UpdateRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _rename_registered_model():\n    request_message = _get_request_message(\n        RenameRegisteredModel(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"new_name\": [_assert_string, _assert_required],\n        },\n    )\n    name = request_message.name\n    new_name = request_message.new_name\n    registered_model = _get_model_registry_store().rename_registered_model(\n        name=name, new_name=new_name\n    )\n    response_message = RenameRegisteredModel.Response(registered_model=registered_model.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_registered_model():\n    request_message = _get_request_message(\n        DeleteRegisteredModel(), schema={\"name\": [_assert_string, _assert_required]}\n    )\n    _get_model_registry_store().delete_registered_model(name=request_message.name)\n    return _wrap_response(DeleteRegisteredModel.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_registered_models():\n    request_message = _get_request_message(\n        SearchRegisteredModels(),\n        schema={\n            \"filter\": [_assert_string],\n            \"max_results\": [_assert_intlike, lambda x: _assert_less_than_or_equal(x, 1000)],\n            \"order_by\": [_assert_array, _assert_item_type_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    store = _get_model_registry_store()\n    registered_models = store.search_registered_models(\n        filter_string=request_message.filter,\n        max_results=request_message.max_results,\n        order_by=request_message.order_by,\n        page_token=request_message.page_token,\n    )\n    response_message = SearchRegisteredModels.Response()\n    response_message.registered_models.extend([e.to_proto() for e in registered_models])\n    if registered_models.token:\n        response_message.next_page_token = registered_models.token\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_latest_versions():\n    request_message = _get_request_message(\n        GetLatestVersions(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"stages\": [_assert_array, _assert_item_type_string],\n        },\n    )\n    latest_versions = _get_model_registry_store().get_latest_versions(\n        name=request_message.name, stages=request_message.stages\n    )\n    response_message = GetLatestVersions.Response()\n    response_message.model_versions.extend([e.to_proto() for e in latest_versions])\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_registered_model_tag():\n    request_message = _get_request_message(\n        SetRegisteredModelTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = RegisteredModelTag(key=request_message.key, value=request_message.value)\n    _get_model_registry_store().set_registered_model_tag(name=request_message.name, tag=tag)\n    return _wrap_response(SetRegisteredModelTag.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_registered_model_tag():\n    request_message = _get_request_message(\n        DeleteRegisteredModelTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_registered_model_tag(\n        name=request_message.name, key=request_message.key\n    )\n    return _wrap_response(DeleteRegisteredModelTag.Response())\n\n\ndef _validate_source(source: str, run_id: str) -> None:\n    if is_local_uri(source):\n        if run_id:\n            store = _get_tracking_store()\n            run = store.get_run(run_id)\n            source = pathlib.Path(local_file_uri_to_path(source)).resolve()\n            run_artifact_dir = pathlib.Path(local_file_uri_to_path(run.info.artifact_uri)).resolve()\n            if run_artifact_dir in [source, *source.parents]:\n                return\n\n        raise MlflowException(\n            f\"Invalid model version source: '{source}'. To use a local path as a model version \"\n            \"source, the run_id request parameter has to be specified and the local path has to be \"\n            \"contained within the artifact directory of the run specified by the run_id.\",\n            INVALID_PARAMETER_VALUE,\n        )\n\n    # There might be file URIs that are local but can bypass the above check. To prevent this, we\n    # disallow using file URIs as model version sources by default unless it's explicitly allowed\n    # by setting the MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE environment variable to True.\n    if not MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE.get() and is_file_uri(source):\n        raise MlflowException(\n            f\"Invalid model version source: '{source}'. MLflow tracking server doesn't allow using \"\n            \"a file URI as a model version source for security reasons. To disable this check, set \"\n            f\"the {MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE.name} environment variable to \"\n            \"True.\",\n            INVALID_PARAMETER_VALUE,\n        )\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _create_model_version():\n    request_message = _get_request_message(\n        CreateModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"source\": [_assert_string, _assert_required],\n            \"run_id\": [_assert_string],\n            \"tags\": [_assert_array],\n            \"run_link\": [_assert_string],\n            \"description\": [_assert_string],\n        },\n    )\n\n    _validate_source(request_message.source, request_message.run_id)\n\n    model_version = _get_model_registry_store().create_model_version(\n        name=request_message.name,\n        source=request_message.source,\n        run_id=request_message.run_id,\n        run_link=request_message.run_link,\n        tags=request_message.tags,\n        description=request_message.description,\n    )\n    response_message = CreateModelVersion.Response(model_version=model_version.to_proto())\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef get_model_version_artifact_handler():\n    from querystring_parser import parser\n\n    query_string = request.query_string.decode(\"utf-8\")\n    request_dict = parser.parse(query_string, normalized=True)\n    name = request_dict.get(\"name\")\n    version = request_dict.get(\"version\")\n    path = request_dict[\"path\"]\n    validate_path_is_safe(path)\n    artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n    if _is_servable_proxied_run_artifact_root(artifact_uri):\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_path = _get_proxied_run_artifact_destination_path(\n            proxied_artifact_root=artifact_uri,\n            relative_path=path,\n        )\n    else:\n        artifact_repo = get_artifact_repository(artifact_uri)\n        artifact_path = path\n\n    return _send_artifact(artifact_repo, artifact_path)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_model_version():\n    request_message = _get_request_message(\n        GetModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n        },\n    )\n    model_version = _get_model_registry_store().get_model_version(\n        name=request_message.name, version=request_message.version\n    )\n    response_proto = model_version.to_proto()\n    response_message = GetModelVersion.Response(model_version=response_proto)\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _update_model_version():\n    request_message = _get_request_message(\n        UpdateModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"description\": [_assert_string],\n        },\n    )\n    new_description = None\n    if request_message.HasField(\"description\"):\n        new_description = request_message.description\n    model_version = _get_model_registry_store().update_model_version(\n        name=request_message.name, version=request_message.version, description=new_description\n    )\n    return _wrap_response(UpdateModelVersion.Response(model_version=model_version.to_proto()))\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _transition_stage():\n    request_message = _get_request_message(\n        TransitionModelVersionStage(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"stage\": [_assert_string, _assert_required],\n            \"archive_existing_versions\": [_assert_bool],\n        },\n    )\n    model_version = _get_model_registry_store().transition_model_version_stage(\n        name=request_message.name,\n        version=request_message.version,\n        stage=request_message.stage,\n        archive_existing_versions=request_message.archive_existing_versions,\n    )\n    return _wrap_response(\n        TransitionModelVersionStage.Response(model_version=model_version.to_proto())\n    )\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_model_version():\n    request_message = _get_request_message(\n        DeleteModelVersion(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_model_version(\n        name=request_message.name, version=request_message.version\n    )\n    return _wrap_response(DeleteModelVersion.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_model_version_download_uri():\n    request_message = _get_request_message(GetModelVersionDownloadUri())\n    download_uri = _get_model_registry_store().get_model_version_download_uri(\n        name=request_message.name, version=request_message.version\n    )\n    response_message = GetModelVersionDownloadUri.Response(artifact_uri=download_uri)\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _search_model_versions():\n    request_message = _get_request_message(\n        SearchModelVersions(),\n        schema={\n            \"filter\": [_assert_string],\n            \"max_results\": [_assert_intlike, lambda x: _assert_less_than_or_equal(x, 200_000)],\n            \"order_by\": [_assert_array, _assert_item_type_string],\n            \"page_token\": [_assert_string],\n        },\n    )\n    store = _get_model_registry_store()\n    model_versions = store.search_model_versions(\n        filter_string=request_message.filter,\n        max_results=request_message.max_results,\n        order_by=request_message.order_by,\n        page_token=request_message.page_token,\n    )\n    response_message = SearchModelVersions.Response()\n    response_message.model_versions.extend([e.to_proto() for e in model_versions])\n    if model_versions.token:\n        response_message.next_page_token = model_versions.token\n    return _wrap_response(response_message)\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_model_version_tag():\n    request_message = _get_request_message(\n        SetModelVersionTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n            \"value\": [_assert_string],\n        },\n    )\n    tag = ModelVersionTag(key=request_message.key, value=request_message.value)\n    _get_model_registry_store().set_model_version_tag(\n        name=request_message.name, version=request_message.version, tag=tag\n    )\n    return _wrap_response(SetModelVersionTag.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_model_version_tag():\n    request_message = _get_request_message(\n        DeleteModelVersionTag(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n            \"key\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_model_version_tag(\n        name=request_message.name, version=request_message.version, key=request_message.key\n    )\n    return _wrap_response(DeleteModelVersionTag.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _set_registered_model_alias():\n    request_message = _get_request_message(\n        SetRegisteredModelAlias(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"alias\": [_assert_string, _assert_required],\n            \"version\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().set_registered_model_alias(\n        name=request_message.name, alias=request_message.alias, version=request_message.version\n    )\n    return _wrap_response(SetRegisteredModelAlias.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _delete_registered_model_alias():\n    request_message = _get_request_message(\n        DeleteRegisteredModelAlias(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"alias\": [_assert_string, _assert_required],\n        },\n    )\n    _get_model_registry_store().delete_registered_model_alias(\n        name=request_message.name, alias=request_message.alias\n    )\n    return _wrap_response(DeleteRegisteredModelAlias.Response())\n\n\n@catch_mlflow_exception\n@_disable_if_artifacts_only\ndef _get_model_version_by_alias():\n    request_message = _get_request_message(\n        GetModelVersionByAlias(),\n        schema={\n            \"name\": [_assert_string, _assert_required],\n            \"alias\": [_assert_string, _assert_required],\n        },\n    )\n    model_version = _get_model_registry_store().get_model_version_by_alias(\n        name=request_message.name, alias=request_message.alias\n    )\n    response_proto = model_version.to_proto()\n    response_message = GetModelVersionByAlias.Response(model_version=response_proto)\n    return _wrap_response(response_message)\n\n\n# MLflow Artifacts APIs\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _download_artifact(artifact_path):\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts/<artifact_path>` to download an artifact\n    from `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    tmp_dir = tempfile.TemporaryDirectory()\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    dst = artifact_repo.download_artifacts(artifact_path, tmp_dir.name)\n\n    # Ref: https://stackoverflow.com/a/24613980/6943581\n    file_handle = open(dst, \"rb\")\n\n    def stream_and_remove_file():\n        yield from file_handle\n        file_handle.close()\n        tmp_dir.cleanup()\n\n    file_sender_response = current_app.response_class(stream_and_remove_file())\n\n    return _response_with_file_attachment_headers(artifact_path, file_sender_response)\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _upload_artifact(artifact_path):\n    \"\"\"\n    A request handler for `PUT /mlflow-artifacts/artifacts/<artifact_path>` to upload an artifact\n    to `artifact_path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    head, tail = posixpath.split(artifact_path)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        tmp_path = os.path.join(tmp_dir, tail)\n        with open(tmp_path, \"wb\") as f:\n            chunk_size = 1024 * 1024  # 1 MB\n            while True:\n                chunk = request.stream.read(chunk_size)\n                if len(chunk) == 0:\n                    break\n                f.write(chunk)\n\n        artifact_repo = _get_artifact_repo_mlflow_artifacts()\n        artifact_repo.log_artifact(tmp_path, artifact_path=head or None)\n\n    return _wrap_response(UploadArtifact.Response())\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _list_artifacts_mlflow_artifacts():\n    \"\"\"\n    A request handler for `GET /mlflow-artifacts/artifacts?path=<value>` to list artifacts in `path`\n    (a relative path from the root artifact directory).\n    \"\"\"\n    request_message = _get_request_message(ListArtifactsMlflowArtifacts())\n    if request_message.HasField(\"path\"):\n        validate_path_is_safe(request_message.path)\n        path = request_message.path\n    else:\n        path = None\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    files = []\n    for file_info in artifact_repo.list_artifacts(path):\n        basename = posixpath.basename(file_info.path)\n        new_file_info = FileInfo(basename, file_info.is_dir, file_info.file_size)\n        files.append(new_file_info.to_proto())\n    response_message = ListArtifacts.Response()\n    response_message.files.extend(files)\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\n@catch_mlflow_exception\n@_disable_unless_serve_artifacts\ndef _delete_artifact_mlflow_artifacts(artifact_path):\n    \"\"\"\n    A request handler for `DELETE /mlflow-artifacts/artifacts?path=<value>` to delete artifacts in\n    `path` (a relative path from the root artifact directory).\n    \"\"\"\n    validate_path_is_safe(artifact_path)\n    _get_request_message(DeleteArtifact())\n    artifact_repo = _get_artifact_repo_mlflow_artifacts()\n    artifact_repo.delete_artifacts(artifact_path)\n    response_message = DeleteArtifact.Response()\n    response = Response(mimetype=\"application/json\")\n    response.set_data(message_to_json(response_message))\n    return response\n\n\ndef _add_static_prefix(route):\n    prefix = os.environ.get(STATIC_PREFIX_ENV_VAR)\n    if prefix:\n        return prefix + route\n    return route\n\n\ndef _get_paths(base_path):\n    \"\"\"\n    A service endpoints base path is typically something like /mlflow/experiment.\n    We should register paths like /api/2.0/mlflow/experiment and\n    /ajax-api/2.0/mlflow/experiment in the Flask router.\n    \"\"\"\n    return [f\"/api/2.0{base_path}\", _add_static_prefix(f\"/ajax-api/2.0{base_path}\")]\n\n\ndef get_handler(request_class):\n    \"\"\"\n    :param request_class: The type of protobuf message\n    :return:\n    \"\"\"\n    return HANDLERS.get(request_class, _not_implemented)\n\n\ndef get_endpoints():\n    \"\"\"\n    :return: List of tuples (path, handler, methods)\n    \"\"\"\n\n    def get_service_endpoints(service):\n        ret = []\n        for service_method in service.DESCRIPTOR.methods:\n            endpoints = service_method.GetOptions().Extensions[databricks_pb2.rpc].endpoints\n            for endpoint in endpoints:\n                for http_path in _get_paths(endpoint.path):\n                    handler = get_handler(service().GetRequestClass(service_method))\n                    ret.append((http_path, handler, [endpoint.method]))\n        return ret\n\n    return (\n        get_service_endpoints(MlflowService)\n        + get_service_endpoints(ModelRegistryService)\n        + get_service_endpoints(MlflowArtifactsService)\n    )\n\n\nHANDLERS = {\n    # Tracking Server APIs\n    CreateExperiment: _create_experiment,\n    GetExperiment: _get_experiment,\n    GetExperimentByName: _get_experiment_by_name,\n    DeleteExperiment: _delete_experiment,\n    RestoreExperiment: _restore_experiment,\n    UpdateExperiment: _update_experiment,\n    CreateRun: _create_run,\n    UpdateRun: _update_run,\n    DeleteRun: _delete_run,\n    RestoreRun: _restore_run,\n    LogParam: _log_param,\n    LogMetric: _log_metric,\n    SetExperimentTag: _set_experiment_tag,\n    SetTag: _set_tag,\n    DeleteTag: _delete_tag,\n    LogBatch: _log_batch,\n    LogModel: _log_model,\n    GetRun: _get_run,\n    SearchRuns: _search_runs,\n    ListArtifacts: _list_artifacts,\n    GetMetricHistory: _get_metric_history,\n    SearchExperiments: _search_experiments,\n    # Model Registry APIs\n    CreateRegisteredModel: _create_registered_model,\n    GetRegisteredModel: _get_registered_model,\n    DeleteRegisteredModel: _delete_registered_model,\n    UpdateRegisteredModel: _update_registered_model,\n    RenameRegisteredModel: _rename_registered_model,\n    SearchRegisteredModels: _search_registered_models,\n    GetLatestVersions: _get_latest_versions,\n    CreateModelVersion: _create_model_version,\n    GetModelVersion: _get_model_version,\n    DeleteModelVersion: _delete_model_version,\n    UpdateModelVersion: _update_model_version,\n    TransitionModelVersionStage: _transition_stage,\n    GetModelVersionDownloadUri: _get_model_version_download_uri,\n    SearchModelVersions: _search_model_versions,\n    SetRegisteredModelTag: _set_registered_model_tag,\n    DeleteRegisteredModelTag: _delete_registered_model_tag,\n    SetModelVersionTag: _set_model_version_tag,\n    DeleteModelVersionTag: _delete_model_version_tag,\n    SetRegisteredModelAlias: _set_registered_model_alias,\n    DeleteRegisteredModelAlias: _delete_registered_model_alias,\n    GetModelVersionByAlias: _get_model_version_by_alias,\n    # MLflow Artifacts APIs\n    DownloadArtifact: _download_artifact,\n    UploadArtifact: _upload_artifact,\n    ListArtifactsMlflowArtifacts: _list_artifacts_mlflow_artifacts,\n    DeleteArtifact: _delete_artifact_mlflow_artifacts,\n}\n", "import pathlib\nimport posixpath\nimport urllib.parse\nimport uuid\n\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE\nfrom mlflow.store.db.db_types import DATABASE_ENGINES\nfrom mlflow.utils.validation import _validate_db_type_string\nfrom mlflow.utils.os import is_windows\n\n_INVALID_DB_URI_MSG = (\n    \"Please refer to https://mlflow.org/docs/latest/tracking.html#storage for \"\n    \"format specifications.\"\n)\n\n_DBFS_FUSE_PREFIX = \"/dbfs/\"\n_DBFS_HDFS_URI_PREFIX = \"dbfs:/\"\n_DATABRICKS_UNITY_CATALOG_SCHEME = \"databricks-uc\"\n\n\ndef is_local_uri(uri):\n    \"\"\"Returns true if this is a local file path (/foo or file:/foo).\"\"\"\n    if uri == \"databricks\":\n        return False\n\n    if is_windows() and uri.startswith(\"\\\\\\\\\"):\n        # windows network drive path looks like: \"\\\\<server name>\\path\\...\"\n        return False\n\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.hostname and not (\n        parsed_uri.hostname == \".\"\n        or parsed_uri.hostname.startswith(\"localhost\")\n        or parsed_uri.hostname.startswith(\"127.0.0.1\")\n    ):\n        return False\n\n    scheme = parsed_uri.scheme\n    if scheme == \"\" or scheme == \"file\":\n        return True\n\n    if is_windows() and len(scheme) == 1 and scheme.lower() == pathlib.Path(uri).drive.lower()[0]:\n        return True\n\n    return False\n\n\ndef is_file_uri(uri):\n    return urllib.parse.urlparse(uri).scheme == \"file\"\n\n\ndef is_http_uri(uri):\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"http\" or scheme == \"https\"\n\n\ndef is_databricks_uri(uri):\n    \"\"\"\n    Databricks URIs look like 'databricks' (default profile) or 'databricks://profile'\n    or 'databricks://secret_scope:secret_key_prefix'.\n    \"\"\"\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == \"databricks\" or uri == \"databricks\"\n\n\ndef is_databricks_unity_catalog_uri(uri):\n    scheme = urllib.parse.urlparse(uri).scheme\n    return scheme == _DATABRICKS_UNITY_CATALOG_SCHEME or uri == _DATABRICKS_UNITY_CATALOG_SCHEME\n\n\ndef construct_db_uri_from_profile(profile):\n    if profile:\n        return \"databricks://\" + profile\n\n\n# Both scope and key_prefix should not contain special chars for URIs, like '/'\n# and ':'.\ndef validate_db_scope_prefix_info(scope, prefix):\n    for c in [\"/\", \":\", \" \"]:\n        if c in scope:\n            raise MlflowException(\n                \"Unsupported Databricks profile name: %s.\" % scope\n                + \" Profile names cannot contain '%s'.\" % c\n            )\n        if prefix and c in prefix:\n            raise MlflowException(\n                \"Unsupported Databricks profile key prefix: %s.\" % prefix\n                + \" Key prefixes cannot contain '%s'.\" % c\n            )\n    if prefix is not None and prefix.strip() == \"\":\n        raise MlflowException(\n            \"Unsupported Databricks profile key prefix: '%s'.\" % prefix\n            + \" Key prefixes cannot be empty.\"\n        )\n\n\ndef get_db_info_from_uri(uri):\n    \"\"\"\n    Get the Databricks profile specified by the tracking URI (if any), otherwise\n    returns None.\n    \"\"\"\n    parsed_uri = urllib.parse.urlparse(uri)\n    if parsed_uri.scheme == \"databricks\" or parsed_uri.scheme == _DATABRICKS_UNITY_CATALOG_SCHEME:\n        # netloc should not be an empty string unless URI is formatted incorrectly.\n        if parsed_uri.netloc == \"\":\n            raise MlflowException(\n                \"URI is formatted incorrectly: no netloc in URI '%s'.\" % uri\n                + \" This may be the case if there is only one slash in the URI.\"\n            )\n        profile_tokens = parsed_uri.netloc.split(\":\")\n        parsed_scope = profile_tokens[0]\n        if len(profile_tokens) == 1:\n            parsed_key_prefix = None\n        elif len(profile_tokens) == 2:\n            parsed_key_prefix = profile_tokens[1]\n        else:\n            # parse the content before the first colon as the profile.\n            parsed_key_prefix = \":\".join(profile_tokens[1:])\n        validate_db_scope_prefix_info(parsed_scope, parsed_key_prefix)\n        return parsed_scope, parsed_key_prefix\n    return None, None\n\n\ndef get_databricks_profile_uri_from_artifact_uri(uri, result_scheme=\"databricks\"):\n    \"\"\"\n    Retrieves the netloc portion of the URI as a ``databricks://`` or `databricks-uc://` URI,\n    if it is a proper Databricks profile specification, e.g.\n    ``profile@databricks`` or ``secret_scope:key_prefix@databricks``.\n    \"\"\"\n    parsed = urllib.parse.urlparse(uri)\n    if not parsed.netloc or parsed.hostname != result_scheme:\n        return None\n    if not parsed.username:  # no profile or scope:key\n        return result_scheme  # the default tracking/registry URI\n    validate_db_scope_prefix_info(parsed.username, parsed.password)\n    key_prefix = \":\" + parsed.password if parsed.password else \"\"\n    return f\"{result_scheme}://\" + parsed.username + key_prefix\n\n\ndef remove_databricks_profile_info_from_artifact_uri(artifact_uri):\n    \"\"\"\n    Only removes the netloc portion of the URI if it is a Databricks\n    profile specification, e.g.\n    ``profile@databricks`` or ``secret_scope:key_prefix@databricks``.\n    \"\"\"\n    parsed = urllib.parse.urlparse(artifact_uri)\n    if not parsed.netloc or parsed.hostname != \"databricks\":\n        return artifact_uri\n    return urllib.parse.urlunparse(parsed._replace(netloc=\"\"))\n\n\ndef add_databricks_profile_info_to_artifact_uri(artifact_uri, databricks_profile_uri):\n    \"\"\"\n    Throws an exception if ``databricks_profile_uri`` is not valid.\n    \"\"\"\n    if not databricks_profile_uri or not is_databricks_uri(databricks_profile_uri):\n        return artifact_uri\n    artifact_uri_parsed = urllib.parse.urlparse(artifact_uri)\n    # Do not overwrite the authority section if there is already one\n    if artifact_uri_parsed.netloc:\n        return artifact_uri\n\n    scheme = artifact_uri_parsed.scheme\n    if scheme == \"dbfs\" or scheme == \"runs\" or scheme == \"models\":\n        if databricks_profile_uri == \"databricks\":\n            netloc = \"databricks\"\n        else:\n            (profile, key_prefix) = get_db_info_from_uri(databricks_profile_uri)\n            prefix = \":\" + key_prefix if key_prefix else \"\"\n            netloc = profile + prefix + \"@databricks\"\n        new_parsed = artifact_uri_parsed._replace(netloc=netloc)\n        return urllib.parse.urlunparse(new_parsed)\n    else:\n        return artifact_uri\n\n\ndef extract_db_type_from_uri(db_uri):\n    \"\"\"\n    Parse the specified DB URI to extract the database type. Confirm the database type is\n    supported. If a driver is specified, confirm it passes a plausible regex.\n    \"\"\"\n    scheme = urllib.parse.urlparse(db_uri).scheme\n    scheme_plus_count = scheme.count(\"+\")\n\n    if scheme_plus_count == 0:\n        db_type = scheme\n    elif scheme_plus_count == 1:\n        db_type, _ = scheme.split(\"+\")\n    else:\n        error_msg = f\"Invalid database URI: '{db_uri}'. {_INVALID_DB_URI_MSG}\"\n        raise MlflowException(error_msg, INVALID_PARAMETER_VALUE)\n\n    _validate_db_type_string(db_type)\n\n    return db_type\n\n\ndef get_uri_scheme(uri_or_path):\n    scheme = urllib.parse.urlparse(uri_or_path).scheme\n    if any(scheme.lower().startswith(db) for db in DATABASE_ENGINES):\n        return extract_db_type_from_uri(uri_or_path)\n    return scheme\n\n\ndef extract_and_normalize_path(uri):\n    parsed_uri_path = urllib.parse.urlparse(uri).path\n    normalized_path = posixpath.normpath(parsed_uri_path)\n    return normalized_path.lstrip(\"/\")\n\n\ndef append_to_uri_path(uri, *paths):\n    \"\"\"\n    Appends the specified POSIX `paths` to the path component of the specified `uri`.\n\n    :param uri: The input URI, represented as a string.\n    :param paths: The POSIX paths to append to the specified `uri`'s path component.\n    :return: A new URI with a path component consisting of the specified `paths` appended to\n             the path component of the specified `uri`.\n\n    >>> uri1 = \"s3://root/base/path?param=value\"\n    >>> uri1 = append_to_uri_path(uri1, \"some/subpath\", \"/anotherpath\")\n    >>> assert uri1 == \"s3://root/base/path/some/subpath/anotherpath?param=value\"\n    >>> uri2 = \"a/posix/path\"\n    >>> uri2 = append_to_uri_path(uri2, \"/some\", \"subpath\")\n    >>> assert uri2 == \"a/posixpath/some/subpath\"\n    \"\"\"\n    path = \"\"\n    for subpath in paths:\n        path = _join_posixpaths_and_append_absolute_suffixes(path, subpath)\n\n    parsed_uri = urllib.parse.urlparse(uri)\n    if len(parsed_uri.scheme) == 0:\n        # If the input URI does not define a scheme, we assume that it is a POSIX path\n        # and join it with the specified input paths\n        return _join_posixpaths_and_append_absolute_suffixes(uri, path)\n\n    prefix = \"\"\n    if not parsed_uri.path.startswith(\"/\"):\n        # For certain URI schemes (e.g., \"file:\"), urllib's unparse routine does\n        # not preserve the relative URI path component properly. In certain cases,\n        # urlunparse converts relative paths to absolute paths. We introduce this logic\n        # to circumvent urlunparse's erroneous conversion\n        prefix = parsed_uri.scheme + \":\"\n        parsed_uri = parsed_uri._replace(scheme=\"\")\n\n    new_uri_path = _join_posixpaths_and_append_absolute_suffixes(parsed_uri.path, path)\n    new_parsed_uri = parsed_uri._replace(path=new_uri_path)\n    return prefix + urllib.parse.urlunparse(new_parsed_uri)\n\n\ndef _join_posixpaths_and_append_absolute_suffixes(prefix_path, suffix_path):\n    \"\"\"\n    Joins the POSIX path `prefix_path` with the POSIX path `suffix_path`. Unlike posixpath.join(),\n    if `suffix_path` is an absolute path, it is appended to prefix_path.\n\n    >>> result1 = _join_posixpaths_and_append_absolute_suffixes(\"relpath1\", \"relpath2\")\n    >>> assert result1 == \"relpath1/relpath2\"\n    >>> result2 = _join_posixpaths_and_append_absolute_suffixes(\"relpath\", \"/absolutepath\")\n    >>> assert result2 == \"relpath/absolutepath\"\n    >>> result3 = _join_posixpaths_and_append_absolute_suffixes(\"/absolutepath\", \"relpath\")\n    >>> assert result3 == \"/absolutepath/relpath\"\n    >>> result4 = _join_posixpaths_and_append_absolute_suffixes(\"/absolutepath1\", \"/absolutepath2\")\n    >>> assert result4 == \"/absolutepath1/absolutepath2\"\n    \"\"\"\n    if len(prefix_path) == 0:\n        return suffix_path\n\n    # If the specified prefix path is non-empty, we must relativize the suffix path by removing\n    # the leading slash, if present. Otherwise, posixpath.join() would omit the prefix from the\n    # joined path\n    suffix_path = suffix_path.lstrip(posixpath.sep)\n    return posixpath.join(prefix_path, suffix_path)\n\n\ndef is_databricks_acled_artifacts_uri(artifact_uri):\n    _ACLED_ARTIFACT_URI = \"databricks/mlflow-tracking/\"\n    artifact_uri_path = extract_and_normalize_path(artifact_uri)\n    return artifact_uri_path.startswith(_ACLED_ARTIFACT_URI)\n\n\ndef is_databricks_model_registry_artifacts_uri(artifact_uri):\n    _MODEL_REGISTRY_ARTIFACT_URI = \"databricks/mlflow-registry/\"\n    artifact_uri_path = extract_and_normalize_path(artifact_uri)\n    return artifact_uri_path.startswith(_MODEL_REGISTRY_ARTIFACT_URI)\n\n\ndef is_valid_dbfs_uri(uri):\n    parsed = urllib.parse.urlparse(uri)\n    if parsed.scheme != \"dbfs\":\n        return False\n    try:\n        db_profile_uri = get_databricks_profile_uri_from_artifact_uri(uri)\n    except MlflowException:\n        db_profile_uri = None\n    return not parsed.netloc or db_profile_uri is not None\n\n\ndef dbfs_hdfs_uri_to_fuse_path(dbfs_uri):\n    \"\"\"\n    Converts the provided DBFS URI into a DBFS FUSE path\n    :param dbfs_uri: A DBFS URI like \"dbfs:/my-directory\". Can also be a scheme-less URI like\n                     \"/my-directory\" if running in an environment where the default HDFS filesystem\n                     is \"dbfs:/\" (e.g. Databricks)\n    :return A DBFS FUSE-style path, e.g. \"/dbfs/my-directory\"\n    \"\"\"\n    if not is_valid_dbfs_uri(dbfs_uri) and dbfs_uri == posixpath.abspath(dbfs_uri):\n        # Convert posixpaths (e.g. \"/tmp/mlflow\") to DBFS URIs by adding \"dbfs:/\" as a prefix\n        dbfs_uri = \"dbfs:\" + dbfs_uri\n    if not dbfs_uri.startswith(_DBFS_HDFS_URI_PREFIX):\n        raise MlflowException(\n            \"Path '%s' did not start with expected DBFS URI prefix '%s'\"\n            % (dbfs_uri, _DBFS_HDFS_URI_PREFIX),\n        )\n\n    return _DBFS_FUSE_PREFIX + dbfs_uri[len(_DBFS_HDFS_URI_PREFIX) :]\n\n\ndef resolve_uri_if_local(local_uri):\n    \"\"\"\n    if `local_uri` is passed in as a relative local path, this function\n    resolves it to absolute path relative to current working directory.\n\n    :param local_uri: Relative or absolute path or local file uri\n\n    :return: a fully-formed absolute uri path or an absolute filesystem path\n    \"\"\"\n    from mlflow.utils.file_utils import local_file_uri_to_path\n\n    if local_uri is not None and is_local_uri(local_uri):\n        scheme = get_uri_scheme(local_uri)\n        cwd = pathlib.Path.cwd()\n        local_path = local_file_uri_to_path(local_uri)\n        if not pathlib.Path(local_path).is_absolute():\n            if scheme == \"\":\n                if is_windows():\n                    return urllib.parse.urlunsplit(\n                        (\n                            \"file\",\n                            None,\n                            cwd.joinpath(local_path).as_posix(),\n                            None,\n                            None,\n                        )\n                    )\n                return cwd.joinpath(local_path).as_posix()\n            local_uri_split = urllib.parse.urlsplit(local_uri)\n            resolved_absolute_uri = urllib.parse.urlunsplit(\n                (\n                    local_uri_split.scheme,\n                    None,\n                    cwd.joinpath(local_path).as_posix(),\n                    local_uri_split.query,\n                    local_uri_split.fragment,\n                )\n            )\n            return resolved_absolute_uri\n    return local_uri\n\n\ndef generate_tmp_dfs_path(dfs_tmp):\n    return posixpath.join(dfs_tmp, str(uuid.uuid4()))\n", "from subprocess import Popen\n\nimport sys\nimport os\nimport logging\nimport socket\nimport time\n\nimport mlflow\nfrom mlflow.server import BACKEND_STORE_URI_ENV_VAR, ARTIFACT_ROOT_ENV_VAR\nfrom tests.helper_functions import LOCALHOST, get_safe_port\n\n_logger = logging.getLogger(__name__)\n\n\ndef _await_server_up_or_die(port, timeout=10):\n    \"\"\"Waits until the local flask server is listening on the given port.\"\"\"\n    _logger.info(f\"Awaiting server to be up on {LOCALHOST}:{port}\")\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n            sock.settimeout(2)\n            if sock.connect_ex((LOCALHOST, port)) == 0:\n                _logger.info(f\"Server is up on {LOCALHOST}:{port}!\")\n                break\n        _logger.info(\"Server not yet up, waiting...\")\n        time.sleep(0.5)\n    else:\n        raise Exception(f\"Failed to connect on {LOCALHOST}:{port} within {timeout} seconds\")\n\n\n# NB: We explicitly wait and timeout on server shutdown in order to ensure that pytest output\n# reveals the cause in the event of a test hang due to the subprocess not exiting.\ndef _terminate_server(process, timeout=10):\n    \"\"\"Waits until the local flask server process is terminated.\"\"\"\n    _logger.info(\"Terminating server...\")\n    process.terminate()\n    process.wait(timeout=timeout)\n\n\ndef _init_server(backend_uri, root_artifact_uri, extra_env=None):\n    \"\"\"\n    Launch a new REST server using the tracking store specified by backend_uri and root artifact\n    directory specified by root_artifact_uri.\n    :returns A tuple (url, process) containing the string URL of the server and a handle to the\n             server process (a multiprocessing.Process object).\n    \"\"\"\n    mlflow.set_tracking_uri(None)\n    server_port = get_safe_port()\n    process = Popen(\n        [\n            sys.executable,\n            \"-c\",\n            f'from mlflow.server import app; app.run(\"{LOCALHOST}\", {server_port})',\n        ],\n        env={\n            **os.environ,\n            BACKEND_STORE_URI_ENV_VAR: backend_uri,\n            ARTIFACT_ROOT_ENV_VAR: root_artifact_uri,\n            **(extra_env or {}),\n        },\n    )\n\n    _await_server_up_or_die(server_port)\n    url = f\"http://{LOCALHOST}:{server_port}\"\n    _logger.info(f\"Launching tracking server against backend URI {backend_uri}. Server URL: {url}\")\n    return url, process\n\n\ndef _send_rest_tracking_post_request(tracking_server_uri, api_path, json_payload):\n    \"\"\"\n    Make a POST request to the specified MLflow Tracking API and retrieve the\n    corresponding `requests.Response` object\n    \"\"\"\n    import requests\n\n    url = tracking_server_uri + api_path\n    response = requests.post(url, json=json_payload)\n    return response\n", "\"\"\"\nIntegration test which starts a local Tracking Server on an ephemeral port,\nand ensures we can use the tracking API to communicate with it.\n\"\"\"\nimport pathlib\n\nimport flask\nimport json\nimport os\nimport sys\nimport posixpath\nimport logging\nimport tempfile\nimport time\nimport urllib.parse\nimport requests\nfrom unittest import mock\n\nimport pytest\n\nfrom mlflow import MlflowClient\nfrom mlflow.artifacts import download_artifacts\nimport mlflow.experiments\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.entities import Metric, Param, RunTag, ViewType\nfrom mlflow.models import Model\nimport mlflow.pyfunc\nfrom mlflow.server.handlers import validate_path_is_safe\nfrom mlflow.store.tracking.sqlalchemy_store import SqlAlchemyStore\nfrom mlflow.utils.file_utils import TempDir\nfrom mlflow.utils.mlflow_tags import (\n    MLFLOW_USER,\n    MLFLOW_PARENT_RUN_ID,\n    MLFLOW_SOURCE_TYPE,\n    MLFLOW_SOURCE_NAME,\n    MLFLOW_PROJECT_ENTRY_POINT,\n    MLFLOW_GIT_COMMIT,\n)\nfrom mlflow.utils.file_utils import path_to_local_file_uri\nfrom mlflow.utils.time_utils import get_current_time_millis\nfrom mlflow.utils.os import is_windows\n\nfrom tests.integration.utils import invoke_cli_runner\nfrom tests.tracking.integration_test_utils import (\n    _terminate_server,\n    _init_server,\n    _send_rest_tracking_post_request,\n)\n\n\n_logger = logging.getLogger(__name__)\n\n\n@pytest.fixture(params=[\"file\", \"sqlalchemy\"])\ndef mlflow_client(request, tmp_path):\n    \"\"\"Provides an MLflow Tracking API client pointed at the local tracking server.\"\"\"\n    if request.param == \"file\":\n        backend_uri = tmp_path.joinpath(\"file\").as_uri()\n    elif request.param == \"sqlalchemy\":\n        path = tmp_path.joinpath(\"sqlalchemy.db\").as_uri()\n        backend_uri = (\"sqlite://\" if sys.platform == \"win32\" else \"sqlite:////\") + path[\n            len(\"file://\") :\n        ]\n\n    url, process = _init_server(backend_uri, root_artifact_uri=tmp_path.as_uri())\n    yield MlflowClient(url)\n\n    _terminate_server(process)\n\n\n@pytest.fixture()\ndef cli_env(mlflow_client):\n    \"\"\"Provides an environment for the MLflow CLI pointed at the local tracking server.\"\"\"\n    cli_env = {\n        \"LC_ALL\": \"en_US.UTF-8\",\n        \"LANG\": \"en_US.UTF-8\",\n        \"MLFLOW_TRACKING_URI\": mlflow_client.tracking_uri,\n    }\n    return cli_env\n\n\ndef create_experiments(client, names):\n    return [client.create_experiment(n) for n in names]\n\n\ndef test_create_get_search_experiment(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        \"My Experiment\", artifact_location=\"my_location\", tags={\"key1\": \"val1\", \"key2\": \"val2\"}\n    )\n    exp = mlflow_client.get_experiment(experiment_id)\n    assert exp.name == \"My Experiment\"\n    if is_windows():\n        assert exp.artifact_location == pathlib.Path.cwd().joinpath(\"my_location\").as_uri()\n    else:\n        assert exp.artifact_location == str(pathlib.Path.cwd().joinpath(\"my_location\"))\n    assert len(exp.tags) == 2\n    assert exp.tags[\"key1\"] == \"val1\"\n    assert exp.tags[\"key2\"] == \"val2\"\n\n    experiments = mlflow_client.search_experiments()\n    assert {e.name for e in experiments} == {\"My Experiment\", \"Default\"}\n    mlflow_client.delete_experiment(experiment_id)\n    assert {e.name for e in mlflow_client.search_experiments()} == {\"Default\"}\n    assert {e.name for e in mlflow_client.search_experiments(view_type=ViewType.ACTIVE_ONLY)} == {\n        \"Default\"\n    }\n    assert {e.name for e in mlflow_client.search_experiments(view_type=ViewType.DELETED_ONLY)} == {\n        \"My Experiment\"\n    }\n    assert {e.name for e in mlflow_client.search_experiments(view_type=ViewType.ALL)} == {\n        \"My Experiment\",\n        \"Default\",\n    }\n    active_exps_paginated = mlflow_client.search_experiments(max_results=1)\n    assert {e.name for e in active_exps_paginated} == {\"Default\"}\n    assert active_exps_paginated.token is None\n\n    all_exps_paginated = mlflow_client.search_experiments(max_results=1, view_type=ViewType.ALL)\n    first_page_names = {e.name for e in all_exps_paginated}\n    all_exps_second_page = mlflow_client.search_experiments(\n        max_results=1, view_type=ViewType.ALL, page_token=all_exps_paginated.token\n    )\n    second_page_names = {e.name for e in all_exps_second_page}\n    assert len(first_page_names) == 1\n    assert len(second_page_names) == 1\n    assert first_page_names.union(second_page_names) == {\"Default\", \"My Experiment\"}\n\n\ndef test_create_experiment_validation(mlflow_client):\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/experiments/create\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"name\": 123,\n        },\n        \"Invalid value 123 for parameter 'name'\",\n    )\n    assert_bad_request({}, \"Missing value for required parameter 'name'\")\n    assert_bad_request(\n        {\n            \"name\": \"experiment name\",\n            \"artifact_location\": 9.0,\n            \"tags\": [{\"key\": \"key\", \"value\": \"value\"}],\n        },\n        \"Invalid value 9.0 for parameter 'artifact_location'\",\n    )\n    assert_bad_request(\n        {\n            \"name\": \"experiment name\",\n            \"artifact_location\": \"my_location\",\n            \"tags\": \"5\",\n        },\n        \"Invalid value 5 for parameter 'tags'\",\n    )\n\n\ndef test_delete_restore_experiment(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Deleterious\")\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n    mlflow_client.delete_experiment(experiment_id)\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"deleted\"\n    mlflow_client.restore_experiment(experiment_id)\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n\n\ndef test_delete_restore_experiment_cli(mlflow_client, cli_env):\n    experiment_name = \"DeleteriousCLI\"\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"create\", \"--experiment-name\", experiment_name], env=cli_env\n    )\n    experiment_id = mlflow_client.get_experiment_by_name(experiment_name).experiment_id\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"delete\", \"-x\", str(experiment_id)], env=cli_env\n    )\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"deleted\"\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"restore\", \"-x\", str(experiment_id)], env=cli_env\n    )\n    assert mlflow_client.get_experiment(experiment_id).lifecycle_stage == \"active\"\n\n\ndef test_rename_experiment(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"BadName\")\n    assert mlflow_client.get_experiment(experiment_id).name == \"BadName\"\n    mlflow_client.rename_experiment(experiment_id, \"GoodName\")\n    assert mlflow_client.get_experiment(experiment_id).name == \"GoodName\"\n\n\ndef test_rename_experiment_cli(mlflow_client, cli_env):\n    bad_experiment_name = \"CLIBadName\"\n    good_experiment_name = \"CLIGoodName\"\n\n    invoke_cli_runner(\n        mlflow.experiments.commands, [\"create\", \"-n\", bad_experiment_name], env=cli_env\n    )\n    experiment_id = mlflow_client.get_experiment_by_name(bad_experiment_name).experiment_id\n    assert mlflow_client.get_experiment(experiment_id).name == bad_experiment_name\n    invoke_cli_runner(\n        mlflow.experiments.commands,\n        [\"rename\", \"--experiment-id\", str(experiment_id), \"--new-name\", good_experiment_name],\n        env=cli_env,\n    )\n    assert mlflow_client.get_experiment(experiment_id).name == good_experiment_name\n\n\n@pytest.mark.parametrize(\"parent_run_id_kwarg\", [None, \"my-parent-id\"])\ndef test_create_run_all_args(mlflow_client, parent_run_id_kwarg):\n    user = \"username\"\n    source_name = \"Hello\"\n    entry_point = \"entry\"\n    source_version = \"abc\"\n    create_run_kwargs = {\n        \"start_time\": 456,\n        \"run_name\": \"my name\",\n        \"tags\": {\n            MLFLOW_USER: user,\n            MLFLOW_SOURCE_TYPE: \"LOCAL\",\n            MLFLOW_SOURCE_NAME: source_name,\n            MLFLOW_PROJECT_ENTRY_POINT: entry_point,\n            MLFLOW_GIT_COMMIT: source_version,\n            MLFLOW_PARENT_RUN_ID: \"7\",\n            \"my\": \"tag\",\n            \"other\": \"tag\",\n        },\n    }\n    experiment_id = mlflow_client.create_experiment(\n        \"Run A Lot (parent_run_id=%s)\" % (parent_run_id_kwarg)\n    )\n    created_run = mlflow_client.create_run(experiment_id, **create_run_kwargs)\n    run_id = created_run.info.run_id\n    _logger.info(f\"Run id={run_id}\")\n    fetched_run = mlflow_client.get_run(run_id)\n    for run in [created_run, fetched_run]:\n        assert run.info.run_id == run_id\n        assert run.info.run_uuid == run_id\n        assert run.info.experiment_id == experiment_id\n        assert run.info.user_id == user\n        assert run.info.start_time == create_run_kwargs[\"start_time\"]\n        assert run.info.run_name == \"my name\"\n        for tag in create_run_kwargs[\"tags\"]:\n            assert tag in run.data.tags\n        assert run.data.tags.get(MLFLOW_USER) == user\n        assert run.data.tags.get(MLFLOW_PARENT_RUN_ID) == parent_run_id_kwarg or \"7\"\n        assert [run.info for run in mlflow_client.search_runs([experiment_id])] == [run.info]\n\n\ndef test_create_run_defaults(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Run A Little\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    run = mlflow_client.get_run(run_id)\n    assert run.info.run_id == run_id\n    assert run.info.experiment_id == experiment_id\n    assert run.info.user_id == \"unknown\"\n\n\ndef test_log_metrics_params_tags(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Oh My\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    mlflow_client.log_metric(run_id, key=\"metric\", value=123.456, timestamp=789, step=2)\n    mlflow_client.log_metric(run_id, key=\"nan_metric\", value=float(\"nan\"))\n    mlflow_client.log_metric(run_id, key=\"inf_metric\", value=float(\"inf\"))\n    mlflow_client.log_metric(run_id, key=\"-inf_metric\", value=-float(\"inf\"))\n    mlflow_client.log_metric(run_id, key=\"stepless-metric\", value=987.654, timestamp=321)\n    mlflow_client.log_param(run_id, \"param\", \"value\")\n    mlflow_client.set_tag(run_id, \"taggity\", \"do-dah\")\n    run = mlflow_client.get_run(run_id)\n    assert run.data.metrics.get(\"metric\") == 123.456\n    import math\n\n    assert math.isnan(run.data.metrics.get(\"nan_metric\"))\n    assert run.data.metrics.get(\"inf_metric\") >= 1.7976931348623157e308\n    assert run.data.metrics.get(\"-inf_metric\") <= -1.7976931348623157e308\n    assert run.data.metrics.get(\"stepless-metric\") == 987.654\n    assert run.data.params.get(\"param\") == \"value\"\n    assert run.data.tags.get(\"taggity\") == \"do-dah\"\n    metric_history0 = mlflow_client.get_metric_history(run_id, \"metric\")\n    assert len(metric_history0) == 1\n    metric0 = metric_history0[0]\n    assert metric0.key == \"metric\"\n    assert metric0.value == 123.456\n    assert metric0.timestamp == 789\n    assert metric0.step == 2\n    metric_history1 = mlflow_client.get_metric_history(run_id, \"stepless-metric\")\n    assert len(metric_history1) == 1\n    metric1 = metric_history1[0]\n    assert metric1.key == \"stepless-metric\"\n    assert metric1.value == 987.654\n    assert metric1.timestamp == 321\n    assert metric1.step == 0\n\n    metric_history = mlflow_client.get_metric_history(run_id, \"a_test_accuracy\")\n    assert metric_history == []\n\n\ndef test_log_metric_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"metrics validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/log-metric\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"run_id\": 31,\n            \"key\": \"metric\",\n            \"value\": 41,\n            \"timestamp\": 59,\n            \"step\": 26,\n        },\n        \"Invalid value 31 for parameter 'run_id' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": 31,\n            \"value\": 41,\n            \"timestamp\": 59,\n            \"step\": 26,\n        },\n        \"Invalid value 31 for parameter 'key' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": \"foo\",\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": \"foo\",\n        },\n        \"Invalid value foo for parameter 'step' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": \"foo\",\n            \"value\": 31,\n            \"timestamp\": \"foo\",\n            \"step\": 41,\n        },\n        \"Invalid value foo for parameter 'timestamp' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": None,\n            \"key\": \"foo\",\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": 41,\n        },\n        \"Missing value for required parameter 'run_id'\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            # Missing key\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": 41,\n        },\n        \"Missing value for required parameter 'key'\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": None,\n            \"value\": 31,\n            \"timestamp\": 59,\n            \"step\": 41,\n        },\n        \"Missing value for required parameter 'key'\",\n    )\n\n\ndef test_log_param_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"params validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/log-parameter\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"run_id\": 31,\n            \"key\": \"param\",\n            \"value\": 41,\n        },\n        \"Invalid value 31 for parameter 'run_id' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": 31,\n            \"value\": 41,\n        },\n        \"Invalid value 31 for parameter 'key' supplied\",\n    )\n\n\ndef test_log_param_with_empty_string_as_value(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_log_param_with_empty_string_as_value.__name__\n    )\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    mlflow_client.log_param(run_id, \"param_key\", \"\")\n    assert {\"param_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.params.items()\n\n\ndef test_set_tag_with_empty_string_as_value(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_set_tag_with_empty_string_as_value.__name__\n    )\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    mlflow_client.set_tag(run_id, \"tag_key\", \"\")\n    assert {\"tag_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.tags.items()\n\n\ndef test_log_batch_containing_params_and_tags_with_empty_string_values(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_log_batch_containing_params_and_tags_with_empty_string_values.__name__\n    )\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    mlflow_client.log_batch(\n        run_id=run_id,\n        params=[Param(\"param_key\", \"\")],\n        tags=[RunTag(\"tag_key\", \"\")],\n    )\n    assert {\"param_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.params.items()\n    assert {\"tag_key\": \"\"}.items() <= mlflow_client.get_run(run_id).data.tags.items()\n\n\ndef test_set_tag_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"tags validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/set-tag\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    assert_bad_request(\n        {\n            \"run_id\": 31,\n            \"key\": \"tag\",\n            \"value\": 41,\n        },\n        \"Invalid value 31 for parameter 'run_id' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            \"key\": \"param\",\n            \"value\": 41,\n        },\n        \"Invalid value 41 for parameter 'value' supplied\",\n    )\n    assert_bad_request(\n        {\n            \"run_id\": run_id,\n            # Missing key\n            \"value\": \"value\",\n        },\n        \"Missing value for required parameter 'key'\",\n    )\n\n    response = _send_rest_tracking_post_request(\n        mlflow_client.tracking_uri,\n        \"/api/2.0/mlflow/runs/set-tag\",\n        {\n            \"run_uuid\": run_id,\n            \"key\": \"key\",\n            \"value\": \"value\",\n        },\n    )\n    assert response.status_code == 200\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"path\",\n        \"path/\",\n        \"path/to/file\",\n    ],\n)\ndef test_validate_path_is_safe_good(path):\n    validate_path_is_safe(path)\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\n        \"/path\",\n        \"../path\",\n        \"../../path\",\n        \"./../path\",\n        \"path/../to/file\",\n        \"path/../../to/file\",\n    ],\n)\ndef test_validate_path_is_safe_bad(path):\n    with pytest.raises(MlflowException, match=\"Invalid path\"):\n        validate_path_is_safe(path)\n\n\ndef test_path_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"tags validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    invalid_path = \"../path\"\n\n    def assert_response(resp):\n        assert resp.status_code == 400\n        assert response.json() == {\n            \"error_code\": \"INVALID_PARAMETER_VALUE\",\n            \"message\": f\"Invalid path: {invalid_path}\",\n        }\n\n    response = requests.get(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/artifacts/list\",\n        params={\"run_id\": run_id, \"path\": invalid_path},\n    )\n    assert_response(response)\n\n    response = requests.get(\n        f\"{mlflow_client.tracking_uri}/get-artifact\",\n        params={\"run_id\": run_id, \"path\": invalid_path},\n    )\n    assert_response(response)\n\n    response = requests.get(\n        f\"{mlflow_client.tracking_uri}//model-versions/get-artifact\",\n        params={\"name\": \"model\", \"version\": 1, \"path\": invalid_path},\n    )\n    assert_response(response)\n\n\ndef test_set_experiment_tag(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"SetExperimentTagTest\")\n    mlflow_client.set_experiment_tag(experiment_id, \"dataset\", \"imagenet1K\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    assert \"dataset\" in experiment.tags and experiment.tags[\"dataset\"] == \"imagenet1K\"\n    # test that updating a tag works\n    mlflow_client.set_experiment_tag(experiment_id, \"dataset\", \"birdbike\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    assert \"dataset\" in experiment.tags and experiment.tags[\"dataset\"] == \"birdbike\"\n    # test that setting a tag on 1 experiment does not impact another experiment.\n    experiment_id_2 = mlflow_client.create_experiment(\"SetExperimentTagTest2\")\n    experiment2 = mlflow_client.get_experiment(experiment_id_2)\n    assert len(experiment2.tags) == 0\n    # test that setting a tag on different experiments maintain different values across experiments\n    mlflow_client.set_experiment_tag(experiment_id_2, \"dataset\", \"birds200\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    experiment2 = mlflow_client.get_experiment(experiment_id_2)\n    assert \"dataset\" in experiment.tags and experiment.tags[\"dataset\"] == \"birdbike\"\n    assert \"dataset\" in experiment2.tags and experiment2.tags[\"dataset\"] == \"birds200\"\n    # test can set multi-line tags\n    mlflow_client.set_experiment_tag(experiment_id, \"multiline tag\", \"value2\\nvalue2\\nvalue2\")\n    experiment = mlflow_client.get_experiment(experiment_id)\n    assert (\n        \"multiline tag\" in experiment.tags\n        and experiment.tags[\"multiline tag\"] == \"value2\\nvalue2\\nvalue2\"\n    )\n\n\ndef test_set_experiment_tag_with_empty_string_as_value(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\n        test_set_experiment_tag_with_empty_string_as_value.__name__\n    )\n    mlflow_client.set_experiment_tag(experiment_id, \"tag_key\", \"\")\n    assert {\"tag_key\": \"\"}.items() <= mlflow_client.get_experiment(experiment_id).tags.items()\n\n\ndef test_delete_tag(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"DeleteTagExperiment\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    mlflow_client.log_metric(run_id, key=\"metric\", value=123.456, timestamp=789, step=2)\n    mlflow_client.log_metric(run_id, key=\"stepless-metric\", value=987.654, timestamp=321)\n    mlflow_client.log_param(run_id, \"param\", \"value\")\n    mlflow_client.set_tag(run_id, \"taggity\", \"do-dah\")\n    run = mlflow_client.get_run(run_id)\n    assert \"taggity\" in run.data.tags and run.data.tags[\"taggity\"] == \"do-dah\"\n    mlflow_client.delete_tag(run_id, \"taggity\")\n    run = mlflow_client.get_run(run_id)\n    assert \"taggity\" not in run.data.tags\n    with pytest.raises(MlflowException, match=r\"Run .+ not found\"):\n        mlflow_client.delete_tag(\"fake_run_id\", \"taggity\")\n    with pytest.raises(MlflowException, match=\"No tag with name: fakeTag\"):\n        mlflow_client.delete_tag(run_id, \"fakeTag\")\n    mlflow_client.delete_run(run_id)\n    with pytest.raises(MlflowException, match=f\"The run {run_id} must be in\"):\n        mlflow_client.delete_tag(run_id, \"taggity\")\n\n\ndef test_log_batch(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Batch em up\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    mlflow_client.log_batch(\n        run_id=run_id,\n        metrics=[Metric(\"metric\", 123.456, 789, 3)],\n        params=[Param(\"param\", \"value\")],\n        tags=[RunTag(\"taggity\", \"do-dah\")],\n    )\n    run = mlflow_client.get_run(run_id)\n    assert run.data.metrics.get(\"metric\") == 123.456\n    assert run.data.params.get(\"param\") == \"value\"\n    assert run.data.tags.get(\"taggity\") == \"do-dah\"\n    metric_history = mlflow_client.get_metric_history(run_id, \"metric\")\n    assert len(metric_history) == 1\n    metric = metric_history[0]\n    assert metric.key == \"metric\"\n    assert metric.value == 123.456\n    assert metric.timestamp == 789\n    assert metric.step == 3\n\n\ndef test_log_batch_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"log_batch validation\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n\n    def assert_bad_request(payload, expected_error_message):\n        response = _send_rest_tracking_post_request(\n            mlflow_client.tracking_uri,\n            \"/api/2.0/mlflow/runs/log-batch\",\n            payload,\n        )\n        assert response.status_code == 400\n        assert expected_error_message in response.text\n\n    for request_parameter in [\"metrics\", \"params\", \"tags\"]:\n        assert_bad_request(\n            {\n                \"run_id\": run_id,\n                request_parameter: \"foo\",\n            },\n            f\"Invalid value foo for parameter '{request_parameter}' supplied\",\n        )\n\n\n@pytest.mark.allow_infer_pip_requirements_fallback\ndef test_log_model(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Log models\")\n    with TempDir(chdr=True):\n        model_paths = [f\"model/path/{i}\" for i in range(3)]\n        mlflow.set_tracking_uri(mlflow_client.tracking_uri)\n        with mlflow.start_run(experiment_id=experiment_id) as run:\n            for i, m in enumerate(model_paths):\n                mlflow.pyfunc.log_model(m, loader_module=\"mlflow.pyfunc\")\n                mlflow.pyfunc.save_model(\n                    m,\n                    mlflow_model=Model(artifact_path=m, run_id=run.info.run_id),\n                    loader_module=\"mlflow.pyfunc\",\n                )\n                model = Model.load(os.path.join(m, \"MLmodel\"))\n                run = mlflow.get_run(run.info.run_id)\n                tag = run.data.tags[\"mlflow.log-model.history\"]\n                models = json.loads(tag)\n                model.utc_time_created = models[i][\"utc_time_created\"]\n\n                history_model_meta = models[i].copy()\n                original_model_uuid = history_model_meta.pop(\"model_uuid\")\n                model_meta = model.to_dict().copy()\n                new_model_uuid = model_meta.pop(\"model_uuid\")\n                assert history_model_meta == model_meta\n                assert original_model_uuid != new_model_uuid\n                assert len(models) == i + 1\n                for j in range(0, i + 1):\n                    assert models[j][\"artifact_path\"] == model_paths[j]\n\n\ndef test_set_terminated_defaults(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Terminator 1\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    assert mlflow_client.get_run(run_id).info.status == \"RUNNING\"\n    assert mlflow_client.get_run(run_id).info.end_time is None\n    mlflow_client.set_terminated(run_id)\n    assert mlflow_client.get_run(run_id).info.status == \"FINISHED\"\n    assert mlflow_client.get_run(run_id).info.end_time <= get_current_time_millis()\n\n\ndef test_set_terminated_status(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"Terminator 2\")\n    created_run = mlflow_client.create_run(experiment_id)\n    run_id = created_run.info.run_id\n    assert mlflow_client.get_run(run_id).info.status == \"RUNNING\"\n    assert mlflow_client.get_run(run_id).info.end_time is None\n    mlflow_client.set_terminated(run_id, \"FAILED\")\n    assert mlflow_client.get_run(run_id).info.status == \"FAILED\"\n    assert mlflow_client.get_run(run_id).info.end_time <= get_current_time_millis()\n\n\ndef test_artifacts(mlflow_client, tmp_path):\n    experiment_id = mlflow_client.create_experiment(\"Art In Fact\")\n    experiment_info = mlflow_client.get_experiment(experiment_id)\n    assert experiment_info.artifact_location.startswith(path_to_local_file_uri(str(tmp_path)))\n    artifact_path = urllib.parse.urlparse(experiment_info.artifact_location).path\n    assert posixpath.split(artifact_path)[-1] == experiment_id\n\n    created_run = mlflow_client.create_run(experiment_id)\n    assert created_run.info.artifact_uri.startswith(experiment_info.artifact_location)\n    run_id = created_run.info.run_id\n    src_dir = tempfile.mkdtemp(\"test_artifacts_src\")\n    src_file = os.path.join(src_dir, \"my.file\")\n    with open(src_file, \"w\") as f:\n        f.write(\"Hello, World!\")\n    mlflow_client.log_artifact(run_id, src_file, None)\n    mlflow_client.log_artifacts(run_id, src_dir, \"dir\")\n\n    root_artifacts_list = mlflow_client.list_artifacts(run_id)\n    assert {a.path for a in root_artifacts_list} == {\"my.file\", \"dir\"}\n\n    dir_artifacts_list = mlflow_client.list_artifacts(run_id, \"dir\")\n    assert {a.path for a in dir_artifacts_list} == {\"dir/my.file\"}\n\n    all_artifacts = download_artifacts(\n        run_id=run_id, artifact_path=\".\", tracking_uri=mlflow_client.tracking_uri\n    )\n    assert open(\"%s/my.file\" % all_artifacts).read() == \"Hello, World!\"\n    assert open(\"%s/dir/my.file\" % all_artifacts).read() == \"Hello, World!\"\n\n    dir_artifacts = download_artifacts(\n        run_id=run_id, artifact_path=\"dir\", tracking_uri=mlflow_client.tracking_uri\n    )\n    assert open(\"%s/my.file\" % dir_artifacts).read() == \"Hello, World!\"\n\n\ndef test_search_pagination(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"search_pagination\")\n    runs = [mlflow_client.create_run(experiment_id, start_time=1).info.run_id for _ in range(0, 10)]\n    runs = sorted(runs)\n    result = mlflow_client.search_runs([experiment_id], max_results=4, page_token=None)\n    assert [r.info.run_id for r in result] == runs[0:4]\n    assert result.token is not None\n    result = mlflow_client.search_runs([experiment_id], max_results=4, page_token=result.token)\n    assert [r.info.run_id for r in result] == runs[4:8]\n    assert result.token is not None\n    result = mlflow_client.search_runs([experiment_id], max_results=4, page_token=result.token)\n    assert [r.info.run_id for r in result] == runs[8:]\n    assert result.token is None\n\n\ndef test_search_validation(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"search_validation\")\n    with pytest.raises(\n        MlflowException, match=r\"Invalid value 123456789 for parameter 'max_results' supplied\"\n    ):\n        mlflow_client.search_runs([experiment_id], max_results=123456789)\n\n\ndef test_get_experiment_by_name(mlflow_client):\n    name = \"test_get_experiment_by_name\"\n    experiment_id = mlflow_client.create_experiment(name)\n    res = mlflow_client.get_experiment_by_name(name)\n    assert res.experiment_id == experiment_id\n    assert res.name == name\n    assert mlflow_client.get_experiment_by_name(\"idontexist\") is None\n\n\ndef test_get_experiment(mlflow_client):\n    name = \"test_get_experiment\"\n    experiment_id = mlflow_client.create_experiment(name)\n    res = mlflow_client.get_experiment(experiment_id)\n    assert res.experiment_id == experiment_id\n    assert res.name == name\n\n\ndef test_search_experiments(mlflow_client):\n    # To ensure the default experiment and non-default experiments have different creation_time\n    # for deterministic search results, send a request to the server and initialize the tracking\n    # store.\n    assert mlflow_client.search_experiments()[0].name == \"Default\"\n\n    experiments = [\n        (\"a\", {\"key\": \"value\"}),\n        (\"ab\", {\"key\": \"vaLue\"}),\n        (\"Abc\", None),\n    ]\n    experiment_ids = []\n    for name, tags in experiments:\n        # sleep for windows file system current_time precision in Python to enforce\n        # deterministic ordering based on last_update_time (creation_time due to no\n        # mutation of experiment state)\n        time.sleep(0.001)\n        experiment_ids.append(mlflow_client.create_experiment(name, tags=tags))\n\n    # filter_string\n    experiments = mlflow_client.search_experiments(filter_string=\"attribute.name = 'a'\")\n    assert [e.name for e in experiments] == [\"a\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"attribute.name != 'a'\")\n    assert [e.name for e in experiments] == [\"Abc\", \"ab\", \"Default\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"name LIKE 'a%'\")\n    assert [e.name for e in experiments] == [\"ab\", \"a\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"tag.key = 'value'\")\n    assert [e.name for e in experiments] == [\"a\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"tag.key != 'value'\")\n    assert [e.name for e in experiments] == [\"ab\"]\n    experiments = mlflow_client.search_experiments(filter_string=\"tag.key ILIKE '%alu%'\")\n    assert [e.name for e in experiments] == [\"ab\", \"a\"]\n\n    # order_by\n    experiments = mlflow_client.search_experiments(order_by=[\"name DESC\"])\n    assert [e.name for e in experiments] == [\"ab\", \"a\", \"Default\", \"Abc\"]\n\n    # max_results\n    experiments = mlflow_client.search_experiments(max_results=2)\n    assert [e.name for e in experiments] == [\"Abc\", \"ab\"]\n    # page_token\n    experiments = mlflow_client.search_experiments(page_token=experiments.token)\n    assert [e.name for e in experiments] == [\"a\", \"Default\"]\n\n    # view_type\n    time.sleep(0.001)\n    mlflow_client.delete_experiment(experiment_ids[1])\n    experiments = mlflow_client.search_experiments(view_type=ViewType.ACTIVE_ONLY)\n    assert [e.name for e in experiments] == [\"Abc\", \"a\", \"Default\"]\n    experiments = mlflow_client.search_experiments(view_type=ViewType.DELETED_ONLY)\n    assert [e.name for e in experiments] == [\"ab\"]\n    experiments = mlflow_client.search_experiments(view_type=ViewType.ALL)\n    assert [e.name for e in experiments] == [\"Abc\", \"ab\", \"a\", \"Default\"]\n\n\ndef test_get_metric_history_bulk_rejects_invalid_requests(mlflow_client):\n    def assert_response(resp, message_part):\n        assert resp.status_code == 400\n        response_json = resp.json()\n        assert response_json.get(\"error_code\") == \"INVALID_PARAMETER_VALUE\"\n        assert message_part in response_json.get(\"message\", \"\")\n\n    response_no_run_ids_field = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"metric_key\": \"key\"},\n    )\n    assert_response(\n        response_no_run_ids_field,\n        \"GetMetricHistoryBulk request must specify at least one run_id\",\n    )\n\n    response_empty_run_ids = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [], \"metric_key\": \"key\"},\n    )\n    assert_response(\n        response_empty_run_ids,\n        \"GetMetricHistoryBulk request must specify at least one run_id\",\n    )\n\n    response_too_many_run_ids = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [f\"id_{i}\" for i in range(1000)], \"metric_key\": \"key\"},\n    )\n    assert_response(\n        response_too_many_run_ids,\n        \"GetMetricHistoryBulk request cannot specify more than\",\n    )\n\n    response_no_metric_key_field = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [\"123\"]},\n    )\n    assert_response(\n        response_no_metric_key_field,\n        \"GetMetricHistoryBulk request must specify a metric_key\",\n    )\n\n\ndef test_get_metric_history_bulk_returns_expected_metrics_in_expected_order(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"get metric history bulk\")\n    created_run1 = mlflow_client.create_run(experiment_id)\n    run_id1 = created_run1.info.run_id\n    created_run2 = mlflow_client.create_run(experiment_id)\n    run_id2 = created_run2.info.run_id\n    created_run3 = mlflow_client.create_run(experiment_id)\n    run_id3 = created_run3.info.run_id\n\n    metricA_history = [\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 2, \"value\": 10.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 11.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 12.0},\n        {\"key\": \"metricA\", \"timestamp\": 2, \"step\": 3, \"value\": 12.0},\n    ]\n    for metric in metricA_history:\n        mlflow_client.log_metric(run_id1, **metric)\n        metric_for_run2 = dict(metric)\n        metric_for_run2[\"value\"] += 1.0\n        mlflow_client.log_metric(run_id2, **metric_for_run2)\n\n    metricB_history = [\n        {\"key\": \"metricB\", \"timestamp\": 7, \"step\": -2, \"value\": -100.0},\n        {\"key\": \"metricB\", \"timestamp\": 8, \"step\": 0, \"value\": 0.0},\n        {\"key\": \"metricB\", \"timestamp\": 8, \"step\": 0, \"value\": 1.0},\n        {\"key\": \"metricB\", \"timestamp\": 9, \"step\": 1, \"value\": 12.0},\n    ]\n    for metric in metricB_history:\n        mlflow_client.log_metric(run_id1, **metric)\n        metric_for_run2 = dict(metric)\n        metric_for_run2[\"value\"] += 1.0\n        mlflow_client.log_metric(run_id2, **metric_for_run2)\n\n    response_run1_metricA = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id1], \"metric_key\": \"metricA\"},\n    )\n    assert response_run1_metricA.status_code == 200\n    assert response_run1_metricA.json().get(\"metrics\") == [\n        {**metric, \"run_id\": run_id1} for metric in metricA_history\n    ]\n\n    response_run2_metricB = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id2], \"metric_key\": \"metricB\"},\n    )\n    assert response_run2_metricB.status_code == 200\n    assert response_run2_metricB.json().get(\"metrics\") == [\n        {**metric, \"run_id\": run_id2, \"value\": metric[\"value\"] + 1.0} for metric in metricB_history\n    ]\n\n    response_run1_run2_metricA = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id1, run_id2], \"metric_key\": \"metricA\"},\n    )\n    assert response_run1_run2_metricA.status_code == 200\n    assert response_run1_run2_metricA.json().get(\"metrics\") == sorted(\n        [{**metric, \"run_id\": run_id1} for metric in metricA_history]\n        + [\n            {**metric, \"run_id\": run_id2, \"value\": metric[\"value\"] + 1.0}\n            for metric in metricA_history\n        ],\n        key=lambda metric: metric[\"run_id\"],\n    )\n\n    response_run1_run2_run_3_metricB = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id1, run_id2, run_id3], \"metric_key\": \"metricB\"},\n    )\n    assert response_run1_run2_run_3_metricB.status_code == 200\n    assert response_run1_run2_run_3_metricB.json().get(\"metrics\") == sorted(\n        [{**metric, \"run_id\": run_id1} for metric in metricB_history]\n        + [\n            {**metric, \"run_id\": run_id2, \"value\": metric[\"value\"] + 1.0}\n            for metric in metricB_history\n        ],\n        key=lambda metric: metric[\"run_id\"],\n    )\n\n\ndef test_get_metric_history_bulk_respects_max_results(mlflow_client):\n    experiment_id = mlflow_client.create_experiment(\"get metric history bulk\")\n    run_id = mlflow_client.create_run(experiment_id).info.run_id\n    max_results = 2\n\n    metricA_history = [\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 2, \"value\": 10.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 11.0},\n        {\"key\": \"metricA\", \"timestamp\": 1, \"step\": 3, \"value\": 12.0},\n        {\"key\": \"metricA\", \"timestamp\": 2, \"step\": 3, \"value\": 12.0},\n    ]\n    for metric in metricA_history:\n        mlflow_client.log_metric(run_id, **metric)\n\n    response_limited = requests.get(\n        f\"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/metrics/get-history-bulk\",\n        params={\"run_id\": [run_id], \"metric_key\": \"metricA\", \"max_results\": max_results},\n    )\n    assert response_limited.status_code == 200\n    assert response_limited.json().get(\"metrics\") == [\n        {**metric, \"run_id\": run_id} for metric in metricA_history[:max_results]\n    ]\n\n\ndef test_get_metric_history_bulk_calls_optimized_impl_when_expected(monkeypatch, tmp_path):\n    from mlflow.server.handlers import get_metric_history_bulk_handler\n\n    path = path_to_local_file_uri(str(tmp_path.joinpath(\"sqlalchemy.db\")))\n    uri = (\"sqlite://\" if sys.platform == \"win32\" else \"sqlite:////\") + path[len(\"file://\") :]\n    mock_store = mock.Mock(wraps=SqlAlchemyStore(uri, str(tmp_path)))\n\n    flask_app = flask.Flask(\"test_flask_app\")\n\n    class MockRequestArgs:\n        def __init__(self, args_dict):\n            self.args_dict = args_dict\n\n        def to_dict(self, flat):\n            return self.args_dict\n\n        def get(self, key, default=None):\n            return self.args_dict.get(key, default)\n\n    with mock.patch(\n        \"mlflow.server.handlers._get_tracking_store\", return_value=mock_store\n    ), flask_app.test_request_context() as mock_context:\n        run_ids = [str(i) for i in range(10)]\n        mock_context.request.args = MockRequestArgs(\n            {\n                \"run_id\": run_ids,\n                \"metric_key\": \"mock_key\",\n            }\n        )\n\n        get_metric_history_bulk_handler()\n\n        mock_store.get_metric_history_bulk.assert_called_once_with(\n            run_ids=run_ids,\n            metric_key=\"mock_key\",\n            max_results=25000,\n        )\n\n\ndef test_create_model_version_with_path_source(mlflow_client):\n    name = \"mode\"\n    mlflow_client.create_registered_model(name)\n    exp_id = mlflow_client.create_experiment(\"test\")\n    run = mlflow_client.create_run(experiment_id=exp_id)\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri[len(\"file://\") :],\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    # run_id is not specified\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri[len(\"file://\") :],\n        },\n    )\n    assert response.status_code == 400\n    assert \"To use a local path as a model version\" in response.json()[\"message\"]\n\n    # run_id is specified but source is not in the run's artifact directory\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": \"/tmp\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 400\n    assert \"To use a local path as a model version\" in response.json()[\"message\"]\n\n\ndef test_create_model_version_with_file_uri(mlflow_client):\n    name = \"test\"\n    mlflow_client.create_registered_model(name)\n    exp_id = mlflow_client.create_experiment(\"test\")\n    run = mlflow_client.create_run(experiment_id=exp_id)\n    assert run.info.artifact_uri.startswith(\"file://\")\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri,\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": f\"{run.info.artifact_uri}/model\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": f\"{run.info.artifact_uri}/.\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": f\"{run.info.artifact_uri}/model/..\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 200\n\n    # run_id is not specified\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": run.info.artifact_uri,\n        },\n    )\n    assert response.status_code == 400\n    assert \"To use a local path as a model version\" in response.json()[\"message\"]\n\n    # run_id is specified but source is not in the run's artifact directory\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": \"file:///tmp\",\n        },\n    )\n    assert response.status_code == 400\n    assert \"To use a local path as a model version\" in response.json()[\"message\"]\n\n    response = requests.post(\n        f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n        json={\n            \"name\": name,\n            \"source\": \"file://123.456.789.123/path/to/source\",\n            \"run_id\": run.info.run_id,\n        },\n    )\n    assert response.status_code == 400\n    assert \"MLflow tracking server doesn't allow\" in response.json()[\"message\"]\n\n\ndef test_create_model_version_with_file_uri_env_var(tmp_path):\n    backend_uri = tmp_path.joinpath(\"file\").as_uri()\n    url, process = _init_server(\n        backend_uri,\n        root_artifact_uri=tmp_path.as_uri(),\n        extra_env={\"MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE\": \"true\"},\n    )\n    try:\n        mlflow_client = MlflowClient(url)\n\n        name = \"test\"\n        mlflow_client.create_registered_model(name)\n        exp_id = mlflow_client.create_experiment(\"test\")\n        run = mlflow_client.create_run(experiment_id=exp_id)\n        response = requests.post(\n            f\"{mlflow_client.tracking_uri}/api/2.0/mlflow/model-versions/create\",\n            json={\n                \"name\": name,\n                \"source\": \"file://123.456.789.123/path/to/source\",\n                \"run_id\": run.info.run_id,\n            },\n        )\n        assert response.status_code == 200\n    finally:\n        _terminate_server(process)\n\n\ndef test_logging_model_with_local_artifact_uri(mlflow_client):\n    from sklearn.linear_model import LogisticRegression\n\n    mlflow.set_tracking_uri(mlflow_client.tracking_uri)\n    with mlflow.start_run() as run:\n        assert run.info.artifact_uri.startswith(\"file://\")\n        mlflow.sklearn.log_model(LogisticRegression(), \"model\", registered_model_name=\"rmn\")\n        mlflow.pyfunc.load_model(\"models:/rmn/1\")\n", "import pathlib\nimport posixpath\nimport pytest\n\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.store.db.db_types import DATABASE_ENGINES\nfrom mlflow.utils.uri import (\n    add_databricks_profile_info_to_artifact_uri,\n    append_to_uri_path,\n    extract_and_normalize_path,\n    extract_db_type_from_uri,\n    get_databricks_profile_uri_from_artifact_uri,\n    get_db_info_from_uri,\n    get_uri_scheme,\n    is_databricks_acled_artifacts_uri,\n    is_databricks_uri,\n    is_http_uri,\n    is_local_uri,\n    is_valid_dbfs_uri,\n    remove_databricks_profile_info_from_artifact_uri,\n    dbfs_hdfs_uri_to_fuse_path,\n    resolve_uri_if_local,\n)\nfrom mlflow.utils.os import is_windows\n\n\ndef test_extract_db_type_from_uri():\n    uri = \"{}://username:password@host:port/database\"\n    for legit_db in DATABASE_ENGINES:\n        assert legit_db == extract_db_type_from_uri(uri.format(legit_db))\n        assert legit_db == get_uri_scheme(uri.format(legit_db))\n\n        with_driver = legit_db + \"+driver-string\"\n        assert legit_db == extract_db_type_from_uri(uri.format(with_driver))\n        assert legit_db == get_uri_scheme(uri.format(with_driver))\n\n    for unsupported_db in [\"a\", \"aa\", \"sql\"]:\n        with pytest.raises(MlflowException, match=\"Invalid database engine\"):\n            extract_db_type_from_uri(unsupported_db)\n\n\n@pytest.mark.parametrize(\n    (\"server_uri\", \"result\"),\n    [\n        (\"databricks://aAbB\", (\"aAbB\", None)),\n        (\"databricks://aAbB/\", (\"aAbB\", None)),\n        (\"databricks://aAbB/path\", (\"aAbB\", None)),\n        (\"databricks://profile:prefix\", (\"profile\", \"prefix\")),\n        (\"databricks://profile:prefix/extra\", (\"profile\", \"prefix\")),\n        (\"nondatabricks://profile:prefix\", (None, None)),\n        (\"databricks://profile\", (\"profile\", None)),\n        (\"databricks://profile/\", (\"profile\", None)),\n        (\"databricks-uc://profile:prefix\", (\"profile\", \"prefix\")),\n        (\"databricks-uc://profile:prefix/extra\", (\"profile\", \"prefix\")),\n        (\"databricks-uc://profile\", (\"profile\", None)),\n        (\"databricks-uc://profile/\", (\"profile\", None)),\n    ],\n)\ndef test_get_db_info_from_uri(server_uri, result):\n    assert get_db_info_from_uri(server_uri) == result\n\n\n@pytest.mark.parametrize(\n    \"server_uri\",\n    [\"databricks:/profile:prefix\", \"databricks:/\", \"databricks://\"],\n)\ndef test_get_db_info_from_uri_errors_no_netloc(server_uri):\n    with pytest.raises(MlflowException, match=\"URI is formatted incorrectly\"):\n        get_db_info_from_uri(server_uri)\n\n\n@pytest.mark.parametrize(\n    \"server_uri\",\n    [\n        \"databricks://profile:prefix:extra\",\n        \"databricks://profile:prefix:extra  \",\n        \"databricks://profile:prefix extra\",\n        \"databricks://profile:prefix  \",\n        \"databricks://profile \",\n        \"databricks://profile:\",\n        \"databricks://profile: \",\n    ],\n)\ndef test_get_db_info_from_uri_errors_invalid_profile(server_uri):\n    with pytest.raises(MlflowException, match=\"Unsupported Databricks profile\"):\n        get_db_info_from_uri(server_uri)\n\n\ndef test_is_local_uri():\n    assert is_local_uri(\"mlruns\")\n    assert is_local_uri(\"./mlruns\")\n    assert is_local_uri(\"file:///foo/mlruns\")\n    assert is_local_uri(\"file:foo/mlruns\")\n    assert is_local_uri(\"file://./mlruns\")\n    assert is_local_uri(\"file://localhost/mlruns\")\n    assert is_local_uri(\"file://localhost:5000/mlruns\")\n    assert is_local_uri(\"file://127.0.0.1/mlruns\")\n    assert is_local_uri(\"file://127.0.0.1:5000/mlruns\")\n\n    assert not is_local_uri(\"file://myhostname/path/to/file\")\n    assert not is_local_uri(\"https://whatever\")\n    assert not is_local_uri(\"http://whatever\")\n    assert not is_local_uri(\"databricks\")\n    assert not is_local_uri(\"databricks:whatever\")\n    assert not is_local_uri(\"databricks://whatever\")\n\n\n@pytest.mark.skipif(not is_windows(), reason=\"Windows-only test\")\ndef test_is_local_uri_windows():\n    assert is_local_uri(\"C:\\\\foo\\\\mlruns\")\n    assert is_local_uri(\"C:/foo/mlruns\")\n    assert is_local_uri(\"file:///C:\\\\foo\\\\mlruns\")\n    assert not is_local_uri(\"\\\\\\\\server\\\\aa\\\\bb\")\n\n\ndef test_is_databricks_uri():\n    assert is_databricks_uri(\"databricks\")\n    assert is_databricks_uri(\"databricks:whatever\")\n    assert is_databricks_uri(\"databricks://whatever\")\n    assert not is_databricks_uri(\"mlruns\")\n    assert not is_databricks_uri(\"http://whatever\")\n\n\ndef test_is_http_uri():\n    assert is_http_uri(\"http://whatever\")\n    assert is_http_uri(\"https://whatever\")\n    assert not is_http_uri(\"file://whatever\")\n    assert not is_http_uri(\"databricks://whatever\")\n    assert not is_http_uri(\"mlruns\")\n\n\ndef validate_append_to_uri_path_test_cases(cases):\n    for input_uri, input_path, expected_output_uri in cases:\n        assert append_to_uri_path(input_uri, input_path) == expected_output_uri\n        assert append_to_uri_path(input_uri, *posixpath.split(input_path)) == expected_output_uri\n\n\ndef test_append_to_uri_path_joins_uri_paths_and_posixpaths_correctly():\n    validate_append_to_uri_path_test_cases(\n        [\n            (\"\", \"path\", \"path\"),\n            (\"\", \"/path\", \"/path\"),\n            (\"path\", \"\", \"path/\"),\n            (\"path\", \"subpath\", \"path/subpath\"),\n            (\"path/\", \"subpath\", \"path/subpath\"),\n            (\"path/\", \"/subpath\", \"path/subpath\"),\n            (\"path\", \"/subpath\", \"path/subpath\"),\n            (\"/path\", \"/subpath\", \"/path/subpath\"),\n            (\"//path\", \"/subpath\", \"//path/subpath\"),\n            (\"///path\", \"/subpath\", \"///path/subpath\"),\n            (\"/path\", \"/subpath/subdir\", \"/path/subpath/subdir\"),\n            (\"file:path\", \"\", \"file:path/\"),\n            (\"file:path/\", \"\", \"file:path/\"),\n            (\"file:path\", \"subpath\", \"file:path/subpath\"),\n            (\"file:path\", \"/subpath\", \"file:path/subpath\"),\n            (\"file:/\", \"\", \"file:///\"),\n            (\"file:/path\", \"/subpath\", \"file:///path/subpath\"),\n            (\"file:///\", \"\", \"file:///\"),\n            (\"file:///\", \"subpath\", \"file:///subpath\"),\n            (\"file:///path\", \"/subpath\", \"file:///path/subpath\"),\n            (\"file:///path/\", \"subpath\", \"file:///path/subpath\"),\n            (\"file:///path\", \"subpath\", \"file:///path/subpath\"),\n            (\"s3://\", \"\", \"s3:\"),\n            (\"s3://\", \"subpath\", \"s3:subpath\"),\n            (\"s3://\", \"/subpath\", \"s3:/subpath\"),\n            (\"s3://host\", \"subpath\", \"s3://host/subpath\"),\n            (\"s3://host\", \"/subpath\", \"s3://host/subpath\"),\n            (\"s3://host/\", \"subpath\", \"s3://host/subpath\"),\n            (\"s3://host/\", \"/subpath\", \"s3://host/subpath\"),\n            (\"s3://host\", \"subpath/subdir\", \"s3://host/subpath/subdir\"),\n        ]\n    )\n\n\ndef test_append_to_uri_path_handles_special_uri_characters_in_posixpaths():\n    \"\"\"\n    Certain characters are treated specially when parsing and interpreting URIs. However, in the\n    case where a URI input for `append_to_uri_path` is simply a POSIX path, these characters should\n    not receive special treatment. This test case verifies that `append_to_uri_path` properly joins\n    POSIX paths containing these characters.\n    \"\"\"\n\n    def create_char_case(special_char):\n        def char_case(*case_args):\n            return tuple([item.format(c=special_char) for item in case_args])\n\n        return char_case\n\n    for special_char in [\n        \".\",\n        \"-\",\n        \"+\",\n        \":\",\n        \"?\",\n        \"@\",\n        \"&\",\n        \"$\",\n        \"%\",\n        \"/\",\n        \"[\",\n        \"]\",\n        \"(\",\n        \")\",\n        \"*\",\n        \"'\",\n        \",\",\n    ]:\n        char_case = create_char_case(special_char)\n        validate_append_to_uri_path_test_cases(\n            [\n                char_case(\"\", \"{c}subpath\", \"{c}subpath\"),\n                char_case(\"\", \"/{c}subpath\", \"/{c}subpath\"),\n                char_case(\"dirwith{c}{c}chars\", \"\", \"dirwith{c}{c}chars/\"),\n                char_case(\"dirwith{c}{c}chars\", \"subpath\", \"dirwith{c}{c}chars/subpath\"),\n                char_case(\"{c}{c}charsdir\", \"\", \"{c}{c}charsdir/\"),\n                char_case(\"/{c}{c}charsdir\", \"\", \"/{c}{c}charsdir/\"),\n                char_case(\"/{c}{c}charsdir\", \"subpath\", \"/{c}{c}charsdir/subpath\"),\n                char_case(\"/{c}{c}charsdir\", \"subpath\", \"/{c}{c}charsdir/subpath\"),\n            ]\n        )\n\n    validate_append_to_uri_path_test_cases(\n        [\n            (\"#?charsdir:\", \":?subpath#\", \"#?charsdir:/:?subpath#\"),\n            (\"/#--+charsdir.//:\", \"/../:?subpath#\", \"/#--+charsdir.//:/../:?subpath#\"),\n            (\"$@''(,\", \")]*%\", \"$@''(,/)]*%\"),\n        ]\n    )\n\n\ndef test_append_to_uri_path_preserves_uri_schemes_hosts_queries_and_fragments():\n    validate_append_to_uri_path_test_cases(\n        [\n            (\"dbscheme+dbdriver:\", \"\", \"dbscheme+dbdriver:\"),\n            (\"dbscheme+dbdriver:\", \"subpath\", \"dbscheme+dbdriver:subpath\"),\n            (\"dbscheme+dbdriver:path\", \"subpath\", \"dbscheme+dbdriver:path/subpath\"),\n            (\"dbscheme+dbdriver://host/path\", \"/subpath\", \"dbscheme+dbdriver://host/path/subpath\"),\n            (\"dbscheme+dbdriver:///path\", \"subpath\", \"dbscheme+dbdriver:/path/subpath\"),\n            (\"dbscheme+dbdriver:?somequery\", \"subpath\", \"dbscheme+dbdriver:subpath?somequery\"),\n            (\"dbscheme+dbdriver:?somequery\", \"/subpath\", \"dbscheme+dbdriver:/subpath?somequery\"),\n            (\"dbscheme+dbdriver:/?somequery\", \"subpath\", \"dbscheme+dbdriver:/subpath?somequery\"),\n            (\"dbscheme+dbdriver://?somequery\", \"subpath\", \"dbscheme+dbdriver:subpath?somequery\"),\n            (\"dbscheme+dbdriver:///?somequery\", \"/subpath\", \"dbscheme+dbdriver:/subpath?somequery\"),\n            (\"dbscheme+dbdriver:#somefrag\", \"subpath\", \"dbscheme+dbdriver:subpath#somefrag\"),\n            (\"dbscheme+dbdriver:#somefrag\", \"/subpath\", \"dbscheme+dbdriver:/subpath#somefrag\"),\n            (\"dbscheme+dbdriver:/#somefrag\", \"subpath\", \"dbscheme+dbdriver:/subpath#somefrag\"),\n            (\"dbscheme+dbdriver://#somefrag\", \"subpath\", \"dbscheme+dbdriver:subpath#somefrag\"),\n            (\"dbscheme+dbdriver:///#somefrag\", \"/subpath\", \"dbscheme+dbdriver:/subpath#somefrag\"),\n            (\n                \"dbscheme+dbdriver://root:password?creds=creds\",\n                \"subpath\",\n                \"dbscheme+dbdriver://root:password/subpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password/path/?creds=creds\",\n                \"/subpath/anotherpath\",\n                \"dbscheme+dbdriver://root:password/path/subpath/anotherpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password///path/?creds=creds\",\n                \"subpath/anotherpath\",\n                \"dbscheme+dbdriver://root:password///path/subpath/anotherpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password///path/?creds=creds\",\n                \"/subpath\",\n                \"dbscheme+dbdriver://root:password///path/subpath?creds=creds\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password#myfragment\",\n                \"/subpath\",\n                \"dbscheme+dbdriver://root:password/subpath#myfragment\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password//path/#fragmentwith$pecial@\",\n                \"subpath/anotherpath\",\n                \"dbscheme+dbdriver://root:password//path/subpath/anotherpath#fragmentwith$pecial@\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password@host?creds=creds#fragmentwith$pecial@\",\n                \"subpath\",\n                \"dbscheme+dbdriver://root:password@host/subpath?creds=creds#fragmentwith$pecial@\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password@host.com/path?creds=creds#*frag@*\",\n                \"subpath/dir\",\n                \"dbscheme+dbdriver://root:password@host.com/path/subpath/dir?creds=creds#*frag@*\",\n            ),\n            (\n                \"dbscheme-dbdriver://root:password@host.com/path?creds=creds#*frag@*\",\n                \"subpath/dir\",\n                \"dbscheme-dbdriver://root:password@host.com/path/subpath/dir?creds=creds#*frag@*\",\n            ),\n            (\n                \"dbscheme+dbdriver://root:password@host.com/path?creds=creds,param=value#*frag@*\",\n                \"subpath/dir\",\n                \"dbscheme+dbdriver://root:password@host.com/path/subpath/dir?\"\n                \"creds=creds,param=value#*frag@*\",\n            ),\n        ]\n    )\n\n\ndef test_extract_and_normalize_path():\n    base_uri = \"databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    assert (\n        extract_and_normalize_path(\"dbfs:databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\"dbfs:/databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\"dbfs:///databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\n            \"dbfs:/databricks///mlflow-tracking///EXP_ID///RUN_ID///artifacts/\"\n        )\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\n            \"dbfs:///databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n        )\n        == base_uri\n    )\n    assert (\n        extract_and_normalize_path(\n            \"dbfs:databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n        )\n        == base_uri\n    )\n\n\ndef test_is_databricks_acled_artifacts_uri():\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:/databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:///databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:/databricks///mlflow-tracking///EXP_ID///RUN_ID///artifacts/\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:///databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n    )\n    assert is_databricks_acled_artifacts_uri(\n        \"dbfs:databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n    )\n    assert not is_databricks_acled_artifacts_uri(\n        \"dbfs:/databricks/mlflow//EXP_ID//RUN_ID///artifacts//\"\n    )\n\n\ndef _get_databricks_profile_uri_test_cases():\n    # Each test case is (uri, result, result_scheme)\n    test_case_groups = [\n        [\n            # URIs with no databricks profile info -> return None\n            (\"ftp://user:pass@realhost:port/path/to/nowhere\", None, result_scheme),\n            (\"dbfs:/path/to/nowhere\", None, result_scheme),\n            (\"dbfs://nondatabricks/path/to/nowhere\", None, result_scheme),\n            (\"dbfs://incorrect:netloc:format/path/to/nowhere\", None, result_scheme),\n            # URIs with legit databricks profile info\n            (f\"dbfs://{result_scheme}\", result_scheme, result_scheme),\n            (f\"dbfs://{result_scheme}/\", result_scheme, result_scheme),\n            (f\"dbfs://{result_scheme}/path/to/nowhere\", result_scheme, result_scheme),\n            (f\"dbfs://{result_scheme}:port/path/to/nowhere\", result_scheme, result_scheme),\n            (f\"dbfs://@{result_scheme}/path/to/nowhere\", result_scheme, result_scheme),\n            (f\"dbfs://@{result_scheme}:port/path/to/nowhere\", result_scheme, result_scheme),\n            (\n                f\"dbfs://profile@{result_scheme}/path/to/nowhere\",\n                f\"{result_scheme}://profile\",\n                result_scheme,\n            ),\n            (\n                f\"dbfs://profile@{result_scheme}:port/path/to/nowhere\",\n                f\"{result_scheme}://profile\",\n                result_scheme,\n            ),\n            (\n                f\"dbfs://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            (\n                f\"dbfs://scope:key_prefix@{result_scheme}:port/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            # Doesn't care about the scheme of the artifact URI\n            (\n                f\"runs://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            (\n                f\"models://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n            (\n                f\"s3://scope:key_prefix@{result_scheme}/path/abc\",\n                f\"{result_scheme}://scope:key_prefix\",\n                result_scheme,\n            ),\n        ]\n        for result_scheme in [\"databricks\", \"databricks-uc\"]\n    ]\n    return [test_case for test_case_group in test_case_groups for test_case in test_case_group]\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\", \"result_scheme\"), _get_databricks_profile_uri_test_cases()\n)\ndef test_get_databricks_profile_uri_from_artifact_uri(uri, result, result_scheme):\n    assert get_databricks_profile_uri_from_artifact_uri(uri, result_scheme=result_scheme) == result\n\n\n@pytest.mark.parametrize(\n    \"uri\",\n    [\n        # Treats secret key prefixes with \":\" to be invalid\n        \"dbfs://incorrect:netloc:format@databricks/path/a\",\n        \"dbfs://scope::key_prefix@databricks/path/abc\",\n        \"dbfs://scope:key_prefix:@databricks/path/abc\",\n    ],\n)\ndef test_get_databricks_profile_uri_from_artifact_uri_error_cases(uri):\n    with pytest.raises(MlflowException, match=\"Unsupported Databricks profile\"):\n        get_databricks_profile_uri_from_artifact_uri(uri)\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\"),\n    [\n        # URIs with no databricks profile info should stay the same\n        (\n            \"ftp://user:pass@realhost:port/path/nowhere\",\n            \"ftp://user:pass@realhost:port/path/nowhere\",\n        ),\n        (\"dbfs:/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://nondatabricks/path/to/nowhere\", \"dbfs://nondatabricks/path/to/nowhere\"),\n        (\"dbfs://incorrect:netloc:format/path/\", \"dbfs://incorrect:netloc:format/path/\"),\n        # URIs with legit databricks profile info\n        (\"dbfs://databricks\", \"dbfs:\"),\n        (\"dbfs://databricks/\", \"dbfs:/\"),\n        (\"dbfs://databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://databricks:port/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://@databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://@databricks:port/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://profile@databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://profile@databricks:port/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        (\"dbfs://scope:key_prefix@databricks/path/abc\", \"dbfs:/path/abc\"),\n        (\"dbfs://scope:key_prefix@databricks:port/path/abc\", \"dbfs:/path/abc\"),\n        # Treats secret key prefixes with \":\" to be valid\n        (\"dbfs://incorrect:netloc:format@databricks/path/to/nowhere\", \"dbfs:/path/to/nowhere\"),\n        # Doesn't care about the scheme of the artifact URI\n        (\"runs://scope:key_prefix@databricks/path/abc\", \"runs:/path/abc\"),\n        (\"models://scope:key_prefix@databricks/path/abc\", \"models:/path/abc\"),\n        (\"s3://scope:key_prefix@databricks/path/abc\", \"s3:/path/abc\"),\n    ],\n)\ndef test_remove_databricks_profile_info_from_artifact_uri(uri, result):\n    assert remove_databricks_profile_info_from_artifact_uri(uri) == result\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"profile_uri\", \"result\"),\n    [\n        # test various profile URIs\n        (\"dbfs:/path/a/b\", \"databricks\", \"dbfs://databricks/path/a/b\"),\n        (\"dbfs:/path/a/b/\", \"databricks\", \"dbfs://databricks/path/a/b/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://Profile\", \"dbfs://Profile@databricks/path/a/b/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://profile/\", \"dbfs://profile@databricks/path/a/b/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope:key\", \"dbfs://scope:key@databricks/path/a/b/\"),\n        (\n            \"dbfs:/path/a/b/\",\n            \"databricks://scope:key/random_stuff\",\n            \"dbfs://scope:key@databricks/path/a/b/\",\n        ),\n        (\"dbfs:/path/a/b/\", \"nondatabricks://profile\", \"dbfs:/path/a/b/\"),\n        # test various artifact schemes\n        (\"runs:/path/a/b/\", \"databricks://Profile\", \"runs://Profile@databricks/path/a/b/\"),\n        (\"runs:/path/a/b/\", \"nondatabricks://profile\", \"runs:/path/a/b/\"),\n        (\"models:/path/a/b/\", \"databricks://profile\", \"models://profile@databricks/path/a/b/\"),\n        (\"models:/path/a/b/\", \"nondatabricks://Profile\", \"models:/path/a/b/\"),\n        (\"s3:/path/a/b/\", \"databricks://Profile\", \"s3:/path/a/b/\"),\n        (\"s3:/path/a/b/\", \"nondatabricks://profile\", \"s3:/path/a/b/\"),\n        (\"ftp:/path/a/b/\", \"databricks://profile\", \"ftp:/path/a/b/\"),\n        (\"ftp:/path/a/b/\", \"nondatabricks://Profile\", \"ftp:/path/a/b/\"),\n        # test artifact URIs already with authority\n        (\"ftp://user:pass@host:port/a/b\", \"databricks://Profile\", \"ftp://user:pass@host:port/a/b\"),\n        (\"ftp://user:pass@host:port/a/b\", \"nothing://Profile\", \"ftp://user:pass@host:port/a/b\"),\n        (\"dbfs://databricks\", \"databricks://OtherProfile\", \"dbfs://databricks\"),\n        (\"dbfs://databricks\", \"nondatabricks://Profile\", \"dbfs://databricks\"),\n        (\"dbfs://databricks/path/a/b\", \"databricks://OtherProfile\", \"dbfs://databricks/path/a/b\"),\n        (\"dbfs://databricks/path/a/b\", \"nondatabricks://Profile\", \"dbfs://databricks/path/a/b\"),\n        (\"dbfs://@databricks/path/a/b\", \"databricks://OtherProfile\", \"dbfs://@databricks/path/a/b\"),\n        (\"dbfs://@databricks/path/a/b\", \"nondatabricks://Profile\", \"dbfs://@databricks/path/a/b\"),\n        (\n            \"dbfs://profile@databricks/pp\",\n            \"databricks://OtherProfile\",\n            \"dbfs://profile@databricks/pp\",\n        ),\n        (\n            \"dbfs://profile@databricks/path\",\n            \"databricks://profile\",\n            \"dbfs://profile@databricks/path\",\n        ),\n        (\n            \"dbfs://profile@databricks/path\",\n            \"nondatabricks://Profile\",\n            \"dbfs://profile@databricks/path\",\n        ),\n    ],\n)\ndef test_add_databricks_profile_info_to_artifact_uri(artifact_uri, profile_uri, result):\n    assert add_databricks_profile_info_to_artifact_uri(artifact_uri, profile_uri) == result\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"profile_uri\"),\n    [\n        (\"dbfs:/path/a/b\", \"databricks://not:legit:auth\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope::key\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope:key:/\"),\n        (\"dbfs:/path/a/b/\", \"databricks://scope:key \"),\n    ],\n)\ndef test_add_databricks_profile_info_to_artifact_uri_errors(artifact_uri, profile_uri):\n    with pytest.raises(MlflowException, match=\"Unsupported Databricks profile\"):\n        add_databricks_profile_info_to_artifact_uri(artifact_uri, profile_uri)\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\"),\n    [\n        (\"dbfs:/path/a/b\", True),\n        (\"dbfs://databricks/a/b\", True),\n        (\"dbfs://@databricks/a/b\", True),\n        (\"dbfs://profile@databricks/a/b\", True),\n        (\"dbfs://scope:key@databricks/a/b\", True),\n        (\"dbfs://scope:key:@databricks/a/b\", False),\n        (\"dbfs://scope::key@databricks/a/b\", False),\n        (\"dbfs://profile@notdatabricks/a/b\", False),\n        (\"dbfs://scope:key@notdatabricks/a/b\", False),\n        (\"dbfs://scope:key/a/b\", False),\n        (\"dbfs://notdatabricks/a/b\", False),\n        (\"s3:/path/a/b\", False),\n        (\"ftp://user:pass@host:port/path/a/b\", False),\n        (\"ftp://user:pass@databricks/path/a/b\", False),\n    ],\n)\ndef test_is_valid_dbfs_uri(uri, result):\n    assert is_valid_dbfs_uri(uri) == result\n\n\n@pytest.mark.parametrize(\n    (\"uri\", \"result\"),\n    [\n        (\"/tmp/path\", \"/dbfs/tmp/path\"),\n        (\"dbfs:/path\", \"/dbfs/path\"),\n        (\"dbfs:/path/a/b\", \"/dbfs/path/a/b\"),\n        (\"dbfs:/dbfs/123/abc\", \"/dbfs/dbfs/123/abc\"),\n    ],\n)\ndef test_dbfs_hdfs_uri_to_fuse_path(uri, result):\n    assert dbfs_hdfs_uri_to_fuse_path(uri) == result\n\n\n@pytest.mark.parametrize(\n    \"path\",\n    [\"some/relative/local/path\", \"s3:/some/s3/path\", \"C:/cool/windows/path\"],\n)\ndef test_dbfs_hdfs_uri_to_fuse_path_raises(path):\n    with pytest.raises(MlflowException, match=\"did not start with expected DBFS URI prefix\"):\n        dbfs_hdfs_uri_to_fuse_path(path)\n\n\ndef _assert_resolve_uri_if_local(input_uri, expected_uri):\n    cwd = pathlib.Path.cwd().as_posix()\n    drive = pathlib.Path.cwd().drive\n    if is_windows():\n        cwd = f\"/{cwd}\"\n        drive = f\"{drive}/\"\n    assert resolve_uri_if_local(input_uri) == expected_uri.format(cwd=cwd, drive=drive)\n\n\n@pytest.mark.skipif(is_windows(), reason=\"This test fails on Windows\")\n@pytest.mark.parametrize(\n    (\"input_uri\", \"expected_uri\"),\n    [\n        (\"my/path\", \"{cwd}/my/path\"),\n        (\"#my/path?a=b\", \"{cwd}/#my/path?a=b\"),\n        (\"file://myhostname/my/path\", \"file://myhostname/my/path\"),\n        (\"file:///my/path\", \"file:///{drive}my/path\"),\n        (\"file:my/path\", \"file://{cwd}/my/path\"),\n        (\"/home/my/path\", \"/home/my/path\"),\n        (\"dbfs://databricks/a/b\", \"dbfs://databricks/a/b\"),\n        (\"s3://host/my/path\", \"s3://host/my/path\"),\n    ],\n)\ndef test_resolve_uri_if_local(input_uri, expected_uri):\n    _assert_resolve_uri_if_local(input_uri, expected_uri)\n\n\n@pytest.mark.skipif(not is_windows(), reason=\"This test only passes on Windows\")\n@pytest.mark.parametrize(\n    (\"input_uri\", \"expected_uri\"),\n    [\n        (\"my/path\", \"file://{cwd}/my/path\"),\n        (\"#my/path?a=b\", \"file://{cwd}/#my/path?a=b\"),\n        (\"file://myhostname/my/path\", \"file://myhostname/my/path\"),\n        (\"file:///my/path\", \"file:///{drive}my/path\"),\n        (\"file:my/path\", \"file://{cwd}/my/path\"),\n        (\"/home/my/path\", \"file:///{drive}home/my/path\"),\n        (\"dbfs://databricks/a/b\", \"dbfs://databricks/a/b\"),\n        (\"s3://host/my/path\", \"s3://host/my/path\"),\n    ],\n)\ndef test_resolve_uri_if_local_on_windows(input_uri, expected_uri):\n    _assert_resolve_uri_if_local(input_uri, expected_uri)\n"], "filenames": ["mlflow/environment_variables.py", "mlflow/server/handlers.py", "mlflow/utils/uri.py", "tests/tracking/integration_test_utils.py", "tests/tracking/test_rest_tracking.py", "tests/utils/test_uri.py"], "buggy_code_start_loc": [204, 87, 32, 41, 1047, 93], "buggy_code_end_loc": [204, 1342, 42, 59, 1125, 93], "fixing_code_start_loc": [205, 87, 32, 41, 1047, 94], "fixing_code_end_loc": [213, 1353, 51, 61, 1192, 99], "type": "CWE-29", "message": "Path Traversal: '\\..\\filename' in GitHub repository mlflow/mlflow prior to 2.3.1.", "other": {"cve": {"id": "CVE-2023-2780", "sourceIdentifier": "security@huntr.dev", "published": "2023-05-17T21:15:09.470", "lastModified": "2023-05-25T17:26:40.920", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Path Traversal: '\\..\\filename' in GitHub repository mlflow/mlflow prior to 2.3.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}]}, "weaknesses": [{"source": "security@huntr.dev", "type": "Primary", "description": [{"lang": "en", "value": "CWE-29"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:lfprojects:mlflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.3.1", "matchCriteriaId": "9D848560-15B3-4F3C-BB4D-A847948CE1EC"}]}]}], "references": [{"url": "https://github.com/mlflow/mlflow/commit/fae77a525dd908c56d6204a4cef1c1c75b4e9857", "source": "security@huntr.dev", "tags": ["Patch"]}, {"url": "https://huntr.dev/bounties/b12b0073-0bb0-4bd1-8fc2-ec7f17fd7689", "source": "security@huntr.dev", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/mlflow/mlflow/commit/fae77a525dd908c56d6204a4cef1c1c75b4e9857"}}