{"buggy_code": ["// +build linux freebsd\n\npackage daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/daemon/links\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/runconfig\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nfunc (daemon *Daemon) setupLinkedContainers(container *container.Container) ([]string, error) {\n\tvar env []string\n\tchildren := daemon.children(container)\n\n\tbridgeSettings := container.NetworkSettings.Networks[runconfig.DefaultDaemonNetworkMode().NetworkName()]\n\tif bridgeSettings == nil || bridgeSettings.EndpointSettings == nil {\n\t\treturn nil, nil\n\t}\n\n\tfor linkAlias, child := range children {\n\t\tif !child.IsRunning() {\n\t\t\treturn nil, fmt.Errorf(\"Cannot link to a non running container: %s AS %s\", child.Name, linkAlias)\n\t\t}\n\n\t\tchildBridgeSettings := child.NetworkSettings.Networks[runconfig.DefaultDaemonNetworkMode().NetworkName()]\n\t\tif childBridgeSettings == nil || childBridgeSettings.EndpointSettings == nil {\n\t\t\treturn nil, fmt.Errorf(\"container %s not attached to default bridge network\", child.ID)\n\t\t}\n\n\t\tlink := links.NewLink(\n\t\t\tbridgeSettings.IPAddress,\n\t\t\tchildBridgeSettings.IPAddress,\n\t\t\tlinkAlias,\n\t\t\tchild.Config.Env,\n\t\t\tchild.Config.ExposedPorts,\n\t\t)\n\n\t\tenv = append(env, link.ToEnv()...)\n\t}\n\n\treturn env, nil\n}\n\nfunc (daemon *Daemon) getIpcContainer(id string) (*container.Container, error) {\n\terrMsg := \"can't join IPC of container \" + id\n\t// Check the container exists\n\tctr, err := daemon.GetContainer(id)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, errMsg)\n\t}\n\t// Check the container is running and not restarting\n\tif err := daemon.checkContainer(ctr, containerIsRunning, containerIsNotRestarting); err != nil {\n\t\treturn nil, errors.Wrap(err, errMsg)\n\t}\n\t// Check the container ipc is shareable\n\tif st, err := os.Stat(ctr.ShmPath); err != nil || !st.IsDir() {\n\t\tif err == nil || os.IsNotExist(err) {\n\t\t\treturn nil, errors.New(errMsg + \": non-shareable IPC (hint: use IpcMode:shareable for the donor container)\")\n\t\t}\n\t\t// stat() failed?\n\t\treturn nil, errors.Wrap(err, errMsg+\": unexpected error from stat \"+ctr.ShmPath)\n\t}\n\n\treturn ctr, nil\n}\n\nfunc (daemon *Daemon) getPidContainer(ctr *container.Container) (*container.Container, error) {\n\tcontainerID := ctr.HostConfig.PidMode.Container()\n\tctr, err := daemon.GetContainer(containerID)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"cannot join PID of a non running container: %s\", containerID)\n\t}\n\treturn ctr, daemon.checkContainer(ctr, containerIsRunning, containerIsNotRestarting)\n}\n\nfunc containerIsRunning(c *container.Container) error {\n\tif !c.IsRunning() {\n\t\treturn errdefs.Conflict(errors.Errorf(\"container %s is not running\", c.ID))\n\t}\n\treturn nil\n}\n\nfunc containerIsNotRestarting(c *container.Container) error {\n\tif c.IsRestarting() {\n\t\treturn errContainerIsRestarting(c.ID)\n\t}\n\treturn nil\n}\n\nfunc (daemon *Daemon) setupIpcDirs(c *container.Container) error {\n\tipcMode := c.HostConfig.IpcMode\n\n\tswitch {\n\tcase ipcMode.IsContainer():\n\t\tic, err := daemon.getIpcContainer(ipcMode.Container())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tc.ShmPath = ic.ShmPath\n\n\tcase ipcMode.IsHost():\n\t\tif _, err := os.Stat(\"/dev/shm\"); err != nil {\n\t\t\treturn fmt.Errorf(\"/dev/shm is not mounted, but must be for --ipc=host\")\n\t\t}\n\t\tc.ShmPath = \"/dev/shm\"\n\n\tcase ipcMode.IsPrivate(), ipcMode.IsNone():\n\t\t// c.ShmPath will/should not be used, so make it empty.\n\t\t// Container's /dev/shm mount comes from OCI spec.\n\t\tc.ShmPath = \"\"\n\n\tcase ipcMode.IsEmpty():\n\t\t// A container was created by an older version of the daemon.\n\t\t// The default behavior used to be what is now called \"shareable\".\n\t\tfallthrough\n\n\tcase ipcMode.IsShareable():\n\t\trootIDs := daemon.idMapping.RootPair()\n\t\tif !c.HasMountFor(\"/dev/shm\") {\n\t\t\tshmPath, err := c.ShmResourcePath()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif err := idtools.MkdirAllAndChown(shmPath, 0700, rootIDs); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tshmproperty := \"mode=1777,size=\" + strconv.FormatInt(c.HostConfig.ShmSize, 10)\n\t\t\tif err := unix.Mount(\"shm\", shmPath, \"tmpfs\", uintptr(unix.MS_NOEXEC|unix.MS_NOSUID|unix.MS_NODEV), label.FormatMountLabel(shmproperty, c.GetMountLabel())); err != nil {\n\t\t\t\treturn fmt.Errorf(\"mounting shm tmpfs: %s\", err)\n\t\t\t}\n\t\t\tif err := os.Chown(shmPath, rootIDs.UID, rootIDs.GID); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tc.ShmPath = shmPath\n\t\t}\n\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid IPC mode: %v\", ipcMode)\n\t}\n\n\treturn nil\n}\n\nfunc (daemon *Daemon) setupSecretDir(c *container.Container) (setupErr error) {\n\tif len(c.SecretReferences) == 0 && len(c.ConfigReferences) == 0 {\n\t\treturn nil\n\t}\n\n\tif err := daemon.createSecretsDir(c); err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif setupErr != nil {\n\t\t\tdaemon.cleanupSecretDir(c)\n\t\t}\n\t}()\n\n\tif c.DependencyStore == nil {\n\t\treturn fmt.Errorf(\"secret store is not initialized\")\n\t}\n\n\t// retrieve possible remapped range start for root UID, GID\n\trootIDs := daemon.idMapping.RootPair()\n\n\tfor _, s := range c.SecretReferences {\n\t\t// TODO (ehazlett): use type switch when more are supported\n\t\tif s.File == nil {\n\t\t\tlogrus.Error(\"secret target type is not a file target\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// secrets are created in the SecretMountPath on the host, at a\n\t\t// single level\n\t\tfPath, err := c.SecretFilePath(*s)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"error getting secret file path\")\n\t\t}\n\t\tif err := idtools.MkdirAllAndChown(filepath.Dir(fPath), 0700, rootIDs); err != nil {\n\t\t\treturn errors.Wrap(err, \"error creating secret mount path\")\n\t\t}\n\n\t\tlogrus.WithFields(logrus.Fields{\n\t\t\t\"name\": s.File.Name,\n\t\t\t\"path\": fPath,\n\t\t}).Debug(\"injecting secret\")\n\t\tsecret, err := c.DependencyStore.Secrets().Get(s.SecretID)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"unable to get secret from secret store\")\n\t\t}\n\t\tif err := ioutil.WriteFile(fPath, secret.Spec.Data, s.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error injecting secret\")\n\t\t}\n\n\t\tuid, err := strconv.Atoi(s.File.UID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgid, err := strconv.Atoi(s.File.GID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := os.Chown(fPath, rootIDs.UID+uid, rootIDs.GID+gid); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting ownership for secret\")\n\t\t}\n\t\tif err := os.Chmod(fPath, s.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting file mode for secret\")\n\t\t}\n\t}\n\n\tfor _, configRef := range c.ConfigReferences {\n\t\t// TODO (ehazlett): use type switch when more are supported\n\t\tif configRef.File == nil {\n\t\t\t// Runtime configs are not mounted into the container, but they're\n\t\t\t// a valid type of config so we should not error when we encounter\n\t\t\t// one.\n\t\t\tif configRef.Runtime == nil {\n\t\t\t\tlogrus.Error(\"config target type is not a file or runtime target\")\n\t\t\t}\n\t\t\t// However, in any case, this isn't a file config, so we have no\n\t\t\t// further work to do\n\t\t\tcontinue\n\t\t}\n\n\t\tfPath, err := c.ConfigFilePath(*configRef)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"error getting config file path for container\")\n\t\t}\n\t\tif err := idtools.MkdirAllAndChown(filepath.Dir(fPath), 0700, rootIDs); err != nil {\n\t\t\treturn errors.Wrap(err, \"error creating config mount path\")\n\t\t}\n\n\t\tlogrus.WithFields(logrus.Fields{\n\t\t\t\"name\": configRef.File.Name,\n\t\t\t\"path\": fPath,\n\t\t}).Debug(\"injecting config\")\n\t\tconfig, err := c.DependencyStore.Configs().Get(configRef.ConfigID)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"unable to get config from config store\")\n\t\t}\n\t\tif err := ioutil.WriteFile(fPath, config.Spec.Data, configRef.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error injecting config\")\n\t\t}\n\n\t\tuid, err := strconv.Atoi(configRef.File.UID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgid, err := strconv.Atoi(configRef.File.GID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := os.Chown(fPath, rootIDs.UID+uid, rootIDs.GID+gid); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting ownership for config\")\n\t\t}\n\t\tif err := os.Chmod(fPath, configRef.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting file mode for config\")\n\t\t}\n\t}\n\n\treturn daemon.remountSecretDir(c)\n}\n\n// createSecretsDir is used to create a dir suitable for storing container secrets.\n// In practice this is using a tmpfs mount and is used for both \"configs\" and \"secrets\"\nfunc (daemon *Daemon) createSecretsDir(c *container.Container) error {\n\t// retrieve possible remapped range start for root UID, GID\n\trootIDs := daemon.idMapping.RootPair()\n\tdir, err := c.SecretMountPath()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"error getting container secrets dir\")\n\t}\n\n\t// create tmpfs\n\tif err := idtools.MkdirAllAndChown(dir, 0700, rootIDs); err != nil {\n\t\treturn errors.Wrap(err, \"error creating secret local mount path\")\n\t}\n\n\ttmpfsOwnership := fmt.Sprintf(\"uid=%d,gid=%d\", rootIDs.UID, rootIDs.GID)\n\tif err := mount.Mount(\"tmpfs\", dir, \"tmpfs\", \"nodev,nosuid,noexec,\"+tmpfsOwnership); err != nil {\n\t\treturn errors.Wrap(err, \"unable to setup secret mount\")\n\t}\n\treturn nil\n}\n\nfunc (daemon *Daemon) remountSecretDir(c *container.Container) error {\n\tdir, err := c.SecretMountPath()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"error getting container secrets path\")\n\t}\n\tif err := label.Relabel(dir, c.MountLabel, false); err != nil {\n\t\tlogrus.WithError(err).WithField(\"dir\", dir).Warn(\"Error while attempting to set selinux label\")\n\t}\n\trootIDs := daemon.idMapping.RootPair()\n\ttmpfsOwnership := fmt.Sprintf(\"uid=%d,gid=%d\", rootIDs.UID, rootIDs.GID)\n\n\t// remount secrets ro\n\tif err := mount.Mount(\"tmpfs\", dir, \"tmpfs\", \"remount,ro,\"+tmpfsOwnership); err != nil {\n\t\treturn errors.Wrap(err, \"unable to remount dir as readonly\")\n\t}\n\n\treturn nil\n}\n\nfunc (daemon *Daemon) cleanupSecretDir(c *container.Container) {\n\tdir, err := c.SecretMountPath()\n\tif err != nil {\n\t\tlogrus.WithError(err).WithField(\"container\", c.ID).Warn(\"error getting secrets mount path for container\")\n\t}\n\tif err := mount.RecursiveUnmount(dir); err != nil {\n\t\tlogrus.WithField(\"dir\", dir).WithError(err).Warn(\"Error while attempting to unmount dir, this may prevent removal of container.\")\n\t}\n\tif err := os.RemoveAll(dir); err != nil {\n\t\tlogrus.WithField(\"dir\", dir).WithError(err).Error(\"Error removing dir.\")\n\t}\n}\n\nfunc killProcessDirectly(cntr *container.Container) error {\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\n\t// Block until the container to stops or timeout.\n\tstatus := <-cntr.Wait(ctx, container.WaitConditionNotRunning)\n\tif status.Err() != nil {\n\t\t// Ensure that we don't kill ourselves\n\t\tif pid := cntr.GetPID(); pid != 0 {\n\t\t\tlogrus.Infof(\"Container %s failed to exit within 10 seconds of kill - trying direct SIGKILL\", stringid.TruncateID(cntr.ID))\n\t\t\tif err := unix.Kill(pid, 9); err != nil {\n\t\t\t\tif err != unix.ESRCH {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\te := errNoSuchProcess{pid, 9}\n\t\t\t\tlogrus.Debug(e)\n\t\t\t\treturn e\n\t\t\t}\n\n\t\t\t// In case there were some exceptions(e.g., state of zombie and D)\n\t\t\tif system.IsProcessAlive(pid) {\n\n\t\t\t\t// Since we can not kill a zombie pid, add zombie check here\n\t\t\t\tisZombie, err := system.IsProcessZombie(pid)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Warnf(\"Container %s state is invalid\", stringid.TruncateID(cntr.ID))\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif isZombie {\n\t\t\t\t\treturn errdefs.System(errors.Errorf(\"container %s PID %d is zombie and can not be killed. Use the --init option when creating containers to run an init inside the container that forwards signals and reaps processes\", stringid.TruncateID(cntr.ID), pid))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc isLinkable(child *container.Container) bool {\n\t// A container is linkable only if it belongs to the default network\n\t_, ok := child.NetworkSettings.Networks[runconfig.DefaultDaemonNetworkMode().NetworkName()]\n\treturn ok\n}\n\nfunc enableIPOnPredefinedNetwork() bool {\n\treturn false\n}\n\n// serviceDiscoveryOnDefaultNetwork indicates if service discovery is supported on the default network\nfunc serviceDiscoveryOnDefaultNetwork() bool {\n\treturn false\n}\n\nfunc (daemon *Daemon) setupPathsAndSandboxOptions(container *container.Container, sboxOptions *[]libnetwork.SandboxOption) error {\n\tvar err error\n\n\t// Set the correct paths for /etc/hosts and /etc/resolv.conf, based on the\n\t// networking-mode of the container. Note that containers with \"container\"\n\t// networking are already handled in \"initializeNetworking()\" before we reach\n\t// this function, so do not have to be accounted for here.\n\tswitch {\n\tcase container.HostConfig.NetworkMode.IsHost():\n\t\t// In host-mode networking, the container does not have its own networking\n\t\t// namespace, so both `/etc/hosts` and `/etc/resolv.conf` should be the same\n\t\t// as on the host itself. The container gets a copy of these files.\n\t\t*sboxOptions = append(\n\t\t\t*sboxOptions,\n\t\t\tlibnetwork.OptionOriginHostsPath(\"/etc/hosts\"),\n\t\t\tlibnetwork.OptionOriginResolvConfPath(\"/etc/resolv.conf\"),\n\t\t)\n\tcase container.HostConfig.NetworkMode.IsUserDefined():\n\t\t// The container uses a user-defined network. We use the embedded DNS\n\t\t// server for container name resolution and to act as a DNS forwarder\n\t\t// for external DNS resolution.\n\t\t// We parse the DNS server(s) that are defined in /etc/resolv.conf on\n\t\t// the host, which may be a local DNS server (for example, if DNSMasq or\n\t\t// systemd-resolvd are in use). The embedded DNS server forwards DNS\n\t\t// resolution to the DNS server configured on the host, which in itself\n\t\t// may act as a forwarder for external DNS servers.\n\t\t// If systemd-resolvd is used, the \"upstream\" DNS servers can be found in\n\t\t// /run/systemd/resolve/resolv.conf. We do not query those DNS servers\n\t\t// directly, as they can be dynamically reconfigured.\n\t\t*sboxOptions = append(\n\t\t\t*sboxOptions,\n\t\t\tlibnetwork.OptionOriginResolvConfPath(\"/etc/resolv.conf\"),\n\t\t)\n\tdefault:\n\t\t// For other situations, such as the default bridge network, container\n\t\t// discovery / name resolution is handled through /etc/hosts, and no\n\t\t// embedded DNS server is available. Without the embedded DNS, we\n\t\t// cannot use local DNS servers on the host (for example, if DNSMasq or\n\t\t// systemd-resolvd is used). If systemd-resolvd is used, we try to\n\t\t// determine the external DNS servers that are used on the host.\n\t\t// This situation is not ideal, because DNS servers configured in the\n\t\t// container are not updated after the container is created, but the\n\t\t// DNS servers on the host can be dynamically updated.\n\t\t//\n\t\t// Copy the host's resolv.conf for the container (/run/systemd/resolve/resolv.conf or /etc/resolv.conf)\n\t\t*sboxOptions = append(\n\t\t\t*sboxOptions,\n\t\t\tlibnetwork.OptionOriginResolvConfPath(daemon.configStore.GetResolvConf()),\n\t\t)\n\t}\n\n\tcontainer.HostsPath, err = container.GetRootResourcePath(\"hosts\")\n\tif err != nil {\n\t\treturn err\n\t}\n\t*sboxOptions = append(*sboxOptions, libnetwork.OptionHostsPath(container.HostsPath))\n\n\tcontainer.ResolvConfPath, err = container.GetRootResourcePath(\"resolv.conf\")\n\tif err != nil {\n\t\treturn err\n\t}\n\t*sboxOptions = append(*sboxOptions, libnetwork.OptionResolvConfPath(container.ResolvConfPath))\n\treturn nil\n}\n\nfunc (daemon *Daemon) initializeNetworkingPaths(container *container.Container, nc *container.Container) error {\n\tcontainer.HostnamePath = nc.HostnamePath\n\tcontainer.HostsPath = nc.HostsPath\n\tcontainer.ResolvConfPath = nc.ResolvConfPath\n\treturn nil\n}\n\nfunc (daemon *Daemon) setupContainerMountsRoot(c *container.Container) error {\n\t// get the root mount path so we can make it unbindable\n\tp, err := c.MountsResourcePath(\"\")\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn idtools.MkdirAllAndChown(p, 0700, daemon.idMapping.RootPair())\n}\n", "package daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"runtime\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/platforms\"\n\t\"github.com/docker/docker/api/types\"\n\tcontainertypes \"github.com/docker/docker/api/types/container\"\n\tnetworktypes \"github.com/docker/docker/api/types/network\"\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/image\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/runconfig\"\n\tv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\n\t\"github.com/opencontainers/selinux/go-selinux\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\ntype createOpts struct {\n\tparams                  types.ContainerCreateConfig\n\tmanaged                 bool\n\tignoreImagesArgsEscaped bool\n}\n\n// CreateManagedContainer creates a container that is managed by a Service\nfunc (daemon *Daemon) CreateManagedContainer(params types.ContainerCreateConfig) (containertypes.ContainerCreateCreatedBody, error) {\n\treturn daemon.containerCreate(createOpts{\n\t\tparams:                  params,\n\t\tmanaged:                 true,\n\t\tignoreImagesArgsEscaped: false})\n}\n\n// ContainerCreate creates a regular container\nfunc (daemon *Daemon) ContainerCreate(params types.ContainerCreateConfig) (containertypes.ContainerCreateCreatedBody, error) {\n\treturn daemon.containerCreate(createOpts{\n\t\tparams:                  params,\n\t\tmanaged:                 false,\n\t\tignoreImagesArgsEscaped: false})\n}\n\n// ContainerCreateIgnoreImagesArgsEscaped creates a regular container. This is called from the builder RUN case\n// and ensures that we do not take the images ArgsEscaped\nfunc (daemon *Daemon) ContainerCreateIgnoreImagesArgsEscaped(params types.ContainerCreateConfig) (containertypes.ContainerCreateCreatedBody, error) {\n\treturn daemon.containerCreate(createOpts{\n\t\tparams:                  params,\n\t\tmanaged:                 false,\n\t\tignoreImagesArgsEscaped: true})\n}\n\nfunc (daemon *Daemon) containerCreate(opts createOpts) (containertypes.ContainerCreateCreatedBody, error) {\n\tstart := time.Now()\n\tif opts.params.Config == nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{}, errdefs.InvalidParameter(errors.New(\"Config cannot be empty in order to create a container\"))\n\t}\n\n\tos := runtime.GOOS\n\tvar img *image.Image\n\tif opts.params.Config.Image != \"\" {\n\t\tvar err error\n\t\timg, err = daemon.imageService.GetImage(opts.params.Config.Image, opts.params.Platform)\n\t\tif err == nil {\n\t\t\tos = img.OS\n\t\t}\n\t} else {\n\t\t// This mean scratch. On Windows, we can safely assume that this is a linux\n\t\t// container. On other platforms, it's the host OS (which it already is)\n\t\tif isWindows && system.LCOWSupported() {\n\t\t\tos = \"linux\"\n\t\t}\n\t}\n\n\twarnings, err := daemon.verifyContainerSettings(os, opts.params.HostConfig, opts.params.Config, false)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err)\n\t}\n\n\tif img != nil && opts.params.Platform == nil {\n\t\tp := platforms.DefaultSpec()\n\t\timgPlat := v1.Platform{\n\t\t\tOS:           img.OS,\n\t\t\tArchitecture: img.Architecture,\n\t\t\tVariant:      img.Variant,\n\t\t}\n\n\t\tif !platforms.Only(p).Match(imgPlat) {\n\t\t\twarnings = append(warnings, fmt.Sprintf(\"The requested image's platform (%s) does not match the detected host platform (%s) and no specific platform was requested\", platforms.Format(imgPlat), platforms.Format(p)))\n\t\t}\n\t}\n\n\terr = verifyNetworkingConfig(opts.params.NetworkingConfig)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err)\n\t}\n\n\tif opts.params.HostConfig == nil {\n\t\topts.params.HostConfig = &containertypes.HostConfig{}\n\t}\n\terr = daemon.adaptContainerSettings(opts.params.HostConfig, opts.params.AdjustCPUShares)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err)\n\t}\n\n\tctr, err := daemon.create(opts)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, err\n\t}\n\tcontainerActions.WithValues(\"create\").UpdateSince(start)\n\n\tif warnings == nil {\n\t\twarnings = make([]string, 0) // Create an empty slice to avoid https://github.com/moby/moby/issues/38222\n\t}\n\n\treturn containertypes.ContainerCreateCreatedBody{ID: ctr.ID, Warnings: warnings}, nil\n}\n\n// Create creates a new container from the given configuration with a given name.\nfunc (daemon *Daemon) create(opts createOpts) (retC *container.Container, retErr error) {\n\tvar (\n\t\tctr   *container.Container\n\t\timg   *image.Image\n\t\timgID image.ID\n\t\terr   error\n\t)\n\n\tos := runtime.GOOS\n\tif opts.params.Config.Image != \"\" {\n\t\timg, err = daemon.imageService.GetImage(opts.params.Config.Image, opts.params.Platform)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif img.OS != \"\" {\n\t\t\tos = img.OS\n\t\t} else {\n\t\t\t// default to the host OS except on Windows with LCOW\n\t\t\tif isWindows && system.LCOWSupported() {\n\t\t\t\tos = \"linux\"\n\t\t\t}\n\t\t}\n\t\timgID = img.ID()\n\n\t\tif isWindows && img.OS == \"linux\" && !system.LCOWSupported() {\n\t\t\treturn nil, errors.New(\"operating system on which parent image was created is not Windows\")\n\t\t}\n\t} else {\n\t\tif isWindows {\n\t\t\tos = \"linux\" // 'scratch' case.\n\t\t}\n\t}\n\n\t// On WCOW, if are not being invoked by the builder to create this container (where\n\t// ignoreImagesArgEscaped will be true) - if the image already has its arguments escaped,\n\t// ensure that this is replicated across to the created container to avoid double-escaping\n\t// of the arguments/command line when the runtime attempts to run the container.\n\tif os == \"windows\" && !opts.ignoreImagesArgsEscaped && img != nil && img.RunConfig().ArgsEscaped {\n\t\topts.params.Config.ArgsEscaped = true\n\t}\n\n\tif err := daemon.mergeAndVerifyConfig(opts.params.Config, img); err != nil {\n\t\treturn nil, errdefs.InvalidParameter(err)\n\t}\n\n\tif err := daemon.mergeAndVerifyLogConfig(&opts.params.HostConfig.LogConfig); err != nil {\n\t\treturn nil, errdefs.InvalidParameter(err)\n\t}\n\n\tif ctr, err = daemon.newContainer(opts.params.Name, os, opts.params.Config, opts.params.HostConfig, imgID, opts.managed); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif err := daemon.cleanupContainer(ctr, true, true); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed to cleanup container on create error: %v\", err)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err := daemon.setSecurityOptions(ctr, opts.params.HostConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tctr.HostConfig.StorageOpt = opts.params.HostConfig.StorageOpt\n\n\t// Set RWLayer for container after mount labels have been set\n\trwLayer, err := daemon.imageService.CreateLayer(ctr, setupInitLayer(daemon.idMapping))\n\tif err != nil {\n\t\treturn nil, errdefs.System(err)\n\t}\n\tctr.RWLayer = rwLayer\n\n\trootIDs := daemon.idMapping.RootPair()\n\n\tif err := idtools.MkdirAndChown(ctr.Root, 0700, rootIDs); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(ctr.CheckpointDir(), 0700, rootIDs); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := daemon.setHostConfig(ctr, opts.params.HostConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := daemon.createContainerOSSpecificSettings(ctr, opts.params.Config, opts.params.HostConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar endpointsConfigs map[string]*networktypes.EndpointSettings\n\tif opts.params.NetworkingConfig != nil {\n\t\tendpointsConfigs = opts.params.NetworkingConfig.EndpointsConfig\n\t}\n\t// Make sure NetworkMode has an acceptable value. We do this to ensure\n\t// backwards API compatibility.\n\trunconfig.SetDefaultNetModeIfBlank(ctr.HostConfig)\n\n\tdaemon.updateContainerNetworkSettings(ctr, endpointsConfigs)\n\tif err := daemon.Register(ctr); err != nil {\n\t\treturn nil, err\n\t}\n\tstateCtr.set(ctr.ID, \"stopped\")\n\tdaemon.LogContainerEvent(ctr, \"create\")\n\treturn ctr, nil\n}\n\nfunc toHostConfigSelinuxLabels(labels []string) []string {\n\tfor i, l := range labels {\n\t\tlabels[i] = \"label=\" + l\n\t}\n\treturn labels\n}\n\nfunc (daemon *Daemon) generateSecurityOpt(hostConfig *containertypes.HostConfig) ([]string, error) {\n\tfor _, opt := range hostConfig.SecurityOpt {\n\t\tcon := strings.Split(opt, \"=\")\n\t\tif con[0] == \"label\" {\n\t\t\t// Caller overrode SecurityOpts\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\tipcMode := hostConfig.IpcMode\n\tpidMode := hostConfig.PidMode\n\tprivileged := hostConfig.Privileged\n\tif ipcMode.IsHost() || pidMode.IsHost() || privileged {\n\t\treturn toHostConfigSelinuxLabels(selinux.DisableSecOpt()), nil\n\t}\n\n\tvar ipcLabel []string\n\tvar pidLabel []string\n\tipcContainer := ipcMode.Container()\n\tpidContainer := pidMode.Container()\n\tif ipcContainer != \"\" {\n\t\tc, err := daemon.GetContainer(ipcContainer)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tipcLabel, err = selinux.DupSecOpt(c.ProcessLabel)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif pidContainer == \"\" {\n\t\t\treturn toHostConfigSelinuxLabels(ipcLabel), err\n\t\t}\n\t}\n\tif pidContainer != \"\" {\n\t\tc, err := daemon.GetContainer(pidContainer)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpidLabel, err = selinux.DupSecOpt(c.ProcessLabel)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif ipcContainer == \"\" {\n\t\t\treturn toHostConfigSelinuxLabels(pidLabel), err\n\t\t}\n\t}\n\n\tif pidLabel != nil && ipcLabel != nil {\n\t\tfor i := 0; i < len(pidLabel); i++ {\n\t\t\tif pidLabel[i] != ipcLabel[i] {\n\t\t\t\treturn nil, fmt.Errorf(\"--ipc and --pid containers SELinux labels aren't the same\")\n\t\t\t}\n\t\t}\n\t\treturn toHostConfigSelinuxLabels(pidLabel), nil\n\t}\n\treturn nil, nil\n}\n\nfunc (daemon *Daemon) mergeAndVerifyConfig(config *containertypes.Config, img *image.Image) error {\n\tif img != nil && img.Config != nil {\n\t\tif err := merge(config, img.Config); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Reset the Entrypoint if it is [\"\"]\n\tif len(config.Entrypoint) == 1 && config.Entrypoint[0] == \"\" {\n\t\tconfig.Entrypoint = nil\n\t}\n\tif len(config.Entrypoint) == 0 && len(config.Cmd) == 0 {\n\t\treturn fmt.Errorf(\"No command specified\")\n\t}\n\treturn nil\n}\n\n// Checks if the client set configurations for more than one network while creating a container\n// Also checks if the IPAMConfig is valid\nfunc verifyNetworkingConfig(nwConfig *networktypes.NetworkingConfig) error {\n\tif nwConfig == nil || len(nwConfig.EndpointsConfig) == 0 {\n\t\treturn nil\n\t}\n\tif len(nwConfig.EndpointsConfig) > 1 {\n\t\tl := make([]string, 0, len(nwConfig.EndpointsConfig))\n\t\tfor k := range nwConfig.EndpointsConfig {\n\t\t\tl = append(l, k)\n\t\t}\n\t\treturn errors.Errorf(\"Container cannot be connected to network endpoints: %s\", strings.Join(l, \", \"))\n\t}\n\n\tfor k, v := range nwConfig.EndpointsConfig {\n\t\tif v == nil {\n\t\t\treturn errdefs.InvalidParameter(errors.Errorf(\"no EndpointSettings for %s\", k))\n\t\t}\n\t\tif v.IPAMConfig != nil {\n\t\t\tif v.IPAMConfig.IPv4Address != \"\" && net.ParseIP(v.IPAMConfig.IPv4Address).To4() == nil {\n\t\t\t\treturn errors.Errorf(\"invalid IPv4 address: %s\", v.IPAMConfig.IPv4Address)\n\t\t\t}\n\t\t\tif v.IPAMConfig.IPv6Address != \"\" {\n\t\t\t\tn := net.ParseIP(v.IPAMConfig.IPv6Address)\n\t\t\t\t// if the address is an invalid network address (ParseIP == nil) or if it is\n\t\t\t\t// an IPv4 address (To4() != nil), then it is an invalid IPv6 address\n\t\t\t\tif n == nil || n.To4() != nil {\n\t\t\t\t\treturn errors.Errorf(\"invalid IPv6 address: %s\", v.IPAMConfig.IPv6Address)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n", "// Package daemon exposes the functions that occur on the host server\n// that the Docker daemon is running.\n//\n// In implementing the various functions of the daemon, there is often\n// a method-specific struct for configuring the runtime behavior.\npackage daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/docker/pkg/fileutils\"\n\t\"go.etcd.io/bbolt\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/backoff\"\n\n\t\"github.com/containerd/containerd\"\n\t\"github.com/containerd/containerd/defaults\"\n\t\"github.com/containerd/containerd/pkg/dialer\"\n\t\"github.com/containerd/containerd/remotes/docker\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/api/types\"\n\tcontainertypes \"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/swarm\"\n\t\"github.com/docker/docker/builder\"\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/daemon/config\"\n\t\"github.com/docker/docker/daemon/discovery\"\n\t\"github.com/docker/docker/daemon/events\"\n\t\"github.com/docker/docker/daemon/exec\"\n\t\"github.com/docker/docker/daemon/images\"\n\t\"github.com/docker/docker/daemon/logger\"\n\t\"github.com/docker/docker/daemon/network\"\n\t\"github.com/docker/docker/errdefs\"\n\tbkconfig \"github.com/moby/buildkit/cmd/buildkitd/config\"\n\t\"github.com/moby/buildkit/util/resolver\"\n\t\"github.com/sirupsen/logrus\"\n\n\t// register graph drivers\n\t_ \"github.com/docker/docker/daemon/graphdriver/register\"\n\t\"github.com/docker/docker/daemon/stats\"\n\tdmetadata \"github.com/docker/docker/distribution/metadata\"\n\t\"github.com/docker/docker/dockerversion\"\n\t\"github.com/docker/docker/image\"\n\t\"github.com/docker/docker/layer\"\n\t\"github.com/docker/docker/libcontainerd\"\n\tlibcontainerdtypes \"github.com/docker/docker/libcontainerd/types\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/plugingetter\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/pkg/truncindex\"\n\t\"github.com/docker/docker/plugin\"\n\tpluginexec \"github.com/docker/docker/plugin/executor/containerd\"\n\trefstore \"github.com/docker/docker/reference\"\n\t\"github.com/docker/docker/registry\"\n\t\"github.com/docker/docker/runconfig\"\n\tvolumesservice \"github.com/docker/docker/volume/service\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/docker/libnetwork/cluster\"\n\tnwconfig \"github.com/docker/libnetwork/config\"\n\t\"github.com/moby/locker\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/semaphore\"\n)\n\n// ContainersNamespace is the name of the namespace used for users containers\nconst (\n\tContainersNamespace = \"moby\"\n)\n\nvar (\n\terrSystemNotSupported = errors.New(\"the Docker daemon is not supported on this platform\")\n)\n\n// Daemon holds information about the Docker daemon.\ntype Daemon struct {\n\tID                string\n\trepository        string\n\tcontainers        container.Store\n\tcontainersReplica container.ViewDB\n\texecCommands      *exec.Store\n\timageService      *images.ImageService\n\tidIndex           *truncindex.TruncIndex\n\tconfigStore       *config.Config\n\tstatsCollector    *stats.Collector\n\tdefaultLogConfig  containertypes.LogConfig\n\tRegistryService   registry.Service\n\tEventsService     *events.Events\n\tnetController     libnetwork.NetworkController\n\tvolumes           *volumesservice.VolumesService\n\tdiscoveryWatcher  discovery.Reloader\n\troot              string\n\tseccompEnabled    bool\n\tapparmorEnabled   bool\n\tshutdown          bool\n\tidMapping         *idtools.IdentityMapping\n\t// TODO: move graphDrivers field to an InfoService\n\tgraphDrivers map[string]string // By operating system\n\n\tPluginStore           *plugin.Store // todo: remove\n\tpluginManager         *plugin.Manager\n\tlinkIndex             *linkIndex\n\tcontainerdCli         *containerd.Client\n\tcontainerd            libcontainerdtypes.Client\n\tdefaultIsolation      containertypes.Isolation // Default isolation mode on Windows\n\tclusterProvider       cluster.Provider\n\tcluster               Cluster\n\tgenericResources      []swarm.GenericResource\n\tmetricsPluginListener net.Listener\n\n\tmachineMemory uint64\n\n\tseccompProfile     []byte\n\tseccompProfilePath string\n\n\tdiskUsageRunning int32\n\tpruneRunning     int32\n\thosts            map[string]bool // hosts stores the addresses the daemon is listening on\n\tstartupDone      chan struct{}\n\n\tattachmentStore       network.AttachmentStore\n\tattachableNetworkLock *locker.Locker\n\n\t// This is used for Windows which doesn't currently support running on containerd\n\t// It stores metadata for the content store (used for manifest caching)\n\t// This needs to be closed on daemon exit\n\tmdDB *bbolt.DB\n}\n\n// StoreHosts stores the addresses the daemon is listening on\nfunc (daemon *Daemon) StoreHosts(hosts []string) {\n\tif daemon.hosts == nil {\n\t\tdaemon.hosts = make(map[string]bool)\n\t}\n\tfor _, h := range hosts {\n\t\tdaemon.hosts[h] = true\n\t}\n}\n\n// HasExperimental returns whether the experimental features of the daemon are enabled or not\nfunc (daemon *Daemon) HasExperimental() bool {\n\treturn daemon.configStore != nil && daemon.configStore.Experimental\n}\n\n// Features returns the features map from configStore\nfunc (daemon *Daemon) Features() *map[string]bool {\n\treturn &daemon.configStore.Features\n}\n\n// RegistryHosts returns registry configuration in containerd resolvers format\nfunc (daemon *Daemon) RegistryHosts() docker.RegistryHosts {\n\tvar (\n\t\tregistryKey = \"docker.io\"\n\t\tmirrors     = make([]string, len(daemon.configStore.Mirrors))\n\t\tm           = map[string]bkconfig.RegistryConfig{}\n\t)\n\t// must trim \"https://\" or \"http://\" prefix\n\tfor i, v := range daemon.configStore.Mirrors {\n\t\tif uri, err := url.Parse(v); err == nil {\n\t\t\tv = uri.Host\n\t\t}\n\t\tmirrors[i] = v\n\t}\n\t// set mirrors for default registry\n\tm[registryKey] = bkconfig.RegistryConfig{Mirrors: mirrors}\n\n\tfor _, v := range daemon.configStore.InsecureRegistries {\n\t\tu, err := url.Parse(v)\n\t\tc := bkconfig.RegistryConfig{}\n\t\tif err == nil {\n\t\t\tv = u.Host\n\t\t\tt := true\n\t\t\tif u.Scheme == \"http\" {\n\t\t\t\tc.PlainHTTP = &t\n\t\t\t} else {\n\t\t\t\tc.Insecure = &t\n\t\t\t}\n\t\t}\n\t\tm[v] = c\n\t}\n\n\tfor k, v := range m {\n\t\tif d, err := registry.HostCertsDir(k); err == nil {\n\t\t\tv.TLSConfigDir = []string{d}\n\t\t\tm[k] = v\n\t\t}\n\t}\n\n\tcertsDir := registry.CertsDir()\n\tif fis, err := ioutil.ReadDir(certsDir); err == nil {\n\t\tfor _, fi := range fis {\n\t\t\tif _, ok := m[fi.Name()]; !ok {\n\t\t\t\tm[fi.Name()] = bkconfig.RegistryConfig{\n\t\t\t\t\tTLSConfigDir: []string{filepath.Join(certsDir, fi.Name())},\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn resolver.NewRegistryConfig(m)\n}\n\nfunc (daemon *Daemon) restore() error {\n\tvar mapLock sync.Mutex\n\tcontainers := make(map[string]*container.Container)\n\n\tlogrus.Info(\"Loading containers: start.\")\n\n\tdir, err := ioutil.ReadDir(daemon.repository)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// parallelLimit is the maximum number of parallel startup jobs that we\n\t// allow (this is the limited used for all startup semaphores). The multipler\n\t// (128) was chosen after some fairly significant benchmarking -- don't change\n\t// it unless you've tested it significantly (this value is adjusted if\n\t// RLIMIT_NOFILE is small to avoid EMFILE).\n\tparallelLimit := adjustParallelLimit(len(dir), 128*runtime.NumCPU())\n\n\t// Re-used for all parallel startup jobs.\n\tvar group sync.WaitGroup\n\tsem := semaphore.NewWeighted(int64(parallelLimit))\n\n\tfor _, v := range dir {\n\t\tgroup.Add(1)\n\t\tgo func(id string) {\n\t\t\tdefer group.Done()\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\t\t\tdefer sem.Release(1)\n\n\t\t\tlog := logrus.WithField(\"container\", id)\n\n\t\t\tc, err := daemon.load(id)\n\t\t\tif err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to load container\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !system.IsOSSupported(c.OS) {\n\t\t\t\tlog.Errorf(\"failed to load container: %s (%q)\", system.ErrNotSupportedOperatingSystem, c.OS)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Ignore the container if it does not support the current driver being used by the graph\n\t\t\tcurrentDriverForContainerOS := daemon.graphDrivers[c.OS]\n\t\t\tif (c.Driver == \"\" && currentDriverForContainerOS == \"aufs\") || c.Driver == currentDriverForContainerOS {\n\t\t\t\trwlayer, err := daemon.imageService.GetLayerByID(c.ID, c.OS)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.WithError(err).Error(\"failed to load container mount\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tc.RWLayer = rwlayer\n\t\t\t\tlog.WithFields(logrus.Fields{\n\t\t\t\t\t\"running\": c.IsRunning(),\n\t\t\t\t\t\"paused\":  c.IsPaused(),\n\t\t\t\t}).Debug(\"loaded container\")\n\n\t\t\t\tmapLock.Lock()\n\t\t\t\tcontainers[c.ID] = c\n\t\t\t\tmapLock.Unlock()\n\t\t\t} else {\n\t\t\t\tlog.Debugf(\"cannot load container because it was created with another storage driver\")\n\t\t\t}\n\t\t}(v.Name())\n\t}\n\tgroup.Wait()\n\n\tremoveContainers := make(map[string]*container.Container)\n\trestartContainers := make(map[*container.Container]chan struct{})\n\tactiveSandboxes := make(map[string]interface{})\n\n\tfor _, c := range containers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\tdefer group.Done()\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\t\t\tdefer sem.Release(1)\n\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\n\t\t\tif err := daemon.registerName(c); err != nil {\n\t\t\t\tlog.WithError(err).Errorf(\"failed to register container name: %s\", c.Name)\n\t\t\t\tmapLock.Lock()\n\t\t\t\tdelete(containers, c.ID)\n\t\t\t\tmapLock.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := daemon.Register(c); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to register container\")\n\t\t\t\tmapLock.Lock()\n\t\t\t\tdelete(containers, c.ID)\n\t\t\t\tmapLock.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// The LogConfig.Type is empty if the container was created before docker 1.12 with default log driver.\n\t\t\t// We should rewrite it to use the daemon defaults.\n\t\t\t// Fixes https://github.com/docker/docker/issues/22536\n\t\t\tif c.HostConfig.LogConfig.Type == \"\" {\n\t\t\t\tif err := daemon.mergeAndVerifyLogConfig(&c.HostConfig.LogConfig); err != nil {\n\t\t\t\t\tlog.WithError(err).Error(\"failed to verify log config for container\")\n\t\t\t\t}\n\t\t\t}\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tfor _, c := range containers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\tdefer group.Done()\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\t\t\tdefer sem.Release(1)\n\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\n\t\t\tdaemon.backportMountSpec(c)\n\t\t\tif err := daemon.checkpointAndSave(c); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"error saving backported mountspec to disk\")\n\t\t\t}\n\n\t\t\tdaemon.setStateCounter(c)\n\n\t\t\tlogger := func(c *container.Container) *logrus.Entry {\n\t\t\t\treturn log.WithFields(logrus.Fields{\n\t\t\t\t\t\"running\":    c.IsRunning(),\n\t\t\t\t\t\"paused\":     c.IsPaused(),\n\t\t\t\t\t\"restarting\": c.IsRestarting(),\n\t\t\t\t})\n\t\t\t}\n\n\t\t\tlogger(c).Debug(\"restoring container\")\n\n\t\t\tvar (\n\t\t\t\terr      error\n\t\t\t\talive    bool\n\t\t\t\tec       uint32\n\t\t\t\texitedAt time.Time\n\t\t\t\tprocess  libcontainerdtypes.Process\n\t\t\t)\n\n\t\t\talive, _, process, err = daemon.containerd.Restore(context.Background(), c.ID, c.InitializeStdio)\n\t\t\tif err != nil && !errdefs.IsNotFound(err) {\n\t\t\t\tlogger(c).WithError(err).Error(\"failed to restore container with containerd\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tlogger(c).Debugf(\"alive: %v\", alive)\n\t\t\tif !alive {\n\t\t\t\t// If process is not nil, cleanup dead container from containerd.\n\t\t\t\t// If process is nil then the above `containerd.Restore` returned an errdefs.NotFoundError,\n\t\t\t\t// and docker's view of the container state will be updated accorrdingly via SetStopped further down.\n\t\t\t\tif process != nil {\n\t\t\t\t\tlogger(c).Debug(\"cleaning up dead container process\")\n\t\t\t\t\tec, exitedAt, err = process.Delete(context.Background())\n\t\t\t\t\tif err != nil && !errdefs.IsNotFound(err) {\n\t\t\t\t\t\tlogger(c).WithError(err).Error(\"failed to delete container from containerd\")\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if !daemon.configStore.LiveRestoreEnabled {\n\t\t\t\tlogger(c).Debug(\"shutting down container considered alive by containerd\")\n\t\t\t\tif err := daemon.shutdownContainer(c); err != nil && !errdefs.IsNotFound(err) {\n\t\t\t\t\tlog.WithError(err).Error(\"error shutting down container\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tc.ResetRestartManager(false)\n\t\t\t}\n\n\t\t\tif c.IsRunning() || c.IsPaused() {\n\t\t\t\tlogger(c).Debug(\"syncing container on disk state with real state\")\n\n\t\t\t\tc.RestartManager().Cancel() // manually start containers because some need to wait for swarm networking\n\n\t\t\t\tif c.IsPaused() && alive {\n\t\t\t\t\ts, err := daemon.containerd.Status(context.Background(), c.ID)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tlogger(c).WithError(err).Error(\"failed to get container status\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogger(c).WithField(\"state\", s).Info(\"restored container paused\")\n\t\t\t\t\t\tswitch s {\n\t\t\t\t\t\tcase containerd.Paused, containerd.Pausing:\n\t\t\t\t\t\t\t// nothing to do\n\t\t\t\t\t\tcase containerd.Stopped:\n\t\t\t\t\t\t\talive = false\n\t\t\t\t\t\tcase containerd.Unknown:\n\t\t\t\t\t\t\tlog.Error(\"unknown status for paused container during restore\")\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t// running\n\t\t\t\t\t\t\tc.Lock()\n\t\t\t\t\t\t\tc.Paused = false\n\t\t\t\t\t\t\tdaemon.setStateCounter(c)\n\t\t\t\t\t\t\tif err := c.CheckpointTo(daemon.containersReplica); err != nil {\n\t\t\t\t\t\t\t\tlog.WithError(err).Error(\"failed to update paused container state\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tc.Unlock()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif !alive {\n\t\t\t\t\tlogger(c).Debug(\"setting stopped state\")\n\t\t\t\t\tc.Lock()\n\t\t\t\t\tc.SetStopped(&container.ExitStatus{ExitCode: int(ec), ExitedAt: exitedAt})\n\t\t\t\t\tdaemon.Cleanup(c)\n\t\t\t\t\tif err := c.CheckpointTo(daemon.containersReplica); err != nil {\n\t\t\t\t\t\tlog.WithError(err).Error(\"failed to update stopped container state\")\n\t\t\t\t\t}\n\t\t\t\t\tc.Unlock()\n\t\t\t\t\tlogger(c).Debug(\"set stopped state\")\n\t\t\t\t}\n\n\t\t\t\t// we call Mount and then Unmount to get BaseFs of the container\n\t\t\t\tif err := daemon.Mount(c); err != nil {\n\t\t\t\t\t// The mount is unlikely to fail. However, in case mount fails\n\t\t\t\t\t// the container should be allowed to restore here. Some functionalities\n\t\t\t\t\t// (like docker exec -u user) might be missing but container is able to be\n\t\t\t\t\t// stopped/restarted/removed.\n\t\t\t\t\t// See #29365 for related information.\n\t\t\t\t\t// The error is only logged here.\n\t\t\t\t\tlogger(c).WithError(err).Warn(\"failed to mount container to get BaseFs path\")\n\t\t\t\t} else {\n\t\t\t\t\tif err := daemon.Unmount(c); err != nil {\n\t\t\t\t\t\tlogger(c).WithError(err).Warn(\"failed to umount container to get BaseFs path\")\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tc.ResetRestartManager(false)\n\t\t\t\tif !c.HostConfig.NetworkMode.IsContainer() && c.IsRunning() {\n\t\t\t\t\toptions, err := daemon.buildSandboxOptions(c)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tlogger(c).WithError(err).Warn(\"failed to build sandbox option to restore container\")\n\t\t\t\t\t}\n\t\t\t\t\tmapLock.Lock()\n\t\t\t\t\tactiveSandboxes[c.NetworkSettings.SandboxID] = options\n\t\t\t\t\tmapLock.Unlock()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// get list of containers we need to restart\n\n\t\t\t// Do not autostart containers which\n\t\t\t// has endpoints in a swarm scope\n\t\t\t// network yet since the cluster is\n\t\t\t// not initialized yet. We will start\n\t\t\t// it after the cluster is\n\t\t\t// initialized.\n\t\t\tif daemon.configStore.AutoRestart && c.ShouldRestart() && !c.NetworkSettings.HasSwarmEndpoint && c.HasBeenStartedBefore {\n\t\t\t\tmapLock.Lock()\n\t\t\t\trestartContainers[c] = make(chan struct{})\n\t\t\t\tmapLock.Unlock()\n\t\t\t} else if c.HostConfig != nil && c.HostConfig.AutoRemove {\n\t\t\t\tmapLock.Lock()\n\t\t\t\tremoveContainers[c.ID] = c\n\t\t\t\tmapLock.Unlock()\n\t\t\t}\n\n\t\t\tc.Lock()\n\t\t\tif c.RemovalInProgress {\n\t\t\t\t// We probably crashed in the middle of a removal, reset\n\t\t\t\t// the flag.\n\t\t\t\t//\n\t\t\t\t// We DO NOT remove the container here as we do not\n\t\t\t\t// know if the user had requested for either the\n\t\t\t\t// associated volumes, network links or both to also\n\t\t\t\t// be removed. So we put the container in the \"dead\"\n\t\t\t\t// state and leave further processing up to them.\n\t\t\t\tc.RemovalInProgress = false\n\t\t\t\tc.Dead = true\n\t\t\t\tif err := c.CheckpointTo(daemon.containersReplica); err != nil {\n\t\t\t\t\tlog.WithError(err).Error(\"failed to update RemovalInProgress container state\")\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debugf(\"reset RemovalInProgress state for container\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.Unlock()\n\t\t\tlogger(c).Debug(\"done restoring container\")\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tdaemon.netController, err = daemon.initNetworkController(daemon.configStore, activeSandboxes)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error initializing network controller: %v\", err)\n\t}\n\n\t// Now that all the containers are registered, register the links\n\tfor _, c := range containers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tif err := daemon.registerLinks(c, c.HostConfig); err != nil {\n\t\t\t\tlogrus.WithField(\"container\", c.ID).WithError(err).Error(\"failed to register link for container\")\n\t\t\t}\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tfor c, notifier := range restartContainers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container, chNotify chan struct{}) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\n\t\t\tlog.Debug(\"starting container\")\n\n\t\t\t// ignore errors here as this is a best effort to wait for children to be\n\t\t\t//   running before we try to start the container\n\t\t\tchildren := daemon.children(c)\n\t\t\ttimeout := time.NewTimer(5 * time.Second)\n\t\t\tdefer timeout.Stop()\n\n\t\t\tfor _, child := range children {\n\t\t\t\tif notifier, exists := restartContainers[child]; exists {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-notifier:\n\t\t\t\t\tcase <-timeout.C:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Make sure networks are available before starting\n\t\t\tdaemon.waitForNetworks(c)\n\t\t\tif err := daemon.containerStart(c, \"\", \"\", true); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to start container\")\n\t\t\t}\n\t\t\tclose(chNotify)\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(c, notifier)\n\t}\n\tgroup.Wait()\n\n\tfor id := range removeContainers {\n\t\tgroup.Add(1)\n\t\tgo func(cid string) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tif err := daemon.ContainerRm(cid, &types.ContainerRmConfig{ForceRemove: true, RemoveVolume: true}); err != nil {\n\t\t\t\tlogrus.WithField(\"container\", cid).WithError(err).Error(\"failed to remove container\")\n\t\t\t}\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(id)\n\t}\n\tgroup.Wait()\n\n\t// any containers that were started above would already have had this done,\n\t// however we need to now prepare the mountpoints for the rest of the containers as well.\n\t// This shouldn't cause any issue running on the containers that already had this run.\n\t// This must be run after any containers with a restart policy so that containerized plugins\n\t// can have a chance to be running before we try to initialize them.\n\tfor _, c := range containers {\n\t\t// if the container has restart policy, do not\n\t\t// prepare the mountpoints since it has been done on restarting.\n\t\t// This is to speed up the daemon start when a restart container\n\t\t// has a volume and the volume driver is not available.\n\t\tif _, ok := restartContainers[c]; ok {\n\t\t\tcontinue\n\t\t} else if _, ok := removeContainers[c.ID]; ok {\n\t\t\t// container is automatically removed, skip it.\n\t\t\tcontinue\n\t\t}\n\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tif err := daemon.prepareMountPoints(c); err != nil {\n\t\t\t\tlogrus.WithField(\"container\", c.ID).WithError(err).Error(\"failed to prepare mountpoints for container\")\n\t\t\t}\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tlogrus.Info(\"Loading containers: done.\")\n\n\treturn nil\n}\n\n// RestartSwarmContainers restarts any autostart container which has a\n// swarm endpoint.\nfunc (daemon *Daemon) RestartSwarmContainers() {\n\tctx := context.Background()\n\n\t// parallelLimit is the maximum number of parallel startup jobs that we\n\t// allow (this is the limited used for all startup semaphores). The multipler\n\t// (128) was chosen after some fairly significant benchmarking -- don't change\n\t// it unless you've tested it significantly (this value is adjusted if\n\t// RLIMIT_NOFILE is small to avoid EMFILE).\n\tparallelLimit := adjustParallelLimit(len(daemon.List()), 128*runtime.NumCPU())\n\n\tvar group sync.WaitGroup\n\tsem := semaphore.NewWeighted(int64(parallelLimit))\n\n\tfor _, c := range daemon.List() {\n\t\tif !c.IsRunning() && !c.IsPaused() {\n\t\t\t// Autostart all the containers which has a\n\t\t\t// swarm endpoint now that the cluster is\n\t\t\t// initialized.\n\t\t\tif daemon.configStore.AutoRestart && c.ShouldRestart() && c.NetworkSettings.HasSwarmEndpoint && c.HasBeenStartedBefore {\n\t\t\t\tgroup.Add(1)\n\t\t\t\tgo func(c *container.Container) {\n\t\t\t\t\tif err := sem.Acquire(ctx, 1); err != nil {\n\t\t\t\t\t\t// ctx is done.\n\t\t\t\t\t\tgroup.Done()\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\tif err := daemon.containerStart(c, \"\", \"\", true); err != nil {\n\t\t\t\t\t\tlogrus.WithField(\"container\", c.ID).WithError(err).Error(\"failed to start swarm container\")\n\t\t\t\t\t}\n\n\t\t\t\t\tsem.Release(1)\n\t\t\t\t\tgroup.Done()\n\t\t\t\t}(c)\n\t\t\t}\n\t\t}\n\t}\n\tgroup.Wait()\n}\n\n// waitForNetworks is used during daemon initialization when starting up containers\n// It ensures that all of a container's networks are available before the daemon tries to start the container.\n// In practice it just makes sure the discovery service is available for containers which use a network that require discovery.\nfunc (daemon *Daemon) waitForNetworks(c *container.Container) {\n\tif daemon.discoveryWatcher == nil {\n\t\treturn\n\t}\n\n\t// Make sure if the container has a network that requires discovery that the discovery service is available before starting\n\tfor netName := range c.NetworkSettings.Networks {\n\t\t// If we get `ErrNoSuchNetwork` here, we can assume that it is due to discovery not being ready\n\t\t// Most likely this is because the K/V store used for discovery is in a container and needs to be started\n\t\tif _, err := daemon.netController.NetworkByName(netName); err != nil {\n\t\t\tif _, ok := err.(libnetwork.ErrNoSuchNetwork); !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// use a longish timeout here due to some slowdowns in libnetwork if the k/v store is on anything other than --net=host\n\t\t\t// FIXME: why is this slow???\n\t\t\tdur := 60 * time.Second\n\t\t\ttimer := time.NewTimer(dur)\n\n\t\t\tlogrus.WithField(\"container\", c.ID).Debugf(\"Container %s waiting for network to be ready\", c.Name)\n\t\t\tselect {\n\t\t\tcase <-daemon.discoveryWatcher.ReadyCh():\n\t\t\tcase <-timer.C:\n\t\t\t}\n\t\t\ttimer.Stop()\n\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (daemon *Daemon) children(c *container.Container) map[string]*container.Container {\n\treturn daemon.linkIndex.children(c)\n}\n\n// parents returns the names of the parent containers of the container\n// with the given name.\nfunc (daemon *Daemon) parents(c *container.Container) map[string]*container.Container {\n\treturn daemon.linkIndex.parents(c)\n}\n\nfunc (daemon *Daemon) registerLink(parent, child *container.Container, alias string) error {\n\tfullName := path.Join(parent.Name, alias)\n\tif err := daemon.containersReplica.ReserveName(fullName, child.ID); err != nil {\n\t\tif err == container.ErrNameReserved {\n\t\t\tlogrus.Warnf(\"error registering link for %s, to %s, as alias %s, ignoring: %v\", parent.ID, child.ID, alias, err)\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\tdaemon.linkIndex.link(parent, child, fullName)\n\treturn nil\n}\n\n// DaemonJoinsCluster informs the daemon has joined the cluster and provides\n// the handler to query the cluster component\nfunc (daemon *Daemon) DaemonJoinsCluster(clusterProvider cluster.Provider) {\n\tdaemon.setClusterProvider(clusterProvider)\n}\n\n// DaemonLeavesCluster informs the daemon has left the cluster\nfunc (daemon *Daemon) DaemonLeavesCluster() {\n\t// Daemon is in charge of removing the attachable networks with\n\t// connected containers when the node leaves the swarm\n\tdaemon.clearAttachableNetworks()\n\t// We no longer need the cluster provider, stop it now so that\n\t// the network agent will stop listening to cluster events.\n\tdaemon.setClusterProvider(nil)\n\t// Wait for the networking cluster agent to stop\n\tdaemon.netController.AgentStopWait()\n\t// Daemon is in charge of removing the ingress network when the\n\t// node leaves the swarm. Wait for job to be done or timeout.\n\t// This is called also on graceful daemon shutdown. We need to\n\t// wait, because the ingress release has to happen before the\n\t// network controller is stopped.\n\n\tif done, err := daemon.ReleaseIngress(); err == nil {\n\t\ttimeout := time.NewTimer(5 * time.Second)\n\t\tdefer timeout.Stop()\n\n\t\tselect {\n\t\tcase <-done:\n\t\tcase <-timeout.C:\n\t\t\tlogrus.Warn(\"timeout while waiting for ingress network removal\")\n\t\t}\n\t} else {\n\t\tlogrus.Warnf(\"failed to initiate ingress network removal: %v\", err)\n\t}\n\n\tdaemon.attachmentStore.ClearAttachments()\n}\n\n// setClusterProvider sets a component for querying the current cluster state.\nfunc (daemon *Daemon) setClusterProvider(clusterProvider cluster.Provider) {\n\tdaemon.clusterProvider = clusterProvider\n\tdaemon.netController.SetClusterProvider(clusterProvider)\n\tdaemon.attachableNetworkLock = locker.New()\n}\n\n// IsSwarmCompatible verifies if the current daemon\n// configuration is compatible with the swarm mode\nfunc (daemon *Daemon) IsSwarmCompatible() error {\n\tif daemon.configStore == nil {\n\t\treturn nil\n\t}\n\treturn daemon.configStore.IsSwarmCompatible()\n}\n\n// NewDaemon sets up everything for the daemon to be able to service\n// requests from the webserver.\nfunc NewDaemon(ctx context.Context, config *config.Config, pluginStore *plugin.Store) (daemon *Daemon, err error) {\n\tsetDefaultMtu(config)\n\n\tregistryService, err := registry.NewService(config.ServiceOptions)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Ensure that we have a correct root key limit for launching containers.\n\tif err := ModifyRootKeyLimit(); err != nil {\n\t\tlogrus.Warnf(\"unable to modify root key limit, number of containers could be limited by this quota: %v\", err)\n\t}\n\n\t// Ensure we have compatible and valid configuration options\n\tif err := verifyDaemonSettings(config); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Do we have a disabled network?\n\tconfig.DisableBridge = isBridgeNetworkDisabled(config)\n\n\t// Setup the resolv.conf\n\tsetupResolvConf(config)\n\n\t// Verify the platform is supported as a daemon\n\tif !platformSupported {\n\t\treturn nil, errSystemNotSupported\n\t}\n\n\t// Validate platform-specific requirements\n\tif err := checkSystem(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tidMapping, err := setupRemappedRoot(config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trootIDs := idMapping.RootPair()\n\tif err := setupDaemonProcess(config); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// set up the tmpDir to use a canonical path\n\ttmp, err := prepareTempDir(config.Root, rootIDs)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Unable to get the TempDir under %s: %s\", config.Root, err)\n\t}\n\trealTmp, err := fileutils.ReadSymlinkedDirectory(tmp)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Unable to get the full path to the TempDir (%s): %s\", tmp, err)\n\t}\n\tif isWindows {\n\t\tif _, err := os.Stat(realTmp); err != nil && os.IsNotExist(err) {\n\t\t\tif err := system.MkdirAll(realTmp, 0700); err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"Unable to create the TempDir (%s): %s\", realTmp, err)\n\t\t\t}\n\t\t}\n\t\tos.Setenv(\"TEMP\", realTmp)\n\t\tos.Setenv(\"TMP\", realTmp)\n\t} else {\n\t\tos.Setenv(\"TMPDIR\", realTmp)\n\t}\n\n\td := &Daemon{\n\t\tconfigStore: config,\n\t\tPluginStore: pluginStore,\n\t\tstartupDone: make(chan struct{}),\n\t}\n\n\t// Ensure the daemon is properly shutdown if there is a failure during\n\t// initialization\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif err := d.Shutdown(); err != nil {\n\t\t\t\tlogrus.Error(err)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err := d.setGenericResources(config); err != nil {\n\t\treturn nil, err\n\t}\n\t// set up SIGUSR1 handler on Unix-like systems, or a Win32 global event\n\t// on Windows to dump Go routine stacks\n\tstackDumpDir := config.Root\n\tif execRoot := config.GetExecRoot(); execRoot != \"\" {\n\t\tstackDumpDir = execRoot\n\t}\n\td.setupDumpStackTrap(stackDumpDir)\n\n\tif err := d.setupSeccompProfile(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Set the default isolation mode (only applicable on Windows)\n\tif err := d.setDefaultIsolation(); err != nil {\n\t\treturn nil, fmt.Errorf(\"error setting default isolation mode: %v\", err)\n\t}\n\n\tif err := configureMaxThreads(config); err != nil {\n\t\tlogrus.Warnf(\"Failed to configure golang's threads limit: %v\", err)\n\t}\n\n\t// ensureDefaultAppArmorProfile does nothing if apparmor is disabled\n\tif err := ensureDefaultAppArmorProfile(); err != nil {\n\t\tlogrus.Errorf(err.Error())\n\t}\n\n\tdaemonRepo := filepath.Join(config.Root, \"containers\")\n\tif err := idtools.MkdirAllAndChown(daemonRepo, 0700, rootIDs); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the directory where we'll store the runtime scripts (i.e. in\n\t// order to support runtimeArgs)\n\tdaemonRuntimes := filepath.Join(config.Root, \"runtimes\")\n\tif err := system.MkdirAll(daemonRuntimes, 0700); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := d.loadRuntimes(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif isWindows {\n\t\tif err := system.MkdirAll(filepath.Join(config.Root, \"credentialspecs\"), 0); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// On Windows we don't support the environment variable, or a user supplied graphdriver\n\t// as Windows has no choice in terms of which graphdrivers to use. It's a case of\n\t// running Windows containers on Windows - windowsfilter, running Linux containers on Windows,\n\t// lcow. Unix platforms however run a single graphdriver for all containers, and it can\n\t// be set through an environment variable, a daemon start parameter, or chosen through\n\t// initialization of the layerstore through driver priority order for example.\n\td.graphDrivers = make(map[string]string)\n\tlayerStores := make(map[string]layer.Store)\n\tif isWindows {\n\t\td.graphDrivers[runtime.GOOS] = \"windowsfilter\"\n\t\tif system.LCOWSupported() {\n\t\t\td.graphDrivers[\"linux\"] = \"lcow\"\n\t\t}\n\t} else {\n\t\tdriverName := os.Getenv(\"DOCKER_DRIVER\")\n\t\tif driverName == \"\" {\n\t\t\tdriverName = config.GraphDriver\n\t\t} else {\n\t\t\tlogrus.Infof(\"Setting the storage driver from the $DOCKER_DRIVER environment variable (%s)\", driverName)\n\t\t}\n\t\td.graphDrivers[runtime.GOOS] = driverName // May still be empty. Layerstore init determines instead.\n\t}\n\n\td.RegistryService = registryService\n\tlogger.RegisterPluginGetter(d.PluginStore)\n\n\tmetricsSockPath, err := d.listenMetricsSock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tregisterMetricsPluginCallback(d.PluginStore, metricsSockPath)\n\n\tbackoffConfig := backoff.DefaultConfig\n\tbackoffConfig.MaxDelay = 3 * time.Second\n\tconnParams := grpc.ConnectParams{\n\t\tBackoff: backoffConfig,\n\t}\n\tgopts := []grpc.DialOption{\n\t\t// WithBlock makes sure that the following containerd request\n\t\t// is reliable.\n\t\t//\n\t\t// NOTE: In one edge case with high load pressure, kernel kills\n\t\t// dockerd, containerd and containerd-shims caused by OOM.\n\t\t// When both dockerd and containerd restart, but containerd\n\t\t// will take time to recover all the existing containers. Before\n\t\t// containerd serving, dockerd will failed with gRPC error.\n\t\t// That bad thing is that restore action will still ignore the\n\t\t// any non-NotFound errors and returns running state for\n\t\t// already stopped container. It is unexpected behavior. And\n\t\t// we need to restart dockerd to make sure that anything is OK.\n\t\t//\n\t\t// It is painful. Add WithBlock can prevent the edge case. And\n\t\t// n common case, the containerd will be serving in shortly.\n\t\t// It is not harm to add WithBlock for containerd connection.\n\t\tgrpc.WithBlock(),\n\n\t\tgrpc.WithInsecure(),\n\t\tgrpc.WithConnectParams(connParams),\n\t\tgrpc.WithContextDialer(dialer.ContextDialer),\n\n\t\t// TODO(stevvooe): We may need to allow configuration of this on the client.\n\t\tgrpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(defaults.DefaultMaxRecvMsgSize)),\n\t\tgrpc.WithDefaultCallOptions(grpc.MaxCallSendMsgSize(defaults.DefaultMaxSendMsgSize)),\n\t}\n\tif config.ContainerdAddr != \"\" {\n\t\td.containerdCli, err = containerd.New(config.ContainerdAddr, containerd.WithDefaultNamespace(config.ContainerdNamespace), containerd.WithDialOpts(gopts), containerd.WithTimeout(60*time.Second))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"failed to dial %q\", config.ContainerdAddr)\n\t\t}\n\t}\n\n\tcreatePluginExec := func(m *plugin.Manager) (plugin.Executor, error) {\n\t\tvar pluginCli *containerd.Client\n\n\t\t// Windows is not currently using containerd, keep the\n\t\t// client as nil\n\t\tif config.ContainerdAddr != \"\" {\n\t\t\tpluginCli, err = containerd.New(config.ContainerdAddr, containerd.WithDefaultNamespace(config.ContainerdPluginNamespace), containerd.WithDialOpts(gopts), containerd.WithTimeout(60*time.Second))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"failed to dial %q\", config.ContainerdAddr)\n\t\t\t}\n\t\t}\n\n\t\tvar rt types.Runtime\n\t\tif runtime.GOOS != \"windows\" {\n\t\t\trtPtr, err := d.getRuntime(config.GetDefaultRuntimeName())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\trt = *rtPtr\n\t\t}\n\t\treturn pluginexec.New(ctx, getPluginExecRoot(config.Root), pluginCli, config.ContainerdPluginNamespace, m, rt)\n\t}\n\n\t// Plugin system initialization should happen before restore. Do not change order.\n\td.pluginManager, err = plugin.NewManager(plugin.ManagerConfig{\n\t\tRoot:               filepath.Join(config.Root, \"plugins\"),\n\t\tExecRoot:           getPluginExecRoot(config.Root),\n\t\tStore:              d.PluginStore,\n\t\tCreateExecutor:     createPluginExec,\n\t\tRegistryService:    registryService,\n\t\tLiveRestoreEnabled: config.LiveRestoreEnabled,\n\t\tLogPluginEvent:     d.LogPluginEvent, // todo: make private\n\t\tAuthzMiddleware:    config.AuthzMiddleware,\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"couldn't create plugin manager\")\n\t}\n\n\tif err := d.setupDefaultLogConfig(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor operatingSystem, gd := range d.graphDrivers {\n\t\tlayerStores[operatingSystem], err = layer.NewStoreFromOptions(layer.StoreOptions{\n\t\t\tRoot:                      config.Root,\n\t\t\tMetadataStorePathTemplate: filepath.Join(config.Root, \"image\", \"%s\", \"layerdb\"),\n\t\t\tGraphDriver:               gd,\n\t\t\tGraphDriverOptions:        config.GraphOptions,\n\t\t\tIDMapping:                 idMapping,\n\t\t\tPluginGetter:              d.PluginStore,\n\t\t\tExperimentalEnabled:       config.Experimental,\n\t\t\tOS:                        operatingSystem,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// As layerstore initialization may set the driver\n\t\td.graphDrivers[operatingSystem] = layerStores[operatingSystem].DriverName()\n\t}\n\n\t// Configure and validate the kernels security support. Note this is a Linux/FreeBSD\n\t// operation only, so it is safe to pass *just* the runtime OS graphdriver.\n\tif err := configureKernelSecuritySupport(config, d.graphDrivers[runtime.GOOS]); err != nil {\n\t\treturn nil, err\n\t}\n\n\timageRoot := filepath.Join(config.Root, \"image\", d.graphDrivers[runtime.GOOS])\n\tifs, err := image.NewFSStoreBackend(filepath.Join(imageRoot, \"imagedb\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlgrMap := make(map[string]image.LayerGetReleaser)\n\tfor los, ls := range layerStores {\n\t\tlgrMap[los] = ls\n\t}\n\timageStore, err := image.NewImageStore(ifs, lgrMap)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\td.volumes, err = volumesservice.NewVolumeService(config.Root, d.PluginStore, rootIDs, d)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttrustKey, err := loadOrCreateTrustKey(config.TrustKeyPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttrustDir := filepath.Join(config.Root, \"trust\")\n\n\tif err := system.MkdirAll(trustDir, 0700); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We have a single tag/reference store for the daemon globally. However, it's\n\t// stored under the graphdriver. On host platforms which only support a single\n\t// container OS, but multiple selectable graphdrivers, this means depending on which\n\t// graphdriver is chosen, the global reference store is under there. For\n\t// platforms which support multiple container operating systems, this is slightly\n\t// more problematic as where does the global ref store get located? Fortunately,\n\t// for Windows, which is currently the only daemon supporting multiple container\n\t// operating systems, the list of graphdrivers available isn't user configurable.\n\t// For backwards compatibility, we just put it under the windowsfilter\n\t// directory regardless.\n\trefStoreLocation := filepath.Join(imageRoot, `repositories.json`)\n\trs, err := refstore.NewReferenceStore(refStoreLocation)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Couldn't create reference store repository: %s\", err)\n\t}\n\n\tdistributionMetadataStore, err := dmetadata.NewFSMetadataStore(filepath.Join(imageRoot, \"distribution\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Discovery is only enabled when the daemon is launched with an address to advertise.  When\n\t// initialized, the daemon is registered and we can store the discovery backend as it's read-only\n\tif err := d.initDiscovery(config); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsysInfo := d.RawSysInfo(false)\n\t// Check if Devices cgroup is mounted, it is hard requirement for container security,\n\t// on Linux.\n\tif runtime.GOOS == \"linux\" && !sysInfo.CgroupDevicesEnabled && !sys.RunningInUserNS() {\n\t\treturn nil, errors.New(\"Devices cgroup isn't mounted\")\n\t}\n\n\td.ID = trustKey.PublicKey().KeyID()\n\td.repository = daemonRepo\n\td.containers = container.NewMemoryStore()\n\tif d.containersReplica, err = container.NewViewDB(); err != nil {\n\t\treturn nil, err\n\t}\n\td.execCommands = exec.NewStore()\n\td.idIndex = truncindex.NewTruncIndex([]string{})\n\td.statsCollector = d.newStatsCollector(1 * time.Second)\n\n\td.EventsService = events.New()\n\td.root = config.Root\n\td.idMapping = idMapping\n\td.seccompEnabled = sysInfo.Seccomp\n\td.apparmorEnabled = sysInfo.AppArmor\n\n\td.linkIndex = newLinkIndex()\n\n\timgSvcConfig := images.ImageServiceConfig{\n\t\tContainerStore:            d.containers,\n\t\tDistributionMetadataStore: distributionMetadataStore,\n\t\tEventsService:             d.EventsService,\n\t\tImageStore:                imageStore,\n\t\tLayerStores:               layerStores,\n\t\tMaxConcurrentDownloads:    *config.MaxConcurrentDownloads,\n\t\tMaxConcurrentUploads:      *config.MaxConcurrentUploads,\n\t\tMaxDownloadAttempts:       *config.MaxDownloadAttempts,\n\t\tReferenceStore:            rs,\n\t\tRegistryService:           registryService,\n\t\tTrustKey:                  trustKey,\n\t\tContentNamespace:          config.ContainerdNamespace,\n\t}\n\n\t// containerd is not currently supported with Windows.\n\t// So sometimes d.containerdCli will be nil\n\t// In that case we'll create a local content store... but otherwise we'll use containerd\n\tif d.containerdCli != nil {\n\t\timgSvcConfig.Leases = d.containerdCli.LeasesService()\n\t\timgSvcConfig.ContentStore = d.containerdCli.ContentStore()\n\t} else {\n\t\tcs, lm, err := d.configureLocalContentStore()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\timgSvcConfig.ContentStore = cs\n\t\timgSvcConfig.Leases = lm\n\t}\n\n\t// TODO: imageStore, distributionMetadataStore, and ReferenceStore are only\n\t// used above to run migration. They could be initialized in ImageService\n\t// if migration is called from daemon/images. layerStore might move as well.\n\td.imageService = images.NewImageService(imgSvcConfig)\n\n\tgo d.execCommandGC()\n\n\td.containerd, err = libcontainerd.NewClient(ctx, d.containerdCli, filepath.Join(config.ExecRoot, \"containerd\"), config.ContainerdNamespace, d)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := d.restore(); err != nil {\n\t\treturn nil, err\n\t}\n\tclose(d.startupDone)\n\n\tinfo := d.SystemInfo()\n\n\tengineInfo.WithValues(\n\t\tdockerversion.Version,\n\t\tdockerversion.GitCommit,\n\t\tinfo.Architecture,\n\t\tinfo.Driver,\n\t\tinfo.KernelVersion,\n\t\tinfo.OperatingSystem,\n\t\tinfo.OSType,\n\t\tinfo.OSVersion,\n\t\tinfo.ID,\n\t).Set(1)\n\tengineCpus.Set(float64(info.NCPU))\n\tengineMemory.Set(float64(info.MemTotal))\n\n\tgd := \"\"\n\tfor os, driver := range d.graphDrivers {\n\t\tif len(gd) > 0 {\n\t\t\tgd += \", \"\n\t\t}\n\t\tgd += driver\n\t\tif len(d.graphDrivers) > 1 {\n\t\t\tgd = fmt.Sprintf(\"%s (%s)\", gd, os)\n\t\t}\n\t}\n\tlogrus.WithFields(logrus.Fields{\n\t\t\"version\":        dockerversion.Version,\n\t\t\"commit\":         dockerversion.GitCommit,\n\t\t\"graphdriver(s)\": gd,\n\t}).Info(\"Docker daemon\")\n\n\treturn d, nil\n}\n\n// DistributionServices returns services controlling daemon storage\nfunc (daemon *Daemon) DistributionServices() images.DistributionServices {\n\treturn daemon.imageService.DistributionServices()\n}\n\nfunc (daemon *Daemon) waitForStartupDone() {\n\t<-daemon.startupDone\n}\n\nfunc (daemon *Daemon) shutdownContainer(c *container.Container) error {\n\tstopTimeout := c.StopTimeout()\n\n\t// If container failed to exit in stopTimeout seconds of SIGTERM, then using the force\n\tif err := daemon.containerStop(c, stopTimeout); err != nil {\n\t\treturn fmt.Errorf(\"Failed to stop container %s with error: %v\", c.ID, err)\n\t}\n\n\t// Wait without timeout for the container to exit.\n\t// Ignore the result.\n\t<-c.Wait(context.Background(), container.WaitConditionNotRunning)\n\treturn nil\n}\n\n// ShutdownTimeout returns the timeout (in seconds) before containers are forcibly\n// killed during shutdown. The default timeout can be configured both on the daemon\n// and per container, and the longest timeout will be used. A grace-period of\n// 5 seconds is added to the configured timeout.\n//\n// A negative (-1) timeout means \"indefinitely\", which means that containers\n// are not forcibly killed, and the daemon shuts down after all containers exit.\nfunc (daemon *Daemon) ShutdownTimeout() int {\n\tshutdownTimeout := daemon.configStore.ShutdownTimeout\n\tif shutdownTimeout < 0 {\n\t\treturn -1\n\t}\n\tif daemon.containers == nil {\n\t\treturn shutdownTimeout\n\t}\n\n\tgraceTimeout := 5\n\tfor _, c := range daemon.containers.List() {\n\t\tstopTimeout := c.StopTimeout()\n\t\tif stopTimeout < 0 {\n\t\t\treturn -1\n\t\t}\n\t\tif stopTimeout+graceTimeout > shutdownTimeout {\n\t\t\tshutdownTimeout = stopTimeout + graceTimeout\n\t\t}\n\t}\n\treturn shutdownTimeout\n}\n\n// Shutdown stops the daemon.\nfunc (daemon *Daemon) Shutdown() error {\n\tdaemon.shutdown = true\n\t// Keep mounts and networking running on daemon shutdown if\n\t// we are to keep containers running and restore them.\n\n\tif daemon.configStore.LiveRestoreEnabled && daemon.containers != nil {\n\t\t// check if there are any running containers, if none we should do some cleanup\n\t\tif ls, err := daemon.Containers(&types.ContainerListOptions{}); len(ls) != 0 || err != nil {\n\t\t\t// metrics plugins still need some cleanup\n\t\t\tdaemon.cleanupMetricsPlugins()\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tif daemon.containers != nil {\n\t\tlogrus.Debugf(\"daemon configured with a %d seconds minimum shutdown timeout\", daemon.configStore.ShutdownTimeout)\n\t\tlogrus.Debugf(\"start clean shutdown of all containers with a %d seconds timeout...\", daemon.ShutdownTimeout())\n\t\tdaemon.containers.ApplyAll(func(c *container.Container) {\n\t\t\tif !c.IsRunning() {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\t\t\tlog.Debug(\"shutting down container\")\n\t\t\tif err := daemon.shutdownContainer(c); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to shut down container\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif mountid, err := daemon.imageService.GetLayerMountID(c.ID, c.OS); err == nil {\n\t\t\t\tdaemon.cleanupMountsByID(mountid)\n\t\t\t}\n\t\t\tlog.Debugf(\"shut down container\")\n\t\t})\n\t}\n\n\tif daemon.volumes != nil {\n\t\tif err := daemon.volumes.Shutdown(); err != nil {\n\t\t\tlogrus.Errorf(\"Error shutting down volume store: %v\", err)\n\t\t}\n\t}\n\n\tif daemon.imageService != nil {\n\t\tdaemon.imageService.Cleanup()\n\t}\n\n\t// If we are part of a cluster, clean up cluster's stuff\n\tif daemon.clusterProvider != nil {\n\t\tlogrus.Debugf(\"start clean shutdown of cluster resources...\")\n\t\tdaemon.DaemonLeavesCluster()\n\t}\n\n\tdaemon.cleanupMetricsPlugins()\n\n\t// Shutdown plugins after containers and layerstore. Don't change the order.\n\tdaemon.pluginShutdown()\n\n\t// trigger libnetwork Stop only if it's initialized\n\tif daemon.netController != nil {\n\t\tdaemon.netController.Stop()\n\t}\n\n\tif daemon.containerdCli != nil {\n\t\tdaemon.containerdCli.Close()\n\t}\n\n\tif daemon.mdDB != nil {\n\t\tdaemon.mdDB.Close()\n\t}\n\n\treturn daemon.cleanupMounts()\n}\n\n// Mount sets container.BaseFS\n// (is it not set coming in? why is it unset?)\nfunc (daemon *Daemon) Mount(container *container.Container) error {\n\tif container.RWLayer == nil {\n\t\treturn errors.New(\"RWLayer of container \" + container.ID + \" is unexpectedly nil\")\n\t}\n\tdir, err := container.RWLayer.Mount(container.GetMountLabel())\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.WithField(\"container\", container.ID).Debugf(\"container mounted via layerStore: %v\", dir)\n\n\tif container.BaseFS != nil && container.BaseFS.Path() != dir.Path() {\n\t\t// The mount path reported by the graph driver should always be trusted on Windows, since the\n\t\t// volume path for a given mounted layer may change over time.  This should only be an error\n\t\t// on non-Windows operating systems.\n\t\tif runtime.GOOS != \"windows\" {\n\t\t\tdaemon.Unmount(container)\n\t\t\treturn fmt.Errorf(\"Error: driver %s is returning inconsistent paths for container %s ('%s' then '%s')\",\n\t\t\t\tdaemon.imageService.GraphDriverForOS(container.OS), container.ID, container.BaseFS, dir)\n\t\t}\n\t}\n\tcontainer.BaseFS = dir // TODO: combine these fields\n\treturn nil\n}\n\n// Unmount unsets the container base filesystem\nfunc (daemon *Daemon) Unmount(container *container.Container) error {\n\tif container.RWLayer == nil {\n\t\treturn errors.New(\"RWLayer of container \" + container.ID + \" is unexpectedly nil\")\n\t}\n\tif err := container.RWLayer.Unmount(); err != nil {\n\t\tlogrus.WithField(\"container\", container.ID).WithError(err).Error(\"error unmounting container\")\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Subnets return the IPv4 and IPv6 subnets of networks that are manager by Docker.\nfunc (daemon *Daemon) Subnets() ([]net.IPNet, []net.IPNet) {\n\tvar v4Subnets []net.IPNet\n\tvar v6Subnets []net.IPNet\n\n\tmanagedNetworks := daemon.netController.Networks()\n\n\tfor _, managedNetwork := range managedNetworks {\n\t\tv4infos, v6infos := managedNetwork.Info().IpamInfo()\n\t\tfor _, info := range v4infos {\n\t\t\tif info.IPAMData.Pool != nil {\n\t\t\t\tv4Subnets = append(v4Subnets, *info.IPAMData.Pool)\n\t\t\t}\n\t\t}\n\t\tfor _, info := range v6infos {\n\t\t\tif info.IPAMData.Pool != nil {\n\t\t\t\tv6Subnets = append(v6Subnets, *info.IPAMData.Pool)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn v4Subnets, v6Subnets\n}\n\n// prepareTempDir prepares and returns the default directory to use\n// for temporary files.\n// If it doesn't exist, it is created. If it exists, its content is removed.\nfunc prepareTempDir(rootDir string, rootIdentity idtools.Identity) (string, error) {\n\tvar tmpDir string\n\tif tmpDir = os.Getenv(\"DOCKER_TMPDIR\"); tmpDir == \"\" {\n\t\ttmpDir = filepath.Join(rootDir, \"tmp\")\n\t\tnewName := tmpDir + \"-old\"\n\t\tif err := os.Rename(tmpDir, newName); err == nil {\n\t\t\tgo func() {\n\t\t\t\tif err := os.RemoveAll(newName); err != nil {\n\t\t\t\t\tlogrus.Warnf(\"failed to delete old tmp directory: %s\", newName)\n\t\t\t\t}\n\t\t\t}()\n\t\t} else if !os.IsNotExist(err) {\n\t\t\tlogrus.Warnf(\"failed to rename %s for background deletion: %s. Deleting synchronously\", tmpDir, err)\n\t\t\tif err := os.RemoveAll(tmpDir); err != nil {\n\t\t\t\tlogrus.Warnf(\"failed to delete old tmp directory: %s\", tmpDir)\n\t\t\t}\n\t\t}\n\t}\n\t// We don't remove the content of tmpdir if it's not the default,\n\t// it may hold things that do not belong to us.\n\treturn tmpDir, idtools.MkdirAllAndChown(tmpDir, 0700, rootIdentity)\n}\n\nfunc (daemon *Daemon) setGenericResources(conf *config.Config) error {\n\tgenericResources, err := config.ParseGenericResources(conf.NodeGenericResources)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdaemon.genericResources = genericResources\n\n\treturn nil\n}\n\nfunc setDefaultMtu(conf *config.Config) {\n\t// do nothing if the config does not have the default 0 value.\n\tif conf.Mtu != 0 {\n\t\treturn\n\t}\n\tconf.Mtu = config.DefaultNetworkMtu\n}\n\n// IsShuttingDown tells whether the daemon is shutting down or not\nfunc (daemon *Daemon) IsShuttingDown() bool {\n\treturn daemon.shutdown\n}\n\n// initDiscovery initializes the discovery watcher for this daemon.\nfunc (daemon *Daemon) initDiscovery(conf *config.Config) error {\n\tadvertise, err := config.ParseClusterAdvertiseSettings(conf.ClusterStore, conf.ClusterAdvertise)\n\tif err != nil {\n\t\tif err == discovery.ErrDiscoveryDisabled {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\n\tconf.ClusterAdvertise = advertise\n\tdiscoveryWatcher, err := discovery.Init(conf.ClusterStore, conf.ClusterAdvertise, conf.ClusterOpts)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"discovery initialization failed (%v)\", err)\n\t}\n\n\tdaemon.discoveryWatcher = discoveryWatcher\n\treturn nil\n}\n\nfunc isBridgeNetworkDisabled(conf *config.Config) bool {\n\treturn conf.BridgeConfig.Iface == config.DisableNetworkBridge\n}\n\nfunc (daemon *Daemon) networkOptions(dconfig *config.Config, pg plugingetter.PluginGetter, activeSandboxes map[string]interface{}) ([]nwconfig.Option, error) {\n\toptions := []nwconfig.Option{}\n\tif dconfig == nil {\n\t\treturn options, nil\n\t}\n\n\toptions = append(options, nwconfig.OptionExperimental(dconfig.Experimental))\n\toptions = append(options, nwconfig.OptionDataDir(dconfig.Root))\n\toptions = append(options, nwconfig.OptionExecRoot(dconfig.GetExecRoot()))\n\n\tdd := runconfig.DefaultDaemonNetworkMode()\n\tdn := runconfig.DefaultDaemonNetworkMode().NetworkName()\n\toptions = append(options, nwconfig.OptionDefaultDriver(string(dd)))\n\toptions = append(options, nwconfig.OptionDefaultNetwork(dn))\n\n\tif strings.TrimSpace(dconfig.ClusterStore) != \"\" {\n\t\tkv := strings.Split(dconfig.ClusterStore, \"://\")\n\t\tif len(kv) != 2 {\n\t\t\treturn nil, errors.New(\"kv store daemon config must be of the form KV-PROVIDER://KV-URL\")\n\t\t}\n\t\toptions = append(options, nwconfig.OptionKVProvider(kv[0]))\n\t\toptions = append(options, nwconfig.OptionKVProviderURL(kv[1]))\n\t}\n\tif len(dconfig.ClusterOpts) > 0 {\n\t\toptions = append(options, nwconfig.OptionKVOpts(dconfig.ClusterOpts))\n\t}\n\n\tif daemon.discoveryWatcher != nil {\n\t\toptions = append(options, nwconfig.OptionDiscoveryWatcher(daemon.discoveryWatcher))\n\t}\n\n\tif dconfig.ClusterAdvertise != \"\" {\n\t\toptions = append(options, nwconfig.OptionDiscoveryAddress(dconfig.ClusterAdvertise))\n\t}\n\n\toptions = append(options, nwconfig.OptionLabels(dconfig.Labels))\n\toptions = append(options, driverOptions(dconfig)...)\n\n\tif len(dconfig.NetworkConfig.DefaultAddressPools.Value()) > 0 {\n\t\toptions = append(options, nwconfig.OptionDefaultAddressPoolConfig(dconfig.NetworkConfig.DefaultAddressPools.Value()))\n\t}\n\n\tif daemon.configStore != nil && daemon.configStore.LiveRestoreEnabled && len(activeSandboxes) != 0 {\n\t\toptions = append(options, nwconfig.OptionActiveSandboxes(activeSandboxes))\n\t}\n\n\tif pg != nil {\n\t\toptions = append(options, nwconfig.OptionPluginGetter(pg))\n\t}\n\n\toptions = append(options, nwconfig.OptionNetworkControlPlaneMTU(dconfig.NetworkControlPlaneMTU))\n\n\treturn options, nil\n}\n\n// GetCluster returns the cluster\nfunc (daemon *Daemon) GetCluster() Cluster {\n\treturn daemon.cluster\n}\n\n// SetCluster sets the cluster\nfunc (daemon *Daemon) SetCluster(cluster Cluster) {\n\tdaemon.cluster = cluster\n}\n\nfunc (daemon *Daemon) pluginShutdown() {\n\tmanager := daemon.pluginManager\n\t// Check for a valid manager object. In error conditions, daemon init can fail\n\t// and shutdown called, before plugin manager is initialized.\n\tif manager != nil {\n\t\tmanager.Shutdown()\n\t}\n}\n\n// PluginManager returns current pluginManager associated with the daemon\nfunc (daemon *Daemon) PluginManager() *plugin.Manager { // set up before daemon to avoid this method\n\treturn daemon.pluginManager\n}\n\n// PluginGetter returns current pluginStore associated with the daemon\nfunc (daemon *Daemon) PluginGetter() *plugin.Store {\n\treturn daemon.PluginStore\n}\n\n// CreateDaemonRoot creates the root for the daemon\nfunc CreateDaemonRoot(config *config.Config) error {\n\t// get the canonical path to the Docker root directory\n\tvar realRoot string\n\tif _, err := os.Stat(config.Root); err != nil && os.IsNotExist(err) {\n\t\trealRoot = config.Root\n\t} else {\n\t\trealRoot, err = fileutils.ReadSymlinkedDirectory(config.Root)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Unable to get the full path to root (%s): %s\", config.Root, err)\n\t\t}\n\t}\n\n\tidMapping, err := setupRemappedRoot(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn setupDaemonRoot(config, realRoot, idMapping.RootPair())\n}\n\n// checkpointAndSave grabs a container lock to safely call container.CheckpointTo\nfunc (daemon *Daemon) checkpointAndSave(container *container.Container) error {\n\tcontainer.Lock()\n\tdefer container.Unlock()\n\tif err := container.CheckpointTo(daemon.containersReplica); err != nil {\n\t\treturn fmt.Errorf(\"Error saving container state: %v\", err)\n\t}\n\treturn nil\n}\n\n// because the CLI sends a -1 when it wants to unset the swappiness value\n// we need to clear it on the server side\nfunc fixMemorySwappiness(resources *containertypes.Resources) {\n\tif resources.MemorySwappiness != nil && *resources.MemorySwappiness == -1 {\n\t\tresources.MemorySwappiness = nil\n\t}\n}\n\n// GetAttachmentStore returns current attachment store associated with the daemon\nfunc (daemon *Daemon) GetAttachmentStore() *network.AttachmentStore {\n\treturn &daemon.attachmentStore\n}\n\n// IdentityMapping returns uid/gid mapping or a SID (in the case of Windows) for the builder\nfunc (daemon *Daemon) IdentityMapping() *idtools.IdentityMapping {\n\treturn daemon.idMapping\n}\n\n// ImageService returns the Daemon's ImageService\nfunc (daemon *Daemon) ImageService() *images.ImageService {\n\treturn daemon.imageService\n}\n\n// BuilderBackend returns the backend used by builder\nfunc (daemon *Daemon) BuilderBackend() builder.Backend {\n\treturn struct {\n\t\t*Daemon\n\t\t*images.ImageService\n\t}{daemon, daemon.imageService}\n}\n", "// +build linux freebsd\n\npackage daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/containerd/cgroups\"\n\tstatsV1 \"github.com/containerd/cgroups/stats/v1\"\n\tstatsV2 \"github.com/containerd/cgroups/v2/stats\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/api/types\"\n\t\"github.com/docker/docker/api/types/blkiodev\"\n\tpblkiodev \"github.com/docker/docker/api/types/blkiodev\"\n\tcontainertypes \"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/daemon/config\"\n\t\"github.com/docker/docker/daemon/initlayer\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/opts\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/parsers/kernel\"\n\t\"github.com/docker/docker/pkg/sysinfo\"\n\t\"github.com/docker/docker/runconfig\"\n\tvolumemounts \"github.com/docker/docker/volume/mounts\"\n\t\"github.com/docker/libnetwork\"\n\tnwconfig \"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/drivers/bridge\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/netutils\"\n\t\"github.com/docker/libnetwork/options\"\n\tlntypes \"github.com/docker/libnetwork/types\"\n\t\"github.com/moby/sys/mount\"\n\tspecs \"github.com/opencontainers/runtime-spec/specs-go\"\n\t\"github.com/opencontainers/selinux/go-selinux\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netlink\"\n\t\"golang.org/x/sys/unix\"\n)\n\nconst (\n\tisWindows = false\n\n\t// DefaultShimBinary is the default shim to be used by containerd if none\n\t// is specified\n\tDefaultShimBinary = \"containerd-shim\"\n\n\t// DefaultRuntimeBinary is the default runtime to be used by\n\t// containerd if none is specified\n\tDefaultRuntimeBinary = \"runc\"\n\n\t// See https://git.kernel.org/cgit/linux/kernel/git/tip/tip.git/tree/kernel/sched/sched.h?id=8cd9234c64c584432f6992fe944ca9e46ca8ea76#n269\n\tlinuxMinCPUShares = 2\n\tlinuxMaxCPUShares = 262144\n\tplatformSupported = true\n\t// It's not kernel limit, we want this 6M limit to account for overhead during startup, and to supply a reasonable functional container\n\tlinuxMinMemory = 6291456\n\t// constants for remapped root settings\n\tdefaultIDSpecifier = \"default\"\n\tdefaultRemappedID  = \"dockremap\"\n\n\t// constant for cgroup drivers\n\tcgroupFsDriver      = \"cgroupfs\"\n\tcgroupSystemdDriver = \"systemd\"\n\tcgroupNoneDriver    = \"none\"\n)\n\ntype containerGetter interface {\n\tGetContainer(string) (*container.Container, error)\n}\n\nfunc getMemoryResources(config containertypes.Resources) *specs.LinuxMemory {\n\tmemory := specs.LinuxMemory{}\n\n\tif config.Memory > 0 {\n\t\tmemory.Limit = &config.Memory\n\t}\n\n\tif config.MemoryReservation > 0 {\n\t\tmemory.Reservation = &config.MemoryReservation\n\t}\n\n\tif config.MemorySwap > 0 {\n\t\tmemory.Swap = &config.MemorySwap\n\t}\n\n\tif config.MemorySwappiness != nil {\n\t\tswappiness := uint64(*config.MemorySwappiness)\n\t\tmemory.Swappiness = &swappiness\n\t}\n\n\tif config.OomKillDisable != nil {\n\t\tmemory.DisableOOMKiller = config.OomKillDisable\n\t}\n\n\tif config.KernelMemory != 0 {\n\t\tmemory.Kernel = &config.KernelMemory\n\t}\n\n\tif config.KernelMemoryTCP != 0 {\n\t\tmemory.KernelTCP = &config.KernelMemoryTCP\n\t}\n\n\treturn &memory\n}\n\nfunc getPidsLimit(config containertypes.Resources) *specs.LinuxPids {\n\tif config.PidsLimit == nil {\n\t\treturn nil\n\t}\n\tif *config.PidsLimit <= 0 {\n\t\t// docker API allows 0 and negative values to unset this to be consistent\n\t\t// with default values. When updating values, runc requires -1 to unset\n\t\t// the previous limit.\n\t\treturn &specs.LinuxPids{Limit: -1}\n\t}\n\treturn &specs.LinuxPids{Limit: *config.PidsLimit}\n}\n\nfunc getCPUResources(config containertypes.Resources) (*specs.LinuxCPU, error) {\n\tcpu := specs.LinuxCPU{}\n\n\tif config.CPUShares < 0 {\n\t\treturn nil, fmt.Errorf(\"shares: invalid argument\")\n\t}\n\tif config.CPUShares >= 0 {\n\t\tshares := uint64(config.CPUShares)\n\t\tcpu.Shares = &shares\n\t}\n\n\tif config.CpusetCpus != \"\" {\n\t\tcpu.Cpus = config.CpusetCpus\n\t}\n\n\tif config.CpusetMems != \"\" {\n\t\tcpu.Mems = config.CpusetMems\n\t}\n\n\tif config.NanoCPUs > 0 {\n\t\t// https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt\n\t\tperiod := uint64(100 * time.Millisecond / time.Microsecond)\n\t\tquota := config.NanoCPUs * int64(period) / 1e9\n\t\tcpu.Period = &period\n\t\tcpu.Quota = &quota\n\t}\n\n\tif config.CPUPeriod != 0 {\n\t\tperiod := uint64(config.CPUPeriod)\n\t\tcpu.Period = &period\n\t}\n\n\tif config.CPUQuota != 0 {\n\t\tq := config.CPUQuota\n\t\tcpu.Quota = &q\n\t}\n\n\tif config.CPURealtimePeriod != 0 {\n\t\tperiod := uint64(config.CPURealtimePeriod)\n\t\tcpu.RealtimePeriod = &period\n\t}\n\n\tif config.CPURealtimeRuntime != 0 {\n\t\tc := config.CPURealtimeRuntime\n\t\tcpu.RealtimeRuntime = &c\n\t}\n\n\treturn &cpu, nil\n}\n\nfunc getBlkioWeightDevices(config containertypes.Resources) ([]specs.LinuxWeightDevice, error) {\n\tvar stat unix.Stat_t\n\tvar blkioWeightDevices []specs.LinuxWeightDevice\n\n\tfor _, weightDevice := range config.BlkioWeightDevice {\n\t\tif err := unix.Stat(weightDevice.Path, &stat); err != nil {\n\t\t\treturn nil, errors.WithStack(&os.PathError{Op: \"stat\", Path: weightDevice.Path, Err: err})\n\t\t}\n\t\tweight := weightDevice.Weight\n\t\td := specs.LinuxWeightDevice{Weight: &weight}\n\t\t// The type is 32bit on mips.\n\t\td.Major = int64(unix.Major(uint64(stat.Rdev))) // nolint: unconvert\n\t\td.Minor = int64(unix.Minor(uint64(stat.Rdev))) // nolint: unconvert\n\t\tblkioWeightDevices = append(blkioWeightDevices, d)\n\t}\n\n\treturn blkioWeightDevices, nil\n}\n\nfunc (daemon *Daemon) parseSecurityOpt(container *container.Container, hostConfig *containertypes.HostConfig) error {\n\tcontainer.NoNewPrivileges = daemon.configStore.NoNewPrivileges\n\treturn parseSecurityOpt(container, hostConfig)\n}\n\nfunc parseSecurityOpt(container *container.Container, config *containertypes.HostConfig) error {\n\tvar (\n\t\tlabelOpts []string\n\t\terr       error\n\t)\n\n\tfor _, opt := range config.SecurityOpt {\n\t\tif opt == \"no-new-privileges\" {\n\t\t\tcontainer.NoNewPrivileges = true\n\t\t\tcontinue\n\t\t}\n\t\tif opt == \"disable\" {\n\t\t\tlabelOpts = append(labelOpts, \"disable\")\n\t\t\tcontinue\n\t\t}\n\n\t\tvar con []string\n\t\tif strings.Contains(opt, \"=\") {\n\t\t\tcon = strings.SplitN(opt, \"=\", 2)\n\t\t} else if strings.Contains(opt, \":\") {\n\t\t\tcon = strings.SplitN(opt, \":\", 2)\n\t\t\tlogrus.Warn(\"Security options with `:` as a separator are deprecated and will be completely unsupported in 17.04, use `=` instead.\")\n\t\t}\n\t\tif len(con) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid --security-opt 1: %q\", opt)\n\t\t}\n\n\t\tswitch con[0] {\n\t\tcase \"label\":\n\t\t\tlabelOpts = append(labelOpts, con[1])\n\t\tcase \"apparmor\":\n\t\t\tcontainer.AppArmorProfile = con[1]\n\t\tcase \"seccomp\":\n\t\t\tcontainer.SeccompProfile = con[1]\n\t\tcase \"no-new-privileges\":\n\t\t\tnoNewPrivileges, err := strconv.ParseBool(con[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid --security-opt 2: %q\", opt)\n\t\t\t}\n\t\t\tcontainer.NoNewPrivileges = noNewPrivileges\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"invalid --security-opt 2: %q\", opt)\n\t\t}\n\t}\n\n\tcontainer.ProcessLabel, container.MountLabel, err = label.InitLabels(labelOpts)\n\treturn err\n}\n\nfunc getBlkioThrottleDevices(devs []*blkiodev.ThrottleDevice) ([]specs.LinuxThrottleDevice, error) {\n\tvar throttleDevices []specs.LinuxThrottleDevice\n\tvar stat unix.Stat_t\n\n\tfor _, d := range devs {\n\t\tif err := unix.Stat(d.Path, &stat); err != nil {\n\t\t\treturn nil, errors.WithStack(&os.PathError{Op: \"stat\", Path: d.Path, Err: err})\n\t\t}\n\t\td := specs.LinuxThrottleDevice{Rate: d.Rate}\n\t\t// the type is 32bit on mips\n\t\td.Major = int64(unix.Major(uint64(stat.Rdev))) // nolint: unconvert\n\t\td.Minor = int64(unix.Minor(uint64(stat.Rdev))) // nolint: unconvert\n\t\tthrottleDevices = append(throttleDevices, d)\n\t}\n\n\treturn throttleDevices, nil\n}\n\n// adjustParallelLimit takes a number of objects and a proposed limit and\n// figures out if it's reasonable (and adjusts it accordingly). This is only\n// used for daemon startup, which does a lot of parallel loading of containers\n// (and if we exceed RLIMIT_NOFILE then we're in trouble).\nfunc adjustParallelLimit(n int, limit int) int {\n\t// Rule-of-thumb overhead factor (how many files will each goroutine open\n\t// simultaneously). Yes, this is ugly but to be frank this whole thing is\n\t// ugly.\n\tconst overhead = 2\n\n\t// On Linux, we need to ensure that parallelStartupJobs doesn't cause us to\n\t// exceed RLIMIT_NOFILE. If parallelStartupJobs is too large, we reduce it\n\t// and give a warning (since in theory the user should increase their\n\t// ulimits to the largest possible value for dockerd).\n\tvar rlim unix.Rlimit\n\tif err := unix.Getrlimit(unix.RLIMIT_NOFILE, &rlim); err != nil {\n\t\tlogrus.Warnf(\"Couldn't find dockerd's RLIMIT_NOFILE to double-check startup parallelism factor: %v\", err)\n\t\treturn limit\n\t}\n\tsoftRlimit := int(rlim.Cur)\n\n\t// Much fewer containers than RLIMIT_NOFILE. No need to adjust anything.\n\tif softRlimit > overhead*n {\n\t\treturn limit\n\t}\n\n\t// RLIMIT_NOFILE big enough, no need to adjust anything.\n\tif softRlimit > overhead*limit {\n\t\treturn limit\n\t}\n\n\tlogrus.Warnf(\"Found dockerd's open file ulimit (%v) is far too small -- consider increasing it significantly (at least %v)\", softRlimit, overhead*limit)\n\treturn softRlimit / overhead\n}\n\nfunc checkKernel() error {\n\t// Check for unsupported kernel versions\n\t// FIXME: it would be cleaner to not test for specific versions, but rather\n\t// test for specific functionalities.\n\t// Unfortunately we can't test for the feature \"does not cause a kernel panic\"\n\t// without actually causing a kernel panic, so we need this workaround until\n\t// the circumstances of pre-3.10 crashes are clearer.\n\t// For details see https://github.com/docker/docker/issues/407\n\t// Docker 1.11 and above doesn't actually run on kernels older than 3.4,\n\t// due to containerd-shim usage of PR_SET_CHILD_SUBREAPER (introduced in 3.4).\n\tif !kernel.CheckKernelVersion(3, 10, 0) {\n\t\tv, _ := kernel.GetKernelVersion()\n\t\tif os.Getenv(\"DOCKER_NOWARN_KERNEL_VERSION\") == \"\" {\n\t\t\tlogrus.Fatalf(\"Your Linux kernel version %s is not supported for running docker. Please upgrade your kernel to 3.10.0 or newer.\", v.String())\n\t\t}\n\t}\n\treturn nil\n}\n\n// adaptContainerSettings is called during container creation to modify any\n// settings necessary in the HostConfig structure.\nfunc (daemon *Daemon) adaptContainerSettings(hostConfig *containertypes.HostConfig, adjustCPUShares bool) error {\n\tif adjustCPUShares && hostConfig.CPUShares > 0 {\n\t\t// Handle unsupported CPUShares\n\t\tif hostConfig.CPUShares < linuxMinCPUShares {\n\t\t\tlogrus.Warnf(\"Changing requested CPUShares of %d to minimum allowed of %d\", hostConfig.CPUShares, linuxMinCPUShares)\n\t\t\thostConfig.CPUShares = linuxMinCPUShares\n\t\t} else if hostConfig.CPUShares > linuxMaxCPUShares {\n\t\t\tlogrus.Warnf(\"Changing requested CPUShares of %d to maximum allowed of %d\", hostConfig.CPUShares, linuxMaxCPUShares)\n\t\t\thostConfig.CPUShares = linuxMaxCPUShares\n\t\t}\n\t}\n\tif hostConfig.Memory > 0 && hostConfig.MemorySwap == 0 {\n\t\t// By default, MemorySwap is set to twice the size of Memory.\n\t\thostConfig.MemorySwap = hostConfig.Memory * 2\n\t}\n\tif hostConfig.ShmSize == 0 {\n\t\thostConfig.ShmSize = config.DefaultShmSize\n\t\tif daemon.configStore != nil {\n\t\t\thostConfig.ShmSize = int64(daemon.configStore.ShmSize)\n\t\t}\n\t}\n\t// Set default IPC mode, if unset for container\n\tif hostConfig.IpcMode.IsEmpty() {\n\t\tm := config.DefaultIpcMode\n\t\tif daemon.configStore != nil {\n\t\t\tm = daemon.configStore.IpcMode\n\t\t}\n\t\thostConfig.IpcMode = containertypes.IpcMode(m)\n\t}\n\n\t// Set default cgroup namespace mode, if unset for container\n\tif hostConfig.CgroupnsMode.IsEmpty() {\n\t\t// for cgroup v2: unshare cgroupns even for privileged containers\n\t\t// https://github.com/containers/libpod/pull/4374#issuecomment-549776387\n\t\tif hostConfig.Privileged && cgroups.Mode() != cgroups.Unified {\n\t\t\thostConfig.CgroupnsMode = containertypes.CgroupnsMode(\"host\")\n\t\t} else {\n\t\t\tm := \"host\"\n\t\t\tif cgroups.Mode() == cgroups.Unified {\n\t\t\t\tm = \"private\"\n\t\t\t}\n\t\t\tif daemon.configStore != nil {\n\t\t\t\tm = daemon.configStore.CgroupNamespaceMode\n\t\t\t}\n\t\t\thostConfig.CgroupnsMode = containertypes.CgroupnsMode(m)\n\t\t}\n\t}\n\n\tadaptSharedNamespaceContainer(daemon, hostConfig)\n\n\tvar err error\n\tsecOpts, err := daemon.generateSecurityOpt(hostConfig)\n\tif err != nil {\n\t\treturn err\n\t}\n\thostConfig.SecurityOpt = append(hostConfig.SecurityOpt, secOpts...)\n\tif hostConfig.OomKillDisable == nil {\n\t\tdefaultOomKillDisable := false\n\t\thostConfig.OomKillDisable = &defaultOomKillDisable\n\t}\n\n\treturn nil\n}\n\n// adaptSharedNamespaceContainer replaces container name with its ID in hostConfig.\n// To be more precisely, it modifies `container:name` to `container:ID` of PidMode, IpcMode\n// and NetworkMode.\n//\n// When a container shares its namespace with another container, use ID can keep the namespace\n// sharing connection between the two containers even the another container is renamed.\nfunc adaptSharedNamespaceContainer(daemon containerGetter, hostConfig *containertypes.HostConfig) {\n\tcontainerPrefix := \"container:\"\n\tif hostConfig.PidMode.IsContainer() {\n\t\tpidContainer := hostConfig.PidMode.Container()\n\t\t// if there is any error returned here, we just ignore it and leave it to be\n\t\t// handled in the following logic\n\t\tif c, err := daemon.GetContainer(pidContainer); err == nil {\n\t\t\thostConfig.PidMode = containertypes.PidMode(containerPrefix + c.ID)\n\t\t}\n\t}\n\tif hostConfig.IpcMode.IsContainer() {\n\t\tipcContainer := hostConfig.IpcMode.Container()\n\t\tif c, err := daemon.GetContainer(ipcContainer); err == nil {\n\t\t\thostConfig.IpcMode = containertypes.IpcMode(containerPrefix + c.ID)\n\t\t}\n\t}\n\tif hostConfig.NetworkMode.IsContainer() {\n\t\tnetContainer := hostConfig.NetworkMode.ConnectedContainer()\n\t\tif c, err := daemon.GetContainer(netContainer); err == nil {\n\t\t\thostConfig.NetworkMode = containertypes.NetworkMode(containerPrefix + c.ID)\n\t\t}\n\t}\n}\n\n// verifyPlatformContainerResources performs platform-specific validation of the container's resource-configuration\nfunc verifyPlatformContainerResources(resources *containertypes.Resources, sysInfo *sysinfo.SysInfo, update bool) (warnings []string, err error) {\n\tfixMemorySwappiness(resources)\n\n\t// memory subsystem checks and adjustments\n\tif resources.Memory != 0 && resources.Memory < linuxMinMemory {\n\t\treturn warnings, fmt.Errorf(\"Minimum memory limit allowed is 6MB\")\n\t}\n\tif resources.Memory > 0 && !sysInfo.MemoryLimit {\n\t\twarnings = append(warnings, \"Your kernel does not support memory limit capabilities or the cgroup is not mounted. Limitation discarded.\")\n\t\tresources.Memory = 0\n\t\tresources.MemorySwap = -1\n\t}\n\tif resources.Memory > 0 && resources.MemorySwap != -1 && !sysInfo.SwapLimit {\n\t\twarnings = append(warnings, \"Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\")\n\t\tresources.MemorySwap = -1\n\t}\n\tif resources.Memory > 0 && resources.MemorySwap > 0 && resources.MemorySwap < resources.Memory {\n\t\treturn warnings, fmt.Errorf(\"Minimum memoryswap limit should be larger than memory limit, see usage\")\n\t}\n\tif resources.Memory == 0 && resources.MemorySwap > 0 && !update {\n\t\treturn warnings, fmt.Errorf(\"You should always set the Memory limit when using Memoryswap limit, see usage\")\n\t}\n\tif resources.MemorySwappiness != nil && !sysInfo.MemorySwappiness {\n\t\twarnings = append(warnings, \"Your kernel does not support memory swappiness capabilities or the cgroup is not mounted. Memory swappiness discarded.\")\n\t\tresources.MemorySwappiness = nil\n\t}\n\tif resources.MemorySwappiness != nil {\n\t\tswappiness := *resources.MemorySwappiness\n\t\tif swappiness < 0 || swappiness > 100 {\n\t\t\treturn warnings, fmt.Errorf(\"Invalid value: %v, valid memory swappiness range is 0-100\", swappiness)\n\t\t}\n\t}\n\tif resources.MemoryReservation > 0 && !sysInfo.MemoryReservation {\n\t\twarnings = append(warnings, \"Your kernel does not support memory soft limit capabilities or the cgroup is not mounted. Limitation discarded.\")\n\t\tresources.MemoryReservation = 0\n\t}\n\tif resources.MemoryReservation > 0 && resources.MemoryReservation < linuxMinMemory {\n\t\treturn warnings, fmt.Errorf(\"Minimum memory reservation allowed is 6MB\")\n\t}\n\tif resources.Memory > 0 && resources.MemoryReservation > 0 && resources.Memory < resources.MemoryReservation {\n\t\treturn warnings, fmt.Errorf(\"Minimum memory limit can not be less than memory reservation limit, see usage\")\n\t}\n\tif resources.KernelMemory > 0 {\n\t\t// Kernel memory limit is not supported on cgroup v2.\n\t\t// Even on cgroup v1, kernel memory limit (`kmem.limit_in_bytes`) has been deprecated since kernel 5.4.\n\t\t// https://github.com/torvalds/linux/commit/0158115f702b0ba208ab0b5adf44cae99b3ebcc7\n\t\twarnings = append(warnings, \"Specifying a kernel memory limit is deprecated and will be removed in a future release.\")\n\t}\n\tif resources.KernelMemory > 0 && !sysInfo.KernelMemory {\n\t\twarnings = append(warnings, \"Your kernel does not support kernel memory limit capabilities or the cgroup is not mounted. Limitation discarded.\")\n\t\tresources.KernelMemory = 0\n\t}\n\tif resources.KernelMemory > 0 && resources.KernelMemory < linuxMinMemory {\n\t\treturn warnings, fmt.Errorf(\"Minimum kernel memory limit allowed is 4MB\")\n\t}\n\tif resources.KernelMemory > 0 && !kernel.CheckKernelVersion(4, 0, 0) {\n\t\twarnings = append(warnings, \"You specified a kernel memory limit on a kernel older than 4.0. Kernel memory limits are experimental on older kernels, it won't work as expected and can cause your system to be unstable.\")\n\t}\n\tif resources.OomKillDisable != nil && !sysInfo.OomKillDisable {\n\t\t// only produce warnings if the setting wasn't to *disable* the OOM Kill; no point\n\t\t// warning the caller if they already wanted the feature to be off\n\t\tif *resources.OomKillDisable {\n\t\t\twarnings = append(warnings, \"Your kernel does not support OomKillDisable. OomKillDisable discarded.\")\n\t\t}\n\t\tresources.OomKillDisable = nil\n\t}\n\tif resources.OomKillDisable != nil && *resources.OomKillDisable && resources.Memory == 0 {\n\t\twarnings = append(warnings, \"OOM killer is disabled for the container, but no memory limit is set, this can result in the system running out of resources.\")\n\t}\n\tif resources.PidsLimit != nil && !sysInfo.PidsLimit {\n\t\tif *resources.PidsLimit > 0 {\n\t\t\twarnings = append(warnings, \"Your kernel does not support PIDs limit capabilities or the cgroup is not mounted. PIDs limit discarded.\")\n\t\t}\n\t\tresources.PidsLimit = nil\n\t}\n\n\t// cpu subsystem checks and adjustments\n\tif resources.NanoCPUs > 0 && resources.CPUPeriod > 0 {\n\t\treturn warnings, fmt.Errorf(\"Conflicting options: Nano CPUs and CPU Period cannot both be set\")\n\t}\n\tif resources.NanoCPUs > 0 && resources.CPUQuota > 0 {\n\t\treturn warnings, fmt.Errorf(\"Conflicting options: Nano CPUs and CPU Quota cannot both be set\")\n\t}\n\tif resources.NanoCPUs > 0 && !sysInfo.CPUCfs {\n\t\treturn warnings, fmt.Errorf(\"NanoCPUs can not be set, as your kernel does not support CPU CFS scheduler or the cgroup is not mounted\")\n\t}\n\t// The highest precision we could get on Linux is 0.001, by setting\n\t//   cpu.cfs_period_us=1000ms\n\t//   cpu.cfs_quota=1ms\n\t// See the following link for details:\n\t// https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt\n\t// Here we don't set the lower limit and it is up to the underlying platform (e.g., Linux) to return an error.\n\t// The error message is 0.01 so that this is consistent with Windows\n\tif resources.NanoCPUs < 0 || resources.NanoCPUs > int64(sysinfo.NumCPU())*1e9 {\n\t\treturn warnings, fmt.Errorf(\"Range of CPUs is from 0.01 to %d.00, as there are only %d CPUs available\", sysinfo.NumCPU(), sysinfo.NumCPU())\n\t}\n\n\tif resources.CPUShares > 0 && !sysInfo.CPUShares {\n\t\twarnings = append(warnings, \"Your kernel does not support CPU shares or the cgroup is not mounted. Shares discarded.\")\n\t\tresources.CPUShares = 0\n\t}\n\tif (resources.CPUPeriod != 0 || resources.CPUQuota != 0) && !sysInfo.CPUCfs {\n\t\twarnings = append(warnings, \"Your kernel does not support CPU CFS scheduler. CPU period/quota discarded.\")\n\t\tresources.CPUPeriod = 0\n\t\tresources.CPUQuota = 0\n\t}\n\tif resources.CPUPeriod != 0 && (resources.CPUPeriod < 1000 || resources.CPUPeriod > 1000000) {\n\t\treturn warnings, fmt.Errorf(\"CPU cfs period can not be less than 1ms (i.e. 1000) or larger than 1s (i.e. 1000000)\")\n\t}\n\tif resources.CPUQuota > 0 && resources.CPUQuota < 1000 {\n\t\treturn warnings, fmt.Errorf(\"CPU cfs quota can not be less than 1ms (i.e. 1000)\")\n\t}\n\tif resources.CPUPercent > 0 {\n\t\twarnings = append(warnings, fmt.Sprintf(\"%s does not support CPU percent. Percent discarded.\", runtime.GOOS))\n\t\tresources.CPUPercent = 0\n\t}\n\n\t// cpuset subsystem checks and adjustments\n\tif (resources.CpusetCpus != \"\" || resources.CpusetMems != \"\") && !sysInfo.Cpuset {\n\t\twarnings = append(warnings, \"Your kernel does not support cpuset or the cgroup is not mounted. Cpuset discarded.\")\n\t\tresources.CpusetCpus = \"\"\n\t\tresources.CpusetMems = \"\"\n\t}\n\tcpusAvailable, err := sysInfo.IsCpusetCpusAvailable(resources.CpusetCpus)\n\tif err != nil {\n\t\treturn warnings, errors.Wrapf(err, \"Invalid value %s for cpuset cpus\", resources.CpusetCpus)\n\t}\n\tif !cpusAvailable {\n\t\treturn warnings, fmt.Errorf(\"Requested CPUs are not available - requested %s, available: %s\", resources.CpusetCpus, sysInfo.Cpus)\n\t}\n\tmemsAvailable, err := sysInfo.IsCpusetMemsAvailable(resources.CpusetMems)\n\tif err != nil {\n\t\treturn warnings, errors.Wrapf(err, \"Invalid value %s for cpuset mems\", resources.CpusetMems)\n\t}\n\tif !memsAvailable {\n\t\treturn warnings, fmt.Errorf(\"Requested memory nodes are not available - requested %s, available: %s\", resources.CpusetMems, sysInfo.Mems)\n\t}\n\n\t// blkio subsystem checks and adjustments\n\tif resources.BlkioWeight > 0 && !sysInfo.BlkioWeight {\n\t\twarnings = append(warnings, \"Your kernel does not support Block I/O weight or the cgroup is not mounted. Weight discarded.\")\n\t\tresources.BlkioWeight = 0\n\t}\n\tif resources.BlkioWeight > 0 && (resources.BlkioWeight < 10 || resources.BlkioWeight > 1000) {\n\t\treturn warnings, fmt.Errorf(\"Range of blkio weight is from 10 to 1000\")\n\t}\n\tif resources.IOMaximumBandwidth != 0 || resources.IOMaximumIOps != 0 {\n\t\treturn warnings, fmt.Errorf(\"Invalid QoS settings: %s does not support Maximum IO Bandwidth or Maximum IO IOps\", runtime.GOOS)\n\t}\n\tif len(resources.BlkioWeightDevice) > 0 && !sysInfo.BlkioWeightDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support Block I/O weight_device or the cgroup is not mounted. Weight-device discarded.\")\n\t\tresources.BlkioWeightDevice = []*pblkiodev.WeightDevice{}\n\t}\n\tif len(resources.BlkioDeviceReadBps) > 0 && !sysInfo.BlkioReadBpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support BPS Block I/O read limit or the cgroup is not mounted. Block I/O BPS read limit discarded.\")\n\t\tresources.BlkioDeviceReadBps = []*pblkiodev.ThrottleDevice{}\n\t}\n\tif len(resources.BlkioDeviceWriteBps) > 0 && !sysInfo.BlkioWriteBpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support BPS Block I/O write limit or the cgroup is not mounted. Block I/O BPS write limit discarded.\")\n\t\tresources.BlkioDeviceWriteBps = []*pblkiodev.ThrottleDevice{}\n\n\t}\n\tif len(resources.BlkioDeviceReadIOps) > 0 && !sysInfo.BlkioReadIOpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support IOPS Block read limit or the cgroup is not mounted. Block I/O IOPS read limit discarded.\")\n\t\tresources.BlkioDeviceReadIOps = []*pblkiodev.ThrottleDevice{}\n\t}\n\tif len(resources.BlkioDeviceWriteIOps) > 0 && !sysInfo.BlkioWriteIOpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support IOPS Block write limit or the cgroup is not mounted. Block I/O IOPS write limit discarded.\")\n\t\tresources.BlkioDeviceWriteIOps = []*pblkiodev.ThrottleDevice{}\n\t}\n\n\treturn warnings, nil\n}\n\nfunc (daemon *Daemon) getCgroupDriver() string {\n\tif UsingSystemd(daemon.configStore) {\n\t\treturn cgroupSystemdDriver\n\t}\n\tif daemon.Rootless() {\n\t\treturn cgroupNoneDriver\n\t}\n\treturn cgroupFsDriver\n}\n\n// getCD gets the raw value of the native.cgroupdriver option, if set.\nfunc getCD(config *config.Config) string {\n\tfor _, option := range config.ExecOptions {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil || !strings.EqualFold(key, \"native.cgroupdriver\") {\n\t\t\tcontinue\n\t\t}\n\t\treturn val\n\t}\n\treturn \"\"\n}\n\n// VerifyCgroupDriver validates native.cgroupdriver\nfunc VerifyCgroupDriver(config *config.Config) error {\n\tcd := getCD(config)\n\tif cd == \"\" || cd == cgroupFsDriver || cd == cgroupSystemdDriver {\n\t\treturn nil\n\t}\n\tif cd == cgroupNoneDriver {\n\t\treturn fmt.Errorf(\"native.cgroupdriver option %s is internally used and cannot be specified manually\", cd)\n\t}\n\treturn fmt.Errorf(\"native.cgroupdriver option %s not supported\", cd)\n}\n\n// UsingSystemd returns true if cli option includes native.cgroupdriver=systemd\nfunc UsingSystemd(config *config.Config) bool {\n\tif getCD(config) == cgroupSystemdDriver {\n\t\treturn true\n\t}\n\t// On cgroup v2 hosts, default to systemd driver\n\tif getCD(config) == \"\" && cgroups.Mode() == cgroups.Unified && IsRunningSystemd() {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// IsRunningSystemd is from https://github.com/opencontainers/runc/blob/46be7b612e2533c494e6a251111de46d8e286ed5/libcontainer/cgroups/systemd/common.go#L27-L33\nfunc IsRunningSystemd() bool {\n\tfi, err := os.Lstat(\"/run/systemd/system\")\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn fi.IsDir()\n}\n\n// verifyPlatformContainerSettings performs platform-specific validation of the\n// hostconfig and config structures.\nfunc verifyPlatformContainerSettings(daemon *Daemon, hostConfig *containertypes.HostConfig, update bool) (warnings []string, err error) {\n\tif hostConfig == nil {\n\t\treturn nil, nil\n\t}\n\tsysInfo := daemon.RawSysInfo(true)\n\n\tw, err := verifyPlatformContainerResources(&hostConfig.Resources, sysInfo, update)\n\n\t// no matter err is nil or not, w could have data in itself.\n\twarnings = append(warnings, w...)\n\n\tif err != nil {\n\t\treturn warnings, err\n\t}\n\n\tif hostConfig.ShmSize < 0 {\n\t\treturn warnings, fmt.Errorf(\"SHM size can not be less than 0\")\n\t}\n\n\tif hostConfig.OomScoreAdj < -1000 || hostConfig.OomScoreAdj > 1000 {\n\t\treturn warnings, fmt.Errorf(\"Invalid value %d, range for oom score adj is [-1000, 1000]\", hostConfig.OomScoreAdj)\n\t}\n\n\t// ip-forwarding does not affect container with '--net=host' (or '--net=none')\n\tif sysInfo.IPv4ForwardingDisabled && !(hostConfig.NetworkMode.IsHost() || hostConfig.NetworkMode.IsNone()) {\n\t\twarnings = append(warnings, \"IPv4 forwarding is disabled. Networking will not work.\")\n\t}\n\tif hostConfig.NetworkMode.IsHost() && len(hostConfig.PortBindings) > 0 {\n\t\twarnings = append(warnings, \"Published ports are discarded when using host network mode\")\n\t}\n\n\t// check for various conflicting options with user namespaces\n\tif daemon.configStore.RemappedRoot != \"\" && hostConfig.UsernsMode.IsPrivate() {\n\t\tif hostConfig.Privileged {\n\t\t\treturn warnings, fmt.Errorf(\"privileged mode is incompatible with user namespaces.  You must run the container in the host namespace when running privileged mode\")\n\t\t}\n\t\tif hostConfig.NetworkMode.IsHost() && !hostConfig.UsernsMode.IsHost() {\n\t\t\treturn warnings, fmt.Errorf(\"cannot share the host's network namespace when user namespaces are enabled\")\n\t\t}\n\t\tif hostConfig.PidMode.IsHost() && !hostConfig.UsernsMode.IsHost() {\n\t\t\treturn warnings, fmt.Errorf(\"cannot share the host PID namespace when user namespaces are enabled\")\n\t\t}\n\t}\n\tif hostConfig.CgroupParent != \"\" && UsingSystemd(daemon.configStore) {\n\t\t// CgroupParent for systemd cgroup should be named as \"xxx.slice\"\n\t\tif len(hostConfig.CgroupParent) <= 6 || !strings.HasSuffix(hostConfig.CgroupParent, \".slice\") {\n\t\t\treturn warnings, fmt.Errorf(\"cgroup-parent for systemd cgroup should be a valid slice named as \\\"xxx.slice\\\"\")\n\t\t}\n\t}\n\tif hostConfig.Runtime == \"\" {\n\t\thostConfig.Runtime = daemon.configStore.GetDefaultRuntimeName()\n\t}\n\n\tif rt := daemon.configStore.GetRuntime(hostConfig.Runtime); rt == nil {\n\t\treturn warnings, fmt.Errorf(\"Unknown runtime specified %s\", hostConfig.Runtime)\n\t}\n\n\tparser := volumemounts.NewParser(runtime.GOOS)\n\tfor dest := range hostConfig.Tmpfs {\n\t\tif err := parser.ValidateTmpfsMountDestination(dest); err != nil {\n\t\t\treturn warnings, err\n\t\t}\n\t}\n\n\tif !hostConfig.CgroupnsMode.Valid() {\n\t\treturn warnings, fmt.Errorf(\"invalid cgroup namespace mode: %v\", hostConfig.CgroupnsMode)\n\t}\n\tif hostConfig.CgroupnsMode.IsPrivate() {\n\t\tif !sysInfo.CgroupNamespaces {\n\t\t\twarnings = append(warnings, \"Your kernel does not support cgroup namespaces.  Cgroup namespace setting discarded.\")\n\t\t}\n\t}\n\n\tif hostConfig.Runtime == config.LinuxV1RuntimeName || (hostConfig.Runtime == \"\" && daemon.configStore.DefaultRuntime == config.LinuxV1RuntimeName) {\n\t\twarnings = append(warnings, fmt.Sprintf(\"Configured runtime %q is deprecated and will be removed in the next release.\", config.LinuxV1RuntimeName))\n\t}\n\n\treturn warnings, nil\n}\n\n// verifyDaemonSettings performs validation of daemon config struct\nfunc verifyDaemonSettings(conf *config.Config) error {\n\tif conf.ContainerdNamespace == conf.ContainerdPluginNamespace {\n\t\treturn errors.New(\"containers namespace and plugins namespace cannot be the same\")\n\t}\n\t// Check for mutually incompatible config options\n\tif conf.BridgeConfig.Iface != \"\" && conf.BridgeConfig.IP != \"\" {\n\t\treturn fmt.Errorf(\"You specified -b & --bip, mutually exclusive options. Please specify only one\")\n\t}\n\tif !conf.BridgeConfig.EnableIPTables && !conf.BridgeConfig.InterContainerCommunication {\n\t\treturn fmt.Errorf(\"You specified --iptables=false with --icc=false. ICC=false uses iptables to function. Please set --icc or --iptables to true\")\n\t}\n\tif conf.BridgeConfig.EnableIP6Tables && !conf.Experimental {\n\t\treturn fmt.Errorf(\"ip6tables rules are only available if experimental features are enabled\")\n\t}\n\tif !conf.BridgeConfig.EnableIPTables && conf.BridgeConfig.EnableIPMasq {\n\t\tconf.BridgeConfig.EnableIPMasq = false\n\t}\n\tif err := VerifyCgroupDriver(conf); err != nil {\n\t\treturn err\n\t}\n\tif conf.CgroupParent != \"\" && UsingSystemd(conf) {\n\t\tif len(conf.CgroupParent) <= 6 || !strings.HasSuffix(conf.CgroupParent, \".slice\") {\n\t\t\treturn fmt.Errorf(\"cgroup-parent for systemd cgroup should be a valid slice named as \\\"xxx.slice\\\"\")\n\t\t}\n\t}\n\n\tif conf.Rootless && UsingSystemd(conf) && cgroups.Mode() != cgroups.Unified {\n\t\treturn fmt.Errorf(\"exec-opt native.cgroupdriver=systemd requires cgroup v2 for rootless mode\")\n\t}\n\n\tconfigureRuntimes(conf)\n\tif rtName := conf.GetDefaultRuntimeName(); rtName != \"\" {\n\t\tif conf.GetRuntime(rtName) == nil {\n\t\t\treturn fmt.Errorf(\"specified default runtime '%s' does not exist\", rtName)\n\t\t}\n\t\tif rtName == config.LinuxV1RuntimeName {\n\t\t\tlogrus.Warnf(\"Configured default runtime %q is deprecated and will be removed in the next release.\", config.LinuxV1RuntimeName)\n\t\t}\n\t}\n\treturn nil\n}\n\n// checkSystem validates platform-specific requirements\nfunc checkSystem() error {\n\treturn checkKernel()\n}\n\n// configureMaxThreads sets the Go runtime max threads threshold\n// which is 90% of the kernel setting from /proc/sys/kernel/threads-max\nfunc configureMaxThreads(config *config.Config) error {\n\tmt, err := ioutil.ReadFile(\"/proc/sys/kernel/threads-max\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tmtint, err := strconv.Atoi(strings.TrimSpace(string(mt)))\n\tif err != nil {\n\t\treturn err\n\t}\n\tmaxThreads := (mtint / 100) * 90\n\tdebug.SetMaxThreads(maxThreads)\n\tlogrus.Debugf(\"Golang's threads limit set to %d\", maxThreads)\n\treturn nil\n}\n\nfunc overlaySupportsSelinux() (bool, error) {\n\tf, err := os.Open(\"/proc/kallsyms\")\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn false, nil\n\t\t}\n\t\treturn false, err\n\t}\n\tdefer f.Close()\n\n\ts := bufio.NewScanner(f)\n\tfor s.Scan() {\n\t\tif strings.HasSuffix(s.Text(), \" security_inode_copy_up\") {\n\t\t\treturn true, nil\n\t\t}\n\t}\n\n\treturn false, s.Err()\n}\n\n// configureKernelSecuritySupport configures and validates security support for the kernel\nfunc configureKernelSecuritySupport(config *config.Config, driverName string) error {\n\tif config.EnableSelinuxSupport {\n\t\tif !selinux.GetEnabled() {\n\t\t\tlogrus.Warn(\"Docker could not enable SELinux on the host system\")\n\t\t\treturn nil\n\t\t}\n\n\t\tif driverName == \"overlay\" || driverName == \"overlay2\" {\n\t\t\t// If driver is overlay or overlay2, make sure kernel\n\t\t\t// supports selinux with overlay.\n\t\t\tsupported, err := overlaySupportsSelinux()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif !supported {\n\t\t\t\tlogrus.Warnf(\"SELinux is not supported with the %v graph driver on this kernel\", driverName)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tselinux.SetDisabled()\n\t}\n\treturn nil\n}\n\nfunc (daemon *Daemon) initNetworkController(config *config.Config, activeSandboxes map[string]interface{}) (libnetwork.NetworkController, error) {\n\tnetOptions, err := daemon.networkOptions(config, daemon.PluginStore, activeSandboxes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontroller, err := libnetwork.New(netOptions...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error obtaining controller instance: %v\", err)\n\t}\n\n\tif len(activeSandboxes) > 0 {\n\t\tlogrus.Info(\"There are old running containers, the network config will not take affect\")\n\t\treturn controller, nil\n\t}\n\n\t// Initialize default network on \"null\"\n\tif n, _ := controller.NetworkByName(\"none\"); n == nil {\n\t\tif _, err := controller.NewNetwork(\"null\", \"none\", \"\", libnetwork.NetworkOptionPersist(true)); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error creating default \\\"null\\\" network: %v\", err)\n\t\t}\n\t}\n\n\t// Initialize default network on \"host\"\n\tif n, _ := controller.NetworkByName(\"host\"); n == nil {\n\t\tif _, err := controller.NewNetwork(\"host\", \"host\", \"\", libnetwork.NetworkOptionPersist(true)); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error creating default \\\"host\\\" network: %v\", err)\n\t\t}\n\t}\n\n\t// Clear stale bridge network\n\tif n, err := controller.NetworkByName(\"bridge\"); err == nil {\n\t\tif err = n.Delete(); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not delete the default bridge network: %v\", err)\n\t\t}\n\t\tif len(config.NetworkConfig.DefaultAddressPools.Value()) > 0 && !daemon.configStore.LiveRestoreEnabled {\n\t\t\tremoveDefaultBridgeInterface()\n\t\t}\n\t}\n\n\tif !config.DisableBridge {\n\t\t// Initialize default driver \"bridge\"\n\t\tif err := initBridgeDriver(controller, config); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\tremoveDefaultBridgeInterface()\n\t}\n\n\t// Set HostGatewayIP to the default bridge's IP  if it is empty\n\tif daemon.configStore.HostGatewayIP == nil && controller != nil {\n\t\tif n, err := controller.NetworkByName(\"bridge\"); err == nil {\n\t\t\tv4Info, v6Info := n.Info().IpamInfo()\n\t\t\tvar gateway net.IP\n\t\t\tif len(v4Info) > 0 {\n\t\t\t\tgateway = v4Info[0].Gateway.IP\n\t\t\t} else if len(v6Info) > 0 {\n\t\t\t\tgateway = v6Info[0].Gateway.IP\n\t\t\t}\n\t\t\tdaemon.configStore.HostGatewayIP = gateway\n\t\t}\n\t}\n\treturn controller, nil\n}\n\nfunc driverOptions(config *config.Config) []nwconfig.Option {\n\tbridgeConfig := options.Generic{\n\t\t\"EnableIPForwarding\":  config.BridgeConfig.EnableIPForward,\n\t\t\"EnableIPTables\":      config.BridgeConfig.EnableIPTables,\n\t\t\"EnableIP6Tables\":     config.BridgeConfig.EnableIP6Tables,\n\t\t\"EnableUserlandProxy\": config.BridgeConfig.EnableUserlandProxy,\n\t\t\"UserlandProxyPath\":   config.BridgeConfig.UserlandProxyPath}\n\tbridgeOption := options.Generic{netlabel.GenericData: bridgeConfig}\n\n\tdOptions := []nwconfig.Option{}\n\tdOptions = append(dOptions, nwconfig.OptionDriverConfig(\"bridge\", bridgeOption))\n\treturn dOptions\n}\n\nfunc initBridgeDriver(controller libnetwork.NetworkController, config *config.Config) error {\n\tbridgeName := bridge.DefaultBridgeName\n\tif config.BridgeConfig.Iface != \"\" {\n\t\tbridgeName = config.BridgeConfig.Iface\n\t}\n\tnetOption := map[string]string{\n\t\tbridge.BridgeName:         bridgeName,\n\t\tbridge.DefaultBridge:      strconv.FormatBool(true),\n\t\tnetlabel.DriverMTU:        strconv.Itoa(config.Mtu),\n\t\tbridge.EnableIPMasquerade: strconv.FormatBool(config.BridgeConfig.EnableIPMasq),\n\t\tbridge.EnableICC:          strconv.FormatBool(config.BridgeConfig.InterContainerCommunication),\n\t}\n\n\t// --ip processing\n\tif config.BridgeConfig.DefaultIP != nil {\n\t\tnetOption[bridge.DefaultBindingIP] = config.BridgeConfig.DefaultIP.String()\n\t}\n\n\tipamV4Conf := &libnetwork.IpamConf{AuxAddresses: make(map[string]string)}\n\n\tnwList, nw6List, err := netutils.ElectInterfaceAddresses(bridgeName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"list bridge addresses failed\")\n\t}\n\n\tnw := nwList[0]\n\tif len(nwList) > 1 && config.BridgeConfig.FixedCIDR != \"\" {\n\t\t_, fCIDR, err := net.ParseCIDR(config.BridgeConfig.FixedCIDR)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"parse CIDR failed\")\n\t\t}\n\t\t// Iterate through in case there are multiple addresses for the bridge\n\t\tfor _, entry := range nwList {\n\t\t\tif fCIDR.Contains(entry.IP) {\n\t\t\t\tnw = entry\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tipamV4Conf.PreferredPool = lntypes.GetIPNetCanonical(nw).String()\n\thip, _ := lntypes.GetHostPartIP(nw.IP, nw.Mask)\n\tif hip.IsGlobalUnicast() {\n\t\tipamV4Conf.Gateway = nw.IP.String()\n\t}\n\n\tif config.BridgeConfig.IP != \"\" {\n\t\tip, ipNet, err := net.ParseCIDR(config.BridgeConfig.IP)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tipamV4Conf.PreferredPool = ipNet.String()\n\t\tipamV4Conf.Gateway = ip.String()\n\t} else if bridgeName == bridge.DefaultBridgeName && ipamV4Conf.PreferredPool != \"\" {\n\t\tlogrus.Infof(\"Default bridge (%s) is assigned with an IP address %s. Daemon option --bip can be used to set a preferred IP address\", bridgeName, ipamV4Conf.PreferredPool)\n\t}\n\n\tif config.BridgeConfig.FixedCIDR != \"\" {\n\t\t_, fCIDR, err := net.ParseCIDR(config.BridgeConfig.FixedCIDR)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tipamV4Conf.SubPool = fCIDR.String()\n\t}\n\n\tif config.BridgeConfig.DefaultGatewayIPv4 != nil {\n\t\tipamV4Conf.AuxAddresses[\"DefaultGatewayIPv4\"] = config.BridgeConfig.DefaultGatewayIPv4.String()\n\t}\n\n\tvar (\n\t\tdeferIPv6Alloc bool\n\t\tipamV6Conf     *libnetwork.IpamConf\n\t)\n\n\tif config.BridgeConfig.EnableIPv6 && config.BridgeConfig.FixedCIDRv6 == \"\" {\n\t\treturn errdefs.InvalidParameter(errors.New(\"IPv6 is enabled for the default bridge, but no subnet is configured. Specify an IPv6 subnet using --fixed-cidr-v6\"))\n\t} else if config.BridgeConfig.FixedCIDRv6 != \"\" {\n\t\t_, fCIDRv6, err := net.ParseCIDR(config.BridgeConfig.FixedCIDRv6)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// In case user has specified the daemon flag --fixed-cidr-v6 and the passed network has\n\t\t// at least 48 host bits, we need to guarantee the current behavior where the containers'\n\t\t// IPv6 addresses will be constructed based on the containers' interface MAC address.\n\t\t// We do so by telling libnetwork to defer the IPv6 address allocation for the endpoints\n\t\t// on this network until after the driver has created the endpoint and returned the\n\t\t// constructed address. Libnetwork will then reserve this address with the ipam driver.\n\t\tones, _ := fCIDRv6.Mask.Size()\n\t\tdeferIPv6Alloc = ones <= 80\n\n\t\tipamV6Conf = &libnetwork.IpamConf{\n\t\t\tAuxAddresses:  make(map[string]string),\n\t\t\tPreferredPool: fCIDRv6.String(),\n\t\t}\n\n\t\t// In case the --fixed-cidr-v6 is specified and the current docker0 bridge IPv6\n\t\t// address belongs to the same network, we need to inform libnetwork about it, so\n\t\t// that it can be reserved with IPAM and it will not be given away to somebody else\n\t\tfor _, nw6 := range nw6List {\n\t\t\tif fCIDRv6.Contains(nw6.IP) {\n\t\t\t\tipamV6Conf.Gateway = nw6.IP.String()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif config.BridgeConfig.DefaultGatewayIPv6 != nil {\n\t\tif ipamV6Conf == nil {\n\t\t\tipamV6Conf = &libnetwork.IpamConf{AuxAddresses: make(map[string]string)}\n\t\t}\n\t\tipamV6Conf.AuxAddresses[\"DefaultGatewayIPv6\"] = config.BridgeConfig.DefaultGatewayIPv6.String()\n\t}\n\n\tv4Conf := []*libnetwork.IpamConf{ipamV4Conf}\n\tv6Conf := []*libnetwork.IpamConf{}\n\tif ipamV6Conf != nil {\n\t\tv6Conf = append(v6Conf, ipamV6Conf)\n\t}\n\t// Initialize default network on \"bridge\" with the same name\n\t_, err = controller.NewNetwork(\"bridge\", \"bridge\", \"\",\n\t\tlibnetwork.NetworkOptionEnableIPv6(config.BridgeConfig.EnableIPv6),\n\t\tlibnetwork.NetworkOptionDriverOpts(netOption),\n\t\tlibnetwork.NetworkOptionIpam(\"default\", \"\", v4Conf, v6Conf, nil),\n\t\tlibnetwork.NetworkOptionDeferIPv6Alloc(deferIPv6Alloc))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error creating default \\\"bridge\\\" network: %v\", err)\n\t}\n\treturn nil\n}\n\n// Remove default bridge interface if present (--bridge=none use case)\nfunc removeDefaultBridgeInterface() {\n\tif lnk, err := netlink.LinkByName(bridge.DefaultBridgeName); err == nil {\n\t\tif err := netlink.LinkDel(lnk); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to remove bridge interface (%s): %v\", bridge.DefaultBridgeName, err)\n\t\t}\n\t}\n}\n\nfunc setupInitLayer(idMapping *idtools.IdentityMapping) func(containerfs.ContainerFS) error {\n\treturn func(initPath containerfs.ContainerFS) error {\n\t\treturn initlayer.Setup(initPath, idMapping.RootPair())\n\t}\n}\n\n// Parse the remapped root (user namespace) option, which can be one of:\n//   username            - valid username from /etc/passwd\n//   username:groupname  - valid username; valid groupname from /etc/group\n//   uid                 - 32-bit unsigned int valid Linux UID value\n//   uid:gid             - uid value; 32-bit unsigned int Linux GID value\n//\n//  If no groupname is specified, and a username is specified, an attempt\n//  will be made to lookup a gid for that username as a groupname\n//\n//  If names are used, they are verified to exist in passwd/group\nfunc parseRemappedRoot(usergrp string) (string, string, error) {\n\n\tvar (\n\t\tuserID, groupID     int\n\t\tusername, groupname string\n\t)\n\n\tidparts := strings.Split(usergrp, \":\")\n\tif len(idparts) > 2 {\n\t\treturn \"\", \"\", fmt.Errorf(\"Invalid user/group specification in --userns-remap: %q\", usergrp)\n\t}\n\n\tif uid, err := strconv.ParseInt(idparts[0], 10, 32); err == nil {\n\t\t// must be a uid; take it as valid\n\t\tuserID = int(uid)\n\t\tluser, err := idtools.LookupUID(userID)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Uid %d has no entry in /etc/passwd: %v\", userID, err)\n\t\t}\n\t\tusername = luser.Name\n\t\tif len(idparts) == 1 {\n\t\t\t// if the uid was numeric and no gid was specified, take the uid as the gid\n\t\t\tgroupID = userID\n\t\t\tlgrp, err := idtools.LookupGID(groupID)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Gid %d has no entry in /etc/group: %v\", groupID, err)\n\t\t\t}\n\t\t\tgroupname = lgrp.Name\n\t\t}\n\t} else {\n\t\tlookupName := idparts[0]\n\t\t// special case: if the user specified \"default\", they want Docker to create or\n\t\t// use (after creation) the \"dockremap\" user/group for root remapping\n\t\tif lookupName == defaultIDSpecifier {\n\t\t\tlookupName = defaultRemappedID\n\t\t}\n\t\tluser, err := idtools.LookupUser(lookupName)\n\t\tif err != nil && idparts[0] != defaultIDSpecifier {\n\t\t\t// error if the name requested isn't the special \"dockremap\" ID\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during uid lookup for %q: %v\", lookupName, err)\n\t\t} else if err != nil {\n\t\t\t// special case-- if the username == \"default\", then we have been asked\n\t\t\t// to create a new entry pair in /etc/{passwd,group} for which the /etc/sub{uid,gid}\n\t\t\t// ranges will be used for the user and group mappings in user namespaced containers\n\t\t\t_, _, err := idtools.AddNamespaceRangesUser(defaultRemappedID)\n\t\t\tif err == nil {\n\t\t\t\treturn defaultRemappedID, defaultRemappedID, nil\n\t\t\t}\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during %q user creation: %v\", defaultRemappedID, err)\n\t\t}\n\t\tusername = luser.Name\n\t\tif len(idparts) == 1 {\n\t\t\t// we only have a string username, and no group specified; look up gid from username as group\n\t\t\tgroup, err := idtools.LookupGroup(lookupName)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during gid lookup for %q: %v\", lookupName, err)\n\t\t\t}\n\t\t\tgroupname = group.Name\n\t\t}\n\t}\n\n\tif len(idparts) == 2 {\n\t\t// groupname or gid is separately specified and must be resolved\n\t\t// to an unsigned 32-bit gid\n\t\tif gid, err := strconv.ParseInt(idparts[1], 10, 32); err == nil {\n\t\t\t// must be a gid, take it as valid\n\t\t\tgroupID = int(gid)\n\t\t\tlgrp, err := idtools.LookupGID(groupID)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Gid %d has no entry in /etc/passwd: %v\", groupID, err)\n\t\t\t}\n\t\t\tgroupname = lgrp.Name\n\t\t} else {\n\t\t\t// not a number; attempt a lookup\n\t\t\tif _, err := idtools.LookupGroup(idparts[1]); err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during groupname lookup for %q: %v\", idparts[1], err)\n\t\t\t}\n\t\t\tgroupname = idparts[1]\n\t\t}\n\t}\n\treturn username, groupname, nil\n}\n\nfunc setupRemappedRoot(config *config.Config) (*idtools.IdentityMapping, error) {\n\tif runtime.GOOS != \"linux\" && config.RemappedRoot != \"\" {\n\t\treturn nil, fmt.Errorf(\"User namespaces are only supported on Linux\")\n\t}\n\n\t// if the daemon was started with remapped root option, parse\n\t// the config option to the int uid,gid values\n\tif config.RemappedRoot != \"\" {\n\t\tusername, groupname, err := parseRemappedRoot(config.RemappedRoot)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif username == \"root\" {\n\t\t\t// Cannot setup user namespaces with a 1-to-1 mapping; \"--root=0:0\" is a no-op\n\t\t\t// effectively\n\t\t\tlogrus.Warn(\"User namespaces: root cannot be remapped with itself; user namespaces are OFF\")\n\t\t\treturn &idtools.IdentityMapping{}, nil\n\t\t}\n\t\tlogrus.Infof(\"User namespaces: ID ranges will be mapped to subuid/subgid ranges of: %s\", username)\n\t\t// update remapped root setting now that we have resolved them to actual names\n\t\tconfig.RemappedRoot = fmt.Sprintf(\"%s:%s\", username, groupname)\n\n\t\tmappings, err := idtools.NewIdentityMapping(username)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't create ID mappings\")\n\t\t}\n\t\treturn mappings, nil\n\t}\n\treturn &idtools.IdentityMapping{}, nil\n}\n\nfunc setupDaemonRoot(config *config.Config, rootDir string, rootIdentity idtools.Identity) error {\n\tconfig.Root = rootDir\n\t// the docker root metadata directory needs to have execute permissions for all users (g+x,o+x)\n\t// so that syscalls executing as non-root, operating on subdirectories of the graph root\n\t// (e.g. mounted layers of a container) can traverse this path.\n\t// The user namespace support will create subdirectories for the remapped root host uid:gid\n\t// pair owned by that same uid:gid pair for proper write access to those needed metadata and\n\t// layer content subtrees.\n\tif _, err := os.Stat(rootDir); err == nil {\n\t\t// root current exists; verify the access bits are correct by setting them\n\t\tif err = os.Chmod(rootDir, 0711); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if os.IsNotExist(err) {\n\t\t// no root exists yet, create it 0711 with root:root ownership\n\t\tif err := os.MkdirAll(rootDir, 0711); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// if user namespaces are enabled we will create a subtree underneath the specified root\n\t// with any/all specified remapped root uid/gid options on the daemon creating\n\t// a new subdirectory with ownership set to the remapped uid/gid (so as to allow\n\t// `chdir()` to work for containers namespaced to that uid/gid)\n\tif config.RemappedRoot != \"\" {\n\t\tconfig.Root = filepath.Join(rootDir, fmt.Sprintf(\"%d.%d\", rootIdentity.UID, rootIdentity.GID))\n\t\tlogrus.Debugf(\"Creating user namespaced daemon root: %s\", config.Root)\n\t\t// Create the root directory if it doesn't exist\n\t\tif err := idtools.MkdirAllAndChown(config.Root, 0700, rootIdentity); err != nil {\n\t\t\treturn fmt.Errorf(\"Cannot create daemon root: %s: %v\", config.Root, err)\n\t\t}\n\t\t// we also need to verify that any pre-existing directories in the path to\n\t\t// the graphroot won't block access to remapped root--if any pre-existing directory\n\t\t// has strict permissions that don't allow \"x\", container start will fail, so\n\t\t// better to warn and fail now\n\t\tdirPath := config.Root\n\t\tfor {\n\t\t\tdirPath = filepath.Dir(dirPath)\n\t\t\tif dirPath == \"/\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif !idtools.CanAccess(dirPath, rootIdentity) {\n\t\t\t\treturn fmt.Errorf(\"a subdirectory in your graphroot path (%s) restricts access to the remapped root uid/gid; please fix by allowing 'o+x' permissions on existing directories\", config.Root)\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := setupDaemonRootPropagation(config); err != nil {\n\t\tlogrus.WithError(err).WithField(\"dir\", config.Root).Warn(\"Error while setting daemon root propagation, this is not generally critical but may cause some functionality to not work or fallback to less desirable behavior\")\n\t}\n\treturn nil\n}\n\nfunc setupDaemonRootPropagation(cfg *config.Config) error {\n\trootParentMount, mountOptions, err := getSourceMount(cfg.Root)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"error getting daemon root's parent mount\")\n\t}\n\n\tvar cleanupOldFile bool\n\tcleanupFile := getUnmountOnShutdownPath(cfg)\n\tdefer func() {\n\t\tif !cleanupOldFile {\n\t\t\treturn\n\t\t}\n\t\tif err := os.Remove(cleanupFile); err != nil && !os.IsNotExist(err) {\n\t\t\tlogrus.WithError(err).WithField(\"file\", cleanupFile).Warn(\"could not clean up old root propagation unmount file\")\n\t\t}\n\t}()\n\n\tif hasMountInfoOption(mountOptions, sharedPropagationOption, slavePropagationOption) {\n\t\tcleanupOldFile = true\n\t\treturn nil\n\t}\n\n\tif err := mount.MakeShared(cfg.Root); err != nil {\n\t\treturn errors.Wrap(err, \"could not setup daemon root propagation to shared\")\n\t}\n\n\t// check the case where this may have already been a mount to itself.\n\t// If so then the daemon only performed a remount and should not try to unmount this later.\n\tif rootParentMount == cfg.Root {\n\t\tcleanupOldFile = true\n\t\treturn nil\n\t}\n\n\tif err := os.MkdirAll(filepath.Dir(cleanupFile), 0700); err != nil {\n\t\treturn errors.Wrap(err, \"error creating dir to store mount cleanup file\")\n\t}\n\n\tif err := ioutil.WriteFile(cleanupFile, nil, 0600); err != nil {\n\t\treturn errors.Wrap(err, \"error writing file to signal mount cleanup on shutdown\")\n\t}\n\treturn nil\n}\n\n// getUnmountOnShutdownPath generates the path to used when writing the file that signals to the daemon that on shutdown\n// the daemon root should be unmounted.\nfunc getUnmountOnShutdownPath(config *config.Config) string {\n\treturn filepath.Join(config.ExecRoot, \"unmount-on-shutdown\")\n}\n\n// registerLinks writes the links to a file.\nfunc (daemon *Daemon) registerLinks(container *container.Container, hostConfig *containertypes.HostConfig) error {\n\tif hostConfig == nil || hostConfig.NetworkMode.IsUserDefined() {\n\t\treturn nil\n\t}\n\n\tfor _, l := range hostConfig.Links {\n\t\tname, alias, err := opts.ParseLink(l)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tchild, err := daemon.GetContainer(name)\n\t\tif err != nil {\n\t\t\tif errdefs.IsNotFound(err) {\n\t\t\t\t// Trying to link to a non-existing container is not valid, and\n\t\t\t\t// should return an \"invalid parameter\" error. Returning a \"not\n\t\t\t\t// found\" error here would make the client report the container's\n\t\t\t\t// image could not be found (see moby/moby#39823)\n\t\t\t\terr = errdefs.InvalidParameter(err)\n\t\t\t}\n\t\t\treturn errors.Wrapf(err, \"could not get container for %s\", name)\n\t\t}\n\t\tfor child.HostConfig.NetworkMode.IsContainer() {\n\t\t\tparts := strings.SplitN(string(child.HostConfig.NetworkMode), \":\", 2)\n\t\t\tchild, err = daemon.GetContainer(parts[1])\n\t\t\tif err != nil {\n\t\t\t\tif errdefs.IsNotFound(err) {\n\t\t\t\t\t// Trying to link to a non-existing container is not valid, and\n\t\t\t\t\t// should return an \"invalid parameter\" error. Returning a \"not\n\t\t\t\t\t// found\" error here would make the client report the container's\n\t\t\t\t\t// image could not be found (see moby/moby#39823)\n\t\t\t\t\terr = errdefs.InvalidParameter(err)\n\t\t\t\t}\n\t\t\t\treturn errors.Wrapf(err, \"Could not get container for %s\", parts[1])\n\t\t\t}\n\t\t}\n\t\tif child.HostConfig.NetworkMode.IsHost() {\n\t\t\treturn runconfig.ErrConflictHostNetworkAndLinks\n\t\t}\n\t\tif err := daemon.registerLink(container, child, alias); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// After we load all the links into the daemon\n\t// set them to nil on the hostconfig\n\t_, err := container.WriteHostConfig()\n\treturn err\n}\n\n// conditionalMountOnStart is a platform specific helper function during the\n// container start to call mount.\nfunc (daemon *Daemon) conditionalMountOnStart(container *container.Container) error {\n\treturn daemon.Mount(container)\n}\n\n// conditionalUnmountOnCleanup is a platform specific helper function called\n// during the cleanup of a container to unmount.\nfunc (daemon *Daemon) conditionalUnmountOnCleanup(container *container.Container) error {\n\treturn daemon.Unmount(container)\n}\n\nfunc copyBlkioEntry(entries []*statsV1.BlkIOEntry) []types.BlkioStatEntry {\n\tout := make([]types.BlkioStatEntry, len(entries))\n\tfor i, re := range entries {\n\t\tout[i] = types.BlkioStatEntry{\n\t\t\tMajor: re.Major,\n\t\t\tMinor: re.Minor,\n\t\t\tOp:    re.Op,\n\t\t\tValue: re.Value,\n\t\t}\n\t}\n\treturn out\n}\n\nfunc (daemon *Daemon) stats(c *container.Container) (*types.StatsJSON, error) {\n\tif !c.IsRunning() {\n\t\treturn nil, errNotRunning(c.ID)\n\t}\n\tcs, err := daemon.containerd.Stats(context.Background(), c.ID)\n\tif err != nil {\n\t\tif strings.Contains(err.Error(), \"container not found\") {\n\t\t\treturn nil, containerNotFound(c.ID)\n\t\t}\n\t\treturn nil, err\n\t}\n\ts := &types.StatsJSON{}\n\ts.Read = cs.Read\n\tstats := cs.Metrics\n\tswitch t := stats.(type) {\n\tcase *statsV1.Metrics:\n\t\treturn daemon.statsV1(s, t)\n\tcase *statsV2.Metrics:\n\t\treturn daemon.statsV2(s, t)\n\tdefault:\n\t\treturn nil, errors.Errorf(\"unexpected type of metrics %+v\", t)\n\t}\n}\n\nfunc (daemon *Daemon) statsV1(s *types.StatsJSON, stats *statsV1.Metrics) (*types.StatsJSON, error) {\n\tif stats.Blkio != nil {\n\t\ts.BlkioStats = types.BlkioStats{\n\t\t\tIoServiceBytesRecursive: copyBlkioEntry(stats.Blkio.IoServiceBytesRecursive),\n\t\t\tIoServicedRecursive:     copyBlkioEntry(stats.Blkio.IoServicedRecursive),\n\t\t\tIoQueuedRecursive:       copyBlkioEntry(stats.Blkio.IoQueuedRecursive),\n\t\t\tIoServiceTimeRecursive:  copyBlkioEntry(stats.Blkio.IoServiceTimeRecursive),\n\t\t\tIoWaitTimeRecursive:     copyBlkioEntry(stats.Blkio.IoWaitTimeRecursive),\n\t\t\tIoMergedRecursive:       copyBlkioEntry(stats.Blkio.IoMergedRecursive),\n\t\t\tIoTimeRecursive:         copyBlkioEntry(stats.Blkio.IoTimeRecursive),\n\t\t\tSectorsRecursive:        copyBlkioEntry(stats.Blkio.SectorsRecursive),\n\t\t}\n\t}\n\tif stats.CPU != nil {\n\t\ts.CPUStats = types.CPUStats{\n\t\t\tCPUUsage: types.CPUUsage{\n\t\t\t\tTotalUsage:        stats.CPU.Usage.Total,\n\t\t\t\tPercpuUsage:       stats.CPU.Usage.PerCPU,\n\t\t\t\tUsageInKernelmode: stats.CPU.Usage.Kernel,\n\t\t\t\tUsageInUsermode:   stats.CPU.Usage.User,\n\t\t\t},\n\t\t\tThrottlingData: types.ThrottlingData{\n\t\t\t\tPeriods:          stats.CPU.Throttling.Periods,\n\t\t\t\tThrottledPeriods: stats.CPU.Throttling.ThrottledPeriods,\n\t\t\t\tThrottledTime:    stats.CPU.Throttling.ThrottledTime,\n\t\t\t},\n\t\t}\n\t}\n\n\tif stats.Memory != nil {\n\t\traw := make(map[string]uint64)\n\t\traw[\"cache\"] = stats.Memory.Cache\n\t\traw[\"rss\"] = stats.Memory.RSS\n\t\traw[\"rss_huge\"] = stats.Memory.RSSHuge\n\t\traw[\"mapped_file\"] = stats.Memory.MappedFile\n\t\traw[\"dirty\"] = stats.Memory.Dirty\n\t\traw[\"writeback\"] = stats.Memory.Writeback\n\t\traw[\"pgpgin\"] = stats.Memory.PgPgIn\n\t\traw[\"pgpgout\"] = stats.Memory.PgPgOut\n\t\traw[\"pgfault\"] = stats.Memory.PgFault\n\t\traw[\"pgmajfault\"] = stats.Memory.PgMajFault\n\t\traw[\"inactive_anon\"] = stats.Memory.InactiveAnon\n\t\traw[\"active_anon\"] = stats.Memory.ActiveAnon\n\t\traw[\"inactive_file\"] = stats.Memory.InactiveFile\n\t\traw[\"active_file\"] = stats.Memory.ActiveFile\n\t\traw[\"unevictable\"] = stats.Memory.Unevictable\n\t\traw[\"hierarchical_memory_limit\"] = stats.Memory.HierarchicalMemoryLimit\n\t\traw[\"hierarchical_memsw_limit\"] = stats.Memory.HierarchicalSwapLimit\n\t\traw[\"total_cache\"] = stats.Memory.TotalCache\n\t\traw[\"total_rss\"] = stats.Memory.TotalRSS\n\t\traw[\"total_rss_huge\"] = stats.Memory.TotalRSSHuge\n\t\traw[\"total_mapped_file\"] = stats.Memory.TotalMappedFile\n\t\traw[\"total_dirty\"] = stats.Memory.TotalDirty\n\t\traw[\"total_writeback\"] = stats.Memory.TotalWriteback\n\t\traw[\"total_pgpgin\"] = stats.Memory.TotalPgPgIn\n\t\traw[\"total_pgpgout\"] = stats.Memory.TotalPgPgOut\n\t\traw[\"total_pgfault\"] = stats.Memory.TotalPgFault\n\t\traw[\"total_pgmajfault\"] = stats.Memory.TotalPgMajFault\n\t\traw[\"total_inactive_anon\"] = stats.Memory.TotalInactiveAnon\n\t\traw[\"total_active_anon\"] = stats.Memory.TotalActiveAnon\n\t\traw[\"total_inactive_file\"] = stats.Memory.TotalInactiveFile\n\t\traw[\"total_active_file\"] = stats.Memory.TotalActiveFile\n\t\traw[\"total_unevictable\"] = stats.Memory.TotalUnevictable\n\n\t\tif stats.Memory.Usage != nil {\n\t\t\ts.MemoryStats = types.MemoryStats{\n\t\t\t\tStats:    raw,\n\t\t\t\tUsage:    stats.Memory.Usage.Usage,\n\t\t\t\tMaxUsage: stats.Memory.Usage.Max,\n\t\t\t\tLimit:    stats.Memory.Usage.Limit,\n\t\t\t\tFailcnt:  stats.Memory.Usage.Failcnt,\n\t\t\t}\n\t\t} else {\n\t\t\ts.MemoryStats = types.MemoryStats{\n\t\t\t\tStats: raw,\n\t\t\t}\n\t\t}\n\n\t\t// if the container does not set memory limit, use the machineMemory\n\t\tif s.MemoryStats.Limit > daemon.machineMemory && daemon.machineMemory > 0 {\n\t\t\ts.MemoryStats.Limit = daemon.machineMemory\n\t\t}\n\t}\n\n\tif stats.Pids != nil {\n\t\ts.PidsStats = types.PidsStats{\n\t\t\tCurrent: stats.Pids.Current,\n\t\t\tLimit:   stats.Pids.Limit,\n\t\t}\n\t}\n\n\treturn s, nil\n}\n\nfunc (daemon *Daemon) statsV2(s *types.StatsJSON, stats *statsV2.Metrics) (*types.StatsJSON, error) {\n\tif stats.Io != nil {\n\t\tvar isbr []types.BlkioStatEntry\n\t\tfor _, re := range stats.Io.Usage {\n\t\t\tisbr = append(isbr,\n\t\t\t\ttypes.BlkioStatEntry{\n\t\t\t\t\tMajor: re.Major,\n\t\t\t\t\tMinor: re.Minor,\n\t\t\t\t\tOp:    \"read\",\n\t\t\t\t\tValue: re.Rbytes,\n\t\t\t\t},\n\t\t\t\ttypes.BlkioStatEntry{\n\t\t\t\t\tMajor: re.Major,\n\t\t\t\t\tMinor: re.Minor,\n\t\t\t\t\tOp:    \"write\",\n\t\t\t\t\tValue: re.Wbytes,\n\t\t\t\t},\n\t\t\t)\n\t\t}\n\t\ts.BlkioStats = types.BlkioStats{\n\t\t\tIoServiceBytesRecursive: isbr,\n\t\t\t// Other fields are unsupported\n\t\t}\n\t}\n\n\tif stats.CPU != nil {\n\t\ts.CPUStats = types.CPUStats{\n\t\t\tCPUUsage: types.CPUUsage{\n\t\t\t\tTotalUsage: stats.CPU.UsageUsec * 1000,\n\t\t\t\t// PercpuUsage is not supported\n\t\t\t\tUsageInKernelmode: stats.CPU.SystemUsec * 1000,\n\t\t\t\tUsageInUsermode:   stats.CPU.UserUsec * 1000,\n\t\t\t},\n\t\t\tThrottlingData: types.ThrottlingData{\n\t\t\t\tPeriods:          stats.CPU.NrPeriods,\n\t\t\t\tThrottledPeriods: stats.CPU.NrThrottled,\n\t\t\t\tThrottledTime:    stats.CPU.ThrottledUsec * 1000,\n\t\t\t},\n\t\t}\n\t}\n\n\tif stats.Memory != nil {\n\t\traw := make(map[string]uint64)\n\t\traw[\"anon\"] = stats.Memory.Anon\n\t\traw[\"file\"] = stats.Memory.File\n\t\traw[\"kernel_stack\"] = stats.Memory.KernelStack\n\t\traw[\"slab\"] = stats.Memory.Slab\n\t\traw[\"sock\"] = stats.Memory.Sock\n\t\traw[\"shmem\"] = stats.Memory.Shmem\n\t\traw[\"file_mapped\"] = stats.Memory.FileMapped\n\t\traw[\"file_dirty\"] = stats.Memory.FileDirty\n\t\traw[\"file_writeback\"] = stats.Memory.FileWriteback\n\t\traw[\"anon_thp\"] = stats.Memory.AnonThp\n\t\traw[\"inactive_anon\"] = stats.Memory.InactiveAnon\n\t\traw[\"active_anon\"] = stats.Memory.ActiveAnon\n\t\traw[\"inactive_file\"] = stats.Memory.InactiveFile\n\t\traw[\"active_file\"] = stats.Memory.ActiveFile\n\t\traw[\"unevictable\"] = stats.Memory.Unevictable\n\t\traw[\"slab_reclaimable\"] = stats.Memory.SlabReclaimable\n\t\traw[\"slab_unreclaimable\"] = stats.Memory.SlabUnreclaimable\n\t\traw[\"pgfault\"] = stats.Memory.Pgfault\n\t\traw[\"pgmajfault\"] = stats.Memory.Pgmajfault\n\t\traw[\"workingset_refault\"] = stats.Memory.WorkingsetRefault\n\t\traw[\"workingset_activate\"] = stats.Memory.WorkingsetActivate\n\t\traw[\"workingset_nodereclaim\"] = stats.Memory.WorkingsetNodereclaim\n\t\traw[\"pgrefill\"] = stats.Memory.Pgrefill\n\t\traw[\"pgscan\"] = stats.Memory.Pgscan\n\t\traw[\"pgsteal\"] = stats.Memory.Pgsteal\n\t\traw[\"pgactivate\"] = stats.Memory.Pgactivate\n\t\traw[\"pgdeactivate\"] = stats.Memory.Pgdeactivate\n\t\traw[\"pglazyfree\"] = stats.Memory.Pglazyfree\n\t\traw[\"pglazyfreed\"] = stats.Memory.Pglazyfreed\n\t\traw[\"thp_fault_alloc\"] = stats.Memory.ThpFaultAlloc\n\t\traw[\"thp_collapse_alloc\"] = stats.Memory.ThpCollapseAlloc\n\t\ts.MemoryStats = types.MemoryStats{\n\t\t\t// Stats is not compatible with v1\n\t\t\tStats: raw,\n\t\t\tUsage: stats.Memory.Usage,\n\t\t\t// MaxUsage is not supported\n\t\t\tLimit: stats.Memory.UsageLimit,\n\t\t}\n\t\t// if the container does not set memory limit, use the machineMemory\n\t\tif s.MemoryStats.Limit > daemon.machineMemory && daemon.machineMemory > 0 {\n\t\t\ts.MemoryStats.Limit = daemon.machineMemory\n\t\t}\n\t\tif stats.MemoryEvents != nil {\n\t\t\t// Failcnt is set to the \"oom\" field of the \"memory.events\" file.\n\t\t\t// See https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html\n\t\t\ts.MemoryStats.Failcnt = stats.MemoryEvents.Oom\n\t\t}\n\t}\n\n\tif stats.Pids != nil {\n\t\ts.PidsStats = types.PidsStats{\n\t\t\tCurrent: stats.Pids.Current,\n\t\t\tLimit:   stats.Pids.Limit,\n\t\t}\n\t}\n\n\treturn s, nil\n}\n\n// setDefaultIsolation determines the default isolation mode for the\n// daemon to run in. This is only applicable on Windows\nfunc (daemon *Daemon) setDefaultIsolation() error {\n\treturn nil\n}\n\n// setupDaemonProcess sets various settings for the daemon's process\nfunc setupDaemonProcess(config *config.Config) error {\n\t// setup the daemons oom_score_adj\n\tif err := setupOOMScoreAdj(config.OOMScoreAdjust); err != nil {\n\t\treturn err\n\t}\n\tif err := setMayDetachMounts(); err != nil {\n\t\tlogrus.WithError(err).Warn(\"Could not set may_detach_mounts kernel parameter\")\n\t}\n\treturn nil\n}\n\n// This is used to allow removal of mountpoints that may be mounted in other\n// namespaces on RHEL based kernels starting from RHEL 7.4.\n// Without this setting, removals on these RHEL based kernels may fail with\n// \"device or resource busy\".\n// This setting is not available in upstream kernels as it is not configurable,\n// but has been in the upstream kernels since 3.15.\nfunc setMayDetachMounts() error {\n\tf, err := os.OpenFile(\"/proc/sys/fs/may_detach_mounts\", os.O_WRONLY, 0)\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errors.Wrap(err, \"error opening may_detach_mounts kernel config file\")\n\t}\n\tdefer f.Close()\n\n\t_, err = f.WriteString(\"1\")\n\tif os.IsPermission(err) {\n\t\t// Setting may_detach_mounts does not work in an\n\t\t// unprivileged container. Ignore the error, but log\n\t\t// it if we appear not to be in that situation.\n\t\tif !sys.RunningInUserNS() {\n\t\t\tlogrus.Debugf(\"Permission denied writing %q to /proc/sys/fs/may_detach_mounts\", \"1\")\n\t\t}\n\t\treturn nil\n\t}\n\treturn err\n}\n\nfunc setupOOMScoreAdj(score int) error {\n\tif score == 0 {\n\t\treturn nil\n\t}\n\tf, err := os.OpenFile(\"/proc/self/oom_score_adj\", os.O_WRONLY, 0)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\tstringScore := strconv.Itoa(score)\n\t_, err = f.WriteString(stringScore)\n\tif os.IsPermission(err) {\n\t\t// Setting oom_score_adj does not work in an\n\t\t// unprivileged container. Ignore the error, but log\n\t\t// it if we appear not to be in that situation.\n\t\tif !sys.RunningInUserNS() {\n\t\t\tlogrus.Debugf(\"Permission denied writing %q to /proc/self/oom_score_adj\", stringScore)\n\t\t}\n\t\treturn nil\n\t}\n\n\treturn err\n}\n\nfunc (daemon *Daemon) initCPURtController(mnt, path string) error {\n\tif path == \"/\" || path == \".\" {\n\t\treturn nil\n\t}\n\n\t// Recursively create cgroup to ensure that the system and all parent cgroups have values set\n\t// for the period and runtime as this limits what the children can be set to.\n\tif err := daemon.initCPURtController(mnt, filepath.Dir(path)); err != nil {\n\t\treturn err\n\t}\n\n\tpath = filepath.Join(mnt, path)\n\tif err := os.MkdirAll(path, 0755); err != nil {\n\t\treturn err\n\t}\n\tif err := maybeCreateCPURealTimeFile(daemon.configStore.CPURealtimePeriod, \"cpu.rt_period_us\", path); err != nil {\n\t\treturn err\n\t}\n\treturn maybeCreateCPURealTimeFile(daemon.configStore.CPURealtimeRuntime, \"cpu.rt_runtime_us\", path)\n}\n\nfunc maybeCreateCPURealTimeFile(configValue int64, file string, path string) error {\n\tif configValue == 0 {\n\t\treturn nil\n\t}\n\treturn ioutil.WriteFile(filepath.Join(path, file), []byte(strconv.FormatInt(configValue, 10)), 0700)\n}\n\nfunc (daemon *Daemon) setupSeccompProfile() error {\n\tif daemon.configStore.SeccompProfile != \"\" {\n\t\tdaemon.seccompProfilePath = daemon.configStore.SeccompProfile\n\t\tb, err := ioutil.ReadFile(daemon.configStore.SeccompProfile)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"opening seccomp profile (%s) failed: %v\", daemon.configStore.SeccompProfile, err)\n\t\t}\n\t\tdaemon.seccompProfile = b\n\t}\n\treturn nil\n}\n\n// RawSysInfo returns *sysinfo.SysInfo .\nfunc (daemon *Daemon) RawSysInfo(quiet bool) *sysinfo.SysInfo {\n\tvar opts []sysinfo.Opt\n\tif daemon.getCgroupDriver() == cgroupSystemdDriver {\n\t\trootlesskitParentEUID := os.Getenv(\"ROOTLESSKIT_PARENT_EUID\")\n\t\tif rootlesskitParentEUID != \"\" {\n\t\t\tgroupPath := fmt.Sprintf(\"/user.slice/user-%s.slice\", rootlesskitParentEUID)\n\t\t\topts = append(opts, sysinfo.WithCgroup2GroupPath(groupPath))\n\t\t}\n\t}\n\treturn sysinfo.New(quiet, opts...)\n}\n\nfunc recursiveUnmount(target string) error {\n\treturn mount.RecursiveUnmount(target)\n}\n", "// +build linux\n\n/*\n\naufs driver directory structure\n\n  .\n  \u251c\u2500\u2500 layers // Metadata of layers\n  \u2502   \u251c\u2500\u2500 1\n  \u2502   \u251c\u2500\u2500 2\n  \u2502   \u2514\u2500\u2500 3\n  \u251c\u2500\u2500 diff  // Content of the layer\n  \u2502   \u251c\u2500\u2500 1  // Contains layers that need to be mounted for the id\n  \u2502   \u251c\u2500\u2500 2\n  \u2502   \u2514\u2500\u2500 3\n  \u2514\u2500\u2500 mnt    // Mount points for the rw layers to be mounted\n      \u251c\u2500\u2500 1\n      \u251c\u2500\u2500 2\n      \u2514\u2500\u2500 3\n\n*/\n\npackage aufs // import \"github.com/docker/docker/daemon/graphdriver/aufs\"\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/chrootarchive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/directory\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vbatts/tar-split/tar/storage\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t// ErrAufsNotSupported is returned if aufs is not supported by the host.\n\tErrAufsNotSupported = fmt.Errorf(\"AUFS was not found in /proc/filesystems\")\n\t// ErrAufsNested means aufs cannot be used bc we are in a user namespace\n\tErrAufsNested = fmt.Errorf(\"AUFS cannot be used in non-init user namespace\")\n\tbackingFs     = \"<unknown>\"\n\n\tenableDirpermLock sync.Once\n\tenableDirperm     bool\n\n\tlogger = logrus.WithField(\"storage-driver\", \"aufs\")\n)\n\nfunc init() {\n\tgraphdriver.Register(\"aufs\", Init)\n}\n\n// Driver contains information about the filesystem mounted.\ntype Driver struct {\n\troot          string\n\tuidMaps       []idtools.IDMap\n\tgidMaps       []idtools.IDMap\n\tctr           *graphdriver.RefCounter\n\tpathCacheLock sync.Mutex\n\tpathCache     map[string]string\n\tnaiveDiff     graphdriver.DiffDriver\n\tlocker        *locker.Locker\n\tmntL          sync.Mutex\n}\n\n// Init returns a new AUFS driver.\n// An error is returned if AUFS is not supported.\nfunc Init(root string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\t// Try to load the aufs kernel module\n\tif err := supportsAufs(); err != nil {\n\t\tlogger.Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\t// Perform feature detection on /var/lib/docker/aufs if it's an existing directory.\n\t// This covers situations where /var/lib/docker/aufs is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := root\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif fsName, ok := graphdriver.FsNames[fsMagic]; ok {\n\t\tbackingFs = fsName\n\t}\n\n\tswitch fsMagic {\n\tcase graphdriver.FsMagicAufs, graphdriver.FsMagicBtrfs, graphdriver.FsMagicEcryptfs:\n\t\tlogger.Errorf(\"AUFS is not supported over %s\", backingFs)\n\t\treturn nil, graphdriver.ErrIncompatibleFS\n\t}\n\n\tpaths := []string{\n\t\t\"mnt\",\n\t\t\"diff\",\n\t\t\"layers\",\n\t}\n\n\ta := &Driver{\n\t\troot:      root,\n\t\tuidMaps:   uidMaps,\n\t\tgidMaps:   gidMaps,\n\t\tpathCache: make(map[string]string),\n\t\tctr:       graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicAufs)),\n\t\tlocker:    locker.New(),\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create the root aufs driver dir\n\tif err := idtools.MkdirAllAndChown(root, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Populate the dir structure\n\tfor _, p := range paths {\n\t\tif err := idtools.MkdirAllAndChown(path.Join(root, p), 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tfor _, path := range []string{\"mnt\", \"diff\"} {\n\t\tp := filepath.Join(root, path)\n\t\tentries, err := ioutil.ReadDir(p)\n\t\tif err != nil {\n\t\t\tlogger.WithError(err).WithField(\"dir\", p).Error(\"error reading dir entries\")\n\t\t\tcontinue\n\t\t}\n\t\tfor _, entry := range entries {\n\t\t\tif !entry.IsDir() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif strings.HasSuffix(entry.Name(), \"-removing\") {\n\t\t\t\tlogger.WithField(\"dir\", entry.Name()).Debug(\"Cleaning up stale layer dir\")\n\t\t\t\tif err := system.EnsureRemoveAll(filepath.Join(p, entry.Name())); err != nil {\n\t\t\t\t\tlogger.WithField(\"dir\", entry.Name()).WithError(err).Error(\"Error removing stale layer dir\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\ta.naiveDiff = graphdriver.NewNaiveDiffDriver(a, uidMaps, gidMaps)\n\treturn a, nil\n}\n\n// Return a nil error if the kernel supports aufs\n// We cannot modprobe because inside dind modprobe fails\n// to run\nfunc supportsAufs() error {\n\t// We can try to modprobe aufs first before looking at\n\t// proc/filesystems for when aufs is supported\n\texec.Command(\"modprobe\", \"aufs\").Run()\n\n\tif sys.RunningInUserNS() {\n\t\treturn ErrAufsNested\n\t}\n\n\tf, err := os.Open(\"/proc/filesystems\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\ts := bufio.NewScanner(f)\n\tfor s.Scan() {\n\t\tif strings.Contains(s.Text(), \"aufs\") {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrAufsNotSupported\n}\n\nfunc (a *Driver) rootPath() string {\n\treturn a.root\n}\n\nfunc (*Driver) String() string {\n\treturn \"aufs\"\n}\n\n// Status returns current information about the filesystem such as root directory, number of directories mounted, etc.\nfunc (a *Driver) Status() [][2]string {\n\tids, _ := loadIds(path.Join(a.rootPath(), \"layers\"))\n\treturn [][2]string{\n\t\t{\"Root Dir\", a.rootPath()},\n\t\t{\"Backing Filesystem\", backingFs},\n\t\t{\"Dirs\", fmt.Sprintf(\"%d\", len(ids))},\n\t\t{\"Dirperm1 Supported\", fmt.Sprintf(\"%v\", useDirperm())},\n\t}\n}\n\n// GetMetadata not implemented\nfunc (a *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn nil, nil\n}\n\n// Exists returns true if the given id is registered with\n// this driver\nfunc (a *Driver) Exists(id string) bool {\n\tif _, err := os.Lstat(path.Join(a.rootPath(), \"layers\", id)); err != nil {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (a *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn a.Create(id, parent, opts)\n}\n\n// Create three folders for each id\n// mnt, layers, and diff\nfunc (a *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported for aufs\")\n\t}\n\n\tif err := a.createDirsFor(id); err != nil {\n\t\treturn err\n\t}\n\t// Write the layers metadata\n\tf, err := os.Create(path.Join(a.rootPath(), \"layers\", id))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\tif parent != \"\" {\n\t\tids, err := getParentIDs(a.rootPath(), parent)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif _, err := fmt.Fprintln(f, parent); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, i := range ids {\n\t\t\tif _, err := fmt.Fprintln(f, i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// createDirsFor creates two directories for the given id.\n// mnt and diff\nfunc (a *Driver) createDirsFor(id string) error {\n\tpaths := []string{\n\t\t\"mnt\",\n\t\t\"diff\",\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(a.uidMaps, a.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Directory permission is 0755.\n\t// The path of directories are <aufs_root_path>/mnt/<image_id>\n\t// and <aufs_root_path>/diff/<image_id>\n\tfor _, p := range paths {\n\t\tif err := idtools.MkdirAllAndChown(path.Join(a.rootPath(), p, id), 0755, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Remove will unmount and remove the given id.\nfunc (a *Driver) Remove(id string) error {\n\ta.locker.Lock(id)\n\tdefer a.locker.Unlock(id)\n\ta.pathCacheLock.Lock()\n\tmountpoint, exists := a.pathCache[id]\n\ta.pathCacheLock.Unlock()\n\tif !exists {\n\t\tmountpoint = a.getMountpoint(id)\n\t}\n\n\tif err := a.unmount(mountpoint); err != nil {\n\t\tlogger.WithError(err).WithField(\"method\", \"Remove()\").Warn()\n\t\treturn err\n\t}\n\n\t// Remove the layers file for the id\n\tif err := os.Remove(path.Join(a.rootPath(), \"layers\", id)); err != nil && !os.IsNotExist(err) {\n\t\treturn errors.Wrapf(err, \"error removing layers dir for %s\", id)\n\t}\n\n\tif err := atomicRemove(a.getDiffPath(id)); err != nil {\n\t\treturn errors.Wrapf(err, \"could not remove diff path for id %s\", id)\n\t}\n\n\t// Atomically remove each directory in turn by first moving it out of the\n\t// way (so that docker doesn't find it anymore) before doing removal of\n\t// the whole tree.\n\tif err := atomicRemove(mountpoint); err != nil {\n\t\tif errors.Is(err, unix.EBUSY) {\n\t\t\tlogger.WithField(\"dir\", mountpoint).WithError(err).Warn(\"error performing atomic remove due to EBUSY\")\n\t\t}\n\t\treturn errors.Wrapf(err, \"could not remove mountpoint for id %s\", id)\n\t}\n\n\ta.pathCacheLock.Lock()\n\tdelete(a.pathCache, id)\n\ta.pathCacheLock.Unlock()\n\treturn nil\n}\n\nfunc atomicRemove(source string) error {\n\ttarget := source + \"-removing\"\n\n\terr := os.Rename(source, target)\n\tswitch {\n\tcase err == nil, os.IsNotExist(err):\n\tcase os.IsExist(err):\n\t\t// Got error saying the target dir already exists, maybe the source doesn't exist due to a previous (failed) remove\n\t\tif _, e := os.Stat(source); !os.IsNotExist(e) {\n\t\t\treturn errors.Wrapf(err, \"target rename dir %q exists but should not, this needs to be manually cleaned up\", target)\n\t\t}\n\tdefault:\n\t\treturn errors.Wrapf(err, \"error preparing atomic delete\")\n\t}\n\n\treturn system.EnsureRemoveAll(target)\n}\n\n// Get returns the rootfs path for the id.\n// This will mount the dir at its given path\nfunc (a *Driver) Get(id, mountLabel string) (containerfs.ContainerFS, error) {\n\ta.locker.Lock(id)\n\tdefer a.locker.Unlock(id)\n\tparents, err := a.getParentLayerPaths(id)\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\ta.pathCacheLock.Lock()\n\tm, exists := a.pathCache[id]\n\ta.pathCacheLock.Unlock()\n\n\tif !exists {\n\t\tm = a.getDiffPath(id)\n\t\tif len(parents) > 0 {\n\t\t\tm = a.getMountpoint(id)\n\t\t}\n\t}\n\tif count := a.ctr.Increment(m); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(m), nil\n\t}\n\n\t// If a dir does not have a parent ( no layers )do not try to mount\n\t// just return the diff path to the data\n\tif len(parents) > 0 {\n\t\tif err := a.mount(id, m, mountLabel, parents); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\ta.pathCacheLock.Lock()\n\ta.pathCache[id] = m\n\ta.pathCacheLock.Unlock()\n\treturn containerfs.NewLocalContainerFS(m), nil\n}\n\n// Put unmounts and updates list of active mounts.\nfunc (a *Driver) Put(id string) error {\n\ta.locker.Lock(id)\n\tdefer a.locker.Unlock(id)\n\ta.pathCacheLock.Lock()\n\tm, exists := a.pathCache[id]\n\tif !exists {\n\t\tm = a.getMountpoint(id)\n\t\ta.pathCache[id] = m\n\t}\n\ta.pathCacheLock.Unlock()\n\tif count := a.ctr.Decrement(m); count > 0 {\n\t\treturn nil\n\t}\n\n\terr := a.unmount(m)\n\tif err != nil {\n\t\tlogger.WithError(err).WithField(\"method\", \"Put()\").Warn()\n\t}\n\treturn err\n}\n\n// isParent returns if the passed in parent is the direct parent of the passed in layer\nfunc (a *Driver) isParent(id, parent string) bool {\n\tparents, _ := getParentIDs(a.rootPath(), id)\n\tif parent == \"\" && len(parents) > 0 {\n\t\treturn false\n\t}\n\treturn !(len(parents) > 0 && parent != parents[0])\n}\n\n// Diff produces an archive of the changes between the specified\n// layer and its parent layer which may be \"\".\nfunc (a *Driver) Diff(id, parent string) (io.ReadCloser, error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.Diff(id, parent)\n\t}\n\n\t// AUFS doesn't need the parent layer to produce a diff.\n\treturn archive.TarWithOptions(path.Join(a.rootPath(), \"diff\", id), &archive.TarOptions{\n\t\tCompression:     archive.Uncompressed,\n\t\tExcludePatterns: []string{archive.WhiteoutMetaPrefix + \"*\", \"!\" + archive.WhiteoutOpaqueDir},\n\t\tUIDMaps:         a.uidMaps,\n\t\tGIDMaps:         a.gidMaps,\n\t})\n}\n\ntype fileGetNilCloser struct {\n\tstorage.FileGetter\n}\n\nfunc (f fileGetNilCloser) Close() error {\n\treturn nil\n}\n\n// DiffGetter returns a FileGetCloser that can read files from the directory that\n// contains files for the layer differences. Used for direct access for tar-split.\nfunc (a *Driver) DiffGetter(id string) (graphdriver.FileGetCloser, error) {\n\tp := path.Join(a.rootPath(), \"diff\", id)\n\treturn fileGetNilCloser{storage.NewPathFileGetter(p)}, nil\n}\n\nfunc (a *Driver) applyDiff(id string, diff io.Reader) error {\n\treturn chrootarchive.UntarUncompressed(diff, path.Join(a.rootPath(), \"diff\", id), &archive.TarOptions{\n\t\tUIDMaps: a.uidMaps,\n\t\tGIDMaps: a.gidMaps,\n\t})\n}\n\n// DiffSize calculates the changes between the specified id\n// and its parent and returns the size in bytes of the changes\n// relative to its base filesystem directory.\nfunc (a *Driver) DiffSize(id, parent string) (size int64, err error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.DiffSize(id, parent)\n\t}\n\t// AUFS doesn't need the parent layer to calculate the diff size.\n\treturn directory.Size(context.TODO(), path.Join(a.rootPath(), \"diff\", id))\n}\n\n// ApplyDiff extracts the changeset from the given diff into the\n// layer with the specified id and parent, returning the size of the\n// new layer in bytes.\nfunc (a *Driver) ApplyDiff(id, parent string, diff io.Reader) (size int64, err error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.ApplyDiff(id, parent, diff)\n\t}\n\n\t// AUFS doesn't need the parent id to apply the diff if it is the direct parent.\n\tif err = a.applyDiff(id, diff); err != nil {\n\t\treturn\n\t}\n\n\treturn a.DiffSize(id, parent)\n}\n\n// Changes produces a list of changes between the specified layer\n// and its parent layer. If parent is \"\", then all changes will be ADD changes.\nfunc (a *Driver) Changes(id, parent string) ([]archive.Change, error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.Changes(id, parent)\n\t}\n\n\t// AUFS doesn't have snapshots, so we need to get changes from all parent\n\t// layers.\n\tlayers, err := a.getParentLayerPaths(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn archive.Changes(layers, path.Join(a.rootPath(), \"diff\", id))\n}\n\nfunc (a *Driver) getParentLayerPaths(id string) ([]string, error) {\n\tparentIds, err := getParentIDs(a.rootPath(), id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlayers := make([]string, len(parentIds))\n\n\t// Get the diff paths for all the parent ids\n\tfor i, p := range parentIds {\n\t\tlayers[i] = path.Join(a.rootPath(), \"diff\", p)\n\t}\n\treturn layers, nil\n}\n\nfunc (a *Driver) mount(id string, target string, mountLabel string, layers []string) error {\n\t// If the id is mounted or we get an error return\n\tif mounted, err := a.mounted(target); err != nil || mounted {\n\t\treturn err\n\t}\n\n\trw := a.getDiffPath(id)\n\n\tif err := a.aufsMount(layers, rw, target, mountLabel); err != nil {\n\t\treturn fmt.Errorf(\"error creating aufs mount to %s: %v\", target, err)\n\t}\n\treturn nil\n}\n\nfunc (a *Driver) unmount(mountPath string) error {\n\tif mounted, err := a.mounted(mountPath); err != nil || !mounted {\n\t\treturn err\n\t}\n\treturn Unmount(mountPath)\n}\n\nfunc (a *Driver) mounted(mountpoint string) (bool, error) {\n\treturn graphdriver.Mounted(graphdriver.FsMagicAufs, mountpoint)\n}\n\n// Cleanup aufs and unmount all mountpoints\nfunc (a *Driver) Cleanup() error {\n\tdir := a.mntPath()\n\tfiles, err := ioutil.ReadDir(dir)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"aufs readdir error\")\n\t}\n\tfor _, f := range files {\n\t\tif !f.IsDir() {\n\t\t\tcontinue\n\t\t}\n\n\t\tm := path.Join(dir, f.Name())\n\n\t\tif err := a.unmount(m); err != nil {\n\t\t\tlogger.WithError(err).WithField(\"method\", \"Cleanup()\").Warn()\n\t\t}\n\t}\n\treturn mount.RecursiveUnmount(a.root)\n}\n\nfunc (a *Driver) aufsMount(ro []string, rw, target, mountLabel string) (err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tmount.Unmount(target)\n\t\t}\n\t}()\n\n\t// Mount options are clipped to page size(4096 bytes). If there are more\n\t// layers then these are remounted individually using append.\n\n\toffset := 54\n\tif useDirperm() {\n\t\toffset += len(\",dirperm1\")\n\t}\n\tb := make([]byte, unix.Getpagesize()-len(mountLabel)-offset) // room for xino & mountLabel\n\tbp := copy(b, fmt.Sprintf(\"br:%s=rw\", rw))\n\n\tindex := 0\n\tfor ; index < len(ro); index++ {\n\t\tlayer := fmt.Sprintf(\":%s=ro+wh\", ro[index])\n\t\tif bp+len(layer) > len(b) {\n\t\t\tbreak\n\t\t}\n\t\tbp += copy(b[bp:], layer)\n\t}\n\n\topts := \"dio,xino=/dev/shm/aufs.xino\"\n\tif useDirperm() {\n\t\topts += \",dirperm1\"\n\t}\n\tdata := label.FormatMountLabel(fmt.Sprintf(\"%s,%s\", string(b[:bp]), opts), mountLabel)\n\ta.mntL.Lock()\n\terr = unix.Mount(\"none\", target, \"aufs\", 0, data)\n\ta.mntL.Unlock()\n\tif err != nil {\n\t\terr = errors.Wrap(err, \"mount target=\"+target+\" data=\"+data)\n\t\treturn\n\t}\n\n\tfor index < len(ro) {\n\t\tbp = 0\n\t\tfor ; index < len(ro); index++ {\n\t\t\tlayer := fmt.Sprintf(\"append:%s=ro+wh,\", ro[index])\n\t\t\tif bp+len(layer) > len(b) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tbp += copy(b[bp:], layer)\n\t\t}\n\t\tdata := label.FormatMountLabel(string(b[:bp]), mountLabel)\n\t\ta.mntL.Lock()\n\t\terr = unix.Mount(\"none\", target, \"aufs\", unix.MS_REMOUNT, data)\n\t\ta.mntL.Unlock()\n\t\tif err != nil {\n\t\t\terr = errors.Wrap(err, \"mount target=\"+target+\" flags=MS_REMOUNT data=\"+data)\n\t\t\treturn\n\t\t}\n\t}\n\n\treturn\n}\n\n// useDirperm checks dirperm1 mount option can be used with the current\n// version of aufs.\nfunc useDirperm() bool {\n\tenableDirpermLock.Do(func() {\n\t\tbase, err := ioutil.TempDir(\"\", \"docker-aufs-base\")\n\t\tif err != nil {\n\t\t\tlogger.Errorf(\"error checking dirperm1: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.RemoveAll(base)\n\n\t\tunion, err := ioutil.TempDir(\"\", \"docker-aufs-union\")\n\t\tif err != nil {\n\t\t\tlogger.Errorf(\"error checking dirperm1: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.RemoveAll(union)\n\n\t\topts := fmt.Sprintf(\"br:%s,dirperm1,xino=/dev/shm/aufs.xino\", base)\n\t\tif err := unix.Mount(\"none\", union, \"aufs\", 0, opts); err != nil {\n\t\t\treturn\n\t\t}\n\t\tenableDirperm = true\n\t\tif err := Unmount(union); err != nil {\n\t\t\tlogger.Errorf(\"error checking dirperm1: failed to unmount %v\", err)\n\t\t}\n\t})\n\treturn enableDirperm\n}\n", "// +build linux\n\npackage btrfs // import \"github.com/docker/docker/daemon/graphdriver/btrfs\"\n\n/*\n#include <stdlib.h>\n#include <dirent.h>\n#include <btrfs/ioctl.h>\n#include <btrfs/ctree.h>\n\nstatic void set_name_btrfs_ioctl_vol_args_v2(struct btrfs_ioctl_vol_args_v2* btrfs_struct, const char* value) {\n    snprintf(btrfs_struct->name, BTRFS_SUBVOL_NAME_MAX, \"%s\", value);\n}\n*/\nimport \"C\"\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\tunits \"github.com/docker/go-units\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nfunc init() {\n\tgraphdriver.Register(\"btrfs\", Init)\n}\n\ntype btrfsOptions struct {\n\tminSpace uint64\n\tsize     uint64\n}\n\n// Init returns a new BTRFS driver.\n// An error is returned if BTRFS is not supported.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\n\t// Perform feature detection on /var/lib/docker/btrfs if it's an existing directory.\n\t// This covers situations where /var/lib/docker/btrfs is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := home\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif fsMagic != graphdriver.FsMagicBtrfs {\n\t\treturn nil, graphdriver.ErrPrerequisites\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAllAndChown(home, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\topt, userDiskQuota, err := parseOptions(options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// For some reason shared mount propagation between a container\n\t// and the host does not work for btrfs, and a remedy is to bind\n\t// mount graphdriver home to itself (even without changing the\n\t// propagation mode).\n\terr = mount.MakeMount(home)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to make %s a mount\", home)\n\t}\n\n\tdriver := &Driver{\n\t\thome:    home,\n\t\tuidMaps: uidMaps,\n\t\tgidMaps: gidMaps,\n\t\toptions: opt,\n\t}\n\n\tif userDiskQuota {\n\t\tif err := driver.subvolEnableQuota(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn graphdriver.NewNaiveDiffDriver(driver, uidMaps, gidMaps), nil\n}\n\nfunc parseOptions(opt []string) (btrfsOptions, bool, error) {\n\tvar options btrfsOptions\n\tuserDiskQuota := false\n\tfor _, option := range opt {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn options, userDiskQuota, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"btrfs.min_space\":\n\t\t\tminSpace, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn options, userDiskQuota, err\n\t\t\t}\n\t\t\tuserDiskQuota = true\n\t\t\toptions.minSpace = uint64(minSpace)\n\t\tdefault:\n\t\t\treturn options, userDiskQuota, fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\treturn options, userDiskQuota, nil\n}\n\n// Driver contains information about the filesystem mounted.\ntype Driver struct {\n\t// root of the file system\n\thome         string\n\tuidMaps      []idtools.IDMap\n\tgidMaps      []idtools.IDMap\n\toptions      btrfsOptions\n\tquotaEnabled bool\n\tonce         sync.Once\n}\n\n// String prints the name of the driver (btrfs).\nfunc (d *Driver) String() string {\n\treturn \"btrfs\"\n}\n\n// Status returns current driver information in a two dimensional string array.\n// Output contains \"Build Version\" and \"Library Version\" of the btrfs libraries used.\n// Version information can be used to check compatibility with your kernel.\nfunc (d *Driver) Status() [][2]string {\n\tstatus := [][2]string{}\n\tif bv := btrfsBuildVersion(); bv != \"-\" {\n\t\tstatus = append(status, [2]string{\"Build Version\", bv})\n\t}\n\tif lv := btrfsLibVersion(); lv != -1 {\n\t\tstatus = append(status, [2]string{\"Library Version\", fmt.Sprintf(\"%d\", lv)})\n\t}\n\treturn status\n}\n\n// GetMetadata returns empty metadata for this driver.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn nil, nil\n}\n\n// Cleanup unmounts the home directory.\nfunc (d *Driver) Cleanup() error {\n\terr := d.subvolDisableQuota()\n\tumountErr := mount.Unmount(d.home)\n\n\t// in case we have two errors, prefer the one from disableQuota()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif umountErr != nil {\n\t\treturn umountErr\n\t}\n\n\treturn nil\n}\n\nfunc free(p *C.char) {\n\tC.free(unsafe.Pointer(p))\n}\n\nfunc openDir(path string) (*C.DIR, error) {\n\tCpath := C.CString(path)\n\tdefer free(Cpath)\n\n\tdir := C.opendir(Cpath)\n\tif dir == nil {\n\t\treturn nil, fmt.Errorf(\"Can't open dir\")\n\t}\n\treturn dir, nil\n}\n\nfunc closeDir(dir *C.DIR) {\n\tif dir != nil {\n\t\tC.closedir(dir)\n\t}\n}\n\nfunc getDirFd(dir *C.DIR) uintptr {\n\treturn uintptr(C.dirfd(dir))\n}\n\nfunc subvolCreate(path, name string) error {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_vol_args\n\tfor i, c := range []byte(name) {\n\t\targs.name[i] = C.char(c)\n\t}\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_SUBVOL_CREATE,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to create btrfs subvolume: %v\", errno.Error())\n\t}\n\treturn nil\n}\n\nfunc subvolSnapshot(src, dest, name string) error {\n\tsrcDir, err := openDir(src)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(srcDir)\n\n\tdestDir, err := openDir(dest)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(destDir)\n\n\tvar args C.struct_btrfs_ioctl_vol_args_v2\n\targs.fd = C.__s64(getDirFd(srcDir))\n\n\tvar cs = C.CString(name)\n\tC.set_name_btrfs_ioctl_vol_args_v2(&args, cs)\n\tC.free(unsafe.Pointer(cs))\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(destDir), C.BTRFS_IOC_SNAP_CREATE_V2,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to create btrfs snapshot: %v\", errno.Error())\n\t}\n\treturn nil\n}\n\nfunc isSubvolume(p string) (bool, error) {\n\tvar bufStat unix.Stat_t\n\tif err := unix.Lstat(p, &bufStat); err != nil {\n\t\treturn false, err\n\t}\n\n\t// return true if it is a btrfs subvolume\n\treturn bufStat.Ino == C.BTRFS_FIRST_FREE_OBJECTID, nil\n}\n\nfunc subvolDelete(dirpath, name string, quotaEnabled bool) error {\n\tdir, err := openDir(dirpath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\tfullPath := path.Join(dirpath, name)\n\n\tvar args C.struct_btrfs_ioctl_vol_args\n\n\t// walk the btrfs subvolumes\n\twalkSubvolumes := func(p string, f os.FileInfo, err error) error {\n\t\tif err != nil {\n\t\t\tif os.IsNotExist(err) && p != fullPath {\n\t\t\t\t// missing most likely because the path was a subvolume that got removed in the previous iteration\n\t\t\t\t// since it's gone anyway, we don't care\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"error walking subvolumes: %v\", err)\n\t\t}\n\t\t// we want to check children only so skip itself\n\t\t// it will be removed after the filepath walk anyways\n\t\tif f.IsDir() && p != fullPath {\n\t\t\tsv, err := isSubvolume(p)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"Failed to test if %s is a btrfs subvolume: %v\", p, err)\n\t\t\t}\n\t\t\tif sv {\n\t\t\t\tif err := subvolDelete(path.Dir(p), f.Name(), quotaEnabled); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"Failed to destroy btrfs child subvolume (%s) of parent (%s): %v\", p, dirpath, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\tif err := filepath.Walk(path.Join(dirpath, name), walkSubvolumes); err != nil {\n\t\treturn fmt.Errorf(\"Recursively walking subvolumes for %s failed: %v\", dirpath, err)\n\t}\n\n\tif quotaEnabled {\n\t\tif qgroupid, err := subvolLookupQgroup(fullPath); err == nil {\n\t\t\tvar args C.struct_btrfs_ioctl_qgroup_create_args\n\t\t\targs.qgroupid = C.__u64(qgroupid)\n\n\t\t\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QGROUP_CREATE,\n\t\t\t\tuintptr(unsafe.Pointer(&args)))\n\t\t\tif errno != 0 {\n\t\t\t\tlogrus.WithField(\"storage-driver\", \"btrfs\").Errorf(\"Failed to delete btrfs qgroup %v for %s: %v\", qgroupid, fullPath, errno.Error())\n\t\t\t}\n\t\t} else {\n\t\t\tlogrus.WithField(\"storage-driver\", \"btrfs\").Errorf(\"Failed to lookup btrfs qgroup for %s: %v\", fullPath, err.Error())\n\t\t}\n\t}\n\n\t// all subvolumes have been removed\n\t// now remove the one originally passed in\n\tfor i, c := range []byte(name) {\n\t\targs.name[i] = C.char(c)\n\t}\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_SNAP_DESTROY,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to destroy btrfs snapshot %s for %s: %v\", dirpath, name, errno.Error())\n\t}\n\treturn nil\n}\n\nfunc (d *Driver) updateQuotaStatus() {\n\td.once.Do(func() {\n\t\tif !d.quotaEnabled {\n\t\t\t// In case quotaEnabled is not set, check qgroup and update quotaEnabled as needed\n\t\t\tif err := subvolQgroupStatus(d.home); err != nil {\n\t\t\t\t// quota is still not enabled\n\t\t\t\treturn\n\t\t\t}\n\t\t\td.quotaEnabled = true\n\t\t}\n\t})\n}\n\nfunc (d *Driver) subvolEnableQuota() error {\n\td.updateQuotaStatus()\n\n\tif d.quotaEnabled {\n\t\treturn nil\n\t}\n\n\tdir, err := openDir(d.home)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_quota_ctl_args\n\targs.cmd = C.BTRFS_QUOTA_CTL_ENABLE\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QUOTA_CTL,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to enable btrfs quota for %s: %v\", dir, errno.Error())\n\t}\n\n\td.quotaEnabled = true\n\n\treturn nil\n}\n\nfunc (d *Driver) subvolDisableQuota() error {\n\td.updateQuotaStatus()\n\n\tif !d.quotaEnabled {\n\t\treturn nil\n\t}\n\n\tdir, err := openDir(d.home)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_quota_ctl_args\n\targs.cmd = C.BTRFS_QUOTA_CTL_DISABLE\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QUOTA_CTL,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to disable btrfs quota for %s: %v\", dir, errno.Error())\n\t}\n\n\td.quotaEnabled = false\n\n\treturn nil\n}\n\nfunc (d *Driver) subvolRescanQuota() error {\n\td.updateQuotaStatus()\n\n\tif !d.quotaEnabled {\n\t\treturn nil\n\t}\n\n\tdir, err := openDir(d.home)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_quota_rescan_args\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QUOTA_RESCAN_WAIT,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to rescan btrfs quota for %s: %v\", dir, errno.Error())\n\t}\n\n\treturn nil\n}\n\nfunc subvolLimitQgroup(path string, size uint64) error {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_qgroup_limit_args\n\targs.lim.max_referenced = C.__u64(size)\n\targs.lim.flags = C.BTRFS_QGROUP_LIMIT_MAX_RFER\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QGROUP_LIMIT,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to limit qgroup for %s: %v\", dir, errno.Error())\n\t}\n\n\treturn nil\n}\n\n// subvolQgroupStatus performs a BTRFS_IOC_TREE_SEARCH on the root path\n// with search key of BTRFS_QGROUP_STATUS_KEY.\n// In case qgroup is enabled, the retuned key type will match BTRFS_QGROUP_STATUS_KEY.\n// For more details please see https://github.com/kdave/btrfs-progs/blob/v4.9/qgroup.c#L1035\nfunc subvolQgroupStatus(path string) error {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_search_args\n\targs.key.tree_id = C.BTRFS_QUOTA_TREE_OBJECTID\n\targs.key.min_type = C.BTRFS_QGROUP_STATUS_KEY\n\targs.key.max_type = C.BTRFS_QGROUP_STATUS_KEY\n\targs.key.max_objectid = C.__u64(math.MaxUint64)\n\targs.key.max_offset = C.__u64(math.MaxUint64)\n\targs.key.max_transid = C.__u64(math.MaxUint64)\n\targs.key.nr_items = 4096\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_TREE_SEARCH,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to search qgroup for %s: %v\", path, errno.Error())\n\t}\n\tsh := (*C.struct_btrfs_ioctl_search_header)(unsafe.Pointer(&args.buf))\n\tif sh._type != C.BTRFS_QGROUP_STATUS_KEY {\n\t\treturn fmt.Errorf(\"Invalid qgroup search header type for %s: %v\", path, sh._type)\n\t}\n\treturn nil\n}\n\nfunc subvolLookupQgroup(path string) (uint64, error) {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_ino_lookup_args\n\targs.objectid = C.BTRFS_FIRST_FREE_OBJECTID\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_INO_LOOKUP,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn 0, fmt.Errorf(\"Failed to lookup qgroup for %s: %v\", dir, errno.Error())\n\t}\n\tif args.treeid == 0 {\n\t\treturn 0, fmt.Errorf(\"Invalid qgroup id for %s: 0\", dir)\n\t}\n\n\treturn uint64(args.treeid), nil\n}\n\nfunc (d *Driver) subvolumesDir() string {\n\treturn path.Join(d.home, \"subvolumes\")\n}\n\nfunc (d *Driver) subvolumesDirID(id string) string {\n\treturn path.Join(d.subvolumesDir(), id)\n}\n\nfunc (d *Driver) quotasDir() string {\n\treturn path.Join(d.home, \"quotas\")\n}\n\nfunc (d *Driver) quotasDirID(id string) string {\n\treturn path.Join(d.quotasDir(), id)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn d.Create(id, parent, opts)\n}\n\n// Create the filesystem with given id.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\tquotas := path.Join(d.home, \"quotas\")\n\tsubvolumes := path.Join(d.home, \"subvolumes\")\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAllAndChown(subvolumes, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn err\n\t}\n\tif parent == \"\" {\n\t\tif err := subvolCreate(subvolumes, id); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tparentDir := d.subvolumesDirID(parent)\n\t\tst, err := os.Stat(parentDir)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !st.IsDir() {\n\t\t\treturn fmt.Errorf(\"%s: not a directory\", parentDir)\n\t\t}\n\t\tif err := subvolSnapshot(parentDir, subvolumes, id); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tvar storageOpt map[string]string\n\tif opts != nil {\n\t\tstorageOpt = opts.StorageOpt\n\t}\n\n\tif _, ok := storageOpt[\"size\"]; ok {\n\t\tdriver := &Driver{}\n\t\tif err := d.parseStorageOpt(storageOpt, driver); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := d.setStorageSize(path.Join(subvolumes, id), driver); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := idtools.MkdirAllAndChown(quotas, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := ioutil.WriteFile(path.Join(quotas, id), []byte(fmt.Sprint(driver.options.size)), 0644); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// if we have a remapped root (user namespaces enabled), change the created snapshot\n\t// dir ownership to match\n\tif rootUID != 0 || rootGID != 0 {\n\t\tif err := os.Chown(path.Join(subvolumes, id), rootUID, rootGID); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tmountLabel := \"\"\n\tif opts != nil {\n\t\tmountLabel = opts.MountLabel\n\t}\n\n\treturn label.Relabel(path.Join(subvolumes, id), mountLabel, false)\n}\n\n// Parse btrfs storage options\nfunc (d *Driver) parseStorageOpt(storageOpt map[string]string, driver *Driver) error {\n\t// Read size to change the subvolume disk quota per container\n\tfor key, val := range storageOpt {\n\t\tkey := strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdriver.options.size = uint64(size)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Set btrfs storage size\nfunc (d *Driver) setStorageSize(dir string, driver *Driver) error {\n\tif driver.options.size == 0 {\n\t\treturn fmt.Errorf(\"btrfs: invalid storage size: %s\", units.HumanSize(float64(driver.options.size)))\n\t}\n\tif d.options.minSpace > 0 && driver.options.size < d.options.minSpace {\n\t\treturn fmt.Errorf(\"btrfs: storage size cannot be less than %s\", units.HumanSize(float64(d.options.minSpace)))\n\t}\n\tif err := d.subvolEnableQuota(); err != nil {\n\t\treturn err\n\t}\n\treturn subvolLimitQgroup(dir, driver.options.size)\n}\n\n// Remove the filesystem with given id.\nfunc (d *Driver) Remove(id string) error {\n\tdir := d.subvolumesDirID(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn err\n\t}\n\tquotasDir := d.quotasDirID(id)\n\tif _, err := os.Stat(quotasDir); err == nil {\n\t\tif err := os.Remove(quotasDir); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\n\t// Call updateQuotaStatus() to invoke status update\n\td.updateQuotaStatus()\n\n\tif err := subvolDelete(d.subvolumesDir(), id, d.quotaEnabled); err != nil {\n\t\treturn err\n\t}\n\tif err := system.EnsureRemoveAll(dir); err != nil {\n\t\treturn err\n\t}\n\treturn d.subvolRescanQuota()\n}\n\n// Get the requested filesystem id.\nfunc (d *Driver) Get(id, mountLabel string) (containerfs.ContainerFS, error) {\n\tdir := d.subvolumesDirID(id)\n\tst, err := os.Stat(dir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif !st.IsDir() {\n\t\treturn nil, fmt.Errorf(\"%s: not a directory\", dir)\n\t}\n\n\tif quota, err := ioutil.ReadFile(d.quotasDirID(id)); err == nil {\n\t\tif size, err := strconv.ParseUint(string(quota), 10, 64); err == nil && size >= d.options.minSpace {\n\t\t\tif err := d.subvolEnableQuota(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif err := subvolLimitQgroup(dir, size); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn containerfs.NewLocalContainerFS(dir), nil\n}\n\n// Put is not implemented for BTRFS as there is no cleanup required for the id.\nfunc (d *Driver) Put(id string) error {\n\t// Get() creates no runtime resources (like e.g. mounts)\n\t// so this doesn't need to do anything.\n\treturn nil\n}\n\n// Exists checks if the id exists in the filesystem.\nfunc (d *Driver) Exists(id string) bool {\n\tdir := d.subvolumesDirID(id)\n\t_, err := os.Stat(dir)\n\treturn err == nil\n}\n", "// +build linux\n\npackage fuseoverlayfs // import \"github.com/docker/docker/daemon/graphdriver/fuse-overlayfs\"\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/daemon/graphdriver/overlayutils\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/chrootarchive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/directory\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers/kernel\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t// untar defines the untar method\n\tuntar = chrootarchive.UntarUncompressed\n)\n\nconst (\n\tdriverName    = \"fuse-overlayfs\"\n\tbinary        = \"fuse-overlayfs\"\n\tlinkDir       = \"l\"\n\tdiffDirName   = \"diff\"\n\tworkDirName   = \"work\"\n\tmergedDirName = \"merged\"\n\tlowerFile     = \"lower\"\n\tmaxDepth      = 128\n\n\t// idLength represents the number of random characters\n\t// which can be used to create the unique link identifier\n\t// for every layer. If this value is too long then the\n\t// page size limit for the mount command may be exceeded.\n\t// The idLength should be selected such that following equation\n\t// is true (512 is a buffer for label metadata).\n\t// ((idLength + len(linkDir) + 1) * maxDepth) <= (pageSize - 512)\n\tidLength = 26\n)\n\n// Driver contains information about the home directory and the list of active\n// mounts that are created using this driver.\ntype Driver struct {\n\thome      string\n\tuidMaps   []idtools.IDMap\n\tgidMaps   []idtools.IDMap\n\tctr       *graphdriver.RefCounter\n\tnaiveDiff graphdriver.DiffDriver\n\tlocker    *locker.Locker\n}\n\nvar (\n\tlogger = logrus.WithField(\"storage-driver\", driverName)\n)\n\nfunc init() {\n\tgraphdriver.Register(driverName, Init)\n}\n\n// Init returns the naive diff driver for fuse-overlayfs.\n// If fuse-overlayfs is not supported on the host, the error\n// graphdriver.ErrNotSupported is returned.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\tif _, err := exec.LookPath(binary); err != nil {\n\t\tlogger.Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\tif !kernel.CheckKernelVersion(4, 18, 0) {\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create the driver home dir\n\tif err := idtools.MkdirAllAndChown(path.Join(home, linkDir), 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\td := &Driver{\n\t\thome:    home,\n\t\tuidMaps: uidMaps,\n\t\tgidMaps: gidMaps,\n\t\tctr:     graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicFUSE)),\n\t\tlocker:  locker.New(),\n\t}\n\n\td.naiveDiff = graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps)\n\n\treturn d, nil\n}\n\nfunc (d *Driver) String() string {\n\treturn driverName\n}\n\n// Status returns current driver information in a two dimensional string array.\nfunc (d *Driver) Status() [][2]string {\n\treturn [][2]string{}\n}\n\n// GetMetadata returns metadata about the overlay driver such as the LowerDir,\n// UpperDir, WorkDir, and MergeDir used to store data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := map[string]string{\n\t\t\"WorkDir\":   path.Join(dir, workDirName),\n\t\t\"MergedDir\": path.Join(dir, mergedDirName),\n\t\t\"UpperDir\":  path.Join(dir, diffDirName),\n\t}\n\n\tlowerDirs, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(lowerDirs) > 0 {\n\t\tmetadata[\"LowerDir\"] = strings.Join(lowerDirs, \":\")\n\t}\n\n\treturn metadata, nil\n}\n\n// Cleanup any state created by overlay which should be cleaned when daemon\n// is being shutdown. For now, we just have to unmount the bind mounted\n// we had created.\nfunc (d *Driver) Cleanup() error {\n\treturn mount.RecursiveUnmount(d.home)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported\")\n\t}\n\treturn d.create(id, parent, opts)\n}\n\n// Create is used to create the upper, lower, and merge directories required for overlay fs for a given id.\n// The parent filesystem is used to configure these directories for the overlay.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported\")\n\t}\n\treturn d.create(id, parent, opts)\n}\n\nfunc (d *Driver) create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tdir := d.dir(id)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\troot := idtools.Identity{UID: rootUID, GID: rootGID}\n\n\tif err := idtools.MkdirAllAndChown(path.Dir(dir), 0700, root); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Clean up on failure\n\t\tif retErr != nil {\n\t\t\tos.RemoveAll(dir)\n\t\t}\n\t}()\n\n\tif opts != nil && len(opts.StorageOpt) > 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported\")\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, diffDirName), 0755, root); err != nil {\n\t\treturn err\n\t}\n\n\tlid := overlayutils.GenerateID(idLength, logger)\n\tif err := os.Symlink(path.Join(\"..\", id, diffDirName), path.Join(d.home, linkDir, lid)); err != nil {\n\t\treturn err\n\t}\n\n\t// Write link id to link file\n\tif err := ioutil.WriteFile(path.Join(dir, \"link\"), []byte(lid), 0644); err != nil {\n\t\treturn err\n\t}\n\n\t// if no parent directory, done\n\tif parent == \"\" {\n\t\treturn nil\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, workDirName), 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\tif err := ioutil.WriteFile(path.Join(d.dir(parent), \"committed\"), []byte{}, 0600); err != nil {\n\t\treturn err\n\t}\n\n\tlower, err := d.getLower(parent)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif lower != \"\" {\n\t\tif err := ioutil.WriteFile(path.Join(dir, lowerFile), []byte(lower), 0666); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (d *Driver) getLower(parent string) (string, error) {\n\tparentDir := d.dir(parent)\n\n\t// Ensure parent exists\n\tif _, err := os.Lstat(parentDir); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Read Parent link fileA\n\tparentLink, err := ioutil.ReadFile(path.Join(parentDir, \"link\"))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tlowers := []string{path.Join(linkDir, string(parentLink))}\n\n\tparentLower, err := ioutil.ReadFile(path.Join(parentDir, lowerFile))\n\tif err == nil {\n\t\tparentLowers := strings.Split(string(parentLower), \":\")\n\t\tlowers = append(lowers, parentLowers...)\n\t}\n\tif len(lowers) > maxDepth {\n\t\treturn \"\", errors.New(\"max depth exceeded\")\n\t}\n\treturn strings.Join(lowers, \":\"), nil\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn path.Join(d.home, id)\n}\n\nfunc (d *Driver) getLowerDirs(id string) ([]string, error) {\n\tvar lowersArray []string\n\tlowers, err := ioutil.ReadFile(path.Join(d.dir(id), lowerFile))\n\tif err == nil {\n\t\tfor _, s := range strings.Split(string(lowers), \":\") {\n\t\t\tlp, err := os.Readlink(path.Join(d.home, s))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlowersArray = append(lowersArray, path.Clean(path.Join(d.home, linkDir, lp)))\n\t\t}\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\treturn lowersArray, nil\n}\n\n// Remove cleans the directories that are created for this id.\nfunc (d *Driver) Remove(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"refusing to remove the directories: id is empty\")\n\t}\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tlid, err := ioutil.ReadFile(path.Join(dir, \"link\"))\n\tif err == nil {\n\t\tif len(lid) == 0 {\n\t\t\tlogger.Errorf(\"refusing to remove empty link for layer %v\", id)\n\t\t} else if err := os.RemoveAll(path.Join(d.home, linkDir, string(lid))); err != nil {\n\t\t\tlogger.Debugf(\"Failed to remove link: %v\", err)\n\t\t}\n\t}\n\n\tif err := system.EnsureRemoveAll(dir); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Get creates and mounts the required file system for the given id and returns the mount path.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, retErr error) {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdiffDir := path.Join(dir, diffDirName)\n\tlowers, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, just return diff directory\n\t\tif os.IsNotExist(err) {\n\t\t\treturn containerfs.NewLocalContainerFS(diffDir), nil\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tmergedDir := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Increment(mergedDir); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif c := d.ctr.Decrement(mergedDir); c <= 0 {\n\t\t\t\tif unmounted := fusermountU(mergedDir); !unmounted {\n\t\t\t\t\tif mntErr := unix.Unmount(mergedDir, 0); mntErr != nil {\n\t\t\t\t\t\tlogger.Errorf(\"error unmounting %v: %v\", mergedDir, mntErr)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Cleanup the created merged directory; see the comment in Put's rmdir\n\t\t\t\tif rmErr := unix.Rmdir(mergedDir); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogger.Debugf(\"Failed to remove %s: %v: %v\", id, rmErr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tworkDir := path.Join(dir, workDirName)\n\tsplitLowers := strings.Split(string(lowers), \":\")\n\tabsLowers := make([]string, len(splitLowers))\n\tfor i, s := range splitLowers {\n\t\tabsLowers[i] = path.Join(d.home, s)\n\t}\n\tvar readonly bool\n\tif _, err := os.Stat(path.Join(dir, \"committed\")); err == nil {\n\t\treadonly = true\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tvar opts string\n\tif readonly {\n\t\topts = \"lowerdir=\" + diffDir + \":\" + strings.Join(absLowers, \":\")\n\t} else {\n\t\topts = \"lowerdir=\" + strings.Join(absLowers, \":\") + \",upperdir=\" + diffDir + \",workdir=\" + workDir\n\t}\n\n\tmountData := label.FormatMountLabel(opts, mountLabel)\n\tmountTarget := mergedDir\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(mergedDir, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmountProgram := exec.Command(binary, \"-o\", mountData, mountTarget)\n\tmountProgram.Dir = d.home\n\tvar b bytes.Buffer\n\tmountProgram.Stderr = &b\n\tif err = mountProgram.Run(); err != nil {\n\t\toutput := b.String()\n\t\tif output == \"\" {\n\t\t\toutput = \"<stderr empty>\"\n\t\t}\n\t\treturn nil, errors.Wrapf(err, \"using mount program %s: %s\", binary, output)\n\t}\n\n\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n}\n\n// Put unmounts the mount path created for the give id.\n// It also removes the 'merged' directory to force the kernel to unmount the\n// overlay mount in other namespaces.\nfunc (d *Driver) Put(id string) error {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\t_, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, no mount happened and just return directly\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\n\tmountpoint := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\tif unmounted := fusermountU(mountpoint); !unmounted {\n\t\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\t\tlogger.Debugf(\"Failed to unmount %s overlay: %s - %v\", id, mountpoint, err)\n\t\t}\n\t}\n\t// Remove the mountpoint here. Removing the mountpoint (in newer kernels)\n\t// will cause all other instances of this mount in other mount namespaces\n\t// to be unmounted. This is necessary to avoid cases where an overlay mount\n\t// that is present in another namespace will cause subsequent mounts\n\t// operations to fail with ebusy.  We ignore any errors here because this may\n\t// fail on older kernels which don't have\n\t// torvalds/linux@8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe applied.\n\tif err := unix.Rmdir(mountpoint); err != nil && !os.IsNotExist(err) {\n\t\tlogger.Debugf(\"Failed to remove %s overlay: %v\", id, err)\n\t}\n\treturn nil\n}\n\n// Exists checks to see if the id is already mounted.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n\n// isParent determines whether the given parent is the direct parent of the\n// given layer id\nfunc (d *Driver) isParent(id, parent string) bool {\n\tlowers, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif parent == \"\" && len(lowers) > 0 {\n\t\treturn false\n\t}\n\n\tparentDir := d.dir(parent)\n\tvar ld string\n\tif len(lowers) > 0 {\n\t\tld = filepath.Dir(lowers[0])\n\t}\n\tif ld == \"\" && parent == \"\" {\n\t\treturn true\n\t}\n\treturn ld == parentDir\n}\n\n// ApplyDiff applies the new layer into a root\nfunc (d *Driver) ApplyDiff(id string, parent string, diff io.Reader) (size int64, err error) {\n\tif !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.ApplyDiff(id, parent, diff)\n\t}\n\n\tapplyDir := d.getDiffPath(id)\n\n\tlogger.Debugf(\"Applying tar in %s\", applyDir)\n\t// Overlay doesn't need the parent id to apply the diff\n\tif err := untar(diff, applyDir, &archive.TarOptions{\n\t\tUIDMaps: d.uidMaps,\n\t\tGIDMaps: d.gidMaps,\n\t\t// Use AUFS whiteout format: https://github.com/containers/storage/blob/39a8d5ed9843844eafb5d2ba6e6a7510e0126f40/drivers/overlay/overlay.go#L1084-L1089\n\t\tWhiteoutFormat: archive.AUFSWhiteoutFormat,\n\t\tInUserNS:       sys.RunningInUserNS(),\n\t}); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn directory.Size(context.TODO(), applyDir)\n}\n\nfunc (d *Driver) getDiffPath(id string) string {\n\tdir := d.dir(id)\n\n\treturn path.Join(dir, diffDirName)\n}\n\n// DiffSize calculates the changes between the specified id\n// and its parent and returns the size in bytes of the changes\n// relative to its base filesystem directory.\nfunc (d *Driver) DiffSize(id, parent string) (size int64, err error) {\n\treturn d.naiveDiff.DiffSize(id, parent)\n}\n\n// Diff produces an archive of the changes between the specified\n// layer and its parent layer which may be \"\".\nfunc (d *Driver) Diff(id, parent string) (io.ReadCloser, error) {\n\treturn d.naiveDiff.Diff(id, parent)\n}\n\n// Changes produces a list of changes between the specified layer and its\n// parent layer. If parent is \"\", then all changes will be ADD changes.\nfunc (d *Driver) Changes(id, parent string) ([]archive.Change, error) {\n\treturn d.naiveDiff.Changes(id, parent)\n}\n\n// fusermountU is from https://github.com/containers/storage/blob/39a8d5ed9843844eafb5d2ba6e6a7510e0126f40/drivers/overlay/overlay.go#L1016-L1040\nfunc fusermountU(mountpoint string) (unmounted bool) {\n\t// Attempt to unmount the FUSE mount using either fusermount or fusermount3.\n\t// If they fail, fallback to unix.Unmount\n\tfor _, v := range []string{\"fusermount3\", \"fusermount\"} {\n\t\terr := exec.Command(v, \"-u\", mountpoint).Run()\n\t\tif err != nil && !os.IsNotExist(err) {\n\t\t\tlogrus.Debugf(\"Error unmounting %s with %s - %v\", mountpoint, v, err)\n\t\t}\n\t\tif err == nil {\n\t\t\tunmounted = true\n\t\t\tbreak\n\t\t}\n\t}\n\t// If fusermount|fusermount3 failed to unmount the FUSE file system, make sure all\n\t// pending changes are propagated to the file system\n\tif !unmounted {\n\t\tfd, err := unix.Open(mountpoint, unix.O_DIRECTORY, 0)\n\t\tif err == nil {\n\t\t\tif err := unix.Syncfs(fd); err != nil {\n\t\t\t\tlogrus.Debugf(\"Error Syncfs(%s) - %v\", mountpoint, err)\n\t\t\t}\n\t\t\tunix.Close(fd)\n\t\t}\n\t}\n\treturn\n}\n", "// +build linux\n\npackage overlay // import \"github.com/docker/docker/daemon/graphdriver/overlay\"\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/daemon/graphdriver/copy\"\n\t\"github.com/docker/docker/daemon/graphdriver/overlayutils\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/fsutils\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\n// This is a small wrapper over the NaiveDiffWriter that lets us have a custom\n// implementation of ApplyDiff()\n\nvar (\n\t// ErrApplyDiffFallback is returned to indicate that a normal ApplyDiff is applied as a fallback from Naive diff writer.\n\tErrApplyDiffFallback = fmt.Errorf(\"Fall back to normal ApplyDiff\")\n\tbackingFs            = \"<unknown>\"\n)\n\n// ApplyDiffProtoDriver wraps the ProtoDriver by extending the interface with ApplyDiff method.\ntype ApplyDiffProtoDriver interface {\n\tgraphdriver.ProtoDriver\n\t// ApplyDiff writes the diff to the archive for the given id and parent id.\n\t// It returns the size in bytes written if successful, an error ErrApplyDiffFallback is returned otherwise.\n\tApplyDiff(id, parent string, diff io.Reader) (size int64, err error)\n}\n\ntype naiveDiffDriverWithApply struct {\n\tgraphdriver.Driver\n\tapplyDiff ApplyDiffProtoDriver\n}\n\n// NaiveDiffDriverWithApply returns a NaiveDiff driver with custom ApplyDiff.\nfunc NaiveDiffDriverWithApply(driver ApplyDiffProtoDriver, uidMaps, gidMaps []idtools.IDMap) graphdriver.Driver {\n\treturn &naiveDiffDriverWithApply{\n\t\tDriver:    graphdriver.NewNaiveDiffDriver(driver, uidMaps, gidMaps),\n\t\tapplyDiff: driver,\n\t}\n}\n\n// ApplyDiff creates a diff layer with either the NaiveDiffDriver or with a fallback.\nfunc (d *naiveDiffDriverWithApply) ApplyDiff(id, parent string, diff io.Reader) (int64, error) {\n\tb, err := d.applyDiff.ApplyDiff(id, parent, diff)\n\tif err == ErrApplyDiffFallback {\n\t\treturn d.Driver.ApplyDiff(id, parent, diff)\n\t}\n\treturn b, err\n}\n\n// This backend uses the overlay union filesystem for containers\n// plus hard link file sharing for images.\n\n// Each container/image can have a \"root\" subdirectory which is a plain\n// filesystem hierarchy, or they can use overlay.\n\n// If they use overlay there is a \"upper\" directory and a \"lower-id\"\n// file, as well as \"merged\" and \"work\" directories. The \"upper\"\n// directory has the upper layer of the overlay, and \"lower-id\" contains\n// the id of the parent whose \"root\" directory shall be used as the lower\n// layer in the overlay. The overlay itself is mounted in the \"merged\"\n// directory, and the \"work\" dir is needed for overlay to work.\n\n// When an overlay layer is created there are two cases, either the\n// parent has a \"root\" dir, then we start out with an empty \"upper\"\n// directory overlaid on the parents root. This is typically the\n// case with the init layer of a container which is based on an image.\n// If there is no \"root\" in the parent, we inherit the lower-id from\n// the parent and start by making a copy in the parent's \"upper\" dir.\n// This is typically the case for a container layer which copies\n// its parent -init upper layer.\n\n// Additionally we also have a custom implementation of ApplyLayer\n// which makes a recursive copy of the parent \"root\" layer using\n// hardlinks to share file data, and then applies the layer on top\n// of that. This means all child images share file (but not directory)\n// data with the parent.\n\ntype overlayOptions struct{}\n\n// Driver contains information about the home directory and the list of active mounts that are created using this driver.\ntype Driver struct {\n\thome          string\n\tuidMaps       []idtools.IDMap\n\tgidMaps       []idtools.IDMap\n\tctr           *graphdriver.RefCounter\n\tsupportsDType bool\n\tlocker        *locker.Locker\n}\n\nfunc init() {\n\tgraphdriver.Register(\"overlay\", Init)\n}\n\n// Init returns the NaiveDiffDriver, a native diff driver for overlay filesystem.\n// If overlay filesystem is not supported on the host, the error\n// graphdriver.ErrNotSupported is returned.\n// If an overlay filesystem is not supported over an existing filesystem then\n// error graphdriver.ErrIncompatibleFS is returned.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\t_, err := parseOptions(options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Perform feature detection on /var/lib/docker/overlay if it's an existing directory.\n\t// This covers situations where /var/lib/docker/overlay is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := home\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tif err := overlayutils.SupportsOverlay(testdir, false); err != nil {\n\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif fsName, ok := graphdriver.FsNames[fsMagic]; ok {\n\t\tbackingFs = fsName\n\t}\n\n\tsupportsDType, err := fsutils.SupportsDType(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !supportsDType {\n\t\tif !graphdriver.IsInitialized(home) {\n\t\t\treturn nil, overlayutils.ErrDTypeNotSupported(\"overlay\", backingFs)\n\t\t}\n\t\t// allow running without d_type only for existing setups (#27443)\n\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Warn(overlayutils.ErrDTypeNotSupported(\"overlay\", backingFs))\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create the driver home dir\n\tif err := idtools.MkdirAllAndChown(home, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\td := &Driver{\n\t\thome:          home,\n\t\tuidMaps:       uidMaps,\n\t\tgidMaps:       gidMaps,\n\t\tctr:           graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicOverlay)),\n\t\tsupportsDType: supportsDType,\n\t\tlocker:        locker.New(),\n\t}\n\n\treturn NaiveDiffDriverWithApply(d, uidMaps, gidMaps), nil\n}\n\nfunc parseOptions(options []string) (*overlayOptions, error) {\n\to := &overlayOptions{}\n\tfor _, option := range options {\n\t\tkey, _, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"overlay: unknown option %s\", key)\n\t\t}\n\t}\n\treturn o, nil\n}\n\nfunc (d *Driver) String() string {\n\treturn \"overlay\"\n}\n\n// Status returns current driver information in a two dimensional string array.\n// Output contains \"Backing Filesystem\" used in this implementation.\nfunc (d *Driver) Status() [][2]string {\n\treturn [][2]string{\n\t\t{\"Backing Filesystem\", backingFs},\n\t\t{\"Supports d_type\", strconv.FormatBool(d.supportsDType)},\n\t}\n}\n\n// GetMetadata returns metadata about the overlay driver such as root,\n// LowerDir, UpperDir, WorkDir and MergeDir used to store data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := make(map[string]string)\n\n\t// If id has a root, it is an image\n\trootDir := path.Join(dir, \"root\")\n\tif _, err := os.Stat(rootDir); err == nil {\n\t\tmetadata[\"RootDir\"] = rootDir\n\t\treturn metadata, nil\n\t}\n\n\tlowerID, err := ioutil.ReadFile(path.Join(dir, \"lower-id\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata[\"LowerDir\"] = path.Join(d.dir(string(lowerID)), \"root\")\n\tmetadata[\"UpperDir\"] = path.Join(dir, \"upper\")\n\tmetadata[\"WorkDir\"] = path.Join(dir, \"work\")\n\tmetadata[\"MergedDir\"] = path.Join(dir, \"merged\")\n\n\treturn metadata, nil\n}\n\n// Cleanup any state created by overlay which should be cleaned when daemon\n// is being shutdown. For now, we just have to unmount the bind mounted\n// we had created.\nfunc (d *Driver) Cleanup() error {\n\treturn mount.RecursiveUnmount(d.home)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn d.Create(id, parent, opts)\n}\n\n// Create is used to create the upper, lower, and merge directories required for overlay fs for a given id.\n// The parent filesystem is used to configure these directories for the overlay.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported for overlay\")\n\t}\n\n\tdir := d.dir(id)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\troot := idtools.Identity{UID: rootUID, GID: rootGID}\n\n\tif err := idtools.MkdirAllAndChown(path.Dir(dir), 0700, root); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Clean up on failure\n\t\tif retErr != nil {\n\t\t\tos.RemoveAll(dir)\n\t\t}\n\t}()\n\n\t// Toplevel images are just a \"root\" dir\n\tif parent == \"\" {\n\t\treturn idtools.MkdirAndChown(path.Join(dir, \"root\"), 0755, root)\n\t}\n\n\tparentDir := d.dir(parent)\n\n\t// Ensure parent exists\n\tif _, err := os.Lstat(parentDir); err != nil {\n\t\treturn err\n\t}\n\n\t// If parent has a root, just do an overlay to it\n\tparentRoot := path.Join(parentDir, \"root\")\n\n\tif s, err := os.Lstat(parentRoot); err == nil {\n\t\tif err := idtools.MkdirAndChown(path.Join(dir, \"upper\"), s.Mode(), root); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := idtools.MkdirAndChown(path.Join(dir, \"work\"), 0700, root); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn ioutil.WriteFile(path.Join(dir, \"lower-id\"), []byte(parent), 0666)\n\t}\n\n\t// Otherwise, copy the upper and the lower-id from the parent\n\n\tlowerID, err := ioutil.ReadFile(path.Join(parentDir, \"lower-id\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := ioutil.WriteFile(path.Join(dir, \"lower-id\"), lowerID, 0666); err != nil {\n\t\treturn err\n\t}\n\n\tparentUpperDir := path.Join(parentDir, \"upper\")\n\ts, err := os.Lstat(parentUpperDir)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tupperDir := path.Join(dir, \"upper\")\n\tif err := idtools.MkdirAndChown(upperDir, s.Mode(), root); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(path.Join(dir, \"work\"), 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\treturn copy.DirCopy(parentUpperDir, upperDir, copy.Content, true)\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn path.Join(d.home, id)\n}\n\n// Remove cleans the directories that are created for this id.\nfunc (d *Driver) Remove(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"refusing to remove the directories: id is empty\")\n\t}\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\treturn system.EnsureRemoveAll(d.dir(id))\n}\n\n// Get creates and mounts the required file system for the given id and returns the mount path.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, err error) {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\t// If id has a root, just return it\n\trootDir := path.Join(dir, \"root\")\n\tif _, err := os.Stat(rootDir); err == nil {\n\t\treturn containerfs.NewLocalContainerFS(rootDir), nil\n\t}\n\n\tmergedDir := path.Join(dir, \"merged\")\n\tif count := d.ctr.Increment(mergedDir); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif c := d.ctr.Decrement(mergedDir); c <= 0 {\n\t\t\t\tif mntErr := unix.Unmount(mergedDir, 0); mntErr != nil {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Debugf(\"Failed to unmount %s: %v: %v\", id, mntErr, err)\n\t\t\t\t}\n\t\t\t\t// Cleanup the created merged directory; see the comment in Put's rmdir\n\t\t\t\tif rmErr := unix.Rmdir(mergedDir); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Warnf(\"Failed to remove %s: %v: %v\", id, rmErr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\tlowerID, err := ioutil.ReadFile(path.Join(dir, \"lower-id\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(mergedDir, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tlowerDir = path.Join(d.dir(string(lowerID)), \"root\")\n\t\tupperDir = path.Join(dir, \"upper\")\n\t\tworkDir  = path.Join(dir, \"work\")\n\t\topts     = fmt.Sprintf(\"lowerdir=%s,upperdir=%s,workdir=%s\", lowerDir, upperDir, workDir)\n\t)\n\tif err := unix.Mount(\"overlay\", mergedDir, \"overlay\", 0, label.FormatMountLabel(opts, mountLabel)); err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating overlay mount to %s: %v\", mergedDir, err)\n\t}\n\t// chown \"workdir/work\" to the remapped root UID/GID. Overlay fs inside a\n\t// user namespace requires this to move a directory from lower to upper.\n\tif err := os.Chown(path.Join(workDir, \"work\"), rootUID, rootGID); err != nil {\n\t\treturn nil, err\n\t}\n\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n}\n\n// Put unmounts the mount path created for the give id.\n// It also removes the 'merged' directory to force the kernel to unmount the\n// overlay mount in other namespaces.\nfunc (d *Driver) Put(id string) error {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\t// If id has a root, just return\n\tif _, err := os.Stat(path.Join(d.dir(id), \"root\")); err == nil {\n\t\treturn nil\n\t}\n\tmountpoint := path.Join(d.dir(id), \"merged\")\n\tlogger := logrus.WithField(\"storage-driver\", \"overlay\")\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\tlogger.Debugf(\"Failed to unmount %s overlay: %v\", id, err)\n\t}\n\n\t// Remove the mountpoint here. Removing the mountpoint (in newer kernels)\n\t// will cause all other instances of this mount in other mount namespaces\n\t// to be unmounted. This is necessary to avoid cases where an overlay mount\n\t// that is present in another namespace will cause subsequent mounts\n\t// operations to fail with ebusy.  We ignore any errors here because this may\n\t// fail on older kernels which don't have\n\t// torvalds/linux@8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe applied.\n\tif err := unix.Rmdir(mountpoint); err != nil {\n\t\tlogger.Debugf(\"Failed to remove %s overlay: %v\", id, err)\n\t}\n\treturn nil\n}\n\n// ApplyDiff applies the new layer on top of the root, if parent does not exist with will return an ErrApplyDiffFallback error.\nfunc (d *Driver) ApplyDiff(id string, parent string, diff io.Reader) (size int64, err error) {\n\tdir := d.dir(id)\n\n\tif parent == \"\" {\n\t\treturn 0, ErrApplyDiffFallback\n\t}\n\n\tparentRootDir := path.Join(d.dir(parent), \"root\")\n\tif _, err := os.Stat(parentRootDir); err != nil {\n\t\treturn 0, ErrApplyDiffFallback\n\t}\n\n\t// We now know there is a parent, and it has a \"root\" directory containing\n\t// the full root filesystem. We can just hardlink it and apply the\n\t// layer. This relies on two things:\n\t// 1) ApplyDiff is only run once on a clean (no writes to upper layer) container\n\t// 2) ApplyDiff doesn't do any in-place writes to files (would break hardlinks)\n\t// These are all currently true and are not expected to break\n\n\ttmpRootDir, err := ioutil.TempDir(dir, \"tmproot\")\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(tmpRootDir)\n\t\t} else {\n\t\t\tos.RemoveAll(path.Join(dir, \"upper\"))\n\t\t\tos.RemoveAll(path.Join(dir, \"work\"))\n\t\t\tos.RemoveAll(path.Join(dir, \"merged\"))\n\t\t\tos.RemoveAll(path.Join(dir, \"lower-id\"))\n\t\t}\n\t}()\n\n\tif err = copy.DirCopy(parentRootDir, tmpRootDir, copy.Hardlink, true); err != nil {\n\t\treturn 0, err\n\t}\n\n\toptions := &archive.TarOptions{UIDMaps: d.uidMaps, GIDMaps: d.gidMaps}\n\tif size, err = graphdriver.ApplyUncompressedLayer(tmpRootDir, diff, options); err != nil {\n\t\treturn 0, err\n\t}\n\n\trootDir := path.Join(dir, \"root\")\n\tif err := os.Rename(tmpRootDir, rootDir); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn\n}\n\n// Exists checks to see if the id is already mounted.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n", "// +build linux\n\npackage overlay2 // import \"github.com/docker/docker/daemon/graphdriver/overlay2\"\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/daemon/graphdriver/overlayutils\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/chrootarchive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/directory\"\n\t\"github.com/docker/docker/pkg/fsutils\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/quota\"\n\tunits \"github.com/docker/go-units\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t// untar defines the untar method\n\tuntar = chrootarchive.UntarUncompressed\n)\n\n// This backend uses the overlay union filesystem for containers\n// with diff directories for each layer.\n\n// This version of the overlay driver requires at least kernel\n// 4.0.0 in order to support mounting multiple diff directories.\n\n// Each container/image has at least a \"diff\" directory and \"link\" file.\n// If there is also a \"lower\" file when there are diff layers\n// below as well as \"merged\" and \"work\" directories. The \"diff\" directory\n// has the upper layer of the overlay and is used to capture any\n// changes to the layer. The \"lower\" file contains all the lower layer\n// mounts separated by \":\" and ordered from uppermost to lowermost\n// layers. The overlay itself is mounted in the \"merged\" directory,\n// and the \"work\" dir is needed for overlay to work.\n\n// The \"link\" file for each layer contains a unique string for the layer.\n// Under the \"l\" directory at the root there will be a symbolic link\n// with that unique string pointing the \"diff\" directory for the layer.\n// The symbolic links are used to reference lower layers in the \"lower\"\n// file and on mount. The links are used to shorten the total length\n// of a layer reference without requiring changes to the layer identifier\n// or root directory. Mounts are always done relative to root and\n// referencing the symbolic links in order to ensure the number of\n// lower directories can fit in a single page for making the mount\n// syscall. A hard upper limit of 128 lower layers is enforced to ensure\n// that mounts do not fail due to length.\n\nconst (\n\tdriverName    = \"overlay2\"\n\tlinkDir       = \"l\"\n\tdiffDirName   = \"diff\"\n\tworkDirName   = \"work\"\n\tmergedDirName = \"merged\"\n\tlowerFile     = \"lower\"\n\tmaxDepth      = 128\n\n\t// idLength represents the number of random characters\n\t// which can be used to create the unique link identifier\n\t// for every layer. If this value is too long then the\n\t// page size limit for the mount command may be exceeded.\n\t// The idLength should be selected such that following equation\n\t// is true (512 is a buffer for label metadata).\n\t// ((idLength + len(linkDir) + 1) * maxDepth) <= (pageSize - 512)\n\tidLength = 26\n)\n\ntype overlayOptions struct {\n\toverrideKernelCheck bool\n\tquota               quota.Quota\n}\n\n// Driver contains information about the home directory and the list of active\n// mounts that are created using this driver.\ntype Driver struct {\n\thome          string\n\tuidMaps       []idtools.IDMap\n\tgidMaps       []idtools.IDMap\n\tctr           *graphdriver.RefCounter\n\tquotaCtl      *quota.Control\n\toptions       overlayOptions\n\tnaiveDiff     graphdriver.DiffDriver\n\tsupportsDType bool\n\tlocker        *locker.Locker\n}\n\nvar (\n\tlogger                = logrus.WithField(\"storage-driver\", \"overlay2\")\n\tbackingFs             = \"<unknown>\"\n\tprojectQuotaSupported = false\n\n\tuseNaiveDiffLock sync.Once\n\tuseNaiveDiffOnly bool\n\n\tindexOff string\n)\n\nfunc init() {\n\tgraphdriver.Register(driverName, Init)\n}\n\n// Init returns the native diff driver for overlay filesystem.\n// If overlay filesystem is not supported on the host, the error\n// graphdriver.ErrNotSupported is returned.\n// If an overlay filesystem is not supported over an existing filesystem then\n// the error graphdriver.ErrIncompatibleFS is returned.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\topts, err := parseOptions(options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Perform feature detection on /var/lib/docker/overlay2 if it's an existing directory.\n\t// This covers situations where /var/lib/docker/overlay2 is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := home\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tif err := overlayutils.SupportsOverlay(testdir, true); err != nil {\n\t\tlogger.Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif fsName, ok := graphdriver.FsNames[fsMagic]; ok {\n\t\tbackingFs = fsName\n\t}\n\n\tsupportsDType, err := fsutils.SupportsDType(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !supportsDType {\n\t\tif !graphdriver.IsInitialized(home) {\n\t\t\treturn nil, overlayutils.ErrDTypeNotSupported(\"overlay2\", backingFs)\n\t\t}\n\t\t// allow running without d_type only for existing setups (#27443)\n\t\tlogger.Warn(overlayutils.ErrDTypeNotSupported(\"overlay2\", backingFs))\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create the driver home dir\n\tif err := idtools.MkdirAllAndChown(path.Join(home, linkDir), 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\td := &Driver{\n\t\thome:          home,\n\t\tuidMaps:       uidMaps,\n\t\tgidMaps:       gidMaps,\n\t\tctr:           graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicOverlay)),\n\t\tsupportsDType: supportsDType,\n\t\tlocker:        locker.New(),\n\t\toptions:       *opts,\n\t}\n\n\td.naiveDiff = graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps)\n\n\tif backingFs == \"xfs\" {\n\t\t// Try to enable project quota support over xfs.\n\t\tif d.quotaCtl, err = quota.NewControl(home); err == nil {\n\t\t\tprojectQuotaSupported = true\n\t\t} else if opts.quota.Size > 0 {\n\t\t\treturn nil, fmt.Errorf(\"Storage option overlay2.size not supported. Filesystem does not support Project Quota: %v\", err)\n\t\t}\n\t} else if opts.quota.Size > 0 {\n\t\t// if xfs is not the backing fs then error out if the storage-opt overlay2.size is used.\n\t\treturn nil, fmt.Errorf(\"Storage Option overlay2.size only supported for backingFS XFS. Found %v\", backingFs)\n\t}\n\n\t// figure out whether \"index=off\" option is recognized by the kernel\n\t_, err = os.Stat(\"/sys/module/overlay/parameters/index\")\n\tswitch {\n\tcase err == nil:\n\t\tindexOff = \"index=off,\"\n\tcase os.IsNotExist(err):\n\t\t// old kernel, no index -- do nothing\n\tdefault:\n\t\tlogger.Warnf(\"Unable to detect whether overlay kernel module supports index parameter: %s\", err)\n\t}\n\n\tlogger.Debugf(\"backingFs=%s, projectQuotaSupported=%v, indexOff=%q\", backingFs, projectQuotaSupported, indexOff)\n\n\treturn d, nil\n}\n\nfunc parseOptions(options []string) (*overlayOptions, error) {\n\to := &overlayOptions{}\n\tfor _, option := range options {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"overlay2.override_kernel_check\":\n\t\t\to.overrideKernelCheck, err = strconv.ParseBool(val)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase \"overlay2.size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\to.quota.Size = uint64(size)\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"overlay2: unknown option %s\", key)\n\t\t}\n\t}\n\treturn o, nil\n}\n\nfunc useNaiveDiff(home string) bool {\n\tuseNaiveDiffLock.Do(func() {\n\t\tif err := doesSupportNativeDiff(home); err != nil {\n\t\t\tlogger.Warnf(\"Not using native diff for overlay2, this may cause degraded performance for building images: %v\", err)\n\t\t\tuseNaiveDiffOnly = true\n\t\t}\n\t})\n\treturn useNaiveDiffOnly\n}\n\nfunc (d *Driver) String() string {\n\treturn driverName\n}\n\n// Status returns current driver information in a two dimensional string array.\n// Output contains \"Backing Filesystem\" used in this implementation.\nfunc (d *Driver) Status() [][2]string {\n\treturn [][2]string{\n\t\t{\"Backing Filesystem\", backingFs},\n\t\t{\"Supports d_type\", strconv.FormatBool(d.supportsDType)},\n\t\t{\"Native Overlay Diff\", strconv.FormatBool(!useNaiveDiff(d.home))},\n\t}\n}\n\n// GetMetadata returns metadata about the overlay driver such as the LowerDir,\n// UpperDir, WorkDir, and MergeDir used to store data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := map[string]string{\n\t\t\"WorkDir\":   path.Join(dir, workDirName),\n\t\t\"MergedDir\": path.Join(dir, mergedDirName),\n\t\t\"UpperDir\":  path.Join(dir, diffDirName),\n\t}\n\n\tlowerDirs, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(lowerDirs) > 0 {\n\t\tmetadata[\"LowerDir\"] = strings.Join(lowerDirs, \":\")\n\t}\n\n\treturn metadata, nil\n}\n\n// Cleanup any state created by overlay which should be cleaned when daemon\n// is being shutdown. For now, we just have to unmount the bind mounted\n// we had created.\nfunc (d *Driver) Cleanup() error {\n\treturn mount.RecursiveUnmount(d.home)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\tif opts == nil {\n\t\topts = &graphdriver.CreateOpts{\n\t\t\tStorageOpt: make(map[string]string),\n\t\t}\n\t} else if opts.StorageOpt == nil {\n\t\topts.StorageOpt = make(map[string]string)\n\t}\n\n\t// Merge daemon default config.\n\tif _, ok := opts.StorageOpt[\"size\"]; !ok && d.options.quota.Size != 0 {\n\t\topts.StorageOpt[\"size\"] = strconv.FormatUint(d.options.quota.Size, 10)\n\t}\n\n\tif _, ok := opts.StorageOpt[\"size\"]; ok && !projectQuotaSupported {\n\t\treturn fmt.Errorf(\"--storage-opt is supported only for overlay over xfs with 'pquota' mount option\")\n\t}\n\n\treturn d.create(id, parent, opts)\n}\n\n// Create is used to create the upper, lower, and merge directories required for overlay fs for a given id.\n// The parent filesystem is used to configure these directories for the overlay.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\tif _, ok := opts.StorageOpt[\"size\"]; ok {\n\t\t\treturn fmt.Errorf(\"--storage-opt size is only supported for ReadWrite Layers\")\n\t\t}\n\t}\n\treturn d.create(id, parent, opts)\n}\n\nfunc (d *Driver) create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tdir := d.dir(id)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\troot := idtools.Identity{UID: rootUID, GID: rootGID}\n\n\tif err := idtools.MkdirAllAndChown(path.Dir(dir), 0700, root); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Clean up on failure\n\t\tif retErr != nil {\n\t\t\tos.RemoveAll(dir)\n\t\t}\n\t}()\n\n\tif opts != nil && len(opts.StorageOpt) > 0 {\n\t\tdriver := &Driver{}\n\t\tif err := d.parseStorageOpt(opts.StorageOpt, driver); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif driver.options.quota.Size > 0 {\n\t\t\t// Set container disk quota limit\n\t\t\tif err := d.quotaCtl.SetQuota(dir, driver.options.quota); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, diffDirName), 0755, root); err != nil {\n\t\treturn err\n\t}\n\n\tlid := overlayutils.GenerateID(idLength, logger)\n\tif err := os.Symlink(path.Join(\"..\", id, diffDirName), path.Join(d.home, linkDir, lid)); err != nil {\n\t\treturn err\n\t}\n\n\t// Write link id to link file\n\tif err := ioutil.WriteFile(path.Join(dir, \"link\"), []byte(lid), 0644); err != nil {\n\t\treturn err\n\t}\n\n\t// if no parent directory, done\n\tif parent == \"\" {\n\t\treturn nil\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, workDirName), 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\tif err := ioutil.WriteFile(path.Join(d.dir(parent), \"committed\"), []byte{}, 0600); err != nil {\n\t\treturn err\n\t}\n\n\tlower, err := d.getLower(parent)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif lower != \"\" {\n\t\tif err := ioutil.WriteFile(path.Join(dir, lowerFile), []byte(lower), 0666); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Parse overlay storage options\nfunc (d *Driver) parseStorageOpt(storageOpt map[string]string, driver *Driver) error {\n\t// Read size to set the disk project quota per container\n\tfor key, val := range storageOpt {\n\t\tkey := strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdriver.options.quota.Size = uint64(size)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (d *Driver) getLower(parent string) (string, error) {\n\tparentDir := d.dir(parent)\n\n\t// Ensure parent exists\n\tif _, err := os.Lstat(parentDir); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Read Parent link fileA\n\tparentLink, err := ioutil.ReadFile(path.Join(parentDir, \"link\"))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tlowers := []string{path.Join(linkDir, string(parentLink))}\n\n\tparentLower, err := ioutil.ReadFile(path.Join(parentDir, lowerFile))\n\tif err == nil {\n\t\tparentLowers := strings.Split(string(parentLower), \":\")\n\t\tlowers = append(lowers, parentLowers...)\n\t}\n\tif len(lowers) > maxDepth {\n\t\treturn \"\", errors.New(\"max depth exceeded\")\n\t}\n\treturn strings.Join(lowers, \":\"), nil\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn path.Join(d.home, id)\n}\n\nfunc (d *Driver) getLowerDirs(id string) ([]string, error) {\n\tvar lowersArray []string\n\tlowers, err := ioutil.ReadFile(path.Join(d.dir(id), lowerFile))\n\tif err == nil {\n\t\tfor _, s := range strings.Split(string(lowers), \":\") {\n\t\t\tlp, err := os.Readlink(path.Join(d.home, s))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlowersArray = append(lowersArray, path.Clean(path.Join(d.home, linkDir, lp)))\n\t\t}\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\treturn lowersArray, nil\n}\n\n// Remove cleans the directories that are created for this id.\nfunc (d *Driver) Remove(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"refusing to remove the directories: id is empty\")\n\t}\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tlid, err := ioutil.ReadFile(path.Join(dir, \"link\"))\n\tif err == nil {\n\t\tif len(lid) == 0 {\n\t\t\tlogger.Errorf(\"refusing to remove empty link for layer %v\", id)\n\t\t} else if err := os.RemoveAll(path.Join(d.home, linkDir, string(lid))); err != nil {\n\t\t\tlogger.Debugf(\"Failed to remove link: %v\", err)\n\t\t}\n\t}\n\n\tif err := system.EnsureRemoveAll(dir); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Get creates and mounts the required file system for the given id and returns the mount path.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, retErr error) {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdiffDir := path.Join(dir, diffDirName)\n\tlowers, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, just return diff directory\n\t\tif os.IsNotExist(err) {\n\t\t\treturn containerfs.NewLocalContainerFS(diffDir), nil\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tmergedDir := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Increment(mergedDir); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif c := d.ctr.Decrement(mergedDir); c <= 0 {\n\t\t\t\tif mntErr := unix.Unmount(mergedDir, 0); mntErr != nil {\n\t\t\t\t\tlogger.Errorf(\"error unmounting %v: %v\", mergedDir, mntErr)\n\t\t\t\t}\n\t\t\t\t// Cleanup the created merged directory; see the comment in Put's rmdir\n\t\t\t\tif rmErr := unix.Rmdir(mergedDir); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogger.Debugf(\"Failed to remove %s: %v: %v\", id, rmErr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tworkDir := path.Join(dir, workDirName)\n\tsplitLowers := strings.Split(string(lowers), \":\")\n\tabsLowers := make([]string, len(splitLowers))\n\tfor i, s := range splitLowers {\n\t\tabsLowers[i] = path.Join(d.home, s)\n\t}\n\tvar readonly bool\n\tif _, err := os.Stat(path.Join(dir, \"committed\")); err == nil {\n\t\treadonly = true\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tvar opts string\n\tif readonly {\n\t\topts = indexOff + \"lowerdir=\" + diffDir + \":\" + strings.Join(absLowers, \":\")\n\t} else {\n\t\topts = indexOff + \"lowerdir=\" + strings.Join(absLowers, \":\") + \",upperdir=\" + diffDir + \",workdir=\" + workDir\n\t}\n\n\tmountData := label.FormatMountLabel(opts, mountLabel)\n\tmount := unix.Mount\n\tmountTarget := mergedDir\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(mergedDir, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\tpageSize := unix.Getpagesize()\n\n\t// Use relative paths and mountFrom when the mount data has exceeded\n\t// the page size. The mount syscall fails if the mount data cannot\n\t// fit within a page and relative links make the mount data much\n\t// smaller at the expense of requiring a fork exec to chroot.\n\tif len(mountData) > pageSize-1 {\n\t\tif readonly {\n\t\t\topts = indexOff + \"lowerdir=\" + path.Join(id, diffDirName) + \":\" + string(lowers)\n\t\t} else {\n\t\t\topts = indexOff + \"lowerdir=\" + string(lowers) + \",upperdir=\" + path.Join(id, diffDirName) + \",workdir=\" + path.Join(id, workDirName)\n\t\t}\n\t\tmountData = label.FormatMountLabel(opts, mountLabel)\n\t\tif len(mountData) > pageSize-1 {\n\t\t\treturn nil, fmt.Errorf(\"cannot mount layer, mount label too large %d\", len(mountData))\n\t\t}\n\n\t\tmount = func(source string, target string, mType string, flags uintptr, label string) error {\n\t\t\treturn mountFrom(d.home, source, target, mType, flags, label)\n\t\t}\n\t\tmountTarget = path.Join(id, mergedDirName)\n\t}\n\n\tif err := mount(\"overlay\", mountTarget, \"overlay\", 0, mountData); err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating overlay mount to %s: %v\", mergedDir, err)\n\t}\n\n\tif !readonly {\n\t\t// chown \"workdir/work\" to the remapped root UID/GID. Overlay fs inside a\n\t\t// user namespace requires this to move a directory from lower to upper.\n\t\tif err := os.Chown(path.Join(workDir, workDirName), rootUID, rootGID); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n}\n\n// Put unmounts the mount path created for the give id.\n// It also removes the 'merged' directory to force the kernel to unmount the\n// overlay mount in other namespaces.\nfunc (d *Driver) Put(id string) error {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\t_, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, no mount happened and just return directly\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\n\tmountpoint := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\tlogger.Debugf(\"Failed to unmount %s overlay: %s - %v\", id, mountpoint, err)\n\t}\n\t// Remove the mountpoint here. Removing the mountpoint (in newer kernels)\n\t// will cause all other instances of this mount in other mount namespaces\n\t// to be unmounted. This is necessary to avoid cases where an overlay mount\n\t// that is present in another namespace will cause subsequent mounts\n\t// operations to fail with ebusy.  We ignore any errors here because this may\n\t// fail on older kernels which don't have\n\t// torvalds/linux@8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe applied.\n\tif err := unix.Rmdir(mountpoint); err != nil && !os.IsNotExist(err) {\n\t\tlogger.Debugf(\"Failed to remove %s overlay: %v\", id, err)\n\t}\n\treturn nil\n}\n\n// Exists checks to see if the id is already mounted.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n\n// isParent determines whether the given parent is the direct parent of the\n// given layer id\nfunc (d *Driver) isParent(id, parent string) bool {\n\tlowers, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif parent == \"\" && len(lowers) > 0 {\n\t\treturn false\n\t}\n\n\tparentDir := d.dir(parent)\n\tvar ld string\n\tif len(lowers) > 0 {\n\t\tld = filepath.Dir(lowers[0])\n\t}\n\tif ld == \"\" && parent == \"\" {\n\t\treturn true\n\t}\n\treturn ld == parentDir\n}\n\n// ApplyDiff applies the new layer into a root\nfunc (d *Driver) ApplyDiff(id string, parent string, diff io.Reader) (size int64, err error) {\n\tif !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.ApplyDiff(id, parent, diff)\n\t}\n\n\tapplyDir := d.getDiffPath(id)\n\n\tlogger.Debugf(\"Applying tar in %s\", applyDir)\n\t// Overlay doesn't need the parent id to apply the diff\n\tif err := untar(diff, applyDir, &archive.TarOptions{\n\t\tUIDMaps:        d.uidMaps,\n\t\tGIDMaps:        d.gidMaps,\n\t\tWhiteoutFormat: archive.OverlayWhiteoutFormat,\n\t\tInUserNS:       sys.RunningInUserNS(),\n\t}); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn directory.Size(context.TODO(), applyDir)\n}\n\nfunc (d *Driver) getDiffPath(id string) string {\n\tdir := d.dir(id)\n\n\treturn path.Join(dir, diffDirName)\n}\n\n// DiffSize calculates the changes between the specified id\n// and its parent and returns the size in bytes of the changes\n// relative to its base filesystem directory.\nfunc (d *Driver) DiffSize(id, parent string) (size int64, err error) {\n\tif useNaiveDiff(d.home) || !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.DiffSize(id, parent)\n\t}\n\treturn directory.Size(context.TODO(), d.getDiffPath(id))\n}\n\n// Diff produces an archive of the changes between the specified\n// layer and its parent layer which may be \"\".\nfunc (d *Driver) Diff(id, parent string) (io.ReadCloser, error) {\n\tif useNaiveDiff(d.home) || !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.Diff(id, parent)\n\t}\n\n\tdiffPath := d.getDiffPath(id)\n\tlogger.Debugf(\"Tar with options on %s\", diffPath)\n\treturn archive.TarWithOptions(diffPath, &archive.TarOptions{\n\t\tCompression:    archive.Uncompressed,\n\t\tUIDMaps:        d.uidMaps,\n\t\tGIDMaps:        d.gidMaps,\n\t\tWhiteoutFormat: archive.OverlayWhiteoutFormat,\n\t})\n}\n\n// Changes produces a list of changes between the specified layer and its\n// parent layer. If parent is \"\", then all changes will be ADD changes.\nfunc (d *Driver) Changes(id, parent string) ([]archive.Change, error) {\n\treturn d.naiveDiff.Changes(id, parent)\n}\n", "package vfs // import \"github.com/docker/docker/daemon/graphdriver/vfs\"\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/quota\"\n\tunits \"github.com/docker/go-units\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n)\n\nvar (\n\t// CopyDir defines the copy method to use.\n\tCopyDir = dirCopy\n)\n\nfunc init() {\n\tgraphdriver.Register(\"vfs\", Init)\n}\n\n// Init returns a new VFS driver.\n// This sets the home directory for the driver and returns NaiveDiffDriver.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\td := &Driver{\n\t\thome:      home,\n\t\tidMapping: idtools.NewIDMappingsFromMaps(uidMaps, gidMaps),\n\t}\n\n\tif err := d.parseOptions(options); err != nil {\n\t\treturn nil, err\n\t}\n\n\trootIDs := d.idMapping.RootPair()\n\tif err := idtools.MkdirAllAndChown(home, 0700, rootIDs); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsetupDriverQuota(d)\n\n\tif size := d.getQuotaOpt(); !d.quotaSupported() && size > 0 {\n\t\treturn nil, quota.ErrQuotaNotSupported\n\t}\n\n\treturn graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps), nil\n}\n\n// Driver holds information about the driver, home directory of the driver.\n// Driver implements graphdriver.ProtoDriver. It uses only basic vfs operations.\n// In order to support layering, files are copied from the parent layer into the new layer. There is no copy-on-write support.\n// Driver must be wrapped in NaiveDiffDriver to be used as a graphdriver.Driver\ntype Driver struct {\n\tdriverQuota\n\thome      string\n\tidMapping *idtools.IdentityMapping\n}\n\nfunc (d *Driver) String() string {\n\treturn \"vfs\"\n}\n\n// Status is used for implementing the graphdriver.ProtoDriver interface. VFS does not currently have any status information.\nfunc (d *Driver) Status() [][2]string {\n\treturn nil\n}\n\n// GetMetadata is used for implementing the graphdriver.ProtoDriver interface. VFS does not currently have any meta data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn nil, nil\n}\n\n// Cleanup is used to implement graphdriver.ProtoDriver. There is no cleanup required for this driver.\nfunc (d *Driver) Cleanup() error {\n\treturn nil\n}\n\nfunc (d *Driver) parseOptions(options []string) error {\n\tfor _, option := range options {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn errdefs.InvalidParameter(err)\n\t\t}\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn errdefs.InvalidParameter(err)\n\t\t\t}\n\t\t\tif err = d.setQuotaOpt(uint64(size)); err != nil {\n\t\t\t\treturn errdefs.InvalidParameter(errors.Wrap(err, \"failed to set option size for vfs\"))\n\t\t\t}\n\t\tdefault:\n\t\t\treturn errdefs.InvalidParameter(errors.Errorf(\"unknown option %s for vfs\", key))\n\t\t}\n\t}\n\treturn nil\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\tquotaSize := d.getQuotaOpt()\n\n\tif opts != nil {\n\t\tfor key, val := range opts.StorageOpt {\n\t\t\tswitch key {\n\t\t\tcase \"size\":\n\t\t\t\tif !d.quotaSupported() {\n\t\t\t\t\treturn quota.ErrQuotaNotSupported\n\t\t\t\t}\n\t\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errdefs.InvalidParameter(err)\n\t\t\t\t}\n\t\t\t\tquotaSize = uint64(size)\n\t\t\tdefault:\n\t\t\t\treturn errdefs.InvalidParameter(errors.Errorf(\"Storage opt %s not supported\", key))\n\t\t\t}\n\t\t}\n\t}\n\n\treturn d.create(id, parent, quotaSize)\n}\n\n// Create prepares the filesystem for the VFS driver and copies the directory for the given id under the parent.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported for vfs on read-only layers\")\n\t}\n\n\treturn d.create(id, parent, 0)\n}\n\nfunc (d *Driver) create(id, parent string, size uint64) error {\n\tdir := d.dir(id)\n\trootIDs := d.idMapping.RootPair()\n\tif err := idtools.MkdirAllAndChown(filepath.Dir(dir), 0700, rootIDs); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0755, rootIDs); err != nil {\n\t\treturn err\n\t}\n\n\tif size != 0 {\n\t\tif err := d.setupQuota(dir, size); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlabelOpts := []string{\"level:s0\"}\n\tif _, mountLabel, err := label.InitLabels(labelOpts); err == nil {\n\t\tlabel.SetFileLabel(dir, mountLabel)\n\t}\n\tif parent == \"\" {\n\t\treturn nil\n\t}\n\tparentDir, err := d.Get(parent, \"\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s: %s\", parent, err)\n\t}\n\treturn CopyDir(parentDir.Path(), dir)\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn filepath.Join(d.home, \"dir\", filepath.Base(id))\n}\n\n// Remove deletes the content from the directory for a given id.\nfunc (d *Driver) Remove(id string) error {\n\treturn system.EnsureRemoveAll(d.dir(id))\n}\n\n// Get returns the directory for the given id.\nfunc (d *Driver) Get(id, mountLabel string) (containerfs.ContainerFS, error) {\n\tdir := d.dir(id)\n\tif st, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t} else if !st.IsDir() {\n\t\treturn nil, fmt.Errorf(\"%s: not a directory\", dir)\n\t}\n\treturn containerfs.NewLocalContainerFS(dir), nil\n}\n\n// Put is a noop for vfs that return nil for the error, since this driver has no runtime resources to clean up.\nfunc (d *Driver) Put(id string) error {\n\t// The vfs driver has no runtime resources (e.g. mounts)\n\t// to clean up, so we don't need anything here\n\treturn nil\n}\n\n// Exists checks to see if the directory exists for the given id.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n", "// +build linux freebsd\n\npackage zfs // import \"github.com/docker/docker/daemon/graphdriver/zfs\"\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\tzfs \"github.com/mistifyio/go-zfs\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/moby/sys/mountinfo\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\ntype zfsOptions struct {\n\tfsName    string\n\tmountPath string\n}\n\nfunc init() {\n\tgraphdriver.Register(\"zfs\", Init)\n}\n\n// Logger returns a zfs logger implementation.\ntype Logger struct{}\n\n// Log wraps log message from ZFS driver with a prefix '[zfs]'.\nfunc (*Logger) Log(cmd []string) {\n\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(\"[zfs] %s\", strings.Join(cmd, \" \"))\n}\n\n// Init returns a new ZFS driver.\n// It takes base mount path and an array of options which are represented as key value pairs.\n// Each option is in the for key=value. 'zfs.fsname' is expected to be a valid key in the options.\nfunc Init(base string, opt []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\tvar err error\n\n\tlogger := logrus.WithField(\"storage-driver\", \"zfs\")\n\n\tif _, err := exec.LookPath(\"zfs\"); err != nil {\n\t\tlogger.Debugf(\"zfs command is not available: %v\", err)\n\t\treturn nil, graphdriver.ErrPrerequisites\n\t}\n\n\tfile, err := os.OpenFile(\"/dev/zfs\", os.O_RDWR, 0600)\n\tif err != nil {\n\t\tlogger.Debugf(\"cannot open /dev/zfs: %v\", err)\n\t\treturn nil, graphdriver.ErrPrerequisites\n\t}\n\tdefer file.Close()\n\n\toptions, err := parseOptions(opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\toptions.mountPath = base\n\n\trootdir := path.Dir(base)\n\n\tif options.fsName == \"\" {\n\t\terr = checkRootdirFs(rootdir)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif options.fsName == \"\" {\n\t\toptions.fsName, err = lookupZfsDataset(rootdir)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tzfs.SetLogger(new(Logger))\n\n\tfilesystems, err := zfs.Filesystems(options.fsName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Cannot find root filesystem %s: %v\", options.fsName, err)\n\t}\n\n\tfilesystemsCache := make(map[string]bool, len(filesystems))\n\tvar rootDataset *zfs.Dataset\n\tfor _, fs := range filesystems {\n\t\tif fs.Name == options.fsName {\n\t\t\trootDataset = fs\n\t\t}\n\t\tfilesystemsCache[fs.Name] = true\n\t}\n\n\tif rootDataset == nil {\n\t\treturn nil, fmt.Errorf(\"BUG: zfs get all -t filesystem -rHp '%s' should contain '%s'\", options.fsName, options.fsName)\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(uidMaps, gidMaps)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Failed to get root uid/guid: %v\", err)\n\t}\n\tif err := idtools.MkdirAllAndChown(base, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, fmt.Errorf(\"Failed to create '%s': %v\", base, err)\n\t}\n\n\td := &Driver{\n\t\tdataset:          rootDataset,\n\t\toptions:          options,\n\t\tfilesystemsCache: filesystemsCache,\n\t\tuidMaps:          uidMaps,\n\t\tgidMaps:          gidMaps,\n\t\tctr:              graphdriver.NewRefCounter(graphdriver.NewDefaultChecker()),\n\t}\n\treturn graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps), nil\n}\n\nfunc parseOptions(opt []string) (zfsOptions, error) {\n\tvar options zfsOptions\n\toptions.fsName = \"\"\n\tfor _, option := range opt {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn options, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"zfs.fsname\":\n\t\t\toptions.fsName = val\n\t\tdefault:\n\t\t\treturn options, fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\treturn options, nil\n}\n\nfunc lookupZfsDataset(rootdir string) (string, error) {\n\tvar stat unix.Stat_t\n\tif err := unix.Stat(rootdir, &stat); err != nil {\n\t\treturn \"\", fmt.Errorf(\"Failed to access '%s': %s\", rootdir, err)\n\t}\n\twantedDev := stat.Dev\n\n\tmounts, err := mountinfo.GetMounts(nil)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tfor _, m := range mounts {\n\t\tif err := unix.Stat(m.Mountpoint, &stat); err != nil {\n\t\t\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(\"failed to stat '%s' while scanning for zfs mount: %v\", m.Mountpoint, err)\n\t\t\tcontinue // may fail on fuse file systems\n\t\t}\n\n\t\tif stat.Dev == wantedDev && m.FSType == \"zfs\" {\n\t\t\treturn m.Source, nil\n\t\t}\n\t}\n\n\treturn \"\", fmt.Errorf(\"Failed to find zfs dataset mounted on '%s' in /proc/mounts\", rootdir)\n}\n\n// Driver holds information about the driver, such as zfs dataset, options and cache.\ntype Driver struct {\n\tdataset          *zfs.Dataset\n\toptions          zfsOptions\n\tsync.Mutex       // protects filesystem cache against concurrent access\n\tfilesystemsCache map[string]bool\n\tuidMaps          []idtools.IDMap\n\tgidMaps          []idtools.IDMap\n\tctr              *graphdriver.RefCounter\n}\n\nfunc (d *Driver) String() string {\n\treturn \"zfs\"\n}\n\n// Cleanup is called on daemon shutdown, it is a no-op for ZFS.\n// TODO(@cpuguy83): Walk layer tree and check mounts?\nfunc (d *Driver) Cleanup() error {\n\treturn nil\n}\n\n// Status returns information about the ZFS filesystem. It returns a two dimensional array of information\n// such as pool name, dataset name, disk usage, parent quota and compression used.\n// Currently it return 'Zpool', 'Zpool Health', 'Parent Dataset', 'Space Used By Parent',\n// 'Space Available', 'Parent Quota' and 'Compression'.\nfunc (d *Driver) Status() [][2]string {\n\tparts := strings.Split(d.dataset.Name, \"/\")\n\tpool, err := zfs.GetZpool(parts[0])\n\n\tvar poolName, poolHealth string\n\tif err == nil {\n\t\tpoolName = pool.Name\n\t\tpoolHealth = pool.Health\n\t} else {\n\t\tpoolName = fmt.Sprintf(\"error while getting pool information %v\", err)\n\t\tpoolHealth = \"not available\"\n\t}\n\n\tquota := \"no\"\n\tif d.dataset.Quota != 0 {\n\t\tquota = strconv.FormatUint(d.dataset.Quota, 10)\n\t}\n\n\treturn [][2]string{\n\t\t{\"Zpool\", poolName},\n\t\t{\"Zpool Health\", poolHealth},\n\t\t{\"Parent Dataset\", d.dataset.Name},\n\t\t{\"Space Used By Parent\", strconv.FormatUint(d.dataset.Used, 10)},\n\t\t{\"Space Available\", strconv.FormatUint(d.dataset.Avail, 10)},\n\t\t{\"Parent Quota\", quota},\n\t\t{\"Compression\", d.dataset.Compression},\n\t}\n}\n\n// GetMetadata returns image/container metadata related to graph driver\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn map[string]string{\n\t\t\"Mountpoint\": d.mountPath(id),\n\t\t\"Dataset\":    d.zfsPath(id),\n\t}, nil\n}\n\nfunc (d *Driver) cloneFilesystem(name, parentName string) error {\n\tsnapshotName := fmt.Sprintf(\"%d\", time.Now().Nanosecond())\n\tparentDataset := zfs.Dataset{Name: parentName}\n\tsnapshot, err := parentDataset.Snapshot(snapshotName /*recursive */, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, err = snapshot.Clone(name, map[string]string{\"mountpoint\": \"legacy\"})\n\tif err == nil {\n\t\td.Lock()\n\t\td.filesystemsCache[name] = true\n\t\td.Unlock()\n\t}\n\n\tif err != nil {\n\t\tsnapshot.Destroy(zfs.DestroyDeferDeletion)\n\t\treturn err\n\t}\n\treturn snapshot.Destroy(zfs.DestroyDeferDeletion)\n}\n\nfunc (d *Driver) zfsPath(id string) string {\n\treturn d.options.fsName + \"/\" + id\n}\n\nfunc (d *Driver) mountPath(id string) string {\n\treturn path.Join(d.options.mountPath, \"graph\", getMountpoint(id))\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn d.Create(id, parent, opts)\n}\n\n// Create prepares the dataset and filesystem for the ZFS driver for the given id under the parent.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\tvar storageOpt map[string]string\n\tif opts != nil {\n\t\tstorageOpt = opts.StorageOpt\n\t}\n\n\terr := d.create(id, parent, storageOpt)\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif zfsError, ok := err.(*zfs.Error); ok {\n\t\tif !strings.HasSuffix(zfsError.Stderr, \"dataset already exists\\n\") {\n\t\t\treturn err\n\t\t}\n\t\t// aborted build -> cleanup\n\t} else {\n\t\treturn err\n\t}\n\n\tdataset := zfs.Dataset{Name: d.zfsPath(id)}\n\tif err := dataset.Destroy(zfs.DestroyRecursiveClones); err != nil {\n\t\treturn err\n\t}\n\n\t// retry\n\treturn d.create(id, parent, storageOpt)\n}\n\nfunc (d *Driver) create(id, parent string, storageOpt map[string]string) error {\n\tname := d.zfsPath(id)\n\tquota, err := parseStorageOpt(storageOpt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif parent == \"\" {\n\t\tmountoptions := map[string]string{\"mountpoint\": \"legacy\"}\n\t\tfs, err := zfs.CreateFilesystem(name, mountoptions)\n\t\tif err == nil {\n\t\t\terr = setQuota(name, quota)\n\t\t\tif err == nil {\n\t\t\t\td.Lock()\n\t\t\t\td.filesystemsCache[fs.Name] = true\n\t\t\t\td.Unlock()\n\t\t\t}\n\t\t}\n\t\treturn err\n\t}\n\terr = d.cloneFilesystem(name, d.zfsPath(parent))\n\tif err == nil {\n\t\terr = setQuota(name, quota)\n\t}\n\treturn err\n}\n\nfunc parseStorageOpt(storageOpt map[string]string) (string, error) {\n\t// Read size to change the disk quota per container\n\tfor k, v := range storageOpt {\n\t\tkey := strings.ToLower(k)\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\treturn v, nil\n\t\tdefault:\n\t\t\treturn \"0\", fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\treturn \"0\", nil\n}\n\nfunc setQuota(name string, quota string) error {\n\tif quota == \"0\" {\n\t\treturn nil\n\t}\n\tfs, err := zfs.GetDataset(name)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn fs.SetProperty(\"quota\", quota)\n}\n\n// Remove deletes the dataset, filesystem and the cache for the given id.\nfunc (d *Driver) Remove(id string) error {\n\tname := d.zfsPath(id)\n\tdataset := zfs.Dataset{Name: name}\n\terr := dataset.Destroy(zfs.DestroyRecursive)\n\tif err == nil {\n\t\td.Lock()\n\t\tdelete(d.filesystemsCache, name)\n\t\td.Unlock()\n\t}\n\treturn err\n}\n\n// Get returns the mountpoint for the given id after creating the target directories if necessary.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, retErr error) {\n\tmountpoint := d.mountPath(id)\n\tif count := d.ctr.Increment(mountpoint); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mountpoint), nil\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif c := d.ctr.Decrement(mountpoint); c <= 0 {\n\t\t\t\tif mntErr := unix.Unmount(mountpoint, 0); mntErr != nil {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"zfs\").Errorf(\"Error unmounting %v: %v\", mountpoint, mntErr)\n\t\t\t\t}\n\t\t\t\tif rmErr := unix.Rmdir(mountpoint); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(\"Failed to remove %s: %v\", id, rmErr)\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}()\n\n\tfilesystem := d.zfsPath(id)\n\toptions := label.FormatMountLabel(\"\", mountLabel)\n\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(`mount(\"%s\", \"%s\", \"%s\")`, filesystem, mountpoint, options)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create the target directories if they don't exist\n\tif err := idtools.MkdirAllAndChown(mountpoint, 0755, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := mount.Mount(filesystem, mountpoint, \"zfs\", options); err != nil {\n\t\treturn nil, errors.Wrap(err, \"error creating zfs mount\")\n\t}\n\n\t// this could be our first mount after creation of the filesystem, and the root dir may still have root\n\t// permissions instead of the remapped root uid:gid (if user namespaces are enabled):\n\tif err := os.Chown(mountpoint, rootUID, rootGID); err != nil {\n\t\treturn nil, fmt.Errorf(\"error modifying zfs mountpoint (%s) directory ownership: %v\", mountpoint, err)\n\t}\n\n\treturn containerfs.NewLocalContainerFS(mountpoint), nil\n}\n\n// Put removes the existing mountpoint for the given id if it exists.\nfunc (d *Driver) Put(id string) error {\n\tmountpoint := d.mountPath(id)\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\n\tlogger := logrus.WithField(\"storage-driver\", \"zfs\")\n\n\tlogger.Debugf(`unmount(\"%s\")`, mountpoint)\n\n\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\tlogger.Warnf(\"Failed to unmount %s mount %s: %v\", id, mountpoint, err)\n\t}\n\tif err := unix.Rmdir(mountpoint); err != nil && !os.IsNotExist(err) {\n\t\tlogger.Debugf(\"Failed to remove %s mount point %s: %v\", id, mountpoint, err)\n\t}\n\n\treturn nil\n}\n\n// Exists checks to see if the cache entry exists for the given id.\nfunc (d *Driver) Exists(id string) bool {\n\td.Lock()\n\tdefer d.Unlock()\n\treturn d.filesystemsCache[d.zfsPath(id)]\n}\n", "package idtools // import \"github.com/docker/docker/pkg/idtools\"\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// IDMap contains a single entry for user namespace range remapping. An array\n// of IDMap entries represents the structure that will be provided to the Linux\n// kernel for creating a user namespace.\ntype IDMap struct {\n\tContainerID int `json:\"container_id\"`\n\tHostID      int `json:\"host_id\"`\n\tSize        int `json:\"size\"`\n}\n\ntype subIDRange struct {\n\tStart  int\n\tLength int\n}\n\ntype ranges []subIDRange\n\nfunc (e ranges) Len() int           { return len(e) }\nfunc (e ranges) Swap(i, j int)      { e[i], e[j] = e[j], e[i] }\nfunc (e ranges) Less(i, j int) bool { return e[i].Start < e[j].Start }\n\nconst (\n\tsubuidFileName = \"/etc/subuid\"\n\tsubgidFileName = \"/etc/subgid\"\n)\n\n// MkdirAllAndChown creates a directory (include any along the path) and then modifies\n// ownership to the requested uid/gid.  If the directory already exists, this\n// function will still change ownership to the requested uid/gid pair.\nfunc MkdirAllAndChown(path string, mode os.FileMode, owner Identity) error {\n\treturn mkdirAs(path, mode, owner, true, true)\n}\n\n// MkdirAndChown creates a directory and then modifies ownership to the requested uid/gid.\n// If the directory already exists, this function still changes ownership.\n// Note that unlike os.Mkdir(), this function does not return IsExist error\n// in case path already exists.\nfunc MkdirAndChown(path string, mode os.FileMode, owner Identity) error {\n\treturn mkdirAs(path, mode, owner, false, true)\n}\n\n// MkdirAllAndChownNew creates a directory (include any along the path) and then modifies\n// ownership ONLY of newly created directories to the requested uid/gid. If the\n// directories along the path exist, no change of ownership will be performed\nfunc MkdirAllAndChownNew(path string, mode os.FileMode, owner Identity) error {\n\treturn mkdirAs(path, mode, owner, true, false)\n}\n\n// GetRootUIDGID retrieves the remapped root uid/gid pair from the set of maps.\n// If the maps are empty, then the root uid/gid will default to \"real\" 0/0\nfunc GetRootUIDGID(uidMap, gidMap []IDMap) (int, int, error) {\n\tuid, err := toHost(0, uidMap)\n\tif err != nil {\n\t\treturn -1, -1, err\n\t}\n\tgid, err := toHost(0, gidMap)\n\tif err != nil {\n\t\treturn -1, -1, err\n\t}\n\treturn uid, gid, nil\n}\n\n// toContainer takes an id mapping, and uses it to translate a\n// host ID to the remapped ID. If no map is provided, then the translation\n// assumes a 1-to-1 mapping and returns the passed in id\nfunc toContainer(hostID int, idMap []IDMap) (int, error) {\n\tif idMap == nil {\n\t\treturn hostID, nil\n\t}\n\tfor _, m := range idMap {\n\t\tif (hostID >= m.HostID) && (hostID <= (m.HostID + m.Size - 1)) {\n\t\t\tcontID := m.ContainerID + (hostID - m.HostID)\n\t\t\treturn contID, nil\n\t\t}\n\t}\n\treturn -1, fmt.Errorf(\"Host ID %d cannot be mapped to a container ID\", hostID)\n}\n\n// toHost takes an id mapping and a remapped ID, and translates the\n// ID to the mapped host ID. If no map is provided, then the translation\n// assumes a 1-to-1 mapping and returns the passed in id #\nfunc toHost(contID int, idMap []IDMap) (int, error) {\n\tif idMap == nil {\n\t\treturn contID, nil\n\t}\n\tfor _, m := range idMap {\n\t\tif (contID >= m.ContainerID) && (contID <= (m.ContainerID + m.Size - 1)) {\n\t\t\thostID := m.HostID + (contID - m.ContainerID)\n\t\t\treturn hostID, nil\n\t\t}\n\t}\n\treturn -1, fmt.Errorf(\"Container ID %d cannot be mapped to a host ID\", contID)\n}\n\n// Identity is either a UID and GID pair or a SID (but not both)\ntype Identity struct {\n\tUID int\n\tGID int\n\tSID string\n}\n\n// IdentityMapping contains a mappings of UIDs and GIDs\ntype IdentityMapping struct {\n\tuids []IDMap\n\tgids []IDMap\n}\n\n// NewIDMappingsFromMaps creates a new mapping from two slices\n// Deprecated: this is a temporary shim while transitioning to IDMapping\nfunc NewIDMappingsFromMaps(uids []IDMap, gids []IDMap) *IdentityMapping {\n\treturn &IdentityMapping{uids: uids, gids: gids}\n}\n\n// RootPair returns a uid and gid pair for the root user. The error is ignored\n// because a root user always exists, and the defaults are correct when the uid\n// and gid maps are empty.\nfunc (i *IdentityMapping) RootPair() Identity {\n\tuid, gid, _ := GetRootUIDGID(i.uids, i.gids)\n\treturn Identity{UID: uid, GID: gid}\n}\n\n// ToHost returns the host UID and GID for the container uid, gid.\n// Remapping is only performed if the ids aren't already the remapped root ids\nfunc (i *IdentityMapping) ToHost(pair Identity) (Identity, error) {\n\tvar err error\n\ttarget := i.RootPair()\n\n\tif pair.UID != target.UID {\n\t\ttarget.UID, err = toHost(pair.UID, i.uids)\n\t\tif err != nil {\n\t\t\treturn target, err\n\t\t}\n\t}\n\n\tif pair.GID != target.GID {\n\t\ttarget.GID, err = toHost(pair.GID, i.gids)\n\t}\n\treturn target, err\n}\n\n// ToContainer returns the container UID and GID for the host uid and gid\nfunc (i *IdentityMapping) ToContainer(pair Identity) (int, int, error) {\n\tuid, err := toContainer(pair.UID, i.uids)\n\tif err != nil {\n\t\treturn -1, -1, err\n\t}\n\tgid, err := toContainer(pair.GID, i.gids)\n\treturn uid, gid, err\n}\n\n// Empty returns true if there are no id mappings\nfunc (i *IdentityMapping) Empty() bool {\n\treturn len(i.uids) == 0 && len(i.gids) == 0\n}\n\n// UIDs return the UID mapping\n// TODO: remove this once everything has been refactored to use pairs\nfunc (i *IdentityMapping) UIDs() []IDMap {\n\treturn i.uids\n}\n\n// GIDs return the UID mapping\n// TODO: remove this once everything has been refactored to use pairs\nfunc (i *IdentityMapping) GIDs() []IDMap {\n\treturn i.gids\n}\n\nfunc createIDMap(subidRanges ranges) []IDMap {\n\tidMap := []IDMap{}\n\n\tcontainerID := 0\n\tfor _, idrange := range subidRanges {\n\t\tidMap = append(idMap, IDMap{\n\t\t\tContainerID: containerID,\n\t\t\tHostID:      idrange.Start,\n\t\t\tSize:        idrange.Length,\n\t\t})\n\t\tcontainerID = containerID + idrange.Length\n\t}\n\treturn idMap\n}\n\nfunc parseSubuid(username string) (ranges, error) {\n\treturn parseSubidFile(subuidFileName, username)\n}\n\nfunc parseSubgid(username string) (ranges, error) {\n\treturn parseSubidFile(subgidFileName, username)\n}\n\n// parseSubidFile will read the appropriate file (/etc/subuid or /etc/subgid)\n// and return all found ranges for a specified username. If the special value\n// \"ALL\" is supplied for username, then all ranges in the file will be returned\nfunc parseSubidFile(path, username string) (ranges, error) {\n\tvar rangeList ranges\n\n\tsubidFile, err := os.Open(path)\n\tif err != nil {\n\t\treturn rangeList, err\n\t}\n\tdefer subidFile.Close()\n\n\ts := bufio.NewScanner(subidFile)\n\tfor s.Scan() {\n\t\ttext := strings.TrimSpace(s.Text())\n\t\tif text == \"\" || strings.HasPrefix(text, \"#\") {\n\t\t\tcontinue\n\t\t}\n\t\tparts := strings.Split(text, \":\")\n\t\tif len(parts) != 3 {\n\t\t\treturn rangeList, fmt.Errorf(\"Cannot parse subuid/gid information: Format not correct for %s file\", path)\n\t\t}\n\t\tif parts[0] == username || username == \"ALL\" {\n\t\t\tstartid, err := strconv.Atoi(parts[1])\n\t\t\tif err != nil {\n\t\t\t\treturn rangeList, fmt.Errorf(\"String to int conversion failed during subuid/gid parsing of %s: %v\", path, err)\n\t\t\t}\n\t\t\tlength, err := strconv.Atoi(parts[2])\n\t\t\tif err != nil {\n\t\t\t\treturn rangeList, fmt.Errorf(\"String to int conversion failed during subuid/gid parsing of %s: %v\", path, err)\n\t\t\t}\n\t\t\trangeList = append(rangeList, subIDRange{startid, length})\n\t\t}\n\t}\n\n\treturn rangeList, s.Err()\n}\n", "// +build !windows\n\npackage idtools // import \"github.com/docker/docker/pkg/idtools\"\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"sync\"\n\t\"syscall\"\n\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/opencontainers/runc/libcontainer/user\"\n\t\"github.com/pkg/errors\"\n)\n\nvar (\n\tentOnce   sync.Once\n\tgetentCmd string\n)\n\nfunc mkdirAs(path string, mode os.FileMode, owner Identity, mkAll, chownExisting bool) error {\n\t// make an array containing the original path asked for, plus (for mkAll == true)\n\t// all path components leading up to the complete path that don't exist before we MkdirAll\n\t// so that we can chown all of them properly at the end.  If chownExisting is false, we won't\n\t// chown the full directory path if it exists\n\n\tvar paths []string\n\n\tstat, err := system.Stat(path)\n\tif err == nil {\n\t\tif !stat.IsDir() {\n\t\t\treturn &os.PathError{Op: \"mkdir\", Path: path, Err: syscall.ENOTDIR}\n\t\t}\n\t\tif !chownExisting {\n\t\t\treturn nil\n\t\t}\n\n\t\t// short-circuit--we were called with an existing directory and chown was requested\n\t\treturn lazyChown(path, owner.UID, owner.GID, stat)\n\t}\n\n\tif os.IsNotExist(err) {\n\t\tpaths = []string{path}\n\t}\n\n\tif mkAll {\n\t\t// walk back to \"/\" looking for directories which do not exist\n\t\t// and add them to the paths array for chown after creation\n\t\tdirPath := path\n\t\tfor {\n\t\t\tdirPath = filepath.Dir(dirPath)\n\t\t\tif dirPath == \"/\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif _, err := os.Stat(dirPath); err != nil && os.IsNotExist(err) {\n\t\t\t\tpaths = append(paths, dirPath)\n\t\t\t}\n\t\t}\n\t\tif err := system.MkdirAll(path, mode); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tif err := os.Mkdir(path, mode); err != nil && !os.IsExist(err) {\n\t\t\treturn err\n\t\t}\n\t}\n\t// even if it existed, we will chown the requested path + any subpaths that\n\t// didn't exist when we called MkdirAll\n\tfor _, pathComponent := range paths {\n\t\tif err := lazyChown(pathComponent, owner.UID, owner.GID, nil); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// CanAccess takes a valid (existing) directory and a uid, gid pair and determines\n// if that uid, gid pair has access (execute bit) to the directory\nfunc CanAccess(path string, pair Identity) bool {\n\tstatInfo, err := system.Stat(path)\n\tif err != nil {\n\t\treturn false\n\t}\n\tfileMode := os.FileMode(statInfo.Mode())\n\tpermBits := fileMode.Perm()\n\treturn accessible(statInfo.UID() == uint32(pair.UID),\n\t\tstatInfo.GID() == uint32(pair.GID), permBits)\n}\n\nfunc accessible(isOwner, isGroup bool, perms os.FileMode) bool {\n\tif isOwner && (perms&0100 == 0100) {\n\t\treturn true\n\t}\n\tif isGroup && (perms&0010 == 0010) {\n\t\treturn true\n\t}\n\tif perms&0001 == 0001 {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// LookupUser uses traditional local system files lookup (from libcontainer/user) on a username,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupUser(name string) (user.User, error) {\n\t// first try a local system files lookup using existing capabilities\n\tusr, err := user.LookupUser(name)\n\tif err == nil {\n\t\treturn usr, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured passwd dbs\n\tusr, err = getentUser(name)\n\tif err != nil {\n\t\treturn user.User{}, err\n\t}\n\treturn usr, nil\n}\n\n// LookupUID uses traditional local system files lookup (from libcontainer/user) on a uid,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupUID(uid int) (user.User, error) {\n\t// first try a local system files lookup using existing capabilities\n\tusr, err := user.LookupUid(uid)\n\tif err == nil {\n\t\treturn usr, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured passwd dbs\n\treturn getentUser(strconv.Itoa(uid))\n}\n\nfunc getentUser(name string) (user.User, error) {\n\treader, err := callGetent(\"passwd\", name)\n\tif err != nil {\n\t\treturn user.User{}, err\n\t}\n\tusers, err := user.ParsePasswd(reader)\n\tif err != nil {\n\t\treturn user.User{}, err\n\t}\n\tif len(users) == 0 {\n\t\treturn user.User{}, fmt.Errorf(\"getent failed to find passwd entry for %q\", name)\n\t}\n\treturn users[0], nil\n}\n\n// LookupGroup uses traditional local system files lookup (from libcontainer/user) on a group name,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupGroup(name string) (user.Group, error) {\n\t// first try a local system files lookup using existing capabilities\n\tgroup, err := user.LookupGroup(name)\n\tif err == nil {\n\t\treturn group, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured group dbs\n\treturn getentGroup(name)\n}\n\n// LookupGID uses traditional local system files lookup (from libcontainer/user) on a group ID,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupGID(gid int) (user.Group, error) {\n\t// first try a local system files lookup using existing capabilities\n\tgroup, err := user.LookupGid(gid)\n\tif err == nil {\n\t\treturn group, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured group dbs\n\treturn getentGroup(strconv.Itoa(gid))\n}\n\nfunc getentGroup(name string) (user.Group, error) {\n\treader, err := callGetent(\"group\", name)\n\tif err != nil {\n\t\treturn user.Group{}, err\n\t}\n\tgroups, err := user.ParseGroup(reader)\n\tif err != nil {\n\t\treturn user.Group{}, err\n\t}\n\tif len(groups) == 0 {\n\t\treturn user.Group{}, fmt.Errorf(\"getent failed to find groups entry for %q\", name)\n\t}\n\treturn groups[0], nil\n}\n\nfunc callGetent(database, key string) (io.Reader, error) {\n\tentOnce.Do(func() { getentCmd, _ = resolveBinary(\"getent\") })\n\t// if no `getent` command on host, can't do anything else\n\tif getentCmd == \"\" {\n\t\treturn nil, fmt.Errorf(\"unable to find getent command\")\n\t}\n\tout, err := execCmd(getentCmd, database, key)\n\tif err != nil {\n\t\texitCode, errC := system.GetExitCode(err)\n\t\tif errC != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tswitch exitCode {\n\t\tcase 1:\n\t\t\treturn nil, fmt.Errorf(\"getent reported invalid parameters/database unknown\")\n\t\tcase 2:\n\t\t\treturn nil, fmt.Errorf(\"getent unable to find entry %q in %s database\", key, database)\n\t\tcase 3:\n\t\t\treturn nil, fmt.Errorf(\"getent database doesn't support enumeration\")\n\t\tdefault:\n\t\t\treturn nil, err\n\t\t}\n\n\t}\n\treturn bytes.NewReader(out), nil\n}\n\n// lazyChown performs a chown only if the uid/gid don't match what's requested\n// Normally a Chown is a no-op if uid/gid match, but in some cases this can still cause an error, e.g. if the\n// dir is on an NFS share, so don't call chown unless we absolutely must.\nfunc lazyChown(p string, uid, gid int, stat *system.StatT) error {\n\tif stat == nil {\n\t\tvar err error\n\t\tstat, err = system.Stat(p)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif stat.UID() == uint32(uid) && stat.GID() == uint32(gid) {\n\t\treturn nil\n\t}\n\treturn os.Chown(p, uid, gid)\n}\n\n// NewIdentityMapping takes a requested username and\n// using the data from /etc/sub{uid,gid} ranges, creates the\n// proper uid and gid remapping ranges for that user/group pair\nfunc NewIdentityMapping(name string) (*IdentityMapping, error) {\n\tusr, err := LookupUser(name)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Could not get user for username %s: %v\", name, err)\n\t}\n\n\tuid := strconv.Itoa(usr.Uid)\n\n\tsubuidRangesWithUserName, err := parseSubuid(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsubgidRangesWithUserName, err := parseSubgid(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsubuidRangesWithUID, err := parseSubuid(uid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsubgidRangesWithUID, err := parseSubgid(uid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsubuidRanges := append(subuidRangesWithUserName, subuidRangesWithUID...)\n\tsubgidRanges := append(subgidRangesWithUserName, subgidRangesWithUID...)\n\n\tif len(subuidRanges) == 0 {\n\t\treturn nil, errors.Errorf(\"no subuid ranges found for user %q\", name)\n\t}\n\tif len(subgidRanges) == 0 {\n\t\treturn nil, errors.Errorf(\"no subgid ranges found for user %q\", name)\n\t}\n\n\treturn &IdentityMapping{\n\t\tuids: createIDMap(subuidRanges),\n\t\tgids: createIDMap(subgidRanges),\n\t}, nil\n}\n", "// Package local provides the default implementation for volumes. It\n// is used to mount data volume containers and directories local to\n// the host server.\npackage local // import \"github.com/docker/docker/volume/local\"\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/docker/docker/daemon/names\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/quota\"\n\t\"github.com/docker/docker/volume\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// VolumeDataPathName is the name of the directory where the volume data is stored.\n// It uses a very distinctive name to avoid collisions migrating data between\n// Docker versions.\nconst (\n\tVolumeDataPathName = \"_data\"\n\tvolumesPathName    = \"volumes\"\n)\n\nvar (\n\t// ErrNotFound is the typed error returned when the requested volume name can't be found\n\tErrNotFound = fmt.Errorf(\"volume not found\")\n\t// volumeNameRegex ensures the name assigned for the volume is valid.\n\t// This name is used to create the bind directory, so we need to avoid characters that\n\t// would make the path to escape the root directory.\n\tvolumeNameRegex = names.RestrictedNamePattern\n)\n\ntype activeMount struct {\n\tcount   uint64\n\tmounted bool\n}\n\n// New instantiates a new Root instance with the provided scope. Scope\n// is the base path that the Root instance uses to store its\n// volumes. The base path is created here if it does not exist.\nfunc New(scope string, rootIdentity idtools.Identity) (*Root, error) {\n\trootDirectory := filepath.Join(scope, volumesPathName)\n\n\tif err := idtools.MkdirAllAndChown(rootDirectory, 0700, rootIdentity); err != nil {\n\t\treturn nil, err\n\t}\n\n\tr := &Root{\n\t\tscope:        scope,\n\t\tpath:         rootDirectory,\n\t\tvolumes:      make(map[string]*localVolume),\n\t\trootIdentity: rootIdentity,\n\t}\n\n\tdirs, err := ioutil.ReadDir(rootDirectory)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif r.quotaCtl, err = quota.NewControl(rootDirectory); err != nil {\n\t\tlogrus.Debugf(\"No quota support for local volumes in %s: %v\", rootDirectory, err)\n\t}\n\n\tfor _, d := range dirs {\n\t\tif !d.IsDir() {\n\t\t\tcontinue\n\t\t}\n\n\t\tname := filepath.Base(d.Name())\n\t\tv := &localVolume{\n\t\t\tdriverName: r.Name(),\n\t\t\tname:       name,\n\t\t\tpath:       r.DataPath(name),\n\t\t\tquotaCtl:   r.quotaCtl,\n\t\t}\n\t\tr.volumes[name] = v\n\t\toptsFilePath := filepath.Join(rootDirectory, name, \"opts.json\")\n\t\tif b, err := ioutil.ReadFile(optsFilePath); err == nil {\n\t\t\topts := optsConfig{}\n\t\t\tif err := json.Unmarshal(b, &opts); err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"error while unmarshaling volume options for volume: %s\", name)\n\t\t\t}\n\t\t\t// Make sure this isn't an empty optsConfig.\n\t\t\t// This could be empty due to buggy behavior in older versions of Docker.\n\t\t\tif !reflect.DeepEqual(opts, optsConfig{}) {\n\t\t\t\tv.opts = &opts\n\t\t\t}\n\t\t\t// unmount anything that may still be mounted (for example, from an\n\t\t\t// unclean shutdown). This is a no-op on windows\n\t\t\tunmount(v.path)\n\t\t}\n\t}\n\n\treturn r, nil\n}\n\n// Root implements the Driver interface for the volume package and\n// manages the creation/removal of volumes. It uses only standard vfs\n// commands to create/remove dirs within its provided scope.\ntype Root struct {\n\tm            sync.Mutex\n\tscope        string\n\tpath         string\n\tquotaCtl     *quota.Control\n\tvolumes      map[string]*localVolume\n\trootIdentity idtools.Identity\n}\n\n// List lists all the volumes\nfunc (r *Root) List() ([]volume.Volume, error) {\n\tvar ls []volume.Volume\n\tr.m.Lock()\n\tfor _, v := range r.volumes {\n\t\tls = append(ls, v)\n\t}\n\tr.m.Unlock()\n\treturn ls, nil\n}\n\n// DataPath returns the constructed path of this volume.\nfunc (r *Root) DataPath(volumeName string) string {\n\treturn filepath.Join(r.path, volumeName, VolumeDataPathName)\n}\n\n// Name returns the name of Root, defined in the volume package in the DefaultDriverName constant.\nfunc (r *Root) Name() string {\n\treturn volume.DefaultDriverName\n}\n\n// Create creates a new volume.Volume with the provided name, creating\n// the underlying directory tree required for this volume in the\n// process.\nfunc (r *Root) Create(name string, opts map[string]string) (volume.Volume, error) {\n\tif err := r.validateName(name); err != nil {\n\t\treturn nil, err\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tv, exists := r.volumes[name]\n\tif exists {\n\t\treturn v, nil\n\t}\n\n\tpath := r.DataPath(name)\n\tif err := idtools.MkdirAllAndChown(path, 0755, r.rootIdentity); err != nil {\n\t\treturn nil, errors.Wrapf(errdefs.System(err), \"error while creating volume path '%s'\", path)\n\t}\n\n\tvar err error\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(filepath.Dir(path))\n\t\t}\n\t}()\n\n\tv = &localVolume{\n\t\tdriverName: r.Name(),\n\t\tname:       name,\n\t\tpath:       path,\n\t\tquotaCtl:   r.quotaCtl,\n\t}\n\n\tif len(opts) != 0 {\n\t\tif err = setOpts(v, opts); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvar b []byte\n\t\tb, err = json.Marshal(v.opts)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err = ioutil.WriteFile(filepath.Join(filepath.Dir(path), \"opts.json\"), b, 0600); err != nil {\n\t\t\treturn nil, errdefs.System(errors.Wrap(err, \"error while persisting volume options\"))\n\t\t}\n\t}\n\n\tr.volumes[name] = v\n\treturn v, nil\n}\n\n// Remove removes the specified volume and all underlying data. If the\n// given volume does not belong to this driver and an error is\n// returned. The volume is reference counted, if all references are\n// not released then the volume is not removed.\nfunc (r *Root) Remove(v volume.Volume) error {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tlv, ok := v.(*localVolume)\n\tif !ok {\n\t\treturn errdefs.System(errors.Errorf(\"unknown volume type %T\", v))\n\t}\n\n\tif lv.active.count > 0 {\n\t\treturn errdefs.System(errors.Errorf(\"volume has active mounts\"))\n\t}\n\n\tif err := lv.unmount(); err != nil {\n\t\treturn err\n\t}\n\n\trealPath, err := filepath.EvalSymlinks(lv.path)\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\trealPath = filepath.Dir(lv.path)\n\t}\n\n\tif !r.scopedPath(realPath) {\n\t\treturn errdefs.System(errors.Errorf(\"Unable to remove a directory outside of the local volume root %s: %s\", r.scope, realPath))\n\t}\n\n\tif err := removePath(realPath); err != nil {\n\t\treturn err\n\t}\n\n\tdelete(r.volumes, lv.name)\n\treturn removePath(filepath.Dir(lv.path))\n}\n\nfunc removePath(path string) error {\n\tif err := os.RemoveAll(path); err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errdefs.System(errors.Wrapf(err, \"error removing volume path '%s'\", path))\n\t}\n\treturn nil\n}\n\n// Get looks up the volume for the given name and returns it if found\nfunc (r *Root) Get(name string) (volume.Volume, error) {\n\tr.m.Lock()\n\tv, exists := r.volumes[name]\n\tr.m.Unlock()\n\tif !exists {\n\t\treturn nil, ErrNotFound\n\t}\n\treturn v, nil\n}\n\n// Scope returns the local volume scope\nfunc (r *Root) Scope() string {\n\treturn volume.LocalScope\n}\n\nfunc (r *Root) validateName(name string) error {\n\tif len(name) == 1 {\n\t\treturn errdefs.InvalidParameter(errors.New(\"volume name is too short, names should be at least two alphanumeric characters\"))\n\t}\n\tif !volumeNameRegex.MatchString(name) {\n\t\treturn errdefs.InvalidParameter(errors.Errorf(\"%q includes invalid characters for a local volume name, only %q are allowed. If you intended to pass a host directory, use absolute path\", name, names.RestrictedNameChars))\n\t}\n\treturn nil\n}\n\n// localVolume implements the Volume interface from the volume package and\n// represents the volumes created by Root.\ntype localVolume struct {\n\tm sync.Mutex\n\t// unique name of the volume\n\tname string\n\t// path is the path on the host where the data lives\n\tpath string\n\t// driverName is the name of the driver that created the volume.\n\tdriverName string\n\t// opts is the parsed list of options used to create the volume\n\topts *optsConfig\n\t// active refcounts the active mounts\n\tactive activeMount\n\t// reference to Root instances quotaCtl\n\tquotaCtl *quota.Control\n}\n\n// Name returns the name of the given Volume.\nfunc (v *localVolume) Name() string {\n\treturn v.name\n}\n\n// DriverName returns the driver that created the given Volume.\nfunc (v *localVolume) DriverName() string {\n\treturn v.driverName\n}\n\n// Path returns the data location.\nfunc (v *localVolume) Path() string {\n\treturn v.path\n}\n\n// CachedPath returns the data location\nfunc (v *localVolume) CachedPath() string {\n\treturn v.path\n}\n\n// Mount implements the localVolume interface, returning the data location.\n// If there are any provided mount options, the resources will be mounted at this point\nfunc (v *localVolume) Mount(id string) (string, error) {\n\tv.m.Lock()\n\tdefer v.m.Unlock()\n\tif v.needsMount() {\n\t\tif !v.active.mounted {\n\t\t\tif err := v.mount(); err != nil {\n\t\t\t\treturn \"\", errdefs.System(err)\n\t\t\t}\n\t\t\tv.active.mounted = true\n\t\t}\n\t\tv.active.count++\n\t}\n\tif err := v.postMount(); err != nil {\n\t\treturn \"\", err\n\t}\n\treturn v.path, nil\n}\n\n// Unmount dereferences the id, and if it is the last reference will unmount any resources\n// that were previously mounted.\nfunc (v *localVolume) Unmount(id string) error {\n\tv.m.Lock()\n\tdefer v.m.Unlock()\n\n\t// Always decrement the count, even if the unmount fails\n\t// Essentially docker doesn't care if this fails, it will send an error, but\n\t// ultimately there's nothing that can be done. If we don't decrement the count\n\t// this volume can never be removed until a daemon restart occurs.\n\tif v.needsMount() {\n\t\tv.active.count--\n\t}\n\n\tif v.active.count > 0 {\n\t\treturn nil\n\t}\n\n\treturn v.unmount()\n}\n\nfunc (v *localVolume) Status() map[string]interface{} {\n\treturn nil\n}\n\n// getAddress finds out address/hostname from options\nfunc getAddress(opts string) string {\n\toptsList := strings.Split(opts, \",\")\n\tfor i := 0; i < len(optsList); i++ {\n\t\tif strings.HasPrefix(optsList[i], \"addr=\") {\n\t\t\taddr := strings.SplitN(optsList[i], \"=\", 2)[1]\n\t\t\treturn addr\n\t\t}\n\t}\n\treturn \"\"\n}\n"], "fixing_code": ["// +build linux freebsd\n\npackage daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/daemon/links\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/stringid\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/runconfig\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nfunc (daemon *Daemon) setupLinkedContainers(container *container.Container) ([]string, error) {\n\tvar env []string\n\tchildren := daemon.children(container)\n\n\tbridgeSettings := container.NetworkSettings.Networks[runconfig.DefaultDaemonNetworkMode().NetworkName()]\n\tif bridgeSettings == nil || bridgeSettings.EndpointSettings == nil {\n\t\treturn nil, nil\n\t}\n\n\tfor linkAlias, child := range children {\n\t\tif !child.IsRunning() {\n\t\t\treturn nil, fmt.Errorf(\"Cannot link to a non running container: %s AS %s\", child.Name, linkAlias)\n\t\t}\n\n\t\tchildBridgeSettings := child.NetworkSettings.Networks[runconfig.DefaultDaemonNetworkMode().NetworkName()]\n\t\tif childBridgeSettings == nil || childBridgeSettings.EndpointSettings == nil {\n\t\t\treturn nil, fmt.Errorf(\"container %s not attached to default bridge network\", child.ID)\n\t\t}\n\n\t\tlink := links.NewLink(\n\t\t\tbridgeSettings.IPAddress,\n\t\t\tchildBridgeSettings.IPAddress,\n\t\t\tlinkAlias,\n\t\t\tchild.Config.Env,\n\t\t\tchild.Config.ExposedPorts,\n\t\t)\n\n\t\tenv = append(env, link.ToEnv()...)\n\t}\n\n\treturn env, nil\n}\n\nfunc (daemon *Daemon) getIpcContainer(id string) (*container.Container, error) {\n\terrMsg := \"can't join IPC of container \" + id\n\t// Check the container exists\n\tctr, err := daemon.GetContainer(id)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, errMsg)\n\t}\n\t// Check the container is running and not restarting\n\tif err := daemon.checkContainer(ctr, containerIsRunning, containerIsNotRestarting); err != nil {\n\t\treturn nil, errors.Wrap(err, errMsg)\n\t}\n\t// Check the container ipc is shareable\n\tif st, err := os.Stat(ctr.ShmPath); err != nil || !st.IsDir() {\n\t\tif err == nil || os.IsNotExist(err) {\n\t\t\treturn nil, errors.New(errMsg + \": non-shareable IPC (hint: use IpcMode:shareable for the donor container)\")\n\t\t}\n\t\t// stat() failed?\n\t\treturn nil, errors.Wrap(err, errMsg+\": unexpected error from stat \"+ctr.ShmPath)\n\t}\n\n\treturn ctr, nil\n}\n\nfunc (daemon *Daemon) getPidContainer(ctr *container.Container) (*container.Container, error) {\n\tcontainerID := ctr.HostConfig.PidMode.Container()\n\tctr, err := daemon.GetContainer(containerID)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"cannot join PID of a non running container: %s\", containerID)\n\t}\n\treturn ctr, daemon.checkContainer(ctr, containerIsRunning, containerIsNotRestarting)\n}\n\nfunc containerIsRunning(c *container.Container) error {\n\tif !c.IsRunning() {\n\t\treturn errdefs.Conflict(errors.Errorf(\"container %s is not running\", c.ID))\n\t}\n\treturn nil\n}\n\nfunc containerIsNotRestarting(c *container.Container) error {\n\tif c.IsRestarting() {\n\t\treturn errContainerIsRestarting(c.ID)\n\t}\n\treturn nil\n}\n\nfunc (daemon *Daemon) setupIpcDirs(c *container.Container) error {\n\tipcMode := c.HostConfig.IpcMode\n\n\tswitch {\n\tcase ipcMode.IsContainer():\n\t\tic, err := daemon.getIpcContainer(ipcMode.Container())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tc.ShmPath = ic.ShmPath\n\n\tcase ipcMode.IsHost():\n\t\tif _, err := os.Stat(\"/dev/shm\"); err != nil {\n\t\t\treturn fmt.Errorf(\"/dev/shm is not mounted, but must be for --ipc=host\")\n\t\t}\n\t\tc.ShmPath = \"/dev/shm\"\n\n\tcase ipcMode.IsPrivate(), ipcMode.IsNone():\n\t\t// c.ShmPath will/should not be used, so make it empty.\n\t\t// Container's /dev/shm mount comes from OCI spec.\n\t\tc.ShmPath = \"\"\n\n\tcase ipcMode.IsEmpty():\n\t\t// A container was created by an older version of the daemon.\n\t\t// The default behavior used to be what is now called \"shareable\".\n\t\tfallthrough\n\n\tcase ipcMode.IsShareable():\n\t\trootIDs := daemon.idMapping.RootPair()\n\t\tif !c.HasMountFor(\"/dev/shm\") {\n\t\t\tshmPath, err := c.ShmResourcePath()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif err := idtools.MkdirAllAndChown(shmPath, 0700, rootIDs); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tshmproperty := \"mode=1777,size=\" + strconv.FormatInt(c.HostConfig.ShmSize, 10)\n\t\t\tif err := unix.Mount(\"shm\", shmPath, \"tmpfs\", uintptr(unix.MS_NOEXEC|unix.MS_NOSUID|unix.MS_NODEV), label.FormatMountLabel(shmproperty, c.GetMountLabel())); err != nil {\n\t\t\t\treturn fmt.Errorf(\"mounting shm tmpfs: %s\", err)\n\t\t\t}\n\t\t\tif err := os.Chown(shmPath, rootIDs.UID, rootIDs.GID); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tc.ShmPath = shmPath\n\t\t}\n\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid IPC mode: %v\", ipcMode)\n\t}\n\n\treturn nil\n}\n\nfunc (daemon *Daemon) setupSecretDir(c *container.Container) (setupErr error) {\n\tif len(c.SecretReferences) == 0 && len(c.ConfigReferences) == 0 {\n\t\treturn nil\n\t}\n\n\tif err := daemon.createSecretsDir(c); err != nil {\n\t\treturn err\n\t}\n\tdefer func() {\n\t\tif setupErr != nil {\n\t\t\tdaemon.cleanupSecretDir(c)\n\t\t}\n\t}()\n\n\tif c.DependencyStore == nil {\n\t\treturn fmt.Errorf(\"secret store is not initialized\")\n\t}\n\n\t// retrieve possible remapped range start for root UID, GID\n\trootIDs := daemon.idMapping.RootPair()\n\n\tfor _, s := range c.SecretReferences {\n\t\t// TODO (ehazlett): use type switch when more are supported\n\t\tif s.File == nil {\n\t\t\tlogrus.Error(\"secret target type is not a file target\")\n\t\t\tcontinue\n\t\t}\n\n\t\t// secrets are created in the SecretMountPath on the host, at a\n\t\t// single level\n\t\tfPath, err := c.SecretFilePath(*s)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"error getting secret file path\")\n\t\t}\n\t\tif err := idtools.MkdirAllAndChown(filepath.Dir(fPath), 0700, rootIDs); err != nil {\n\t\t\treturn errors.Wrap(err, \"error creating secret mount path\")\n\t\t}\n\n\t\tlogrus.WithFields(logrus.Fields{\n\t\t\t\"name\": s.File.Name,\n\t\t\t\"path\": fPath,\n\t\t}).Debug(\"injecting secret\")\n\t\tsecret, err := c.DependencyStore.Secrets().Get(s.SecretID)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"unable to get secret from secret store\")\n\t\t}\n\t\tif err := ioutil.WriteFile(fPath, secret.Spec.Data, s.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error injecting secret\")\n\t\t}\n\n\t\tuid, err := strconv.Atoi(s.File.UID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgid, err := strconv.Atoi(s.File.GID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := os.Chown(fPath, rootIDs.UID+uid, rootIDs.GID+gid); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting ownership for secret\")\n\t\t}\n\t\tif err := os.Chmod(fPath, s.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting file mode for secret\")\n\t\t}\n\t}\n\n\tfor _, configRef := range c.ConfigReferences {\n\t\t// TODO (ehazlett): use type switch when more are supported\n\t\tif configRef.File == nil {\n\t\t\t// Runtime configs are not mounted into the container, but they're\n\t\t\t// a valid type of config so we should not error when we encounter\n\t\t\t// one.\n\t\t\tif configRef.Runtime == nil {\n\t\t\t\tlogrus.Error(\"config target type is not a file or runtime target\")\n\t\t\t}\n\t\t\t// However, in any case, this isn't a file config, so we have no\n\t\t\t// further work to do\n\t\t\tcontinue\n\t\t}\n\n\t\tfPath, err := c.ConfigFilePath(*configRef)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"error getting config file path for container\")\n\t\t}\n\t\tif err := idtools.MkdirAllAndChown(filepath.Dir(fPath), 0700, rootIDs); err != nil {\n\t\t\treturn errors.Wrap(err, \"error creating config mount path\")\n\t\t}\n\n\t\tlogrus.WithFields(logrus.Fields{\n\t\t\t\"name\": configRef.File.Name,\n\t\t\t\"path\": fPath,\n\t\t}).Debug(\"injecting config\")\n\t\tconfig, err := c.DependencyStore.Configs().Get(configRef.ConfigID)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"unable to get config from config store\")\n\t\t}\n\t\tif err := ioutil.WriteFile(fPath, config.Spec.Data, configRef.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error injecting config\")\n\t\t}\n\n\t\tuid, err := strconv.Atoi(configRef.File.UID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgid, err := strconv.Atoi(configRef.File.GID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := os.Chown(fPath, rootIDs.UID+uid, rootIDs.GID+gid); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting ownership for config\")\n\t\t}\n\t\tif err := os.Chmod(fPath, configRef.File.Mode); err != nil {\n\t\t\treturn errors.Wrap(err, \"error setting file mode for config\")\n\t\t}\n\t}\n\n\treturn daemon.remountSecretDir(c)\n}\n\n// createSecretsDir is used to create a dir suitable for storing container secrets.\n// In practice this is using a tmpfs mount and is used for both \"configs\" and \"secrets\"\nfunc (daemon *Daemon) createSecretsDir(c *container.Container) error {\n\t// retrieve possible remapped range start for root UID, GID\n\trootIDs := daemon.idMapping.RootPair()\n\tdir, err := c.SecretMountPath()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"error getting container secrets dir\")\n\t}\n\n\t// create tmpfs\n\tif err := idtools.MkdirAllAndChown(dir, 0700, rootIDs); err != nil {\n\t\treturn errors.Wrap(err, \"error creating secret local mount path\")\n\t}\n\n\ttmpfsOwnership := fmt.Sprintf(\"uid=%d,gid=%d\", rootIDs.UID, rootIDs.GID)\n\tif err := mount.Mount(\"tmpfs\", dir, \"tmpfs\", \"nodev,nosuid,noexec,\"+tmpfsOwnership); err != nil {\n\t\treturn errors.Wrap(err, \"unable to setup secret mount\")\n\t}\n\treturn nil\n}\n\nfunc (daemon *Daemon) remountSecretDir(c *container.Container) error {\n\tdir, err := c.SecretMountPath()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"error getting container secrets path\")\n\t}\n\tif err := label.Relabel(dir, c.MountLabel, false); err != nil {\n\t\tlogrus.WithError(err).WithField(\"dir\", dir).Warn(\"Error while attempting to set selinux label\")\n\t}\n\trootIDs := daemon.idMapping.RootPair()\n\ttmpfsOwnership := fmt.Sprintf(\"uid=%d,gid=%d\", rootIDs.UID, rootIDs.GID)\n\n\t// remount secrets ro\n\tif err := mount.Mount(\"tmpfs\", dir, \"tmpfs\", \"remount,ro,\"+tmpfsOwnership); err != nil {\n\t\treturn errors.Wrap(err, \"unable to remount dir as readonly\")\n\t}\n\n\treturn nil\n}\n\nfunc (daemon *Daemon) cleanupSecretDir(c *container.Container) {\n\tdir, err := c.SecretMountPath()\n\tif err != nil {\n\t\tlogrus.WithError(err).WithField(\"container\", c.ID).Warn(\"error getting secrets mount path for container\")\n\t}\n\tif err := mount.RecursiveUnmount(dir); err != nil {\n\t\tlogrus.WithField(\"dir\", dir).WithError(err).Warn(\"Error while attempting to unmount dir, this may prevent removal of container.\")\n\t}\n\tif err := os.RemoveAll(dir); err != nil {\n\t\tlogrus.WithField(\"dir\", dir).WithError(err).Error(\"Error removing dir.\")\n\t}\n}\n\nfunc killProcessDirectly(cntr *container.Container) error {\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\n\t// Block until the container to stops or timeout.\n\tstatus := <-cntr.Wait(ctx, container.WaitConditionNotRunning)\n\tif status.Err() != nil {\n\t\t// Ensure that we don't kill ourselves\n\t\tif pid := cntr.GetPID(); pid != 0 {\n\t\t\tlogrus.Infof(\"Container %s failed to exit within 10 seconds of kill - trying direct SIGKILL\", stringid.TruncateID(cntr.ID))\n\t\t\tif err := unix.Kill(pid, 9); err != nil {\n\t\t\t\tif err != unix.ESRCH {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\te := errNoSuchProcess{pid, 9}\n\t\t\t\tlogrus.Debug(e)\n\t\t\t\treturn e\n\t\t\t}\n\n\t\t\t// In case there were some exceptions(e.g., state of zombie and D)\n\t\t\tif system.IsProcessAlive(pid) {\n\n\t\t\t\t// Since we can not kill a zombie pid, add zombie check here\n\t\t\t\tisZombie, err := system.IsProcessZombie(pid)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogrus.Warnf(\"Container %s state is invalid\", stringid.TruncateID(cntr.ID))\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif isZombie {\n\t\t\t\t\treturn errdefs.System(errors.Errorf(\"container %s PID %d is zombie and can not be killed. Use the --init option when creating containers to run an init inside the container that forwards signals and reaps processes\", stringid.TruncateID(cntr.ID), pid))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc isLinkable(child *container.Container) bool {\n\t// A container is linkable only if it belongs to the default network\n\t_, ok := child.NetworkSettings.Networks[runconfig.DefaultDaemonNetworkMode().NetworkName()]\n\treturn ok\n}\n\nfunc enableIPOnPredefinedNetwork() bool {\n\treturn false\n}\n\n// serviceDiscoveryOnDefaultNetwork indicates if service discovery is supported on the default network\nfunc serviceDiscoveryOnDefaultNetwork() bool {\n\treturn false\n}\n\nfunc (daemon *Daemon) setupPathsAndSandboxOptions(container *container.Container, sboxOptions *[]libnetwork.SandboxOption) error {\n\tvar err error\n\n\t// Set the correct paths for /etc/hosts and /etc/resolv.conf, based on the\n\t// networking-mode of the container. Note that containers with \"container\"\n\t// networking are already handled in \"initializeNetworking()\" before we reach\n\t// this function, so do not have to be accounted for here.\n\tswitch {\n\tcase container.HostConfig.NetworkMode.IsHost():\n\t\t// In host-mode networking, the container does not have its own networking\n\t\t// namespace, so both `/etc/hosts` and `/etc/resolv.conf` should be the same\n\t\t// as on the host itself. The container gets a copy of these files.\n\t\t*sboxOptions = append(\n\t\t\t*sboxOptions,\n\t\t\tlibnetwork.OptionOriginHostsPath(\"/etc/hosts\"),\n\t\t\tlibnetwork.OptionOriginResolvConfPath(\"/etc/resolv.conf\"),\n\t\t)\n\tcase container.HostConfig.NetworkMode.IsUserDefined():\n\t\t// The container uses a user-defined network. We use the embedded DNS\n\t\t// server for container name resolution and to act as a DNS forwarder\n\t\t// for external DNS resolution.\n\t\t// We parse the DNS server(s) that are defined in /etc/resolv.conf on\n\t\t// the host, which may be a local DNS server (for example, if DNSMasq or\n\t\t// systemd-resolvd are in use). The embedded DNS server forwards DNS\n\t\t// resolution to the DNS server configured on the host, which in itself\n\t\t// may act as a forwarder for external DNS servers.\n\t\t// If systemd-resolvd is used, the \"upstream\" DNS servers can be found in\n\t\t// /run/systemd/resolve/resolv.conf. We do not query those DNS servers\n\t\t// directly, as they can be dynamically reconfigured.\n\t\t*sboxOptions = append(\n\t\t\t*sboxOptions,\n\t\t\tlibnetwork.OptionOriginResolvConfPath(\"/etc/resolv.conf\"),\n\t\t)\n\tdefault:\n\t\t// For other situations, such as the default bridge network, container\n\t\t// discovery / name resolution is handled through /etc/hosts, and no\n\t\t// embedded DNS server is available. Without the embedded DNS, we\n\t\t// cannot use local DNS servers on the host (for example, if DNSMasq or\n\t\t// systemd-resolvd is used). If systemd-resolvd is used, we try to\n\t\t// determine the external DNS servers that are used on the host.\n\t\t// This situation is not ideal, because DNS servers configured in the\n\t\t// container are not updated after the container is created, but the\n\t\t// DNS servers on the host can be dynamically updated.\n\t\t//\n\t\t// Copy the host's resolv.conf for the container (/run/systemd/resolve/resolv.conf or /etc/resolv.conf)\n\t\t*sboxOptions = append(\n\t\t\t*sboxOptions,\n\t\t\tlibnetwork.OptionOriginResolvConfPath(daemon.configStore.GetResolvConf()),\n\t\t)\n\t}\n\n\tcontainer.HostsPath, err = container.GetRootResourcePath(\"hosts\")\n\tif err != nil {\n\t\treturn err\n\t}\n\t*sboxOptions = append(*sboxOptions, libnetwork.OptionHostsPath(container.HostsPath))\n\n\tcontainer.ResolvConfPath, err = container.GetRootResourcePath(\"resolv.conf\")\n\tif err != nil {\n\t\treturn err\n\t}\n\t*sboxOptions = append(*sboxOptions, libnetwork.OptionResolvConfPath(container.ResolvConfPath))\n\treturn nil\n}\n\nfunc (daemon *Daemon) initializeNetworkingPaths(container *container.Container, nc *container.Container) error {\n\tcontainer.HostnamePath = nc.HostnamePath\n\tcontainer.HostsPath = nc.HostsPath\n\tcontainer.ResolvConfPath = nc.ResolvConfPath\n\treturn nil\n}\n\nfunc (daemon *Daemon) setupContainerMountsRoot(c *container.Container) error {\n\t// get the root mount path so we can make it unbindable\n\tp, err := c.MountsResourcePath(\"\")\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn idtools.MkdirAllAndChown(p, 0701, idtools.CurrentIdentity())\n}\n", "package daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"runtime\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/platforms\"\n\t\"github.com/docker/docker/api/types\"\n\tcontainertypes \"github.com/docker/docker/api/types/container\"\n\tnetworktypes \"github.com/docker/docker/api/types/network\"\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/image\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/runconfig\"\n\tv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\n\t\"github.com/opencontainers/selinux/go-selinux\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\ntype createOpts struct {\n\tparams                  types.ContainerCreateConfig\n\tmanaged                 bool\n\tignoreImagesArgsEscaped bool\n}\n\n// CreateManagedContainer creates a container that is managed by a Service\nfunc (daemon *Daemon) CreateManagedContainer(params types.ContainerCreateConfig) (containertypes.ContainerCreateCreatedBody, error) {\n\treturn daemon.containerCreate(createOpts{\n\t\tparams:                  params,\n\t\tmanaged:                 true,\n\t\tignoreImagesArgsEscaped: false})\n}\n\n// ContainerCreate creates a regular container\nfunc (daemon *Daemon) ContainerCreate(params types.ContainerCreateConfig) (containertypes.ContainerCreateCreatedBody, error) {\n\treturn daemon.containerCreate(createOpts{\n\t\tparams:                  params,\n\t\tmanaged:                 false,\n\t\tignoreImagesArgsEscaped: false})\n}\n\n// ContainerCreateIgnoreImagesArgsEscaped creates a regular container. This is called from the builder RUN case\n// and ensures that we do not take the images ArgsEscaped\nfunc (daemon *Daemon) ContainerCreateIgnoreImagesArgsEscaped(params types.ContainerCreateConfig) (containertypes.ContainerCreateCreatedBody, error) {\n\treturn daemon.containerCreate(createOpts{\n\t\tparams:                  params,\n\t\tmanaged:                 false,\n\t\tignoreImagesArgsEscaped: true})\n}\n\nfunc (daemon *Daemon) containerCreate(opts createOpts) (containertypes.ContainerCreateCreatedBody, error) {\n\tstart := time.Now()\n\tif opts.params.Config == nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{}, errdefs.InvalidParameter(errors.New(\"Config cannot be empty in order to create a container\"))\n\t}\n\n\tos := runtime.GOOS\n\tvar img *image.Image\n\tif opts.params.Config.Image != \"\" {\n\t\tvar err error\n\t\timg, err = daemon.imageService.GetImage(opts.params.Config.Image, opts.params.Platform)\n\t\tif err == nil {\n\t\t\tos = img.OS\n\t\t}\n\t} else {\n\t\t// This mean scratch. On Windows, we can safely assume that this is a linux\n\t\t// container. On other platforms, it's the host OS (which it already is)\n\t\tif isWindows && system.LCOWSupported() {\n\t\t\tos = \"linux\"\n\t\t}\n\t}\n\n\twarnings, err := daemon.verifyContainerSettings(os, opts.params.HostConfig, opts.params.Config, false)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err)\n\t}\n\n\tif img != nil && opts.params.Platform == nil {\n\t\tp := platforms.DefaultSpec()\n\t\timgPlat := v1.Platform{\n\t\t\tOS:           img.OS,\n\t\t\tArchitecture: img.Architecture,\n\t\t\tVariant:      img.Variant,\n\t\t}\n\n\t\tif !platforms.Only(p).Match(imgPlat) {\n\t\t\twarnings = append(warnings, fmt.Sprintf(\"The requested image's platform (%s) does not match the detected host platform (%s) and no specific platform was requested\", platforms.Format(imgPlat), platforms.Format(p)))\n\t\t}\n\t}\n\n\terr = verifyNetworkingConfig(opts.params.NetworkingConfig)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err)\n\t}\n\n\tif opts.params.HostConfig == nil {\n\t\topts.params.HostConfig = &containertypes.HostConfig{}\n\t}\n\terr = daemon.adaptContainerSettings(opts.params.HostConfig, opts.params.AdjustCPUShares)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, errdefs.InvalidParameter(err)\n\t}\n\n\tctr, err := daemon.create(opts)\n\tif err != nil {\n\t\treturn containertypes.ContainerCreateCreatedBody{Warnings: warnings}, err\n\t}\n\tcontainerActions.WithValues(\"create\").UpdateSince(start)\n\n\tif warnings == nil {\n\t\twarnings = make([]string, 0) // Create an empty slice to avoid https://github.com/moby/moby/issues/38222\n\t}\n\n\treturn containertypes.ContainerCreateCreatedBody{ID: ctr.ID, Warnings: warnings}, nil\n}\n\n// Create creates a new container from the given configuration with a given name.\nfunc (daemon *Daemon) create(opts createOpts) (retC *container.Container, retErr error) {\n\tvar (\n\t\tctr   *container.Container\n\t\timg   *image.Image\n\t\timgID image.ID\n\t\terr   error\n\t)\n\n\tos := runtime.GOOS\n\tif opts.params.Config.Image != \"\" {\n\t\timg, err = daemon.imageService.GetImage(opts.params.Config.Image, opts.params.Platform)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif img.OS != \"\" {\n\t\t\tos = img.OS\n\t\t} else {\n\t\t\t// default to the host OS except on Windows with LCOW\n\t\t\tif isWindows && system.LCOWSupported() {\n\t\t\t\tos = \"linux\"\n\t\t\t}\n\t\t}\n\t\timgID = img.ID()\n\n\t\tif isWindows && img.OS == \"linux\" && !system.LCOWSupported() {\n\t\t\treturn nil, errors.New(\"operating system on which parent image was created is not Windows\")\n\t\t}\n\t} else {\n\t\tif isWindows {\n\t\t\tos = \"linux\" // 'scratch' case.\n\t\t}\n\t}\n\n\t// On WCOW, if are not being invoked by the builder to create this container (where\n\t// ignoreImagesArgEscaped will be true) - if the image already has its arguments escaped,\n\t// ensure that this is replicated across to the created container to avoid double-escaping\n\t// of the arguments/command line when the runtime attempts to run the container.\n\tif os == \"windows\" && !opts.ignoreImagesArgsEscaped && img != nil && img.RunConfig().ArgsEscaped {\n\t\topts.params.Config.ArgsEscaped = true\n\t}\n\n\tif err := daemon.mergeAndVerifyConfig(opts.params.Config, img); err != nil {\n\t\treturn nil, errdefs.InvalidParameter(err)\n\t}\n\n\tif err := daemon.mergeAndVerifyLogConfig(&opts.params.HostConfig.LogConfig); err != nil {\n\t\treturn nil, errdefs.InvalidParameter(err)\n\t}\n\n\tif ctr, err = daemon.newContainer(opts.params.Name, os, opts.params.Config, opts.params.HostConfig, imgID, opts.managed); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif err := daemon.cleanupContainer(ctr, true, true); err != nil {\n\t\t\t\tlogrus.Errorf(\"failed to cleanup container on create error: %v\", err)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err := daemon.setSecurityOptions(ctr, opts.params.HostConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tctr.HostConfig.StorageOpt = opts.params.HostConfig.StorageOpt\n\n\t// Set RWLayer for container after mount labels have been set\n\trwLayer, err := daemon.imageService.CreateLayer(ctr, setupInitLayer(daemon.idMapping))\n\tif err != nil {\n\t\treturn nil, errdefs.System(err)\n\t}\n\tctr.RWLayer = rwLayer\n\n\tif err := idtools.MkdirAndChown(ctr.Root, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(ctr.CheckpointDir(), 0700, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := daemon.setHostConfig(ctr, opts.params.HostConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := daemon.createContainerOSSpecificSettings(ctr, opts.params.Config, opts.params.HostConfig); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar endpointsConfigs map[string]*networktypes.EndpointSettings\n\tif opts.params.NetworkingConfig != nil {\n\t\tendpointsConfigs = opts.params.NetworkingConfig.EndpointsConfig\n\t}\n\t// Make sure NetworkMode has an acceptable value. We do this to ensure\n\t// backwards API compatibility.\n\trunconfig.SetDefaultNetModeIfBlank(ctr.HostConfig)\n\n\tdaemon.updateContainerNetworkSettings(ctr, endpointsConfigs)\n\tif err := daemon.Register(ctr); err != nil {\n\t\treturn nil, err\n\t}\n\tstateCtr.set(ctr.ID, \"stopped\")\n\tdaemon.LogContainerEvent(ctr, \"create\")\n\treturn ctr, nil\n}\n\nfunc toHostConfigSelinuxLabels(labels []string) []string {\n\tfor i, l := range labels {\n\t\tlabels[i] = \"label=\" + l\n\t}\n\treturn labels\n}\n\nfunc (daemon *Daemon) generateSecurityOpt(hostConfig *containertypes.HostConfig) ([]string, error) {\n\tfor _, opt := range hostConfig.SecurityOpt {\n\t\tcon := strings.Split(opt, \"=\")\n\t\tif con[0] == \"label\" {\n\t\t\t// Caller overrode SecurityOpts\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\tipcMode := hostConfig.IpcMode\n\tpidMode := hostConfig.PidMode\n\tprivileged := hostConfig.Privileged\n\tif ipcMode.IsHost() || pidMode.IsHost() || privileged {\n\t\treturn toHostConfigSelinuxLabels(selinux.DisableSecOpt()), nil\n\t}\n\n\tvar ipcLabel []string\n\tvar pidLabel []string\n\tipcContainer := ipcMode.Container()\n\tpidContainer := pidMode.Container()\n\tif ipcContainer != \"\" {\n\t\tc, err := daemon.GetContainer(ipcContainer)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tipcLabel, err = selinux.DupSecOpt(c.ProcessLabel)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif pidContainer == \"\" {\n\t\t\treturn toHostConfigSelinuxLabels(ipcLabel), err\n\t\t}\n\t}\n\tif pidContainer != \"\" {\n\t\tc, err := daemon.GetContainer(pidContainer)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tpidLabel, err = selinux.DupSecOpt(c.ProcessLabel)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif ipcContainer == \"\" {\n\t\t\treturn toHostConfigSelinuxLabels(pidLabel), err\n\t\t}\n\t}\n\n\tif pidLabel != nil && ipcLabel != nil {\n\t\tfor i := 0; i < len(pidLabel); i++ {\n\t\t\tif pidLabel[i] != ipcLabel[i] {\n\t\t\t\treturn nil, fmt.Errorf(\"--ipc and --pid containers SELinux labels aren't the same\")\n\t\t\t}\n\t\t}\n\t\treturn toHostConfigSelinuxLabels(pidLabel), nil\n\t}\n\treturn nil, nil\n}\n\nfunc (daemon *Daemon) mergeAndVerifyConfig(config *containertypes.Config, img *image.Image) error {\n\tif img != nil && img.Config != nil {\n\t\tif err := merge(config, img.Config); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// Reset the Entrypoint if it is [\"\"]\n\tif len(config.Entrypoint) == 1 && config.Entrypoint[0] == \"\" {\n\t\tconfig.Entrypoint = nil\n\t}\n\tif len(config.Entrypoint) == 0 && len(config.Cmd) == 0 {\n\t\treturn fmt.Errorf(\"No command specified\")\n\t}\n\treturn nil\n}\n\n// Checks if the client set configurations for more than one network while creating a container\n// Also checks if the IPAMConfig is valid\nfunc verifyNetworkingConfig(nwConfig *networktypes.NetworkingConfig) error {\n\tif nwConfig == nil || len(nwConfig.EndpointsConfig) == 0 {\n\t\treturn nil\n\t}\n\tif len(nwConfig.EndpointsConfig) > 1 {\n\t\tl := make([]string, 0, len(nwConfig.EndpointsConfig))\n\t\tfor k := range nwConfig.EndpointsConfig {\n\t\t\tl = append(l, k)\n\t\t}\n\t\treturn errors.Errorf(\"Container cannot be connected to network endpoints: %s\", strings.Join(l, \", \"))\n\t}\n\n\tfor k, v := range nwConfig.EndpointsConfig {\n\t\tif v == nil {\n\t\t\treturn errdefs.InvalidParameter(errors.Errorf(\"no EndpointSettings for %s\", k))\n\t\t}\n\t\tif v.IPAMConfig != nil {\n\t\t\tif v.IPAMConfig.IPv4Address != \"\" && net.ParseIP(v.IPAMConfig.IPv4Address).To4() == nil {\n\t\t\t\treturn errors.Errorf(\"invalid IPv4 address: %s\", v.IPAMConfig.IPv4Address)\n\t\t\t}\n\t\t\tif v.IPAMConfig.IPv6Address != \"\" {\n\t\t\t\tn := net.ParseIP(v.IPAMConfig.IPv6Address)\n\t\t\t\t// if the address is an invalid network address (ParseIP == nil) or if it is\n\t\t\t\t// an IPv4 address (To4() != nil), then it is an invalid IPv6 address\n\t\t\t\tif n == nil || n.To4() != nil {\n\t\t\t\t\treturn errors.Errorf(\"invalid IPv6 address: %s\", v.IPAMConfig.IPv6Address)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n", "// Package daemon exposes the functions that occur on the host server\n// that the Docker daemon is running.\n//\n// In implementing the various functions of the daemon, there is often\n// a method-specific struct for configuring the runtime behavior.\npackage daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/docker/pkg/fileutils\"\n\t\"go.etcd.io/bbolt\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/backoff\"\n\n\t\"github.com/containerd/containerd\"\n\t\"github.com/containerd/containerd/defaults\"\n\t\"github.com/containerd/containerd/pkg/dialer\"\n\t\"github.com/containerd/containerd/remotes/docker\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/api/types\"\n\tcontainertypes \"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/swarm\"\n\t\"github.com/docker/docker/builder\"\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/daemon/config\"\n\t\"github.com/docker/docker/daemon/discovery\"\n\t\"github.com/docker/docker/daemon/events\"\n\t\"github.com/docker/docker/daemon/exec\"\n\t\"github.com/docker/docker/daemon/images\"\n\t\"github.com/docker/docker/daemon/logger\"\n\t\"github.com/docker/docker/daemon/network\"\n\t\"github.com/docker/docker/errdefs\"\n\tbkconfig \"github.com/moby/buildkit/cmd/buildkitd/config\"\n\t\"github.com/moby/buildkit/util/resolver\"\n\t\"github.com/sirupsen/logrus\"\n\n\t// register graph drivers\n\t_ \"github.com/docker/docker/daemon/graphdriver/register\"\n\t\"github.com/docker/docker/daemon/stats\"\n\tdmetadata \"github.com/docker/docker/distribution/metadata\"\n\t\"github.com/docker/docker/dockerversion\"\n\t\"github.com/docker/docker/image\"\n\t\"github.com/docker/docker/layer\"\n\t\"github.com/docker/docker/libcontainerd\"\n\tlibcontainerdtypes \"github.com/docker/docker/libcontainerd/types\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/plugingetter\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/pkg/truncindex\"\n\t\"github.com/docker/docker/plugin\"\n\tpluginexec \"github.com/docker/docker/plugin/executor/containerd\"\n\trefstore \"github.com/docker/docker/reference\"\n\t\"github.com/docker/docker/registry\"\n\t\"github.com/docker/docker/runconfig\"\n\tvolumesservice \"github.com/docker/docker/volume/service\"\n\t\"github.com/docker/libnetwork\"\n\t\"github.com/docker/libnetwork/cluster\"\n\tnwconfig \"github.com/docker/libnetwork/config\"\n\t\"github.com/moby/locker\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/sync/semaphore\"\n)\n\n// ContainersNamespace is the name of the namespace used for users containers\nconst (\n\tContainersNamespace = \"moby\"\n)\n\nvar (\n\terrSystemNotSupported = errors.New(\"the Docker daemon is not supported on this platform\")\n)\n\n// Daemon holds information about the Docker daemon.\ntype Daemon struct {\n\tID                string\n\trepository        string\n\tcontainers        container.Store\n\tcontainersReplica container.ViewDB\n\texecCommands      *exec.Store\n\timageService      *images.ImageService\n\tidIndex           *truncindex.TruncIndex\n\tconfigStore       *config.Config\n\tstatsCollector    *stats.Collector\n\tdefaultLogConfig  containertypes.LogConfig\n\tRegistryService   registry.Service\n\tEventsService     *events.Events\n\tnetController     libnetwork.NetworkController\n\tvolumes           *volumesservice.VolumesService\n\tdiscoveryWatcher  discovery.Reloader\n\troot              string\n\tseccompEnabled    bool\n\tapparmorEnabled   bool\n\tshutdown          bool\n\tidMapping         *idtools.IdentityMapping\n\t// TODO: move graphDrivers field to an InfoService\n\tgraphDrivers map[string]string // By operating system\n\n\tPluginStore           *plugin.Store // todo: remove\n\tpluginManager         *plugin.Manager\n\tlinkIndex             *linkIndex\n\tcontainerdCli         *containerd.Client\n\tcontainerd            libcontainerdtypes.Client\n\tdefaultIsolation      containertypes.Isolation // Default isolation mode on Windows\n\tclusterProvider       cluster.Provider\n\tcluster               Cluster\n\tgenericResources      []swarm.GenericResource\n\tmetricsPluginListener net.Listener\n\n\tmachineMemory uint64\n\n\tseccompProfile     []byte\n\tseccompProfilePath string\n\n\tdiskUsageRunning int32\n\tpruneRunning     int32\n\thosts            map[string]bool // hosts stores the addresses the daemon is listening on\n\tstartupDone      chan struct{}\n\n\tattachmentStore       network.AttachmentStore\n\tattachableNetworkLock *locker.Locker\n\n\t// This is used for Windows which doesn't currently support running on containerd\n\t// It stores metadata for the content store (used for manifest caching)\n\t// This needs to be closed on daemon exit\n\tmdDB *bbolt.DB\n}\n\n// StoreHosts stores the addresses the daemon is listening on\nfunc (daemon *Daemon) StoreHosts(hosts []string) {\n\tif daemon.hosts == nil {\n\t\tdaemon.hosts = make(map[string]bool)\n\t}\n\tfor _, h := range hosts {\n\t\tdaemon.hosts[h] = true\n\t}\n}\n\n// HasExperimental returns whether the experimental features of the daemon are enabled or not\nfunc (daemon *Daemon) HasExperimental() bool {\n\treturn daemon.configStore != nil && daemon.configStore.Experimental\n}\n\n// Features returns the features map from configStore\nfunc (daemon *Daemon) Features() *map[string]bool {\n\treturn &daemon.configStore.Features\n}\n\n// RegistryHosts returns registry configuration in containerd resolvers format\nfunc (daemon *Daemon) RegistryHosts() docker.RegistryHosts {\n\tvar (\n\t\tregistryKey = \"docker.io\"\n\t\tmirrors     = make([]string, len(daemon.configStore.Mirrors))\n\t\tm           = map[string]bkconfig.RegistryConfig{}\n\t)\n\t// must trim \"https://\" or \"http://\" prefix\n\tfor i, v := range daemon.configStore.Mirrors {\n\t\tif uri, err := url.Parse(v); err == nil {\n\t\t\tv = uri.Host\n\t\t}\n\t\tmirrors[i] = v\n\t}\n\t// set mirrors for default registry\n\tm[registryKey] = bkconfig.RegistryConfig{Mirrors: mirrors}\n\n\tfor _, v := range daemon.configStore.InsecureRegistries {\n\t\tu, err := url.Parse(v)\n\t\tc := bkconfig.RegistryConfig{}\n\t\tif err == nil {\n\t\t\tv = u.Host\n\t\t\tt := true\n\t\t\tif u.Scheme == \"http\" {\n\t\t\t\tc.PlainHTTP = &t\n\t\t\t} else {\n\t\t\t\tc.Insecure = &t\n\t\t\t}\n\t\t}\n\t\tm[v] = c\n\t}\n\n\tfor k, v := range m {\n\t\tif d, err := registry.HostCertsDir(k); err == nil {\n\t\t\tv.TLSConfigDir = []string{d}\n\t\t\tm[k] = v\n\t\t}\n\t}\n\n\tcertsDir := registry.CertsDir()\n\tif fis, err := ioutil.ReadDir(certsDir); err == nil {\n\t\tfor _, fi := range fis {\n\t\t\tif _, ok := m[fi.Name()]; !ok {\n\t\t\t\tm[fi.Name()] = bkconfig.RegistryConfig{\n\t\t\t\t\tTLSConfigDir: []string{filepath.Join(certsDir, fi.Name())},\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn resolver.NewRegistryConfig(m)\n}\n\nfunc (daemon *Daemon) restore() error {\n\tvar mapLock sync.Mutex\n\tcontainers := make(map[string]*container.Container)\n\n\tlogrus.Info(\"Loading containers: start.\")\n\n\tdir, err := ioutil.ReadDir(daemon.repository)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// parallelLimit is the maximum number of parallel startup jobs that we\n\t// allow (this is the limited used for all startup semaphores). The multipler\n\t// (128) was chosen after some fairly significant benchmarking -- don't change\n\t// it unless you've tested it significantly (this value is adjusted if\n\t// RLIMIT_NOFILE is small to avoid EMFILE).\n\tparallelLimit := adjustParallelLimit(len(dir), 128*runtime.NumCPU())\n\n\t// Re-used for all parallel startup jobs.\n\tvar group sync.WaitGroup\n\tsem := semaphore.NewWeighted(int64(parallelLimit))\n\n\tfor _, v := range dir {\n\t\tgroup.Add(1)\n\t\tgo func(id string) {\n\t\t\tdefer group.Done()\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\t\t\tdefer sem.Release(1)\n\n\t\t\tlog := logrus.WithField(\"container\", id)\n\n\t\t\tc, err := daemon.load(id)\n\t\t\tif err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to load container\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !system.IsOSSupported(c.OS) {\n\t\t\t\tlog.Errorf(\"failed to load container: %s (%q)\", system.ErrNotSupportedOperatingSystem, c.OS)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Ignore the container if it does not support the current driver being used by the graph\n\t\t\tcurrentDriverForContainerOS := daemon.graphDrivers[c.OS]\n\t\t\tif (c.Driver == \"\" && currentDriverForContainerOS == \"aufs\") || c.Driver == currentDriverForContainerOS {\n\t\t\t\trwlayer, err := daemon.imageService.GetLayerByID(c.ID, c.OS)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.WithError(err).Error(\"failed to load container mount\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tc.RWLayer = rwlayer\n\t\t\t\tlog.WithFields(logrus.Fields{\n\t\t\t\t\t\"running\": c.IsRunning(),\n\t\t\t\t\t\"paused\":  c.IsPaused(),\n\t\t\t\t}).Debug(\"loaded container\")\n\n\t\t\t\tmapLock.Lock()\n\t\t\t\tcontainers[c.ID] = c\n\t\t\t\tmapLock.Unlock()\n\t\t\t} else {\n\t\t\t\tlog.Debugf(\"cannot load container because it was created with another storage driver\")\n\t\t\t}\n\t\t}(v.Name())\n\t}\n\tgroup.Wait()\n\n\tremoveContainers := make(map[string]*container.Container)\n\trestartContainers := make(map[*container.Container]chan struct{})\n\tactiveSandboxes := make(map[string]interface{})\n\n\tfor _, c := range containers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\tdefer group.Done()\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\t\t\tdefer sem.Release(1)\n\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\n\t\t\tif err := daemon.registerName(c); err != nil {\n\t\t\t\tlog.WithError(err).Errorf(\"failed to register container name: %s\", c.Name)\n\t\t\t\tmapLock.Lock()\n\t\t\t\tdelete(containers, c.ID)\n\t\t\t\tmapLock.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := daemon.Register(c); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to register container\")\n\t\t\t\tmapLock.Lock()\n\t\t\t\tdelete(containers, c.ID)\n\t\t\t\tmapLock.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// The LogConfig.Type is empty if the container was created before docker 1.12 with default log driver.\n\t\t\t// We should rewrite it to use the daemon defaults.\n\t\t\t// Fixes https://github.com/docker/docker/issues/22536\n\t\t\tif c.HostConfig.LogConfig.Type == \"\" {\n\t\t\t\tif err := daemon.mergeAndVerifyLogConfig(&c.HostConfig.LogConfig); err != nil {\n\t\t\t\t\tlog.WithError(err).Error(\"failed to verify log config for container\")\n\t\t\t\t}\n\t\t\t}\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tfor _, c := range containers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\tdefer group.Done()\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\t\t\tdefer sem.Release(1)\n\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\n\t\t\tdaemon.backportMountSpec(c)\n\t\t\tif err := daemon.checkpointAndSave(c); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"error saving backported mountspec to disk\")\n\t\t\t}\n\n\t\t\tdaemon.setStateCounter(c)\n\n\t\t\tlogger := func(c *container.Container) *logrus.Entry {\n\t\t\t\treturn log.WithFields(logrus.Fields{\n\t\t\t\t\t\"running\":    c.IsRunning(),\n\t\t\t\t\t\"paused\":     c.IsPaused(),\n\t\t\t\t\t\"restarting\": c.IsRestarting(),\n\t\t\t\t})\n\t\t\t}\n\n\t\t\tlogger(c).Debug(\"restoring container\")\n\n\t\t\tvar (\n\t\t\t\terr      error\n\t\t\t\talive    bool\n\t\t\t\tec       uint32\n\t\t\t\texitedAt time.Time\n\t\t\t\tprocess  libcontainerdtypes.Process\n\t\t\t)\n\n\t\t\talive, _, process, err = daemon.containerd.Restore(context.Background(), c.ID, c.InitializeStdio)\n\t\t\tif err != nil && !errdefs.IsNotFound(err) {\n\t\t\t\tlogger(c).WithError(err).Error(\"failed to restore container with containerd\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tlogger(c).Debugf(\"alive: %v\", alive)\n\t\t\tif !alive {\n\t\t\t\t// If process is not nil, cleanup dead container from containerd.\n\t\t\t\t// If process is nil then the above `containerd.Restore` returned an errdefs.NotFoundError,\n\t\t\t\t// and docker's view of the container state will be updated accorrdingly via SetStopped further down.\n\t\t\t\tif process != nil {\n\t\t\t\t\tlogger(c).Debug(\"cleaning up dead container process\")\n\t\t\t\t\tec, exitedAt, err = process.Delete(context.Background())\n\t\t\t\t\tif err != nil && !errdefs.IsNotFound(err) {\n\t\t\t\t\t\tlogger(c).WithError(err).Error(\"failed to delete container from containerd\")\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if !daemon.configStore.LiveRestoreEnabled {\n\t\t\t\tlogger(c).Debug(\"shutting down container considered alive by containerd\")\n\t\t\t\tif err := daemon.shutdownContainer(c); err != nil && !errdefs.IsNotFound(err) {\n\t\t\t\t\tlog.WithError(err).Error(\"error shutting down container\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tc.ResetRestartManager(false)\n\t\t\t}\n\n\t\t\tif c.IsRunning() || c.IsPaused() {\n\t\t\t\tlogger(c).Debug(\"syncing container on disk state with real state\")\n\n\t\t\t\tc.RestartManager().Cancel() // manually start containers because some need to wait for swarm networking\n\n\t\t\t\tif c.IsPaused() && alive {\n\t\t\t\t\ts, err := daemon.containerd.Status(context.Background(), c.ID)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tlogger(c).WithError(err).Error(\"failed to get container status\")\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogger(c).WithField(\"state\", s).Info(\"restored container paused\")\n\t\t\t\t\t\tswitch s {\n\t\t\t\t\t\tcase containerd.Paused, containerd.Pausing:\n\t\t\t\t\t\t\t// nothing to do\n\t\t\t\t\t\tcase containerd.Stopped:\n\t\t\t\t\t\t\talive = false\n\t\t\t\t\t\tcase containerd.Unknown:\n\t\t\t\t\t\t\tlog.Error(\"unknown status for paused container during restore\")\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t// running\n\t\t\t\t\t\t\tc.Lock()\n\t\t\t\t\t\t\tc.Paused = false\n\t\t\t\t\t\t\tdaemon.setStateCounter(c)\n\t\t\t\t\t\t\tif err := c.CheckpointTo(daemon.containersReplica); err != nil {\n\t\t\t\t\t\t\t\tlog.WithError(err).Error(\"failed to update paused container state\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tc.Unlock()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif !alive {\n\t\t\t\t\tlogger(c).Debug(\"setting stopped state\")\n\t\t\t\t\tc.Lock()\n\t\t\t\t\tc.SetStopped(&container.ExitStatus{ExitCode: int(ec), ExitedAt: exitedAt})\n\t\t\t\t\tdaemon.Cleanup(c)\n\t\t\t\t\tif err := c.CheckpointTo(daemon.containersReplica); err != nil {\n\t\t\t\t\t\tlog.WithError(err).Error(\"failed to update stopped container state\")\n\t\t\t\t\t}\n\t\t\t\t\tc.Unlock()\n\t\t\t\t\tlogger(c).Debug(\"set stopped state\")\n\t\t\t\t}\n\n\t\t\t\t// we call Mount and then Unmount to get BaseFs of the container\n\t\t\t\tif err := daemon.Mount(c); err != nil {\n\t\t\t\t\t// The mount is unlikely to fail. However, in case mount fails\n\t\t\t\t\t// the container should be allowed to restore here. Some functionalities\n\t\t\t\t\t// (like docker exec -u user) might be missing but container is able to be\n\t\t\t\t\t// stopped/restarted/removed.\n\t\t\t\t\t// See #29365 for related information.\n\t\t\t\t\t// The error is only logged here.\n\t\t\t\t\tlogger(c).WithError(err).Warn(\"failed to mount container to get BaseFs path\")\n\t\t\t\t} else {\n\t\t\t\t\tif err := daemon.Unmount(c); err != nil {\n\t\t\t\t\t\tlogger(c).WithError(err).Warn(\"failed to umount container to get BaseFs path\")\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tc.ResetRestartManager(false)\n\t\t\t\tif !c.HostConfig.NetworkMode.IsContainer() && c.IsRunning() {\n\t\t\t\t\toptions, err := daemon.buildSandboxOptions(c)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tlogger(c).WithError(err).Warn(\"failed to build sandbox option to restore container\")\n\t\t\t\t\t}\n\t\t\t\t\tmapLock.Lock()\n\t\t\t\t\tactiveSandboxes[c.NetworkSettings.SandboxID] = options\n\t\t\t\t\tmapLock.Unlock()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// get list of containers we need to restart\n\n\t\t\t// Do not autostart containers which\n\t\t\t// has endpoints in a swarm scope\n\t\t\t// network yet since the cluster is\n\t\t\t// not initialized yet. We will start\n\t\t\t// it after the cluster is\n\t\t\t// initialized.\n\t\t\tif daemon.configStore.AutoRestart && c.ShouldRestart() && !c.NetworkSettings.HasSwarmEndpoint && c.HasBeenStartedBefore {\n\t\t\t\tmapLock.Lock()\n\t\t\t\trestartContainers[c] = make(chan struct{})\n\t\t\t\tmapLock.Unlock()\n\t\t\t} else if c.HostConfig != nil && c.HostConfig.AutoRemove {\n\t\t\t\tmapLock.Lock()\n\t\t\t\tremoveContainers[c.ID] = c\n\t\t\t\tmapLock.Unlock()\n\t\t\t}\n\n\t\t\tc.Lock()\n\t\t\tif c.RemovalInProgress {\n\t\t\t\t// We probably crashed in the middle of a removal, reset\n\t\t\t\t// the flag.\n\t\t\t\t//\n\t\t\t\t// We DO NOT remove the container here as we do not\n\t\t\t\t// know if the user had requested for either the\n\t\t\t\t// associated volumes, network links or both to also\n\t\t\t\t// be removed. So we put the container in the \"dead\"\n\t\t\t\t// state and leave further processing up to them.\n\t\t\t\tc.RemovalInProgress = false\n\t\t\t\tc.Dead = true\n\t\t\t\tif err := c.CheckpointTo(daemon.containersReplica); err != nil {\n\t\t\t\t\tlog.WithError(err).Error(\"failed to update RemovalInProgress container state\")\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debugf(\"reset RemovalInProgress state for container\")\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.Unlock()\n\t\t\tlogger(c).Debug(\"done restoring container\")\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tdaemon.netController, err = daemon.initNetworkController(daemon.configStore, activeSandboxes)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error initializing network controller: %v\", err)\n\t}\n\n\t// Now that all the containers are registered, register the links\n\tfor _, c := range containers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tif err := daemon.registerLinks(c, c.HostConfig); err != nil {\n\t\t\t\tlogrus.WithField(\"container\", c.ID).WithError(err).Error(\"failed to register link for container\")\n\t\t\t}\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tfor c, notifier := range restartContainers {\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container, chNotify chan struct{}) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\n\t\t\tlog.Debug(\"starting container\")\n\n\t\t\t// ignore errors here as this is a best effort to wait for children to be\n\t\t\t//   running before we try to start the container\n\t\t\tchildren := daemon.children(c)\n\t\t\ttimeout := time.NewTimer(5 * time.Second)\n\t\t\tdefer timeout.Stop()\n\n\t\t\tfor _, child := range children {\n\t\t\t\tif notifier, exists := restartContainers[child]; exists {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-notifier:\n\t\t\t\t\tcase <-timeout.C:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Make sure networks are available before starting\n\t\t\tdaemon.waitForNetworks(c)\n\t\t\tif err := daemon.containerStart(c, \"\", \"\", true); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to start container\")\n\t\t\t}\n\t\t\tclose(chNotify)\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(c, notifier)\n\t}\n\tgroup.Wait()\n\n\tfor id := range removeContainers {\n\t\tgroup.Add(1)\n\t\tgo func(cid string) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tif err := daemon.ContainerRm(cid, &types.ContainerRmConfig{ForceRemove: true, RemoveVolume: true}); err != nil {\n\t\t\t\tlogrus.WithField(\"container\", cid).WithError(err).Error(\"failed to remove container\")\n\t\t\t}\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(id)\n\t}\n\tgroup.Wait()\n\n\t// any containers that were started above would already have had this done,\n\t// however we need to now prepare the mountpoints for the rest of the containers as well.\n\t// This shouldn't cause any issue running on the containers that already had this run.\n\t// This must be run after any containers with a restart policy so that containerized plugins\n\t// can have a chance to be running before we try to initialize them.\n\tfor _, c := range containers {\n\t\t// if the container has restart policy, do not\n\t\t// prepare the mountpoints since it has been done on restarting.\n\t\t// This is to speed up the daemon start when a restart container\n\t\t// has a volume and the volume driver is not available.\n\t\tif _, ok := restartContainers[c]; ok {\n\t\t\tcontinue\n\t\t} else if _, ok := removeContainers[c.ID]; ok {\n\t\t\t// container is automatically removed, skip it.\n\t\t\tcontinue\n\t\t}\n\n\t\tgroup.Add(1)\n\t\tgo func(c *container.Container) {\n\t\t\t_ = sem.Acquire(context.Background(), 1)\n\n\t\t\tif err := daemon.prepareMountPoints(c); err != nil {\n\t\t\t\tlogrus.WithField(\"container\", c.ID).WithError(err).Error(\"failed to prepare mountpoints for container\")\n\t\t\t}\n\n\t\t\tsem.Release(1)\n\t\t\tgroup.Done()\n\t\t}(c)\n\t}\n\tgroup.Wait()\n\n\tlogrus.Info(\"Loading containers: done.\")\n\n\treturn nil\n}\n\n// RestartSwarmContainers restarts any autostart container which has a\n// swarm endpoint.\nfunc (daemon *Daemon) RestartSwarmContainers() {\n\tctx := context.Background()\n\n\t// parallelLimit is the maximum number of parallel startup jobs that we\n\t// allow (this is the limited used for all startup semaphores). The multipler\n\t// (128) was chosen after some fairly significant benchmarking -- don't change\n\t// it unless you've tested it significantly (this value is adjusted if\n\t// RLIMIT_NOFILE is small to avoid EMFILE).\n\tparallelLimit := adjustParallelLimit(len(daemon.List()), 128*runtime.NumCPU())\n\n\tvar group sync.WaitGroup\n\tsem := semaphore.NewWeighted(int64(parallelLimit))\n\n\tfor _, c := range daemon.List() {\n\t\tif !c.IsRunning() && !c.IsPaused() {\n\t\t\t// Autostart all the containers which has a\n\t\t\t// swarm endpoint now that the cluster is\n\t\t\t// initialized.\n\t\t\tif daemon.configStore.AutoRestart && c.ShouldRestart() && c.NetworkSettings.HasSwarmEndpoint && c.HasBeenStartedBefore {\n\t\t\t\tgroup.Add(1)\n\t\t\t\tgo func(c *container.Container) {\n\t\t\t\t\tif err := sem.Acquire(ctx, 1); err != nil {\n\t\t\t\t\t\t// ctx is done.\n\t\t\t\t\t\tgroup.Done()\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\tif err := daemon.containerStart(c, \"\", \"\", true); err != nil {\n\t\t\t\t\t\tlogrus.WithField(\"container\", c.ID).WithError(err).Error(\"failed to start swarm container\")\n\t\t\t\t\t}\n\n\t\t\t\t\tsem.Release(1)\n\t\t\t\t\tgroup.Done()\n\t\t\t\t}(c)\n\t\t\t}\n\t\t}\n\t}\n\tgroup.Wait()\n}\n\n// waitForNetworks is used during daemon initialization when starting up containers\n// It ensures that all of a container's networks are available before the daemon tries to start the container.\n// In practice it just makes sure the discovery service is available for containers which use a network that require discovery.\nfunc (daemon *Daemon) waitForNetworks(c *container.Container) {\n\tif daemon.discoveryWatcher == nil {\n\t\treturn\n\t}\n\n\t// Make sure if the container has a network that requires discovery that the discovery service is available before starting\n\tfor netName := range c.NetworkSettings.Networks {\n\t\t// If we get `ErrNoSuchNetwork` here, we can assume that it is due to discovery not being ready\n\t\t// Most likely this is because the K/V store used for discovery is in a container and needs to be started\n\t\tif _, err := daemon.netController.NetworkByName(netName); err != nil {\n\t\t\tif _, ok := err.(libnetwork.ErrNoSuchNetwork); !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// use a longish timeout here due to some slowdowns in libnetwork if the k/v store is on anything other than --net=host\n\t\t\t// FIXME: why is this slow???\n\t\t\tdur := 60 * time.Second\n\t\t\ttimer := time.NewTimer(dur)\n\n\t\t\tlogrus.WithField(\"container\", c.ID).Debugf(\"Container %s waiting for network to be ready\", c.Name)\n\t\t\tselect {\n\t\t\tcase <-daemon.discoveryWatcher.ReadyCh():\n\t\t\tcase <-timer.C:\n\t\t\t}\n\t\t\ttimer.Stop()\n\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (daemon *Daemon) children(c *container.Container) map[string]*container.Container {\n\treturn daemon.linkIndex.children(c)\n}\n\n// parents returns the names of the parent containers of the container\n// with the given name.\nfunc (daemon *Daemon) parents(c *container.Container) map[string]*container.Container {\n\treturn daemon.linkIndex.parents(c)\n}\n\nfunc (daemon *Daemon) registerLink(parent, child *container.Container, alias string) error {\n\tfullName := path.Join(parent.Name, alias)\n\tif err := daemon.containersReplica.ReserveName(fullName, child.ID); err != nil {\n\t\tif err == container.ErrNameReserved {\n\t\t\tlogrus.Warnf(\"error registering link for %s, to %s, as alias %s, ignoring: %v\", parent.ID, child.ID, alias, err)\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\tdaemon.linkIndex.link(parent, child, fullName)\n\treturn nil\n}\n\n// DaemonJoinsCluster informs the daemon has joined the cluster and provides\n// the handler to query the cluster component\nfunc (daemon *Daemon) DaemonJoinsCluster(clusterProvider cluster.Provider) {\n\tdaemon.setClusterProvider(clusterProvider)\n}\n\n// DaemonLeavesCluster informs the daemon has left the cluster\nfunc (daemon *Daemon) DaemonLeavesCluster() {\n\t// Daemon is in charge of removing the attachable networks with\n\t// connected containers when the node leaves the swarm\n\tdaemon.clearAttachableNetworks()\n\t// We no longer need the cluster provider, stop it now so that\n\t// the network agent will stop listening to cluster events.\n\tdaemon.setClusterProvider(nil)\n\t// Wait for the networking cluster agent to stop\n\tdaemon.netController.AgentStopWait()\n\t// Daemon is in charge of removing the ingress network when the\n\t// node leaves the swarm. Wait for job to be done or timeout.\n\t// This is called also on graceful daemon shutdown. We need to\n\t// wait, because the ingress release has to happen before the\n\t// network controller is stopped.\n\n\tif done, err := daemon.ReleaseIngress(); err == nil {\n\t\ttimeout := time.NewTimer(5 * time.Second)\n\t\tdefer timeout.Stop()\n\n\t\tselect {\n\t\tcase <-done:\n\t\tcase <-timeout.C:\n\t\t\tlogrus.Warn(\"timeout while waiting for ingress network removal\")\n\t\t}\n\t} else {\n\t\tlogrus.Warnf(\"failed to initiate ingress network removal: %v\", err)\n\t}\n\n\tdaemon.attachmentStore.ClearAttachments()\n}\n\n// setClusterProvider sets a component for querying the current cluster state.\nfunc (daemon *Daemon) setClusterProvider(clusterProvider cluster.Provider) {\n\tdaemon.clusterProvider = clusterProvider\n\tdaemon.netController.SetClusterProvider(clusterProvider)\n\tdaemon.attachableNetworkLock = locker.New()\n}\n\n// IsSwarmCompatible verifies if the current daemon\n// configuration is compatible with the swarm mode\nfunc (daemon *Daemon) IsSwarmCompatible() error {\n\tif daemon.configStore == nil {\n\t\treturn nil\n\t}\n\treturn daemon.configStore.IsSwarmCompatible()\n}\n\n// NewDaemon sets up everything for the daemon to be able to service\n// requests from the webserver.\nfunc NewDaemon(ctx context.Context, config *config.Config, pluginStore *plugin.Store) (daemon *Daemon, err error) {\n\tsetDefaultMtu(config)\n\n\tregistryService, err := registry.NewService(config.ServiceOptions)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Ensure that we have a correct root key limit for launching containers.\n\tif err := ModifyRootKeyLimit(); err != nil {\n\t\tlogrus.Warnf(\"unable to modify root key limit, number of containers could be limited by this quota: %v\", err)\n\t}\n\n\t// Ensure we have compatible and valid configuration options\n\tif err := verifyDaemonSettings(config); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Do we have a disabled network?\n\tconfig.DisableBridge = isBridgeNetworkDisabled(config)\n\n\t// Setup the resolv.conf\n\tsetupResolvConf(config)\n\n\t// Verify the platform is supported as a daemon\n\tif !platformSupported {\n\t\treturn nil, errSystemNotSupported\n\t}\n\n\t// Validate platform-specific requirements\n\tif err := checkSystem(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tidMapping, err := setupRemappedRoot(config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trootIDs := idMapping.RootPair()\n\tif err := setupDaemonProcess(config); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// set up the tmpDir to use a canonical path\n\ttmp, err := prepareTempDir(config.Root)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Unable to get the TempDir under %s: %s\", config.Root, err)\n\t}\n\trealTmp, err := fileutils.ReadSymlinkedDirectory(tmp)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Unable to get the full path to the TempDir (%s): %s\", tmp, err)\n\t}\n\tif isWindows {\n\t\tif _, err := os.Stat(realTmp); err != nil && os.IsNotExist(err) {\n\t\t\tif err := system.MkdirAll(realTmp, 0700); err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"Unable to create the TempDir (%s): %s\", realTmp, err)\n\t\t\t}\n\t\t}\n\t\tos.Setenv(\"TEMP\", realTmp)\n\t\tos.Setenv(\"TMP\", realTmp)\n\t} else {\n\t\tos.Setenv(\"TMPDIR\", realTmp)\n\t}\n\n\td := &Daemon{\n\t\tconfigStore: config,\n\t\tPluginStore: pluginStore,\n\t\tstartupDone: make(chan struct{}),\n\t}\n\n\t// Ensure the daemon is properly shutdown if there is a failure during\n\t// initialization\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif err := d.Shutdown(); err != nil {\n\t\t\t\tlogrus.Error(err)\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err := d.setGenericResources(config); err != nil {\n\t\treturn nil, err\n\t}\n\t// set up SIGUSR1 handler on Unix-like systems, or a Win32 global event\n\t// on Windows to dump Go routine stacks\n\tstackDumpDir := config.Root\n\tif execRoot := config.GetExecRoot(); execRoot != \"\" {\n\t\tstackDumpDir = execRoot\n\t}\n\td.setupDumpStackTrap(stackDumpDir)\n\n\tif err := d.setupSeccompProfile(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Set the default isolation mode (only applicable on Windows)\n\tif err := d.setDefaultIsolation(); err != nil {\n\t\treturn nil, fmt.Errorf(\"error setting default isolation mode: %v\", err)\n\t}\n\n\tif err := configureMaxThreads(config); err != nil {\n\t\tlogrus.Warnf(\"Failed to configure golang's threads limit: %v\", err)\n\t}\n\n\t// ensureDefaultAppArmorProfile does nothing if apparmor is disabled\n\tif err := ensureDefaultAppArmorProfile(); err != nil {\n\t\tlogrus.Errorf(err.Error())\n\t}\n\n\tdaemonRepo := filepath.Join(config.Root, \"containers\")\n\tif err := idtools.MkdirAllAndChown(daemonRepo, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the directory where we'll store the runtime scripts (i.e. in\n\t// order to support runtimeArgs)\n\tdaemonRuntimes := filepath.Join(config.Root, \"runtimes\")\n\tif err := system.MkdirAll(daemonRuntimes, 0700); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := d.loadRuntimes(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif isWindows {\n\t\tif err := system.MkdirAll(filepath.Join(config.Root, \"credentialspecs\"), 0); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// On Windows we don't support the environment variable, or a user supplied graphdriver\n\t// as Windows has no choice in terms of which graphdrivers to use. It's a case of\n\t// running Windows containers on Windows - windowsfilter, running Linux containers on Windows,\n\t// lcow. Unix platforms however run a single graphdriver for all containers, and it can\n\t// be set through an environment variable, a daemon start parameter, or chosen through\n\t// initialization of the layerstore through driver priority order for example.\n\td.graphDrivers = make(map[string]string)\n\tlayerStores := make(map[string]layer.Store)\n\tif isWindows {\n\t\td.graphDrivers[runtime.GOOS] = \"windowsfilter\"\n\t\tif system.LCOWSupported() {\n\t\t\td.graphDrivers[\"linux\"] = \"lcow\"\n\t\t}\n\t} else {\n\t\tdriverName := os.Getenv(\"DOCKER_DRIVER\")\n\t\tif driverName == \"\" {\n\t\t\tdriverName = config.GraphDriver\n\t\t} else {\n\t\t\tlogrus.Infof(\"Setting the storage driver from the $DOCKER_DRIVER environment variable (%s)\", driverName)\n\t\t}\n\t\td.graphDrivers[runtime.GOOS] = driverName // May still be empty. Layerstore init determines instead.\n\t}\n\n\td.RegistryService = registryService\n\tlogger.RegisterPluginGetter(d.PluginStore)\n\n\tmetricsSockPath, err := d.listenMetricsSock()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tregisterMetricsPluginCallback(d.PluginStore, metricsSockPath)\n\n\tbackoffConfig := backoff.DefaultConfig\n\tbackoffConfig.MaxDelay = 3 * time.Second\n\tconnParams := grpc.ConnectParams{\n\t\tBackoff: backoffConfig,\n\t}\n\tgopts := []grpc.DialOption{\n\t\t// WithBlock makes sure that the following containerd request\n\t\t// is reliable.\n\t\t//\n\t\t// NOTE: In one edge case with high load pressure, kernel kills\n\t\t// dockerd, containerd and containerd-shims caused by OOM.\n\t\t// When both dockerd and containerd restart, but containerd\n\t\t// will take time to recover all the existing containers. Before\n\t\t// containerd serving, dockerd will failed with gRPC error.\n\t\t// That bad thing is that restore action will still ignore the\n\t\t// any non-NotFound errors and returns running state for\n\t\t// already stopped container. It is unexpected behavior. And\n\t\t// we need to restart dockerd to make sure that anything is OK.\n\t\t//\n\t\t// It is painful. Add WithBlock can prevent the edge case. And\n\t\t// n common case, the containerd will be serving in shortly.\n\t\t// It is not harm to add WithBlock for containerd connection.\n\t\tgrpc.WithBlock(),\n\n\t\tgrpc.WithInsecure(),\n\t\tgrpc.WithConnectParams(connParams),\n\t\tgrpc.WithContextDialer(dialer.ContextDialer),\n\n\t\t// TODO(stevvooe): We may need to allow configuration of this on the client.\n\t\tgrpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(defaults.DefaultMaxRecvMsgSize)),\n\t\tgrpc.WithDefaultCallOptions(grpc.MaxCallSendMsgSize(defaults.DefaultMaxSendMsgSize)),\n\t}\n\tif config.ContainerdAddr != \"\" {\n\t\td.containerdCli, err = containerd.New(config.ContainerdAddr, containerd.WithDefaultNamespace(config.ContainerdNamespace), containerd.WithDialOpts(gopts), containerd.WithTimeout(60*time.Second))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"failed to dial %q\", config.ContainerdAddr)\n\t\t}\n\t}\n\n\tcreatePluginExec := func(m *plugin.Manager) (plugin.Executor, error) {\n\t\tvar pluginCli *containerd.Client\n\n\t\t// Windows is not currently using containerd, keep the\n\t\t// client as nil\n\t\tif config.ContainerdAddr != \"\" {\n\t\t\tpluginCli, err = containerd.New(config.ContainerdAddr, containerd.WithDefaultNamespace(config.ContainerdPluginNamespace), containerd.WithDialOpts(gopts), containerd.WithTimeout(60*time.Second))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"failed to dial %q\", config.ContainerdAddr)\n\t\t\t}\n\t\t}\n\n\t\tvar rt types.Runtime\n\t\tif runtime.GOOS != \"windows\" {\n\t\t\trtPtr, err := d.getRuntime(config.GetDefaultRuntimeName())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\trt = *rtPtr\n\t\t}\n\t\treturn pluginexec.New(ctx, getPluginExecRoot(config.Root), pluginCli, config.ContainerdPluginNamespace, m, rt)\n\t}\n\n\t// Plugin system initialization should happen before restore. Do not change order.\n\td.pluginManager, err = plugin.NewManager(plugin.ManagerConfig{\n\t\tRoot:               filepath.Join(config.Root, \"plugins\"),\n\t\tExecRoot:           getPluginExecRoot(config.Root),\n\t\tStore:              d.PluginStore,\n\t\tCreateExecutor:     createPluginExec,\n\t\tRegistryService:    registryService,\n\t\tLiveRestoreEnabled: config.LiveRestoreEnabled,\n\t\tLogPluginEvent:     d.LogPluginEvent, // todo: make private\n\t\tAuthzMiddleware:    config.AuthzMiddleware,\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"couldn't create plugin manager\")\n\t}\n\n\tif err := d.setupDefaultLogConfig(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor operatingSystem, gd := range d.graphDrivers {\n\t\tlayerStores[operatingSystem], err = layer.NewStoreFromOptions(layer.StoreOptions{\n\t\t\tRoot:                      config.Root,\n\t\t\tMetadataStorePathTemplate: filepath.Join(config.Root, \"image\", \"%s\", \"layerdb\"),\n\t\t\tGraphDriver:               gd,\n\t\t\tGraphDriverOptions:        config.GraphOptions,\n\t\t\tIDMapping:                 idMapping,\n\t\t\tPluginGetter:              d.PluginStore,\n\t\t\tExperimentalEnabled:       config.Experimental,\n\t\t\tOS:                        operatingSystem,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// As layerstore initialization may set the driver\n\t\td.graphDrivers[operatingSystem] = layerStores[operatingSystem].DriverName()\n\t}\n\n\t// Configure and validate the kernels security support. Note this is a Linux/FreeBSD\n\t// operation only, so it is safe to pass *just* the runtime OS graphdriver.\n\tif err := configureKernelSecuritySupport(config, d.graphDrivers[runtime.GOOS]); err != nil {\n\t\treturn nil, err\n\t}\n\n\timageRoot := filepath.Join(config.Root, \"image\", d.graphDrivers[runtime.GOOS])\n\tifs, err := image.NewFSStoreBackend(filepath.Join(imageRoot, \"imagedb\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlgrMap := make(map[string]image.LayerGetReleaser)\n\tfor los, ls := range layerStores {\n\t\tlgrMap[los] = ls\n\t}\n\timageStore, err := image.NewImageStore(ifs, lgrMap)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\td.volumes, err = volumesservice.NewVolumeService(config.Root, d.PluginStore, rootIDs, d)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttrustKey, err := loadOrCreateTrustKey(config.TrustKeyPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttrustDir := filepath.Join(config.Root, \"trust\")\n\n\tif err := system.MkdirAll(trustDir, 0700); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We have a single tag/reference store for the daemon globally. However, it's\n\t// stored under the graphdriver. On host platforms which only support a single\n\t// container OS, but multiple selectable graphdrivers, this means depending on which\n\t// graphdriver is chosen, the global reference store is under there. For\n\t// platforms which support multiple container operating systems, this is slightly\n\t// more problematic as where does the global ref store get located? Fortunately,\n\t// for Windows, which is currently the only daemon supporting multiple container\n\t// operating systems, the list of graphdrivers available isn't user configurable.\n\t// For backwards compatibility, we just put it under the windowsfilter\n\t// directory regardless.\n\trefStoreLocation := filepath.Join(imageRoot, `repositories.json`)\n\trs, err := refstore.NewReferenceStore(refStoreLocation)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Couldn't create reference store repository: %s\", err)\n\t}\n\n\tdistributionMetadataStore, err := dmetadata.NewFSMetadataStore(filepath.Join(imageRoot, \"distribution\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Discovery is only enabled when the daemon is launched with an address to advertise.  When\n\t// initialized, the daemon is registered and we can store the discovery backend as it's read-only\n\tif err := d.initDiscovery(config); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsysInfo := d.RawSysInfo(false)\n\t// Check if Devices cgroup is mounted, it is hard requirement for container security,\n\t// on Linux.\n\tif runtime.GOOS == \"linux\" && !sysInfo.CgroupDevicesEnabled && !sys.RunningInUserNS() {\n\t\treturn nil, errors.New(\"Devices cgroup isn't mounted\")\n\t}\n\n\td.ID = trustKey.PublicKey().KeyID()\n\td.repository = daemonRepo\n\td.containers = container.NewMemoryStore()\n\tif d.containersReplica, err = container.NewViewDB(); err != nil {\n\t\treturn nil, err\n\t}\n\td.execCommands = exec.NewStore()\n\td.idIndex = truncindex.NewTruncIndex([]string{})\n\td.statsCollector = d.newStatsCollector(1 * time.Second)\n\n\td.EventsService = events.New()\n\td.root = config.Root\n\td.idMapping = idMapping\n\td.seccompEnabled = sysInfo.Seccomp\n\td.apparmorEnabled = sysInfo.AppArmor\n\n\td.linkIndex = newLinkIndex()\n\n\timgSvcConfig := images.ImageServiceConfig{\n\t\tContainerStore:            d.containers,\n\t\tDistributionMetadataStore: distributionMetadataStore,\n\t\tEventsService:             d.EventsService,\n\t\tImageStore:                imageStore,\n\t\tLayerStores:               layerStores,\n\t\tMaxConcurrentDownloads:    *config.MaxConcurrentDownloads,\n\t\tMaxConcurrentUploads:      *config.MaxConcurrentUploads,\n\t\tMaxDownloadAttempts:       *config.MaxDownloadAttempts,\n\t\tReferenceStore:            rs,\n\t\tRegistryService:           registryService,\n\t\tTrustKey:                  trustKey,\n\t\tContentNamespace:          config.ContainerdNamespace,\n\t}\n\n\t// containerd is not currently supported with Windows.\n\t// So sometimes d.containerdCli will be nil\n\t// In that case we'll create a local content store... but otherwise we'll use containerd\n\tif d.containerdCli != nil {\n\t\timgSvcConfig.Leases = d.containerdCli.LeasesService()\n\t\timgSvcConfig.ContentStore = d.containerdCli.ContentStore()\n\t} else {\n\t\tcs, lm, err := d.configureLocalContentStore()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\timgSvcConfig.ContentStore = cs\n\t\timgSvcConfig.Leases = lm\n\t}\n\n\t// TODO: imageStore, distributionMetadataStore, and ReferenceStore are only\n\t// used above to run migration. They could be initialized in ImageService\n\t// if migration is called from daemon/images. layerStore might move as well.\n\td.imageService = images.NewImageService(imgSvcConfig)\n\n\tgo d.execCommandGC()\n\n\td.containerd, err = libcontainerd.NewClient(ctx, d.containerdCli, filepath.Join(config.ExecRoot, \"containerd\"), config.ContainerdNamespace, d)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := d.restore(); err != nil {\n\t\treturn nil, err\n\t}\n\tclose(d.startupDone)\n\n\tinfo := d.SystemInfo()\n\n\tengineInfo.WithValues(\n\t\tdockerversion.Version,\n\t\tdockerversion.GitCommit,\n\t\tinfo.Architecture,\n\t\tinfo.Driver,\n\t\tinfo.KernelVersion,\n\t\tinfo.OperatingSystem,\n\t\tinfo.OSType,\n\t\tinfo.OSVersion,\n\t\tinfo.ID,\n\t).Set(1)\n\tengineCpus.Set(float64(info.NCPU))\n\tengineMemory.Set(float64(info.MemTotal))\n\n\tgd := \"\"\n\tfor os, driver := range d.graphDrivers {\n\t\tif len(gd) > 0 {\n\t\t\tgd += \", \"\n\t\t}\n\t\tgd += driver\n\t\tif len(d.graphDrivers) > 1 {\n\t\t\tgd = fmt.Sprintf(\"%s (%s)\", gd, os)\n\t\t}\n\t}\n\tlogrus.WithFields(logrus.Fields{\n\t\t\"version\":        dockerversion.Version,\n\t\t\"commit\":         dockerversion.GitCommit,\n\t\t\"graphdriver(s)\": gd,\n\t}).Info(\"Docker daemon\")\n\n\treturn d, nil\n}\n\n// DistributionServices returns services controlling daemon storage\nfunc (daemon *Daemon) DistributionServices() images.DistributionServices {\n\treturn daemon.imageService.DistributionServices()\n}\n\nfunc (daemon *Daemon) waitForStartupDone() {\n\t<-daemon.startupDone\n}\n\nfunc (daemon *Daemon) shutdownContainer(c *container.Container) error {\n\tstopTimeout := c.StopTimeout()\n\n\t// If container failed to exit in stopTimeout seconds of SIGTERM, then using the force\n\tif err := daemon.containerStop(c, stopTimeout); err != nil {\n\t\treturn fmt.Errorf(\"Failed to stop container %s with error: %v\", c.ID, err)\n\t}\n\n\t// Wait without timeout for the container to exit.\n\t// Ignore the result.\n\t<-c.Wait(context.Background(), container.WaitConditionNotRunning)\n\treturn nil\n}\n\n// ShutdownTimeout returns the timeout (in seconds) before containers are forcibly\n// killed during shutdown. The default timeout can be configured both on the daemon\n// and per container, and the longest timeout will be used. A grace-period of\n// 5 seconds is added to the configured timeout.\n//\n// A negative (-1) timeout means \"indefinitely\", which means that containers\n// are not forcibly killed, and the daemon shuts down after all containers exit.\nfunc (daemon *Daemon) ShutdownTimeout() int {\n\tshutdownTimeout := daemon.configStore.ShutdownTimeout\n\tif shutdownTimeout < 0 {\n\t\treturn -1\n\t}\n\tif daemon.containers == nil {\n\t\treturn shutdownTimeout\n\t}\n\n\tgraceTimeout := 5\n\tfor _, c := range daemon.containers.List() {\n\t\tstopTimeout := c.StopTimeout()\n\t\tif stopTimeout < 0 {\n\t\t\treturn -1\n\t\t}\n\t\tif stopTimeout+graceTimeout > shutdownTimeout {\n\t\t\tshutdownTimeout = stopTimeout + graceTimeout\n\t\t}\n\t}\n\treturn shutdownTimeout\n}\n\n// Shutdown stops the daemon.\nfunc (daemon *Daemon) Shutdown() error {\n\tdaemon.shutdown = true\n\t// Keep mounts and networking running on daemon shutdown if\n\t// we are to keep containers running and restore them.\n\n\tif daemon.configStore.LiveRestoreEnabled && daemon.containers != nil {\n\t\t// check if there are any running containers, if none we should do some cleanup\n\t\tif ls, err := daemon.Containers(&types.ContainerListOptions{}); len(ls) != 0 || err != nil {\n\t\t\t// metrics plugins still need some cleanup\n\t\t\tdaemon.cleanupMetricsPlugins()\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tif daemon.containers != nil {\n\t\tlogrus.Debugf(\"daemon configured with a %d seconds minimum shutdown timeout\", daemon.configStore.ShutdownTimeout)\n\t\tlogrus.Debugf(\"start clean shutdown of all containers with a %d seconds timeout...\", daemon.ShutdownTimeout())\n\t\tdaemon.containers.ApplyAll(func(c *container.Container) {\n\t\t\tif !c.IsRunning() {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tlog := logrus.WithField(\"container\", c.ID)\n\t\t\tlog.Debug(\"shutting down container\")\n\t\t\tif err := daemon.shutdownContainer(c); err != nil {\n\t\t\t\tlog.WithError(err).Error(\"failed to shut down container\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif mountid, err := daemon.imageService.GetLayerMountID(c.ID, c.OS); err == nil {\n\t\t\t\tdaemon.cleanupMountsByID(mountid)\n\t\t\t}\n\t\t\tlog.Debugf(\"shut down container\")\n\t\t})\n\t}\n\n\tif daemon.volumes != nil {\n\t\tif err := daemon.volumes.Shutdown(); err != nil {\n\t\t\tlogrus.Errorf(\"Error shutting down volume store: %v\", err)\n\t\t}\n\t}\n\n\tif daemon.imageService != nil {\n\t\tdaemon.imageService.Cleanup()\n\t}\n\n\t// If we are part of a cluster, clean up cluster's stuff\n\tif daemon.clusterProvider != nil {\n\t\tlogrus.Debugf(\"start clean shutdown of cluster resources...\")\n\t\tdaemon.DaemonLeavesCluster()\n\t}\n\n\tdaemon.cleanupMetricsPlugins()\n\n\t// Shutdown plugins after containers and layerstore. Don't change the order.\n\tdaemon.pluginShutdown()\n\n\t// trigger libnetwork Stop only if it's initialized\n\tif daemon.netController != nil {\n\t\tdaemon.netController.Stop()\n\t}\n\n\tif daemon.containerdCli != nil {\n\t\tdaemon.containerdCli.Close()\n\t}\n\n\tif daemon.mdDB != nil {\n\t\tdaemon.mdDB.Close()\n\t}\n\n\treturn daemon.cleanupMounts()\n}\n\n// Mount sets container.BaseFS\n// (is it not set coming in? why is it unset?)\nfunc (daemon *Daemon) Mount(container *container.Container) error {\n\tif container.RWLayer == nil {\n\t\treturn errors.New(\"RWLayer of container \" + container.ID + \" is unexpectedly nil\")\n\t}\n\tdir, err := container.RWLayer.Mount(container.GetMountLabel())\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.WithField(\"container\", container.ID).Debugf(\"container mounted via layerStore: %v\", dir)\n\n\tif container.BaseFS != nil && container.BaseFS.Path() != dir.Path() {\n\t\t// The mount path reported by the graph driver should always be trusted on Windows, since the\n\t\t// volume path for a given mounted layer may change over time.  This should only be an error\n\t\t// on non-Windows operating systems.\n\t\tif runtime.GOOS != \"windows\" {\n\t\t\tdaemon.Unmount(container)\n\t\t\treturn fmt.Errorf(\"Error: driver %s is returning inconsistent paths for container %s ('%s' then '%s')\",\n\t\t\t\tdaemon.imageService.GraphDriverForOS(container.OS), container.ID, container.BaseFS, dir)\n\t\t}\n\t}\n\tcontainer.BaseFS = dir // TODO: combine these fields\n\treturn nil\n}\n\n// Unmount unsets the container base filesystem\nfunc (daemon *Daemon) Unmount(container *container.Container) error {\n\tif container.RWLayer == nil {\n\t\treturn errors.New(\"RWLayer of container \" + container.ID + \" is unexpectedly nil\")\n\t}\n\tif err := container.RWLayer.Unmount(); err != nil {\n\t\tlogrus.WithField(\"container\", container.ID).WithError(err).Error(\"error unmounting container\")\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Subnets return the IPv4 and IPv6 subnets of networks that are manager by Docker.\nfunc (daemon *Daemon) Subnets() ([]net.IPNet, []net.IPNet) {\n\tvar v4Subnets []net.IPNet\n\tvar v6Subnets []net.IPNet\n\n\tmanagedNetworks := daemon.netController.Networks()\n\n\tfor _, managedNetwork := range managedNetworks {\n\t\tv4infos, v6infos := managedNetwork.Info().IpamInfo()\n\t\tfor _, info := range v4infos {\n\t\t\tif info.IPAMData.Pool != nil {\n\t\t\t\tv4Subnets = append(v4Subnets, *info.IPAMData.Pool)\n\t\t\t}\n\t\t}\n\t\tfor _, info := range v6infos {\n\t\t\tif info.IPAMData.Pool != nil {\n\t\t\t\tv6Subnets = append(v6Subnets, *info.IPAMData.Pool)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn v4Subnets, v6Subnets\n}\n\n// prepareTempDir prepares and returns the default directory to use\n// for temporary files.\n// If it doesn't exist, it is created. If it exists, its content is removed.\nfunc prepareTempDir(rootDir string) (string, error) {\n\tvar tmpDir string\n\tif tmpDir = os.Getenv(\"DOCKER_TMPDIR\"); tmpDir == \"\" {\n\t\ttmpDir = filepath.Join(rootDir, \"tmp\")\n\t\tnewName := tmpDir + \"-old\"\n\t\tif err := os.Rename(tmpDir, newName); err == nil {\n\t\t\tgo func() {\n\t\t\t\tif err := os.RemoveAll(newName); err != nil {\n\t\t\t\t\tlogrus.Warnf(\"failed to delete old tmp directory: %s\", newName)\n\t\t\t\t}\n\t\t\t}()\n\t\t} else if !os.IsNotExist(err) {\n\t\t\tlogrus.Warnf(\"failed to rename %s for background deletion: %s. Deleting synchronously\", tmpDir, err)\n\t\t\tif err := os.RemoveAll(tmpDir); err != nil {\n\t\t\t\tlogrus.Warnf(\"failed to delete old tmp directory: %s\", tmpDir)\n\t\t\t}\n\t\t}\n\t}\n\treturn tmpDir, idtools.MkdirAllAndChown(tmpDir, 0700, idtools.CurrentIdentity())\n}\n\nfunc (daemon *Daemon) setGenericResources(conf *config.Config) error {\n\tgenericResources, err := config.ParseGenericResources(conf.NodeGenericResources)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdaemon.genericResources = genericResources\n\n\treturn nil\n}\n\nfunc setDefaultMtu(conf *config.Config) {\n\t// do nothing if the config does not have the default 0 value.\n\tif conf.Mtu != 0 {\n\t\treturn\n\t}\n\tconf.Mtu = config.DefaultNetworkMtu\n}\n\n// IsShuttingDown tells whether the daemon is shutting down or not\nfunc (daemon *Daemon) IsShuttingDown() bool {\n\treturn daemon.shutdown\n}\n\n// initDiscovery initializes the discovery watcher for this daemon.\nfunc (daemon *Daemon) initDiscovery(conf *config.Config) error {\n\tadvertise, err := config.ParseClusterAdvertiseSettings(conf.ClusterStore, conf.ClusterAdvertise)\n\tif err != nil {\n\t\tif err == discovery.ErrDiscoveryDisabled {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\n\tconf.ClusterAdvertise = advertise\n\tdiscoveryWatcher, err := discovery.Init(conf.ClusterStore, conf.ClusterAdvertise, conf.ClusterOpts)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"discovery initialization failed (%v)\", err)\n\t}\n\n\tdaemon.discoveryWatcher = discoveryWatcher\n\treturn nil\n}\n\nfunc isBridgeNetworkDisabled(conf *config.Config) bool {\n\treturn conf.BridgeConfig.Iface == config.DisableNetworkBridge\n}\n\nfunc (daemon *Daemon) networkOptions(dconfig *config.Config, pg plugingetter.PluginGetter, activeSandboxes map[string]interface{}) ([]nwconfig.Option, error) {\n\toptions := []nwconfig.Option{}\n\tif dconfig == nil {\n\t\treturn options, nil\n\t}\n\n\toptions = append(options, nwconfig.OptionExperimental(dconfig.Experimental))\n\toptions = append(options, nwconfig.OptionDataDir(dconfig.Root))\n\toptions = append(options, nwconfig.OptionExecRoot(dconfig.GetExecRoot()))\n\n\tdd := runconfig.DefaultDaemonNetworkMode()\n\tdn := runconfig.DefaultDaemonNetworkMode().NetworkName()\n\toptions = append(options, nwconfig.OptionDefaultDriver(string(dd)))\n\toptions = append(options, nwconfig.OptionDefaultNetwork(dn))\n\n\tif strings.TrimSpace(dconfig.ClusterStore) != \"\" {\n\t\tkv := strings.Split(dconfig.ClusterStore, \"://\")\n\t\tif len(kv) != 2 {\n\t\t\treturn nil, errors.New(\"kv store daemon config must be of the form KV-PROVIDER://KV-URL\")\n\t\t}\n\t\toptions = append(options, nwconfig.OptionKVProvider(kv[0]))\n\t\toptions = append(options, nwconfig.OptionKVProviderURL(kv[1]))\n\t}\n\tif len(dconfig.ClusterOpts) > 0 {\n\t\toptions = append(options, nwconfig.OptionKVOpts(dconfig.ClusterOpts))\n\t}\n\n\tif daemon.discoveryWatcher != nil {\n\t\toptions = append(options, nwconfig.OptionDiscoveryWatcher(daemon.discoveryWatcher))\n\t}\n\n\tif dconfig.ClusterAdvertise != \"\" {\n\t\toptions = append(options, nwconfig.OptionDiscoveryAddress(dconfig.ClusterAdvertise))\n\t}\n\n\toptions = append(options, nwconfig.OptionLabels(dconfig.Labels))\n\toptions = append(options, driverOptions(dconfig)...)\n\n\tif len(dconfig.NetworkConfig.DefaultAddressPools.Value()) > 0 {\n\t\toptions = append(options, nwconfig.OptionDefaultAddressPoolConfig(dconfig.NetworkConfig.DefaultAddressPools.Value()))\n\t}\n\n\tif daemon.configStore != nil && daemon.configStore.LiveRestoreEnabled && len(activeSandboxes) != 0 {\n\t\toptions = append(options, nwconfig.OptionActiveSandboxes(activeSandboxes))\n\t}\n\n\tif pg != nil {\n\t\toptions = append(options, nwconfig.OptionPluginGetter(pg))\n\t}\n\n\toptions = append(options, nwconfig.OptionNetworkControlPlaneMTU(dconfig.NetworkControlPlaneMTU))\n\n\treturn options, nil\n}\n\n// GetCluster returns the cluster\nfunc (daemon *Daemon) GetCluster() Cluster {\n\treturn daemon.cluster\n}\n\n// SetCluster sets the cluster\nfunc (daemon *Daemon) SetCluster(cluster Cluster) {\n\tdaemon.cluster = cluster\n}\n\nfunc (daemon *Daemon) pluginShutdown() {\n\tmanager := daemon.pluginManager\n\t// Check for a valid manager object. In error conditions, daemon init can fail\n\t// and shutdown called, before plugin manager is initialized.\n\tif manager != nil {\n\t\tmanager.Shutdown()\n\t}\n}\n\n// PluginManager returns current pluginManager associated with the daemon\nfunc (daemon *Daemon) PluginManager() *plugin.Manager { // set up before daemon to avoid this method\n\treturn daemon.pluginManager\n}\n\n// PluginGetter returns current pluginStore associated with the daemon\nfunc (daemon *Daemon) PluginGetter() *plugin.Store {\n\treturn daemon.PluginStore\n}\n\n// CreateDaemonRoot creates the root for the daemon\nfunc CreateDaemonRoot(config *config.Config) error {\n\t// get the canonical path to the Docker root directory\n\tvar realRoot string\n\tif _, err := os.Stat(config.Root); err != nil && os.IsNotExist(err) {\n\t\trealRoot = config.Root\n\t} else {\n\t\trealRoot, err = fileutils.ReadSymlinkedDirectory(config.Root)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Unable to get the full path to root (%s): %s\", config.Root, err)\n\t\t}\n\t}\n\n\tidMapping, err := setupRemappedRoot(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn setupDaemonRoot(config, realRoot, idMapping.RootPair())\n}\n\n// checkpointAndSave grabs a container lock to safely call container.CheckpointTo\nfunc (daemon *Daemon) checkpointAndSave(container *container.Container) error {\n\tcontainer.Lock()\n\tdefer container.Unlock()\n\tif err := container.CheckpointTo(daemon.containersReplica); err != nil {\n\t\treturn fmt.Errorf(\"Error saving container state: %v\", err)\n\t}\n\treturn nil\n}\n\n// because the CLI sends a -1 when it wants to unset the swappiness value\n// we need to clear it on the server side\nfunc fixMemorySwappiness(resources *containertypes.Resources) {\n\tif resources.MemorySwappiness != nil && *resources.MemorySwappiness == -1 {\n\t\tresources.MemorySwappiness = nil\n\t}\n}\n\n// GetAttachmentStore returns current attachment store associated with the daemon\nfunc (daemon *Daemon) GetAttachmentStore() *network.AttachmentStore {\n\treturn &daemon.attachmentStore\n}\n\n// IdentityMapping returns uid/gid mapping or a SID (in the case of Windows) for the builder\nfunc (daemon *Daemon) IdentityMapping() *idtools.IdentityMapping {\n\treturn daemon.idMapping\n}\n\n// ImageService returns the Daemon's ImageService\nfunc (daemon *Daemon) ImageService() *images.ImageService {\n\treturn daemon.imageService\n}\n\n// BuilderBackend returns the backend used by builder\nfunc (daemon *Daemon) BuilderBackend() builder.Backend {\n\treturn struct {\n\t\t*Daemon\n\t\t*images.ImageService\n\t}{daemon, daemon.imageService}\n}\n", "// +build linux freebsd\n\npackage daemon // import \"github.com/docker/docker/daemon\"\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/containerd/cgroups\"\n\tstatsV1 \"github.com/containerd/cgroups/stats/v1\"\n\tstatsV2 \"github.com/containerd/cgroups/v2/stats\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/api/types\"\n\t\"github.com/docker/docker/api/types/blkiodev\"\n\tpblkiodev \"github.com/docker/docker/api/types/blkiodev\"\n\tcontainertypes \"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/container\"\n\t\"github.com/docker/docker/daemon/config\"\n\t\"github.com/docker/docker/daemon/initlayer\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/opts\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/parsers/kernel\"\n\t\"github.com/docker/docker/pkg/sysinfo\"\n\t\"github.com/docker/docker/runconfig\"\n\tvolumemounts \"github.com/docker/docker/volume/mounts\"\n\t\"github.com/docker/libnetwork\"\n\tnwconfig \"github.com/docker/libnetwork/config\"\n\t\"github.com/docker/libnetwork/drivers/bridge\"\n\t\"github.com/docker/libnetwork/netlabel\"\n\t\"github.com/docker/libnetwork/netutils\"\n\t\"github.com/docker/libnetwork/options\"\n\tlntypes \"github.com/docker/libnetwork/types\"\n\t\"github.com/moby/sys/mount\"\n\tspecs \"github.com/opencontainers/runtime-spec/specs-go\"\n\t\"github.com/opencontainers/selinux/go-selinux\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vishvananda/netlink\"\n\t\"golang.org/x/sys/unix\"\n)\n\nconst (\n\tisWindows = false\n\n\t// DefaultShimBinary is the default shim to be used by containerd if none\n\t// is specified\n\tDefaultShimBinary = \"containerd-shim\"\n\n\t// DefaultRuntimeBinary is the default runtime to be used by\n\t// containerd if none is specified\n\tDefaultRuntimeBinary = \"runc\"\n\n\t// See https://git.kernel.org/cgit/linux/kernel/git/tip/tip.git/tree/kernel/sched/sched.h?id=8cd9234c64c584432f6992fe944ca9e46ca8ea76#n269\n\tlinuxMinCPUShares = 2\n\tlinuxMaxCPUShares = 262144\n\tplatformSupported = true\n\t// It's not kernel limit, we want this 6M limit to account for overhead during startup, and to supply a reasonable functional container\n\tlinuxMinMemory = 6291456\n\t// constants for remapped root settings\n\tdefaultIDSpecifier = \"default\"\n\tdefaultRemappedID  = \"dockremap\"\n\n\t// constant for cgroup drivers\n\tcgroupFsDriver      = \"cgroupfs\"\n\tcgroupSystemdDriver = \"systemd\"\n\tcgroupNoneDriver    = \"none\"\n)\n\ntype containerGetter interface {\n\tGetContainer(string) (*container.Container, error)\n}\n\nfunc getMemoryResources(config containertypes.Resources) *specs.LinuxMemory {\n\tmemory := specs.LinuxMemory{}\n\n\tif config.Memory > 0 {\n\t\tmemory.Limit = &config.Memory\n\t}\n\n\tif config.MemoryReservation > 0 {\n\t\tmemory.Reservation = &config.MemoryReservation\n\t}\n\n\tif config.MemorySwap > 0 {\n\t\tmemory.Swap = &config.MemorySwap\n\t}\n\n\tif config.MemorySwappiness != nil {\n\t\tswappiness := uint64(*config.MemorySwappiness)\n\t\tmemory.Swappiness = &swappiness\n\t}\n\n\tif config.OomKillDisable != nil {\n\t\tmemory.DisableOOMKiller = config.OomKillDisable\n\t}\n\n\tif config.KernelMemory != 0 {\n\t\tmemory.Kernel = &config.KernelMemory\n\t}\n\n\tif config.KernelMemoryTCP != 0 {\n\t\tmemory.KernelTCP = &config.KernelMemoryTCP\n\t}\n\n\treturn &memory\n}\n\nfunc getPidsLimit(config containertypes.Resources) *specs.LinuxPids {\n\tif config.PidsLimit == nil {\n\t\treturn nil\n\t}\n\tif *config.PidsLimit <= 0 {\n\t\t// docker API allows 0 and negative values to unset this to be consistent\n\t\t// with default values. When updating values, runc requires -1 to unset\n\t\t// the previous limit.\n\t\treturn &specs.LinuxPids{Limit: -1}\n\t}\n\treturn &specs.LinuxPids{Limit: *config.PidsLimit}\n}\n\nfunc getCPUResources(config containertypes.Resources) (*specs.LinuxCPU, error) {\n\tcpu := specs.LinuxCPU{}\n\n\tif config.CPUShares < 0 {\n\t\treturn nil, fmt.Errorf(\"shares: invalid argument\")\n\t}\n\tif config.CPUShares >= 0 {\n\t\tshares := uint64(config.CPUShares)\n\t\tcpu.Shares = &shares\n\t}\n\n\tif config.CpusetCpus != \"\" {\n\t\tcpu.Cpus = config.CpusetCpus\n\t}\n\n\tif config.CpusetMems != \"\" {\n\t\tcpu.Mems = config.CpusetMems\n\t}\n\n\tif config.NanoCPUs > 0 {\n\t\t// https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt\n\t\tperiod := uint64(100 * time.Millisecond / time.Microsecond)\n\t\tquota := config.NanoCPUs * int64(period) / 1e9\n\t\tcpu.Period = &period\n\t\tcpu.Quota = &quota\n\t}\n\n\tif config.CPUPeriod != 0 {\n\t\tperiod := uint64(config.CPUPeriod)\n\t\tcpu.Period = &period\n\t}\n\n\tif config.CPUQuota != 0 {\n\t\tq := config.CPUQuota\n\t\tcpu.Quota = &q\n\t}\n\n\tif config.CPURealtimePeriod != 0 {\n\t\tperiod := uint64(config.CPURealtimePeriod)\n\t\tcpu.RealtimePeriod = &period\n\t}\n\n\tif config.CPURealtimeRuntime != 0 {\n\t\tc := config.CPURealtimeRuntime\n\t\tcpu.RealtimeRuntime = &c\n\t}\n\n\treturn &cpu, nil\n}\n\nfunc getBlkioWeightDevices(config containertypes.Resources) ([]specs.LinuxWeightDevice, error) {\n\tvar stat unix.Stat_t\n\tvar blkioWeightDevices []specs.LinuxWeightDevice\n\n\tfor _, weightDevice := range config.BlkioWeightDevice {\n\t\tif err := unix.Stat(weightDevice.Path, &stat); err != nil {\n\t\t\treturn nil, errors.WithStack(&os.PathError{Op: \"stat\", Path: weightDevice.Path, Err: err})\n\t\t}\n\t\tweight := weightDevice.Weight\n\t\td := specs.LinuxWeightDevice{Weight: &weight}\n\t\t// The type is 32bit on mips.\n\t\td.Major = int64(unix.Major(uint64(stat.Rdev))) // nolint: unconvert\n\t\td.Minor = int64(unix.Minor(uint64(stat.Rdev))) // nolint: unconvert\n\t\tblkioWeightDevices = append(blkioWeightDevices, d)\n\t}\n\n\treturn blkioWeightDevices, nil\n}\n\nfunc (daemon *Daemon) parseSecurityOpt(container *container.Container, hostConfig *containertypes.HostConfig) error {\n\tcontainer.NoNewPrivileges = daemon.configStore.NoNewPrivileges\n\treturn parseSecurityOpt(container, hostConfig)\n}\n\nfunc parseSecurityOpt(container *container.Container, config *containertypes.HostConfig) error {\n\tvar (\n\t\tlabelOpts []string\n\t\terr       error\n\t)\n\n\tfor _, opt := range config.SecurityOpt {\n\t\tif opt == \"no-new-privileges\" {\n\t\t\tcontainer.NoNewPrivileges = true\n\t\t\tcontinue\n\t\t}\n\t\tif opt == \"disable\" {\n\t\t\tlabelOpts = append(labelOpts, \"disable\")\n\t\t\tcontinue\n\t\t}\n\n\t\tvar con []string\n\t\tif strings.Contains(opt, \"=\") {\n\t\t\tcon = strings.SplitN(opt, \"=\", 2)\n\t\t} else if strings.Contains(opt, \":\") {\n\t\t\tcon = strings.SplitN(opt, \":\", 2)\n\t\t\tlogrus.Warn(\"Security options with `:` as a separator are deprecated and will be completely unsupported in 17.04, use `=` instead.\")\n\t\t}\n\t\tif len(con) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid --security-opt 1: %q\", opt)\n\t\t}\n\n\t\tswitch con[0] {\n\t\tcase \"label\":\n\t\t\tlabelOpts = append(labelOpts, con[1])\n\t\tcase \"apparmor\":\n\t\t\tcontainer.AppArmorProfile = con[1]\n\t\tcase \"seccomp\":\n\t\t\tcontainer.SeccompProfile = con[1]\n\t\tcase \"no-new-privileges\":\n\t\t\tnoNewPrivileges, err := strconv.ParseBool(con[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid --security-opt 2: %q\", opt)\n\t\t\t}\n\t\t\tcontainer.NoNewPrivileges = noNewPrivileges\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"invalid --security-opt 2: %q\", opt)\n\t\t}\n\t}\n\n\tcontainer.ProcessLabel, container.MountLabel, err = label.InitLabels(labelOpts)\n\treturn err\n}\n\nfunc getBlkioThrottleDevices(devs []*blkiodev.ThrottleDevice) ([]specs.LinuxThrottleDevice, error) {\n\tvar throttleDevices []specs.LinuxThrottleDevice\n\tvar stat unix.Stat_t\n\n\tfor _, d := range devs {\n\t\tif err := unix.Stat(d.Path, &stat); err != nil {\n\t\t\treturn nil, errors.WithStack(&os.PathError{Op: \"stat\", Path: d.Path, Err: err})\n\t\t}\n\t\td := specs.LinuxThrottleDevice{Rate: d.Rate}\n\t\t// the type is 32bit on mips\n\t\td.Major = int64(unix.Major(uint64(stat.Rdev))) // nolint: unconvert\n\t\td.Minor = int64(unix.Minor(uint64(stat.Rdev))) // nolint: unconvert\n\t\tthrottleDevices = append(throttleDevices, d)\n\t}\n\n\treturn throttleDevices, nil\n}\n\n// adjustParallelLimit takes a number of objects and a proposed limit and\n// figures out if it's reasonable (and adjusts it accordingly). This is only\n// used for daemon startup, which does a lot of parallel loading of containers\n// (and if we exceed RLIMIT_NOFILE then we're in trouble).\nfunc adjustParallelLimit(n int, limit int) int {\n\t// Rule-of-thumb overhead factor (how many files will each goroutine open\n\t// simultaneously). Yes, this is ugly but to be frank this whole thing is\n\t// ugly.\n\tconst overhead = 2\n\n\t// On Linux, we need to ensure that parallelStartupJobs doesn't cause us to\n\t// exceed RLIMIT_NOFILE. If parallelStartupJobs is too large, we reduce it\n\t// and give a warning (since in theory the user should increase their\n\t// ulimits to the largest possible value for dockerd).\n\tvar rlim unix.Rlimit\n\tif err := unix.Getrlimit(unix.RLIMIT_NOFILE, &rlim); err != nil {\n\t\tlogrus.Warnf(\"Couldn't find dockerd's RLIMIT_NOFILE to double-check startup parallelism factor: %v\", err)\n\t\treturn limit\n\t}\n\tsoftRlimit := int(rlim.Cur)\n\n\t// Much fewer containers than RLIMIT_NOFILE. No need to adjust anything.\n\tif softRlimit > overhead*n {\n\t\treturn limit\n\t}\n\n\t// RLIMIT_NOFILE big enough, no need to adjust anything.\n\tif softRlimit > overhead*limit {\n\t\treturn limit\n\t}\n\n\tlogrus.Warnf(\"Found dockerd's open file ulimit (%v) is far too small -- consider increasing it significantly (at least %v)\", softRlimit, overhead*limit)\n\treturn softRlimit / overhead\n}\n\nfunc checkKernel() error {\n\t// Check for unsupported kernel versions\n\t// FIXME: it would be cleaner to not test for specific versions, but rather\n\t// test for specific functionalities.\n\t// Unfortunately we can't test for the feature \"does not cause a kernel panic\"\n\t// without actually causing a kernel panic, so we need this workaround until\n\t// the circumstances of pre-3.10 crashes are clearer.\n\t// For details see https://github.com/docker/docker/issues/407\n\t// Docker 1.11 and above doesn't actually run on kernels older than 3.4,\n\t// due to containerd-shim usage of PR_SET_CHILD_SUBREAPER (introduced in 3.4).\n\tif !kernel.CheckKernelVersion(3, 10, 0) {\n\t\tv, _ := kernel.GetKernelVersion()\n\t\tif os.Getenv(\"DOCKER_NOWARN_KERNEL_VERSION\") == \"\" {\n\t\t\tlogrus.Fatalf(\"Your Linux kernel version %s is not supported for running docker. Please upgrade your kernel to 3.10.0 or newer.\", v.String())\n\t\t}\n\t}\n\treturn nil\n}\n\n// adaptContainerSettings is called during container creation to modify any\n// settings necessary in the HostConfig structure.\nfunc (daemon *Daemon) adaptContainerSettings(hostConfig *containertypes.HostConfig, adjustCPUShares bool) error {\n\tif adjustCPUShares && hostConfig.CPUShares > 0 {\n\t\t// Handle unsupported CPUShares\n\t\tif hostConfig.CPUShares < linuxMinCPUShares {\n\t\t\tlogrus.Warnf(\"Changing requested CPUShares of %d to minimum allowed of %d\", hostConfig.CPUShares, linuxMinCPUShares)\n\t\t\thostConfig.CPUShares = linuxMinCPUShares\n\t\t} else if hostConfig.CPUShares > linuxMaxCPUShares {\n\t\t\tlogrus.Warnf(\"Changing requested CPUShares of %d to maximum allowed of %d\", hostConfig.CPUShares, linuxMaxCPUShares)\n\t\t\thostConfig.CPUShares = linuxMaxCPUShares\n\t\t}\n\t}\n\tif hostConfig.Memory > 0 && hostConfig.MemorySwap == 0 {\n\t\t// By default, MemorySwap is set to twice the size of Memory.\n\t\thostConfig.MemorySwap = hostConfig.Memory * 2\n\t}\n\tif hostConfig.ShmSize == 0 {\n\t\thostConfig.ShmSize = config.DefaultShmSize\n\t\tif daemon.configStore != nil {\n\t\t\thostConfig.ShmSize = int64(daemon.configStore.ShmSize)\n\t\t}\n\t}\n\t// Set default IPC mode, if unset for container\n\tif hostConfig.IpcMode.IsEmpty() {\n\t\tm := config.DefaultIpcMode\n\t\tif daemon.configStore != nil {\n\t\t\tm = daemon.configStore.IpcMode\n\t\t}\n\t\thostConfig.IpcMode = containertypes.IpcMode(m)\n\t}\n\n\t// Set default cgroup namespace mode, if unset for container\n\tif hostConfig.CgroupnsMode.IsEmpty() {\n\t\t// for cgroup v2: unshare cgroupns even for privileged containers\n\t\t// https://github.com/containers/libpod/pull/4374#issuecomment-549776387\n\t\tif hostConfig.Privileged && cgroups.Mode() != cgroups.Unified {\n\t\t\thostConfig.CgroupnsMode = containertypes.CgroupnsMode(\"host\")\n\t\t} else {\n\t\t\tm := \"host\"\n\t\t\tif cgroups.Mode() == cgroups.Unified {\n\t\t\t\tm = \"private\"\n\t\t\t}\n\t\t\tif daemon.configStore != nil {\n\t\t\t\tm = daemon.configStore.CgroupNamespaceMode\n\t\t\t}\n\t\t\thostConfig.CgroupnsMode = containertypes.CgroupnsMode(m)\n\t\t}\n\t}\n\n\tadaptSharedNamespaceContainer(daemon, hostConfig)\n\n\tvar err error\n\tsecOpts, err := daemon.generateSecurityOpt(hostConfig)\n\tif err != nil {\n\t\treturn err\n\t}\n\thostConfig.SecurityOpt = append(hostConfig.SecurityOpt, secOpts...)\n\tif hostConfig.OomKillDisable == nil {\n\t\tdefaultOomKillDisable := false\n\t\thostConfig.OomKillDisable = &defaultOomKillDisable\n\t}\n\n\treturn nil\n}\n\n// adaptSharedNamespaceContainer replaces container name with its ID in hostConfig.\n// To be more precisely, it modifies `container:name` to `container:ID` of PidMode, IpcMode\n// and NetworkMode.\n//\n// When a container shares its namespace with another container, use ID can keep the namespace\n// sharing connection between the two containers even the another container is renamed.\nfunc adaptSharedNamespaceContainer(daemon containerGetter, hostConfig *containertypes.HostConfig) {\n\tcontainerPrefix := \"container:\"\n\tif hostConfig.PidMode.IsContainer() {\n\t\tpidContainer := hostConfig.PidMode.Container()\n\t\t// if there is any error returned here, we just ignore it and leave it to be\n\t\t// handled in the following logic\n\t\tif c, err := daemon.GetContainer(pidContainer); err == nil {\n\t\t\thostConfig.PidMode = containertypes.PidMode(containerPrefix + c.ID)\n\t\t}\n\t}\n\tif hostConfig.IpcMode.IsContainer() {\n\t\tipcContainer := hostConfig.IpcMode.Container()\n\t\tif c, err := daemon.GetContainer(ipcContainer); err == nil {\n\t\t\thostConfig.IpcMode = containertypes.IpcMode(containerPrefix + c.ID)\n\t\t}\n\t}\n\tif hostConfig.NetworkMode.IsContainer() {\n\t\tnetContainer := hostConfig.NetworkMode.ConnectedContainer()\n\t\tif c, err := daemon.GetContainer(netContainer); err == nil {\n\t\t\thostConfig.NetworkMode = containertypes.NetworkMode(containerPrefix + c.ID)\n\t\t}\n\t}\n}\n\n// verifyPlatformContainerResources performs platform-specific validation of the container's resource-configuration\nfunc verifyPlatformContainerResources(resources *containertypes.Resources, sysInfo *sysinfo.SysInfo, update bool) (warnings []string, err error) {\n\tfixMemorySwappiness(resources)\n\n\t// memory subsystem checks and adjustments\n\tif resources.Memory != 0 && resources.Memory < linuxMinMemory {\n\t\treturn warnings, fmt.Errorf(\"Minimum memory limit allowed is 6MB\")\n\t}\n\tif resources.Memory > 0 && !sysInfo.MemoryLimit {\n\t\twarnings = append(warnings, \"Your kernel does not support memory limit capabilities or the cgroup is not mounted. Limitation discarded.\")\n\t\tresources.Memory = 0\n\t\tresources.MemorySwap = -1\n\t}\n\tif resources.Memory > 0 && resources.MemorySwap != -1 && !sysInfo.SwapLimit {\n\t\twarnings = append(warnings, \"Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.\")\n\t\tresources.MemorySwap = -1\n\t}\n\tif resources.Memory > 0 && resources.MemorySwap > 0 && resources.MemorySwap < resources.Memory {\n\t\treturn warnings, fmt.Errorf(\"Minimum memoryswap limit should be larger than memory limit, see usage\")\n\t}\n\tif resources.Memory == 0 && resources.MemorySwap > 0 && !update {\n\t\treturn warnings, fmt.Errorf(\"You should always set the Memory limit when using Memoryswap limit, see usage\")\n\t}\n\tif resources.MemorySwappiness != nil && !sysInfo.MemorySwappiness {\n\t\twarnings = append(warnings, \"Your kernel does not support memory swappiness capabilities or the cgroup is not mounted. Memory swappiness discarded.\")\n\t\tresources.MemorySwappiness = nil\n\t}\n\tif resources.MemorySwappiness != nil {\n\t\tswappiness := *resources.MemorySwappiness\n\t\tif swappiness < 0 || swappiness > 100 {\n\t\t\treturn warnings, fmt.Errorf(\"Invalid value: %v, valid memory swappiness range is 0-100\", swappiness)\n\t\t}\n\t}\n\tif resources.MemoryReservation > 0 && !sysInfo.MemoryReservation {\n\t\twarnings = append(warnings, \"Your kernel does not support memory soft limit capabilities or the cgroup is not mounted. Limitation discarded.\")\n\t\tresources.MemoryReservation = 0\n\t}\n\tif resources.MemoryReservation > 0 && resources.MemoryReservation < linuxMinMemory {\n\t\treturn warnings, fmt.Errorf(\"Minimum memory reservation allowed is 6MB\")\n\t}\n\tif resources.Memory > 0 && resources.MemoryReservation > 0 && resources.Memory < resources.MemoryReservation {\n\t\treturn warnings, fmt.Errorf(\"Minimum memory limit can not be less than memory reservation limit, see usage\")\n\t}\n\tif resources.KernelMemory > 0 {\n\t\t// Kernel memory limit is not supported on cgroup v2.\n\t\t// Even on cgroup v1, kernel memory limit (`kmem.limit_in_bytes`) has been deprecated since kernel 5.4.\n\t\t// https://github.com/torvalds/linux/commit/0158115f702b0ba208ab0b5adf44cae99b3ebcc7\n\t\twarnings = append(warnings, \"Specifying a kernel memory limit is deprecated and will be removed in a future release.\")\n\t}\n\tif resources.KernelMemory > 0 && !sysInfo.KernelMemory {\n\t\twarnings = append(warnings, \"Your kernel does not support kernel memory limit capabilities or the cgroup is not mounted. Limitation discarded.\")\n\t\tresources.KernelMemory = 0\n\t}\n\tif resources.KernelMemory > 0 && resources.KernelMemory < linuxMinMemory {\n\t\treturn warnings, fmt.Errorf(\"Minimum kernel memory limit allowed is 4MB\")\n\t}\n\tif resources.KernelMemory > 0 && !kernel.CheckKernelVersion(4, 0, 0) {\n\t\twarnings = append(warnings, \"You specified a kernel memory limit on a kernel older than 4.0. Kernel memory limits are experimental on older kernels, it won't work as expected and can cause your system to be unstable.\")\n\t}\n\tif resources.OomKillDisable != nil && !sysInfo.OomKillDisable {\n\t\t// only produce warnings if the setting wasn't to *disable* the OOM Kill; no point\n\t\t// warning the caller if they already wanted the feature to be off\n\t\tif *resources.OomKillDisable {\n\t\t\twarnings = append(warnings, \"Your kernel does not support OomKillDisable. OomKillDisable discarded.\")\n\t\t}\n\t\tresources.OomKillDisable = nil\n\t}\n\tif resources.OomKillDisable != nil && *resources.OomKillDisable && resources.Memory == 0 {\n\t\twarnings = append(warnings, \"OOM killer is disabled for the container, but no memory limit is set, this can result in the system running out of resources.\")\n\t}\n\tif resources.PidsLimit != nil && !sysInfo.PidsLimit {\n\t\tif *resources.PidsLimit > 0 {\n\t\t\twarnings = append(warnings, \"Your kernel does not support PIDs limit capabilities or the cgroup is not mounted. PIDs limit discarded.\")\n\t\t}\n\t\tresources.PidsLimit = nil\n\t}\n\n\t// cpu subsystem checks and adjustments\n\tif resources.NanoCPUs > 0 && resources.CPUPeriod > 0 {\n\t\treturn warnings, fmt.Errorf(\"Conflicting options: Nano CPUs and CPU Period cannot both be set\")\n\t}\n\tif resources.NanoCPUs > 0 && resources.CPUQuota > 0 {\n\t\treturn warnings, fmt.Errorf(\"Conflicting options: Nano CPUs and CPU Quota cannot both be set\")\n\t}\n\tif resources.NanoCPUs > 0 && !sysInfo.CPUCfs {\n\t\treturn warnings, fmt.Errorf(\"NanoCPUs can not be set, as your kernel does not support CPU CFS scheduler or the cgroup is not mounted\")\n\t}\n\t// The highest precision we could get on Linux is 0.001, by setting\n\t//   cpu.cfs_period_us=1000ms\n\t//   cpu.cfs_quota=1ms\n\t// See the following link for details:\n\t// https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt\n\t// Here we don't set the lower limit and it is up to the underlying platform (e.g., Linux) to return an error.\n\t// The error message is 0.01 so that this is consistent with Windows\n\tif resources.NanoCPUs < 0 || resources.NanoCPUs > int64(sysinfo.NumCPU())*1e9 {\n\t\treturn warnings, fmt.Errorf(\"Range of CPUs is from 0.01 to %d.00, as there are only %d CPUs available\", sysinfo.NumCPU(), sysinfo.NumCPU())\n\t}\n\n\tif resources.CPUShares > 0 && !sysInfo.CPUShares {\n\t\twarnings = append(warnings, \"Your kernel does not support CPU shares or the cgroup is not mounted. Shares discarded.\")\n\t\tresources.CPUShares = 0\n\t}\n\tif (resources.CPUPeriod != 0 || resources.CPUQuota != 0) && !sysInfo.CPUCfs {\n\t\twarnings = append(warnings, \"Your kernel does not support CPU CFS scheduler. CPU period/quota discarded.\")\n\t\tresources.CPUPeriod = 0\n\t\tresources.CPUQuota = 0\n\t}\n\tif resources.CPUPeriod != 0 && (resources.CPUPeriod < 1000 || resources.CPUPeriod > 1000000) {\n\t\treturn warnings, fmt.Errorf(\"CPU cfs period can not be less than 1ms (i.e. 1000) or larger than 1s (i.e. 1000000)\")\n\t}\n\tif resources.CPUQuota > 0 && resources.CPUQuota < 1000 {\n\t\treturn warnings, fmt.Errorf(\"CPU cfs quota can not be less than 1ms (i.e. 1000)\")\n\t}\n\tif resources.CPUPercent > 0 {\n\t\twarnings = append(warnings, fmt.Sprintf(\"%s does not support CPU percent. Percent discarded.\", runtime.GOOS))\n\t\tresources.CPUPercent = 0\n\t}\n\n\t// cpuset subsystem checks and adjustments\n\tif (resources.CpusetCpus != \"\" || resources.CpusetMems != \"\") && !sysInfo.Cpuset {\n\t\twarnings = append(warnings, \"Your kernel does not support cpuset or the cgroup is not mounted. Cpuset discarded.\")\n\t\tresources.CpusetCpus = \"\"\n\t\tresources.CpusetMems = \"\"\n\t}\n\tcpusAvailable, err := sysInfo.IsCpusetCpusAvailable(resources.CpusetCpus)\n\tif err != nil {\n\t\treturn warnings, errors.Wrapf(err, \"Invalid value %s for cpuset cpus\", resources.CpusetCpus)\n\t}\n\tif !cpusAvailable {\n\t\treturn warnings, fmt.Errorf(\"Requested CPUs are not available - requested %s, available: %s\", resources.CpusetCpus, sysInfo.Cpus)\n\t}\n\tmemsAvailable, err := sysInfo.IsCpusetMemsAvailable(resources.CpusetMems)\n\tif err != nil {\n\t\treturn warnings, errors.Wrapf(err, \"Invalid value %s for cpuset mems\", resources.CpusetMems)\n\t}\n\tif !memsAvailable {\n\t\treturn warnings, fmt.Errorf(\"Requested memory nodes are not available - requested %s, available: %s\", resources.CpusetMems, sysInfo.Mems)\n\t}\n\n\t// blkio subsystem checks and adjustments\n\tif resources.BlkioWeight > 0 && !sysInfo.BlkioWeight {\n\t\twarnings = append(warnings, \"Your kernel does not support Block I/O weight or the cgroup is not mounted. Weight discarded.\")\n\t\tresources.BlkioWeight = 0\n\t}\n\tif resources.BlkioWeight > 0 && (resources.BlkioWeight < 10 || resources.BlkioWeight > 1000) {\n\t\treturn warnings, fmt.Errorf(\"Range of blkio weight is from 10 to 1000\")\n\t}\n\tif resources.IOMaximumBandwidth != 0 || resources.IOMaximumIOps != 0 {\n\t\treturn warnings, fmt.Errorf(\"Invalid QoS settings: %s does not support Maximum IO Bandwidth or Maximum IO IOps\", runtime.GOOS)\n\t}\n\tif len(resources.BlkioWeightDevice) > 0 && !sysInfo.BlkioWeightDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support Block I/O weight_device or the cgroup is not mounted. Weight-device discarded.\")\n\t\tresources.BlkioWeightDevice = []*pblkiodev.WeightDevice{}\n\t}\n\tif len(resources.BlkioDeviceReadBps) > 0 && !sysInfo.BlkioReadBpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support BPS Block I/O read limit or the cgroup is not mounted. Block I/O BPS read limit discarded.\")\n\t\tresources.BlkioDeviceReadBps = []*pblkiodev.ThrottleDevice{}\n\t}\n\tif len(resources.BlkioDeviceWriteBps) > 0 && !sysInfo.BlkioWriteBpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support BPS Block I/O write limit or the cgroup is not mounted. Block I/O BPS write limit discarded.\")\n\t\tresources.BlkioDeviceWriteBps = []*pblkiodev.ThrottleDevice{}\n\n\t}\n\tif len(resources.BlkioDeviceReadIOps) > 0 && !sysInfo.BlkioReadIOpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support IOPS Block read limit or the cgroup is not mounted. Block I/O IOPS read limit discarded.\")\n\t\tresources.BlkioDeviceReadIOps = []*pblkiodev.ThrottleDevice{}\n\t}\n\tif len(resources.BlkioDeviceWriteIOps) > 0 && !sysInfo.BlkioWriteIOpsDevice {\n\t\twarnings = append(warnings, \"Your kernel does not support IOPS Block write limit or the cgroup is not mounted. Block I/O IOPS write limit discarded.\")\n\t\tresources.BlkioDeviceWriteIOps = []*pblkiodev.ThrottleDevice{}\n\t}\n\n\treturn warnings, nil\n}\n\nfunc (daemon *Daemon) getCgroupDriver() string {\n\tif UsingSystemd(daemon.configStore) {\n\t\treturn cgroupSystemdDriver\n\t}\n\tif daemon.Rootless() {\n\t\treturn cgroupNoneDriver\n\t}\n\treturn cgroupFsDriver\n}\n\n// getCD gets the raw value of the native.cgroupdriver option, if set.\nfunc getCD(config *config.Config) string {\n\tfor _, option := range config.ExecOptions {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil || !strings.EqualFold(key, \"native.cgroupdriver\") {\n\t\t\tcontinue\n\t\t}\n\t\treturn val\n\t}\n\treturn \"\"\n}\n\n// VerifyCgroupDriver validates native.cgroupdriver\nfunc VerifyCgroupDriver(config *config.Config) error {\n\tcd := getCD(config)\n\tif cd == \"\" || cd == cgroupFsDriver || cd == cgroupSystemdDriver {\n\t\treturn nil\n\t}\n\tif cd == cgroupNoneDriver {\n\t\treturn fmt.Errorf(\"native.cgroupdriver option %s is internally used and cannot be specified manually\", cd)\n\t}\n\treturn fmt.Errorf(\"native.cgroupdriver option %s not supported\", cd)\n}\n\n// UsingSystemd returns true if cli option includes native.cgroupdriver=systemd\nfunc UsingSystemd(config *config.Config) bool {\n\tif getCD(config) == cgroupSystemdDriver {\n\t\treturn true\n\t}\n\t// On cgroup v2 hosts, default to systemd driver\n\tif getCD(config) == \"\" && cgroups.Mode() == cgroups.Unified && IsRunningSystemd() {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// IsRunningSystemd is from https://github.com/opencontainers/runc/blob/46be7b612e2533c494e6a251111de46d8e286ed5/libcontainer/cgroups/systemd/common.go#L27-L33\nfunc IsRunningSystemd() bool {\n\tfi, err := os.Lstat(\"/run/systemd/system\")\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn fi.IsDir()\n}\n\n// verifyPlatformContainerSettings performs platform-specific validation of the\n// hostconfig and config structures.\nfunc verifyPlatformContainerSettings(daemon *Daemon, hostConfig *containertypes.HostConfig, update bool) (warnings []string, err error) {\n\tif hostConfig == nil {\n\t\treturn nil, nil\n\t}\n\tsysInfo := daemon.RawSysInfo(true)\n\n\tw, err := verifyPlatformContainerResources(&hostConfig.Resources, sysInfo, update)\n\n\t// no matter err is nil or not, w could have data in itself.\n\twarnings = append(warnings, w...)\n\n\tif err != nil {\n\t\treturn warnings, err\n\t}\n\n\tif hostConfig.ShmSize < 0 {\n\t\treturn warnings, fmt.Errorf(\"SHM size can not be less than 0\")\n\t}\n\n\tif hostConfig.OomScoreAdj < -1000 || hostConfig.OomScoreAdj > 1000 {\n\t\treturn warnings, fmt.Errorf(\"Invalid value %d, range for oom score adj is [-1000, 1000]\", hostConfig.OomScoreAdj)\n\t}\n\n\t// ip-forwarding does not affect container with '--net=host' (or '--net=none')\n\tif sysInfo.IPv4ForwardingDisabled && !(hostConfig.NetworkMode.IsHost() || hostConfig.NetworkMode.IsNone()) {\n\t\twarnings = append(warnings, \"IPv4 forwarding is disabled. Networking will not work.\")\n\t}\n\tif hostConfig.NetworkMode.IsHost() && len(hostConfig.PortBindings) > 0 {\n\t\twarnings = append(warnings, \"Published ports are discarded when using host network mode\")\n\t}\n\n\t// check for various conflicting options with user namespaces\n\tif daemon.configStore.RemappedRoot != \"\" && hostConfig.UsernsMode.IsPrivate() {\n\t\tif hostConfig.Privileged {\n\t\t\treturn warnings, fmt.Errorf(\"privileged mode is incompatible with user namespaces.  You must run the container in the host namespace when running privileged mode\")\n\t\t}\n\t\tif hostConfig.NetworkMode.IsHost() && !hostConfig.UsernsMode.IsHost() {\n\t\t\treturn warnings, fmt.Errorf(\"cannot share the host's network namespace when user namespaces are enabled\")\n\t\t}\n\t\tif hostConfig.PidMode.IsHost() && !hostConfig.UsernsMode.IsHost() {\n\t\t\treturn warnings, fmt.Errorf(\"cannot share the host PID namespace when user namespaces are enabled\")\n\t\t}\n\t}\n\tif hostConfig.CgroupParent != \"\" && UsingSystemd(daemon.configStore) {\n\t\t// CgroupParent for systemd cgroup should be named as \"xxx.slice\"\n\t\tif len(hostConfig.CgroupParent) <= 6 || !strings.HasSuffix(hostConfig.CgroupParent, \".slice\") {\n\t\t\treturn warnings, fmt.Errorf(\"cgroup-parent for systemd cgroup should be a valid slice named as \\\"xxx.slice\\\"\")\n\t\t}\n\t}\n\tif hostConfig.Runtime == \"\" {\n\t\thostConfig.Runtime = daemon.configStore.GetDefaultRuntimeName()\n\t}\n\n\tif rt := daemon.configStore.GetRuntime(hostConfig.Runtime); rt == nil {\n\t\treturn warnings, fmt.Errorf(\"Unknown runtime specified %s\", hostConfig.Runtime)\n\t}\n\n\tparser := volumemounts.NewParser(runtime.GOOS)\n\tfor dest := range hostConfig.Tmpfs {\n\t\tif err := parser.ValidateTmpfsMountDestination(dest); err != nil {\n\t\t\treturn warnings, err\n\t\t}\n\t}\n\n\tif !hostConfig.CgroupnsMode.Valid() {\n\t\treturn warnings, fmt.Errorf(\"invalid cgroup namespace mode: %v\", hostConfig.CgroupnsMode)\n\t}\n\tif hostConfig.CgroupnsMode.IsPrivate() {\n\t\tif !sysInfo.CgroupNamespaces {\n\t\t\twarnings = append(warnings, \"Your kernel does not support cgroup namespaces.  Cgroup namespace setting discarded.\")\n\t\t}\n\t}\n\n\tif hostConfig.Runtime == config.LinuxV1RuntimeName || (hostConfig.Runtime == \"\" && daemon.configStore.DefaultRuntime == config.LinuxV1RuntimeName) {\n\t\twarnings = append(warnings, fmt.Sprintf(\"Configured runtime %q is deprecated and will be removed in the next release.\", config.LinuxV1RuntimeName))\n\t}\n\n\treturn warnings, nil\n}\n\n// verifyDaemonSettings performs validation of daemon config struct\nfunc verifyDaemonSettings(conf *config.Config) error {\n\tif conf.ContainerdNamespace == conf.ContainerdPluginNamespace {\n\t\treturn errors.New(\"containers namespace and plugins namespace cannot be the same\")\n\t}\n\t// Check for mutually incompatible config options\n\tif conf.BridgeConfig.Iface != \"\" && conf.BridgeConfig.IP != \"\" {\n\t\treturn fmt.Errorf(\"You specified -b & --bip, mutually exclusive options. Please specify only one\")\n\t}\n\tif !conf.BridgeConfig.EnableIPTables && !conf.BridgeConfig.InterContainerCommunication {\n\t\treturn fmt.Errorf(\"You specified --iptables=false with --icc=false. ICC=false uses iptables to function. Please set --icc or --iptables to true\")\n\t}\n\tif conf.BridgeConfig.EnableIP6Tables && !conf.Experimental {\n\t\treturn fmt.Errorf(\"ip6tables rules are only available if experimental features are enabled\")\n\t}\n\tif !conf.BridgeConfig.EnableIPTables && conf.BridgeConfig.EnableIPMasq {\n\t\tconf.BridgeConfig.EnableIPMasq = false\n\t}\n\tif err := VerifyCgroupDriver(conf); err != nil {\n\t\treturn err\n\t}\n\tif conf.CgroupParent != \"\" && UsingSystemd(conf) {\n\t\tif len(conf.CgroupParent) <= 6 || !strings.HasSuffix(conf.CgroupParent, \".slice\") {\n\t\t\treturn fmt.Errorf(\"cgroup-parent for systemd cgroup should be a valid slice named as \\\"xxx.slice\\\"\")\n\t\t}\n\t}\n\n\tif conf.Rootless && UsingSystemd(conf) && cgroups.Mode() != cgroups.Unified {\n\t\treturn fmt.Errorf(\"exec-opt native.cgroupdriver=systemd requires cgroup v2 for rootless mode\")\n\t}\n\n\tconfigureRuntimes(conf)\n\tif rtName := conf.GetDefaultRuntimeName(); rtName != \"\" {\n\t\tif conf.GetRuntime(rtName) == nil {\n\t\t\treturn fmt.Errorf(\"specified default runtime '%s' does not exist\", rtName)\n\t\t}\n\t\tif rtName == config.LinuxV1RuntimeName {\n\t\t\tlogrus.Warnf(\"Configured default runtime %q is deprecated and will be removed in the next release.\", config.LinuxV1RuntimeName)\n\t\t}\n\t}\n\treturn nil\n}\n\n// checkSystem validates platform-specific requirements\nfunc checkSystem() error {\n\treturn checkKernel()\n}\n\n// configureMaxThreads sets the Go runtime max threads threshold\n// which is 90% of the kernel setting from /proc/sys/kernel/threads-max\nfunc configureMaxThreads(config *config.Config) error {\n\tmt, err := ioutil.ReadFile(\"/proc/sys/kernel/threads-max\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tmtint, err := strconv.Atoi(strings.TrimSpace(string(mt)))\n\tif err != nil {\n\t\treturn err\n\t}\n\tmaxThreads := (mtint / 100) * 90\n\tdebug.SetMaxThreads(maxThreads)\n\tlogrus.Debugf(\"Golang's threads limit set to %d\", maxThreads)\n\treturn nil\n}\n\nfunc overlaySupportsSelinux() (bool, error) {\n\tf, err := os.Open(\"/proc/kallsyms\")\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn false, nil\n\t\t}\n\t\treturn false, err\n\t}\n\tdefer f.Close()\n\n\ts := bufio.NewScanner(f)\n\tfor s.Scan() {\n\t\tif strings.HasSuffix(s.Text(), \" security_inode_copy_up\") {\n\t\t\treturn true, nil\n\t\t}\n\t}\n\n\treturn false, s.Err()\n}\n\n// configureKernelSecuritySupport configures and validates security support for the kernel\nfunc configureKernelSecuritySupport(config *config.Config, driverName string) error {\n\tif config.EnableSelinuxSupport {\n\t\tif !selinux.GetEnabled() {\n\t\t\tlogrus.Warn(\"Docker could not enable SELinux on the host system\")\n\t\t\treturn nil\n\t\t}\n\n\t\tif driverName == \"overlay\" || driverName == \"overlay2\" {\n\t\t\t// If driver is overlay or overlay2, make sure kernel\n\t\t\t// supports selinux with overlay.\n\t\t\tsupported, err := overlaySupportsSelinux()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif !supported {\n\t\t\t\tlogrus.Warnf(\"SELinux is not supported with the %v graph driver on this kernel\", driverName)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tselinux.SetDisabled()\n\t}\n\treturn nil\n}\n\nfunc (daemon *Daemon) initNetworkController(config *config.Config, activeSandboxes map[string]interface{}) (libnetwork.NetworkController, error) {\n\tnetOptions, err := daemon.networkOptions(config, daemon.PluginStore, activeSandboxes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcontroller, err := libnetwork.New(netOptions...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error obtaining controller instance: %v\", err)\n\t}\n\n\tif len(activeSandboxes) > 0 {\n\t\tlogrus.Info(\"There are old running containers, the network config will not take affect\")\n\t\treturn controller, nil\n\t}\n\n\t// Initialize default network on \"null\"\n\tif n, _ := controller.NetworkByName(\"none\"); n == nil {\n\t\tif _, err := controller.NewNetwork(\"null\", \"none\", \"\", libnetwork.NetworkOptionPersist(true)); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error creating default \\\"null\\\" network: %v\", err)\n\t\t}\n\t}\n\n\t// Initialize default network on \"host\"\n\tif n, _ := controller.NetworkByName(\"host\"); n == nil {\n\t\tif _, err := controller.NewNetwork(\"host\", \"host\", \"\", libnetwork.NetworkOptionPersist(true)); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"Error creating default \\\"host\\\" network: %v\", err)\n\t\t}\n\t}\n\n\t// Clear stale bridge network\n\tif n, err := controller.NetworkByName(\"bridge\"); err == nil {\n\t\tif err = n.Delete(); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not delete the default bridge network: %v\", err)\n\t\t}\n\t\tif len(config.NetworkConfig.DefaultAddressPools.Value()) > 0 && !daemon.configStore.LiveRestoreEnabled {\n\t\t\tremoveDefaultBridgeInterface()\n\t\t}\n\t}\n\n\tif !config.DisableBridge {\n\t\t// Initialize default driver \"bridge\"\n\t\tif err := initBridgeDriver(controller, config); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\tremoveDefaultBridgeInterface()\n\t}\n\n\t// Set HostGatewayIP to the default bridge's IP  if it is empty\n\tif daemon.configStore.HostGatewayIP == nil && controller != nil {\n\t\tif n, err := controller.NetworkByName(\"bridge\"); err == nil {\n\t\t\tv4Info, v6Info := n.Info().IpamInfo()\n\t\t\tvar gateway net.IP\n\t\t\tif len(v4Info) > 0 {\n\t\t\t\tgateway = v4Info[0].Gateway.IP\n\t\t\t} else if len(v6Info) > 0 {\n\t\t\t\tgateway = v6Info[0].Gateway.IP\n\t\t\t}\n\t\t\tdaemon.configStore.HostGatewayIP = gateway\n\t\t}\n\t}\n\treturn controller, nil\n}\n\nfunc driverOptions(config *config.Config) []nwconfig.Option {\n\tbridgeConfig := options.Generic{\n\t\t\"EnableIPForwarding\":  config.BridgeConfig.EnableIPForward,\n\t\t\"EnableIPTables\":      config.BridgeConfig.EnableIPTables,\n\t\t\"EnableIP6Tables\":     config.BridgeConfig.EnableIP6Tables,\n\t\t\"EnableUserlandProxy\": config.BridgeConfig.EnableUserlandProxy,\n\t\t\"UserlandProxyPath\":   config.BridgeConfig.UserlandProxyPath}\n\tbridgeOption := options.Generic{netlabel.GenericData: bridgeConfig}\n\n\tdOptions := []nwconfig.Option{}\n\tdOptions = append(dOptions, nwconfig.OptionDriverConfig(\"bridge\", bridgeOption))\n\treturn dOptions\n}\n\nfunc initBridgeDriver(controller libnetwork.NetworkController, config *config.Config) error {\n\tbridgeName := bridge.DefaultBridgeName\n\tif config.BridgeConfig.Iface != \"\" {\n\t\tbridgeName = config.BridgeConfig.Iface\n\t}\n\tnetOption := map[string]string{\n\t\tbridge.BridgeName:         bridgeName,\n\t\tbridge.DefaultBridge:      strconv.FormatBool(true),\n\t\tnetlabel.DriverMTU:        strconv.Itoa(config.Mtu),\n\t\tbridge.EnableIPMasquerade: strconv.FormatBool(config.BridgeConfig.EnableIPMasq),\n\t\tbridge.EnableICC:          strconv.FormatBool(config.BridgeConfig.InterContainerCommunication),\n\t}\n\n\t// --ip processing\n\tif config.BridgeConfig.DefaultIP != nil {\n\t\tnetOption[bridge.DefaultBindingIP] = config.BridgeConfig.DefaultIP.String()\n\t}\n\n\tipamV4Conf := &libnetwork.IpamConf{AuxAddresses: make(map[string]string)}\n\n\tnwList, nw6List, err := netutils.ElectInterfaceAddresses(bridgeName)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"list bridge addresses failed\")\n\t}\n\n\tnw := nwList[0]\n\tif len(nwList) > 1 && config.BridgeConfig.FixedCIDR != \"\" {\n\t\t_, fCIDR, err := net.ParseCIDR(config.BridgeConfig.FixedCIDR)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"parse CIDR failed\")\n\t\t}\n\t\t// Iterate through in case there are multiple addresses for the bridge\n\t\tfor _, entry := range nwList {\n\t\t\tif fCIDR.Contains(entry.IP) {\n\t\t\t\tnw = entry\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tipamV4Conf.PreferredPool = lntypes.GetIPNetCanonical(nw).String()\n\thip, _ := lntypes.GetHostPartIP(nw.IP, nw.Mask)\n\tif hip.IsGlobalUnicast() {\n\t\tipamV4Conf.Gateway = nw.IP.String()\n\t}\n\n\tif config.BridgeConfig.IP != \"\" {\n\t\tip, ipNet, err := net.ParseCIDR(config.BridgeConfig.IP)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tipamV4Conf.PreferredPool = ipNet.String()\n\t\tipamV4Conf.Gateway = ip.String()\n\t} else if bridgeName == bridge.DefaultBridgeName && ipamV4Conf.PreferredPool != \"\" {\n\t\tlogrus.Infof(\"Default bridge (%s) is assigned with an IP address %s. Daemon option --bip can be used to set a preferred IP address\", bridgeName, ipamV4Conf.PreferredPool)\n\t}\n\n\tif config.BridgeConfig.FixedCIDR != \"\" {\n\t\t_, fCIDR, err := net.ParseCIDR(config.BridgeConfig.FixedCIDR)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tipamV4Conf.SubPool = fCIDR.String()\n\t}\n\n\tif config.BridgeConfig.DefaultGatewayIPv4 != nil {\n\t\tipamV4Conf.AuxAddresses[\"DefaultGatewayIPv4\"] = config.BridgeConfig.DefaultGatewayIPv4.String()\n\t}\n\n\tvar (\n\t\tdeferIPv6Alloc bool\n\t\tipamV6Conf     *libnetwork.IpamConf\n\t)\n\n\tif config.BridgeConfig.EnableIPv6 && config.BridgeConfig.FixedCIDRv6 == \"\" {\n\t\treturn errdefs.InvalidParameter(errors.New(\"IPv6 is enabled for the default bridge, but no subnet is configured. Specify an IPv6 subnet using --fixed-cidr-v6\"))\n\t} else if config.BridgeConfig.FixedCIDRv6 != \"\" {\n\t\t_, fCIDRv6, err := net.ParseCIDR(config.BridgeConfig.FixedCIDRv6)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// In case user has specified the daemon flag --fixed-cidr-v6 and the passed network has\n\t\t// at least 48 host bits, we need to guarantee the current behavior where the containers'\n\t\t// IPv6 addresses will be constructed based on the containers' interface MAC address.\n\t\t// We do so by telling libnetwork to defer the IPv6 address allocation for the endpoints\n\t\t// on this network until after the driver has created the endpoint and returned the\n\t\t// constructed address. Libnetwork will then reserve this address with the ipam driver.\n\t\tones, _ := fCIDRv6.Mask.Size()\n\t\tdeferIPv6Alloc = ones <= 80\n\n\t\tipamV6Conf = &libnetwork.IpamConf{\n\t\t\tAuxAddresses:  make(map[string]string),\n\t\t\tPreferredPool: fCIDRv6.String(),\n\t\t}\n\n\t\t// In case the --fixed-cidr-v6 is specified and the current docker0 bridge IPv6\n\t\t// address belongs to the same network, we need to inform libnetwork about it, so\n\t\t// that it can be reserved with IPAM and it will not be given away to somebody else\n\t\tfor _, nw6 := range nw6List {\n\t\t\tif fCIDRv6.Contains(nw6.IP) {\n\t\t\t\tipamV6Conf.Gateway = nw6.IP.String()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif config.BridgeConfig.DefaultGatewayIPv6 != nil {\n\t\tif ipamV6Conf == nil {\n\t\t\tipamV6Conf = &libnetwork.IpamConf{AuxAddresses: make(map[string]string)}\n\t\t}\n\t\tipamV6Conf.AuxAddresses[\"DefaultGatewayIPv6\"] = config.BridgeConfig.DefaultGatewayIPv6.String()\n\t}\n\n\tv4Conf := []*libnetwork.IpamConf{ipamV4Conf}\n\tv6Conf := []*libnetwork.IpamConf{}\n\tif ipamV6Conf != nil {\n\t\tv6Conf = append(v6Conf, ipamV6Conf)\n\t}\n\t// Initialize default network on \"bridge\" with the same name\n\t_, err = controller.NewNetwork(\"bridge\", \"bridge\", \"\",\n\t\tlibnetwork.NetworkOptionEnableIPv6(config.BridgeConfig.EnableIPv6),\n\t\tlibnetwork.NetworkOptionDriverOpts(netOption),\n\t\tlibnetwork.NetworkOptionIpam(\"default\", \"\", v4Conf, v6Conf, nil),\n\t\tlibnetwork.NetworkOptionDeferIPv6Alloc(deferIPv6Alloc))\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Error creating default \\\"bridge\\\" network: %v\", err)\n\t}\n\treturn nil\n}\n\n// Remove default bridge interface if present (--bridge=none use case)\nfunc removeDefaultBridgeInterface() {\n\tif lnk, err := netlink.LinkByName(bridge.DefaultBridgeName); err == nil {\n\t\tif err := netlink.LinkDel(lnk); err != nil {\n\t\t\tlogrus.Warnf(\"Failed to remove bridge interface (%s): %v\", bridge.DefaultBridgeName, err)\n\t\t}\n\t}\n}\n\nfunc setupInitLayer(idMapping *idtools.IdentityMapping) func(containerfs.ContainerFS) error {\n\treturn func(initPath containerfs.ContainerFS) error {\n\t\treturn initlayer.Setup(initPath, idMapping.RootPair())\n\t}\n}\n\n// Parse the remapped root (user namespace) option, which can be one of:\n//   username            - valid username from /etc/passwd\n//   username:groupname  - valid username; valid groupname from /etc/group\n//   uid                 - 32-bit unsigned int valid Linux UID value\n//   uid:gid             - uid value; 32-bit unsigned int Linux GID value\n//\n//  If no groupname is specified, and a username is specified, an attempt\n//  will be made to lookup a gid for that username as a groupname\n//\n//  If names are used, they are verified to exist in passwd/group\nfunc parseRemappedRoot(usergrp string) (string, string, error) {\n\n\tvar (\n\t\tuserID, groupID     int\n\t\tusername, groupname string\n\t)\n\n\tidparts := strings.Split(usergrp, \":\")\n\tif len(idparts) > 2 {\n\t\treturn \"\", \"\", fmt.Errorf(\"Invalid user/group specification in --userns-remap: %q\", usergrp)\n\t}\n\n\tif uid, err := strconv.ParseInt(idparts[0], 10, 32); err == nil {\n\t\t// must be a uid; take it as valid\n\t\tuserID = int(uid)\n\t\tluser, err := idtools.LookupUID(userID)\n\t\tif err != nil {\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Uid %d has no entry in /etc/passwd: %v\", userID, err)\n\t\t}\n\t\tusername = luser.Name\n\t\tif len(idparts) == 1 {\n\t\t\t// if the uid was numeric and no gid was specified, take the uid as the gid\n\t\t\tgroupID = userID\n\t\t\tlgrp, err := idtools.LookupGID(groupID)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Gid %d has no entry in /etc/group: %v\", groupID, err)\n\t\t\t}\n\t\t\tgroupname = lgrp.Name\n\t\t}\n\t} else {\n\t\tlookupName := idparts[0]\n\t\t// special case: if the user specified \"default\", they want Docker to create or\n\t\t// use (after creation) the \"dockremap\" user/group for root remapping\n\t\tif lookupName == defaultIDSpecifier {\n\t\t\tlookupName = defaultRemappedID\n\t\t}\n\t\tluser, err := idtools.LookupUser(lookupName)\n\t\tif err != nil && idparts[0] != defaultIDSpecifier {\n\t\t\t// error if the name requested isn't the special \"dockremap\" ID\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during uid lookup for %q: %v\", lookupName, err)\n\t\t} else if err != nil {\n\t\t\t// special case-- if the username == \"default\", then we have been asked\n\t\t\t// to create a new entry pair in /etc/{passwd,group} for which the /etc/sub{uid,gid}\n\t\t\t// ranges will be used for the user and group mappings in user namespaced containers\n\t\t\t_, _, err := idtools.AddNamespaceRangesUser(defaultRemappedID)\n\t\t\tif err == nil {\n\t\t\t\treturn defaultRemappedID, defaultRemappedID, nil\n\t\t\t}\n\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during %q user creation: %v\", defaultRemappedID, err)\n\t\t}\n\t\tusername = luser.Name\n\t\tif len(idparts) == 1 {\n\t\t\t// we only have a string username, and no group specified; look up gid from username as group\n\t\t\tgroup, err := idtools.LookupGroup(lookupName)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during gid lookup for %q: %v\", lookupName, err)\n\t\t\t}\n\t\t\tgroupname = group.Name\n\t\t}\n\t}\n\n\tif len(idparts) == 2 {\n\t\t// groupname or gid is separately specified and must be resolved\n\t\t// to an unsigned 32-bit gid\n\t\tif gid, err := strconv.ParseInt(idparts[1], 10, 32); err == nil {\n\t\t\t// must be a gid, take it as valid\n\t\t\tgroupID = int(gid)\n\t\t\tlgrp, err := idtools.LookupGID(groupID)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Gid %d has no entry in /etc/passwd: %v\", groupID, err)\n\t\t\t}\n\t\t\tgroupname = lgrp.Name\n\t\t} else {\n\t\t\t// not a number; attempt a lookup\n\t\t\tif _, err := idtools.LookupGroup(idparts[1]); err != nil {\n\t\t\t\treturn \"\", \"\", fmt.Errorf(\"Error during groupname lookup for %q: %v\", idparts[1], err)\n\t\t\t}\n\t\t\tgroupname = idparts[1]\n\t\t}\n\t}\n\treturn username, groupname, nil\n}\n\nfunc setupRemappedRoot(config *config.Config) (*idtools.IdentityMapping, error) {\n\tif runtime.GOOS != \"linux\" && config.RemappedRoot != \"\" {\n\t\treturn nil, fmt.Errorf(\"User namespaces are only supported on Linux\")\n\t}\n\n\t// if the daemon was started with remapped root option, parse\n\t// the config option to the int uid,gid values\n\tif config.RemappedRoot != \"\" {\n\t\tusername, groupname, err := parseRemappedRoot(config.RemappedRoot)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif username == \"root\" {\n\t\t\t// Cannot setup user namespaces with a 1-to-1 mapping; \"--root=0:0\" is a no-op\n\t\t\t// effectively\n\t\t\tlogrus.Warn(\"User namespaces: root cannot be remapped with itself; user namespaces are OFF\")\n\t\t\treturn &idtools.IdentityMapping{}, nil\n\t\t}\n\t\tlogrus.Infof(\"User namespaces: ID ranges will be mapped to subuid/subgid ranges of: %s\", username)\n\t\t// update remapped root setting now that we have resolved them to actual names\n\t\tconfig.RemappedRoot = fmt.Sprintf(\"%s:%s\", username, groupname)\n\n\t\tmappings, err := idtools.NewIdentityMapping(username)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"Can't create ID mappings\")\n\t\t}\n\t\treturn mappings, nil\n\t}\n\treturn &idtools.IdentityMapping{}, nil\n}\n\nfunc setupDaemonRoot(config *config.Config, rootDir string, remappedRoot idtools.Identity) error {\n\tconfig.Root = rootDir\n\t// the docker root metadata directory needs to have execute permissions for all users (g+x,o+x)\n\t// so that syscalls executing as non-root, operating on subdirectories of the graph root\n\t// (e.g. mounted layers of a container) can traverse this path.\n\t// The user namespace support will create subdirectories for the remapped root host uid:gid\n\t// pair owned by that same uid:gid pair for proper write access to those needed metadata and\n\t// layer content subtrees.\n\tif _, err := os.Stat(rootDir); err == nil {\n\t\t// root current exists; verify the access bits are correct by setting them\n\t\tif err = os.Chmod(rootDir, 0711); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if os.IsNotExist(err) {\n\t\t// no root exists yet, create it 0711 with root:root ownership\n\t\tif err := os.MkdirAll(rootDir, 0711); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// if user namespaces are enabled we will create a subtree underneath the specified root\n\t// with any/all specified remapped root uid/gid options on the daemon creating\n\t// a new subdirectory with ownership set to the remapped uid/gid (so as to allow\n\t// `chdir()` to work for containers namespaced to that uid/gid)\n\tif config.RemappedRoot != \"\" {\n\t\tid := idtools.CurrentIdentity()\n\t\t// First make sure the current root dir has the correct perms.\n\t\tif err := idtools.MkdirAllAndChown(config.Root, 0701, id); err != nil {\n\t\t\treturn errors.Wrapf(err, \"could not create or set daemon root permissions: %s\", config.Root)\n\t\t}\n\n\t\tconfig.Root = filepath.Join(rootDir, fmt.Sprintf(\"%d.%d\", remappedRoot.UID, remappedRoot.GID))\n\t\tlogrus.Debugf(\"Creating user namespaced daemon root: %s\", config.Root)\n\t\t// Create the root directory if it doesn't exist\n\t\tif err := idtools.MkdirAllAndChown(config.Root, 0701, id); err != nil {\n\t\t\treturn fmt.Errorf(\"Cannot create daemon root: %s: %v\", config.Root, err)\n\t\t}\n\t\t// we also need to verify that any pre-existing directories in the path to\n\t\t// the graphroot won't block access to remapped root--if any pre-existing directory\n\t\t// has strict permissions that don't allow \"x\", container start will fail, so\n\t\t// better to warn and fail now\n\t\tdirPath := config.Root\n\t\tfor {\n\t\t\tdirPath = filepath.Dir(dirPath)\n\t\t\tif dirPath == \"/\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif !idtools.CanAccess(dirPath, remappedRoot) {\n\t\t\t\treturn fmt.Errorf(\"a subdirectory in your graphroot path (%s) restricts access to the remapped root uid/gid; please fix by allowing 'o+x' permissions on existing directories\", config.Root)\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := setupDaemonRootPropagation(config); err != nil {\n\t\tlogrus.WithError(err).WithField(\"dir\", config.Root).Warn(\"Error while setting daemon root propagation, this is not generally critical but may cause some functionality to not work or fallback to less desirable behavior\")\n\t}\n\treturn nil\n}\n\nfunc setupDaemonRootPropagation(cfg *config.Config) error {\n\trootParentMount, mountOptions, err := getSourceMount(cfg.Root)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"error getting daemon root's parent mount\")\n\t}\n\n\tvar cleanupOldFile bool\n\tcleanupFile := getUnmountOnShutdownPath(cfg)\n\tdefer func() {\n\t\tif !cleanupOldFile {\n\t\t\treturn\n\t\t}\n\t\tif err := os.Remove(cleanupFile); err != nil && !os.IsNotExist(err) {\n\t\t\tlogrus.WithError(err).WithField(\"file\", cleanupFile).Warn(\"could not clean up old root propagation unmount file\")\n\t\t}\n\t}()\n\n\tif hasMountInfoOption(mountOptions, sharedPropagationOption, slavePropagationOption) {\n\t\tcleanupOldFile = true\n\t\treturn nil\n\t}\n\n\tif err := mount.MakeShared(cfg.Root); err != nil {\n\t\treturn errors.Wrap(err, \"could not setup daemon root propagation to shared\")\n\t}\n\n\t// check the case where this may have already been a mount to itself.\n\t// If so then the daemon only performed a remount and should not try to unmount this later.\n\tif rootParentMount == cfg.Root {\n\t\tcleanupOldFile = true\n\t\treturn nil\n\t}\n\n\tif err := os.MkdirAll(filepath.Dir(cleanupFile), 0700); err != nil {\n\t\treturn errors.Wrap(err, \"error creating dir to store mount cleanup file\")\n\t}\n\n\tif err := ioutil.WriteFile(cleanupFile, nil, 0600); err != nil {\n\t\treturn errors.Wrap(err, \"error writing file to signal mount cleanup on shutdown\")\n\t}\n\treturn nil\n}\n\n// getUnmountOnShutdownPath generates the path to used when writing the file that signals to the daemon that on shutdown\n// the daemon root should be unmounted.\nfunc getUnmountOnShutdownPath(config *config.Config) string {\n\treturn filepath.Join(config.ExecRoot, \"unmount-on-shutdown\")\n}\n\n// registerLinks writes the links to a file.\nfunc (daemon *Daemon) registerLinks(container *container.Container, hostConfig *containertypes.HostConfig) error {\n\tif hostConfig == nil || hostConfig.NetworkMode.IsUserDefined() {\n\t\treturn nil\n\t}\n\n\tfor _, l := range hostConfig.Links {\n\t\tname, alias, err := opts.ParseLink(l)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tchild, err := daemon.GetContainer(name)\n\t\tif err != nil {\n\t\t\tif errdefs.IsNotFound(err) {\n\t\t\t\t// Trying to link to a non-existing container is not valid, and\n\t\t\t\t// should return an \"invalid parameter\" error. Returning a \"not\n\t\t\t\t// found\" error here would make the client report the container's\n\t\t\t\t// image could not be found (see moby/moby#39823)\n\t\t\t\terr = errdefs.InvalidParameter(err)\n\t\t\t}\n\t\t\treturn errors.Wrapf(err, \"could not get container for %s\", name)\n\t\t}\n\t\tfor child.HostConfig.NetworkMode.IsContainer() {\n\t\t\tparts := strings.SplitN(string(child.HostConfig.NetworkMode), \":\", 2)\n\t\t\tchild, err = daemon.GetContainer(parts[1])\n\t\t\tif err != nil {\n\t\t\t\tif errdefs.IsNotFound(err) {\n\t\t\t\t\t// Trying to link to a non-existing container is not valid, and\n\t\t\t\t\t// should return an \"invalid parameter\" error. Returning a \"not\n\t\t\t\t\t// found\" error here would make the client report the container's\n\t\t\t\t\t// image could not be found (see moby/moby#39823)\n\t\t\t\t\terr = errdefs.InvalidParameter(err)\n\t\t\t\t}\n\t\t\t\treturn errors.Wrapf(err, \"Could not get container for %s\", parts[1])\n\t\t\t}\n\t\t}\n\t\tif child.HostConfig.NetworkMode.IsHost() {\n\t\t\treturn runconfig.ErrConflictHostNetworkAndLinks\n\t\t}\n\t\tif err := daemon.registerLink(container, child, alias); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// After we load all the links into the daemon\n\t// set them to nil on the hostconfig\n\t_, err := container.WriteHostConfig()\n\treturn err\n}\n\n// conditionalMountOnStart is a platform specific helper function during the\n// container start to call mount.\nfunc (daemon *Daemon) conditionalMountOnStart(container *container.Container) error {\n\treturn daemon.Mount(container)\n}\n\n// conditionalUnmountOnCleanup is a platform specific helper function called\n// during the cleanup of a container to unmount.\nfunc (daemon *Daemon) conditionalUnmountOnCleanup(container *container.Container) error {\n\treturn daemon.Unmount(container)\n}\n\nfunc copyBlkioEntry(entries []*statsV1.BlkIOEntry) []types.BlkioStatEntry {\n\tout := make([]types.BlkioStatEntry, len(entries))\n\tfor i, re := range entries {\n\t\tout[i] = types.BlkioStatEntry{\n\t\t\tMajor: re.Major,\n\t\t\tMinor: re.Minor,\n\t\t\tOp:    re.Op,\n\t\t\tValue: re.Value,\n\t\t}\n\t}\n\treturn out\n}\n\nfunc (daemon *Daemon) stats(c *container.Container) (*types.StatsJSON, error) {\n\tif !c.IsRunning() {\n\t\treturn nil, errNotRunning(c.ID)\n\t}\n\tcs, err := daemon.containerd.Stats(context.Background(), c.ID)\n\tif err != nil {\n\t\tif strings.Contains(err.Error(), \"container not found\") {\n\t\t\treturn nil, containerNotFound(c.ID)\n\t\t}\n\t\treturn nil, err\n\t}\n\ts := &types.StatsJSON{}\n\ts.Read = cs.Read\n\tstats := cs.Metrics\n\tswitch t := stats.(type) {\n\tcase *statsV1.Metrics:\n\t\treturn daemon.statsV1(s, t)\n\tcase *statsV2.Metrics:\n\t\treturn daemon.statsV2(s, t)\n\tdefault:\n\t\treturn nil, errors.Errorf(\"unexpected type of metrics %+v\", t)\n\t}\n}\n\nfunc (daemon *Daemon) statsV1(s *types.StatsJSON, stats *statsV1.Metrics) (*types.StatsJSON, error) {\n\tif stats.Blkio != nil {\n\t\ts.BlkioStats = types.BlkioStats{\n\t\t\tIoServiceBytesRecursive: copyBlkioEntry(stats.Blkio.IoServiceBytesRecursive),\n\t\t\tIoServicedRecursive:     copyBlkioEntry(stats.Blkio.IoServicedRecursive),\n\t\t\tIoQueuedRecursive:       copyBlkioEntry(stats.Blkio.IoQueuedRecursive),\n\t\t\tIoServiceTimeRecursive:  copyBlkioEntry(stats.Blkio.IoServiceTimeRecursive),\n\t\t\tIoWaitTimeRecursive:     copyBlkioEntry(stats.Blkio.IoWaitTimeRecursive),\n\t\t\tIoMergedRecursive:       copyBlkioEntry(stats.Blkio.IoMergedRecursive),\n\t\t\tIoTimeRecursive:         copyBlkioEntry(stats.Blkio.IoTimeRecursive),\n\t\t\tSectorsRecursive:        copyBlkioEntry(stats.Blkio.SectorsRecursive),\n\t\t}\n\t}\n\tif stats.CPU != nil {\n\t\ts.CPUStats = types.CPUStats{\n\t\t\tCPUUsage: types.CPUUsage{\n\t\t\t\tTotalUsage:        stats.CPU.Usage.Total,\n\t\t\t\tPercpuUsage:       stats.CPU.Usage.PerCPU,\n\t\t\t\tUsageInKernelmode: stats.CPU.Usage.Kernel,\n\t\t\t\tUsageInUsermode:   stats.CPU.Usage.User,\n\t\t\t},\n\t\t\tThrottlingData: types.ThrottlingData{\n\t\t\t\tPeriods:          stats.CPU.Throttling.Periods,\n\t\t\t\tThrottledPeriods: stats.CPU.Throttling.ThrottledPeriods,\n\t\t\t\tThrottledTime:    stats.CPU.Throttling.ThrottledTime,\n\t\t\t},\n\t\t}\n\t}\n\n\tif stats.Memory != nil {\n\t\traw := make(map[string]uint64)\n\t\traw[\"cache\"] = stats.Memory.Cache\n\t\traw[\"rss\"] = stats.Memory.RSS\n\t\traw[\"rss_huge\"] = stats.Memory.RSSHuge\n\t\traw[\"mapped_file\"] = stats.Memory.MappedFile\n\t\traw[\"dirty\"] = stats.Memory.Dirty\n\t\traw[\"writeback\"] = stats.Memory.Writeback\n\t\traw[\"pgpgin\"] = stats.Memory.PgPgIn\n\t\traw[\"pgpgout\"] = stats.Memory.PgPgOut\n\t\traw[\"pgfault\"] = stats.Memory.PgFault\n\t\traw[\"pgmajfault\"] = stats.Memory.PgMajFault\n\t\traw[\"inactive_anon\"] = stats.Memory.InactiveAnon\n\t\traw[\"active_anon\"] = stats.Memory.ActiveAnon\n\t\traw[\"inactive_file\"] = stats.Memory.InactiveFile\n\t\traw[\"active_file\"] = stats.Memory.ActiveFile\n\t\traw[\"unevictable\"] = stats.Memory.Unevictable\n\t\traw[\"hierarchical_memory_limit\"] = stats.Memory.HierarchicalMemoryLimit\n\t\traw[\"hierarchical_memsw_limit\"] = stats.Memory.HierarchicalSwapLimit\n\t\traw[\"total_cache\"] = stats.Memory.TotalCache\n\t\traw[\"total_rss\"] = stats.Memory.TotalRSS\n\t\traw[\"total_rss_huge\"] = stats.Memory.TotalRSSHuge\n\t\traw[\"total_mapped_file\"] = stats.Memory.TotalMappedFile\n\t\traw[\"total_dirty\"] = stats.Memory.TotalDirty\n\t\traw[\"total_writeback\"] = stats.Memory.TotalWriteback\n\t\traw[\"total_pgpgin\"] = stats.Memory.TotalPgPgIn\n\t\traw[\"total_pgpgout\"] = stats.Memory.TotalPgPgOut\n\t\traw[\"total_pgfault\"] = stats.Memory.TotalPgFault\n\t\traw[\"total_pgmajfault\"] = stats.Memory.TotalPgMajFault\n\t\traw[\"total_inactive_anon\"] = stats.Memory.TotalInactiveAnon\n\t\traw[\"total_active_anon\"] = stats.Memory.TotalActiveAnon\n\t\traw[\"total_inactive_file\"] = stats.Memory.TotalInactiveFile\n\t\traw[\"total_active_file\"] = stats.Memory.TotalActiveFile\n\t\traw[\"total_unevictable\"] = stats.Memory.TotalUnevictable\n\n\t\tif stats.Memory.Usage != nil {\n\t\t\ts.MemoryStats = types.MemoryStats{\n\t\t\t\tStats:    raw,\n\t\t\t\tUsage:    stats.Memory.Usage.Usage,\n\t\t\t\tMaxUsage: stats.Memory.Usage.Max,\n\t\t\t\tLimit:    stats.Memory.Usage.Limit,\n\t\t\t\tFailcnt:  stats.Memory.Usage.Failcnt,\n\t\t\t}\n\t\t} else {\n\t\t\ts.MemoryStats = types.MemoryStats{\n\t\t\t\tStats: raw,\n\t\t\t}\n\t\t}\n\n\t\t// if the container does not set memory limit, use the machineMemory\n\t\tif s.MemoryStats.Limit > daemon.machineMemory && daemon.machineMemory > 0 {\n\t\t\ts.MemoryStats.Limit = daemon.machineMemory\n\t\t}\n\t}\n\n\tif stats.Pids != nil {\n\t\ts.PidsStats = types.PidsStats{\n\t\t\tCurrent: stats.Pids.Current,\n\t\t\tLimit:   stats.Pids.Limit,\n\t\t}\n\t}\n\n\treturn s, nil\n}\n\nfunc (daemon *Daemon) statsV2(s *types.StatsJSON, stats *statsV2.Metrics) (*types.StatsJSON, error) {\n\tif stats.Io != nil {\n\t\tvar isbr []types.BlkioStatEntry\n\t\tfor _, re := range stats.Io.Usage {\n\t\t\tisbr = append(isbr,\n\t\t\t\ttypes.BlkioStatEntry{\n\t\t\t\t\tMajor: re.Major,\n\t\t\t\t\tMinor: re.Minor,\n\t\t\t\t\tOp:    \"read\",\n\t\t\t\t\tValue: re.Rbytes,\n\t\t\t\t},\n\t\t\t\ttypes.BlkioStatEntry{\n\t\t\t\t\tMajor: re.Major,\n\t\t\t\t\tMinor: re.Minor,\n\t\t\t\t\tOp:    \"write\",\n\t\t\t\t\tValue: re.Wbytes,\n\t\t\t\t},\n\t\t\t)\n\t\t}\n\t\ts.BlkioStats = types.BlkioStats{\n\t\t\tIoServiceBytesRecursive: isbr,\n\t\t\t// Other fields are unsupported\n\t\t}\n\t}\n\n\tif stats.CPU != nil {\n\t\ts.CPUStats = types.CPUStats{\n\t\t\tCPUUsage: types.CPUUsage{\n\t\t\t\tTotalUsage: stats.CPU.UsageUsec * 1000,\n\t\t\t\t// PercpuUsage is not supported\n\t\t\t\tUsageInKernelmode: stats.CPU.SystemUsec * 1000,\n\t\t\t\tUsageInUsermode:   stats.CPU.UserUsec * 1000,\n\t\t\t},\n\t\t\tThrottlingData: types.ThrottlingData{\n\t\t\t\tPeriods:          stats.CPU.NrPeriods,\n\t\t\t\tThrottledPeriods: stats.CPU.NrThrottled,\n\t\t\t\tThrottledTime:    stats.CPU.ThrottledUsec * 1000,\n\t\t\t},\n\t\t}\n\t}\n\n\tif stats.Memory != nil {\n\t\traw := make(map[string]uint64)\n\t\traw[\"anon\"] = stats.Memory.Anon\n\t\traw[\"file\"] = stats.Memory.File\n\t\traw[\"kernel_stack\"] = stats.Memory.KernelStack\n\t\traw[\"slab\"] = stats.Memory.Slab\n\t\traw[\"sock\"] = stats.Memory.Sock\n\t\traw[\"shmem\"] = stats.Memory.Shmem\n\t\traw[\"file_mapped\"] = stats.Memory.FileMapped\n\t\traw[\"file_dirty\"] = stats.Memory.FileDirty\n\t\traw[\"file_writeback\"] = stats.Memory.FileWriteback\n\t\traw[\"anon_thp\"] = stats.Memory.AnonThp\n\t\traw[\"inactive_anon\"] = stats.Memory.InactiveAnon\n\t\traw[\"active_anon\"] = stats.Memory.ActiveAnon\n\t\traw[\"inactive_file\"] = stats.Memory.InactiveFile\n\t\traw[\"active_file\"] = stats.Memory.ActiveFile\n\t\traw[\"unevictable\"] = stats.Memory.Unevictable\n\t\traw[\"slab_reclaimable\"] = stats.Memory.SlabReclaimable\n\t\traw[\"slab_unreclaimable\"] = stats.Memory.SlabUnreclaimable\n\t\traw[\"pgfault\"] = stats.Memory.Pgfault\n\t\traw[\"pgmajfault\"] = stats.Memory.Pgmajfault\n\t\traw[\"workingset_refault\"] = stats.Memory.WorkingsetRefault\n\t\traw[\"workingset_activate\"] = stats.Memory.WorkingsetActivate\n\t\traw[\"workingset_nodereclaim\"] = stats.Memory.WorkingsetNodereclaim\n\t\traw[\"pgrefill\"] = stats.Memory.Pgrefill\n\t\traw[\"pgscan\"] = stats.Memory.Pgscan\n\t\traw[\"pgsteal\"] = stats.Memory.Pgsteal\n\t\traw[\"pgactivate\"] = stats.Memory.Pgactivate\n\t\traw[\"pgdeactivate\"] = stats.Memory.Pgdeactivate\n\t\traw[\"pglazyfree\"] = stats.Memory.Pglazyfree\n\t\traw[\"pglazyfreed\"] = stats.Memory.Pglazyfreed\n\t\traw[\"thp_fault_alloc\"] = stats.Memory.ThpFaultAlloc\n\t\traw[\"thp_collapse_alloc\"] = stats.Memory.ThpCollapseAlloc\n\t\ts.MemoryStats = types.MemoryStats{\n\t\t\t// Stats is not compatible with v1\n\t\t\tStats: raw,\n\t\t\tUsage: stats.Memory.Usage,\n\t\t\t// MaxUsage is not supported\n\t\t\tLimit: stats.Memory.UsageLimit,\n\t\t}\n\t\t// if the container does not set memory limit, use the machineMemory\n\t\tif s.MemoryStats.Limit > daemon.machineMemory && daemon.machineMemory > 0 {\n\t\t\ts.MemoryStats.Limit = daemon.machineMemory\n\t\t}\n\t\tif stats.MemoryEvents != nil {\n\t\t\t// Failcnt is set to the \"oom\" field of the \"memory.events\" file.\n\t\t\t// See https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html\n\t\t\ts.MemoryStats.Failcnt = stats.MemoryEvents.Oom\n\t\t}\n\t}\n\n\tif stats.Pids != nil {\n\t\ts.PidsStats = types.PidsStats{\n\t\t\tCurrent: stats.Pids.Current,\n\t\t\tLimit:   stats.Pids.Limit,\n\t\t}\n\t}\n\n\treturn s, nil\n}\n\n// setDefaultIsolation determines the default isolation mode for the\n// daemon to run in. This is only applicable on Windows\nfunc (daemon *Daemon) setDefaultIsolation() error {\n\treturn nil\n}\n\n// setupDaemonProcess sets various settings for the daemon's process\nfunc setupDaemonProcess(config *config.Config) error {\n\t// setup the daemons oom_score_adj\n\tif err := setupOOMScoreAdj(config.OOMScoreAdjust); err != nil {\n\t\treturn err\n\t}\n\tif err := setMayDetachMounts(); err != nil {\n\t\tlogrus.WithError(err).Warn(\"Could not set may_detach_mounts kernel parameter\")\n\t}\n\treturn nil\n}\n\n// This is used to allow removal of mountpoints that may be mounted in other\n// namespaces on RHEL based kernels starting from RHEL 7.4.\n// Without this setting, removals on these RHEL based kernels may fail with\n// \"device or resource busy\".\n// This setting is not available in upstream kernels as it is not configurable,\n// but has been in the upstream kernels since 3.15.\nfunc setMayDetachMounts() error {\n\tf, err := os.OpenFile(\"/proc/sys/fs/may_detach_mounts\", os.O_WRONLY, 0)\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errors.Wrap(err, \"error opening may_detach_mounts kernel config file\")\n\t}\n\tdefer f.Close()\n\n\t_, err = f.WriteString(\"1\")\n\tif os.IsPermission(err) {\n\t\t// Setting may_detach_mounts does not work in an\n\t\t// unprivileged container. Ignore the error, but log\n\t\t// it if we appear not to be in that situation.\n\t\tif !sys.RunningInUserNS() {\n\t\t\tlogrus.Debugf(\"Permission denied writing %q to /proc/sys/fs/may_detach_mounts\", \"1\")\n\t\t}\n\t\treturn nil\n\t}\n\treturn err\n}\n\nfunc setupOOMScoreAdj(score int) error {\n\tif score == 0 {\n\t\treturn nil\n\t}\n\tf, err := os.OpenFile(\"/proc/self/oom_score_adj\", os.O_WRONLY, 0)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\tstringScore := strconv.Itoa(score)\n\t_, err = f.WriteString(stringScore)\n\tif os.IsPermission(err) {\n\t\t// Setting oom_score_adj does not work in an\n\t\t// unprivileged container. Ignore the error, but log\n\t\t// it if we appear not to be in that situation.\n\t\tif !sys.RunningInUserNS() {\n\t\t\tlogrus.Debugf(\"Permission denied writing %q to /proc/self/oom_score_adj\", stringScore)\n\t\t}\n\t\treturn nil\n\t}\n\n\treturn err\n}\n\nfunc (daemon *Daemon) initCPURtController(mnt, path string) error {\n\tif path == \"/\" || path == \".\" {\n\t\treturn nil\n\t}\n\n\t// Recursively create cgroup to ensure that the system and all parent cgroups have values set\n\t// for the period and runtime as this limits what the children can be set to.\n\tif err := daemon.initCPURtController(mnt, filepath.Dir(path)); err != nil {\n\t\treturn err\n\t}\n\n\tpath = filepath.Join(mnt, path)\n\tif err := os.MkdirAll(path, 0755); err != nil {\n\t\treturn err\n\t}\n\tif err := maybeCreateCPURealTimeFile(daemon.configStore.CPURealtimePeriod, \"cpu.rt_period_us\", path); err != nil {\n\t\treturn err\n\t}\n\treturn maybeCreateCPURealTimeFile(daemon.configStore.CPURealtimeRuntime, \"cpu.rt_runtime_us\", path)\n}\n\nfunc maybeCreateCPURealTimeFile(configValue int64, file string, path string) error {\n\tif configValue == 0 {\n\t\treturn nil\n\t}\n\treturn ioutil.WriteFile(filepath.Join(path, file), []byte(strconv.FormatInt(configValue, 10)), 0700)\n}\n\nfunc (daemon *Daemon) setupSeccompProfile() error {\n\tif daemon.configStore.SeccompProfile != \"\" {\n\t\tdaemon.seccompProfilePath = daemon.configStore.SeccompProfile\n\t\tb, err := ioutil.ReadFile(daemon.configStore.SeccompProfile)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"opening seccomp profile (%s) failed: %v\", daemon.configStore.SeccompProfile, err)\n\t\t}\n\t\tdaemon.seccompProfile = b\n\t}\n\treturn nil\n}\n\n// RawSysInfo returns *sysinfo.SysInfo .\nfunc (daemon *Daemon) RawSysInfo(quiet bool) *sysinfo.SysInfo {\n\tvar opts []sysinfo.Opt\n\tif daemon.getCgroupDriver() == cgroupSystemdDriver {\n\t\trootlesskitParentEUID := os.Getenv(\"ROOTLESSKIT_PARENT_EUID\")\n\t\tif rootlesskitParentEUID != \"\" {\n\t\t\tgroupPath := fmt.Sprintf(\"/user.slice/user-%s.slice\", rootlesskitParentEUID)\n\t\t\topts = append(opts, sysinfo.WithCgroup2GroupPath(groupPath))\n\t\t}\n\t}\n\treturn sysinfo.New(quiet, opts...)\n}\n\nfunc recursiveUnmount(target string) error {\n\treturn mount.RecursiveUnmount(target)\n}\n", "// +build linux\n\n/*\n\naufs driver directory structure\n\n  .\n  \u251c\u2500\u2500 layers // Metadata of layers\n  \u2502   \u251c\u2500\u2500 1\n  \u2502   \u251c\u2500\u2500 2\n  \u2502   \u2514\u2500\u2500 3\n  \u251c\u2500\u2500 diff  // Content of the layer\n  \u2502   \u251c\u2500\u2500 1  // Contains layers that need to be mounted for the id\n  \u2502   \u251c\u2500\u2500 2\n  \u2502   \u2514\u2500\u2500 3\n  \u2514\u2500\u2500 mnt    // Mount points for the rw layers to be mounted\n      \u251c\u2500\u2500 1\n      \u251c\u2500\u2500 2\n      \u2514\u2500\u2500 3\n\n*/\n\npackage aufs // import \"github.com/docker/docker/daemon/graphdriver/aufs\"\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/chrootarchive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/directory\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/vbatts/tar-split/tar/storage\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t// ErrAufsNotSupported is returned if aufs is not supported by the host.\n\tErrAufsNotSupported = fmt.Errorf(\"AUFS was not found in /proc/filesystems\")\n\t// ErrAufsNested means aufs cannot be used bc we are in a user namespace\n\tErrAufsNested = fmt.Errorf(\"AUFS cannot be used in non-init user namespace\")\n\tbackingFs     = \"<unknown>\"\n\n\tenableDirpermLock sync.Once\n\tenableDirperm     bool\n\n\tlogger = logrus.WithField(\"storage-driver\", \"aufs\")\n)\n\nfunc init() {\n\tgraphdriver.Register(\"aufs\", Init)\n}\n\n// Driver contains information about the filesystem mounted.\ntype Driver struct {\n\troot          string\n\tuidMaps       []idtools.IDMap\n\tgidMaps       []idtools.IDMap\n\tctr           *graphdriver.RefCounter\n\tpathCacheLock sync.Mutex\n\tpathCache     map[string]string\n\tnaiveDiff     graphdriver.DiffDriver\n\tlocker        *locker.Locker\n\tmntL          sync.Mutex\n}\n\n// Init returns a new AUFS driver.\n// An error is returned if AUFS is not supported.\nfunc Init(root string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\t// Try to load the aufs kernel module\n\tif err := supportsAufs(); err != nil {\n\t\tlogger.Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\t// Perform feature detection on /var/lib/docker/aufs if it's an existing directory.\n\t// This covers situations where /var/lib/docker/aufs is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := root\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif fsName, ok := graphdriver.FsNames[fsMagic]; ok {\n\t\tbackingFs = fsName\n\t}\n\n\tswitch fsMagic {\n\tcase graphdriver.FsMagicAufs, graphdriver.FsMagicBtrfs, graphdriver.FsMagicEcryptfs:\n\t\tlogger.Errorf(\"AUFS is not supported over %s\", backingFs)\n\t\treturn nil, graphdriver.ErrIncompatibleFS\n\t}\n\n\tpaths := []string{\n\t\t\"mnt\",\n\t\t\"diff\",\n\t\t\"layers\",\n\t}\n\n\ta := &Driver{\n\t\troot:      root,\n\t\tuidMaps:   uidMaps,\n\t\tgidMaps:   gidMaps,\n\t\tpathCache: make(map[string]string),\n\t\tctr:       graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicAufs)),\n\t\tlocker:    locker.New(),\n\t}\n\n\tcurrentID := idtools.CurrentIdentity()\n\t// Create the root aufs driver dir\n\tif err := idtools.MkdirAllAndChown(root, 0701, currentID); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Populate the dir structure\n\tfor _, p := range paths {\n\t\tif err := idtools.MkdirAllAndChown(path.Join(root, p), 0701, currentID); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tfor _, path := range []string{\"mnt\", \"diff\"} {\n\t\tp := filepath.Join(root, path)\n\t\tentries, err := ioutil.ReadDir(p)\n\t\tif err != nil {\n\t\t\tlogger.WithError(err).WithField(\"dir\", p).Error(\"error reading dir entries\")\n\t\t\tcontinue\n\t\t}\n\t\tfor _, entry := range entries {\n\t\t\tif !entry.IsDir() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif strings.HasSuffix(entry.Name(), \"-removing\") {\n\t\t\t\tlogger.WithField(\"dir\", entry.Name()).Debug(\"Cleaning up stale layer dir\")\n\t\t\t\tif err := system.EnsureRemoveAll(filepath.Join(p, entry.Name())); err != nil {\n\t\t\t\t\tlogger.WithField(\"dir\", entry.Name()).WithError(err).Error(\"Error removing stale layer dir\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\ta.naiveDiff = graphdriver.NewNaiveDiffDriver(a, uidMaps, gidMaps)\n\treturn a, nil\n}\n\n// Return a nil error if the kernel supports aufs\n// We cannot modprobe because inside dind modprobe fails\n// to run\nfunc supportsAufs() error {\n\t// We can try to modprobe aufs first before looking at\n\t// proc/filesystems for when aufs is supported\n\texec.Command(\"modprobe\", \"aufs\").Run()\n\n\tif sys.RunningInUserNS() {\n\t\treturn ErrAufsNested\n\t}\n\n\tf, err := os.Open(\"/proc/filesystems\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\ts := bufio.NewScanner(f)\n\tfor s.Scan() {\n\t\tif strings.Contains(s.Text(), \"aufs\") {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrAufsNotSupported\n}\n\nfunc (a *Driver) rootPath() string {\n\treturn a.root\n}\n\nfunc (*Driver) String() string {\n\treturn \"aufs\"\n}\n\n// Status returns current information about the filesystem such as root directory, number of directories mounted, etc.\nfunc (a *Driver) Status() [][2]string {\n\tids, _ := loadIds(path.Join(a.rootPath(), \"layers\"))\n\treturn [][2]string{\n\t\t{\"Root Dir\", a.rootPath()},\n\t\t{\"Backing Filesystem\", backingFs},\n\t\t{\"Dirs\", fmt.Sprintf(\"%d\", len(ids))},\n\t\t{\"Dirperm1 Supported\", fmt.Sprintf(\"%v\", useDirperm())},\n\t}\n}\n\n// GetMetadata not implemented\nfunc (a *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn nil, nil\n}\n\n// Exists returns true if the given id is registered with\n// this driver\nfunc (a *Driver) Exists(id string) bool {\n\tif _, err := os.Lstat(path.Join(a.rootPath(), \"layers\", id)); err != nil {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (a *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn a.Create(id, parent, opts)\n}\n\n// Create three folders for each id\n// mnt, layers, and diff\nfunc (a *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported for aufs\")\n\t}\n\n\tif err := a.createDirsFor(id); err != nil {\n\t\treturn err\n\t}\n\t// Write the layers metadata\n\tf, err := os.Create(path.Join(a.rootPath(), \"layers\", id))\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\tif parent != \"\" {\n\t\tids, err := getParentIDs(a.rootPath(), parent)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif _, err := fmt.Fprintln(f, parent); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, i := range ids {\n\t\t\tif _, err := fmt.Fprintln(f, i); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// createDirsFor creates two directories for the given id.\n// mnt and diff\nfunc (a *Driver) createDirsFor(id string) error {\n\tpaths := []string{\n\t\t\"mnt\",\n\t\t\"diff\",\n\t}\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(a.uidMaps, a.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Directory permission is 0755.\n\t// The path of directories are <aufs_root_path>/mnt/<image_id>\n\t// and <aufs_root_path>/diff/<image_id>\n\tfor _, p := range paths {\n\t\tif err := idtools.MkdirAllAndChown(path.Join(a.rootPath(), p, id), 0755, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Remove will unmount and remove the given id.\nfunc (a *Driver) Remove(id string) error {\n\ta.locker.Lock(id)\n\tdefer a.locker.Unlock(id)\n\ta.pathCacheLock.Lock()\n\tmountpoint, exists := a.pathCache[id]\n\ta.pathCacheLock.Unlock()\n\tif !exists {\n\t\tmountpoint = a.getMountpoint(id)\n\t}\n\n\tif err := a.unmount(mountpoint); err != nil {\n\t\tlogger.WithError(err).WithField(\"method\", \"Remove()\").Warn()\n\t\treturn err\n\t}\n\n\t// Remove the layers file for the id\n\tif err := os.Remove(path.Join(a.rootPath(), \"layers\", id)); err != nil && !os.IsNotExist(err) {\n\t\treturn errors.Wrapf(err, \"error removing layers dir for %s\", id)\n\t}\n\n\tif err := atomicRemove(a.getDiffPath(id)); err != nil {\n\t\treturn errors.Wrapf(err, \"could not remove diff path for id %s\", id)\n\t}\n\n\t// Atomically remove each directory in turn by first moving it out of the\n\t// way (so that docker doesn't find it anymore) before doing removal of\n\t// the whole tree.\n\tif err := atomicRemove(mountpoint); err != nil {\n\t\tif errors.Is(err, unix.EBUSY) {\n\t\t\tlogger.WithField(\"dir\", mountpoint).WithError(err).Warn(\"error performing atomic remove due to EBUSY\")\n\t\t}\n\t\treturn errors.Wrapf(err, \"could not remove mountpoint for id %s\", id)\n\t}\n\n\ta.pathCacheLock.Lock()\n\tdelete(a.pathCache, id)\n\ta.pathCacheLock.Unlock()\n\treturn nil\n}\n\nfunc atomicRemove(source string) error {\n\ttarget := source + \"-removing\"\n\n\terr := os.Rename(source, target)\n\tswitch {\n\tcase err == nil, os.IsNotExist(err):\n\tcase os.IsExist(err):\n\t\t// Got error saying the target dir already exists, maybe the source doesn't exist due to a previous (failed) remove\n\t\tif _, e := os.Stat(source); !os.IsNotExist(e) {\n\t\t\treturn errors.Wrapf(err, \"target rename dir %q exists but should not, this needs to be manually cleaned up\", target)\n\t\t}\n\tdefault:\n\t\treturn errors.Wrapf(err, \"error preparing atomic delete\")\n\t}\n\n\treturn system.EnsureRemoveAll(target)\n}\n\n// Get returns the rootfs path for the id.\n// This will mount the dir at its given path\nfunc (a *Driver) Get(id, mountLabel string) (containerfs.ContainerFS, error) {\n\ta.locker.Lock(id)\n\tdefer a.locker.Unlock(id)\n\tparents, err := a.getParentLayerPaths(id)\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\ta.pathCacheLock.Lock()\n\tm, exists := a.pathCache[id]\n\ta.pathCacheLock.Unlock()\n\n\tif !exists {\n\t\tm = a.getDiffPath(id)\n\t\tif len(parents) > 0 {\n\t\t\tm = a.getMountpoint(id)\n\t\t}\n\t}\n\tif count := a.ctr.Increment(m); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(m), nil\n\t}\n\n\t// If a dir does not have a parent ( no layers )do not try to mount\n\t// just return the diff path to the data\n\tif len(parents) > 0 {\n\t\tif err := a.mount(id, m, mountLabel, parents); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\ta.pathCacheLock.Lock()\n\ta.pathCache[id] = m\n\ta.pathCacheLock.Unlock()\n\treturn containerfs.NewLocalContainerFS(m), nil\n}\n\n// Put unmounts and updates list of active mounts.\nfunc (a *Driver) Put(id string) error {\n\ta.locker.Lock(id)\n\tdefer a.locker.Unlock(id)\n\ta.pathCacheLock.Lock()\n\tm, exists := a.pathCache[id]\n\tif !exists {\n\t\tm = a.getMountpoint(id)\n\t\ta.pathCache[id] = m\n\t}\n\ta.pathCacheLock.Unlock()\n\tif count := a.ctr.Decrement(m); count > 0 {\n\t\treturn nil\n\t}\n\n\terr := a.unmount(m)\n\tif err != nil {\n\t\tlogger.WithError(err).WithField(\"method\", \"Put()\").Warn()\n\t}\n\treturn err\n}\n\n// isParent returns if the passed in parent is the direct parent of the passed in layer\nfunc (a *Driver) isParent(id, parent string) bool {\n\tparents, _ := getParentIDs(a.rootPath(), id)\n\tif parent == \"\" && len(parents) > 0 {\n\t\treturn false\n\t}\n\treturn !(len(parents) > 0 && parent != parents[0])\n}\n\n// Diff produces an archive of the changes between the specified\n// layer and its parent layer which may be \"\".\nfunc (a *Driver) Diff(id, parent string) (io.ReadCloser, error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.Diff(id, parent)\n\t}\n\n\t// AUFS doesn't need the parent layer to produce a diff.\n\treturn archive.TarWithOptions(path.Join(a.rootPath(), \"diff\", id), &archive.TarOptions{\n\t\tCompression:     archive.Uncompressed,\n\t\tExcludePatterns: []string{archive.WhiteoutMetaPrefix + \"*\", \"!\" + archive.WhiteoutOpaqueDir},\n\t\tUIDMaps:         a.uidMaps,\n\t\tGIDMaps:         a.gidMaps,\n\t})\n}\n\ntype fileGetNilCloser struct {\n\tstorage.FileGetter\n}\n\nfunc (f fileGetNilCloser) Close() error {\n\treturn nil\n}\n\n// DiffGetter returns a FileGetCloser that can read files from the directory that\n// contains files for the layer differences. Used for direct access for tar-split.\nfunc (a *Driver) DiffGetter(id string) (graphdriver.FileGetCloser, error) {\n\tp := path.Join(a.rootPath(), \"diff\", id)\n\treturn fileGetNilCloser{storage.NewPathFileGetter(p)}, nil\n}\n\nfunc (a *Driver) applyDiff(id string, diff io.Reader) error {\n\treturn chrootarchive.UntarUncompressed(diff, path.Join(a.rootPath(), \"diff\", id), &archive.TarOptions{\n\t\tUIDMaps: a.uidMaps,\n\t\tGIDMaps: a.gidMaps,\n\t})\n}\n\n// DiffSize calculates the changes between the specified id\n// and its parent and returns the size in bytes of the changes\n// relative to its base filesystem directory.\nfunc (a *Driver) DiffSize(id, parent string) (size int64, err error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.DiffSize(id, parent)\n\t}\n\t// AUFS doesn't need the parent layer to calculate the diff size.\n\treturn directory.Size(context.TODO(), path.Join(a.rootPath(), \"diff\", id))\n}\n\n// ApplyDiff extracts the changeset from the given diff into the\n// layer with the specified id and parent, returning the size of the\n// new layer in bytes.\nfunc (a *Driver) ApplyDiff(id, parent string, diff io.Reader) (size int64, err error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.ApplyDiff(id, parent, diff)\n\t}\n\n\t// AUFS doesn't need the parent id to apply the diff if it is the direct parent.\n\tif err = a.applyDiff(id, diff); err != nil {\n\t\treturn\n\t}\n\n\treturn a.DiffSize(id, parent)\n}\n\n// Changes produces a list of changes between the specified layer\n// and its parent layer. If parent is \"\", then all changes will be ADD changes.\nfunc (a *Driver) Changes(id, parent string) ([]archive.Change, error) {\n\tif !a.isParent(id, parent) {\n\t\treturn a.naiveDiff.Changes(id, parent)\n\t}\n\n\t// AUFS doesn't have snapshots, so we need to get changes from all parent\n\t// layers.\n\tlayers, err := a.getParentLayerPaths(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn archive.Changes(layers, path.Join(a.rootPath(), \"diff\", id))\n}\n\nfunc (a *Driver) getParentLayerPaths(id string) ([]string, error) {\n\tparentIds, err := getParentIDs(a.rootPath(), id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlayers := make([]string, len(parentIds))\n\n\t// Get the diff paths for all the parent ids\n\tfor i, p := range parentIds {\n\t\tlayers[i] = path.Join(a.rootPath(), \"diff\", p)\n\t}\n\treturn layers, nil\n}\n\nfunc (a *Driver) mount(id string, target string, mountLabel string, layers []string) error {\n\t// If the id is mounted or we get an error return\n\tif mounted, err := a.mounted(target); err != nil || mounted {\n\t\treturn err\n\t}\n\n\trw := a.getDiffPath(id)\n\n\tif err := a.aufsMount(layers, rw, target, mountLabel); err != nil {\n\t\treturn fmt.Errorf(\"error creating aufs mount to %s: %v\", target, err)\n\t}\n\treturn nil\n}\n\nfunc (a *Driver) unmount(mountPath string) error {\n\tif mounted, err := a.mounted(mountPath); err != nil || !mounted {\n\t\treturn err\n\t}\n\treturn Unmount(mountPath)\n}\n\nfunc (a *Driver) mounted(mountpoint string) (bool, error) {\n\treturn graphdriver.Mounted(graphdriver.FsMagicAufs, mountpoint)\n}\n\n// Cleanup aufs and unmount all mountpoints\nfunc (a *Driver) Cleanup() error {\n\tdir := a.mntPath()\n\tfiles, err := ioutil.ReadDir(dir)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"aufs readdir error\")\n\t}\n\tfor _, f := range files {\n\t\tif !f.IsDir() {\n\t\t\tcontinue\n\t\t}\n\n\t\tm := path.Join(dir, f.Name())\n\n\t\tif err := a.unmount(m); err != nil {\n\t\t\tlogger.WithError(err).WithField(\"method\", \"Cleanup()\").Warn()\n\t\t}\n\t}\n\treturn mount.RecursiveUnmount(a.root)\n}\n\nfunc (a *Driver) aufsMount(ro []string, rw, target, mountLabel string) (err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tmount.Unmount(target)\n\t\t}\n\t}()\n\n\t// Mount options are clipped to page size(4096 bytes). If there are more\n\t// layers then these are remounted individually using append.\n\n\toffset := 54\n\tif useDirperm() {\n\t\toffset += len(\",dirperm1\")\n\t}\n\tb := make([]byte, unix.Getpagesize()-len(mountLabel)-offset) // room for xino & mountLabel\n\tbp := copy(b, fmt.Sprintf(\"br:%s=rw\", rw))\n\n\tindex := 0\n\tfor ; index < len(ro); index++ {\n\t\tlayer := fmt.Sprintf(\":%s=ro+wh\", ro[index])\n\t\tif bp+len(layer) > len(b) {\n\t\t\tbreak\n\t\t}\n\t\tbp += copy(b[bp:], layer)\n\t}\n\n\topts := \"dio,xino=/dev/shm/aufs.xino\"\n\tif useDirperm() {\n\t\topts += \",dirperm1\"\n\t}\n\tdata := label.FormatMountLabel(fmt.Sprintf(\"%s,%s\", string(b[:bp]), opts), mountLabel)\n\ta.mntL.Lock()\n\terr = unix.Mount(\"none\", target, \"aufs\", 0, data)\n\ta.mntL.Unlock()\n\tif err != nil {\n\t\terr = errors.Wrap(err, \"mount target=\"+target+\" data=\"+data)\n\t\treturn\n\t}\n\n\tfor index < len(ro) {\n\t\tbp = 0\n\t\tfor ; index < len(ro); index++ {\n\t\t\tlayer := fmt.Sprintf(\"append:%s=ro+wh,\", ro[index])\n\t\t\tif bp+len(layer) > len(b) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tbp += copy(b[bp:], layer)\n\t\t}\n\t\tdata := label.FormatMountLabel(string(b[:bp]), mountLabel)\n\t\ta.mntL.Lock()\n\t\terr = unix.Mount(\"none\", target, \"aufs\", unix.MS_REMOUNT, data)\n\t\ta.mntL.Unlock()\n\t\tif err != nil {\n\t\t\terr = errors.Wrap(err, \"mount target=\"+target+\" flags=MS_REMOUNT data=\"+data)\n\t\t\treturn\n\t\t}\n\t}\n\n\treturn\n}\n\n// useDirperm checks dirperm1 mount option can be used with the current\n// version of aufs.\nfunc useDirperm() bool {\n\tenableDirpermLock.Do(func() {\n\t\tbase, err := ioutil.TempDir(\"\", \"docker-aufs-base\")\n\t\tif err != nil {\n\t\t\tlogger.Errorf(\"error checking dirperm1: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.RemoveAll(base)\n\n\t\tunion, err := ioutil.TempDir(\"\", \"docker-aufs-union\")\n\t\tif err != nil {\n\t\t\tlogger.Errorf(\"error checking dirperm1: %v\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer os.RemoveAll(union)\n\n\t\topts := fmt.Sprintf(\"br:%s,dirperm1,xino=/dev/shm/aufs.xino\", base)\n\t\tif err := unix.Mount(\"none\", union, \"aufs\", 0, opts); err != nil {\n\t\t\treturn\n\t\t}\n\t\tenableDirperm = true\n\t\tif err := Unmount(union); err != nil {\n\t\t\tlogger.Errorf(\"error checking dirperm1: failed to unmount %v\", err)\n\t\t}\n\t})\n\treturn enableDirperm\n}\n", "// +build linux\n\npackage btrfs // import \"github.com/docker/docker/daemon/graphdriver/btrfs\"\n\n/*\n#include <stdlib.h>\n#include <dirent.h>\n#include <btrfs/ioctl.h>\n#include <btrfs/ctree.h>\n\nstatic void set_name_btrfs_ioctl_vol_args_v2(struct btrfs_ioctl_vol_args_v2* btrfs_struct, const char* value) {\n    snprintf(btrfs_struct->name, BTRFS_SUBVOL_NAME_MAX, \"%s\", value);\n}\n*/\nimport \"C\"\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\tunits \"github.com/docker/go-units\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nfunc init() {\n\tgraphdriver.Register(\"btrfs\", Init)\n}\n\ntype btrfsOptions struct {\n\tminSpace uint64\n\tsize     uint64\n}\n\n// Init returns a new BTRFS driver.\n// An error is returned if BTRFS is not supported.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\n\t// Perform feature detection on /var/lib/docker/btrfs if it's an existing directory.\n\t// This covers situations where /var/lib/docker/btrfs is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := home\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif fsMagic != graphdriver.FsMagicBtrfs {\n\t\treturn nil, graphdriver.ErrPrerequisites\n\t}\n\n\tif err := idtools.MkdirAllAndChown(home, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\topt, userDiskQuota, err := parseOptions(options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// For some reason shared mount propagation between a container\n\t// and the host does not work for btrfs, and a remedy is to bind\n\t// mount graphdriver home to itself (even without changing the\n\t// propagation mode).\n\terr = mount.MakeMount(home)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to make %s a mount\", home)\n\t}\n\n\tdriver := &Driver{\n\t\thome:    home,\n\t\tuidMaps: uidMaps,\n\t\tgidMaps: gidMaps,\n\t\toptions: opt,\n\t}\n\n\tif userDiskQuota {\n\t\tif err := driver.subvolEnableQuota(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn graphdriver.NewNaiveDiffDriver(driver, uidMaps, gidMaps), nil\n}\n\nfunc parseOptions(opt []string) (btrfsOptions, bool, error) {\n\tvar options btrfsOptions\n\tuserDiskQuota := false\n\tfor _, option := range opt {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn options, userDiskQuota, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"btrfs.min_space\":\n\t\t\tminSpace, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn options, userDiskQuota, err\n\t\t\t}\n\t\t\tuserDiskQuota = true\n\t\t\toptions.minSpace = uint64(minSpace)\n\t\tdefault:\n\t\t\treturn options, userDiskQuota, fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\treturn options, userDiskQuota, nil\n}\n\n// Driver contains information about the filesystem mounted.\ntype Driver struct {\n\t// root of the file system\n\thome         string\n\tuidMaps      []idtools.IDMap\n\tgidMaps      []idtools.IDMap\n\toptions      btrfsOptions\n\tquotaEnabled bool\n\tonce         sync.Once\n}\n\n// String prints the name of the driver (btrfs).\nfunc (d *Driver) String() string {\n\treturn \"btrfs\"\n}\n\n// Status returns current driver information in a two dimensional string array.\n// Output contains \"Build Version\" and \"Library Version\" of the btrfs libraries used.\n// Version information can be used to check compatibility with your kernel.\nfunc (d *Driver) Status() [][2]string {\n\tstatus := [][2]string{}\n\tif bv := btrfsBuildVersion(); bv != \"-\" {\n\t\tstatus = append(status, [2]string{\"Build Version\", bv})\n\t}\n\tif lv := btrfsLibVersion(); lv != -1 {\n\t\tstatus = append(status, [2]string{\"Library Version\", fmt.Sprintf(\"%d\", lv)})\n\t}\n\treturn status\n}\n\n// GetMetadata returns empty metadata for this driver.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn nil, nil\n}\n\n// Cleanup unmounts the home directory.\nfunc (d *Driver) Cleanup() error {\n\terr := d.subvolDisableQuota()\n\tumountErr := mount.Unmount(d.home)\n\n\t// in case we have two errors, prefer the one from disableQuota()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif umountErr != nil {\n\t\treturn umountErr\n\t}\n\n\treturn nil\n}\n\nfunc free(p *C.char) {\n\tC.free(unsafe.Pointer(p))\n}\n\nfunc openDir(path string) (*C.DIR, error) {\n\tCpath := C.CString(path)\n\tdefer free(Cpath)\n\n\tdir := C.opendir(Cpath)\n\tif dir == nil {\n\t\treturn nil, fmt.Errorf(\"Can't open dir\")\n\t}\n\treturn dir, nil\n}\n\nfunc closeDir(dir *C.DIR) {\n\tif dir != nil {\n\t\tC.closedir(dir)\n\t}\n}\n\nfunc getDirFd(dir *C.DIR) uintptr {\n\treturn uintptr(C.dirfd(dir))\n}\n\nfunc subvolCreate(path, name string) error {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_vol_args\n\tfor i, c := range []byte(name) {\n\t\targs.name[i] = C.char(c)\n\t}\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_SUBVOL_CREATE,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to create btrfs subvolume: %v\", errno.Error())\n\t}\n\treturn nil\n}\n\nfunc subvolSnapshot(src, dest, name string) error {\n\tsrcDir, err := openDir(src)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(srcDir)\n\n\tdestDir, err := openDir(dest)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(destDir)\n\n\tvar args C.struct_btrfs_ioctl_vol_args_v2\n\targs.fd = C.__s64(getDirFd(srcDir))\n\n\tvar cs = C.CString(name)\n\tC.set_name_btrfs_ioctl_vol_args_v2(&args, cs)\n\tC.free(unsafe.Pointer(cs))\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(destDir), C.BTRFS_IOC_SNAP_CREATE_V2,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to create btrfs snapshot: %v\", errno.Error())\n\t}\n\treturn nil\n}\n\nfunc isSubvolume(p string) (bool, error) {\n\tvar bufStat unix.Stat_t\n\tif err := unix.Lstat(p, &bufStat); err != nil {\n\t\treturn false, err\n\t}\n\n\t// return true if it is a btrfs subvolume\n\treturn bufStat.Ino == C.BTRFS_FIRST_FREE_OBJECTID, nil\n}\n\nfunc subvolDelete(dirpath, name string, quotaEnabled bool) error {\n\tdir, err := openDir(dirpath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\tfullPath := path.Join(dirpath, name)\n\n\tvar args C.struct_btrfs_ioctl_vol_args\n\n\t// walk the btrfs subvolumes\n\twalkSubvolumes := func(p string, f os.FileInfo, err error) error {\n\t\tif err != nil {\n\t\t\tif os.IsNotExist(err) && p != fullPath {\n\t\t\t\t// missing most likely because the path was a subvolume that got removed in the previous iteration\n\t\t\t\t// since it's gone anyway, we don't care\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"error walking subvolumes: %v\", err)\n\t\t}\n\t\t// we want to check children only so skip itself\n\t\t// it will be removed after the filepath walk anyways\n\t\tif f.IsDir() && p != fullPath {\n\t\t\tsv, err := isSubvolume(p)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"Failed to test if %s is a btrfs subvolume: %v\", p, err)\n\t\t\t}\n\t\t\tif sv {\n\t\t\t\tif err := subvolDelete(path.Dir(p), f.Name(), quotaEnabled); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"Failed to destroy btrfs child subvolume (%s) of parent (%s): %v\", p, dirpath, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\tif err := filepath.Walk(path.Join(dirpath, name), walkSubvolumes); err != nil {\n\t\treturn fmt.Errorf(\"Recursively walking subvolumes for %s failed: %v\", dirpath, err)\n\t}\n\n\tif quotaEnabled {\n\t\tif qgroupid, err := subvolLookupQgroup(fullPath); err == nil {\n\t\t\tvar args C.struct_btrfs_ioctl_qgroup_create_args\n\t\t\targs.qgroupid = C.__u64(qgroupid)\n\n\t\t\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QGROUP_CREATE,\n\t\t\t\tuintptr(unsafe.Pointer(&args)))\n\t\t\tif errno != 0 {\n\t\t\t\tlogrus.WithField(\"storage-driver\", \"btrfs\").Errorf(\"Failed to delete btrfs qgroup %v for %s: %v\", qgroupid, fullPath, errno.Error())\n\t\t\t}\n\t\t} else {\n\t\t\tlogrus.WithField(\"storage-driver\", \"btrfs\").Errorf(\"Failed to lookup btrfs qgroup for %s: %v\", fullPath, err.Error())\n\t\t}\n\t}\n\n\t// all subvolumes have been removed\n\t// now remove the one originally passed in\n\tfor i, c := range []byte(name) {\n\t\targs.name[i] = C.char(c)\n\t}\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_SNAP_DESTROY,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to destroy btrfs snapshot %s for %s: %v\", dirpath, name, errno.Error())\n\t}\n\treturn nil\n}\n\nfunc (d *Driver) updateQuotaStatus() {\n\td.once.Do(func() {\n\t\tif !d.quotaEnabled {\n\t\t\t// In case quotaEnabled is not set, check qgroup and update quotaEnabled as needed\n\t\t\tif err := subvolQgroupStatus(d.home); err != nil {\n\t\t\t\t// quota is still not enabled\n\t\t\t\treturn\n\t\t\t}\n\t\t\td.quotaEnabled = true\n\t\t}\n\t})\n}\n\nfunc (d *Driver) subvolEnableQuota() error {\n\td.updateQuotaStatus()\n\n\tif d.quotaEnabled {\n\t\treturn nil\n\t}\n\n\tdir, err := openDir(d.home)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_quota_ctl_args\n\targs.cmd = C.BTRFS_QUOTA_CTL_ENABLE\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QUOTA_CTL,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to enable btrfs quota for %s: %v\", dir, errno.Error())\n\t}\n\n\td.quotaEnabled = true\n\n\treturn nil\n}\n\nfunc (d *Driver) subvolDisableQuota() error {\n\td.updateQuotaStatus()\n\n\tif !d.quotaEnabled {\n\t\treturn nil\n\t}\n\n\tdir, err := openDir(d.home)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_quota_ctl_args\n\targs.cmd = C.BTRFS_QUOTA_CTL_DISABLE\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QUOTA_CTL,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to disable btrfs quota for %s: %v\", dir, errno.Error())\n\t}\n\n\td.quotaEnabled = false\n\n\treturn nil\n}\n\nfunc (d *Driver) subvolRescanQuota() error {\n\td.updateQuotaStatus()\n\n\tif !d.quotaEnabled {\n\t\treturn nil\n\t}\n\n\tdir, err := openDir(d.home)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_quota_rescan_args\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QUOTA_RESCAN_WAIT,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to rescan btrfs quota for %s: %v\", dir, errno.Error())\n\t}\n\n\treturn nil\n}\n\nfunc subvolLimitQgroup(path string, size uint64) error {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_qgroup_limit_args\n\targs.lim.max_referenced = C.__u64(size)\n\targs.lim.flags = C.BTRFS_QGROUP_LIMIT_MAX_RFER\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_QGROUP_LIMIT,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to limit qgroup for %s: %v\", dir, errno.Error())\n\t}\n\n\treturn nil\n}\n\n// subvolQgroupStatus performs a BTRFS_IOC_TREE_SEARCH on the root path\n// with search key of BTRFS_QGROUP_STATUS_KEY.\n// In case qgroup is enabled, the retuned key type will match BTRFS_QGROUP_STATUS_KEY.\n// For more details please see https://github.com/kdave/btrfs-progs/blob/v4.9/qgroup.c#L1035\nfunc subvolQgroupStatus(path string) error {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_search_args\n\targs.key.tree_id = C.BTRFS_QUOTA_TREE_OBJECTID\n\targs.key.min_type = C.BTRFS_QGROUP_STATUS_KEY\n\targs.key.max_type = C.BTRFS_QGROUP_STATUS_KEY\n\targs.key.max_objectid = C.__u64(math.MaxUint64)\n\targs.key.max_offset = C.__u64(math.MaxUint64)\n\targs.key.max_transid = C.__u64(math.MaxUint64)\n\targs.key.nr_items = 4096\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_TREE_SEARCH,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn fmt.Errorf(\"Failed to search qgroup for %s: %v\", path, errno.Error())\n\t}\n\tsh := (*C.struct_btrfs_ioctl_search_header)(unsafe.Pointer(&args.buf))\n\tif sh._type != C.BTRFS_QGROUP_STATUS_KEY {\n\t\treturn fmt.Errorf(\"Invalid qgroup search header type for %s: %v\", path, sh._type)\n\t}\n\treturn nil\n}\n\nfunc subvolLookupQgroup(path string) (uint64, error) {\n\tdir, err := openDir(path)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer closeDir(dir)\n\n\tvar args C.struct_btrfs_ioctl_ino_lookup_args\n\targs.objectid = C.BTRFS_FIRST_FREE_OBJECTID\n\n\t_, _, errno := unix.Syscall(unix.SYS_IOCTL, getDirFd(dir), C.BTRFS_IOC_INO_LOOKUP,\n\t\tuintptr(unsafe.Pointer(&args)))\n\tif errno != 0 {\n\t\treturn 0, fmt.Errorf(\"Failed to lookup qgroup for %s: %v\", dir, errno.Error())\n\t}\n\tif args.treeid == 0 {\n\t\treturn 0, fmt.Errorf(\"Invalid qgroup id for %s: 0\", dir)\n\t}\n\n\treturn uint64(args.treeid), nil\n}\n\nfunc (d *Driver) subvolumesDir() string {\n\treturn path.Join(d.home, \"subvolumes\")\n}\n\nfunc (d *Driver) subvolumesDirID(id string) string {\n\treturn path.Join(d.subvolumesDir(), id)\n}\n\nfunc (d *Driver) quotasDir() string {\n\treturn path.Join(d.home, \"quotas\")\n}\n\nfunc (d *Driver) quotasDirID(id string) string {\n\treturn path.Join(d.quotasDir(), id)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn d.Create(id, parent, opts)\n}\n\n// Create the filesystem with given id.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\tquotas := path.Join(d.home, \"quotas\")\n\tsubvolumes := path.Join(d.home, \"subvolumes\")\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAllAndChown(subvolumes, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn err\n\t}\n\tif parent == \"\" {\n\t\tif err := subvolCreate(subvolumes, id); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tparentDir := d.subvolumesDirID(parent)\n\t\tst, err := os.Stat(parentDir)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif !st.IsDir() {\n\t\t\treturn fmt.Errorf(\"%s: not a directory\", parentDir)\n\t\t}\n\t\tif err := subvolSnapshot(parentDir, subvolumes, id); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tvar storageOpt map[string]string\n\tif opts != nil {\n\t\tstorageOpt = opts.StorageOpt\n\t}\n\n\tif _, ok := storageOpt[\"size\"]; ok {\n\t\tdriver := &Driver{}\n\t\tif err := d.parseStorageOpt(storageOpt, driver); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := d.setStorageSize(path.Join(subvolumes, id), driver); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := idtools.MkdirAllAndChown(quotas, 0700, idtools.CurrentIdentity()); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := ioutil.WriteFile(path.Join(quotas, id), []byte(fmt.Sprint(driver.options.size)), 0644); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// if we have a remapped root (user namespaces enabled), change the created snapshot\n\t// dir ownership to match\n\tif rootUID != 0 || rootGID != 0 {\n\t\tif err := os.Chown(path.Join(subvolumes, id), rootUID, rootGID); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tmountLabel := \"\"\n\tif opts != nil {\n\t\tmountLabel = opts.MountLabel\n\t}\n\n\treturn label.Relabel(path.Join(subvolumes, id), mountLabel, false)\n}\n\n// Parse btrfs storage options\nfunc (d *Driver) parseStorageOpt(storageOpt map[string]string, driver *Driver) error {\n\t// Read size to change the subvolume disk quota per container\n\tfor key, val := range storageOpt {\n\t\tkey := strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdriver.options.size = uint64(size)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Set btrfs storage size\nfunc (d *Driver) setStorageSize(dir string, driver *Driver) error {\n\tif driver.options.size == 0 {\n\t\treturn fmt.Errorf(\"btrfs: invalid storage size: %s\", units.HumanSize(float64(driver.options.size)))\n\t}\n\tif d.options.minSpace > 0 && driver.options.size < d.options.minSpace {\n\t\treturn fmt.Errorf(\"btrfs: storage size cannot be less than %s\", units.HumanSize(float64(d.options.minSpace)))\n\t}\n\tif err := d.subvolEnableQuota(); err != nil {\n\t\treturn err\n\t}\n\treturn subvolLimitQgroup(dir, driver.options.size)\n}\n\n// Remove the filesystem with given id.\nfunc (d *Driver) Remove(id string) error {\n\tdir := d.subvolumesDirID(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn err\n\t}\n\tquotasDir := d.quotasDirID(id)\n\tif _, err := os.Stat(quotasDir); err == nil {\n\t\tif err := os.Remove(quotasDir); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\n\t// Call updateQuotaStatus() to invoke status update\n\td.updateQuotaStatus()\n\n\tif err := subvolDelete(d.subvolumesDir(), id, d.quotaEnabled); err != nil {\n\t\treturn err\n\t}\n\tif err := system.EnsureRemoveAll(dir); err != nil {\n\t\treturn err\n\t}\n\treturn d.subvolRescanQuota()\n}\n\n// Get the requested filesystem id.\nfunc (d *Driver) Get(id, mountLabel string) (containerfs.ContainerFS, error) {\n\tdir := d.subvolumesDirID(id)\n\tst, err := os.Stat(dir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif !st.IsDir() {\n\t\treturn nil, fmt.Errorf(\"%s: not a directory\", dir)\n\t}\n\n\tif quota, err := ioutil.ReadFile(d.quotasDirID(id)); err == nil {\n\t\tif size, err := strconv.ParseUint(string(quota), 10, 64); err == nil && size >= d.options.minSpace {\n\t\t\tif err := d.subvolEnableQuota(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif err := subvolLimitQgroup(dir, size); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn containerfs.NewLocalContainerFS(dir), nil\n}\n\n// Put is not implemented for BTRFS as there is no cleanup required for the id.\nfunc (d *Driver) Put(id string) error {\n\t// Get() creates no runtime resources (like e.g. mounts)\n\t// so this doesn't need to do anything.\n\treturn nil\n}\n\n// Exists checks if the id exists in the filesystem.\nfunc (d *Driver) Exists(id string) bool {\n\tdir := d.subvolumesDirID(id)\n\t_, err := os.Stat(dir)\n\treturn err == nil\n}\n", "// +build linux\n\npackage fuseoverlayfs // import \"github.com/docker/docker/daemon/graphdriver/fuse-overlayfs\"\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/daemon/graphdriver/overlayutils\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/chrootarchive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/directory\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers/kernel\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t// untar defines the untar method\n\tuntar = chrootarchive.UntarUncompressed\n)\n\nconst (\n\tdriverName    = \"fuse-overlayfs\"\n\tbinary        = \"fuse-overlayfs\"\n\tlinkDir       = \"l\"\n\tdiffDirName   = \"diff\"\n\tworkDirName   = \"work\"\n\tmergedDirName = \"merged\"\n\tlowerFile     = \"lower\"\n\tmaxDepth      = 128\n\n\t// idLength represents the number of random characters\n\t// which can be used to create the unique link identifier\n\t// for every layer. If this value is too long then the\n\t// page size limit for the mount command may be exceeded.\n\t// The idLength should be selected such that following equation\n\t// is true (512 is a buffer for label metadata).\n\t// ((idLength + len(linkDir) + 1) * maxDepth) <= (pageSize - 512)\n\tidLength = 26\n)\n\n// Driver contains information about the home directory and the list of active\n// mounts that are created using this driver.\ntype Driver struct {\n\thome      string\n\tuidMaps   []idtools.IDMap\n\tgidMaps   []idtools.IDMap\n\tctr       *graphdriver.RefCounter\n\tnaiveDiff graphdriver.DiffDriver\n\tlocker    *locker.Locker\n}\n\nvar (\n\tlogger = logrus.WithField(\"storage-driver\", driverName)\n)\n\nfunc init() {\n\tgraphdriver.Register(driverName, Init)\n}\n\n// Init returns the naive diff driver for fuse-overlayfs.\n// If fuse-overlayfs is not supported on the host, the error\n// graphdriver.ErrNotSupported is returned.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\tif _, err := exec.LookPath(binary); err != nil {\n\t\tlogger.Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\tif !kernel.CheckKernelVersion(4, 18, 0) {\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\tif err := idtools.MkdirAllAndChown(path.Join(home, linkDir), 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\td := &Driver{\n\t\thome:    home,\n\t\tuidMaps: uidMaps,\n\t\tgidMaps: gidMaps,\n\t\tctr:     graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicFUSE)),\n\t\tlocker:  locker.New(),\n\t}\n\n\td.naiveDiff = graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps)\n\n\treturn d, nil\n}\n\nfunc (d *Driver) String() string {\n\treturn driverName\n}\n\n// Status returns current driver information in a two dimensional string array.\nfunc (d *Driver) Status() [][2]string {\n\treturn [][2]string{}\n}\n\n// GetMetadata returns metadata about the overlay driver such as the LowerDir,\n// UpperDir, WorkDir, and MergeDir used to store data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := map[string]string{\n\t\t\"WorkDir\":   path.Join(dir, workDirName),\n\t\t\"MergedDir\": path.Join(dir, mergedDirName),\n\t\t\"UpperDir\":  path.Join(dir, diffDirName),\n\t}\n\n\tlowerDirs, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(lowerDirs) > 0 {\n\t\tmetadata[\"LowerDir\"] = strings.Join(lowerDirs, \":\")\n\t}\n\n\treturn metadata, nil\n}\n\n// Cleanup any state created by overlay which should be cleaned when daemon\n// is being shutdown. For now, we just have to unmount the bind mounted\n// we had created.\nfunc (d *Driver) Cleanup() error {\n\treturn mount.RecursiveUnmount(d.home)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported\")\n\t}\n\treturn d.create(id, parent, opts)\n}\n\n// Create is used to create the upper, lower, and merge directories required for overlay fs for a given id.\n// The parent filesystem is used to configure these directories for the overlay.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported\")\n\t}\n\treturn d.create(id, parent, opts)\n}\n\nfunc (d *Driver) create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tdir := d.dir(id)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\troot := idtools.Identity{UID: rootUID, GID: rootGID}\n\n\tcurrentID := idtools.CurrentIdentity()\n\tif err := idtools.MkdirAllAndChown(path.Dir(dir), 0701, currentID); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0701, currentID); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Clean up on failure\n\t\tif retErr != nil {\n\t\t\tos.RemoveAll(dir)\n\t\t}\n\t}()\n\n\tif opts != nil && len(opts.StorageOpt) > 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported\")\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, diffDirName), 0755, root); err != nil {\n\t\treturn err\n\t}\n\n\tlid := overlayutils.GenerateID(idLength, logger)\n\tif err := os.Symlink(path.Join(\"..\", id, diffDirName), path.Join(d.home, linkDir, lid)); err != nil {\n\t\treturn err\n\t}\n\n\t// Write link id to link file\n\tif err := ioutil.WriteFile(path.Join(dir, \"link\"), []byte(lid), 0644); err != nil {\n\t\treturn err\n\t}\n\n\t// if no parent directory, done\n\tif parent == \"\" {\n\t\treturn nil\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, workDirName), 0701, currentID); err != nil {\n\t\treturn err\n\t}\n\n\tif err := ioutil.WriteFile(path.Join(d.dir(parent), \"committed\"), []byte{}, 0600); err != nil {\n\t\treturn err\n\t}\n\n\tlower, err := d.getLower(parent)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif lower != \"\" {\n\t\tif err := ioutil.WriteFile(path.Join(dir, lowerFile), []byte(lower), 0666); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (d *Driver) getLower(parent string) (string, error) {\n\tparentDir := d.dir(parent)\n\n\t// Ensure parent exists\n\tif _, err := os.Lstat(parentDir); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Read Parent link fileA\n\tparentLink, err := ioutil.ReadFile(path.Join(parentDir, \"link\"))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tlowers := []string{path.Join(linkDir, string(parentLink))}\n\n\tparentLower, err := ioutil.ReadFile(path.Join(parentDir, lowerFile))\n\tif err == nil {\n\t\tparentLowers := strings.Split(string(parentLower), \":\")\n\t\tlowers = append(lowers, parentLowers...)\n\t}\n\tif len(lowers) > maxDepth {\n\t\treturn \"\", errors.New(\"max depth exceeded\")\n\t}\n\treturn strings.Join(lowers, \":\"), nil\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn path.Join(d.home, id)\n}\n\nfunc (d *Driver) getLowerDirs(id string) ([]string, error) {\n\tvar lowersArray []string\n\tlowers, err := ioutil.ReadFile(path.Join(d.dir(id), lowerFile))\n\tif err == nil {\n\t\tfor _, s := range strings.Split(string(lowers), \":\") {\n\t\t\tlp, err := os.Readlink(path.Join(d.home, s))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlowersArray = append(lowersArray, path.Clean(path.Join(d.home, linkDir, lp)))\n\t\t}\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\treturn lowersArray, nil\n}\n\n// Remove cleans the directories that are created for this id.\nfunc (d *Driver) Remove(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"refusing to remove the directories: id is empty\")\n\t}\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tlid, err := ioutil.ReadFile(path.Join(dir, \"link\"))\n\tif err == nil {\n\t\tif len(lid) == 0 {\n\t\t\tlogger.Errorf(\"refusing to remove empty link for layer %v\", id)\n\t\t} else if err := os.RemoveAll(path.Join(d.home, linkDir, string(lid))); err != nil {\n\t\t\tlogger.Debugf(\"Failed to remove link: %v\", err)\n\t\t}\n\t}\n\n\tif err := system.EnsureRemoveAll(dir); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Get creates and mounts the required file system for the given id and returns the mount path.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, retErr error) {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdiffDir := path.Join(dir, diffDirName)\n\tlowers, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, just return diff directory\n\t\tif os.IsNotExist(err) {\n\t\t\treturn containerfs.NewLocalContainerFS(diffDir), nil\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tmergedDir := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Increment(mergedDir); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif c := d.ctr.Decrement(mergedDir); c <= 0 {\n\t\t\t\tif unmounted := fusermountU(mergedDir); !unmounted {\n\t\t\t\t\tif mntErr := unix.Unmount(mergedDir, 0); mntErr != nil {\n\t\t\t\t\t\tlogger.Errorf(\"error unmounting %v: %v\", mergedDir, mntErr)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// Cleanup the created merged directory; see the comment in Put's rmdir\n\t\t\t\tif rmErr := unix.Rmdir(mergedDir); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogger.Debugf(\"Failed to remove %s: %v: %v\", id, rmErr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tworkDir := path.Join(dir, workDirName)\n\tsplitLowers := strings.Split(string(lowers), \":\")\n\tabsLowers := make([]string, len(splitLowers))\n\tfor i, s := range splitLowers {\n\t\tabsLowers[i] = path.Join(d.home, s)\n\t}\n\tvar readonly bool\n\tif _, err := os.Stat(path.Join(dir, \"committed\")); err == nil {\n\t\treadonly = true\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tvar opts string\n\tif readonly {\n\t\topts = \"lowerdir=\" + diffDir + \":\" + strings.Join(absLowers, \":\")\n\t} else {\n\t\topts = \"lowerdir=\" + strings.Join(absLowers, \":\") + \",upperdir=\" + diffDir + \",workdir=\" + workDir\n\t}\n\n\tmountData := label.FormatMountLabel(opts, mountLabel)\n\tmountTarget := mergedDir\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(mergedDir, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmountProgram := exec.Command(binary, \"-o\", mountData, mountTarget)\n\tmountProgram.Dir = d.home\n\tvar b bytes.Buffer\n\tmountProgram.Stderr = &b\n\tif err = mountProgram.Run(); err != nil {\n\t\toutput := b.String()\n\t\tif output == \"\" {\n\t\t\toutput = \"<stderr empty>\"\n\t\t}\n\t\treturn nil, errors.Wrapf(err, \"using mount program %s: %s\", binary, output)\n\t}\n\n\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n}\n\n// Put unmounts the mount path created for the give id.\n// It also removes the 'merged' directory to force the kernel to unmount the\n// overlay mount in other namespaces.\nfunc (d *Driver) Put(id string) error {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\t_, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, no mount happened and just return directly\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\n\tmountpoint := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\tif unmounted := fusermountU(mountpoint); !unmounted {\n\t\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\t\tlogger.Debugf(\"Failed to unmount %s overlay: %s - %v\", id, mountpoint, err)\n\t\t}\n\t}\n\t// Remove the mountpoint here. Removing the mountpoint (in newer kernels)\n\t// will cause all other instances of this mount in other mount namespaces\n\t// to be unmounted. This is necessary to avoid cases where an overlay mount\n\t// that is present in another namespace will cause subsequent mounts\n\t// operations to fail with ebusy.  We ignore any errors here because this may\n\t// fail on older kernels which don't have\n\t// torvalds/linux@8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe applied.\n\tif err := unix.Rmdir(mountpoint); err != nil && !os.IsNotExist(err) {\n\t\tlogger.Debugf(\"Failed to remove %s overlay: %v\", id, err)\n\t}\n\treturn nil\n}\n\n// Exists checks to see if the id is already mounted.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n\n// isParent determines whether the given parent is the direct parent of the\n// given layer id\nfunc (d *Driver) isParent(id, parent string) bool {\n\tlowers, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif parent == \"\" && len(lowers) > 0 {\n\t\treturn false\n\t}\n\n\tparentDir := d.dir(parent)\n\tvar ld string\n\tif len(lowers) > 0 {\n\t\tld = filepath.Dir(lowers[0])\n\t}\n\tif ld == \"\" && parent == \"\" {\n\t\treturn true\n\t}\n\treturn ld == parentDir\n}\n\n// ApplyDiff applies the new layer into a root\nfunc (d *Driver) ApplyDiff(id string, parent string, diff io.Reader) (size int64, err error) {\n\tif !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.ApplyDiff(id, parent, diff)\n\t}\n\n\tapplyDir := d.getDiffPath(id)\n\n\tlogger.Debugf(\"Applying tar in %s\", applyDir)\n\t// Overlay doesn't need the parent id to apply the diff\n\tif err := untar(diff, applyDir, &archive.TarOptions{\n\t\tUIDMaps: d.uidMaps,\n\t\tGIDMaps: d.gidMaps,\n\t\t// Use AUFS whiteout format: https://github.com/containers/storage/blob/39a8d5ed9843844eafb5d2ba6e6a7510e0126f40/drivers/overlay/overlay.go#L1084-L1089\n\t\tWhiteoutFormat: archive.AUFSWhiteoutFormat,\n\t\tInUserNS:       sys.RunningInUserNS(),\n\t}); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn directory.Size(context.TODO(), applyDir)\n}\n\nfunc (d *Driver) getDiffPath(id string) string {\n\tdir := d.dir(id)\n\n\treturn path.Join(dir, diffDirName)\n}\n\n// DiffSize calculates the changes between the specified id\n// and its parent and returns the size in bytes of the changes\n// relative to its base filesystem directory.\nfunc (d *Driver) DiffSize(id, parent string) (size int64, err error) {\n\treturn d.naiveDiff.DiffSize(id, parent)\n}\n\n// Diff produces an archive of the changes between the specified\n// layer and its parent layer which may be \"\".\nfunc (d *Driver) Diff(id, parent string) (io.ReadCloser, error) {\n\treturn d.naiveDiff.Diff(id, parent)\n}\n\n// Changes produces a list of changes between the specified layer and its\n// parent layer. If parent is \"\", then all changes will be ADD changes.\nfunc (d *Driver) Changes(id, parent string) ([]archive.Change, error) {\n\treturn d.naiveDiff.Changes(id, parent)\n}\n\n// fusermountU is from https://github.com/containers/storage/blob/39a8d5ed9843844eafb5d2ba6e6a7510e0126f40/drivers/overlay/overlay.go#L1016-L1040\nfunc fusermountU(mountpoint string) (unmounted bool) {\n\t// Attempt to unmount the FUSE mount using either fusermount or fusermount3.\n\t// If they fail, fallback to unix.Unmount\n\tfor _, v := range []string{\"fusermount3\", \"fusermount\"} {\n\t\terr := exec.Command(v, \"-u\", mountpoint).Run()\n\t\tif err != nil && !os.IsNotExist(err) {\n\t\t\tlogrus.Debugf(\"Error unmounting %s with %s - %v\", mountpoint, v, err)\n\t\t}\n\t\tif err == nil {\n\t\t\tunmounted = true\n\t\t\tbreak\n\t\t}\n\t}\n\t// If fusermount|fusermount3 failed to unmount the FUSE file system, make sure all\n\t// pending changes are propagated to the file system\n\tif !unmounted {\n\t\tfd, err := unix.Open(mountpoint, unix.O_DIRECTORY, 0)\n\t\tif err == nil {\n\t\t\tif err := unix.Syncfs(fd); err != nil {\n\t\t\t\tlogrus.Debugf(\"Error Syncfs(%s) - %v\", mountpoint, err)\n\t\t\t}\n\t\t\tunix.Close(fd)\n\t\t}\n\t}\n\treturn\n}\n", "// +build linux\n\npackage overlay // import \"github.com/docker/docker/daemon/graphdriver/overlay\"\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/daemon/graphdriver/copy\"\n\t\"github.com/docker/docker/daemon/graphdriver/overlayutils\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/fsutils\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\n// This is a small wrapper over the NaiveDiffWriter that lets us have a custom\n// implementation of ApplyDiff()\n\nvar (\n\t// ErrApplyDiffFallback is returned to indicate that a normal ApplyDiff is applied as a fallback from Naive diff writer.\n\tErrApplyDiffFallback = fmt.Errorf(\"Fall back to normal ApplyDiff\")\n\tbackingFs            = \"<unknown>\"\n)\n\n// ApplyDiffProtoDriver wraps the ProtoDriver by extending the interface with ApplyDiff method.\ntype ApplyDiffProtoDriver interface {\n\tgraphdriver.ProtoDriver\n\t// ApplyDiff writes the diff to the archive for the given id and parent id.\n\t// It returns the size in bytes written if successful, an error ErrApplyDiffFallback is returned otherwise.\n\tApplyDiff(id, parent string, diff io.Reader) (size int64, err error)\n}\n\ntype naiveDiffDriverWithApply struct {\n\tgraphdriver.Driver\n\tapplyDiff ApplyDiffProtoDriver\n}\n\n// NaiveDiffDriverWithApply returns a NaiveDiff driver with custom ApplyDiff.\nfunc NaiveDiffDriverWithApply(driver ApplyDiffProtoDriver, uidMaps, gidMaps []idtools.IDMap) graphdriver.Driver {\n\treturn &naiveDiffDriverWithApply{\n\t\tDriver:    graphdriver.NewNaiveDiffDriver(driver, uidMaps, gidMaps),\n\t\tapplyDiff: driver,\n\t}\n}\n\n// ApplyDiff creates a diff layer with either the NaiveDiffDriver or with a fallback.\nfunc (d *naiveDiffDriverWithApply) ApplyDiff(id, parent string, diff io.Reader) (int64, error) {\n\tb, err := d.applyDiff.ApplyDiff(id, parent, diff)\n\tif err == ErrApplyDiffFallback {\n\t\treturn d.Driver.ApplyDiff(id, parent, diff)\n\t}\n\treturn b, err\n}\n\n// This backend uses the overlay union filesystem for containers\n// plus hard link file sharing for images.\n\n// Each container/image can have a \"root\" subdirectory which is a plain\n// filesystem hierarchy, or they can use overlay.\n\n// If they use overlay there is a \"upper\" directory and a \"lower-id\"\n// file, as well as \"merged\" and \"work\" directories. The \"upper\"\n// directory has the upper layer of the overlay, and \"lower-id\" contains\n// the id of the parent whose \"root\" directory shall be used as the lower\n// layer in the overlay. The overlay itself is mounted in the \"merged\"\n// directory, and the \"work\" dir is needed for overlay to work.\n\n// When an overlay layer is created there are two cases, either the\n// parent has a \"root\" dir, then we start out with an empty \"upper\"\n// directory overlaid on the parents root. This is typically the\n// case with the init layer of a container which is based on an image.\n// If there is no \"root\" in the parent, we inherit the lower-id from\n// the parent and start by making a copy in the parent's \"upper\" dir.\n// This is typically the case for a container layer which copies\n// its parent -init upper layer.\n\n// Additionally we also have a custom implementation of ApplyLayer\n// which makes a recursive copy of the parent \"root\" layer using\n// hardlinks to share file data, and then applies the layer on top\n// of that. This means all child images share file (but not directory)\n// data with the parent.\n\ntype overlayOptions struct{}\n\n// Driver contains information about the home directory and the list of active mounts that are created using this driver.\ntype Driver struct {\n\thome          string\n\tuidMaps       []idtools.IDMap\n\tgidMaps       []idtools.IDMap\n\tctr           *graphdriver.RefCounter\n\tsupportsDType bool\n\tlocker        *locker.Locker\n}\n\nfunc init() {\n\tgraphdriver.Register(\"overlay\", Init)\n}\n\n// Init returns the NaiveDiffDriver, a native diff driver for overlay filesystem.\n// If overlay filesystem is not supported on the host, the error\n// graphdriver.ErrNotSupported is returned.\n// If an overlay filesystem is not supported over an existing filesystem then\n// error graphdriver.ErrIncompatibleFS is returned.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\t_, err := parseOptions(options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Perform feature detection on /var/lib/docker/overlay if it's an existing directory.\n\t// This covers situations where /var/lib/docker/overlay is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := home\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tif err := overlayutils.SupportsOverlay(testdir, false); err != nil {\n\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif fsName, ok := graphdriver.FsNames[fsMagic]; ok {\n\t\tbackingFs = fsName\n\t}\n\n\tsupportsDType, err := fsutils.SupportsDType(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !supportsDType {\n\t\tif !graphdriver.IsInitialized(home) {\n\t\t\treturn nil, overlayutils.ErrDTypeNotSupported(\"overlay\", backingFs)\n\t\t}\n\t\t// allow running without d_type only for existing setups (#27443)\n\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Warn(overlayutils.ErrDTypeNotSupported(\"overlay\", backingFs))\n\t}\n\n\t// Create the driver home dir\n\tif err := idtools.MkdirAllAndChown(home, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\td := &Driver{\n\t\thome:          home,\n\t\tuidMaps:       uidMaps,\n\t\tgidMaps:       gidMaps,\n\t\tctr:           graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicOverlay)),\n\t\tsupportsDType: supportsDType,\n\t\tlocker:        locker.New(),\n\t}\n\n\treturn NaiveDiffDriverWithApply(d, uidMaps, gidMaps), nil\n}\n\nfunc parseOptions(options []string) (*overlayOptions, error) {\n\to := &overlayOptions{}\n\tfor _, option := range options {\n\t\tkey, _, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"overlay: unknown option %s\", key)\n\t\t}\n\t}\n\treturn o, nil\n}\n\nfunc (d *Driver) String() string {\n\treturn \"overlay\"\n}\n\n// Status returns current driver information in a two dimensional string array.\n// Output contains \"Backing Filesystem\" used in this implementation.\nfunc (d *Driver) Status() [][2]string {\n\treturn [][2]string{\n\t\t{\"Backing Filesystem\", backingFs},\n\t\t{\"Supports d_type\", strconv.FormatBool(d.supportsDType)},\n\t}\n}\n\n// GetMetadata returns metadata about the overlay driver such as root,\n// LowerDir, UpperDir, WorkDir and MergeDir used to store data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := make(map[string]string)\n\n\t// If id has a root, it is an image\n\trootDir := path.Join(dir, \"root\")\n\tif _, err := os.Stat(rootDir); err == nil {\n\t\tmetadata[\"RootDir\"] = rootDir\n\t\treturn metadata, nil\n\t}\n\n\tlowerID, err := ioutil.ReadFile(path.Join(dir, \"lower-id\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata[\"LowerDir\"] = path.Join(d.dir(string(lowerID)), \"root\")\n\tmetadata[\"UpperDir\"] = path.Join(dir, \"upper\")\n\tmetadata[\"WorkDir\"] = path.Join(dir, \"work\")\n\tmetadata[\"MergedDir\"] = path.Join(dir, \"merged\")\n\n\treturn metadata, nil\n}\n\n// Cleanup any state created by overlay which should be cleaned when daemon\n// is being shutdown. For now, we just have to unmount the bind mounted\n// we had created.\nfunc (d *Driver) Cleanup() error {\n\treturn mount.RecursiveUnmount(d.home)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn d.Create(id, parent, opts)\n}\n\n// Create is used to create the upper, lower, and merge directories required for overlay fs for a given id.\n// The parent filesystem is used to configure these directories for the overlay.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported for overlay\")\n\t}\n\n\tdir := d.dir(id)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\troot := idtools.Identity{UID: rootUID, GID: rootGID}\n\n\tcurrentID := idtools.CurrentIdentity()\n\tif err := idtools.MkdirAllAndChown(path.Dir(dir), 0701, currentID); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0701, currentID); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Clean up on failure\n\t\tif retErr != nil {\n\t\t\tos.RemoveAll(dir)\n\t\t}\n\t}()\n\n\t// Toplevel images are just a \"root\" dir\n\tif parent == \"\" {\n\t\t// This must be 0755 otherwise unprivileged users will in the container will not be able to read / in the container\n\t\treturn idtools.MkdirAndChown(path.Join(dir, \"root\"), 0755, root)\n\t}\n\n\tparentDir := d.dir(parent)\n\n\t// Ensure parent exists\n\tif _, err := os.Lstat(parentDir); err != nil {\n\t\treturn err\n\t}\n\n\t// If parent has a root, just do an overlay to it\n\tparentRoot := path.Join(parentDir, \"root\")\n\n\tif s, err := os.Lstat(parentRoot); err == nil {\n\t\tif err := idtools.MkdirAndChown(path.Join(dir, \"upper\"), s.Mode(), root); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := idtools.MkdirAndChown(path.Join(dir, \"work\"), 0700, root); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn ioutil.WriteFile(path.Join(dir, \"lower-id\"), []byte(parent), 0600)\n\t}\n\n\t// Otherwise, copy the upper and the lower-id from the parent\n\n\tlowerID, err := ioutil.ReadFile(path.Join(parentDir, \"lower-id\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := ioutil.WriteFile(path.Join(dir, \"lower-id\"), lowerID, 0600); err != nil {\n\t\treturn err\n\t}\n\n\tparentUpperDir := path.Join(parentDir, \"upper\")\n\ts, err := os.Lstat(parentUpperDir)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tupperDir := path.Join(dir, \"upper\")\n\tif err := idtools.MkdirAndChown(upperDir, s.Mode(), root); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(path.Join(dir, \"work\"), 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\treturn copy.DirCopy(parentUpperDir, upperDir, copy.Content, true)\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn path.Join(d.home, id)\n}\n\n// Remove cleans the directories that are created for this id.\nfunc (d *Driver) Remove(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"refusing to remove the directories: id is empty\")\n\t}\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\treturn system.EnsureRemoveAll(d.dir(id))\n}\n\n// Get creates and mounts the required file system for the given id and returns the mount path.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, err error) {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\t// If id has a root, just return it\n\trootDir := path.Join(dir, \"root\")\n\tif _, err := os.Stat(rootDir); err == nil {\n\t\treturn containerfs.NewLocalContainerFS(rootDir), nil\n\t}\n\n\tmergedDir := path.Join(dir, \"merged\")\n\tif count := d.ctr.Increment(mergedDir); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tif c := d.ctr.Decrement(mergedDir); c <= 0 {\n\t\t\t\tif mntErr := unix.Unmount(mergedDir, 0); mntErr != nil {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Debugf(\"Failed to unmount %s: %v: %v\", id, mntErr, err)\n\t\t\t\t}\n\t\t\t\t// Cleanup the created merged directory; see the comment in Put's rmdir\n\t\t\t\tif rmErr := unix.Rmdir(mergedDir); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"overlay\").Warnf(\"Failed to remove %s: %v: %v\", id, rmErr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\tlowerID, err := ioutil.ReadFile(path.Join(dir, \"lower-id\"))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(mergedDir, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\tlowerDir = path.Join(d.dir(string(lowerID)), \"root\")\n\t\tupperDir = path.Join(dir, \"upper\")\n\t\tworkDir  = path.Join(dir, \"work\")\n\t\topts     = fmt.Sprintf(\"lowerdir=%s,upperdir=%s,workdir=%s\", lowerDir, upperDir, workDir)\n\t)\n\tif err := unix.Mount(\"overlay\", mergedDir, \"overlay\", 0, label.FormatMountLabel(opts, mountLabel)); err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating overlay mount to %s: %v\", mergedDir, err)\n\t}\n\t// chown \"workdir/work\" to the remapped root UID/GID. Overlay fs inside a\n\t// user namespace requires this to move a directory from lower to upper.\n\tif err := os.Chown(path.Join(workDir, \"work\"), rootUID, rootGID); err != nil {\n\t\treturn nil, err\n\t}\n\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n}\n\n// Put unmounts the mount path created for the give id.\n// It also removes the 'merged' directory to force the kernel to unmount the\n// overlay mount in other namespaces.\nfunc (d *Driver) Put(id string) error {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\t// If id has a root, just return\n\tif _, err := os.Stat(path.Join(d.dir(id), \"root\")); err == nil {\n\t\treturn nil\n\t}\n\tmountpoint := path.Join(d.dir(id), \"merged\")\n\tlogger := logrus.WithField(\"storage-driver\", \"overlay\")\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\tlogger.Debugf(\"Failed to unmount %s overlay: %v\", id, err)\n\t}\n\n\t// Remove the mountpoint here. Removing the mountpoint (in newer kernels)\n\t// will cause all other instances of this mount in other mount namespaces\n\t// to be unmounted. This is necessary to avoid cases where an overlay mount\n\t// that is present in another namespace will cause subsequent mounts\n\t// operations to fail with ebusy.  We ignore any errors here because this may\n\t// fail on older kernels which don't have\n\t// torvalds/linux@8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe applied.\n\tif err := unix.Rmdir(mountpoint); err != nil {\n\t\tlogger.Debugf(\"Failed to remove %s overlay: %v\", id, err)\n\t}\n\treturn nil\n}\n\n// ApplyDiff applies the new layer on top of the root, if parent does not exist with will return an ErrApplyDiffFallback error.\nfunc (d *Driver) ApplyDiff(id string, parent string, diff io.Reader) (size int64, err error) {\n\tdir := d.dir(id)\n\n\tif parent == \"\" {\n\t\treturn 0, ErrApplyDiffFallback\n\t}\n\n\tparentRootDir := path.Join(d.dir(parent), \"root\")\n\tif _, err := os.Stat(parentRootDir); err != nil {\n\t\treturn 0, ErrApplyDiffFallback\n\t}\n\n\t// We now know there is a parent, and it has a \"root\" directory containing\n\t// the full root filesystem. We can just hardlink it and apply the\n\t// layer. This relies on two things:\n\t// 1) ApplyDiff is only run once on a clean (no writes to upper layer) container\n\t// 2) ApplyDiff doesn't do any in-place writes to files (would break hardlinks)\n\t// These are all currently true and are not expected to break\n\n\ttmpRootDir, err := ioutil.TempDir(dir, \"tmproot\")\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(tmpRootDir)\n\t\t} else {\n\t\t\tos.RemoveAll(path.Join(dir, \"upper\"))\n\t\t\tos.RemoveAll(path.Join(dir, \"work\"))\n\t\t\tos.RemoveAll(path.Join(dir, \"merged\"))\n\t\t\tos.RemoveAll(path.Join(dir, \"lower-id\"))\n\t\t}\n\t}()\n\n\tif err = copy.DirCopy(parentRootDir, tmpRootDir, copy.Hardlink, true); err != nil {\n\t\treturn 0, err\n\t}\n\n\toptions := &archive.TarOptions{UIDMaps: d.uidMaps, GIDMaps: d.gidMaps}\n\tif size, err = graphdriver.ApplyUncompressedLayer(tmpRootDir, diff, options); err != nil {\n\t\treturn 0, err\n\t}\n\n\trootDir := path.Join(dir, \"root\")\n\tif err := os.Rename(tmpRootDir, rootDir); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn\n}\n\n// Exists checks to see if the id is already mounted.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n", "// +build linux\n\npackage overlay2 // import \"github.com/docker/docker/daemon/graphdriver/overlay2\"\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/daemon/graphdriver/overlayutils\"\n\t\"github.com/docker/docker/pkg/archive\"\n\t\"github.com/docker/docker/pkg/chrootarchive\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/directory\"\n\t\"github.com/docker/docker/pkg/fsutils\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/quota\"\n\tunits \"github.com/docker/go-units\"\n\t\"github.com/moby/locker\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t// untar defines the untar method\n\tuntar = chrootarchive.UntarUncompressed\n)\n\n// This backend uses the overlay union filesystem for containers\n// with diff directories for each layer.\n\n// This version of the overlay driver requires at least kernel\n// 4.0.0 in order to support mounting multiple diff directories.\n\n// Each container/image has at least a \"diff\" directory and \"link\" file.\n// If there is also a \"lower\" file when there are diff layers\n// below as well as \"merged\" and \"work\" directories. The \"diff\" directory\n// has the upper layer of the overlay and is used to capture any\n// changes to the layer. The \"lower\" file contains all the lower layer\n// mounts separated by \":\" and ordered from uppermost to lowermost\n// layers. The overlay itself is mounted in the \"merged\" directory,\n// and the \"work\" dir is needed for overlay to work.\n\n// The \"link\" file for each layer contains a unique string for the layer.\n// Under the \"l\" directory at the root there will be a symbolic link\n// with that unique string pointing the \"diff\" directory for the layer.\n// The symbolic links are used to reference lower layers in the \"lower\"\n// file and on mount. The links are used to shorten the total length\n// of a layer reference without requiring changes to the layer identifier\n// or root directory. Mounts are always done relative to root and\n// referencing the symbolic links in order to ensure the number of\n// lower directories can fit in a single page for making the mount\n// syscall. A hard upper limit of 128 lower layers is enforced to ensure\n// that mounts do not fail due to length.\n\nconst (\n\tdriverName    = \"overlay2\"\n\tlinkDir       = \"l\"\n\tdiffDirName   = \"diff\"\n\tworkDirName   = \"work\"\n\tmergedDirName = \"merged\"\n\tlowerFile     = \"lower\"\n\tmaxDepth      = 128\n\n\t// idLength represents the number of random characters\n\t// which can be used to create the unique link identifier\n\t// for every layer. If this value is too long then the\n\t// page size limit for the mount command may be exceeded.\n\t// The idLength should be selected such that following equation\n\t// is true (512 is a buffer for label metadata).\n\t// ((idLength + len(linkDir) + 1) * maxDepth) <= (pageSize - 512)\n\tidLength = 26\n)\n\ntype overlayOptions struct {\n\toverrideKernelCheck bool\n\tquota               quota.Quota\n}\n\n// Driver contains information about the home directory and the list of active\n// mounts that are created using this driver.\ntype Driver struct {\n\thome          string\n\tuidMaps       []idtools.IDMap\n\tgidMaps       []idtools.IDMap\n\tctr           *graphdriver.RefCounter\n\tquotaCtl      *quota.Control\n\toptions       overlayOptions\n\tnaiveDiff     graphdriver.DiffDriver\n\tsupportsDType bool\n\tlocker        *locker.Locker\n}\n\nvar (\n\tlogger                = logrus.WithField(\"storage-driver\", \"overlay2\")\n\tbackingFs             = \"<unknown>\"\n\tprojectQuotaSupported = false\n\n\tuseNaiveDiffLock sync.Once\n\tuseNaiveDiffOnly bool\n\n\tindexOff string\n)\n\nfunc init() {\n\tgraphdriver.Register(driverName, Init)\n}\n\n// Init returns the native diff driver for overlay filesystem.\n// If overlay filesystem is not supported on the host, the error\n// graphdriver.ErrNotSupported is returned.\n// If an overlay filesystem is not supported over an existing filesystem then\n// the error graphdriver.ErrIncompatibleFS is returned.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\topts, err := parseOptions(options)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Perform feature detection on /var/lib/docker/overlay2 if it's an existing directory.\n\t// This covers situations where /var/lib/docker/overlay2 is a mount, and on a different\n\t// filesystem than /var/lib/docker.\n\t// If the path does not exist, fall back to using /var/lib/docker for feature detection.\n\ttestdir := home\n\tif _, err := os.Stat(testdir); os.IsNotExist(err) {\n\t\ttestdir = filepath.Dir(testdir)\n\t}\n\n\tif err := overlayutils.SupportsOverlay(testdir, true); err != nil {\n\t\tlogger.Error(err)\n\t\treturn nil, graphdriver.ErrNotSupported\n\t}\n\n\tfsMagic, err := graphdriver.GetFSMagic(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif fsName, ok := graphdriver.FsNames[fsMagic]; ok {\n\t\tbackingFs = fsName\n\t}\n\n\tsupportsDType, err := fsutils.SupportsDType(testdir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !supportsDType {\n\t\tif !graphdriver.IsInitialized(home) {\n\t\t\treturn nil, overlayutils.ErrDTypeNotSupported(\"overlay2\", backingFs)\n\t\t}\n\t\t// allow running without d_type only for existing setups (#27443)\n\t\tlogger.Warn(overlayutils.ErrDTypeNotSupported(\"overlay2\", backingFs))\n\t}\n\n\tif err := idtools.MkdirAllAndChown(path.Join(home, linkDir), 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\td := &Driver{\n\t\thome:          home,\n\t\tuidMaps:       uidMaps,\n\t\tgidMaps:       gidMaps,\n\t\tctr:           graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicOverlay)),\n\t\tsupportsDType: supportsDType,\n\t\tlocker:        locker.New(),\n\t\toptions:       *opts,\n\t}\n\n\td.naiveDiff = graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps)\n\n\tif backingFs == \"xfs\" {\n\t\t// Try to enable project quota support over xfs.\n\t\tif d.quotaCtl, err = quota.NewControl(home); err == nil {\n\t\t\tprojectQuotaSupported = true\n\t\t} else if opts.quota.Size > 0 {\n\t\t\treturn nil, fmt.Errorf(\"Storage option overlay2.size not supported. Filesystem does not support Project Quota: %v\", err)\n\t\t}\n\t} else if opts.quota.Size > 0 {\n\t\t// if xfs is not the backing fs then error out if the storage-opt overlay2.size is used.\n\t\treturn nil, fmt.Errorf(\"Storage Option overlay2.size only supported for backingFS XFS. Found %v\", backingFs)\n\t}\n\n\t// figure out whether \"index=off\" option is recognized by the kernel\n\t_, err = os.Stat(\"/sys/module/overlay/parameters/index\")\n\tswitch {\n\tcase err == nil:\n\t\tindexOff = \"index=off,\"\n\tcase os.IsNotExist(err):\n\t\t// old kernel, no index -- do nothing\n\tdefault:\n\t\tlogger.Warnf(\"Unable to detect whether overlay kernel module supports index parameter: %s\", err)\n\t}\n\n\tlogger.Debugf(\"backingFs=%s, projectQuotaSupported=%v, indexOff=%q\", backingFs, projectQuotaSupported, indexOff)\n\n\treturn d, nil\n}\n\nfunc parseOptions(options []string) (*overlayOptions, error) {\n\to := &overlayOptions{}\n\tfor _, option := range options {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"overlay2.override_kernel_check\":\n\t\t\to.overrideKernelCheck, err = strconv.ParseBool(val)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase \"overlay2.size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\to.quota.Size = uint64(size)\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"overlay2: unknown option %s\", key)\n\t\t}\n\t}\n\treturn o, nil\n}\n\nfunc useNaiveDiff(home string) bool {\n\tuseNaiveDiffLock.Do(func() {\n\t\tif err := doesSupportNativeDiff(home); err != nil {\n\t\t\tlogger.Warnf(\"Not using native diff for overlay2, this may cause degraded performance for building images: %v\", err)\n\t\t\tuseNaiveDiffOnly = true\n\t\t}\n\t})\n\treturn useNaiveDiffOnly\n}\n\nfunc (d *Driver) String() string {\n\treturn driverName\n}\n\n// Status returns current driver information in a two dimensional string array.\n// Output contains \"Backing Filesystem\" used in this implementation.\nfunc (d *Driver) Status() [][2]string {\n\treturn [][2]string{\n\t\t{\"Backing Filesystem\", backingFs},\n\t\t{\"Supports d_type\", strconv.FormatBool(d.supportsDType)},\n\t\t{\"Native Overlay Diff\", strconv.FormatBool(!useNaiveDiff(d.home))},\n\t}\n}\n\n// GetMetadata returns metadata about the overlay driver such as the LowerDir,\n// UpperDir, WorkDir, and MergeDir used to store data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tmetadata := map[string]string{\n\t\t\"WorkDir\":   path.Join(dir, workDirName),\n\t\t\"MergedDir\": path.Join(dir, mergedDirName),\n\t\t\"UpperDir\":  path.Join(dir, diffDirName),\n\t}\n\n\tlowerDirs, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(lowerDirs) > 0 {\n\t\tmetadata[\"LowerDir\"] = strings.Join(lowerDirs, \":\")\n\t}\n\n\treturn metadata, nil\n}\n\n// Cleanup any state created by overlay which should be cleaned when daemon\n// is being shutdown. For now, we just have to unmount the bind mounted\n// we had created.\nfunc (d *Driver) Cleanup() error {\n\treturn mount.RecursiveUnmount(d.home)\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\tif opts == nil {\n\t\topts = &graphdriver.CreateOpts{\n\t\t\tStorageOpt: make(map[string]string),\n\t\t}\n\t} else if opts.StorageOpt == nil {\n\t\topts.StorageOpt = make(map[string]string)\n\t}\n\n\t// Merge daemon default config.\n\tif _, ok := opts.StorageOpt[\"size\"]; !ok && d.options.quota.Size != 0 {\n\t\topts.StorageOpt[\"size\"] = strconv.FormatUint(d.options.quota.Size, 10)\n\t}\n\n\tif _, ok := opts.StorageOpt[\"size\"]; ok && !projectQuotaSupported {\n\t\treturn fmt.Errorf(\"--storage-opt is supported only for overlay over xfs with 'pquota' mount option\")\n\t}\n\n\treturn d.create(id, parent, opts)\n}\n\n// Create is used to create the upper, lower, and merge directories required for overlay fs for a given id.\n// The parent filesystem is used to configure these directories for the overlay.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\tif _, ok := opts.StorageOpt[\"size\"]; ok {\n\t\t\treturn fmt.Errorf(\"--storage-opt size is only supported for ReadWrite Layers\")\n\t\t}\n\t}\n\treturn d.create(id, parent, opts)\n}\n\nfunc (d *Driver) create(id, parent string, opts *graphdriver.CreateOpts) (retErr error) {\n\tdir := d.dir(id)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn err\n\t}\n\troot := idtools.Identity{UID: rootUID, GID: rootGID}\n\tcurrent := idtools.CurrentIdentity()\n\n\tif err := idtools.MkdirAllAndChown(path.Dir(dir), 0701, current); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0701, current); err != nil {\n\t\treturn err\n\t}\n\n\tdefer func() {\n\t\t// Clean up on failure\n\t\tif retErr != nil {\n\t\t\tos.RemoveAll(dir)\n\t\t}\n\t}()\n\n\tif opts != nil && len(opts.StorageOpt) > 0 {\n\t\tdriver := &Driver{}\n\t\tif err := d.parseStorageOpt(opts.StorageOpt, driver); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif driver.options.quota.Size > 0 {\n\t\t\t// Set container disk quota limit\n\t\t\tif err := d.quotaCtl.SetQuota(dir, driver.options.quota); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, diffDirName), 0755, root); err != nil {\n\t\treturn err\n\t}\n\n\tlid := overlayutils.GenerateID(idLength, logger)\n\tif err := os.Symlink(path.Join(\"..\", id, diffDirName), path.Join(d.home, linkDir, lid)); err != nil {\n\t\treturn err\n\t}\n\n\t// Write link id to link file\n\tif err := ioutil.WriteFile(path.Join(dir, \"link\"), []byte(lid), 0644); err != nil {\n\t\treturn err\n\t}\n\n\t// if no parent directory, done\n\tif parent == \"\" {\n\t\treturn nil\n\t}\n\n\tif err := idtools.MkdirAndChown(path.Join(dir, workDirName), 0700, root); err != nil {\n\t\treturn err\n\t}\n\n\tif err := ioutil.WriteFile(path.Join(d.dir(parent), \"committed\"), []byte{}, 0600); err != nil {\n\t\treturn err\n\t}\n\n\tlower, err := d.getLower(parent)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif lower != \"\" {\n\t\tif err := ioutil.WriteFile(path.Join(dir, lowerFile), []byte(lower), 0666); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Parse overlay storage options\nfunc (d *Driver) parseStorageOpt(storageOpt map[string]string, driver *Driver) error {\n\t// Read size to set the disk project quota per container\n\tfor key, val := range storageOpt {\n\t\tkey := strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tdriver.options.quota.Size = uint64(size)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (d *Driver) getLower(parent string) (string, error) {\n\tparentDir := d.dir(parent)\n\n\t// Ensure parent exists\n\tif _, err := os.Lstat(parentDir); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Read Parent link fileA\n\tparentLink, err := ioutil.ReadFile(path.Join(parentDir, \"link\"))\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tlowers := []string{path.Join(linkDir, string(parentLink))}\n\n\tparentLower, err := ioutil.ReadFile(path.Join(parentDir, lowerFile))\n\tif err == nil {\n\t\tparentLowers := strings.Split(string(parentLower), \":\")\n\t\tlowers = append(lowers, parentLowers...)\n\t}\n\tif len(lowers) > maxDepth {\n\t\treturn \"\", errors.New(\"max depth exceeded\")\n\t}\n\treturn strings.Join(lowers, \":\"), nil\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn path.Join(d.home, id)\n}\n\nfunc (d *Driver) getLowerDirs(id string) ([]string, error) {\n\tvar lowersArray []string\n\tlowers, err := ioutil.ReadFile(path.Join(d.dir(id), lowerFile))\n\tif err == nil {\n\t\tfor _, s := range strings.Split(string(lowers), \":\") {\n\t\t\tlp, err := os.Readlink(path.Join(d.home, s))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlowersArray = append(lowersArray, path.Clean(path.Join(d.home, linkDir, lp)))\n\t\t}\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\treturn lowersArray, nil\n}\n\n// Remove cleans the directories that are created for this id.\nfunc (d *Driver) Remove(id string) error {\n\tif id == \"\" {\n\t\treturn fmt.Errorf(\"refusing to remove the directories: id is empty\")\n\t}\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tlid, err := ioutil.ReadFile(path.Join(dir, \"link\"))\n\tif err == nil {\n\t\tif len(lid) == 0 {\n\t\t\tlogger.Errorf(\"refusing to remove empty link for layer %v\", id)\n\t\t} else if err := os.RemoveAll(path.Join(d.home, linkDir, string(lid))); err != nil {\n\t\t\tlogger.Debugf(\"Failed to remove link: %v\", err)\n\t\t}\n\t}\n\n\tif err := system.EnsureRemoveAll(dir); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Get creates and mounts the required file system for the given id and returns the mount path.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, retErr error) {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\tif _, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdiffDir := path.Join(dir, diffDirName)\n\tlowers, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, just return diff directory\n\t\tif os.IsNotExist(err) {\n\t\t\treturn containerfs.NewLocalContainerFS(diffDir), nil\n\t\t}\n\t\treturn nil, err\n\t}\n\n\tmergedDir := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Increment(mergedDir); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif c := d.ctr.Decrement(mergedDir); c <= 0 {\n\t\t\t\tif mntErr := unix.Unmount(mergedDir, 0); mntErr != nil {\n\t\t\t\t\tlogger.Errorf(\"error unmounting %v: %v\", mergedDir, mntErr)\n\t\t\t\t}\n\t\t\t\t// Cleanup the created merged directory; see the comment in Put's rmdir\n\t\t\t\tif rmErr := unix.Rmdir(mergedDir); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogger.Debugf(\"Failed to remove %s: %v: %v\", id, rmErr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tworkDir := path.Join(dir, workDirName)\n\tsplitLowers := strings.Split(string(lowers), \":\")\n\tabsLowers := make([]string, len(splitLowers))\n\tfor i, s := range splitLowers {\n\t\tabsLowers[i] = path.Join(d.home, s)\n\t}\n\tvar readonly bool\n\tif _, err := os.Stat(path.Join(dir, \"committed\")); err == nil {\n\t\treadonly = true\n\t} else if !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tvar opts string\n\tif readonly {\n\t\topts = indexOff + \"lowerdir=\" + diffDir + \":\" + strings.Join(absLowers, \":\")\n\t} else {\n\t\topts = indexOff + \"lowerdir=\" + strings.Join(absLowers, \":\") + \",upperdir=\" + diffDir + \",workdir=\" + workDir\n\t}\n\n\tmountData := label.FormatMountLabel(opts, mountLabel)\n\tmount := unix.Mount\n\tmountTarget := mergedDir\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := idtools.MkdirAndChown(mergedDir, 0700, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\tpageSize := unix.Getpagesize()\n\n\t// Use relative paths and mountFrom when the mount data has exceeded\n\t// the page size. The mount syscall fails if the mount data cannot\n\t// fit within a page and relative links make the mount data much\n\t// smaller at the expense of requiring a fork exec to chroot.\n\tif len(mountData) > pageSize-1 {\n\t\tif readonly {\n\t\t\topts = indexOff + \"lowerdir=\" + path.Join(id, diffDirName) + \":\" + string(lowers)\n\t\t} else {\n\t\t\topts = indexOff + \"lowerdir=\" + string(lowers) + \",upperdir=\" + path.Join(id, diffDirName) + \",workdir=\" + path.Join(id, workDirName)\n\t\t}\n\t\tmountData = label.FormatMountLabel(opts, mountLabel)\n\t\tif len(mountData) > pageSize-1 {\n\t\t\treturn nil, fmt.Errorf(\"cannot mount layer, mount label too large %d\", len(mountData))\n\t\t}\n\n\t\tmount = func(source string, target string, mType string, flags uintptr, label string) error {\n\t\t\treturn mountFrom(d.home, source, target, mType, flags, label)\n\t\t}\n\t\tmountTarget = path.Join(id, mergedDirName)\n\t}\n\n\tif err := mount(\"overlay\", mountTarget, \"overlay\", 0, mountData); err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating overlay mount to %s: %v\", mergedDir, err)\n\t}\n\n\tif !readonly {\n\t\t// chown \"workdir/work\" to the remapped root UID/GID. Overlay fs inside a\n\t\t// user namespace requires this to move a directory from lower to upper.\n\t\tif err := os.Chown(path.Join(workDir, workDirName), rootUID, rootGID); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn containerfs.NewLocalContainerFS(mergedDir), nil\n}\n\n// Put unmounts the mount path created for the give id.\n// It also removes the 'merged' directory to force the kernel to unmount the\n// overlay mount in other namespaces.\nfunc (d *Driver) Put(id string) error {\n\td.locker.Lock(id)\n\tdefer d.locker.Unlock(id)\n\tdir := d.dir(id)\n\t_, err := ioutil.ReadFile(path.Join(dir, lowerFile))\n\tif err != nil {\n\t\t// If no lower, no mount happened and just return directly\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\n\tmountpoint := path.Join(dir, mergedDirName)\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\tlogger.Debugf(\"Failed to unmount %s overlay: %s - %v\", id, mountpoint, err)\n\t}\n\t// Remove the mountpoint here. Removing the mountpoint (in newer kernels)\n\t// will cause all other instances of this mount in other mount namespaces\n\t// to be unmounted. This is necessary to avoid cases where an overlay mount\n\t// that is present in another namespace will cause subsequent mounts\n\t// operations to fail with ebusy.  We ignore any errors here because this may\n\t// fail on older kernels which don't have\n\t// torvalds/linux@8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe applied.\n\tif err := unix.Rmdir(mountpoint); err != nil && !os.IsNotExist(err) {\n\t\tlogger.Debugf(\"Failed to remove %s overlay: %v\", id, err)\n\t}\n\treturn nil\n}\n\n// Exists checks to see if the id is already mounted.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n\n// isParent determines whether the given parent is the direct parent of the\n// given layer id\nfunc (d *Driver) isParent(id, parent string) bool {\n\tlowers, err := d.getLowerDirs(id)\n\tif err != nil {\n\t\treturn false\n\t}\n\tif parent == \"\" && len(lowers) > 0 {\n\t\treturn false\n\t}\n\n\tparentDir := d.dir(parent)\n\tvar ld string\n\tif len(lowers) > 0 {\n\t\tld = filepath.Dir(lowers[0])\n\t}\n\tif ld == \"\" && parent == \"\" {\n\t\treturn true\n\t}\n\treturn ld == parentDir\n}\n\n// ApplyDiff applies the new layer into a root\nfunc (d *Driver) ApplyDiff(id string, parent string, diff io.Reader) (size int64, err error) {\n\tif !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.ApplyDiff(id, parent, diff)\n\t}\n\n\tapplyDir := d.getDiffPath(id)\n\n\tlogger.Debugf(\"Applying tar in %s\", applyDir)\n\t// Overlay doesn't need the parent id to apply the diff\n\tif err := untar(diff, applyDir, &archive.TarOptions{\n\t\tUIDMaps:        d.uidMaps,\n\t\tGIDMaps:        d.gidMaps,\n\t\tWhiteoutFormat: archive.OverlayWhiteoutFormat,\n\t\tInUserNS:       sys.RunningInUserNS(),\n\t}); err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn directory.Size(context.TODO(), applyDir)\n}\n\nfunc (d *Driver) getDiffPath(id string) string {\n\tdir := d.dir(id)\n\n\treturn path.Join(dir, diffDirName)\n}\n\n// DiffSize calculates the changes between the specified id\n// and its parent and returns the size in bytes of the changes\n// relative to its base filesystem directory.\nfunc (d *Driver) DiffSize(id, parent string) (size int64, err error) {\n\tif useNaiveDiff(d.home) || !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.DiffSize(id, parent)\n\t}\n\treturn directory.Size(context.TODO(), d.getDiffPath(id))\n}\n\n// Diff produces an archive of the changes between the specified\n// layer and its parent layer which may be \"\".\nfunc (d *Driver) Diff(id, parent string) (io.ReadCloser, error) {\n\tif useNaiveDiff(d.home) || !d.isParent(id, parent) {\n\t\treturn d.naiveDiff.Diff(id, parent)\n\t}\n\n\tdiffPath := d.getDiffPath(id)\n\tlogger.Debugf(\"Tar with options on %s\", diffPath)\n\treturn archive.TarWithOptions(diffPath, &archive.TarOptions{\n\t\tCompression:    archive.Uncompressed,\n\t\tUIDMaps:        d.uidMaps,\n\t\tGIDMaps:        d.gidMaps,\n\t\tWhiteoutFormat: archive.OverlayWhiteoutFormat,\n\t})\n}\n\n// Changes produces a list of changes between the specified layer and its\n// parent layer. If parent is \"\", then all changes will be ADD changes.\nfunc (d *Driver) Changes(id, parent string) ([]archive.Change, error) {\n\treturn d.naiveDiff.Changes(id, parent)\n}\n", "package vfs // import \"github.com/docker/docker/daemon/graphdriver/vfs\"\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/docker/docker/quota\"\n\tunits \"github.com/docker/go-units\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n)\n\nvar (\n\t// CopyDir defines the copy method to use.\n\tCopyDir = dirCopy\n)\n\nfunc init() {\n\tgraphdriver.Register(\"vfs\", Init)\n}\n\n// Init returns a new VFS driver.\n// This sets the home directory for the driver and returns NaiveDiffDriver.\nfunc Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\td := &Driver{\n\t\thome:      home,\n\t\tidMapping: idtools.NewIDMappingsFromMaps(uidMaps, gidMaps),\n\t}\n\n\tif err := d.parseOptions(options); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := idtools.MkdirAllAndChown(home, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsetupDriverQuota(d)\n\n\tif size := d.getQuotaOpt(); !d.quotaSupported() && size > 0 {\n\t\treturn nil, quota.ErrQuotaNotSupported\n\t}\n\n\treturn graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps), nil\n}\n\n// Driver holds information about the driver, home directory of the driver.\n// Driver implements graphdriver.ProtoDriver. It uses only basic vfs operations.\n// In order to support layering, files are copied from the parent layer into the new layer. There is no copy-on-write support.\n// Driver must be wrapped in NaiveDiffDriver to be used as a graphdriver.Driver\ntype Driver struct {\n\tdriverQuota\n\thome      string\n\tidMapping *idtools.IdentityMapping\n}\n\nfunc (d *Driver) String() string {\n\treturn \"vfs\"\n}\n\n// Status is used for implementing the graphdriver.ProtoDriver interface. VFS does not currently have any status information.\nfunc (d *Driver) Status() [][2]string {\n\treturn nil\n}\n\n// GetMetadata is used for implementing the graphdriver.ProtoDriver interface. VFS does not currently have any meta data.\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn nil, nil\n}\n\n// Cleanup is used to implement graphdriver.ProtoDriver. There is no cleanup required for this driver.\nfunc (d *Driver) Cleanup() error {\n\treturn nil\n}\n\nfunc (d *Driver) parseOptions(options []string) error {\n\tfor _, option := range options {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn errdefs.InvalidParameter(err)\n\t\t}\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\tif err != nil {\n\t\t\t\treturn errdefs.InvalidParameter(err)\n\t\t\t}\n\t\t\tif err = d.setQuotaOpt(uint64(size)); err != nil {\n\t\t\t\treturn errdefs.InvalidParameter(errors.Wrap(err, \"failed to set option size for vfs\"))\n\t\t\t}\n\t\tdefault:\n\t\t\treturn errdefs.InvalidParameter(errors.Errorf(\"unknown option %s for vfs\", key))\n\t\t}\n\t}\n\treturn nil\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\tquotaSize := d.getQuotaOpt()\n\n\tif opts != nil {\n\t\tfor key, val := range opts.StorageOpt {\n\t\t\tswitch key {\n\t\t\tcase \"size\":\n\t\t\t\tif !d.quotaSupported() {\n\t\t\t\t\treturn quota.ErrQuotaNotSupported\n\t\t\t\t}\n\t\t\t\tsize, err := units.RAMInBytes(val)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn errdefs.InvalidParameter(err)\n\t\t\t\t}\n\t\t\t\tquotaSize = uint64(size)\n\t\t\tdefault:\n\t\t\t\treturn errdefs.InvalidParameter(errors.Errorf(\"Storage opt %s not supported\", key))\n\t\t\t}\n\t\t}\n\t}\n\n\treturn d.create(id, parent, quotaSize)\n}\n\n// Create prepares the filesystem for the VFS driver and copies the directory for the given id under the parent.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\tif opts != nil && len(opts.StorageOpt) != 0 {\n\t\treturn fmt.Errorf(\"--storage-opt is not supported for vfs on read-only layers\")\n\t}\n\n\treturn d.create(id, parent, 0)\n}\n\nfunc (d *Driver) create(id, parent string, size uint64) error {\n\tdir := d.dir(id)\n\trootIDs := d.idMapping.RootPair()\n\tif err := idtools.MkdirAllAndChown(filepath.Dir(dir), 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn err\n\t}\n\tif err := idtools.MkdirAndChown(dir, 0755, rootIDs); err != nil {\n\t\treturn err\n\t}\n\n\tif size != 0 {\n\t\tif err := d.setupQuota(dir, size); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tlabelOpts := []string{\"level:s0\"}\n\tif _, mountLabel, err := label.InitLabels(labelOpts); err == nil {\n\t\tlabel.SetFileLabel(dir, mountLabel)\n\t}\n\tif parent == \"\" {\n\t\treturn nil\n\t}\n\tparentDir, err := d.Get(parent, \"\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"%s: %s\", parent, err)\n\t}\n\treturn CopyDir(parentDir.Path(), dir)\n}\n\nfunc (d *Driver) dir(id string) string {\n\treturn filepath.Join(d.home, \"dir\", filepath.Base(id))\n}\n\n// Remove deletes the content from the directory for a given id.\nfunc (d *Driver) Remove(id string) error {\n\treturn system.EnsureRemoveAll(d.dir(id))\n}\n\n// Get returns the directory for the given id.\nfunc (d *Driver) Get(id, mountLabel string) (containerfs.ContainerFS, error) {\n\tdir := d.dir(id)\n\tif st, err := os.Stat(dir); err != nil {\n\t\treturn nil, err\n\t} else if !st.IsDir() {\n\t\treturn nil, fmt.Errorf(\"%s: not a directory\", dir)\n\t}\n\treturn containerfs.NewLocalContainerFS(dir), nil\n}\n\n// Put is a noop for vfs that return nil for the error, since this driver has no runtime resources to clean up.\nfunc (d *Driver) Put(id string) error {\n\t// The vfs driver has no runtime resources (e.g. mounts)\n\t// to clean up, so we don't need anything here\n\treturn nil\n}\n\n// Exists checks to see if the directory exists for the given id.\nfunc (d *Driver) Exists(id string) bool {\n\t_, err := os.Stat(d.dir(id))\n\treturn err == nil\n}\n", "// +build linux freebsd\n\npackage zfs // import \"github.com/docker/docker/daemon/graphdriver/zfs\"\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/docker/docker/daemon/graphdriver\"\n\t\"github.com/docker/docker/pkg/containerfs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/pkg/parsers\"\n\tzfs \"github.com/mistifyio/go-zfs\"\n\t\"github.com/moby/sys/mount\"\n\t\"github.com/moby/sys/mountinfo\"\n\t\"github.com/opencontainers/selinux/go-selinux/label\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\ntype zfsOptions struct {\n\tfsName    string\n\tmountPath string\n}\n\nfunc init() {\n\tgraphdriver.Register(\"zfs\", Init)\n}\n\n// Logger returns a zfs logger implementation.\ntype Logger struct{}\n\n// Log wraps log message from ZFS driver with a prefix '[zfs]'.\nfunc (*Logger) Log(cmd []string) {\n\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(\"[zfs] %s\", strings.Join(cmd, \" \"))\n}\n\n// Init returns a new ZFS driver.\n// It takes base mount path and an array of options which are represented as key value pairs.\n// Each option is in the for key=value. 'zfs.fsname' is expected to be a valid key in the options.\nfunc Init(base string, opt []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) {\n\tvar err error\n\n\tlogger := logrus.WithField(\"storage-driver\", \"zfs\")\n\n\tif _, err := exec.LookPath(\"zfs\"); err != nil {\n\t\tlogger.Debugf(\"zfs command is not available: %v\", err)\n\t\treturn nil, graphdriver.ErrPrerequisites\n\t}\n\n\tfile, err := os.OpenFile(\"/dev/zfs\", os.O_RDWR, 0600)\n\tif err != nil {\n\t\tlogger.Debugf(\"cannot open /dev/zfs: %v\", err)\n\t\treturn nil, graphdriver.ErrPrerequisites\n\t}\n\tdefer file.Close()\n\n\toptions, err := parseOptions(opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\toptions.mountPath = base\n\n\trootdir := path.Dir(base)\n\n\tif options.fsName == \"\" {\n\t\terr = checkRootdirFs(rootdir)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif options.fsName == \"\" {\n\t\toptions.fsName, err = lookupZfsDataset(rootdir)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tzfs.SetLogger(new(Logger))\n\n\tfilesystems, err := zfs.Filesystems(options.fsName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Cannot find root filesystem %s: %v\", options.fsName, err)\n\t}\n\n\tfilesystemsCache := make(map[string]bool, len(filesystems))\n\tvar rootDataset *zfs.Dataset\n\tfor _, fs := range filesystems {\n\t\tif fs.Name == options.fsName {\n\t\t\trootDataset = fs\n\t\t}\n\t\tfilesystemsCache[fs.Name] = true\n\t}\n\n\tif rootDataset == nil {\n\t\treturn nil, fmt.Errorf(\"BUG: zfs get all -t filesystem -rHp '%s' should contain '%s'\", options.fsName, options.fsName)\n\t}\n\n\tif err := idtools.MkdirAllAndChown(base, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, fmt.Errorf(\"Failed to create '%s': %v\", base, err)\n\t}\n\n\td := &Driver{\n\t\tdataset:          rootDataset,\n\t\toptions:          options,\n\t\tfilesystemsCache: filesystemsCache,\n\t\tuidMaps:          uidMaps,\n\t\tgidMaps:          gidMaps,\n\t\tctr:              graphdriver.NewRefCounter(graphdriver.NewDefaultChecker()),\n\t}\n\treturn graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps), nil\n}\n\nfunc parseOptions(opt []string) (zfsOptions, error) {\n\tvar options zfsOptions\n\toptions.fsName = \"\"\n\tfor _, option := range opt {\n\t\tkey, val, err := parsers.ParseKeyValueOpt(option)\n\t\tif err != nil {\n\t\t\treturn options, err\n\t\t}\n\t\tkey = strings.ToLower(key)\n\t\tswitch key {\n\t\tcase \"zfs.fsname\":\n\t\t\toptions.fsName = val\n\t\tdefault:\n\t\t\treturn options, fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\treturn options, nil\n}\n\nfunc lookupZfsDataset(rootdir string) (string, error) {\n\tvar stat unix.Stat_t\n\tif err := unix.Stat(rootdir, &stat); err != nil {\n\t\treturn \"\", fmt.Errorf(\"Failed to access '%s': %s\", rootdir, err)\n\t}\n\twantedDev := stat.Dev\n\n\tmounts, err := mountinfo.GetMounts(nil)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tfor _, m := range mounts {\n\t\tif err := unix.Stat(m.Mountpoint, &stat); err != nil {\n\t\t\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(\"failed to stat '%s' while scanning for zfs mount: %v\", m.Mountpoint, err)\n\t\t\tcontinue // may fail on fuse file systems\n\t\t}\n\n\t\tif stat.Dev == wantedDev && m.FSType == \"zfs\" {\n\t\t\treturn m.Source, nil\n\t\t}\n\t}\n\n\treturn \"\", fmt.Errorf(\"Failed to find zfs dataset mounted on '%s' in /proc/mounts\", rootdir)\n}\n\n// Driver holds information about the driver, such as zfs dataset, options and cache.\ntype Driver struct {\n\tdataset          *zfs.Dataset\n\toptions          zfsOptions\n\tsync.Mutex       // protects filesystem cache against concurrent access\n\tfilesystemsCache map[string]bool\n\tuidMaps          []idtools.IDMap\n\tgidMaps          []idtools.IDMap\n\tctr              *graphdriver.RefCounter\n}\n\nfunc (d *Driver) String() string {\n\treturn \"zfs\"\n}\n\n// Cleanup is called on daemon shutdown, it is a no-op for ZFS.\n// TODO(@cpuguy83): Walk layer tree and check mounts?\nfunc (d *Driver) Cleanup() error {\n\treturn nil\n}\n\n// Status returns information about the ZFS filesystem. It returns a two dimensional array of information\n// such as pool name, dataset name, disk usage, parent quota and compression used.\n// Currently it return 'Zpool', 'Zpool Health', 'Parent Dataset', 'Space Used By Parent',\n// 'Space Available', 'Parent Quota' and 'Compression'.\nfunc (d *Driver) Status() [][2]string {\n\tparts := strings.Split(d.dataset.Name, \"/\")\n\tpool, err := zfs.GetZpool(parts[0])\n\n\tvar poolName, poolHealth string\n\tif err == nil {\n\t\tpoolName = pool.Name\n\t\tpoolHealth = pool.Health\n\t} else {\n\t\tpoolName = fmt.Sprintf(\"error while getting pool information %v\", err)\n\t\tpoolHealth = \"not available\"\n\t}\n\n\tquota := \"no\"\n\tif d.dataset.Quota != 0 {\n\t\tquota = strconv.FormatUint(d.dataset.Quota, 10)\n\t}\n\n\treturn [][2]string{\n\t\t{\"Zpool\", poolName},\n\t\t{\"Zpool Health\", poolHealth},\n\t\t{\"Parent Dataset\", d.dataset.Name},\n\t\t{\"Space Used By Parent\", strconv.FormatUint(d.dataset.Used, 10)},\n\t\t{\"Space Available\", strconv.FormatUint(d.dataset.Avail, 10)},\n\t\t{\"Parent Quota\", quota},\n\t\t{\"Compression\", d.dataset.Compression},\n\t}\n}\n\n// GetMetadata returns image/container metadata related to graph driver\nfunc (d *Driver) GetMetadata(id string) (map[string]string, error) {\n\treturn map[string]string{\n\t\t\"Mountpoint\": d.mountPath(id),\n\t\t\"Dataset\":    d.zfsPath(id),\n\t}, nil\n}\n\nfunc (d *Driver) cloneFilesystem(name, parentName string) error {\n\tsnapshotName := fmt.Sprintf(\"%d\", time.Now().Nanosecond())\n\tparentDataset := zfs.Dataset{Name: parentName}\n\tsnapshot, err := parentDataset.Snapshot(snapshotName /*recursive */, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, err = snapshot.Clone(name, map[string]string{\"mountpoint\": \"legacy\"})\n\tif err == nil {\n\t\td.Lock()\n\t\td.filesystemsCache[name] = true\n\t\td.Unlock()\n\t}\n\n\tif err != nil {\n\t\tsnapshot.Destroy(zfs.DestroyDeferDeletion)\n\t\treturn err\n\t}\n\treturn snapshot.Destroy(zfs.DestroyDeferDeletion)\n}\n\nfunc (d *Driver) zfsPath(id string) string {\n\treturn d.options.fsName + \"/\" + id\n}\n\nfunc (d *Driver) mountPath(id string) string {\n\treturn path.Join(d.options.mountPath, \"graph\", getMountpoint(id))\n}\n\n// CreateReadWrite creates a layer that is writable for use as a container\n// file system.\nfunc (d *Driver) CreateReadWrite(id, parent string, opts *graphdriver.CreateOpts) error {\n\treturn d.Create(id, parent, opts)\n}\n\n// Create prepares the dataset and filesystem for the ZFS driver for the given id under the parent.\nfunc (d *Driver) Create(id, parent string, opts *graphdriver.CreateOpts) error {\n\tvar storageOpt map[string]string\n\tif opts != nil {\n\t\tstorageOpt = opts.StorageOpt\n\t}\n\n\terr := d.create(id, parent, storageOpt)\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif zfsError, ok := err.(*zfs.Error); ok {\n\t\tif !strings.HasSuffix(zfsError.Stderr, \"dataset already exists\\n\") {\n\t\t\treturn err\n\t\t}\n\t\t// aborted build -> cleanup\n\t} else {\n\t\treturn err\n\t}\n\n\tdataset := zfs.Dataset{Name: d.zfsPath(id)}\n\tif err := dataset.Destroy(zfs.DestroyRecursiveClones); err != nil {\n\t\treturn err\n\t}\n\n\t// retry\n\treturn d.create(id, parent, storageOpt)\n}\n\nfunc (d *Driver) create(id, parent string, storageOpt map[string]string) error {\n\tname := d.zfsPath(id)\n\tquota, err := parseStorageOpt(storageOpt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif parent == \"\" {\n\t\tmountoptions := map[string]string{\"mountpoint\": \"legacy\"}\n\t\tfs, err := zfs.CreateFilesystem(name, mountoptions)\n\t\tif err == nil {\n\t\t\terr = setQuota(name, quota)\n\t\t\tif err == nil {\n\t\t\t\td.Lock()\n\t\t\t\td.filesystemsCache[fs.Name] = true\n\t\t\t\td.Unlock()\n\t\t\t}\n\t\t}\n\t\treturn err\n\t}\n\terr = d.cloneFilesystem(name, d.zfsPath(parent))\n\tif err == nil {\n\t\terr = setQuota(name, quota)\n\t}\n\treturn err\n}\n\nfunc parseStorageOpt(storageOpt map[string]string) (string, error) {\n\t// Read size to change the disk quota per container\n\tfor k, v := range storageOpt {\n\t\tkey := strings.ToLower(k)\n\t\tswitch key {\n\t\tcase \"size\":\n\t\t\treturn v, nil\n\t\tdefault:\n\t\t\treturn \"0\", fmt.Errorf(\"Unknown option %s\", key)\n\t\t}\n\t}\n\treturn \"0\", nil\n}\n\nfunc setQuota(name string, quota string) error {\n\tif quota == \"0\" {\n\t\treturn nil\n\t}\n\tfs, err := zfs.GetDataset(name)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn fs.SetProperty(\"quota\", quota)\n}\n\n// Remove deletes the dataset, filesystem and the cache for the given id.\nfunc (d *Driver) Remove(id string) error {\n\tname := d.zfsPath(id)\n\tdataset := zfs.Dataset{Name: name}\n\terr := dataset.Destroy(zfs.DestroyRecursive)\n\tif err == nil {\n\t\td.Lock()\n\t\tdelete(d.filesystemsCache, name)\n\t\td.Unlock()\n\t}\n\treturn err\n}\n\n// Get returns the mountpoint for the given id after creating the target directories if necessary.\nfunc (d *Driver) Get(id, mountLabel string) (_ containerfs.ContainerFS, retErr error) {\n\tmountpoint := d.mountPath(id)\n\tif count := d.ctr.Increment(mountpoint); count > 1 {\n\t\treturn containerfs.NewLocalContainerFS(mountpoint), nil\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tif c := d.ctr.Decrement(mountpoint); c <= 0 {\n\t\t\t\tif mntErr := unix.Unmount(mountpoint, 0); mntErr != nil {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"zfs\").Errorf(\"Error unmounting %v: %v\", mountpoint, mntErr)\n\t\t\t\t}\n\t\t\t\tif rmErr := unix.Rmdir(mountpoint); rmErr != nil && !os.IsNotExist(rmErr) {\n\t\t\t\t\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(\"Failed to remove %s: %v\", id, rmErr)\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}()\n\n\tfilesystem := d.zfsPath(id)\n\toptions := label.FormatMountLabel(\"\", mountLabel)\n\tlogrus.WithField(\"storage-driver\", \"zfs\").Debugf(`mount(\"%s\", \"%s\", \"%s\")`, filesystem, mountpoint, options)\n\n\trootUID, rootGID, err := idtools.GetRootUIDGID(d.uidMaps, d.gidMaps)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create the target directories if they don't exist\n\tif err := idtools.MkdirAllAndChown(mountpoint, 0755, idtools.Identity{UID: rootUID, GID: rootGID}); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := mount.Mount(filesystem, mountpoint, \"zfs\", options); err != nil {\n\t\treturn nil, errors.Wrap(err, \"error creating zfs mount\")\n\t}\n\n\t// this could be our first mount after creation of the filesystem, and the root dir may still have root\n\t// permissions instead of the remapped root uid:gid (if user namespaces are enabled):\n\tif err := os.Chown(mountpoint, rootUID, rootGID); err != nil {\n\t\treturn nil, fmt.Errorf(\"error modifying zfs mountpoint (%s) directory ownership: %v\", mountpoint, err)\n\t}\n\n\treturn containerfs.NewLocalContainerFS(mountpoint), nil\n}\n\n// Put removes the existing mountpoint for the given id if it exists.\nfunc (d *Driver) Put(id string) error {\n\tmountpoint := d.mountPath(id)\n\tif count := d.ctr.Decrement(mountpoint); count > 0 {\n\t\treturn nil\n\t}\n\n\tlogger := logrus.WithField(\"storage-driver\", \"zfs\")\n\n\tlogger.Debugf(`unmount(\"%s\")`, mountpoint)\n\n\tif err := unix.Unmount(mountpoint, unix.MNT_DETACH); err != nil {\n\t\tlogger.Warnf(\"Failed to unmount %s mount %s: %v\", id, mountpoint, err)\n\t}\n\tif err := unix.Rmdir(mountpoint); err != nil && !os.IsNotExist(err) {\n\t\tlogger.Debugf(\"Failed to remove %s mount point %s: %v\", id, mountpoint, err)\n\t}\n\n\treturn nil\n}\n\n// Exists checks to see if the cache entry exists for the given id.\nfunc (d *Driver) Exists(id string) bool {\n\td.Lock()\n\tdefer d.Unlock()\n\treturn d.filesystemsCache[d.zfsPath(id)]\n}\n", "package idtools // import \"github.com/docker/docker/pkg/idtools\"\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// IDMap contains a single entry for user namespace range remapping. An array\n// of IDMap entries represents the structure that will be provided to the Linux\n// kernel for creating a user namespace.\ntype IDMap struct {\n\tContainerID int `json:\"container_id\"`\n\tHostID      int `json:\"host_id\"`\n\tSize        int `json:\"size\"`\n}\n\ntype subIDRange struct {\n\tStart  int\n\tLength int\n}\n\ntype ranges []subIDRange\n\nfunc (e ranges) Len() int           { return len(e) }\nfunc (e ranges) Swap(i, j int)      { e[i], e[j] = e[j], e[i] }\nfunc (e ranges) Less(i, j int) bool { return e[i].Start < e[j].Start }\n\nconst (\n\tsubuidFileName = \"/etc/subuid\"\n\tsubgidFileName = \"/etc/subgid\"\n)\n\n// MkdirAllAndChown creates a directory (include any along the path) and then modifies\n// ownership to the requested uid/gid.  If the directory already exists, this\n// function will still change ownership and permissions.\nfunc MkdirAllAndChown(path string, mode os.FileMode, owner Identity) error {\n\treturn mkdirAs(path, mode, owner, true, true)\n}\n\n// MkdirAndChown creates a directory and then modifies ownership to the requested uid/gid.\n// If the directory already exists, this function still changes ownership and permissions.\n// Note that unlike os.Mkdir(), this function does not return IsExist error\n// in case path already exists.\nfunc MkdirAndChown(path string, mode os.FileMode, owner Identity) error {\n\treturn mkdirAs(path, mode, owner, false, true)\n}\n\n// MkdirAllAndChownNew creates a directory (include any along the path) and then modifies\n// ownership ONLY of newly created directories to the requested uid/gid. If the\n// directories along the path exist, no change of ownership or permissions will be performed\nfunc MkdirAllAndChownNew(path string, mode os.FileMode, owner Identity) error {\n\treturn mkdirAs(path, mode, owner, true, false)\n}\n\n// GetRootUIDGID retrieves the remapped root uid/gid pair from the set of maps.\n// If the maps are empty, then the root uid/gid will default to \"real\" 0/0\nfunc GetRootUIDGID(uidMap, gidMap []IDMap) (int, int, error) {\n\tuid, err := toHost(0, uidMap)\n\tif err != nil {\n\t\treturn -1, -1, err\n\t}\n\tgid, err := toHost(0, gidMap)\n\tif err != nil {\n\t\treturn -1, -1, err\n\t}\n\treturn uid, gid, nil\n}\n\n// toContainer takes an id mapping, and uses it to translate a\n// host ID to the remapped ID. If no map is provided, then the translation\n// assumes a 1-to-1 mapping and returns the passed in id\nfunc toContainer(hostID int, idMap []IDMap) (int, error) {\n\tif idMap == nil {\n\t\treturn hostID, nil\n\t}\n\tfor _, m := range idMap {\n\t\tif (hostID >= m.HostID) && (hostID <= (m.HostID + m.Size - 1)) {\n\t\t\tcontID := m.ContainerID + (hostID - m.HostID)\n\t\t\treturn contID, nil\n\t\t}\n\t}\n\treturn -1, fmt.Errorf(\"Host ID %d cannot be mapped to a container ID\", hostID)\n}\n\n// toHost takes an id mapping and a remapped ID, and translates the\n// ID to the mapped host ID. If no map is provided, then the translation\n// assumes a 1-to-1 mapping and returns the passed in id #\nfunc toHost(contID int, idMap []IDMap) (int, error) {\n\tif idMap == nil {\n\t\treturn contID, nil\n\t}\n\tfor _, m := range idMap {\n\t\tif (contID >= m.ContainerID) && (contID <= (m.ContainerID + m.Size - 1)) {\n\t\t\thostID := m.HostID + (contID - m.ContainerID)\n\t\t\treturn hostID, nil\n\t\t}\n\t}\n\treturn -1, fmt.Errorf(\"Container ID %d cannot be mapped to a host ID\", contID)\n}\n\n// Identity is either a UID and GID pair or a SID (but not both)\ntype Identity struct {\n\tUID int\n\tGID int\n\tSID string\n}\n\n// IdentityMapping contains a mappings of UIDs and GIDs\ntype IdentityMapping struct {\n\tuids []IDMap\n\tgids []IDMap\n}\n\n// NewIDMappingsFromMaps creates a new mapping from two slices\n// Deprecated: this is a temporary shim while transitioning to IDMapping\nfunc NewIDMappingsFromMaps(uids []IDMap, gids []IDMap) *IdentityMapping {\n\treturn &IdentityMapping{uids: uids, gids: gids}\n}\n\n// RootPair returns a uid and gid pair for the root user. The error is ignored\n// because a root user always exists, and the defaults are correct when the uid\n// and gid maps are empty.\nfunc (i *IdentityMapping) RootPair() Identity {\n\tuid, gid, _ := GetRootUIDGID(i.uids, i.gids)\n\treturn Identity{UID: uid, GID: gid}\n}\n\n// ToHost returns the host UID and GID for the container uid, gid.\n// Remapping is only performed if the ids aren't already the remapped root ids\nfunc (i *IdentityMapping) ToHost(pair Identity) (Identity, error) {\n\tvar err error\n\ttarget := i.RootPair()\n\n\tif pair.UID != target.UID {\n\t\ttarget.UID, err = toHost(pair.UID, i.uids)\n\t\tif err != nil {\n\t\t\treturn target, err\n\t\t}\n\t}\n\n\tif pair.GID != target.GID {\n\t\ttarget.GID, err = toHost(pair.GID, i.gids)\n\t}\n\treturn target, err\n}\n\n// ToContainer returns the container UID and GID for the host uid and gid\nfunc (i *IdentityMapping) ToContainer(pair Identity) (int, int, error) {\n\tuid, err := toContainer(pair.UID, i.uids)\n\tif err != nil {\n\t\treturn -1, -1, err\n\t}\n\tgid, err := toContainer(pair.GID, i.gids)\n\treturn uid, gid, err\n}\n\n// Empty returns true if there are no id mappings\nfunc (i *IdentityMapping) Empty() bool {\n\treturn len(i.uids) == 0 && len(i.gids) == 0\n}\n\n// UIDs return the UID mapping\n// TODO: remove this once everything has been refactored to use pairs\nfunc (i *IdentityMapping) UIDs() []IDMap {\n\treturn i.uids\n}\n\n// GIDs return the UID mapping\n// TODO: remove this once everything has been refactored to use pairs\nfunc (i *IdentityMapping) GIDs() []IDMap {\n\treturn i.gids\n}\n\nfunc createIDMap(subidRanges ranges) []IDMap {\n\tidMap := []IDMap{}\n\n\tcontainerID := 0\n\tfor _, idrange := range subidRanges {\n\t\tidMap = append(idMap, IDMap{\n\t\t\tContainerID: containerID,\n\t\t\tHostID:      idrange.Start,\n\t\t\tSize:        idrange.Length,\n\t\t})\n\t\tcontainerID = containerID + idrange.Length\n\t}\n\treturn idMap\n}\n\nfunc parseSubuid(username string) (ranges, error) {\n\treturn parseSubidFile(subuidFileName, username)\n}\n\nfunc parseSubgid(username string) (ranges, error) {\n\treturn parseSubidFile(subgidFileName, username)\n}\n\n// parseSubidFile will read the appropriate file (/etc/subuid or /etc/subgid)\n// and return all found ranges for a specified username. If the special value\n// \"ALL\" is supplied for username, then all ranges in the file will be returned\nfunc parseSubidFile(path, username string) (ranges, error) {\n\tvar rangeList ranges\n\n\tsubidFile, err := os.Open(path)\n\tif err != nil {\n\t\treturn rangeList, err\n\t}\n\tdefer subidFile.Close()\n\n\ts := bufio.NewScanner(subidFile)\n\tfor s.Scan() {\n\t\ttext := strings.TrimSpace(s.Text())\n\t\tif text == \"\" || strings.HasPrefix(text, \"#\") {\n\t\t\tcontinue\n\t\t}\n\t\tparts := strings.Split(text, \":\")\n\t\tif len(parts) != 3 {\n\t\t\treturn rangeList, fmt.Errorf(\"Cannot parse subuid/gid information: Format not correct for %s file\", path)\n\t\t}\n\t\tif parts[0] == username || username == \"ALL\" {\n\t\t\tstartid, err := strconv.Atoi(parts[1])\n\t\t\tif err != nil {\n\t\t\t\treturn rangeList, fmt.Errorf(\"String to int conversion failed during subuid/gid parsing of %s: %v\", path, err)\n\t\t\t}\n\t\t\tlength, err := strconv.Atoi(parts[2])\n\t\t\tif err != nil {\n\t\t\t\treturn rangeList, fmt.Errorf(\"String to int conversion failed during subuid/gid parsing of %s: %v\", path, err)\n\t\t\t}\n\t\t\trangeList = append(rangeList, subIDRange{startid, length})\n\t\t}\n\t}\n\n\treturn rangeList, s.Err()\n}\n\n// CurrentIdentity returns the identity of the current process\nfunc CurrentIdentity() Identity {\n\treturn Identity{UID: os.Getuid(), GID: os.Getegid()}\n}\n", "// +build !windows\n\npackage idtools // import \"github.com/docker/docker/pkg/idtools\"\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"sync\"\n\t\"syscall\"\n\n\t\"github.com/docker/docker/pkg/system\"\n\t\"github.com/opencontainers/runc/libcontainer/user\"\n\t\"github.com/pkg/errors\"\n)\n\nvar (\n\tentOnce   sync.Once\n\tgetentCmd string\n)\n\nfunc mkdirAs(path string, mode os.FileMode, owner Identity, mkAll, chownExisting bool) error {\n\t// make an array containing the original path asked for, plus (for mkAll == true)\n\t// all path components leading up to the complete path that don't exist before we MkdirAll\n\t// so that we can chown all of them properly at the end.  If chownExisting is false, we won't\n\t// chown the full directory path if it exists\n\n\tvar paths []string\n\n\tstat, err := system.Stat(path)\n\tif err == nil {\n\t\tif !stat.IsDir() {\n\t\t\treturn &os.PathError{Op: \"mkdir\", Path: path, Err: syscall.ENOTDIR}\n\t\t}\n\t\tif !chownExisting {\n\t\t\treturn nil\n\t\t}\n\n\t\t// short-circuit--we were called with an existing directory and chown was requested\n\t\treturn setPermissions(path, mode, owner.UID, owner.GID, stat)\n\t}\n\n\tif os.IsNotExist(err) {\n\t\tpaths = []string{path}\n\t}\n\n\tif mkAll {\n\t\t// walk back to \"/\" looking for directories which do not exist\n\t\t// and add them to the paths array for chown after creation\n\t\tdirPath := path\n\t\tfor {\n\t\t\tdirPath = filepath.Dir(dirPath)\n\t\t\tif dirPath == \"/\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif _, err := os.Stat(dirPath); err != nil && os.IsNotExist(err) {\n\t\t\t\tpaths = append(paths, dirPath)\n\t\t\t}\n\t\t}\n\t\tif err := system.MkdirAll(path, mode); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tif err := os.Mkdir(path, mode); err != nil && !os.IsExist(err) {\n\t\t\treturn err\n\t\t}\n\t}\n\t// even if it existed, we will chown the requested path + any subpaths that\n\t// didn't exist when we called MkdirAll\n\tfor _, pathComponent := range paths {\n\t\tif err := setPermissions(pathComponent, mode, owner.UID, owner.GID, nil); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// CanAccess takes a valid (existing) directory and a uid, gid pair and determines\n// if that uid, gid pair has access (execute bit) to the directory\nfunc CanAccess(path string, pair Identity) bool {\n\tstatInfo, err := system.Stat(path)\n\tif err != nil {\n\t\treturn false\n\t}\n\tfileMode := os.FileMode(statInfo.Mode())\n\tpermBits := fileMode.Perm()\n\treturn accessible(statInfo.UID() == uint32(pair.UID),\n\t\tstatInfo.GID() == uint32(pair.GID), permBits)\n}\n\nfunc accessible(isOwner, isGroup bool, perms os.FileMode) bool {\n\tif isOwner && (perms&0100 == 0100) {\n\t\treturn true\n\t}\n\tif isGroup && (perms&0010 == 0010) {\n\t\treturn true\n\t}\n\tif perms&0001 == 0001 {\n\t\treturn true\n\t}\n\treturn false\n}\n\n// LookupUser uses traditional local system files lookup (from libcontainer/user) on a username,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupUser(name string) (user.User, error) {\n\t// first try a local system files lookup using existing capabilities\n\tusr, err := user.LookupUser(name)\n\tif err == nil {\n\t\treturn usr, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured passwd dbs\n\tusr, err = getentUser(name)\n\tif err != nil {\n\t\treturn user.User{}, err\n\t}\n\treturn usr, nil\n}\n\n// LookupUID uses traditional local system files lookup (from libcontainer/user) on a uid,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupUID(uid int) (user.User, error) {\n\t// first try a local system files lookup using existing capabilities\n\tusr, err := user.LookupUid(uid)\n\tif err == nil {\n\t\treturn usr, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured passwd dbs\n\treturn getentUser(strconv.Itoa(uid))\n}\n\nfunc getentUser(name string) (user.User, error) {\n\treader, err := callGetent(\"passwd\", name)\n\tif err != nil {\n\t\treturn user.User{}, err\n\t}\n\tusers, err := user.ParsePasswd(reader)\n\tif err != nil {\n\t\treturn user.User{}, err\n\t}\n\tif len(users) == 0 {\n\t\treturn user.User{}, fmt.Errorf(\"getent failed to find passwd entry for %q\", name)\n\t}\n\treturn users[0], nil\n}\n\n// LookupGroup uses traditional local system files lookup (from libcontainer/user) on a group name,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupGroup(name string) (user.Group, error) {\n\t// first try a local system files lookup using existing capabilities\n\tgroup, err := user.LookupGroup(name)\n\tif err == nil {\n\t\treturn group, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured group dbs\n\treturn getentGroup(name)\n}\n\n// LookupGID uses traditional local system files lookup (from libcontainer/user) on a group ID,\n// followed by a call to `getent` for supporting host configured non-files passwd and group dbs\nfunc LookupGID(gid int) (user.Group, error) {\n\t// first try a local system files lookup using existing capabilities\n\tgroup, err := user.LookupGid(gid)\n\tif err == nil {\n\t\treturn group, nil\n\t}\n\t// local files lookup failed; attempt to call `getent` to query configured group dbs\n\treturn getentGroup(strconv.Itoa(gid))\n}\n\nfunc getentGroup(name string) (user.Group, error) {\n\treader, err := callGetent(\"group\", name)\n\tif err != nil {\n\t\treturn user.Group{}, err\n\t}\n\tgroups, err := user.ParseGroup(reader)\n\tif err != nil {\n\t\treturn user.Group{}, err\n\t}\n\tif len(groups) == 0 {\n\t\treturn user.Group{}, fmt.Errorf(\"getent failed to find groups entry for %q\", name)\n\t}\n\treturn groups[0], nil\n}\n\nfunc callGetent(database, key string) (io.Reader, error) {\n\tentOnce.Do(func() { getentCmd, _ = resolveBinary(\"getent\") })\n\t// if no `getent` command on host, can't do anything else\n\tif getentCmd == \"\" {\n\t\treturn nil, fmt.Errorf(\"unable to find getent command\")\n\t}\n\tout, err := execCmd(getentCmd, database, key)\n\tif err != nil {\n\t\texitCode, errC := system.GetExitCode(err)\n\t\tif errC != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tswitch exitCode {\n\t\tcase 1:\n\t\t\treturn nil, fmt.Errorf(\"getent reported invalid parameters/database unknown\")\n\t\tcase 2:\n\t\t\treturn nil, fmt.Errorf(\"getent unable to find entry %q in %s database\", key, database)\n\t\tcase 3:\n\t\t\treturn nil, fmt.Errorf(\"getent database doesn't support enumeration\")\n\t\tdefault:\n\t\t\treturn nil, err\n\t\t}\n\n\t}\n\treturn bytes.NewReader(out), nil\n}\n\n// setPermissions performs a chown/chmod only if the uid/gid don't match what's requested\n// Normally a Chown is a no-op if uid/gid match, but in some cases this can still cause an error, e.g. if the\n// dir is on an NFS share, so don't call chown unless we absolutely must.\n// Likewise for setting permissions.\nfunc setPermissions(p string, mode os.FileMode, uid, gid int, stat *system.StatT) error {\n\tif stat == nil {\n\t\tvar err error\n\t\tstat, err = system.Stat(p)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif os.FileMode(stat.Mode()).Perm() != mode.Perm() {\n\t\tif err := os.Chmod(p, mode.Perm()); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif stat.UID() == uint32(uid) && stat.GID() == uint32(gid) {\n\t\treturn nil\n\t}\n\treturn os.Chown(p, uid, gid)\n}\n\n// NewIdentityMapping takes a requested username and\n// using the data from /etc/sub{uid,gid} ranges, creates the\n// proper uid and gid remapping ranges for that user/group pair\nfunc NewIdentityMapping(name string) (*IdentityMapping, error) {\n\tusr, err := LookupUser(name)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Could not get user for username %s: %v\", name, err)\n\t}\n\n\tuid := strconv.Itoa(usr.Uid)\n\n\tsubuidRangesWithUserName, err := parseSubuid(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsubgidRangesWithUserName, err := parseSubgid(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsubuidRangesWithUID, err := parseSubuid(uid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsubgidRangesWithUID, err := parseSubgid(uid)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsubuidRanges := append(subuidRangesWithUserName, subuidRangesWithUID...)\n\tsubgidRanges := append(subgidRangesWithUserName, subgidRangesWithUID...)\n\n\tif len(subuidRanges) == 0 {\n\t\treturn nil, errors.Errorf(\"no subuid ranges found for user %q\", name)\n\t}\n\tif len(subgidRanges) == 0 {\n\t\treturn nil, errors.Errorf(\"no subgid ranges found for user %q\", name)\n\t}\n\n\treturn &IdentityMapping{\n\t\tuids: createIDMap(subuidRanges),\n\t\tgids: createIDMap(subgidRanges),\n\t}, nil\n}\n", "// Package local provides the default implementation for volumes. It\n// is used to mount data volume containers and directories local to\n// the host server.\npackage local // import \"github.com/docker/docker/volume/local\"\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/docker/docker/daemon/names\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"github.com/docker/docker/pkg/idtools\"\n\t\"github.com/docker/docker/quota\"\n\t\"github.com/docker/docker/volume\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// VolumeDataPathName is the name of the directory where the volume data is stored.\n// It uses a very distinctive name to avoid collisions migrating data between\n// Docker versions.\nconst (\n\tVolumeDataPathName = \"_data\"\n\tvolumesPathName    = \"volumes\"\n)\n\nvar (\n\t// ErrNotFound is the typed error returned when the requested volume name can't be found\n\tErrNotFound = fmt.Errorf(\"volume not found\")\n\t// volumeNameRegex ensures the name assigned for the volume is valid.\n\t// This name is used to create the bind directory, so we need to avoid characters that\n\t// would make the path to escape the root directory.\n\tvolumeNameRegex = names.RestrictedNamePattern\n)\n\ntype activeMount struct {\n\tcount   uint64\n\tmounted bool\n}\n\n// New instantiates a new Root instance with the provided scope. Scope\n// is the base path that the Root instance uses to store its\n// volumes. The base path is created here if it does not exist.\nfunc New(scope string, rootIdentity idtools.Identity) (*Root, error) {\n\trootDirectory := filepath.Join(scope, volumesPathName)\n\n\tif err := idtools.MkdirAllAndChown(rootDirectory, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, err\n\t}\n\n\tr := &Root{\n\t\tscope:        scope,\n\t\tpath:         rootDirectory,\n\t\tvolumes:      make(map[string]*localVolume),\n\t\trootIdentity: rootIdentity,\n\t}\n\n\tdirs, err := ioutil.ReadDir(rootDirectory)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif r.quotaCtl, err = quota.NewControl(rootDirectory); err != nil {\n\t\tlogrus.Debugf(\"No quota support for local volumes in %s: %v\", rootDirectory, err)\n\t}\n\n\tfor _, d := range dirs {\n\t\tif !d.IsDir() {\n\t\t\tcontinue\n\t\t}\n\n\t\tname := filepath.Base(d.Name())\n\t\tv := &localVolume{\n\t\t\tdriverName: r.Name(),\n\t\t\tname:       name,\n\t\t\tpath:       r.DataPath(name),\n\t\t\tquotaCtl:   r.quotaCtl,\n\t\t}\n\t\tr.volumes[name] = v\n\t\toptsFilePath := filepath.Join(rootDirectory, name, \"opts.json\")\n\t\tif b, err := ioutil.ReadFile(optsFilePath); err == nil {\n\t\t\topts := optsConfig{}\n\t\t\tif err := json.Unmarshal(b, &opts); err != nil {\n\t\t\t\treturn nil, errors.Wrapf(err, \"error while unmarshaling volume options for volume: %s\", name)\n\t\t\t}\n\t\t\t// Make sure this isn't an empty optsConfig.\n\t\t\t// This could be empty due to buggy behavior in older versions of Docker.\n\t\t\tif !reflect.DeepEqual(opts, optsConfig{}) {\n\t\t\t\tv.opts = &opts\n\t\t\t}\n\t\t\t// unmount anything that may still be mounted (for example, from an\n\t\t\t// unclean shutdown). This is a no-op on windows\n\t\t\tunmount(v.path)\n\t\t}\n\t}\n\n\treturn r, nil\n}\n\n// Root implements the Driver interface for the volume package and\n// manages the creation/removal of volumes. It uses only standard vfs\n// commands to create/remove dirs within its provided scope.\ntype Root struct {\n\tm            sync.Mutex\n\tscope        string\n\tpath         string\n\tquotaCtl     *quota.Control\n\tvolumes      map[string]*localVolume\n\trootIdentity idtools.Identity\n}\n\n// List lists all the volumes\nfunc (r *Root) List() ([]volume.Volume, error) {\n\tvar ls []volume.Volume\n\tr.m.Lock()\n\tfor _, v := range r.volumes {\n\t\tls = append(ls, v)\n\t}\n\tr.m.Unlock()\n\treturn ls, nil\n}\n\n// DataPath returns the constructed path of this volume.\nfunc (r *Root) DataPath(volumeName string) string {\n\treturn filepath.Join(r.path, volumeName, VolumeDataPathName)\n}\n\n// Name returns the name of Root, defined in the volume package in the DefaultDriverName constant.\nfunc (r *Root) Name() string {\n\treturn volume.DefaultDriverName\n}\n\n// Create creates a new volume.Volume with the provided name, creating\n// the underlying directory tree required for this volume in the\n// process.\nfunc (r *Root) Create(name string, opts map[string]string) (volume.Volume, error) {\n\tif err := r.validateName(name); err != nil {\n\t\treturn nil, err\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tv, exists := r.volumes[name]\n\tif exists {\n\t\treturn v, nil\n\t}\n\n\tpath := r.DataPath(name)\n\tvolRoot := filepath.Dir(path)\n\t// Root dir does not need to be accessed by the remapped root\n\tif err := idtools.MkdirAllAndChown(volRoot, 0701, idtools.CurrentIdentity()); err != nil {\n\t\treturn nil, errors.Wrapf(errdefs.System(err), \"error while creating volume root path '%s'\", volRoot)\n\t}\n\n\t// Remapped root does need access to the data path\n\tif err := idtools.MkdirAllAndChown(path, 0755, r.rootIdentity); err != nil {\n\t\treturn nil, errors.Wrapf(errdefs.System(err), \"error while creating volume data path '%s'\", path)\n\t}\n\n\tvar err error\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(filepath.Dir(path))\n\t\t}\n\t}()\n\n\tv = &localVolume{\n\t\tdriverName: r.Name(),\n\t\tname:       name,\n\t\tpath:       path,\n\t\tquotaCtl:   r.quotaCtl,\n\t}\n\n\tif len(opts) != 0 {\n\t\tif err = setOpts(v, opts); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tvar b []byte\n\t\tb, err = json.Marshal(v.opts)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err = ioutil.WriteFile(filepath.Join(filepath.Dir(path), \"opts.json\"), b, 0600); err != nil {\n\t\t\treturn nil, errdefs.System(errors.Wrap(err, \"error while persisting volume options\"))\n\t\t}\n\t}\n\n\tr.volumes[name] = v\n\treturn v, nil\n}\n\n// Remove removes the specified volume and all underlying data. If the\n// given volume does not belong to this driver and an error is\n// returned. The volume is reference counted, if all references are\n// not released then the volume is not removed.\nfunc (r *Root) Remove(v volume.Volume) error {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tlv, ok := v.(*localVolume)\n\tif !ok {\n\t\treturn errdefs.System(errors.Errorf(\"unknown volume type %T\", v))\n\t}\n\n\tif lv.active.count > 0 {\n\t\treturn errdefs.System(errors.Errorf(\"volume has active mounts\"))\n\t}\n\n\tif err := lv.unmount(); err != nil {\n\t\treturn err\n\t}\n\n\trealPath, err := filepath.EvalSymlinks(lv.path)\n\tif err != nil {\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\trealPath = filepath.Dir(lv.path)\n\t}\n\n\tif !r.scopedPath(realPath) {\n\t\treturn errdefs.System(errors.Errorf(\"Unable to remove a directory outside of the local volume root %s: %s\", r.scope, realPath))\n\t}\n\n\tif err := removePath(realPath); err != nil {\n\t\treturn err\n\t}\n\n\tdelete(r.volumes, lv.name)\n\treturn removePath(filepath.Dir(lv.path))\n}\n\nfunc removePath(path string) error {\n\tif err := os.RemoveAll(path); err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errdefs.System(errors.Wrapf(err, \"error removing volume path '%s'\", path))\n\t}\n\treturn nil\n}\n\n// Get looks up the volume for the given name and returns it if found\nfunc (r *Root) Get(name string) (volume.Volume, error) {\n\tr.m.Lock()\n\tv, exists := r.volumes[name]\n\tr.m.Unlock()\n\tif !exists {\n\t\treturn nil, ErrNotFound\n\t}\n\treturn v, nil\n}\n\n// Scope returns the local volume scope\nfunc (r *Root) Scope() string {\n\treturn volume.LocalScope\n}\n\nfunc (r *Root) validateName(name string) error {\n\tif len(name) == 1 {\n\t\treturn errdefs.InvalidParameter(errors.New(\"volume name is too short, names should be at least two alphanumeric characters\"))\n\t}\n\tif !volumeNameRegex.MatchString(name) {\n\t\treturn errdefs.InvalidParameter(errors.Errorf(\"%q includes invalid characters for a local volume name, only %q are allowed. If you intended to pass a host directory, use absolute path\", name, names.RestrictedNameChars))\n\t}\n\treturn nil\n}\n\n// localVolume implements the Volume interface from the volume package and\n// represents the volumes created by Root.\ntype localVolume struct {\n\tm sync.Mutex\n\t// unique name of the volume\n\tname string\n\t// path is the path on the host where the data lives\n\tpath string\n\t// driverName is the name of the driver that created the volume.\n\tdriverName string\n\t// opts is the parsed list of options used to create the volume\n\topts *optsConfig\n\t// active refcounts the active mounts\n\tactive activeMount\n\t// reference to Root instances quotaCtl\n\tquotaCtl *quota.Control\n}\n\n// Name returns the name of the given Volume.\nfunc (v *localVolume) Name() string {\n\treturn v.name\n}\n\n// DriverName returns the driver that created the given Volume.\nfunc (v *localVolume) DriverName() string {\n\treturn v.driverName\n}\n\n// Path returns the data location.\nfunc (v *localVolume) Path() string {\n\treturn v.path\n}\n\n// CachedPath returns the data location\nfunc (v *localVolume) CachedPath() string {\n\treturn v.path\n}\n\n// Mount implements the localVolume interface, returning the data location.\n// If there are any provided mount options, the resources will be mounted at this point\nfunc (v *localVolume) Mount(id string) (string, error) {\n\tv.m.Lock()\n\tdefer v.m.Unlock()\n\tif v.needsMount() {\n\t\tif !v.active.mounted {\n\t\t\tif err := v.mount(); err != nil {\n\t\t\t\treturn \"\", errdefs.System(err)\n\t\t\t}\n\t\t\tv.active.mounted = true\n\t\t}\n\t\tv.active.count++\n\t}\n\tif err := v.postMount(); err != nil {\n\t\treturn \"\", err\n\t}\n\treturn v.path, nil\n}\n\n// Unmount dereferences the id, and if it is the last reference will unmount any resources\n// that were previously mounted.\nfunc (v *localVolume) Unmount(id string) error {\n\tv.m.Lock()\n\tdefer v.m.Unlock()\n\n\t// Always decrement the count, even if the unmount fails\n\t// Essentially docker doesn't care if this fails, it will send an error, but\n\t// ultimately there's nothing that can be done. If we don't decrement the count\n\t// this volume can never be removed until a daemon restart occurs.\n\tif v.needsMount() {\n\t\tv.active.count--\n\t}\n\n\tif v.active.count > 0 {\n\t\treturn nil\n\t}\n\n\treturn v.unmount()\n}\n\nfunc (v *localVolume) Status() map[string]interface{} {\n\treturn nil\n}\n\n// getAddress finds out address/hostname from options\nfunc getAddress(opts string) string {\n\toptsList := strings.Split(opts, \",\")\n\tfor i := 0; i < len(optsList); i++ {\n\t\tif strings.HasPrefix(optsList[i], \"addr=\") {\n\t\t\taddr := strings.SplitN(optsList[i], \"=\", 2)[1]\n\t\t\treturn addr\n\t\t}\n\t}\n\treturn \"\"\n}\n"], "filenames": ["daemon/container_operations_unix.go", "daemon/create.go", "daemon/daemon.go", "daemon/daemon_unix.go", "daemon/graphdriver/aufs/aufs.go", "daemon/graphdriver/btrfs/btrfs.go", "daemon/graphdriver/fuse-overlayfs/fuseoverlayfs.go", "daemon/graphdriver/overlay/overlay.go", "daemon/graphdriver/overlay2/overlay.go", "daemon/graphdriver/vfs/driver.go", "daemon/graphdriver/zfs/zfs.go", "pkg/idtools/idtools.go", "pkg/idtools/idtools_unix.go", "volume/local/local.go"], "buggy_code_start_loc": [469, 197, 798, 1199, 132, 73, 91, 159, 168, 41, 107, 38, 43, 53], "buggy_code_end_loc": [471, 203, 1398, 1241, 144, 564, 219, 315, 347, 145, 112, 236, 223, 158], "fixing_code_start_loc": [469, 197, 798, 1199, 132, 73, 91, 158, 168, 41, 107, 38, 43, 53], "fixing_code_end_loc": [471, 201, 1396, 1247, 141, 560, 215, 313, 343, 144, 108, 242, 230, 165], "type": "CWE-22", "message": "In Docker before versions 9.03.15, 20.10.3 there is a vulnerability involving the --userns-remap option in which access to remapped root allows privilege escalation to real root. When using \"--userns-remap\", if the root user in the remapped namespace has access to the host filesystem they can modify files under \"/var/lib/docker/<remapping>\" that cause writing files with extended privileges. Versions 20.10.3 and 19.03.15 contain patches that prevent privilege escalation from remapped user.", "other": {"cve": {"id": "CVE-2021-21284", "sourceIdentifier": "security-advisories@github.com", "published": "2021-02-02T18:15:11.827", "lastModified": "2022-04-29T19:22:01.677", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In Docker before versions 9.03.15, 20.10.3 there is a vulnerability involving the --userns-remap option in which access to remapped root allows privilege escalation to real root. When using \"--userns-remap\", if the root user in the remapped namespace has access to the host filesystem they can modify files under \"/var/lib/docker/<remapping>\" that cause writing files with extended privileges. Versions 20.10.3 and 19.03.15 contain patches that prevent privilege escalation from remapped user."}, {"lang": "es", "value": "En Docker versiones anteriores a 9.03.15, 20.10.3, se presenta una vulnerabilidad que involucra la opci\u00f3n --userns-remap en la que un acceso a una root reasignada permite una escalada de privilegios a la root actual.&#xa0;Cuando se usa \"--userns-remap\", si el usuario root en el espacio de nombres reasignado tiene acceso al sistema de archivos del host, puede modificar archivos en \"/var/lib/docker/(remapping)\" que causa la escritura de archivos con privilegios extendidos.&#xa0;Las versiones 20.10.3 y 19.03.15 contienen parches que evitan una escalada de privilegios del usuario reasignado"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:A/AC:L/PR:L/UI:N/S:C/C:N/I:H/A:N", "attackVector": "ADJACENT_NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 6.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.3, "impactScore": 4.0}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:A/AC:L/PR:L/UI:N/S:C/C:N/I:H/A:N", "attackVector": "ADJACENT_NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 6.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.3, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:A/AC:L/Au:S/C:N/I:P/A:N", "accessVector": "ADJACENT_NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 2.7}, "baseSeverity": "LOW", "exploitabilityScore": 5.1, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:docker:docker:*:*:*:*:*:*:*:*", "versionEndExcluding": "19.03.15", "matchCriteriaId": "C6B22016-AB78-4E82-9F65-AEC2526F3EDF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:docker:docker:*:*:*:*:*:*:*:*", "versionStartIncluding": "20.0.0", "versionEndExcluding": "20.10.3", "matchCriteriaId": "B4427D14-D219-490B-8467-40FE253775A1"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:e-series_santricity_os_controller:*:*:*:*:*:*:*:*", "versionStartIncluding": "11.0.0", "versionEndIncluding": "11.60.3", "matchCriteriaId": "BD1E9594-C46F-40D1-8BC2-6B16635B55C4"}]}]}], "references": [{"url": "https://docs.docker.com/engine/release-notes/#20103", "source": "security-advisories@github.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/moby/moby/commit/64bd4485b3a66a597c02c95f5776395e540b2c7c", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/moby/moby/releases/tag/v19.03.15", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/moby/moby/releases/tag/v20.10.3", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/moby/moby/security/advisories/GHSA-7452-xqpj-6rpc", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://security.gentoo.org/glsa/202107-23", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20210226-0005/", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2021/dsa-4865", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/moby/moby/commit/64bd4485b3a66a597c02c95f5776395e540b2c7c"}}