{"buggy_code": ["Documentation for /proc/sys/fs/*\tkernel version 2.2.10\n\t(c) 1998, 1999,  Rik van Riel <riel@nl.linux.org>\n\t(c) 2009,        Shen Feng<shen@cn.fujitsu.com>\n\nFor general info and legal blurb, please look in README.\n\n==============================================================\n\nThis file contains documentation for the sysctl files in\n/proc/sys/fs/ and is valid for Linux kernel version 2.2.\n\nThe files in this directory can be used to tune and monitor\nmiscellaneous and general things in the operation of the Linux\nkernel. Since some of the files _can_ be used to screw up your\nsystem, it is advisable to read both documentation and source\nbefore actually making adjustments.\n\n1. /proc/sys/fs\n----------------------------------------------------------\n\nCurrently, these files are in /proc/sys/fs:\n- aio-max-nr\n- aio-nr\n- dentry-state\n- dquot-max\n- dquot-nr\n- file-max\n- file-nr\n- inode-max\n- inode-nr\n- inode-state\n- nr_open\n- overflowuid\n- overflowgid\n- pipe-user-pages-hard\n- pipe-user-pages-soft\n- protected_hardlinks\n- protected_symlinks\n- suid_dumpable\n- super-max\n- super-nr\n\n==============================================================\n\naio-nr & aio-max-nr:\n\naio-nr is the running total of the number of events specified on the\nio_setup system call for all currently active aio contexts.  If aio-nr\nreaches aio-max-nr then io_setup will fail with EAGAIN.  Note that\nraising aio-max-nr does not result in the pre-allocation or re-sizing\nof any kernel data structures.\n\n==============================================================\n\ndentry-state:\n\nFrom linux/fs/dentry.c:\n--------------------------------------------------------------\nstruct {\n        int nr_dentry;\n        int nr_unused;\n        int age_limit;         /* age in seconds */\n        int want_pages;        /* pages requested by system */\n        int dummy[2];\n} dentry_stat = {0, 0, 45, 0,};\n-------------------------------------------------------------- \n\nDentries are dynamically allocated and deallocated, and\nnr_dentry seems to be 0 all the time. Hence it's safe to\nassume that only nr_unused, age_limit and want_pages are\nused. Nr_unused seems to be exactly what its name says.\nAge_limit is the age in seconds after which dcache entries\ncan be reclaimed when memory is short and want_pages is\nnonzero when shrink_dcache_pages() has been called and the\ndcache isn't pruned yet.\n\n==============================================================\n\ndquot-max & dquot-nr:\n\nThe file dquot-max shows the maximum number of cached disk\nquota entries.\n\nThe file dquot-nr shows the number of allocated disk quota\nentries and the number of free disk quota entries.\n\nIf the number of free cached disk quotas is very low and\nyou have some awesome number of simultaneous system users,\nyou might want to raise the limit.\n\n==============================================================\n\nfile-max & file-nr:\n\nThe value in file-max denotes the maximum number of file-\nhandles that the Linux kernel will allocate. When you get lots\nof error messages about running out of file handles, you might\nwant to increase this limit.\n\nHistorically,the kernel was able to allocate file handles\ndynamically, but not to free them again. The three values in\nfile-nr denote the number of allocated file handles, the number\nof allocated but unused file handles, and the maximum number of\nfile handles. Linux 2.6 always reports 0 as the number of free\nfile handles -- this is not an error, it just means that the\nnumber of allocated file handles exactly matches the number of\nused file handles.\n\nAttempts to allocate more file descriptors than file-max are\nreported with printk, look for \"VFS: file-max limit <number>\nreached\".\n==============================================================\n\nnr_open:\n\nThis denotes the maximum number of file-handles a process can\nallocate. Default value is 1024*1024 (1048576) which should be\nenough for most machines. Actual limit depends on RLIMIT_NOFILE\nresource limit.\n\n==============================================================\n\ninode-max, inode-nr & inode-state:\n\nAs with file handles, the kernel allocates the inode structures\ndynamically, but can't free them yet.\n\nThe value in inode-max denotes the maximum number of inode\nhandlers. This value should be 3-4 times larger than the value\nin file-max, since stdin, stdout and network sockets also\nneed an inode struct to handle them. When you regularly run\nout of inodes, you need to increase this value.\n\nThe file inode-nr contains the first two items from\ninode-state, so we'll skip to that file...\n\nInode-state contains three actual numbers and four dummies.\nThe actual numbers are, in order of appearance, nr_inodes,\nnr_free_inodes and preshrink.\n\nNr_inodes stands for the number of inodes the system has\nallocated, this can be slightly more than inode-max because\nLinux allocates them one pageful at a time.\n\nNr_free_inodes represents the number of free inodes (?) and\npreshrink is nonzero when the nr_inodes > inode-max and the\nsystem needs to prune the inode list instead of allocating\nmore.\n\n==============================================================\n\noverflowgid & overflowuid:\n\nSome filesystems only support 16-bit UIDs and GIDs, although in Linux\nUIDs and GIDs are 32 bits. When one of these filesystems is mounted\nwith writes enabled, any UID or GID that would exceed 65535 is translated\nto a fixed value before being written to disk.\n\nThese sysctls allow you to change the value of the fixed UID and GID.\nThe default is 65534.\n\n==============================================================\n\npipe-user-pages-hard:\n\nMaximum total number of pages a non-privileged user may allocate for pipes.\nOnce this limit is reached, no new pipes may be allocated until usage goes\nbelow the limit again. When set to 0, no limit is applied, which is the default\nsetting.\n\n==============================================================\n\npipe-user-pages-soft:\n\nMaximum total number of pages a non-privileged user may allocate for pipes\nbefore the pipe size gets limited to a single page. Once this limit is reached,\nnew pipes will be limited to a single page in size for this user in order to\nlimit total memory usage, and trying to increase them using fcntl() will be\ndenied until usage goes below the limit again. The default value allows to\nallocate up to 1024 pipes at their default size. When set to 0, no limit is\napplied.\n\n==============================================================\n\nprotected_hardlinks:\n\nA long-standing class of security issues is the hardlink-based\ntime-of-check-time-of-use race, most commonly seen in world-writable\ndirectories like /tmp. The common method of exploitation of this flaw\nis to cross privilege boundaries when following a given hardlink (i.e. a\nroot process follows a hardlink created by another user). Additionally,\non systems without separated partitions, this stops unauthorized users\nfrom \"pinning\" vulnerable setuid/setgid files against being upgraded by\nthe administrator, or linking to special files.\n\nWhen set to \"0\", hardlink creation behavior is unrestricted.\n\nWhen set to \"1\" hardlinks cannot be created by users if they do not\nalready own the source file, or do not have read/write access to it.\n\nThis protection is based on the restrictions in Openwall and grsecurity.\n\n==============================================================\n\nprotected_symlinks:\n\nA long-standing class of security issues is the symlink-based\ntime-of-check-time-of-use race, most commonly seen in world-writable\ndirectories like /tmp. The common method of exploitation of this flaw\nis to cross privilege boundaries when following a given symlink (i.e. a\nroot process follows a symlink belonging to another user). For a likely\nincomplete list of hundreds of examples across the years, please see:\nhttp://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=/tmp\n\nWhen set to \"0\", symlink following behavior is unrestricted.\n\nWhen set to \"1\" symlinks are permitted to be followed only when outside\na sticky world-writable directory, or when the uid of the symlink and\nfollower match, or when the directory owner matches the symlink's owner.\n\nThis protection is based on the restrictions in Openwall and grsecurity.\n\n==============================================================\n\nsuid_dumpable:\n\nThis value can be used to query and set the core dump mode for setuid\nor otherwise protected/tainted binaries. The modes are\n\n0 - (default) - traditional behaviour. Any process which has changed\n\tprivilege levels or is execute only will not be dumped.\n1 - (debug) - all processes dump core when possible. The core dump is\n\towned by the current user and no security is applied. This is\n\tintended for system debugging situations only. Ptrace is unchecked.\n\tThis is insecure as it allows regular users to examine the memory\n\tcontents of privileged processes.\n2 - (suidsafe) - any binary which normally would not be dumped is dumped\n\tanyway, but only if the \"core_pattern\" kernel sysctl is set to\n\teither a pipe handler or a fully qualified path. (For more details\n\ton this limitation, see CVE-2006-2451.) This mode is appropriate\n\twhen administrators are attempting to debug problems in a normal\n\tenvironment, and either have a core dump pipe handler that knows\n\tto treat privileged core dumps with care, or specific directory\n\tdefined for catching core dumps. If a core dump happens without\n\ta pipe handler or fully qualifid path, a message will be emitted\n\tto syslog warning about the lack of a correct setting.\n\n==============================================================\n\nsuper-max & super-nr:\n\nThese numbers control the maximum number of superblocks, and\nthus the maximum number of mounted filesystems the kernel\ncan have. You only need to increase super-max if you need to\nmount more filesystems than the current value in super-max\nallows you to.\n\n==============================================================\n\naio-nr & aio-max-nr:\n\naio-nr shows the current system-wide number of asynchronous io\nrequests.  aio-max-nr allows you to change the maximum value\naio-nr can grow to.\n\n==============================================================\n\n\n2. /proc/sys/fs/binfmt_misc\n----------------------------------------------------------\n\nDocumentation for the files in /proc/sys/fs/binfmt_misc is\nin Documentation/binfmt_misc.txt.\n\n\n3. /proc/sys/fs/mqueue - POSIX message queues filesystem\n----------------------------------------------------------\n\nThe \"mqueue\"  filesystem provides  the necessary kernel features to enable the\ncreation of a  user space  library that  implements  the  POSIX message queues\nAPI (as noted by the  MSG tag in the  POSIX 1003.1-2001 version  of the System\nInterfaces specification.)\n\nThe \"mqueue\" filesystem contains values for determining/setting  the amount of\nresources used by the file system.\n\n/proc/sys/fs/mqueue/queues_max is a read/write  file for  setting/getting  the\nmaximum number of message queues allowed on the system.\n\n/proc/sys/fs/mqueue/msg_max  is  a  read/write file  for  setting/getting  the\nmaximum number of messages in a queue value.  In fact it is the limiting value\nfor another (user) limit which is set in mq_open invocation. This attribute of\na queue must be less or equal then msg_max.\n\n/proc/sys/fs/mqueue/msgsize_max is  a read/write  file for setting/getting the\nmaximum  message size value (it is every  message queue's attribute set during\nits creation).\n\n/proc/sys/fs/mqueue/msg_default is  a read/write  file for setting/getting the\ndefault number of messages in a queue value if attr parameter of mq_open(2) is\nNULL. If it exceed msg_max, the default value is initialized msg_max.\n\n/proc/sys/fs/mqueue/msgsize_default is a read/write file for setting/getting\nthe default message size value if attr parameter of mq_open(2) is NULL. If it\nexceed msgsize_max, the default value is initialized msgsize_max.\n\n4. /proc/sys/fs/epoll - Configuration options for the epoll interface\n--------------------------------------------------------\n\nThis directory contains configuration options for the epoll(7) interface.\n\nmax_user_watches\n----------------\n\nEvery epoll file descriptor can store a number of files to be monitored\nfor event readiness. Each one of these monitored files constitutes a \"watch\".\nThis configuration option sets the maximum number of \"watches\" that are\nallowed for each user.\nEach \"watch\" costs roughly 90 bytes on a 32bit kernel, and roughly 160 bytes\non a 64bit one.\nThe current default value for  max_user_watches  is the 1/32 of the available\nlow memory, divided for the \"watch\" cost in bytes.\n\n", "#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/ns_common.h>\n#include <linux/fs_pin.h>\n\nstruct mnt_namespace {\n\tatomic_t\t\tcount;\n\tstruct ns_common\tns;\n\tstruct mount *\troot;\n\tstruct list_head\tlist;\n\tstruct user_namespace\t*user_ns;\n\tstruct ucounts\t\t*ucounts;\n\tu64\t\t\tseq;\t/* Sequence number to prevent loops */\n\twait_queue_head_t poll;\n\tu64 event;\n};\n\nstruct mnt_pcp {\n\tint mnt_count;\n\tint mnt_writers;\n};\n\nstruct mountpoint {\n\tstruct hlist_node m_hash;\n\tstruct dentry *m_dentry;\n\tstruct hlist_head m_list;\n\tint m_count;\n};\n\nstruct mount {\n\tstruct hlist_node mnt_hash;\n\tstruct mount *mnt_parent;\n\tstruct dentry *mnt_mountpoint;\n\tstruct vfsmount mnt;\n\tunion {\n\t\tstruct rcu_head mnt_rcu;\n\t\tstruct llist_node mnt_llist;\n\t};\n#ifdef CONFIG_SMP\n\tstruct mnt_pcp __percpu *mnt_pcp;\n#else\n\tint mnt_count;\n\tint mnt_writers;\n#endif\n\tstruct list_head mnt_mounts;\t/* list of children, anchored here */\n\tstruct list_head mnt_child;\t/* and going through their mnt_child */\n\tstruct list_head mnt_instance;\t/* mount instance on sb->s_mounts */\n\tconst char *mnt_devname;\t/* Name of device e.g. /dev/dsk/hda1 */\n\tstruct list_head mnt_list;\n\tstruct list_head mnt_expire;\t/* link in fs-specific expiry list */\n\tstruct list_head mnt_share;\t/* circular list of shared mounts */\n\tstruct list_head mnt_slave_list;/* list of slave mounts */\n\tstruct list_head mnt_slave;\t/* slave list entry */\n\tstruct mount *mnt_master;\t/* slave is on master->mnt_slave_list */\n\tstruct mnt_namespace *mnt_ns;\t/* containing namespace */\n\tstruct mountpoint *mnt_mp;\t/* where is it mounted */\n\tstruct hlist_node mnt_mp_list;\t/* list mounts with the same mountpoint */\n#ifdef CONFIG_FSNOTIFY\n\tstruct hlist_head mnt_fsnotify_marks;\n\t__u32 mnt_fsnotify_mask;\n#endif\n\tint mnt_id;\t\t\t/* mount identifier */\n\tint mnt_group_id;\t\t/* peer group identifier */\n\tint mnt_expiry_mark;\t\t/* true if marked for expiry */\n\tstruct hlist_head mnt_pins;\n\tstruct fs_pin mnt_umount;\n\tstruct dentry *mnt_ex_mountpoint;\n};\n\n#define MNT_NS_INTERNAL ERR_PTR(-EINVAL) /* distinct from any mnt_namespace */\n\nstatic inline struct mount *real_mount(struct vfsmount *mnt)\n{\n\treturn container_of(mnt, struct mount, mnt);\n}\n\nstatic inline int mnt_has_parent(struct mount *mnt)\n{\n\treturn mnt != mnt->mnt_parent;\n}\n\nstatic inline int is_mounted(struct vfsmount *mnt)\n{\n\t/* neither detached nor internal? */\n\treturn !IS_ERR_OR_NULL(real_mount(mnt)->mnt_ns);\n}\n\nextern struct mount *__lookup_mnt(struct vfsmount *, struct dentry *);\nextern struct mount *__lookup_mnt_last(struct vfsmount *, struct dentry *);\n\nextern int __legitimize_mnt(struct vfsmount *, unsigned);\nextern bool legitimize_mnt(struct vfsmount *, unsigned);\n\nextern void __detach_mounts(struct dentry *dentry);\n\nstatic inline void detach_mounts(struct dentry *dentry)\n{\n\tif (!d_mountpoint(dentry))\n\t\treturn;\n\t__detach_mounts(dentry);\n}\n\nstatic inline void get_mnt_ns(struct mnt_namespace *ns)\n{\n\tatomic_inc(&ns->count);\n}\n\nextern seqlock_t mount_lock;\n\nstatic inline void lock_mount_hash(void)\n{\n\twrite_seqlock(&mount_lock);\n}\n\nstatic inline void unlock_mount_hash(void)\n{\n\twrite_sequnlock(&mount_lock);\n}\n\nstruct proc_mounts {\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\tint (*show)(struct seq_file *, struct vfsmount *);\n\tvoid *cached_mount;\n\tu64 cached_event;\n\tloff_t cached_index;\n};\n\nextern const struct seq_operations mounts_op;\n\nextern bool __is_local_mountpoint(struct dentry *dentry);\nstatic inline bool is_local_mountpoint(struct dentry *dentry)\n{\n\tif (!d_mountpoint(dentry))\n\t\treturn false;\n\n\treturn __is_local_mountpoint(dentry);\n}\n", "/*\n *  linux/fs/namespace.c\n *\n * (C) Copyright Al Viro 2000, 2001\n *\tReleased under GPL v2.\n *\n * Based on code from fs/super.c, copyright Linus Torvalds and others.\n * Heavily rewritten.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/capability.h>\n#include <linux/mnt_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/idr.h>\n#include <linux/init.h>\t\t/* init_rootfs */\n#include <linux/fs_struct.h>\t/* get_fs_root et.al. */\n#include <linux/fsnotify.h>\t/* fsnotify_vfsmount_delete */\n#include <linux/uaccess.h>\n#include <linux/proc_ns.h>\n#include <linux/magic.h>\n#include <linux/bootmem.h>\n#include <linux/task_work.h>\n#include \"pnode.h\"\n#include \"internal.h\"\n\nstatic unsigned int m_hash_mask __read_mostly;\nstatic unsigned int m_hash_shift __read_mostly;\nstatic unsigned int mp_hash_mask __read_mostly;\nstatic unsigned int mp_hash_shift __read_mostly;\n\nstatic __initdata unsigned long mhash_entries;\nstatic int __init set_mhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmhash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mhash_entries=\", set_mhash_entries);\n\nstatic __initdata unsigned long mphash_entries;\nstatic int __init set_mphash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmphash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mphash_entries=\", set_mphash_entries);\n\nstatic u64 event;\nstatic DEFINE_IDA(mnt_id_ida);\nstatic DEFINE_IDA(mnt_group_ida);\nstatic DEFINE_SPINLOCK(mnt_id_lock);\nstatic int mnt_id_start = 0;\nstatic int mnt_group_start = 1;\n\nstatic struct hlist_head *mount_hashtable __read_mostly;\nstatic struct hlist_head *mountpoint_hashtable __read_mostly;\nstatic struct kmem_cache *mnt_cache __read_mostly;\nstatic DECLARE_RWSEM(namespace_sem);\n\n/* /sys/fs */\nstruct kobject *fs_kobj;\nEXPORT_SYMBOL_GPL(fs_kobj);\n\n/*\n * vfsmount lock may be taken for read to prevent changes to the\n * vfsmount hash, ie. during mountpoint lookups or walking back\n * up the tree.\n *\n * It should be taken for write in all cases where the vfsmount\n * tree or hash is modified or when a vfsmount structure is modified.\n */\n__cacheline_aligned_in_smp DEFINE_SEQLOCK(mount_lock);\n\nstatic inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);\n\ttmp += ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> m_hash_shift);\n\treturn &mount_hashtable[tmp & m_hash_mask];\n}\n\nstatic inline struct hlist_head *mp_hash(struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> mp_hash_shift);\n\treturn &mountpoint_hashtable[tmp & mp_hash_mask];\n}\n\n/*\n * allocation is serialized by namespace_sem, but we need the spinlock to\n * serialize with freeing.\n */\nstatic int mnt_alloc_id(struct mount *mnt)\n{\n\tint res;\n\nretry:\n\tida_pre_get(&mnt_id_ida, GFP_KERNEL);\n\tspin_lock(&mnt_id_lock);\n\tres = ida_get_new_above(&mnt_id_ida, mnt_id_start, &mnt->mnt_id);\n\tif (!res)\n\t\tmnt_id_start = mnt->mnt_id + 1;\n\tspin_unlock(&mnt_id_lock);\n\tif (res == -EAGAIN)\n\t\tgoto retry;\n\n\treturn res;\n}\n\nstatic void mnt_free_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_id;\n\tspin_lock(&mnt_id_lock);\n\tida_remove(&mnt_id_ida, id);\n\tif (mnt_id_start > id)\n\t\tmnt_id_start = id;\n\tspin_unlock(&mnt_id_lock);\n}\n\n/*\n * Allocate a new peer group ID\n *\n * mnt_group_ida is protected by namespace_sem\n */\nstatic int mnt_alloc_group_id(struct mount *mnt)\n{\n\tint res;\n\n\tif (!ida_pre_get(&mnt_group_ida, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tres = ida_get_new_above(&mnt_group_ida,\n\t\t\t\tmnt_group_start,\n\t\t\t\t&mnt->mnt_group_id);\n\tif (!res)\n\t\tmnt_group_start = mnt->mnt_group_id + 1;\n\n\treturn res;\n}\n\n/*\n * Release a peer group ID\n */\nvoid mnt_release_group_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_group_id;\n\tida_remove(&mnt_group_ida, id);\n\tif (mnt_group_start > id)\n\t\tmnt_group_start = id;\n\tmnt->mnt_group_id = 0;\n}\n\n/*\n * vfsmount lock must be held for read\n */\nstatic inline void mnt_add_count(struct mount *mnt, int n)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_add(mnt->mnt_pcp->mnt_count, n);\n#else\n\tpreempt_disable();\n\tmnt->mnt_count += n;\n\tpreempt_enable();\n#endif\n}\n\n/*\n * vfsmount lock must be held for write\n */\nunsigned int mnt_get_count(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_count;\n#endif\n}\n\nstatic void drop_mountpoint(struct fs_pin *p)\n{\n\tstruct mount *m = container_of(p, struct mount, mnt_umount);\n\tdput(m->mnt_ex_mountpoint);\n\tpin_remove(p);\n\tmntput(&m->mnt);\n}\n\nstatic struct mount *alloc_vfsmnt(const char *name)\n{\n\tstruct mount *mnt = kmem_cache_zalloc(mnt_cache, GFP_KERNEL);\n\tif (mnt) {\n\t\tint err;\n\n\t\terr = mnt_alloc_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free_cache;\n\n\t\tif (name) {\n\t\t\tmnt->mnt_devname = kstrdup_const(name, GFP_KERNEL);\n\t\t\tif (!mnt->mnt_devname)\n\t\t\t\tgoto out_free_id;\n\t\t}\n\n#ifdef CONFIG_SMP\n\t\tmnt->mnt_pcp = alloc_percpu(struct mnt_pcp);\n\t\tif (!mnt->mnt_pcp)\n\t\t\tgoto out_free_devname;\n\n\t\tthis_cpu_add(mnt->mnt_pcp->mnt_count, 1);\n#else\n\t\tmnt->mnt_count = 1;\n\t\tmnt->mnt_writers = 0;\n#endif\n\n\t\tINIT_HLIST_NODE(&mnt->mnt_hash);\n\t\tINIT_LIST_HEAD(&mnt->mnt_child);\n\t\tINIT_LIST_HEAD(&mnt->mnt_mounts);\n\t\tINIT_LIST_HEAD(&mnt->mnt_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_expire);\n\t\tINIT_LIST_HEAD(&mnt->mnt_share);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave);\n\t\tINIT_HLIST_NODE(&mnt->mnt_mp_list);\n#ifdef CONFIG_FSNOTIFY\n\t\tINIT_HLIST_HEAD(&mnt->mnt_fsnotify_marks);\n#endif\n\t\tinit_fs_pin(&mnt->mnt_umount, drop_mountpoint);\n\t}\n\treturn mnt;\n\n#ifdef CONFIG_SMP\nout_free_devname:\n\tkfree_const(mnt->mnt_devname);\n#endif\nout_free_id:\n\tmnt_free_id(mnt);\nout_free_cache:\n\tkmem_cache_free(mnt_cache, mnt);\n\treturn NULL;\n}\n\n/*\n * Most r/o checks on a fs are for operations that take\n * discrete amounts of time, like a write() or unlink().\n * We must keep track of when those operations start\n * (for permission checks) and when they end, so that\n * we can determine when writes are able to occur to\n * a filesystem.\n */\n/*\n * __mnt_is_readonly: check whether a mount is read-only\n * @mnt: the mount to check for its write status\n *\n * This shouldn't be used directly ouside of the VFS.\n * It does not guarantee that the filesystem will stay\n * r/w, just that it is right *now*.  This can not and\n * should not be used in place of IS_RDONLY(inode).\n * mnt_want/drop_write() will _keep_ the filesystem\n * r/w.\n */\nint __mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_flags & MNT_READONLY)\n\t\treturn 1;\n\tif (mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__mnt_is_readonly);\n\nstatic inline void mnt_inc_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_inc(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers++;\n#endif\n}\n\nstatic inline void mnt_dec_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_dec(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers--;\n#endif\n}\n\nstatic unsigned int mnt_get_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_writers;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_writers;\n#endif\n}\n\nstatic int mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_sb->s_readonly_remount)\n\t\treturn 1;\n\t/* Order wrt setting s_flags/s_readonly_remount in do_remount() */\n\tsmp_rmb();\n\treturn __mnt_is_readonly(mnt);\n}\n\n/*\n * Most r/o & frozen checks on a fs are for operations that take discrete\n * amounts of time, like a write() or unlink().  We must keep track of when\n * those operations start (for permission checks) and when they end, so that we\n * can determine when writes are able to occur to a filesystem.\n */\n/**\n * __mnt_want_write - get write access to a mount without freeze protection\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mnt it read-write) before\n * returning success. This operation does not protect against filesystem being\n * frozen. When the write operation is finished, __mnt_drop_write() must be\n * called. This is effectively a refcount.\n */\nint __mnt_want_write(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint ret = 0;\n\n\tpreempt_disable();\n\tmnt_inc_writers(mnt);\n\t/*\n\t * The store to mnt_inc_writers must be visible before we pass\n\t * MNT_WRITE_HOLD loop below, so that the slowpath can see our\n\t * incremented count after it has set MNT_WRITE_HOLD.\n\t */\n\tsmp_mb();\n\twhile (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)\n\t\tcpu_relax();\n\t/*\n\t * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will\n\t * be set to match its requirements. So we must not load that until\n\t * MNT_WRITE_HOLD is cleared.\n\t */\n\tsmp_rmb();\n\tif (mnt_is_readonly(m)) {\n\t\tmnt_dec_writers(mnt);\n\t\tret = -EROFS;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * mnt_want_write - get write access to a mount\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mount is read-write, filesystem\n * is not frozen) before returning success.  When the write operation is\n * finished, mnt_drop_write() must be called.  This is effectively a refcount.\n */\nint mnt_want_write(struct vfsmount *m)\n{\n\tint ret;\n\n\tsb_start_write(m->mnt_sb);\n\tret = __mnt_want_write(m);\n\tif (ret)\n\t\tsb_end_write(m->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write);\n\n/**\n * mnt_clone_write - get write access to a mount\n * @mnt: the mount on which to take a write\n *\n * This is effectively like mnt_want_write, except\n * it must only be used to take an extra write reference\n * on a mountpoint that we already know has a write reference\n * on it. This allows some optimisation.\n *\n * After finished, mnt_drop_write must be called as usual to\n * drop the reference.\n */\nint mnt_clone_write(struct vfsmount *mnt)\n{\n\t/* superblock may be r/o */\n\tif (__mnt_is_readonly(mnt))\n\t\treturn -EROFS;\n\tpreempt_disable();\n\tmnt_inc_writers(real_mount(mnt));\n\tpreempt_enable();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mnt_clone_write);\n\n/**\n * __mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like __mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint __mnt_want_write_file(struct file *file)\n{\n\tif (!(file->f_mode & FMODE_WRITER))\n\t\treturn __mnt_want_write(file->f_path.mnt);\n\telse\n\t\treturn mnt_clone_write(file->f_path.mnt);\n}\n\n/**\n * mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint mnt_want_write_file(struct file *file)\n{\n\tint ret;\n\n\tsb_start_write(file->f_path.mnt->mnt_sb);\n\tret = __mnt_want_write_file(file);\n\tif (ret)\n\t\tsb_end_write(file->f_path.mnt->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write_file);\n\n/**\n * __mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done\n * performing writes to it.  Must be matched with\n * __mnt_want_write() call above.\n */\nvoid __mnt_drop_write(struct vfsmount *mnt)\n{\n\tpreempt_disable();\n\tmnt_dec_writers(real_mount(mnt));\n\tpreempt_enable();\n}\n\n/**\n * mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done performing writes to it and\n * also allows filesystem to be frozen again.  Must be matched with\n * mnt_want_write() call above.\n */\nvoid mnt_drop_write(struct vfsmount *mnt)\n{\n\t__mnt_drop_write(mnt);\n\tsb_end_write(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(mnt_drop_write);\n\nvoid __mnt_drop_write_file(struct file *file)\n{\n\t__mnt_drop_write(file->f_path.mnt);\n}\n\nvoid mnt_drop_write_file(struct file *file)\n{\n\tmnt_drop_write(file->f_path.mnt);\n}\nEXPORT_SYMBOL(mnt_drop_write_file);\n\nstatic int mnt_make_readonly(struct mount *mnt)\n{\n\tint ret = 0;\n\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t/*\n\t * After storing MNT_WRITE_HOLD, we'll read the counters. This store\n\t * should be visible before we do.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * With writers on hold, if this value is zero, then there are\n\t * definitely no active writers (although held writers may subsequently\n\t * increment the count, they'll have to wait, and decrement it after\n\t * seeing MNT_READONLY).\n\t *\n\t * It is OK to have counter incremented on one CPU and decremented on\n\t * another: the sum will add up correctly. The danger would be when we\n\t * sum up each counter, if we read a counter before it is incremented,\n\t * but then read another CPU's count which it has been subsequently\n\t * decremented from -- we would see more decrements than we should.\n\t * MNT_WRITE_HOLD protects against this scenario, because\n\t * mnt_want_write first increments count, then smp_mb, then spins on\n\t * MNT_WRITE_HOLD, so it can't be decremented by another CPU while\n\t * we're counting up here.\n\t */\n\tif (mnt_get_writers(mnt) > 0)\n\t\tret = -EBUSY;\n\telse\n\t\tmnt->mnt.mnt_flags |= MNT_READONLY;\n\t/*\n\t * MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers\n\t * that become unheld will see MNT_READONLY.\n\t */\n\tsmp_wmb();\n\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\tunlock_mount_hash();\n\treturn ret;\n}\n\nstatic void __mnt_unmake_readonly(struct mount *mnt)\n{\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags &= ~MNT_READONLY;\n\tunlock_mount_hash();\n}\n\nint sb_prepare_remount_readonly(struct super_block *sb)\n{\n\tstruct mount *mnt;\n\tint err = 0;\n\n\t/* Racy optimization.  Recheck the counter under MNT_WRITE_HOLD */\n\tif (atomic_long_read(&sb->s_remove_count))\n\t\treturn -EBUSY;\n\n\tlock_mount_hash();\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (!(mnt->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t\t\tsmp_mb();\n\t\t\tif (mnt_get_writers(mnt) > 0) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!err && atomic_long_read(&sb->s_remove_count))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tsb->s_readonly_remount = 1;\n\t\tsmp_wmb();\n\t}\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (mnt->mnt.mnt_flags & MNT_WRITE_HOLD)\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\t}\n\tunlock_mount_hash();\n\n\treturn err;\n}\n\nstatic void free_vfsmnt(struct mount *mnt)\n{\n\tkfree_const(mnt->mnt_devname);\n#ifdef CONFIG_SMP\n\tfree_percpu(mnt->mnt_pcp);\n#endif\n\tkmem_cache_free(mnt_cache, mnt);\n}\n\nstatic void delayed_free_vfsmnt(struct rcu_head *head)\n{\n\tfree_vfsmnt(container_of(head, struct mount, mnt_rcu));\n}\n\n/* call under rcu_read_lock */\nint __legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tstruct mount *mnt;\n\tif (read_seqretry(&mount_lock, seq))\n\t\treturn 1;\n\tif (bastard == NULL)\n\t\treturn 0;\n\tmnt = real_mount(bastard);\n\tmnt_add_count(mnt, 1);\n\tif (likely(!read_seqretry(&mount_lock, seq)))\n\t\treturn 0;\n\tif (bastard->mnt_flags & MNT_SYNC_UMOUNT) {\n\t\tmnt_add_count(mnt, -1);\n\t\treturn 1;\n\t}\n\treturn -1;\n}\n\n/* call under rcu_read_lock */\nbool legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tint res = __legitimize_mnt(bastard, seq);\n\tif (likely(!res))\n\t\treturn true;\n\tif (unlikely(res < 0)) {\n\t\trcu_read_unlock();\n\t\tmntput(bastard);\n\t\trcu_read_lock();\n\t}\n\treturn false;\n}\n\n/*\n * find the first mount at @dentry on vfsmount @mnt.\n * call under rcu_read_lock()\n */\nstruct mount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct hlist_head *head = m_hash(mnt, dentry);\n\tstruct mount *p;\n\n\thlist_for_each_entry_rcu(p, head, mnt_hash)\n\t\tif (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)\n\t\t\treturn p;\n\treturn NULL;\n}\n\n/*\n * find the last mount at @dentry on vfsmount @mnt.\n * mount_lock must be held.\n */\nstruct mount *__lookup_mnt_last(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct mount *p, *res = NULL;\n\tp = __lookup_mnt(mnt, dentry);\n\tif (!p)\n\t\tgoto out;\n\tif (!(p->mnt.mnt_flags & MNT_UMOUNT))\n\t\tres = p;\n\thlist_for_each_entry_continue(p, mnt_hash) {\n\t\tif (&p->mnt_parent->mnt != mnt || p->mnt_mountpoint != dentry)\n\t\t\tbreak;\n\t\tif (!(p->mnt.mnt_flags & MNT_UMOUNT))\n\t\t\tres = p;\n\t}\nout:\n\treturn res;\n}\n\n/*\n * lookup_mnt - Return the first child mount mounted at path\n *\n * \"First\" means first mounted chronologically.  If you create the\n * following mounts:\n *\n * mount /dev/sda1 /mnt\n * mount /dev/sda2 /mnt\n * mount /dev/sda3 /mnt\n *\n * Then lookup_mnt() on the base /mnt dentry in the root mount will\n * return successively the root dentry and vfsmount of /dev/sda1, then\n * /dev/sda2, then /dev/sda3, then NULL.\n *\n * lookup_mnt takes a reference to the found vfsmount.\n */\nstruct vfsmount *lookup_mnt(struct path *path)\n{\n\tstruct mount *child_mnt;\n\tstruct vfsmount *m;\n\tunsigned seq;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tchild_mnt = __lookup_mnt(path->mnt, path->dentry);\n\t\tm = child_mnt ? &child_mnt->mnt : NULL;\n\t} while (!legitimize_mnt(m, seq));\n\trcu_read_unlock();\n\treturn m;\n}\n\n/*\n * __is_local_mountpoint - Test to see if dentry is a mountpoint in the\n *                         current mount namespace.\n *\n * The common case is dentries are not mountpoints at all and that\n * test is handled inline.  For the slow case when we are actually\n * dealing with a mountpoint of some kind, walk through all of the\n * mounts in the current mount namespace and test to see if the dentry\n * is a mountpoint.\n *\n * The mount_hashtable is not usable in the context because we\n * need to identify all mounts that may be in the current mount\n * namespace not just a mount that happens to have some specified\n * parent mount.\n */\nbool __is_local_mountpoint(struct dentry *dentry)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tstruct mount *mnt;\n\tbool is_covered = false;\n\n\tif (!d_mountpoint(dentry))\n\t\tgoto out;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tis_covered = (mnt->mnt_mountpoint == dentry);\n\t\tif (is_covered)\n\t\t\tbreak;\n\t}\n\tup_read(&namespace_sem);\nout:\n\treturn is_covered;\n}\n\nstatic struct mountpoint *lookup_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\n\thlist_for_each_entry(mp, chain, m_hash) {\n\t\tif (mp->m_dentry == dentry) {\n\t\t\t/* might be worth a WARN_ON() */\n\t\t\tif (d_unlinked(dentry))\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t\tmp->m_count++;\n\t\t\treturn mp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct mountpoint *new_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\tint ret;\n\n\tmp = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!mp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = d_set_mounted(dentry);\n\tif (ret) {\n\t\tkfree(mp);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tmp->m_dentry = dentry;\n\tmp->m_count = 1;\n\thlist_add_head(&mp->m_hash, chain);\n\tINIT_HLIST_HEAD(&mp->m_list);\n\treturn mp;\n}\n\nstatic void put_mountpoint(struct mountpoint *mp)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tBUG_ON(!hlist_empty(&mp->m_list));\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}\n\nstatic inline int check_mnt(struct mount *mnt)\n{\n\treturn mnt->mnt_ns == current->nsproxy->mnt_ns;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns) {\n\t\tns->event = ++event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void __touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns && ns->event != event) {\n\t\tns->event = event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void unhash_mnt(struct mount *mnt)\n{\n\tmnt->mnt_parent = mnt;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\thlist_del_init(&mnt->mnt_mp_list);\n\tput_mountpoint(mnt->mnt_mp);\n\tmnt->mnt_mp = NULL;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void detach_mnt(struct mount *mnt, struct path *old_path)\n{\n\told_path->dentry = mnt->mnt_mountpoint;\n\told_path->mnt = &mnt->mnt_parent->mnt;\n\tunhash_mnt(mnt);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void umount_mnt(struct mount *mnt)\n{\n\t/* old mountpoint will be dropped when we can do that */\n\tmnt->mnt_ex_mountpoint = mnt->mnt_mountpoint;\n\tunhash_mnt(mnt);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void attach_mnt(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mountpoint *mp)\n{\n\tmnt_set_mountpoint(parent, mp, mnt);\n\thlist_add_head_rcu(&mnt->mnt_hash, m_hash(&parent->mnt, mp->m_dentry));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n}\n\nstatic void attach_shadowed(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mount *shadows)\n{\n\tif (shadows) {\n\t\thlist_add_behind_rcu(&mnt->mnt_hash, &shadows->mnt_hash);\n\t\tlist_add(&mnt->mnt_child, &shadows->mnt_child);\n\t} else {\n\t\thlist_add_head_rcu(&mnt->mnt_hash,\n\t\t\t\tm_hash(&parent->mnt, mnt->mnt_mountpoint));\n\t\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void commit_tree(struct mount *mnt, struct mount *shadows)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tattach_shadowed(mnt, parent, shadows);\n\ttouch_mnt_namespace(n);\n}\n\nstatic struct mount *next_mnt(struct mount *p, struct mount *root)\n{\n\tstruct list_head *next = p->mnt_mounts.next;\n\tif (next == &p->mnt_mounts) {\n\t\twhile (1) {\n\t\t\tif (p == root)\n\t\t\t\treturn NULL;\n\t\t\tnext = p->mnt_child.next;\n\t\t\tif (next != &p->mnt_parent->mnt_mounts)\n\t\t\t\tbreak;\n\t\t\tp = p->mnt_parent;\n\t\t}\n\t}\n\treturn list_entry(next, struct mount, mnt_child);\n}\n\nstatic struct mount *skip_mnt_tree(struct mount *p)\n{\n\tstruct list_head *prev = p->mnt_mounts.prev;\n\twhile (prev != &p->mnt_mounts) {\n\t\tp = list_entry(prev, struct mount, mnt_child);\n\t\tprev = p->mnt_mounts.prev;\n\t}\n\treturn p;\n}\n\nstruct vfsmount *\nvfs_kern_mount(struct file_system_type *type, int flags, const char *name, void *data)\n{\n\tstruct mount *mnt;\n\tstruct dentry *root;\n\n\tif (!type)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmnt = alloc_vfsmnt(name);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flags & MS_KERNMOUNT)\n\t\tmnt->mnt.mnt_flags = MNT_INTERNAL;\n\n\troot = mount_fs(type, flags, name, data);\n\tif (IS_ERR(root)) {\n\t\tmnt_free_id(mnt);\n\t\tfree_vfsmnt(mnt);\n\t\treturn ERR_CAST(root);\n\t}\n\n\tmnt->mnt.mnt_root = root;\n\tmnt->mnt.mnt_sb = root->d_sb;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);\n\tunlock_mount_hash();\n\treturn &mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(vfs_kern_mount);\n\nstatic struct mount *clone_mnt(struct mount *old, struct dentry *root,\n\t\t\t\t\tint flag)\n{\n\tstruct super_block *sb = old->mnt.mnt_sb;\n\tstruct mount *mnt;\n\tint err;\n\n\tmnt = alloc_vfsmnt(old->mnt_devname);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))\n\t\tmnt->mnt_group_id = 0; /* not a peer of original */\n\telse\n\t\tmnt->mnt_group_id = old->mnt_group_id;\n\n\tif ((flag & CL_MAKE_SHARED) && !mnt->mnt_group_id) {\n\t\terr = mnt_alloc_group_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tmnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);\n\t/* Don't allow unprivileged users to change mount flags */\n\tif (flag & CL_UNPRIVILEGED) {\n\t\tmnt->mnt.mnt_flags |= MNT_LOCK_ATIME;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_READONLY)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_READONLY;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NODEV)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NODEV;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NOSUID)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NOSUID;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NOEXEC)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NOEXEC;\n\t}\n\n\t/* Don't allow unprivileged users to reveal what is under a mount */\n\tif ((flag & CL_UNPRIVILEGED) &&\n\t    (!(flag & CL_EXPIRE) || list_empty(&old->mnt_expire)))\n\t\tmnt->mnt.mnt_flags |= MNT_LOCKED;\n\n\tatomic_inc(&sb->s_active);\n\tmnt->mnt.mnt_sb = sb;\n\tmnt->mnt.mnt_root = dget(root);\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &sb->s_mounts);\n\tunlock_mount_hash();\n\n\tif ((flag & CL_SLAVE) ||\n\t    ((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {\n\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave_list);\n\t\tmnt->mnt_master = old;\n\t\tCLEAR_MNT_SHARED(mnt);\n\t} else if (!(flag & CL_PRIVATE)) {\n\t\tif ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))\n\t\t\tlist_add(&mnt->mnt_share, &old->mnt_share);\n\t\tif (IS_MNT_SLAVE(old))\n\t\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave);\n\t\tmnt->mnt_master = old->mnt_master;\n\t}\n\tif (flag & CL_MAKE_SHARED)\n\t\tset_mnt_shared(mnt);\n\n\t/* stick the duplicate mount on the same expiry list\n\t * as the original if that was on one */\n\tif (flag & CL_EXPIRE) {\n\t\tif (!list_empty(&old->mnt_expire))\n\t\t\tlist_add(&mnt->mnt_expire, &old->mnt_expire);\n\t}\n\n\treturn mnt;\n\n out_free:\n\tmnt_free_id(mnt);\n\tfree_vfsmnt(mnt);\n\treturn ERR_PTR(err);\n}\n\nstatic void cleanup_mnt(struct mount *mnt)\n{\n\t/*\n\t * This probably indicates that somebody messed\n\t * up a mnt_want/drop_write() pair.  If this\n\t * happens, the filesystem was probably unable\n\t * to make r/w->r/o transitions.\n\t */\n\t/*\n\t * The locking used to deal with mnt_count decrement provides barriers,\n\t * so mnt_get_writers() below is safe.\n\t */\n\tWARN_ON(mnt_get_writers(mnt));\n\tif (unlikely(mnt->mnt_pins.first))\n\t\tmnt_pin_kill(mnt);\n\tfsnotify_vfsmount_delete(&mnt->mnt);\n\tdput(mnt->mnt.mnt_root);\n\tdeactivate_super(mnt->mnt.mnt_sb);\n\tmnt_free_id(mnt);\n\tcall_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);\n}\n\nstatic void __cleanup_mnt(struct rcu_head *head)\n{\n\tcleanup_mnt(container_of(head, struct mount, mnt_rcu));\n}\n\nstatic LLIST_HEAD(delayed_mntput_list);\nstatic void delayed_mntput(struct work_struct *unused)\n{\n\tstruct llist_node *node = llist_del_all(&delayed_mntput_list);\n\tstruct llist_node *next;\n\n\tfor (; node; node = next) {\n\t\tnext = llist_next(node);\n\t\tcleanup_mnt(llist_entry(node, struct mount, mnt_llist));\n\t}\n}\nstatic DECLARE_DELAYED_WORK(delayed_mntput_work, delayed_mntput);\n\nstatic void mntput_no_expire(struct mount *mnt)\n{\n\trcu_read_lock();\n\tmnt_add_count(mnt, -1);\n\tif (likely(mnt->mnt_ns)) { /* shouldn't be the last one */\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\tumount_mnt(p);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}\n\nvoid mntput(struct vfsmount *mnt)\n{\n\tif (mnt) {\n\t\tstruct mount *m = real_mount(mnt);\n\t\t/* avoid cacheline pingpong, hope gcc doesn't get \"smart\" */\n\t\tif (unlikely(m->mnt_expiry_mark))\n\t\t\tm->mnt_expiry_mark = 0;\n\t\tmntput_no_expire(m);\n\t}\n}\nEXPORT_SYMBOL(mntput);\n\nstruct vfsmount *mntget(struct vfsmount *mnt)\n{\n\tif (mnt)\n\t\tmnt_add_count(real_mount(mnt), 1);\n\treturn mnt;\n}\nEXPORT_SYMBOL(mntget);\n\nstruct vfsmount *mnt_clone_internal(struct path *path)\n{\n\tstruct mount *p;\n\tp = clone_mnt(real_mount(path->mnt), path->dentry, CL_PRIVATE);\n\tif (IS_ERR(p))\n\t\treturn ERR_CAST(p);\n\tp->mnt.mnt_flags |= MNT_INTERNAL;\n\treturn &p->mnt;\n}\n\nstatic inline void mangle(struct seq_file *m, const char *s)\n{\n\tseq_escape(m, s, \" \\t\\n\\\\\");\n}\n\n/*\n * Simple .show_options callback for filesystems which don't want to\n * implement more complex mount option showing.\n *\n * See also save_mount_options().\n */\nint generic_show_options(struct seq_file *m, struct dentry *root)\n{\n\tconst char *options;\n\n\trcu_read_lock();\n\toptions = rcu_dereference(root->d_sb->s_options);\n\n\tif (options != NULL && options[0]) {\n\t\tseq_putc(m, ',');\n\t\tmangle(m, options);\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL(generic_show_options);\n\n/*\n * If filesystem uses generic_show_options(), this function should be\n * called from the fill_super() callback.\n *\n * The .remount_fs callback usually needs to be handled in a special\n * way, to make sure, that previous options are not overwritten if the\n * remount fails.\n *\n * Also note, that if the filesystem's .remount_fs function doesn't\n * reset all options to their default value, but changes only newly\n * given options, then the displayed options will not reflect reality\n * any more.\n */\nvoid save_mount_options(struct super_block *sb, char *options)\n{\n\tBUG_ON(sb->s_options);\n\trcu_assign_pointer(sb->s_options, kstrdup(options, GFP_KERNEL));\n}\nEXPORT_SYMBOL(save_mount_options);\n\nvoid replace_mount_options(struct super_block *sb, char *options)\n{\n\tchar *old = sb->s_options;\n\trcu_assign_pointer(sb->s_options, options);\n\tif (old) {\n\t\tsynchronize_rcu();\n\t\tkfree(old);\n\t}\n}\nEXPORT_SYMBOL(replace_mount_options);\n\n#ifdef CONFIG_PROC_FS\n/* iterator; we want it to have access to namespace_sem, thus here... */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\n\tdown_read(&namespace_sem);\n\tif (p->cached_event == p->ns->event) {\n\t\tvoid *v = p->cached_mount;\n\t\tif (*pos == p->cached_index)\n\t\t\treturn v;\n\t\tif (*pos == p->cached_index + 1) {\n\t\t\tv = seq_list_next(v, &p->ns->list, &p->cached_index);\n\t\t\treturn p->cached_mount = v;\n\t\t}\n\t}\n\n\tp->cached_event = p->ns->event;\n\tp->cached_mount = seq_list_start(&p->ns->list, *pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\n\tp->cached_mount = seq_list_next(v, &p->ns->list, pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tup_read(&namespace_sem);\n}\n\nstatic int m_show(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *r = list_entry(v, struct mount, mnt_list);\n\treturn p->show(m, &r->mnt);\n}\n\nconst struct seq_operations mounts_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show,\n};\n#endif  /* CONFIG_PROC_FS */\n\n/**\n * may_umount_tree - check if a mount tree is busy\n * @mnt: root of mount tree\n *\n * This is called to check if a tree of mounts has any\n * open files, pwds, chroots or sub mounts that are\n * busy.\n */\nint may_umount_tree(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint actual_refs = 0;\n\tint minimum_refs = 0;\n\tstruct mount *p;\n\tBUG_ON(!m);\n\n\t/* write lock needed for mnt_get_count */\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tactual_refs += mnt_get_count(p);\n\t\tminimum_refs += 2;\n\t}\n\tunlock_mount_hash();\n\n\tif (actual_refs > minimum_refs)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nEXPORT_SYMBOL(may_umount_tree);\n\n/**\n * may_umount - check if a mount point is busy\n * @mnt: root of mount\n *\n * This is called to check if a mount point has any\n * open files, pwds, chroots or sub mounts. If the\n * mount has sub mounts this will return busy\n * regardless of whether the sub mounts are busy.\n *\n * Doesn't take quota and stuff into account. IOW, in some cases it will\n * give false negatives. The main reason why it's here is that we need\n * a non-destructive way to look for easily umountable filesystems.\n */\nint may_umount(struct vfsmount *mnt)\n{\n\tint ret = 1;\n\tdown_read(&namespace_sem);\n\tlock_mount_hash();\n\tif (propagate_mount_busy(real_mount(mnt), 2))\n\t\tret = 0;\n\tunlock_mount_hash();\n\tup_read(&namespace_sem);\n\treturn ret;\n}\n\nEXPORT_SYMBOL(may_umount);\n\nstatic HLIST_HEAD(unmounted);\t/* protected by namespace_sem */\n\nstatic void namespace_unlock(void)\n{\n\tstruct hlist_head head;\n\n\thlist_move_list(&unmounted, &head);\n\n\tup_write(&namespace_sem);\n\n\tif (likely(hlist_empty(&head)))\n\t\treturn;\n\n\tsynchronize_rcu();\n\n\tgroup_pin_kill(&head);\n}\n\nstatic inline void namespace_lock(void)\n{\n\tdown_write(&namespace_sem);\n}\n\nenum umount_tree_flags {\n\tUMOUNT_SYNC = 1,\n\tUMOUNT_PROPAGATE = 2,\n\tUMOUNT_CONNECTED = 4,\n};\n\nstatic bool disconnect_mount(struct mount *mnt, enum umount_tree_flags how)\n{\n\t/* Leaving mounts connected is only valid for lazy umounts */\n\tif (how & UMOUNT_SYNC)\n\t\treturn true;\n\n\t/* A mount without a parent has nothing to be connected to */\n\tif (!mnt_has_parent(mnt))\n\t\treturn true;\n\n\t/* Because the reference counting rules change when mounts are\n\t * unmounted and connected, umounted mounts may not be\n\t * connected to mounted mounts.\n\t */\n\tif (!(mnt->mnt_parent->mnt.mnt_flags & MNT_UMOUNT))\n\t\treturn true;\n\n\t/* Has it been requested that the mount remain connected? */\n\tif (how & UMOUNT_CONNECTED)\n\t\treturn false;\n\n\t/* Is the mount locked such that it needs to remain connected? */\n\tif (IS_MNT_LOCKED(mnt))\n\t\treturn false;\n\n\t/* By default disconnect the mount */\n\treturn true;\n}\n\n/*\n * mount_lock must be held\n * namespace_sem must be held for write\n */\nstatic void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\t__touch_mnt_namespace(p->mnt_ns);\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = disconnect_mount(p, how);\n\n\t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n\t\t\t\t disconnect ? &unmounted : NULL);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t}\n}\n\nstatic void shrink_submounts(struct mount *mnt);\n\nstatic int do_umount(struct mount *mnt, int flags)\n{\n\tstruct super_block *sb = mnt->mnt.mnt_sb;\n\tint retval;\n\n\tretval = security_sb_umount(&mnt->mnt, flags);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Allow userspace to request a mountpoint be expired rather than\n\t * unmounting unconditionally. Unmount only happens if:\n\t *  (1) the mark is already set (the mark is cleared by mntput())\n\t *  (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]\n\t */\n\tif (flags & MNT_EXPIRE) {\n\t\tif (&mnt->mnt == current->fs->root.mnt ||\n\t\t    flags & (MNT_FORCE | MNT_DETACH))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * probably don't strictly need the lock here if we examined\n\t\t * all race cases, but it's a slowpath.\n\t\t */\n\t\tlock_mount_hash();\n\t\tif (mnt_get_count(mnt) != 2) {\n\t\t\tunlock_mount_hash();\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tunlock_mount_hash();\n\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * If we may have to abort operations to get out of this\n\t * mount, and they will themselves hold resources we must\n\t * allow the fs to do things. In the Unix tradition of\n\t * 'Gee thats tricky lets do it in userspace' the umount_begin\n\t * might fail to complete on the first run through as other tasks\n\t * must return, and the like. Thats for the mount program to worry\n\t * about for the moment.\n\t */\n\n\tif (flags & MNT_FORCE && sb->s_op->umount_begin) {\n\t\tsb->s_op->umount_begin(sb);\n\t}\n\n\t/*\n\t * No sense to grab the lock for this test, but test itself looks\n\t * somewhat bogus. Suggestions for better replacement?\n\t * Ho-hum... In principle, we might treat that as umount + switch\n\t * to rootfs. GC would eventually take care of the old vfsmount.\n\t * Actually it makes sense, especially if rootfs would contain a\n\t * /reboot - static binary that would close all descriptors and\n\t * call reboot(9). Then init(8) could umount root and exec /reboot.\n\t */\n\tif (&mnt->mnt == current->fs->root.mnt && !(flags & MNT_DETACH)) {\n\t\t/*\n\t\t * Special case for \"unmounting\" root ...\n\t\t * we just try to remount it readonly.\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tdown_write(&sb->s_umount);\n\t\tif (!(sb->s_flags & MS_RDONLY))\n\t\t\tretval = do_remount_sb(sb, MS_RDONLY, NULL, 0);\n\t\tup_write(&sb->s_umount);\n\t\treturn retval;\n\t}\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\tevent++;\n\n\tif (flags & MNT_DETACH) {\n\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE);\n\t\tretval = 0;\n\t} else {\n\t\tshrink_submounts(mnt);\n\t\tretval = -EBUSY;\n\t\tif (!propagate_mount_busy(mnt, 2)) {\n\t\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t\tretval = 0;\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\treturn retval;\n}\n\n/*\n * __detach_mounts - lazily unmount all mounts on the specified dentry\n *\n * During unlink, rmdir, and d_drop it is possible to loose the path\n * to an existing mountpoint, and wind up leaking the mount.\n * detach_mounts allows lazily unmounting those mounts instead of\n * leaking them.\n *\n * The caller may hold dentry->d_inode->i_mutex.\n */\nvoid __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\n\tnamespace_lock();\n\tmp = lookup_mountpoint(dentry);\n\tif (IS_ERR_OR_NULL(mp))\n\t\tgoto out_unlock;\n\n\tlock_mount_hash();\n\tevent++;\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\thlist_add_head(&mnt->mnt_umount.s_list, &unmounted);\n\t\t\tumount_mnt(mnt);\n\t\t}\n\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n\t}\n\tunlock_mount_hash();\n\tput_mountpoint(mp);\nout_unlock:\n\tnamespace_unlock();\n}\n\n/* \n * Is the caller allowed to modify his namespace?\n */\nstatic inline bool may_mount(void)\n{\n\treturn ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN);\n}\n\nstatic inline bool may_mandlock(void)\n{\n#ifndef\tCONFIG_MANDATORY_FILE_LOCKING\n\treturn false;\n#endif\n\treturn capable(CAP_SYS_ADMIN);\n}\n\n/*\n * Now umount can handle mount points as well as block devices.\n * This is important for filesystems which use unnamed block devices.\n *\n * We now support a flag for forced unmount like the other 'big iron'\n * unixes. Our API is identical to OSF/1 to avoid making a mess of AMD\n */\n\nSYSCALL_DEFINE2(umount, char __user *, name, int, flags)\n{\n\tstruct path path;\n\tstruct mount *mnt;\n\tint retval;\n\tint lookup_flags = 0;\n\n\tif (flags & ~(MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW))\n\t\treturn -EINVAL;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif (!(flags & UMOUNT_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\n\tretval = user_path_mountpoint_at(AT_FDCWD, name, lookup_flags, &path);\n\tif (retval)\n\t\tgoto out;\n\tmnt = real_mount(path.mnt);\n\tretval = -EINVAL;\n\tif (path.dentry != path.mnt->mnt_root)\n\t\tgoto dput_and_out;\n\tif (!check_mnt(mnt))\n\t\tgoto dput_and_out;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto dput_and_out;\n\tretval = -EPERM;\n\tif (flags & MNT_FORCE && !capable(CAP_SYS_ADMIN))\n\t\tgoto dput_and_out;\n\n\tretval = do_umount(mnt, flags);\ndput_and_out:\n\t/* we mustn't call path_put() as that would clear mnt_expiry_mark */\n\tdput(path.dentry);\n\tmntput_no_expire(mnt);\nout:\n\treturn retval;\n}\n\n#ifdef __ARCH_WANT_SYS_OLDUMOUNT\n\n/*\n *\tThe 2.0 compatible umount. No flags.\n */\nSYSCALL_DEFINE1(oldumount, char __user *, name)\n{\n\treturn sys_umount(name, 0);\n}\n\n#endif\n\nstatic bool is_mnt_ns_file(struct dentry *dentry)\n{\n\t/* Is this a proxy for a mount namespace? */\n\treturn dentry->d_op == &ns_dentry_operations &&\n\t       dentry->d_fsdata == &mntns_operations;\n}\n\nstruct mnt_namespace *to_mnt_ns(struct ns_common *ns)\n{\n\treturn container_of(ns, struct mnt_namespace, ns);\n}\n\nstatic bool mnt_ns_loop(struct dentry *dentry)\n{\n\t/* Could bind mounting the mount namespace inode cause a\n\t * mount namespace loop?\n\t */\n\tstruct mnt_namespace *mnt_ns;\n\tif (!is_mnt_ns_file(dentry))\n\t\treturn false;\n\n\tmnt_ns = to_mnt_ns(get_proc_ns(dentry->d_inode));\n\treturn current->nsproxy->mnt_ns->seq >= mnt_ns->seq;\n}\n\nstruct mount *copy_tree(struct mount *mnt, struct dentry *dentry,\n\t\t\t\t\tint flag)\n{\n\tstruct mount *res, *p, *q, *r, *parent;\n\n\tif (!(flag & CL_COPY_UNBINDABLE) && IS_MNT_UNBINDABLE(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!(flag & CL_COPY_MNT_NS_FILE) && is_mnt_ns_file(dentry))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tres = q = clone_mnt(mnt, dentry, flag);\n\tif (IS_ERR(q))\n\t\treturn q;\n\n\tq->mnt_mountpoint = mnt->mnt_mountpoint;\n\n\tp = mnt;\n\tlist_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {\n\t\tstruct mount *s;\n\t\tif (!is_subdir(r->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tfor (s = r; s; s = next_mnt(s, r)) {\n\t\t\tstruct mount *t = NULL;\n\t\t\tif (!(flag & CL_COPY_UNBINDABLE) &&\n\t\t\t    IS_MNT_UNBINDABLE(s)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!(flag & CL_COPY_MNT_NS_FILE) &&\n\t\t\t    is_mnt_ns_file(s->mnt.mnt_root)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twhile (p != s->mnt_parent) {\n\t\t\t\tp = p->mnt_parent;\n\t\t\t\tq = q->mnt_parent;\n\t\t\t}\n\t\t\tp = s;\n\t\t\tparent = q;\n\t\t\tq = clone_mnt(p, p->mnt.mnt_root, flag);\n\t\t\tif (IS_ERR(q))\n\t\t\t\tgoto out;\n\t\t\tlock_mount_hash();\n\t\t\tlist_add_tail(&q->mnt_list, &res->mnt_list);\n\t\t\tmnt_set_mountpoint(parent, p->mnt_mp, q);\n\t\t\tif (!list_empty(&parent->mnt_mounts)) {\n\t\t\t\tt = list_last_entry(&parent->mnt_mounts,\n\t\t\t\t\tstruct mount, mnt_child);\n\t\t\t\tif (t->mnt_mp != p->mnt_mp)\n\t\t\t\t\tt = NULL;\n\t\t\t}\n\t\t\tattach_shadowed(q, parent, t);\n\t\t\tunlock_mount_hash();\n\t\t}\n\t}\n\treturn res;\nout:\n\tif (res) {\n\t\tlock_mount_hash();\n\t\tumount_tree(res, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\n\treturn q;\n}\n\n/* Caller should check returned pointer for errors */\n\nstruct vfsmount *collect_mounts(struct path *path)\n{\n\tstruct mount *tree;\n\tnamespace_lock();\n\tif (!check_mnt(real_mount(path->mnt)))\n\t\ttree = ERR_PTR(-EINVAL);\n\telse\n\t\ttree = copy_tree(real_mount(path->mnt), path->dentry,\n\t\t\t\t CL_COPY_ALL | CL_PRIVATE);\n\tnamespace_unlock();\n\tif (IS_ERR(tree))\n\t\treturn ERR_CAST(tree);\n\treturn &tree->mnt;\n}\n\nvoid drop_collected_mounts(struct vfsmount *mnt)\n{\n\tnamespace_lock();\n\tlock_mount_hash();\n\tumount_tree(real_mount(mnt), UMOUNT_SYNC);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\n/**\n * clone_private_mount - create a private clone of a path\n *\n * This creates a new vfsmount, which will be the clone of @path.  The new will\n * not be attached anywhere in the namespace and will be private (i.e. changes\n * to the originating mount won't be propagated into this).\n *\n * Release with mntput().\n */\nstruct vfsmount *clone_private_mount(struct path *path)\n{\n\tstruct mount *old_mnt = real_mount(path->mnt);\n\tstruct mount *new_mnt;\n\n\tif (IS_MNT_UNBINDABLE(old_mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&namespace_sem);\n\tnew_mnt = clone_mnt(old_mnt, path->dentry, CL_PRIVATE);\n\tup_read(&namespace_sem);\n\tif (IS_ERR(new_mnt))\n\t\treturn ERR_CAST(new_mnt);\n\n\treturn &new_mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(clone_private_mount);\n\nint iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,\n\t\t   struct vfsmount *root)\n{\n\tstruct mount *mnt;\n\tint res = f(root, arg);\n\tif (res)\n\t\treturn res;\n\tlist_for_each_entry(mnt, &real_mount(root)->mnt_list, mnt_list) {\n\t\tres = f(&mnt->mnt, arg);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\treturn 0;\n}\n\nstatic void cleanup_group_ids(struct mount *mnt, struct mount *end)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p != end; p = next_mnt(p, mnt)) {\n\t\tif (p->mnt_group_id && !IS_MNT_SHARED(p))\n\t\t\tmnt_release_group_id(p);\n\t}\n}\n\nstatic int invent_group_ids(struct mount *mnt, bool recurse)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = recurse ? next_mnt(p, mnt) : NULL) {\n\t\tif (!p->mnt_group_id && !IS_MNT_SHARED(p)) {\n\t\t\tint err = mnt_alloc_group_id(p);\n\t\t\tif (err) {\n\t\t\t\tcleanup_group_ids(mnt, p);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n *  @source_mnt : mount tree to be attached\n *  @nd         : place the mount tree @source_mnt is attached\n *  @parent_nd  : if non-null, detach the source_mnt from its parent and\n *  \t\t   store the parent mount and mountpoint dentry.\n *  \t\t   (done when source_mnt is moved)\n *\n *  NOTE: in the table below explains the semantics when a source mount\n *  of a given type is attached to a destination mount of a given type.\n * ---------------------------------------------------------------------------\n * |         BIND MOUNT OPERATION                                            |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+)    |      private   |      slave (*) |  invalid   |\n * ***************************************************************************\n * A bind operation clones the source mount and mounts the clone on the\n * destination mount.\n *\n * (++)  the cloned mount is propagated to all the mounts in the propagation\n * \t tree of the destination mount and the cloned mount is added to\n * \t the peer group of the source mount.\n * (+)   the cloned mount is created under the destination mount and is marked\n *       as shared. The cloned mount is added to the peer group of the source\n *       mount.\n * (+++) the mount is propagated to all the mounts in the propagation tree\n *       of the destination mount and the cloned mount is made slave\n *       of the same master as that of the source mount. The cloned mount\n *       is marked as 'shared and slave'.\n * (*)   the cloned mount is made a slave of the same master as that of the\n * \t source mount.\n *\n * ---------------------------------------------------------------------------\n * |         \t\tMOVE MOUNT OPERATION                                 |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+*)   |      private   |    slave (*)   | unbindable |\n * ***************************************************************************\n *\n * (+)  the mount is moved to the destination. And is then propagated to\n * \tall the mounts in the propagation tree of the destination mount.\n * (+*)  the mount is moved to the destination.\n * (+++)  the mount is moved to the destination and is then propagated to\n * \tall the mounts belonging to the destination mount's propagation tree.\n * \tthe mount is marked as 'shared and slave'.\n * (*)\tthe mount continues to be a slave at the new location.\n *\n * if the source mount is a tree, the operations explained above is\n * applied to each mount in the tree.\n * Must be called without spinlocks held, since this function can sleep\n * in allocations.\n */\nstatic int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tstruct path *parent_path)\n{\n\tHLIST_HEAD(tree_list);\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (parent_path) {\n\t\tdetach_mnt(source_mnt, parent_path);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt, NULL);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt_last(&child->mnt_parent->mnt,\n\t\t\t\t      child->mnt_mountpoint);\n\t\tcommit_tree(child, q);\n\t}\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tumount_tree(child, UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\treturn err;\n}\n\nstatic struct mountpoint *lock_mount(struct path *path)\n{\n\tstruct vfsmount *mnt;\n\tstruct dentry *dentry = path->dentry;\nretry:\n\tinode_lock(dentry->d_inode);\n\tif (unlikely(cant_mount(dentry))) {\n\t\tinode_unlock(dentry->d_inode);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tnamespace_lock();\n\tmnt = lookup_mnt(path);\n\tif (likely(!mnt)) {\n\t\tstruct mountpoint *mp = lookup_mountpoint(dentry);\n\t\tif (!mp)\n\t\t\tmp = new_mountpoint(dentry);\n\t\tif (IS_ERR(mp)) {\n\t\t\tnamespace_unlock();\n\t\t\tinode_unlock(dentry->d_inode);\n\t\t\treturn mp;\n\t\t}\n\t\treturn mp;\n\t}\n\tnamespace_unlock();\n\tinode_unlock(path->dentry->d_inode);\n\tpath_put(path);\n\tpath->mnt = mnt;\n\tdentry = path->dentry = dget(mnt->mnt_root);\n\tgoto retry;\n}\n\nstatic void unlock_mount(struct mountpoint *where)\n{\n\tstruct dentry *dentry = where->m_dentry;\n\tput_mountpoint(where);\n\tnamespace_unlock();\n\tinode_unlock(dentry->d_inode);\n}\n\nstatic int graft_tree(struct mount *mnt, struct mount *p, struct mountpoint *mp)\n{\n\tif (mnt->mnt.mnt_sb->s_flags & MS_NOUSER)\n\t\treturn -EINVAL;\n\n\tif (d_is_dir(mp->m_dentry) !=\n\t      d_is_dir(mnt->mnt.mnt_root))\n\t\treturn -ENOTDIR;\n\n\treturn attach_recursive_mnt(mnt, p, mp, NULL);\n}\n\n/*\n * Sanity check the flags to change_mnt_propagation.\n */\n\nstatic int flags_to_propagation_type(int flags)\n{\n\tint type = flags & ~(MS_REC | MS_SILENT);\n\n\t/* Fail if any non-propagation flags are set */\n\tif (type & ~(MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn 0;\n\t/* Only one propagation flag should be set */\n\tif (!is_power_of_2(type))\n\t\treturn 0;\n\treturn type;\n}\n\n/*\n * recursively change the type of the mountpoint.\n */\nstatic int do_change_type(struct path *path, int flag)\n{\n\tstruct mount *m;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint recurse = flag & MS_REC;\n\tint type;\n\tint err = 0;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\ttype = flags_to_propagation_type(flag);\n\tif (!type)\n\t\treturn -EINVAL;\n\n\tnamespace_lock();\n\tif (type == MS_SHARED) {\n\t\terr = invent_group_ids(mnt, recurse);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tlock_mount_hash();\n\tfor (m = mnt; m; m = (recurse ? next_mnt(m, mnt) : NULL))\n\t\tchange_mnt_propagation(m, type);\n\tunlock_mount_hash();\n\n out_unlock:\n\tnamespace_unlock();\n\treturn err;\n}\n\nstatic bool has_locked_children(struct mount *mnt, struct dentry *dentry)\n{\n\tstruct mount *child;\n\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\tif (!is_subdir(child->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tif (child->mnt.mnt_flags & MNT_LOCKED)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * do loopback mount.\n */\nstatic int do_loopback(struct path *path, const char *old_name,\n\t\t\t\tint recurse)\n{\n\tstruct path old_path;\n\tstruct mount *mnt = NULL, *old, *parent;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = -EINVAL;\n\tif (mnt_ns_loop(old_path.dentry))\n\t\tgoto out; \n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tparent = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (IS_MNT_UNBINDABLE(old))\n\t\tgoto out2;\n\n\tif (!check_mnt(parent))\n\t\tgoto out2;\n\n\tif (!check_mnt(old) && old_path.dentry->d_op != &ns_dentry_operations)\n\t\tgoto out2;\n\n\tif (!recurse && has_locked_children(old, old_path.dentry))\n\t\tgoto out2;\n\n\tif (recurse)\n\t\tmnt = copy_tree(old, old_path.dentry, CL_COPY_MNT_NS_FILE);\n\telse\n\t\tmnt = clone_mnt(old, old_path.dentry, 0);\n\n\tif (IS_ERR(mnt)) {\n\t\terr = PTR_ERR(mnt);\n\t\tgoto out2;\n\t}\n\n\tmnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\n\terr = graft_tree(mnt, parent, mp);\n\tif (err) {\n\t\tlock_mount_hash();\n\t\tumount_tree(mnt, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\nout2:\n\tunlock_mount(mp);\nout:\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic int change_mount_flags(struct vfsmount *mnt, int ms_flags)\n{\n\tint error = 0;\n\tint readonly_request = 0;\n\n\tif (ms_flags & MS_RDONLY)\n\t\treadonly_request = 1;\n\tif (readonly_request == __mnt_is_readonly(mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\terror = mnt_make_readonly(real_mount(mnt));\n\telse\n\t\t__mnt_unmake_readonly(real_mount(mnt));\n\treturn error;\n}\n\n/*\n * change filesystem flags. dir should be a physical root of filesystem.\n * If you've mounted a non-root directory somewhere and want to do remount\n * on it - tough luck.\n */\nstatic int do_remount(struct path *path, int flags, int mnt_flags,\n\t\t      void *data)\n{\n\tint err;\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\t/* Don't allow changing of locked mnt flags.\n\t *\n\t * No locks need to be held here while testing the various\n\t * MNT_LOCK flags because those flags can never be cleared\n\t * once they are set.\n\t */\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_READONLY) &&\n\t    !(mnt_flags & MNT_READONLY)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NODEV) &&\n\t    !(mnt_flags & MNT_NODEV)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NOSUID) &&\n\t    !(mnt_flags & MNT_NOSUID)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NOEXEC) &&\n\t    !(mnt_flags & MNT_NOEXEC)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_ATIME) &&\n\t    ((mnt->mnt.mnt_flags & MNT_ATIME_MASK) != (mnt_flags & MNT_ATIME_MASK))) {\n\t\treturn -EPERM;\n\t}\n\n\terr = security_sb_remount(sb, data);\n\tif (err)\n\t\treturn err;\n\n\tdown_write(&sb->s_umount);\n\tif (flags & MS_BIND)\n\t\terr = change_mount_flags(path->mnt, flags);\n\telse if (!capable(CAP_SYS_ADMIN))\n\t\terr = -EPERM;\n\telse\n\t\terr = do_remount_sb(sb, flags, data, 0);\n\tif (!err) {\n\t\tlock_mount_hash();\n\t\tmnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;\n\t\tmnt->mnt.mnt_flags = mnt_flags;\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tunlock_mount_hash();\n\t}\n\tup_write(&sb->s_umount);\n\treturn err;\n}\n\nstatic inline int tree_contains_unbindable(struct mount *mnt)\n{\n\tstruct mount *p;\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tif (IS_MNT_UNBINDABLE(p))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int do_move_mount(struct path *path, const char *old_name)\n{\n\tstruct path old_path, parent_path;\n\tstruct mount *p;\n\tstruct mount *old;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW, &old_path);\n\tif (err)\n\t\treturn err;\n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tp = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (!check_mnt(p) || !check_mnt(old))\n\t\tgoto out1;\n\n\tif (old->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out1;\n\n\terr = -EINVAL;\n\tif (old_path.dentry != old_path.mnt->mnt_root)\n\t\tgoto out1;\n\n\tif (!mnt_has_parent(old))\n\t\tgoto out1;\n\n\tif (d_is_dir(path->dentry) !=\n\t      d_is_dir(old_path.dentry))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount residing in a shared parent.\n\t */\n\tif (IS_MNT_SHARED(old->mnt_parent))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount tree containing unbindable mounts to a destination\n\t * mount which is shared.\n\t */\n\tif (IS_MNT_SHARED(p) && tree_contains_unbindable(old))\n\t\tgoto out1;\n\terr = -ELOOP;\n\tfor (; mnt_has_parent(p); p = p->mnt_parent)\n\t\tif (p == old)\n\t\t\tgoto out1;\n\n\terr = attach_recursive_mnt(old, real_mount(path->mnt), mp, &parent_path);\n\tif (err)\n\t\tgoto out1;\n\n\t/* if the mount is moved, it should no longer be expire\n\t * automatically */\n\tlist_del_init(&old->mnt_expire);\nout1:\n\tunlock_mount(mp);\nout:\n\tif (!err)\n\t\tpath_put(&parent_path);\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic struct vfsmount *fs_set_subtype(struct vfsmount *mnt, const char *fstype)\n{\n\tint err;\n\tconst char *subtype = strchr(fstype, '.');\n\tif (subtype) {\n\t\tsubtype++;\n\t\terr = -EINVAL;\n\t\tif (!subtype[0])\n\t\t\tgoto err;\n\t} else\n\t\tsubtype = \"\";\n\n\tmnt->mnt_sb->s_subtype = kstrdup(subtype, GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!mnt->mnt_sb->s_subtype)\n\t\tgoto err;\n\treturn mnt;\n\n err:\n\tmntput(mnt);\n\treturn ERR_PTR(err);\n}\n\n/*\n * add a mount into a namespace's mount tree\n */\nstatic int do_add_mount(struct mount *newmnt, struct path *path, int mnt_flags)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *parent;\n\tint err;\n\n\tmnt_flags &= ~MNT_INTERNAL_FLAGS;\n\n\tmp = lock_mount(path);\n\tif (IS_ERR(mp))\n\t\treturn PTR_ERR(mp);\n\n\tparent = real_mount(path->mnt);\n\terr = -EINVAL;\n\tif (unlikely(!check_mnt(parent))) {\n\t\t/* that's acceptable only for automounts done in private ns */\n\t\tif (!(mnt_flags & MNT_SHRINKABLE))\n\t\t\tgoto unlock;\n\t\t/* ... and for those we'd better have mountpoint still alive */\n\t\tif (!parent->mnt_ns)\n\t\t\tgoto unlock;\n\t}\n\n\t/* Refuse the same filesystem on the same mount point */\n\terr = -EBUSY;\n\tif (path->mnt->mnt_sb == newmnt->mnt.mnt_sb &&\n\t    path->mnt->mnt_root == path->dentry)\n\t\tgoto unlock;\n\n\terr = -EINVAL;\n\tif (d_is_symlink(newmnt->mnt.mnt_root))\n\t\tgoto unlock;\n\n\tnewmnt->mnt.mnt_flags = mnt_flags;\n\terr = graft_tree(newmnt, parent, mp);\n\nunlock:\n\tunlock_mount(mp);\n\treturn err;\n}\n\nstatic bool mount_too_revealing(struct vfsmount *mnt, int *new_mnt_flags);\n\n/*\n * create a new mount for userspace and request it to be added into the\n * namespace's tree\n */\nstatic int do_new_mount(struct path *path, const char *fstype, int flags,\n\t\t\tint mnt_flags, const char *name, void *data)\n{\n\tstruct file_system_type *type;\n\tstruct vfsmount *mnt;\n\tint err;\n\n\tif (!fstype)\n\t\treturn -EINVAL;\n\n\ttype = get_fs_type(fstype);\n\tif (!type)\n\t\treturn -ENODEV;\n\n\tmnt = vfs_kern_mount(type, flags, name, data);\n\tif (!IS_ERR(mnt) && (type->fs_flags & FS_HAS_SUBTYPE) &&\n\t    !mnt->mnt_sb->s_subtype)\n\t\tmnt = fs_set_subtype(mnt, fstype);\n\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn PTR_ERR(mnt);\n\n\tif (mount_too_revealing(mnt, &mnt_flags)) {\n\t\tmntput(mnt);\n\t\treturn -EPERM;\n\t}\n\n\terr = do_add_mount(real_mount(mnt), path, mnt_flags);\n\tif (err)\n\t\tmntput(mnt);\n\treturn err;\n}\n\nint finish_automount(struct vfsmount *m, struct path *path)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint err;\n\t/* The new mount record should have at least 2 refs to prevent it being\n\t * expired before we get a chance to add it\n\t */\n\tBUG_ON(mnt_get_count(mnt) < 2);\n\n\tif (m->mnt_sb == path->mnt->mnt_sb &&\n\t    m->mnt_root == path->dentry) {\n\t\terr = -ELOOP;\n\t\tgoto fail;\n\t}\n\n\terr = do_add_mount(mnt, path, path->mnt->mnt_flags | MNT_SHRINKABLE);\n\tif (!err)\n\t\treturn 0;\nfail:\n\t/* remove m from any expiration list it may be on */\n\tif (!list_empty(&mnt->mnt_expire)) {\n\t\tnamespace_lock();\n\t\tlist_del_init(&mnt->mnt_expire);\n\t\tnamespace_unlock();\n\t}\n\tmntput(m);\n\tmntput(m);\n\treturn err;\n}\n\n/**\n * mnt_set_expiry - Put a mount on an expiration list\n * @mnt: The mount to list.\n * @expiry_list: The list to add the mount to.\n */\nvoid mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)\n{\n\tnamespace_lock();\n\n\tlist_add_tail(&real_mount(mnt)->mnt_expire, expiry_list);\n\n\tnamespace_unlock();\n}\nEXPORT_SYMBOL(mnt_set_expiry);\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * mountpoints that aren't in use and haven't been touched since last we came\n * here\n */\nvoid mark_mounts_for_expiry(struct list_head *mounts)\n{\n\tstruct mount *mnt, *next;\n\tLIST_HEAD(graveyard);\n\n\tif (list_empty(mounts))\n\t\treturn;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* extract from the expiration list every vfsmount that matches the\n\t * following criteria:\n\t * - only referenced by its parent vfsmount\n\t * - still marked for expiry (marked on the last call here; marks are\n\t *   cleared by mntput())\n\t */\n\tlist_for_each_entry_safe(mnt, next, mounts, mnt_expire) {\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1) ||\n\t\t\tpropagate_mount_busy(mnt, 1))\n\t\t\tcontinue;\n\t\tlist_move(&mnt->mnt_expire, &graveyard);\n\t}\n\twhile (!list_empty(&graveyard)) {\n\t\tmnt = list_first_entry(&graveyard, struct mount, mnt_expire);\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nEXPORT_SYMBOL_GPL(mark_mounts_for_expiry);\n\n/*\n * Ripoff of 'select_parent()'\n *\n * search the list of submounts for a given mountpoint, and move any\n * shrinkable submounts to the 'graveyard' list.\n */\nstatic int select_submounts(struct mount *parent, struct list_head *graveyard)\n{\n\tstruct mount *this_parent = parent;\n\tstruct list_head *next;\n\tint found = 0;\n\nrepeat:\n\tnext = this_parent->mnt_mounts.next;\nresume:\n\twhile (next != &this_parent->mnt_mounts) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct mount *mnt = list_entry(tmp, struct mount, mnt_child);\n\n\t\tnext = tmp->next;\n\t\tif (!(mnt->mnt.mnt_flags & MNT_SHRINKABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Descend a level if the d_mounts list is non-empty.\n\t\t */\n\t\tif (!list_empty(&mnt->mnt_mounts)) {\n\t\t\tthis_parent = mnt;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tif (!propagate_mount_busy(mnt, 1)) {\n\t\t\tlist_move_tail(&mnt->mnt_expire, graveyard);\n\t\t\tfound++;\n\t\t}\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search\n\t */\n\tif (this_parent != parent) {\n\t\tnext = this_parent->mnt_child.next;\n\t\tthis_parent = this_parent->mnt_parent;\n\t\tgoto resume;\n\t}\n\treturn found;\n}\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * submounts of a specific parent mountpoint\n *\n * mount_lock must be held for write\n */\nstatic void shrink_submounts(struct mount *mnt)\n{\n\tLIST_HEAD(graveyard);\n\tstruct mount *m;\n\n\t/* extract submounts of 'mountpoint' from the expiration list */\n\twhile (select_submounts(mnt, &graveyard)) {\n\t\twhile (!list_empty(&graveyard)) {\n\t\t\tm = list_first_entry(&graveyard, struct mount,\n\t\t\t\t\t\tmnt_expire);\n\t\t\ttouch_mnt_namespace(m->mnt_ns);\n\t\t\tumount_tree(m, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t}\n\t}\n}\n\n/*\n * Some copy_from_user() implementations do not return the exact number of\n * bytes remaining to copy on a fault.  But copy_mount_options() requires that.\n * Note that this function differs from copy_from_user() in that it will oops\n * on bad values of `to', rather than returning a short copy.\n */\nstatic long exact_copy_from_user(void *to, const void __user * from,\n\t\t\t\t unsigned long n)\n{\n\tchar *t = to;\n\tconst char __user *f = from;\n\tchar c;\n\n\tif (!access_ok(VERIFY_READ, from, n))\n\t\treturn n;\n\n\twhile (n) {\n\t\tif (__get_user(c, f)) {\n\t\t\tmemset(t, 0, n);\n\t\t\tbreak;\n\t\t}\n\t\t*t++ = c;\n\t\tf++;\n\t\tn--;\n\t}\n\treturn n;\n}\n\nvoid *copy_mount_options(const void __user * data)\n{\n\tint i;\n\tunsigned long size;\n\tchar *copy;\n\n\tif (!data)\n\t\treturn NULL;\n\n\tcopy = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!copy)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/* We only care that *some* data at the address the user\n\t * gave us is valid.  Just in case, we'll zero\n\t * the remainder of the page.\n\t */\n\t/* copy_from_user cannot cross TASK_SIZE ! */\n\tsize = TASK_SIZE - (unsigned long)data;\n\tif (size > PAGE_SIZE)\n\t\tsize = PAGE_SIZE;\n\n\ti = size - exact_copy_from_user(copy, data, size);\n\tif (!i) {\n\t\tkfree(copy);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\tif (i != PAGE_SIZE)\n\t\tmemset(copy + i, 0, PAGE_SIZE - i);\n\treturn copy;\n}\n\nchar *copy_mount_string(const void __user *data)\n{\n\treturn data ? strndup_user(data, PAGE_SIZE) : NULL;\n}\n\n/*\n * Flags is a 32-bit value that allows up to 31 non-fs dependent flags to\n * be given to the mount() call (ie: read-only, no-dev, no-suid etc).\n *\n * data is a (void *) that can point to any structure up to\n * PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent\n * information (or be NULL).\n *\n * Pre-0.97 versions of mount() didn't have a flags word.\n * When the flags word was introduced its top half was required\n * to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.\n * Therefore, if this magic number is present, it carries no information\n * and must be discarded.\n */\nlong do_mount(const char *dev_name, const char __user *dir_name,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tstruct path path;\n\tint retval = 0;\n\tint mnt_flags = 0;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\t/* Basic sanity checks */\n\tif (data_page)\n\t\t((char *)data_page)[PAGE_SIZE - 1] = 0;\n\n\t/* ... and get the mountpoint */\n\tretval = user_path(dir_name, &path);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = security_sb_mount(dev_name, &path,\n\t\t\t\t   type_page, flags, data_page);\n\tif (!retval && !may_mount())\n\t\tretval = -EPERM;\n\tif (!retval && (flags & MS_MANDLOCK) && !may_mandlock())\n\t\tretval = -EPERM;\n\tif (retval)\n\t\tgoto dput_out;\n\n\t/* Default to relatime unless overriden */\n\tif (!(flags & MS_NOATIME))\n\t\tmnt_flags |= MNT_RELATIME;\n\n\t/* Separate the per-mountpoint flags */\n\tif (flags & MS_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (flags & MS_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (flags & MS_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (flags & MS_NOATIME)\n\t\tmnt_flags |= MNT_NOATIME;\n\tif (flags & MS_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (flags & MS_STRICTATIME)\n\t\tmnt_flags &= ~(MNT_RELATIME | MNT_NOATIME);\n\tif (flags & MS_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\n\t/* The default atime for remount is preservation */\n\tif ((flags & MS_REMOUNT) &&\n\t    ((flags & (MS_NOATIME | MS_NODIRATIME | MS_RELATIME |\n\t\t       MS_STRICTATIME)) == 0)) {\n\t\tmnt_flags &= ~MNT_ATIME_MASK;\n\t\tmnt_flags |= path.mnt->mnt_flags & MNT_ATIME_MASK;\n\t}\n\n\tflags &= ~(MS_NOSUID | MS_NOEXEC | MS_NODEV | MS_ACTIVE | MS_BORN |\n\t\t   MS_NOATIME | MS_NODIRATIME | MS_RELATIME| MS_KERNMOUNT |\n\t\t   MS_STRICTATIME);\n\n\tif (flags & MS_REMOUNT)\n\t\tretval = do_remount(&path, flags & ~MS_REMOUNT, mnt_flags,\n\t\t\t\t    data_page);\n\telse if (flags & MS_BIND)\n\t\tretval = do_loopback(&path, dev_name, flags & MS_REC);\n\telse if (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\tretval = do_change_type(&path, flags);\n\telse if (flags & MS_MOVE)\n\t\tretval = do_move_mount(&path, dev_name);\n\telse\n\t\tretval = do_new_mount(&path, type_page, flags, mnt_flags,\n\t\t\t\t      dev_name, data_page);\ndput_out:\n\tpath_put(&path);\n\treturn retval;\n}\n\nstatic struct ucounts *inc_mnt_namespaces(struct user_namespace *ns)\n{\n\treturn inc_ucount(ns, current_euid(), UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void dec_mnt_namespaces(struct ucounts *ucounts)\n{\n\tdec_ucount(ucounts, UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *ns)\n{\n\tns_free_inum(&ns->ns);\n\tdec_mnt_namespaces(ns->ucounts);\n\tput_user_ns(ns->user_ns);\n\tkfree(ns);\n}\n\n/*\n * Assign a sequence number so we can detect when we attempt to bind\n * mount a reference to an older mount namespace into the current\n * mount namespace, preventing reference counting loops.  A 64bit\n * number incrementing at 10Ghz will take 12,427 years to wrap which\n * is effectively never, so we can ignore the possibility.\n */\nstatic atomic64_t mnt_ns_seq = ATOMIC64_INIT(1);\n\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct ucounts *ucounts;\n\tint ret;\n\n\tucounts = inc_mnt_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnew_ns = kmalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns) {\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tret = ns_alloc_inum(&new_ns->ns);\n\tif (ret) {\n\t\tkfree(new_ns);\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(ret);\n\t}\n\tnew_ns->ns.ops = &mntns_operations;\n\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\tatomic_set(&new_ns->count, 1);\n\tnew_ns->root = NULL;\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tnew_ns->event = 0;\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\tnew_ns->ucounts = ucounts;\n\treturn new_ns;\n}\n\nstruct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}\n\n/**\n * create_mnt_ns - creates a private namespace and adds a root filesystem\n * @mnt: pointer to the new root filesystem mountpoint\n */\nstatic struct mnt_namespace *create_mnt_ns(struct vfsmount *m)\n{\n\tstruct mnt_namespace *new_ns = alloc_mnt_ns(&init_user_ns);\n\tif (!IS_ERR(new_ns)) {\n\t\tstruct mount *mnt = real_mount(m);\n\t\tmnt->mnt_ns = new_ns;\n\t\tnew_ns->root = mnt;\n\t\tlist_add(&mnt->mnt_list, &new_ns->list);\n\t} else {\n\t\tmntput(m);\n\t}\n\treturn new_ns;\n}\n\nstruct dentry *mount_subtree(struct vfsmount *mnt, const char *name)\n{\n\tstruct mnt_namespace *ns;\n\tstruct super_block *s;\n\tstruct path path;\n\tint err;\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\treturn ERR_CAST(ns);\n\n\terr = vfs_path_lookup(mnt->mnt_root, mnt,\n\t\t\tname, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);\n\n\tput_mnt_ns(ns);\n\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* trade a vfsmount reference for active sb one */\n\ts = path.mnt->mnt_sb;\n\tatomic_inc(&s->s_active);\n\tmntput(path.mnt);\n\t/* lock the sucker */\n\tdown_write(&s->s_umount);\n\t/* ... and return the root of (sub)tree on it */\n\treturn path.dentry;\n}\nEXPORT_SYMBOL(mount_subtree);\n\nSYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,\n\t\tchar __user *, type, unsigned long, flags, void __user *, data)\n{\n\tint ret;\n\tchar *kernel_type;\n\tchar *kernel_dev;\n\tvoid *options;\n\n\tkernel_type = copy_mount_string(type);\n\tret = PTR_ERR(kernel_type);\n\tif (IS_ERR(kernel_type))\n\t\tgoto out_type;\n\n\tkernel_dev = copy_mount_string(dev_name);\n\tret = PTR_ERR(kernel_dev);\n\tif (IS_ERR(kernel_dev))\n\t\tgoto out_dev;\n\n\toptions = copy_mount_options(data);\n\tret = PTR_ERR(options);\n\tif (IS_ERR(options))\n\t\tgoto out_data;\n\n\tret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);\n\n\tkfree(options);\nout_data:\n\tkfree(kernel_dev);\nout_dev:\n\tkfree(kernel_type);\nout_type:\n\treturn ret;\n}\n\n/*\n * Return true if path is reachable from root\n *\n * namespace_sem or mount_lock is held\n */\nbool is_path_reachable(struct mount *mnt, struct dentry *dentry,\n\t\t\t const struct path *root)\n{\n\twhile (&mnt->mnt != root->mnt && mnt_has_parent(mnt)) {\n\t\tdentry = mnt->mnt_mountpoint;\n\t\tmnt = mnt->mnt_parent;\n\t}\n\treturn &mnt->mnt == root->mnt && is_subdir(dentry, root->dentry);\n}\n\nbool path_is_under(struct path *path1, struct path *path2)\n{\n\tbool res;\n\tread_seqlock_excl(&mount_lock);\n\tres = is_path_reachable(real_mount(path1->mnt), path1->dentry, path2);\n\tread_sequnlock_excl(&mount_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_under);\n\n/*\n * pivot_root Semantics:\n * Moves the root file system of the current process to the directory put_old,\n * makes new_root as the new root file system of the current process, and sets\n * root/cwd of all processes which had them on the current root to new_root.\n *\n * Restrictions:\n * The new_root and put_old must be directories, and  must not be on the\n * same file  system as the current process root. The put_old  must  be\n * underneath new_root,  i.e. adding a non-zero number of /.. to the string\n * pointed to by put_old must yield the same directory as new_root. No other\n * file system may be mounted on put_old. After all, new_root is a mountpoint.\n *\n * Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.\n * See Documentation/filesystems/ramfs-rootfs-initramfs.txt for alternatives\n * in this situation.\n *\n * Notes:\n *  - we don't move root/cwd if they are not at the root (reason: if something\n *    cared enough to change them, it's probably wrong to force them elsewhere)\n *  - it's okay to pick a root that isn't the root of a file system, e.g.\n *    /nfs/my_root where /nfs is the mount point. It must be a mountpoint,\n *    though, so you may need to say mount --bind /nfs/my_root /nfs/my_root\n *    first.\n */\nSYSCALL_DEFINE2(pivot_root, const char __user *, new_root,\n\t\tconst char __user *, put_old)\n{\n\tstruct path new, old, parent_path, root_parent, root;\n\tstruct mount *new_mnt, *root_mnt, *old_mnt;\n\tstruct mountpoint *old_mp, *root_mp;\n\tint error;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terror = user_path_dir(new_root, &new);\n\tif (error)\n\t\tgoto out0;\n\n\terror = user_path_dir(put_old, &old);\n\tif (error)\n\t\tgoto out1;\n\n\terror = security_sb_pivotroot(&old, &new);\n\tif (error)\n\t\tgoto out2;\n\n\tget_fs_root(current->fs, &root);\n\told_mp = lock_mount(&old);\n\terror = PTR_ERR(old_mp);\n\tif (IS_ERR(old_mp))\n\t\tgoto out3;\n\n\terror = -EINVAL;\n\tnew_mnt = real_mount(new.mnt);\n\troot_mnt = real_mount(root.mnt);\n\told_mnt = real_mount(old.mnt);\n\tif (IS_MNT_SHARED(old_mnt) ||\n\t\tIS_MNT_SHARED(new_mnt->mnt_parent) ||\n\t\tIS_MNT_SHARED(root_mnt->mnt_parent))\n\t\tgoto out4;\n\tif (!check_mnt(root_mnt) || !check_mnt(new_mnt))\n\t\tgoto out4;\n\tif (new_mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out4;\n\terror = -ENOENT;\n\tif (d_unlinked(new.dentry))\n\t\tgoto out4;\n\terror = -EBUSY;\n\tif (new_mnt == root_mnt || old_mnt == root_mnt)\n\t\tgoto out4; /* loop, on the same file system  */\n\terror = -EINVAL;\n\tif (root.mnt->mnt_root != root.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(root_mnt))\n\t\tgoto out4; /* not attached */\n\troot_mp = root_mnt->mnt_mp;\n\tif (new.mnt->mnt_root != new.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(new_mnt))\n\t\tgoto out4; /* not attached */\n\t/* make sure we can reach put_old from new_root */\n\tif (!is_path_reachable(old_mnt, old.dentry, &new))\n\t\tgoto out4;\n\t/* make certain new is below the root */\n\tif (!is_path_reachable(new_mnt, new.dentry, &root))\n\t\tgoto out4;\n\troot_mp->m_count++; /* pin it so it won't go away */\n\tlock_mount_hash();\n\tdetach_mnt(new_mnt, &parent_path);\n\tdetach_mnt(root_mnt, &root_parent);\n\tif (root_mnt->mnt.mnt_flags & MNT_LOCKED) {\n\t\tnew_mnt->mnt.mnt_flags |= MNT_LOCKED;\n\t\troot_mnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n\t/* mount old root on put_old */\n\tattach_mnt(root_mnt, old_mnt, old_mp);\n\t/* mount new_root on / */\n\tattach_mnt(new_mnt, real_mount(root_parent.mnt), root_mp);\n\ttouch_mnt_namespace(current->nsproxy->mnt_ns);\n\t/* A moved mount should not expire automatically */\n\tlist_del_init(&new_mnt->mnt_expire);\n\tunlock_mount_hash();\n\tchroot_fs_refs(&root, &new);\n\tput_mountpoint(root_mp);\n\terror = 0;\nout4:\n\tunlock_mount(old_mp);\n\tif (!error) {\n\t\tpath_put(&root_parent);\n\t\tpath_put(&parent_path);\n\t}\nout3:\n\tpath_put(&root);\nout2:\n\tpath_put(&old);\nout1:\n\tpath_put(&new);\nout0:\n\treturn error;\n}\n\nstatic void __init init_mount_tree(void)\n{\n\tstruct vfsmount *mnt;\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\tstruct file_system_type *type;\n\n\ttype = get_fs_type(\"rootfs\");\n\tif (!type)\n\t\tpanic(\"Can't find rootfs type\");\n\tmnt = vfs_kern_mount(type, 0, \"rootfs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\tpanic(\"Can't create rootfs\");\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\tpanic(\"Can't allocate initial namespace\");\n\n\tinit_task.nsproxy->mnt_ns = ns;\n\tget_mnt_ns(ns);\n\n\troot.mnt = mnt;\n\troot.dentry = mnt->mnt_root;\n\tmnt->mnt_flags |= MNT_LOCKED;\n\n\tset_fs_pwd(current->fs, &root);\n\tset_fs_root(current->fs, &root);\n}\n\nvoid __init mnt_init(void)\n{\n\tunsigned u;\n\tint err;\n\n\tmnt_cache = kmem_cache_create(\"mnt_cache\", sizeof(struct mount),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\tmount_hashtable = alloc_large_system_hash(\"Mount-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmhash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&m_hash_shift, &m_hash_mask, 0, 0);\n\tmountpoint_hashtable = alloc_large_system_hash(\"Mountpoint-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmphash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&mp_hash_shift, &mp_hash_mask, 0, 0);\n\n\tif (!mount_hashtable || !mountpoint_hashtable)\n\t\tpanic(\"Failed to allocate mount hash table\\n\");\n\n\tfor (u = 0; u <= m_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mount_hashtable[u]);\n\tfor (u = 0; u <= mp_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mountpoint_hashtable[u]);\n\n\tkernfs_init();\n\n\terr = sysfs_init();\n\tif (err)\n\t\tprintk(KERN_WARNING \"%s: sysfs_init error: %d\\n\",\n\t\t\t__func__, err);\n\tfs_kobj = kobject_create_and_add(\"fs\", NULL);\n\tif (!fs_kobj)\n\t\tprintk(KERN_WARNING \"%s: kobj create error\\n\", __func__);\n\tinit_rootfs();\n\tinit_mount_tree();\n}\n\nvoid put_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!atomic_dec_and_test(&ns->count))\n\t\treturn;\n\tdrop_collected_mounts(&ns->root->mnt);\n\tfree_mnt_ns(ns);\n}\n\nstruct vfsmount *kern_mount_data(struct file_system_type *type, void *data)\n{\n\tstruct vfsmount *mnt;\n\tmnt = vfs_kern_mount(type, MS_KERNMOUNT, type->name, data);\n\tif (!IS_ERR(mnt)) {\n\t\t/*\n\t\t * it is a longterm mount, don't release mnt until\n\t\t * we unmount before file sys is unregistered\n\t\t*/\n\t\treal_mount(mnt)->mnt_ns = MNT_NS_INTERNAL;\n\t}\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(kern_mount_data);\n\nvoid kern_unmount(struct vfsmount *mnt)\n{\n\t/* release long term mount so mount point can be released */\n\tif (!IS_ERR_OR_NULL(mnt)) {\n\t\treal_mount(mnt)->mnt_ns = NULL;\n\t\tsynchronize_rcu();\t/* yecchhh... */\n\t\tmntput(mnt);\n\t}\n}\nEXPORT_SYMBOL(kern_unmount);\n\nbool our_mnt(struct vfsmount *mnt)\n{\n\treturn check_mnt(real_mount(mnt));\n}\n\nbool current_chrooted(void)\n{\n\t/* Does the current process have a non-standard root */\n\tstruct path ns_root;\n\tstruct path fs_root;\n\tbool chrooted;\n\n\t/* Find the namespace root */\n\tns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;\n\tns_root.dentry = ns_root.mnt->mnt_root;\n\tpath_get(&ns_root);\n\twhile (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))\n\t\t;\n\n\tget_fs_root(current->fs, &fs_root);\n\n\tchrooted = !path_equal(&fs_root, &ns_root);\n\n\tpath_put(&fs_root);\n\tpath_put(&ns_root);\n\n\treturn chrooted;\n}\n\nstatic bool mnt_already_visible(struct mnt_namespace *ns, struct vfsmount *new,\n\t\t\t\tint *new_mnt_flags)\n{\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tint mnt_flags;\n\n\t\tif (mnt->mnt.mnt_sb->s_type != new->mnt_sb->s_type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* A local view of the mount flags */\n\t\tmnt_flags = mnt->mnt.mnt_flags;\n\n\t\t/* Don't miss readonly hidden in the superblock flags */\n\t\tif (mnt->mnt.mnt_sb->s_flags & MS_RDONLY)\n\t\t\tmnt_flags |= MNT_LOCK_READONLY;\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(child->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t       MNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}\n\nstatic bool mount_too_revealing(struct vfsmount *mnt, int *new_mnt_flags)\n{\n\tconst unsigned long required_iflags = SB_I_NOEXEC | SB_I_NODEV;\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tunsigned long s_iflags;\n\n\tif (ns->user_ns == &init_user_ns)\n\t\treturn false;\n\n\t/* Can this filesystem be too revealing? */\n\ts_iflags = mnt->mnt_sb->s_iflags;\n\tif (!(s_iflags & SB_I_USERNS_VISIBLE))\n\t\treturn false;\n\n\tif ((s_iflags & required_iflags) != required_iflags) {\n\t\tWARN_ONCE(1, \"Expected s_iflags to contain 0x%lx\\n\",\n\t\t\t  required_iflags);\n\t\treturn true;\n\t}\n\n\treturn !mnt_already_visible(ns, mnt, new_mnt_flags);\n}\n\nbool mnt_may_suid(struct vfsmount *mnt)\n{\n\t/*\n\t * Foreign mounts (accessed via fchdir or through /proc\n\t * symlinks) are always treated as if they are nosuid.  This\n\t * prevents namespaces from trusting potentially unsafe\n\t * suid/sgid bits, file caps, or security labels that originate\n\t * in other namespaces.\n\t */\n\treturn !(mnt->mnt_flags & MNT_NOSUID) && check_mnt(real_mount(mnt)) &&\n\t       current_in_userns(mnt->mnt_sb->s_user_ns);\n}\n\nstatic struct ns_common *mntns_get(struct task_struct *task)\n{\n\tstruct ns_common *ns = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy) {\n\t\tns = &nsproxy->mnt_ns->ns;\n\t\tget_mnt_ns(to_mnt_ns(ns));\n\t}\n\ttask_unlock(task);\n\n\treturn ns;\n}\n\nstatic void mntns_put(struct ns_common *ns)\n{\n\tput_mnt_ns(to_mnt_ns(ns));\n}\n\nstatic int mntns_install(struct nsproxy *nsproxy, struct ns_common *ns)\n{\n\tstruct fs_struct *fs = current->fs;\n\tstruct mnt_namespace *mnt_ns = to_mnt_ns(ns);\n\tstruct path root;\n\n\tif (!ns_capable(mnt_ns->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_CHROOT) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (fs->users != 1)\n\t\treturn -EINVAL;\n\n\tget_mnt_ns(mnt_ns);\n\tput_mnt_ns(nsproxy->mnt_ns);\n\tnsproxy->mnt_ns = mnt_ns;\n\n\t/* Find the root */\n\troot.mnt    = &mnt_ns->root->mnt;\n\troot.dentry = mnt_ns->root->mnt.mnt_root;\n\tpath_get(&root);\n\twhile(d_mountpoint(root.dentry) && follow_down_one(&root))\n\t\t;\n\n\t/* Update the pwd and root */\n\tset_fs_pwd(fs, &root);\n\tset_fs_root(fs, &root);\n\n\tpath_put(&root);\n\treturn 0;\n}\n\nstatic struct user_namespace *mntns_owner(struct ns_common *ns)\n{\n\treturn to_mnt_ns(ns)->user_ns;\n}\n\nconst struct proc_ns_operations mntns_operations = {\n\t.name\t\t= \"mnt\",\n\t.type\t\t= CLONE_NEWNS,\n\t.get\t\t= mntns_get,\n\t.put\t\t= mntns_put,\n\t.install\t= mntns_install,\n\t.owner\t\t= mntns_owner,\n};\n", "/*\n *  linux/fs/pnode.c\n *\n * (C) Copyright IBM Corporation 2005.\n *\tReleased under GPL v2.\n *\tAuthor : Ram Pai (linuxram@us.ibm.com)\n *\n */\n#include <linux/mnt_namespace.h>\n#include <linux/mount.h>\n#include <linux/fs.h>\n#include <linux/nsproxy.h>\n#include \"internal.h\"\n#include \"pnode.h\"\n\n/* return the next shared peer mount of @p */\nstatic inline struct mount *next_peer(struct mount *p)\n{\n\treturn list_entry(p->mnt_share.next, struct mount, mnt_share);\n}\n\nstatic inline struct mount *first_slave(struct mount *p)\n{\n\treturn list_entry(p->mnt_slave_list.next, struct mount, mnt_slave);\n}\n\nstatic inline struct mount *next_slave(struct mount *p)\n{\n\treturn list_entry(p->mnt_slave.next, struct mount, mnt_slave);\n}\n\nstatic struct mount *get_peer_under_root(struct mount *mnt,\n\t\t\t\t\t struct mnt_namespace *ns,\n\t\t\t\t\t const struct path *root)\n{\n\tstruct mount *m = mnt;\n\n\tdo {\n\t\t/* Check the namespace first for optimization */\n\t\tif (m->mnt_ns == ns && is_path_reachable(m, m->mnt.mnt_root, root))\n\t\t\treturn m;\n\n\t\tm = next_peer(m);\n\t} while (m != mnt);\n\n\treturn NULL;\n}\n\n/*\n * Get ID of closest dominating peer group having a representative\n * under the given root.\n *\n * Caller must hold namespace_sem\n */\nint get_dominating_id(struct mount *mnt, const struct path *root)\n{\n\tstruct mount *m;\n\n\tfor (m = mnt->mnt_master; m != NULL; m = m->mnt_master) {\n\t\tstruct mount *d = get_peer_under_root(m, mnt->mnt_ns, root);\n\t\tif (d)\n\t\t\treturn d->mnt_group_id;\n\t}\n\n\treturn 0;\n}\n\nstatic int do_make_slave(struct mount *mnt)\n{\n\tstruct mount *peer_mnt = mnt, *master = mnt->mnt_master;\n\tstruct mount *slave_mnt;\n\n\t/*\n\t * slave 'mnt' to a peer mount that has the\n\t * same root dentry. If none is available then\n\t * slave it to anything that is available.\n\t */\n\twhile ((peer_mnt = next_peer(peer_mnt)) != mnt &&\n\t       peer_mnt->mnt.mnt_root != mnt->mnt.mnt_root) ;\n\n\tif (peer_mnt == mnt) {\n\t\tpeer_mnt = next_peer(mnt);\n\t\tif (peer_mnt == mnt)\n\t\t\tpeer_mnt = NULL;\n\t}\n\tif (mnt->mnt_group_id && IS_MNT_SHARED(mnt) &&\n\t    list_empty(&mnt->mnt_share))\n\t\tmnt_release_group_id(mnt);\n\n\tlist_del_init(&mnt->mnt_share);\n\tmnt->mnt_group_id = 0;\n\n\tif (peer_mnt)\n\t\tmaster = peer_mnt;\n\n\tif (master) {\n\t\tlist_for_each_entry(slave_mnt, &mnt->mnt_slave_list, mnt_slave)\n\t\t\tslave_mnt->mnt_master = master;\n\t\tlist_move(&mnt->mnt_slave, &master->mnt_slave_list);\n\t\tlist_splice(&mnt->mnt_slave_list, master->mnt_slave_list.prev);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t} else {\n\t\tstruct list_head *p = &mnt->mnt_slave_list;\n\t\twhile (!list_empty(p)) {\n                        slave_mnt = list_first_entry(p,\n\t\t\t\t\tstruct mount, mnt_slave);\n\t\t\tlist_del_init(&slave_mnt->mnt_slave);\n\t\t\tslave_mnt->mnt_master = NULL;\n\t\t}\n\t}\n\tmnt->mnt_master = master;\n\tCLEAR_MNT_SHARED(mnt);\n\treturn 0;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid change_mnt_propagation(struct mount *mnt, int type)\n{\n\tif (type == MS_SHARED) {\n\t\tset_mnt_shared(mnt);\n\t\treturn;\n\t}\n\tdo_make_slave(mnt);\n\tif (type != MS_SLAVE) {\n\t\tlist_del_init(&mnt->mnt_slave);\n\t\tmnt->mnt_master = NULL;\n\t\tif (type == MS_UNBINDABLE)\n\t\t\tmnt->mnt.mnt_flags |= MNT_UNBINDABLE;\n\t\telse\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_UNBINDABLE;\n\t}\n}\n\n/*\n * get the next mount in the propagation tree.\n * @m: the mount seen last\n * @origin: the original mount from where the tree walk initiated\n *\n * Note that peer groups form contiguous segments of slave lists.\n * We rely on that in get_source() to be able to find out if\n * vfsmount found while iterating with propagation_next() is\n * a peer of one we'd found earlier.\n */\nstatic struct mount *propagation_next(struct mount *m,\n\t\t\t\t\t struct mount *origin)\n{\n\t/* are there any slaves of this mount? */\n\tif (!IS_MNT_NEW(m) && !list_empty(&m->mnt_slave_list))\n\t\treturn first_slave(m);\n\n\twhile (1) {\n\t\tstruct mount *master = m->mnt_master;\n\n\t\tif (master == origin->mnt_master) {\n\t\t\tstruct mount *next = next_peer(m);\n\t\t\treturn (next == origin) ? NULL : next;\n\t\t} else if (m->mnt_slave.next != &master->mnt_slave_list)\n\t\t\treturn next_slave(m);\n\n\t\t/* back at master */\n\t\tm = master;\n\t}\n}\n\nstatic struct mount *next_group(struct mount *m, struct mount *origin)\n{\n\twhile (1) {\n\t\twhile (1) {\n\t\t\tstruct mount *next;\n\t\t\tif (!IS_MNT_NEW(m) && !list_empty(&m->mnt_slave_list))\n\t\t\t\treturn first_slave(m);\n\t\t\tnext = next_peer(m);\n\t\t\tif (m->mnt_group_id == origin->mnt_group_id) {\n\t\t\t\tif (next == origin)\n\t\t\t\t\treturn NULL;\n\t\t\t} else if (m->mnt_slave.next != &next->mnt_slave)\n\t\t\t\tbreak;\n\t\t\tm = next;\n\t\t}\n\t\t/* m is the last peer */\n\t\twhile (1) {\n\t\t\tstruct mount *master = m->mnt_master;\n\t\t\tif (m->mnt_slave.next != &master->mnt_slave_list)\n\t\t\t\treturn next_slave(m);\n\t\t\tm = next_peer(master);\n\t\t\tif (master->mnt_group_id == origin->mnt_group_id)\n\t\t\t\tbreak;\n\t\t\tif (master->mnt_slave.next == &m->mnt_slave)\n\t\t\t\tbreak;\n\t\t\tm = master;\n\t\t}\n\t\tif (m == origin)\n\t\t\treturn NULL;\n\t}\n}\n\n/* all accesses are serialized by namespace_sem */\nstatic struct user_namespace *user_ns;\nstatic struct mount *last_dest, *first_source, *last_source, *dest_master;\nstatic struct mountpoint *mp;\nstatic struct hlist_head *list;\n\nstatic inline bool peers(struct mount *m1, struct mount *m2)\n{\n\treturn m1->mnt_group_id == m2->mnt_group_id && m1->mnt_group_id;\n}\n\nstatic int propagate_one(struct mount *m)\n{\n\tstruct mount *child;\n\tint type;\n\t/* skip ones added by this propagate_mnt() */\n\tif (IS_MNT_NEW(m))\n\t\treturn 0;\n\t/* skip if mountpoint isn't covered by it */\n\tif (!is_subdir(mp->m_dentry, m->mnt.mnt_root))\n\t\treturn 0;\n\tif (peers(m, last_dest)) {\n\t\ttype = CL_MAKE_SHARED;\n\t} else {\n\t\tstruct mount *n, *p;\n\t\tbool done;\n\t\tfor (n = m; ; n = p) {\n\t\t\tp = n->mnt_master;\n\t\t\tif (p == dest_master || IS_MNT_MARKED(p))\n\t\t\t\tbreak;\n\t\t}\n\t\tdo {\n\t\t\tstruct mount *parent = last_source->mnt_parent;\n\t\t\tif (last_source == first_source)\n\t\t\t\tbreak;\n\t\t\tdone = parent->mnt_master == p;\n\t\t\tif (done && peers(n, parent))\n\t\t\t\tbreak;\n\t\t\tlast_source = last_source->mnt_master;\n\t\t} while (!done);\n\n\t\ttype = CL_SLAVE;\n\t\t/* beginning of peer group among the slaves? */\n\t\tif (IS_MNT_SHARED(m))\n\t\t\ttype |= CL_MAKE_SHARED;\n\t}\n\t\t\n\t/* Notice when we are propagating across user namespaces */\n\tif (m->mnt_ns->user_ns != user_ns)\n\t\ttype |= CL_UNPRIVILEGED;\n\tchild = copy_tree(last_source, last_source->mnt.mnt_root, type);\n\tif (IS_ERR(child))\n\t\treturn PTR_ERR(child);\n\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\tmnt_set_mountpoint(m, mp, child);\n\tlast_dest = m;\n\tlast_source = child;\n\tif (m->mnt_master != dest_master) {\n\t\tread_seqlock_excl(&mount_lock);\n\t\tSET_MNT_MARK(m->mnt_master);\n\t\tread_sequnlock_excl(&mount_lock);\n\t}\n\thlist_add_head(&child->mnt_hash, list);\n\treturn 0;\n}\n\n/*\n * mount 'source_mnt' under the destination 'dest_mnt' at\n * dentry 'dest_dentry'. And propagate that mount to\n * all the peer and slave mounts of 'dest_mnt'.\n * Link all the new mounts into a propagation tree headed at\n * source_mnt. Also link all the new mounts using ->mnt_list\n * headed at source_mnt's ->mnt_list\n *\n * @dest_mnt: destination mount.\n * @dest_dentry: destination dentry.\n * @source_mnt: source mount.\n * @tree_list : list of heads of trees to be attached.\n */\nint propagate_mnt(struct mount *dest_mnt, struct mountpoint *dest_mp,\n\t\t    struct mount *source_mnt, struct hlist_head *tree_list)\n{\n\tstruct mount *m, *n;\n\tint ret = 0;\n\n\t/*\n\t * we don't want to bother passing tons of arguments to\n\t * propagate_one(); everything is serialized by namespace_sem,\n\t * so globals will do just fine.\n\t */\n\tuser_ns = current->nsproxy->mnt_ns->user_ns;\n\tlast_dest = dest_mnt;\n\tfirst_source = source_mnt;\n\tlast_source = source_mnt;\n\tmp = dest_mp;\n\tlist = tree_list;\n\tdest_master = dest_mnt->mnt_master;\n\n\t/* all peers of dest_mnt, except dest_mnt itself */\n\tfor (n = next_peer(dest_mnt); n != dest_mnt; n = next_peer(n)) {\n\t\tret = propagate_one(n);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t/* all slave groups */\n\tfor (m = next_group(dest_mnt, dest_mnt); m;\n\t\t\tm = next_group(m, dest_mnt)) {\n\t\t/* everything in that slave group */\n\t\tn = m;\n\t\tdo {\n\t\t\tret = propagate_one(n);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tn = next_peer(n);\n\t\t} while (n != m);\n\t}\nout:\n\tread_seqlock_excl(&mount_lock);\n\thlist_for_each_entry(n, tree_list, mnt_hash) {\n\t\tm = n->mnt_parent;\n\t\tif (m->mnt_master != dest_mnt->mnt_master)\n\t\t\tCLEAR_MNT_MARK(m->mnt_master);\n\t}\n\tread_sequnlock_excl(&mount_lock);\n\treturn ret;\n}\n\n/*\n * return true if the refcount is greater than count\n */\nstatic inline int do_refcount_check(struct mount *mnt, int count)\n{\n\treturn mnt_get_count(mnt) > count;\n}\n\n/*\n * check if the mount 'mnt' can be unmounted successfully.\n * @mnt: the mount to be checked for unmount\n * NOTE: unmounting 'mnt' would naturally propagate to all\n * other mounts its parent propagates to.\n * Check if any of these mounts that **do not have submounts**\n * have more references than 'refcnt'. If so return busy.\n *\n * vfsmount lock must be held for write\n */\nint propagate_mount_busy(struct mount *mnt, int refcnt)\n{\n\tstruct mount *m, *child;\n\tstruct mount *parent = mnt->mnt_parent;\n\tint ret = 0;\n\n\tif (mnt == parent)\n\t\treturn do_refcount_check(mnt, refcnt);\n\n\t/*\n\t * quickly check if the current mount can be unmounted.\n\t * If not, we don't have to go checking for all other\n\t * mounts\n\t */\n\tif (!list_empty(&mnt->mnt_mounts) || do_refcount_check(mnt, refcnt))\n\t\treturn 1;\n\n\tfor (m = propagation_next(parent, parent); m;\n\t     \t\tm = propagation_next(m, parent)) {\n\t\tchild = __lookup_mnt_last(&m->mnt, mnt->mnt_mountpoint);\n\t\tif (child && list_empty(&child->mnt_mounts) &&\n\t\t    (ret = do_refcount_check(child, 1)))\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * Clear MNT_LOCKED when it can be shown to be safe.\n *\n * mount_lock lock must be held for write\n */\nvoid propagate_mount_unlock(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m, *child;\n\n\tBUG_ON(parent == mnt);\n\n\tfor (m = propagation_next(parent, parent); m;\n\t\t\tm = propagation_next(m, parent)) {\n\t\tchild = __lookup_mnt_last(&m->mnt, mnt->mnt_mountpoint);\n\t\tif (child)\n\t\t\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n}\n\n/*\n * Mark all mounts that the MNT_LOCKED logic will allow to be unmounted.\n */\nstatic void mark_umount_candidates(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\n\tBUG_ON(parent == mnt);\n\n\tfor (m = propagation_next(parent, parent); m;\n\t\t\tm = propagation_next(m, parent)) {\n\t\tstruct mount *child = __lookup_mnt_last(&m->mnt,\n\t\t\t\t\t\tmnt->mnt_mountpoint);\n\t\tif (child && (!IS_MNT_LOCKED(child) || IS_MNT_MARKED(m))) {\n\t\t\tSET_MNT_MARK(child);\n\t\t}\n\t}\n}\n\n/*\n * NOTE: unmounting 'mnt' naturally propagates to all other mounts its\n * parent propagates to.\n */\nstatic void __propagate_umount(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\n\tBUG_ON(parent == mnt);\n\n\tfor (m = propagation_next(parent, parent); m;\n\t\t\tm = propagation_next(m, parent)) {\n\n\t\tstruct mount *child = __lookup_mnt_last(&m->mnt,\n\t\t\t\t\t\tmnt->mnt_mountpoint);\n\t\t/*\n\t\t * umount the child only if the child has no children\n\t\t * and the child is marked safe to unmount.\n\t\t */\n\t\tif (!child || !IS_MNT_MARKED(child))\n\t\t\tcontinue;\n\t\tCLEAR_MNT_MARK(child);\n\t\tif (list_empty(&child->mnt_mounts)) {\n\t\t\tlist_del_init(&child->mnt_child);\n\t\t\tchild->mnt.mnt_flags |= MNT_UMOUNT;\n\t\t\tlist_move_tail(&child->mnt_list, &mnt->mnt_list);\n\t\t}\n\t}\n}\n\n/*\n * collect all mounts that receive propagation from the mount in @list,\n * and return these additional mounts in the same list.\n * @list: the list of mounts to be unmounted.\n *\n * vfsmount lock must be held for write\n */\nint propagate_umount(struct list_head *list)\n{\n\tstruct mount *mnt;\n\n\tlist_for_each_entry_reverse(mnt, list, mnt_list)\n\t\tmark_umount_candidates(mnt);\n\n\tlist_for_each_entry(mnt, list, mnt_list)\n\t\t__propagate_umount(mnt);\n\treturn 0;\n}\n", "/*\n *  linux/fs/pnode.h\n *\n * (C) Copyright IBM Corporation 2005.\n *\tReleased under GPL v2.\n *\n */\n#ifndef _LINUX_PNODE_H\n#define _LINUX_PNODE_H\n\n#include <linux/list.h>\n#include \"mount.h\"\n\n#define IS_MNT_SHARED(m) ((m)->mnt.mnt_flags & MNT_SHARED)\n#define IS_MNT_SLAVE(m) ((m)->mnt_master)\n#define IS_MNT_NEW(m)  (!(m)->mnt_ns)\n#define CLEAR_MNT_SHARED(m) ((m)->mnt.mnt_flags &= ~MNT_SHARED)\n#define IS_MNT_UNBINDABLE(m) ((m)->mnt.mnt_flags & MNT_UNBINDABLE)\n#define IS_MNT_MARKED(m) ((m)->mnt.mnt_flags & MNT_MARKED)\n#define SET_MNT_MARK(m) ((m)->mnt.mnt_flags |= MNT_MARKED)\n#define CLEAR_MNT_MARK(m) ((m)->mnt.mnt_flags &= ~MNT_MARKED)\n#define IS_MNT_LOCKED(m) ((m)->mnt.mnt_flags & MNT_LOCKED)\n\n#define CL_EXPIRE    \t\t0x01\n#define CL_SLAVE     \t\t0x02\n#define CL_COPY_UNBINDABLE\t0x04\n#define CL_MAKE_SHARED \t\t0x08\n#define CL_PRIVATE \t\t0x10\n#define CL_SHARED_TO_SLAVE\t0x20\n#define CL_UNPRIVILEGED\t\t0x40\n#define CL_COPY_MNT_NS_FILE\t0x80\n\n#define CL_COPY_ALL\t\t(CL_COPY_UNBINDABLE | CL_COPY_MNT_NS_FILE)\n\nstatic inline void set_mnt_shared(struct mount *mnt)\n{\n\tmnt->mnt.mnt_flags &= ~MNT_SHARED_MASK;\n\tmnt->mnt.mnt_flags |= MNT_SHARED;\n}\n\nvoid change_mnt_propagation(struct mount *, int);\nint propagate_mnt(struct mount *, struct mountpoint *, struct mount *,\n\t\tstruct hlist_head *);\nint propagate_umount(struct list_head *);\nint propagate_mount_busy(struct mount *, int);\nvoid propagate_mount_unlock(struct mount *);\nvoid mnt_release_group_id(struct mount *);\nint get_dominating_id(struct mount *mnt, const struct path *root);\nunsigned int mnt_get_count(struct mount *mnt);\nvoid mnt_set_mountpoint(struct mount *, struct mountpoint *,\n\t\t\tstruct mount *);\nstruct mount *copy_tree(struct mount *, struct dentry *, int);\nbool is_path_reachable(struct mount *, struct dentry *,\n\t\t\t const struct path *root);\n#endif /* _LINUX_PNODE_H */\n", "/*\n *\n * Definitions for mount interface. This describes the in the kernel build \n * linkedlist with mounted filesystems.\n *\n * Author:  Marco van Wieringen <mvw@planets.elm.net>\n *\n */\n#ifndef _LINUX_MOUNT_H\n#define _LINUX_MOUNT_H\n\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/nodemask.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/atomic.h>\n\nstruct super_block;\nstruct vfsmount;\nstruct dentry;\nstruct mnt_namespace;\n\n#define MNT_NOSUID\t0x01\n#define MNT_NODEV\t0x02\n#define MNT_NOEXEC\t0x04\n#define MNT_NOATIME\t0x08\n#define MNT_NODIRATIME\t0x10\n#define MNT_RELATIME\t0x20\n#define MNT_READONLY\t0x40\t/* does the user want this to be r/o? */\n\n#define MNT_SHRINKABLE\t0x100\n#define MNT_WRITE_HOLD\t0x200\n\n#define MNT_SHARED\t0x1000\t/* if the vfsmount is a shared mount */\n#define MNT_UNBINDABLE\t0x2000\t/* if the vfsmount is a unbindable mount */\n/*\n * MNT_SHARED_MASK is the set of flags that should be cleared when a\n * mount becomes shared.  Currently, this is only the flag that says a\n * mount cannot be bind mounted, since this is how we create a mount\n * that shares events with another mount.  If you add a new MNT_*\n * flag, consider how it interacts with shared mounts.\n */\n#define MNT_SHARED_MASK\t(MNT_UNBINDABLE)\n#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \\\n\t\t\t\t | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \\\n\t\t\t\t | MNT_READONLY)\n#define MNT_ATIME_MASK (MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME )\n\n#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \\\n\t\t\t    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)\n\n#define MNT_INTERNAL\t0x4000\n\n#define MNT_LOCK_ATIME\t\t0x040000\n#define MNT_LOCK_NOEXEC\t\t0x080000\n#define MNT_LOCK_NOSUID\t\t0x100000\n#define MNT_LOCK_NODEV\t\t0x200000\n#define MNT_LOCK_READONLY\t0x400000\n#define MNT_LOCKED\t\t0x800000\n#define MNT_DOOMED\t\t0x1000000\n#define MNT_SYNC_UMOUNT\t\t0x2000000\n#define MNT_MARKED\t\t0x4000000\n#define MNT_UMOUNT\t\t0x8000000\n\nstruct vfsmount {\n\tstruct dentry *mnt_root;\t/* root of the mounted tree */\n\tstruct super_block *mnt_sb;\t/* pointer to superblock */\n\tint mnt_flags;\n};\n\nstruct file; /* forward dec */\nstruct path;\n\nextern int mnt_want_write(struct vfsmount *mnt);\nextern int mnt_want_write_file(struct file *file);\nextern int mnt_clone_write(struct vfsmount *mnt);\nextern void mnt_drop_write(struct vfsmount *mnt);\nextern void mnt_drop_write_file(struct file *file);\nextern void mntput(struct vfsmount *mnt);\nextern struct vfsmount *mntget(struct vfsmount *mnt);\nextern struct vfsmount *mnt_clone_internal(struct path *path);\nextern int __mnt_is_readonly(struct vfsmount *mnt);\nextern bool mnt_may_suid(struct vfsmount *mnt);\n\nstruct path;\nextern struct vfsmount *clone_private_mount(struct path *path);\n\nstruct file_system_type;\nextern struct vfsmount *vfs_kern_mount(struct file_system_type *type,\n\t\t\t\t      int flags, const char *name,\n\t\t\t\t      void *data);\n\nextern void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list);\nextern void mark_mounts_for_expiry(struct list_head *mounts);\n\nextern dev_t name_to_dev_t(const char *name);\n\n#endif /* _LINUX_MOUNT_H */\n", "/*\n * sysctl.c: General linux system control interface\n *\n * Begun 24 March 1995, Stephen Tweedie\n * Added /proc support, Dec 1995\n * Added bdflush entry and intvec min/max checking, 2/23/96, Tom Dyas.\n * Added hooks for /proc/sys/net (minor, minor patch), 96/4/1, Mike Shaver.\n * Added kernel/java-{interpreter,appletviewer}, 96/5/10, Mike Shaver.\n * Dynamic registration fixes, Stephen Tweedie.\n * Added kswapd-interval, ctrl-alt-del, printk stuff, 1/8/97, Chris Horn.\n * Made sysctl support optional via CONFIG_SYSCTL, 1/10/97, Chris\n *  Horn.\n * Added proc_doulongvec_ms_jiffies_minmax, 09/08/99, Carlos H. Bauer.\n * Added proc_doulongvec_minmax, 09/08/99, Carlos H. Bauer.\n * Changed linked lists to use list.h instead of lists.h, 02/24/00, Bill\n *  Wendling.\n * The list_for_each() macro wasn't appropriate for the sysctl loop.\n *  Removed it and replaced it with older style, 03/23/00, Bill Wendling\n */\n\n#include <linux/module.h>\n#include <linux/aio.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/bitmap.h>\n#include <linux/signal.h>\n#include <linux/printk.h>\n#include <linux/proc_fs.h>\n#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/kmemcheck.h>\n#include <linux/kmemleak.h>\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/kobject.h>\n#include <linux/net.h>\n#include <linux/sysrq.h>\n#include <linux/highuid.h>\n#include <linux/writeback.h>\n#include <linux/ratelimit.h>\n#include <linux/compaction.h>\n#include <linux/hugetlb.h>\n#include <linux/initrd.h>\n#include <linux/key.h>\n#include <linux/times.h>\n#include <linux/limits.h>\n#include <linux/dcache.h>\n#include <linux/dnotify.h>\n#include <linux/syscalls.h>\n#include <linux/vmstat.h>\n#include <linux/nfs_fs.h>\n#include <linux/acpi.h>\n#include <linux/reboot.h>\n#include <linux/ftrace.h>\n#include <linux/perf_event.h>\n#include <linux/kprobes.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/oom.h>\n#include <linux/kmod.h>\n#include <linux/capability.h>\n#include <linux/binfmts.h>\n#include <linux/sched/sysctl.h>\n#include <linux/kexec.h>\n#include <linux/bpf.h>\n\n#include <asm/uaccess.h>\n#include <asm/processor.h>\n\n#ifdef CONFIG_X86\n#include <asm/nmi.h>\n#include <asm/stacktrace.h>\n#include <asm/io.h>\n#endif\n#ifdef CONFIG_SPARC\n#include <asm/setup.h>\n#endif\n#ifdef CONFIG_BSD_PROCESS_ACCT\n#include <linux/acct.h>\n#endif\n#ifdef CONFIG_RT_MUTEXES\n#include <linux/rtmutex.h>\n#endif\n#if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_LOCK_STAT)\n#include <linux/lockdep.h>\n#endif\n#ifdef CONFIG_CHR_DEV_SG\n#include <scsi/sg.h>\n#endif\n\n#ifdef CONFIG_LOCKUP_DETECTOR\n#include <linux/nmi.h>\n#endif\n\n#if defined(CONFIG_SYSCTL)\n\n/* External variables not in a header file. */\nextern int suid_dumpable;\n#ifdef CONFIG_COREDUMP\nextern int core_uses_pid;\nextern char core_pattern[];\nextern unsigned int core_pipe_limit;\n#endif\nextern int pid_max;\nextern int pid_max_min, pid_max_max;\nextern int percpu_pagelist_fraction;\nextern int compat_log;\nextern int latencytop_enabled;\nextern int sysctl_nr_open_min, sysctl_nr_open_max;\n#ifndef CONFIG_MMU\nextern int sysctl_nr_trim_pages;\n#endif\n\n/* Constants used for minimum and  maximum */\n#ifdef CONFIG_LOCKUP_DETECTOR\nstatic int sixty = 60;\n#endif\n\nstatic int __maybe_unused neg_one = -1;\n\nstatic int zero;\nstatic int __maybe_unused one = 1;\nstatic int __maybe_unused two = 2;\nstatic int __maybe_unused four = 4;\nstatic unsigned long one_ul = 1;\nstatic int one_hundred = 100;\nstatic int one_thousand = 1000;\n#ifdef CONFIG_PRINTK\nstatic int ten_thousand = 10000;\n#endif\n#ifdef CONFIG_PERF_EVENTS\nstatic int six_hundred_forty_kb = 640 * 1024;\n#endif\n\n/* this is needed for the proc_doulongvec_minmax of vm_dirty_bytes */\nstatic unsigned long dirty_bytes_min = 2 * PAGE_SIZE;\n\n/* this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID */\nstatic int maxolduid = 65535;\nstatic int minolduid;\n\nstatic int ngroups_max = NGROUPS_MAX;\nstatic const int cap_last_cap = CAP_LAST_CAP;\n\n/*this is needed for proc_doulongvec_minmax of sysctl_hung_task_timeout_secs */\n#ifdef CONFIG_DETECT_HUNG_TASK\nstatic unsigned long hung_task_timeout_max = (LONG_MAX/HZ);\n#endif\n\n#ifdef CONFIG_INOTIFY_USER\n#include <linux/inotify.h>\n#endif\n#ifdef CONFIG_SPARC\n#endif\n\n#ifdef __hppa__\nextern int pwrsw_enabled;\n#endif\n\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\nextern int unaligned_enabled;\n#endif\n\n#ifdef CONFIG_IA64\nextern int unaligned_dump_stack;\n#endif\n\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\nextern int no_unaligned_warning;\n#endif\n\n#ifdef CONFIG_PROC_SYSCTL\n\n#define SYSCTL_WRITES_LEGACY\t-1\n#define SYSCTL_WRITES_WARN\t 0\n#define SYSCTL_WRITES_STRICT\t 1\n\nstatic int sysctl_writes_strict = SYSCTL_WRITES_STRICT;\n\nstatic int proc_do_cad_pid(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_taint(struct ctl_table *table, int write,\n\t\t\t       void __user *buffer, size_t *lenp, loff_t *ppos);\n#endif\n\n#ifdef CONFIG_PRINTK\nstatic int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n#endif\n\nstatic int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n#ifdef CONFIG_COREDUMP\nstatic int proc_dostring_coredump(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\n/* Note: sysrq code uses it's own private copy */\nstatic int __sysrq_enabled = CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE;\n\nstatic int sysrq_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tint error;\n\n\terror = proc_dointvec(table, write, buffer, lenp, ppos);\n\tif (error)\n\t\treturn error;\n\n\tif (write)\n\t\tsysrq_toggle_support(__sysrq_enabled);\n\n\treturn 0;\n}\n\n#endif\n\nstatic struct ctl_table kern_table[];\nstatic struct ctl_table vm_table[];\nstatic struct ctl_table fs_table[];\nstatic struct ctl_table debug_table[];\nstatic struct ctl_table dev_table[];\nextern struct ctl_table random_table[];\n#ifdef CONFIG_EPOLL\nextern struct ctl_table epoll_table[];\n#endif\n\n#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT\nint sysctl_legacy_va_layout;\n#endif\n\n/* The default sysctl tables: */\n\nstatic struct ctl_table sysctl_base_table[] = {\n\t{\n\t\t.procname\t= \"kernel\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= kern_table,\n\t},\n\t{\n\t\t.procname\t= \"vm\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= vm_table,\n\t},\n\t{\n\t\t.procname\t= \"fs\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= fs_table,\n\t},\n\t{\n\t\t.procname\t= \"debug\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= debug_table,\n\t},\n\t{\n\t\t.procname\t= \"dev\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= dev_table,\n\t},\n\t{ }\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nstatic int min_sched_granularity_ns = 100000;\t\t/* 100 usecs */\nstatic int max_sched_granularity_ns = NSEC_PER_SEC;\t/* 1 second */\nstatic int min_wakeup_granularity_ns;\t\t\t/* 0 usecs */\nstatic int max_wakeup_granularity_ns = NSEC_PER_SEC;\t/* 1 second */\n#ifdef CONFIG_SMP\nstatic int min_sched_tunable_scaling = SCHED_TUNABLESCALING_NONE;\nstatic int max_sched_tunable_scaling = SCHED_TUNABLESCALING_END-1;\n#endif /* CONFIG_SMP */\n#endif /* CONFIG_SCHED_DEBUG */\n\n#ifdef CONFIG_COMPACTION\nstatic int min_extfrag_threshold;\nstatic int max_extfrag_threshold = 1000;\n#endif\n\nstatic struct ctl_table kern_table[] = {\n\t{\n\t\t.procname\t= \"sched_child_runs_first\",\n\t\t.data\t\t= &sysctl_sched_child_runs_first,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_SCHED_DEBUG\n\t{\n\t\t.procname\t= \"sched_min_granularity_ns\",\n\t\t.data\t\t= &sysctl_sched_min_granularity,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_sched_granularity_ns,\n\t\t.extra2\t\t= &max_sched_granularity_ns,\n\t},\n\t{\n\t\t.procname\t= \"sched_latency_ns\",\n\t\t.data\t\t= &sysctl_sched_latency,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_sched_granularity_ns,\n\t\t.extra2\t\t= &max_sched_granularity_ns,\n\t},\n\t{\n\t\t.procname\t= \"sched_wakeup_granularity_ns\",\n\t\t.data\t\t= &sysctl_sched_wakeup_granularity,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_wakeup_granularity_ns,\n\t\t.extra2\t\t= &max_wakeup_granularity_ns,\n\t},\n#ifdef CONFIG_SMP\n\t{\n\t\t.procname\t= \"sched_tunable_scaling\",\n\t\t.data\t\t= &sysctl_sched_tunable_scaling,\n\t\t.maxlen\t\t= sizeof(enum sched_tunable_scaling),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_sched_tunable_scaling,\n\t\t.extra2\t\t= &max_sched_tunable_scaling,\n\t},\n\t{\n\t\t.procname\t= \"sched_migration_cost_ns\",\n\t\t.data\t\t= &sysctl_sched_migration_cost,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sched_nr_migrate\",\n\t\t.data\t\t= &sysctl_sched_nr_migrate,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sched_time_avg_ms\",\n\t\t.data\t\t= &sysctl_sched_time_avg,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sched_shares_window_ns\",\n\t\t.data\t\t= &sysctl_sched_shares_window,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_SCHEDSTATS\n\t{\n\t\t.procname\t= \"sched_schedstats\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_schedstats,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif /* CONFIG_SCHEDSTATS */\n#endif /* CONFIG_SMP */\n#ifdef CONFIG_NUMA_BALANCING\n\t{\n\t\t.procname\t= \"numa_balancing_scan_delay_ms\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_delay,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing_scan_period_min_ms\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_period_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing_scan_period_max_ms\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_period_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing_scan_size_mb\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_size,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing\",\n\t\t.data\t\t= NULL, /* filled in by handler */\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_numa_balancing,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\t{\n\t\t.procname\t= \"sched_rt_period_us\",\n\t\t.data\t\t= &sysctl_sched_rt_period,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_rt_handler,\n\t},\n\t{\n\t\t.procname\t= \"sched_rt_runtime_us\",\n\t\t.data\t\t= &sysctl_sched_rt_runtime,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_rt_handler,\n\t},\n\t{\n\t\t.procname\t= \"sched_rr_timeslice_ms\",\n\t\t.data\t\t= &sched_rr_timeslice,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_rr_handler,\n\t},\n#ifdef CONFIG_SCHED_AUTOGROUP\n\t{\n\t\t.procname\t= \"sched_autogroup_enabled\",\n\t\t.data\t\t= &sysctl_sched_autogroup_enabled,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\t.procname\t= \"sched_cfs_bandwidth_slice_us\",\n\t\t.data\t\t= &sysctl_sched_cfs_bandwidth_slice,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\t.procname\t= \"prove_locking\",\n\t\t.data\t\t= &prove_locking,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_LOCK_STAT\n\t{\n\t\t.procname\t= \"lock_stat\",\n\t\t.data\t\t= &lock_stat,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"panic\",\n\t\t.data\t\t= &panic_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_COREDUMP\n\t{\n\t\t.procname\t= \"core_uses_pid\",\n\t\t.data\t\t= &core_uses_pid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"core_pattern\",\n\t\t.data\t\t= core_pattern,\n\t\t.maxlen\t\t= CORENAME_MAX_SIZE,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring_coredump,\n\t},\n\t{\n\t\t.procname\t= \"core_pipe_limit\",\n\t\t.data\t\t= &core_pipe_limit,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_PROC_SYSCTL\n\t{\n\t\t.procname\t= \"tainted\",\n\t\t.maxlen \t= sizeof(long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_taint,\n\t},\n\t{\n\t\t.procname\t= \"sysctl_writes_strict\",\n\t\t.data\t\t= &sysctl_writes_strict,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &neg_one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_LATENCYTOP\n\t{\n\t\t.procname\t= \"latencytop\",\n\t\t.data\t\t= &latencytop_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_latencytop,\n\t},\n#endif\n#ifdef CONFIG_BLK_DEV_INITRD\n\t{\n\t\t.procname\t= \"real-root-dev\",\n\t\t.data\t\t= &real_root_dev,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"print-fatal-signals\",\n\t\t.data\t\t= &print_fatal_signals,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_SPARC\n\t{\n\t\t.procname\t= \"reboot-cmd\",\n\t\t.data\t\t= reboot_command,\n\t\t.maxlen\t\t= 256,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n\t{\n\t\t.procname\t= \"stop-a\",\n\t\t.data\t\t= &stop_a_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"scons-poweroff\",\n\t\t.data\t\t= &scons_pwroff,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_SPARC64\n\t{\n\t\t.procname\t= \"tsb-ratio\",\n\t\t.data\t\t= &sysctl_tsb_ratio,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef __hppa__\n\t{\n\t\t.procname\t= \"soft-power\",\n\t\t.data\t\t= &pwrsw_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t \t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\n\t{\n\t\t.procname\t= \"unaligned-trap\",\n\t\t.data\t\t= &unaligned_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"ctrl-alt-del\",\n\t\t.data\t\t= &C_A_D,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_FUNCTION_TRACER\n\t{\n\t\t.procname\t= \"ftrace_enabled\",\n\t\t.data\t\t= &ftrace_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= ftrace_enable_sysctl,\n\t},\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t{\n\t\t.procname\t= \"stack_tracer_enabled\",\n\t\t.data\t\t= &stack_tracer_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= stack_trace_sysctl,\n\t},\n#endif\n#ifdef CONFIG_TRACING\n\t{\n\t\t.procname\t= \"ftrace_dump_on_oops\",\n\t\t.data\t\t= &ftrace_dump_on_oops,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"traceoff_on_warning\",\n\t\t.data\t\t= &__disable_trace_on_warning,\n\t\t.maxlen\t\t= sizeof(__disable_trace_on_warning),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"tracepoint_printk\",\n\t\t.data\t\t= &tracepoint_printk,\n\t\t.maxlen\t\t= sizeof(tracepoint_printk),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_KEXEC_CORE\n\t{\n\t\t.procname\t= \"kexec_load_disabled\",\n\t\t.data\t\t= &kexec_load_disabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t/* only handle a transition from default \"0\" to \"1\" */\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_MODULES\n\t{\n\t\t.procname\t= \"modprobe\",\n\t\t.data\t\t= &modprobe_path,\n\t\t.maxlen\t\t= KMOD_PATH_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n\t{\n\t\t.procname\t= \"modules_disabled\",\n\t\t.data\t\t= &modules_disabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t/* only handle a transition from default \"0\" to \"1\" */\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_UEVENT_HELPER\n\t{\n\t\t.procname\t= \"hotplug\",\n\t\t.data\t\t= &uevent_helper,\n\t\t.maxlen\t\t= UEVENT_HELPER_PATH_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n#endif\n#ifdef CONFIG_CHR_DEV_SG\n\t{\n\t\t.procname\t= \"sg-big-buff\",\n\t\t.data\t\t= &sg_big_buff,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_BSD_PROCESS_ACCT\n\t{\n\t\t.procname\t= \"acct\",\n\t\t.data\t\t= &acct_parm,\n\t\t.maxlen\t\t= 3*sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_MAGIC_SYSRQ\n\t{\n\t\t.procname\t= \"sysrq\",\n\t\t.data\t\t= &__sysrq_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysrq_sysctl_handler,\n\t},\n#endif\n#ifdef CONFIG_PROC_SYSCTL\n\t{\n\t\t.procname\t= \"cad_pid\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_do_cad_pid,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"threads-max\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_max_threads,\n\t},\n\t{\n\t\t.procname\t= \"random\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= random_table,\n\t},\n\t{\n\t\t.procname\t= \"usermodehelper\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= usermodehelper_table,\n\t},\n\t{\n\t\t.procname\t= \"overflowuid\",\n\t\t.data\t\t= &overflowuid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n\t{\n\t\t.procname\t= \"overflowgid\",\n\t\t.data\t\t= &overflowgid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n#ifdef CONFIG_S390\n#ifdef CONFIG_MATHEMU\n\t{\n\t\t.procname\t= \"ieee_emulation_warnings\",\n\t\t.data\t\t= &sysctl_ieee_emulation_warnings,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"userprocess_debug\",\n\t\t.data\t\t= &show_unhandled_signals,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pid_max\",\n\t\t.data\t\t= &pid_max,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &pid_max_min,\n\t\t.extra2\t\t= &pid_max_max,\n\t},\n\t{\n\t\t.procname\t= \"panic_on_oops\",\n\t\t.data\t\t= &panic_on_oops,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#if defined CONFIG_PRINTK\n\t{\n\t\t.procname\t= \"printk\",\n\t\t.data\t\t= &console_loglevel,\n\t\t.maxlen\t\t= 4*sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"printk_ratelimit\",\n\t\t.data\t\t= &printk_ratelimit_state.interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"printk_ratelimit_burst\",\n\t\t.data\t\t= &printk_ratelimit_state.burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"printk_delay\",\n\t\t.data\t\t= &printk_delay_msec,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &ten_thousand,\n\t},\n\t{\n\t\t.procname\t= \"printk_devkmsg\",\n\t\t.data\t\t= devkmsg_log_str,\n\t\t.maxlen\t\t= DEVKMSG_STR_MAX_SIZE,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= devkmsg_sysctl_set_loglvl,\n\t},\n\t{\n\t\t.procname\t= \"dmesg_restrict\",\n\t\t.data\t\t= &dmesg_restrict,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_sysadmin,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"kptr_restrict\",\n\t\t.data\t\t= &kptr_restrict,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_sysadmin,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"ngroups_max\",\n\t\t.data\t\t= &ngroups_max,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cap_last_cap\",\n\t\t.data\t\t= (void *)&cap_last_cap,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#if defined(CONFIG_LOCKUP_DETECTOR)\n\t{\n\t\t.procname       = \"watchdog\",\n\t\t.data           = &watchdog_user_enabled,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_watchdog,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"watchdog_thresh\",\n\t\t.data\t\t= &watchdog_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_watchdog_thresh,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &sixty,\n\t},\n\t{\n\t\t.procname       = \"nmi_watchdog\",\n\t\t.data           = &nmi_watchdog_enabled,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_nmi_watchdog,\n\t\t.extra1\t\t= &zero,\n#if defined(CONFIG_HAVE_NMI_WATCHDOG) || defined(CONFIG_HARDLOCKUP_DETECTOR)\n\t\t.extra2\t\t= &one,\n#else\n\t\t.extra2\t\t= &zero,\n#endif\n\t},\n\t{\n\t\t.procname       = \"soft_watchdog\",\n\t\t.data           = &soft_watchdog_enabled,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_soft_watchdog,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"watchdog_cpumask\",\n\t\t.data\t\t= &watchdog_cpumask_bits,\n\t\t.maxlen\t\t= NR_CPUS,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_watchdog_cpumask,\n\t},\n\t{\n\t\t.procname\t= \"softlockup_panic\",\n\t\t.data\t\t= &softlockup_panic,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\n\t{\n\t\t.procname\t= \"hardlockup_panic\",\n\t\t.data\t\t= &hardlockup_panic,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_SMP\n\t{\n\t\t.procname\t= \"softlockup_all_cpu_backtrace\",\n\t\t.data\t\t= &sysctl_softlockup_all_cpu_backtrace,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"hardlockup_all_cpu_backtrace\",\n\t\t.data\t\t= &sysctl_hardlockup_all_cpu_backtrace,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif /* CONFIG_SMP */\n#endif\n#if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_X86)\n\t{\n\t\t.procname       = \"unknown_nmi_panic\",\n\t\t.data           = &unknown_nmi_panic,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_dointvec,\n\t},\n#endif\n#if defined(CONFIG_X86)\n\t{\n\t\t.procname\t= \"panic_on_unrecovered_nmi\",\n\t\t.data\t\t= &panic_on_unrecovered_nmi,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"panic_on_io_nmi\",\n\t\t.data\t\t= &panic_on_io_nmi,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_DEBUG_STACKOVERFLOW\n\t{\n\t\t.procname\t= \"panic_on_stackoverflow\",\n\t\t.data\t\t= &sysctl_panic_on_stackoverflow,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"bootloader_type\",\n\t\t.data\t\t= &bootloader_type,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"bootloader_version\",\n\t\t.data\t\t= &bootloader_version,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"kstack_depth_to_print\",\n\t\t.data\t\t= &kstack_depth_to_print,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"io_delay_type\",\n\t\t.data\t\t= &io_delay_type,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#if defined(CONFIG_MMU)\n\t{\n\t\t.procname\t= \"randomize_va_space\",\n\t\t.data\t\t= &randomize_va_space,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#if defined(CONFIG_S390) && defined(CONFIG_SMP)\n\t{\n\t\t.procname\t= \"spin_retry\",\n\t\t.data\t\t= &spin_retry,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#if\tdefined(CONFIG_ACPI_SLEEP) && defined(CONFIG_X86)\n\t{\n\t\t.procname\t= \"acpi_video_flags\",\n\t\t.data\t\t= &acpi_realmode_flags,\n\t\t.maxlen\t\t= sizeof (unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#endif\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\n\t{\n\t\t.procname\t= \"ignore-unaligned-usertrap\",\n\t\t.data\t\t= &no_unaligned_warning,\n\t\t.maxlen\t\t= sizeof (int),\n\t \t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_IA64\n\t{\n\t\t.procname\t= \"unaligned-dump-stack\",\n\t\t.data\t\t= &unaligned_dump_stack,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n\t{\n\t\t.procname\t= \"hung_task_panic\",\n\t\t.data\t\t= &sysctl_hung_task_panic,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"hung_task_check_count\",\n\t\t.data\t\t= &sysctl_hung_task_check_count,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"hung_task_timeout_secs\",\n\t\t.data\t\t= &sysctl_hung_task_timeout_secs,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dohung_task_timeout_secs,\n\t\t.extra2\t\t= &hung_task_timeout_max,\n\t},\n\t{\n\t\t.procname\t= \"hung_task_warnings\",\n\t\t.data\t\t= &sysctl_hung_task_warnings,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &neg_one,\n\t},\n#endif\n#ifdef CONFIG_COMPAT\n\t{\n\t\t.procname\t= \"compat-log\",\n\t\t.data\t\t= &compat_log,\n\t\t.maxlen\t\t= sizeof (int),\n\t \t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_RT_MUTEXES\n\t{\n\t\t.procname\t= \"max_lock_depth\",\n\t\t.data\t\t= &max_lock_depth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"poweroff_cmd\",\n\t\t.data\t\t= &poweroff_cmd,\n\t\t.maxlen\t\t= POWEROFF_CMD_PATH_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n#ifdef CONFIG_KEYS\n\t{\n\t\t.procname\t= \"keys\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= key_sysctls,\n\t},\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\t/*\n\t * User-space scripts rely on the existence of this file\n\t * as a feature check for perf_events being enabled.\n\t *\n\t * So it's an ABI, do not remove!\n\t */\n\t{\n\t\t.procname\t= \"perf_event_paranoid\",\n\t\t.data\t\t= &sysctl_perf_event_paranoid,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_paranoid),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_mlock_kb\",\n\t\t.data\t\t= &sysctl_perf_event_mlock,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_mlock),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_max_sample_rate\",\n\t\t.data\t\t= &sysctl_perf_event_sample_rate,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_sample_rate),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_proc_update_handler,\n\t\t.extra1\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"perf_cpu_time_max_percent\",\n\t\t.data\t\t= &sysctl_perf_cpu_time_max_percent,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_cpu_time_max_percent),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_cpu_time_max_percent_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_max_stack\",\n\t\t.data\t\t= &sysctl_perf_event_max_stack,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_max_stack),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_event_max_stack_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &six_hundred_forty_kb,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_max_contexts_per_stack\",\n\t\t.data\t\t= &sysctl_perf_event_max_contexts_per_stack,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_max_contexts_per_stack),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_event_max_stack_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_thousand,\n\t},\n#endif\n#ifdef CONFIG_KMEMCHECK\n\t{\n\t\t.procname\t= \"kmemcheck\",\n\t\t.data\t\t= &kmemcheck_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"panic_on_warn\",\n\t\t.data\t\t= &panic_on_warn,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\n\t{\n\t\t.procname\t= \"timer_migration\",\n\t\t.data\t\t= &sysctl_timer_migration,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= timer_migration_handler,\n\t},\n#endif\n#ifdef CONFIG_BPF_SYSCALL\n\t{\n\t\t.procname\t= \"unprivileged_bpf_disabled\",\n\t\t.data\t\t= &sysctl_unprivileged_bpf_disabled,\n\t\t.maxlen\t\t= sizeof(sysctl_unprivileged_bpf_disabled),\n\t\t.mode\t\t= 0644,\n\t\t/* only handle a transition from default \"0\" to \"1\" */\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#if defined(CONFIG_TREE_RCU) || defined(CONFIG_PREEMPT_RCU)\n\t{\n\t\t.procname\t= \"panic_on_rcu_stall\",\n\t\t.data\t\t= &sysctl_panic_on_rcu_stall,\n\t\t.maxlen\t\t= sizeof(sysctl_panic_on_rcu_stall),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n\t{ }\n};\n\nstatic struct ctl_table vm_table[] = {\n\t{\n\t\t.procname\t= \"overcommit_memory\",\n\t\t.data\t\t= &sysctl_overcommit_memory,\n\t\t.maxlen\t\t= sizeof(sysctl_overcommit_memory),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n\t{\n\t\t.procname\t= \"panic_on_oom\",\n\t\t.data\t\t= &sysctl_panic_on_oom,\n\t\t.maxlen\t\t= sizeof(sysctl_panic_on_oom),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n\t{\n\t\t.procname\t= \"oom_kill_allocating_task\",\n\t\t.data\t\t= &sysctl_oom_kill_allocating_task,\n\t\t.maxlen\t\t= sizeof(sysctl_oom_kill_allocating_task),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"oom_dump_tasks\",\n\t\t.data\t\t= &sysctl_oom_dump_tasks,\n\t\t.maxlen\t\t= sizeof(sysctl_oom_dump_tasks),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"overcommit_ratio\",\n\t\t.data\t\t= &sysctl_overcommit_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_overcommit_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= overcommit_ratio_handler,\n\t},\n\t{\n\t\t.procname\t= \"overcommit_kbytes\",\n\t\t.data\t\t= &sysctl_overcommit_kbytes,\n\t\t.maxlen\t\t= sizeof(sysctl_overcommit_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= overcommit_kbytes_handler,\n\t},\n\t{\n\t\t.procname\t= \"page-cluster\", \n\t\t.data\t\t= &page_cluster,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"dirty_background_ratio\",\n\t\t.data\t\t= &dirty_background_ratio,\n\t\t.maxlen\t\t= sizeof(dirty_background_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_background_ratio_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"dirty_background_bytes\",\n\t\t.data\t\t= &dirty_background_bytes,\n\t\t.maxlen\t\t= sizeof(dirty_background_bytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_background_bytes_handler,\n\t\t.extra1\t\t= &one_ul,\n\t},\n\t{\n\t\t.procname\t= \"dirty_ratio\",\n\t\t.data\t\t= &vm_dirty_ratio,\n\t\t.maxlen\t\t= sizeof(vm_dirty_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_ratio_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"dirty_bytes\",\n\t\t.data\t\t= &vm_dirty_bytes,\n\t\t.maxlen\t\t= sizeof(vm_dirty_bytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_bytes_handler,\n\t\t.extra1\t\t= &dirty_bytes_min,\n\t},\n\t{\n\t\t.procname\t= \"dirty_writeback_centisecs\",\n\t\t.data\t\t= &dirty_writeback_interval,\n\t\t.maxlen\t\t= sizeof(dirty_writeback_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_writeback_centisecs_handler,\n\t},\n\t{\n\t\t.procname\t= \"dirty_expire_centisecs\",\n\t\t.data\t\t= &dirty_expire_interval,\n\t\t.maxlen\t\t= sizeof(dirty_expire_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"dirtytime_expire_seconds\",\n\t\t.data\t\t= &dirtytime_expire_interval,\n\t\t.maxlen\t\t= sizeof(dirty_expire_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirtytime_interval_handler,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname       = \"nr_pdflush_threads\",\n\t\t.mode           = 0444 /* read-only */,\n\t\t.proc_handler   = pdflush_proc_obsolete,\n\t},\n\t{\n\t\t.procname\t= \"swappiness\",\n\t\t.data\t\t= &vm_swappiness,\n\t\t.maxlen\t\t= sizeof(vm_swappiness),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n#ifdef CONFIG_HUGETLB_PAGE\n\t{\n\t\t.procname\t= \"nr_hugepages\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= hugetlb_sysctl_handler,\n\t},\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname       = \"nr_hugepages_mempolicy\",\n\t\t.data           = NULL,\n\t\t.maxlen         = sizeof(unsigned long),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = &hugetlb_mempolicy_sysctl_handler,\n\t},\n#endif\n\t {\n\t\t.procname\t= \"hugetlb_shm_group\",\n\t\t.data\t\t= &sysctl_hugetlb_shm_group,\n\t\t.maxlen\t\t= sizeof(gid_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t },\n\t {\n\t\t.procname\t= \"hugepages_treat_as_movable\",\n\t\t.data\t\t= &hugepages_treat_as_movable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"nr_overcommit_hugepages\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= hugetlb_overcommit_handler,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"lowmem_reserve_ratio\",\n\t\t.data\t\t= &sysctl_lowmem_reserve_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_lowmem_reserve_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= lowmem_reserve_ratio_sysctl_handler,\n\t},\n\t{\n\t\t.procname\t= \"drop_caches\",\n\t\t.data\t\t= &sysctl_drop_caches,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= drop_caches_sysctl_handler,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &four,\n\t},\n#ifdef CONFIG_COMPACTION\n\t{\n\t\t.procname\t= \"compact_memory\",\n\t\t.data\t\t= &sysctl_compact_memory,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0200,\n\t\t.proc_handler\t= sysctl_compaction_handler,\n\t},\n\t{\n\t\t.procname\t= \"extfrag_threshold\",\n\t\t.data\t\t= &sysctl_extfrag_threshold,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_extfrag_handler,\n\t\t.extra1\t\t= &min_extfrag_threshold,\n\t\t.extra2\t\t= &max_extfrag_threshold,\n\t},\n\t{\n\t\t.procname\t= \"compact_unevictable_allowed\",\n\t\t.data\t\t= &sysctl_compact_unevictable_allowed,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\n#endif /* CONFIG_COMPACTION */\n\t{\n\t\t.procname\t= \"min_free_kbytes\",\n\t\t.data\t\t= &min_free_kbytes,\n\t\t.maxlen\t\t= sizeof(min_free_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= min_free_kbytes_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"watermark_scale_factor\",\n\t\t.data\t\t= &watermark_scale_factor,\n\t\t.maxlen\t\t= sizeof(watermark_scale_factor),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= watermark_scale_factor_sysctl_handler,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one_thousand,\n\t},\n\t{\n\t\t.procname\t= \"percpu_pagelist_fraction\",\n\t\t.data\t\t= &percpu_pagelist_fraction,\n\t\t.maxlen\t\t= sizeof(percpu_pagelist_fraction),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= percpu_pagelist_fraction_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t},\n#ifdef CONFIG_MMU\n\t{\n\t\t.procname\t= \"max_map_count\",\n\t\t.data\t\t= &sysctl_max_map_count,\n\t\t.maxlen\t\t= sizeof(sysctl_max_map_count),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n#else\n\t{\n\t\t.procname\t= \"nr_trim_pages\",\n\t\t.data\t\t= &sysctl_nr_trim_pages,\n\t\t.maxlen\t\t= sizeof(sysctl_nr_trim_pages),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"laptop_mode\",\n\t\t.data\t\t= &laptop_mode,\n\t\t.maxlen\t\t= sizeof(laptop_mode),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"block_dump\",\n\t\t.data\t\t= &block_dump,\n\t\t.maxlen\t\t= sizeof(block_dump),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"vfs_cache_pressure\",\n\t\t.data\t\t= &sysctl_vfs_cache_pressure,\n\t\t.maxlen\t\t= sizeof(sysctl_vfs_cache_pressure),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT\n\t{\n\t\t.procname\t= \"legacy_va_layout\",\n\t\t.data\t\t= &sysctl_legacy_va_layout,\n\t\t.maxlen\t\t= sizeof(sysctl_legacy_va_layout),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n#endif\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname\t= \"zone_reclaim_mode\",\n\t\t.data\t\t= &node_reclaim_mode,\n\t\t.maxlen\t\t= sizeof(node_reclaim_mode),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"min_unmapped_ratio\",\n\t\t.data\t\t= &sysctl_min_unmapped_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_min_unmapped_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_min_unmapped_ratio_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"min_slab_ratio\",\n\t\t.data\t\t= &sysctl_min_slab_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_min_slab_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_min_slab_ratio_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n#endif\n#ifdef CONFIG_SMP\n\t{\n\t\t.procname\t= \"stat_interval\",\n\t\t.data\t\t= &sysctl_stat_interval,\n\t\t.maxlen\t\t= sizeof(sysctl_stat_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"stat_refresh\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= 0,\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= vmstat_refresh,\n\t},\n#endif\n#ifdef CONFIG_MMU\n\t{\n\t\t.procname\t= \"mmap_min_addr\",\n\t\t.data\t\t= &dac_mmap_min_addr,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= mmap_min_addr_handler,\n\t},\n#endif\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname\t= \"numa_zonelist_order\",\n\t\t.data\t\t= &numa_zonelist_order,\n\t\t.maxlen\t\t= NUMA_ZONELIST_ORDER_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= numa_zonelist_order_handler,\n\t},\n#endif\n#if (defined(CONFIG_X86_32) && !defined(CONFIG_UML))|| \\\n   (defined(CONFIG_SUPERH) && defined(CONFIG_VSYSCALL))\n\t{\n\t\t.procname\t= \"vdso_enabled\",\n#ifdef CONFIG_X86_32\n\t\t.data\t\t= &vdso32_enabled,\n\t\t.maxlen\t\t= sizeof(vdso32_enabled),\n#else\n\t\t.data\t\t= &vdso_enabled,\n\t\t.maxlen\t\t= sizeof(vdso_enabled),\n#endif\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n#endif\n#ifdef CONFIG_HIGHMEM\n\t{\n\t\t.procname\t= \"highmem_is_dirtyable\",\n\t\t.data\t\t= &vm_highmem_is_dirtyable,\n\t\t.maxlen\t\t= sizeof(vm_highmem_is_dirtyable),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_MEMORY_FAILURE\n\t{\n\t\t.procname\t= \"memory_failure_early_kill\",\n\t\t.data\t\t= &sysctl_memory_failure_early_kill,\n\t\t.maxlen\t\t= sizeof(sysctl_memory_failure_early_kill),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"memory_failure_recovery\",\n\t\t.data\t\t= &sysctl_memory_failure_recovery,\n\t\t.maxlen\t\t= sizeof(sysctl_memory_failure_recovery),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"user_reserve_kbytes\",\n\t\t.data\t\t= &sysctl_user_reserve_kbytes,\n\t\t.maxlen\t\t= sizeof(sysctl_user_reserve_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"admin_reserve_kbytes\",\n\t\t.data\t\t= &sysctl_admin_reserve_kbytes,\n\t\t.maxlen\t\t= sizeof(sysctl_admin_reserve_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\n\t{\n\t\t.procname\t= \"mmap_rnd_bits\",\n\t\t.data\t\t= &mmap_rnd_bits,\n\t\t.maxlen\t\t= sizeof(mmap_rnd_bits),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (void *)&mmap_rnd_bits_min,\n\t\t.extra2\t\t= (void *)&mmap_rnd_bits_max,\n\t},\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\n\t{\n\t\t.procname\t= \"mmap_rnd_compat_bits\",\n\t\t.data\t\t= &mmap_rnd_compat_bits,\n\t\t.maxlen\t\t= sizeof(mmap_rnd_compat_bits),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (void *)&mmap_rnd_compat_bits_min,\n\t\t.extra2\t\t= (void *)&mmap_rnd_compat_bits_max,\n\t},\n#endif\n\t{ }\n};\n\nstatic struct ctl_table fs_table[] = {\n\t{\n\t\t.procname\t= \"inode-nr\",\n\t\t.data\t\t= &inodes_stat,\n\t\t.maxlen\t\t= 2*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_inodes,\n\t},\n\t{\n\t\t.procname\t= \"inode-state\",\n\t\t.data\t\t= &inodes_stat,\n\t\t.maxlen\t\t= 7*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_inodes,\n\t},\n\t{\n\t\t.procname\t= \"file-nr\",\n\t\t.data\t\t= &files_stat,\n\t\t.maxlen\t\t= sizeof(files_stat),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_files,\n\t},\n\t{\n\t\t.procname\t= \"file-max\",\n\t\t.data\t\t= &files_stat.max_files,\n\t\t.maxlen\t\t= sizeof(files_stat.max_files),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"nr_open\",\n\t\t.data\t\t= &sysctl_nr_open,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &sysctl_nr_open_min,\n\t\t.extra2\t\t= &sysctl_nr_open_max,\n\t},\n\t{\n\t\t.procname\t= \"dentry-state\",\n\t\t.data\t\t= &dentry_stat,\n\t\t.maxlen\t\t= 6*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_dentry,\n\t},\n\t{\n\t\t.procname\t= \"overflowuid\",\n\t\t.data\t\t= &fs_overflowuid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n\t{\n\t\t.procname\t= \"overflowgid\",\n\t\t.data\t\t= &fs_overflowgid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n#ifdef CONFIG_FILE_LOCKING\n\t{\n\t\t.procname\t= \"leases-enable\",\n\t\t.data\t\t= &leases_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_DNOTIFY\n\t{\n\t\t.procname\t= \"dir-notify-enable\",\n\t\t.data\t\t= &dir_notify_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_MMU\n#ifdef CONFIG_FILE_LOCKING\n\t{\n\t\t.procname\t= \"lease-break-time\",\n\t\t.data\t\t= &lease_break_time,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_AIO\n\t{\n\t\t.procname\t= \"aio-nr\",\n\t\t.data\t\t= &aio_nr,\n\t\t.maxlen\t\t= sizeof(aio_nr),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"aio-max-nr\",\n\t\t.data\t\t= &aio_max_nr,\n\t\t.maxlen\t\t= sizeof(aio_max_nr),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#endif /* CONFIG_AIO */\n#ifdef CONFIG_INOTIFY_USER\n\t{\n\t\t.procname\t= \"inotify\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= inotify_table,\n\t},\n#endif\t\n#ifdef CONFIG_EPOLL\n\t{\n\t\t.procname\t= \"epoll\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= epoll_table,\n\t},\n#endif\n#endif\n\t{\n\t\t.procname\t= \"protected_symlinks\",\n\t\t.data\t\t= &sysctl_protected_symlinks,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"protected_hardlinks\",\n\t\t.data\t\t= &sysctl_protected_hardlinks,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"suid_dumpable\",\n\t\t.data\t\t= &suid_dumpable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_coredump,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n#if defined(CONFIG_BINFMT_MISC) || defined(CONFIG_BINFMT_MISC_MODULE)\n\t{\n\t\t.procname\t= \"binfmt_misc\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= sysctl_mount_point,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pipe-max-size\",\n\t\t.data\t\t= &pipe_max_size,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &pipe_proc_fn,\n\t\t.extra1\t\t= &pipe_min_size,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-hard\",\n\t\t.data\t\t= &pipe_user_pages_hard,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_hard),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-soft\",\n\t\t.data\t\t= &pipe_user_pages_soft,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_soft),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table debug_table[] = {\n#ifdef CONFIG_SYSCTL_EXCEPTION_TRACE\n\t{\n\t\t.procname\t= \"exception-trace\",\n\t\t.data\t\t= &show_unhandled_signals,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec\n\t},\n#endif\n#if defined(CONFIG_OPTPROBES)\n\t{\n\t\t.procname\t= \"kprobes-optimization\",\n\t\t.data\t\t= &sysctl_kprobes_optimization,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_kprobes_optimization_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n\t{ }\n};\n\nstatic struct ctl_table dev_table[] = {\n\t{ }\n};\n\nint __init sysctl_init(void)\n{\n\tstruct ctl_table_header *hdr;\n\n\thdr = register_sysctl_table(sysctl_base_table);\n\tkmemleak_not_leak(hdr);\n\treturn 0;\n}\n\n#endif /* CONFIG_SYSCTL */\n\n/*\n * /proc/sys support\n */\n\n#ifdef CONFIG_PROC_SYSCTL\n\nstatic int _proc_do_string(char *data, int maxlen, int write,\n\t\t\t   char __user *buffer,\n\t\t\t   size_t *lenp, loff_t *ppos)\n{\n\tsize_t len;\n\tchar __user *p;\n\tchar c;\n\n\tif (!data || !maxlen || !*lenp) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\tif (write) {\n\t\tif (sysctl_writes_strict == SYSCTL_WRITES_STRICT) {\n\t\t\t/* Only continue writes not past the end of buffer. */\n\t\t\tlen = strlen(data);\n\t\t\tif (len > maxlen - 1)\n\t\t\t\tlen = maxlen - 1;\n\n\t\t\tif (*ppos > len)\n\t\t\t\treturn 0;\n\t\t\tlen = *ppos;\n\t\t} else {\n\t\t\t/* Start writing from beginning of buffer. */\n\t\t\tlen = 0;\n\t\t}\n\n\t\t*ppos += *lenp;\n\t\tp = buffer;\n\t\twhile ((p - buffer) < *lenp && len < maxlen - 1) {\n\t\t\tif (get_user(c, p++))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (c == 0 || c == '\\n')\n\t\t\t\tbreak;\n\t\t\tdata[len++] = c;\n\t\t}\n\t\tdata[len] = 0;\n\t} else {\n\t\tlen = strlen(data);\n\t\tif (len > maxlen)\n\t\t\tlen = maxlen;\n\n\t\tif (*ppos > len) {\n\t\t\t*lenp = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tdata += *ppos;\n\t\tlen  -= *ppos;\n\n\t\tif (len > *lenp)\n\t\t\tlen = *lenp;\n\t\tif (len)\n\t\t\tif (copy_to_user(buffer, data, len))\n\t\t\t\treturn -EFAULT;\n\t\tif (len < *lenp) {\n\t\t\tif (put_user('\\n', buffer + len))\n\t\t\t\treturn -EFAULT;\n\t\t\tlen++;\n\t\t}\n\t\t*lenp = len;\n\t\t*ppos += len;\n\t}\n\treturn 0;\n}\n\nstatic void warn_sysctl_write(struct ctl_table *table)\n{\n\tpr_warn_once(\"%s wrote to %s when file position was not 0!\\n\"\n\t\t\"This will not be supported in the future. To silence this\\n\"\n\t\t\"warning, set kernel.sysctl_writes_strict = -1\\n\",\n\t\tcurrent->comm, table->procname);\n}\n\n/**\n * proc_dostring - read a string sysctl\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes a string from/to the user buffer. If the kernel\n * buffer provided is not large enough to hold the string, the\n * string is truncated. The copied string is %NULL-terminated.\n * If the string is being read by the user process, it is copied\n * and a newline '\\n' is added. It is truncated if the buffer is\n * not large enough.\n *\n * Returns 0 on success.\n */\nint proc_dostring(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write && *ppos && sysctl_writes_strict == SYSCTL_WRITES_WARN)\n\t\twarn_sysctl_write(table);\n\n\treturn _proc_do_string((char *)(table->data), table->maxlen, write,\n\t\t\t       (char __user *)buffer, lenp, ppos);\n}\n\nstatic size_t proc_skip_spaces(char **buf)\n{\n\tsize_t ret;\n\tchar *tmp = skip_spaces(*buf);\n\tret = tmp - *buf;\n\t*buf = tmp;\n\treturn ret;\n}\n\nstatic void proc_skip_char(char **buf, size_t *size, const char v)\n{\n\twhile (*size) {\n\t\tif (**buf != v)\n\t\t\tbreak;\n\t\t(*size)--;\n\t\t(*buf)++;\n\t}\n}\n\n#define TMPBUFLEN 22\n/**\n * proc_get_long - reads an ASCII formatted integer from a user buffer\n *\n * @buf: a kernel buffer\n * @size: size of the kernel buffer\n * @val: this is where the number will be stored\n * @neg: set to %TRUE if number is negative\n * @perm_tr: a vector which contains the allowed trailers\n * @perm_tr_len: size of the perm_tr vector\n * @tr: pointer to store the trailer character\n *\n * In case of success %0 is returned and @buf and @size are updated with\n * the amount of bytes read. If @tr is non-NULL and a trailing\n * character exists (size is non-zero after returning from this\n * function), @tr is updated with the trailing character.\n */\nstatic int proc_get_long(char **buf, size_t *size,\n\t\t\t  unsigned long *val, bool *neg,\n\t\t\t  const char *perm_tr, unsigned perm_tr_len, char *tr)\n{\n\tint len;\n\tchar *p, tmp[TMPBUFLEN];\n\n\tif (!*size)\n\t\treturn -EINVAL;\n\n\tlen = *size;\n\tif (len > TMPBUFLEN - 1)\n\t\tlen = TMPBUFLEN - 1;\n\n\tmemcpy(tmp, *buf, len);\n\n\ttmp[len] = 0;\n\tp = tmp;\n\tif (*p == '-' && *size > 1) {\n\t\t*neg = true;\n\t\tp++;\n\t} else\n\t\t*neg = false;\n\tif (!isdigit(*p))\n\t\treturn -EINVAL;\n\n\t*val = simple_strtoul(p, &p, 0);\n\n\tlen = p - tmp;\n\n\t/* We don't know if the next char is whitespace thus we may accept\n\t * invalid integers (e.g. 1234...a) or two integers instead of one\n\t * (e.g. 123...1). So lets not allow such large numbers. */\n\tif (len == TMPBUFLEN - 1)\n\t\treturn -EINVAL;\n\n\tif (len < *size && perm_tr_len && !memchr(perm_tr, *p, perm_tr_len))\n\t\treturn -EINVAL;\n\n\tif (tr && (len < *size))\n\t\t*tr = *p;\n\n\t*buf += len;\n\t*size -= len;\n\n\treturn 0;\n}\n\n/**\n * proc_put_long - converts an integer to a decimal ASCII formatted string\n *\n * @buf: the user buffer\n * @size: the size of the user buffer\n * @val: the integer to be converted\n * @neg: sign of the number, %TRUE for negative\n *\n * In case of success %0 is returned and @buf and @size are updated with\n * the amount of bytes written.\n */\nstatic int proc_put_long(void __user **buf, size_t *size, unsigned long val,\n\t\t\t  bool neg)\n{\n\tint len;\n\tchar tmp[TMPBUFLEN], *p = tmp;\n\n\tsprintf(p, \"%s%lu\", neg ? \"-\" : \"\", val);\n\tlen = strlen(tmp);\n\tif (len > *size)\n\t\tlen = *size;\n\tif (copy_to_user(*buf, tmp, len))\n\t\treturn -EFAULT;\n\t*size -= len;\n\t*buf += len;\n\treturn 0;\n}\n#undef TMPBUFLEN\n\nstatic int proc_put_char(void __user **buf, size_t *size, char c)\n{\n\tif (*size) {\n\t\tchar __user **buffer = (char __user **)buf;\n\t\tif (put_user(c, *buffer))\n\t\t\treturn -EFAULT;\n\t\t(*size)--, (*buffer)++;\n\t\t*buf = *buffer;\n\t}\n\treturn 0;\n}\n\nstatic int do_proc_dointvec_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t int *valp,\n\t\t\t\t int write, void *data)\n{\n\tif (write) {\n\t\tif (*negp) {\n\t\t\tif (*lvalp > (unsigned long) INT_MAX + 1)\n\t\t\t\treturn -EINVAL;\n\t\t\t*valp = -*lvalp;\n\t\t} else {\n\t\t\tif (*lvalp > (unsigned long) INT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\t*valp = *lvalp;\n\t\t}\n\t} else {\n\t\tint val = *valp;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\t*lvalp = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\t*lvalp = (unsigned long)val;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic const char proc_wspace_sep[] = { ' ', '\\t', '\\n' };\n\nstatic int __do_proc_dointvec(void *tbl_data, struct ctl_table *table,\n\t\t  int write, void __user *buffer,\n\t\t  size_t *lenp, loff_t *ppos,\n\t\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\n\t\t\t      int write, void *data),\n\t\t  void *data)\n{\n\tint *i, vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *kbuf = NULL, *p;\n\t\n\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\t\n\ti = (int *) tbl_data;\n\tvleft = table->maxlen / sizeof(*i);\n\tleft = *lenp;\n\n\tif (!conv)\n\t\tconv = do_proc_dointvec_conv;\n\n\tif (write) {\n\t\tif (*ppos) {\n\t\t\tswitch (sysctl_writes_strict) {\n\t\t\tcase SYSCTL_WRITES_STRICT:\n\t\t\t\tgoto out;\n\t\t\tcase SYSCTL_WRITES_WARN:\n\t\t\t\twarn_sysctl_write(table);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = kbuf = memdup_user_nul(buffer, left);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t}\n\n\tfor (; left && vleft--; i++, first=0) {\n\t\tunsigned long lval;\n\t\tbool neg;\n\n\t\tif (write) {\n\t\t\tleft -= proc_skip_spaces(&p);\n\n\t\t\tif (!left)\n\t\t\t\tbreak;\n\t\t\terr = proc_get_long(&p, &left, &lval, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (conv(&neg, &lval, i, 1, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (conv(&neg, &lval, i, 0, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!first)\n\t\t\t\terr = proc_put_char(&buffer, &left, '\\t');\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\terr = proc_put_long(&buffer, &left, lval, neg);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!write && !first && left && !err)\n\t\terr = proc_put_char(&buffer, &left, '\\n');\n\tif (write && !err && left)\n\t\tleft -= proc_skip_spaces(&p);\n\tif (write) {\n\t\tkfree(kbuf);\n\t\tif (first)\n\t\t\treturn err ? : -EINVAL;\n\t}\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}\n\nstatic int do_proc_dointvec(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos,\n\t\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\n\t\t\t      int write, void *data),\n\t\t  void *data)\n{\n\treturn __do_proc_dointvec(table->data, table, write,\n\t\t\tbuffer, lenp, ppos, conv, data);\n}\n\n/**\n * proc_dointvec - read a vector of integers\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n *\n * Returns 0 on success.\n */\nint proc_dointvec(struct ctl_table *table, int write,\n\t\t     void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\n\t\t    \t    NULL,NULL);\n}\n\n/*\n * Taint values can only be increased\n * This means we can safely use a temporary.\n */\nstatic int proc_taint(struct ctl_table *table, int write,\n\t\t\t       void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table t;\n\tunsigned long tmptaint = get_taint();\n\tint err;\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tt = *table;\n\tt.data = &tmptaint;\n\terr = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (write) {\n\t\t/*\n\t\t * Poor man's atomic or. Not worth adding a primitive\n\t\t * to everyone's atomic.h for this\n\t\t */\n\t\tint i;\n\t\tfor (i = 0; i < BITS_PER_LONG && tmptaint >> i; i++) {\n\t\t\tif ((tmptaint >> i) & 1)\n\t\t\t\tadd_taint(i, LOCKDEP_STILL_OK);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n#ifdef CONFIG_PRINTK\nstatic int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n}\n#endif\n\nstruct do_proc_dointvec_minmax_conv_param {\n\tint *min;\n\tint *max;\n};\n\nstatic int do_proc_dointvec_minmax_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\tint *valp,\n\t\t\t\t\tint write, void *data)\n{\n\tstruct do_proc_dointvec_minmax_conv_param *param = data;\n\tif (write) {\n\t\tint val = *negp ? -*lvalp : *lvalp;\n\t\tif ((param->min && *param->min > val) ||\n\t\t    (param->max && *param->max < val))\n\t\t\treturn -EINVAL;\n\t\t*valp = val;\n\t} else {\n\t\tint val = *valp;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\t*lvalp = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\t*lvalp = (unsigned long)val;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n * proc_dointvec_minmax - read a vector of integers with min/max values\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string.\n *\n * This routine will ensure the values are within the range specified by\n * table->extra1 (min) and table->extra2 (max).\n *\n * Returns 0 on success.\n */\nint proc_dointvec_minmax(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct do_proc_dointvec_minmax_conv_param param = {\n\t\t.min = (int *) table->extra1,\n\t\t.max = (int *) table->extra2,\n\t};\n\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\n\t\t\t\tdo_proc_dointvec_minmax_conv, &param);\n}\n\nstatic void validate_coredump_safety(void)\n{\n#ifdef CONFIG_COREDUMP\n\tif (suid_dumpable == SUID_DUMP_ROOT &&\n\t    core_pattern[0] != '/' && core_pattern[0] != '|') {\n\t\tprintk(KERN_WARNING \"Unsafe core_pattern used with \"\\\n\t\t\t\"suid_dumpable=2. Pipe handler or fully qualified \"\\\n\t\t\t\"core dump path required.\\n\");\n\t}\n#endif\n}\n\nstatic int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint error = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (!error)\n\t\tvalidate_coredump_safety();\n\treturn error;\n}\n\n#ifdef CONFIG_COREDUMP\nstatic int proc_dostring_coredump(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint error = proc_dostring(table, write, buffer, lenp, ppos);\n\tif (!error)\n\t\tvalidate_coredump_safety();\n\treturn error;\n}\n#endif\n\nstatic int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table, int write,\n\t\t\t\t     void __user *buffer,\n\t\t\t\t     size_t *lenp, loff_t *ppos,\n\t\t\t\t     unsigned long convmul,\n\t\t\t\t     unsigned long convdiv)\n{\n\tunsigned long *i, *min, *max;\n\tint vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *kbuf = NULL, *p;\n\n\tif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\ti = (unsigned long *) data;\n\tmin = (unsigned long *) table->extra1;\n\tmax = (unsigned long *) table->extra2;\n\tvleft = table->maxlen / sizeof(unsigned long);\n\tleft = *lenp;\n\n\tif (write) {\n\t\tif (*ppos) {\n\t\t\tswitch (sysctl_writes_strict) {\n\t\t\tcase SYSCTL_WRITES_STRICT:\n\t\t\t\tgoto out;\n\t\t\tcase SYSCTL_WRITES_WARN:\n\t\t\t\twarn_sysctl_write(table);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = kbuf = memdup_user_nul(buffer, left);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t}\n\n\tfor (; left && vleft--; i++, first = 0) {\n\t\tunsigned long val;\n\n\t\tif (write) {\n\t\t\tbool neg;\n\n\t\t\tleft -= proc_skip_spaces(&p);\n\n\t\t\terr = proc_get_long(&p, &left, &val, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (neg)\n\t\t\t\tcontinue;\n\t\t\tif ((min && val < *min) || (max && val > *max))\n\t\t\t\tcontinue;\n\t\t\t*i = val;\n\t\t} else {\n\t\t\tval = convdiv * (*i) / convmul;\n\t\t\tif (!first) {\n\t\t\t\terr = proc_put_char(&buffer, &left, '\\t');\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = proc_put_long(&buffer, &left, val, false);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!write && !first && left && !err)\n\t\terr = proc_put_char(&buffer, &left, '\\n');\n\tif (write && !err)\n\t\tleft -= proc_skip_spaces(&p);\n\tif (write) {\n\t\tkfree(kbuf);\n\t\tif (first)\n\t\t\treturn err ? : -EINVAL;\n\t}\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}\n\nstatic int do_proc_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t\t\t     void __user *buffer,\n\t\t\t\t     size_t *lenp, loff_t *ppos,\n\t\t\t\t     unsigned long convmul,\n\t\t\t\t     unsigned long convdiv)\n{\n\treturn __do_proc_doulongvec_minmax(table->data, table, write,\n\t\t\tbuffer, lenp, ppos, convmul, convdiv);\n}\n\n/**\n * proc_doulongvec_minmax - read a vector of long integers with min/max values\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long\n * values from/to the user buffer, treated as an ASCII string.\n *\n * This routine will ensure the values are within the range specified by\n * table->extra1 (min) and table->extra2 (max).\n *\n * Returns 0 on success.\n */\nint proc_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t\t   void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_doulongvec_minmax(table, write, buffer, lenp, ppos, 1l, 1l);\n}\n\n/**\n * proc_doulongvec_ms_jiffies_minmax - read a vector of millisecond values with min/max values\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long\n * values from/to the user buffer, treated as an ASCII string. The values\n * are treated as milliseconds, and converted to jiffies when they are stored.\n *\n * This routine will ensure the values are within the range specified by\n * table->extra1 (min) and table->extra2 (max).\n *\n * Returns 0 on success.\n */\nint proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int write,\n\t\t\t\t      void __user *buffer,\n\t\t\t\t      size_t *lenp, loff_t *ppos)\n{\n    return do_proc_doulongvec_minmax(table, write, buffer,\n\t\t\t\t     lenp, ppos, HZ, 1000l);\n}\n\n\nstatic int do_proc_dointvec_jiffies_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\t int *valp,\n\t\t\t\t\t int write, void *data)\n{\n\tif (write) {\n\t\tif (*lvalp > LONG_MAX / HZ)\n\t\t\treturn 1;\n\t\t*valp = *negp ? -(*lvalp*HZ) : (*lvalp*HZ);\n\t} else {\n\t\tint val = *valp;\n\t\tunsigned long lval;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\tlval = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\tlval = (unsigned long)val;\n\t\t}\n\t\t*lvalp = lval / HZ;\n\t}\n\treturn 0;\n}\n\nstatic int do_proc_dointvec_userhz_jiffies_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\t\tint *valp,\n\t\t\t\t\t\tint write, void *data)\n{\n\tif (write) {\n\t\tif (USER_HZ < HZ && *lvalp > (LONG_MAX / HZ) * USER_HZ)\n\t\t\treturn 1;\n\t\t*valp = clock_t_to_jiffies(*negp ? -*lvalp : *lvalp);\n\t} else {\n\t\tint val = *valp;\n\t\tunsigned long lval;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\tlval = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\tlval = (unsigned long)val;\n\t\t}\n\t\t*lvalp = jiffies_to_clock_t(lval);\n\t}\n\treturn 0;\n}\n\nstatic int do_proc_dointvec_ms_jiffies_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\t    int *valp,\n\t\t\t\t\t    int write, void *data)\n{\n\tif (write) {\n\t\tunsigned long jif = msecs_to_jiffies(*negp ? -*lvalp : *lvalp);\n\n\t\tif (jif > INT_MAX)\n\t\t\treturn 1;\n\t\t*valp = (int)jif;\n\t} else {\n\t\tint val = *valp;\n\t\tunsigned long lval;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\tlval = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\tlval = (unsigned long)val;\n\t\t}\n\t\t*lvalp = jiffies_to_msecs(lval);\n\t}\n\treturn 0;\n}\n\n/**\n * proc_dointvec_jiffies - read a vector of integers as seconds\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n * The values read are assumed to be in seconds, and are converted into\n * jiffies.\n *\n * Returns 0 on success.\n */\nint proc_dointvec_jiffies(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\n\t\t    \t    do_proc_dointvec_jiffies_conv,NULL);\n}\n\n/**\n * proc_dointvec_userhz_jiffies - read a vector of integers as 1/USER_HZ seconds\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: pointer to the file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n * The values read are assumed to be in 1/USER_HZ seconds, and \n * are converted into jiffies.\n *\n * Returns 0 on success.\n */\nint proc_dointvec_userhz_jiffies(struct ctl_table *table, int write,\n\t\t\t\t void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\n\t\t    \t    do_proc_dointvec_userhz_jiffies_conv,NULL);\n}\n\n/**\n * proc_dointvec_ms_jiffies - read a vector of integers as 1 milliseconds\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n * @ppos: the current position in the file\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n * The values read are assumed to be in 1/1000 seconds, and \n * are converted into jiffies.\n *\n * Returns 0 on success.\n */\nint proc_dointvec_ms_jiffies(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\n\t\t\t\tdo_proc_dointvec_ms_jiffies_conv, NULL);\n}\n\nstatic int proc_do_cad_pid(struct ctl_table *table, int write,\n\t\t\t   void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct pid *new_pid;\n\tpid_t tmp;\n\tint r;\n\n\ttmp = pid_vnr(cad_pid);\n\n\tr = __do_proc_dointvec(&tmp, table, write, buffer,\n\t\t\t       lenp, ppos, NULL, NULL);\n\tif (r || !write)\n\t\treturn r;\n\n\tnew_pid = find_get_pid(tmp);\n\tif (!new_pid)\n\t\treturn -ESRCH;\n\n\tput_pid(xchg(&cad_pid, new_pid));\n\treturn 0;\n}\n\n/**\n * proc_do_large_bitmap - read/write from/to a large bitmap\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * The bitmap is stored at table->data and the bitmap length (in bits)\n * in table->maxlen.\n *\n * We use a range comma separated format (e.g. 1,3-4,10-10) so that\n * large bitmaps may be represented in a compact manner. Writing into\n * the file will clear the bitmap then update it with the given input.\n *\n * Returns 0 on success.\n */\nint proc_do_large_bitmap(struct ctl_table *table, int write,\n\t\t\t void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint err = 0;\n\tbool first = 1;\n\tsize_t left = *lenp;\n\tunsigned long bitmap_len = table->maxlen;\n\tunsigned long *bitmap = *(unsigned long **) table->data;\n\tunsigned long *tmp_bitmap = NULL;\n\tchar tr_a[] = { '-', ',', '\\n' }, tr_b[] = { ',', '\\n', 0 }, c;\n\n\tif (!bitmap || !bitmap_len || !left || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\tif (write) {\n\t\tchar *kbuf, *p;\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\n\t\tp = kbuf = memdup_user_nul(buffer, left);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\n\t\ttmp_bitmap = kzalloc(BITS_TO_LONGS(bitmap_len) * sizeof(unsigned long),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (!tmp_bitmap) {\n\t\t\tkfree(kbuf);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tproc_skip_char(&p, &left, '\\n');\n\t\twhile (!err && left) {\n\t\t\tunsigned long val_a, val_b;\n\t\t\tbool neg;\n\n\t\t\terr = proc_get_long(&p, &left, &val_a, &neg, tr_a,\n\t\t\t\t\t     sizeof(tr_a), &c);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (val_a >= bitmap_len || neg) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tval_b = val_a;\n\t\t\tif (left) {\n\t\t\t\tp++;\n\t\t\t\tleft--;\n\t\t\t}\n\n\t\t\tif (c == '-') {\n\t\t\t\terr = proc_get_long(&p, &left, &val_b,\n\t\t\t\t\t\t     &neg, tr_b, sizeof(tr_b),\n\t\t\t\t\t\t     &c);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t\tif (val_b >= bitmap_len || neg ||\n\t\t\t\t    val_a > val_b) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (left) {\n\t\t\t\t\tp++;\n\t\t\t\t\tleft--;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbitmap_set(tmp_bitmap, val_a, val_b - val_a + 1);\n\t\t\tfirst = 0;\n\t\t\tproc_skip_char(&p, &left, '\\n');\n\t\t}\n\t\tkfree(kbuf);\n\t} else {\n\t\tunsigned long bit_a, bit_b = 0;\n\n\t\twhile (left) {\n\t\t\tbit_a = find_next_bit(bitmap, bitmap_len, bit_b);\n\t\t\tif (bit_a >= bitmap_len)\n\t\t\t\tbreak;\n\t\t\tbit_b = find_next_zero_bit(bitmap, bitmap_len,\n\t\t\t\t\t\t   bit_a + 1) - 1;\n\n\t\t\tif (!first) {\n\t\t\t\terr = proc_put_char(&buffer, &left, ',');\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = proc_put_long(&buffer, &left, bit_a, false);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (bit_a != bit_b) {\n\t\t\t\terr = proc_put_char(&buffer, &left, '-');\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t\terr = proc_put_long(&buffer, &left, bit_b, false);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfirst = 0; bit_b++;\n\t\t}\n\t\tif (!err)\n\t\t\terr = proc_put_char(&buffer, &left, '\\n');\n\t}\n\n\tif (!err) {\n\t\tif (write) {\n\t\t\tif (*ppos)\n\t\t\t\tbitmap_or(bitmap, bitmap, tmp_bitmap, bitmap_len);\n\t\t\telse\n\t\t\t\tbitmap_copy(bitmap, tmp_bitmap, bitmap_len);\n\t\t}\n\t\tkfree(tmp_bitmap);\n\t\t*lenp -= left;\n\t\t*ppos += *lenp;\n\t\treturn 0;\n\t} else {\n\t\tkfree(tmp_bitmap);\n\t\treturn err;\n\t}\n}\n\n#else /* CONFIG_PROC_SYSCTL */\n\nint proc_dostring(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_minmax(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_jiffies(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_userhz_jiffies(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_ms_jiffies(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int write,\n\t\t\t\t      void __user *buffer,\n\t\t\t\t      size_t *lenp, loff_t *ppos)\n{\n    return -ENOSYS;\n}\n\n\n#endif /* CONFIG_PROC_SYSCTL */\n\n/*\n * No sense putting this after each symbol definition, twice,\n * exception granted :-)\n */\nEXPORT_SYMBOL(proc_dointvec);\nEXPORT_SYMBOL(proc_dointvec_jiffies);\nEXPORT_SYMBOL(proc_dointvec_minmax);\nEXPORT_SYMBOL(proc_dointvec_userhz_jiffies);\nEXPORT_SYMBOL(proc_dointvec_ms_jiffies);\nEXPORT_SYMBOL(proc_dostring);\nEXPORT_SYMBOL(proc_doulongvec_minmax);\nEXPORT_SYMBOL(proc_doulongvec_ms_jiffies_minmax);\n"], "fixing_code": ["Documentation for /proc/sys/fs/*\tkernel version 2.2.10\n\t(c) 1998, 1999,  Rik van Riel <riel@nl.linux.org>\n\t(c) 2009,        Shen Feng<shen@cn.fujitsu.com>\n\nFor general info and legal blurb, please look in README.\n\n==============================================================\n\nThis file contains documentation for the sysctl files in\n/proc/sys/fs/ and is valid for Linux kernel version 2.2.\n\nThe files in this directory can be used to tune and monitor\nmiscellaneous and general things in the operation of the Linux\nkernel. Since some of the files _can_ be used to screw up your\nsystem, it is advisable to read both documentation and source\nbefore actually making adjustments.\n\n1. /proc/sys/fs\n----------------------------------------------------------\n\nCurrently, these files are in /proc/sys/fs:\n- aio-max-nr\n- aio-nr\n- dentry-state\n- dquot-max\n- dquot-nr\n- file-max\n- file-nr\n- inode-max\n- inode-nr\n- inode-state\n- nr_open\n- overflowuid\n- overflowgid\n- pipe-user-pages-hard\n- pipe-user-pages-soft\n- protected_hardlinks\n- protected_symlinks\n- suid_dumpable\n- super-max\n- super-nr\n\n==============================================================\n\naio-nr & aio-max-nr:\n\naio-nr is the running total of the number of events specified on the\nio_setup system call for all currently active aio contexts.  If aio-nr\nreaches aio-max-nr then io_setup will fail with EAGAIN.  Note that\nraising aio-max-nr does not result in the pre-allocation or re-sizing\nof any kernel data structures.\n\n==============================================================\n\ndentry-state:\n\nFrom linux/fs/dentry.c:\n--------------------------------------------------------------\nstruct {\n        int nr_dentry;\n        int nr_unused;\n        int age_limit;         /* age in seconds */\n        int want_pages;        /* pages requested by system */\n        int dummy[2];\n} dentry_stat = {0, 0, 45, 0,};\n-------------------------------------------------------------- \n\nDentries are dynamically allocated and deallocated, and\nnr_dentry seems to be 0 all the time. Hence it's safe to\nassume that only nr_unused, age_limit and want_pages are\nused. Nr_unused seems to be exactly what its name says.\nAge_limit is the age in seconds after which dcache entries\ncan be reclaimed when memory is short and want_pages is\nnonzero when shrink_dcache_pages() has been called and the\ndcache isn't pruned yet.\n\n==============================================================\n\ndquot-max & dquot-nr:\n\nThe file dquot-max shows the maximum number of cached disk\nquota entries.\n\nThe file dquot-nr shows the number of allocated disk quota\nentries and the number of free disk quota entries.\n\nIf the number of free cached disk quotas is very low and\nyou have some awesome number of simultaneous system users,\nyou might want to raise the limit.\n\n==============================================================\n\nfile-max & file-nr:\n\nThe value in file-max denotes the maximum number of file-\nhandles that the Linux kernel will allocate. When you get lots\nof error messages about running out of file handles, you might\nwant to increase this limit.\n\nHistorically,the kernel was able to allocate file handles\ndynamically, but not to free them again. The three values in\nfile-nr denote the number of allocated file handles, the number\nof allocated but unused file handles, and the maximum number of\nfile handles. Linux 2.6 always reports 0 as the number of free\nfile handles -- this is not an error, it just means that the\nnumber of allocated file handles exactly matches the number of\nused file handles.\n\nAttempts to allocate more file descriptors than file-max are\nreported with printk, look for \"VFS: file-max limit <number>\nreached\".\n==============================================================\n\nnr_open:\n\nThis denotes the maximum number of file-handles a process can\nallocate. Default value is 1024*1024 (1048576) which should be\nenough for most machines. Actual limit depends on RLIMIT_NOFILE\nresource limit.\n\n==============================================================\n\ninode-max, inode-nr & inode-state:\n\nAs with file handles, the kernel allocates the inode structures\ndynamically, but can't free them yet.\n\nThe value in inode-max denotes the maximum number of inode\nhandlers. This value should be 3-4 times larger than the value\nin file-max, since stdin, stdout and network sockets also\nneed an inode struct to handle them. When you regularly run\nout of inodes, you need to increase this value.\n\nThe file inode-nr contains the first two items from\ninode-state, so we'll skip to that file...\n\nInode-state contains three actual numbers and four dummies.\nThe actual numbers are, in order of appearance, nr_inodes,\nnr_free_inodes and preshrink.\n\nNr_inodes stands for the number of inodes the system has\nallocated, this can be slightly more than inode-max because\nLinux allocates them one pageful at a time.\n\nNr_free_inodes represents the number of free inodes (?) and\npreshrink is nonzero when the nr_inodes > inode-max and the\nsystem needs to prune the inode list instead of allocating\nmore.\n\n==============================================================\n\noverflowgid & overflowuid:\n\nSome filesystems only support 16-bit UIDs and GIDs, although in Linux\nUIDs and GIDs are 32 bits. When one of these filesystems is mounted\nwith writes enabled, any UID or GID that would exceed 65535 is translated\nto a fixed value before being written to disk.\n\nThese sysctls allow you to change the value of the fixed UID and GID.\nThe default is 65534.\n\n==============================================================\n\npipe-user-pages-hard:\n\nMaximum total number of pages a non-privileged user may allocate for pipes.\nOnce this limit is reached, no new pipes may be allocated until usage goes\nbelow the limit again. When set to 0, no limit is applied, which is the default\nsetting.\n\n==============================================================\n\npipe-user-pages-soft:\n\nMaximum total number of pages a non-privileged user may allocate for pipes\nbefore the pipe size gets limited to a single page. Once this limit is reached,\nnew pipes will be limited to a single page in size for this user in order to\nlimit total memory usage, and trying to increase them using fcntl() will be\ndenied until usage goes below the limit again. The default value allows to\nallocate up to 1024 pipes at their default size. When set to 0, no limit is\napplied.\n\n==============================================================\n\nprotected_hardlinks:\n\nA long-standing class of security issues is the hardlink-based\ntime-of-check-time-of-use race, most commonly seen in world-writable\ndirectories like /tmp. The common method of exploitation of this flaw\nis to cross privilege boundaries when following a given hardlink (i.e. a\nroot process follows a hardlink created by another user). Additionally,\non systems without separated partitions, this stops unauthorized users\nfrom \"pinning\" vulnerable setuid/setgid files against being upgraded by\nthe administrator, or linking to special files.\n\nWhen set to \"0\", hardlink creation behavior is unrestricted.\n\nWhen set to \"1\" hardlinks cannot be created by users if they do not\nalready own the source file, or do not have read/write access to it.\n\nThis protection is based on the restrictions in Openwall and grsecurity.\n\n==============================================================\n\nprotected_symlinks:\n\nA long-standing class of security issues is the symlink-based\ntime-of-check-time-of-use race, most commonly seen in world-writable\ndirectories like /tmp. The common method of exploitation of this flaw\nis to cross privilege boundaries when following a given symlink (i.e. a\nroot process follows a symlink belonging to another user). For a likely\nincomplete list of hundreds of examples across the years, please see:\nhttp://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=/tmp\n\nWhen set to \"0\", symlink following behavior is unrestricted.\n\nWhen set to \"1\" symlinks are permitted to be followed only when outside\na sticky world-writable directory, or when the uid of the symlink and\nfollower match, or when the directory owner matches the symlink's owner.\n\nThis protection is based on the restrictions in Openwall and grsecurity.\n\n==============================================================\n\nsuid_dumpable:\n\nThis value can be used to query and set the core dump mode for setuid\nor otherwise protected/tainted binaries. The modes are\n\n0 - (default) - traditional behaviour. Any process which has changed\n\tprivilege levels or is execute only will not be dumped.\n1 - (debug) - all processes dump core when possible. The core dump is\n\towned by the current user and no security is applied. This is\n\tintended for system debugging situations only. Ptrace is unchecked.\n\tThis is insecure as it allows regular users to examine the memory\n\tcontents of privileged processes.\n2 - (suidsafe) - any binary which normally would not be dumped is dumped\n\tanyway, but only if the \"core_pattern\" kernel sysctl is set to\n\teither a pipe handler or a fully qualified path. (For more details\n\ton this limitation, see CVE-2006-2451.) This mode is appropriate\n\twhen administrators are attempting to debug problems in a normal\n\tenvironment, and either have a core dump pipe handler that knows\n\tto treat privileged core dumps with care, or specific directory\n\tdefined for catching core dumps. If a core dump happens without\n\ta pipe handler or fully qualifid path, a message will be emitted\n\tto syslog warning about the lack of a correct setting.\n\n==============================================================\n\nsuper-max & super-nr:\n\nThese numbers control the maximum number of superblocks, and\nthus the maximum number of mounted filesystems the kernel\ncan have. You only need to increase super-max if you need to\nmount more filesystems than the current value in super-max\nallows you to.\n\n==============================================================\n\naio-nr & aio-max-nr:\n\naio-nr shows the current system-wide number of asynchronous io\nrequests.  aio-max-nr allows you to change the maximum value\naio-nr can grow to.\n\n==============================================================\n\nmount-max:\n\nThis denotes the maximum number of mounts that may exist\nin a mount namespace.\n\n==============================================================\n\n\n2. /proc/sys/fs/binfmt_misc\n----------------------------------------------------------\n\nDocumentation for the files in /proc/sys/fs/binfmt_misc is\nin Documentation/binfmt_misc.txt.\n\n\n3. /proc/sys/fs/mqueue - POSIX message queues filesystem\n----------------------------------------------------------\n\nThe \"mqueue\"  filesystem provides  the necessary kernel features to enable the\ncreation of a  user space  library that  implements  the  POSIX message queues\nAPI (as noted by the  MSG tag in the  POSIX 1003.1-2001 version  of the System\nInterfaces specification.)\n\nThe \"mqueue\" filesystem contains values for determining/setting  the amount of\nresources used by the file system.\n\n/proc/sys/fs/mqueue/queues_max is a read/write  file for  setting/getting  the\nmaximum number of message queues allowed on the system.\n\n/proc/sys/fs/mqueue/msg_max  is  a  read/write file  for  setting/getting  the\nmaximum number of messages in a queue value.  In fact it is the limiting value\nfor another (user) limit which is set in mq_open invocation. This attribute of\na queue must be less or equal then msg_max.\n\n/proc/sys/fs/mqueue/msgsize_max is  a read/write  file for setting/getting the\nmaximum  message size value (it is every  message queue's attribute set during\nits creation).\n\n/proc/sys/fs/mqueue/msg_default is  a read/write  file for setting/getting the\ndefault number of messages in a queue value if attr parameter of mq_open(2) is\nNULL. If it exceed msg_max, the default value is initialized msg_max.\n\n/proc/sys/fs/mqueue/msgsize_default is a read/write file for setting/getting\nthe default message size value if attr parameter of mq_open(2) is NULL. If it\nexceed msgsize_max, the default value is initialized msgsize_max.\n\n4. /proc/sys/fs/epoll - Configuration options for the epoll interface\n--------------------------------------------------------\n\nThis directory contains configuration options for the epoll(7) interface.\n\nmax_user_watches\n----------------\n\nEvery epoll file descriptor can store a number of files to be monitored\nfor event readiness. Each one of these monitored files constitutes a \"watch\".\nThis configuration option sets the maximum number of \"watches\" that are\nallowed for each user.\nEach \"watch\" costs roughly 90 bytes on a 32bit kernel, and roughly 160 bytes\non a 64bit one.\nThe current default value for  max_user_watches  is the 1/32 of the available\nlow memory, divided for the \"watch\" cost in bytes.\n\n", "#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/ns_common.h>\n#include <linux/fs_pin.h>\n\nstruct mnt_namespace {\n\tatomic_t\t\tcount;\n\tstruct ns_common\tns;\n\tstruct mount *\troot;\n\tstruct list_head\tlist;\n\tstruct user_namespace\t*user_ns;\n\tstruct ucounts\t\t*ucounts;\n\tu64\t\t\tseq;\t/* Sequence number to prevent loops */\n\twait_queue_head_t poll;\n\tu64 event;\n\tunsigned int\t\tmounts; /* # of mounts in the namespace */\n\tunsigned int\t\tpending_mounts;\n};\n\nstruct mnt_pcp {\n\tint mnt_count;\n\tint mnt_writers;\n};\n\nstruct mountpoint {\n\tstruct hlist_node m_hash;\n\tstruct dentry *m_dentry;\n\tstruct hlist_head m_list;\n\tint m_count;\n};\n\nstruct mount {\n\tstruct hlist_node mnt_hash;\n\tstruct mount *mnt_parent;\n\tstruct dentry *mnt_mountpoint;\n\tstruct vfsmount mnt;\n\tunion {\n\t\tstruct rcu_head mnt_rcu;\n\t\tstruct llist_node mnt_llist;\n\t};\n#ifdef CONFIG_SMP\n\tstruct mnt_pcp __percpu *mnt_pcp;\n#else\n\tint mnt_count;\n\tint mnt_writers;\n#endif\n\tstruct list_head mnt_mounts;\t/* list of children, anchored here */\n\tstruct list_head mnt_child;\t/* and going through their mnt_child */\n\tstruct list_head mnt_instance;\t/* mount instance on sb->s_mounts */\n\tconst char *mnt_devname;\t/* Name of device e.g. /dev/dsk/hda1 */\n\tstruct list_head mnt_list;\n\tstruct list_head mnt_expire;\t/* link in fs-specific expiry list */\n\tstruct list_head mnt_share;\t/* circular list of shared mounts */\n\tstruct list_head mnt_slave_list;/* list of slave mounts */\n\tstruct list_head mnt_slave;\t/* slave list entry */\n\tstruct mount *mnt_master;\t/* slave is on master->mnt_slave_list */\n\tstruct mnt_namespace *mnt_ns;\t/* containing namespace */\n\tstruct mountpoint *mnt_mp;\t/* where is it mounted */\n\tstruct hlist_node mnt_mp_list;\t/* list mounts with the same mountpoint */\n#ifdef CONFIG_FSNOTIFY\n\tstruct hlist_head mnt_fsnotify_marks;\n\t__u32 mnt_fsnotify_mask;\n#endif\n\tint mnt_id;\t\t\t/* mount identifier */\n\tint mnt_group_id;\t\t/* peer group identifier */\n\tint mnt_expiry_mark;\t\t/* true if marked for expiry */\n\tstruct hlist_head mnt_pins;\n\tstruct fs_pin mnt_umount;\n\tstruct dentry *mnt_ex_mountpoint;\n};\n\n#define MNT_NS_INTERNAL ERR_PTR(-EINVAL) /* distinct from any mnt_namespace */\n\nstatic inline struct mount *real_mount(struct vfsmount *mnt)\n{\n\treturn container_of(mnt, struct mount, mnt);\n}\n\nstatic inline int mnt_has_parent(struct mount *mnt)\n{\n\treturn mnt != mnt->mnt_parent;\n}\n\nstatic inline int is_mounted(struct vfsmount *mnt)\n{\n\t/* neither detached nor internal? */\n\treturn !IS_ERR_OR_NULL(real_mount(mnt)->mnt_ns);\n}\n\nextern struct mount *__lookup_mnt(struct vfsmount *, struct dentry *);\nextern struct mount *__lookup_mnt_last(struct vfsmount *, struct dentry *);\n\nextern int __legitimize_mnt(struct vfsmount *, unsigned);\nextern bool legitimize_mnt(struct vfsmount *, unsigned);\n\nextern void __detach_mounts(struct dentry *dentry);\n\nstatic inline void detach_mounts(struct dentry *dentry)\n{\n\tif (!d_mountpoint(dentry))\n\t\treturn;\n\t__detach_mounts(dentry);\n}\n\nstatic inline void get_mnt_ns(struct mnt_namespace *ns)\n{\n\tatomic_inc(&ns->count);\n}\n\nextern seqlock_t mount_lock;\n\nstatic inline void lock_mount_hash(void)\n{\n\twrite_seqlock(&mount_lock);\n}\n\nstatic inline void unlock_mount_hash(void)\n{\n\twrite_sequnlock(&mount_lock);\n}\n\nstruct proc_mounts {\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\tint (*show)(struct seq_file *, struct vfsmount *);\n\tvoid *cached_mount;\n\tu64 cached_event;\n\tloff_t cached_index;\n};\n\nextern const struct seq_operations mounts_op;\n\nextern bool __is_local_mountpoint(struct dentry *dentry);\nstatic inline bool is_local_mountpoint(struct dentry *dentry)\n{\n\tif (!d_mountpoint(dentry))\n\t\treturn false;\n\n\treturn __is_local_mountpoint(dentry);\n}\n", "/*\n *  linux/fs/namespace.c\n *\n * (C) Copyright Al Viro 2000, 2001\n *\tReleased under GPL v2.\n *\n * Based on code from fs/super.c, copyright Linus Torvalds and others.\n * Heavily rewritten.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/capability.h>\n#include <linux/mnt_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/idr.h>\n#include <linux/init.h>\t\t/* init_rootfs */\n#include <linux/fs_struct.h>\t/* get_fs_root et.al. */\n#include <linux/fsnotify.h>\t/* fsnotify_vfsmount_delete */\n#include <linux/uaccess.h>\n#include <linux/proc_ns.h>\n#include <linux/magic.h>\n#include <linux/bootmem.h>\n#include <linux/task_work.h>\n#include \"pnode.h\"\n#include \"internal.h\"\n\n/* Maximum number of mounts in a mount namespace */\nunsigned int sysctl_mount_max __read_mostly = 100000;\n\nstatic unsigned int m_hash_mask __read_mostly;\nstatic unsigned int m_hash_shift __read_mostly;\nstatic unsigned int mp_hash_mask __read_mostly;\nstatic unsigned int mp_hash_shift __read_mostly;\n\nstatic __initdata unsigned long mhash_entries;\nstatic int __init set_mhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmhash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mhash_entries=\", set_mhash_entries);\n\nstatic __initdata unsigned long mphash_entries;\nstatic int __init set_mphash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmphash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mphash_entries=\", set_mphash_entries);\n\nstatic u64 event;\nstatic DEFINE_IDA(mnt_id_ida);\nstatic DEFINE_IDA(mnt_group_ida);\nstatic DEFINE_SPINLOCK(mnt_id_lock);\nstatic int mnt_id_start = 0;\nstatic int mnt_group_start = 1;\n\nstatic struct hlist_head *mount_hashtable __read_mostly;\nstatic struct hlist_head *mountpoint_hashtable __read_mostly;\nstatic struct kmem_cache *mnt_cache __read_mostly;\nstatic DECLARE_RWSEM(namespace_sem);\n\n/* /sys/fs */\nstruct kobject *fs_kobj;\nEXPORT_SYMBOL_GPL(fs_kobj);\n\n/*\n * vfsmount lock may be taken for read to prevent changes to the\n * vfsmount hash, ie. during mountpoint lookups or walking back\n * up the tree.\n *\n * It should be taken for write in all cases where the vfsmount\n * tree or hash is modified or when a vfsmount structure is modified.\n */\n__cacheline_aligned_in_smp DEFINE_SEQLOCK(mount_lock);\n\nstatic inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);\n\ttmp += ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> m_hash_shift);\n\treturn &mount_hashtable[tmp & m_hash_mask];\n}\n\nstatic inline struct hlist_head *mp_hash(struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> mp_hash_shift);\n\treturn &mountpoint_hashtable[tmp & mp_hash_mask];\n}\n\n/*\n * allocation is serialized by namespace_sem, but we need the spinlock to\n * serialize with freeing.\n */\nstatic int mnt_alloc_id(struct mount *mnt)\n{\n\tint res;\n\nretry:\n\tida_pre_get(&mnt_id_ida, GFP_KERNEL);\n\tspin_lock(&mnt_id_lock);\n\tres = ida_get_new_above(&mnt_id_ida, mnt_id_start, &mnt->mnt_id);\n\tif (!res)\n\t\tmnt_id_start = mnt->mnt_id + 1;\n\tspin_unlock(&mnt_id_lock);\n\tif (res == -EAGAIN)\n\t\tgoto retry;\n\n\treturn res;\n}\n\nstatic void mnt_free_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_id;\n\tspin_lock(&mnt_id_lock);\n\tida_remove(&mnt_id_ida, id);\n\tif (mnt_id_start > id)\n\t\tmnt_id_start = id;\n\tspin_unlock(&mnt_id_lock);\n}\n\n/*\n * Allocate a new peer group ID\n *\n * mnt_group_ida is protected by namespace_sem\n */\nstatic int mnt_alloc_group_id(struct mount *mnt)\n{\n\tint res;\n\n\tif (!ida_pre_get(&mnt_group_ida, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tres = ida_get_new_above(&mnt_group_ida,\n\t\t\t\tmnt_group_start,\n\t\t\t\t&mnt->mnt_group_id);\n\tif (!res)\n\t\tmnt_group_start = mnt->mnt_group_id + 1;\n\n\treturn res;\n}\n\n/*\n * Release a peer group ID\n */\nvoid mnt_release_group_id(struct mount *mnt)\n{\n\tint id = mnt->mnt_group_id;\n\tida_remove(&mnt_group_ida, id);\n\tif (mnt_group_start > id)\n\t\tmnt_group_start = id;\n\tmnt->mnt_group_id = 0;\n}\n\n/*\n * vfsmount lock must be held for read\n */\nstatic inline void mnt_add_count(struct mount *mnt, int n)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_add(mnt->mnt_pcp->mnt_count, n);\n#else\n\tpreempt_disable();\n\tmnt->mnt_count += n;\n\tpreempt_enable();\n#endif\n}\n\n/*\n * vfsmount lock must be held for write\n */\nunsigned int mnt_get_count(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_count;\n#endif\n}\n\nstatic void drop_mountpoint(struct fs_pin *p)\n{\n\tstruct mount *m = container_of(p, struct mount, mnt_umount);\n\tdput(m->mnt_ex_mountpoint);\n\tpin_remove(p);\n\tmntput(&m->mnt);\n}\n\nstatic struct mount *alloc_vfsmnt(const char *name)\n{\n\tstruct mount *mnt = kmem_cache_zalloc(mnt_cache, GFP_KERNEL);\n\tif (mnt) {\n\t\tint err;\n\n\t\terr = mnt_alloc_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free_cache;\n\n\t\tif (name) {\n\t\t\tmnt->mnt_devname = kstrdup_const(name, GFP_KERNEL);\n\t\t\tif (!mnt->mnt_devname)\n\t\t\t\tgoto out_free_id;\n\t\t}\n\n#ifdef CONFIG_SMP\n\t\tmnt->mnt_pcp = alloc_percpu(struct mnt_pcp);\n\t\tif (!mnt->mnt_pcp)\n\t\t\tgoto out_free_devname;\n\n\t\tthis_cpu_add(mnt->mnt_pcp->mnt_count, 1);\n#else\n\t\tmnt->mnt_count = 1;\n\t\tmnt->mnt_writers = 0;\n#endif\n\n\t\tINIT_HLIST_NODE(&mnt->mnt_hash);\n\t\tINIT_LIST_HEAD(&mnt->mnt_child);\n\t\tINIT_LIST_HEAD(&mnt->mnt_mounts);\n\t\tINIT_LIST_HEAD(&mnt->mnt_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_expire);\n\t\tINIT_LIST_HEAD(&mnt->mnt_share);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave);\n\t\tINIT_HLIST_NODE(&mnt->mnt_mp_list);\n#ifdef CONFIG_FSNOTIFY\n\t\tINIT_HLIST_HEAD(&mnt->mnt_fsnotify_marks);\n#endif\n\t\tinit_fs_pin(&mnt->mnt_umount, drop_mountpoint);\n\t}\n\treturn mnt;\n\n#ifdef CONFIG_SMP\nout_free_devname:\n\tkfree_const(mnt->mnt_devname);\n#endif\nout_free_id:\n\tmnt_free_id(mnt);\nout_free_cache:\n\tkmem_cache_free(mnt_cache, mnt);\n\treturn NULL;\n}\n\n/*\n * Most r/o checks on a fs are for operations that take\n * discrete amounts of time, like a write() or unlink().\n * We must keep track of when those operations start\n * (for permission checks) and when they end, so that\n * we can determine when writes are able to occur to\n * a filesystem.\n */\n/*\n * __mnt_is_readonly: check whether a mount is read-only\n * @mnt: the mount to check for its write status\n *\n * This shouldn't be used directly ouside of the VFS.\n * It does not guarantee that the filesystem will stay\n * r/w, just that it is right *now*.  This can not and\n * should not be used in place of IS_RDONLY(inode).\n * mnt_want/drop_write() will _keep_ the filesystem\n * r/w.\n */\nint __mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_flags & MNT_READONLY)\n\t\treturn 1;\n\tif (mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__mnt_is_readonly);\n\nstatic inline void mnt_inc_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_inc(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers++;\n#endif\n}\n\nstatic inline void mnt_dec_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_dec(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers--;\n#endif\n}\n\nstatic unsigned int mnt_get_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_writers;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_writers;\n#endif\n}\n\nstatic int mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_sb->s_readonly_remount)\n\t\treturn 1;\n\t/* Order wrt setting s_flags/s_readonly_remount in do_remount() */\n\tsmp_rmb();\n\treturn __mnt_is_readonly(mnt);\n}\n\n/*\n * Most r/o & frozen checks on a fs are for operations that take discrete\n * amounts of time, like a write() or unlink().  We must keep track of when\n * those operations start (for permission checks) and when they end, so that we\n * can determine when writes are able to occur to a filesystem.\n */\n/**\n * __mnt_want_write - get write access to a mount without freeze protection\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mnt it read-write) before\n * returning success. This operation does not protect against filesystem being\n * frozen. When the write operation is finished, __mnt_drop_write() must be\n * called. This is effectively a refcount.\n */\nint __mnt_want_write(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint ret = 0;\n\n\tpreempt_disable();\n\tmnt_inc_writers(mnt);\n\t/*\n\t * The store to mnt_inc_writers must be visible before we pass\n\t * MNT_WRITE_HOLD loop below, so that the slowpath can see our\n\t * incremented count after it has set MNT_WRITE_HOLD.\n\t */\n\tsmp_mb();\n\twhile (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)\n\t\tcpu_relax();\n\t/*\n\t * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will\n\t * be set to match its requirements. So we must not load that until\n\t * MNT_WRITE_HOLD is cleared.\n\t */\n\tsmp_rmb();\n\tif (mnt_is_readonly(m)) {\n\t\tmnt_dec_writers(mnt);\n\t\tret = -EROFS;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * mnt_want_write - get write access to a mount\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mount is read-write, filesystem\n * is not frozen) before returning success.  When the write operation is\n * finished, mnt_drop_write() must be called.  This is effectively a refcount.\n */\nint mnt_want_write(struct vfsmount *m)\n{\n\tint ret;\n\n\tsb_start_write(m->mnt_sb);\n\tret = __mnt_want_write(m);\n\tif (ret)\n\t\tsb_end_write(m->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write);\n\n/**\n * mnt_clone_write - get write access to a mount\n * @mnt: the mount on which to take a write\n *\n * This is effectively like mnt_want_write, except\n * it must only be used to take an extra write reference\n * on a mountpoint that we already know has a write reference\n * on it. This allows some optimisation.\n *\n * After finished, mnt_drop_write must be called as usual to\n * drop the reference.\n */\nint mnt_clone_write(struct vfsmount *mnt)\n{\n\t/* superblock may be r/o */\n\tif (__mnt_is_readonly(mnt))\n\t\treturn -EROFS;\n\tpreempt_disable();\n\tmnt_inc_writers(real_mount(mnt));\n\tpreempt_enable();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mnt_clone_write);\n\n/**\n * __mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like __mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint __mnt_want_write_file(struct file *file)\n{\n\tif (!(file->f_mode & FMODE_WRITER))\n\t\treturn __mnt_want_write(file->f_path.mnt);\n\telse\n\t\treturn mnt_clone_write(file->f_path.mnt);\n}\n\n/**\n * mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like mnt_want_write, but it takes a file and can\n * do some optimisations if the file is open for write already\n */\nint mnt_want_write_file(struct file *file)\n{\n\tint ret;\n\n\tsb_start_write(file->f_path.mnt->mnt_sb);\n\tret = __mnt_want_write_file(file);\n\tif (ret)\n\t\tsb_end_write(file->f_path.mnt->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write_file);\n\n/**\n * __mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done\n * performing writes to it.  Must be matched with\n * __mnt_want_write() call above.\n */\nvoid __mnt_drop_write(struct vfsmount *mnt)\n{\n\tpreempt_disable();\n\tmnt_dec_writers(real_mount(mnt));\n\tpreempt_enable();\n}\n\n/**\n * mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done performing writes to it and\n * also allows filesystem to be frozen again.  Must be matched with\n * mnt_want_write() call above.\n */\nvoid mnt_drop_write(struct vfsmount *mnt)\n{\n\t__mnt_drop_write(mnt);\n\tsb_end_write(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(mnt_drop_write);\n\nvoid __mnt_drop_write_file(struct file *file)\n{\n\t__mnt_drop_write(file->f_path.mnt);\n}\n\nvoid mnt_drop_write_file(struct file *file)\n{\n\tmnt_drop_write(file->f_path.mnt);\n}\nEXPORT_SYMBOL(mnt_drop_write_file);\n\nstatic int mnt_make_readonly(struct mount *mnt)\n{\n\tint ret = 0;\n\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t/*\n\t * After storing MNT_WRITE_HOLD, we'll read the counters. This store\n\t * should be visible before we do.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * With writers on hold, if this value is zero, then there are\n\t * definitely no active writers (although held writers may subsequently\n\t * increment the count, they'll have to wait, and decrement it after\n\t * seeing MNT_READONLY).\n\t *\n\t * It is OK to have counter incremented on one CPU and decremented on\n\t * another: the sum will add up correctly. The danger would be when we\n\t * sum up each counter, if we read a counter before it is incremented,\n\t * but then read another CPU's count which it has been subsequently\n\t * decremented from -- we would see more decrements than we should.\n\t * MNT_WRITE_HOLD protects against this scenario, because\n\t * mnt_want_write first increments count, then smp_mb, then spins on\n\t * MNT_WRITE_HOLD, so it can't be decremented by another CPU while\n\t * we're counting up here.\n\t */\n\tif (mnt_get_writers(mnt) > 0)\n\t\tret = -EBUSY;\n\telse\n\t\tmnt->mnt.mnt_flags |= MNT_READONLY;\n\t/*\n\t * MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers\n\t * that become unheld will see MNT_READONLY.\n\t */\n\tsmp_wmb();\n\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\tunlock_mount_hash();\n\treturn ret;\n}\n\nstatic void __mnt_unmake_readonly(struct mount *mnt)\n{\n\tlock_mount_hash();\n\tmnt->mnt.mnt_flags &= ~MNT_READONLY;\n\tunlock_mount_hash();\n}\n\nint sb_prepare_remount_readonly(struct super_block *sb)\n{\n\tstruct mount *mnt;\n\tint err = 0;\n\n\t/* Racy optimization.  Recheck the counter under MNT_WRITE_HOLD */\n\tif (atomic_long_read(&sb->s_remove_count))\n\t\treturn -EBUSY;\n\n\tlock_mount_hash();\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (!(mnt->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t\t\tsmp_mb();\n\t\t\tif (mnt_get_writers(mnt) > 0) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!err && atomic_long_read(&sb->s_remove_count))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tsb->s_readonly_remount = 1;\n\t\tsmp_wmb();\n\t}\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (mnt->mnt.mnt_flags & MNT_WRITE_HOLD)\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\t}\n\tunlock_mount_hash();\n\n\treturn err;\n}\n\nstatic void free_vfsmnt(struct mount *mnt)\n{\n\tkfree_const(mnt->mnt_devname);\n#ifdef CONFIG_SMP\n\tfree_percpu(mnt->mnt_pcp);\n#endif\n\tkmem_cache_free(mnt_cache, mnt);\n}\n\nstatic void delayed_free_vfsmnt(struct rcu_head *head)\n{\n\tfree_vfsmnt(container_of(head, struct mount, mnt_rcu));\n}\n\n/* call under rcu_read_lock */\nint __legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tstruct mount *mnt;\n\tif (read_seqretry(&mount_lock, seq))\n\t\treturn 1;\n\tif (bastard == NULL)\n\t\treturn 0;\n\tmnt = real_mount(bastard);\n\tmnt_add_count(mnt, 1);\n\tif (likely(!read_seqretry(&mount_lock, seq)))\n\t\treturn 0;\n\tif (bastard->mnt_flags & MNT_SYNC_UMOUNT) {\n\t\tmnt_add_count(mnt, -1);\n\t\treturn 1;\n\t}\n\treturn -1;\n}\n\n/* call under rcu_read_lock */\nbool legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tint res = __legitimize_mnt(bastard, seq);\n\tif (likely(!res))\n\t\treturn true;\n\tif (unlikely(res < 0)) {\n\t\trcu_read_unlock();\n\t\tmntput(bastard);\n\t\trcu_read_lock();\n\t}\n\treturn false;\n}\n\n/*\n * find the first mount at @dentry on vfsmount @mnt.\n * call under rcu_read_lock()\n */\nstruct mount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct hlist_head *head = m_hash(mnt, dentry);\n\tstruct mount *p;\n\n\thlist_for_each_entry_rcu(p, head, mnt_hash)\n\t\tif (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)\n\t\t\treturn p;\n\treturn NULL;\n}\n\n/*\n * find the last mount at @dentry on vfsmount @mnt.\n * mount_lock must be held.\n */\nstruct mount *__lookup_mnt_last(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct mount *p, *res = NULL;\n\tp = __lookup_mnt(mnt, dentry);\n\tif (!p)\n\t\tgoto out;\n\tif (!(p->mnt.mnt_flags & MNT_UMOUNT))\n\t\tres = p;\n\thlist_for_each_entry_continue(p, mnt_hash) {\n\t\tif (&p->mnt_parent->mnt != mnt || p->mnt_mountpoint != dentry)\n\t\t\tbreak;\n\t\tif (!(p->mnt.mnt_flags & MNT_UMOUNT))\n\t\t\tres = p;\n\t}\nout:\n\treturn res;\n}\n\n/*\n * lookup_mnt - Return the first child mount mounted at path\n *\n * \"First\" means first mounted chronologically.  If you create the\n * following mounts:\n *\n * mount /dev/sda1 /mnt\n * mount /dev/sda2 /mnt\n * mount /dev/sda3 /mnt\n *\n * Then lookup_mnt() on the base /mnt dentry in the root mount will\n * return successively the root dentry and vfsmount of /dev/sda1, then\n * /dev/sda2, then /dev/sda3, then NULL.\n *\n * lookup_mnt takes a reference to the found vfsmount.\n */\nstruct vfsmount *lookup_mnt(struct path *path)\n{\n\tstruct mount *child_mnt;\n\tstruct vfsmount *m;\n\tunsigned seq;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tchild_mnt = __lookup_mnt(path->mnt, path->dentry);\n\t\tm = child_mnt ? &child_mnt->mnt : NULL;\n\t} while (!legitimize_mnt(m, seq));\n\trcu_read_unlock();\n\treturn m;\n}\n\n/*\n * __is_local_mountpoint - Test to see if dentry is a mountpoint in the\n *                         current mount namespace.\n *\n * The common case is dentries are not mountpoints at all and that\n * test is handled inline.  For the slow case when we are actually\n * dealing with a mountpoint of some kind, walk through all of the\n * mounts in the current mount namespace and test to see if the dentry\n * is a mountpoint.\n *\n * The mount_hashtable is not usable in the context because we\n * need to identify all mounts that may be in the current mount\n * namespace not just a mount that happens to have some specified\n * parent mount.\n */\nbool __is_local_mountpoint(struct dentry *dentry)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tstruct mount *mnt;\n\tbool is_covered = false;\n\n\tif (!d_mountpoint(dentry))\n\t\tgoto out;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tis_covered = (mnt->mnt_mountpoint == dentry);\n\t\tif (is_covered)\n\t\t\tbreak;\n\t}\n\tup_read(&namespace_sem);\nout:\n\treturn is_covered;\n}\n\nstatic struct mountpoint *lookup_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\n\thlist_for_each_entry(mp, chain, m_hash) {\n\t\tif (mp->m_dentry == dentry) {\n\t\t\t/* might be worth a WARN_ON() */\n\t\t\tif (d_unlinked(dentry))\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t\tmp->m_count++;\n\t\t\treturn mp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct mountpoint *new_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\tint ret;\n\n\tmp = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!mp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = d_set_mounted(dentry);\n\tif (ret) {\n\t\tkfree(mp);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tmp->m_dentry = dentry;\n\tmp->m_count = 1;\n\thlist_add_head(&mp->m_hash, chain);\n\tINIT_HLIST_HEAD(&mp->m_list);\n\treturn mp;\n}\n\nstatic void put_mountpoint(struct mountpoint *mp)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tBUG_ON(!hlist_empty(&mp->m_list));\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}\n\nstatic inline int check_mnt(struct mount *mnt)\n{\n\treturn mnt->mnt_ns == current->nsproxy->mnt_ns;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns) {\n\t\tns->event = ++event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void __touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns && ns->event != event) {\n\t\tns->event = event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void unhash_mnt(struct mount *mnt)\n{\n\tmnt->mnt_parent = mnt;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\thlist_del_init(&mnt->mnt_mp_list);\n\tput_mountpoint(mnt->mnt_mp);\n\tmnt->mnt_mp = NULL;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void detach_mnt(struct mount *mnt, struct path *old_path)\n{\n\told_path->dentry = mnt->mnt_mountpoint;\n\told_path->mnt = &mnt->mnt_parent->mnt;\n\tunhash_mnt(mnt);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void umount_mnt(struct mount *mnt)\n{\n\t/* old mountpoint will be dropped when we can do that */\n\tmnt->mnt_ex_mountpoint = mnt->mnt_mountpoint;\n\tunhash_mnt(mnt);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void attach_mnt(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mountpoint *mp)\n{\n\tmnt_set_mountpoint(parent, mp, mnt);\n\thlist_add_head_rcu(&mnt->mnt_hash, m_hash(&parent->mnt, mp->m_dentry));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n}\n\nstatic void attach_shadowed(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mount *shadows)\n{\n\tif (shadows) {\n\t\thlist_add_behind_rcu(&mnt->mnt_hash, &shadows->mnt_hash);\n\t\tlist_add(&mnt->mnt_child, &shadows->mnt_child);\n\t} else {\n\t\thlist_add_head_rcu(&mnt->mnt_hash,\n\t\t\t\tm_hash(&parent->mnt, mnt->mnt_mountpoint));\n\t\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void commit_tree(struct mount *mnt, struct mount *shadows)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tn->mounts += n->pending_mounts;\n\tn->pending_mounts = 0;\n\n\tattach_shadowed(mnt, parent, shadows);\n\ttouch_mnt_namespace(n);\n}\n\nstatic struct mount *next_mnt(struct mount *p, struct mount *root)\n{\n\tstruct list_head *next = p->mnt_mounts.next;\n\tif (next == &p->mnt_mounts) {\n\t\twhile (1) {\n\t\t\tif (p == root)\n\t\t\t\treturn NULL;\n\t\t\tnext = p->mnt_child.next;\n\t\t\tif (next != &p->mnt_parent->mnt_mounts)\n\t\t\t\tbreak;\n\t\t\tp = p->mnt_parent;\n\t\t}\n\t}\n\treturn list_entry(next, struct mount, mnt_child);\n}\n\nstatic struct mount *skip_mnt_tree(struct mount *p)\n{\n\tstruct list_head *prev = p->mnt_mounts.prev;\n\twhile (prev != &p->mnt_mounts) {\n\t\tp = list_entry(prev, struct mount, mnt_child);\n\t\tprev = p->mnt_mounts.prev;\n\t}\n\treturn p;\n}\n\nstruct vfsmount *\nvfs_kern_mount(struct file_system_type *type, int flags, const char *name, void *data)\n{\n\tstruct mount *mnt;\n\tstruct dentry *root;\n\n\tif (!type)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmnt = alloc_vfsmnt(name);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flags & MS_KERNMOUNT)\n\t\tmnt->mnt.mnt_flags = MNT_INTERNAL;\n\n\troot = mount_fs(type, flags, name, data);\n\tif (IS_ERR(root)) {\n\t\tmnt_free_id(mnt);\n\t\tfree_vfsmnt(mnt);\n\t\treturn ERR_CAST(root);\n\t}\n\n\tmnt->mnt.mnt_root = root;\n\tmnt->mnt.mnt_sb = root->d_sb;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);\n\tunlock_mount_hash();\n\treturn &mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(vfs_kern_mount);\n\nstatic struct mount *clone_mnt(struct mount *old, struct dentry *root,\n\t\t\t\t\tint flag)\n{\n\tstruct super_block *sb = old->mnt.mnt_sb;\n\tstruct mount *mnt;\n\tint err;\n\n\tmnt = alloc_vfsmnt(old->mnt_devname);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))\n\t\tmnt->mnt_group_id = 0; /* not a peer of original */\n\telse\n\t\tmnt->mnt_group_id = old->mnt_group_id;\n\n\tif ((flag & CL_MAKE_SHARED) && !mnt->mnt_group_id) {\n\t\terr = mnt_alloc_group_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tmnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);\n\t/* Don't allow unprivileged users to change mount flags */\n\tif (flag & CL_UNPRIVILEGED) {\n\t\tmnt->mnt.mnt_flags |= MNT_LOCK_ATIME;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_READONLY)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_READONLY;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NODEV)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NODEV;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NOSUID)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NOSUID;\n\n\t\tif (mnt->mnt.mnt_flags & MNT_NOEXEC)\n\t\t\tmnt->mnt.mnt_flags |= MNT_LOCK_NOEXEC;\n\t}\n\n\t/* Don't allow unprivileged users to reveal what is under a mount */\n\tif ((flag & CL_UNPRIVILEGED) &&\n\t    (!(flag & CL_EXPIRE) || list_empty(&old->mnt_expire)))\n\t\tmnt->mnt.mnt_flags |= MNT_LOCKED;\n\n\tatomic_inc(&sb->s_active);\n\tmnt->mnt.mnt_sb = sb;\n\tmnt->mnt.mnt_root = dget(root);\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &sb->s_mounts);\n\tunlock_mount_hash();\n\n\tif ((flag & CL_SLAVE) ||\n\t    ((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {\n\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave_list);\n\t\tmnt->mnt_master = old;\n\t\tCLEAR_MNT_SHARED(mnt);\n\t} else if (!(flag & CL_PRIVATE)) {\n\t\tif ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))\n\t\t\tlist_add(&mnt->mnt_share, &old->mnt_share);\n\t\tif (IS_MNT_SLAVE(old))\n\t\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave);\n\t\tmnt->mnt_master = old->mnt_master;\n\t}\n\tif (flag & CL_MAKE_SHARED)\n\t\tset_mnt_shared(mnt);\n\n\t/* stick the duplicate mount on the same expiry list\n\t * as the original if that was on one */\n\tif (flag & CL_EXPIRE) {\n\t\tif (!list_empty(&old->mnt_expire))\n\t\t\tlist_add(&mnt->mnt_expire, &old->mnt_expire);\n\t}\n\n\treturn mnt;\n\n out_free:\n\tmnt_free_id(mnt);\n\tfree_vfsmnt(mnt);\n\treturn ERR_PTR(err);\n}\n\nstatic void cleanup_mnt(struct mount *mnt)\n{\n\t/*\n\t * This probably indicates that somebody messed\n\t * up a mnt_want/drop_write() pair.  If this\n\t * happens, the filesystem was probably unable\n\t * to make r/w->r/o transitions.\n\t */\n\t/*\n\t * The locking used to deal with mnt_count decrement provides barriers,\n\t * so mnt_get_writers() below is safe.\n\t */\n\tWARN_ON(mnt_get_writers(mnt));\n\tif (unlikely(mnt->mnt_pins.first))\n\t\tmnt_pin_kill(mnt);\n\tfsnotify_vfsmount_delete(&mnt->mnt);\n\tdput(mnt->mnt.mnt_root);\n\tdeactivate_super(mnt->mnt.mnt_sb);\n\tmnt_free_id(mnt);\n\tcall_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);\n}\n\nstatic void __cleanup_mnt(struct rcu_head *head)\n{\n\tcleanup_mnt(container_of(head, struct mount, mnt_rcu));\n}\n\nstatic LLIST_HEAD(delayed_mntput_list);\nstatic void delayed_mntput(struct work_struct *unused)\n{\n\tstruct llist_node *node = llist_del_all(&delayed_mntput_list);\n\tstruct llist_node *next;\n\n\tfor (; node; node = next) {\n\t\tnext = llist_next(node);\n\t\tcleanup_mnt(llist_entry(node, struct mount, mnt_llist));\n\t}\n}\nstatic DECLARE_DELAYED_WORK(delayed_mntput_work, delayed_mntput);\n\nstatic void mntput_no_expire(struct mount *mnt)\n{\n\trcu_read_lock();\n\tmnt_add_count(mnt, -1);\n\tif (likely(mnt->mnt_ns)) { /* shouldn't be the last one */\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\tumount_mnt(p);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}\n\nvoid mntput(struct vfsmount *mnt)\n{\n\tif (mnt) {\n\t\tstruct mount *m = real_mount(mnt);\n\t\t/* avoid cacheline pingpong, hope gcc doesn't get \"smart\" */\n\t\tif (unlikely(m->mnt_expiry_mark))\n\t\t\tm->mnt_expiry_mark = 0;\n\t\tmntput_no_expire(m);\n\t}\n}\nEXPORT_SYMBOL(mntput);\n\nstruct vfsmount *mntget(struct vfsmount *mnt)\n{\n\tif (mnt)\n\t\tmnt_add_count(real_mount(mnt), 1);\n\treturn mnt;\n}\nEXPORT_SYMBOL(mntget);\n\nstruct vfsmount *mnt_clone_internal(struct path *path)\n{\n\tstruct mount *p;\n\tp = clone_mnt(real_mount(path->mnt), path->dentry, CL_PRIVATE);\n\tif (IS_ERR(p))\n\t\treturn ERR_CAST(p);\n\tp->mnt.mnt_flags |= MNT_INTERNAL;\n\treturn &p->mnt;\n}\n\nstatic inline void mangle(struct seq_file *m, const char *s)\n{\n\tseq_escape(m, s, \" \\t\\n\\\\\");\n}\n\n/*\n * Simple .show_options callback for filesystems which don't want to\n * implement more complex mount option showing.\n *\n * See also save_mount_options().\n */\nint generic_show_options(struct seq_file *m, struct dentry *root)\n{\n\tconst char *options;\n\n\trcu_read_lock();\n\toptions = rcu_dereference(root->d_sb->s_options);\n\n\tif (options != NULL && options[0]) {\n\t\tseq_putc(m, ',');\n\t\tmangle(m, options);\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL(generic_show_options);\n\n/*\n * If filesystem uses generic_show_options(), this function should be\n * called from the fill_super() callback.\n *\n * The .remount_fs callback usually needs to be handled in a special\n * way, to make sure, that previous options are not overwritten if the\n * remount fails.\n *\n * Also note, that if the filesystem's .remount_fs function doesn't\n * reset all options to their default value, but changes only newly\n * given options, then the displayed options will not reflect reality\n * any more.\n */\nvoid save_mount_options(struct super_block *sb, char *options)\n{\n\tBUG_ON(sb->s_options);\n\trcu_assign_pointer(sb->s_options, kstrdup(options, GFP_KERNEL));\n}\nEXPORT_SYMBOL(save_mount_options);\n\nvoid replace_mount_options(struct super_block *sb, char *options)\n{\n\tchar *old = sb->s_options;\n\trcu_assign_pointer(sb->s_options, options);\n\tif (old) {\n\t\tsynchronize_rcu();\n\t\tkfree(old);\n\t}\n}\nEXPORT_SYMBOL(replace_mount_options);\n\n#ifdef CONFIG_PROC_FS\n/* iterator; we want it to have access to namespace_sem, thus here... */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\n\tdown_read(&namespace_sem);\n\tif (p->cached_event == p->ns->event) {\n\t\tvoid *v = p->cached_mount;\n\t\tif (*pos == p->cached_index)\n\t\t\treturn v;\n\t\tif (*pos == p->cached_index + 1) {\n\t\t\tv = seq_list_next(v, &p->ns->list, &p->cached_index);\n\t\t\treturn p->cached_mount = v;\n\t\t}\n\t}\n\n\tp->cached_event = p->ns->event;\n\tp->cached_mount = seq_list_start(&p->ns->list, *pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\n\tp->cached_mount = seq_list_next(v, &p->ns->list, pos);\n\tp->cached_index = *pos;\n\treturn p->cached_mount;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tup_read(&namespace_sem);\n}\n\nstatic int m_show(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *r = list_entry(v, struct mount, mnt_list);\n\treturn p->show(m, &r->mnt);\n}\n\nconst struct seq_operations mounts_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show,\n};\n#endif  /* CONFIG_PROC_FS */\n\n/**\n * may_umount_tree - check if a mount tree is busy\n * @mnt: root of mount tree\n *\n * This is called to check if a tree of mounts has any\n * open files, pwds, chroots or sub mounts that are\n * busy.\n */\nint may_umount_tree(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint actual_refs = 0;\n\tint minimum_refs = 0;\n\tstruct mount *p;\n\tBUG_ON(!m);\n\n\t/* write lock needed for mnt_get_count */\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tactual_refs += mnt_get_count(p);\n\t\tminimum_refs += 2;\n\t}\n\tunlock_mount_hash();\n\n\tif (actual_refs > minimum_refs)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nEXPORT_SYMBOL(may_umount_tree);\n\n/**\n * may_umount - check if a mount point is busy\n * @mnt: root of mount\n *\n * This is called to check if a mount point has any\n * open files, pwds, chroots or sub mounts. If the\n * mount has sub mounts this will return busy\n * regardless of whether the sub mounts are busy.\n *\n * Doesn't take quota and stuff into account. IOW, in some cases it will\n * give false negatives. The main reason why it's here is that we need\n * a non-destructive way to look for easily umountable filesystems.\n */\nint may_umount(struct vfsmount *mnt)\n{\n\tint ret = 1;\n\tdown_read(&namespace_sem);\n\tlock_mount_hash();\n\tif (propagate_mount_busy(real_mount(mnt), 2))\n\t\tret = 0;\n\tunlock_mount_hash();\n\tup_read(&namespace_sem);\n\treturn ret;\n}\n\nEXPORT_SYMBOL(may_umount);\n\nstatic HLIST_HEAD(unmounted);\t/* protected by namespace_sem */\n\nstatic void namespace_unlock(void)\n{\n\tstruct hlist_head head;\n\n\thlist_move_list(&unmounted, &head);\n\n\tup_write(&namespace_sem);\n\n\tif (likely(hlist_empty(&head)))\n\t\treturn;\n\n\tsynchronize_rcu();\n\n\tgroup_pin_kill(&head);\n}\n\nstatic inline void namespace_lock(void)\n{\n\tdown_write(&namespace_sem);\n}\n\nenum umount_tree_flags {\n\tUMOUNT_SYNC = 1,\n\tUMOUNT_PROPAGATE = 2,\n\tUMOUNT_CONNECTED = 4,\n};\n\nstatic bool disconnect_mount(struct mount *mnt, enum umount_tree_flags how)\n{\n\t/* Leaving mounts connected is only valid for lazy umounts */\n\tif (how & UMOUNT_SYNC)\n\t\treturn true;\n\n\t/* A mount without a parent has nothing to be connected to */\n\tif (!mnt_has_parent(mnt))\n\t\treturn true;\n\n\t/* Because the reference counting rules change when mounts are\n\t * unmounted and connected, umounted mounts may not be\n\t * connected to mounted mounts.\n\t */\n\tif (!(mnt->mnt_parent->mnt.mnt_flags & MNT_UMOUNT))\n\t\treturn true;\n\n\t/* Has it been requested that the mount remain connected? */\n\tif (how & UMOUNT_CONNECTED)\n\t\treturn false;\n\n\t/* Is the mount locked such that it needs to remain connected? */\n\tif (IS_MNT_LOCKED(mnt))\n\t\treturn false;\n\n\t/* By default disconnect the mount */\n\treturn true;\n}\n\n/*\n * mount_lock must be held\n * namespace_sem must be held for write\n */\nstatic void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tstruct mnt_namespace *ns;\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\tns = p->mnt_ns;\n\t\tif (ns) {\n\t\t\tns->mounts--;\n\t\t\t__touch_mnt_namespace(ns);\n\t\t}\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = disconnect_mount(p, how);\n\n\t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n\t\t\t\t disconnect ? &unmounted : NULL);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t}\n}\n\nstatic void shrink_submounts(struct mount *mnt);\n\nstatic int do_umount(struct mount *mnt, int flags)\n{\n\tstruct super_block *sb = mnt->mnt.mnt_sb;\n\tint retval;\n\n\tretval = security_sb_umount(&mnt->mnt, flags);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Allow userspace to request a mountpoint be expired rather than\n\t * unmounting unconditionally. Unmount only happens if:\n\t *  (1) the mark is already set (the mark is cleared by mntput())\n\t *  (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]\n\t */\n\tif (flags & MNT_EXPIRE) {\n\t\tif (&mnt->mnt == current->fs->root.mnt ||\n\t\t    flags & (MNT_FORCE | MNT_DETACH))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * probably don't strictly need the lock here if we examined\n\t\t * all race cases, but it's a slowpath.\n\t\t */\n\t\tlock_mount_hash();\n\t\tif (mnt_get_count(mnt) != 2) {\n\t\t\tunlock_mount_hash();\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tunlock_mount_hash();\n\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * If we may have to abort operations to get out of this\n\t * mount, and they will themselves hold resources we must\n\t * allow the fs to do things. In the Unix tradition of\n\t * 'Gee thats tricky lets do it in userspace' the umount_begin\n\t * might fail to complete on the first run through as other tasks\n\t * must return, and the like. Thats for the mount program to worry\n\t * about for the moment.\n\t */\n\n\tif (flags & MNT_FORCE && sb->s_op->umount_begin) {\n\t\tsb->s_op->umount_begin(sb);\n\t}\n\n\t/*\n\t * No sense to grab the lock for this test, but test itself looks\n\t * somewhat bogus. Suggestions for better replacement?\n\t * Ho-hum... In principle, we might treat that as umount + switch\n\t * to rootfs. GC would eventually take care of the old vfsmount.\n\t * Actually it makes sense, especially if rootfs would contain a\n\t * /reboot - static binary that would close all descriptors and\n\t * call reboot(9). Then init(8) could umount root and exec /reboot.\n\t */\n\tif (&mnt->mnt == current->fs->root.mnt && !(flags & MNT_DETACH)) {\n\t\t/*\n\t\t * Special case for \"unmounting\" root ...\n\t\t * we just try to remount it readonly.\n\t\t */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tdown_write(&sb->s_umount);\n\t\tif (!(sb->s_flags & MS_RDONLY))\n\t\t\tretval = do_remount_sb(sb, MS_RDONLY, NULL, 0);\n\t\tup_write(&sb->s_umount);\n\t\treturn retval;\n\t}\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\tevent++;\n\n\tif (flags & MNT_DETACH) {\n\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE);\n\t\tretval = 0;\n\t} else {\n\t\tshrink_submounts(mnt);\n\t\tretval = -EBUSY;\n\t\tif (!propagate_mount_busy(mnt, 2)) {\n\t\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t\tretval = 0;\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\treturn retval;\n}\n\n/*\n * __detach_mounts - lazily unmount all mounts on the specified dentry\n *\n * During unlink, rmdir, and d_drop it is possible to loose the path\n * to an existing mountpoint, and wind up leaking the mount.\n * detach_mounts allows lazily unmounting those mounts instead of\n * leaking them.\n *\n * The caller may hold dentry->d_inode->i_mutex.\n */\nvoid __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\n\tnamespace_lock();\n\tmp = lookup_mountpoint(dentry);\n\tif (IS_ERR_OR_NULL(mp))\n\t\tgoto out_unlock;\n\n\tlock_mount_hash();\n\tevent++;\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\thlist_add_head(&mnt->mnt_umount.s_list, &unmounted);\n\t\t\tumount_mnt(mnt);\n\t\t}\n\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n\t}\n\tunlock_mount_hash();\n\tput_mountpoint(mp);\nout_unlock:\n\tnamespace_unlock();\n}\n\n/* \n * Is the caller allowed to modify his namespace?\n */\nstatic inline bool may_mount(void)\n{\n\treturn ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN);\n}\n\nstatic inline bool may_mandlock(void)\n{\n#ifndef\tCONFIG_MANDATORY_FILE_LOCKING\n\treturn false;\n#endif\n\treturn capable(CAP_SYS_ADMIN);\n}\n\n/*\n * Now umount can handle mount points as well as block devices.\n * This is important for filesystems which use unnamed block devices.\n *\n * We now support a flag for forced unmount like the other 'big iron'\n * unixes. Our API is identical to OSF/1 to avoid making a mess of AMD\n */\n\nSYSCALL_DEFINE2(umount, char __user *, name, int, flags)\n{\n\tstruct path path;\n\tstruct mount *mnt;\n\tint retval;\n\tint lookup_flags = 0;\n\n\tif (flags & ~(MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW))\n\t\treturn -EINVAL;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif (!(flags & UMOUNT_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\n\tretval = user_path_mountpoint_at(AT_FDCWD, name, lookup_flags, &path);\n\tif (retval)\n\t\tgoto out;\n\tmnt = real_mount(path.mnt);\n\tretval = -EINVAL;\n\tif (path.dentry != path.mnt->mnt_root)\n\t\tgoto dput_and_out;\n\tif (!check_mnt(mnt))\n\t\tgoto dput_and_out;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto dput_and_out;\n\tretval = -EPERM;\n\tif (flags & MNT_FORCE && !capable(CAP_SYS_ADMIN))\n\t\tgoto dput_and_out;\n\n\tretval = do_umount(mnt, flags);\ndput_and_out:\n\t/* we mustn't call path_put() as that would clear mnt_expiry_mark */\n\tdput(path.dentry);\n\tmntput_no_expire(mnt);\nout:\n\treturn retval;\n}\n\n#ifdef __ARCH_WANT_SYS_OLDUMOUNT\n\n/*\n *\tThe 2.0 compatible umount. No flags.\n */\nSYSCALL_DEFINE1(oldumount, char __user *, name)\n{\n\treturn sys_umount(name, 0);\n}\n\n#endif\n\nstatic bool is_mnt_ns_file(struct dentry *dentry)\n{\n\t/* Is this a proxy for a mount namespace? */\n\treturn dentry->d_op == &ns_dentry_operations &&\n\t       dentry->d_fsdata == &mntns_operations;\n}\n\nstruct mnt_namespace *to_mnt_ns(struct ns_common *ns)\n{\n\treturn container_of(ns, struct mnt_namespace, ns);\n}\n\nstatic bool mnt_ns_loop(struct dentry *dentry)\n{\n\t/* Could bind mounting the mount namespace inode cause a\n\t * mount namespace loop?\n\t */\n\tstruct mnt_namespace *mnt_ns;\n\tif (!is_mnt_ns_file(dentry))\n\t\treturn false;\n\n\tmnt_ns = to_mnt_ns(get_proc_ns(dentry->d_inode));\n\treturn current->nsproxy->mnt_ns->seq >= mnt_ns->seq;\n}\n\nstruct mount *copy_tree(struct mount *mnt, struct dentry *dentry,\n\t\t\t\t\tint flag)\n{\n\tstruct mount *res, *p, *q, *r, *parent;\n\n\tif (!(flag & CL_COPY_UNBINDABLE) && IS_MNT_UNBINDABLE(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!(flag & CL_COPY_MNT_NS_FILE) && is_mnt_ns_file(dentry))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tres = q = clone_mnt(mnt, dentry, flag);\n\tif (IS_ERR(q))\n\t\treturn q;\n\n\tq->mnt_mountpoint = mnt->mnt_mountpoint;\n\n\tp = mnt;\n\tlist_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {\n\t\tstruct mount *s;\n\t\tif (!is_subdir(r->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tfor (s = r; s; s = next_mnt(s, r)) {\n\t\t\tstruct mount *t = NULL;\n\t\t\tif (!(flag & CL_COPY_UNBINDABLE) &&\n\t\t\t    IS_MNT_UNBINDABLE(s)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!(flag & CL_COPY_MNT_NS_FILE) &&\n\t\t\t    is_mnt_ns_file(s->mnt.mnt_root)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twhile (p != s->mnt_parent) {\n\t\t\t\tp = p->mnt_parent;\n\t\t\t\tq = q->mnt_parent;\n\t\t\t}\n\t\t\tp = s;\n\t\t\tparent = q;\n\t\t\tq = clone_mnt(p, p->mnt.mnt_root, flag);\n\t\t\tif (IS_ERR(q))\n\t\t\t\tgoto out;\n\t\t\tlock_mount_hash();\n\t\t\tlist_add_tail(&q->mnt_list, &res->mnt_list);\n\t\t\tmnt_set_mountpoint(parent, p->mnt_mp, q);\n\t\t\tif (!list_empty(&parent->mnt_mounts)) {\n\t\t\t\tt = list_last_entry(&parent->mnt_mounts,\n\t\t\t\t\tstruct mount, mnt_child);\n\t\t\t\tif (t->mnt_mp != p->mnt_mp)\n\t\t\t\t\tt = NULL;\n\t\t\t}\n\t\t\tattach_shadowed(q, parent, t);\n\t\t\tunlock_mount_hash();\n\t\t}\n\t}\n\treturn res;\nout:\n\tif (res) {\n\t\tlock_mount_hash();\n\t\tumount_tree(res, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\n\treturn q;\n}\n\n/* Caller should check returned pointer for errors */\n\nstruct vfsmount *collect_mounts(struct path *path)\n{\n\tstruct mount *tree;\n\tnamespace_lock();\n\tif (!check_mnt(real_mount(path->mnt)))\n\t\ttree = ERR_PTR(-EINVAL);\n\telse\n\t\ttree = copy_tree(real_mount(path->mnt), path->dentry,\n\t\t\t\t CL_COPY_ALL | CL_PRIVATE);\n\tnamespace_unlock();\n\tif (IS_ERR(tree))\n\t\treturn ERR_CAST(tree);\n\treturn &tree->mnt;\n}\n\nvoid drop_collected_mounts(struct vfsmount *mnt)\n{\n\tnamespace_lock();\n\tlock_mount_hash();\n\tumount_tree(real_mount(mnt), UMOUNT_SYNC);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\n/**\n * clone_private_mount - create a private clone of a path\n *\n * This creates a new vfsmount, which will be the clone of @path.  The new will\n * not be attached anywhere in the namespace and will be private (i.e. changes\n * to the originating mount won't be propagated into this).\n *\n * Release with mntput().\n */\nstruct vfsmount *clone_private_mount(struct path *path)\n{\n\tstruct mount *old_mnt = real_mount(path->mnt);\n\tstruct mount *new_mnt;\n\n\tif (IS_MNT_UNBINDABLE(old_mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&namespace_sem);\n\tnew_mnt = clone_mnt(old_mnt, path->dentry, CL_PRIVATE);\n\tup_read(&namespace_sem);\n\tif (IS_ERR(new_mnt))\n\t\treturn ERR_CAST(new_mnt);\n\n\treturn &new_mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(clone_private_mount);\n\nint iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,\n\t\t   struct vfsmount *root)\n{\n\tstruct mount *mnt;\n\tint res = f(root, arg);\n\tif (res)\n\t\treturn res;\n\tlist_for_each_entry(mnt, &real_mount(root)->mnt_list, mnt_list) {\n\t\tres = f(&mnt->mnt, arg);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\treturn 0;\n}\n\nstatic void cleanup_group_ids(struct mount *mnt, struct mount *end)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p != end; p = next_mnt(p, mnt)) {\n\t\tif (p->mnt_group_id && !IS_MNT_SHARED(p))\n\t\t\tmnt_release_group_id(p);\n\t}\n}\n\nstatic int invent_group_ids(struct mount *mnt, bool recurse)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = recurse ? next_mnt(p, mnt) : NULL) {\n\t\tif (!p->mnt_group_id && !IS_MNT_SHARED(p)) {\n\t\t\tint err = mnt_alloc_group_id(p);\n\t\t\tif (err) {\n\t\t\t\tcleanup_group_ids(mnt, p);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint count_mounts(struct mnt_namespace *ns, struct mount *mnt)\n{\n\tunsigned int max = READ_ONCE(sysctl_mount_max);\n\tunsigned int mounts = 0, old, pending, sum;\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt))\n\t\tmounts++;\n\n\told = ns->mounts;\n\tpending = ns->pending_mounts;\n\tsum = old + pending;\n\tif ((old > sum) ||\n\t    (pending > sum) ||\n\t    (max < sum) ||\n\t    (mounts > (max - sum)))\n\t\treturn -ENOSPC;\n\n\tns->pending_mounts = pending + mounts;\n\treturn 0;\n}\n\n/*\n *  @source_mnt : mount tree to be attached\n *  @nd         : place the mount tree @source_mnt is attached\n *  @parent_nd  : if non-null, detach the source_mnt from its parent and\n *  \t\t   store the parent mount and mountpoint dentry.\n *  \t\t   (done when source_mnt is moved)\n *\n *  NOTE: in the table below explains the semantics when a source mount\n *  of a given type is attached to a destination mount of a given type.\n * ---------------------------------------------------------------------------\n * |         BIND MOUNT OPERATION                                            |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+)    |      private   |      slave (*) |  invalid   |\n * ***************************************************************************\n * A bind operation clones the source mount and mounts the clone on the\n * destination mount.\n *\n * (++)  the cloned mount is propagated to all the mounts in the propagation\n * \t tree of the destination mount and the cloned mount is added to\n * \t the peer group of the source mount.\n * (+)   the cloned mount is created under the destination mount and is marked\n *       as shared. The cloned mount is added to the peer group of the source\n *       mount.\n * (+++) the mount is propagated to all the mounts in the propagation tree\n *       of the destination mount and the cloned mount is made slave\n *       of the same master as that of the source mount. The cloned mount\n *       is marked as 'shared and slave'.\n * (*)   the cloned mount is made a slave of the same master as that of the\n * \t source mount.\n *\n * ---------------------------------------------------------------------------\n * |         \t\tMOVE MOUNT OPERATION                                 |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+*)   |      private   |    slave (*)   | unbindable |\n * ***************************************************************************\n *\n * (+)  the mount is moved to the destination. And is then propagated to\n * \tall the mounts in the propagation tree of the destination mount.\n * (+*)  the mount is moved to the destination.\n * (+++)  the mount is moved to the destination and is then propagated to\n * \tall the mounts belonging to the destination mount's propagation tree.\n * \tthe mount is marked as 'shared and slave'.\n * (*)\tthe mount continues to be a slave at the new location.\n *\n * if the source mount is a tree, the operations explained above is\n * applied to each mount in the tree.\n * Must be called without spinlocks held, since this function can sleep\n * in allocations.\n */\nstatic int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tstruct path *parent_path)\n{\n\tHLIST_HEAD(tree_list);\n\tstruct mnt_namespace *ns = dest_mnt->mnt_ns;\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\t/* Is there space to add these mounts to the mount namespace? */\n\tif (!parent_path) {\n\t\terr = count_mounts(ns, source_mnt);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (parent_path) {\n\t\tdetach_mnt(source_mnt, parent_path);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt, NULL);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt_last(&child->mnt_parent->mnt,\n\t\t\t\t      child->mnt_mountpoint);\n\t\tcommit_tree(child, q);\n\t}\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tchild->mnt_parent->mnt_ns->pending_mounts = 0;\n\t\tumount_tree(child, UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\tns->pending_mounts = 0;\n\treturn err;\n}\n\nstatic struct mountpoint *lock_mount(struct path *path)\n{\n\tstruct vfsmount *mnt;\n\tstruct dentry *dentry = path->dentry;\nretry:\n\tinode_lock(dentry->d_inode);\n\tif (unlikely(cant_mount(dentry))) {\n\t\tinode_unlock(dentry->d_inode);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tnamespace_lock();\n\tmnt = lookup_mnt(path);\n\tif (likely(!mnt)) {\n\t\tstruct mountpoint *mp = lookup_mountpoint(dentry);\n\t\tif (!mp)\n\t\t\tmp = new_mountpoint(dentry);\n\t\tif (IS_ERR(mp)) {\n\t\t\tnamespace_unlock();\n\t\t\tinode_unlock(dentry->d_inode);\n\t\t\treturn mp;\n\t\t}\n\t\treturn mp;\n\t}\n\tnamespace_unlock();\n\tinode_unlock(path->dentry->d_inode);\n\tpath_put(path);\n\tpath->mnt = mnt;\n\tdentry = path->dentry = dget(mnt->mnt_root);\n\tgoto retry;\n}\n\nstatic void unlock_mount(struct mountpoint *where)\n{\n\tstruct dentry *dentry = where->m_dentry;\n\tput_mountpoint(where);\n\tnamespace_unlock();\n\tinode_unlock(dentry->d_inode);\n}\n\nstatic int graft_tree(struct mount *mnt, struct mount *p, struct mountpoint *mp)\n{\n\tif (mnt->mnt.mnt_sb->s_flags & MS_NOUSER)\n\t\treturn -EINVAL;\n\n\tif (d_is_dir(mp->m_dentry) !=\n\t      d_is_dir(mnt->mnt.mnt_root))\n\t\treturn -ENOTDIR;\n\n\treturn attach_recursive_mnt(mnt, p, mp, NULL);\n}\n\n/*\n * Sanity check the flags to change_mnt_propagation.\n */\n\nstatic int flags_to_propagation_type(int flags)\n{\n\tint type = flags & ~(MS_REC | MS_SILENT);\n\n\t/* Fail if any non-propagation flags are set */\n\tif (type & ~(MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn 0;\n\t/* Only one propagation flag should be set */\n\tif (!is_power_of_2(type))\n\t\treturn 0;\n\treturn type;\n}\n\n/*\n * recursively change the type of the mountpoint.\n */\nstatic int do_change_type(struct path *path, int flag)\n{\n\tstruct mount *m;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint recurse = flag & MS_REC;\n\tint type;\n\tint err = 0;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\ttype = flags_to_propagation_type(flag);\n\tif (!type)\n\t\treturn -EINVAL;\n\n\tnamespace_lock();\n\tif (type == MS_SHARED) {\n\t\terr = invent_group_ids(mnt, recurse);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tlock_mount_hash();\n\tfor (m = mnt; m; m = (recurse ? next_mnt(m, mnt) : NULL))\n\t\tchange_mnt_propagation(m, type);\n\tunlock_mount_hash();\n\n out_unlock:\n\tnamespace_unlock();\n\treturn err;\n}\n\nstatic bool has_locked_children(struct mount *mnt, struct dentry *dentry)\n{\n\tstruct mount *child;\n\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\tif (!is_subdir(child->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tif (child->mnt.mnt_flags & MNT_LOCKED)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * do loopback mount.\n */\nstatic int do_loopback(struct path *path, const char *old_name,\n\t\t\t\tint recurse)\n{\n\tstruct path old_path;\n\tstruct mount *mnt = NULL, *old, *parent;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = -EINVAL;\n\tif (mnt_ns_loop(old_path.dentry))\n\t\tgoto out; \n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tparent = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (IS_MNT_UNBINDABLE(old))\n\t\tgoto out2;\n\n\tif (!check_mnt(parent))\n\t\tgoto out2;\n\n\tif (!check_mnt(old) && old_path.dentry->d_op != &ns_dentry_operations)\n\t\tgoto out2;\n\n\tif (!recurse && has_locked_children(old, old_path.dentry))\n\t\tgoto out2;\n\n\tif (recurse)\n\t\tmnt = copy_tree(old, old_path.dentry, CL_COPY_MNT_NS_FILE);\n\telse\n\t\tmnt = clone_mnt(old, old_path.dentry, 0);\n\n\tif (IS_ERR(mnt)) {\n\t\terr = PTR_ERR(mnt);\n\t\tgoto out2;\n\t}\n\n\tmnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\n\terr = graft_tree(mnt, parent, mp);\n\tif (err) {\n\t\tlock_mount_hash();\n\t\tumount_tree(mnt, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\nout2:\n\tunlock_mount(mp);\nout:\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic int change_mount_flags(struct vfsmount *mnt, int ms_flags)\n{\n\tint error = 0;\n\tint readonly_request = 0;\n\n\tif (ms_flags & MS_RDONLY)\n\t\treadonly_request = 1;\n\tif (readonly_request == __mnt_is_readonly(mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\terror = mnt_make_readonly(real_mount(mnt));\n\telse\n\t\t__mnt_unmake_readonly(real_mount(mnt));\n\treturn error;\n}\n\n/*\n * change filesystem flags. dir should be a physical root of filesystem.\n * If you've mounted a non-root directory somewhere and want to do remount\n * on it - tough luck.\n */\nstatic int do_remount(struct path *path, int flags, int mnt_flags,\n\t\t      void *data)\n{\n\tint err;\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\t/* Don't allow changing of locked mnt flags.\n\t *\n\t * No locks need to be held here while testing the various\n\t * MNT_LOCK flags because those flags can never be cleared\n\t * once they are set.\n\t */\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_READONLY) &&\n\t    !(mnt_flags & MNT_READONLY)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NODEV) &&\n\t    !(mnt_flags & MNT_NODEV)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NOSUID) &&\n\t    !(mnt_flags & MNT_NOSUID)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NOEXEC) &&\n\t    !(mnt_flags & MNT_NOEXEC)) {\n\t\treturn -EPERM;\n\t}\n\tif ((mnt->mnt.mnt_flags & MNT_LOCK_ATIME) &&\n\t    ((mnt->mnt.mnt_flags & MNT_ATIME_MASK) != (mnt_flags & MNT_ATIME_MASK))) {\n\t\treturn -EPERM;\n\t}\n\n\terr = security_sb_remount(sb, data);\n\tif (err)\n\t\treturn err;\n\n\tdown_write(&sb->s_umount);\n\tif (flags & MS_BIND)\n\t\terr = change_mount_flags(path->mnt, flags);\n\telse if (!capable(CAP_SYS_ADMIN))\n\t\terr = -EPERM;\n\telse\n\t\terr = do_remount_sb(sb, flags, data, 0);\n\tif (!err) {\n\t\tlock_mount_hash();\n\t\tmnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;\n\t\tmnt->mnt.mnt_flags = mnt_flags;\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tunlock_mount_hash();\n\t}\n\tup_write(&sb->s_umount);\n\treturn err;\n}\n\nstatic inline int tree_contains_unbindable(struct mount *mnt)\n{\n\tstruct mount *p;\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tif (IS_MNT_UNBINDABLE(p))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int do_move_mount(struct path *path, const char *old_name)\n{\n\tstruct path old_path, parent_path;\n\tstruct mount *p;\n\tstruct mount *old;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW, &old_path);\n\tif (err)\n\t\treturn err;\n\n\tmp = lock_mount(path);\n\terr = PTR_ERR(mp);\n\tif (IS_ERR(mp))\n\t\tgoto out;\n\n\told = real_mount(old_path.mnt);\n\tp = real_mount(path->mnt);\n\n\terr = -EINVAL;\n\tif (!check_mnt(p) || !check_mnt(old))\n\t\tgoto out1;\n\n\tif (old->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out1;\n\n\terr = -EINVAL;\n\tif (old_path.dentry != old_path.mnt->mnt_root)\n\t\tgoto out1;\n\n\tif (!mnt_has_parent(old))\n\t\tgoto out1;\n\n\tif (d_is_dir(path->dentry) !=\n\t      d_is_dir(old_path.dentry))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount residing in a shared parent.\n\t */\n\tif (IS_MNT_SHARED(old->mnt_parent))\n\t\tgoto out1;\n\t/*\n\t * Don't move a mount tree containing unbindable mounts to a destination\n\t * mount which is shared.\n\t */\n\tif (IS_MNT_SHARED(p) && tree_contains_unbindable(old))\n\t\tgoto out1;\n\terr = -ELOOP;\n\tfor (; mnt_has_parent(p); p = p->mnt_parent)\n\t\tif (p == old)\n\t\t\tgoto out1;\n\n\terr = attach_recursive_mnt(old, real_mount(path->mnt), mp, &parent_path);\n\tif (err)\n\t\tgoto out1;\n\n\t/* if the mount is moved, it should no longer be expire\n\t * automatically */\n\tlist_del_init(&old->mnt_expire);\nout1:\n\tunlock_mount(mp);\nout:\n\tif (!err)\n\t\tpath_put(&parent_path);\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic struct vfsmount *fs_set_subtype(struct vfsmount *mnt, const char *fstype)\n{\n\tint err;\n\tconst char *subtype = strchr(fstype, '.');\n\tif (subtype) {\n\t\tsubtype++;\n\t\terr = -EINVAL;\n\t\tif (!subtype[0])\n\t\t\tgoto err;\n\t} else\n\t\tsubtype = \"\";\n\n\tmnt->mnt_sb->s_subtype = kstrdup(subtype, GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!mnt->mnt_sb->s_subtype)\n\t\tgoto err;\n\treturn mnt;\n\n err:\n\tmntput(mnt);\n\treturn ERR_PTR(err);\n}\n\n/*\n * add a mount into a namespace's mount tree\n */\nstatic int do_add_mount(struct mount *newmnt, struct path *path, int mnt_flags)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *parent;\n\tint err;\n\n\tmnt_flags &= ~MNT_INTERNAL_FLAGS;\n\n\tmp = lock_mount(path);\n\tif (IS_ERR(mp))\n\t\treturn PTR_ERR(mp);\n\n\tparent = real_mount(path->mnt);\n\terr = -EINVAL;\n\tif (unlikely(!check_mnt(parent))) {\n\t\t/* that's acceptable only for automounts done in private ns */\n\t\tif (!(mnt_flags & MNT_SHRINKABLE))\n\t\t\tgoto unlock;\n\t\t/* ... and for those we'd better have mountpoint still alive */\n\t\tif (!parent->mnt_ns)\n\t\t\tgoto unlock;\n\t}\n\n\t/* Refuse the same filesystem on the same mount point */\n\terr = -EBUSY;\n\tif (path->mnt->mnt_sb == newmnt->mnt.mnt_sb &&\n\t    path->mnt->mnt_root == path->dentry)\n\t\tgoto unlock;\n\n\terr = -EINVAL;\n\tif (d_is_symlink(newmnt->mnt.mnt_root))\n\t\tgoto unlock;\n\n\tnewmnt->mnt.mnt_flags = mnt_flags;\n\terr = graft_tree(newmnt, parent, mp);\n\nunlock:\n\tunlock_mount(mp);\n\treturn err;\n}\n\nstatic bool mount_too_revealing(struct vfsmount *mnt, int *new_mnt_flags);\n\n/*\n * create a new mount for userspace and request it to be added into the\n * namespace's tree\n */\nstatic int do_new_mount(struct path *path, const char *fstype, int flags,\n\t\t\tint mnt_flags, const char *name, void *data)\n{\n\tstruct file_system_type *type;\n\tstruct vfsmount *mnt;\n\tint err;\n\n\tif (!fstype)\n\t\treturn -EINVAL;\n\n\ttype = get_fs_type(fstype);\n\tif (!type)\n\t\treturn -ENODEV;\n\n\tmnt = vfs_kern_mount(type, flags, name, data);\n\tif (!IS_ERR(mnt) && (type->fs_flags & FS_HAS_SUBTYPE) &&\n\t    !mnt->mnt_sb->s_subtype)\n\t\tmnt = fs_set_subtype(mnt, fstype);\n\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn PTR_ERR(mnt);\n\n\tif (mount_too_revealing(mnt, &mnt_flags)) {\n\t\tmntput(mnt);\n\t\treturn -EPERM;\n\t}\n\n\terr = do_add_mount(real_mount(mnt), path, mnt_flags);\n\tif (err)\n\t\tmntput(mnt);\n\treturn err;\n}\n\nint finish_automount(struct vfsmount *m, struct path *path)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint err;\n\t/* The new mount record should have at least 2 refs to prevent it being\n\t * expired before we get a chance to add it\n\t */\n\tBUG_ON(mnt_get_count(mnt) < 2);\n\n\tif (m->mnt_sb == path->mnt->mnt_sb &&\n\t    m->mnt_root == path->dentry) {\n\t\terr = -ELOOP;\n\t\tgoto fail;\n\t}\n\n\terr = do_add_mount(mnt, path, path->mnt->mnt_flags | MNT_SHRINKABLE);\n\tif (!err)\n\t\treturn 0;\nfail:\n\t/* remove m from any expiration list it may be on */\n\tif (!list_empty(&mnt->mnt_expire)) {\n\t\tnamespace_lock();\n\t\tlist_del_init(&mnt->mnt_expire);\n\t\tnamespace_unlock();\n\t}\n\tmntput(m);\n\tmntput(m);\n\treturn err;\n}\n\n/**\n * mnt_set_expiry - Put a mount on an expiration list\n * @mnt: The mount to list.\n * @expiry_list: The list to add the mount to.\n */\nvoid mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)\n{\n\tnamespace_lock();\n\n\tlist_add_tail(&real_mount(mnt)->mnt_expire, expiry_list);\n\n\tnamespace_unlock();\n}\nEXPORT_SYMBOL(mnt_set_expiry);\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * mountpoints that aren't in use and haven't been touched since last we came\n * here\n */\nvoid mark_mounts_for_expiry(struct list_head *mounts)\n{\n\tstruct mount *mnt, *next;\n\tLIST_HEAD(graveyard);\n\n\tif (list_empty(mounts))\n\t\treturn;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* extract from the expiration list every vfsmount that matches the\n\t * following criteria:\n\t * - only referenced by its parent vfsmount\n\t * - still marked for expiry (marked on the last call here; marks are\n\t *   cleared by mntput())\n\t */\n\tlist_for_each_entry_safe(mnt, next, mounts, mnt_expire) {\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1) ||\n\t\t\tpropagate_mount_busy(mnt, 1))\n\t\t\tcontinue;\n\t\tlist_move(&mnt->mnt_expire, &graveyard);\n\t}\n\twhile (!list_empty(&graveyard)) {\n\t\tmnt = list_first_entry(&graveyard, struct mount, mnt_expire);\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nEXPORT_SYMBOL_GPL(mark_mounts_for_expiry);\n\n/*\n * Ripoff of 'select_parent()'\n *\n * search the list of submounts for a given mountpoint, and move any\n * shrinkable submounts to the 'graveyard' list.\n */\nstatic int select_submounts(struct mount *parent, struct list_head *graveyard)\n{\n\tstruct mount *this_parent = parent;\n\tstruct list_head *next;\n\tint found = 0;\n\nrepeat:\n\tnext = this_parent->mnt_mounts.next;\nresume:\n\twhile (next != &this_parent->mnt_mounts) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct mount *mnt = list_entry(tmp, struct mount, mnt_child);\n\n\t\tnext = tmp->next;\n\t\tif (!(mnt->mnt.mnt_flags & MNT_SHRINKABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Descend a level if the d_mounts list is non-empty.\n\t\t */\n\t\tif (!list_empty(&mnt->mnt_mounts)) {\n\t\t\tthis_parent = mnt;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tif (!propagate_mount_busy(mnt, 1)) {\n\t\t\tlist_move_tail(&mnt->mnt_expire, graveyard);\n\t\t\tfound++;\n\t\t}\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search\n\t */\n\tif (this_parent != parent) {\n\t\tnext = this_parent->mnt_child.next;\n\t\tthis_parent = this_parent->mnt_parent;\n\t\tgoto resume;\n\t}\n\treturn found;\n}\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * submounts of a specific parent mountpoint\n *\n * mount_lock must be held for write\n */\nstatic void shrink_submounts(struct mount *mnt)\n{\n\tLIST_HEAD(graveyard);\n\tstruct mount *m;\n\n\t/* extract submounts of 'mountpoint' from the expiration list */\n\twhile (select_submounts(mnt, &graveyard)) {\n\t\twhile (!list_empty(&graveyard)) {\n\t\t\tm = list_first_entry(&graveyard, struct mount,\n\t\t\t\t\t\tmnt_expire);\n\t\t\ttouch_mnt_namespace(m->mnt_ns);\n\t\t\tumount_tree(m, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t}\n\t}\n}\n\n/*\n * Some copy_from_user() implementations do not return the exact number of\n * bytes remaining to copy on a fault.  But copy_mount_options() requires that.\n * Note that this function differs from copy_from_user() in that it will oops\n * on bad values of `to', rather than returning a short copy.\n */\nstatic long exact_copy_from_user(void *to, const void __user * from,\n\t\t\t\t unsigned long n)\n{\n\tchar *t = to;\n\tconst char __user *f = from;\n\tchar c;\n\n\tif (!access_ok(VERIFY_READ, from, n))\n\t\treturn n;\n\n\twhile (n) {\n\t\tif (__get_user(c, f)) {\n\t\t\tmemset(t, 0, n);\n\t\t\tbreak;\n\t\t}\n\t\t*t++ = c;\n\t\tf++;\n\t\tn--;\n\t}\n\treturn n;\n}\n\nvoid *copy_mount_options(const void __user * data)\n{\n\tint i;\n\tunsigned long size;\n\tchar *copy;\n\n\tif (!data)\n\t\treturn NULL;\n\n\tcopy = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!copy)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/* We only care that *some* data at the address the user\n\t * gave us is valid.  Just in case, we'll zero\n\t * the remainder of the page.\n\t */\n\t/* copy_from_user cannot cross TASK_SIZE ! */\n\tsize = TASK_SIZE - (unsigned long)data;\n\tif (size > PAGE_SIZE)\n\t\tsize = PAGE_SIZE;\n\n\ti = size - exact_copy_from_user(copy, data, size);\n\tif (!i) {\n\t\tkfree(copy);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\tif (i != PAGE_SIZE)\n\t\tmemset(copy + i, 0, PAGE_SIZE - i);\n\treturn copy;\n}\n\nchar *copy_mount_string(const void __user *data)\n{\n\treturn data ? strndup_user(data, PAGE_SIZE) : NULL;\n}\n\n/*\n * Flags is a 32-bit value that allows up to 31 non-fs dependent flags to\n * be given to the mount() call (ie: read-only, no-dev, no-suid etc).\n *\n * data is a (void *) that can point to any structure up to\n * PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent\n * information (or be NULL).\n *\n * Pre-0.97 versions of mount() didn't have a flags word.\n * When the flags word was introduced its top half was required\n * to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.\n * Therefore, if this magic number is present, it carries no information\n * and must be discarded.\n */\nlong do_mount(const char *dev_name, const char __user *dir_name,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tstruct path path;\n\tint retval = 0;\n\tint mnt_flags = 0;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\t/* Basic sanity checks */\n\tif (data_page)\n\t\t((char *)data_page)[PAGE_SIZE - 1] = 0;\n\n\t/* ... and get the mountpoint */\n\tretval = user_path(dir_name, &path);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = security_sb_mount(dev_name, &path,\n\t\t\t\t   type_page, flags, data_page);\n\tif (!retval && !may_mount())\n\t\tretval = -EPERM;\n\tif (!retval && (flags & MS_MANDLOCK) && !may_mandlock())\n\t\tretval = -EPERM;\n\tif (retval)\n\t\tgoto dput_out;\n\n\t/* Default to relatime unless overriden */\n\tif (!(flags & MS_NOATIME))\n\t\tmnt_flags |= MNT_RELATIME;\n\n\t/* Separate the per-mountpoint flags */\n\tif (flags & MS_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (flags & MS_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (flags & MS_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (flags & MS_NOATIME)\n\t\tmnt_flags |= MNT_NOATIME;\n\tif (flags & MS_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (flags & MS_STRICTATIME)\n\t\tmnt_flags &= ~(MNT_RELATIME | MNT_NOATIME);\n\tif (flags & MS_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\n\t/* The default atime for remount is preservation */\n\tif ((flags & MS_REMOUNT) &&\n\t    ((flags & (MS_NOATIME | MS_NODIRATIME | MS_RELATIME |\n\t\t       MS_STRICTATIME)) == 0)) {\n\t\tmnt_flags &= ~MNT_ATIME_MASK;\n\t\tmnt_flags |= path.mnt->mnt_flags & MNT_ATIME_MASK;\n\t}\n\n\tflags &= ~(MS_NOSUID | MS_NOEXEC | MS_NODEV | MS_ACTIVE | MS_BORN |\n\t\t   MS_NOATIME | MS_NODIRATIME | MS_RELATIME| MS_KERNMOUNT |\n\t\t   MS_STRICTATIME);\n\n\tif (flags & MS_REMOUNT)\n\t\tretval = do_remount(&path, flags & ~MS_REMOUNT, mnt_flags,\n\t\t\t\t    data_page);\n\telse if (flags & MS_BIND)\n\t\tretval = do_loopback(&path, dev_name, flags & MS_REC);\n\telse if (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\tretval = do_change_type(&path, flags);\n\telse if (flags & MS_MOVE)\n\t\tretval = do_move_mount(&path, dev_name);\n\telse\n\t\tretval = do_new_mount(&path, type_page, flags, mnt_flags,\n\t\t\t\t      dev_name, data_page);\ndput_out:\n\tpath_put(&path);\n\treturn retval;\n}\n\nstatic struct ucounts *inc_mnt_namespaces(struct user_namespace *ns)\n{\n\treturn inc_ucount(ns, current_euid(), UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void dec_mnt_namespaces(struct ucounts *ucounts)\n{\n\tdec_ucount(ucounts, UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *ns)\n{\n\tns_free_inum(&ns->ns);\n\tdec_mnt_namespaces(ns->ucounts);\n\tput_user_ns(ns->user_ns);\n\tkfree(ns);\n}\n\n/*\n * Assign a sequence number so we can detect when we attempt to bind\n * mount a reference to an older mount namespace into the current\n * mount namespace, preventing reference counting loops.  A 64bit\n * number incrementing at 10Ghz will take 12,427 years to wrap which\n * is effectively never, so we can ignore the possibility.\n */\nstatic atomic64_t mnt_ns_seq = ATOMIC64_INIT(1);\n\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct ucounts *ucounts;\n\tint ret;\n\n\tucounts = inc_mnt_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnew_ns = kmalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns) {\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tret = ns_alloc_inum(&new_ns->ns);\n\tif (ret) {\n\t\tkfree(new_ns);\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(ret);\n\t}\n\tnew_ns->ns.ops = &mntns_operations;\n\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\tatomic_set(&new_ns->count, 1);\n\tnew_ns->root = NULL;\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tnew_ns->event = 0;\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\tnew_ns->ucounts = ucounts;\n\tnew_ns->mounts = 0;\n\tnew_ns->pending_mounts = 0;\n\treturn new_ns;\n}\n\nstruct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tnew_ns->mounts++;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}\n\n/**\n * create_mnt_ns - creates a private namespace and adds a root filesystem\n * @mnt: pointer to the new root filesystem mountpoint\n */\nstatic struct mnt_namespace *create_mnt_ns(struct vfsmount *m)\n{\n\tstruct mnt_namespace *new_ns = alloc_mnt_ns(&init_user_ns);\n\tif (!IS_ERR(new_ns)) {\n\t\tstruct mount *mnt = real_mount(m);\n\t\tmnt->mnt_ns = new_ns;\n\t\tnew_ns->root = mnt;\n\t\tnew_ns->mounts++;\n\t\tlist_add(&mnt->mnt_list, &new_ns->list);\n\t} else {\n\t\tmntput(m);\n\t}\n\treturn new_ns;\n}\n\nstruct dentry *mount_subtree(struct vfsmount *mnt, const char *name)\n{\n\tstruct mnt_namespace *ns;\n\tstruct super_block *s;\n\tstruct path path;\n\tint err;\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\treturn ERR_CAST(ns);\n\n\terr = vfs_path_lookup(mnt->mnt_root, mnt,\n\t\t\tname, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);\n\n\tput_mnt_ns(ns);\n\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* trade a vfsmount reference for active sb one */\n\ts = path.mnt->mnt_sb;\n\tatomic_inc(&s->s_active);\n\tmntput(path.mnt);\n\t/* lock the sucker */\n\tdown_write(&s->s_umount);\n\t/* ... and return the root of (sub)tree on it */\n\treturn path.dentry;\n}\nEXPORT_SYMBOL(mount_subtree);\n\nSYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,\n\t\tchar __user *, type, unsigned long, flags, void __user *, data)\n{\n\tint ret;\n\tchar *kernel_type;\n\tchar *kernel_dev;\n\tvoid *options;\n\n\tkernel_type = copy_mount_string(type);\n\tret = PTR_ERR(kernel_type);\n\tif (IS_ERR(kernel_type))\n\t\tgoto out_type;\n\n\tkernel_dev = copy_mount_string(dev_name);\n\tret = PTR_ERR(kernel_dev);\n\tif (IS_ERR(kernel_dev))\n\t\tgoto out_dev;\n\n\toptions = copy_mount_options(data);\n\tret = PTR_ERR(options);\n\tif (IS_ERR(options))\n\t\tgoto out_data;\n\n\tret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);\n\n\tkfree(options);\nout_data:\n\tkfree(kernel_dev);\nout_dev:\n\tkfree(kernel_type);\nout_type:\n\treturn ret;\n}\n\n/*\n * Return true if path is reachable from root\n *\n * namespace_sem or mount_lock is held\n */\nbool is_path_reachable(struct mount *mnt, struct dentry *dentry,\n\t\t\t const struct path *root)\n{\n\twhile (&mnt->mnt != root->mnt && mnt_has_parent(mnt)) {\n\t\tdentry = mnt->mnt_mountpoint;\n\t\tmnt = mnt->mnt_parent;\n\t}\n\treturn &mnt->mnt == root->mnt && is_subdir(dentry, root->dentry);\n}\n\nbool path_is_under(struct path *path1, struct path *path2)\n{\n\tbool res;\n\tread_seqlock_excl(&mount_lock);\n\tres = is_path_reachable(real_mount(path1->mnt), path1->dentry, path2);\n\tread_sequnlock_excl(&mount_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_under);\n\n/*\n * pivot_root Semantics:\n * Moves the root file system of the current process to the directory put_old,\n * makes new_root as the new root file system of the current process, and sets\n * root/cwd of all processes which had them on the current root to new_root.\n *\n * Restrictions:\n * The new_root and put_old must be directories, and  must not be on the\n * same file  system as the current process root. The put_old  must  be\n * underneath new_root,  i.e. adding a non-zero number of /.. to the string\n * pointed to by put_old must yield the same directory as new_root. No other\n * file system may be mounted on put_old. After all, new_root is a mountpoint.\n *\n * Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.\n * See Documentation/filesystems/ramfs-rootfs-initramfs.txt for alternatives\n * in this situation.\n *\n * Notes:\n *  - we don't move root/cwd if they are not at the root (reason: if something\n *    cared enough to change them, it's probably wrong to force them elsewhere)\n *  - it's okay to pick a root that isn't the root of a file system, e.g.\n *    /nfs/my_root where /nfs is the mount point. It must be a mountpoint,\n *    though, so you may need to say mount --bind /nfs/my_root /nfs/my_root\n *    first.\n */\nSYSCALL_DEFINE2(pivot_root, const char __user *, new_root,\n\t\tconst char __user *, put_old)\n{\n\tstruct path new, old, parent_path, root_parent, root;\n\tstruct mount *new_mnt, *root_mnt, *old_mnt;\n\tstruct mountpoint *old_mp, *root_mp;\n\tint error;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terror = user_path_dir(new_root, &new);\n\tif (error)\n\t\tgoto out0;\n\n\terror = user_path_dir(put_old, &old);\n\tif (error)\n\t\tgoto out1;\n\n\terror = security_sb_pivotroot(&old, &new);\n\tif (error)\n\t\tgoto out2;\n\n\tget_fs_root(current->fs, &root);\n\told_mp = lock_mount(&old);\n\terror = PTR_ERR(old_mp);\n\tif (IS_ERR(old_mp))\n\t\tgoto out3;\n\n\terror = -EINVAL;\n\tnew_mnt = real_mount(new.mnt);\n\troot_mnt = real_mount(root.mnt);\n\told_mnt = real_mount(old.mnt);\n\tif (IS_MNT_SHARED(old_mnt) ||\n\t\tIS_MNT_SHARED(new_mnt->mnt_parent) ||\n\t\tIS_MNT_SHARED(root_mnt->mnt_parent))\n\t\tgoto out4;\n\tif (!check_mnt(root_mnt) || !check_mnt(new_mnt))\n\t\tgoto out4;\n\tif (new_mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out4;\n\terror = -ENOENT;\n\tif (d_unlinked(new.dentry))\n\t\tgoto out4;\n\terror = -EBUSY;\n\tif (new_mnt == root_mnt || old_mnt == root_mnt)\n\t\tgoto out4; /* loop, on the same file system  */\n\terror = -EINVAL;\n\tif (root.mnt->mnt_root != root.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(root_mnt))\n\t\tgoto out4; /* not attached */\n\troot_mp = root_mnt->mnt_mp;\n\tif (new.mnt->mnt_root != new.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(new_mnt))\n\t\tgoto out4; /* not attached */\n\t/* make sure we can reach put_old from new_root */\n\tif (!is_path_reachable(old_mnt, old.dentry, &new))\n\t\tgoto out4;\n\t/* make certain new is below the root */\n\tif (!is_path_reachable(new_mnt, new.dentry, &root))\n\t\tgoto out4;\n\troot_mp->m_count++; /* pin it so it won't go away */\n\tlock_mount_hash();\n\tdetach_mnt(new_mnt, &parent_path);\n\tdetach_mnt(root_mnt, &root_parent);\n\tif (root_mnt->mnt.mnt_flags & MNT_LOCKED) {\n\t\tnew_mnt->mnt.mnt_flags |= MNT_LOCKED;\n\t\troot_mnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n\t/* mount old root on put_old */\n\tattach_mnt(root_mnt, old_mnt, old_mp);\n\t/* mount new_root on / */\n\tattach_mnt(new_mnt, real_mount(root_parent.mnt), root_mp);\n\ttouch_mnt_namespace(current->nsproxy->mnt_ns);\n\t/* A moved mount should not expire automatically */\n\tlist_del_init(&new_mnt->mnt_expire);\n\tunlock_mount_hash();\n\tchroot_fs_refs(&root, &new);\n\tput_mountpoint(root_mp);\n\terror = 0;\nout4:\n\tunlock_mount(old_mp);\n\tif (!error) {\n\t\tpath_put(&root_parent);\n\t\tpath_put(&parent_path);\n\t}\nout3:\n\tpath_put(&root);\nout2:\n\tpath_put(&old);\nout1:\n\tpath_put(&new);\nout0:\n\treturn error;\n}\n\nstatic void __init init_mount_tree(void)\n{\n\tstruct vfsmount *mnt;\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\tstruct file_system_type *type;\n\n\ttype = get_fs_type(\"rootfs\");\n\tif (!type)\n\t\tpanic(\"Can't find rootfs type\");\n\tmnt = vfs_kern_mount(type, 0, \"rootfs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\tpanic(\"Can't create rootfs\");\n\n\tns = create_mnt_ns(mnt);\n\tif (IS_ERR(ns))\n\t\tpanic(\"Can't allocate initial namespace\");\n\n\tinit_task.nsproxy->mnt_ns = ns;\n\tget_mnt_ns(ns);\n\n\troot.mnt = mnt;\n\troot.dentry = mnt->mnt_root;\n\tmnt->mnt_flags |= MNT_LOCKED;\n\n\tset_fs_pwd(current->fs, &root);\n\tset_fs_root(current->fs, &root);\n}\n\nvoid __init mnt_init(void)\n{\n\tunsigned u;\n\tint err;\n\n\tmnt_cache = kmem_cache_create(\"mnt_cache\", sizeof(struct mount),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\tmount_hashtable = alloc_large_system_hash(\"Mount-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmhash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&m_hash_shift, &m_hash_mask, 0, 0);\n\tmountpoint_hashtable = alloc_large_system_hash(\"Mountpoint-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmphash_entries, 19,\n\t\t\t\t0,\n\t\t\t\t&mp_hash_shift, &mp_hash_mask, 0, 0);\n\n\tif (!mount_hashtable || !mountpoint_hashtable)\n\t\tpanic(\"Failed to allocate mount hash table\\n\");\n\n\tfor (u = 0; u <= m_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mount_hashtable[u]);\n\tfor (u = 0; u <= mp_hash_mask; u++)\n\t\tINIT_HLIST_HEAD(&mountpoint_hashtable[u]);\n\n\tkernfs_init();\n\n\terr = sysfs_init();\n\tif (err)\n\t\tprintk(KERN_WARNING \"%s: sysfs_init error: %d\\n\",\n\t\t\t__func__, err);\n\tfs_kobj = kobject_create_and_add(\"fs\", NULL);\n\tif (!fs_kobj)\n\t\tprintk(KERN_WARNING \"%s: kobj create error\\n\", __func__);\n\tinit_rootfs();\n\tinit_mount_tree();\n}\n\nvoid put_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!atomic_dec_and_test(&ns->count))\n\t\treturn;\n\tdrop_collected_mounts(&ns->root->mnt);\n\tfree_mnt_ns(ns);\n}\n\nstruct vfsmount *kern_mount_data(struct file_system_type *type, void *data)\n{\n\tstruct vfsmount *mnt;\n\tmnt = vfs_kern_mount(type, MS_KERNMOUNT, type->name, data);\n\tif (!IS_ERR(mnt)) {\n\t\t/*\n\t\t * it is a longterm mount, don't release mnt until\n\t\t * we unmount before file sys is unregistered\n\t\t*/\n\t\treal_mount(mnt)->mnt_ns = MNT_NS_INTERNAL;\n\t}\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(kern_mount_data);\n\nvoid kern_unmount(struct vfsmount *mnt)\n{\n\t/* release long term mount so mount point can be released */\n\tif (!IS_ERR_OR_NULL(mnt)) {\n\t\treal_mount(mnt)->mnt_ns = NULL;\n\t\tsynchronize_rcu();\t/* yecchhh... */\n\t\tmntput(mnt);\n\t}\n}\nEXPORT_SYMBOL(kern_unmount);\n\nbool our_mnt(struct vfsmount *mnt)\n{\n\treturn check_mnt(real_mount(mnt));\n}\n\nbool current_chrooted(void)\n{\n\t/* Does the current process have a non-standard root */\n\tstruct path ns_root;\n\tstruct path fs_root;\n\tbool chrooted;\n\n\t/* Find the namespace root */\n\tns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;\n\tns_root.dentry = ns_root.mnt->mnt_root;\n\tpath_get(&ns_root);\n\twhile (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))\n\t\t;\n\n\tget_fs_root(current->fs, &fs_root);\n\n\tchrooted = !path_equal(&fs_root, &ns_root);\n\n\tpath_put(&fs_root);\n\tpath_put(&ns_root);\n\n\treturn chrooted;\n}\n\nstatic bool mnt_already_visible(struct mnt_namespace *ns, struct vfsmount *new,\n\t\t\t\tint *new_mnt_flags)\n{\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tint mnt_flags;\n\n\t\tif (mnt->mnt.mnt_sb->s_type != new->mnt_sb->s_type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* A local view of the mount flags */\n\t\tmnt_flags = mnt->mnt.mnt_flags;\n\n\t\t/* Don't miss readonly hidden in the superblock flags */\n\t\tif (mnt->mnt.mnt_sb->s_flags & MS_RDONLY)\n\t\t\tmnt_flags |= MNT_LOCK_READONLY;\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(child->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t       MNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}\n\nstatic bool mount_too_revealing(struct vfsmount *mnt, int *new_mnt_flags)\n{\n\tconst unsigned long required_iflags = SB_I_NOEXEC | SB_I_NODEV;\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tunsigned long s_iflags;\n\n\tif (ns->user_ns == &init_user_ns)\n\t\treturn false;\n\n\t/* Can this filesystem be too revealing? */\n\ts_iflags = mnt->mnt_sb->s_iflags;\n\tif (!(s_iflags & SB_I_USERNS_VISIBLE))\n\t\treturn false;\n\n\tif ((s_iflags & required_iflags) != required_iflags) {\n\t\tWARN_ONCE(1, \"Expected s_iflags to contain 0x%lx\\n\",\n\t\t\t  required_iflags);\n\t\treturn true;\n\t}\n\n\treturn !mnt_already_visible(ns, mnt, new_mnt_flags);\n}\n\nbool mnt_may_suid(struct vfsmount *mnt)\n{\n\t/*\n\t * Foreign mounts (accessed via fchdir or through /proc\n\t * symlinks) are always treated as if they are nosuid.  This\n\t * prevents namespaces from trusting potentially unsafe\n\t * suid/sgid bits, file caps, or security labels that originate\n\t * in other namespaces.\n\t */\n\treturn !(mnt->mnt_flags & MNT_NOSUID) && check_mnt(real_mount(mnt)) &&\n\t       current_in_userns(mnt->mnt_sb->s_user_ns);\n}\n\nstatic struct ns_common *mntns_get(struct task_struct *task)\n{\n\tstruct ns_common *ns = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy) {\n\t\tns = &nsproxy->mnt_ns->ns;\n\t\tget_mnt_ns(to_mnt_ns(ns));\n\t}\n\ttask_unlock(task);\n\n\treturn ns;\n}\n\nstatic void mntns_put(struct ns_common *ns)\n{\n\tput_mnt_ns(to_mnt_ns(ns));\n}\n\nstatic int mntns_install(struct nsproxy *nsproxy, struct ns_common *ns)\n{\n\tstruct fs_struct *fs = current->fs;\n\tstruct mnt_namespace *mnt_ns = to_mnt_ns(ns);\n\tstruct path root;\n\n\tif (!ns_capable(mnt_ns->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_CHROOT) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (fs->users != 1)\n\t\treturn -EINVAL;\n\n\tget_mnt_ns(mnt_ns);\n\tput_mnt_ns(nsproxy->mnt_ns);\n\tnsproxy->mnt_ns = mnt_ns;\n\n\t/* Find the root */\n\troot.mnt    = &mnt_ns->root->mnt;\n\troot.dentry = mnt_ns->root->mnt.mnt_root;\n\tpath_get(&root);\n\twhile(d_mountpoint(root.dentry) && follow_down_one(&root))\n\t\t;\n\n\t/* Update the pwd and root */\n\tset_fs_pwd(fs, &root);\n\tset_fs_root(fs, &root);\n\n\tpath_put(&root);\n\treturn 0;\n}\n\nstatic struct user_namespace *mntns_owner(struct ns_common *ns)\n{\n\treturn to_mnt_ns(ns)->user_ns;\n}\n\nconst struct proc_ns_operations mntns_operations = {\n\t.name\t\t= \"mnt\",\n\t.type\t\t= CLONE_NEWNS,\n\t.get\t\t= mntns_get,\n\t.put\t\t= mntns_put,\n\t.install\t= mntns_install,\n\t.owner\t\t= mntns_owner,\n};\n", "/*\n *  linux/fs/pnode.c\n *\n * (C) Copyright IBM Corporation 2005.\n *\tReleased under GPL v2.\n *\tAuthor : Ram Pai (linuxram@us.ibm.com)\n *\n */\n#include <linux/mnt_namespace.h>\n#include <linux/mount.h>\n#include <linux/fs.h>\n#include <linux/nsproxy.h>\n#include \"internal.h\"\n#include \"pnode.h\"\n\n/* return the next shared peer mount of @p */\nstatic inline struct mount *next_peer(struct mount *p)\n{\n\treturn list_entry(p->mnt_share.next, struct mount, mnt_share);\n}\n\nstatic inline struct mount *first_slave(struct mount *p)\n{\n\treturn list_entry(p->mnt_slave_list.next, struct mount, mnt_slave);\n}\n\nstatic inline struct mount *next_slave(struct mount *p)\n{\n\treturn list_entry(p->mnt_slave.next, struct mount, mnt_slave);\n}\n\nstatic struct mount *get_peer_under_root(struct mount *mnt,\n\t\t\t\t\t struct mnt_namespace *ns,\n\t\t\t\t\t const struct path *root)\n{\n\tstruct mount *m = mnt;\n\n\tdo {\n\t\t/* Check the namespace first for optimization */\n\t\tif (m->mnt_ns == ns && is_path_reachable(m, m->mnt.mnt_root, root))\n\t\t\treturn m;\n\n\t\tm = next_peer(m);\n\t} while (m != mnt);\n\n\treturn NULL;\n}\n\n/*\n * Get ID of closest dominating peer group having a representative\n * under the given root.\n *\n * Caller must hold namespace_sem\n */\nint get_dominating_id(struct mount *mnt, const struct path *root)\n{\n\tstruct mount *m;\n\n\tfor (m = mnt->mnt_master; m != NULL; m = m->mnt_master) {\n\t\tstruct mount *d = get_peer_under_root(m, mnt->mnt_ns, root);\n\t\tif (d)\n\t\t\treturn d->mnt_group_id;\n\t}\n\n\treturn 0;\n}\n\nstatic int do_make_slave(struct mount *mnt)\n{\n\tstruct mount *peer_mnt = mnt, *master = mnt->mnt_master;\n\tstruct mount *slave_mnt;\n\n\t/*\n\t * slave 'mnt' to a peer mount that has the\n\t * same root dentry. If none is available then\n\t * slave it to anything that is available.\n\t */\n\twhile ((peer_mnt = next_peer(peer_mnt)) != mnt &&\n\t       peer_mnt->mnt.mnt_root != mnt->mnt.mnt_root) ;\n\n\tif (peer_mnt == mnt) {\n\t\tpeer_mnt = next_peer(mnt);\n\t\tif (peer_mnt == mnt)\n\t\t\tpeer_mnt = NULL;\n\t}\n\tif (mnt->mnt_group_id && IS_MNT_SHARED(mnt) &&\n\t    list_empty(&mnt->mnt_share))\n\t\tmnt_release_group_id(mnt);\n\n\tlist_del_init(&mnt->mnt_share);\n\tmnt->mnt_group_id = 0;\n\n\tif (peer_mnt)\n\t\tmaster = peer_mnt;\n\n\tif (master) {\n\t\tlist_for_each_entry(slave_mnt, &mnt->mnt_slave_list, mnt_slave)\n\t\t\tslave_mnt->mnt_master = master;\n\t\tlist_move(&mnt->mnt_slave, &master->mnt_slave_list);\n\t\tlist_splice(&mnt->mnt_slave_list, master->mnt_slave_list.prev);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t} else {\n\t\tstruct list_head *p = &mnt->mnt_slave_list;\n\t\twhile (!list_empty(p)) {\n                        slave_mnt = list_first_entry(p,\n\t\t\t\t\tstruct mount, mnt_slave);\n\t\t\tlist_del_init(&slave_mnt->mnt_slave);\n\t\t\tslave_mnt->mnt_master = NULL;\n\t\t}\n\t}\n\tmnt->mnt_master = master;\n\tCLEAR_MNT_SHARED(mnt);\n\treturn 0;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid change_mnt_propagation(struct mount *mnt, int type)\n{\n\tif (type == MS_SHARED) {\n\t\tset_mnt_shared(mnt);\n\t\treturn;\n\t}\n\tdo_make_slave(mnt);\n\tif (type != MS_SLAVE) {\n\t\tlist_del_init(&mnt->mnt_slave);\n\t\tmnt->mnt_master = NULL;\n\t\tif (type == MS_UNBINDABLE)\n\t\t\tmnt->mnt.mnt_flags |= MNT_UNBINDABLE;\n\t\telse\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_UNBINDABLE;\n\t}\n}\n\n/*\n * get the next mount in the propagation tree.\n * @m: the mount seen last\n * @origin: the original mount from where the tree walk initiated\n *\n * Note that peer groups form contiguous segments of slave lists.\n * We rely on that in get_source() to be able to find out if\n * vfsmount found while iterating with propagation_next() is\n * a peer of one we'd found earlier.\n */\nstatic struct mount *propagation_next(struct mount *m,\n\t\t\t\t\t struct mount *origin)\n{\n\t/* are there any slaves of this mount? */\n\tif (!IS_MNT_NEW(m) && !list_empty(&m->mnt_slave_list))\n\t\treturn first_slave(m);\n\n\twhile (1) {\n\t\tstruct mount *master = m->mnt_master;\n\n\t\tif (master == origin->mnt_master) {\n\t\t\tstruct mount *next = next_peer(m);\n\t\t\treturn (next == origin) ? NULL : next;\n\t\t} else if (m->mnt_slave.next != &master->mnt_slave_list)\n\t\t\treturn next_slave(m);\n\n\t\t/* back at master */\n\t\tm = master;\n\t}\n}\n\nstatic struct mount *next_group(struct mount *m, struct mount *origin)\n{\n\twhile (1) {\n\t\twhile (1) {\n\t\t\tstruct mount *next;\n\t\t\tif (!IS_MNT_NEW(m) && !list_empty(&m->mnt_slave_list))\n\t\t\t\treturn first_slave(m);\n\t\t\tnext = next_peer(m);\n\t\t\tif (m->mnt_group_id == origin->mnt_group_id) {\n\t\t\t\tif (next == origin)\n\t\t\t\t\treturn NULL;\n\t\t\t} else if (m->mnt_slave.next != &next->mnt_slave)\n\t\t\t\tbreak;\n\t\t\tm = next;\n\t\t}\n\t\t/* m is the last peer */\n\t\twhile (1) {\n\t\t\tstruct mount *master = m->mnt_master;\n\t\t\tif (m->mnt_slave.next != &master->mnt_slave_list)\n\t\t\t\treturn next_slave(m);\n\t\t\tm = next_peer(master);\n\t\t\tif (master->mnt_group_id == origin->mnt_group_id)\n\t\t\t\tbreak;\n\t\t\tif (master->mnt_slave.next == &m->mnt_slave)\n\t\t\t\tbreak;\n\t\t\tm = master;\n\t\t}\n\t\tif (m == origin)\n\t\t\treturn NULL;\n\t}\n}\n\n/* all accesses are serialized by namespace_sem */\nstatic struct user_namespace *user_ns;\nstatic struct mount *last_dest, *first_source, *last_source, *dest_master;\nstatic struct mountpoint *mp;\nstatic struct hlist_head *list;\n\nstatic inline bool peers(struct mount *m1, struct mount *m2)\n{\n\treturn m1->mnt_group_id == m2->mnt_group_id && m1->mnt_group_id;\n}\n\nstatic int propagate_one(struct mount *m)\n{\n\tstruct mount *child;\n\tint type;\n\t/* skip ones added by this propagate_mnt() */\n\tif (IS_MNT_NEW(m))\n\t\treturn 0;\n\t/* skip if mountpoint isn't covered by it */\n\tif (!is_subdir(mp->m_dentry, m->mnt.mnt_root))\n\t\treturn 0;\n\tif (peers(m, last_dest)) {\n\t\ttype = CL_MAKE_SHARED;\n\t} else {\n\t\tstruct mount *n, *p;\n\t\tbool done;\n\t\tfor (n = m; ; n = p) {\n\t\t\tp = n->mnt_master;\n\t\t\tif (p == dest_master || IS_MNT_MARKED(p))\n\t\t\t\tbreak;\n\t\t}\n\t\tdo {\n\t\t\tstruct mount *parent = last_source->mnt_parent;\n\t\t\tif (last_source == first_source)\n\t\t\t\tbreak;\n\t\t\tdone = parent->mnt_master == p;\n\t\t\tif (done && peers(n, parent))\n\t\t\t\tbreak;\n\t\t\tlast_source = last_source->mnt_master;\n\t\t} while (!done);\n\n\t\ttype = CL_SLAVE;\n\t\t/* beginning of peer group among the slaves? */\n\t\tif (IS_MNT_SHARED(m))\n\t\t\ttype |= CL_MAKE_SHARED;\n\t}\n\t\t\n\t/* Notice when we are propagating across user namespaces */\n\tif (m->mnt_ns->user_ns != user_ns)\n\t\ttype |= CL_UNPRIVILEGED;\n\tchild = copy_tree(last_source, last_source->mnt.mnt_root, type);\n\tif (IS_ERR(child))\n\t\treturn PTR_ERR(child);\n\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\tmnt_set_mountpoint(m, mp, child);\n\tlast_dest = m;\n\tlast_source = child;\n\tif (m->mnt_master != dest_master) {\n\t\tread_seqlock_excl(&mount_lock);\n\t\tSET_MNT_MARK(m->mnt_master);\n\t\tread_sequnlock_excl(&mount_lock);\n\t}\n\thlist_add_head(&child->mnt_hash, list);\n\treturn count_mounts(m->mnt_ns, child);\n}\n\n/*\n * mount 'source_mnt' under the destination 'dest_mnt' at\n * dentry 'dest_dentry'. And propagate that mount to\n * all the peer and slave mounts of 'dest_mnt'.\n * Link all the new mounts into a propagation tree headed at\n * source_mnt. Also link all the new mounts using ->mnt_list\n * headed at source_mnt's ->mnt_list\n *\n * @dest_mnt: destination mount.\n * @dest_dentry: destination dentry.\n * @source_mnt: source mount.\n * @tree_list : list of heads of trees to be attached.\n */\nint propagate_mnt(struct mount *dest_mnt, struct mountpoint *dest_mp,\n\t\t    struct mount *source_mnt, struct hlist_head *tree_list)\n{\n\tstruct mount *m, *n;\n\tint ret = 0;\n\n\t/*\n\t * we don't want to bother passing tons of arguments to\n\t * propagate_one(); everything is serialized by namespace_sem,\n\t * so globals will do just fine.\n\t */\n\tuser_ns = current->nsproxy->mnt_ns->user_ns;\n\tlast_dest = dest_mnt;\n\tfirst_source = source_mnt;\n\tlast_source = source_mnt;\n\tmp = dest_mp;\n\tlist = tree_list;\n\tdest_master = dest_mnt->mnt_master;\n\n\t/* all peers of dest_mnt, except dest_mnt itself */\n\tfor (n = next_peer(dest_mnt); n != dest_mnt; n = next_peer(n)) {\n\t\tret = propagate_one(n);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t/* all slave groups */\n\tfor (m = next_group(dest_mnt, dest_mnt); m;\n\t\t\tm = next_group(m, dest_mnt)) {\n\t\t/* everything in that slave group */\n\t\tn = m;\n\t\tdo {\n\t\t\tret = propagate_one(n);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tn = next_peer(n);\n\t\t} while (n != m);\n\t}\nout:\n\tread_seqlock_excl(&mount_lock);\n\thlist_for_each_entry(n, tree_list, mnt_hash) {\n\t\tm = n->mnt_parent;\n\t\tif (m->mnt_master != dest_mnt->mnt_master)\n\t\t\tCLEAR_MNT_MARK(m->mnt_master);\n\t}\n\tread_sequnlock_excl(&mount_lock);\n\treturn ret;\n}\n\n/*\n * return true if the refcount is greater than count\n */\nstatic inline int do_refcount_check(struct mount *mnt, int count)\n{\n\treturn mnt_get_count(mnt) > count;\n}\n\n/*\n * check if the mount 'mnt' can be unmounted successfully.\n * @mnt: the mount to be checked for unmount\n * NOTE: unmounting 'mnt' would naturally propagate to all\n * other mounts its parent propagates to.\n * Check if any of these mounts that **do not have submounts**\n * have more references than 'refcnt'. If so return busy.\n *\n * vfsmount lock must be held for write\n */\nint propagate_mount_busy(struct mount *mnt, int refcnt)\n{\n\tstruct mount *m, *child;\n\tstruct mount *parent = mnt->mnt_parent;\n\tint ret = 0;\n\n\tif (mnt == parent)\n\t\treturn do_refcount_check(mnt, refcnt);\n\n\t/*\n\t * quickly check if the current mount can be unmounted.\n\t * If not, we don't have to go checking for all other\n\t * mounts\n\t */\n\tif (!list_empty(&mnt->mnt_mounts) || do_refcount_check(mnt, refcnt))\n\t\treturn 1;\n\n\tfor (m = propagation_next(parent, parent); m;\n\t     \t\tm = propagation_next(m, parent)) {\n\t\tchild = __lookup_mnt_last(&m->mnt, mnt->mnt_mountpoint);\n\t\tif (child && list_empty(&child->mnt_mounts) &&\n\t\t    (ret = do_refcount_check(child, 1)))\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\n/*\n * Clear MNT_LOCKED when it can be shown to be safe.\n *\n * mount_lock lock must be held for write\n */\nvoid propagate_mount_unlock(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m, *child;\n\n\tBUG_ON(parent == mnt);\n\n\tfor (m = propagation_next(parent, parent); m;\n\t\t\tm = propagation_next(m, parent)) {\n\t\tchild = __lookup_mnt_last(&m->mnt, mnt->mnt_mountpoint);\n\t\tif (child)\n\t\t\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n}\n\n/*\n * Mark all mounts that the MNT_LOCKED logic will allow to be unmounted.\n */\nstatic void mark_umount_candidates(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\n\tBUG_ON(parent == mnt);\n\n\tfor (m = propagation_next(parent, parent); m;\n\t\t\tm = propagation_next(m, parent)) {\n\t\tstruct mount *child = __lookup_mnt_last(&m->mnt,\n\t\t\t\t\t\tmnt->mnt_mountpoint);\n\t\tif (child && (!IS_MNT_LOCKED(child) || IS_MNT_MARKED(m))) {\n\t\t\tSET_MNT_MARK(child);\n\t\t}\n\t}\n}\n\n/*\n * NOTE: unmounting 'mnt' naturally propagates to all other mounts its\n * parent propagates to.\n */\nstatic void __propagate_umount(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\n\tBUG_ON(parent == mnt);\n\n\tfor (m = propagation_next(parent, parent); m;\n\t\t\tm = propagation_next(m, parent)) {\n\n\t\tstruct mount *child = __lookup_mnt_last(&m->mnt,\n\t\t\t\t\t\tmnt->mnt_mountpoint);\n\t\t/*\n\t\t * umount the child only if the child has no children\n\t\t * and the child is marked safe to unmount.\n\t\t */\n\t\tif (!child || !IS_MNT_MARKED(child))\n\t\t\tcontinue;\n\t\tCLEAR_MNT_MARK(child);\n\t\tif (list_empty(&child->mnt_mounts)) {\n\t\t\tlist_del_init(&child->mnt_child);\n\t\t\tchild->mnt.mnt_flags |= MNT_UMOUNT;\n\t\t\tlist_move_tail(&child->mnt_list, &mnt->mnt_list);\n\t\t}\n\t}\n}\n\n/*\n * collect all mounts that receive propagation from the mount in @list,\n * and return these additional mounts in the same list.\n * @list: the list of mounts to be unmounted.\n *\n * vfsmount lock must be held for write\n */\nint propagate_umount(struct list_head *list)\n{\n\tstruct mount *mnt;\n\n\tlist_for_each_entry_reverse(mnt, list, mnt_list)\n\t\tmark_umount_candidates(mnt);\n\n\tlist_for_each_entry(mnt, list, mnt_list)\n\t\t__propagate_umount(mnt);\n\treturn 0;\n}\n", "/*\n *  linux/fs/pnode.h\n *\n * (C) Copyright IBM Corporation 2005.\n *\tReleased under GPL v2.\n *\n */\n#ifndef _LINUX_PNODE_H\n#define _LINUX_PNODE_H\n\n#include <linux/list.h>\n#include \"mount.h\"\n\n#define IS_MNT_SHARED(m) ((m)->mnt.mnt_flags & MNT_SHARED)\n#define IS_MNT_SLAVE(m) ((m)->mnt_master)\n#define IS_MNT_NEW(m)  (!(m)->mnt_ns)\n#define CLEAR_MNT_SHARED(m) ((m)->mnt.mnt_flags &= ~MNT_SHARED)\n#define IS_MNT_UNBINDABLE(m) ((m)->mnt.mnt_flags & MNT_UNBINDABLE)\n#define IS_MNT_MARKED(m) ((m)->mnt.mnt_flags & MNT_MARKED)\n#define SET_MNT_MARK(m) ((m)->mnt.mnt_flags |= MNT_MARKED)\n#define CLEAR_MNT_MARK(m) ((m)->mnt.mnt_flags &= ~MNT_MARKED)\n#define IS_MNT_LOCKED(m) ((m)->mnt.mnt_flags & MNT_LOCKED)\n\n#define CL_EXPIRE    \t\t0x01\n#define CL_SLAVE     \t\t0x02\n#define CL_COPY_UNBINDABLE\t0x04\n#define CL_MAKE_SHARED \t\t0x08\n#define CL_PRIVATE \t\t0x10\n#define CL_SHARED_TO_SLAVE\t0x20\n#define CL_UNPRIVILEGED\t\t0x40\n#define CL_COPY_MNT_NS_FILE\t0x80\n\n#define CL_COPY_ALL\t\t(CL_COPY_UNBINDABLE | CL_COPY_MNT_NS_FILE)\n\nstatic inline void set_mnt_shared(struct mount *mnt)\n{\n\tmnt->mnt.mnt_flags &= ~MNT_SHARED_MASK;\n\tmnt->mnt.mnt_flags |= MNT_SHARED;\n}\n\nvoid change_mnt_propagation(struct mount *, int);\nint propagate_mnt(struct mount *, struct mountpoint *, struct mount *,\n\t\tstruct hlist_head *);\nint propagate_umount(struct list_head *);\nint propagate_mount_busy(struct mount *, int);\nvoid propagate_mount_unlock(struct mount *);\nvoid mnt_release_group_id(struct mount *);\nint get_dominating_id(struct mount *mnt, const struct path *root);\nunsigned int mnt_get_count(struct mount *mnt);\nvoid mnt_set_mountpoint(struct mount *, struct mountpoint *,\n\t\t\tstruct mount *);\nstruct mount *copy_tree(struct mount *, struct dentry *, int);\nbool is_path_reachable(struct mount *, struct dentry *,\n\t\t\t const struct path *root);\nint count_mounts(struct mnt_namespace *ns, struct mount *mnt);\n#endif /* _LINUX_PNODE_H */\n", "/*\n *\n * Definitions for mount interface. This describes the in the kernel build \n * linkedlist with mounted filesystems.\n *\n * Author:  Marco van Wieringen <mvw@planets.elm.net>\n *\n */\n#ifndef _LINUX_MOUNT_H\n#define _LINUX_MOUNT_H\n\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/nodemask.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/atomic.h>\n\nstruct super_block;\nstruct vfsmount;\nstruct dentry;\nstruct mnt_namespace;\n\n#define MNT_NOSUID\t0x01\n#define MNT_NODEV\t0x02\n#define MNT_NOEXEC\t0x04\n#define MNT_NOATIME\t0x08\n#define MNT_NODIRATIME\t0x10\n#define MNT_RELATIME\t0x20\n#define MNT_READONLY\t0x40\t/* does the user want this to be r/o? */\n\n#define MNT_SHRINKABLE\t0x100\n#define MNT_WRITE_HOLD\t0x200\n\n#define MNT_SHARED\t0x1000\t/* if the vfsmount is a shared mount */\n#define MNT_UNBINDABLE\t0x2000\t/* if the vfsmount is a unbindable mount */\n/*\n * MNT_SHARED_MASK is the set of flags that should be cleared when a\n * mount becomes shared.  Currently, this is only the flag that says a\n * mount cannot be bind mounted, since this is how we create a mount\n * that shares events with another mount.  If you add a new MNT_*\n * flag, consider how it interacts with shared mounts.\n */\n#define MNT_SHARED_MASK\t(MNT_UNBINDABLE)\n#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \\\n\t\t\t\t | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \\\n\t\t\t\t | MNT_READONLY)\n#define MNT_ATIME_MASK (MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME )\n\n#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \\\n\t\t\t    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)\n\n#define MNT_INTERNAL\t0x4000\n\n#define MNT_LOCK_ATIME\t\t0x040000\n#define MNT_LOCK_NOEXEC\t\t0x080000\n#define MNT_LOCK_NOSUID\t\t0x100000\n#define MNT_LOCK_NODEV\t\t0x200000\n#define MNT_LOCK_READONLY\t0x400000\n#define MNT_LOCKED\t\t0x800000\n#define MNT_DOOMED\t\t0x1000000\n#define MNT_SYNC_UMOUNT\t\t0x2000000\n#define MNT_MARKED\t\t0x4000000\n#define MNT_UMOUNT\t\t0x8000000\n\nstruct vfsmount {\n\tstruct dentry *mnt_root;\t/* root of the mounted tree */\n\tstruct super_block *mnt_sb;\t/* pointer to superblock */\n\tint mnt_flags;\n};\n\nstruct file; /* forward dec */\nstruct path;\n\nextern int mnt_want_write(struct vfsmount *mnt);\nextern int mnt_want_write_file(struct file *file);\nextern int mnt_clone_write(struct vfsmount *mnt);\nextern void mnt_drop_write(struct vfsmount *mnt);\nextern void mnt_drop_write_file(struct file *file);\nextern void mntput(struct vfsmount *mnt);\nextern struct vfsmount *mntget(struct vfsmount *mnt);\nextern struct vfsmount *mnt_clone_internal(struct path *path);\nextern int __mnt_is_readonly(struct vfsmount *mnt);\nextern bool mnt_may_suid(struct vfsmount *mnt);\n\nstruct path;\nextern struct vfsmount *clone_private_mount(struct path *path);\n\nstruct file_system_type;\nextern struct vfsmount *vfs_kern_mount(struct file_system_type *type,\n\t\t\t\t      int flags, const char *name,\n\t\t\t\t      void *data);\n\nextern void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list);\nextern void mark_mounts_for_expiry(struct list_head *mounts);\n\nextern dev_t name_to_dev_t(const char *name);\n\nextern unsigned int sysctl_mount_max;\n\n#endif /* _LINUX_MOUNT_H */\n", "/*\n * sysctl.c: General linux system control interface\n *\n * Begun 24 March 1995, Stephen Tweedie\n * Added /proc support, Dec 1995\n * Added bdflush entry and intvec min/max checking, 2/23/96, Tom Dyas.\n * Added hooks for /proc/sys/net (minor, minor patch), 96/4/1, Mike Shaver.\n * Added kernel/java-{interpreter,appletviewer}, 96/5/10, Mike Shaver.\n * Dynamic registration fixes, Stephen Tweedie.\n * Added kswapd-interval, ctrl-alt-del, printk stuff, 1/8/97, Chris Horn.\n * Made sysctl support optional via CONFIG_SYSCTL, 1/10/97, Chris\n *  Horn.\n * Added proc_doulongvec_ms_jiffies_minmax, 09/08/99, Carlos H. Bauer.\n * Added proc_doulongvec_minmax, 09/08/99, Carlos H. Bauer.\n * Changed linked lists to use list.h instead of lists.h, 02/24/00, Bill\n *  Wendling.\n * The list_for_each() macro wasn't appropriate for the sysctl loop.\n *  Removed it and replaced it with older style, 03/23/00, Bill Wendling\n */\n\n#include <linux/module.h>\n#include <linux/aio.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/bitmap.h>\n#include <linux/signal.h>\n#include <linux/printk.h>\n#include <linux/proc_fs.h>\n#include <linux/security.h>\n#include <linux/ctype.h>\n#include <linux/kmemcheck.h>\n#include <linux/kmemleak.h>\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/kobject.h>\n#include <linux/net.h>\n#include <linux/sysrq.h>\n#include <linux/highuid.h>\n#include <linux/writeback.h>\n#include <linux/ratelimit.h>\n#include <linux/compaction.h>\n#include <linux/hugetlb.h>\n#include <linux/initrd.h>\n#include <linux/key.h>\n#include <linux/times.h>\n#include <linux/limits.h>\n#include <linux/dcache.h>\n#include <linux/dnotify.h>\n#include <linux/syscalls.h>\n#include <linux/vmstat.h>\n#include <linux/nfs_fs.h>\n#include <linux/acpi.h>\n#include <linux/reboot.h>\n#include <linux/ftrace.h>\n#include <linux/perf_event.h>\n#include <linux/kprobes.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/oom.h>\n#include <linux/kmod.h>\n#include <linux/capability.h>\n#include <linux/binfmts.h>\n#include <linux/sched/sysctl.h>\n#include <linux/kexec.h>\n#include <linux/bpf.h>\n#include <linux/mount.h>\n\n#include <asm/uaccess.h>\n#include <asm/processor.h>\n\n#ifdef CONFIG_X86\n#include <asm/nmi.h>\n#include <asm/stacktrace.h>\n#include <asm/io.h>\n#endif\n#ifdef CONFIG_SPARC\n#include <asm/setup.h>\n#endif\n#ifdef CONFIG_BSD_PROCESS_ACCT\n#include <linux/acct.h>\n#endif\n#ifdef CONFIG_RT_MUTEXES\n#include <linux/rtmutex.h>\n#endif\n#if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_LOCK_STAT)\n#include <linux/lockdep.h>\n#endif\n#ifdef CONFIG_CHR_DEV_SG\n#include <scsi/sg.h>\n#endif\n\n#ifdef CONFIG_LOCKUP_DETECTOR\n#include <linux/nmi.h>\n#endif\n\n#if defined(CONFIG_SYSCTL)\n\n/* External variables not in a header file. */\nextern int suid_dumpable;\n#ifdef CONFIG_COREDUMP\nextern int core_uses_pid;\nextern char core_pattern[];\nextern unsigned int core_pipe_limit;\n#endif\nextern int pid_max;\nextern int pid_max_min, pid_max_max;\nextern int percpu_pagelist_fraction;\nextern int compat_log;\nextern int latencytop_enabled;\nextern int sysctl_nr_open_min, sysctl_nr_open_max;\n#ifndef CONFIG_MMU\nextern int sysctl_nr_trim_pages;\n#endif\n\n/* Constants used for minimum and  maximum */\n#ifdef CONFIG_LOCKUP_DETECTOR\nstatic int sixty = 60;\n#endif\n\nstatic int __maybe_unused neg_one = -1;\n\nstatic int zero;\nstatic int __maybe_unused one = 1;\nstatic int __maybe_unused two = 2;\nstatic int __maybe_unused four = 4;\nstatic unsigned long one_ul = 1;\nstatic int one_hundred = 100;\nstatic int one_thousand = 1000;\n#ifdef CONFIG_PRINTK\nstatic int ten_thousand = 10000;\n#endif\n#ifdef CONFIG_PERF_EVENTS\nstatic int six_hundred_forty_kb = 640 * 1024;\n#endif\n\n/* this is needed for the proc_doulongvec_minmax of vm_dirty_bytes */\nstatic unsigned long dirty_bytes_min = 2 * PAGE_SIZE;\n\n/* this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID */\nstatic int maxolduid = 65535;\nstatic int minolduid;\n\nstatic int ngroups_max = NGROUPS_MAX;\nstatic const int cap_last_cap = CAP_LAST_CAP;\n\n/*this is needed for proc_doulongvec_minmax of sysctl_hung_task_timeout_secs */\n#ifdef CONFIG_DETECT_HUNG_TASK\nstatic unsigned long hung_task_timeout_max = (LONG_MAX/HZ);\n#endif\n\n#ifdef CONFIG_INOTIFY_USER\n#include <linux/inotify.h>\n#endif\n#ifdef CONFIG_SPARC\n#endif\n\n#ifdef __hppa__\nextern int pwrsw_enabled;\n#endif\n\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\nextern int unaligned_enabled;\n#endif\n\n#ifdef CONFIG_IA64\nextern int unaligned_dump_stack;\n#endif\n\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\nextern int no_unaligned_warning;\n#endif\n\n#ifdef CONFIG_PROC_SYSCTL\n\n#define SYSCTL_WRITES_LEGACY\t-1\n#define SYSCTL_WRITES_WARN\t 0\n#define SYSCTL_WRITES_STRICT\t 1\n\nstatic int sysctl_writes_strict = SYSCTL_WRITES_STRICT;\n\nstatic int proc_do_cad_pid(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_taint(struct ctl_table *table, int write,\n\t\t\t       void __user *buffer, size_t *lenp, loff_t *ppos);\n#endif\n\n#ifdef CONFIG_PRINTK\nstatic int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n#endif\n\nstatic int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n#ifdef CONFIG_COREDUMP\nstatic int proc_dostring_coredump(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\n#endif\n\n#ifdef CONFIG_MAGIC_SYSRQ\n/* Note: sysrq code uses it's own private copy */\nstatic int __sysrq_enabled = CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE;\n\nstatic int sysrq_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tint error;\n\n\terror = proc_dointvec(table, write, buffer, lenp, ppos);\n\tif (error)\n\t\treturn error;\n\n\tif (write)\n\t\tsysrq_toggle_support(__sysrq_enabled);\n\n\treturn 0;\n}\n\n#endif\n\nstatic struct ctl_table kern_table[];\nstatic struct ctl_table vm_table[];\nstatic struct ctl_table fs_table[];\nstatic struct ctl_table debug_table[];\nstatic struct ctl_table dev_table[];\nextern struct ctl_table random_table[];\n#ifdef CONFIG_EPOLL\nextern struct ctl_table epoll_table[];\n#endif\n\n#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT\nint sysctl_legacy_va_layout;\n#endif\n\n/* The default sysctl tables: */\n\nstatic struct ctl_table sysctl_base_table[] = {\n\t{\n\t\t.procname\t= \"kernel\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= kern_table,\n\t},\n\t{\n\t\t.procname\t= \"vm\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= vm_table,\n\t},\n\t{\n\t\t.procname\t= \"fs\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= fs_table,\n\t},\n\t{\n\t\t.procname\t= \"debug\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= debug_table,\n\t},\n\t{\n\t\t.procname\t= \"dev\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= dev_table,\n\t},\n\t{ }\n};\n\n#ifdef CONFIG_SCHED_DEBUG\nstatic int min_sched_granularity_ns = 100000;\t\t/* 100 usecs */\nstatic int max_sched_granularity_ns = NSEC_PER_SEC;\t/* 1 second */\nstatic int min_wakeup_granularity_ns;\t\t\t/* 0 usecs */\nstatic int max_wakeup_granularity_ns = NSEC_PER_SEC;\t/* 1 second */\n#ifdef CONFIG_SMP\nstatic int min_sched_tunable_scaling = SCHED_TUNABLESCALING_NONE;\nstatic int max_sched_tunable_scaling = SCHED_TUNABLESCALING_END-1;\n#endif /* CONFIG_SMP */\n#endif /* CONFIG_SCHED_DEBUG */\n\n#ifdef CONFIG_COMPACTION\nstatic int min_extfrag_threshold;\nstatic int max_extfrag_threshold = 1000;\n#endif\n\nstatic struct ctl_table kern_table[] = {\n\t{\n\t\t.procname\t= \"sched_child_runs_first\",\n\t\t.data\t\t= &sysctl_sched_child_runs_first,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_SCHED_DEBUG\n\t{\n\t\t.procname\t= \"sched_min_granularity_ns\",\n\t\t.data\t\t= &sysctl_sched_min_granularity,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_sched_granularity_ns,\n\t\t.extra2\t\t= &max_sched_granularity_ns,\n\t},\n\t{\n\t\t.procname\t= \"sched_latency_ns\",\n\t\t.data\t\t= &sysctl_sched_latency,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_sched_granularity_ns,\n\t\t.extra2\t\t= &max_sched_granularity_ns,\n\t},\n\t{\n\t\t.procname\t= \"sched_wakeup_granularity_ns\",\n\t\t.data\t\t= &sysctl_sched_wakeup_granularity,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_wakeup_granularity_ns,\n\t\t.extra2\t\t= &max_wakeup_granularity_ns,\n\t},\n#ifdef CONFIG_SMP\n\t{\n\t\t.procname\t= \"sched_tunable_scaling\",\n\t\t.data\t\t= &sysctl_sched_tunable_scaling,\n\t\t.maxlen\t\t= sizeof(enum sched_tunable_scaling),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_proc_update_handler,\n\t\t.extra1\t\t= &min_sched_tunable_scaling,\n\t\t.extra2\t\t= &max_sched_tunable_scaling,\n\t},\n\t{\n\t\t.procname\t= \"sched_migration_cost_ns\",\n\t\t.data\t\t= &sysctl_sched_migration_cost,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sched_nr_migrate\",\n\t\t.data\t\t= &sysctl_sched_nr_migrate,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sched_time_avg_ms\",\n\t\t.data\t\t= &sysctl_sched_time_avg,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sched_shares_window_ns\",\n\t\t.data\t\t= &sysctl_sched_shares_window,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_SCHEDSTATS\n\t{\n\t\t.procname\t= \"sched_schedstats\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_schedstats,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif /* CONFIG_SCHEDSTATS */\n#endif /* CONFIG_SMP */\n#ifdef CONFIG_NUMA_BALANCING\n\t{\n\t\t.procname\t= \"numa_balancing_scan_delay_ms\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_delay,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing_scan_period_min_ms\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_period_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing_scan_period_max_ms\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_period_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing_scan_size_mb\",\n\t\t.data\t\t= &sysctl_numa_balancing_scan_size,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"numa_balancing\",\n\t\t.data\t\t= NULL, /* filled in by handler */\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_numa_balancing,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_SCHED_DEBUG */\n\t{\n\t\t.procname\t= \"sched_rt_period_us\",\n\t\t.data\t\t= &sysctl_sched_rt_period,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_rt_handler,\n\t},\n\t{\n\t\t.procname\t= \"sched_rt_runtime_us\",\n\t\t.data\t\t= &sysctl_sched_rt_runtime,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_rt_handler,\n\t},\n\t{\n\t\t.procname\t= \"sched_rr_timeslice_ms\",\n\t\t.data\t\t= &sched_rr_timeslice,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sched_rr_handler,\n\t},\n#ifdef CONFIG_SCHED_AUTOGROUP\n\t{\n\t\t.procname\t= \"sched_autogroup_enabled\",\n\t\t.data\t\t= &sysctl_sched_autogroup_enabled,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_CFS_BANDWIDTH\n\t{\n\t\t.procname\t= \"sched_cfs_bandwidth_slice_us\",\n\t\t.data\t\t= &sysctl_sched_cfs_bandwidth_slice,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_PROVE_LOCKING\n\t{\n\t\t.procname\t= \"prove_locking\",\n\t\t.data\t\t= &prove_locking,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_LOCK_STAT\n\t{\n\t\t.procname\t= \"lock_stat\",\n\t\t.data\t\t= &lock_stat,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"panic\",\n\t\t.data\t\t= &panic_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_COREDUMP\n\t{\n\t\t.procname\t= \"core_uses_pid\",\n\t\t.data\t\t= &core_uses_pid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"core_pattern\",\n\t\t.data\t\t= core_pattern,\n\t\t.maxlen\t\t= CORENAME_MAX_SIZE,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring_coredump,\n\t},\n\t{\n\t\t.procname\t= \"core_pipe_limit\",\n\t\t.data\t\t= &core_pipe_limit,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_PROC_SYSCTL\n\t{\n\t\t.procname\t= \"tainted\",\n\t\t.maxlen \t= sizeof(long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_taint,\n\t},\n\t{\n\t\t.procname\t= \"sysctl_writes_strict\",\n\t\t.data\t\t= &sysctl_writes_strict,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &neg_one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_LATENCYTOP\n\t{\n\t\t.procname\t= \"latencytop\",\n\t\t.data\t\t= &latencytop_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_latencytop,\n\t},\n#endif\n#ifdef CONFIG_BLK_DEV_INITRD\n\t{\n\t\t.procname\t= \"real-root-dev\",\n\t\t.data\t\t= &real_root_dev,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"print-fatal-signals\",\n\t\t.data\t\t= &print_fatal_signals,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_SPARC\n\t{\n\t\t.procname\t= \"reboot-cmd\",\n\t\t.data\t\t= reboot_command,\n\t\t.maxlen\t\t= 256,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n\t{\n\t\t.procname\t= \"stop-a\",\n\t\t.data\t\t= &stop_a_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"scons-poweroff\",\n\t\t.data\t\t= &scons_pwroff,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_SPARC64\n\t{\n\t\t.procname\t= \"tsb-ratio\",\n\t\t.data\t\t= &sysctl_tsb_ratio,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef __hppa__\n\t{\n\t\t.procname\t= \"soft-power\",\n\t\t.data\t\t= &pwrsw_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t \t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\n\t{\n\t\t.procname\t= \"unaligned-trap\",\n\t\t.data\t\t= &unaligned_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"ctrl-alt-del\",\n\t\t.data\t\t= &C_A_D,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_FUNCTION_TRACER\n\t{\n\t\t.procname\t= \"ftrace_enabled\",\n\t\t.data\t\t= &ftrace_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= ftrace_enable_sysctl,\n\t},\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t{\n\t\t.procname\t= \"stack_tracer_enabled\",\n\t\t.data\t\t= &stack_tracer_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= stack_trace_sysctl,\n\t},\n#endif\n#ifdef CONFIG_TRACING\n\t{\n\t\t.procname\t= \"ftrace_dump_on_oops\",\n\t\t.data\t\t= &ftrace_dump_on_oops,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"traceoff_on_warning\",\n\t\t.data\t\t= &__disable_trace_on_warning,\n\t\t.maxlen\t\t= sizeof(__disable_trace_on_warning),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"tracepoint_printk\",\n\t\t.data\t\t= &tracepoint_printk,\n\t\t.maxlen\t\t= sizeof(tracepoint_printk),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_KEXEC_CORE\n\t{\n\t\t.procname\t= \"kexec_load_disabled\",\n\t\t.data\t\t= &kexec_load_disabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t/* only handle a transition from default \"0\" to \"1\" */\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_MODULES\n\t{\n\t\t.procname\t= \"modprobe\",\n\t\t.data\t\t= &modprobe_path,\n\t\t.maxlen\t\t= KMOD_PATH_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n\t{\n\t\t.procname\t= \"modules_disabled\",\n\t\t.data\t\t= &modules_disabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t/* only handle a transition from default \"0\" to \"1\" */\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_UEVENT_HELPER\n\t{\n\t\t.procname\t= \"hotplug\",\n\t\t.data\t\t= &uevent_helper,\n\t\t.maxlen\t\t= UEVENT_HELPER_PATH_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n#endif\n#ifdef CONFIG_CHR_DEV_SG\n\t{\n\t\t.procname\t= \"sg-big-buff\",\n\t\t.data\t\t= &sg_big_buff,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_BSD_PROCESS_ACCT\n\t{\n\t\t.procname\t= \"acct\",\n\t\t.data\t\t= &acct_parm,\n\t\t.maxlen\t\t= 3*sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_MAGIC_SYSRQ\n\t{\n\t\t.procname\t= \"sysrq\",\n\t\t.data\t\t= &__sysrq_enabled,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysrq_sysctl_handler,\n\t},\n#endif\n#ifdef CONFIG_PROC_SYSCTL\n\t{\n\t\t.procname\t= \"cad_pid\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_do_cad_pid,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"threads-max\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_max_threads,\n\t},\n\t{\n\t\t.procname\t= \"random\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= random_table,\n\t},\n\t{\n\t\t.procname\t= \"usermodehelper\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= usermodehelper_table,\n\t},\n\t{\n\t\t.procname\t= \"overflowuid\",\n\t\t.data\t\t= &overflowuid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n\t{\n\t\t.procname\t= \"overflowgid\",\n\t\t.data\t\t= &overflowgid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n#ifdef CONFIG_S390\n#ifdef CONFIG_MATHEMU\n\t{\n\t\t.procname\t= \"ieee_emulation_warnings\",\n\t\t.data\t\t= &sysctl_ieee_emulation_warnings,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"userprocess_debug\",\n\t\t.data\t\t= &show_unhandled_signals,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pid_max\",\n\t\t.data\t\t= &pid_max,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &pid_max_min,\n\t\t.extra2\t\t= &pid_max_max,\n\t},\n\t{\n\t\t.procname\t= \"panic_on_oops\",\n\t\t.data\t\t= &panic_on_oops,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#if defined CONFIG_PRINTK\n\t{\n\t\t.procname\t= \"printk\",\n\t\t.data\t\t= &console_loglevel,\n\t\t.maxlen\t\t= 4*sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"printk_ratelimit\",\n\t\t.data\t\t= &printk_ratelimit_state.interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"printk_ratelimit_burst\",\n\t\t.data\t\t= &printk_ratelimit_state.burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"printk_delay\",\n\t\t.data\t\t= &printk_delay_msec,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &ten_thousand,\n\t},\n\t{\n\t\t.procname\t= \"printk_devkmsg\",\n\t\t.data\t\t= devkmsg_log_str,\n\t\t.maxlen\t\t= DEVKMSG_STR_MAX_SIZE,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= devkmsg_sysctl_set_loglvl,\n\t},\n\t{\n\t\t.procname\t= \"dmesg_restrict\",\n\t\t.data\t\t= &dmesg_restrict,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_sysadmin,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"kptr_restrict\",\n\t\t.data\t\t= &kptr_restrict,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_sysadmin,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"ngroups_max\",\n\t\t.data\t\t= &ngroups_max,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cap_last_cap\",\n\t\t.data\t\t= (void *)&cap_last_cap,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#if defined(CONFIG_LOCKUP_DETECTOR)\n\t{\n\t\t.procname       = \"watchdog\",\n\t\t.data           = &watchdog_user_enabled,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_watchdog,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"watchdog_thresh\",\n\t\t.data\t\t= &watchdog_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_watchdog_thresh,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &sixty,\n\t},\n\t{\n\t\t.procname       = \"nmi_watchdog\",\n\t\t.data           = &nmi_watchdog_enabled,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_nmi_watchdog,\n\t\t.extra1\t\t= &zero,\n#if defined(CONFIG_HAVE_NMI_WATCHDOG) || defined(CONFIG_HARDLOCKUP_DETECTOR)\n\t\t.extra2\t\t= &one,\n#else\n\t\t.extra2\t\t= &zero,\n#endif\n\t},\n\t{\n\t\t.procname       = \"soft_watchdog\",\n\t\t.data           = &soft_watchdog_enabled,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_soft_watchdog,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"watchdog_cpumask\",\n\t\t.data\t\t= &watchdog_cpumask_bits,\n\t\t.maxlen\t\t= NR_CPUS,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_watchdog_cpumask,\n\t},\n\t{\n\t\t.procname\t= \"softlockup_panic\",\n\t\t.data\t\t= &softlockup_panic,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\n\t{\n\t\t.procname\t= \"hardlockup_panic\",\n\t\t.data\t\t= &hardlockup_panic,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_SMP\n\t{\n\t\t.procname\t= \"softlockup_all_cpu_backtrace\",\n\t\t.data\t\t= &sysctl_softlockup_all_cpu_backtrace,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"hardlockup_all_cpu_backtrace\",\n\t\t.data\t\t= &sysctl_hardlockup_all_cpu_backtrace,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif /* CONFIG_SMP */\n#endif\n#if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_X86)\n\t{\n\t\t.procname       = \"unknown_nmi_panic\",\n\t\t.data           = &unknown_nmi_panic,\n\t\t.maxlen         = sizeof (int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = proc_dointvec,\n\t},\n#endif\n#if defined(CONFIG_X86)\n\t{\n\t\t.procname\t= \"panic_on_unrecovered_nmi\",\n\t\t.data\t\t= &panic_on_unrecovered_nmi,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"panic_on_io_nmi\",\n\t\t.data\t\t= &panic_on_io_nmi,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#ifdef CONFIG_DEBUG_STACKOVERFLOW\n\t{\n\t\t.procname\t= \"panic_on_stackoverflow\",\n\t\t.data\t\t= &sysctl_panic_on_stackoverflow,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"bootloader_type\",\n\t\t.data\t\t= &bootloader_type,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"bootloader_version\",\n\t\t.data\t\t= &bootloader_version,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"kstack_depth_to_print\",\n\t\t.data\t\t= &kstack_depth_to_print,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"io_delay_type\",\n\t\t.data\t\t= &io_delay_type,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#if defined(CONFIG_MMU)\n\t{\n\t\t.procname\t= \"randomize_va_space\",\n\t\t.data\t\t= &randomize_va_space,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#if defined(CONFIG_S390) && defined(CONFIG_SMP)\n\t{\n\t\t.procname\t= \"spin_retry\",\n\t\t.data\t\t= &spin_retry,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#if\tdefined(CONFIG_ACPI_SLEEP) && defined(CONFIG_X86)\n\t{\n\t\t.procname\t= \"acpi_video_flags\",\n\t\t.data\t\t= &acpi_realmode_flags,\n\t\t.maxlen\t\t= sizeof (unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#endif\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\n\t{\n\t\t.procname\t= \"ignore-unaligned-usertrap\",\n\t\t.data\t\t= &no_unaligned_warning,\n\t\t.maxlen\t\t= sizeof (int),\n\t \t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_IA64\n\t{\n\t\t.procname\t= \"unaligned-dump-stack\",\n\t\t.data\t\t= &unaligned_dump_stack,\n\t\t.maxlen\t\t= sizeof (int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n\t{\n\t\t.procname\t= \"hung_task_panic\",\n\t\t.data\t\t= &sysctl_hung_task_panic,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"hung_task_check_count\",\n\t\t.data\t\t= &sysctl_hung_task_check_count,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"hung_task_timeout_secs\",\n\t\t.data\t\t= &sysctl_hung_task_timeout_secs,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dohung_task_timeout_secs,\n\t\t.extra2\t\t= &hung_task_timeout_max,\n\t},\n\t{\n\t\t.procname\t= \"hung_task_warnings\",\n\t\t.data\t\t= &sysctl_hung_task_warnings,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &neg_one,\n\t},\n#endif\n#ifdef CONFIG_COMPAT\n\t{\n\t\t.procname\t= \"compat-log\",\n\t\t.data\t\t= &compat_log,\n\t\t.maxlen\t\t= sizeof (int),\n\t \t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_RT_MUTEXES\n\t{\n\t\t.procname\t= \"max_lock_depth\",\n\t\t.data\t\t= &max_lock_depth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"poweroff_cmd\",\n\t\t.data\t\t= &poweroff_cmd,\n\t\t.maxlen\t\t= POWEROFF_CMD_PATH_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dostring,\n\t},\n#ifdef CONFIG_KEYS\n\t{\n\t\t.procname\t= \"keys\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= key_sysctls,\n\t},\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\t/*\n\t * User-space scripts rely on the existence of this file\n\t * as a feature check for perf_events being enabled.\n\t *\n\t * So it's an ABI, do not remove!\n\t */\n\t{\n\t\t.procname\t= \"perf_event_paranoid\",\n\t\t.data\t\t= &sysctl_perf_event_paranoid,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_paranoid),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_mlock_kb\",\n\t\t.data\t\t= &sysctl_perf_event_mlock,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_mlock),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_max_sample_rate\",\n\t\t.data\t\t= &sysctl_perf_event_sample_rate,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_sample_rate),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_proc_update_handler,\n\t\t.extra1\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"perf_cpu_time_max_percent\",\n\t\t.data\t\t= &sysctl_perf_cpu_time_max_percent,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_cpu_time_max_percent),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_cpu_time_max_percent_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_max_stack\",\n\t\t.data\t\t= &sysctl_perf_event_max_stack,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_max_stack),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_event_max_stack_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &six_hundred_forty_kb,\n\t},\n\t{\n\t\t.procname\t= \"perf_event_max_contexts_per_stack\",\n\t\t.data\t\t= &sysctl_perf_event_max_contexts_per_stack,\n\t\t.maxlen\t\t= sizeof(sysctl_perf_event_max_contexts_per_stack),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= perf_event_max_stack_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_thousand,\n\t},\n#endif\n#ifdef CONFIG_KMEMCHECK\n\t{\n\t\t.procname\t= \"kmemcheck\",\n\t\t.data\t\t= &kmemcheck_enabled,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"panic_on_warn\",\n\t\t.data\t\t= &panic_on_warn,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\n\t{\n\t\t.procname\t= \"timer_migration\",\n\t\t.data\t\t= &sysctl_timer_migration,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= timer_migration_handler,\n\t},\n#endif\n#ifdef CONFIG_BPF_SYSCALL\n\t{\n\t\t.procname\t= \"unprivileged_bpf_disabled\",\n\t\t.data\t\t= &sysctl_unprivileged_bpf_disabled,\n\t\t.maxlen\t\t= sizeof(sysctl_unprivileged_bpf_disabled),\n\t\t.mode\t\t= 0644,\n\t\t/* only handle a transition from default \"0\" to \"1\" */\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#if defined(CONFIG_TREE_RCU) || defined(CONFIG_PREEMPT_RCU)\n\t{\n\t\t.procname\t= \"panic_on_rcu_stall\",\n\t\t.data\t\t= &sysctl_panic_on_rcu_stall,\n\t\t.maxlen\t\t= sizeof(sysctl_panic_on_rcu_stall),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n\t{ }\n};\n\nstatic struct ctl_table vm_table[] = {\n\t{\n\t\t.procname\t= \"overcommit_memory\",\n\t\t.data\t\t= &sysctl_overcommit_memory,\n\t\t.maxlen\t\t= sizeof(sysctl_overcommit_memory),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n\t{\n\t\t.procname\t= \"panic_on_oom\",\n\t\t.data\t\t= &sysctl_panic_on_oom,\n\t\t.maxlen\t\t= sizeof(sysctl_panic_on_oom),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n\t{\n\t\t.procname\t= \"oom_kill_allocating_task\",\n\t\t.data\t\t= &sysctl_oom_kill_allocating_task,\n\t\t.maxlen\t\t= sizeof(sysctl_oom_kill_allocating_task),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"oom_dump_tasks\",\n\t\t.data\t\t= &sysctl_oom_dump_tasks,\n\t\t.maxlen\t\t= sizeof(sysctl_oom_dump_tasks),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"overcommit_ratio\",\n\t\t.data\t\t= &sysctl_overcommit_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_overcommit_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= overcommit_ratio_handler,\n\t},\n\t{\n\t\t.procname\t= \"overcommit_kbytes\",\n\t\t.data\t\t= &sysctl_overcommit_kbytes,\n\t\t.maxlen\t\t= sizeof(sysctl_overcommit_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= overcommit_kbytes_handler,\n\t},\n\t{\n\t\t.procname\t= \"page-cluster\", \n\t\t.data\t\t= &page_cluster,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"dirty_background_ratio\",\n\t\t.data\t\t= &dirty_background_ratio,\n\t\t.maxlen\t\t= sizeof(dirty_background_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_background_ratio_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"dirty_background_bytes\",\n\t\t.data\t\t= &dirty_background_bytes,\n\t\t.maxlen\t\t= sizeof(dirty_background_bytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_background_bytes_handler,\n\t\t.extra1\t\t= &one_ul,\n\t},\n\t{\n\t\t.procname\t= \"dirty_ratio\",\n\t\t.data\t\t= &vm_dirty_ratio,\n\t\t.maxlen\t\t= sizeof(vm_dirty_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_ratio_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"dirty_bytes\",\n\t\t.data\t\t= &vm_dirty_bytes,\n\t\t.maxlen\t\t= sizeof(vm_dirty_bytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_bytes_handler,\n\t\t.extra1\t\t= &dirty_bytes_min,\n\t},\n\t{\n\t\t.procname\t= \"dirty_writeback_centisecs\",\n\t\t.data\t\t= &dirty_writeback_interval,\n\t\t.maxlen\t\t= sizeof(dirty_writeback_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirty_writeback_centisecs_handler,\n\t},\n\t{\n\t\t.procname\t= \"dirty_expire_centisecs\",\n\t\t.data\t\t= &dirty_expire_interval,\n\t\t.maxlen\t\t= sizeof(dirty_expire_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"dirtytime_expire_seconds\",\n\t\t.data\t\t= &dirtytime_expire_interval,\n\t\t.maxlen\t\t= sizeof(dirty_expire_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= dirtytime_interval_handler,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname       = \"nr_pdflush_threads\",\n\t\t.mode           = 0444 /* read-only */,\n\t\t.proc_handler   = pdflush_proc_obsolete,\n\t},\n\t{\n\t\t.procname\t= \"swappiness\",\n\t\t.data\t\t= &vm_swappiness,\n\t\t.maxlen\t\t= sizeof(vm_swappiness),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n#ifdef CONFIG_HUGETLB_PAGE\n\t{\n\t\t.procname\t= \"nr_hugepages\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= hugetlb_sysctl_handler,\n\t},\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname       = \"nr_hugepages_mempolicy\",\n\t\t.data           = NULL,\n\t\t.maxlen         = sizeof(unsigned long),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = &hugetlb_mempolicy_sysctl_handler,\n\t},\n#endif\n\t {\n\t\t.procname\t= \"hugetlb_shm_group\",\n\t\t.data\t\t= &sysctl_hugetlb_shm_group,\n\t\t.maxlen\t\t= sizeof(gid_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t },\n\t {\n\t\t.procname\t= \"hugepages_treat_as_movable\",\n\t\t.data\t\t= &hugepages_treat_as_movable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"nr_overcommit_hugepages\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= hugetlb_overcommit_handler,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"lowmem_reserve_ratio\",\n\t\t.data\t\t= &sysctl_lowmem_reserve_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_lowmem_reserve_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= lowmem_reserve_ratio_sysctl_handler,\n\t},\n\t{\n\t\t.procname\t= \"drop_caches\",\n\t\t.data\t\t= &sysctl_drop_caches,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= drop_caches_sysctl_handler,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &four,\n\t},\n#ifdef CONFIG_COMPACTION\n\t{\n\t\t.procname\t= \"compact_memory\",\n\t\t.data\t\t= &sysctl_compact_memory,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0200,\n\t\t.proc_handler\t= sysctl_compaction_handler,\n\t},\n\t{\n\t\t.procname\t= \"extfrag_threshold\",\n\t\t.data\t\t= &sysctl_extfrag_threshold,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_extfrag_handler,\n\t\t.extra1\t\t= &min_extfrag_threshold,\n\t\t.extra2\t\t= &max_extfrag_threshold,\n\t},\n\t{\n\t\t.procname\t= \"compact_unevictable_allowed\",\n\t\t.data\t\t= &sysctl_compact_unevictable_allowed,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\n#endif /* CONFIG_COMPACTION */\n\t{\n\t\t.procname\t= \"min_free_kbytes\",\n\t\t.data\t\t= &min_free_kbytes,\n\t\t.maxlen\t\t= sizeof(min_free_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= min_free_kbytes_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"watermark_scale_factor\",\n\t\t.data\t\t= &watermark_scale_factor,\n\t\t.maxlen\t\t= sizeof(watermark_scale_factor),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= watermark_scale_factor_sysctl_handler,\n\t\t.extra1\t\t= &one,\n\t\t.extra2\t\t= &one_thousand,\n\t},\n\t{\n\t\t.procname\t= \"percpu_pagelist_fraction\",\n\t\t.data\t\t= &percpu_pagelist_fraction,\n\t\t.maxlen\t\t= sizeof(percpu_pagelist_fraction),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= percpu_pagelist_fraction_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t},\n#ifdef CONFIG_MMU\n\t{\n\t\t.procname\t= \"max_map_count\",\n\t\t.data\t\t= &sysctl_max_map_count,\n\t\t.maxlen\t\t= sizeof(sysctl_max_map_count),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n#else\n\t{\n\t\t.procname\t= \"nr_trim_pages\",\n\t\t.data\t\t= &sysctl_nr_trim_pages,\n\t\t.maxlen\t\t= sizeof(sysctl_nr_trim_pages),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"laptop_mode\",\n\t\t.data\t\t= &laptop_mode,\n\t\t.maxlen\t\t= sizeof(laptop_mode),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"block_dump\",\n\t\t.data\t\t= &block_dump,\n\t\t.maxlen\t\t= sizeof(block_dump),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"vfs_cache_pressure\",\n\t\t.data\t\t= &sysctl_vfs_cache_pressure,\n\t\t.maxlen\t\t= sizeof(sysctl_vfs_cache_pressure),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT\n\t{\n\t\t.procname\t= \"legacy_va_layout\",\n\t\t.data\t\t= &sysctl_legacy_va_layout,\n\t\t.maxlen\t\t= sizeof(sysctl_legacy_va_layout),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n#endif\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname\t= \"zone_reclaim_mode\",\n\t\t.data\t\t= &node_reclaim_mode,\n\t\t.maxlen\t\t= sizeof(node_reclaim_mode),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"min_unmapped_ratio\",\n\t\t.data\t\t= &sysctl_min_unmapped_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_min_unmapped_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_min_unmapped_ratio_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n\t{\n\t\t.procname\t= \"min_slab_ratio\",\n\t\t.data\t\t= &sysctl_min_slab_ratio,\n\t\t.maxlen\t\t= sizeof(sysctl_min_slab_ratio),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= sysctl_min_slab_ratio_sysctl_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one_hundred,\n\t},\n#endif\n#ifdef CONFIG_SMP\n\t{\n\t\t.procname\t= \"stat_interval\",\n\t\t.data\t\t= &sysctl_stat_interval,\n\t\t.maxlen\t\t= sizeof(sysctl_stat_interval),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname\t= \"stat_refresh\",\n\t\t.data\t\t= NULL,\n\t\t.maxlen\t\t= 0,\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= vmstat_refresh,\n\t},\n#endif\n#ifdef CONFIG_MMU\n\t{\n\t\t.procname\t= \"mmap_min_addr\",\n\t\t.data\t\t= &dac_mmap_min_addr,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= mmap_min_addr_handler,\n\t},\n#endif\n#ifdef CONFIG_NUMA\n\t{\n\t\t.procname\t= \"numa_zonelist_order\",\n\t\t.data\t\t= &numa_zonelist_order,\n\t\t.maxlen\t\t= NUMA_ZONELIST_ORDER_LEN,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= numa_zonelist_order_handler,\n\t},\n#endif\n#if (defined(CONFIG_X86_32) && !defined(CONFIG_UML))|| \\\n   (defined(CONFIG_SUPERH) && defined(CONFIG_VSYSCALL))\n\t{\n\t\t.procname\t= \"vdso_enabled\",\n#ifdef CONFIG_X86_32\n\t\t.data\t\t= &vdso32_enabled,\n\t\t.maxlen\t\t= sizeof(vdso32_enabled),\n#else\n\t\t.data\t\t= &vdso_enabled,\n\t\t.maxlen\t\t= sizeof(vdso_enabled),\n#endif\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.extra1\t\t= &zero,\n\t},\n#endif\n#ifdef CONFIG_HIGHMEM\n\t{\n\t\t.procname\t= \"highmem_is_dirtyable\",\n\t\t.data\t\t= &vm_highmem_is_dirtyable,\n\t\t.maxlen\t\t= sizeof(vm_highmem_is_dirtyable),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n#ifdef CONFIG_MEMORY_FAILURE\n\t{\n\t\t.procname\t= \"memory_failure_early_kill\",\n\t\t.data\t\t= &sysctl_memory_failure_early_kill,\n\t\t.maxlen\t\t= sizeof(sysctl_memory_failure_early_kill),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"memory_failure_recovery\",\n\t\t.data\t\t= &sysctl_memory_failure_recovery,\n\t\t.maxlen\t\t= sizeof(sysctl_memory_failure_recovery),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"user_reserve_kbytes\",\n\t\t.data\t\t= &sysctl_user_reserve_kbytes,\n\t\t.maxlen\t\t= sizeof(sysctl_user_reserve_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"admin_reserve_kbytes\",\n\t\t.data\t\t= &sysctl_admin_reserve_kbytes,\n\t\t.maxlen\t\t= sizeof(sysctl_admin_reserve_kbytes),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\n\t{\n\t\t.procname\t= \"mmap_rnd_bits\",\n\t\t.data\t\t= &mmap_rnd_bits,\n\t\t.maxlen\t\t= sizeof(mmap_rnd_bits),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (void *)&mmap_rnd_bits_min,\n\t\t.extra2\t\t= (void *)&mmap_rnd_bits_max,\n\t},\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\n\t{\n\t\t.procname\t= \"mmap_rnd_compat_bits\",\n\t\t.data\t\t= &mmap_rnd_compat_bits,\n\t\t.maxlen\t\t= sizeof(mmap_rnd_compat_bits),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (void *)&mmap_rnd_compat_bits_min,\n\t\t.extra2\t\t= (void *)&mmap_rnd_compat_bits_max,\n\t},\n#endif\n\t{ }\n};\n\nstatic struct ctl_table fs_table[] = {\n\t{\n\t\t.procname\t= \"inode-nr\",\n\t\t.data\t\t= &inodes_stat,\n\t\t.maxlen\t\t= 2*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_inodes,\n\t},\n\t{\n\t\t.procname\t= \"inode-state\",\n\t\t.data\t\t= &inodes_stat,\n\t\t.maxlen\t\t= 7*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_inodes,\n\t},\n\t{\n\t\t.procname\t= \"file-nr\",\n\t\t.data\t\t= &files_stat,\n\t\t.maxlen\t\t= sizeof(files_stat),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_files,\n\t},\n\t{\n\t\t.procname\t= \"file-max\",\n\t\t.data\t\t= &files_stat.max_files,\n\t\t.maxlen\t\t= sizeof(files_stat.max_files),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"nr_open\",\n\t\t.data\t\t= &sysctl_nr_open,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &sysctl_nr_open_min,\n\t\t.extra2\t\t= &sysctl_nr_open_max,\n\t},\n\t{\n\t\t.procname\t= \"dentry-state\",\n\t\t.data\t\t= &dentry_stat,\n\t\t.maxlen\t\t= 6*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_dentry,\n\t},\n\t{\n\t\t.procname\t= \"overflowuid\",\n\t\t.data\t\t= &fs_overflowuid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n\t{\n\t\t.procname\t= \"overflowgid\",\n\t\t.data\t\t= &fs_overflowgid,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &minolduid,\n\t\t.extra2\t\t= &maxolduid,\n\t},\n#ifdef CONFIG_FILE_LOCKING\n\t{\n\t\t.procname\t= \"leases-enable\",\n\t\t.data\t\t= &leases_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_DNOTIFY\n\t{\n\t\t.procname\t= \"dir-notify-enable\",\n\t\t.data\t\t= &dir_notify_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_MMU\n#ifdef CONFIG_FILE_LOCKING\n\t{\n\t\t.procname\t= \"lease-break-time\",\n\t\t.data\t\t= &lease_break_time,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n#endif\n#ifdef CONFIG_AIO\n\t{\n\t\t.procname\t= \"aio-nr\",\n\t\t.data\t\t= &aio_nr,\n\t\t.maxlen\t\t= sizeof(aio_nr),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"aio-max-nr\",\n\t\t.data\t\t= &aio_max_nr,\n\t\t.maxlen\t\t= sizeof(aio_max_nr),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n#endif /* CONFIG_AIO */\n#ifdef CONFIG_INOTIFY_USER\n\t{\n\t\t.procname\t= \"inotify\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= inotify_table,\n\t},\n#endif\t\n#ifdef CONFIG_EPOLL\n\t{\n\t\t.procname\t= \"epoll\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= epoll_table,\n\t},\n#endif\n#endif\n\t{\n\t\t.procname\t= \"protected_symlinks\",\n\t\t.data\t\t= &sysctl_protected_symlinks,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"protected_hardlinks\",\n\t\t.data\t\t= &sysctl_protected_hardlinks,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0600,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n\t{\n\t\t.procname\t= \"suid_dumpable\",\n\t\t.data\t\t= &suid_dumpable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax_coredump,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &two,\n\t},\n#if defined(CONFIG_BINFMT_MISC) || defined(CONFIG_BINFMT_MISC_MODULE)\n\t{\n\t\t.procname\t= \"binfmt_misc\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= sysctl_mount_point,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pipe-max-size\",\n\t\t.data\t\t= &pipe_max_size,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &pipe_proc_fn,\n\t\t.extra1\t\t= &pipe_min_size,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-hard\",\n\t\t.data\t\t= &pipe_user_pages_hard,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_hard),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"pipe-user-pages-soft\",\n\t\t.data\t\t= &pipe_user_pages_soft,\n\t\t.maxlen\t\t= sizeof(pipe_user_pages_soft),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"mount-max\",\n\t\t.data\t\t= &sysctl_mount_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &one,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table debug_table[] = {\n#ifdef CONFIG_SYSCTL_EXCEPTION_TRACE\n\t{\n\t\t.procname\t= \"exception-trace\",\n\t\t.data\t\t= &show_unhandled_signals,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec\n\t},\n#endif\n#if defined(CONFIG_OPTPROBES)\n\t{\n\t\t.procname\t= \"kprobes-optimization\",\n\t\t.data\t\t= &sysctl_kprobes_optimization,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_kprobes_optimization_handler,\n\t\t.extra1\t\t= &zero,\n\t\t.extra2\t\t= &one,\n\t},\n#endif\n\t{ }\n};\n\nstatic struct ctl_table dev_table[] = {\n\t{ }\n};\n\nint __init sysctl_init(void)\n{\n\tstruct ctl_table_header *hdr;\n\n\thdr = register_sysctl_table(sysctl_base_table);\n\tkmemleak_not_leak(hdr);\n\treturn 0;\n}\n\n#endif /* CONFIG_SYSCTL */\n\n/*\n * /proc/sys support\n */\n\n#ifdef CONFIG_PROC_SYSCTL\n\nstatic int _proc_do_string(char *data, int maxlen, int write,\n\t\t\t   char __user *buffer,\n\t\t\t   size_t *lenp, loff_t *ppos)\n{\n\tsize_t len;\n\tchar __user *p;\n\tchar c;\n\n\tif (!data || !maxlen || !*lenp) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\tif (write) {\n\t\tif (sysctl_writes_strict == SYSCTL_WRITES_STRICT) {\n\t\t\t/* Only continue writes not past the end of buffer. */\n\t\t\tlen = strlen(data);\n\t\t\tif (len > maxlen - 1)\n\t\t\t\tlen = maxlen - 1;\n\n\t\t\tif (*ppos > len)\n\t\t\t\treturn 0;\n\t\t\tlen = *ppos;\n\t\t} else {\n\t\t\t/* Start writing from beginning of buffer. */\n\t\t\tlen = 0;\n\t\t}\n\n\t\t*ppos += *lenp;\n\t\tp = buffer;\n\t\twhile ((p - buffer) < *lenp && len < maxlen - 1) {\n\t\t\tif (get_user(c, p++))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (c == 0 || c == '\\n')\n\t\t\t\tbreak;\n\t\t\tdata[len++] = c;\n\t\t}\n\t\tdata[len] = 0;\n\t} else {\n\t\tlen = strlen(data);\n\t\tif (len > maxlen)\n\t\t\tlen = maxlen;\n\n\t\tif (*ppos > len) {\n\t\t\t*lenp = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tdata += *ppos;\n\t\tlen  -= *ppos;\n\n\t\tif (len > *lenp)\n\t\t\tlen = *lenp;\n\t\tif (len)\n\t\t\tif (copy_to_user(buffer, data, len))\n\t\t\t\treturn -EFAULT;\n\t\tif (len < *lenp) {\n\t\t\tif (put_user('\\n', buffer + len))\n\t\t\t\treturn -EFAULT;\n\t\t\tlen++;\n\t\t}\n\t\t*lenp = len;\n\t\t*ppos += len;\n\t}\n\treturn 0;\n}\n\nstatic void warn_sysctl_write(struct ctl_table *table)\n{\n\tpr_warn_once(\"%s wrote to %s when file position was not 0!\\n\"\n\t\t\"This will not be supported in the future. To silence this\\n\"\n\t\t\"warning, set kernel.sysctl_writes_strict = -1\\n\",\n\t\tcurrent->comm, table->procname);\n}\n\n/**\n * proc_dostring - read a string sysctl\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes a string from/to the user buffer. If the kernel\n * buffer provided is not large enough to hold the string, the\n * string is truncated. The copied string is %NULL-terminated.\n * If the string is being read by the user process, it is copied\n * and a newline '\\n' is added. It is truncated if the buffer is\n * not large enough.\n *\n * Returns 0 on success.\n */\nint proc_dostring(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write && *ppos && sysctl_writes_strict == SYSCTL_WRITES_WARN)\n\t\twarn_sysctl_write(table);\n\n\treturn _proc_do_string((char *)(table->data), table->maxlen, write,\n\t\t\t       (char __user *)buffer, lenp, ppos);\n}\n\nstatic size_t proc_skip_spaces(char **buf)\n{\n\tsize_t ret;\n\tchar *tmp = skip_spaces(*buf);\n\tret = tmp - *buf;\n\t*buf = tmp;\n\treturn ret;\n}\n\nstatic void proc_skip_char(char **buf, size_t *size, const char v)\n{\n\twhile (*size) {\n\t\tif (**buf != v)\n\t\t\tbreak;\n\t\t(*size)--;\n\t\t(*buf)++;\n\t}\n}\n\n#define TMPBUFLEN 22\n/**\n * proc_get_long - reads an ASCII formatted integer from a user buffer\n *\n * @buf: a kernel buffer\n * @size: size of the kernel buffer\n * @val: this is where the number will be stored\n * @neg: set to %TRUE if number is negative\n * @perm_tr: a vector which contains the allowed trailers\n * @perm_tr_len: size of the perm_tr vector\n * @tr: pointer to store the trailer character\n *\n * In case of success %0 is returned and @buf and @size are updated with\n * the amount of bytes read. If @tr is non-NULL and a trailing\n * character exists (size is non-zero after returning from this\n * function), @tr is updated with the trailing character.\n */\nstatic int proc_get_long(char **buf, size_t *size,\n\t\t\t  unsigned long *val, bool *neg,\n\t\t\t  const char *perm_tr, unsigned perm_tr_len, char *tr)\n{\n\tint len;\n\tchar *p, tmp[TMPBUFLEN];\n\n\tif (!*size)\n\t\treturn -EINVAL;\n\n\tlen = *size;\n\tif (len > TMPBUFLEN - 1)\n\t\tlen = TMPBUFLEN - 1;\n\n\tmemcpy(tmp, *buf, len);\n\n\ttmp[len] = 0;\n\tp = tmp;\n\tif (*p == '-' && *size > 1) {\n\t\t*neg = true;\n\t\tp++;\n\t} else\n\t\t*neg = false;\n\tif (!isdigit(*p))\n\t\treturn -EINVAL;\n\n\t*val = simple_strtoul(p, &p, 0);\n\n\tlen = p - tmp;\n\n\t/* We don't know if the next char is whitespace thus we may accept\n\t * invalid integers (e.g. 1234...a) or two integers instead of one\n\t * (e.g. 123...1). So lets not allow such large numbers. */\n\tif (len == TMPBUFLEN - 1)\n\t\treturn -EINVAL;\n\n\tif (len < *size && perm_tr_len && !memchr(perm_tr, *p, perm_tr_len))\n\t\treturn -EINVAL;\n\n\tif (tr && (len < *size))\n\t\t*tr = *p;\n\n\t*buf += len;\n\t*size -= len;\n\n\treturn 0;\n}\n\n/**\n * proc_put_long - converts an integer to a decimal ASCII formatted string\n *\n * @buf: the user buffer\n * @size: the size of the user buffer\n * @val: the integer to be converted\n * @neg: sign of the number, %TRUE for negative\n *\n * In case of success %0 is returned and @buf and @size are updated with\n * the amount of bytes written.\n */\nstatic int proc_put_long(void __user **buf, size_t *size, unsigned long val,\n\t\t\t  bool neg)\n{\n\tint len;\n\tchar tmp[TMPBUFLEN], *p = tmp;\n\n\tsprintf(p, \"%s%lu\", neg ? \"-\" : \"\", val);\n\tlen = strlen(tmp);\n\tif (len > *size)\n\t\tlen = *size;\n\tif (copy_to_user(*buf, tmp, len))\n\t\treturn -EFAULT;\n\t*size -= len;\n\t*buf += len;\n\treturn 0;\n}\n#undef TMPBUFLEN\n\nstatic int proc_put_char(void __user **buf, size_t *size, char c)\n{\n\tif (*size) {\n\t\tchar __user **buffer = (char __user **)buf;\n\t\tif (put_user(c, *buffer))\n\t\t\treturn -EFAULT;\n\t\t(*size)--, (*buffer)++;\n\t\t*buf = *buffer;\n\t}\n\treturn 0;\n}\n\nstatic int do_proc_dointvec_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t int *valp,\n\t\t\t\t int write, void *data)\n{\n\tif (write) {\n\t\tif (*negp) {\n\t\t\tif (*lvalp > (unsigned long) INT_MAX + 1)\n\t\t\t\treturn -EINVAL;\n\t\t\t*valp = -*lvalp;\n\t\t} else {\n\t\t\tif (*lvalp > (unsigned long) INT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\t*valp = *lvalp;\n\t\t}\n\t} else {\n\t\tint val = *valp;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\t*lvalp = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\t*lvalp = (unsigned long)val;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic const char proc_wspace_sep[] = { ' ', '\\t', '\\n' };\n\nstatic int __do_proc_dointvec(void *tbl_data, struct ctl_table *table,\n\t\t  int write, void __user *buffer,\n\t\t  size_t *lenp, loff_t *ppos,\n\t\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\n\t\t\t      int write, void *data),\n\t\t  void *data)\n{\n\tint *i, vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *kbuf = NULL, *p;\n\t\n\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\t\n\ti = (int *) tbl_data;\n\tvleft = table->maxlen / sizeof(*i);\n\tleft = *lenp;\n\n\tif (!conv)\n\t\tconv = do_proc_dointvec_conv;\n\n\tif (write) {\n\t\tif (*ppos) {\n\t\t\tswitch (sysctl_writes_strict) {\n\t\t\tcase SYSCTL_WRITES_STRICT:\n\t\t\t\tgoto out;\n\t\t\tcase SYSCTL_WRITES_WARN:\n\t\t\t\twarn_sysctl_write(table);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = kbuf = memdup_user_nul(buffer, left);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t}\n\n\tfor (; left && vleft--; i++, first=0) {\n\t\tunsigned long lval;\n\t\tbool neg;\n\n\t\tif (write) {\n\t\t\tleft -= proc_skip_spaces(&p);\n\n\t\t\tif (!left)\n\t\t\t\tbreak;\n\t\t\terr = proc_get_long(&p, &left, &lval, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (conv(&neg, &lval, i, 1, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (conv(&neg, &lval, i, 0, data)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!first)\n\t\t\t\terr = proc_put_char(&buffer, &left, '\\t');\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\terr = proc_put_long(&buffer, &left, lval, neg);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!write && !first && left && !err)\n\t\terr = proc_put_char(&buffer, &left, '\\n');\n\tif (write && !err && left)\n\t\tleft -= proc_skip_spaces(&p);\n\tif (write) {\n\t\tkfree(kbuf);\n\t\tif (first)\n\t\t\treturn err ? : -EINVAL;\n\t}\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}\n\nstatic int do_proc_dointvec(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos,\n\t\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\n\t\t\t      int write, void *data),\n\t\t  void *data)\n{\n\treturn __do_proc_dointvec(table->data, table, write,\n\t\t\tbuffer, lenp, ppos, conv, data);\n}\n\n/**\n * proc_dointvec - read a vector of integers\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n *\n * Returns 0 on success.\n */\nint proc_dointvec(struct ctl_table *table, int write,\n\t\t     void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\n\t\t    \t    NULL,NULL);\n}\n\n/*\n * Taint values can only be increased\n * This means we can safely use a temporary.\n */\nstatic int proc_taint(struct ctl_table *table, int write,\n\t\t\t       void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table t;\n\tunsigned long tmptaint = get_taint();\n\tint err;\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tt = *table;\n\tt.data = &tmptaint;\n\terr = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (write) {\n\t\t/*\n\t\t * Poor man's atomic or. Not worth adding a primitive\n\t\t * to everyone's atomic.h for this\n\t\t */\n\t\tint i;\n\t\tfor (i = 0; i < BITS_PER_LONG && tmptaint >> i; i++) {\n\t\t\tif ((tmptaint >> i) & 1)\n\t\t\t\tadd_taint(i, LOCKDEP_STILL_OK);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n#ifdef CONFIG_PRINTK\nstatic int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n}\n#endif\n\nstruct do_proc_dointvec_minmax_conv_param {\n\tint *min;\n\tint *max;\n};\n\nstatic int do_proc_dointvec_minmax_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\tint *valp,\n\t\t\t\t\tint write, void *data)\n{\n\tstruct do_proc_dointvec_minmax_conv_param *param = data;\n\tif (write) {\n\t\tint val = *negp ? -*lvalp : *lvalp;\n\t\tif ((param->min && *param->min > val) ||\n\t\t    (param->max && *param->max < val))\n\t\t\treturn -EINVAL;\n\t\t*valp = val;\n\t} else {\n\t\tint val = *valp;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\t*lvalp = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\t*lvalp = (unsigned long)val;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n * proc_dointvec_minmax - read a vector of integers with min/max values\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string.\n *\n * This routine will ensure the values are within the range specified by\n * table->extra1 (min) and table->extra2 (max).\n *\n * Returns 0 on success.\n */\nint proc_dointvec_minmax(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct do_proc_dointvec_minmax_conv_param param = {\n\t\t.min = (int *) table->extra1,\n\t\t.max = (int *) table->extra2,\n\t};\n\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\n\t\t\t\tdo_proc_dointvec_minmax_conv, &param);\n}\n\nstatic void validate_coredump_safety(void)\n{\n#ifdef CONFIG_COREDUMP\n\tif (suid_dumpable == SUID_DUMP_ROOT &&\n\t    core_pattern[0] != '/' && core_pattern[0] != '|') {\n\t\tprintk(KERN_WARNING \"Unsafe core_pattern used with \"\\\n\t\t\t\"suid_dumpable=2. Pipe handler or fully qualified \"\\\n\t\t\t\"core dump path required.\\n\");\n\t}\n#endif\n}\n\nstatic int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,\n\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint error = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (!error)\n\t\tvalidate_coredump_safety();\n\treturn error;\n}\n\n#ifdef CONFIG_COREDUMP\nstatic int proc_dostring_coredump(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint error = proc_dostring(table, write, buffer, lenp, ppos);\n\tif (!error)\n\t\tvalidate_coredump_safety();\n\treturn error;\n}\n#endif\n\nstatic int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table, int write,\n\t\t\t\t     void __user *buffer,\n\t\t\t\t     size_t *lenp, loff_t *ppos,\n\t\t\t\t     unsigned long convmul,\n\t\t\t\t     unsigned long convdiv)\n{\n\tunsigned long *i, *min, *max;\n\tint vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *kbuf = NULL, *p;\n\n\tif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\ti = (unsigned long *) data;\n\tmin = (unsigned long *) table->extra1;\n\tmax = (unsigned long *) table->extra2;\n\tvleft = table->maxlen / sizeof(unsigned long);\n\tleft = *lenp;\n\n\tif (write) {\n\t\tif (*ppos) {\n\t\t\tswitch (sysctl_writes_strict) {\n\t\t\tcase SYSCTL_WRITES_STRICT:\n\t\t\t\tgoto out;\n\t\t\tcase SYSCTL_WRITES_WARN:\n\t\t\t\twarn_sysctl_write(table);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = kbuf = memdup_user_nul(buffer, left);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t}\n\n\tfor (; left && vleft--; i++, first = 0) {\n\t\tunsigned long val;\n\n\t\tif (write) {\n\t\t\tbool neg;\n\n\t\t\tleft -= proc_skip_spaces(&p);\n\n\t\t\terr = proc_get_long(&p, &left, &val, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (neg)\n\t\t\t\tcontinue;\n\t\t\tif ((min && val < *min) || (max && val > *max))\n\t\t\t\tcontinue;\n\t\t\t*i = val;\n\t\t} else {\n\t\t\tval = convdiv * (*i) / convmul;\n\t\t\tif (!first) {\n\t\t\t\terr = proc_put_char(&buffer, &left, '\\t');\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = proc_put_long(&buffer, &left, val, false);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!write && !first && left && !err)\n\t\terr = proc_put_char(&buffer, &left, '\\n');\n\tif (write && !err)\n\t\tleft -= proc_skip_spaces(&p);\n\tif (write) {\n\t\tkfree(kbuf);\n\t\tif (first)\n\t\t\treturn err ? : -EINVAL;\n\t}\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}\n\nstatic int do_proc_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t\t\t     void __user *buffer,\n\t\t\t\t     size_t *lenp, loff_t *ppos,\n\t\t\t\t     unsigned long convmul,\n\t\t\t\t     unsigned long convdiv)\n{\n\treturn __do_proc_doulongvec_minmax(table->data, table, write,\n\t\t\tbuffer, lenp, ppos, convmul, convdiv);\n}\n\n/**\n * proc_doulongvec_minmax - read a vector of long integers with min/max values\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long\n * values from/to the user buffer, treated as an ASCII string.\n *\n * This routine will ensure the values are within the range specified by\n * table->extra1 (min) and table->extra2 (max).\n *\n * Returns 0 on success.\n */\nint proc_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t\t   void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_doulongvec_minmax(table, write, buffer, lenp, ppos, 1l, 1l);\n}\n\n/**\n * proc_doulongvec_ms_jiffies_minmax - read a vector of millisecond values with min/max values\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long\n * values from/to the user buffer, treated as an ASCII string. The values\n * are treated as milliseconds, and converted to jiffies when they are stored.\n *\n * This routine will ensure the values are within the range specified by\n * table->extra1 (min) and table->extra2 (max).\n *\n * Returns 0 on success.\n */\nint proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int write,\n\t\t\t\t      void __user *buffer,\n\t\t\t\t      size_t *lenp, loff_t *ppos)\n{\n    return do_proc_doulongvec_minmax(table, write, buffer,\n\t\t\t\t     lenp, ppos, HZ, 1000l);\n}\n\n\nstatic int do_proc_dointvec_jiffies_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\t int *valp,\n\t\t\t\t\t int write, void *data)\n{\n\tif (write) {\n\t\tif (*lvalp > LONG_MAX / HZ)\n\t\t\treturn 1;\n\t\t*valp = *negp ? -(*lvalp*HZ) : (*lvalp*HZ);\n\t} else {\n\t\tint val = *valp;\n\t\tunsigned long lval;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\tlval = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\tlval = (unsigned long)val;\n\t\t}\n\t\t*lvalp = lval / HZ;\n\t}\n\treturn 0;\n}\n\nstatic int do_proc_dointvec_userhz_jiffies_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\t\tint *valp,\n\t\t\t\t\t\tint write, void *data)\n{\n\tif (write) {\n\t\tif (USER_HZ < HZ && *lvalp > (LONG_MAX / HZ) * USER_HZ)\n\t\t\treturn 1;\n\t\t*valp = clock_t_to_jiffies(*negp ? -*lvalp : *lvalp);\n\t} else {\n\t\tint val = *valp;\n\t\tunsigned long lval;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\tlval = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\tlval = (unsigned long)val;\n\t\t}\n\t\t*lvalp = jiffies_to_clock_t(lval);\n\t}\n\treturn 0;\n}\n\nstatic int do_proc_dointvec_ms_jiffies_conv(bool *negp, unsigned long *lvalp,\n\t\t\t\t\t    int *valp,\n\t\t\t\t\t    int write, void *data)\n{\n\tif (write) {\n\t\tunsigned long jif = msecs_to_jiffies(*negp ? -*lvalp : *lvalp);\n\n\t\tif (jif > INT_MAX)\n\t\t\treturn 1;\n\t\t*valp = (int)jif;\n\t} else {\n\t\tint val = *valp;\n\t\tunsigned long lval;\n\t\tif (val < 0) {\n\t\t\t*negp = true;\n\t\t\tlval = -(unsigned long)val;\n\t\t} else {\n\t\t\t*negp = false;\n\t\t\tlval = (unsigned long)val;\n\t\t}\n\t\t*lvalp = jiffies_to_msecs(lval);\n\t}\n\treturn 0;\n}\n\n/**\n * proc_dointvec_jiffies - read a vector of integers as seconds\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n * The values read are assumed to be in seconds, and are converted into\n * jiffies.\n *\n * Returns 0 on success.\n */\nint proc_dointvec_jiffies(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\n\t\t    \t    do_proc_dointvec_jiffies_conv,NULL);\n}\n\n/**\n * proc_dointvec_userhz_jiffies - read a vector of integers as 1/USER_HZ seconds\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: pointer to the file position\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n * The values read are assumed to be in 1/USER_HZ seconds, and \n * are converted into jiffies.\n *\n * Returns 0 on success.\n */\nint proc_dointvec_userhz_jiffies(struct ctl_table *table, int write,\n\t\t\t\t void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n    return do_proc_dointvec(table,write,buffer,lenp,ppos,\n\t\t    \t    do_proc_dointvec_userhz_jiffies_conv,NULL);\n}\n\n/**\n * proc_dointvec_ms_jiffies - read a vector of integers as 1 milliseconds\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n * @ppos: the current position in the file\n *\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\n * values from/to the user buffer, treated as an ASCII string. \n * The values read are assumed to be in 1/1000 seconds, and \n * are converted into jiffies.\n *\n * Returns 0 on success.\n */\nint proc_dointvec_ms_jiffies(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn do_proc_dointvec(table, write, buffer, lenp, ppos,\n\t\t\t\tdo_proc_dointvec_ms_jiffies_conv, NULL);\n}\n\nstatic int proc_do_cad_pid(struct ctl_table *table, int write,\n\t\t\t   void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct pid *new_pid;\n\tpid_t tmp;\n\tint r;\n\n\ttmp = pid_vnr(cad_pid);\n\n\tr = __do_proc_dointvec(&tmp, table, write, buffer,\n\t\t\t       lenp, ppos, NULL, NULL);\n\tif (r || !write)\n\t\treturn r;\n\n\tnew_pid = find_get_pid(tmp);\n\tif (!new_pid)\n\t\treturn -ESRCH;\n\n\tput_pid(xchg(&cad_pid, new_pid));\n\treturn 0;\n}\n\n/**\n * proc_do_large_bitmap - read/write from/to a large bitmap\n * @table: the sysctl table\n * @write: %TRUE if this is a write to the sysctl file\n * @buffer: the user buffer\n * @lenp: the size of the user buffer\n * @ppos: file position\n *\n * The bitmap is stored at table->data and the bitmap length (in bits)\n * in table->maxlen.\n *\n * We use a range comma separated format (e.g. 1,3-4,10-10) so that\n * large bitmaps may be represented in a compact manner. Writing into\n * the file will clear the bitmap then update it with the given input.\n *\n * Returns 0 on success.\n */\nint proc_do_large_bitmap(struct ctl_table *table, int write,\n\t\t\t void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint err = 0;\n\tbool first = 1;\n\tsize_t left = *lenp;\n\tunsigned long bitmap_len = table->maxlen;\n\tunsigned long *bitmap = *(unsigned long **) table->data;\n\tunsigned long *tmp_bitmap = NULL;\n\tchar tr_a[] = { '-', ',', '\\n' }, tr_b[] = { ',', '\\n', 0 }, c;\n\n\tif (!bitmap || !bitmap_len || !left || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\tif (write) {\n\t\tchar *kbuf, *p;\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\n\t\tp = kbuf = memdup_user_nul(buffer, left);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\n\t\ttmp_bitmap = kzalloc(BITS_TO_LONGS(bitmap_len) * sizeof(unsigned long),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (!tmp_bitmap) {\n\t\t\tkfree(kbuf);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tproc_skip_char(&p, &left, '\\n');\n\t\twhile (!err && left) {\n\t\t\tunsigned long val_a, val_b;\n\t\t\tbool neg;\n\n\t\t\terr = proc_get_long(&p, &left, &val_a, &neg, tr_a,\n\t\t\t\t\t     sizeof(tr_a), &c);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (val_a >= bitmap_len || neg) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tval_b = val_a;\n\t\t\tif (left) {\n\t\t\t\tp++;\n\t\t\t\tleft--;\n\t\t\t}\n\n\t\t\tif (c == '-') {\n\t\t\t\terr = proc_get_long(&p, &left, &val_b,\n\t\t\t\t\t\t     &neg, tr_b, sizeof(tr_b),\n\t\t\t\t\t\t     &c);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t\tif (val_b >= bitmap_len || neg ||\n\t\t\t\t    val_a > val_b) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (left) {\n\t\t\t\t\tp++;\n\t\t\t\t\tleft--;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbitmap_set(tmp_bitmap, val_a, val_b - val_a + 1);\n\t\t\tfirst = 0;\n\t\t\tproc_skip_char(&p, &left, '\\n');\n\t\t}\n\t\tkfree(kbuf);\n\t} else {\n\t\tunsigned long bit_a, bit_b = 0;\n\n\t\twhile (left) {\n\t\t\tbit_a = find_next_bit(bitmap, bitmap_len, bit_b);\n\t\t\tif (bit_a >= bitmap_len)\n\t\t\t\tbreak;\n\t\t\tbit_b = find_next_zero_bit(bitmap, bitmap_len,\n\t\t\t\t\t\t   bit_a + 1) - 1;\n\n\t\t\tif (!first) {\n\t\t\t\terr = proc_put_char(&buffer, &left, ',');\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = proc_put_long(&buffer, &left, bit_a, false);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (bit_a != bit_b) {\n\t\t\t\terr = proc_put_char(&buffer, &left, '-');\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t\terr = proc_put_long(&buffer, &left, bit_b, false);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfirst = 0; bit_b++;\n\t\t}\n\t\tif (!err)\n\t\t\terr = proc_put_char(&buffer, &left, '\\n');\n\t}\n\n\tif (!err) {\n\t\tif (write) {\n\t\t\tif (*ppos)\n\t\t\t\tbitmap_or(bitmap, bitmap, tmp_bitmap, bitmap_len);\n\t\t\telse\n\t\t\t\tbitmap_copy(bitmap, tmp_bitmap, bitmap_len);\n\t\t}\n\t\tkfree(tmp_bitmap);\n\t\t*lenp -= left;\n\t\t*ppos += *lenp;\n\t\treturn 0;\n\t} else {\n\t\tkfree(tmp_bitmap);\n\t\treturn err;\n\t}\n}\n\n#else /* CONFIG_PROC_SYSCTL */\n\nint proc_dostring(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec(struct ctl_table *table, int write,\n\t\t  void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_minmax(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_jiffies(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_userhz_jiffies(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_dointvec_ms_jiffies(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_doulongvec_minmax(struct ctl_table *table, int write,\n\t\t    void __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\treturn -ENOSYS;\n}\n\nint proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int write,\n\t\t\t\t      void __user *buffer,\n\t\t\t\t      size_t *lenp, loff_t *ppos)\n{\n    return -ENOSYS;\n}\n\n\n#endif /* CONFIG_PROC_SYSCTL */\n\n/*\n * No sense putting this after each symbol definition, twice,\n * exception granted :-)\n */\nEXPORT_SYMBOL(proc_dointvec);\nEXPORT_SYMBOL(proc_dointvec_jiffies);\nEXPORT_SYMBOL(proc_dointvec_minmax);\nEXPORT_SYMBOL(proc_dointvec_userhz_jiffies);\nEXPORT_SYMBOL(proc_dointvec_ms_jiffies);\nEXPORT_SYMBOL(proc_dostring);\nEXPORT_SYMBOL(proc_doulongvec_minmax);\nEXPORT_SYMBOL(proc_doulongvec_ms_jiffies_minmax);\n"], "filenames": ["Documentation/sysctl/fs.txt", "fs/mount.h", "fs/namespace.c", "fs/pnode.c", "fs/pnode.h", "include/linux/mount.h", "kernel/sysctl.c"], "buggy_code_start_loc": [267, 16, 28, 262, 54, 98, 67], "buggy_code_end_loc": [267, 16, 2865, 263, 54, 98, 1839], "fixing_code_start_loc": [268, 17, 29, 262, 55, 99, 68], "fixing_code_end_loc": [275, 19, 2913, 263, 56, 101, 1849], "type": "CWE-400", "message": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.", "other": {"cve": {"id": "CVE-2016-6213", "sourceIdentifier": "cve@mitre.org", "published": "2016-12-28T07:59:00.180", "lastModified": "2018-01-05T02:31:05.040", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts."}, {"lang": "es", "value": "fs/namespace.c en el kernel de Linux en versiones anteriores a 4.9 no restringe la cantidad de montajes que pueden existir en un espacio de nombre del montaje, lo que permite a usuarios locales provocar una denegaci\u00f3n de servicio (consumo de memoria y punto muerto) a trav\u00e9s de llamadas al sistema de montaje MS_BIND, seg\u00fan lo demostrado por un bucle que desencadena un crecimiento exponencial en el n\u00famero de montajes."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.8.15", "matchCriteriaId": "3C328078-DE6F-4775-B855-1B0759412F43"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d29216842a85c7970c536108e093963f02714498", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/07/13/8", "source": "cve@mitre.org", "tags": ["Mailing List"]}, {"url": "http://www.securityfocus.com/bid/91754", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "cve@mitre.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1356471", "source": "cve@mitre.org", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498", "source": "cve@mitre.org", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498"}}