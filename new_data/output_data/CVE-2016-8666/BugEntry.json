{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/timer.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/prefetch.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/dmaengine.h>\n#include <linux/workqueue.h>\n#include <linux/dynamic_queue_limits.h>\n\n#include <linux/ethtool.h>\n#include <net/net_namespace.h>\n#include <net/dsa.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n#include <net/netprio_cgroup.h>\n\n#include <linux/netdev_features.h>\n#include <linux/neighbour.h>\n#include <uapi/linux/netdevice.h>\n#include <uapi/linux/if_bonding.h>\n#include <uapi/linux/pkt_cls.h>\n\nstruct netpoll_info;\nstruct device;\nstruct phy_device;\n/* 802.11 specific */\nstruct wireless_dev;\n/* 802.15.4 specific */\nstruct wpan_dev;\nstruct mpls_dev;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops);\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously, in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_POLICED\t0x03\t/* skb is shot by police\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n\tNETDEV_TX_LOCKED = 0x20,\t/* driver tx lock was already taken */\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tCompute the worst case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_HYPERV_NET)\n# define LL_MAX_HEADER 128\n#elif defined(CONFIG_WLAN) || IS_ENABLED(CONFIG_AX25)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !IS_ENABLED(CONFIG_NET_IPIP) && !IS_ENABLED(CONFIG_NET_IPGRE) && \\\n    !IS_ENABLED(CONFIG_IPV6_SIT) && !IS_ENABLED(CONFIG_IPV6_TUNNEL)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\n#ifdef CONFIG_RPS\n#include <linux/static_key.h>\nextern struct static_key rps_needed;\n#endif\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_SLAVE\t\t3\n#define NETDEV_HW_ADDR_T_UNICAST\t4\n#define NETDEV_HW_ADDR_T_MULTICAST\t5\n\tbool\t\t\tglobal_use;\n\tint\t\t\tsync_cnt;\n\tint\t\t\trefcount;\n\tint\t\t\tsynced;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tu16\t\thh_len;\n\tu16\t\t__pad;\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\n/* Reserve HH_DATA_MOD byte aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned int len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n\tbool\t(*validate)(const char *ll_header, unsigned int len);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer, they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n};\n\n\n/*\n * This structure holds at boot time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nint __init netdev_boot_setup(char *str);\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-cpu poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tunsigned int\t\tgro_count;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tspinlock_t\t\tpoll_lock;\n\tint\t\t\tpoll_owner;\n#endif\n\tstruct net_device\t*dev;\n\tstruct sk_buff\t\t*gro_list;\n\tstruct sk_buff\t\t*skb;\n\tstruct hrtimer\t\ttimer;\n\tstruct list_head\tdev_list;\n\tstruct hlist_node\tnapi_hash_node;\n\tunsigned int\t\tnapi_id;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t/* Poll is scheduled */\n\tNAPI_STATE_DISABLE,\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t/* Netpoll - don't dequeue from poll_list */\n\tNAPI_STATE_HASHED,\t/* In NAPI hash (busy polling possible) */\n\tNAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_DROP,\n};\ntypedef enum gro_result gro_result_t;\n\n/*\n * enum rx_handler_result - Possible return values for rx_handlers.\n * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it\n * further.\n * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in\n * case skb->dev was changed by rx_handler.\n * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.\n * @RX_HANDLER_PASS: Do nothing, passe the skb as if no rx_handler was called.\n *\n * rx_handlers are functions called from inside __netif_receive_skb(), to do\n * special processing of the skb, prior to delivery to protocol handlers.\n *\n * Currently, a net_device can only have a single rx_handler registered. Trying\n * to register a second rx_handler will return -EBUSY.\n *\n * To register a rx_handler on a net_device, use netdev_rx_handler_register().\n * To unregister a rx_handler on a net_device, use\n * netdev_rx_handler_unregister().\n *\n * Upon return, rx_handler is expected to tell __netif_receive_skb() what to\n * do with the skb.\n *\n * If the rx_handler consumed to skb in some way, it should return\n * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for\n * the skb to be delivered in some other ways.\n *\n * If the rx_handler changed skb->dev, to divert the skb to another\n * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the\n * new device will be called if it exists.\n *\n * If the rx_handler consider the skb should be ignored, it should return\n * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that\n * are registered on exact device (ptype->dev == skb->dev).\n *\n * If the rx_handler didn't changed skb->dev, but want the skb to be normally\n * delivered, it should return RX_HANDLER_PASS.\n *\n * A device without a registered rx_handler will behave as if rx_handler\n * returned RX_HANDLER_PASS.\n */\n\nenum rx_handler_result {\n\tRX_HANDLER_CONSUMED,\n\tRX_HANDLER_ANOTHER,\n\tRX_HANDLER_EXACT,\n\tRX_HANDLER_PASS,\n};\ntypedef enum rx_handler_result rx_handler_result_t;\ntypedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);\n\nvoid __napi_schedule(struct napi_struct *n);\nvoid __napi_schedule_irqoff(struct napi_struct *n);\n\nstatic inline bool napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nstatic inline bool napi_schedule_prep(struct napi_struct *n)\n{\n\treturn !napi_disable_pending(n) &&\n\t\t!test_and_set_bit(NAPI_STATE_SCHED, &n->state);\n}\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: napi context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/**\n *\tnapi_schedule_irqoff - schedule NAPI poll\n *\t@n: napi context\n *\n * Variant of napi_schedule(), assuming hard irqs are masked.\n */\nstatic inline void napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule_irqoff(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline bool napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid __napi_complete(struct napi_struct *n);\nvoid napi_complete_done(struct napi_struct *n, int work_done);\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: napi context\n *\n * Mark NAPI processing as complete.\n * Consider using napi_complete_done() instead.\n */\nstatic inline void napi_complete(struct napi_struct *n)\n{\n\treturn napi_complete_done(n, 0);\n}\n\n/**\n *\tnapi_hash_add - add a NAPI to global hashtable\n *\t@napi: napi context\n *\n * generate a new napi_id and store a @napi under it in napi_hash\n * Used for busy polling (CONFIG_NET_RX_BUSY_POLL)\n * Note: This is normally automatically done from netif_napi_add(),\n * so might disappear in a future linux version.\n */\nvoid napi_hash_add(struct napi_struct *napi);\n\n/**\n *\tnapi_hash_del - remove a NAPI from global table\n *\t@napi: napi context\n *\n * Warning: caller must observe rcu grace period\n * before freeing memory containing @napi, if\n * this function returns true.\n * Note: core networking stack automatically calls it\n * from netif_napi_del()\n * Drivers might want to call this helper to combine all\n * the needed rcu grace periods into a single one.\n */\nbool napi_hash_del(struct napi_struct *napi);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: napi context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nvoid napi_disable(struct napi_struct *n);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: napi context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nstatic inline void napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n\tclear_bit(NAPI_STATE_NPSVC, &n->state);\n}\n\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: napi context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\tif (IS_ENABLED(CONFIG_SMP))\n\t\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\t\tmsleep(1);\n\telse\n\t\tbarrier();\n}\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_DRV_XOFF,\n\t__QUEUE_STATE_STACK_XOFF,\n\t__QUEUE_STATE_FROZEN,\n};\n\n#define QUEUE_STATE_DRV_XOFF\t(1 << __QUEUE_STATE_DRV_XOFF)\n#define QUEUE_STATE_STACK_XOFF\t(1 << __QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_FROZEN\t(1 << __QUEUE_STATE_FROZEN)\n\n#define QUEUE_STATE_ANY_XOFF\t(QUEUE_STATE_DRV_XOFF | QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_ANY_XOFF_OR_FROZEN (QUEUE_STATE_ANY_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n#define QUEUE_STATE_DRV_XOFF_OR_FROZEN (QUEUE_STATE_DRV_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n\n/*\n * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The\n * netif_tx_* functions below are used to manipulate this flag.  The\n * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit\n * queue independently.  The netif_xmit_*stopped functions below are called\n * to check if the queue has been stopped by the driver or stack (either\n * of the XOFF bits are set in the state).  Drivers should not need to call\n * netif_xmit*stopped functions, they should only be using netif_tx_*.\n */\n\nstruct netdev_queue {\n/*\n * read mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc __rcu\t*qdisc;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_SYSFS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n/*\n * write mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * please use this field instead of dev->trans_start\n\t */\n\tunsigned long\t\ttrans_start;\n\n\t/*\n\t * Number of TX timeouts for this queue\n\t * (/sys/class/net/DEV/Q/trans_timeout)\n\t */\n\tunsigned long\t\ttrans_timeout;\n\n\tunsigned long\t\tstate;\n\n#ifdef CONFIG_BQL\n\tstruct dql\t\tdql;\n#endif\n\tunsigned long\t\ttx_maxrate;\n} ____cacheline_aligned_in_smp;\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[0];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU, the\n * tail pointer for that CPU's input queue at the time of last enqueue, and\n * a hardware filter index.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 filter;\n\tunsigned int last_qtail;\n};\n#define RPS_NO_FILTER 0xffff\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct rps_dev_flow flows[0];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    ((_num) * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n * Each entry is a 32bit value. Upper part is the high order bits\n * of flow hash, lower part is cpu number.\n * rps_cpu_mask is used to partition the space, depending on number of\n * possible cpus : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1\n * For example, if 64 cpus are possible, rps_cpu_mask = 0x3f,\n * meaning we use 32-6=26 bits for the hash.\n */\nstruct rps_sock_flow_table {\n\tu32\tmask;\n\n\tu32\tents[0] ____cacheline_aligned_in_smp;\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (offsetof(struct rps_sock_flow_table, ents[_num]))\n\n#define RPS_NO_CPU 0xffff\n\nextern u32 rps_cpu_mask;\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int index = hash & table->mask;\n\t\tu32 val = hash & ~rps_cpu_mask;\n\n\t\t/* We only give a hint, preemption can change cpu under us */\n\t\tval |= raw_smp_processor_id();\n\n\t\tif (table->ents[index] != val)\n\t\t\ttable->ents[index] = val;\n\t}\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id,\n\t\t\t u16 filter_id);\n#endif\n#endif /* CONFIG_RPS */\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n#ifdef CONFIG_RPS\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n#endif\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n} ____cacheline_aligned_in_smp;\n\n/*\n * RX queue sysfs structures and functions.\n */\nstruct rx_queue_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct netdev_rx_queue *queue,\n\t    struct rx_queue_attribute *attr, char *buf);\n\tssize_t (*store)(struct netdev_rx_queue *queue,\n\t    struct rx_queue_attribute *attr, const char *buf, size_t len);\n};\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[0];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + ((_num) * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_ALIGN(offsetof(struct xps_map, queues[1])) \\\n       - sizeof(struct xps_map)) / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tstruct xps_map __rcu *cpu_map[0];\n};\n#define XPS_DEV_MAPS_SIZE (sizeof(struct xps_dev_maps) +\t\t\\\n    (nr_cpu_ids * sizeof(struct xps_map *)))\n#endif /* CONFIG_XPS */\n\n#define TC_MAX_QUEUE\t16\n#define TC_BITMASK\t15\n/* HW offloaded queuing disciplines txq count and offset maps */\nstruct netdev_tc_txq {\n\tu16 count;\n\tu16 offset;\n};\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n/*\n * This structure is to hold information about the device\n * configured to run FCoE protocol stack.\n */\nstruct netdev_fcoe_hbainfo {\n\tchar\tmanufacturer[64];\n\tchar\tserial_number[64];\n\tchar\thardware_version[64];\n\tchar\tdriver_version[64];\n\tchar\toptionrom_version[64];\n\tchar\tfirmware_version[64];\n\tchar\tmodel[256];\n\tchar\tmodel_description[256];\n};\n#endif\n\n#define MAX_PHYS_ITEM_ID_LEN 32\n\n/* This structure holds a unique identifier to identify some\n * physical item (port for example) used by a netdevice.\n */\nstruct netdev_phys_item_id {\n\tunsigned char id[MAX_PHYS_ITEM_ID_LEN];\n\tunsigned char id_len;\n};\n\nstatic inline bool netdev_phys_item_id_same(struct netdev_phys_item_id *a,\n\t\t\t\t\t    struct netdev_phys_item_id *b)\n{\n\treturn a->id_len == b->id_len &&\n\t       memcmp(a->id, b->id, a->id_len) == 0;\n}\n\ntypedef u16 (*select_queue_fallback_t)(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb);\n\n/* These structures hold the attributes of qdisc and classifiers\n * that are being passed to the netdevice through the setup_tc op.\n */\nenum {\n\tTC_SETUP_MQPRIO,\n\tTC_SETUP_CLSU32,\n\tTC_SETUP_CLSFLOWER,\n};\n\nstruct tc_cls_u32_offload;\n\nstruct tc_to_netdev {\n\tunsigned int type;\n\tunion {\n\t\tu8 tc;\n\t\tstruct tc_cls_u32_offload *cls_u32;\n\t\tstruct tc_cls_flower_offload *cls_flower;\n\t};\n};\n\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when network device is registered.\n *     The network device can use this to any late stage initializaton\n *     or semantic validattion. It can fail with an error code which will\n *     be propogated back to register_netdev\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when network device transistions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when network device transistions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tReturns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop\n *\tthe queue before that can happen; it's for obsolete devices and weird\n *\tcorner cases, but the stack really does a non-trivial amount\n *\tof useless work if you return NETDEV_TX_BUSY.\n *        (can also return NETDEV_TX_LOCKED iff NETIF_F_LLTX)\n *\tRequired can not be NULL.\n *\n * netdev_features_t (*ndo_fix_features)(struct net_device *dev,\n *\t\tnetdev_features_t features);\n *\tAdjusts the requested feature flags according to device-specific\n *\tconstraints, and returns the resulting flags. Must not modify\n *\tthe device state.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,\n *                         void *accel_priv, select_queue_fallback_t fallback);\n *\tCalled to decide which queue to when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscious is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\tIf driver handles unicast address filtering, it should set\n *\tIFF_UNICAST_FLT to its priv_flags.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tmac address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user request an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctl's return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reason, new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device. If not defined, any request to change MTU will\n *\twill return an error.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev);\n *\tCallback uses when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n *                      struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device support VLAN filtering this function is called when a\n *\tVLAN id is registered.\n *\n * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device support VLAN filtering this function is called when a\n *\tVLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos);\n * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,\n *\t\t\t  int max_tx_rate);\n * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n *\n *      Enable or disable the VF ability to query its RSS Redirection Table and\n *      Hash Key. This is needed since on some devices VF share this information\n *      with PF and querying it may adduce a theoretical security risk.\n * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n * int (*ndo_setup_tc)(struct net_device *dev, u8 tc)\n * \tCalled to setup 'tc' number of traffic classes in the net device. This\n * \tis always called from the stack with the rtnl lock held and netif tx\n * \tqueues stopped. This allows the netdevice to perform queue management\n * \tsafely.\n *\n *\tFiber Channel over Ethernet (FCoE) offload functions.\n * int (*ndo_fcoe_enable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to start using LLD for FCoE\n *\tso the underlying device can perform whatever needed configuration or\n *\tinitialization to support acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_disable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to stop using LLD for FCoE\n *\tso the underlying device can perform whatever needed clean-ups to\n *\tstop supporting acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,\n *\t\t\t     struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Initiator wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);\n *\tCalled when the FCoE Initiator/Target is done with the DDPed I/O as\n *\tindicated by the FC exchange id 'xid', so the underlying device can\n *\tclean up and reuse resources for later DDP requests.\n *\n * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,\n *\t\t\t      struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Target wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n *\t\t\t       struct netdev_fcoe_hbainfo *hbainfo);\n *\tCalled when the FCoE Protocol stack wants information on the underlying\n *\tdevice. This information is utilized by the FCoE protocol stack to\n *\tregister attributes with Fiber Channel management service as per the\n *\tFC-GS Fabric Device Management Information(FDMI) specification.\n *\n * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);\n *\tCalled when the underlying device wants to override default World Wide\n *\tName (WWN) generation mechanism in FCoE protocol stack to pass its own\n *\tWorld Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE\n *\tprotocol stack to use.\n *\n *\tRFS acceleration.\n * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,\n *\t\t\t    u16 rxq_index, u32 flow_id);\n *\tSet hardware filter for RFS.  rxq_index is the target queue index;\n *\tflow_id is a flow ID to be passed to rps_may_expire_flow() later.\n *\tReturn the filter ID on success, or a negative error code.\n *\n *\tSlave management functions (for bridge, bonding, etc).\n * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to make another netdev an underling.\n *\n * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to release previously enslaved netdev.\n *\n *      Feature/offload setting functions.\n * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);\n *\tCalled to update device configuration to new features. Passed\n *\tfeature set might be less than what was returned by ndo_fix_features()).\n *\tMust return >0 or -errno if it changed dev->features itself.\n *\n * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid, u16 flags)\n *\tAdds an FDB entry to dev for addr.\n * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid)\n *\tDeletes the FDB entry from dev coresponding to addr.\n * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,\n *\t\t       struct net_device *dev, struct net_device *filter_dev,\n *\t\t       int idx)\n *\tUsed to add FDB entries to dump requests. Implementers should add\n *\tentries to skb and update idx with the number of entries.\n *\n * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags)\n * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,\n *\t\t\t     struct net_device *dev, u32 filter_mask,\n *\t\t\t     int nlflags)\n * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags);\n *\n * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);\n *\tCalled to change device carrier. Soft-devices (like dummy, team, etc)\n *\twhich do not represent real hardware may define this to allow their\n *\tuserspace components to manage their virtual carrier state. Devices\n *\tthat determine carrier state from physical hardware properties (eg\n *\tnetwork cables) or protocol-dependent mechanisms (eg\n *\tUSB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.\n *\n * int (*ndo_get_phys_port_id)(struct net_device *dev,\n *\t\t\t       struct netdev_phys_item_id *ppid);\n *\tCalled to get ID of physical port of this device. If driver does\n *\tnot implement this, it is assumed that the hw is not able to have\n *\tmultiple net devices on single physical port.\n *\n * void (*ndo_add_vxlan_port)(struct  net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by vxlan to notiy a driver about the UDP port and socket\n *\taddress family that vxlan is listnening to. It is called only when\n *\ta new port starts listening. The operation is protected by the\n *\tvxlan_net->sock_lock.\n *\n * void (*ndo_add_geneve_port)(struct net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by geneve to notify a driver about the UDP port and socket\n *\taddress family that geneve is listnening to. It is called only when\n *\ta new port starts listening. The operation is protected by the\n *\tgeneve_net->sock_lock.\n *\n * void (*ndo_del_geneve_port)(struct net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by geneve to notify the driver about a UDP port and socket\n *\taddress family that geneve is not listening to anymore. The operation\n *\tis protected by the geneve_net->sock_lock.\n *\n * void (*ndo_del_vxlan_port)(struct  net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by vxlan to notify the driver about a UDP port and socket\n *\taddress family that vxlan is not listening to anymore. The operation\n *\tis protected by the vxlan_net->sock_lock.\n *\n * void* (*ndo_dfwd_add_station)(struct net_device *pdev,\n *\t\t\t\t struct net_device *dev)\n *\tCalled by upper layer devices to accelerate switching or other\n *\tstation functionality into hardware. 'pdev is the lowerdev\n *\tto use for the offload and 'dev' is the net device that will\n *\tback the offload. Returns a pointer to the private structure\n *\tthe upper layer will maintain.\n * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)\n *\tCalled by upper layer device to delete the station created\n *\tby 'ndo_dfwd_add_station'. 'pdev' is the net device backing\n *\tthe station and priv is the structure returned by the add\n *\toperation.\n * netdev_tx_t (*ndo_dfwd_start_xmit)(struct sk_buff *skb,\n *\t\t\t\t      struct net_device *dev,\n *\t\t\t\t      void *priv);\n *\tCallback to use for xmit over the accelerated station. This\n *\tis used in place of ndo_start_xmit on accelerated net\n *\tdevices.\n * netdev_features_t (*ndo_features_check) (struct sk_buff *skb,\n *\t\t\t\t\t    struct net_device *dev\n *\t\t\t\t\t    netdev_features_t features);\n *\tCalled by core transmit path to determine if device is capable of\n *\tperforming offload operations on a given packet. This is to give\n *\tthe device an opportunity to implement any restrictions that cannot\n *\tbe otherwise expressed by feature flags. The check is called with\n *\tthe set of features that the stack has calculated and it returns\n *\tthose the driver believes to be appropriate.\n * int (*ndo_set_tx_maxrate)(struct net_device *dev,\n *\t\t\t     int queue_index, u32 maxrate);\n *\tCalled when a user wants to set a max-rate limitation of specific\n *\tTX queue.\n * int (*ndo_get_iflink)(const struct net_device *dev);\n *\tCalled to get the iflink value of this device.\n * void (*ndo_change_proto_down)(struct net_device *dev,\n *\t\t\t\t  bool proto_down);\n *\tThis function is used to pass protocol port error state information\n *\tto the switch driver. The switch driver can react to the proto_down\n *      by doing a phys down on the associated switch port.\n * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);\n *\tThis function is used to get egress tunnel information for given skb.\n *\tThis is useful for retrieving outer tunnel header parameters while\n *\tsampling packet.\n * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);\n *\tThis function is used to specify the headroom that the skb must\n *\tconsider when allocation skb during packet reception. Setting\n *\tappropriate rx headroom value allows avoiding skb head copy on\n *\tforward. Setting a negative value reset the rx headroom to the\n *\tdefault value.\n *\n */\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit)(struct sk_buff *skb,\n\t\t\t\t\t\t  struct net_device *dev);\n\tnetdev_features_t\t(*ndo_features_check)(struct sk_buff *skb,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      netdev_features_t features);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    void *accel_priv,\n\t\t\t\t\t\t    select_queue_fallback_t fallback);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev);\n\n\tstruct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t     struct rtnl_link_stats64 *storage);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tint\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       __be16 proto, u16 vid);\n\tint\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        __be16 proto, u16 vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tint\t\t\t(*ndo_busy_poll)(struct napi_struct *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan, u8 qos);\n\tint\t\t\t(*ndo_set_vf_rate)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, int min_tx_rate,\n\t\t\t\t\t\t   int max_tx_rate);\n\tint\t\t\t(*ndo_set_vf_spoofchk)(struct net_device *dev,\n\t\t\t\t\t\t       int vf, bool setting);\n\tint\t\t\t(*ndo_set_vf_trust)(struct net_device *dev,\n\t\t\t\t\t\t    int vf, bool setting);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_link_state)(struct net_device *dev,\n\t\t\t\t\t\t\t int vf, int link_state);\n\tint\t\t\t(*ndo_get_vf_stats)(struct net_device *dev,\n\t\t\t\t\t\t    int vf,\n\t\t\t\t\t\t    struct ifla_vf_stats\n\t\t\t\t\t\t    *vf_stats);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n\tint\t\t\t(*ndo_set_vf_rss_query_en)(\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   int vf, bool setting);\n\tint\t\t\t(*ndo_setup_tc)(struct net_device *dev,\n\t\t\t\t\t\tu32 handle,\n\t\t\t\t\t\t__be16 protocol,\n\t\t\t\t\t\tstruct tc_to_netdev *tc);\n#if IS_ENABLED(CONFIG_FCOE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n\tint\t\t\t(*ndo_fcoe_ddp_target)(struct net_device *dev,\n\t\t\t\t\t\t       u16 xid,\n\t\t\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t\t\t       unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_fcoe_hbainfo *hbainfo);\n#endif\n\n#if IS_ENABLED(CONFIG_LIBFCOE)\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n\n#ifdef CONFIG_RFS_ACCEL\n\tint\t\t\t(*ndo_rx_flow_steer)(struct net_device *dev,\n\t\t\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t\t\t     u16 rxq_index,\n\t\t\t\t\t\t     u32 flow_id);\n#endif\n\tint\t\t\t(*ndo_add_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tint\t\t\t(*ndo_del_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tnetdev_features_t\t(*ndo_fix_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_set_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_neigh_construct)(struct neighbour *n);\n\tvoid\t\t\t(*ndo_neigh_destroy)(struct neighbour *n);\n\n\tint\t\t\t(*ndo_fdb_add)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid,\n\t\t\t\t\t       u16 flags);\n\tint\t\t\t(*ndo_fdb_del)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid);\n\tint\t\t\t(*ndo_fdb_dump)(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\tstruct net_device *filter_dev,\n\t\t\t\t\t\tint idx);\n\n\tint\t\t\t(*ndo_bridge_setlink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_bridge_getlink)(struct sk_buff *skb,\n\t\t\t\t\t\t      u32 pid, u32 seq,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      u32 filter_mask,\n\t\t\t\t\t\t      int nlflags);\n\tint\t\t\t(*ndo_bridge_dellink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_change_carrier)(struct net_device *dev,\n\t\t\t\t\t\t      bool new_carrier);\n\tint\t\t\t(*ndo_get_phys_port_id)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_phys_port_name)(struct net_device *dev,\n\t\t\t\t\t\t\t  char *name, size_t len);\n\tvoid\t\t\t(*ndo_add_vxlan_port)(struct  net_device *dev,\n\t\t\t\t\t\t      sa_family_t sa_family,\n\t\t\t\t\t\t      __be16 port);\n\tvoid\t\t\t(*ndo_del_vxlan_port)(struct  net_device *dev,\n\t\t\t\t\t\t      sa_family_t sa_family,\n\t\t\t\t\t\t      __be16 port);\n\tvoid\t\t\t(*ndo_add_geneve_port)(struct  net_device *dev,\n\t\t\t\t\t\t       sa_family_t sa_family,\n\t\t\t\t\t\t       __be16 port);\n\tvoid\t\t\t(*ndo_del_geneve_port)(struct  net_device *dev,\n\t\t\t\t\t\t       sa_family_t sa_family,\n\t\t\t\t\t\t       __be16 port);\n\tvoid*\t\t\t(*ndo_dfwd_add_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tstruct net_device *dev);\n\tvoid\t\t\t(*ndo_dfwd_del_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tvoid *priv);\n\n\tnetdev_tx_t\t\t(*ndo_dfwd_start_xmit) (struct sk_buff *skb,\n\t\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\t\tvoid *priv);\n\tint\t\t\t(*ndo_get_lock_subclass)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_tx_maxrate)(struct net_device *dev,\n\t\t\t\t\t\t      int queue_index,\n\t\t\t\t\t\t      u32 maxrate);\n\tint\t\t\t(*ndo_get_iflink)(const struct net_device *dev);\n\tint\t\t\t(*ndo_change_proto_down)(struct net_device *dev,\n\t\t\t\t\t\t\t bool proto_down);\n\tint\t\t\t(*ndo_fill_metadata_dst)(struct net_device *dev,\n\t\t\t\t\t\t       struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_set_rx_headroom)(struct net_device *dev,\n\t\t\t\t\t\t       int needed_headroom);\n};\n\n/**\n * enum net_device_priv_flags - &struct net_device priv_flags\n *\n * These are the &struct net_device, they are only set internally\n * by drivers and used in the kernel. These flags are invisible to\n * userspace, this means that the order of these flags can change\n * during any kernel release.\n *\n * You should have a pretty good reason to be extending these flags.\n *\n * @IFF_802_1Q_VLAN: 802.1Q VLAN device\n * @IFF_EBRIDGE: Ethernet bridging device\n * @IFF_BONDING: bonding master or slave\n * @IFF_ISATAP: ISATAP interface (RFC4214)\n * @IFF_WAN_HDLC: WAN HDLC device\n * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to\n *\trelease skb->dst\n * @IFF_DONT_BRIDGE: disallow bridging this ether dev\n * @IFF_DISABLE_NETPOLL: disable netpoll at run-time\n * @IFF_MACVLAN_PORT: device used as macvlan port\n * @IFF_BRIDGE_PORT: device used as bridge port\n * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port\n * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit\n * @IFF_UNICAST_FLT: Supports unicast filtering\n * @IFF_TEAM_PORT: device used as team port\n * @IFF_SUPP_NOFCS: device supports sending custom FCS\n * @IFF_LIVE_ADDR_CHANGE: device supports hardware address\n *\tchange when it's running\n * @IFF_MACVLAN: Macvlan device\n * @IFF_L3MDEV_MASTER: device is an L3 master device\n * @IFF_NO_QUEUE: device can run without qdisc attached\n * @IFF_OPENVSWITCH: device is a Open vSwitch master\n * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device\n * @IFF_TEAM: device is a team device\n * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured\n * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external\n *\tentity (i.e. the master device for bridged veth)\n * @IFF_MACSEC: device is a MACsec device\n */\nenum netdev_priv_flags {\n\tIFF_802_1Q_VLAN\t\t\t= 1<<0,\n\tIFF_EBRIDGE\t\t\t= 1<<1,\n\tIFF_BONDING\t\t\t= 1<<2,\n\tIFF_ISATAP\t\t\t= 1<<3,\n\tIFF_WAN_HDLC\t\t\t= 1<<4,\n\tIFF_XMIT_DST_RELEASE\t\t= 1<<5,\n\tIFF_DONT_BRIDGE\t\t\t= 1<<6,\n\tIFF_DISABLE_NETPOLL\t\t= 1<<7,\n\tIFF_MACVLAN_PORT\t\t= 1<<8,\n\tIFF_BRIDGE_PORT\t\t\t= 1<<9,\n\tIFF_OVS_DATAPATH\t\t= 1<<10,\n\tIFF_TX_SKB_SHARING\t\t= 1<<11,\n\tIFF_UNICAST_FLT\t\t\t= 1<<12,\n\tIFF_TEAM_PORT\t\t\t= 1<<13,\n\tIFF_SUPP_NOFCS\t\t\t= 1<<14,\n\tIFF_LIVE_ADDR_CHANGE\t\t= 1<<15,\n\tIFF_MACVLAN\t\t\t= 1<<16,\n\tIFF_XMIT_DST_RELEASE_PERM\t= 1<<17,\n\tIFF_IPVLAN_MASTER\t\t= 1<<18,\n\tIFF_IPVLAN_SLAVE\t\t= 1<<19,\n\tIFF_L3MDEV_MASTER\t\t= 1<<20,\n\tIFF_NO_QUEUE\t\t\t= 1<<21,\n\tIFF_OPENVSWITCH\t\t\t= 1<<22,\n\tIFF_L3MDEV_SLAVE\t\t= 1<<23,\n\tIFF_TEAM\t\t\t= 1<<24,\n\tIFF_RXFH_CONFIGURED\t\t= 1<<25,\n\tIFF_PHONY_HEADROOM\t\t= 1<<26,\n\tIFF_MACSEC\t\t\t= 1<<27,\n};\n\n#define IFF_802_1Q_VLAN\t\t\tIFF_802_1Q_VLAN\n#define IFF_EBRIDGE\t\t\tIFF_EBRIDGE\n#define IFF_BONDING\t\t\tIFF_BONDING\n#define IFF_ISATAP\t\t\tIFF_ISATAP\n#define IFF_WAN_HDLC\t\t\tIFF_WAN_HDLC\n#define IFF_XMIT_DST_RELEASE\t\tIFF_XMIT_DST_RELEASE\n#define IFF_DONT_BRIDGE\t\t\tIFF_DONT_BRIDGE\n#define IFF_DISABLE_NETPOLL\t\tIFF_DISABLE_NETPOLL\n#define IFF_MACVLAN_PORT\t\tIFF_MACVLAN_PORT\n#define IFF_BRIDGE_PORT\t\t\tIFF_BRIDGE_PORT\n#define IFF_OVS_DATAPATH\t\tIFF_OVS_DATAPATH\n#define IFF_TX_SKB_SHARING\t\tIFF_TX_SKB_SHARING\n#define IFF_UNICAST_FLT\t\t\tIFF_UNICAST_FLT\n#define IFF_TEAM_PORT\t\t\tIFF_TEAM_PORT\n#define IFF_SUPP_NOFCS\t\t\tIFF_SUPP_NOFCS\n#define IFF_LIVE_ADDR_CHANGE\t\tIFF_LIVE_ADDR_CHANGE\n#define IFF_MACVLAN\t\t\tIFF_MACVLAN\n#define IFF_XMIT_DST_RELEASE_PERM\tIFF_XMIT_DST_RELEASE_PERM\n#define IFF_IPVLAN_MASTER\t\tIFF_IPVLAN_MASTER\n#define IFF_IPVLAN_SLAVE\t\tIFF_IPVLAN_SLAVE\n#define IFF_L3MDEV_MASTER\t\tIFF_L3MDEV_MASTER\n#define IFF_NO_QUEUE\t\t\tIFF_NO_QUEUE\n#define IFF_OPENVSWITCH\t\t\tIFF_OPENVSWITCH\n#define IFF_L3MDEV_SLAVE\t\tIFF_L3MDEV_SLAVE\n#define IFF_TEAM\t\t\tIFF_TEAM\n#define IFF_RXFH_CONFIGURED\t\tIFF_RXFH_CONFIGURED\n#define IFF_MACSEC\t\t\tIFF_MACSEC\n\n/**\n *\tstruct net_device - The DEVICE structure.\n *\t\tActually, this whole structure is a big mistake.  It mixes I/O\n *\t\tdata with strictly \"high-level\" data, and it has to know about\n *\t\talmost every data structure used in the INET module.\n *\n *\t@name:\tThis is the first field of the \"visible\" part of this structure\n *\t\t(i.e. as seen by users in the \"Space.c\" file).  It is the name\n *\t \tof the interface.\n *\n *\t@name_hlist: \tDevice name hash chain, please keep it close to name[]\n *\t@ifalias:\tSNMP alias\n *\t@mem_end:\tShared memory end\n *\t@mem_start:\tShared memory start\n *\t@base_addr:\tDevice I/O address\n *\t@irq:\t\tDevice IRQ number\n *\n *\t@carrier_changes:\tStats to monitor carrier on<->off transitions\n *\n *\t@state:\t\tGeneric network queuing layer state, see netdev_state_t\n *\t@dev_list:\tThe global list of network devices\n *\t@napi_list:\tList entry, that is used for polling napi devices\n *\t@unreg_list:\tList entry, that is used, when we are unregistering the\n *\t\t\tdevice, see the function unregister_netdev\n *\t@close_list:\tList entry, that is used, when we are closing the device\n *\n *\t@adj_list:\tDirectly linked devices, like slaves for bonding\n *\t@all_adj_list:\tAll linked devices, *including* neighbours\n *\t@features:\tCurrently active device features\n *\t@hw_features:\tUser-changeable features\n *\n *\t@wanted_features:\tUser-requested features\n *\t@vlan_features:\t\tMask of features inheritable by VLAN devices\n *\n *\t@hw_enc_features:\tMask of features inherited by encapsulating devices\n *\t\t\t\tThis field indicates what encapsulation\n *\t\t\t\toffloads the hardware is capable of doing,\n *\t\t\t\tand drivers will need to set them appropriately.\n *\n *\t@mpls_features:\tMask of features inheritable by MPLS\n *\n *\t@ifindex:\tinterface index\n *\t@group:\t\tThe group, that the device belongs to\n *\n *\t@stats:\t\tStatistics struct, which was left as a legacy, use\n *\t\t\trtnl_link_stats64 instead\n *\n *\t@rx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@tx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@rx_nohandler:\tnohandler dropped packets by core network on\n *\t\t\tinactive devices, do not use this in drivers\n *\n *\t@wireless_handlers:\tList of functions to handle Wireless Extensions,\n *\t\t\t\tinstead of ioctl,\n *\t\t\t\tsee <net/iw_handler.h> for details.\n *\t@wireless_data:\tInstance data managed by the core of wireless extensions\n *\n *\t@netdev_ops:\tIncludes several pointers to callbacks,\n *\t\t\tif one wants to override the ndo_*() functions\n *\t@ethtool_ops:\tManagement operations\n *\t@header_ops:\tIncludes callbacks for creating,parsing,caching,etc\n *\t\t\tof Layer 2 headers.\n *\n *\t@flags:\t\tInterface flags (a la BSD)\n *\t@priv_flags:\tLike 'flags' but invisible to userspace,\n *\t\t\tsee if.h for the definitions\n *\t@gflags:\tGlobal flags ( kept as legacy )\n *\t@padded:\tHow much padding added by alloc_netdev()\n *\t@operstate:\tRFC2863 operstate\n *\t@link_mode:\tMapping policy to operstate\n *\t@if_port:\tSelectable AUI, TP, ...\n *\t@dma:\t\tDMA channel\n *\t@mtu:\t\tInterface MTU value\n *\t@type:\t\tInterface hardware type\n *\t@hard_header_len: Maximum hardware header length.\n *\n *\t@needed_headroom: Extra headroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed\n *\t@needed_tailroom: Extra tailroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed. Some cases also use\n *\t\t\t  LL_MAX_HEADER instead to allocate the skb\n *\n *\tinterface address info:\n *\n * \t@perm_addr:\t\tPermanent hw address\n * \t@addr_assign_type:\tHw address assignment type\n * \t@addr_len:\t\tHardware address length\n * \t@neigh_priv_len;\tUsed in neigh_alloc(),\n * \t\t\t\tinitialized only in atm/clip.c\n * \t@dev_id:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same link layer address\n * \t@dev_port:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same function\n *\t@addr_list_lock:\tXXX: need comments on this one\n *\t@uc_promisc:\t\tCounter, that indicates, that promiscuous mode\n *\t\t\t\thas been enabled due to the need to listen to\n *\t\t\t\tadditional unicast addresses in a device that\n *\t\t\t\tdoes not implement ndo_set_rx_mode()\n *\t@uc:\t\t\tunicast mac addresses\n *\t@mc:\t\t\tmulticast mac addresses\n *\t@dev_addrs:\t\tlist of device hw addresses\n *\t@queues_kset:\t\tGroup of all Kobjects in the Tx and RX queues\n *\t@promiscuity:\t\tNumber of times, the NIC is told to work in\n *\t\t\t\tPromiscuous mode, if it becomes 0 the NIC will\n *\t\t\t\texit from working in Promiscuous mode\n *\t@allmulti:\t\tCounter, enables or disables allmulticast mode\n *\n *\t@vlan_info:\tVLAN info\n *\t@dsa_ptr:\tdsa specific data\n *\t@tipc_ptr:\tTIPC specific data\n *\t@atalk_ptr:\tAppleTalk link\n *\t@ip_ptr:\tIPv4 specific data\n *\t@dn_ptr:\tDECnet specific data\n *\t@ip6_ptr:\tIPv6 specific data\n *\t@ax25_ptr:\tAX.25 specific data\n *\t@ieee80211_ptr:\tIEEE 802.11 specific data, assign before registering\n *\n *\t@last_rx:\tTime of last Rx\n *\t@dev_addr:\tHw address (before bcast,\n *\t\t\tbecause most packets are unicast)\n *\n *\t@_rx:\t\t\tArray of RX queues\n *\t@num_rx_queues:\t\tNumber of RX queues\n *\t\t\t\tallocated at register_netdev() time\n *\t@real_num_rx_queues: \tNumber of RX queues currently active in device\n *\n *\t@rx_handler:\t\thandler for received packets\n *\t@rx_handler_data: \tXXX: need comments on this one\n *\t@ingress_queue:\t\tXXX: need comments on this one\n *\t@broadcast:\t\thw bcast address\n *\n *\t@rx_cpu_rmap:\tCPU reverse-mapping for RX completion interrupts,\n *\t\t\tindexed by RX queue number. Assigned by driver.\n *\t\t\tThis must only be set if the ndo_rx_flow_steer\n *\t\t\toperation is defined\n *\t@index_hlist:\t\tDevice index hash chain\n *\n *\t@_tx:\t\t\tArray of TX queues\n *\t@num_tx_queues:\t\tNumber of TX queues allocated at alloc_netdev_mq() time\n *\t@real_num_tx_queues: \tNumber of TX queues currently active in device\n *\t@qdisc:\t\t\tRoot qdisc from userspace point of view\n *\t@tx_queue_len:\t\tMax frames per queue allowed\n *\t@tx_global_lock: \tXXX: need comments on this one\n *\n *\t@xps_maps:\tXXX: need comments on this one\n *\n *\t@offload_fwd_mark:\tOffload device fwding mark\n *\n *\t@trans_start:\t\tTime (in jiffies) of last Tx\n *\t@watchdog_timeo:\tRepresents the timeout that is used by\n *\t\t\t\tthe watchdog ( see dev_watchdog() )\n *\t@watchdog_timer:\tList of timers\n *\n *\t@pcpu_refcnt:\t\tNumber of references to this device\n *\t@todo_list:\t\tDelayed register/unregister\n *\t@link_watch_list:\tXXX: need comments on this one\n *\n *\t@reg_state:\t\tRegister/unregister state machine\n *\t@dismantle:\t\tDevice is going to be freed\n *\t@rtnl_link_state:\tThis enum represents the phases of creating\n *\t\t\t\ta new link\n *\n *\t@destructor:\t\tCalled from unregister,\n *\t\t\t\tcan be used to call free_netdev\n *\t@npinfo:\t\tXXX: need comments on this one\n * \t@nd_net:\t\tNetwork namespace this network device is inside\n *\n * \t@ml_priv:\tMid-layer private\n * \t@lstats:\tLoopback statistics\n * \t@tstats:\tTunnel statistics\n * \t@dstats:\tDummy statistics\n * \t@vstats:\tVirtual ethernet statistics\n *\n *\t@garp_port:\tGARP\n *\t@mrp_port:\tMRP\n *\n *\t@dev:\t\tClass/net/name entry\n *\t@sysfs_groups:\tSpace for optional device, statistics and wireless\n *\t\t\tsysfs groups\n *\n *\t@sysfs_rx_queue_group:\tSpace for optional per-rx queue attributes\n *\t@rtnl_link_ops:\tRtnl_link_ops\n *\n *\t@gso_max_size:\tMaximum size of generic segmentation offload\n *\t@gso_max_segs:\tMaximum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\t@gso_min_segs:\tMinimum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\n *\t@dcbnl_ops:\tData Center Bridging netlink ops\n *\t@num_tc:\tNumber of traffic classes in the net device\n *\t@tc_to_txq:\tXXX: need comments on this one\n *\t@prio_tc_map\tXXX: need comments on this one\n *\n *\t@fcoe_ddp_xid:\tMax exchange id for FCoE LRO by ddp\n *\n *\t@priomap:\tXXX: need comments on this one\n *\t@phydev:\tPhysical device may attach itself\n *\t\t\tfor hardware timestamping\n *\n *\t@qdisc_tx_busylock:\tXXX: need comments on this one\n *\n *\t@proto_down:\tprotocol port state information can be sent to the\n *\t\t\tswitch driver and used to set the phys state of the\n *\t\t\tswitch port.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\tchar\t\t\tname[IFNAMSIZ];\n\tstruct hlist_node\tname_hlist;\n\tchar \t\t\t*ifalias;\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\n\tunsigned long\t\tmem_start;\n\tunsigned long\t\tbase_addr;\n\tint\t\t\tirq;\n\n\tatomic_t\t\tcarrier_changes;\n\n\t/*\n\t *\tSome hardware also needs these fields (state,dev_list,\n\t *\tnapi_list,unreg_list,close_list) but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\tstruct list_head\tclose_list;\n\tstruct list_head\tptype_all;\n\tstruct list_head\tptype_specific;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} adj_list;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} all_adj_list;\n\n\tnetdev_features_t\tfeatures;\n\tnetdev_features_t\thw_features;\n\tnetdev_features_t\twanted_features;\n\tnetdev_features_t\tvlan_features;\n\tnetdev_features_t\thw_enc_features;\n\tnetdev_features_t\tmpls_features;\n\n\tint\t\t\tifindex;\n\tint\t\t\tgroup;\n\n\tstruct net_device_stats\tstats;\n\n\tatomic_long_t\t\trx_dropped;\n\tatomic_long_t\t\ttx_dropped;\n\tatomic_long_t\t\trx_nohandler;\n\n#ifdef CONFIG_WIRELESS_EXT\n\tconst struct iw_handler_def *\twireless_handlers;\n\tstruct iw_public_data *\twireless_data;\n#endif\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_SWITCHDEV\n\tconst struct switchdev_ops *switchdev_ops;\n#endif\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tconst struct l3mdev_ops\t*l3mdev_ops;\n#endif\n\n\tconst struct header_ops *header_ops;\n\n\tunsigned int\t\tflags;\n\tunsigned int\t\tpriv_flags;\n\n\tunsigned short\t\tgflags;\n\tunsigned short\t\tpadded;\n\n\tunsigned char\t\toperstate;\n\tunsigned char\t\tlink_mode;\n\n\tunsigned char\t\tif_port;\n\tunsigned char\t\tdma;\n\n\tunsigned int\t\tmtu;\n\tunsigned short\t\ttype;\n\tunsigned short\t\thard_header_len;\n\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN];\n\tunsigned char\t\taddr_assign_type;\n\tunsigned char\t\taddr_len;\n\tunsigned short\t\tneigh_priv_len;\n\tunsigned short          dev_id;\n\tunsigned short          dev_port;\n\tspinlock_t\t\taddr_list_lock;\n\tunsigned char\t\tname_assign_type;\n\tbool\t\t\tuc_promisc;\n\tstruct netdev_hw_addr_list\tuc;\n\tstruct netdev_hw_addr_list\tmc;\n\tstruct netdev_hw_addr_list\tdev_addrs;\n\n#ifdef CONFIG_SYSFS\n\tstruct kset\t\t*queues_kset;\n#endif\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\n\n\t/* Protocol specific pointers */\n\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\n\tstruct vlan_info __rcu\t*vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tstruct dsa_switch_tree\t*dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\n\tstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n\tvoid \t\t\t*atalk_ptr;\n\tstruct in_device __rcu\t*ip_ptr;\n\tstruct dn_dev __rcu     *dn_ptr;\n\tstruct inet6_dev __rcu\t*ip6_ptr;\n\tvoid\t\t\t*ax25_ptr;\n\tstruct wireless_dev\t*ieee80211_ptr;\n\tstruct wpan_dev\t\t*ieee802154_ptr;\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\n\tstruct mpls_dev __rcu\t*mpls_ptr;\n#endif\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\tunsigned long\t\tlast_rx;\n\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\n\n\n#ifdef CONFIG_SYSFS\n\tstruct netdev_rx_queue\t*_rx;\n\n\tunsigned int\t\tnum_rx_queues;\n\tunsigned int\t\treal_num_rx_queues;\n\n#endif\n\n\tunsigned long\t\tgro_flush_timeout;\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *ingress_cl_list;\n#endif\n\tstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\n\tstruct list_head\tnf_hooks_ingress;\n#endif\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap\t\t*rx_cpu_rmap;\n#endif\n\tstruct hlist_node\tindex_hlist;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\tunsigned int\t\tnum_tx_queues;\n\tunsigned int\t\treal_num_tx_queues;\n\tstruct Qdisc\t\t*qdisc;\n\tunsigned long\t\ttx_queue_len;\n\tspinlock_t\t\ttx_global_lock;\n\tint\t\t\twatchdog_timeo;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *egress_cl_list;\n#endif\n#ifdef CONFIG_NET_SWITCHDEV\n\tu32\t\t\toffload_fwd_mark;\n#endif\n\n\t/* These may be needed for future network-power-down code. */\n\n\t/*\n\t * trans_start here is expensive for high speed devices on SMP,\n\t * please use netdev_queue->trans_start instead.\n\t */\n\tunsigned long\t\ttrans_start;\n\n\tstruct timer_list\twatchdog_timer;\n\n\tint __percpu\t\t*pcpu_refcnt;\n\tstruct list_head\ttodo_list;\n\n\tstruct list_head\tlink_watch_list;\n\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:8;\n\n\tbool dismantle;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\tvoid (*destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info __rcu\t*npinfo;\n#endif\n\n\tpossible_net_t\t\t\tnd_net;\n\n\t/* mid-layer private */\n\tunion {\n\t\tvoid\t\t\t\t\t*ml_priv;\n\t\tstruct pcpu_lstats __percpu\t\t*lstats;\n\t\tstruct pcpu_sw_netstats __percpu\t*tstats;\n\t\tstruct pcpu_dstats __percpu\t\t*dstats;\n\t\tstruct pcpu_vstats __percpu\t\t*vstats;\n\t};\n\n\tstruct garp_port __rcu\t*garp_port;\n\tstruct mrp_port __rcu\t*mrp_port;\n\n\tstruct device\tdev;\n\tconst struct attribute_group *sysfs_groups[4];\n\tconst struct attribute_group *sysfs_rx_queue_group;\n\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n#define GSO_MAX_SEGS\t\t65535\n\tu16\t\t\tgso_max_segs;\n\tu16\t\t\tgso_min_segs;\n#ifdef CONFIG_DCB\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\tu8 num_tc;\n\tstruct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];\n\tu8 prio_tc_map[TC_BITMASK + 1];\n\n#if IS_ENABLED(CONFIG_FCOE)\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\n\tstruct netprio_map __rcu *priomap;\n#endif\n\tstruct phy_device *phydev;\n\tstruct lock_class_key *qdisc_tx_busylock;\n\tbool proto_down;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nint netdev_get_prio_tc_map(const struct net_device *dev, u32 prio)\n{\n\treturn dev->prio_tc_map[prio & TC_BITMASK];\n}\n\nstatic inline\nint netdev_set_prio_tc_map(struct net_device *dev, u8 prio, u8 tc)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->prio_tc_map[prio & TC_BITMASK] = tc & TC_BITMASK;\n\treturn 0;\n}\n\nstatic inline\nvoid netdev_reset_tc(struct net_device *dev)\n{\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\n\nstatic inline\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\n\nstatic inline\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\n\nstatic inline\nint netdev_get_num_tc(struct net_device *dev)\n{\n\treturn dev->num_tc;\n}\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline struct netdev_queue *skb_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t\t    const struct sk_buff *skb)\n{\n\treturn netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv);\n\n/* returns the headroom that the master device needs to take in account\n * when forwarding to this dev\n */\nstatic inline unsigned netdev_get_fwd_headroom(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_PHONY_HEADROOM ? 0 : dev->needed_headroom;\n}\n\nstatic inline void netdev_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tif (dev->netdev_ops->ndo_set_rx_headroom)\n\t\tdev->netdev_ops->ndo_set_rx_headroom(dev, new_hr);\n}\n\n/* set the device rx headroom to the dev's default */\nstatic inline void netdev_reset_rx_headroom(struct net_device *dev)\n{\n\tnetdev_set_rx_headroom(dev, -1);\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n\twrite_pnet(&dev->nd_net, net);\n}\n\nstatic inline bool netdev_uses_dsa(struct net_device *dev)\n{\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tif (dev->dsa_ptr != NULL)\n\t\treturn dsa_uses_tagged_protocol(dev->dsa_ptr);\n#endif\n\treturn false;\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fine-grained identification of different network device types. For\n * example Ethernet, Wirelss LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/* Default NAPI poll() weight\n * Device drivers are strongly advised to not use bigger value\n */\n#define NAPI_POLL_WEIGHT 64\n\n/**\n *\tnetif_napi_add - initialize a napi context\n *\t@dev:  network device\n *\t@napi: napi context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a napi context prior to calling\n * *any* of the other napi related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *\tnetif_tx_napi_add - initialize a napi context\n *\t@dev:  network device\n *\t@napi: napi context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * This variant of netif_napi_add() should be used from drivers using NAPI\n * to exclusively poll a TX queue.\n * This will avoid we add it into napi_hash[], thus polluting this hash table.\n */\nstatic inline void netif_tx_napi_add(struct net_device *dev,\n\t\t\t\t     struct napi_struct *napi,\n\t\t\t\t     int (*poll)(struct napi_struct *, int),\n\t\t\t\t     int weight)\n{\n\tset_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state);\n\tnetif_napi_add(dev, napi, poll, weight);\n}\n\n/**\n *  netif_napi_del - remove a napi context\n *  @napi: napi context\n *\n *  netif_napi_del() removes a napi context from the network device napi list\n */\nvoid netif_napi_del(struct napi_struct *napi);\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid *frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint data_offset;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tu16\tflush;\n\n\t/* Save the IP ID here and check when we get to the transport layer */\n\tu16\tflush_id;\n\n\t/* Number of segments aggregated. */\n\tu16\tcount;\n\n\t/* Start offset for remote checksum offload */\n\tu16\tgro_remcsum_start;\n\n\t/* jiffies when first packet was created/queued */\n\tunsigned long age;\n\n\t/* Used in ipv6_gro_receive() and foo-over-udp */\n\tu16\tproto;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tu8\tsame_flow:1;\n\n\t/* Used in udp_gro_receive */\n\tu8\tudp_mark:1;\n\n\t/* GRO checksum is valid */\n\tu8\tcsum_valid:1;\n\n\t/* Number of checksums via CHECKSUM_UNNECESSARY */\n\tu8\tcsum_cnt:3;\n\n\t/* Free the skb? */\n\tu8\tfree:2;\n#define NAPI_GRO_FREE\t\t  1\n#define NAPI_GRO_FREE_STOLEN_HEAD 2\n\n\t/* Used in foo-over-udp, set in udp[46]_gro_receive */\n\tu8\tis_ipv6:1;\n\n\t/* 7 bit hole */\n\n\t/* used to support CHECKSUM_COMPLETE for tunneling protocols */\n\t__wsum\tcsum;\n\n\t/* used in skb_gro_receive() slow path */\n\tstruct sk_buff *last;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tbool\t\t\t(*id_match)(struct packet_type *ptype,\n\t\t\t\t\t    struct sock *sk);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\nstruct offload_callbacks {\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tnetdev_features_t features);\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t\t struct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb, int nhoff);\n};\n\nstruct packet_offload {\n\t__be16\t\t\t type;\t/* This is really htons(ether_type). */\n\tu16\t\t\t priority;\n\tstruct offload_callbacks callbacks;\n\tstruct list_head\t list;\n};\n\nstruct udp_offload;\n\nstruct udp_offload_callbacks {\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t\t struct udp_offload *uoff);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb,\n\t\t\t\t\t\tint nhoff,\n\t\t\t\t\t\tstruct udp_offload *uoff);\n};\n\nstruct udp_offload {\n\t__be16\t\t\t port;\n\tu8\t\t\t ipproto;\n\tstruct udp_offload_callbacks callbacks;\n};\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_sw_netstats {\n\tu64     rx_packets;\n\tu64     rx_bytes;\n\tu64     tx_packets;\n\tu64     tx_bytes;\n\tstruct u64_stats_sync   syncp;\n};\n\n#define __netdev_alloc_pcpu_stats(type, gfp)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = alloc_percpu_gfp(type, gfp);\\\n\tif (pcpu_stats)\t{\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\n#define netdev_alloc_pcpu_stats(type)\t\t\t\t\t\\\n\t__netdev_alloc_pcpu_stats(type, GFP_KERNEL)\n\nenum netdev_lag_tx_type {\n\tNETDEV_LAG_TX_TYPE_UNKNOWN,\n\tNETDEV_LAG_TX_TYPE_RANDOM,\n\tNETDEV_LAG_TX_TYPE_BROADCAST,\n\tNETDEV_LAG_TX_TYPE_ROUNDROBIN,\n\tNETDEV_LAG_TX_TYPE_ACTIVEBACKUP,\n\tNETDEV_LAG_TX_TYPE_HASH,\n};\n\nstruct netdev_lag_upper_info {\n\tenum netdev_lag_tx_type tx_type;\n};\n\nstruct netdev_lag_lower_state_info {\n\tu8 link_up : 1,\n\t   tx_enabled : 1;\n};\n\n#include <linux/notifier.h>\n\n/* netdevice notifier chain. Please remember to update the rtnetlink\n * notification exclusion list in rtnetlink_event() when adding new\n * types.\n */\n#define NETDEV_UP\t0x0001\t/* For now you can't veto a device up/down */\n#define NETDEV_DOWN\t0x0002\n#define NETDEV_REBOOT\t0x0003\t/* Tell a protocol stack a network interface\n\t\t\t\t   detected a hardware crash and restarted\n\t\t\t\t   - we can use this eg to kick tcp sessions\n\t\t\t\t   once done */\n#define NETDEV_CHANGE\t0x0004\t/* Notify device state change */\n#define NETDEV_REGISTER 0x0005\n#define NETDEV_UNREGISTER\t0x0006\n#define NETDEV_CHANGEMTU\t0x0007 /* notify after mtu change happened */\n#define NETDEV_CHANGEADDR\t0x0008\n#define NETDEV_GOING_DOWN\t0x0009\n#define NETDEV_CHANGENAME\t0x000A\n#define NETDEV_FEAT_CHANGE\t0x000B\n#define NETDEV_BONDING_FAILOVER 0x000C\n#define NETDEV_PRE_UP\t\t0x000D\n#define NETDEV_PRE_TYPE_CHANGE\t0x000E\n#define NETDEV_POST_TYPE_CHANGE\t0x000F\n#define NETDEV_POST_INIT\t0x0010\n#define NETDEV_UNREGISTER_FINAL 0x0011\n#define NETDEV_RELEASE\t\t0x0012\n#define NETDEV_NOTIFY_PEERS\t0x0013\n#define NETDEV_JOIN\t\t0x0014\n#define NETDEV_CHANGEUPPER\t0x0015\n#define NETDEV_RESEND_IGMP\t0x0016\n#define NETDEV_PRECHANGEMTU\t0x0017 /* notify before mtu change happened */\n#define NETDEV_CHANGEINFODATA\t0x0018\n#define NETDEV_BONDING_INFO\t0x0019\n#define NETDEV_PRECHANGEUPPER\t0x001A\n#define NETDEV_CHANGELOWERSTATE\t0x001B\n\nint register_netdevice_notifier(struct notifier_block *nb);\nint unregister_netdevice_notifier(struct notifier_block *nb);\n\nstruct netdev_notifier_info {\n\tstruct net_device *dev;\n};\n\nstruct netdev_notifier_change_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tunsigned int flags_changed;\n};\n\nstruct netdev_notifier_changeupper_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct net_device *upper_dev; /* new upper dev */\n\tbool master; /* is upper dev master */\n\tbool linking; /* is the nofication for link or unlink */\n\tvoid *upper_info; /* upper dev info */\n};\n\nstruct netdev_notifier_changelowerstate_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tvoid *lower_state_info; /* is lower dev state */\n};\n\nstatic inline void netdev_notifier_info_init(struct netdev_notifier_info *info,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tinfo->dev = dev;\n}\n\nstatic inline struct net_device *\nnetdev_notifier_info_to_dev(const struct netdev_notifier_info *info)\n{\n\treturn info->dev;\n}\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev);\n\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_in_bond_rcu(bond, slave)\t\\\n\t\tfor_each_netdev_rcu(&init_net, slave)\t\\\n\t\t\tif (netdev_master_upper_dev_get_rcu(slave) == (bond))\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(list_next_rcu(&dev->dev_list));\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nstatic inline struct net_device *first_net_device_rcu(struct net *net)\n{\n\tstruct list_head *lh = rcu_dereference(list_next_rcu(&net->dev_base_head));\n\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nint netdev_boot_setup_check(struct net_device *dev);\nunsigned long netdev_boot_base(const char *prefix, int unit);\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *hwaddr);\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type);\nvoid dev_add_pack(struct packet_type *pt);\nvoid dev_remove_pack(struct packet_type *pt);\nvoid __dev_remove_pack(struct packet_type *pt);\nvoid dev_add_offload(struct packet_offload *po);\nvoid dev_remove_offload(struct packet_offload *po);\n\nint dev_get_iflink(const struct net_device *dev);\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb);\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short flags,\n\t\t\t\t      unsigned short mask);\nstruct net_device *dev_get_by_name(struct net *net, const char *name);\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name);\nstruct net_device *__dev_get_by_name(struct net *net, const char *name);\nint dev_alloc_name(struct net_device *dev, const char *name);\nint dev_open(struct net_device *dev);\nint dev_close(struct net_device *dev);\nint dev_close_many(struct list_head *head, bool unlink);\nvoid dev_disable_lro(struct net_device *dev);\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *newskb);\nint dev_queue_xmit(struct sk_buff *skb);\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv);\nint register_netdevice(struct net_device *dev);\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head);\nvoid unregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nint netdev_refcnt_read(const struct net_device *dev);\nvoid free_netdev(struct net_device *dev);\nvoid netdev_freemem(struct net_device *dev);\nvoid synchronize_net(void);\nint init_dummy_netdev(struct net_device *dev);\n\nDECLARE_PER_CPU(int, xmit_recursion);\nstatic inline int dev_recursion_level(void)\n{\n\treturn this_cpu_read(xmit_recursion);\n}\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex);\nint netdev_get_name(struct net *net, char *name, int ifindex);\nint dev_restart(struct net_device *dev);\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tif (!pskb_may_pull(skb, hlen))\n\t\treturn NULL;\n\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\treturn skb->data + offset;\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline void skb_gro_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t\tconst void *start, unsigned int len)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid)\n\t\tNAPI_GRO_CB(skb)->csum = csum_sub(NAPI_GRO_CB(skb)->csum,\n\t\t\t\t\t\t  csum_partial(start, len, 0));\n}\n\n/* GRO checksum functions. These are logical equivalents of the normal\n * checksum functions (in skbuff.h) except that they operate on the GRO\n * offsets and fields in sk_buff.\n */\n\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb);\n\nstatic inline bool skb_at_gro_remcsum_start(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->gro_remcsum_start == skb_gro_offset(skb));\n}\n\nstatic inline bool __skb_gro_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t      bool zero_okay,\n\t\t\t\t\t\t      __sum16 check)\n{\n\treturn ((skb->ip_summed != CHECKSUM_PARTIAL ||\n\t\tskb_checksum_start_offset(skb) <\n\t\t skb_gro_offset(skb)) &&\n\t\t!skb_at_gro_remcsum_start(skb) &&\n\t\tNAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t(!zero_okay || check));\n}\n\nstatic inline __sum16 __skb_gro_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t\t   __wsum psum)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid &&\n\t    !csum_fold(csum_add(psum, NAPI_GRO_CB(skb)->csum)))\n\t\treturn 0;\n\n\tNAPI_GRO_CB(skb)->csum = psum;\n\n\treturn __skb_gro_checksum_complete(skb);\n}\n\nstatic inline void skb_gro_incr_csum_unnecessary(struct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->csum_cnt > 0) {\n\t\t/* Consume a checksum from CHECKSUM_UNNECESSARY */\n\t\tNAPI_GRO_CB(skb)->csum_cnt--;\n\t} else {\n\t\t/* Update skb for CHECKSUM_UNNECESSARY and csum_level when we\n\t\t * verified a new top level checksum or an encapsulated one\n\t\t * during GRO. This saves work if we fallback to normal path.\n\t\t */\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t}\n}\n\n#define __skb_gro_checksum_validate(skb, proto, zero_okay, check,\t\\\n\t\t\t\t    compute_pseudo)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_gro_checksum_validate_complete(skb,\t\\\n\t\t\t\tcompute_pseudo(skb, proto));\t\t\\\n\tif (__ret)\t\t\t\t\t\t\t\\\n\t\t__skb_mark_checksum_bad(skb);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tskb_gro_incr_csum_unnecessary(skb);\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_gro_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, false, 0, compute_pseudo)\n\n#define skb_gro_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t     compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, true, check, compute_pseudo)\n\n#define skb_gro_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_gro_checksum_validate(skb, 0, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_gro_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t!NAPI_GRO_CB(skb)->csum_valid);\n}\n\nstatic inline void __skb_gro_checksum_convert(struct sk_buff *skb,\n\t\t\t\t\t      __sum16 check, __wsum pseudo)\n{\n\tNAPI_GRO_CB(skb)->csum = ~pseudo;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n}\n\n#define skb_gro_checksum_try_convert(skb, proto, check, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_convert_check(skb))\t\t\t\\\n\t\t__skb_gro_checksum_convert(skb, check,\t\t\t\\\n\t\t\t\t\t   compute_pseudo(skb, proto));\t\\\n} while (0)\n\nstruct gro_remcsum {\n\tint offset;\n\t__wsum delta;\n};\n\nstatic inline void skb_gro_remcsum_init(struct gro_remcsum *grc)\n{\n\tgrc->offset = 0;\n\tgrc->delta = 0;\n}\n\nstatic inline void *skb_gro_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t    unsigned int off, size_t hdrlen,\n\t\t\t\t\t    int start, int offset,\n\t\t\t\t\t    struct gro_remcsum *grc,\n\t\t\t\t\t    bool nopartial)\n{\n\t__wsum delta;\n\tsize_t plen = hdrlen + max_t(size_t, offset + sizeof(u16), start);\n\n\tBUG_ON(!NAPI_GRO_CB(skb)->csum_valid);\n\n\tif (!nopartial) {\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = off + hdrlen + start;\n\t\treturn ptr;\n\t}\n\n\tptr = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, off + plen)) {\n\t\tptr = skb_gro_header_slow(skb, off + plen, off);\n\t\tif (!ptr)\n\t\t\treturn NULL;\n\t}\n\n\tdelta = remcsum_adjust(ptr + hdrlen, NAPI_GRO_CB(skb)->csum,\n\t\t\t       start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tNAPI_GRO_CB(skb)->csum = csum_add(NAPI_GRO_CB(skb)->csum, delta);\n\n\tgrc->offset = off + hdrlen + offset;\n\tgrc->delta = delta;\n\n\treturn ptr;\n}\n\nstatic inline void skb_gro_remcsum_cleanup(struct sk_buff *skb,\n\t\t\t\t\t   struct gro_remcsum *grc)\n{\n\tvoid *ptr;\n\tsize_t plen = grc->offset + sizeof(u16);\n\n\tif (!grc->delta)\n\t\treturn;\n\n\tptr = skb_gro_header_fast(skb, grc->offset);\n\tif (skb_gro_header_hard(skb, grc->offset + sizeof(u16))) {\n\t\tptr = skb_gro_header_slow(skb, plen, grc->offset);\n\t\tif (!ptr)\n\t\t\treturn;\n\t}\n\n\tremcsum_unadjust((__sum16 *)ptr, grc->delta);\n}\n\nstruct skb_csum_offl_spec {\n\t__u16\t\tipv4_okay:1,\n\t\t\tipv6_okay:1,\n\t\t\tencap_okay:1,\n\t\t\tip_options_okay:1,\n\t\t\text_hdrs_okay:1,\n\t\t\ttcp_okay:1,\n\t\t\tudp_okay:1,\n\t\t\tsctp_okay:1,\n\t\t\tvlan_okay:1,\n\t\t\tno_encapped_ipv6:1,\n\t\t\tno_not_encapped:1;\n};\n\nbool __skb_csum_offload_chk(struct sk_buff *skb,\n\t\t\t    const struct skb_csum_offl_spec *spec,\n\t\t\t    bool *csum_encapped,\n\t\t\t    bool csum_help);\n\nstatic inline bool skb_csum_offload_chk(struct sk_buff *skb,\n\t\t\t\t\tconst struct skb_csum_offl_spec *spec,\n\t\t\t\t\tbool *csum_encapped,\n\t\t\t\t\tbool csum_help)\n{\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn false;\n\n\treturn __skb_csum_offload_chk(skb, spec, csum_encapped, csum_help);\n}\n\nstatic inline bool skb_csum_offload_chk_help(struct sk_buff *skb,\n\t\t\t\t\t     const struct skb_csum_offl_spec *spec)\n{\n\tbool csum_encapped;\n\n\treturn skb_csum_offload_chk(skb, spec, &csum_encapped, true);\n}\n\nstatic inline bool skb_csum_off_chk_help_cmn(struct sk_buff *skb)\n{\n\tstatic const struct skb_csum_offl_spec csum_offl_spec = {\n\t\t.ipv4_okay = 1,\n\t\t.ip_options_okay = 1,\n\t\t.ipv6_okay = 1,\n\t\t.vlan_okay = 1,\n\t\t.tcp_okay = 1,\n\t\t.udp_okay = 1,\n\t};\n\n\treturn skb_csum_offload_chk_help(skb, &csum_offl_spec);\n}\n\nstatic inline bool skb_csum_off_chk_help_cmn_v4_only(struct sk_buff *skb)\n{\n\tstatic const struct skb_csum_offl_spec csum_offl_spec = {\n\t\t.ipv4_okay = 1,\n\t\t.ip_options_okay = 1,\n\t\t.tcp_okay = 1,\n\t\t.udp_okay = 1,\n\t\t.vlan_okay = 1,\n\t};\n\n\treturn skb_csum_offload_chk_help(skb, &csum_offl_spec);\n}\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned int len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\n/* ll_header must have at least hard_header_len allocated */\nstatic inline bool dev_validate_header(const struct net_device *dev,\n\t\t\t\t       char *ll_header, int len)\n{\n\tif (likely(len >= dev->hard_header_len))\n\t\treturn true;\n\n\tif (capable(CAP_SYS_RAWIO)) {\n\t\tmemset(ll_header + len, 0, dev->hard_header_len - len);\n\t\treturn true;\n\t}\n\n\tif (dev->header_ops && dev->header_ops->validate)\n\t\treturn dev->header_ops->validate(ll_header, len);\n\n\treturn false;\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr, int len);\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\n#define FLOW_LIMIT_HISTORY\t(1 << 7)  /* must be ^2 and !overflow buckets */\nstruct sd_flow_limit {\n\tu64\t\t\tcount;\n\tunsigned int\t\tnum_buckets;\n\tunsigned int\t\thistory_head;\n\tu16\t\t\thistory[FLOW_LIMIT_HISTORY];\n\tu8\t\t\tbuckets[];\n};\n\nextern int netdev_flow_limit_table_len;\n#endif /* CONFIG_NET_FLOW_LIMIT */\n\n/*\n * Incoming packets are placed on per-cpu queues\n */\nstruct softnet_data {\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\tcpu_collision;\n\tunsigned int\t\treceived_rps;\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n#endif\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit __rcu *flow_limit;\n#endif\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct sk_buff\t\t*completion_queue;\n\n#ifdef CONFIG_RPS\n\t/* Elements below can be accessed between CPUs for RPS */\n\tstruct call_single_data\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_head;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned int\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\nvoid __netif_schedule(struct Qdisc *q);\nvoid netif_schedule_queue(struct netdev_queue *txq);\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue);\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tset_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev);\n\nstatic inline bool netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline bool netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline bool netif_xmit_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF_OR_FROZEN;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_drv_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_DRV_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetdev_txq_bql_enqueue_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their ndo_start_xmit(),\n * to give appropriate hint to the cpu.\n */\nstatic inline void netdev_txq_bql_enqueue_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.num_queued);\n#endif\n}\n\n/**\n *\tnetdev_txq_bql_complete_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their TX completion path,\n * to give appropriate hint to the cpu.\n */\nstatic inline void netdev_txq_bql_complete_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.limit);\n#endif\n}\n\nstatic inline void netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\tunsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tdql_queued(&dev_queue->dql, bytes);\n\n\tif (likely(dql_avail(&dev_queue->dql) >= 0))\n\t\treturn;\n\n\tset_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n\n\t/*\n\t * The XOFF flag must be set before checking the dql_avail below,\n\t * because in netdev_tx_completed_queue we update the dql_completed\n\t * before checking the XOFF flag.\n\t */\n\tsmp_mb();\n\n\t/* check again in case another CPU has just made room avail */\n\tif (unlikely(dql_avail(&dev_queue->dql) >= 0))\n\t\tclear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n#endif\n}\n\n/**\n * \tnetdev_sent_queue - report the number of bytes queued to hardware\n * \t@dev: network device\n * \t@bytes: number of bytes queued to the hardware device queue\n *\n * \tReport the number of bytes queued for sending/completion to the network\n * \tdevice hardware queue. @bytes should be a good approximation and should\n * \texactly match netdev_completed_queue() @bytes\n */\nstatic inline void netdev_sent_queue(struct net_device *dev, unsigned int bytes)\n{\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes);\n}\n\nstatic inline void netdev_tx_completed_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t     unsigned int pkts, unsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tif (unlikely(!bytes))\n\t\treturn;\n\n\tdql_completed(&dev_queue->dql, bytes);\n\n\t/*\n\t * Without the memory barrier there is a small possiblity that\n\t * netdev_tx_sent_queue will miss the update and cause the queue to\n\t * be stopped forever\n\t */\n\tsmp_mb();\n\n\tif (dql_avail(&dev_queue->dql) < 0)\n\t\treturn;\n\n\tif (test_and_clear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state))\n\t\tnetif_schedule_queue(dev_queue);\n#endif\n}\n\n/**\n * \tnetdev_completed_queue - report bytes and packets completed by device\n * \t@dev: network device\n * \t@pkts: actual number of packets sent over the medium\n * \t@bytes: actual number of bytes sent over the medium\n *\n * \tReport the number of bytes and packets transmitted by the network device\n * \thardware queue over the physical medium, @bytes must exactly match the\n * \t@bytes amount passed to netdev_sent_queue()\n */\nstatic inline void netdev_completed_queue(struct net_device *dev,\n\t\t\t\t\t  unsigned int pkts, unsigned int bytes)\n{\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, 0), pkts, bytes);\n}\n\nstatic inline void netdev_tx_reset_queue(struct netdev_queue *q)\n{\n#ifdef CONFIG_BQL\n\tclear_bit(__QUEUE_STATE_STACK_XOFF, &q->state);\n\tdql_reset(&q->dql);\n#endif\n}\n\n/**\n * \tnetdev_reset_queue - reset the packets and bytes count of a network device\n * \t@dev_queue: network device\n *\n * \tReset the bytes and packet count of a network device and clear the\n * \tsoftware flow control OFF bit for this network device\n */\nstatic inline void netdev_reset_queue(struct net_device *dev_queue)\n{\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));\n}\n\n/**\n * \tnetdev_cap_txqueue - check if selected tx queue exceeds device queues\n * \t@dev: network device\n * \t@queue_index: given tx queue index\n *\n * \tReturns 0 if given tx queue index >= number of device tx queues,\n * \totherwise returns the originally passed tx queue index.\n */\nstatic inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tnet_warn_ratelimited(\"%s selects TX queue %d, but real number of TX queues is %d\\n\",\n\t\t\t\t     dev->name, queue_index,\n\t\t\t\t     dev->real_num_tx_queues);\n\t\treturn 0;\n\t}\n\n\treturn queue_index;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline bool netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline bool __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t    u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\nstatic inline bool netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\nvoid netif_wake_subqueue(struct net_device *dev, u16 queue_index);\n\n#ifdef CONFIG_XPS\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index);\n#else\nstatic inline int netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t      const struct cpumask *mask,\n\t\t\t\t      u16 index)\n{\n\treturn 0;\n}\n#endif\n\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues);\n\n/*\n * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used\n * as a distribution range limit for the returned value.\n */\nstatic inline u16 skb_tx_hash(const struct net_device *dev,\n\t\t\t      struct sk_buff *skb)\n{\n\treturn __skb_tx_hash(dev, skb, dev->real_num_tx_queues);\n}\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline bool netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq);\n\n#ifdef CONFIG_SYSFS\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxq)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SYSFS\nstatic inline unsigned int get_netdev_rx_queue_index(\n\t\tstruct netdev_rx_queue *queue)\n{\n\tstruct net_device *dev = queue->dev;\n\tint index = queue - dev->_rx;\n\n\tBUG_ON(index >= dev->num_rx_queues);\n\treturn index;\n}\n#endif\n\n#define DEFAULT_MAX_NUM_RSS_QUEUES\t(8)\nint netif_get_num_default_rss_queues(void);\n\nenum skb_free_reason {\n\tSKB_REASON_CONSUMED,\n\tSKB_REASON_DROPPED,\n};\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason);\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason);\n\n/*\n * It is not allowed to call kfree_skb() or consume_skb() from hardware\n * interrupt context or with hardware interrupts being disabled.\n * (in_irq() || irqs_disabled())\n *\n * We provide four helpers that can be used in following contexts :\n *\n * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.\n *  Typically used in place of consume_skb(skb) in TX completion path\n *\n * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_any(skb) when caller doesn't know its current irq context,\n *  and consumed a packet. Used in place of consume_skb(skb)\n */\nstatic inline void dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_CONSUMED);\n}\n\nstatic inline void dev_kfree_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_CONSUMED);\n}\n\nint netif_rx(struct sk_buff *skb);\nint netif_rx_ni(struct sk_buff *skb);\nint netif_receive_skb(struct sk_buff *skb);\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb);\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old);\nstruct sk_buff *napi_get_frags(struct napi_struct *napi);\ngro_result_t napi_gro_frags(struct napi_struct *napi);\nstruct packet_offload *gro_find_receive_by_type(__be16 type);\nstruct packet_offload *gro_find_complete_by_type(__be16 type);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data);\nvoid netdev_rx_handler_unregister(struct net_device *dev);\n\nbool dev_valid_name(const char *name);\nint dev_ioctl(struct net *net, unsigned int cmd, void __user *);\nint dev_ethtool(struct net *net, struct ifreq *);\nunsigned int dev_get_flags(const struct net_device *);\nint __dev_change_flags(struct net_device *, unsigned int flags);\nint dev_change_flags(struct net_device *, unsigned int);\nvoid __dev_notify_flags(struct net_device *, unsigned int old_flags,\n\t\t\tunsigned int gchanges);\nint dev_change_name(struct net_device *, const char *);\nint dev_set_alias(struct net_device *, const char *, size_t);\nint dev_change_net_namespace(struct net_device *, struct net *, const char *);\nint dev_set_mtu(struct net_device *, int);\nvoid dev_set_group(struct net_device *, int);\nint dev_set_mac_address(struct net_device *, struct sockaddr *);\nint dev_change_carrier(struct net_device *, bool new_carrier);\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid);\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len);\nint dev_change_proto_down(struct net_device *dev, bool proto_down);\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev);\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret);\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nbool is_skb_forwardable(struct net_device *dev, struct sk_buff *skb);\n\nextern int\t\tnetdev_budget;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nvoid netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n\tthis_cpu_dec(*dev->pcpu_refcnt);\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n\tthis_cpu_inc(*dev->pcpu_refcnt);\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nvoid linkwatch_init_dev(struct net_device *dev);\nvoid linkwatch_fire_event(struct net_device *dev);\nvoid linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nunsigned long dev_trans_start(struct net_device *dev);\n\nvoid __netdev_watchdog_up(struct net_device *dev);\n\nvoid netif_carrier_on(struct net_device *dev);\n\nvoid netif_carrier_off(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n *\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline bool netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline bool netif_device_present(struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nvoid netif_device_detach(struct net_device *dev);\n\nvoid netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n\nenum {\n\tNETIF_MSG_DRV\t\t= 0x0001,\n\tNETIF_MSG_PROBE\t\t= 0x0002,\n\tNETIF_MSG_LINK\t\t= 0x0004,\n\tNETIF_MSG_TIMER\t\t= 0x0008,\n\tNETIF_MSG_IFDOWN\t= 0x0010,\n\tNETIF_MSG_IFUP\t\t= 0x0020,\n\tNETIF_MSG_RX_ERR\t= 0x0040,\n\tNETIF_MSG_TX_ERR\t= 0x0080,\n\tNETIF_MSG_TX_QUEUED\t= 0x0100,\n\tNETIF_MSG_INTR\t\t= 0x0200,\n\tNETIF_MSG_TX_DONE\t= 0x0400,\n\tNETIF_MSG_RX_STATUS\t= 0x0800,\n\tNETIF_MSG_PKTDATA\t= 0x1000,\n\tNETIF_MSG_HW\t\t= 0x2000,\n\tNETIF_MSG_WOL\t\t= 0x4000,\n};\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1 << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline bool __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tbool ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_TRYLOCK(dev, txq)\t\t\t\\\n\t(((dev->features & NETIF_F_LLTX) == 0) ?\t\\\n\t\t__netif_tx_trylock(txq) :\t\t\\\n\t\ttrue )\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_lock_nested(struct net_device *dev)\n{\n\tint subclass = SINGLE_DEPTH_NESTING;\n\n\tif (dev->netdev_ops->ndo_get_lock_subclass)\n\t\tsubclass = dev->netdev_ops->ndo_get_lock_subclass(dev);\n\n\tspin_lock_nested(&dev->addr_list_lock, subclass);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tspin_lock_bh(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nvoid ether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t    unsigned char name_assign_type,\n\t\t\t\t    void (*setup)(struct net_device *),\n\t\t\t\t    unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, name_assign_type, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, count, \\\n\t\t\t count)\n\nint register_netdev(struct net_device *dev);\nvoid unregister_netdev(struct net_device *dev);\n\n/* General hardware address lists handling functions */\nint __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t   struct netdev_hw_addr_list *from_list, int addr_len);\nvoid __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t      struct netdev_hw_addr_list *from_list, int addr_len);\nint __hw_addr_sync_dev(struct netdev_hw_addr_list *list,\n\t\t       struct net_device *dev,\n\t\t       int (*sync)(struct net_device *, const unsigned char *),\n\t\t       int (*unsync)(struct net_device *,\n\t\t\t\t     const unsigned char *));\nvoid __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t  struct net_device *dev,\n\t\t\t  int (*unsync)(struct net_device *,\n\t\t\t\t\tconst unsigned char *));\nvoid __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nint dev_addr_add(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nint dev_addr_del(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nvoid dev_addr_flush(struct net_device *dev);\nint dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nint dev_uc_add(struct net_device *dev, const unsigned char *addr);\nint dev_uc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_uc_del(struct net_device *dev, const unsigned char *addr);\nint dev_uc_sync(struct net_device *to, struct net_device *from);\nint dev_uc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_uc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_uc_flush(struct net_device *dev);\nvoid dev_uc_init(struct net_device *dev);\n\n/**\n *  __dev_uc_sync - Synchonize device's unicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n **/\nstatic inline int __dev_uc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->uc, dev, sync, unsync);\n}\n\n/**\n *  __dev_uc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_uc_sync().\n **/\nstatic inline void __dev_uc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->uc, dev, unsync);\n}\n\n/* Functions used for multicast addresses handling */\nint dev_mc_add(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_sync(struct net_device *to, struct net_device *from);\nint dev_mc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_mc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_mc_flush(struct net_device *dev);\nvoid dev_mc_init(struct net_device *dev);\n\n/**\n *  __dev_mc_sync - Synchonize device's multicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n **/\nstatic inline int __dev_mc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->mc, dev, sync, unsync);\n}\n\n/**\n *  __dev_mc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_mc_sync().\n **/\nstatic inline void __dev_mc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->mc, dev, unsync);\n}\n\n/* Functions used for secondary unicast and multicast support */\nvoid dev_set_rx_mode(struct net_device *dev);\nvoid __dev_set_rx_mode(struct net_device *dev);\nint dev_set_promiscuity(struct net_device *dev, int inc);\nint dev_set_allmulti(struct net_device *dev, int inc);\nvoid netdev_state_change(struct net_device *dev);\nvoid netdev_notify_peers(struct net_device *dev);\nvoid netdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nvoid dev_load(struct net *net, const char *name);\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage);\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tbpf_jit_enable;\n\nbool netdev_has_upper_dev(struct net_device *dev, struct net_device *upper_dev);\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->adj_list.upper, \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)))\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_all_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->all_adj_list.upper, \\\n\t     updev = netdev_all_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_all_upper_get_next_dev_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter);\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_private(dev, priv, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)))\n\n#define netdev_for_each_lower_private_rcu(dev, priv, iter) \\\n\tfor (iter = &(dev)->adj_list.lower, \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next(struct net_device *dev,\n\t\t\t\tstruct list_head **iter);\n#define netdev_for_each_lower_dev(dev, ldev, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)); \\\n\t     ldev; \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)))\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list);\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev);\nint netdev_upper_dev_link(struct net_device *dev, struct net_device *upper_dev);\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info);\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev);\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname);\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev);\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info);\n\n/* RSS keys are 40 or 52 bytes long */\n#define NETDEV_RSS_KEY_LEN 52\nextern u8 netdev_rss_key[NETDEV_RSS_KEY_LEN] __read_mostly;\nvoid netdev_rss_key_fill(void *buffer, size_t len);\n\nint dev_get_nest_level(struct net_device *dev,\n\t\t       bool (*type_check)(const struct net_device *dev));\nint skb_checksum_help(struct sk_buff *skb);\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path);\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features);\n\nstruct netdev_bonding_info {\n\tifslave\tslave;\n\tifbond\tmaster;\n};\n\nstruct netdev_notifier_bonding_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct netdev_bonding_info  bonding_info;\n};\n\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info);\n\nstatic inline\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn __skb_gso_segment(skb, features, true);\n}\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth);\n\nstatic inline bool can_checksum_protocol(netdev_features_t features,\n\t\t\t\t\t __be16 protocol)\n{\n\tif (protocol == htons(ETH_P_FCOE))\n\t\treturn !!(features & NETIF_F_FCOE_CRC);\n\n\t/* Assume this is an IP checksum (not SCTP CRC) */\n\n\tif (features & NETIF_F_HW_CSUM) {\n\t\t/* Can checksum everything */\n\t\treturn true;\n\t}\n\n\tswitch (protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn !!(features & NETIF_F_IP_CSUM);\n\tcase htons(ETH_P_IPV6):\n\t\treturn !!(features & NETIF_F_IPV6_CSUM);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Map an ethertype into IP protocol if possible */\nstatic inline int eproto_to_ipproto(int eproto)\n{\n\tswitch (eproto) {\n\tcase htons(ETH_P_IP):\n\t\treturn IPPROTO_IP;\n\tcase htons(ETH_P_IPV6):\n\t\treturn IPPROTO_IPV6;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev)\n{\n}\n#endif\n/* rx skb timestamps */\nvoid net_enable_timestamp(void);\nvoid net_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nint __init dev_proc_init(void);\n#else\n#define dev_proc_init() 0\n#endif\n\nstatic inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,\n\t\t\t\t\t      struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t      bool more)\n{\n\tskb->xmit_more = more ? 1 : 0;\n\treturn ops->ndo_start_xmit(skb, dev);\n}\n\nstatic inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq, bool more)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint rc;\n\n\trc = __netdev_start_xmit(ops, skb, dev, more);\n\tif (rc == NETDEV_TX_OK)\n\t\ttxq_trans_update(txq);\n\n\treturn rc;\n}\n\nint netdev_class_create_file_ns(struct class_attribute *class_attr,\n\t\t\t\tconst void *ns);\nvoid netdev_class_remove_file_ns(struct class_attribute *class_attr,\n\t\t\t\t const void *ns);\n\nstatic inline int netdev_class_create_file(struct class_attribute *class_attr)\n{\n\treturn netdev_class_create_file_ns(class_attr, NULL);\n}\n\nstatic inline void netdev_class_remove_file(struct class_attribute *class_attr)\n{\n\tnetdev_class_remove_file_ns(class_attr, NULL);\n}\n\nextern struct kobj_ns_type_operations net_ns_type_operations;\n\nconst char *netdev_drivername(const struct net_device *dev);\n\nvoid linkwatch_run_queue(void);\n\nstatic inline netdev_features_t netdev_intersect_features(netdev_features_t f1,\n\t\t\t\t\t\t\t  netdev_features_t f2)\n{\n\tif ((f1 ^ f2) & NETIF_F_HW_CSUM) {\n\t\tif (f1 & NETIF_F_HW_CSUM)\n\t\t\tf1 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t\telse\n\t\t\tf2 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\treturn f1 & f2;\n}\n\nstatic inline netdev_features_t netdev_get_wanted_features(\n\tstruct net_device *dev)\n{\n\treturn (dev->features & ~dev->hw_features) | dev->wanted_features;\n}\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask);\n\n/* Allow TSO being used on stacked device :\n * Performing the GSO segmentation before last device\n * is a performance improvement.\n */\nstatic inline netdev_features_t netdev_add_tso_features(netdev_features_t features,\n\t\t\t\t\t\t\tnetdev_features_t mask)\n{\n\treturn netdev_increment_features(features, NETIF_F_ALL_TSO, mask);\n}\n\nint __netdev_update_features(struct net_device *dev);\nvoid netdev_update_features(struct net_device *dev);\nvoid netdev_change_features(struct net_device *dev);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features);\nnetdev_features_t netif_skb_features(struct sk_buff *skb);\n\nstatic inline bool net_gso_ok(netdev_features_t features, int gso_type)\n{\n\tnetdev_features_t feature = gso_type << NETIF_F_GSO_SHIFT;\n\n\t/* check flags correspondence */\n\tBUILD_BUG_ON(SKB_GSO_TCPV4   != (NETIF_F_TSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP     != (NETIF_F_UFO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_DODGY   != (NETIF_F_GSO_ROBUST >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_ECN != (NETIF_F_TSO_ECN >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCPV6   != (NETIF_F_TSO6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FCOE    != (NETIF_F_FSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE     != (NETIF_F_GSO_GRE >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE_CSUM != (NETIF_F_GSO_GRE_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPIP    != (NETIF_F_GSO_IPIP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_SIT     != (NETIF_F_GSO_SIT >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL != (NETIF_F_GSO_UDP_TUNNEL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL_CSUM != (NETIF_F_GSO_UDP_TUNNEL_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TUNNEL_REMCSUM != (NETIF_F_GSO_TUNNEL_REMCSUM >> NETIF_F_GSO_SHIFT));\n\n\treturn (features & feature) == feature;\n}\n\nstatic inline bool skb_gso_ok(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline bool netif_needs_gso(struct sk_buff *skb,\n\t\t\t\t   netdev_features_t features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely((skb->ip_summed != CHECKSUM_PARTIAL) &&\n\t\t\t (skb->ip_summed != CHECKSUM_UNNECESSARY)));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nstatic inline void skb_gso_error_unwind(struct sk_buff *skb, __be16 protocol,\n\t\t\t\t\tint pulled_hlen, u16 mac_offset,\n\t\t\t\t\tint mac_len)\n{\n\tskb->protocol = protocol;\n\tskb->encapsulation = 1;\n\tskb_push(skb, pulled_hlen);\n\tskb_reset_transport_header(skb);\n\tskb->mac_header = mac_offset;\n\tskb->network_header = skb->mac_header + mac_len;\n\tskb->mac_len = mac_len;\n}\n\nstatic inline bool netif_is_macsec(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nstatic inline bool netif_is_macvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN;\n}\n\nstatic inline bool netif_is_macvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN_PORT;\n}\n\nstatic inline bool netif_is_ipvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_SLAVE;\n}\n\nstatic inline bool netif_is_ipvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_MASTER;\n}\n\nstatic inline bool netif_is_bond_master(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_MASTER && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_is_bond_slave(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_SLAVE && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_supports_nofcs(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_SUPP_NOFCS;\n}\n\nstatic inline bool netif_is_l3_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_MASTER;\n}\n\nstatic inline bool netif_is_l3_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_SLAVE;\n}\n\nstatic inline bool netif_is_bridge_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_EBRIDGE;\n}\n\nstatic inline bool netif_is_bridge_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_BRIDGE_PORT;\n}\n\nstatic inline bool netif_is_ovs_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OPENVSWITCH;\n}\n\nstatic inline bool netif_is_team_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM;\n}\n\nstatic inline bool netif_is_team_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM_PORT;\n}\n\nstatic inline bool netif_is_lag_master(const struct net_device *dev)\n{\n\treturn netif_is_bond_master(dev) || netif_is_team_master(dev);\n}\n\nstatic inline bool netif_is_lag_port(const struct net_device *dev)\n{\n\treturn netif_is_bond_slave(dev) || netif_is_team_port(dev);\n}\n\nstatic inline bool netif_is_rxfh_configured(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_RXFH_CONFIGURED;\n}\n\n/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */\nstatic inline void netif_keep_dst(struct net_device *dev)\n{\n\tdev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM);\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (!dev->name[0] || strchr(dev->name, '%'))\n\t\treturn \"(unnamed net_device)\";\n\treturn dev->name;\n}\n\nstatic inline const char *netdev_reg_state(const struct net_device *dev)\n{\n\tswitch (dev->reg_state) {\n\tcase NETREG_UNINITIALIZED: return \" (uninitialized)\";\n\tcase NETREG_REGISTERED: return \"\";\n\tcase NETREG_UNREGISTERING: return \" (unregistering)\";\n\tcase NETREG_UNREGISTERED: return \" (unregistered)\";\n\tcase NETREG_RELEASED: return \" (released)\";\n\tcase NETREG_DUMMY: return \" (dummy)\";\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown reg_state %d\\n\", dev->name, dev->reg_state);\n\treturn \" (unknown)\";\n}\n\n__printf(3, 4)\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...);\n__printf(2, 3)\nvoid netdev_emerg(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_alert(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_crit(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_err(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_warn(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_notice(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_info(const struct net_device *dev, const char *format, ...);\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_netdev_dbg(__dev, format, ##args);\t\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s%s\\n\" format, netdev_name(dev),\t\\\n\t     netdev_reg_state(dev), ##args)\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_netdev_dbg(netdev, format, ##args);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *      NOTE:  That is no longer true with the addition of VLAN tags.  Not\n *             sure which should go first, but I bet it won't make much\n *             difference if we are running VLANs.  The good news is that\n *             this protocol won't be in the list unless compiled in, so\n *             the average user (w/out VLANs) will not be adversely affected.\n *             --BLG\n *\n *\t\t0800\tIP\n *\t\t8100    802.1Q VLAN\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\n#endif\t/* _LINUX_NETDEVICE_H */\n", "/*\n * \tNET3\tProtocol independent device support routines.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n * \t\tAuthors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *              \t\t\tto 2 if register_netdev gets called\n *              \t\t\tbefore net_dev_init & also removed a\n *              \t\t\tfew lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *\t\tAlan Cox\t: \tFix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *\t\tAlan Cox\t: \tFixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *              \t\t\tindefinitely on dev->refcnt\n * \t\tJ Hadi Salim\t:\t- Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <asm/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/notifier.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/pkt_sched.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/pci.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_ingress.h>\n#include <linux/sctp.h>\n\n#include \"net-sysfs.h\"\n\n/* Instead of increasing this, you should create a hash table. */\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic DEFINE_SPINLOCK(offload_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\nstatic struct list_head offload_base __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic seqcount_t devnet_rename_seq;\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0);\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\thlist_del_rcu(&dev->name_hlist);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] =\n\t{ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] =\n\t{\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t \"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t \"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t \"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t \"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t \"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t \"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t \"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t \"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t \"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t \"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t \"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t \"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t \"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t \"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n\n\t\tProtocol management and registration routines\n\n*******************************************************************************/\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/**\n *\tdev_add_offload - register offload handlers\n *\t@po: protocol offload declaration\n *\n *\tAdd protocol offload handlers to the networking stack. The passed\n *\t&proto_offload is linked into kernel lists and may not be freed until\n *\tit has been removed from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new offload handlers (until the next received packet).\n */\nvoid dev_add_offload(struct packet_offload *po)\n{\n\tstruct packet_offload *elem;\n\n\tspin_lock(&offload_lock);\n\tlist_for_each_entry(elem, &offload_base, list) {\n\t\tif (po->priority < elem->priority)\n\t\t\tbreak;\n\t}\n\tlist_add_rcu(&po->list, elem->list.prev);\n\tspin_unlock(&offload_lock);\n}\nEXPORT_SYMBOL(dev_add_offload);\n\n/**\n *\t__dev_remove_offload\t - remove offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a protocol offload handler that was previously added to the\n *\tkernel offload handlers by dev_add_offload(). The passed &offload_type\n *\tis removed from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nstatic void __dev_remove_offload(struct packet_offload *po)\n{\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *po1;\n\n\tspin_lock(&offload_lock);\n\n\tlist_for_each_entry(po1, head, list) {\n\t\tif (po == po1) {\n\t\t\tlist_del_rcu(&po->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_offload: %p not found\\n\", po);\nout:\n\tspin_unlock(&offload_lock);\n}\n\n/**\n *\tdev_remove_offload\t - remove packet offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a packet offload handler that was previously added to the kernel\n *\toffload handlers by dev_add_offload(). The passed &offload_type is\n *\tremoved from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_offload(struct packet_offload *po)\n{\n\t__dev_remove_offload(po);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_offload);\n\n/******************************************************************************\n\n\t\t      Device Boot-time Settings Routines\n\n*******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n *\tnetdev_boot_setup_check\t- check boot time settings\n *\t@dev: the netdevice\n *\n * \tCheck boot time settings for the device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq \t= s[i].map.irq;\n\t\t\tdev->base_addr \t= s[i].map.base_addr;\n\t\t\tdev->mem_start \t= s[i].map.mem_start;\n\t\t\tdev->mem_end \t= s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n *\tnetdev_boot_base\t- get address from boot time settings\n *\t@prefix: prefix for network device\n *\t@unit: id for network device\n *\n * \tCheck boot time settings for the base address of device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n\n\t\t\t    Device Interface Subroutines\n\n*******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n *\tdev_get_by_name_rcu\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name.\n *\tIf the name is found a pointer to the device is returned.\n * \tIf the name is not found then %NULL is returned.\n *\tThe reference counters are not incremented so the caller must be\n *\tcareful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry_rcu(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n *\n *\tThe use of raw_seqcount_begin() and cond_resched() before\n *\tretrying is required as we want to give the writers a chance\n *\tto complete when CONFIG_PREEMPT is not set.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tunsigned int seq;\n\nretry:\n\tseq = raw_seqcount_begin(&devnet_rename_seq);\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\n\tstrcpy(name, dev->name);\n\trcu_read_unlock();\n\tif (read_seqcount_retry(&devnet_rename_seq, seq)) {\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tfor_each_netdev(net, dev)\n\t\tif (dev->type == type)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_getfirstbyhwtype);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tto allow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strlen(name) >= IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tp = strnchr(name, IFNAMSIZ-1, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tif (buf != name)\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tstruct net *net;\n\tint ret;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\nstatic int dev_get_valid_name(struct net *net,\n\t\t\t      struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\tif (dev->flags & IFF_UP)\n\t\treturn -EBUSY;\n\n\twrite_seqcount_begin(&devnet_rename_seq);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn ret;\n\t}\n\n\twrite_seqcount_end(&devnet_rename_seq);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_del_rcu(&dev->name_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\twrite_seqcount_begin(&devnet_rename_seq);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tpr_err(\"%s: name change rollback failed: %d\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tchar *new_ifalias;\n\n\tASSERT_RTNL();\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (!len) {\n\t\tkfree(dev->ifalias);\n\t\tdev->ifalias = NULL;\n\t\treturn 0;\n\t}\n\n\tnew_ifalias = krealloc(dev->ifalias, len + 1, GFP_KERNEL);\n\tif (!new_ifalias)\n\t\treturn -ENOMEM;\n\tdev->ifalias = new_ifalias;\n\n\tstrlcpy(dev->ifalias, alias, len+1);\n\treturn len;\n}\n\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = 0;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * \tnetdev_notify_peers - notify network peers about existence of @dev\n * \t@dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int __dev_open(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers(NETDEV_PRE_UP, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev:\tdevice to open\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic int __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic int __dev_close(struct net_device *dev)\n{\n\tint retval;\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\tretval = __dev_close_many(&single);\n\tlist_del(&single);\n\n\treturn retval;\n}\n\nint dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nint dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\tnetdev_notifier_info_init(&info, dev);\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n *\tregister_netdevice_notifier - register a network notifier block\n *\t@nb: notifier\n *\n *\tRegister a notifier to be called when network device events occur.\n *\tThe notifier passed is linked into the kernel structures and must\n *\tnot be reused until it has been unregistered. A negative errno code\n *\tis returned on a failure.\n *\n * \tWhen registered all registration and up events are replayed\n *\tto the new notifier to allow device to have a race free\n *\tview of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net_device *last;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\t\t\terr = notifier_to_errno(err);\n\t\t\tif (err)\n\t\t\t\tgoto rollback;\n\n\t\t\tif (!(dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\t\t}\n\t}\n\nunlock:\n\trtnl_unlock();\n\treturn err;\n\nrollback:\n\tlast = dev;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev == last)\n\t\t\t\tgoto outroll;\n\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\n\noutroll:\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n *\tunregister_netdevice_notifier - unregister a network notifier block\n *\t@nb: notifier\n *\n *\tUnregister a notifier previously registered by\n *\tregister_netdevice_notifier(). The notifier is unlinked into the\n *\tkernel structures and may then be reused. A negative errno code\n *\tis returned on a failure.\n *\n * \tAfter unregistering unregister and down device events are synthesized\n *\tfor all devices on the device list to the removed notifier to remove\n *\tthe need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\nunlock:\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tASSERT_RTNL();\n\tnetdev_notifier_info_init(info, dev);\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\treturn call_netdevice_notifiers_info(val, dev, &info);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n#ifdef CONFIG_NET_INGRESS\nstatic struct static_key ingress_needed __read_mostly;\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_key_slow_inc(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_key_slow_dec(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct static_key egress_needed __read_mostly;\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_key_slow_inc(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_key_slow_dec(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nstatic struct static_key netstamp_needed __read_mostly;\n#ifdef HAVE_JUMP_LABEL\n/* We are not allowed to call static_key_slow_dec() from irq context\n * If net_disable_timestamp() is called from irq context, defer the\n * static_key_slow_dec() calls.\n */\nstatic atomic_t netstamp_needed_deferred;\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\n\tif (deferred) {\n\t\twhile (--deferred)\n\t\t\tstatic_key_slow_dec(&netstamp_needed);\n\t\treturn;\n\t}\n#endif\n\tstatic_key_slow_inc(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tif (in_interrupt()) {\n\t\tatomic_inc(&netstamp_needed_deferred);\n\t\treturn;\n\t}\n#endif\n\tstatic_key_slow_dec(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp.tv64 = 0;\n\tif (static_key_false(&netstamp_needed))\n\t\t__net_timestamp(skb);\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\\\n\tif (static_key_false(&netstamp_needed)) {\t\t\\\n\t\tif ((COND) && !(SKB)->tstamp.tv64)\t\\\n\t\t\t__net_timestamp(SKB);\t\t\\\n\t}\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(struct net_device *dev, struct sk_buff *skb)\n{\n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tlen = dev->mtu + dev->hard_header_len + VLAN_HLEN;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t/* if TSO is enabled, we don't care about the length as the packet\n\t * could be forwarded without being segmented before\n\t */\n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tif (skb_orphan_frags(skb, GFP_ATOMIC) ||\n\t    unlikely(!is_skb_forwardable(dev, skb))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tskb_scrub_packet(skb, true);\n\tskb->priority = 0;\n\tskb->protocol = eth_type_trans(skb, dev);\n\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\tatomic_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nstatic void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev)\n\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\trcu_read_unlock();\n}\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tpr_warn(\"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tpr_warn(\"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\ti, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_XPS\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic struct xps_map *remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t\t\tint cpu, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->cpu_map[cpu]);\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] == index) {\n\t\t\tif (map->len > 1) {\n\t\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\t} else {\n\t\t\t\tRCU_INIT_POINTER(dev_maps->cpu_map[cpu], NULL);\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t\t\tmap = NULL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn map;\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tint cpu, i;\n\tbool active = false;\n\n\tmutex_lock(&xps_map_mutex);\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = index; i < dev->num_tx_queues; i++) {\n\t\t\tif (!remove_xps_queue(dev_maps, cpu, i))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == dev->num_tx_queues)\n\t\t\tactive = true;\n\t}\n\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\n\tfor (i = index; i < dev->num_tx_queues; i++)\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, i),\n\t\t\t\t\t     NUMA_NO_NODE);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map,\n\t\t\t\t      int cpu, u16 index)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add queue to this CPU's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store queue on this CPU's map */\n\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t       cpu_to_node(cpu));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL;\n\tstruct xps_map *map, *new_map;\n\tint maps_sz = max_t(unsigned int, XPS_DEV_MAPS_SIZE, L1_CACHE_BYTES);\n\tint cpu, numa_node_id = -2;\n\tbool active = false;\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\t/* allocate memory for queue storage */\n\tfor_each_online_cpu(cpu) {\n\t\tif (!cpumask_test_cpu(cpu, mask))\n\t\t\tcontinue;\n\n\t\tif (!new_dev_maps)\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\tif (!new_dev_maps) {\n\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->cpu_map[cpu]) :\n\t\t\t\t NULL;\n\n\t\tmap = expand_xps_map(map, cpu, index);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[cpu], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpumask_test_cpu(cpu, mask) && cpu_online(cpu)) {\n\t\t\t/* add queue to CPU maps */\n\t\t\tint pos = 0;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->cpu_map[cpu]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (numa_node_id == -2)\n\t\t\t\tnuma_node_id = cpu_to_node(cpu);\n\t\t\telse if (numa_node_id != cpu_to_node(cpu))\n\t\t\t\tnuma_node_id = -1;\n#endif\n\t\t} else if (dev_maps) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[cpu]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[cpu], map);\n\t\t}\n\n\t}\n\n\trcu_assign_pointer(dev->xps_maps, new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (dev_maps) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[cpu]);\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[cpu]);\n\t\t\tif (map && map != new_map)\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\t/* update Tx queue numa node */\n\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t     (numa_node_id >= 0) ? numa_node_id :\n\t\t\t\t     NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes queue from unused CPUs */\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpumask_test_cpu(cpu, mask) && cpu_online(cpu))\n\t\t\tcontinue;\n\n\t\tif (remove_xps_queue(dev_maps, cpu, index))\n\t\t\tactive = true;\n\t}\n\n\t/* free map if not active */\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor_each_possible_cpu(cpu) {\n\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[cpu]);\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->cpu_map[cpu]) :\n\t\t\t\t NULL;\n\t\tif (new_map && new_map != map)\n\t\t\tkfree(new_map);\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater then real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tint rc;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tif (txq < dev->real_num_tx_queues) {\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t}\n\n\tdev->real_num_tx_queues = txq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * This routine should set an upper limit on the number of RSS queues\n * used by default by multiqueue devices.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\treturn min_t(int, DEFAULT_MAX_NUM_RSS_QUEUES, num_online_cpus());\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic inline void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!(txq->state & QUEUE_STATE_ANY_XOFF)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nvoid netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &txq->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(txq->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_wake_subqueue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (likely(atomic_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\tatomic_set(&skb->users, 0);\n\t} else if (likely(!atomic_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_irq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = num_tx_queues;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\twhile (unlikely(hash >= num_tx_queues))\n\t\t\thash -= num_tx_queues;\n\t\treturn hash;\n\t}\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\t\tqoffset = dev->tc_to_txq[tc].offset;\n\t\tqcount = dev->tc_to_txq[tc].count;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\nEXPORT_SYMBOL(__skb_tx_hash);\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features = 0;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tWARN(1, \"%s: caps=(%pNF, %pNF) len=%d data_len=%d gso_size=%d \"\n\t     \"gso_type=%d ip_summed=%d\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features,\n\t     skb->len, skb->data_len, skb_shinfo(skb)->gso_size,\n\t     skb_shinfo(skb)->gso_type, skb->ip_summed);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_shinfo(skb)->gso_size)) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__sum16))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\n/* skb_csum_offload_check - Driver helper function to determine if a device\n * with limited checksum offload capabilities is able to offload the checksum\n * for a given packet.\n *\n * Arguments:\n *   skb - sk_buff for the packet in question\n *   spec - contains the description of what device can offload\n *   csum_encapped - returns true if the checksum being offloaded is\n *\t      encpasulated. That is it is checksum for the transport header\n *\t      in the inner headers.\n *   checksum_help - when set indicates that helper function should\n *\t      call skb_checksum_help if offload checks fail\n *\n * Returns:\n *   true: Packet has passed the checksum checks and should be offloadable to\n *\t   the device (a driver may still need to check for additional\n *\t   restrictions of its device)\n *   false: Checksum is not offloadable. If checksum_help was set then\n *\t   skb_checksum_help was called to resolve checksum for non-GSO\n *\t   packets and when IP protocol is not SCTP\n */\nbool __skb_csum_offload_chk(struct sk_buff *skb,\n\t\t\t    const struct skb_csum_offl_spec *spec,\n\t\t\t    bool *csum_encapped,\n\t\t\t    bool csum_help)\n{\n\tstruct iphdr *iph;\n\tstruct ipv6hdr *ipv6;\n\tvoid *nhdr;\n\tint protocol;\n\tu8 ip_proto;\n\n\tif (skb->protocol == htons(ETH_P_8021Q) ||\n\t    skb->protocol == htons(ETH_P_8021AD)) {\n\t\tif (!spec->vlan_okay)\n\t\t\tgoto need_help;\n\t}\n\n\t/* We check whether the checksum refers to a transport layer checksum in\n\t * the outermost header or an encapsulated transport layer checksum that\n\t * corresponds to the inner headers of the skb. If the checksum is for\n\t * something else in the packet we need help.\n\t */\n\tif (skb_checksum_start_offset(skb) == skb_transport_offset(skb)) {\n\t\t/* Non-encapsulated checksum */\n\t\tprotocol = eproto_to_ipproto(vlan_get_protocol(skb));\n\t\tnhdr = skb_network_header(skb);\n\t\t*csum_encapped = false;\n\t\tif (spec->no_not_encapped)\n\t\t\tgoto need_help;\n\t} else if (skb->encapsulation && spec->encap_okay &&\n\t\t   skb_checksum_start_offset(skb) ==\n\t\t   skb_inner_transport_offset(skb)) {\n\t\t/* Encapsulated checksum */\n\t\t*csum_encapped = true;\n\t\tswitch (skb->inner_protocol_type) {\n\t\tcase ENCAP_TYPE_ETHER:\n\t\t\tprotocol = eproto_to_ipproto(skb->inner_protocol);\n\t\t\tbreak;\n\t\tcase ENCAP_TYPE_IPPROTO:\n\t\t\tprotocol = skb->inner_protocol;\n\t\t\tbreak;\n\t\t}\n\t\tnhdr = skb_inner_network_header(skb);\n\t} else {\n\t\tgoto need_help;\n\t}\n\n\tswitch (protocol) {\n\tcase IPPROTO_IP:\n\t\tif (!spec->ipv4_okay)\n\t\t\tgoto need_help;\n\t\tiph = nhdr;\n\t\tip_proto = iph->protocol;\n\t\tif (iph->ihl != 5 && !spec->ip_options_okay)\n\t\t\tgoto need_help;\n\t\tbreak;\n\tcase IPPROTO_IPV6:\n\t\tif (!spec->ipv6_okay)\n\t\t\tgoto need_help;\n\t\tif (spec->no_encapped_ipv6 && *csum_encapped)\n\t\t\tgoto need_help;\n\t\tipv6 = nhdr;\n\t\tnhdr += sizeof(*ipv6);\n\t\tip_proto = ipv6->nexthdr;\n\t\tbreak;\n\tdefault:\n\t\tgoto need_help;\n\t}\n\nip_proto_again:\n\tswitch (ip_proto) {\n\tcase IPPROTO_TCP:\n\t\tif (!spec->tcp_okay ||\n\t\t    skb->csum_offset != offsetof(struct tcphdr, check))\n\t\t\tgoto need_help;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tif (!spec->udp_okay ||\n\t\t    skb->csum_offset != offsetof(struct udphdr, check))\n\t\t\tgoto need_help;\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\tif (!spec->sctp_okay ||\n\t\t    skb->csum_offset != offsetof(struct sctphdr, checksum))\n\t\t\tgoto cant_help;\n\t\tbreak;\n\tcase NEXTHDR_HOP:\n\tcase NEXTHDR_ROUTING:\n\tcase NEXTHDR_DEST: {\n\t\tu8 *opthdr = nhdr;\n\n\t\tif (protocol != IPPROTO_IPV6 || !spec->ext_hdrs_okay)\n\t\t\tgoto need_help;\n\n\t\tip_proto = opthdr[0];\n\t\tnhdr += (opthdr[1] + 1) << 3;\n\n\t\tgoto ip_proto_again;\n\t}\n\tdefault:\n\t\tgoto need_help;\n\t}\n\n\t/* Passed the tests for offloading checksum */\n\treturn true;\n\nneed_help:\n\tif (csum_help && !skb_shinfo(skb)->gso_size)\n\t\tskb_checksum_help(skb);\ncant_help:\n\treturn false;\n}\nEXPORT_SYMBOL(__skb_csum_offload_chk);\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb_mac_header(skb);\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn __vlan_get_protocol(skb, type, depth);\n}\n\n/**\n *\tskb_mac_gso_segment - mac layer segmentation handler.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n */\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_offload *ptype;\n\tint vlan_depth = skb->mac_len;\n\t__be16 type = skb_network_protocol(skb, &vlan_depth);\n\n\tif (unlikely(!type))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__skb_pull(skb, vlan_depth);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &offload_base, list) {\n\t\tif (ptype->type == type && ptype->callbacks.gso_segment) {\n\t\t\tsegs = ptype->callbacks.gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_mac_gso_segment);\n\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL;\n\telse\n\t\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_SGO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\tskb_warn_bad_offload(skb);\n\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\tBUILD_BUG_ON(SKB_SGO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn skb_mac_gso_segment(skb, features);\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev)\n{\n\tif (net_ratelimit()) {\n\t\tpr_err(\"%s: hw csum failure\\n\", dev ? dev->name : \"<unknown>\");\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* Actually, we should eliminate this check as soon as we know, that:\n * 1. IOMMU is present and allows to map all the memory.\n * 2. No high memory really exists on this machine.\n */\n\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (PCI_DMA_BUS_IS_PHYS) {\n\t\tstruct device *pdev = dev->dev.parent;\n\n\t\tif (!pdev)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tdma_addr_t addr = page_to_phys(skb_frag_page(frag));\n\t\t\tif (!pdev->dma_mask || addr + PAGE_SIZE - 1 > *pdev->dma_mask)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\tint tmp;\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, &tmp);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\t} else if (illegal_highdma(skb->dev, skb)) {\n\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(const struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > dev->gso_max_segs || gso_segs < dev->gso_min_segs)\n\t\tfeatures &= ~NETIF_F_GSO_MASK;\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (!list_empty(&ptype_all) || !list_empty(&dev->ptype_all))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb->next = NULL;\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_xmit_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)\n{\n\tnetdev_features_t features;\n\n\tif (skb->next)\n\t\treturn skb;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (!(features & NETIF_F_CSUM_MASK) &&\n\t\t\t    skb_checksum_help(skb))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size)  {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\thdr_len += tcp_hdrlen(skb);\n\t\telse\n\t\t\thdr_len += sizeof(struct udphdr);\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits __QDISC___STATE_RUNNING owner to get the lock more\n\t * often and dequeue packets faster.\n\t */\n\tcontended = qdisc_is_running(q);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\tkfree_skb(skb);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t} else\n\t\t\tqdisc_run_end(q);\n\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = q->enqueue(skb, q) & NET_XMIT_MASK;\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tstruct netprio_map *map = rcu_dereference_bh(skb->dev->priomap);\n\n\tif (!skb->priority && skb->sk && map) {\n\t\tunsigned int prioidx =\n\t\t\tsock_cgroup_prioidx(&skb->sk->sk_cgrp_data);\n\n\t\tif (prioidx < map->priomap_len)\n\t\t\tskb->priority = map->priomap[prioidx];\n\t}\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\nDEFINE_PER_CPU(int, xmit_recursion);\nEXPORT_SYMBOL(xmit_recursion);\n\n#define RECURSION_LIMIT 10\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx_ni(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct tcf_proto *cl = rcu_dereference_bh(dev->egress_cl_list);\n\tstruct tcf_result cl_res;\n\n\tif (!cl)\n\t\treturn skb;\n\n\t/* skb->tc_verd and qdisc_skb_cb(skb)->pkt_len were already set\n\t * earlier by the caller.\n\t */\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tc_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\t\t*ret = NET_XMIT_DROP;\n\t\tgoto drop;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\t\t*ret = NET_XMIT_SUCCESS;\ndrop:\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn skb;\n}\n#endif /* CONFIG_NET_EGRESS */\n\nstatic inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\trcu_read_lock();\n\tdev_maps = rcu_dereference(dev->xps_maps);\n\tif (dev_maps) {\n\t\tmap = rcu_dereference(\n\t\t    dev_maps->cpu_map[skb->sender_cpu - 1]);\n\t\tif (map) {\n\t\t\tif (map->len == 1)\n\t\t\t\tqueue_index = map->queues[0];\n\t\t\telse\n\t\t\t\tqueue_index = map->queues[reciprocal_scale(skb_get_hash(skb),\n\t\t\t\t\t\t\t\t\t   map->len)];\n\t\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\t\tqueue_index = -1;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nstatic u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, skb);\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, accel_priv,\n\t\t\t\t\t\t\t    __netdev_pick_tx);\n\t\telse\n\t\t\tqueue_index = __netdev_pick_tx(dev, skb);\n\n\t\tif (!accel_priv)\n\t\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n *\t__dev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\t@accel_priv: private data used for L2 forwarding offload\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nstatic int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\n\tskb_reset_mac_header(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_EGRESS);\n# ifdef CONFIG_NET_EGRESS\n\tif (static_key_false(&egress_needed)) {\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n# endif\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n#ifdef CONFIG_NET_SWITCHDEV\n\t/* Don't forward if offload device already forwarded */\n\tif (skb->offload_fwd_mark &&\n\t    skb->offload_fwd_mark == dev->offload_fwd_mark) {\n\t\tconsume_skb(skb);\n\t\trc = NET_XMIT_SUCCESS;\n\t\tgoto out;\n\t}\n#endif\n\n\ttxq = netdev_pick_tx(dev, skb, accel_priv);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t   loopback, all the sorts of tunnels...\n\n\t   Really, it is unlikely that netif_tx_lock protection is necessary\n\t   here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t   counters.)\n\t   However, it is possible, that they rely on protection\n\t   made by us here.\n\n\t   Check this and shot the lock. It is not prone from deadlocks.\n\t   Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\n\t\t\tif (__this_cpu_read(xmit_recursion) > RECURSION_LIMIT)\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto drop;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\t__this_cpu_inc(xmit_recursion);\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\t__this_cpu_dec(xmit_recursion);\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\ndrop:\n\trcu_read_unlock_bh();\n\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\n\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv)\n{\n\treturn __dev_queue_xmit(skb, accel_priv);\n}\nEXPORT_SYMBOL(dev_queue_xmit_accel);\n\n\n/*=======================================================================\n\t\t\tReceiver routines\n  =======================================================================*/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\nint weight_p __read_mostly = 64;            /* old backlog weight */\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key rps_needed __read_mostly;\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match */\n\t\tident = sock_flow_table->ents[hash & sock_flow_table->mask];\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = ACCESS_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (netdev_max_backlog >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\ndrop:\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_rx(skb);\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n\treturn ret;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\ttrace_netif_rx_entry(skb);\n\n\treturn netif_rx_internal(skb);\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_netif_rx_ni_entry(skb);\n\n\tpreempt_disable();\n\terr = netif_rx_internal(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nstatic void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(atomic_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\n\t\t__kfree_skb_flush();\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock;\n\n\t\t\thead = head->next_sched;\n\n\t\t\troot_lock = qdisc_lock(q);\n\t\t\tif (spin_trylock(root_lock)) {\n\t\t\t\tsmp_mb__before_atomic();\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t  &q->state);\n\t\t\t\tqdisc_run(q);\n\t\t\t\tspin_unlock(root_lock);\n\t\t\t} else {\n\t\t\t\tif (!test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t      &q->state)) {\n\t\t\t\t\t__netif_reschedule(q);\n\t\t\t\t} else {\n\t\t\t\t\tsmp_mb__before_atomic();\n\t\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t\t  &q->state);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n#if (defined(CONFIG_BRIDGE) || defined(CONFIG_BRIDGE_MODULE)) && \\\n    (defined(CONFIG_ATM_LANE) || defined(CONFIG_ATM_LANE_MODULE))\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto *cl = rcu_dereference_bh(skb->dev->ingress_cl_list);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!cl)\n\t\treturn skb;\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_INGRESS);\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tc_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tskb_do_redirect(skb);\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tASSERT_RTNL();\n\n\tif (dev->rx_handler)\n\t\treturn -EBUSY;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n#ifdef CONFIG_NETFILTER_INGRESS\n\tif (nf_hook_ingress_active(skb)) {\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\treturn nf_hook_ingress(skb);\n\t}\n#endif /* CONFIG_NETFILTER_INGRESS */\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||\n\t    skb->protocol == cpu_to_be16(ETH_P_8021AD)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (skb->tc_verd & TC_NCLS) {\n\t\tskb->tc_verd = CLR_TC_NCLS(skb->tc_verd);\n\t\tgoto ncls;\n\t}\n#endif\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_key_false(&ingress_needed)) {\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = 0;\nncls:\n#endif\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\tif (skb_vlan_tag_get_id(skb))\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\tskb->vlan_tci = 0;\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\telse\n\t\t\tret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\telse\n\t\t\tatomic_long_inc(&skb->dev->rx_nohandler);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned long pflags = current->flags;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tcurrent->flags |= PF_MEMALLOC;\n\t\tret = __netif_receive_skb_core(skb, true);\n\t\ttsk_restore_flags(current, pflags, PF_MEMALLOC);\n\t} else\n\t\tret = __netif_receive_skb_core(skb, false);\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\ttrace_netif_receive_skb_entry(skb);\n\n\treturn netif_receive_skb_internal(skb);\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/* Network device is going away, flush any packets still pending\n * Called with irqs disabled.\n */\nstatic void flush_backlog(void *arg)\n{\n\tstruct net_device *dev = arg;\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tstruct sk_buff *skb, *tmp;\n\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n}\n\nstatic int napi_gro_complete(struct sk_buff *skb)\n{\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint err = -ENOENT;\n\n\tBUILD_BUG_ON(sizeof(struct napi_gro_cb) > sizeof(skb->cb));\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\n\t\terr = ptype->callbacks.gro_complete(skb, 0);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\treturn netif_receive_skb_internal(skb);\n}\n\n/* napi->gro_list contains packets ordered by age.\n * youngest packets at the head of it.\n * Complete skbs in reverse order to reduce latencies.\n */\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old)\n{\n\tstruct sk_buff *skb, *prev = NULL;\n\n\t/* scan list and build reverse chain */\n\tfor (skb = napi->gro_list; skb != NULL; skb = skb->next) {\n\t\tskb->prev = prev;\n\t\tprev = skb;\n\t}\n\n\tfor (skb = prev; skb; skb = prev) {\n\t\tskb->next = NULL;\n\n\t\tif (flush_old && NAPI_GRO_CB(skb)->age == jiffies)\n\t\t\treturn;\n\n\t\tprev = skb->prev;\n\t\tnapi_gro_complete(skb);\n\t\tnapi->gro_count--;\n\t}\n\n\tnapi->gro_list = NULL;\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nstatic void gro_list_prepare(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff *p;\n\tunsigned int maclen = skb->dev->hard_header_len;\n\tu32 hash = skb_get_hash_raw(skb);\n\n\tfor (p = napi->gro_list; p; p = p->next) {\n\t\tunsigned long diffs;\n\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\n\t\tif (hash != skb_get_hash_raw(p)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= p->vlan_tci ^ skb->vlan_tci;\n\t\tdiffs |= skb_metadata_dst_cmp(p, skb);\n\t\tif (maclen == ETH_HLEN)\n\t\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t\t      skb_mac_header(skb));\n\t\telse if (!diffs)\n\t\t\tdiffs = memcmp(skb_mac_header(p),\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       maclen);\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t}\n}\n\nstatic void skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *pinfo = skb_shinfo(skb);\n\tconst skb_frag_t *frag0 = &pinfo->frags[0];\n\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (skb_mac_header(skb) == skb_tail_pointer(skb) &&\n\t    pinfo->nr_frags &&\n\t    !PageHighMem(skb_frag_page(frag0))) {\n\t\tNAPI_GRO_CB(skb)->frag0 = skb_frag_address(frag0);\n\t\tNAPI_GRO_CB(skb)->frag0_len = skb_frag_size(frag0);\n\t}\n}\n\nstatic void gro_pull_from_frag0(struct sk_buff *skb, int grow)\n{\n\tstruct skb_shared_info *pinfo = skb_shinfo(skb);\n\n\tBUG_ON(skb->end - skb->tail < grow);\n\n\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\tskb->data_len -= grow;\n\tskb->tail += grow;\n\n\tpinfo->frags[0].page_offset += grow;\n\tskb_frag_size_sub(&pinfo->frags[0], grow);\n\n\tif (unlikely(!skb_frag_size(&pinfo->frags[0]))) {\n\t\tskb_frag_unref(skb, 0);\n\t\tmemmove(pinfo->frags, pinfo->frags + 1,\n\t\t\t--pinfo->nr_frags * sizeof(pinfo->frags[0]));\n\t}\n}\n\nstatic enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint same_flow;\n\tenum gro_result ret;\n\tint grow;\n\n\tif (!(skb->dev->features & NETIF_F_GRO))\n\t\tgoto normal;\n\n\tif (skb_is_gso(skb) || skb_has_frag_list(skb) || skb->csum_bad)\n\t\tgoto normal;\n\n\tgro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = 0;\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->udp_mark = 0;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = ptype->callbacks.gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_count >= MAX_GRO_SKBS)) {\n\t\tstruct sk_buff *nskb = napi->gro_list;\n\n\t\t/* locate the end of the list to select the 'oldest' flow */\n\t\twhile (nskb->next) {\n\t\t\tpp = &nskb->next;\n\t\t\tnskb = *pp;\n\t\t}\n\t\t*pp = NULL;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t} else {\n\t\tnapi->gro_count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\n\nstruct packet_offload *gro_find_receive_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_receive_by_type);\n\nstruct packet_offload *gro_find_complete_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_complete_by_type);\n\nstatic gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tif (netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD) {\n\t\t\tskb_dst_drop(skb);\n\t\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\t} else {\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tskb_mark_napi_id(skb, napi);\n\ttrace_napi_gro_receive_entry(skb);\n\n\tskb_gro_reset_offset(skb);\n\n\treturn napi_skb_finish(dev_gro_receive(napi, skb), skb);\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tif (unlikely(skb->pfmemalloc)) {\n\t\tconsume_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, skb_headlen(skb));\n\t/* restore the reserve we had after netdev_alloc_skb_ip_align() */\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN - skb_headroom(skb));\n\tskb->vlan_tci = 0;\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\tskb->encapsulation = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = napi_alloc_skb(napi, GRO_MAX_HEAD);\n\t\tif (skb) {\n\t\t\tnapi->skb = skb;\n\t\t\tskb_mark_napi_id(skb, napi);\n\t\t}\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\nstatic gro_result_t napi_frags_finish(struct napi_struct *napi,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tif (ret == GRO_NORMAL && netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\tcase GRO_MERGED_FREE:\n\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/* Upper GRO stack assumes network header starts at gro_offset=0\n * Drivers could call both napi_gro_frags() and napi_gro_receive()\n * We copy ethernet header into skb->data to have a common layout.\n */\nstatic struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\teth = skb_gro_header_fast(skb, 0);\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\tif (!skb)\n\t\treturn GRO_DROP;\n\n\ttrace_napi_gro_frags_entry(skb);\n\n\treturn napi_frags_finish(napi, skb, dev_gro_receive(napi, skb));\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/* Compute the checksum from gro_offset and return the folded value\n * after adding in any pseudo checksum.\n */\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum wsum;\n\t__sum16 sum;\n\n\twsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);\n\n\t/* NAPI_GRO_CB(skb)->csum holds pseudo checksum */\n\tsum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev);\n\t}\n\n\tNAPI_GRO_CB(skb)->csum = wsum;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_gro_checksum_complete);\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\twhile (remsd) {\n\t\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\t\tif (cpu_online(remsd->cpu))\n\t\t\t\tsmp_call_function_single_async(remsd->cpu,\n\t\t\t\t\t\t\t   &remsd->csd);\n\t\t\tremsd = next;\n\t\t}\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tint work = 0;\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = weight_p;\n\tlocal_irq_disable();\n\twhile (1) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\tlocal_irq_enable();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_disable();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota) {\n\t\t\t\tlocal_irq_enable();\n\t\t\t\treturn work;\n\t\t\t}\n\t\t}\n\n\t\trps_lock(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\trps_unlock(sd);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t   &sd->process_queue);\n\t\trps_unlock(sd);\n\t}\n\tlocal_irq_enable();\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nvoid __napi_complete(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\n\tlist_del_init(&n->poll_list);\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n}\nEXPORT_SYMBOL(__napi_complete);\n\nvoid napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags;\n\n\t/*\n\t * don't let napi dequeue from the cpu poll list\n\t * just in case its running on a different cpu\n\t */\n\tif (unlikely(test_bit(NAPI_STATE_NPSVC, &n->state)))\n\t\treturn;\n\n\tif (n->gro_list) {\n\t\tunsigned long timeout = 0;\n\n\t\tif (work_done)\n\t\t\ttimeout = n->dev->gro_flush_timeout;\n\n\t\tif (timeout)\n\t\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t\t      HRTIMER_MODE_REL_PINNED);\n\t\telse\n\t\t\tnapi_gro_flush(n, false);\n\t}\n\tif (likely(list_empty(&n->poll_list))) {\n\t\tWARN_ON_ONCE(!test_and_clear_bit(NAPI_STATE_SCHED, &n->state));\n\t} else {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\t__napi_complete(n);\n\t\tlocal_irq_restore(flags);\n\t}\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n#define BUSY_POLL_BUDGET 8\nbool sk_busy_loop(struct sock *sk, int nonblock)\n{\n\tunsigned long end_time = !nonblock ? sk_busy_loop_end_time(sk) : 0;\n\tint (*busy_poll)(struct napi_struct *dev);\n\tstruct napi_struct *napi;\n\tint rc = false;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(sk->sk_napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\t/* Note: ndo_busy_poll method is optional in linux-4.5 */\n\tbusy_poll = napi->dev->netdev_ops->ndo_busy_poll;\n\n\tdo {\n\t\trc = 0;\n\t\tlocal_bh_disable();\n\t\tif (busy_poll) {\n\t\t\trc = busy_poll(napi);\n\t\t} else if (napi_schedule_prep(napi)) {\n\t\t\tvoid *have = netpoll_poll_lock(napi);\n\n\t\t\tif (test_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\t\t\trc = napi->poll(napi, BUSY_POLL_BUDGET);\n\t\t\t\ttrace_napi_poll(napi);\n\t\t\t\tif (rc == BUSY_POLL_BUDGET) {\n\t\t\t\t\tnapi_complete_done(napi, rc);\n\t\t\t\t\tnapi_schedule(napi);\n\t\t\t\t}\n\t\t\t}\n\t\t\tnetpoll_poll_unlock(have);\n\t\t}\n\t\tif (rc > 0)\n\t\t\tNET_ADD_STATS_BH(sock_net(sk),\n\t\t\t\t\t LINUX_MIB_BUSYPOLLRXPACKETS, rc);\n\t\tlocal_bh_enable();\n\n\t\tif (rc == LL_FLUSH_FAILED)\n\t\t\tbreak; /* permanent failure */\n\n\t\tcpu_relax();\n\t} while (!nonblock && skb_queue_empty(&sk->sk_receive_queue) &&\n\t\t !need_resched() && !busy_loop_timeout(end_time));\n\n\trc = !skb_queue_empty(&sk->sk_receive_queue);\nout:\n\trcu_read_unlock();\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nvoid napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state) ||\n\t    test_and_set_bit(NAPI_STATE_HASHED, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS+1 range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < NR_CPUS + 1))\n\t\t\tnapi_gen_id = NR_CPUS + 1;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\nEXPORT_SYMBOL_GPL(napi_hash_add);\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nbool napi_hash_del(struct napi_struct *napi)\n{\n\tbool rcu_sync_needed = false;\n\n\tspin_lock(&napi_hash_lock);\n\n\tif (test_and_clear_bit(NAPI_STATE_HASHED, &napi->state)) {\n\t\trcu_sync_needed = true;\n\t\thlist_del_rcu(&napi->napi_hash_node);\n\t}\n\tspin_unlock(&napi_hash_lock);\n\treturn rcu_sync_needed;\n}\nEXPORT_SYMBOL_GPL(napi_hash_del);\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\tif (napi->gro_list)\n\t\tnapi_schedule(napi);\n\n\treturn HRTIMER_NORESTART;\n}\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tINIT_LIST_HEAD(&napi->poll_list);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n\tnapi->skb = NULL;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tpr_err_once(\"netif_napi_add() called with weight %d on device %s\\n\",\n\t\t\t    weight, dev->name);\n\tnapi->weight = weight;\n\tlist_add(&napi->dev_list, &dev->napi_list);\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tspin_lock_init(&napi->poll_lock);\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tnapi_hash_add(napi);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\twhile (test_and_set_bit(NAPI_STATE_NPSVC, &n->state))\n\t\tmsleep(1);\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/* Must be called in process context */\nvoid netif_napi_del(struct napi_struct *napi)\n{\n\tmight_sleep();\n\tif (napi_hash_del(napi))\n\t\tsynchronize_net();\n\tlist_del_init(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tkfree_skb_list(napi->gro_list);\n\tnapi->gro_list = NULL;\n\tnapi->gro_count = 0;\n}\nEXPORT_SYMBOL(netif_napi_del);\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tvoid *have;\n\tint work, weight;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n);\n\t}\n\n\tWARN_ON_ONCE(work > weight);\n\n\tif (likely(work < weight))\n\t\tgoto out_unlock;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\tgoto out_unlock;\n\t}\n\n\tif (n->gro_list) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\tgoto out_unlock;\n\t}\n\n\tlist_add_tail(&n->poll_list, repoll);\n\nout_unlock:\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies + 2;\n\tint budget = netdev_budget;\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\treturn;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t__kfree_skb_flush();\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tASSERT_RTNL();\n\n\treturn __netdev_find_adj(upper_dev, &dev->all_adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->all_adj_list.upper);\n}\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\n/**\n * netdev_all_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->all_adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_all_upper_get_next_dev_rcu);\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr++;\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tdev_hold(adj_dev);\n\n\tpr_debug(\"dev_hold for %s, because of link added from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tkfree(adj);\n\tdev_put(adj_dev);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"tried to remove device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tBUG();\n\t}\n\n\tif (adj->ref_nr > 1) {\n\t\tpr_debug(\"%s to %s ref_nr-- = %d\\n\", dev->name, adj_dev->name,\n\t\t\t adj->ref_nr-1);\n\t\tadj->ref_nr--;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tdev_put(adj_dev);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list, private,\n\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list, private,\n\t\t\t\t\t   false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int __netdev_adjacent_dev_link(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->all_adj_list.upper,\n\t\t\t\t\t\t&upper_dev->all_adj_list.lower,\n\t\t\t\t\t\tNULL, false);\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, down_list);\n}\n\nstatic void __netdev_adjacent_dev_unlink(struct net_device *dev,\n\t\t\t\t\t struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev,\n\t\t\t\t\t   &dev->all_adj_list.upper,\n\t\t\t\t\t   &upper_dev->all_adj_list.lower);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\tint ret = __netdev_adjacent_dev_link(dev, upper_dev);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t       &dev->adj_list.upper,\n\t\t\t\t\t       &upper_dev->adj_list.lower,\n\t\t\t\t\t       private, master);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_unlink(dev, upper_dev);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink(dev, upper_dev);\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\tstruct netdev_adjacent *i, *j, *to_i, *to_j;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_find_adj(dev, &upper_dev->all_adj_list.upper))\n\t\treturn -EBUSY;\n\n\tif (__netdev_find_adj(upper_dev, &dev->adj_list.upper))\n\t\treturn -EEXIST;\n\n\tif (master && netdev_master_upper_dev_get(dev))\n\t\treturn -EBUSY;\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = master;\n\tchangeupper_info.linking = true;\n\tchangeupper_info.upper_info = upper_info;\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Now that we linked these devs, make all the upper_dev's\n\t * all_adj_list.upper visible to every dev's all_adj_list.lower an\n\t * versa, and don't forget the devices itself. All of these\n\t * links are non-neighbours.\n\t */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tlist_for_each_entry(j, &upper_dev->all_adj_list.upper, list) {\n\t\t\tpr_debug(\"Interlinking %s with %s, non-neighbour\\n\",\n\t\t\t\t i->dev->name, j->dev->name);\n\t\t\tret = __netdev_adjacent_dev_link(i->dev, j->dev);\n\t\t\tif (ret)\n\t\t\t\tgoto rollback_mesh;\n\t\t}\n\t}\n\n\t/* add dev to every upper_dev's upper device */\n\tlist_for_each_entry(i, &upper_dev->all_adj_list.upper, list) {\n\t\tpr_debug(\"linking %s's upper device %s with %s\\n\",\n\t\t\t upper_dev->name, i->dev->name, dev->name);\n\t\tret = __netdev_adjacent_dev_link(dev, i->dev);\n\t\tif (ret)\n\t\t\tgoto rollback_upper_mesh;\n\t}\n\n\t/* add upper_dev to every dev's lower device */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tpr_debug(\"linking %s's lower device %s with %s\\n\", dev->name,\n\t\t\t i->dev->name, upper_dev->name);\n\t\tret = __netdev_adjacent_dev_link(i->dev, upper_dev);\n\t\tif (ret)\n\t\t\tgoto rollback_lower_mesh;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback_lower_mesh;\n\n\treturn 0;\n\nrollback_lower_mesh:\n\tto_i = i;\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tif (i == to_i)\n\t\t\tbreak;\n\t\t__netdev_adjacent_dev_unlink(i->dev, upper_dev);\n\t}\n\n\ti = NULL;\n\nrollback_upper_mesh:\n\tto_i = i;\n\tlist_for_each_entry(i, &upper_dev->all_adj_list.upper, list) {\n\t\tif (i == to_i)\n\t\t\tbreak;\n\t\t__netdev_adjacent_dev_unlink(dev, i->dev);\n\t}\n\n\ti = j = NULL;\n\nrollback_mesh:\n\tto_i = i;\n\tto_j = j;\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tlist_for_each_entry(j, &upper_dev->all_adj_list.upper, list) {\n\t\t\tif (i == to_i && j == to_j)\n\t\t\t\tbreak;\n\t\t\t__netdev_adjacent_dev_unlink(i->dev, j->dev);\n\t\t}\n\t\tif (i == to_i)\n\t\t\tbreak;\n\t}\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, false, NULL, NULL);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\tstruct netdev_adjacent *i, *j;\n\tASSERT_RTNL();\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\tchangeupper_info.linking = false;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\t/* Here is the tricky part. We must remove all dev's lower\n\t * devices from all upper_dev's upper devices and vice\n\t * versa, to maintain the graph relationship.\n\t */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list)\n\t\tlist_for_each_entry(j, &upper_dev->all_adj_list.upper, list)\n\t\t\t__netdev_adjacent_dev_unlink(i->dev, j->dev);\n\n\t/* remove also the devices itself from lower/upper device\n\t * list\n\t */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list)\n\t\t__netdev_adjacent_dev_unlink(i->dev, upper_dev);\n\n\tlist_for_each_entry(i, &upper_dev->all_adj_list.upper, list)\n\t\t__netdev_adjacent_dev_unlink(dev, i->dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info\tinfo;\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO, dev,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\nint dev_get_nest_level(struct net_device *dev,\n\t\t       bool (*type_check)(const struct net_device *dev))\n{\n\tstruct net_device *lower = NULL;\n\tstruct list_head *iter;\n\tint max_nest = -1;\n\tint nest;\n\n\tASSERT_RTNL();\n\n\tnetdev_for_each_lower_dev(dev, lower, iter) {\n\t\tnest = dev_get_nest_level(lower, type_check);\n\t\tif (max_nest < nest)\n\t\t\tmax_nest = nest;\n\t}\n\n\tif (type_check(dev))\n\t\tmax_nest++;\n\n\treturn max_nest;\n}\nEXPORT_SYMBOL(dev_get_nest_level);\n\n/**\n * netdev_lower_change - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info;\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE, lower_dev,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tpr_warn(\"%s: promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(current->audit_context, GFP_ATOMIC,\n\t\t\t\tAUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t\"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\tdev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t(old_flags & IFF_PROMISC),\n\t\t\t\tfrom_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\tfrom_kuid(&init_user_ns, uid),\n\t\t\t\tfrom_kgid(&init_user_ns, gid),\n\t\t\t\taudit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tpr_warn(\"%s: allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP)\n\t\tret = ((old_flags & IFF_UP) ? __dev_close : __dev_open)(dev);\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t   is important. Some (broken) drivers set IFF_PROMISC, when\n\t   IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = changes;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nstatic int __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\t/*\tMTU must be positive.\t */\n\tif (new_mtu < 0)\n\t\treturn -EINVAL;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\t}\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\nEXPORT_SYMBOL(dev_set_group);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\nEXPORT_SYMBOL(dev_change_carrier);\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\nEXPORT_SYMBOL(dev_get_phys_port_id);\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_name)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_name(dev, name, len);\n}\nEXPORT_SYMBOL(dev_get_phys_port_name);\n\n/**\n *\tdev_change_proto_down - update protocol port state information\n *\t@dev: device\n *\t@proto_down: new value\n *\n *\tThis info can be used by switch drivers to set the phys state of the\n *\tport.\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_proto_down(dev, proto_down);\n}\nEXPORT_SYMBOL(dev_change_proto_down);\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tdev_net(dev)->dev_unreg_count++;\n}\n\nstatic void rollback_registered_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t\ton_each_cpu(flush_backlog, dev, 1);\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t   this device. They should clean all the things.\n\t\t*/\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U,\n\t\t\t\t\t\t     GFP_KERNEL);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tdev_put(dev);\n}\n\nstatic void rollback_registered(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\trollback_registered_many(&single);\n\tlist_del(&single);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\tnetdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* UFO needs SG and checksumming */\n\tif (features & NETIF_F_UFO) {\n\t\t/* maybe split UFO into V4 and V6? */\n\t\tif (!(features & NETIF_F_HW_CSUM) &&\n\t\t    ((features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) !=\n\t\t     (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM))) {\n\t\t\tnetdev_dbg(dev,\n\t\t\t\t\"Dropping NETIF_F_UFO since no checksum offload features.\\n\");\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\n\t\tif (!(features & NETIF_F_SG)) {\n\t\t\tnetdev_dbg(dev,\n\t\t\t\t\"Dropping NETIF_F_UFO since no NETIF_F_SG feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tif (dev->netdev_ops->ndo_busy_poll)\n\t\tfeatures |= NETIF_F_BUSY_POLL;\n\telse\n#endif\n\t\tfeatures &= ~NETIF_F_BUSY_POLL;\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off an an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err)\n\t\tdev->features = features;\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (netif_carrier_ok(rootdev)) {\n\t\tif (!netif_carrier_ok(dev))\n\t\t\tnetif_carrier_on(dev);\n\t} else {\n\t\tif (netif_carrier_ok(dev))\n\t\t\tnetif_carrier_off(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\n#ifdef CONFIG_SYSFS\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\n\tBUG_ON(count < 1);\n\n\trx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\n\tif (!rx) {\n\t\trx = vzalloc(sz);\n\t\tif (!rx)\n\t\t\treturn -ENOMEM;\n\t}\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++)\n\t\trx[i].dev = dev;\n\treturn 0;\n}\n#endif\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\n\tif (!tx) {\n\t\ttx = vzalloc(sz);\n\t\tif (!tx)\n\t\t\treturn -ENOMEM;\n\t}\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_uninit;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= NETIF_F_SOFT_FEATURES;\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK)) {\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\t}\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\trollback_registered(dev);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n/**\n * netdev_wait_allrefs - wait until all references are gone.\n * @dev: target net_device\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tmsleep(250);\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\trtnl_lock();\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t__rtnl_unlock();\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tpr_err(\"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\t\tWARN_ON(dev->dn_ptr);\n\n\t\tif (dev->destructor)\n\t\t\tdev->destructor(dev);\n\n\t\t/* Report a network device has been unregistered */\n\t\trtnl_lock();\n\t\tdev_net(dev)->dev_unreg_count--;\n\t\t__rtnl_unlock();\n\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n\tBUILD_BUG_ON(sizeof(*stats64) < sizeof(*netdev_stats));\n\tmemcpy(stats64, netdev_stats, sizeof(*stats64));\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + sizeof(*netdev_stats), 0,\n\t       sizeof(*stats64) - sizeof(*netdev_stats));\n#else\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(unsigned long);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n#endif\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += atomic_long_read(&dev->rx_dropped);\n\tstorage->tx_dropped += atomic_long_read(&dev->tx_dropped);\n\tstorage->rx_nohandler += atomic_long_read(&dev->rx_nohandler);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n *\talloc_netdev_mqs - allocate network device\n *\t@sizeof_priv:\t\tsize of private data to allocate space for\n *\t@name:\t\t\tdevice name format string\n *\t@name_assign_type: \torigin of device name\n *\t@setup:\t\t\tcallback to initialize device\n *\t@txqs:\t\t\tthe number of TX subqueues to allocate\n *\t@rxqs:\t\t\tthe number of RX subqueues to allocate\n *\n *\tAllocates a struct net_device with private data area for driver use\n *\tand performs basic initialization.  Also allocates subqueue structs\n *\tfor each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tsize_t alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYSFS\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n#endif\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kzalloc(alloc_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\n\tif (!p)\n\t\tp = vzalloc(alloc_size);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->gso_min_segs = 0;\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->all_adj_list.upper);\n\tINIT_LIST_HEAD(&dev->all_adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = 1;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n#ifdef CONFIG_SYSFS\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n#endif\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_ingress_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n *\tfree_netdev - free network device\n *\t@dev: device\n *\n *\tThis function does the last stage of destroying an allocated device\n * \tinterface. The reference to the device object is released.\n *\tIf this is the last reference then it will be freed.\n *\tMust be called in process context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\tnetif_free_tx_queues(dev);\n#ifdef CONFIG_SYSFS\n\tkvfree(dev->_rx);\n#endif\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\trollback_registered(dev);\n\t\t/* Finish processing unregister after unlock */\n\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tif (!list_empty(head)) {\n\t\trollback_registered_many(head);\n\t\tlist_for_each_entry(dev, head, unreg_list)\n\t\t\tnet_set_todo(dev);\n\t\tlist_del(head);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(dev_net(dev), net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\tif (dev_get_valid_name(net, dev, pat) < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\terr = -ENODEV;\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t   this device. They should clean all the things.\n\n\t   Note that dev->reg_state stays at NETREG_REGISTERED.\n\t   This is wanted because this way 8021q and macvlan know\n\t   the device is just moving and can keep their slaves up.\n\t*/\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\trtmsg_ifinfo(RTM_DELLINK, dev, ~0U, GFP_KERNEL);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex))\n\t\tdev->ifindex = dev_new_index(net);\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_callback(struct notifier_block *nfb,\n\t\t\t    unsigned long action,\n\t\t\t    void *ocpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu, oldcpu = (unsigned long)ocpu;\n\tstruct softnet_data *sd, *oldsd;\n\n\tif (action != CPU_DEAD && action != CPU_DEAD_FROZEN)\n\t\treturn NOTIFY_OK;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn NOTIFY_OK;\n}\n\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc(sizeof(*hash) * NETDEV_HASHENTRIES, GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tif (net != &init_net)\n\t\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit rtnl_lock_unregistering(struct list_head *net_list)\n{\n\t/* Return with the rtnl_lock held when there are no network\n\t * devices unregistering in any network namespace in net_list.\n\t */\n\tstruct net *net;\n\tbool unregistering;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tadd_wait_queue(&netdev_unregistering_wq, &wait);\n\tfor (;;) {\n\t\tunregistering = false;\n\t\trtnl_lock();\n\t\tlist_for_each_entry(net, net_list, exit_list) {\n\t\t\tif (net->dev_unreg_count > 0) {\n\t\t\t\tunregistering = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!unregistering)\n\t\t\tbreak;\n\t\t__rtnl_unlock();\n\n\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n\t}\n\tremove_wait_queue(&netdev_unregistering_wq, &wait);\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\t/* To prevent network device cleanup code from dereferencing\n\t * loopback devices or network devices that have been freed\n\t * wait here for all pending unregistrations to complete,\n\t * before unregistring the loopback device and allowing the\n\t * network namespace be freed.\n\t *\n\t * The netdev todo list containing all network devices\n\t * unregistrations that happen in default_device_exit_batch\n\t * will run in the rtnl_unlock() at the end of\n\t * default_device_exit_batch.\n\t */\n\trtnl_lock_unregistering(net_list);\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tINIT_LIST_HEAD(&offload_base);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tsd->csd.func = rps_trigger_softirq;\n\t\tsd->csd.info = sd;\n\t\tsd->cpu = i;\n#endif\n\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\thotcpu_notifier(dev_cpu_callback, 0);\n\tdst_subsys_init();\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPF_INET protocol family socket handler.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Changes (see also sock.c)\n *\n *\t\tpiggy,\n *\t\tKarl Knutson\t:\tSocket protocol table\n *\t\tA.N.Kuznetsov\t:\tSocket death error in accept().\n *\t\tJohn Richardson :\tFix non blocking error in connect()\n *\t\t\t\t\tso sockets that fail to connect\n *\t\t\t\t\tdon't return -EINPROGRESS.\n *\t\tAlan Cox\t:\tAsynchronous I/O support\n *\t\tAlan Cox\t:\tKeep correct socket pointer on sock\n *\t\t\t\t\tstructures\n *\t\t\t\t\twhen accept() ed\n *\t\tAlan Cox\t:\tSemantics of SO_LINGER aren't state\n *\t\t\t\t\tmoved to close when you look carefully.\n *\t\t\t\t\tWith this fixed and the accept bug fixed\n *\t\t\t\t\tsome RPC stuff seems happier.\n *\t\tNiibe Yutaka\t:\t4.4BSD style write async I/O\n *\t\tAlan Cox,\n *\t\tTony Gale \t:\tFixed reuse semantics.\n *\t\tAlan Cox\t:\tbind() shouldn't abort existing but dead\n *\t\t\t\t\tsockets. Stops FTP netin:.. I hope.\n *\t\tAlan Cox\t:\tbind() works correctly for RAW sockets.\n *\t\t\t\t\tNote that FreeBSD at least was broken\n *\t\t\t\t\tin this respect so be careful with\n *\t\t\t\t\tcompatibility tests...\n *\t\tAlan Cox\t:\trouting cache support\n *\t\tAlan Cox\t:\tmemzero the socket structure for\n *\t\t\t\t\tcompactness.\n *\t\tMatt Day\t:\tnonblock connect error handler\n *\t\tAlan Cox\t:\tAllow large numbers of pending sockets\n *\t\t\t\t\t(eg for big web sites), but only if\n *\t\t\t\t\tspecifically application requested.\n *\t\tAlan Cox\t:\tNew buffering throughout IP. Used\n *\t\t\t\t\tdumbly.\n *\t\tAlan Cox\t:\tNew buffering now used smartly.\n *\t\tAlan Cox\t:\tBSD rather than common sense\n *\t\t\t\t\tinterpretation of listen.\n *\t\tGermano Caronni\t:\tAssorted small races.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg basic support.\n *\t\tAlan Cox\t:\tOnly sendmsg/recvmsg now supported.\n *\t\tAlan Cox\t:\tLocked down bind (see security list).\n *\t\tAlan Cox\t:\tLoosened bind a little.\n *\t\tMike McLagan\t:\tADD/DEL DLCI Ioctls\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tSome other random speedups.\n *\t\tCyrus Durgin\t:\tCleaned up file for kmod hacks.\n *\t\tAndi Kleen\t:\tFix inet_stream_connect TCP race.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"IPv4: \" fmt\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/inet.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <net/checksum.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/arp.h>\n#include <net/route.h>\n#include <net/ip_fib.h>\n#include <net/inet_connection_sock.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <net/ping.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/raw.h>\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/ip_tunnels.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/secure_seq.h>\n#ifdef CONFIG_IP_MROUTE\n#include <linux/mroute.h>\n#endif\n#include <net/l3mdev.h>\n\n\n/* The inetsw table contains everything that inet_create needs to\n * build a new socket.\n */\nstatic struct list_head inetsw[SOCK_MAX];\nstatic DEFINE_SPINLOCK(inetsw_lock);\n\n/* New destruction routine */\n\nvoid inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(rcu_dereference_protected(inet->inet_opt, 1));\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tdst_release(sk->sk_rx_dst);\n\tsk_refcnt_debug_dec(sk);\n}\nEXPORT_SYMBOL(inet_sock_destruct);\n\n/*\n *\tThe routines beyond this point handle the behaviour of an AF_INET\n *\tsocket object. Mostly it punts to the subprotocols of IP to do\n *\tthe work.\n */\n\n/*\n *\tAutomatically bind an unbound socket.\n */\n\nstatic int inet_autobind(struct sock *sk)\n{\n\tstruct inet_sock *inet;\n\t/* We may need to bind the socket. */\n\tlock_sock(sk);\n\tinet = inet_sk(sk);\n\tif (!inet->inet_num) {\n\t\tif (sk->sk_prot->get_port(sk, 0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t}\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/*\n *\tMove a socket into listening state.\n */\nint inet_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\tunsigned char old_state;\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)\n\t\tgoto out;\n\n\told_state = sk->sk_state;\n\tif (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\tgoto out;\n\n\t/* Really, if the socket is already in listen state\n\t * we can only allow the backlog to be adjusted.\n\t */\n\tif (old_state != TCP_LISTEN) {\n\t\t/* Check special setups for testing purpose to enable TFO w/o\n\t\t * requiring TCP_FASTOPEN sockopt.\n\t\t * Note that only TCP sockets (SOCK_STREAM) will reach here.\n\t\t * Also fastopenq may already been allocated because this\n\t\t * socket was in TCP_LISTEN state previously but was\n\t\t * shutdown() (rather than close()).\n\t\t */\n\t\tif ((sysctl_tcp_fastopen & TFO_SERVER_ENABLE) != 0 &&\n\t\t    !inet_csk(sk)->icsk_accept_queue.fastopenq.max_qlen) {\n\t\t\tif ((sysctl_tcp_fastopen & TFO_SERVER_WO_SOCKOPT1) != 0)\n\t\t\t\tfastopen_queue_tune(sk, backlog);\n\t\t\telse if ((sysctl_tcp_fastopen &\n\t\t\t\t  TFO_SERVER_WO_SOCKOPT2) != 0)\n\t\t\t\tfastopen_queue_tune(sk,\n\t\t\t\t    ((uint)sysctl_tcp_fastopen) >> 16);\n\n\t\t\ttcp_fastopen_init_key_once(true);\n\t\t}\n\t\terr = inet_csk_listen_start(sk, backlog);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\tsk->sk_max_ack_backlog = backlog;\n\terr = 0;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_listen);\n\n/*\n *\tCreate an inet socket.\n */\n\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\n\t\t       int kern)\n{\n\tstruct sock *sk;\n\tstruct inet_protosw *answer;\n\tstruct inet_sock *inet;\n\tstruct proto *answer_prot;\n\tunsigned char answer_flags;\n\tint try_loading_module = 0;\n\tint err;\n\n\tif (protocol < 0 || protocol >= IPPROTO_MAX)\n\t\treturn -EINVAL;\n\n\tsock->state = SS_UNCONNECTED;\n\n\t/* Look for the requested type/protocol pair. */\nlookup_protocol:\n\terr = -ESOCKTNOSUPPORT;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(answer, &inetsw[sock->type], list) {\n\n\t\terr = 0;\n\t\t/* Check the non-wild match. */\n\t\tif (protocol == answer->protocol) {\n\t\t\tif (protocol != IPPROTO_IP)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* Check for the two wild cases. */\n\t\t\tif (IPPROTO_IP == protocol) {\n\t\t\t\tprotocol = answer->protocol;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IPPROTO_IP == answer->protocol)\n\t\t\t\tbreak;\n\t\t}\n\t\terr = -EPROTONOSUPPORT;\n\t}\n\n\tif (unlikely(err)) {\n\t\tif (try_loading_module < 2) {\n\t\t\trcu_read_unlock();\n\t\t\t/*\n\t\t\t * Be more specific, e.g. net-pf-2-proto-132-type-1\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)\n\t\t\t */\n\t\t\tif (++try_loading_module == 1)\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d-type-%d\",\n\t\t\t\t\t       PF_INET, protocol, sock->type);\n\t\t\t/*\n\t\t\t * Fall back to generic, e.g. net-pf-2-proto-132\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP)\n\t\t\t */\n\t\t\telse\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d\",\n\t\t\t\t\t       PF_INET, protocol);\n\t\t\tgoto lookup_protocol;\n\t\t} else\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\terr = -EPERM;\n\tif (sock->type == SOCK_RAW && !kern &&\n\t    !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out_rcu_unlock;\n\n\tsock->ops = answer->ops;\n\tanswer_prot = answer->prot;\n\tanswer_flags = answer->flags;\n\trcu_read_unlock();\n\n\tWARN_ON(!answer_prot->slab);\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);\n\tif (!sk)\n\t\tgoto out;\n\n\terr = 0;\n\tif (INET_PROTOSW_REUSE & answer_flags)\n\t\tsk->sk_reuse = SK_CAN_REUSE;\n\n\tinet = inet_sk(sk);\n\tinet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;\n\n\tinet->nodefrag = 0;\n\n\tif (SOCK_RAW == sock->type) {\n\t\tinet->inet_num = protocol;\n\t\tif (IPPROTO_RAW == protocol)\n\t\t\tinet->hdrincl = 1;\n\t}\n\n\tif (net->ipv4.sysctl_ip_no_pmtu_disc)\n\t\tinet->pmtudisc = IP_PMTUDISC_DONT;\n\telse\n\t\tinet->pmtudisc = IP_PMTUDISC_WANT;\n\n\tinet->inet_id = 0;\n\n\tsock_init_data(sock, sk);\n\n\tsk->sk_destruct\t   = inet_sock_destruct;\n\tsk->sk_protocol\t   = protocol;\n\tsk->sk_backlog_rcv = sk->sk_prot->backlog_rcv;\n\n\tinet->uc_ttl\t= -1;\n\tinet->mc_loop\t= 1;\n\tinet->mc_ttl\t= 1;\n\tinet->mc_all\t= 1;\n\tinet->mc_index\t= 0;\n\tinet->mc_list\t= NULL;\n\tinet->rcv_tos\t= 0;\n\n\tsk_refcnt_debug_inc(sk);\n\n\tif (inet->inet_num) {\n\t\t/* It assumes that any protocol which allows\n\t\t * the user to assign a number at socket\n\t\t * creation time automatically\n\t\t * shares.\n\t\t */\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t\t/* Add to protocol hash chains. */\n\t\terr = sk->sk_prot->hash(sk);\n\t\tif (err) {\n\t\t\tsk_common_release(sk);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (sk->sk_prot->init) {\n\t\terr = sk->sk_prot->init(sk);\n\t\tif (err)\n\t\t\tsk_common_release(sk);\n\t}\nout:\n\treturn err;\nout_rcu_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\n\n\n/*\n *\tThe peer socket should always be NULL (or else). When we call this\n *\tfunction we are destroying the object and from then on nobody\n *\tshould refer to it.\n */\nint inet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk) {\n\t\tlong timeout;\n\n\t\t/* Applications forget to leave groups before exiting */\n\t\tip_mc_drop_socket(sk);\n\n\t\t/* If linger is set, we don't return until the close\n\t\t * is complete.  Otherwise we return immediately. The\n\t\t * actually closing is done the same either way.\n\t\t *\n\t\t * If the close is due to the process exiting, we never\n\t\t * linger..\n\t\t */\n\t\ttimeout = 0;\n\t\tif (sock_flag(sk, SOCK_LINGER) &&\n\t\t    !(current->flags & PF_EXITING))\n\t\t\ttimeout = sk->sk_lingertime;\n\t\tsock->sk = NULL;\n\t\tsk->sk_prot->close(sk, timeout);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_release);\n\nint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *addr = (struct sockaddr_in *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned short snum;\n\tint chk_addr_ret;\n\tu32 tb_id = RT_TABLE_LOCAL;\n\tint err;\n\n\t/* If the socket has its own bind function then use it. (RAW) */\n\tif (sk->sk_prot->bind) {\n\t\terr = sk->sk_prot->bind(sk, uaddr, addr_len);\n\t\tgoto out;\n\t}\n\terr = -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\tgoto out;\n\n\tif (addr->sin_family != AF_INET) {\n\t\t/* Compatibility games : accept AF_UNSPEC (mapped to AF_INET)\n\t\t * only if s_addr is INADDR_ANY.\n\t\t */\n\t\terr = -EAFNOSUPPORT;\n\t\tif (addr->sin_family != AF_UNSPEC ||\n\t\t    addr->sin_addr.s_addr != htonl(INADDR_ANY))\n\t\t\tgoto out;\n\t}\n\n\ttb_id = l3mdev_fib_table_by_index(net, sk->sk_bound_dev_if) ? : tb_id;\n\tchk_addr_ret = inet_addr_type_table(net, addr->sin_addr.s_addr, tb_id);\n\n\t/* Not specified by any standard per-se, however it breaks too\n\t * many applications when removed.  It is unfortunate since\n\t * allowing applications to make a non-local bind solves\n\t * several problems with systems using dynamic addressing.\n\t * (ie. your servers still start up even if your ISDN link\n\t *  is temporarily down)\n\t */\n\terr = -EADDRNOTAVAIL;\n\tif (!net->ipv4.sysctl_ip_nonlocal_bind &&\n\t    !(inet->freebind || inet->transparent) &&\n\t    addr->sin_addr.s_addr != htonl(INADDR_ANY) &&\n\t    chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST &&\n\t    chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tsnum = ntohs(addr->sin_port);\n\terr = -EACCES;\n\tif (snum && snum < PROT_SOCK &&\n\t    !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE))\n\t\tgoto out;\n\n\t/*      We keep a pair of addresses. rcv_saddr is the one\n\t *      used by hash lookups, and saddr is used for transmit.\n\t *\n\t *      In the BSD API these are the same except where it\n\t *      would be illegal to use them (multicast/broadcast) in\n\t *      which case the sending device address is used.\n\t */\n\tlock_sock(sk);\n\n\t/* Check these errors (active socket, double bind). */\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE || inet->inet_num)\n\t\tgoto out_release_sock;\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->sin_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\n\t/* Make sure we are allowed to bind here. */\n\tif ((snum || !inet->bind_address_no_port) &&\n\t    sk->sk_prot->get_port(sk, snum)) {\n\t\tinet->inet_saddr = inet->inet_rcv_saddr = 0;\n\t\terr = -EADDRINUSE;\n\t\tgoto out_release_sock;\n\t}\n\n\tif (inet->inet_rcv_saddr)\n\t\tsk->sk_userlocks |= SOCK_BINDADDR_LOCK;\n\tif (snum)\n\t\tsk->sk_userlocks |= SOCK_BINDPORT_LOCK;\n\tinet->inet_sport = htons(inet->inet_num);\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsk_dst_reset(sk);\n\terr = 0;\nout_release_sock:\n\trelease_sock(sk);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_bind);\n\nint inet_dgram_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn sk->sk_prot->disconnect(sk, flags);\n\n\tif (!inet_sk(sk)->inet_num && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn sk->sk_prot->connect(sk, uaddr, addr_len);\n}\nEXPORT_SYMBOL(inet_dgram_connect);\n\nstatic long inet_wait_for_connect(struct sock *sk, long timeo, int writebias)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\tsk->sk_write_pending += writebias;\n\n\t/* Basic assumption: if someone sets sk->sk_err, he _must_\n\t * change state of the socket from TCP_SYN_*.\n\t * Connect() does not allow to get error notifications\n\t * without closing the socket.\n\t */\n\twhile ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\trelease_sock(sk);\n\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock(sk);\n\t\tif (signal_pending(current) || !timeo)\n\t\t\tbreak;\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\tsk->sk_write_pending -= writebias;\n\treturn timeo;\n}\n\n/*\n *\tConnect to a remote host. There is regrettably still a little\n *\tTCP 'magic' in here.\n */\nint __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\tlong timeo;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\n\tif (uaddr->sa_family == AF_UNSPEC) {\n\t\terr = sk->sk_prot->disconnect(sk, flags);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tgoto out;\n\t}\n\n\tswitch (sock->state) {\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\tcase SS_CONNECTED:\n\t\terr = -EISCONN;\n\t\tgoto out;\n\tcase SS_CONNECTING:\n\t\terr = -EALREADY;\n\t\t/* Fall out of switch with err, set for this state */\n\t\tbreak;\n\tcase SS_UNCONNECTED:\n\t\terr = -EISCONN;\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\tgoto out;\n\n\t\terr = sk->sk_prot->connect(sk, uaddr, addr_len);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tsock->state = SS_CONNECTING;\n\n\t\t/* Just entered SS_CONNECTING state; the only\n\t\t * difference is that return value in non-blocking\n\t\t * case is EINPROGRESS, rather than EALREADY.\n\t\t */\n\t\terr = -EINPROGRESS;\n\t\tbreak;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\tint writebias = (sk->sk_protocol == IPPROTO_TCP) &&\n\t\t\t\ttcp_sk(sk)->fastopen_req &&\n\t\t\t\ttcp_sk(sk)->fastopen_req->data ? 1 : 0;\n\n\t\t/* Error code is set above */\n\t\tif (!timeo || !inet_wait_for_connect(sk, timeo, writebias))\n\t\t\tgoto out;\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/* Connection was closed by RST, timeout, ICMP error\n\t * or another process disconnected us.\n\t */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto sock_error;\n\n\t/* sk->sk_err may be not zero now, if RECVERR was ordered by user\n\t * and error was received after socket entered established state.\n\t * Hence, it is handled normally after connect() return successfully.\n\t */\n\n\tsock->state = SS_CONNECTED;\n\terr = 0;\nout:\n\treturn err;\n\nsock_error:\n\terr = sock_error(sk) ? : -ECONNABORTED;\n\tsock->state = SS_UNCONNECTED;\n\tif (sk->sk_prot->disconnect(sk, flags))\n\t\tsock->state = SS_DISCONNECTING;\n\tgoto out;\n}\nEXPORT_SYMBOL(__inet_stream_connect);\n\nint inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint addr_len, int flags)\n{\n\tint err;\n\n\tlock_sock(sock->sk);\n\terr = __inet_stream_connect(sock, uaddr, addr_len, flags);\n\trelease_sock(sock->sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_stream_connect);\n\n/*\n *\tAccept a pending connection. The TCP layer now gives BSD semantics.\n */\n\nint inet_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_accept);\n\n\n/*\n *\tThis does both peername and sockname.\n */\nint inet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint *uaddr_len, int peer)\n{\n\tstruct sock *sk\t\t= sock->sk;\n\tstruct inet_sock *inet\t= inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, uaddr);\n\n\tsin->sin_family = AF_INET;\n\tif (peer) {\n\t\tif (!inet->inet_dport ||\n\t\t    (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&\n\t\t     peer == 1))\n\t\t\treturn -ENOTCONN;\n\t\tsin->sin_port = inet->inet_dport;\n\t\tsin->sin_addr.s_addr = inet->inet_daddr;\n\t} else {\n\t\t__be32 addr = inet->inet_rcv_saddr;\n\t\tif (!addr)\n\t\t\taddr = inet->inet_saddr;\n\t\tsin->sin_port = inet->inet_sport;\n\t\tsin->sin_addr.s_addr = addr;\n\t}\n\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t*uaddr_len = sizeof(*sin);\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_getname);\n\nint inet_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\treturn sk->sk_prot->sendmsg(sk, msg, size);\n}\nEXPORT_SYMBOL(inet_sendmsg);\n\nssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\tif (sk->sk_prot->sendpage)\n\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}\nEXPORT_SYMBOL(inet_sendpage);\n\nint inet_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\tsock_rps_record_flow(sk);\n\n\terr = sk->sk_prot->recvmsg(sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_recvmsg);\n\nint inet_shutdown(struct socket *sock, int how)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\t/* This should really check to make sure\n\t * the socket is a TCP socket. (WHY AC...)\n\t */\n\thow++; /* maps 0->1 has the advantage of making bit 1 rcvs and\n\t\t       1->2 bit 2 snds.\n\t\t       2->3 */\n\tif ((how & ~SHUTDOWN_MASK) || !how)\t/* MAXINT->0 */\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tif (sock->state == SS_CONNECTING) {\n\t\tif ((1 << sk->sk_state) &\n\t\t    (TCPF_SYN_SENT | TCPF_SYN_RECV | TCPF_CLOSE))\n\t\t\tsock->state = SS_DISCONNECTING;\n\t\telse\n\t\t\tsock->state = SS_CONNECTED;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\terr = -ENOTCONN;\n\t\t/* Hack to wake up other listeners, who can poll for\n\t\t   POLLHUP, even on eg. unconnected UDP sockets -- RR */\n\tdefault:\n\t\tsk->sk_shutdown |= how;\n\t\tif (sk->sk_prot->shutdown)\n\t\t\tsk->sk_prot->shutdown(sk, how);\n\t\tbreak;\n\n\t/* Remaining two branches are temporary solution for missing\n\t * close() in multithreaded environment. It is _not_ a good idea,\n\t * but we have no choice until close() is repaired at VFS level.\n\t */\n\tcase TCP_LISTEN:\n\t\tif (!(how & RCV_SHUTDOWN))\n\t\t\tbreak;\n\t\t/* Fall through */\n\tcase TCP_SYN_SENT:\n\t\terr = sk->sk_prot->disconnect(sk, O_NONBLOCK);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tbreak;\n\t}\n\n\t/* Wake up anyone sleeping in poll. */\n\tsk->sk_state_change(sk);\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_shutdown);\n\n/*\n *\tioctl() calls you can issue on an INET socket. Most of these are\n *\tdevice configuration and stuff and very rarely used. Some ioctls\n *\tpass on to the socket itself.\n *\n *\tNOTE: I like the idea of a module for the config stuff. ie ifconfig\n *\tloads the devconfigure module does its configuring and unloads it.\n *\tThere's a good 20K of config code hanging around the kernel.\n */\n\nint inet_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\tstruct net *net = sock_net(sk);\n\n\tswitch (cmd) {\n\tcase SIOCGSTAMP:\n\t\terr = sock_get_timestamp(sk, (struct timeval __user *)arg);\n\t\tbreak;\n\tcase SIOCGSTAMPNS:\n\t\terr = sock_get_timestampns(sk, (struct timespec __user *)arg);\n\t\tbreak;\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCRTMSG:\n\t\terr = ip_rt_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\t\terr = arp_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFPFLAGS:\n\tcase SIOCGIFPFLAGS:\n\tcase SIOCSIFFLAGS:\n\t\terr = devinet_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tif (sk->sk_prot->ioctl)\n\t\t\terr = sk->sk_prot->ioctl(sk, cmd, arg);\n\t\telse\n\t\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(inet_ioctl);\n\n#ifdef CONFIG_COMPAT\nstatic int inet_compat_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = -ENOIOCTLCMD;\n\n\tif (sk->sk_prot->compat_ioctl)\n\t\terr = sk->sk_prot->compat_ioctl(sk, cmd, arg);\n\n\treturn err;\n}\n#endif\n\nconst struct proto_ops inet_stream_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = tcp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = inet_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n\t.splice_read\t   = tcp_splice_read,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_stream_ops);\n\nconst struct proto_ops inet_dgram_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = udp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_dgram_ops);\n\n/*\n * For SOCK_RAW sockets; should be the same as inet_dgram_ops but without\n * udp_poll\n */\nstatic const struct proto_ops inet_sockraw_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\n\nstatic const struct net_proto_family inet_family_ops = {\n\t.family = PF_INET,\n\t.create = inet_create,\n\t.owner\t= THIS_MODULE,\n};\n\n/* Upon startup we insert all the elements in inetsw_array[] into\n * the linked list inetsw.\n */\nstatic struct inet_protosw inetsw_array[] =\n{\n\t{\n\t\t.type =       SOCK_STREAM,\n\t\t.protocol =   IPPROTO_TCP,\n\t\t.prot =       &tcp_prot,\n\t\t.ops =        &inet_stream_ops,\n\t\t.flags =      INET_PROTOSW_PERMANENT |\n\t\t\t      INET_PROTOSW_ICSK,\n\t},\n\n\t{\n\t\t.type =       SOCK_DGRAM,\n\t\t.protocol =   IPPROTO_UDP,\n\t\t.prot =       &udp_prot,\n\t\t.ops =        &inet_dgram_ops,\n\t\t.flags =      INET_PROTOSW_PERMANENT,\n       },\n\n       {\n\t\t.type =       SOCK_DGRAM,\n\t\t.protocol =   IPPROTO_ICMP,\n\t\t.prot =       &ping_prot,\n\t\t.ops =        &inet_dgram_ops,\n\t\t.flags =      INET_PROTOSW_REUSE,\n       },\n\n       {\n\t       .type =       SOCK_RAW,\n\t       .protocol =   IPPROTO_IP,\t/* wild card */\n\t       .prot =       &raw_prot,\n\t       .ops =        &inet_sockraw_ops,\n\t       .flags =      INET_PROTOSW_REUSE,\n       }\n};\n\n#define INETSW_ARRAY_LEN ARRAY_SIZE(inetsw_array)\n\nvoid inet_register_protosw(struct inet_protosw *p)\n{\n\tstruct list_head *lh;\n\tstruct inet_protosw *answer;\n\tint protocol = p->protocol;\n\tstruct list_head *last_perm;\n\n\tspin_lock_bh(&inetsw_lock);\n\n\tif (p->type >= SOCK_MAX)\n\t\tgoto out_illegal;\n\n\t/* If we are trying to override a permanent protocol, bail. */\n\tlast_perm = &inetsw[p->type];\n\tlist_for_each(lh, &inetsw[p->type]) {\n\t\tanswer = list_entry(lh, struct inet_protosw, list);\n\t\t/* Check only the non-wild match. */\n\t\tif ((INET_PROTOSW_PERMANENT & answer->flags) == 0)\n\t\t\tbreak;\n\t\tif (protocol == answer->protocol)\n\t\t\tgoto out_permanent;\n\t\tlast_perm = lh;\n\t}\n\n\t/* Add the new entry after the last permanent entry if any, so that\n\t * the new entry does not override a permanent entry when matched with\n\t * a wild-card protocol. But it is allowed to override any existing\n\t * non-permanent entry.  This means that when we remove this entry, the\n\t * system automatically returns to the old behavior.\n\t */\n\tlist_add_rcu(&p->list, last_perm);\nout:\n\tspin_unlock_bh(&inetsw_lock);\n\n\treturn;\n\nout_permanent:\n\tpr_err(\"Attempt to override permanent protocol %d\\n\", protocol);\n\tgoto out;\n\nout_illegal:\n\tpr_err(\"Ignoring attempt to register invalid socket type %d\\n\",\n\t       p->type);\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_register_protosw);\n\nvoid inet_unregister_protosw(struct inet_protosw *p)\n{\n\tif (INET_PROTOSW_PERMANENT & p->flags) {\n\t\tpr_err(\"Attempt to unregister permanent protocol %d\\n\",\n\t\t       p->protocol);\n\t} else {\n\t\tspin_lock_bh(&inetsw_lock);\n\t\tlist_del_rcu(&p->list);\n\t\tspin_unlock_bh(&inetsw_lock);\n\n\t\tsynchronize_net();\n\t}\n}\nEXPORT_SYMBOL(inet_unregister_protosw);\n\nstatic int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\tstruct ip_options_rcu *inet_opt;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\n\t/* Query new route. */\n\tfl4 = &inet->cork.fl.u.ip4;\n\trt = ip_route_connect(fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = fl4->saddr;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sock_net(sk)->ipv4.sysctl_ip_dynaddr > 1) {\n\t\tpr_info(\"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t\t__func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\treturn __sk_prot_rehash(sk);\n}\n\nint inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tstruct ip_options_rcu *inet_opt;\n\tstruct flowi4 *fl4;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tdaddr = inet->inet_daddr;\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trcu_read_unlock();\n\tfl4 = &inet->cork.fl.u.ip4;\n\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sock_net(sk)->ipv4.sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(inet_sk_rebuild_header);\n\nstatic struct sk_buff *inet_gso_segment(struct sk_buff *skb,\n\t\t\t\t\tnetdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tconst struct net_offload *ops;\n\tunsigned int offset = 0;\n\tbool udpfrag, encap;\n\tstruct iphdr *iph;\n\tint proto;\n\tint nhoff;\n\tint ihl;\n\tint id;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t     ~(SKB_GSO_TCPV4 |\n\t\t       SKB_GSO_UDP |\n\t\t       SKB_GSO_DODGY |\n\t\t       SKB_GSO_TCP_ECN |\n\t\t       SKB_GSO_GRE |\n\t\t       SKB_GSO_GRE_CSUM |\n\t\t       SKB_GSO_IPIP |\n\t\t       SKB_GSO_SIT |\n\t\t       SKB_GSO_TCPV6 |\n\t\t       SKB_GSO_UDP_TUNNEL |\n\t\t       SKB_GSO_UDP_TUNNEL_CSUM |\n\t\t       SKB_GSO_TUNNEL_REMCSUM |\n\t\t       0)))\n\t\tgoto out;\n\n\tskb_reset_network_header(skb);\n\tnhoff = skb_network_header(skb) - skb_mac_header(skb);\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*iph))))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\tif (ihl < sizeof(*iph))\n\t\tgoto out;\n\n\tid = ntohs(iph->id);\n\tproto = iph->protocol;\n\n\t/* Warning: after this point, iph might be no longer valid */\n\tif (unlikely(!pskb_may_pull(skb, ihl)))\n\t\tgoto out;\n\t__skb_pull(skb, ihl);\n\n\tencap = SKB_GSO_CB(skb)->encap_level > 0;\n\tif (encap)\n\t\tfeatures &= skb->dev->hw_enc_features;\n\tSKB_GSO_CB(skb)->encap_level += ihl;\n\n\tskb_reset_transport_header(skb);\n\n\tsegs = ERR_PTR(-EPROTONOSUPPORT);\n\n\tif (skb->encapsulation &&\n\t    skb_shinfo(skb)->gso_type & (SKB_GSO_SIT|SKB_GSO_IPIP))\n\t\tudpfrag = proto == IPPROTO_UDP && encap;\n\telse\n\t\tudpfrag = proto == IPPROTO_UDP && !skb->encapsulation;\n\n\tops = rcu_dereference(inet_offloads[proto]);\n\tif (likely(ops && ops->callbacks.gso_segment))\n\t\tsegs = ops->callbacks.gso_segment(skb, features);\n\n\tif (IS_ERR_OR_NULL(segs))\n\t\tgoto out;\n\n\tskb = segs;\n\tdo {\n\t\tiph = (struct iphdr *)(skb_mac_header(skb) + nhoff);\n\t\tif (udpfrag) {\n\t\t\tiph->id = htons(id);\n\t\t\tiph->frag_off = htons(offset >> 3);\n\t\t\tif (skb->next)\n\t\t\t\tiph->frag_off |= htons(IP_MF);\n\t\t\toffset += skb->len - nhoff - ihl;\n\t\t} else {\n\t\t\tiph->id = htons(id++);\n\t\t}\n\t\tiph->tot_len = htons(skb->len - nhoff);\n\t\tip_send_check(iph);\n\t\tif (encap)\n\t\t\tskb_reset_inner_headers(skb);\n\t\tskb->network_header = (u8 *)iph - skb->head;\n\t} while ((skb = skb->next));\n\nout:\n\treturn segs;\n}\n\nstatic struct sk_buff **inet_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct net_offload *ops;\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct iphdr *iph;\n\tunsigned int hlen;\n\tunsigned int off;\n\tunsigned int id;\n\tint flush = 1;\n\tint proto;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*iph);\n\tiph = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tiph = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!iph))\n\t\t\tgoto out;\n\t}\n\n\tproto = iph->protocol;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_offloads[proto]);\n\tif (!ops || !ops->callbacks.gro_receive)\n\t\tgoto out_unlock;\n\n\tif (*(u8 *)iph != 0x45)\n\t\tgoto out_unlock;\n\n\tif (unlikely(ip_fast_csum((u8 *)iph, 5)))\n\t\tgoto out_unlock;\n\n\tid = ntohl(*(__be32 *)&iph->id);\n\tflush = (u16)((ntohl(*(__be32 *)iph) ^ skb_gro_len(skb)) | (id & ~IP_DF));\n\tid >>= 16;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tstruct iphdr *iph2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tiph2 = (struct iphdr *)(p->data + off);\n\t\t/* The above works because, with the exception of the top\n\t\t * (inner most) layer, we only aggregate pkts with the same\n\t\t * hdr length so all the hdrs we'll need to verify will start\n\t\t * at the same offset.\n\t\t */\n\t\tif ((iph->protocol ^ iph2->protocol) |\n\t\t    ((__force u32)iph->saddr ^ (__force u32)iph2->saddr) |\n\t\t    ((__force u32)iph->daddr ^ (__force u32)iph2->daddr)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* All fields must match except length and checksum. */\n\t\tNAPI_GRO_CB(p)->flush |=\n\t\t\t(iph->ttl ^ iph2->ttl) |\n\t\t\t(iph->tos ^ iph2->tos) |\n\t\t\t((iph->frag_off ^ iph2->frag_off) & htons(IP_DF));\n\n\t\t/* Save the IP ID check to be included later when we get to\n\t\t * the transport layer so only the inner most IP ID is checked.\n\t\t * This is because some GSO/TSO implementations do not\n\t\t * correctly increment the IP ID for the outer hdrs.\n\t\t */\n\t\tNAPI_GRO_CB(p)->flush_id =\n\t\t\t    ((u16)(ntohs(iph2->id) + NAPI_GRO_CB(p)->count) ^ id);\n\t\tNAPI_GRO_CB(p)->flush |= flush;\n\t}\n\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\tskb_set_network_header(skb, off);\n\t/* The above will be needed by the transport layer if there is one\n\t * immediately following this IP hdr.\n\t */\n\n\t/* Note : No need to call skb_gro_postpull_rcsum() here,\n\t * as we already checked checksum over ipv4 header was 0\n\t */\n\tskb_gro_pull(skb, sizeof(*iph));\n\tskb_set_transport_header(skb, skb_gro_offset(skb));\n\n\tpp = ops->callbacks.gro_receive(head, skb);\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\n#define SECONDS_PER_DAY\t86400\n\n/* inet_current_timestamp - Return IP network timestamp\n *\n * Return milliseconds since midnight in network byte order.\n */\n__be32 inet_current_timestamp(void)\n{\n\tu32 secs;\n\tu32 msecs;\n\tstruct timespec64 ts;\n\n\tktime_get_real_ts64(&ts);\n\n\t/* Get secs since midnight. */\n\t(void)div_u64_rem(ts.tv_sec, SECONDS_PER_DAY, &secs);\n\t/* Convert to msecs. */\n\tmsecs = secs * MSEC_PER_SEC;\n\t/* Convert nsec to msec. */\n\tmsecs += (u32)ts.tv_nsec / NSEC_PER_MSEC;\n\n\t/* Convert to network byte order. */\n\treturn htons(msecs);\n}\nEXPORT_SYMBOL(inet_current_timestamp);\n\nint inet_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len)\n{\n\tif (sk->sk_family == AF_INET)\n\t\treturn ip_recv_error(sk, msg, len, addr_len);\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\treturn pingv6_ops.ipv6_recv_error(sk, msg, len, addr_len);\n#endif\n\treturn -EINVAL;\n}\n\nstatic int inet_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\t__be16 newlen = htons(skb->len - nhoff);\n\tstruct iphdr *iph = (struct iphdr *)(skb->data + nhoff);\n\tconst struct net_offload *ops;\n\tint proto = iph->protocol;\n\tint err = -ENOSYS;\n\n\tif (skb->encapsulation)\n\t\tskb_set_inner_network_header(skb, nhoff);\n\n\tcsum_replace2(&iph->check, iph->tot_len, newlen);\n\tiph->tot_len = newlen;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_offloads[proto]);\n\tif (WARN_ON(!ops || !ops->callbacks.gro_complete))\n\t\tgoto out_unlock;\n\n\t/* Only need to add sizeof(*iph) to get to the next hdr below\n\t * because any hdr with option will have been flushed in\n\t * inet_gro_receive().\n\t */\n\terr = ops->callbacks.gro_complete(skb, nhoff + sizeof(*iph));\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic int ipip_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tskb->encapsulation = 1;\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_IPIP;\n\treturn inet_gro_complete(skb, nhoff);\n}\n\nint inet_ctl_sock_create(struct sock **sk, unsigned short family,\n\t\t\t unsigned short type, unsigned char protocol,\n\t\t\t struct net *net)\n{\n\tstruct socket *sock;\n\tint rc = sock_create_kern(net, family, type, protocol, &sock);\n\n\tif (rc == 0) {\n\t\t*sk = sock->sk;\n\t\t(*sk)->sk_allocation = GFP_ATOMIC;\n\t\t/*\n\t\t * Unhash it so that IP input processing does not even see it,\n\t\t * we do not wish this socket to see incoming packets.\n\t\t */\n\t\t(*sk)->sk_prot->unhash(*sk);\n\t}\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(inet_ctl_sock_create);\n\nu64 snmp_get_cpu_field(void __percpu *mib, int cpu, int offt)\n{\n\treturn  *(((unsigned long *)per_cpu_ptr(mib, cpu)) + offt);\n}\nEXPORT_SYMBOL_GPL(snmp_get_cpu_field);\n\nunsigned long snmp_fold_field(void __percpu *mib, int offt)\n{\n\tunsigned long res = 0;\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tres += snmp_get_cpu_field(mib, i, offt);\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field);\n\n#if BITS_PER_LONG==32\n\nu64 snmp_get_cpu_field64(void __percpu *mib, int cpu, int offt,\n\t\t\t size_t syncp_offset)\n{\n\tvoid *bhptr;\n\tstruct u64_stats_sync *syncp;\n\tu64 v;\n\tunsigned int start;\n\n\tbhptr = per_cpu_ptr(mib, cpu);\n\tsyncp = (struct u64_stats_sync *)(bhptr + syncp_offset);\n\tdo {\n\t\tstart = u64_stats_fetch_begin_irq(syncp);\n\t\tv = *(((u64 *)bhptr) + offt);\n\t} while (u64_stats_fetch_retry_irq(syncp, start));\n\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(snmp_get_cpu_field64);\n\nu64 snmp_fold_field64(void __percpu *mib, int offt, size_t syncp_offset)\n{\n\tu64 res = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tres += snmp_get_cpu_field64(mib, cpu, offt, syncp_offset);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field64);\n#endif\n\n#ifdef CONFIG_IP_MULTICAST\nstatic const struct net_protocol igmp_protocol = {\n\t.handler =\tigmp_rcv,\n\t.netns_ok =\t1,\n};\n#endif\n\nstatic const struct net_protocol tcp_protocol = {\n\t.early_demux\t=\ttcp_v4_early_demux,\n\t.handler\t=\ttcp_v4_rcv,\n\t.err_handler\t=\ttcp_v4_err,\n\t.no_policy\t=\t1,\n\t.netns_ok\t=\t1,\n\t.icmp_strict_tag_validation = 1,\n};\n\nstatic const struct net_protocol udp_protocol = {\n\t.early_demux =\tudp_v4_early_demux,\n\t.handler =\tudp_rcv,\n\t.err_handler =\tudp_err,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic const struct net_protocol icmp_protocol = {\n\t.handler =\ticmp_rcv,\n\t.err_handler =\ticmp_err,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic __net_init int ipv4_mib_init_net(struct net *net)\n{\n\tint i;\n\n\tnet->mib.tcp_statistics = alloc_percpu(struct tcp_mib);\n\tif (!net->mib.tcp_statistics)\n\t\tgoto err_tcp_mib;\n\tnet->mib.ip_statistics = alloc_percpu(struct ipstats_mib);\n\tif (!net->mib.ip_statistics)\n\t\tgoto err_ip_mib;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct ipstats_mib *af_inet_stats;\n\t\taf_inet_stats = per_cpu_ptr(net->mib.ip_statistics, i);\n\t\tu64_stats_init(&af_inet_stats->syncp);\n\t}\n\n\tnet->mib.net_statistics = alloc_percpu(struct linux_mib);\n\tif (!net->mib.net_statistics)\n\t\tgoto err_net_mib;\n\tnet->mib.udp_statistics = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udp_statistics)\n\t\tgoto err_udp_mib;\n\tnet->mib.udplite_statistics = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udplite_statistics)\n\t\tgoto err_udplite_mib;\n\tnet->mib.icmp_statistics = alloc_percpu(struct icmp_mib);\n\tif (!net->mib.icmp_statistics)\n\t\tgoto err_icmp_mib;\n\tnet->mib.icmpmsg_statistics = kzalloc(sizeof(struct icmpmsg_mib),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!net->mib.icmpmsg_statistics)\n\t\tgoto err_icmpmsg_mib;\n\n\ttcp_mib_init(net);\n\treturn 0;\n\nerr_icmpmsg_mib:\n\tfree_percpu(net->mib.icmp_statistics);\nerr_icmp_mib:\n\tfree_percpu(net->mib.udplite_statistics);\nerr_udplite_mib:\n\tfree_percpu(net->mib.udp_statistics);\nerr_udp_mib:\n\tfree_percpu(net->mib.net_statistics);\nerr_net_mib:\n\tfree_percpu(net->mib.ip_statistics);\nerr_ip_mib:\n\tfree_percpu(net->mib.tcp_statistics);\nerr_tcp_mib:\n\treturn -ENOMEM;\n}\n\nstatic __net_exit void ipv4_mib_exit_net(struct net *net)\n{\n\tkfree(net->mib.icmpmsg_statistics);\n\tfree_percpu(net->mib.icmp_statistics);\n\tfree_percpu(net->mib.udplite_statistics);\n\tfree_percpu(net->mib.udp_statistics);\n\tfree_percpu(net->mib.net_statistics);\n\tfree_percpu(net->mib.ip_statistics);\n\tfree_percpu(net->mib.tcp_statistics);\n}\n\nstatic __net_initdata struct pernet_operations ipv4_mib_ops = {\n\t.init = ipv4_mib_init_net,\n\t.exit = ipv4_mib_exit_net,\n};\n\nstatic int __init init_ipv4_mibs(void)\n{\n\treturn register_pernet_subsys(&ipv4_mib_ops);\n}\n\nstatic __net_init int inet_init_net(struct net *net)\n{\n\t/*\n\t * Set defaults for local port range\n\t */\n\tseqlock_init(&net->ipv4.ip_local_ports.lock);\n\tnet->ipv4.ip_local_ports.range[0] =  32768;\n\tnet->ipv4.ip_local_ports.range[1] =  60999;\n\n\tseqlock_init(&net->ipv4.ping_group_range.lock);\n\t/*\n\t * Sane defaults - nobody may create ping sockets.\n\t * Boot scripts should set this to distro-specific group.\n\t */\n\tnet->ipv4.ping_group_range.range[0] = make_kgid(&init_user_ns, 1);\n\tnet->ipv4.ping_group_range.range[1] = make_kgid(&init_user_ns, 0);\n\treturn 0;\n}\n\nstatic __net_exit void inet_exit_net(struct net *net)\n{\n}\n\nstatic __net_initdata struct pernet_operations af_inet_ops = {\n\t.init = inet_init_net,\n\t.exit = inet_exit_net,\n};\n\nstatic int __init init_inet_pernet_ops(void)\n{\n\treturn register_pernet_subsys(&af_inet_ops);\n}\n\nstatic int ipv4_proc_init(void);\n\n/*\n *\tIP protocol layer initialiser\n */\n\nstatic struct packet_offload ip_packet_offload __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.callbacks = {\n\t\t.gso_segment = inet_gso_segment,\n\t\t.gro_receive = inet_gro_receive,\n\t\t.gro_complete = inet_gro_complete,\n\t},\n};\n\nstatic const struct net_offload ipip_offload = {\n\t.callbacks = {\n\t\t.gso_segment\t= inet_gso_segment,\n\t\t.gro_receive\t= inet_gro_receive,\n\t\t.gro_complete\t= ipip_gro_complete,\n\t},\n};\n\nstatic int __init ipv4_offload_init(void)\n{\n\t/*\n\t * Add offloads\n\t */\n\tif (udpv4_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add UDP protocol offload\\n\", __func__);\n\tif (tcpv4_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add TCP protocol offload\\n\", __func__);\n\n\tdev_add_offload(&ip_packet_offload);\n\tinet_add_offload(&ipip_offload, IPPROTO_IPIP);\n\treturn 0;\n}\n\nfs_initcall(ipv4_offload_init);\n\nstatic struct packet_type ip_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.func = ip_rcv,\n};\n\nstatic int __init inet_init(void)\n{\n\tstruct inet_protosw *q;\n\tstruct list_head *r;\n\tint rc = -EINVAL;\n\n\tsock_skb_cb_check_size(sizeof(struct inet_skb_parm));\n\n\trc = proto_register(&tcp_prot, 1);\n\tif (rc)\n\t\tgoto out;\n\n\trc = proto_register(&udp_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_tcp_proto;\n\n\trc = proto_register(&raw_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_udp_proto;\n\n\trc = proto_register(&ping_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_raw_proto;\n\n\t/*\n\t *\tTell SOCKET that we are alive...\n\t */\n\n\t(void)sock_register(&inet_family_ops);\n\n#ifdef CONFIG_SYSCTL\n\tip_static_sysctl_init();\n#endif\n\n\t/*\n\t *\tAdd all the base protocols.\n\t */\n\n\tif (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)\n\t\tpr_crit(\"%s: Cannot add ICMP protocol\\n\", __func__);\n\tif (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)\n\t\tpr_crit(\"%s: Cannot add UDP protocol\\n\", __func__);\n\tif (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) < 0)\n\t\tpr_crit(\"%s: Cannot add TCP protocol\\n\", __func__);\n#ifdef CONFIG_IP_MULTICAST\n\tif (inet_add_protocol(&igmp_protocol, IPPROTO_IGMP) < 0)\n\t\tpr_crit(\"%s: Cannot add IGMP protocol\\n\", __func__);\n#endif\n\n\t/* Register the socket-side information for inet_create. */\n\tfor (r = &inetsw[0]; r < &inetsw[SOCK_MAX]; ++r)\n\t\tINIT_LIST_HEAD(r);\n\n\tfor (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)\n\t\tinet_register_protosw(q);\n\n\t/*\n\t *\tSet the ARP module up\n\t */\n\n\tarp_init();\n\n\t/*\n\t *\tSet the IP module up\n\t */\n\n\tip_init();\n\n\ttcp_v4_init();\n\n\t/* Setup TCP slab cache for open requests. */\n\ttcp_init();\n\n\t/* Setup UDP memory threshold */\n\tudp_init();\n\n\t/* Add UDP-Lite (RFC 3828) */\n\tudplite4_register();\n\n\tping_init();\n\n\t/*\n\t *\tSet the ICMP layer up\n\t */\n\n\tif (icmp_init() < 0)\n\t\tpanic(\"Failed to create the ICMP control socket.\\n\");\n\n\t/*\n\t *\tInitialise the multicast router\n\t */\n#if defined(CONFIG_IP_MROUTE)\n\tif (ip_mr_init())\n\t\tpr_crit(\"%s: Cannot init ipv4 mroute\\n\", __func__);\n#endif\n\n\tif (init_inet_pernet_ops())\n\t\tpr_crit(\"%s: Cannot init ipv4 inet pernet ops\\n\", __func__);\n\t/*\n\t *\tInitialise per-cpu ipv4 mibs\n\t */\n\n\tif (init_ipv4_mibs())\n\t\tpr_crit(\"%s: Cannot init ipv4 mibs\\n\", __func__);\n\n\tipv4_proc_init();\n\n\tipfrag_init();\n\n\tdev_add_pack(&ip_packet_type);\n\n\tip_tunnel_core_init();\n\n\trc = 0;\nout:\n\treturn rc;\nout_unregister_raw_proto:\n\tproto_unregister(&raw_prot);\nout_unregister_udp_proto:\n\tproto_unregister(&udp_prot);\nout_unregister_tcp_proto:\n\tproto_unregister(&tcp_prot);\n\tgoto out;\n}\n\nfs_initcall(inet_init);\n\n/* ------------------------------------------------------------------------ */\n\n#ifdef CONFIG_PROC_FS\nstatic int __init ipv4_proc_init(void)\n{\n\tint rc = 0;\n\n\tif (raw_proc_init())\n\t\tgoto out_raw;\n\tif (tcp4_proc_init())\n\t\tgoto out_tcp;\n\tif (udp4_proc_init())\n\t\tgoto out_udp;\n\tif (ping_proc_init())\n\t\tgoto out_ping;\n\tif (ip_misc_proc_init())\n\t\tgoto out_misc;\nout:\n\treturn rc;\nout_misc:\n\tping_proc_exit();\nout_ping:\n\tudp4_proc_exit();\nout_udp:\n\ttcp4_proc_exit();\nout_tcp:\n\traw_proc_exit();\nout_raw:\n\trc = -ENOMEM;\n\tgoto out;\n}\n\n#else /* CONFIG_PROC_FS */\nstatic int __init ipv4_proc_init(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_PROC_FS */\n\nMODULE_ALIAS_NETPROTO(PF_INET);\n\n", "/*\n *\tIPV4 GSO/GRO offload support\n *\tLinux INET implementation\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *\tGRE GSO support\n */\n\n#include <linux/skbuff.h>\n#include <linux/init.h>\n#include <net/protocol.h>\n#include <net/gre.h>\n\nstatic struct sk_buff *gre_gso_segment(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\tint tnl_hlen = skb_inner_mac_header(skb) - skb_transport_header(skb);\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tu16 mac_offset = skb->mac_header;\n\t__be16 protocol = skb->protocol;\n\tu16 mac_len = skb->mac_len;\n\tint gre_offset, outer_hlen;\n\tbool need_csum, ufo;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t\t\t~(SKB_GSO_TCPV4 |\n\t\t\t\t  SKB_GSO_TCPV6 |\n\t\t\t\t  SKB_GSO_UDP |\n\t\t\t\t  SKB_GSO_DODGY |\n\t\t\t\t  SKB_GSO_TCP_ECN |\n\t\t\t\t  SKB_GSO_GRE |\n\t\t\t\t  SKB_GSO_GRE_CSUM |\n\t\t\t\t  SKB_GSO_IPIP |\n\t\t\t\t  SKB_GSO_SIT)))\n\t\tgoto out;\n\n\tif (!skb->encapsulation)\n\t\tgoto out;\n\n\tif (unlikely(tnl_hlen < sizeof(struct gre_base_hdr)))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, tnl_hlen)))\n\t\tgoto out;\n\n\t/* setup inner skb. */\n\tskb->encapsulation = 0;\n\t__skb_pull(skb, tnl_hlen);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, skb_inner_network_offset(skb));\n\tskb->mac_len = skb_inner_network_offset(skb);\n\tskb->protocol = skb->inner_protocol;\n\n\tneed_csum = !!(skb_shinfo(skb)->gso_type & SKB_GSO_GRE_CSUM);\n\tskb->encap_hdr_csum = need_csum;\n\n\tufo = !!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP);\n\n\tfeatures &= skb->dev->hw_enc_features;\n\n\t/* The only checksum offload we care about from here on out is the\n\t * outer one so strip the existing checksum feature flags based\n\t * on the fact that we will be computing our checksum in software.\n\t */\n\tif (ufo) {\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\t\tif (!need_csum)\n\t\t\tfeatures |= NETIF_F_HW_CSUM;\n\t}\n\n\t/* segment inner packet. */\n\tsegs = skb_mac_gso_segment(skb, features);\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tskb_gso_error_unwind(skb, protocol, tnl_hlen, mac_offset,\n\t\t\t\t     mac_len);\n\t\tgoto out;\n\t}\n\n\touter_hlen = skb_tnl_header_len(skb);\n\tgre_offset = outer_hlen - tnl_hlen;\n\tskb = segs;\n\tdo {\n\t\tstruct gre_base_hdr *greh;\n\t\t__be32 *pcsum;\n\n\t\t/* Set up inner headers if we are offloading inner checksum */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_reset_inner_headers(skb);\n\t\t\tskb->encapsulation = 1;\n\t\t}\n\n\t\tskb->mac_len = mac_len;\n\t\tskb->protocol = protocol;\n\n\t\t__skb_push(skb, outer_hlen);\n\t\tskb_reset_mac_header(skb);\n\t\tskb_set_network_header(skb, mac_len);\n\t\tskb_set_transport_header(skb, gre_offset);\n\n\t\tif (!need_csum)\n\t\t\tcontinue;\n\n\t\tgreh = (struct gre_base_hdr *)skb_transport_header(skb);\n\t\tpcsum = (__be32 *)(greh + 1);\n\n\t\t*pcsum = 0;\n\t\t*(__sum16 *)pcsum = gso_make_checksum(skb, 0);\n\t} while ((skb = skb->next));\nout:\n\treturn segs;\n}\n\nstatic struct sk_buff **gre_gro_receive(struct sk_buff **head,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct gre_base_hdr *greh;\n\tunsigned int hlen, grehlen;\n\tunsigned int off;\n\tint flush = 1;\n\tstruct packet_offload *ptype;\n\t__be16 type;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*greh);\n\tgreh = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out;\n\t}\n\n\t/* Only support version 0 and K (key), C (csum) flags. Note that\n\t * although the support for the S (seq#) flag can be added easily\n\t * for GRO, this is problematic for GSO hence can not be enabled\n\t * here because a GRO pkt may end up in the forwarding path, thus\n\t * requiring GSO support to break it up correctly.\n\t */\n\tif ((greh->flags & ~(GRE_KEY|GRE_CSUM)) != 0)\n\t\tgoto out;\n\n\ttype = greh->protocol;\n\n\trcu_read_lock();\n\tptype = gro_find_receive_by_type(type);\n\tif (!ptype)\n\t\tgoto out_unlock;\n\n\tgrehlen = GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_KEY)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_CSUM)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\thlen = off + grehlen;\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Don't bother verifying checksum if we're going to flush anyway. */\n\tif ((greh->flags & GRE_CSUM) && !NAPI_GRO_CB(skb)->flush) {\n\t\tif (skb_gro_checksum_simple_validate(skb))\n\t\t\tgoto out_unlock;\n\n\t\tskb_gro_checksum_try_convert(skb, IPPROTO_GRE, 0,\n\t\t\t\t\t     null_compute_pseudo);\n\t}\n\n\tfor (p = *head; p; p = p->next) {\n\t\tconst struct gre_base_hdr *greh2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\t/* The following checks are needed to ensure only pkts\n\t\t * from the same tunnel are considered for aggregation.\n\t\t * The criteria for \"the same tunnel\" includes:\n\t\t * 1) same version (we only support version 0 here)\n\t\t * 2) same protocol (we only support ETH_P_IP for now)\n\t\t * 3) same set of flags\n\t\t * 4) same key if the key field is present.\n\t\t */\n\t\tgreh2 = (struct gre_base_hdr *)(p->data + off);\n\n\t\tif (greh2->flags != greh->flags ||\n\t\t    greh2->protocol != greh->protocol) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (greh->flags & GRE_KEY) {\n\t\t\t/* compare keys */\n\t\t\tif (*(__be32 *)(greh2+1) != *(__be32 *)(greh+1)) {\n\t\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, grehlen);\n\n\t/* Adjusted NAPI_GRO_CB(skb)->csum after skb_gro_pull()*/\n\tskb_gro_postpull_rcsum(skb, greh, grehlen);\n\n\tpp = ptype->callbacks.gro_receive(head, skb);\n\tflush = 0;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic int gre_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tstruct gre_base_hdr *greh = (struct gre_base_hdr *)(skb->data + nhoff);\n\tstruct packet_offload *ptype;\n\tunsigned int grehlen = sizeof(*greh);\n\tint err = -ENOENT;\n\t__be16 type;\n\n\tskb->encapsulation = 1;\n\tskb_shinfo(skb)->gso_type = SKB_GSO_GRE;\n\n\ttype = greh->protocol;\n\tif (greh->flags & GRE_KEY)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_CSUM)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\trcu_read_lock();\n\tptype = gro_find_complete_by_type(type);\n\tif (ptype)\n\t\terr = ptype->callbacks.gro_complete(skb, nhoff + grehlen);\n\n\trcu_read_unlock();\n\n\tskb_set_inner_mac_header(skb, nhoff + grehlen);\n\n\treturn err;\n}\n\nstatic const struct net_offload gre_offload = {\n\t.callbacks = {\n\t\t.gso_segment = gre_gso_segment,\n\t\t.gro_receive = gre_gro_receive,\n\t\t.gro_complete = gre_gro_complete,\n\t},\n};\n\nstatic int __init gre_offload_init(void)\n{\n\treturn inet_add_offload(&gre_offload, IPPROTO_GRE);\n}\ndevice_initcall(gre_offload_init);\n", "/*\n *\tIPV4 GSO/GRO offload support\n *\tLinux INET implementation\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *\tUDPv4 GSO support\n */\n\n#include <linux/skbuff.h>\n#include <net/udp.h>\n#include <net/protocol.h>\n\nstatic DEFINE_SPINLOCK(udp_offload_lock);\nstatic struct udp_offload_priv __rcu *udp_offload_base __read_mostly;\n\n#define udp_deref_protected(X) rcu_dereference_protected(X, lockdep_is_held(&udp_offload_lock))\n\nstruct udp_offload_priv {\n\tstruct udp_offload\t*offload;\n\tpossible_net_t\tnet;\n\tstruct rcu_head\t\trcu;\n\tstruct udp_offload_priv __rcu *next;\n};\n\nstatic struct sk_buff *__skb_udp_tunnel_segment(struct sk_buff *skb,\n\tnetdev_features_t features,\n\tstruct sk_buff *(*gso_inner_segment)(struct sk_buff *skb,\n\t\t\t\t\t     netdev_features_t features),\n\t__be16 new_protocol, bool is_ipv6)\n{\n\tint tnl_hlen = skb_inner_mac_header(skb) - skb_transport_header(skb);\n\tbool remcsum, need_csum, offload_csum, ufo;\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct udphdr *uh = udp_hdr(skb);\n\tu16 mac_offset = skb->mac_header;\n\t__be16 protocol = skb->protocol;\n\tu16 mac_len = skb->mac_len;\n\tint udp_offset, outer_hlen;\n\t__wsum partial;\n\n\tif (unlikely(!pskb_may_pull(skb, tnl_hlen)))\n\t\tgoto out;\n\n\t/* Adjust partial header checksum to negate old length.\n\t * We cannot rely on the value contained in uh->len as it is\n\t * possible that the actual value exceeds the boundaries of the\n\t * 16 bit length field due to the header being added outside of an\n\t * IP or IPv6 frame that was already limited to 64K - 1.\n\t */\n\tpartial = csum_sub(csum_unfold(uh->check),\n\t\t\t   (__force __wsum)htonl(skb->len));\n\n\t/* setup inner skb. */\n\tskb->encapsulation = 0;\n\t__skb_pull(skb, tnl_hlen);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, skb_inner_network_offset(skb));\n\tskb->mac_len = skb_inner_network_offset(skb);\n\tskb->protocol = new_protocol;\n\n\tneed_csum = !!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM);\n\tskb->encap_hdr_csum = need_csum;\n\n\tremcsum = !!(skb_shinfo(skb)->gso_type & SKB_GSO_TUNNEL_REMCSUM);\n\tskb->remcsum_offload = remcsum;\n\n\tufo = !!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP);\n\n\t/* Try to offload checksum if possible */\n\toffload_csum = !!(need_csum &&\n\t\t\t  (skb->dev->features &\n\t\t\t   (is_ipv6 ? (NETIF_F_HW_CSUM | NETIF_F_IPV6_CSUM) :\n\t\t\t\t      (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM))));\n\n\tfeatures &= skb->dev->hw_enc_features;\n\n\t/* The only checksum offload we care about from here on out is the\n\t * outer one so strip the existing checksum feature flags and\n\t * instead set the flag based on our outer checksum offload value.\n\t */\n\tif (remcsum || ufo) {\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\t\tif (!need_csum || offload_csum)\n\t\t\tfeatures |= NETIF_F_HW_CSUM;\n\t}\n\n\t/* segment inner packet. */\n\tsegs = gso_inner_segment(skb, features);\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tskb_gso_error_unwind(skb, protocol, tnl_hlen, mac_offset,\n\t\t\t\t     mac_len);\n\t\tgoto out;\n\t}\n\n\touter_hlen = skb_tnl_header_len(skb);\n\tudp_offset = outer_hlen - tnl_hlen;\n\tskb = segs;\n\tdo {\n\t\t__be16 len;\n\n\t\tif (remcsum)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t/* Set up inner headers if we are offloading inner checksum */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_reset_inner_headers(skb);\n\t\t\tskb->encapsulation = 1;\n\t\t}\n\n\t\tskb->mac_len = mac_len;\n\t\tskb->protocol = protocol;\n\n\t\t__skb_push(skb, outer_hlen);\n\t\tskb_reset_mac_header(skb);\n\t\tskb_set_network_header(skb, mac_len);\n\t\tskb_set_transport_header(skb, udp_offset);\n\t\tlen = htons(skb->len - udp_offset);\n\t\tuh = udp_hdr(skb);\n\t\tuh->len = len;\n\n\t\tif (!need_csum)\n\t\t\tcontinue;\n\n\t\tuh->check = ~csum_fold(csum_add(partial, (__force __wsum)len));\n\n\t\tif (skb->encapsulation || !offload_csum) {\n\t\t\tuh->check = gso_make_checksum(skb, ~uh->check);\n\t\t\tif (uh->check == 0)\n\t\t\t\tuh->check = CSUM_MANGLED_0;\n\t\t} else {\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\t}\n\t} while ((skb = skb->next));\nout:\n\treturn segs;\n}\n\nstruct sk_buff *skb_udp_tunnel_segment(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features,\n\t\t\t\t       bool is_ipv6)\n{\n\t__be16 protocol = skb->protocol;\n\tconst struct net_offload **offloads;\n\tconst struct net_offload *ops;\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct sk_buff *(*gso_inner_segment)(struct sk_buff *skb,\n\t\t\t\t\t     netdev_features_t features);\n\n\trcu_read_lock();\n\n\tswitch (skb->inner_protocol_type) {\n\tcase ENCAP_TYPE_ETHER:\n\t\tprotocol = skb->inner_protocol;\n\t\tgso_inner_segment = skb_mac_gso_segment;\n\t\tbreak;\n\tcase ENCAP_TYPE_IPPROTO:\n\t\toffloads = is_ipv6 ? inet6_offloads : inet_offloads;\n\t\tops = rcu_dereference(offloads[skb->inner_ipproto]);\n\t\tif (!ops || !ops->callbacks.gso_segment)\n\t\t\tgoto out_unlock;\n\t\tgso_inner_segment = ops->callbacks.gso_segment;\n\t\tbreak;\n\tdefault:\n\t\tgoto out_unlock;\n\t}\n\n\tsegs = __skb_udp_tunnel_segment(skb, features, gso_inner_segment,\n\t\t\t\t\tprotocol, is_ipv6);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn segs;\n}\n\nstatic struct sk_buff *udp4_ufo_fragment(struct sk_buff *skb,\n\t\t\t\t\t netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\t__wsum csum;\n\tstruct udphdr *uh;\n\tstruct iphdr *iph;\n\n\tif (skb->encapsulation &&\n\t    (skb_shinfo(skb)->gso_type &\n\t     (SKB_GSO_UDP_TUNNEL|SKB_GSO_UDP_TUNNEL_CSUM))) {\n\t\tsegs = skb_udp_tunnel_segment(skb, features, false);\n\t\tgoto out;\n\t}\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto out;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP | SKB_GSO_DODGY |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL_CSUM |\n\t\t\t\t      SKB_GSO_TUNNEL_REMCSUM |\n\t\t\t\t      SKB_GSO_IPIP |\n\t\t\t\t      SKB_GSO_GRE | SKB_GSO_GRE_CSUM) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\t/* Do software UFO. Complete and fill in the UDP checksum as\n\t * HW cannot do checksum of UDP packets sent as multiple\n\t * IP fragments.\n\t */\n\n\tuh = udp_hdr(skb);\n\tiph = ip_hdr(skb);\n\n\tuh->check = 0;\n\tcsum = skb_checksum(skb, 0, skb->len, 0);\n\tuh->check = udp_v4_check(skb->len, iph->saddr, iph->daddr, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t/* If there is no outer header we can fake a checksum offload\n\t * due to the fact that we have already done the checksum in\n\t * software prior to segmenting the frame.\n\t */\n\tif (!skb->encap_hdr_csum)\n\t\tfeatures |= NETIF_F_HW_CSUM;\n\n\t/* Fragment the skb. IP headers of the fragments are updated in\n\t * inet_gso_segment()\n\t */\n\tsegs = skb_segment(skb, features);\nout:\n\treturn segs;\n}\n\nint udp_add_offload(struct net *net, struct udp_offload *uo)\n{\n\tstruct udp_offload_priv *new_offload = kzalloc(sizeof(*new_offload), GFP_ATOMIC);\n\n\tif (!new_offload)\n\t\treturn -ENOMEM;\n\n\twrite_pnet(&new_offload->net, net);\n\tnew_offload->offload = uo;\n\n\tspin_lock(&udp_offload_lock);\n\tnew_offload->next = udp_offload_base;\n\trcu_assign_pointer(udp_offload_base, new_offload);\n\tspin_unlock(&udp_offload_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_add_offload);\n\nstatic void udp_offload_free_routine(struct rcu_head *head)\n{\n\tstruct udp_offload_priv *ou_priv = container_of(head, struct udp_offload_priv, rcu);\n\tkfree(ou_priv);\n}\n\nvoid udp_del_offload(struct udp_offload *uo)\n{\n\tstruct udp_offload_priv __rcu **head = &udp_offload_base;\n\tstruct udp_offload_priv *uo_priv;\n\n\tspin_lock(&udp_offload_lock);\n\n\tuo_priv = udp_deref_protected(*head);\n\tfor (; uo_priv != NULL;\n\t     uo_priv = udp_deref_protected(*head)) {\n\t\tif (uo_priv->offload == uo) {\n\t\t\trcu_assign_pointer(*head,\n\t\t\t\t\t   udp_deref_protected(uo_priv->next));\n\t\t\tgoto unlock;\n\t\t}\n\t\thead = &uo_priv->next;\n\t}\n\tpr_warn(\"udp_del_offload: didn't find offload for port %d\\n\", ntohs(uo->port));\nunlock:\n\tspin_unlock(&udp_offload_lock);\n\tif (uo_priv)\n\t\tcall_rcu(&uo_priv->rcu, udp_offload_free_routine);\n}\nEXPORT_SYMBOL(udp_del_offload);\n\nstruct sk_buff **udp_gro_receive(struct sk_buff **head, struct sk_buff *skb,\n\t\t\t\t struct udphdr *uh)\n{\n\tstruct udp_offload_priv *uo_priv;\n\tstruct sk_buff *p, **pp = NULL;\n\tstruct udphdr *uh2;\n\tunsigned int off = skb_gro_offset(skb);\n\tint flush = 1;\n\n\tif (NAPI_GRO_CB(skb)->udp_mark ||\n\t    (skb->ip_summed != CHECKSUM_PARTIAL &&\n\t     NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t     !NAPI_GRO_CB(skb)->csum_valid))\n\t\tgoto out;\n\n\t/* mark that this skb passed once through the udp gro layer */\n\tNAPI_GRO_CB(skb)->udp_mark = 1;\n\n\trcu_read_lock();\n\tuo_priv = rcu_dereference(udp_offload_base);\n\tfor (; uo_priv != NULL; uo_priv = rcu_dereference(uo_priv->next)) {\n\t\tif (net_eq(read_pnet(&uo_priv->net), dev_net(skb->dev)) &&\n\t\t    uo_priv->offload->port == uh->dest &&\n\t\t    uo_priv->offload->callbacks.gro_receive)\n\t\t\tgoto unflush;\n\t}\n\tgoto out_unlock;\n\nunflush:\n\tflush = 0;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tuh2 = (struct udphdr   *)(p->data + off);\n\n\t\t/* Match ports and either checksums are either both zero\n\t\t * or nonzero.\n\t\t */\n\t\tif ((*(u32 *)&uh->source != *(u32 *)&uh2->source) ||\n\t\t    (!uh->check ^ !uh2->check)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, sizeof(struct udphdr)); /* pull encapsulating udp header */\n\tskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\n\tNAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;\n\tpp = uo_priv->offload->callbacks.gro_receive(head, skb,\n\t\t\t\t\t\t     uo_priv->offload);\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\treturn pp;\n}\n\nstatic struct sk_buff **udp4_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct udphdr *uh = udp_gro_udphdr(skb);\n\n\tif (unlikely(!uh))\n\t\tgoto flush;\n\n\t/* Don't bother verifying checksum if we're going to flush anyway. */\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto skip;\n\n\tif (skb_gro_checksum_validate_zero_check(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t inet_gro_compute_pseudo))\n\t\tgoto flush;\n\telse if (uh->check)\n\t\tskb_gro_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t     inet_gro_compute_pseudo);\nskip:\n\tNAPI_GRO_CB(skb)->is_ipv6 = 0;\n\treturn udp_gro_receive(head, skb, uh);\n\nflush:\n\tNAPI_GRO_CB(skb)->flush = 1;\n\treturn NULL;\n}\n\nint udp_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tstruct udp_offload_priv *uo_priv;\n\t__be16 newlen = htons(skb->len - nhoff);\n\tstruct udphdr *uh = (struct udphdr *)(skb->data + nhoff);\n\tint err = -ENOSYS;\n\n\tuh->len = newlen;\n\n\trcu_read_lock();\n\n\tuo_priv = rcu_dereference(udp_offload_base);\n\tfor (; uo_priv != NULL; uo_priv = rcu_dereference(uo_priv->next)) {\n\t\tif (net_eq(read_pnet(&uo_priv->net), dev_net(skb->dev)) &&\n\t\t    uo_priv->offload->port == uh->dest &&\n\t\t    uo_priv->offload->callbacks.gro_complete)\n\t\t\tbreak;\n\t}\n\n\tif (uo_priv) {\n\t\tNAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;\n\t\terr = uo_priv->offload->callbacks.gro_complete(skb,\n\t\t\t\tnhoff + sizeof(struct udphdr),\n\t\t\t\tuo_priv->offload);\n\t}\n\n\trcu_read_unlock();\n\n\tif (skb->remcsum_offload)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TUNNEL_REMCSUM;\n\n\tskb->encapsulation = 1;\n\tskb_set_inner_mac_header(skb, nhoff + sizeof(struct udphdr));\n\n\treturn err;\n}\n\nstatic int udp4_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct udphdr *uh = (struct udphdr *)(skb->data + nhoff);\n\n\tif (uh->check) {\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_UDP_TUNNEL_CSUM;\n\t\tuh->check = ~udp_v4_check(skb->len - nhoff, iph->saddr,\n\t\t\t\t\t  iph->daddr, 0);\n\t} else {\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_UDP_TUNNEL;\n\t}\n\n\treturn udp_gro_complete(skb, nhoff);\n}\n\nstatic const struct net_offload udpv4_offload = {\n\t.callbacks = {\n\t\t.gso_segment = udp4_ufo_fragment,\n\t\t.gro_receive  =\tudp4_gro_receive,\n\t\t.gro_complete =\tudp4_gro_complete,\n\t},\n};\n\nint __init udpv4_offload_init(void)\n{\n\treturn inet_add_offload(&udpv4_offload, IPPROTO_UDP);\n}\n", "/*\n *\tIPV6 GSO/GRO offload support\n *\tLinux INET6 implementation\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/kernel.h>\n#include <linux/socket.h>\n#include <linux/netdevice.h>\n#include <linux/skbuff.h>\n#include <linux/printk.h>\n\n#include <net/protocol.h>\n#include <net/ipv6.h>\n\n#include \"ip6_offload.h\"\n\nstatic int ipv6_gso_pull_exthdrs(struct sk_buff *skb, int proto)\n{\n\tconst struct net_offload *ops = NULL;\n\n\tfor (;;) {\n\t\tstruct ipv6_opt_hdr *opth;\n\t\tint len;\n\n\t\tif (proto != NEXTHDR_HOP) {\n\t\t\tops = rcu_dereference(inet6_offloads[proto]);\n\n\t\t\tif (unlikely(!ops))\n\t\t\t\tbreak;\n\n\t\t\tif (!(ops->flags & INET6_PROTO_GSO_EXTHDR))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!pskb_may_pull(skb, 8)))\n\t\t\tbreak;\n\n\t\topth = (void *)skb->data;\n\t\tlen = ipv6_optlen(opth);\n\n\t\tif (unlikely(!pskb_may_pull(skb, len)))\n\t\t\tbreak;\n\n\t\topth = (void *)skb->data;\n\t\tproto = opth->nexthdr;\n\t\t__skb_pull(skb, len);\n\t}\n\n\treturn proto;\n}\n\nstatic struct sk_buff *ipv6_gso_segment(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct ipv6hdr *ipv6h;\n\tconst struct net_offload *ops;\n\tint proto;\n\tstruct frag_hdr *fptr;\n\tunsigned int unfrag_ip6hlen;\n\tu8 *prevhdr;\n\tint offset = 0;\n\tbool encap, udpfrag;\n\tint nhoff;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t     ~(SKB_GSO_TCPV4 |\n\t\t       SKB_GSO_UDP |\n\t\t       SKB_GSO_DODGY |\n\t\t       SKB_GSO_TCP_ECN |\n\t\t       SKB_GSO_GRE |\n\t\t       SKB_GSO_GRE_CSUM |\n\t\t       SKB_GSO_IPIP |\n\t\t       SKB_GSO_SIT |\n\t\t       SKB_GSO_UDP_TUNNEL |\n\t\t       SKB_GSO_UDP_TUNNEL_CSUM |\n\t\t       SKB_GSO_TUNNEL_REMCSUM |\n\t\t       SKB_GSO_TCPV6 |\n\t\t       0)))\n\t\tgoto out;\n\n\tskb_reset_network_header(skb);\n\tnhoff = skb_network_header(skb) - skb_mac_header(skb);\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*ipv6h))))\n\t\tgoto out;\n\n\tencap = SKB_GSO_CB(skb)->encap_level > 0;\n\tif (encap)\n\t\tfeatures &= skb->dev->hw_enc_features;\n\tSKB_GSO_CB(skb)->encap_level += sizeof(*ipv6h);\n\n\tipv6h = ipv6_hdr(skb);\n\t__skb_pull(skb, sizeof(*ipv6h));\n\tsegs = ERR_PTR(-EPROTONOSUPPORT);\n\n\tproto = ipv6_gso_pull_exthdrs(skb, ipv6h->nexthdr);\n\n\tif (skb->encapsulation &&\n\t    skb_shinfo(skb)->gso_type & (SKB_GSO_SIT|SKB_GSO_IPIP))\n\t\tudpfrag = proto == IPPROTO_UDP && encap;\n\telse\n\t\tudpfrag = proto == IPPROTO_UDP && !skb->encapsulation;\n\n\tops = rcu_dereference(inet6_offloads[proto]);\n\tif (likely(ops && ops->callbacks.gso_segment)) {\n\t\tskb_reset_transport_header(skb);\n\t\tsegs = ops->callbacks.gso_segment(skb, features);\n\t}\n\n\tif (IS_ERR(segs))\n\t\tgoto out;\n\n\tfor (skb = segs; skb; skb = skb->next) {\n\t\tipv6h = (struct ipv6hdr *)(skb_mac_header(skb) + nhoff);\n\t\tipv6h->payload_len = htons(skb->len - nhoff - sizeof(*ipv6h));\n\t\tskb->network_header = (u8 *)ipv6h - skb->head;\n\n\t\tif (udpfrag) {\n\t\t\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\t\t\tfptr = (struct frag_hdr *)((u8 *)ipv6h + unfrag_ip6hlen);\n\t\t\tfptr->frag_off = htons(offset);\n\t\t\tif (skb->next)\n\t\t\t\tfptr->frag_off |= htons(IP6_MF);\n\t\t\toffset += (ntohs(ipv6h->payload_len) -\n\t\t\t\t   sizeof(struct frag_hdr));\n\t\t}\n\t\tif (encap)\n\t\t\tskb_reset_inner_headers(skb);\n\t}\n\nout:\n\treturn segs;\n}\n\n/* Return the total length of all the extension hdrs, following the same\n * logic in ipv6_gso_pull_exthdrs() when parsing ext-hdrs.\n */\nstatic int ipv6_exthdrs_len(struct ipv6hdr *iph,\n\t\t\t    const struct net_offload **opps)\n{\n\tstruct ipv6_opt_hdr *opth = (void *)iph;\n\tint len = 0, proto, optlen = sizeof(*iph);\n\n\tproto = iph->nexthdr;\n\tfor (;;) {\n\t\tif (proto != NEXTHDR_HOP) {\n\t\t\t*opps = rcu_dereference(inet6_offloads[proto]);\n\t\t\tif (unlikely(!(*opps)))\n\t\t\t\tbreak;\n\t\t\tif (!((*opps)->flags & INET6_PROTO_GSO_EXTHDR))\n\t\t\t\tbreak;\n\t\t}\n\t\topth = (void *)opth + optlen;\n\t\toptlen = ipv6_optlen(opth);\n\t\tlen += optlen;\n\t\tproto = opth->nexthdr;\n\t}\n\treturn len;\n}\n\nstatic struct sk_buff **ipv6_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct net_offload *ops;\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tstruct ipv6hdr *iph;\n\tunsigned int nlen;\n\tunsigned int hlen;\n\tunsigned int off;\n\tu16 flush = 1;\n\tint proto;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*iph);\n\tiph = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tiph = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!iph))\n\t\t\tgoto out;\n\t}\n\n\tskb_set_network_header(skb, off);\n\tskb_gro_pull(skb, sizeof(*iph));\n\tskb_set_transport_header(skb, skb_gro_offset(skb));\n\n\tflush += ntohs(iph->payload_len) != skb_gro_len(skb);\n\n\trcu_read_lock();\n\tproto = iph->nexthdr;\n\tops = rcu_dereference(inet6_offloads[proto]);\n\tif (!ops || !ops->callbacks.gro_receive) {\n\t\t__pskb_pull(skb, skb_gro_offset(skb));\n\t\tproto = ipv6_gso_pull_exthdrs(skb, proto);\n\t\tskb_gro_pull(skb, -skb_transport_offset(skb));\n\t\tskb_reset_transport_header(skb);\n\t\t__skb_push(skb, skb_gro_offset(skb));\n\n\t\tops = rcu_dereference(inet6_offloads[proto]);\n\t\tif (!ops || !ops->callbacks.gro_receive)\n\t\t\tgoto out_unlock;\n\n\t\tiph = ipv6_hdr(skb);\n\t}\n\n\tNAPI_GRO_CB(skb)->proto = proto;\n\n\tflush--;\n\tnlen = skb_network_header_len(skb);\n\n\tfor (p = *head; p; p = p->next) {\n\t\tconst struct ipv6hdr *iph2;\n\t\t__be32 first_word; /* <Version:4><Traffic_Class:8><Flow_Label:20> */\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tiph2 = (struct ipv6hdr *)(p->data + off);\n\t\tfirst_word = *(__be32 *)iph ^ *(__be32 *)iph2;\n\n\t\t/* All fields must match except length and Traffic Class.\n\t\t * XXX skbs on the gro_list have all been parsed and pulled\n\t\t * already so we don't need to compare nlen\n\t\t * (nlen != (sizeof(*iph2) + ipv6_exthdrs_len(iph2, &ops)))\n\t\t * memcmp() alone below is suffcient, right?\n\t\t */\n\t\t if ((first_word & htonl(0xF00FFFFF)) ||\n\t\t    memcmp(&iph->nexthdr, &iph2->nexthdr,\n\t\t\t   nlen - offsetof(struct ipv6hdr, nexthdr))) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t\t/* flush if Traffic Class fields are different */\n\t\tNAPI_GRO_CB(p)->flush |= !!(first_word & htonl(0x0FF00000));\n\t\tNAPI_GRO_CB(p)->flush |= flush;\n\n\t\t/* Clear flush_id, there's really no concept of ID in IPv6. */\n\t\tNAPI_GRO_CB(p)->flush_id = 0;\n\t}\n\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\tskb_gro_postpull_rcsum(skb, iph, nlen);\n\n\tpp = ops->callbacks.gro_receive(head, skb);\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic int ipv6_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tconst struct net_offload *ops;\n\tstruct ipv6hdr *iph = (struct ipv6hdr *)(skb->data + nhoff);\n\tint err = -ENOSYS;\n\n\tif (skb->encapsulation)\n\t\tskb_set_inner_network_header(skb, nhoff);\n\n\tiph->payload_len = htons(skb->len - nhoff - sizeof(*iph));\n\n\trcu_read_lock();\n\n\tnhoff += sizeof(*iph) + ipv6_exthdrs_len(iph, &ops);\n\tif (WARN_ON(!ops || !ops->callbacks.gro_complete))\n\t\tgoto out_unlock;\n\n\terr = ops->callbacks.gro_complete(skb, nhoff);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic int sit_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tskb->encapsulation = 1;\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_SIT;\n\treturn ipv6_gro_complete(skb, nhoff);\n}\n\nstatic struct packet_offload ipv6_packet_offload __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IPV6),\n\t.callbacks = {\n\t\t.gso_segment = ipv6_gso_segment,\n\t\t.gro_receive = ipv6_gro_receive,\n\t\t.gro_complete = ipv6_gro_complete,\n\t},\n};\n\nstatic const struct net_offload sit_offload = {\n\t.callbacks = {\n\t\t.gso_segment\t= ipv6_gso_segment,\n\t\t.gro_receive    = ipv6_gro_receive,\n\t\t.gro_complete   = sit_gro_complete,\n\t},\n};\n\nstatic int __init ipv6_offload_init(void)\n{\n\n\tif (tcpv6_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add TCP protocol offload\\n\", __func__);\n\tif (udp_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add UDP protocol offload\\n\", __func__);\n\tif (ipv6_exthdrs_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add EXTHDRS protocol offload\\n\", __func__);\n\n\tdev_add_offload(&ipv6_packet_offload);\n\n\tinet_add_offload(&sit_offload, IPPROTO_IPV6);\n\n\treturn 0;\n}\n\nfs_initcall(ipv6_offload_init);\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/timer.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/prefetch.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/dmaengine.h>\n#include <linux/workqueue.h>\n#include <linux/dynamic_queue_limits.h>\n\n#include <linux/ethtool.h>\n#include <net/net_namespace.h>\n#include <net/dsa.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n#include <net/netprio_cgroup.h>\n\n#include <linux/netdev_features.h>\n#include <linux/neighbour.h>\n#include <uapi/linux/netdevice.h>\n#include <uapi/linux/if_bonding.h>\n#include <uapi/linux/pkt_cls.h>\n\nstruct netpoll_info;\nstruct device;\nstruct phy_device;\n/* 802.11 specific */\nstruct wireless_dev;\n/* 802.15.4 specific */\nstruct wpan_dev;\nstruct mpls_dev;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops);\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously, in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_POLICED\t0x03\t/* skb is shot by police\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n\tNETDEV_TX_LOCKED = 0x20,\t/* driver tx lock was already taken */\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tCompute the worst case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_HYPERV_NET)\n# define LL_MAX_HEADER 128\n#elif defined(CONFIG_WLAN) || IS_ENABLED(CONFIG_AX25)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !IS_ENABLED(CONFIG_NET_IPIP) && !IS_ENABLED(CONFIG_NET_IPGRE) && \\\n    !IS_ENABLED(CONFIG_IPV6_SIT) && !IS_ENABLED(CONFIG_IPV6_TUNNEL)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\n#ifdef CONFIG_RPS\n#include <linux/static_key.h>\nextern struct static_key rps_needed;\n#endif\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_SLAVE\t\t3\n#define NETDEV_HW_ADDR_T_UNICAST\t4\n#define NETDEV_HW_ADDR_T_MULTICAST\t5\n\tbool\t\t\tglobal_use;\n\tint\t\t\tsync_cnt;\n\tint\t\t\trefcount;\n\tint\t\t\tsynced;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tu16\t\thh_len;\n\tu16\t\t__pad;\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\n/* Reserve HH_DATA_MOD byte aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned int len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n\tbool\t(*validate)(const char *ll_header, unsigned int len);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer, they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n};\n\n\n/*\n * This structure holds at boot time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nint __init netdev_boot_setup(char *str);\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-cpu poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tunsigned int\t\tgro_count;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tspinlock_t\t\tpoll_lock;\n\tint\t\t\tpoll_owner;\n#endif\n\tstruct net_device\t*dev;\n\tstruct sk_buff\t\t*gro_list;\n\tstruct sk_buff\t\t*skb;\n\tstruct hrtimer\t\ttimer;\n\tstruct list_head\tdev_list;\n\tstruct hlist_node\tnapi_hash_node;\n\tunsigned int\t\tnapi_id;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t/* Poll is scheduled */\n\tNAPI_STATE_DISABLE,\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t/* Netpoll - don't dequeue from poll_list */\n\tNAPI_STATE_HASHED,\t/* In NAPI hash (busy polling possible) */\n\tNAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_DROP,\n};\ntypedef enum gro_result gro_result_t;\n\n/*\n * enum rx_handler_result - Possible return values for rx_handlers.\n * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it\n * further.\n * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in\n * case skb->dev was changed by rx_handler.\n * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.\n * @RX_HANDLER_PASS: Do nothing, passe the skb as if no rx_handler was called.\n *\n * rx_handlers are functions called from inside __netif_receive_skb(), to do\n * special processing of the skb, prior to delivery to protocol handlers.\n *\n * Currently, a net_device can only have a single rx_handler registered. Trying\n * to register a second rx_handler will return -EBUSY.\n *\n * To register a rx_handler on a net_device, use netdev_rx_handler_register().\n * To unregister a rx_handler on a net_device, use\n * netdev_rx_handler_unregister().\n *\n * Upon return, rx_handler is expected to tell __netif_receive_skb() what to\n * do with the skb.\n *\n * If the rx_handler consumed to skb in some way, it should return\n * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for\n * the skb to be delivered in some other ways.\n *\n * If the rx_handler changed skb->dev, to divert the skb to another\n * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the\n * new device will be called if it exists.\n *\n * If the rx_handler consider the skb should be ignored, it should return\n * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that\n * are registered on exact device (ptype->dev == skb->dev).\n *\n * If the rx_handler didn't changed skb->dev, but want the skb to be normally\n * delivered, it should return RX_HANDLER_PASS.\n *\n * A device without a registered rx_handler will behave as if rx_handler\n * returned RX_HANDLER_PASS.\n */\n\nenum rx_handler_result {\n\tRX_HANDLER_CONSUMED,\n\tRX_HANDLER_ANOTHER,\n\tRX_HANDLER_EXACT,\n\tRX_HANDLER_PASS,\n};\ntypedef enum rx_handler_result rx_handler_result_t;\ntypedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);\n\nvoid __napi_schedule(struct napi_struct *n);\nvoid __napi_schedule_irqoff(struct napi_struct *n);\n\nstatic inline bool napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nstatic inline bool napi_schedule_prep(struct napi_struct *n)\n{\n\treturn !napi_disable_pending(n) &&\n\t\t!test_and_set_bit(NAPI_STATE_SCHED, &n->state);\n}\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: napi context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/**\n *\tnapi_schedule_irqoff - schedule NAPI poll\n *\t@n: napi context\n *\n * Variant of napi_schedule(), assuming hard irqs are masked.\n */\nstatic inline void napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule_irqoff(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline bool napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid __napi_complete(struct napi_struct *n);\nvoid napi_complete_done(struct napi_struct *n, int work_done);\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: napi context\n *\n * Mark NAPI processing as complete.\n * Consider using napi_complete_done() instead.\n */\nstatic inline void napi_complete(struct napi_struct *n)\n{\n\treturn napi_complete_done(n, 0);\n}\n\n/**\n *\tnapi_hash_add - add a NAPI to global hashtable\n *\t@napi: napi context\n *\n * generate a new napi_id and store a @napi under it in napi_hash\n * Used for busy polling (CONFIG_NET_RX_BUSY_POLL)\n * Note: This is normally automatically done from netif_napi_add(),\n * so might disappear in a future linux version.\n */\nvoid napi_hash_add(struct napi_struct *napi);\n\n/**\n *\tnapi_hash_del - remove a NAPI from global table\n *\t@napi: napi context\n *\n * Warning: caller must observe rcu grace period\n * before freeing memory containing @napi, if\n * this function returns true.\n * Note: core networking stack automatically calls it\n * from netif_napi_del()\n * Drivers might want to call this helper to combine all\n * the needed rcu grace periods into a single one.\n */\nbool napi_hash_del(struct napi_struct *napi);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: napi context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nvoid napi_disable(struct napi_struct *n);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: napi context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nstatic inline void napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n\tclear_bit(NAPI_STATE_NPSVC, &n->state);\n}\n\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: napi context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\tif (IS_ENABLED(CONFIG_SMP))\n\t\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\t\tmsleep(1);\n\telse\n\t\tbarrier();\n}\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_DRV_XOFF,\n\t__QUEUE_STATE_STACK_XOFF,\n\t__QUEUE_STATE_FROZEN,\n};\n\n#define QUEUE_STATE_DRV_XOFF\t(1 << __QUEUE_STATE_DRV_XOFF)\n#define QUEUE_STATE_STACK_XOFF\t(1 << __QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_FROZEN\t(1 << __QUEUE_STATE_FROZEN)\n\n#define QUEUE_STATE_ANY_XOFF\t(QUEUE_STATE_DRV_XOFF | QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_ANY_XOFF_OR_FROZEN (QUEUE_STATE_ANY_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n#define QUEUE_STATE_DRV_XOFF_OR_FROZEN (QUEUE_STATE_DRV_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n\n/*\n * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The\n * netif_tx_* functions below are used to manipulate this flag.  The\n * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit\n * queue independently.  The netif_xmit_*stopped functions below are called\n * to check if the queue has been stopped by the driver or stack (either\n * of the XOFF bits are set in the state).  Drivers should not need to call\n * netif_xmit*stopped functions, they should only be using netif_tx_*.\n */\n\nstruct netdev_queue {\n/*\n * read mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc __rcu\t*qdisc;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_SYSFS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n/*\n * write mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * please use this field instead of dev->trans_start\n\t */\n\tunsigned long\t\ttrans_start;\n\n\t/*\n\t * Number of TX timeouts for this queue\n\t * (/sys/class/net/DEV/Q/trans_timeout)\n\t */\n\tunsigned long\t\ttrans_timeout;\n\n\tunsigned long\t\tstate;\n\n#ifdef CONFIG_BQL\n\tstruct dql\t\tdql;\n#endif\n\tunsigned long\t\ttx_maxrate;\n} ____cacheline_aligned_in_smp;\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[0];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU, the\n * tail pointer for that CPU's input queue at the time of last enqueue, and\n * a hardware filter index.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 filter;\n\tunsigned int last_qtail;\n};\n#define RPS_NO_FILTER 0xffff\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct rps_dev_flow flows[0];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    ((_num) * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n * Each entry is a 32bit value. Upper part is the high order bits\n * of flow hash, lower part is cpu number.\n * rps_cpu_mask is used to partition the space, depending on number of\n * possible cpus : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1\n * For example, if 64 cpus are possible, rps_cpu_mask = 0x3f,\n * meaning we use 32-6=26 bits for the hash.\n */\nstruct rps_sock_flow_table {\n\tu32\tmask;\n\n\tu32\tents[0] ____cacheline_aligned_in_smp;\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (offsetof(struct rps_sock_flow_table, ents[_num]))\n\n#define RPS_NO_CPU 0xffff\n\nextern u32 rps_cpu_mask;\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int index = hash & table->mask;\n\t\tu32 val = hash & ~rps_cpu_mask;\n\n\t\t/* We only give a hint, preemption can change cpu under us */\n\t\tval |= raw_smp_processor_id();\n\n\t\tif (table->ents[index] != val)\n\t\t\ttable->ents[index] = val;\n\t}\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id,\n\t\t\t u16 filter_id);\n#endif\n#endif /* CONFIG_RPS */\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n#ifdef CONFIG_RPS\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n#endif\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n} ____cacheline_aligned_in_smp;\n\n/*\n * RX queue sysfs structures and functions.\n */\nstruct rx_queue_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct netdev_rx_queue *queue,\n\t    struct rx_queue_attribute *attr, char *buf);\n\tssize_t (*store)(struct netdev_rx_queue *queue,\n\t    struct rx_queue_attribute *attr, const char *buf, size_t len);\n};\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[0];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + ((_num) * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_ALIGN(offsetof(struct xps_map, queues[1])) \\\n       - sizeof(struct xps_map)) / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tstruct xps_map __rcu *cpu_map[0];\n};\n#define XPS_DEV_MAPS_SIZE (sizeof(struct xps_dev_maps) +\t\t\\\n    (nr_cpu_ids * sizeof(struct xps_map *)))\n#endif /* CONFIG_XPS */\n\n#define TC_MAX_QUEUE\t16\n#define TC_BITMASK\t15\n/* HW offloaded queuing disciplines txq count and offset maps */\nstruct netdev_tc_txq {\n\tu16 count;\n\tu16 offset;\n};\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n/*\n * This structure is to hold information about the device\n * configured to run FCoE protocol stack.\n */\nstruct netdev_fcoe_hbainfo {\n\tchar\tmanufacturer[64];\n\tchar\tserial_number[64];\n\tchar\thardware_version[64];\n\tchar\tdriver_version[64];\n\tchar\toptionrom_version[64];\n\tchar\tfirmware_version[64];\n\tchar\tmodel[256];\n\tchar\tmodel_description[256];\n};\n#endif\n\n#define MAX_PHYS_ITEM_ID_LEN 32\n\n/* This structure holds a unique identifier to identify some\n * physical item (port for example) used by a netdevice.\n */\nstruct netdev_phys_item_id {\n\tunsigned char id[MAX_PHYS_ITEM_ID_LEN];\n\tunsigned char id_len;\n};\n\nstatic inline bool netdev_phys_item_id_same(struct netdev_phys_item_id *a,\n\t\t\t\t\t    struct netdev_phys_item_id *b)\n{\n\treturn a->id_len == b->id_len &&\n\t       memcmp(a->id, b->id, a->id_len) == 0;\n}\n\ntypedef u16 (*select_queue_fallback_t)(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb);\n\n/* These structures hold the attributes of qdisc and classifiers\n * that are being passed to the netdevice through the setup_tc op.\n */\nenum {\n\tTC_SETUP_MQPRIO,\n\tTC_SETUP_CLSU32,\n\tTC_SETUP_CLSFLOWER,\n};\n\nstruct tc_cls_u32_offload;\n\nstruct tc_to_netdev {\n\tunsigned int type;\n\tunion {\n\t\tu8 tc;\n\t\tstruct tc_cls_u32_offload *cls_u32;\n\t\tstruct tc_cls_flower_offload *cls_flower;\n\t};\n};\n\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when network device is registered.\n *     The network device can use this to any late stage initializaton\n *     or semantic validattion. It can fail with an error code which will\n *     be propogated back to register_netdev\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when network device transistions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when network device transistions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tReturns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop\n *\tthe queue before that can happen; it's for obsolete devices and weird\n *\tcorner cases, but the stack really does a non-trivial amount\n *\tof useless work if you return NETDEV_TX_BUSY.\n *        (can also return NETDEV_TX_LOCKED iff NETIF_F_LLTX)\n *\tRequired can not be NULL.\n *\n * netdev_features_t (*ndo_fix_features)(struct net_device *dev,\n *\t\tnetdev_features_t features);\n *\tAdjusts the requested feature flags according to device-specific\n *\tconstraints, and returns the resulting flags. Must not modify\n *\tthe device state.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,\n *                         void *accel_priv, select_queue_fallback_t fallback);\n *\tCalled to decide which queue to when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscious is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\tIf driver handles unicast address filtering, it should set\n *\tIFF_UNICAST_FLT to its priv_flags.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tmac address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user request an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctl's return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reason, new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device. If not defined, any request to change MTU will\n *\twill return an error.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev);\n *\tCallback uses when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n *                      struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device support VLAN filtering this function is called when a\n *\tVLAN id is registered.\n *\n * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device support VLAN filtering this function is called when a\n *\tVLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos);\n * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,\n *\t\t\t  int max_tx_rate);\n * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n *\n *      Enable or disable the VF ability to query its RSS Redirection Table and\n *      Hash Key. This is needed since on some devices VF share this information\n *      with PF and querying it may adduce a theoretical security risk.\n * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n * int (*ndo_setup_tc)(struct net_device *dev, u8 tc)\n * \tCalled to setup 'tc' number of traffic classes in the net device. This\n * \tis always called from the stack with the rtnl lock held and netif tx\n * \tqueues stopped. This allows the netdevice to perform queue management\n * \tsafely.\n *\n *\tFiber Channel over Ethernet (FCoE) offload functions.\n * int (*ndo_fcoe_enable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to start using LLD for FCoE\n *\tso the underlying device can perform whatever needed configuration or\n *\tinitialization to support acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_disable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to stop using LLD for FCoE\n *\tso the underlying device can perform whatever needed clean-ups to\n *\tstop supporting acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,\n *\t\t\t     struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Initiator wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);\n *\tCalled when the FCoE Initiator/Target is done with the DDPed I/O as\n *\tindicated by the FC exchange id 'xid', so the underlying device can\n *\tclean up and reuse resources for later DDP requests.\n *\n * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,\n *\t\t\t      struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Target wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n *\t\t\t       struct netdev_fcoe_hbainfo *hbainfo);\n *\tCalled when the FCoE Protocol stack wants information on the underlying\n *\tdevice. This information is utilized by the FCoE protocol stack to\n *\tregister attributes with Fiber Channel management service as per the\n *\tFC-GS Fabric Device Management Information(FDMI) specification.\n *\n * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);\n *\tCalled when the underlying device wants to override default World Wide\n *\tName (WWN) generation mechanism in FCoE protocol stack to pass its own\n *\tWorld Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE\n *\tprotocol stack to use.\n *\n *\tRFS acceleration.\n * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,\n *\t\t\t    u16 rxq_index, u32 flow_id);\n *\tSet hardware filter for RFS.  rxq_index is the target queue index;\n *\tflow_id is a flow ID to be passed to rps_may_expire_flow() later.\n *\tReturn the filter ID on success, or a negative error code.\n *\n *\tSlave management functions (for bridge, bonding, etc).\n * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to make another netdev an underling.\n *\n * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to release previously enslaved netdev.\n *\n *      Feature/offload setting functions.\n * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);\n *\tCalled to update device configuration to new features. Passed\n *\tfeature set might be less than what was returned by ndo_fix_features()).\n *\tMust return >0 or -errno if it changed dev->features itself.\n *\n * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid, u16 flags)\n *\tAdds an FDB entry to dev for addr.\n * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid)\n *\tDeletes the FDB entry from dev coresponding to addr.\n * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,\n *\t\t       struct net_device *dev, struct net_device *filter_dev,\n *\t\t       int idx)\n *\tUsed to add FDB entries to dump requests. Implementers should add\n *\tentries to skb and update idx with the number of entries.\n *\n * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags)\n * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,\n *\t\t\t     struct net_device *dev, u32 filter_mask,\n *\t\t\t     int nlflags)\n * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags);\n *\n * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);\n *\tCalled to change device carrier. Soft-devices (like dummy, team, etc)\n *\twhich do not represent real hardware may define this to allow their\n *\tuserspace components to manage their virtual carrier state. Devices\n *\tthat determine carrier state from physical hardware properties (eg\n *\tnetwork cables) or protocol-dependent mechanisms (eg\n *\tUSB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.\n *\n * int (*ndo_get_phys_port_id)(struct net_device *dev,\n *\t\t\t       struct netdev_phys_item_id *ppid);\n *\tCalled to get ID of physical port of this device. If driver does\n *\tnot implement this, it is assumed that the hw is not able to have\n *\tmultiple net devices on single physical port.\n *\n * void (*ndo_add_vxlan_port)(struct  net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by vxlan to notiy a driver about the UDP port and socket\n *\taddress family that vxlan is listnening to. It is called only when\n *\ta new port starts listening. The operation is protected by the\n *\tvxlan_net->sock_lock.\n *\n * void (*ndo_add_geneve_port)(struct net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by geneve to notify a driver about the UDP port and socket\n *\taddress family that geneve is listnening to. It is called only when\n *\ta new port starts listening. The operation is protected by the\n *\tgeneve_net->sock_lock.\n *\n * void (*ndo_del_geneve_port)(struct net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by geneve to notify the driver about a UDP port and socket\n *\taddress family that geneve is not listening to anymore. The operation\n *\tis protected by the geneve_net->sock_lock.\n *\n * void (*ndo_del_vxlan_port)(struct  net_device *dev,\n *\t\t\t      sa_family_t sa_family, __be16 port);\n *\tCalled by vxlan to notify the driver about a UDP port and socket\n *\taddress family that vxlan is not listening to anymore. The operation\n *\tis protected by the vxlan_net->sock_lock.\n *\n * void* (*ndo_dfwd_add_station)(struct net_device *pdev,\n *\t\t\t\t struct net_device *dev)\n *\tCalled by upper layer devices to accelerate switching or other\n *\tstation functionality into hardware. 'pdev is the lowerdev\n *\tto use for the offload and 'dev' is the net device that will\n *\tback the offload. Returns a pointer to the private structure\n *\tthe upper layer will maintain.\n * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)\n *\tCalled by upper layer device to delete the station created\n *\tby 'ndo_dfwd_add_station'. 'pdev' is the net device backing\n *\tthe station and priv is the structure returned by the add\n *\toperation.\n * netdev_tx_t (*ndo_dfwd_start_xmit)(struct sk_buff *skb,\n *\t\t\t\t      struct net_device *dev,\n *\t\t\t\t      void *priv);\n *\tCallback to use for xmit over the accelerated station. This\n *\tis used in place of ndo_start_xmit on accelerated net\n *\tdevices.\n * netdev_features_t (*ndo_features_check) (struct sk_buff *skb,\n *\t\t\t\t\t    struct net_device *dev\n *\t\t\t\t\t    netdev_features_t features);\n *\tCalled by core transmit path to determine if device is capable of\n *\tperforming offload operations on a given packet. This is to give\n *\tthe device an opportunity to implement any restrictions that cannot\n *\tbe otherwise expressed by feature flags. The check is called with\n *\tthe set of features that the stack has calculated and it returns\n *\tthose the driver believes to be appropriate.\n * int (*ndo_set_tx_maxrate)(struct net_device *dev,\n *\t\t\t     int queue_index, u32 maxrate);\n *\tCalled when a user wants to set a max-rate limitation of specific\n *\tTX queue.\n * int (*ndo_get_iflink)(const struct net_device *dev);\n *\tCalled to get the iflink value of this device.\n * void (*ndo_change_proto_down)(struct net_device *dev,\n *\t\t\t\t  bool proto_down);\n *\tThis function is used to pass protocol port error state information\n *\tto the switch driver. The switch driver can react to the proto_down\n *      by doing a phys down on the associated switch port.\n * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);\n *\tThis function is used to get egress tunnel information for given skb.\n *\tThis is useful for retrieving outer tunnel header parameters while\n *\tsampling packet.\n * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);\n *\tThis function is used to specify the headroom that the skb must\n *\tconsider when allocation skb during packet reception. Setting\n *\tappropriate rx headroom value allows avoiding skb head copy on\n *\tforward. Setting a negative value reset the rx headroom to the\n *\tdefault value.\n *\n */\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit)(struct sk_buff *skb,\n\t\t\t\t\t\t  struct net_device *dev);\n\tnetdev_features_t\t(*ndo_features_check)(struct sk_buff *skb,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      netdev_features_t features);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    void *accel_priv,\n\t\t\t\t\t\t    select_queue_fallback_t fallback);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev);\n\n\tstruct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t     struct rtnl_link_stats64 *storage);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tint\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       __be16 proto, u16 vid);\n\tint\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        __be16 proto, u16 vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tint\t\t\t(*ndo_busy_poll)(struct napi_struct *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan, u8 qos);\n\tint\t\t\t(*ndo_set_vf_rate)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, int min_tx_rate,\n\t\t\t\t\t\t   int max_tx_rate);\n\tint\t\t\t(*ndo_set_vf_spoofchk)(struct net_device *dev,\n\t\t\t\t\t\t       int vf, bool setting);\n\tint\t\t\t(*ndo_set_vf_trust)(struct net_device *dev,\n\t\t\t\t\t\t    int vf, bool setting);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_link_state)(struct net_device *dev,\n\t\t\t\t\t\t\t int vf, int link_state);\n\tint\t\t\t(*ndo_get_vf_stats)(struct net_device *dev,\n\t\t\t\t\t\t    int vf,\n\t\t\t\t\t\t    struct ifla_vf_stats\n\t\t\t\t\t\t    *vf_stats);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n\tint\t\t\t(*ndo_set_vf_rss_query_en)(\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   int vf, bool setting);\n\tint\t\t\t(*ndo_setup_tc)(struct net_device *dev,\n\t\t\t\t\t\tu32 handle,\n\t\t\t\t\t\t__be16 protocol,\n\t\t\t\t\t\tstruct tc_to_netdev *tc);\n#if IS_ENABLED(CONFIG_FCOE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n\tint\t\t\t(*ndo_fcoe_ddp_target)(struct net_device *dev,\n\t\t\t\t\t\t       u16 xid,\n\t\t\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t\t\t       unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_fcoe_hbainfo *hbainfo);\n#endif\n\n#if IS_ENABLED(CONFIG_LIBFCOE)\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n\n#ifdef CONFIG_RFS_ACCEL\n\tint\t\t\t(*ndo_rx_flow_steer)(struct net_device *dev,\n\t\t\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t\t\t     u16 rxq_index,\n\t\t\t\t\t\t     u32 flow_id);\n#endif\n\tint\t\t\t(*ndo_add_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tint\t\t\t(*ndo_del_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tnetdev_features_t\t(*ndo_fix_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_set_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_neigh_construct)(struct neighbour *n);\n\tvoid\t\t\t(*ndo_neigh_destroy)(struct neighbour *n);\n\n\tint\t\t\t(*ndo_fdb_add)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid,\n\t\t\t\t\t       u16 flags);\n\tint\t\t\t(*ndo_fdb_del)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid);\n\tint\t\t\t(*ndo_fdb_dump)(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\tstruct net_device *filter_dev,\n\t\t\t\t\t\tint idx);\n\n\tint\t\t\t(*ndo_bridge_setlink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_bridge_getlink)(struct sk_buff *skb,\n\t\t\t\t\t\t      u32 pid, u32 seq,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      u32 filter_mask,\n\t\t\t\t\t\t      int nlflags);\n\tint\t\t\t(*ndo_bridge_dellink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_change_carrier)(struct net_device *dev,\n\t\t\t\t\t\t      bool new_carrier);\n\tint\t\t\t(*ndo_get_phys_port_id)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_phys_port_name)(struct net_device *dev,\n\t\t\t\t\t\t\t  char *name, size_t len);\n\tvoid\t\t\t(*ndo_add_vxlan_port)(struct  net_device *dev,\n\t\t\t\t\t\t      sa_family_t sa_family,\n\t\t\t\t\t\t      __be16 port);\n\tvoid\t\t\t(*ndo_del_vxlan_port)(struct  net_device *dev,\n\t\t\t\t\t\t      sa_family_t sa_family,\n\t\t\t\t\t\t      __be16 port);\n\tvoid\t\t\t(*ndo_add_geneve_port)(struct  net_device *dev,\n\t\t\t\t\t\t       sa_family_t sa_family,\n\t\t\t\t\t\t       __be16 port);\n\tvoid\t\t\t(*ndo_del_geneve_port)(struct  net_device *dev,\n\t\t\t\t\t\t       sa_family_t sa_family,\n\t\t\t\t\t\t       __be16 port);\n\tvoid*\t\t\t(*ndo_dfwd_add_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tstruct net_device *dev);\n\tvoid\t\t\t(*ndo_dfwd_del_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tvoid *priv);\n\n\tnetdev_tx_t\t\t(*ndo_dfwd_start_xmit) (struct sk_buff *skb,\n\t\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\t\tvoid *priv);\n\tint\t\t\t(*ndo_get_lock_subclass)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_tx_maxrate)(struct net_device *dev,\n\t\t\t\t\t\t      int queue_index,\n\t\t\t\t\t\t      u32 maxrate);\n\tint\t\t\t(*ndo_get_iflink)(const struct net_device *dev);\n\tint\t\t\t(*ndo_change_proto_down)(struct net_device *dev,\n\t\t\t\t\t\t\t bool proto_down);\n\tint\t\t\t(*ndo_fill_metadata_dst)(struct net_device *dev,\n\t\t\t\t\t\t       struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_set_rx_headroom)(struct net_device *dev,\n\t\t\t\t\t\t       int needed_headroom);\n};\n\n/**\n * enum net_device_priv_flags - &struct net_device priv_flags\n *\n * These are the &struct net_device, they are only set internally\n * by drivers and used in the kernel. These flags are invisible to\n * userspace, this means that the order of these flags can change\n * during any kernel release.\n *\n * You should have a pretty good reason to be extending these flags.\n *\n * @IFF_802_1Q_VLAN: 802.1Q VLAN device\n * @IFF_EBRIDGE: Ethernet bridging device\n * @IFF_BONDING: bonding master or slave\n * @IFF_ISATAP: ISATAP interface (RFC4214)\n * @IFF_WAN_HDLC: WAN HDLC device\n * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to\n *\trelease skb->dst\n * @IFF_DONT_BRIDGE: disallow bridging this ether dev\n * @IFF_DISABLE_NETPOLL: disable netpoll at run-time\n * @IFF_MACVLAN_PORT: device used as macvlan port\n * @IFF_BRIDGE_PORT: device used as bridge port\n * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port\n * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit\n * @IFF_UNICAST_FLT: Supports unicast filtering\n * @IFF_TEAM_PORT: device used as team port\n * @IFF_SUPP_NOFCS: device supports sending custom FCS\n * @IFF_LIVE_ADDR_CHANGE: device supports hardware address\n *\tchange when it's running\n * @IFF_MACVLAN: Macvlan device\n * @IFF_L3MDEV_MASTER: device is an L3 master device\n * @IFF_NO_QUEUE: device can run without qdisc attached\n * @IFF_OPENVSWITCH: device is a Open vSwitch master\n * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device\n * @IFF_TEAM: device is a team device\n * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured\n * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external\n *\tentity (i.e. the master device for bridged veth)\n * @IFF_MACSEC: device is a MACsec device\n */\nenum netdev_priv_flags {\n\tIFF_802_1Q_VLAN\t\t\t= 1<<0,\n\tIFF_EBRIDGE\t\t\t= 1<<1,\n\tIFF_BONDING\t\t\t= 1<<2,\n\tIFF_ISATAP\t\t\t= 1<<3,\n\tIFF_WAN_HDLC\t\t\t= 1<<4,\n\tIFF_XMIT_DST_RELEASE\t\t= 1<<5,\n\tIFF_DONT_BRIDGE\t\t\t= 1<<6,\n\tIFF_DISABLE_NETPOLL\t\t= 1<<7,\n\tIFF_MACVLAN_PORT\t\t= 1<<8,\n\tIFF_BRIDGE_PORT\t\t\t= 1<<9,\n\tIFF_OVS_DATAPATH\t\t= 1<<10,\n\tIFF_TX_SKB_SHARING\t\t= 1<<11,\n\tIFF_UNICAST_FLT\t\t\t= 1<<12,\n\tIFF_TEAM_PORT\t\t\t= 1<<13,\n\tIFF_SUPP_NOFCS\t\t\t= 1<<14,\n\tIFF_LIVE_ADDR_CHANGE\t\t= 1<<15,\n\tIFF_MACVLAN\t\t\t= 1<<16,\n\tIFF_XMIT_DST_RELEASE_PERM\t= 1<<17,\n\tIFF_IPVLAN_MASTER\t\t= 1<<18,\n\tIFF_IPVLAN_SLAVE\t\t= 1<<19,\n\tIFF_L3MDEV_MASTER\t\t= 1<<20,\n\tIFF_NO_QUEUE\t\t\t= 1<<21,\n\tIFF_OPENVSWITCH\t\t\t= 1<<22,\n\tIFF_L3MDEV_SLAVE\t\t= 1<<23,\n\tIFF_TEAM\t\t\t= 1<<24,\n\tIFF_RXFH_CONFIGURED\t\t= 1<<25,\n\tIFF_PHONY_HEADROOM\t\t= 1<<26,\n\tIFF_MACSEC\t\t\t= 1<<27,\n};\n\n#define IFF_802_1Q_VLAN\t\t\tIFF_802_1Q_VLAN\n#define IFF_EBRIDGE\t\t\tIFF_EBRIDGE\n#define IFF_BONDING\t\t\tIFF_BONDING\n#define IFF_ISATAP\t\t\tIFF_ISATAP\n#define IFF_WAN_HDLC\t\t\tIFF_WAN_HDLC\n#define IFF_XMIT_DST_RELEASE\t\tIFF_XMIT_DST_RELEASE\n#define IFF_DONT_BRIDGE\t\t\tIFF_DONT_BRIDGE\n#define IFF_DISABLE_NETPOLL\t\tIFF_DISABLE_NETPOLL\n#define IFF_MACVLAN_PORT\t\tIFF_MACVLAN_PORT\n#define IFF_BRIDGE_PORT\t\t\tIFF_BRIDGE_PORT\n#define IFF_OVS_DATAPATH\t\tIFF_OVS_DATAPATH\n#define IFF_TX_SKB_SHARING\t\tIFF_TX_SKB_SHARING\n#define IFF_UNICAST_FLT\t\t\tIFF_UNICAST_FLT\n#define IFF_TEAM_PORT\t\t\tIFF_TEAM_PORT\n#define IFF_SUPP_NOFCS\t\t\tIFF_SUPP_NOFCS\n#define IFF_LIVE_ADDR_CHANGE\t\tIFF_LIVE_ADDR_CHANGE\n#define IFF_MACVLAN\t\t\tIFF_MACVLAN\n#define IFF_XMIT_DST_RELEASE_PERM\tIFF_XMIT_DST_RELEASE_PERM\n#define IFF_IPVLAN_MASTER\t\tIFF_IPVLAN_MASTER\n#define IFF_IPVLAN_SLAVE\t\tIFF_IPVLAN_SLAVE\n#define IFF_L3MDEV_MASTER\t\tIFF_L3MDEV_MASTER\n#define IFF_NO_QUEUE\t\t\tIFF_NO_QUEUE\n#define IFF_OPENVSWITCH\t\t\tIFF_OPENVSWITCH\n#define IFF_L3MDEV_SLAVE\t\tIFF_L3MDEV_SLAVE\n#define IFF_TEAM\t\t\tIFF_TEAM\n#define IFF_RXFH_CONFIGURED\t\tIFF_RXFH_CONFIGURED\n#define IFF_MACSEC\t\t\tIFF_MACSEC\n\n/**\n *\tstruct net_device - The DEVICE structure.\n *\t\tActually, this whole structure is a big mistake.  It mixes I/O\n *\t\tdata with strictly \"high-level\" data, and it has to know about\n *\t\talmost every data structure used in the INET module.\n *\n *\t@name:\tThis is the first field of the \"visible\" part of this structure\n *\t\t(i.e. as seen by users in the \"Space.c\" file).  It is the name\n *\t \tof the interface.\n *\n *\t@name_hlist: \tDevice name hash chain, please keep it close to name[]\n *\t@ifalias:\tSNMP alias\n *\t@mem_end:\tShared memory end\n *\t@mem_start:\tShared memory start\n *\t@base_addr:\tDevice I/O address\n *\t@irq:\t\tDevice IRQ number\n *\n *\t@carrier_changes:\tStats to monitor carrier on<->off transitions\n *\n *\t@state:\t\tGeneric network queuing layer state, see netdev_state_t\n *\t@dev_list:\tThe global list of network devices\n *\t@napi_list:\tList entry, that is used for polling napi devices\n *\t@unreg_list:\tList entry, that is used, when we are unregistering the\n *\t\t\tdevice, see the function unregister_netdev\n *\t@close_list:\tList entry, that is used, when we are closing the device\n *\n *\t@adj_list:\tDirectly linked devices, like slaves for bonding\n *\t@all_adj_list:\tAll linked devices, *including* neighbours\n *\t@features:\tCurrently active device features\n *\t@hw_features:\tUser-changeable features\n *\n *\t@wanted_features:\tUser-requested features\n *\t@vlan_features:\t\tMask of features inheritable by VLAN devices\n *\n *\t@hw_enc_features:\tMask of features inherited by encapsulating devices\n *\t\t\t\tThis field indicates what encapsulation\n *\t\t\t\toffloads the hardware is capable of doing,\n *\t\t\t\tand drivers will need to set them appropriately.\n *\n *\t@mpls_features:\tMask of features inheritable by MPLS\n *\n *\t@ifindex:\tinterface index\n *\t@group:\t\tThe group, that the device belongs to\n *\n *\t@stats:\t\tStatistics struct, which was left as a legacy, use\n *\t\t\trtnl_link_stats64 instead\n *\n *\t@rx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@tx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@rx_nohandler:\tnohandler dropped packets by core network on\n *\t\t\tinactive devices, do not use this in drivers\n *\n *\t@wireless_handlers:\tList of functions to handle Wireless Extensions,\n *\t\t\t\tinstead of ioctl,\n *\t\t\t\tsee <net/iw_handler.h> for details.\n *\t@wireless_data:\tInstance data managed by the core of wireless extensions\n *\n *\t@netdev_ops:\tIncludes several pointers to callbacks,\n *\t\t\tif one wants to override the ndo_*() functions\n *\t@ethtool_ops:\tManagement operations\n *\t@header_ops:\tIncludes callbacks for creating,parsing,caching,etc\n *\t\t\tof Layer 2 headers.\n *\n *\t@flags:\t\tInterface flags (a la BSD)\n *\t@priv_flags:\tLike 'flags' but invisible to userspace,\n *\t\t\tsee if.h for the definitions\n *\t@gflags:\tGlobal flags ( kept as legacy )\n *\t@padded:\tHow much padding added by alloc_netdev()\n *\t@operstate:\tRFC2863 operstate\n *\t@link_mode:\tMapping policy to operstate\n *\t@if_port:\tSelectable AUI, TP, ...\n *\t@dma:\t\tDMA channel\n *\t@mtu:\t\tInterface MTU value\n *\t@type:\t\tInterface hardware type\n *\t@hard_header_len: Maximum hardware header length.\n *\n *\t@needed_headroom: Extra headroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed\n *\t@needed_tailroom: Extra tailroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed. Some cases also use\n *\t\t\t  LL_MAX_HEADER instead to allocate the skb\n *\n *\tinterface address info:\n *\n * \t@perm_addr:\t\tPermanent hw address\n * \t@addr_assign_type:\tHw address assignment type\n * \t@addr_len:\t\tHardware address length\n * \t@neigh_priv_len;\tUsed in neigh_alloc(),\n * \t\t\t\tinitialized only in atm/clip.c\n * \t@dev_id:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same link layer address\n * \t@dev_port:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same function\n *\t@addr_list_lock:\tXXX: need comments on this one\n *\t@uc_promisc:\t\tCounter, that indicates, that promiscuous mode\n *\t\t\t\thas been enabled due to the need to listen to\n *\t\t\t\tadditional unicast addresses in a device that\n *\t\t\t\tdoes not implement ndo_set_rx_mode()\n *\t@uc:\t\t\tunicast mac addresses\n *\t@mc:\t\t\tmulticast mac addresses\n *\t@dev_addrs:\t\tlist of device hw addresses\n *\t@queues_kset:\t\tGroup of all Kobjects in the Tx and RX queues\n *\t@promiscuity:\t\tNumber of times, the NIC is told to work in\n *\t\t\t\tPromiscuous mode, if it becomes 0 the NIC will\n *\t\t\t\texit from working in Promiscuous mode\n *\t@allmulti:\t\tCounter, enables or disables allmulticast mode\n *\n *\t@vlan_info:\tVLAN info\n *\t@dsa_ptr:\tdsa specific data\n *\t@tipc_ptr:\tTIPC specific data\n *\t@atalk_ptr:\tAppleTalk link\n *\t@ip_ptr:\tIPv4 specific data\n *\t@dn_ptr:\tDECnet specific data\n *\t@ip6_ptr:\tIPv6 specific data\n *\t@ax25_ptr:\tAX.25 specific data\n *\t@ieee80211_ptr:\tIEEE 802.11 specific data, assign before registering\n *\n *\t@last_rx:\tTime of last Rx\n *\t@dev_addr:\tHw address (before bcast,\n *\t\t\tbecause most packets are unicast)\n *\n *\t@_rx:\t\t\tArray of RX queues\n *\t@num_rx_queues:\t\tNumber of RX queues\n *\t\t\t\tallocated at register_netdev() time\n *\t@real_num_rx_queues: \tNumber of RX queues currently active in device\n *\n *\t@rx_handler:\t\thandler for received packets\n *\t@rx_handler_data: \tXXX: need comments on this one\n *\t@ingress_queue:\t\tXXX: need comments on this one\n *\t@broadcast:\t\thw bcast address\n *\n *\t@rx_cpu_rmap:\tCPU reverse-mapping for RX completion interrupts,\n *\t\t\tindexed by RX queue number. Assigned by driver.\n *\t\t\tThis must only be set if the ndo_rx_flow_steer\n *\t\t\toperation is defined\n *\t@index_hlist:\t\tDevice index hash chain\n *\n *\t@_tx:\t\t\tArray of TX queues\n *\t@num_tx_queues:\t\tNumber of TX queues allocated at alloc_netdev_mq() time\n *\t@real_num_tx_queues: \tNumber of TX queues currently active in device\n *\t@qdisc:\t\t\tRoot qdisc from userspace point of view\n *\t@tx_queue_len:\t\tMax frames per queue allowed\n *\t@tx_global_lock: \tXXX: need comments on this one\n *\n *\t@xps_maps:\tXXX: need comments on this one\n *\n *\t@offload_fwd_mark:\tOffload device fwding mark\n *\n *\t@trans_start:\t\tTime (in jiffies) of last Tx\n *\t@watchdog_timeo:\tRepresents the timeout that is used by\n *\t\t\t\tthe watchdog ( see dev_watchdog() )\n *\t@watchdog_timer:\tList of timers\n *\n *\t@pcpu_refcnt:\t\tNumber of references to this device\n *\t@todo_list:\t\tDelayed register/unregister\n *\t@link_watch_list:\tXXX: need comments on this one\n *\n *\t@reg_state:\t\tRegister/unregister state machine\n *\t@dismantle:\t\tDevice is going to be freed\n *\t@rtnl_link_state:\tThis enum represents the phases of creating\n *\t\t\t\ta new link\n *\n *\t@destructor:\t\tCalled from unregister,\n *\t\t\t\tcan be used to call free_netdev\n *\t@npinfo:\t\tXXX: need comments on this one\n * \t@nd_net:\t\tNetwork namespace this network device is inside\n *\n * \t@ml_priv:\tMid-layer private\n * \t@lstats:\tLoopback statistics\n * \t@tstats:\tTunnel statistics\n * \t@dstats:\tDummy statistics\n * \t@vstats:\tVirtual ethernet statistics\n *\n *\t@garp_port:\tGARP\n *\t@mrp_port:\tMRP\n *\n *\t@dev:\t\tClass/net/name entry\n *\t@sysfs_groups:\tSpace for optional device, statistics and wireless\n *\t\t\tsysfs groups\n *\n *\t@sysfs_rx_queue_group:\tSpace for optional per-rx queue attributes\n *\t@rtnl_link_ops:\tRtnl_link_ops\n *\n *\t@gso_max_size:\tMaximum size of generic segmentation offload\n *\t@gso_max_segs:\tMaximum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\t@gso_min_segs:\tMinimum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\n *\t@dcbnl_ops:\tData Center Bridging netlink ops\n *\t@num_tc:\tNumber of traffic classes in the net device\n *\t@tc_to_txq:\tXXX: need comments on this one\n *\t@prio_tc_map\tXXX: need comments on this one\n *\n *\t@fcoe_ddp_xid:\tMax exchange id for FCoE LRO by ddp\n *\n *\t@priomap:\tXXX: need comments on this one\n *\t@phydev:\tPhysical device may attach itself\n *\t\t\tfor hardware timestamping\n *\n *\t@qdisc_tx_busylock:\tXXX: need comments on this one\n *\n *\t@proto_down:\tprotocol port state information can be sent to the\n *\t\t\tswitch driver and used to set the phys state of the\n *\t\t\tswitch port.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\tchar\t\t\tname[IFNAMSIZ];\n\tstruct hlist_node\tname_hlist;\n\tchar \t\t\t*ifalias;\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\n\tunsigned long\t\tmem_start;\n\tunsigned long\t\tbase_addr;\n\tint\t\t\tirq;\n\n\tatomic_t\t\tcarrier_changes;\n\n\t/*\n\t *\tSome hardware also needs these fields (state,dev_list,\n\t *\tnapi_list,unreg_list,close_list) but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\tstruct list_head\tclose_list;\n\tstruct list_head\tptype_all;\n\tstruct list_head\tptype_specific;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} adj_list;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} all_adj_list;\n\n\tnetdev_features_t\tfeatures;\n\tnetdev_features_t\thw_features;\n\tnetdev_features_t\twanted_features;\n\tnetdev_features_t\tvlan_features;\n\tnetdev_features_t\thw_enc_features;\n\tnetdev_features_t\tmpls_features;\n\n\tint\t\t\tifindex;\n\tint\t\t\tgroup;\n\n\tstruct net_device_stats\tstats;\n\n\tatomic_long_t\t\trx_dropped;\n\tatomic_long_t\t\ttx_dropped;\n\tatomic_long_t\t\trx_nohandler;\n\n#ifdef CONFIG_WIRELESS_EXT\n\tconst struct iw_handler_def *\twireless_handlers;\n\tstruct iw_public_data *\twireless_data;\n#endif\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_SWITCHDEV\n\tconst struct switchdev_ops *switchdev_ops;\n#endif\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tconst struct l3mdev_ops\t*l3mdev_ops;\n#endif\n\n\tconst struct header_ops *header_ops;\n\n\tunsigned int\t\tflags;\n\tunsigned int\t\tpriv_flags;\n\n\tunsigned short\t\tgflags;\n\tunsigned short\t\tpadded;\n\n\tunsigned char\t\toperstate;\n\tunsigned char\t\tlink_mode;\n\n\tunsigned char\t\tif_port;\n\tunsigned char\t\tdma;\n\n\tunsigned int\t\tmtu;\n\tunsigned short\t\ttype;\n\tunsigned short\t\thard_header_len;\n\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN];\n\tunsigned char\t\taddr_assign_type;\n\tunsigned char\t\taddr_len;\n\tunsigned short\t\tneigh_priv_len;\n\tunsigned short          dev_id;\n\tunsigned short          dev_port;\n\tspinlock_t\t\taddr_list_lock;\n\tunsigned char\t\tname_assign_type;\n\tbool\t\t\tuc_promisc;\n\tstruct netdev_hw_addr_list\tuc;\n\tstruct netdev_hw_addr_list\tmc;\n\tstruct netdev_hw_addr_list\tdev_addrs;\n\n#ifdef CONFIG_SYSFS\n\tstruct kset\t\t*queues_kset;\n#endif\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\n\n\t/* Protocol specific pointers */\n\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\n\tstruct vlan_info __rcu\t*vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tstruct dsa_switch_tree\t*dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\n\tstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n\tvoid \t\t\t*atalk_ptr;\n\tstruct in_device __rcu\t*ip_ptr;\n\tstruct dn_dev __rcu     *dn_ptr;\n\tstruct inet6_dev __rcu\t*ip6_ptr;\n\tvoid\t\t\t*ax25_ptr;\n\tstruct wireless_dev\t*ieee80211_ptr;\n\tstruct wpan_dev\t\t*ieee802154_ptr;\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\n\tstruct mpls_dev __rcu\t*mpls_ptr;\n#endif\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\tunsigned long\t\tlast_rx;\n\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\n\n\n#ifdef CONFIG_SYSFS\n\tstruct netdev_rx_queue\t*_rx;\n\n\tunsigned int\t\tnum_rx_queues;\n\tunsigned int\t\treal_num_rx_queues;\n\n#endif\n\n\tunsigned long\t\tgro_flush_timeout;\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *ingress_cl_list;\n#endif\n\tstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\n\tstruct list_head\tnf_hooks_ingress;\n#endif\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap\t\t*rx_cpu_rmap;\n#endif\n\tstruct hlist_node\tindex_hlist;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\tunsigned int\t\tnum_tx_queues;\n\tunsigned int\t\treal_num_tx_queues;\n\tstruct Qdisc\t\t*qdisc;\n\tunsigned long\t\ttx_queue_len;\n\tspinlock_t\t\ttx_global_lock;\n\tint\t\t\twatchdog_timeo;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *egress_cl_list;\n#endif\n#ifdef CONFIG_NET_SWITCHDEV\n\tu32\t\t\toffload_fwd_mark;\n#endif\n\n\t/* These may be needed for future network-power-down code. */\n\n\t/*\n\t * trans_start here is expensive for high speed devices on SMP,\n\t * please use netdev_queue->trans_start instead.\n\t */\n\tunsigned long\t\ttrans_start;\n\n\tstruct timer_list\twatchdog_timer;\n\n\tint __percpu\t\t*pcpu_refcnt;\n\tstruct list_head\ttodo_list;\n\n\tstruct list_head\tlink_watch_list;\n\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:8;\n\n\tbool dismantle;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\tvoid (*destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info __rcu\t*npinfo;\n#endif\n\n\tpossible_net_t\t\t\tnd_net;\n\n\t/* mid-layer private */\n\tunion {\n\t\tvoid\t\t\t\t\t*ml_priv;\n\t\tstruct pcpu_lstats __percpu\t\t*lstats;\n\t\tstruct pcpu_sw_netstats __percpu\t*tstats;\n\t\tstruct pcpu_dstats __percpu\t\t*dstats;\n\t\tstruct pcpu_vstats __percpu\t\t*vstats;\n\t};\n\n\tstruct garp_port __rcu\t*garp_port;\n\tstruct mrp_port __rcu\t*mrp_port;\n\n\tstruct device\tdev;\n\tconst struct attribute_group *sysfs_groups[4];\n\tconst struct attribute_group *sysfs_rx_queue_group;\n\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n#define GSO_MAX_SEGS\t\t65535\n\tu16\t\t\tgso_max_segs;\n\tu16\t\t\tgso_min_segs;\n#ifdef CONFIG_DCB\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\tu8 num_tc;\n\tstruct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];\n\tu8 prio_tc_map[TC_BITMASK + 1];\n\n#if IS_ENABLED(CONFIG_FCOE)\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\n\tstruct netprio_map __rcu *priomap;\n#endif\n\tstruct phy_device *phydev;\n\tstruct lock_class_key *qdisc_tx_busylock;\n\tbool proto_down;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nint netdev_get_prio_tc_map(const struct net_device *dev, u32 prio)\n{\n\treturn dev->prio_tc_map[prio & TC_BITMASK];\n}\n\nstatic inline\nint netdev_set_prio_tc_map(struct net_device *dev, u8 prio, u8 tc)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->prio_tc_map[prio & TC_BITMASK] = tc & TC_BITMASK;\n\treturn 0;\n}\n\nstatic inline\nvoid netdev_reset_tc(struct net_device *dev)\n{\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\n\nstatic inline\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\n\nstatic inline\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\n\nstatic inline\nint netdev_get_num_tc(struct net_device *dev)\n{\n\treturn dev->num_tc;\n}\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline struct netdev_queue *skb_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t\t    const struct sk_buff *skb)\n{\n\treturn netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv);\n\n/* returns the headroom that the master device needs to take in account\n * when forwarding to this dev\n */\nstatic inline unsigned netdev_get_fwd_headroom(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_PHONY_HEADROOM ? 0 : dev->needed_headroom;\n}\n\nstatic inline void netdev_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tif (dev->netdev_ops->ndo_set_rx_headroom)\n\t\tdev->netdev_ops->ndo_set_rx_headroom(dev, new_hr);\n}\n\n/* set the device rx headroom to the dev's default */\nstatic inline void netdev_reset_rx_headroom(struct net_device *dev)\n{\n\tnetdev_set_rx_headroom(dev, -1);\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n\twrite_pnet(&dev->nd_net, net);\n}\n\nstatic inline bool netdev_uses_dsa(struct net_device *dev)\n{\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tif (dev->dsa_ptr != NULL)\n\t\treturn dsa_uses_tagged_protocol(dev->dsa_ptr);\n#endif\n\treturn false;\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fine-grained identification of different network device types. For\n * example Ethernet, Wirelss LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/* Default NAPI poll() weight\n * Device drivers are strongly advised to not use bigger value\n */\n#define NAPI_POLL_WEIGHT 64\n\n/**\n *\tnetif_napi_add - initialize a napi context\n *\t@dev:  network device\n *\t@napi: napi context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a napi context prior to calling\n * *any* of the other napi related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *\tnetif_tx_napi_add - initialize a napi context\n *\t@dev:  network device\n *\t@napi: napi context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * This variant of netif_napi_add() should be used from drivers using NAPI\n * to exclusively poll a TX queue.\n * This will avoid we add it into napi_hash[], thus polluting this hash table.\n */\nstatic inline void netif_tx_napi_add(struct net_device *dev,\n\t\t\t\t     struct napi_struct *napi,\n\t\t\t\t     int (*poll)(struct napi_struct *, int),\n\t\t\t\t     int weight)\n{\n\tset_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state);\n\tnetif_napi_add(dev, napi, poll, weight);\n}\n\n/**\n *  netif_napi_del - remove a napi context\n *  @napi: napi context\n *\n *  netif_napi_del() removes a napi context from the network device napi list\n */\nvoid netif_napi_del(struct napi_struct *napi);\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid *frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint data_offset;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tu16\tflush;\n\n\t/* Save the IP ID here and check when we get to the transport layer */\n\tu16\tflush_id;\n\n\t/* Number of segments aggregated. */\n\tu16\tcount;\n\n\t/* Start offset for remote checksum offload */\n\tu16\tgro_remcsum_start;\n\n\t/* jiffies when first packet was created/queued */\n\tunsigned long age;\n\n\t/* Used in ipv6_gro_receive() and foo-over-udp */\n\tu16\tproto;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tu8\tsame_flow:1;\n\n\t/* Used in tunnel GRO receive */\n\tu8\tencap_mark:1;\n\n\t/* GRO checksum is valid */\n\tu8\tcsum_valid:1;\n\n\t/* Number of checksums via CHECKSUM_UNNECESSARY */\n\tu8\tcsum_cnt:3;\n\n\t/* Free the skb? */\n\tu8\tfree:2;\n#define NAPI_GRO_FREE\t\t  1\n#define NAPI_GRO_FREE_STOLEN_HEAD 2\n\n\t/* Used in foo-over-udp, set in udp[46]_gro_receive */\n\tu8\tis_ipv6:1;\n\n\t/* 7 bit hole */\n\n\t/* used to support CHECKSUM_COMPLETE for tunneling protocols */\n\t__wsum\tcsum;\n\n\t/* used in skb_gro_receive() slow path */\n\tstruct sk_buff *last;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tbool\t\t\t(*id_match)(struct packet_type *ptype,\n\t\t\t\t\t    struct sock *sk);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\nstruct offload_callbacks {\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tnetdev_features_t features);\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t\t struct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb, int nhoff);\n};\n\nstruct packet_offload {\n\t__be16\t\t\t type;\t/* This is really htons(ether_type). */\n\tu16\t\t\t priority;\n\tstruct offload_callbacks callbacks;\n\tstruct list_head\t list;\n};\n\nstruct udp_offload;\n\nstruct udp_offload_callbacks {\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t\t struct udp_offload *uoff);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb,\n\t\t\t\t\t\tint nhoff,\n\t\t\t\t\t\tstruct udp_offload *uoff);\n};\n\nstruct udp_offload {\n\t__be16\t\t\t port;\n\tu8\t\t\t ipproto;\n\tstruct udp_offload_callbacks callbacks;\n};\n\n/* often modified stats are per cpu, other are shared (netdev->stats) */\nstruct pcpu_sw_netstats {\n\tu64     rx_packets;\n\tu64     rx_bytes;\n\tu64     tx_packets;\n\tu64     tx_bytes;\n\tstruct u64_stats_sync   syncp;\n};\n\n#define __netdev_alloc_pcpu_stats(type, gfp)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = alloc_percpu_gfp(type, gfp);\\\n\tif (pcpu_stats)\t{\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\n#define netdev_alloc_pcpu_stats(type)\t\t\t\t\t\\\n\t__netdev_alloc_pcpu_stats(type, GFP_KERNEL)\n\nenum netdev_lag_tx_type {\n\tNETDEV_LAG_TX_TYPE_UNKNOWN,\n\tNETDEV_LAG_TX_TYPE_RANDOM,\n\tNETDEV_LAG_TX_TYPE_BROADCAST,\n\tNETDEV_LAG_TX_TYPE_ROUNDROBIN,\n\tNETDEV_LAG_TX_TYPE_ACTIVEBACKUP,\n\tNETDEV_LAG_TX_TYPE_HASH,\n};\n\nstruct netdev_lag_upper_info {\n\tenum netdev_lag_tx_type tx_type;\n};\n\nstruct netdev_lag_lower_state_info {\n\tu8 link_up : 1,\n\t   tx_enabled : 1;\n};\n\n#include <linux/notifier.h>\n\n/* netdevice notifier chain. Please remember to update the rtnetlink\n * notification exclusion list in rtnetlink_event() when adding new\n * types.\n */\n#define NETDEV_UP\t0x0001\t/* For now you can't veto a device up/down */\n#define NETDEV_DOWN\t0x0002\n#define NETDEV_REBOOT\t0x0003\t/* Tell a protocol stack a network interface\n\t\t\t\t   detected a hardware crash and restarted\n\t\t\t\t   - we can use this eg to kick tcp sessions\n\t\t\t\t   once done */\n#define NETDEV_CHANGE\t0x0004\t/* Notify device state change */\n#define NETDEV_REGISTER 0x0005\n#define NETDEV_UNREGISTER\t0x0006\n#define NETDEV_CHANGEMTU\t0x0007 /* notify after mtu change happened */\n#define NETDEV_CHANGEADDR\t0x0008\n#define NETDEV_GOING_DOWN\t0x0009\n#define NETDEV_CHANGENAME\t0x000A\n#define NETDEV_FEAT_CHANGE\t0x000B\n#define NETDEV_BONDING_FAILOVER 0x000C\n#define NETDEV_PRE_UP\t\t0x000D\n#define NETDEV_PRE_TYPE_CHANGE\t0x000E\n#define NETDEV_POST_TYPE_CHANGE\t0x000F\n#define NETDEV_POST_INIT\t0x0010\n#define NETDEV_UNREGISTER_FINAL 0x0011\n#define NETDEV_RELEASE\t\t0x0012\n#define NETDEV_NOTIFY_PEERS\t0x0013\n#define NETDEV_JOIN\t\t0x0014\n#define NETDEV_CHANGEUPPER\t0x0015\n#define NETDEV_RESEND_IGMP\t0x0016\n#define NETDEV_PRECHANGEMTU\t0x0017 /* notify before mtu change happened */\n#define NETDEV_CHANGEINFODATA\t0x0018\n#define NETDEV_BONDING_INFO\t0x0019\n#define NETDEV_PRECHANGEUPPER\t0x001A\n#define NETDEV_CHANGELOWERSTATE\t0x001B\n\nint register_netdevice_notifier(struct notifier_block *nb);\nint unregister_netdevice_notifier(struct notifier_block *nb);\n\nstruct netdev_notifier_info {\n\tstruct net_device *dev;\n};\n\nstruct netdev_notifier_change_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tunsigned int flags_changed;\n};\n\nstruct netdev_notifier_changeupper_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct net_device *upper_dev; /* new upper dev */\n\tbool master; /* is upper dev master */\n\tbool linking; /* is the nofication for link or unlink */\n\tvoid *upper_info; /* upper dev info */\n};\n\nstruct netdev_notifier_changelowerstate_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tvoid *lower_state_info; /* is lower dev state */\n};\n\nstatic inline void netdev_notifier_info_init(struct netdev_notifier_info *info,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tinfo->dev = dev;\n}\n\nstatic inline struct net_device *\nnetdev_notifier_info_to_dev(const struct netdev_notifier_info *info)\n{\n\treturn info->dev;\n}\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev);\n\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_in_bond_rcu(bond, slave)\t\\\n\t\tfor_each_netdev_rcu(&init_net, slave)\t\\\n\t\t\tif (netdev_master_upper_dev_get_rcu(slave) == (bond))\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(list_next_rcu(&dev->dev_list));\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nstatic inline struct net_device *first_net_device_rcu(struct net *net)\n{\n\tstruct list_head *lh = rcu_dereference(list_next_rcu(&net->dev_base_head));\n\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nint netdev_boot_setup_check(struct net_device *dev);\nunsigned long netdev_boot_base(const char *prefix, int unit);\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *hwaddr);\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type);\nvoid dev_add_pack(struct packet_type *pt);\nvoid dev_remove_pack(struct packet_type *pt);\nvoid __dev_remove_pack(struct packet_type *pt);\nvoid dev_add_offload(struct packet_offload *po);\nvoid dev_remove_offload(struct packet_offload *po);\n\nint dev_get_iflink(const struct net_device *dev);\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb);\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short flags,\n\t\t\t\t      unsigned short mask);\nstruct net_device *dev_get_by_name(struct net *net, const char *name);\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name);\nstruct net_device *__dev_get_by_name(struct net *net, const char *name);\nint dev_alloc_name(struct net_device *dev, const char *name);\nint dev_open(struct net_device *dev);\nint dev_close(struct net_device *dev);\nint dev_close_many(struct list_head *head, bool unlink);\nvoid dev_disable_lro(struct net_device *dev);\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *newskb);\nint dev_queue_xmit(struct sk_buff *skb);\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv);\nint register_netdevice(struct net_device *dev);\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head);\nvoid unregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nint netdev_refcnt_read(const struct net_device *dev);\nvoid free_netdev(struct net_device *dev);\nvoid netdev_freemem(struct net_device *dev);\nvoid synchronize_net(void);\nint init_dummy_netdev(struct net_device *dev);\n\nDECLARE_PER_CPU(int, xmit_recursion);\nstatic inline int dev_recursion_level(void)\n{\n\treturn this_cpu_read(xmit_recursion);\n}\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex);\nint netdev_get_name(struct net *net, char *name, int ifindex);\nint dev_restart(struct net_device *dev);\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tif (!pskb_may_pull(skb, hlen))\n\t\treturn NULL;\n\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\treturn skb->data + offset;\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline void skb_gro_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t\tconst void *start, unsigned int len)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid)\n\t\tNAPI_GRO_CB(skb)->csum = csum_sub(NAPI_GRO_CB(skb)->csum,\n\t\t\t\t\t\t  csum_partial(start, len, 0));\n}\n\n/* GRO checksum functions. These are logical equivalents of the normal\n * checksum functions (in skbuff.h) except that they operate on the GRO\n * offsets and fields in sk_buff.\n */\n\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb);\n\nstatic inline bool skb_at_gro_remcsum_start(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->gro_remcsum_start == skb_gro_offset(skb));\n}\n\nstatic inline bool __skb_gro_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t      bool zero_okay,\n\t\t\t\t\t\t      __sum16 check)\n{\n\treturn ((skb->ip_summed != CHECKSUM_PARTIAL ||\n\t\tskb_checksum_start_offset(skb) <\n\t\t skb_gro_offset(skb)) &&\n\t\t!skb_at_gro_remcsum_start(skb) &&\n\t\tNAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t(!zero_okay || check));\n}\n\nstatic inline __sum16 __skb_gro_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t\t   __wsum psum)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid &&\n\t    !csum_fold(csum_add(psum, NAPI_GRO_CB(skb)->csum)))\n\t\treturn 0;\n\n\tNAPI_GRO_CB(skb)->csum = psum;\n\n\treturn __skb_gro_checksum_complete(skb);\n}\n\nstatic inline void skb_gro_incr_csum_unnecessary(struct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->csum_cnt > 0) {\n\t\t/* Consume a checksum from CHECKSUM_UNNECESSARY */\n\t\tNAPI_GRO_CB(skb)->csum_cnt--;\n\t} else {\n\t\t/* Update skb for CHECKSUM_UNNECESSARY and csum_level when we\n\t\t * verified a new top level checksum or an encapsulated one\n\t\t * during GRO. This saves work if we fallback to normal path.\n\t\t */\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t}\n}\n\n#define __skb_gro_checksum_validate(skb, proto, zero_okay, check,\t\\\n\t\t\t\t    compute_pseudo)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_gro_checksum_validate_complete(skb,\t\\\n\t\t\t\tcompute_pseudo(skb, proto));\t\t\\\n\tif (__ret)\t\t\t\t\t\t\t\\\n\t\t__skb_mark_checksum_bad(skb);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tskb_gro_incr_csum_unnecessary(skb);\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_gro_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, false, 0, compute_pseudo)\n\n#define skb_gro_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t     compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, true, check, compute_pseudo)\n\n#define skb_gro_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_gro_checksum_validate(skb, 0, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_gro_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t!NAPI_GRO_CB(skb)->csum_valid);\n}\n\nstatic inline void __skb_gro_checksum_convert(struct sk_buff *skb,\n\t\t\t\t\t      __sum16 check, __wsum pseudo)\n{\n\tNAPI_GRO_CB(skb)->csum = ~pseudo;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n}\n\n#define skb_gro_checksum_try_convert(skb, proto, check, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_convert_check(skb))\t\t\t\\\n\t\t__skb_gro_checksum_convert(skb, check,\t\t\t\\\n\t\t\t\t\t   compute_pseudo(skb, proto));\t\\\n} while (0)\n\nstruct gro_remcsum {\n\tint offset;\n\t__wsum delta;\n};\n\nstatic inline void skb_gro_remcsum_init(struct gro_remcsum *grc)\n{\n\tgrc->offset = 0;\n\tgrc->delta = 0;\n}\n\nstatic inline void *skb_gro_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t    unsigned int off, size_t hdrlen,\n\t\t\t\t\t    int start, int offset,\n\t\t\t\t\t    struct gro_remcsum *grc,\n\t\t\t\t\t    bool nopartial)\n{\n\t__wsum delta;\n\tsize_t plen = hdrlen + max_t(size_t, offset + sizeof(u16), start);\n\n\tBUG_ON(!NAPI_GRO_CB(skb)->csum_valid);\n\n\tif (!nopartial) {\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = off + hdrlen + start;\n\t\treturn ptr;\n\t}\n\n\tptr = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, off + plen)) {\n\t\tptr = skb_gro_header_slow(skb, off + plen, off);\n\t\tif (!ptr)\n\t\t\treturn NULL;\n\t}\n\n\tdelta = remcsum_adjust(ptr + hdrlen, NAPI_GRO_CB(skb)->csum,\n\t\t\t       start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tNAPI_GRO_CB(skb)->csum = csum_add(NAPI_GRO_CB(skb)->csum, delta);\n\n\tgrc->offset = off + hdrlen + offset;\n\tgrc->delta = delta;\n\n\treturn ptr;\n}\n\nstatic inline void skb_gro_remcsum_cleanup(struct sk_buff *skb,\n\t\t\t\t\t   struct gro_remcsum *grc)\n{\n\tvoid *ptr;\n\tsize_t plen = grc->offset + sizeof(u16);\n\n\tif (!grc->delta)\n\t\treturn;\n\n\tptr = skb_gro_header_fast(skb, grc->offset);\n\tif (skb_gro_header_hard(skb, grc->offset + sizeof(u16))) {\n\t\tptr = skb_gro_header_slow(skb, plen, grc->offset);\n\t\tif (!ptr)\n\t\t\treturn;\n\t}\n\n\tremcsum_unadjust((__sum16 *)ptr, grc->delta);\n}\n\nstruct skb_csum_offl_spec {\n\t__u16\t\tipv4_okay:1,\n\t\t\tipv6_okay:1,\n\t\t\tencap_okay:1,\n\t\t\tip_options_okay:1,\n\t\t\text_hdrs_okay:1,\n\t\t\ttcp_okay:1,\n\t\t\tudp_okay:1,\n\t\t\tsctp_okay:1,\n\t\t\tvlan_okay:1,\n\t\t\tno_encapped_ipv6:1,\n\t\t\tno_not_encapped:1;\n};\n\nbool __skb_csum_offload_chk(struct sk_buff *skb,\n\t\t\t    const struct skb_csum_offl_spec *spec,\n\t\t\t    bool *csum_encapped,\n\t\t\t    bool csum_help);\n\nstatic inline bool skb_csum_offload_chk(struct sk_buff *skb,\n\t\t\t\t\tconst struct skb_csum_offl_spec *spec,\n\t\t\t\t\tbool *csum_encapped,\n\t\t\t\t\tbool csum_help)\n{\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn false;\n\n\treturn __skb_csum_offload_chk(skb, spec, csum_encapped, csum_help);\n}\n\nstatic inline bool skb_csum_offload_chk_help(struct sk_buff *skb,\n\t\t\t\t\t     const struct skb_csum_offl_spec *spec)\n{\n\tbool csum_encapped;\n\n\treturn skb_csum_offload_chk(skb, spec, &csum_encapped, true);\n}\n\nstatic inline bool skb_csum_off_chk_help_cmn(struct sk_buff *skb)\n{\n\tstatic const struct skb_csum_offl_spec csum_offl_spec = {\n\t\t.ipv4_okay = 1,\n\t\t.ip_options_okay = 1,\n\t\t.ipv6_okay = 1,\n\t\t.vlan_okay = 1,\n\t\t.tcp_okay = 1,\n\t\t.udp_okay = 1,\n\t};\n\n\treturn skb_csum_offload_chk_help(skb, &csum_offl_spec);\n}\n\nstatic inline bool skb_csum_off_chk_help_cmn_v4_only(struct sk_buff *skb)\n{\n\tstatic const struct skb_csum_offl_spec csum_offl_spec = {\n\t\t.ipv4_okay = 1,\n\t\t.ip_options_okay = 1,\n\t\t.tcp_okay = 1,\n\t\t.udp_okay = 1,\n\t\t.vlan_okay = 1,\n\t};\n\n\treturn skb_csum_offload_chk_help(skb, &csum_offl_spec);\n}\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned int len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\n/* ll_header must have at least hard_header_len allocated */\nstatic inline bool dev_validate_header(const struct net_device *dev,\n\t\t\t\t       char *ll_header, int len)\n{\n\tif (likely(len >= dev->hard_header_len))\n\t\treturn true;\n\n\tif (capable(CAP_SYS_RAWIO)) {\n\t\tmemset(ll_header + len, 0, dev->hard_header_len - len);\n\t\treturn true;\n\t}\n\n\tif (dev->header_ops && dev->header_ops->validate)\n\t\treturn dev->header_ops->validate(ll_header, len);\n\n\treturn false;\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr, int len);\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\n#define FLOW_LIMIT_HISTORY\t(1 << 7)  /* must be ^2 and !overflow buckets */\nstruct sd_flow_limit {\n\tu64\t\t\tcount;\n\tunsigned int\t\tnum_buckets;\n\tunsigned int\t\thistory_head;\n\tu16\t\t\thistory[FLOW_LIMIT_HISTORY];\n\tu8\t\t\tbuckets[];\n};\n\nextern int netdev_flow_limit_table_len;\n#endif /* CONFIG_NET_FLOW_LIMIT */\n\n/*\n * Incoming packets are placed on per-cpu queues\n */\nstruct softnet_data {\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\tcpu_collision;\n\tunsigned int\t\treceived_rps;\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n#endif\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit __rcu *flow_limit;\n#endif\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct sk_buff\t\t*completion_queue;\n\n#ifdef CONFIG_RPS\n\t/* Elements below can be accessed between CPUs for RPS */\n\tstruct call_single_data\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_head;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned int\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\nvoid __netif_schedule(struct Qdisc *q);\nvoid netif_schedule_queue(struct netdev_queue *txq);\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue);\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tset_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev);\n\nstatic inline bool netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline bool netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline bool netif_xmit_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF_OR_FROZEN;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_drv_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_DRV_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetdev_txq_bql_enqueue_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their ndo_start_xmit(),\n * to give appropriate hint to the cpu.\n */\nstatic inline void netdev_txq_bql_enqueue_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.num_queued);\n#endif\n}\n\n/**\n *\tnetdev_txq_bql_complete_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their TX completion path,\n * to give appropriate hint to the cpu.\n */\nstatic inline void netdev_txq_bql_complete_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.limit);\n#endif\n}\n\nstatic inline void netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\tunsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tdql_queued(&dev_queue->dql, bytes);\n\n\tif (likely(dql_avail(&dev_queue->dql) >= 0))\n\t\treturn;\n\n\tset_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n\n\t/*\n\t * The XOFF flag must be set before checking the dql_avail below,\n\t * because in netdev_tx_completed_queue we update the dql_completed\n\t * before checking the XOFF flag.\n\t */\n\tsmp_mb();\n\n\t/* check again in case another CPU has just made room avail */\n\tif (unlikely(dql_avail(&dev_queue->dql) >= 0))\n\t\tclear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n#endif\n}\n\n/**\n * \tnetdev_sent_queue - report the number of bytes queued to hardware\n * \t@dev: network device\n * \t@bytes: number of bytes queued to the hardware device queue\n *\n * \tReport the number of bytes queued for sending/completion to the network\n * \tdevice hardware queue. @bytes should be a good approximation and should\n * \texactly match netdev_completed_queue() @bytes\n */\nstatic inline void netdev_sent_queue(struct net_device *dev, unsigned int bytes)\n{\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes);\n}\n\nstatic inline void netdev_tx_completed_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t     unsigned int pkts, unsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tif (unlikely(!bytes))\n\t\treturn;\n\n\tdql_completed(&dev_queue->dql, bytes);\n\n\t/*\n\t * Without the memory barrier there is a small possiblity that\n\t * netdev_tx_sent_queue will miss the update and cause the queue to\n\t * be stopped forever\n\t */\n\tsmp_mb();\n\n\tif (dql_avail(&dev_queue->dql) < 0)\n\t\treturn;\n\n\tif (test_and_clear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state))\n\t\tnetif_schedule_queue(dev_queue);\n#endif\n}\n\n/**\n * \tnetdev_completed_queue - report bytes and packets completed by device\n * \t@dev: network device\n * \t@pkts: actual number of packets sent over the medium\n * \t@bytes: actual number of bytes sent over the medium\n *\n * \tReport the number of bytes and packets transmitted by the network device\n * \thardware queue over the physical medium, @bytes must exactly match the\n * \t@bytes amount passed to netdev_sent_queue()\n */\nstatic inline void netdev_completed_queue(struct net_device *dev,\n\t\t\t\t\t  unsigned int pkts, unsigned int bytes)\n{\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, 0), pkts, bytes);\n}\n\nstatic inline void netdev_tx_reset_queue(struct netdev_queue *q)\n{\n#ifdef CONFIG_BQL\n\tclear_bit(__QUEUE_STATE_STACK_XOFF, &q->state);\n\tdql_reset(&q->dql);\n#endif\n}\n\n/**\n * \tnetdev_reset_queue - reset the packets and bytes count of a network device\n * \t@dev_queue: network device\n *\n * \tReset the bytes and packet count of a network device and clear the\n * \tsoftware flow control OFF bit for this network device\n */\nstatic inline void netdev_reset_queue(struct net_device *dev_queue)\n{\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));\n}\n\n/**\n * \tnetdev_cap_txqueue - check if selected tx queue exceeds device queues\n * \t@dev: network device\n * \t@queue_index: given tx queue index\n *\n * \tReturns 0 if given tx queue index >= number of device tx queues,\n * \totherwise returns the originally passed tx queue index.\n */\nstatic inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tnet_warn_ratelimited(\"%s selects TX queue %d, but real number of TX queues is %d\\n\",\n\t\t\t\t     dev->name, queue_index,\n\t\t\t\t     dev->real_num_tx_queues);\n\t\treturn 0;\n\t}\n\n\treturn queue_index;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline bool netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline bool __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t    u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\nstatic inline bool netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\nvoid netif_wake_subqueue(struct net_device *dev, u16 queue_index);\n\n#ifdef CONFIG_XPS\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index);\n#else\nstatic inline int netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t      const struct cpumask *mask,\n\t\t\t\t      u16 index)\n{\n\treturn 0;\n}\n#endif\n\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues);\n\n/*\n * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used\n * as a distribution range limit for the returned value.\n */\nstatic inline u16 skb_tx_hash(const struct net_device *dev,\n\t\t\t      struct sk_buff *skb)\n{\n\treturn __skb_tx_hash(dev, skb, dev->real_num_tx_queues);\n}\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline bool netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq);\n\n#ifdef CONFIG_SYSFS\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxq)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SYSFS\nstatic inline unsigned int get_netdev_rx_queue_index(\n\t\tstruct netdev_rx_queue *queue)\n{\n\tstruct net_device *dev = queue->dev;\n\tint index = queue - dev->_rx;\n\n\tBUG_ON(index >= dev->num_rx_queues);\n\treturn index;\n}\n#endif\n\n#define DEFAULT_MAX_NUM_RSS_QUEUES\t(8)\nint netif_get_num_default_rss_queues(void);\n\nenum skb_free_reason {\n\tSKB_REASON_CONSUMED,\n\tSKB_REASON_DROPPED,\n};\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason);\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason);\n\n/*\n * It is not allowed to call kfree_skb() or consume_skb() from hardware\n * interrupt context or with hardware interrupts being disabled.\n * (in_irq() || irqs_disabled())\n *\n * We provide four helpers that can be used in following contexts :\n *\n * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.\n *  Typically used in place of consume_skb(skb) in TX completion path\n *\n * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_any(skb) when caller doesn't know its current irq context,\n *  and consumed a packet. Used in place of consume_skb(skb)\n */\nstatic inline void dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_CONSUMED);\n}\n\nstatic inline void dev_kfree_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_CONSUMED);\n}\n\nint netif_rx(struct sk_buff *skb);\nint netif_rx_ni(struct sk_buff *skb);\nint netif_receive_skb(struct sk_buff *skb);\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb);\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old);\nstruct sk_buff *napi_get_frags(struct napi_struct *napi);\ngro_result_t napi_gro_frags(struct napi_struct *napi);\nstruct packet_offload *gro_find_receive_by_type(__be16 type);\nstruct packet_offload *gro_find_complete_by_type(__be16 type);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data);\nvoid netdev_rx_handler_unregister(struct net_device *dev);\n\nbool dev_valid_name(const char *name);\nint dev_ioctl(struct net *net, unsigned int cmd, void __user *);\nint dev_ethtool(struct net *net, struct ifreq *);\nunsigned int dev_get_flags(const struct net_device *);\nint __dev_change_flags(struct net_device *, unsigned int flags);\nint dev_change_flags(struct net_device *, unsigned int);\nvoid __dev_notify_flags(struct net_device *, unsigned int old_flags,\n\t\t\tunsigned int gchanges);\nint dev_change_name(struct net_device *, const char *);\nint dev_set_alias(struct net_device *, const char *, size_t);\nint dev_change_net_namespace(struct net_device *, struct net *, const char *);\nint dev_set_mtu(struct net_device *, int);\nvoid dev_set_group(struct net_device *, int);\nint dev_set_mac_address(struct net_device *, struct sockaddr *);\nint dev_change_carrier(struct net_device *, bool new_carrier);\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid);\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len);\nint dev_change_proto_down(struct net_device *dev, bool proto_down);\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev);\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret);\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nbool is_skb_forwardable(struct net_device *dev, struct sk_buff *skb);\n\nextern int\t\tnetdev_budget;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nvoid netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n\tthis_cpu_dec(*dev->pcpu_refcnt);\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n\tthis_cpu_inc(*dev->pcpu_refcnt);\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nvoid linkwatch_init_dev(struct net_device *dev);\nvoid linkwatch_fire_event(struct net_device *dev);\nvoid linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nunsigned long dev_trans_start(struct net_device *dev);\n\nvoid __netdev_watchdog_up(struct net_device *dev);\n\nvoid netif_carrier_on(struct net_device *dev);\n\nvoid netif_carrier_off(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n *\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline bool netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline bool netif_device_present(struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nvoid netif_device_detach(struct net_device *dev);\n\nvoid netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n\nenum {\n\tNETIF_MSG_DRV\t\t= 0x0001,\n\tNETIF_MSG_PROBE\t\t= 0x0002,\n\tNETIF_MSG_LINK\t\t= 0x0004,\n\tNETIF_MSG_TIMER\t\t= 0x0008,\n\tNETIF_MSG_IFDOWN\t= 0x0010,\n\tNETIF_MSG_IFUP\t\t= 0x0020,\n\tNETIF_MSG_RX_ERR\t= 0x0040,\n\tNETIF_MSG_TX_ERR\t= 0x0080,\n\tNETIF_MSG_TX_QUEUED\t= 0x0100,\n\tNETIF_MSG_INTR\t\t= 0x0200,\n\tNETIF_MSG_TX_DONE\t= 0x0400,\n\tNETIF_MSG_RX_STATUS\t= 0x0800,\n\tNETIF_MSG_PKTDATA\t= 0x1000,\n\tNETIF_MSG_HW\t\t= 0x2000,\n\tNETIF_MSG_WOL\t\t= 0x4000,\n};\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1 << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline bool __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tbool ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_TRYLOCK(dev, txq)\t\t\t\\\n\t(((dev->features & NETIF_F_LLTX) == 0) ?\t\\\n\t\t__netif_tx_trylock(txq) :\t\t\\\n\t\ttrue )\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_lock_nested(struct net_device *dev)\n{\n\tint subclass = SINGLE_DEPTH_NESTING;\n\n\tif (dev->netdev_ops->ndo_get_lock_subclass)\n\t\tsubclass = dev->netdev_ops->ndo_get_lock_subclass(dev);\n\n\tspin_lock_nested(&dev->addr_list_lock, subclass);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tspin_lock_bh(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nvoid ether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t    unsigned char name_assign_type,\n\t\t\t\t    void (*setup)(struct net_device *),\n\t\t\t\t    unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, name_assign_type, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, count, \\\n\t\t\t count)\n\nint register_netdev(struct net_device *dev);\nvoid unregister_netdev(struct net_device *dev);\n\n/* General hardware address lists handling functions */\nint __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t   struct netdev_hw_addr_list *from_list, int addr_len);\nvoid __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t      struct netdev_hw_addr_list *from_list, int addr_len);\nint __hw_addr_sync_dev(struct netdev_hw_addr_list *list,\n\t\t       struct net_device *dev,\n\t\t       int (*sync)(struct net_device *, const unsigned char *),\n\t\t       int (*unsync)(struct net_device *,\n\t\t\t\t     const unsigned char *));\nvoid __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t  struct net_device *dev,\n\t\t\t  int (*unsync)(struct net_device *,\n\t\t\t\t\tconst unsigned char *));\nvoid __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nint dev_addr_add(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nint dev_addr_del(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nvoid dev_addr_flush(struct net_device *dev);\nint dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nint dev_uc_add(struct net_device *dev, const unsigned char *addr);\nint dev_uc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_uc_del(struct net_device *dev, const unsigned char *addr);\nint dev_uc_sync(struct net_device *to, struct net_device *from);\nint dev_uc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_uc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_uc_flush(struct net_device *dev);\nvoid dev_uc_init(struct net_device *dev);\n\n/**\n *  __dev_uc_sync - Synchonize device's unicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n **/\nstatic inline int __dev_uc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->uc, dev, sync, unsync);\n}\n\n/**\n *  __dev_uc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_uc_sync().\n **/\nstatic inline void __dev_uc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->uc, dev, unsync);\n}\n\n/* Functions used for multicast addresses handling */\nint dev_mc_add(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_sync(struct net_device *to, struct net_device *from);\nint dev_mc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_mc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_mc_flush(struct net_device *dev);\nvoid dev_mc_init(struct net_device *dev);\n\n/**\n *  __dev_mc_sync - Synchonize device's multicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n **/\nstatic inline int __dev_mc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->mc, dev, sync, unsync);\n}\n\n/**\n *  __dev_mc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_mc_sync().\n **/\nstatic inline void __dev_mc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->mc, dev, unsync);\n}\n\n/* Functions used for secondary unicast and multicast support */\nvoid dev_set_rx_mode(struct net_device *dev);\nvoid __dev_set_rx_mode(struct net_device *dev);\nint dev_set_promiscuity(struct net_device *dev, int inc);\nint dev_set_allmulti(struct net_device *dev, int inc);\nvoid netdev_state_change(struct net_device *dev);\nvoid netdev_notify_peers(struct net_device *dev);\nvoid netdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nvoid dev_load(struct net *net, const char *name);\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage);\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tbpf_jit_enable;\n\nbool netdev_has_upper_dev(struct net_device *dev, struct net_device *upper_dev);\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->adj_list.upper, \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)))\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_all_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->all_adj_list.upper, \\\n\t     updev = netdev_all_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_all_upper_get_next_dev_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter);\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_private(dev, priv, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)))\n\n#define netdev_for_each_lower_private_rcu(dev, priv, iter) \\\n\tfor (iter = &(dev)->adj_list.lower, \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next(struct net_device *dev,\n\t\t\t\tstruct list_head **iter);\n#define netdev_for_each_lower_dev(dev, ldev, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)); \\\n\t     ldev; \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)))\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list);\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev);\nint netdev_upper_dev_link(struct net_device *dev, struct net_device *upper_dev);\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info);\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev);\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname);\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev);\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info);\n\n/* RSS keys are 40 or 52 bytes long */\n#define NETDEV_RSS_KEY_LEN 52\nextern u8 netdev_rss_key[NETDEV_RSS_KEY_LEN] __read_mostly;\nvoid netdev_rss_key_fill(void *buffer, size_t len);\n\nint dev_get_nest_level(struct net_device *dev,\n\t\t       bool (*type_check)(const struct net_device *dev));\nint skb_checksum_help(struct sk_buff *skb);\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path);\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features);\n\nstruct netdev_bonding_info {\n\tifslave\tslave;\n\tifbond\tmaster;\n};\n\nstruct netdev_notifier_bonding_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct netdev_bonding_info  bonding_info;\n};\n\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info);\n\nstatic inline\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn __skb_gso_segment(skb, features, true);\n}\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth);\n\nstatic inline bool can_checksum_protocol(netdev_features_t features,\n\t\t\t\t\t __be16 protocol)\n{\n\tif (protocol == htons(ETH_P_FCOE))\n\t\treturn !!(features & NETIF_F_FCOE_CRC);\n\n\t/* Assume this is an IP checksum (not SCTP CRC) */\n\n\tif (features & NETIF_F_HW_CSUM) {\n\t\t/* Can checksum everything */\n\t\treturn true;\n\t}\n\n\tswitch (protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn !!(features & NETIF_F_IP_CSUM);\n\tcase htons(ETH_P_IPV6):\n\t\treturn !!(features & NETIF_F_IPV6_CSUM);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Map an ethertype into IP protocol if possible */\nstatic inline int eproto_to_ipproto(int eproto)\n{\n\tswitch (eproto) {\n\tcase htons(ETH_P_IP):\n\t\treturn IPPROTO_IP;\n\tcase htons(ETH_P_IPV6):\n\t\treturn IPPROTO_IPV6;\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev)\n{\n}\n#endif\n/* rx skb timestamps */\nvoid net_enable_timestamp(void);\nvoid net_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nint __init dev_proc_init(void);\n#else\n#define dev_proc_init() 0\n#endif\n\nstatic inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,\n\t\t\t\t\t      struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t      bool more)\n{\n\tskb->xmit_more = more ? 1 : 0;\n\treturn ops->ndo_start_xmit(skb, dev);\n}\n\nstatic inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq, bool more)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint rc;\n\n\trc = __netdev_start_xmit(ops, skb, dev, more);\n\tif (rc == NETDEV_TX_OK)\n\t\ttxq_trans_update(txq);\n\n\treturn rc;\n}\n\nint netdev_class_create_file_ns(struct class_attribute *class_attr,\n\t\t\t\tconst void *ns);\nvoid netdev_class_remove_file_ns(struct class_attribute *class_attr,\n\t\t\t\t const void *ns);\n\nstatic inline int netdev_class_create_file(struct class_attribute *class_attr)\n{\n\treturn netdev_class_create_file_ns(class_attr, NULL);\n}\n\nstatic inline void netdev_class_remove_file(struct class_attribute *class_attr)\n{\n\tnetdev_class_remove_file_ns(class_attr, NULL);\n}\n\nextern struct kobj_ns_type_operations net_ns_type_operations;\n\nconst char *netdev_drivername(const struct net_device *dev);\n\nvoid linkwatch_run_queue(void);\n\nstatic inline netdev_features_t netdev_intersect_features(netdev_features_t f1,\n\t\t\t\t\t\t\t  netdev_features_t f2)\n{\n\tif ((f1 ^ f2) & NETIF_F_HW_CSUM) {\n\t\tif (f1 & NETIF_F_HW_CSUM)\n\t\t\tf1 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t\telse\n\t\t\tf2 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\treturn f1 & f2;\n}\n\nstatic inline netdev_features_t netdev_get_wanted_features(\n\tstruct net_device *dev)\n{\n\treturn (dev->features & ~dev->hw_features) | dev->wanted_features;\n}\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask);\n\n/* Allow TSO being used on stacked device :\n * Performing the GSO segmentation before last device\n * is a performance improvement.\n */\nstatic inline netdev_features_t netdev_add_tso_features(netdev_features_t features,\n\t\t\t\t\t\t\tnetdev_features_t mask)\n{\n\treturn netdev_increment_features(features, NETIF_F_ALL_TSO, mask);\n}\n\nint __netdev_update_features(struct net_device *dev);\nvoid netdev_update_features(struct net_device *dev);\nvoid netdev_change_features(struct net_device *dev);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features);\nnetdev_features_t netif_skb_features(struct sk_buff *skb);\n\nstatic inline bool net_gso_ok(netdev_features_t features, int gso_type)\n{\n\tnetdev_features_t feature = gso_type << NETIF_F_GSO_SHIFT;\n\n\t/* check flags correspondence */\n\tBUILD_BUG_ON(SKB_GSO_TCPV4   != (NETIF_F_TSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP     != (NETIF_F_UFO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_DODGY   != (NETIF_F_GSO_ROBUST >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_ECN != (NETIF_F_TSO_ECN >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCPV6   != (NETIF_F_TSO6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FCOE    != (NETIF_F_FSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE     != (NETIF_F_GSO_GRE >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE_CSUM != (NETIF_F_GSO_GRE_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPIP    != (NETIF_F_GSO_IPIP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_SIT     != (NETIF_F_GSO_SIT >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL != (NETIF_F_GSO_UDP_TUNNEL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL_CSUM != (NETIF_F_GSO_UDP_TUNNEL_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TUNNEL_REMCSUM != (NETIF_F_GSO_TUNNEL_REMCSUM >> NETIF_F_GSO_SHIFT));\n\n\treturn (features & feature) == feature;\n}\n\nstatic inline bool skb_gso_ok(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline bool netif_needs_gso(struct sk_buff *skb,\n\t\t\t\t   netdev_features_t features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely((skb->ip_summed != CHECKSUM_PARTIAL) &&\n\t\t\t (skb->ip_summed != CHECKSUM_UNNECESSARY)));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nstatic inline void skb_gso_error_unwind(struct sk_buff *skb, __be16 protocol,\n\t\t\t\t\tint pulled_hlen, u16 mac_offset,\n\t\t\t\t\tint mac_len)\n{\n\tskb->protocol = protocol;\n\tskb->encapsulation = 1;\n\tskb_push(skb, pulled_hlen);\n\tskb_reset_transport_header(skb);\n\tskb->mac_header = mac_offset;\n\tskb->network_header = skb->mac_header + mac_len;\n\tskb->mac_len = mac_len;\n}\n\nstatic inline bool netif_is_macsec(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nstatic inline bool netif_is_macvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN;\n}\n\nstatic inline bool netif_is_macvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN_PORT;\n}\n\nstatic inline bool netif_is_ipvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_SLAVE;\n}\n\nstatic inline bool netif_is_ipvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_MASTER;\n}\n\nstatic inline bool netif_is_bond_master(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_MASTER && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_is_bond_slave(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_SLAVE && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_supports_nofcs(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_SUPP_NOFCS;\n}\n\nstatic inline bool netif_is_l3_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_MASTER;\n}\n\nstatic inline bool netif_is_l3_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_SLAVE;\n}\n\nstatic inline bool netif_is_bridge_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_EBRIDGE;\n}\n\nstatic inline bool netif_is_bridge_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_BRIDGE_PORT;\n}\n\nstatic inline bool netif_is_ovs_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OPENVSWITCH;\n}\n\nstatic inline bool netif_is_team_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM;\n}\n\nstatic inline bool netif_is_team_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM_PORT;\n}\n\nstatic inline bool netif_is_lag_master(const struct net_device *dev)\n{\n\treturn netif_is_bond_master(dev) || netif_is_team_master(dev);\n}\n\nstatic inline bool netif_is_lag_port(const struct net_device *dev)\n{\n\treturn netif_is_bond_slave(dev) || netif_is_team_port(dev);\n}\n\nstatic inline bool netif_is_rxfh_configured(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_RXFH_CONFIGURED;\n}\n\n/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */\nstatic inline void netif_keep_dst(struct net_device *dev)\n{\n\tdev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM);\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (!dev->name[0] || strchr(dev->name, '%'))\n\t\treturn \"(unnamed net_device)\";\n\treturn dev->name;\n}\n\nstatic inline const char *netdev_reg_state(const struct net_device *dev)\n{\n\tswitch (dev->reg_state) {\n\tcase NETREG_UNINITIALIZED: return \" (uninitialized)\";\n\tcase NETREG_REGISTERED: return \"\";\n\tcase NETREG_UNREGISTERING: return \" (unregistering)\";\n\tcase NETREG_UNREGISTERED: return \" (unregistered)\";\n\tcase NETREG_RELEASED: return \" (released)\";\n\tcase NETREG_DUMMY: return \" (dummy)\";\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown reg_state %d\\n\", dev->name, dev->reg_state);\n\treturn \" (unknown)\";\n}\n\n__printf(3, 4)\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...);\n__printf(2, 3)\nvoid netdev_emerg(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_alert(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_crit(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_err(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_warn(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_notice(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_info(const struct net_device *dev, const char *format, ...);\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_netdev_dbg(__dev, format, ##args);\t\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s%s\\n\" format, netdev_name(dev),\t\\\n\t     netdev_reg_state(dev), ##args)\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_netdev_dbg(netdev, format, ##args);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *      NOTE:  That is no longer true with the addition of VLAN tags.  Not\n *             sure which should go first, but I bet it won't make much\n *             difference if we are running VLANs.  The good news is that\n *             this protocol won't be in the list unless compiled in, so\n *             the average user (w/out VLANs) will not be adversely affected.\n *             --BLG\n *\n *\t\t0800\tIP\n *\t\t8100    802.1Q VLAN\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\n#endif\t/* _LINUX_NETDEVICE_H */\n", "/*\n * \tNET3\tProtocol independent device support routines.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n * \t\tAuthors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *              \t\t\tto 2 if register_netdev gets called\n *              \t\t\tbefore net_dev_init & also removed a\n *              \t\t\tfew lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *\t\tAlan Cox\t: \tFix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *\t\tAlan Cox\t: \tFixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *              \t\t\tindefinitely on dev->refcnt\n * \t\tJ Hadi Salim\t:\t- Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <asm/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/mutex.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/notifier.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/pkt_sched.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/pci.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_ingress.h>\n#include <linux/sctp.h>\n\n#include \"net-sysfs.h\"\n\n/* Instead of increasing this, you should create a hash table. */\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic DEFINE_SPINLOCK(offload_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\nstatic struct list_head offload_base __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic seqcount_t devnet_rename_seq;\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0);\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\thlist_del_rcu(&dev->name_hlist);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] =\n\t{ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] =\n\t{\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t \"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t \"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t \"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t \"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t \"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t \"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t \"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t \"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t \"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t \"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t \"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t \"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t \"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t \"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n\n\t\tProtocol management and registration routines\n\n*******************************************************************************/\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/**\n *\tdev_add_offload - register offload handlers\n *\t@po: protocol offload declaration\n *\n *\tAdd protocol offload handlers to the networking stack. The passed\n *\t&proto_offload is linked into kernel lists and may not be freed until\n *\tit has been removed from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new offload handlers (until the next received packet).\n */\nvoid dev_add_offload(struct packet_offload *po)\n{\n\tstruct packet_offload *elem;\n\n\tspin_lock(&offload_lock);\n\tlist_for_each_entry(elem, &offload_base, list) {\n\t\tif (po->priority < elem->priority)\n\t\t\tbreak;\n\t}\n\tlist_add_rcu(&po->list, elem->list.prev);\n\tspin_unlock(&offload_lock);\n}\nEXPORT_SYMBOL(dev_add_offload);\n\n/**\n *\t__dev_remove_offload\t - remove offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a protocol offload handler that was previously added to the\n *\tkernel offload handlers by dev_add_offload(). The passed &offload_type\n *\tis removed from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nstatic void __dev_remove_offload(struct packet_offload *po)\n{\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *po1;\n\n\tspin_lock(&offload_lock);\n\n\tlist_for_each_entry(po1, head, list) {\n\t\tif (po == po1) {\n\t\t\tlist_del_rcu(&po->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_offload: %p not found\\n\", po);\nout:\n\tspin_unlock(&offload_lock);\n}\n\n/**\n *\tdev_remove_offload\t - remove packet offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a packet offload handler that was previously added to the kernel\n *\toffload handlers by dev_add_offload(). The passed &offload_type is\n *\tremoved from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_offload(struct packet_offload *po)\n{\n\t__dev_remove_offload(po);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_offload);\n\n/******************************************************************************\n\n\t\t      Device Boot-time Settings Routines\n\n*******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n *\tnetdev_boot_setup_check\t- check boot time settings\n *\t@dev: the netdevice\n *\n * \tCheck boot time settings for the device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq \t= s[i].map.irq;\n\t\t\tdev->base_addr \t= s[i].map.base_addr;\n\t\t\tdev->mem_start \t= s[i].map.mem_start;\n\t\t\tdev->mem_end \t= s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n *\tnetdev_boot_base\t- get address from boot time settings\n *\t@prefix: prefix for network device\n *\t@unit: id for network device\n *\n * \tCheck boot time settings for the base address of device.\n *\tThe found settings are set for the device to be used\n *\tlater in the device probing.\n *\tReturns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n\n\t\t\t    Device Interface Subroutines\n\n*******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n *\tdev_get_by_name_rcu\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name.\n *\tIf the name is found a pointer to the device is returned.\n * \tIf the name is not found then %NULL is returned.\n *\tThe reference counters are not incremented so the caller must be\n *\tcareful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry_rcu(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n *\n *\tThe use of raw_seqcount_begin() and cond_resched() before\n *\tretrying is required as we want to give the writers a chance\n *\tto complete when CONFIG_PREEMPT is not set.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tunsigned int seq;\n\nretry:\n\tseq = raw_seqcount_begin(&devnet_rename_seq);\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\n\tstrcpy(name, dev->name);\n\trcu_read_unlock();\n\tif (read_seqcount_retry(&devnet_rename_seq, seq)) {\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tfor_each_netdev(net, dev)\n\t\tif (dev->type == type)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_getfirstbyhwtype);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tto allow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strlen(name) >= IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tp = strnchr(name, IFNAMSIZ-1, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tif (buf != name)\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tstruct net *net;\n\tint ret;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\nstatic int dev_get_valid_name(struct net *net,\n\t\t\t      struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\tif (dev->flags & IFF_UP)\n\t\treturn -EBUSY;\n\n\twrite_seqcount_begin(&devnet_rename_seq);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn ret;\n\t}\n\n\twrite_seqcount_end(&devnet_rename_seq);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_del_rcu(&dev->name_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\twrite_seqcount_begin(&devnet_rename_seq);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tpr_err(\"%s: name change rollback failed: %d\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tchar *new_ifalias;\n\n\tASSERT_RTNL();\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (!len) {\n\t\tkfree(dev->ifalias);\n\t\tdev->ifalias = NULL;\n\t\treturn 0;\n\t}\n\n\tnew_ifalias = krealloc(dev->ifalias, len + 1, GFP_KERNEL);\n\tif (!new_ifalias)\n\t\treturn -ENOMEM;\n\tdev->ifalias = new_ifalias;\n\n\tstrlcpy(dev->ifalias, alias, len+1);\n\treturn len;\n}\n\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = 0;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * \tnetdev_notify_peers - notify network peers about existence of @dev\n * \t@dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int __dev_open(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers(NETDEV_PRE_UP, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev:\tdevice to open\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic int __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic int __dev_close(struct net_device *dev)\n{\n\tint retval;\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\tretval = __dev_close_many(&single);\n\tlist_del(&single);\n\n\treturn retval;\n}\n\nint dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nint dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\tnetdev_notifier_info_init(&info, dev);\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n *\tregister_netdevice_notifier - register a network notifier block\n *\t@nb: notifier\n *\n *\tRegister a notifier to be called when network device events occur.\n *\tThe notifier passed is linked into the kernel structures and must\n *\tnot be reused until it has been unregistered. A negative errno code\n *\tis returned on a failure.\n *\n * \tWhen registered all registration and up events are replayed\n *\tto the new notifier to allow device to have a race free\n *\tview of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net_device *last;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\t\t\terr = notifier_to_errno(err);\n\t\t\tif (err)\n\t\t\t\tgoto rollback;\n\n\t\t\tif (!(dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\t\t}\n\t}\n\nunlock:\n\trtnl_unlock();\n\treturn err;\n\nrollback:\n\tlast = dev;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev == last)\n\t\t\t\tgoto outroll;\n\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\n\noutroll:\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n *\tunregister_netdevice_notifier - unregister a network notifier block\n *\t@nb: notifier\n *\n *\tUnregister a notifier previously registered by\n *\tregister_netdevice_notifier(). The notifier is unlinked into the\n *\tkernel structures and may then be reused. A negative errno code\n *\tis returned on a failure.\n *\n * \tAfter unregistering unregister and down device events are synthesized\n *\tfor all devices on the device list to the removed notifier to remove\n *\tthe need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\nunlock:\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tASSERT_RTNL();\n\tnetdev_notifier_info_init(info, dev);\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\treturn call_netdevice_notifiers_info(val, dev, &info);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n#ifdef CONFIG_NET_INGRESS\nstatic struct static_key ingress_needed __read_mostly;\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_key_slow_inc(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_key_slow_dec(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct static_key egress_needed __read_mostly;\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_key_slow_inc(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_key_slow_dec(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nstatic struct static_key netstamp_needed __read_mostly;\n#ifdef HAVE_JUMP_LABEL\n/* We are not allowed to call static_key_slow_dec() from irq context\n * If net_disable_timestamp() is called from irq context, defer the\n * static_key_slow_dec() calls.\n */\nstatic atomic_t netstamp_needed_deferred;\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\n\tif (deferred) {\n\t\twhile (--deferred)\n\t\t\tstatic_key_slow_dec(&netstamp_needed);\n\t\treturn;\n\t}\n#endif\n\tstatic_key_slow_inc(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tif (in_interrupt()) {\n\t\tatomic_inc(&netstamp_needed_deferred);\n\t\treturn;\n\t}\n#endif\n\tstatic_key_slow_dec(&netstamp_needed);\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp.tv64 = 0;\n\tif (static_key_false(&netstamp_needed))\n\t\t__net_timestamp(skb);\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\\\n\tif (static_key_false(&netstamp_needed)) {\t\t\\\n\t\tif ((COND) && !(SKB)->tstamp.tv64)\t\\\n\t\t\t__net_timestamp(SKB);\t\t\\\n\t}\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(struct net_device *dev, struct sk_buff *skb)\n{\n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tlen = dev->mtu + dev->hard_header_len + VLAN_HLEN;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t/* if TSO is enabled, we don't care about the length as the packet\n\t * could be forwarded without being segmented before\n\t */\n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tif (skb_orphan_frags(skb, GFP_ATOMIC) ||\n\t    unlikely(!is_skb_forwardable(dev, skb))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tskb_scrub_packet(skb, true);\n\tskb->priority = 0;\n\tskb->protocol = eth_type_trans(skb, dev);\n\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\tatomic_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nstatic void dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev)\n\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\trcu_read_unlock();\n}\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tpr_warn(\"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tpr_warn(\"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\ti, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_XPS\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic struct xps_map *remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t\t\tint cpu, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->cpu_map[cpu]);\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] == index) {\n\t\t\tif (map->len > 1) {\n\t\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\t} else {\n\t\t\t\tRCU_INIT_POINTER(dev_maps->cpu_map[cpu], NULL);\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t\t\tmap = NULL;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn map;\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tint cpu, i;\n\tbool active = false;\n\n\tmutex_lock(&xps_map_mutex);\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = index; i < dev->num_tx_queues; i++) {\n\t\t\tif (!remove_xps_queue(dev_maps, cpu, i))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == dev->num_tx_queues)\n\t\t\tactive = true;\n\t}\n\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\n\tfor (i = index; i < dev->num_tx_queues; i++)\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, i),\n\t\t\t\t\t     NUMA_NO_NODE);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map,\n\t\t\t\t      int cpu, u16 index)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add queue to this CPU's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store queue on this CPU's map */\n\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t       cpu_to_node(cpu));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL;\n\tstruct xps_map *map, *new_map;\n\tint maps_sz = max_t(unsigned int, XPS_DEV_MAPS_SIZE, L1_CACHE_BYTES);\n\tint cpu, numa_node_id = -2;\n\tbool active = false;\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\t/* allocate memory for queue storage */\n\tfor_each_online_cpu(cpu) {\n\t\tif (!cpumask_test_cpu(cpu, mask))\n\t\t\tcontinue;\n\n\t\tif (!new_dev_maps)\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\tif (!new_dev_maps) {\n\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->cpu_map[cpu]) :\n\t\t\t\t NULL;\n\n\t\tmap = expand_xps_map(map, cpu, index);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[cpu], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpumask_test_cpu(cpu, mask) && cpu_online(cpu)) {\n\t\t\t/* add queue to CPU maps */\n\t\t\tint pos = 0;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->cpu_map[cpu]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (numa_node_id == -2)\n\t\t\t\tnuma_node_id = cpu_to_node(cpu);\n\t\t\telse if (numa_node_id != cpu_to_node(cpu))\n\t\t\t\tnuma_node_id = -1;\n#endif\n\t\t} else if (dev_maps) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[cpu]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[cpu], map);\n\t\t}\n\n\t}\n\n\trcu_assign_pointer(dev->xps_maps, new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (dev_maps) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[cpu]);\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[cpu]);\n\t\t\tif (map && map != new_map)\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\t/* update Tx queue numa node */\n\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t     (numa_node_id >= 0) ? numa_node_id :\n\t\t\t\t     NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes queue from unused CPUs */\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpumask_test_cpu(cpu, mask) && cpu_online(cpu))\n\t\t\tcontinue;\n\n\t\tif (remove_xps_queue(dev_maps, cpu, index))\n\t\t\tactive = true;\n\t}\n\n\t/* free map if not active */\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor_each_possible_cpu(cpu) {\n\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[cpu]);\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->cpu_map[cpu]) :\n\t\t\t\t NULL;\n\t\tif (new_map && new_map != map)\n\t\t\tkfree(new_map);\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater then real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tint rc;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tif (txq < dev->real_num_tx_queues) {\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t}\n\n\tdev->real_num_tx_queues = txq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * This routine should set an upper limit on the number of RSS queues\n * used by default by multiqueue devices.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\treturn min_t(int, DEFAULT_MAX_NUM_RSS_QUEUES, num_online_cpus());\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic inline void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!(txq->state & QUEUE_STATE_ANY_XOFF)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nvoid netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &txq->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(txq->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_wake_subqueue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (likely(atomic_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\tatomic_set(&skb->users, 0);\n\t} else if (likely(!atomic_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_irq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = num_tx_queues;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\twhile (unlikely(hash >= num_tx_queues))\n\t\t\thash -= num_tx_queues;\n\t\treturn hash;\n\t}\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\t\tqoffset = dev->tc_to_txq[tc].offset;\n\t\tqcount = dev->tc_to_txq[tc].count;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\nEXPORT_SYMBOL(__skb_tx_hash);\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features = 0;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tWARN(1, \"%s: caps=(%pNF, %pNF) len=%d data_len=%d gso_size=%d \"\n\t     \"gso_type=%d ip_summed=%d\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features,\n\t     skb->len, skb->data_len, skb_shinfo(skb)->gso_size,\n\t     skb_shinfo(skb)->gso_type, skb->ip_summed);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_shinfo(skb)->gso_size)) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__sum16))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\n/* skb_csum_offload_check - Driver helper function to determine if a device\n * with limited checksum offload capabilities is able to offload the checksum\n * for a given packet.\n *\n * Arguments:\n *   skb - sk_buff for the packet in question\n *   spec - contains the description of what device can offload\n *   csum_encapped - returns true if the checksum being offloaded is\n *\t      encpasulated. That is it is checksum for the transport header\n *\t      in the inner headers.\n *   checksum_help - when set indicates that helper function should\n *\t      call skb_checksum_help if offload checks fail\n *\n * Returns:\n *   true: Packet has passed the checksum checks and should be offloadable to\n *\t   the device (a driver may still need to check for additional\n *\t   restrictions of its device)\n *   false: Checksum is not offloadable. If checksum_help was set then\n *\t   skb_checksum_help was called to resolve checksum for non-GSO\n *\t   packets and when IP protocol is not SCTP\n */\nbool __skb_csum_offload_chk(struct sk_buff *skb,\n\t\t\t    const struct skb_csum_offl_spec *spec,\n\t\t\t    bool *csum_encapped,\n\t\t\t    bool csum_help)\n{\n\tstruct iphdr *iph;\n\tstruct ipv6hdr *ipv6;\n\tvoid *nhdr;\n\tint protocol;\n\tu8 ip_proto;\n\n\tif (skb->protocol == htons(ETH_P_8021Q) ||\n\t    skb->protocol == htons(ETH_P_8021AD)) {\n\t\tif (!spec->vlan_okay)\n\t\t\tgoto need_help;\n\t}\n\n\t/* We check whether the checksum refers to a transport layer checksum in\n\t * the outermost header or an encapsulated transport layer checksum that\n\t * corresponds to the inner headers of the skb. If the checksum is for\n\t * something else in the packet we need help.\n\t */\n\tif (skb_checksum_start_offset(skb) == skb_transport_offset(skb)) {\n\t\t/* Non-encapsulated checksum */\n\t\tprotocol = eproto_to_ipproto(vlan_get_protocol(skb));\n\t\tnhdr = skb_network_header(skb);\n\t\t*csum_encapped = false;\n\t\tif (spec->no_not_encapped)\n\t\t\tgoto need_help;\n\t} else if (skb->encapsulation && spec->encap_okay &&\n\t\t   skb_checksum_start_offset(skb) ==\n\t\t   skb_inner_transport_offset(skb)) {\n\t\t/* Encapsulated checksum */\n\t\t*csum_encapped = true;\n\t\tswitch (skb->inner_protocol_type) {\n\t\tcase ENCAP_TYPE_ETHER:\n\t\t\tprotocol = eproto_to_ipproto(skb->inner_protocol);\n\t\t\tbreak;\n\t\tcase ENCAP_TYPE_IPPROTO:\n\t\t\tprotocol = skb->inner_protocol;\n\t\t\tbreak;\n\t\t}\n\t\tnhdr = skb_inner_network_header(skb);\n\t} else {\n\t\tgoto need_help;\n\t}\n\n\tswitch (protocol) {\n\tcase IPPROTO_IP:\n\t\tif (!spec->ipv4_okay)\n\t\t\tgoto need_help;\n\t\tiph = nhdr;\n\t\tip_proto = iph->protocol;\n\t\tif (iph->ihl != 5 && !spec->ip_options_okay)\n\t\t\tgoto need_help;\n\t\tbreak;\n\tcase IPPROTO_IPV6:\n\t\tif (!spec->ipv6_okay)\n\t\t\tgoto need_help;\n\t\tif (spec->no_encapped_ipv6 && *csum_encapped)\n\t\t\tgoto need_help;\n\t\tipv6 = nhdr;\n\t\tnhdr += sizeof(*ipv6);\n\t\tip_proto = ipv6->nexthdr;\n\t\tbreak;\n\tdefault:\n\t\tgoto need_help;\n\t}\n\nip_proto_again:\n\tswitch (ip_proto) {\n\tcase IPPROTO_TCP:\n\t\tif (!spec->tcp_okay ||\n\t\t    skb->csum_offset != offsetof(struct tcphdr, check))\n\t\t\tgoto need_help;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tif (!spec->udp_okay ||\n\t\t    skb->csum_offset != offsetof(struct udphdr, check))\n\t\t\tgoto need_help;\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\tif (!spec->sctp_okay ||\n\t\t    skb->csum_offset != offsetof(struct sctphdr, checksum))\n\t\t\tgoto cant_help;\n\t\tbreak;\n\tcase NEXTHDR_HOP:\n\tcase NEXTHDR_ROUTING:\n\tcase NEXTHDR_DEST: {\n\t\tu8 *opthdr = nhdr;\n\n\t\tif (protocol != IPPROTO_IPV6 || !spec->ext_hdrs_okay)\n\t\t\tgoto need_help;\n\n\t\tip_proto = opthdr[0];\n\t\tnhdr += (opthdr[1] + 1) << 3;\n\n\t\tgoto ip_proto_again;\n\t}\n\tdefault:\n\t\tgoto need_help;\n\t}\n\n\t/* Passed the tests for offloading checksum */\n\treturn true;\n\nneed_help:\n\tif (csum_help && !skb_shinfo(skb)->gso_size)\n\t\tskb_checksum_help(skb);\ncant_help:\n\treturn false;\n}\nEXPORT_SYMBOL(__skb_csum_offload_chk);\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb_mac_header(skb);\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn __vlan_get_protocol(skb, type, depth);\n}\n\n/**\n *\tskb_mac_gso_segment - mac layer segmentation handler.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n */\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_offload *ptype;\n\tint vlan_depth = skb->mac_len;\n\t__be16 type = skb_network_protocol(skb, &vlan_depth);\n\n\tif (unlikely(!type))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__skb_pull(skb, vlan_depth);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &offload_base, list) {\n\t\tif (ptype->type == type && ptype->callbacks.gso_segment) {\n\t\t\tsegs = ptype->callbacks.gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_mac_gso_segment);\n\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL;\n\telse\n\t\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_SGO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\tskb_warn_bad_offload(skb);\n\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\tBUILD_BUG_ON(SKB_SGO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\treturn skb_mac_gso_segment(skb, features);\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev)\n{\n\tif (net_ratelimit()) {\n\t\tpr_err(\"%s: hw csum failure\\n\", dev ? dev->name : \"<unknown>\");\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* Actually, we should eliminate this check as soon as we know, that:\n * 1. IOMMU is present and allows to map all the memory.\n * 2. No high memory really exists on this machine.\n */\n\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (PCI_DMA_BUS_IS_PHYS) {\n\t\tstruct device *pdev = dev->dev.parent;\n\n\t\tif (!pdev)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tdma_addr_t addr = page_to_phys(skb_frag_page(frag));\n\t\t\tif (!pdev->dma_mask || addr + PAGE_SIZE - 1 > *pdev->dma_mask)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\tint tmp;\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, &tmp);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\t} else if (illegal_highdma(skb->dev, skb)) {\n\t\tfeatures &= ~NETIF_F_SG;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(const struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > dev->gso_max_segs || gso_segs < dev->gso_min_segs)\n\t\tfeatures &= ~NETIF_F_GSO_MASK;\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (!list_empty(&ptype_all) || !list_empty(&dev->ptype_all))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb->next = NULL;\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_xmit_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)\n{\n\tnetdev_features_t features;\n\n\tif (skb->next)\n\t\treturn skb;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (!(features & NETIF_F_CSUM_MASK) &&\n\t\t\t    skb_checksum_help(skb))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size)  {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\thdr_len += tcp_hdrlen(skb);\n\t\telse\n\t\t\thdr_len += sizeof(struct udphdr);\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits __QDISC___STATE_RUNNING owner to get the lock more\n\t * often and dequeue packets faster.\n\t */\n\tcontended = qdisc_is_running(q);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\tkfree_skb(skb);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t} else\n\t\t\tqdisc_run_end(q);\n\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = q->enqueue(skb, q) & NET_XMIT_MASK;\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tstruct netprio_map *map = rcu_dereference_bh(skb->dev->priomap);\n\n\tif (!skb->priority && skb->sk && map) {\n\t\tunsigned int prioidx =\n\t\t\tsock_cgroup_prioidx(&skb->sk->sk_cgrp_data);\n\n\t\tif (prioidx < map->priomap_len)\n\t\t\tskb->priority = map->priomap[prioidx];\n\t}\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\nDEFINE_PER_CPU(int, xmit_recursion);\nEXPORT_SYMBOL(xmit_recursion);\n\n#define RECURSION_LIMIT 10\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx_ni(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct tcf_proto *cl = rcu_dereference_bh(dev->egress_cl_list);\n\tstruct tcf_result cl_res;\n\n\tif (!cl)\n\t\treturn skb;\n\n\t/* skb->tc_verd and qdisc_skb_cb(skb)->pkt_len were already set\n\t * earlier by the caller.\n\t */\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tc_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\t\t*ret = NET_XMIT_DROP;\n\t\tgoto drop;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\t\t*ret = NET_XMIT_SUCCESS;\ndrop:\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn skb;\n}\n#endif /* CONFIG_NET_EGRESS */\n\nstatic inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\trcu_read_lock();\n\tdev_maps = rcu_dereference(dev->xps_maps);\n\tif (dev_maps) {\n\t\tmap = rcu_dereference(\n\t\t    dev_maps->cpu_map[skb->sender_cpu - 1]);\n\t\tif (map) {\n\t\t\tif (map->len == 1)\n\t\t\t\tqueue_index = map->queues[0];\n\t\t\telse\n\t\t\t\tqueue_index = map->queues[reciprocal_scale(skb_get_hash(skb),\n\t\t\t\t\t\t\t\t\t   map->len)];\n\t\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\t\tqueue_index = -1;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nstatic u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, skb);\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, accel_priv,\n\t\t\t\t\t\t\t    __netdev_pick_tx);\n\t\telse\n\t\t\tqueue_index = __netdev_pick_tx(dev, skb);\n\n\t\tif (!accel_priv)\n\t\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n *\t__dev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\t@accel_priv: private data used for L2 forwarding offload\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nstatic int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\n\tskb_reset_mac_header(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_EGRESS);\n# ifdef CONFIG_NET_EGRESS\n\tif (static_key_false(&egress_needed)) {\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n# endif\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n#ifdef CONFIG_NET_SWITCHDEV\n\t/* Don't forward if offload device already forwarded */\n\tif (skb->offload_fwd_mark &&\n\t    skb->offload_fwd_mark == dev->offload_fwd_mark) {\n\t\tconsume_skb(skb);\n\t\trc = NET_XMIT_SUCCESS;\n\t\tgoto out;\n\t}\n#endif\n\n\ttxq = netdev_pick_tx(dev, skb, accel_priv);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t   loopback, all the sorts of tunnels...\n\n\t   Really, it is unlikely that netif_tx_lock protection is necessary\n\t   here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t   counters.)\n\t   However, it is possible, that they rely on protection\n\t   made by us here.\n\n\t   Check this and shot the lock. It is not prone from deadlocks.\n\t   Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\n\t\t\tif (__this_cpu_read(xmit_recursion) > RECURSION_LIMIT)\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto drop;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\t__this_cpu_inc(xmit_recursion);\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\t__this_cpu_dec(xmit_recursion);\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\ndrop:\n\trcu_read_unlock_bh();\n\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\n\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv)\n{\n\treturn __dev_queue_xmit(skb, accel_priv);\n}\nEXPORT_SYMBOL(dev_queue_xmit_accel);\n\n\n/*=======================================================================\n\t\t\tReceiver routines\n  =======================================================================*/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\nint weight_p __read_mostly = 64;            /* old backlog weight */\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key rps_needed __read_mostly;\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match */\n\t\tident = sock_flow_table->ents[hash & sock_flow_table->mask];\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = ACCESS_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (netdev_max_backlog >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\ndrop:\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_rx(skb);\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n\treturn ret;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\ttrace_netif_rx_entry(skb);\n\n\treturn netif_rx_internal(skb);\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_netif_rx_ni_entry(skb);\n\n\tpreempt_disable();\n\terr = netif_rx_internal(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nstatic void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(atomic_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\n\t\t__kfree_skb_flush();\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock;\n\n\t\t\thead = head->next_sched;\n\n\t\t\troot_lock = qdisc_lock(q);\n\t\t\tif (spin_trylock(root_lock)) {\n\t\t\t\tsmp_mb__before_atomic();\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t  &q->state);\n\t\t\t\tqdisc_run(q);\n\t\t\t\tspin_unlock(root_lock);\n\t\t\t} else {\n\t\t\t\tif (!test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t      &q->state)) {\n\t\t\t\t\t__netif_reschedule(q);\n\t\t\t\t} else {\n\t\t\t\t\tsmp_mb__before_atomic();\n\t\t\t\t\tclear_bit(__QDISC_STATE_SCHED,\n\t\t\t\t\t\t  &q->state);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n#if (defined(CONFIG_BRIDGE) || defined(CONFIG_BRIDGE_MODULE)) && \\\n    (defined(CONFIG_ATM_LANE) || defined(CONFIG_ATM_LANE_MODULE))\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto *cl = rcu_dereference_bh(skb->dev->ingress_cl_list);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!cl)\n\t\treturn skb;\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\tskb->tc_verd = SET_TC_AT(skb->tc_verd, AT_INGRESS);\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tc_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tskb_do_redirect(skb);\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tASSERT_RTNL();\n\n\tif (dev->rx_handler)\n\t\treturn -EBUSY;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n#ifdef CONFIG_NETFILTER_INGRESS\n\tif (nf_hook_ingress_active(skb)) {\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\treturn nf_hook_ingress(skb);\n\t}\n#endif /* CONFIG_NETFILTER_INGRESS */\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||\n\t    skb->protocol == cpu_to_be16(ETH_P_8021AD)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (skb->tc_verd & TC_NCLS) {\n\t\tskb->tc_verd = CLR_TC_NCLS(skb->tc_verd);\n\t\tgoto ncls;\n\t}\n#endif\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_key_false(&ingress_needed)) {\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_verd = 0;\nncls:\n#endif\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\tif (skb_vlan_tag_get_id(skb))\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\tskb->vlan_tci = 0;\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\telse\n\t\t\tret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\telse\n\t\t\tatomic_long_inc(&skb->dev->rx_nohandler);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned long pflags = current->flags;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tcurrent->flags |= PF_MEMALLOC;\n\t\tret = __netif_receive_skb_core(skb, true);\n\t\ttsk_restore_flags(current, pflags, PF_MEMALLOC);\n\t} else\n\t\tret = __netif_receive_skb_core(skb, false);\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\ttrace_netif_receive_skb_entry(skb);\n\n\treturn netif_receive_skb_internal(skb);\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/* Network device is going away, flush any packets still pending\n * Called with irqs disabled.\n */\nstatic void flush_backlog(void *arg)\n{\n\tstruct net_device *dev = arg;\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tstruct sk_buff *skb, *tmp;\n\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev == dev) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n}\n\nstatic int napi_gro_complete(struct sk_buff *skb)\n{\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint err = -ENOENT;\n\n\tBUILD_BUG_ON(sizeof(struct napi_gro_cb) > sizeof(skb->cb));\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\n\t\terr = ptype->callbacks.gro_complete(skb, 0);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\treturn netif_receive_skb_internal(skb);\n}\n\n/* napi->gro_list contains packets ordered by age.\n * youngest packets at the head of it.\n * Complete skbs in reverse order to reduce latencies.\n */\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old)\n{\n\tstruct sk_buff *skb, *prev = NULL;\n\n\t/* scan list and build reverse chain */\n\tfor (skb = napi->gro_list; skb != NULL; skb = skb->next) {\n\t\tskb->prev = prev;\n\t\tprev = skb;\n\t}\n\n\tfor (skb = prev; skb; skb = prev) {\n\t\tskb->next = NULL;\n\n\t\tif (flush_old && NAPI_GRO_CB(skb)->age == jiffies)\n\t\t\treturn;\n\n\t\tprev = skb->prev;\n\t\tnapi_gro_complete(skb);\n\t\tnapi->gro_count--;\n\t}\n\n\tnapi->gro_list = NULL;\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nstatic void gro_list_prepare(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff *p;\n\tunsigned int maclen = skb->dev->hard_header_len;\n\tu32 hash = skb_get_hash_raw(skb);\n\n\tfor (p = napi->gro_list; p; p = p->next) {\n\t\tunsigned long diffs;\n\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\n\t\tif (hash != skb_get_hash_raw(p)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= p->vlan_tci ^ skb->vlan_tci;\n\t\tdiffs |= skb_metadata_dst_cmp(p, skb);\n\t\tif (maclen == ETH_HLEN)\n\t\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t\t      skb_mac_header(skb));\n\t\telse if (!diffs)\n\t\t\tdiffs = memcmp(skb_mac_header(p),\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       maclen);\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t}\n}\n\nstatic void skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *pinfo = skb_shinfo(skb);\n\tconst skb_frag_t *frag0 = &pinfo->frags[0];\n\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (skb_mac_header(skb) == skb_tail_pointer(skb) &&\n\t    pinfo->nr_frags &&\n\t    !PageHighMem(skb_frag_page(frag0))) {\n\t\tNAPI_GRO_CB(skb)->frag0 = skb_frag_address(frag0);\n\t\tNAPI_GRO_CB(skb)->frag0_len = skb_frag_size(frag0);\n\t}\n}\n\nstatic void gro_pull_from_frag0(struct sk_buff *skb, int grow)\n{\n\tstruct skb_shared_info *pinfo = skb_shinfo(skb);\n\n\tBUG_ON(skb->end - skb->tail < grow);\n\n\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\tskb->data_len -= grow;\n\tskb->tail += grow;\n\n\tpinfo->frags[0].page_offset += grow;\n\tskb_frag_size_sub(&pinfo->frags[0], grow);\n\n\tif (unlikely(!skb_frag_size(&pinfo->frags[0]))) {\n\t\tskb_frag_unref(skb, 0);\n\t\tmemmove(pinfo->frags, pinfo->frags + 1,\n\t\t\t--pinfo->nr_frags * sizeof(pinfo->frags[0]));\n\t}\n}\n\nstatic enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint same_flow;\n\tenum gro_result ret;\n\tint grow;\n\n\tif (!(skb->dev->features & NETIF_F_GRO))\n\t\tgoto normal;\n\n\tif (skb_is_gso(skb) || skb_has_frag_list(skb) || skb->csum_bad)\n\t\tgoto normal;\n\n\tgro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = 0;\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = ptype->callbacks.gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_count >= MAX_GRO_SKBS)) {\n\t\tstruct sk_buff *nskb = napi->gro_list;\n\n\t\t/* locate the end of the list to select the 'oldest' flow */\n\t\twhile (nskb->next) {\n\t\t\tpp = &nskb->next;\n\t\t\tnskb = *pp;\n\t\t}\n\t\t*pp = NULL;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t} else {\n\t\tnapi->gro_count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\n\nstruct packet_offload *gro_find_receive_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_receive_by_type);\n\nstruct packet_offload *gro_find_complete_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_complete_by_type);\n\nstatic gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tif (netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD) {\n\t\t\tskb_dst_drop(skb);\n\t\t\tkmem_cache_free(skbuff_head_cache, skb);\n\t\t} else {\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tskb_mark_napi_id(skb, napi);\n\ttrace_napi_gro_receive_entry(skb);\n\n\tskb_gro_reset_offset(skb);\n\n\treturn napi_skb_finish(dev_gro_receive(napi, skb), skb);\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tif (unlikely(skb->pfmemalloc)) {\n\t\tconsume_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, skb_headlen(skb));\n\t/* restore the reserve we had after netdev_alloc_skb_ip_align() */\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN - skb_headroom(skb));\n\tskb->vlan_tci = 0;\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\tskb->encapsulation = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = napi_alloc_skb(napi, GRO_MAX_HEAD);\n\t\tif (skb) {\n\t\t\tnapi->skb = skb;\n\t\t\tskb_mark_napi_id(skb, napi);\n\t\t}\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\nstatic gro_result_t napi_frags_finish(struct napi_struct *napi,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tif (ret == GRO_NORMAL && netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\tcase GRO_MERGED_FREE:\n\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/* Upper GRO stack assumes network header starts at gro_offset=0\n * Drivers could call both napi_gro_frags() and napi_gro_receive()\n * We copy ethernet header into skb->data to have a common layout.\n */\nstatic struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\teth = skb_gro_header_fast(skb, 0);\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\tif (!skb)\n\t\treturn GRO_DROP;\n\n\ttrace_napi_gro_frags_entry(skb);\n\n\treturn napi_frags_finish(napi, skb, dev_gro_receive(napi, skb));\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/* Compute the checksum from gro_offset and return the folded value\n * after adding in any pseudo checksum.\n */\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum wsum;\n\t__sum16 sum;\n\n\twsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);\n\n\t/* NAPI_GRO_CB(skb)->csum holds pseudo checksum */\n\tsum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev);\n\t}\n\n\tNAPI_GRO_CB(skb)->csum = wsum;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_gro_checksum_complete);\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\twhile (remsd) {\n\t\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\t\tif (cpu_online(remsd->cpu))\n\t\t\t\tsmp_call_function_single_async(remsd->cpu,\n\t\t\t\t\t\t\t   &remsd->csd);\n\t\t\tremsd = next;\n\t\t}\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tint work = 0;\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = weight_p;\n\tlocal_irq_disable();\n\twhile (1) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\tlocal_irq_enable();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tlocal_irq_disable();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota) {\n\t\t\t\tlocal_irq_enable();\n\t\t\t\treturn work;\n\t\t\t}\n\t\t}\n\n\t\trps_lock(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\trps_unlock(sd);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t   &sd->process_queue);\n\t\trps_unlock(sd);\n\t}\n\tlocal_irq_enable();\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nvoid __napi_complete(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\n\tlist_del_init(&n->poll_list);\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n}\nEXPORT_SYMBOL(__napi_complete);\n\nvoid napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags;\n\n\t/*\n\t * don't let napi dequeue from the cpu poll list\n\t * just in case its running on a different cpu\n\t */\n\tif (unlikely(test_bit(NAPI_STATE_NPSVC, &n->state)))\n\t\treturn;\n\n\tif (n->gro_list) {\n\t\tunsigned long timeout = 0;\n\n\t\tif (work_done)\n\t\t\ttimeout = n->dev->gro_flush_timeout;\n\n\t\tif (timeout)\n\t\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t\t      HRTIMER_MODE_REL_PINNED);\n\t\telse\n\t\t\tnapi_gro_flush(n, false);\n\t}\n\tif (likely(list_empty(&n->poll_list))) {\n\t\tWARN_ON_ONCE(!test_and_clear_bit(NAPI_STATE_SCHED, &n->state));\n\t} else {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\t__napi_complete(n);\n\t\tlocal_irq_restore(flags);\n\t}\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n#define BUSY_POLL_BUDGET 8\nbool sk_busy_loop(struct sock *sk, int nonblock)\n{\n\tunsigned long end_time = !nonblock ? sk_busy_loop_end_time(sk) : 0;\n\tint (*busy_poll)(struct napi_struct *dev);\n\tstruct napi_struct *napi;\n\tint rc = false;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(sk->sk_napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\t/* Note: ndo_busy_poll method is optional in linux-4.5 */\n\tbusy_poll = napi->dev->netdev_ops->ndo_busy_poll;\n\n\tdo {\n\t\trc = 0;\n\t\tlocal_bh_disable();\n\t\tif (busy_poll) {\n\t\t\trc = busy_poll(napi);\n\t\t} else if (napi_schedule_prep(napi)) {\n\t\t\tvoid *have = netpoll_poll_lock(napi);\n\n\t\t\tif (test_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\t\t\trc = napi->poll(napi, BUSY_POLL_BUDGET);\n\t\t\t\ttrace_napi_poll(napi);\n\t\t\t\tif (rc == BUSY_POLL_BUDGET) {\n\t\t\t\t\tnapi_complete_done(napi, rc);\n\t\t\t\t\tnapi_schedule(napi);\n\t\t\t\t}\n\t\t\t}\n\t\t\tnetpoll_poll_unlock(have);\n\t\t}\n\t\tif (rc > 0)\n\t\t\tNET_ADD_STATS_BH(sock_net(sk),\n\t\t\t\t\t LINUX_MIB_BUSYPOLLRXPACKETS, rc);\n\t\tlocal_bh_enable();\n\n\t\tif (rc == LL_FLUSH_FAILED)\n\t\t\tbreak; /* permanent failure */\n\n\t\tcpu_relax();\n\t} while (!nonblock && skb_queue_empty(&sk->sk_receive_queue) &&\n\t\t !need_resched() && !busy_loop_timeout(end_time));\n\n\trc = !skb_queue_empty(&sk->sk_receive_queue);\nout:\n\trcu_read_unlock();\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nvoid napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state) ||\n\t    test_and_set_bit(NAPI_STATE_HASHED, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS+1 range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < NR_CPUS + 1))\n\t\t\tnapi_gen_id = NR_CPUS + 1;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\nEXPORT_SYMBOL_GPL(napi_hash_add);\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nbool napi_hash_del(struct napi_struct *napi)\n{\n\tbool rcu_sync_needed = false;\n\n\tspin_lock(&napi_hash_lock);\n\n\tif (test_and_clear_bit(NAPI_STATE_HASHED, &napi->state)) {\n\t\trcu_sync_needed = true;\n\t\thlist_del_rcu(&napi->napi_hash_node);\n\t}\n\tspin_unlock(&napi_hash_lock);\n\treturn rcu_sync_needed;\n}\nEXPORT_SYMBOL_GPL(napi_hash_del);\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\tif (napi->gro_list)\n\t\tnapi_schedule(napi);\n\n\treturn HRTIMER_NORESTART;\n}\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tINIT_LIST_HEAD(&napi->poll_list);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n\tnapi->skb = NULL;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tpr_err_once(\"netif_napi_add() called with weight %d on device %s\\n\",\n\t\t\t    weight, dev->name);\n\tnapi->weight = weight;\n\tlist_add(&napi->dev_list, &dev->napi_list);\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tspin_lock_init(&napi->poll_lock);\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tnapi_hash_add(napi);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\twhile (test_and_set_bit(NAPI_STATE_NPSVC, &n->state))\n\t\tmsleep(1);\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/* Must be called in process context */\nvoid netif_napi_del(struct napi_struct *napi)\n{\n\tmight_sleep();\n\tif (napi_hash_del(napi))\n\t\tsynchronize_net();\n\tlist_del_init(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tkfree_skb_list(napi->gro_list);\n\tnapi->gro_list = NULL;\n\tnapi->gro_count = 0;\n}\nEXPORT_SYMBOL(netif_napi_del);\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tvoid *have;\n\tint work, weight;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n);\n\t}\n\n\tWARN_ON_ONCE(work > weight);\n\n\tif (likely(work < weight))\n\t\tgoto out_unlock;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\tgoto out_unlock;\n\t}\n\n\tif (n->gro_list) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\tgoto out_unlock;\n\t}\n\n\tlist_add_tail(&n->poll_list, repoll);\n\nout_unlock:\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies + 2;\n\tint budget = netdev_budget;\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\treturn;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t__kfree_skb_flush();\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tASSERT_RTNL();\n\n\treturn __netdev_find_adj(upper_dev, &dev->all_adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->all_adj_list.upper);\n}\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\n/**\n * netdev_all_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->all_adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_all_upper_get_next_dev_rcu);\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr++;\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tdev_hold(adj_dev);\n\n\tpr_debug(\"dev_hold for %s, because of link added from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tkfree(adj);\n\tdev_put(adj_dev);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"tried to remove device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tBUG();\n\t}\n\n\tif (adj->ref_nr > 1) {\n\t\tpr_debug(\"%s to %s ref_nr-- = %d\\n\", dev->name, adj_dev->name,\n\t\t\t adj->ref_nr-1);\n\t\tadj->ref_nr--;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tdev_put(adj_dev);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list, private,\n\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list, private,\n\t\t\t\t\t   false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int __netdev_adjacent_dev_link(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->all_adj_list.upper,\n\t\t\t\t\t\t&upper_dev->all_adj_list.lower,\n\t\t\t\t\t\tNULL, false);\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, down_list);\n}\n\nstatic void __netdev_adjacent_dev_unlink(struct net_device *dev,\n\t\t\t\t\t struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev,\n\t\t\t\t\t   &dev->all_adj_list.upper,\n\t\t\t\t\t   &upper_dev->all_adj_list.lower);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\tint ret = __netdev_adjacent_dev_link(dev, upper_dev);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t       &dev->adj_list.upper,\n\t\t\t\t\t       &upper_dev->adj_list.lower,\n\t\t\t\t\t       private, master);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_unlink(dev, upper_dev);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink(dev, upper_dev);\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\tstruct netdev_adjacent *i, *j, *to_i, *to_j;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_find_adj(dev, &upper_dev->all_adj_list.upper))\n\t\treturn -EBUSY;\n\n\tif (__netdev_find_adj(upper_dev, &dev->adj_list.upper))\n\t\treturn -EEXIST;\n\n\tif (master && netdev_master_upper_dev_get(dev))\n\t\treturn -EBUSY;\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = master;\n\tchangeupper_info.linking = true;\n\tchangeupper_info.upper_info = upper_info;\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Now that we linked these devs, make all the upper_dev's\n\t * all_adj_list.upper visible to every dev's all_adj_list.lower an\n\t * versa, and don't forget the devices itself. All of these\n\t * links are non-neighbours.\n\t */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tlist_for_each_entry(j, &upper_dev->all_adj_list.upper, list) {\n\t\t\tpr_debug(\"Interlinking %s with %s, non-neighbour\\n\",\n\t\t\t\t i->dev->name, j->dev->name);\n\t\t\tret = __netdev_adjacent_dev_link(i->dev, j->dev);\n\t\t\tif (ret)\n\t\t\t\tgoto rollback_mesh;\n\t\t}\n\t}\n\n\t/* add dev to every upper_dev's upper device */\n\tlist_for_each_entry(i, &upper_dev->all_adj_list.upper, list) {\n\t\tpr_debug(\"linking %s's upper device %s with %s\\n\",\n\t\t\t upper_dev->name, i->dev->name, dev->name);\n\t\tret = __netdev_adjacent_dev_link(dev, i->dev);\n\t\tif (ret)\n\t\t\tgoto rollback_upper_mesh;\n\t}\n\n\t/* add upper_dev to every dev's lower device */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tpr_debug(\"linking %s's lower device %s with %s\\n\", dev->name,\n\t\t\t i->dev->name, upper_dev->name);\n\t\tret = __netdev_adjacent_dev_link(i->dev, upper_dev);\n\t\tif (ret)\n\t\t\tgoto rollback_lower_mesh;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback_lower_mesh;\n\n\treturn 0;\n\nrollback_lower_mesh:\n\tto_i = i;\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tif (i == to_i)\n\t\t\tbreak;\n\t\t__netdev_adjacent_dev_unlink(i->dev, upper_dev);\n\t}\n\n\ti = NULL;\n\nrollback_upper_mesh:\n\tto_i = i;\n\tlist_for_each_entry(i, &upper_dev->all_adj_list.upper, list) {\n\t\tif (i == to_i)\n\t\t\tbreak;\n\t\t__netdev_adjacent_dev_unlink(dev, i->dev);\n\t}\n\n\ti = j = NULL;\n\nrollback_mesh:\n\tto_i = i;\n\tto_j = j;\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list) {\n\t\tlist_for_each_entry(j, &upper_dev->all_adj_list.upper, list) {\n\t\t\tif (i == to_i && j == to_j)\n\t\t\t\tbreak;\n\t\t\t__netdev_adjacent_dev_unlink(i->dev, j->dev);\n\t\t}\n\t\tif (i == to_i)\n\t\t\tbreak;\n\t}\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, false, NULL, NULL);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\tstruct netdev_adjacent *i, *j;\n\tASSERT_RTNL();\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\tchangeupper_info.linking = false;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\t/* Here is the tricky part. We must remove all dev's lower\n\t * devices from all upper_dev's upper devices and vice\n\t * versa, to maintain the graph relationship.\n\t */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list)\n\t\tlist_for_each_entry(j, &upper_dev->all_adj_list.upper, list)\n\t\t\t__netdev_adjacent_dev_unlink(i->dev, j->dev);\n\n\t/* remove also the devices itself from lower/upper device\n\t * list\n\t */\n\tlist_for_each_entry(i, &dev->all_adj_list.lower, list)\n\t\t__netdev_adjacent_dev_unlink(i->dev, upper_dev);\n\n\tlist_for_each_entry(i, &upper_dev->all_adj_list.upper, list)\n\t\t__netdev_adjacent_dev_unlink(dev, i->dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info\tinfo;\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO, dev,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net,dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\nint dev_get_nest_level(struct net_device *dev,\n\t\t       bool (*type_check)(const struct net_device *dev))\n{\n\tstruct net_device *lower = NULL;\n\tstruct list_head *iter;\n\tint max_nest = -1;\n\tint nest;\n\n\tASSERT_RTNL();\n\n\tnetdev_for_each_lower_dev(dev, lower, iter) {\n\t\tnest = dev_get_nest_level(lower, type_check);\n\t\tif (max_nest < nest)\n\t\t\tmax_nest = nest;\n\t}\n\n\tif (type_check(dev))\n\t\tmax_nest++;\n\n\treturn max_nest;\n}\nEXPORT_SYMBOL(dev_get_nest_level);\n\n/**\n * netdev_lower_change - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info;\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE, lower_dev,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tpr_warn(\"%s: promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(current->audit_context, GFP_ATOMIC,\n\t\t\t\tAUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t\"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\tdev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t(old_flags & IFF_PROMISC),\n\t\t\t\tfrom_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\tfrom_kuid(&init_user_ns, uid),\n\t\t\t\tfrom_kgid(&init_user_ns, gid),\n\t\t\t\taudit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tpr_warn(\"%s: allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP)\n\t\tret = ((old_flags & IFF_UP) ? __dev_close : __dev_open)(dev);\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t   is important. Some (broken) drivers set IFF_PROMISC, when\n\t   IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = changes;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nstatic int __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\t/*\tMTU must be positive.\t */\n\tif (new_mtu < 0)\n\t\treturn -EINVAL;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\t}\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\nEXPORT_SYMBOL(dev_set_group);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\nEXPORT_SYMBOL(dev_change_carrier);\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\nEXPORT_SYMBOL(dev_get_phys_port_id);\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_name)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_name(dev, name, len);\n}\nEXPORT_SYMBOL(dev_get_phys_port_name);\n\n/**\n *\tdev_change_proto_down - update protocol port state information\n *\t@dev: device\n *\t@proto_down: new value\n *\n *\tThis info can be used by switch drivers to set the phys state of the\n *\tport.\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_proto_down(dev, proto_down);\n}\nEXPORT_SYMBOL(dev_change_proto_down);\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tdev_net(dev)->dev_unreg_count++;\n}\n\nstatic void rollback_registered_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t\ton_each_cpu(flush_backlog, dev, 1);\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t   this device. They should clean all the things.\n\t\t*/\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U,\n\t\t\t\t\t\t     GFP_KERNEL);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tdev_put(dev);\n}\n\nstatic void rollback_registered(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\trollback_registered_many(&single);\n\tlist_del(&single);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\tnetdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* UFO needs SG and checksumming */\n\tif (features & NETIF_F_UFO) {\n\t\t/* maybe split UFO into V4 and V6? */\n\t\tif (!(features & NETIF_F_HW_CSUM) &&\n\t\t    ((features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) !=\n\t\t     (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM))) {\n\t\t\tnetdev_dbg(dev,\n\t\t\t\t\"Dropping NETIF_F_UFO since no checksum offload features.\\n\");\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\n\t\tif (!(features & NETIF_F_SG)) {\n\t\t\tnetdev_dbg(dev,\n\t\t\t\t\"Dropping NETIF_F_UFO since no NETIF_F_SG feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_UFO;\n\t\t}\n\t}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tif (dev->netdev_ops->ndo_busy_poll)\n\t\tfeatures |= NETIF_F_BUSY_POLL;\n\telse\n#endif\n\t\tfeatures &= ~NETIF_F_BUSY_POLL;\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off an an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err)\n\t\tdev->features = features;\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (netif_carrier_ok(rootdev)) {\n\t\tif (!netif_carrier_ok(dev))\n\t\t\tnetif_carrier_on(dev);\n\t} else {\n\t\tif (netif_carrier_ok(dev))\n\t\t\tnetif_carrier_off(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\n#ifdef CONFIG_SYSFS\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\n\tBUG_ON(count < 1);\n\n\trx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\n\tif (!rx) {\n\t\trx = vzalloc(sz);\n\t\tif (!rx)\n\t\t\treturn -ENOMEM;\n\t}\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++)\n\t\trx[i].dev = dev;\n\treturn 0;\n}\n#endif\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\n\tif (!tx) {\n\t\ttx = vzalloc(sz);\n\t\tif (!tx)\n\t\t\treturn -ENOMEM;\n\t}\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_uninit;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= NETIF_F_SOFT_FEATURES;\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK)) {\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\t}\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\trollback_registered(dev);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n/**\n * netdev_wait_allrefs - wait until all references are gone.\n * @dev: target net_device\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tmsleep(250);\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\trtnl_lock();\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t__rtnl_unlock();\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tpr_err(\"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\t\tWARN_ON(dev->dn_ptr);\n\n\t\tif (dev->destructor)\n\t\t\tdev->destructor(dev);\n\n\t\t/* Report a network device has been unregistered */\n\t\trtnl_lock();\n\t\tdev_net(dev)->dev_unreg_count--;\n\t\t__rtnl_unlock();\n\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n\tBUILD_BUG_ON(sizeof(*stats64) < sizeof(*netdev_stats));\n\tmemcpy(stats64, netdev_stats, sizeof(*stats64));\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + sizeof(*netdev_stats), 0,\n\t       sizeof(*stats64) - sizeof(*netdev_stats));\n#else\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(unsigned long);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n#endif\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += atomic_long_read(&dev->rx_dropped);\n\tstorage->tx_dropped += atomic_long_read(&dev->tx_dropped);\n\tstorage->rx_nohandler += atomic_long_read(&dev->rx_nohandler);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n *\talloc_netdev_mqs - allocate network device\n *\t@sizeof_priv:\t\tsize of private data to allocate space for\n *\t@name:\t\t\tdevice name format string\n *\t@name_assign_type: \torigin of device name\n *\t@setup:\t\t\tcallback to initialize device\n *\t@txqs:\t\t\tthe number of TX subqueues to allocate\n *\t@rxqs:\t\t\tthe number of RX subqueues to allocate\n *\n *\tAllocates a struct net_device with private data area for driver use\n *\tand performs basic initialization.  Also allocates subqueue structs\n *\tfor each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tsize_t alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYSFS\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n#endif\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kzalloc(alloc_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\n\tif (!p)\n\t\tp = vzalloc(alloc_size);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->gso_min_segs = 0;\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->all_adj_list.upper);\n\tINIT_LIST_HEAD(&dev->all_adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = 1;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n#ifdef CONFIG_SYSFS\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n#endif\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_ingress_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n *\tfree_netdev - free network device\n *\t@dev: device\n *\n *\tThis function does the last stage of destroying an allocated device\n * \tinterface. The reference to the device object is released.\n *\tIf this is the last reference then it will be freed.\n *\tMust be called in process context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\tnetif_free_tx_queues(dev);\n#ifdef CONFIG_SYSFS\n\tkvfree(dev->_rx);\n#endif\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\trollback_registered(dev);\n\t\t/* Finish processing unregister after unlock */\n\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tif (!list_empty(head)) {\n\t\trollback_registered_many(head);\n\t\tlist_for_each_entry(dev, head, unreg_list)\n\t\t\tnet_set_todo(dev);\n\t\tlist_del(head);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(dev_net(dev), net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\tif (dev_get_valid_name(net, dev, pat) < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\terr = -ENODEV;\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t   this device. They should clean all the things.\n\n\t   Note that dev->reg_state stays at NETREG_REGISTERED.\n\t   This is wanted because this way 8021q and macvlan know\n\t   the device is just moving and can keep their slaves up.\n\t*/\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\trtmsg_ifinfo(RTM_DELLINK, dev, ~0U, GFP_KERNEL);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex))\n\t\tdev->ifindex = dev_new_index(net);\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_callback(struct notifier_block *nfb,\n\t\t\t    unsigned long action,\n\t\t\t    void *ocpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu, oldcpu = (unsigned long)ocpu;\n\tstruct softnet_data *sd, *oldsd;\n\n\tif (action != CPU_DEAD && action != CPU_DEAD_FROZEN)\n\t\treturn NOTIFY_OK;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn NOTIFY_OK;\n}\n\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc(sizeof(*hash) * NETDEV_HASHENTRIES, GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tif (net != &init_net)\n\t\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit rtnl_lock_unregistering(struct list_head *net_list)\n{\n\t/* Return with the rtnl_lock held when there are no network\n\t * devices unregistering in any network namespace in net_list.\n\t */\n\tstruct net *net;\n\tbool unregistering;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tadd_wait_queue(&netdev_unregistering_wq, &wait);\n\tfor (;;) {\n\t\tunregistering = false;\n\t\trtnl_lock();\n\t\tlist_for_each_entry(net, net_list, exit_list) {\n\t\t\tif (net->dev_unreg_count > 0) {\n\t\t\t\tunregistering = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!unregistering)\n\t\t\tbreak;\n\t\t__rtnl_unlock();\n\n\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n\t}\n\tremove_wait_queue(&netdev_unregistering_wq, &wait);\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\t/* To prevent network device cleanup code from dereferencing\n\t * loopback devices or network devices that have been freed\n\t * wait here for all pending unregistrations to complete,\n\t * before unregistring the loopback device and allowing the\n\t * network namespace be freed.\n\t *\n\t * The netdev todo list containing all network devices\n\t * unregistrations that happen in default_device_exit_batch\n\t * will run in the rtnl_unlock() at the end of\n\t * default_device_exit_batch.\n\t */\n\trtnl_lock_unregistering(net_list);\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tINIT_LIST_HEAD(&offload_base);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tsd->csd.func = rps_trigger_softirq;\n\t\tsd->csd.info = sd;\n\t\tsd->cpu = i;\n#endif\n\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\thotcpu_notifier(dev_cpu_callback, 0);\n\tdst_subsys_init();\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPF_INET protocol family socket handler.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Changes (see also sock.c)\n *\n *\t\tpiggy,\n *\t\tKarl Knutson\t:\tSocket protocol table\n *\t\tA.N.Kuznetsov\t:\tSocket death error in accept().\n *\t\tJohn Richardson :\tFix non blocking error in connect()\n *\t\t\t\t\tso sockets that fail to connect\n *\t\t\t\t\tdon't return -EINPROGRESS.\n *\t\tAlan Cox\t:\tAsynchronous I/O support\n *\t\tAlan Cox\t:\tKeep correct socket pointer on sock\n *\t\t\t\t\tstructures\n *\t\t\t\t\twhen accept() ed\n *\t\tAlan Cox\t:\tSemantics of SO_LINGER aren't state\n *\t\t\t\t\tmoved to close when you look carefully.\n *\t\t\t\t\tWith this fixed and the accept bug fixed\n *\t\t\t\t\tsome RPC stuff seems happier.\n *\t\tNiibe Yutaka\t:\t4.4BSD style write async I/O\n *\t\tAlan Cox,\n *\t\tTony Gale \t:\tFixed reuse semantics.\n *\t\tAlan Cox\t:\tbind() shouldn't abort existing but dead\n *\t\t\t\t\tsockets. Stops FTP netin:.. I hope.\n *\t\tAlan Cox\t:\tbind() works correctly for RAW sockets.\n *\t\t\t\t\tNote that FreeBSD at least was broken\n *\t\t\t\t\tin this respect so be careful with\n *\t\t\t\t\tcompatibility tests...\n *\t\tAlan Cox\t:\trouting cache support\n *\t\tAlan Cox\t:\tmemzero the socket structure for\n *\t\t\t\t\tcompactness.\n *\t\tMatt Day\t:\tnonblock connect error handler\n *\t\tAlan Cox\t:\tAllow large numbers of pending sockets\n *\t\t\t\t\t(eg for big web sites), but only if\n *\t\t\t\t\tspecifically application requested.\n *\t\tAlan Cox\t:\tNew buffering throughout IP. Used\n *\t\t\t\t\tdumbly.\n *\t\tAlan Cox\t:\tNew buffering now used smartly.\n *\t\tAlan Cox\t:\tBSD rather than common sense\n *\t\t\t\t\tinterpretation of listen.\n *\t\tGermano Caronni\t:\tAssorted small races.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg basic support.\n *\t\tAlan Cox\t:\tOnly sendmsg/recvmsg now supported.\n *\t\tAlan Cox\t:\tLocked down bind (see security list).\n *\t\tAlan Cox\t:\tLoosened bind a little.\n *\t\tMike McLagan\t:\tADD/DEL DLCI Ioctls\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tSome other random speedups.\n *\t\tCyrus Durgin\t:\tCleaned up file for kmod hacks.\n *\t\tAndi Kleen\t:\tFix inet_stream_connect TCP race.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"IPv4: \" fmt\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/inet.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <net/checksum.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/arp.h>\n#include <net/route.h>\n#include <net/ip_fib.h>\n#include <net/inet_connection_sock.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <net/ping.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/raw.h>\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/ip_tunnels.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/secure_seq.h>\n#ifdef CONFIG_IP_MROUTE\n#include <linux/mroute.h>\n#endif\n#include <net/l3mdev.h>\n\n\n/* The inetsw table contains everything that inet_create needs to\n * build a new socket.\n */\nstatic struct list_head inetsw[SOCK_MAX];\nstatic DEFINE_SPINLOCK(inetsw_lock);\n\n/* New destruction routine */\n\nvoid inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(rcu_dereference_protected(inet->inet_opt, 1));\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tdst_release(sk->sk_rx_dst);\n\tsk_refcnt_debug_dec(sk);\n}\nEXPORT_SYMBOL(inet_sock_destruct);\n\n/*\n *\tThe routines beyond this point handle the behaviour of an AF_INET\n *\tsocket object. Mostly it punts to the subprotocols of IP to do\n *\tthe work.\n */\n\n/*\n *\tAutomatically bind an unbound socket.\n */\n\nstatic int inet_autobind(struct sock *sk)\n{\n\tstruct inet_sock *inet;\n\t/* We may need to bind the socket. */\n\tlock_sock(sk);\n\tinet = inet_sk(sk);\n\tif (!inet->inet_num) {\n\t\tif (sk->sk_prot->get_port(sk, 0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t}\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/*\n *\tMove a socket into listening state.\n */\nint inet_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\tunsigned char old_state;\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)\n\t\tgoto out;\n\n\told_state = sk->sk_state;\n\tif (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\tgoto out;\n\n\t/* Really, if the socket is already in listen state\n\t * we can only allow the backlog to be adjusted.\n\t */\n\tif (old_state != TCP_LISTEN) {\n\t\t/* Check special setups for testing purpose to enable TFO w/o\n\t\t * requiring TCP_FASTOPEN sockopt.\n\t\t * Note that only TCP sockets (SOCK_STREAM) will reach here.\n\t\t * Also fastopenq may already been allocated because this\n\t\t * socket was in TCP_LISTEN state previously but was\n\t\t * shutdown() (rather than close()).\n\t\t */\n\t\tif ((sysctl_tcp_fastopen & TFO_SERVER_ENABLE) != 0 &&\n\t\t    !inet_csk(sk)->icsk_accept_queue.fastopenq.max_qlen) {\n\t\t\tif ((sysctl_tcp_fastopen & TFO_SERVER_WO_SOCKOPT1) != 0)\n\t\t\t\tfastopen_queue_tune(sk, backlog);\n\t\t\telse if ((sysctl_tcp_fastopen &\n\t\t\t\t  TFO_SERVER_WO_SOCKOPT2) != 0)\n\t\t\t\tfastopen_queue_tune(sk,\n\t\t\t\t    ((uint)sysctl_tcp_fastopen) >> 16);\n\n\t\t\ttcp_fastopen_init_key_once(true);\n\t\t}\n\t\terr = inet_csk_listen_start(sk, backlog);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\tsk->sk_max_ack_backlog = backlog;\n\terr = 0;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_listen);\n\n/*\n *\tCreate an inet socket.\n */\n\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\n\t\t       int kern)\n{\n\tstruct sock *sk;\n\tstruct inet_protosw *answer;\n\tstruct inet_sock *inet;\n\tstruct proto *answer_prot;\n\tunsigned char answer_flags;\n\tint try_loading_module = 0;\n\tint err;\n\n\tif (protocol < 0 || protocol >= IPPROTO_MAX)\n\t\treturn -EINVAL;\n\n\tsock->state = SS_UNCONNECTED;\n\n\t/* Look for the requested type/protocol pair. */\nlookup_protocol:\n\terr = -ESOCKTNOSUPPORT;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(answer, &inetsw[sock->type], list) {\n\n\t\terr = 0;\n\t\t/* Check the non-wild match. */\n\t\tif (protocol == answer->protocol) {\n\t\t\tif (protocol != IPPROTO_IP)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* Check for the two wild cases. */\n\t\t\tif (IPPROTO_IP == protocol) {\n\t\t\t\tprotocol = answer->protocol;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IPPROTO_IP == answer->protocol)\n\t\t\t\tbreak;\n\t\t}\n\t\terr = -EPROTONOSUPPORT;\n\t}\n\n\tif (unlikely(err)) {\n\t\tif (try_loading_module < 2) {\n\t\t\trcu_read_unlock();\n\t\t\t/*\n\t\t\t * Be more specific, e.g. net-pf-2-proto-132-type-1\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)\n\t\t\t */\n\t\t\tif (++try_loading_module == 1)\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d-type-%d\",\n\t\t\t\t\t       PF_INET, protocol, sock->type);\n\t\t\t/*\n\t\t\t * Fall back to generic, e.g. net-pf-2-proto-132\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP)\n\t\t\t */\n\t\t\telse\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d\",\n\t\t\t\t\t       PF_INET, protocol);\n\t\t\tgoto lookup_protocol;\n\t\t} else\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\terr = -EPERM;\n\tif (sock->type == SOCK_RAW && !kern &&\n\t    !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out_rcu_unlock;\n\n\tsock->ops = answer->ops;\n\tanswer_prot = answer->prot;\n\tanswer_flags = answer->flags;\n\trcu_read_unlock();\n\n\tWARN_ON(!answer_prot->slab);\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot, kern);\n\tif (!sk)\n\t\tgoto out;\n\n\terr = 0;\n\tif (INET_PROTOSW_REUSE & answer_flags)\n\t\tsk->sk_reuse = SK_CAN_REUSE;\n\n\tinet = inet_sk(sk);\n\tinet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;\n\n\tinet->nodefrag = 0;\n\n\tif (SOCK_RAW == sock->type) {\n\t\tinet->inet_num = protocol;\n\t\tif (IPPROTO_RAW == protocol)\n\t\t\tinet->hdrincl = 1;\n\t}\n\n\tif (net->ipv4.sysctl_ip_no_pmtu_disc)\n\t\tinet->pmtudisc = IP_PMTUDISC_DONT;\n\telse\n\t\tinet->pmtudisc = IP_PMTUDISC_WANT;\n\n\tinet->inet_id = 0;\n\n\tsock_init_data(sock, sk);\n\n\tsk->sk_destruct\t   = inet_sock_destruct;\n\tsk->sk_protocol\t   = protocol;\n\tsk->sk_backlog_rcv = sk->sk_prot->backlog_rcv;\n\n\tinet->uc_ttl\t= -1;\n\tinet->mc_loop\t= 1;\n\tinet->mc_ttl\t= 1;\n\tinet->mc_all\t= 1;\n\tinet->mc_index\t= 0;\n\tinet->mc_list\t= NULL;\n\tinet->rcv_tos\t= 0;\n\n\tsk_refcnt_debug_inc(sk);\n\n\tif (inet->inet_num) {\n\t\t/* It assumes that any protocol which allows\n\t\t * the user to assign a number at socket\n\t\t * creation time automatically\n\t\t * shares.\n\t\t */\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t\t/* Add to protocol hash chains. */\n\t\terr = sk->sk_prot->hash(sk);\n\t\tif (err) {\n\t\t\tsk_common_release(sk);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (sk->sk_prot->init) {\n\t\terr = sk->sk_prot->init(sk);\n\t\tif (err)\n\t\t\tsk_common_release(sk);\n\t}\nout:\n\treturn err;\nout_rcu_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\n\n\n/*\n *\tThe peer socket should always be NULL (or else). When we call this\n *\tfunction we are destroying the object and from then on nobody\n *\tshould refer to it.\n */\nint inet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk) {\n\t\tlong timeout;\n\n\t\t/* Applications forget to leave groups before exiting */\n\t\tip_mc_drop_socket(sk);\n\n\t\t/* If linger is set, we don't return until the close\n\t\t * is complete.  Otherwise we return immediately. The\n\t\t * actually closing is done the same either way.\n\t\t *\n\t\t * If the close is due to the process exiting, we never\n\t\t * linger..\n\t\t */\n\t\ttimeout = 0;\n\t\tif (sock_flag(sk, SOCK_LINGER) &&\n\t\t    !(current->flags & PF_EXITING))\n\t\t\ttimeout = sk->sk_lingertime;\n\t\tsock->sk = NULL;\n\t\tsk->sk_prot->close(sk, timeout);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_release);\n\nint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *addr = (struct sockaddr_in *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned short snum;\n\tint chk_addr_ret;\n\tu32 tb_id = RT_TABLE_LOCAL;\n\tint err;\n\n\t/* If the socket has its own bind function then use it. (RAW) */\n\tif (sk->sk_prot->bind) {\n\t\terr = sk->sk_prot->bind(sk, uaddr, addr_len);\n\t\tgoto out;\n\t}\n\terr = -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\tgoto out;\n\n\tif (addr->sin_family != AF_INET) {\n\t\t/* Compatibility games : accept AF_UNSPEC (mapped to AF_INET)\n\t\t * only if s_addr is INADDR_ANY.\n\t\t */\n\t\terr = -EAFNOSUPPORT;\n\t\tif (addr->sin_family != AF_UNSPEC ||\n\t\t    addr->sin_addr.s_addr != htonl(INADDR_ANY))\n\t\t\tgoto out;\n\t}\n\n\ttb_id = l3mdev_fib_table_by_index(net, sk->sk_bound_dev_if) ? : tb_id;\n\tchk_addr_ret = inet_addr_type_table(net, addr->sin_addr.s_addr, tb_id);\n\n\t/* Not specified by any standard per-se, however it breaks too\n\t * many applications when removed.  It is unfortunate since\n\t * allowing applications to make a non-local bind solves\n\t * several problems with systems using dynamic addressing.\n\t * (ie. your servers still start up even if your ISDN link\n\t *  is temporarily down)\n\t */\n\terr = -EADDRNOTAVAIL;\n\tif (!net->ipv4.sysctl_ip_nonlocal_bind &&\n\t    !(inet->freebind || inet->transparent) &&\n\t    addr->sin_addr.s_addr != htonl(INADDR_ANY) &&\n\t    chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST &&\n\t    chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tsnum = ntohs(addr->sin_port);\n\terr = -EACCES;\n\tif (snum && snum < PROT_SOCK &&\n\t    !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE))\n\t\tgoto out;\n\n\t/*      We keep a pair of addresses. rcv_saddr is the one\n\t *      used by hash lookups, and saddr is used for transmit.\n\t *\n\t *      In the BSD API these are the same except where it\n\t *      would be illegal to use them (multicast/broadcast) in\n\t *      which case the sending device address is used.\n\t */\n\tlock_sock(sk);\n\n\t/* Check these errors (active socket, double bind). */\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE || inet->inet_num)\n\t\tgoto out_release_sock;\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->sin_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\n\t/* Make sure we are allowed to bind here. */\n\tif ((snum || !inet->bind_address_no_port) &&\n\t    sk->sk_prot->get_port(sk, snum)) {\n\t\tinet->inet_saddr = inet->inet_rcv_saddr = 0;\n\t\terr = -EADDRINUSE;\n\t\tgoto out_release_sock;\n\t}\n\n\tif (inet->inet_rcv_saddr)\n\t\tsk->sk_userlocks |= SOCK_BINDADDR_LOCK;\n\tif (snum)\n\t\tsk->sk_userlocks |= SOCK_BINDPORT_LOCK;\n\tinet->inet_sport = htons(inet->inet_num);\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsk_dst_reset(sk);\n\terr = 0;\nout_release_sock:\n\trelease_sock(sk);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_bind);\n\nint inet_dgram_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn sk->sk_prot->disconnect(sk, flags);\n\n\tif (!inet_sk(sk)->inet_num && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn sk->sk_prot->connect(sk, uaddr, addr_len);\n}\nEXPORT_SYMBOL(inet_dgram_connect);\n\nstatic long inet_wait_for_connect(struct sock *sk, long timeo, int writebias)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\tsk->sk_write_pending += writebias;\n\n\t/* Basic assumption: if someone sets sk->sk_err, he _must_\n\t * change state of the socket from TCP_SYN_*.\n\t * Connect() does not allow to get error notifications\n\t * without closing the socket.\n\t */\n\twhile ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\trelease_sock(sk);\n\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock(sk);\n\t\tif (signal_pending(current) || !timeo)\n\t\t\tbreak;\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\tsk->sk_write_pending -= writebias;\n\treturn timeo;\n}\n\n/*\n *\tConnect to a remote host. There is regrettably still a little\n *\tTCP 'magic' in here.\n */\nint __inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\tlong timeo;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\n\tif (uaddr->sa_family == AF_UNSPEC) {\n\t\terr = sk->sk_prot->disconnect(sk, flags);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tgoto out;\n\t}\n\n\tswitch (sock->state) {\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\tcase SS_CONNECTED:\n\t\terr = -EISCONN;\n\t\tgoto out;\n\tcase SS_CONNECTING:\n\t\terr = -EALREADY;\n\t\t/* Fall out of switch with err, set for this state */\n\t\tbreak;\n\tcase SS_UNCONNECTED:\n\t\terr = -EISCONN;\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\tgoto out;\n\n\t\terr = sk->sk_prot->connect(sk, uaddr, addr_len);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tsock->state = SS_CONNECTING;\n\n\t\t/* Just entered SS_CONNECTING state; the only\n\t\t * difference is that return value in non-blocking\n\t\t * case is EINPROGRESS, rather than EALREADY.\n\t\t */\n\t\terr = -EINPROGRESS;\n\t\tbreak;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\tint writebias = (sk->sk_protocol == IPPROTO_TCP) &&\n\t\t\t\ttcp_sk(sk)->fastopen_req &&\n\t\t\t\ttcp_sk(sk)->fastopen_req->data ? 1 : 0;\n\n\t\t/* Error code is set above */\n\t\tif (!timeo || !inet_wait_for_connect(sk, timeo, writebias))\n\t\t\tgoto out;\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/* Connection was closed by RST, timeout, ICMP error\n\t * or another process disconnected us.\n\t */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto sock_error;\n\n\t/* sk->sk_err may be not zero now, if RECVERR was ordered by user\n\t * and error was received after socket entered established state.\n\t * Hence, it is handled normally after connect() return successfully.\n\t */\n\n\tsock->state = SS_CONNECTED;\n\terr = 0;\nout:\n\treturn err;\n\nsock_error:\n\terr = sock_error(sk) ? : -ECONNABORTED;\n\tsock->state = SS_UNCONNECTED;\n\tif (sk->sk_prot->disconnect(sk, flags))\n\t\tsock->state = SS_DISCONNECTING;\n\tgoto out;\n}\nEXPORT_SYMBOL(__inet_stream_connect);\n\nint inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint addr_len, int flags)\n{\n\tint err;\n\n\tlock_sock(sock->sk);\n\terr = __inet_stream_connect(sock, uaddr, addr_len, flags);\n\trelease_sock(sock->sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_stream_connect);\n\n/*\n *\tAccept a pending connection. The TCP layer now gives BSD semantics.\n */\n\nint inet_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_accept);\n\n\n/*\n *\tThis does both peername and sockname.\n */\nint inet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint *uaddr_len, int peer)\n{\n\tstruct sock *sk\t\t= sock->sk;\n\tstruct inet_sock *inet\t= inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, uaddr);\n\n\tsin->sin_family = AF_INET;\n\tif (peer) {\n\t\tif (!inet->inet_dport ||\n\t\t    (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&\n\t\t     peer == 1))\n\t\t\treturn -ENOTCONN;\n\t\tsin->sin_port = inet->inet_dport;\n\t\tsin->sin_addr.s_addr = inet->inet_daddr;\n\t} else {\n\t\t__be32 addr = inet->inet_rcv_saddr;\n\t\tif (!addr)\n\t\t\taddr = inet->inet_saddr;\n\t\tsin->sin_port = inet->inet_sport;\n\t\tsin->sin_addr.s_addr = addr;\n\t}\n\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t*uaddr_len = sizeof(*sin);\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_getname);\n\nint inet_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\treturn sk->sk_prot->sendmsg(sk, msg, size);\n}\nEXPORT_SYMBOL(inet_sendmsg);\n\nssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\tif (sk->sk_prot->sendpage)\n\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}\nEXPORT_SYMBOL(inet_sendpage);\n\nint inet_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\tsock_rps_record_flow(sk);\n\n\terr = sk->sk_prot->recvmsg(sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_recvmsg);\n\nint inet_shutdown(struct socket *sock, int how)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\t/* This should really check to make sure\n\t * the socket is a TCP socket. (WHY AC...)\n\t */\n\thow++; /* maps 0->1 has the advantage of making bit 1 rcvs and\n\t\t       1->2 bit 2 snds.\n\t\t       2->3 */\n\tif ((how & ~SHUTDOWN_MASK) || !how)\t/* MAXINT->0 */\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tif (sock->state == SS_CONNECTING) {\n\t\tif ((1 << sk->sk_state) &\n\t\t    (TCPF_SYN_SENT | TCPF_SYN_RECV | TCPF_CLOSE))\n\t\t\tsock->state = SS_DISCONNECTING;\n\t\telse\n\t\t\tsock->state = SS_CONNECTED;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\terr = -ENOTCONN;\n\t\t/* Hack to wake up other listeners, who can poll for\n\t\t   POLLHUP, even on eg. unconnected UDP sockets -- RR */\n\tdefault:\n\t\tsk->sk_shutdown |= how;\n\t\tif (sk->sk_prot->shutdown)\n\t\t\tsk->sk_prot->shutdown(sk, how);\n\t\tbreak;\n\n\t/* Remaining two branches are temporary solution for missing\n\t * close() in multithreaded environment. It is _not_ a good idea,\n\t * but we have no choice until close() is repaired at VFS level.\n\t */\n\tcase TCP_LISTEN:\n\t\tif (!(how & RCV_SHUTDOWN))\n\t\t\tbreak;\n\t\t/* Fall through */\n\tcase TCP_SYN_SENT:\n\t\terr = sk->sk_prot->disconnect(sk, O_NONBLOCK);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tbreak;\n\t}\n\n\t/* Wake up anyone sleeping in poll. */\n\tsk->sk_state_change(sk);\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_shutdown);\n\n/*\n *\tioctl() calls you can issue on an INET socket. Most of these are\n *\tdevice configuration and stuff and very rarely used. Some ioctls\n *\tpass on to the socket itself.\n *\n *\tNOTE: I like the idea of a module for the config stuff. ie ifconfig\n *\tloads the devconfigure module does its configuring and unloads it.\n *\tThere's a good 20K of config code hanging around the kernel.\n */\n\nint inet_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\tstruct net *net = sock_net(sk);\n\n\tswitch (cmd) {\n\tcase SIOCGSTAMP:\n\t\terr = sock_get_timestamp(sk, (struct timeval __user *)arg);\n\t\tbreak;\n\tcase SIOCGSTAMPNS:\n\t\terr = sock_get_timestampns(sk, (struct timespec __user *)arg);\n\t\tbreak;\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCRTMSG:\n\t\terr = ip_rt_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\t\terr = arp_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFPFLAGS:\n\tcase SIOCGIFPFLAGS:\n\tcase SIOCSIFFLAGS:\n\t\terr = devinet_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tif (sk->sk_prot->ioctl)\n\t\t\terr = sk->sk_prot->ioctl(sk, cmd, arg);\n\t\telse\n\t\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(inet_ioctl);\n\n#ifdef CONFIG_COMPAT\nstatic int inet_compat_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = -ENOIOCTLCMD;\n\n\tif (sk->sk_prot->compat_ioctl)\n\t\terr = sk->sk_prot->compat_ioctl(sk, cmd, arg);\n\n\treturn err;\n}\n#endif\n\nconst struct proto_ops inet_stream_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = tcp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = inet_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n\t.splice_read\t   = tcp_splice_read,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_stream_ops);\n\nconst struct proto_ops inet_dgram_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = udp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_dgram_ops);\n\n/*\n * For SOCK_RAW sockets; should be the same as inet_dgram_ops but without\n * udp_poll\n */\nstatic const struct proto_ops inet_sockraw_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\n\nstatic const struct net_proto_family inet_family_ops = {\n\t.family = PF_INET,\n\t.create = inet_create,\n\t.owner\t= THIS_MODULE,\n};\n\n/* Upon startup we insert all the elements in inetsw_array[] into\n * the linked list inetsw.\n */\nstatic struct inet_protosw inetsw_array[] =\n{\n\t{\n\t\t.type =       SOCK_STREAM,\n\t\t.protocol =   IPPROTO_TCP,\n\t\t.prot =       &tcp_prot,\n\t\t.ops =        &inet_stream_ops,\n\t\t.flags =      INET_PROTOSW_PERMANENT |\n\t\t\t      INET_PROTOSW_ICSK,\n\t},\n\n\t{\n\t\t.type =       SOCK_DGRAM,\n\t\t.protocol =   IPPROTO_UDP,\n\t\t.prot =       &udp_prot,\n\t\t.ops =        &inet_dgram_ops,\n\t\t.flags =      INET_PROTOSW_PERMANENT,\n       },\n\n       {\n\t\t.type =       SOCK_DGRAM,\n\t\t.protocol =   IPPROTO_ICMP,\n\t\t.prot =       &ping_prot,\n\t\t.ops =        &inet_dgram_ops,\n\t\t.flags =      INET_PROTOSW_REUSE,\n       },\n\n       {\n\t       .type =       SOCK_RAW,\n\t       .protocol =   IPPROTO_IP,\t/* wild card */\n\t       .prot =       &raw_prot,\n\t       .ops =        &inet_sockraw_ops,\n\t       .flags =      INET_PROTOSW_REUSE,\n       }\n};\n\n#define INETSW_ARRAY_LEN ARRAY_SIZE(inetsw_array)\n\nvoid inet_register_protosw(struct inet_protosw *p)\n{\n\tstruct list_head *lh;\n\tstruct inet_protosw *answer;\n\tint protocol = p->protocol;\n\tstruct list_head *last_perm;\n\n\tspin_lock_bh(&inetsw_lock);\n\n\tif (p->type >= SOCK_MAX)\n\t\tgoto out_illegal;\n\n\t/* If we are trying to override a permanent protocol, bail. */\n\tlast_perm = &inetsw[p->type];\n\tlist_for_each(lh, &inetsw[p->type]) {\n\t\tanswer = list_entry(lh, struct inet_protosw, list);\n\t\t/* Check only the non-wild match. */\n\t\tif ((INET_PROTOSW_PERMANENT & answer->flags) == 0)\n\t\t\tbreak;\n\t\tif (protocol == answer->protocol)\n\t\t\tgoto out_permanent;\n\t\tlast_perm = lh;\n\t}\n\n\t/* Add the new entry after the last permanent entry if any, so that\n\t * the new entry does not override a permanent entry when matched with\n\t * a wild-card protocol. But it is allowed to override any existing\n\t * non-permanent entry.  This means that when we remove this entry, the\n\t * system automatically returns to the old behavior.\n\t */\n\tlist_add_rcu(&p->list, last_perm);\nout:\n\tspin_unlock_bh(&inetsw_lock);\n\n\treturn;\n\nout_permanent:\n\tpr_err(\"Attempt to override permanent protocol %d\\n\", protocol);\n\tgoto out;\n\nout_illegal:\n\tpr_err(\"Ignoring attempt to register invalid socket type %d\\n\",\n\t       p->type);\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_register_protosw);\n\nvoid inet_unregister_protosw(struct inet_protosw *p)\n{\n\tif (INET_PROTOSW_PERMANENT & p->flags) {\n\t\tpr_err(\"Attempt to unregister permanent protocol %d\\n\",\n\t\t       p->protocol);\n\t} else {\n\t\tspin_lock_bh(&inetsw_lock);\n\t\tlist_del_rcu(&p->list);\n\t\tspin_unlock_bh(&inetsw_lock);\n\n\t\tsynchronize_net();\n\t}\n}\nEXPORT_SYMBOL(inet_unregister_protosw);\n\nstatic int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 *fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\tstruct ip_options_rcu *inet_opt;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\n\t/* Query new route. */\n\tfl4 = &inet->cork.fl.u.ip4;\n\trt = ip_route_connect(fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = fl4->saddr;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sock_net(sk)->ipv4.sysctl_ip_dynaddr > 1) {\n\t\tpr_info(\"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t\t__func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\treturn __sk_prot_rehash(sk);\n}\n\nint inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tstruct ip_options_rcu *inet_opt;\n\tstruct flowi4 *fl4;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tdaddr = inet->inet_daddr;\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trcu_read_unlock();\n\tfl4 = &inet->cork.fl.u.ip4;\n\trt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sock_net(sk)->ipv4.sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(inet_sk_rebuild_header);\n\nstatic struct sk_buff *inet_gso_segment(struct sk_buff *skb,\n\t\t\t\t\tnetdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tconst struct net_offload *ops;\n\tunsigned int offset = 0;\n\tbool udpfrag, encap;\n\tstruct iphdr *iph;\n\tint proto;\n\tint nhoff;\n\tint ihl;\n\tint id;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t     ~(SKB_GSO_TCPV4 |\n\t\t       SKB_GSO_UDP |\n\t\t       SKB_GSO_DODGY |\n\t\t       SKB_GSO_TCP_ECN |\n\t\t       SKB_GSO_GRE |\n\t\t       SKB_GSO_GRE_CSUM |\n\t\t       SKB_GSO_IPIP |\n\t\t       SKB_GSO_SIT |\n\t\t       SKB_GSO_TCPV6 |\n\t\t       SKB_GSO_UDP_TUNNEL |\n\t\t       SKB_GSO_UDP_TUNNEL_CSUM |\n\t\t       SKB_GSO_TUNNEL_REMCSUM |\n\t\t       0)))\n\t\tgoto out;\n\n\tskb_reset_network_header(skb);\n\tnhoff = skb_network_header(skb) - skb_mac_header(skb);\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*iph))))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\tif (ihl < sizeof(*iph))\n\t\tgoto out;\n\n\tid = ntohs(iph->id);\n\tproto = iph->protocol;\n\n\t/* Warning: after this point, iph might be no longer valid */\n\tif (unlikely(!pskb_may_pull(skb, ihl)))\n\t\tgoto out;\n\t__skb_pull(skb, ihl);\n\n\tencap = SKB_GSO_CB(skb)->encap_level > 0;\n\tif (encap)\n\t\tfeatures &= skb->dev->hw_enc_features;\n\tSKB_GSO_CB(skb)->encap_level += ihl;\n\n\tskb_reset_transport_header(skb);\n\n\tsegs = ERR_PTR(-EPROTONOSUPPORT);\n\n\tif (skb->encapsulation &&\n\t    skb_shinfo(skb)->gso_type & (SKB_GSO_SIT|SKB_GSO_IPIP))\n\t\tudpfrag = proto == IPPROTO_UDP && encap;\n\telse\n\t\tudpfrag = proto == IPPROTO_UDP && !skb->encapsulation;\n\n\tops = rcu_dereference(inet_offloads[proto]);\n\tif (likely(ops && ops->callbacks.gso_segment))\n\t\tsegs = ops->callbacks.gso_segment(skb, features);\n\n\tif (IS_ERR_OR_NULL(segs))\n\t\tgoto out;\n\n\tskb = segs;\n\tdo {\n\t\tiph = (struct iphdr *)(skb_mac_header(skb) + nhoff);\n\t\tif (udpfrag) {\n\t\t\tiph->id = htons(id);\n\t\t\tiph->frag_off = htons(offset >> 3);\n\t\t\tif (skb->next)\n\t\t\t\tiph->frag_off |= htons(IP_MF);\n\t\t\toffset += skb->len - nhoff - ihl;\n\t\t} else {\n\t\t\tiph->id = htons(id++);\n\t\t}\n\t\tiph->tot_len = htons(skb->len - nhoff);\n\t\tip_send_check(iph);\n\t\tif (encap)\n\t\t\tskb_reset_inner_headers(skb);\n\t\tskb->network_header = (u8 *)iph - skb->head;\n\t} while ((skb = skb->next));\n\nout:\n\treturn segs;\n}\n\nstatic struct sk_buff **inet_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct net_offload *ops;\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct iphdr *iph;\n\tunsigned int hlen;\n\tunsigned int off;\n\tunsigned int id;\n\tint flush = 1;\n\tint proto;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*iph);\n\tiph = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tiph = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!iph))\n\t\t\tgoto out;\n\t}\n\n\tproto = iph->protocol;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_offloads[proto]);\n\tif (!ops || !ops->callbacks.gro_receive)\n\t\tgoto out_unlock;\n\n\tif (*(u8 *)iph != 0x45)\n\t\tgoto out_unlock;\n\n\tif (unlikely(ip_fast_csum((u8 *)iph, 5)))\n\t\tgoto out_unlock;\n\n\tid = ntohl(*(__be32 *)&iph->id);\n\tflush = (u16)((ntohl(*(__be32 *)iph) ^ skb_gro_len(skb)) | (id & ~IP_DF));\n\tid >>= 16;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tstruct iphdr *iph2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tiph2 = (struct iphdr *)(p->data + off);\n\t\t/* The above works because, with the exception of the top\n\t\t * (inner most) layer, we only aggregate pkts with the same\n\t\t * hdr length so all the hdrs we'll need to verify will start\n\t\t * at the same offset.\n\t\t */\n\t\tif ((iph->protocol ^ iph2->protocol) |\n\t\t    ((__force u32)iph->saddr ^ (__force u32)iph2->saddr) |\n\t\t    ((__force u32)iph->daddr ^ (__force u32)iph2->daddr)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* All fields must match except length and checksum. */\n\t\tNAPI_GRO_CB(p)->flush |=\n\t\t\t(iph->ttl ^ iph2->ttl) |\n\t\t\t(iph->tos ^ iph2->tos) |\n\t\t\t((iph->frag_off ^ iph2->frag_off) & htons(IP_DF));\n\n\t\t/* Save the IP ID check to be included later when we get to\n\t\t * the transport layer so only the inner most IP ID is checked.\n\t\t * This is because some GSO/TSO implementations do not\n\t\t * correctly increment the IP ID for the outer hdrs.\n\t\t */\n\t\tNAPI_GRO_CB(p)->flush_id =\n\t\t\t    ((u16)(ntohs(iph2->id) + NAPI_GRO_CB(p)->count) ^ id);\n\t\tNAPI_GRO_CB(p)->flush |= flush;\n\t}\n\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\tskb_set_network_header(skb, off);\n\t/* The above will be needed by the transport layer if there is one\n\t * immediately following this IP hdr.\n\t */\n\n\t/* Note : No need to call skb_gro_postpull_rcsum() here,\n\t * as we already checked checksum over ipv4 header was 0\n\t */\n\tskb_gro_pull(skb, sizeof(*iph));\n\tskb_set_transport_header(skb, skb_gro_offset(skb));\n\n\tpp = ops->callbacks.gro_receive(head, skb);\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic struct sk_buff **ipip_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->encap_mark) {\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\tNAPI_GRO_CB(skb)->encap_mark = 1;\n\n\treturn inet_gro_receive(head, skb);\n}\n\n#define SECONDS_PER_DAY\t86400\n\n/* inet_current_timestamp - Return IP network timestamp\n *\n * Return milliseconds since midnight in network byte order.\n */\n__be32 inet_current_timestamp(void)\n{\n\tu32 secs;\n\tu32 msecs;\n\tstruct timespec64 ts;\n\n\tktime_get_real_ts64(&ts);\n\n\t/* Get secs since midnight. */\n\t(void)div_u64_rem(ts.tv_sec, SECONDS_PER_DAY, &secs);\n\t/* Convert to msecs. */\n\tmsecs = secs * MSEC_PER_SEC;\n\t/* Convert nsec to msec. */\n\tmsecs += (u32)ts.tv_nsec / NSEC_PER_MSEC;\n\n\t/* Convert to network byte order. */\n\treturn htons(msecs);\n}\nEXPORT_SYMBOL(inet_current_timestamp);\n\nint inet_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len)\n{\n\tif (sk->sk_family == AF_INET)\n\t\treturn ip_recv_error(sk, msg, len, addr_len);\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\treturn pingv6_ops.ipv6_recv_error(sk, msg, len, addr_len);\n#endif\n\treturn -EINVAL;\n}\n\nstatic int inet_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\t__be16 newlen = htons(skb->len - nhoff);\n\tstruct iphdr *iph = (struct iphdr *)(skb->data + nhoff);\n\tconst struct net_offload *ops;\n\tint proto = iph->protocol;\n\tint err = -ENOSYS;\n\n\tif (skb->encapsulation)\n\t\tskb_set_inner_network_header(skb, nhoff);\n\n\tcsum_replace2(&iph->check, iph->tot_len, newlen);\n\tiph->tot_len = newlen;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_offloads[proto]);\n\tif (WARN_ON(!ops || !ops->callbacks.gro_complete))\n\t\tgoto out_unlock;\n\n\t/* Only need to add sizeof(*iph) to get to the next hdr below\n\t * because any hdr with option will have been flushed in\n\t * inet_gro_receive().\n\t */\n\terr = ops->callbacks.gro_complete(skb, nhoff + sizeof(*iph));\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic int ipip_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tskb->encapsulation = 1;\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_IPIP;\n\treturn inet_gro_complete(skb, nhoff);\n}\n\nint inet_ctl_sock_create(struct sock **sk, unsigned short family,\n\t\t\t unsigned short type, unsigned char protocol,\n\t\t\t struct net *net)\n{\n\tstruct socket *sock;\n\tint rc = sock_create_kern(net, family, type, protocol, &sock);\n\n\tif (rc == 0) {\n\t\t*sk = sock->sk;\n\t\t(*sk)->sk_allocation = GFP_ATOMIC;\n\t\t/*\n\t\t * Unhash it so that IP input processing does not even see it,\n\t\t * we do not wish this socket to see incoming packets.\n\t\t */\n\t\t(*sk)->sk_prot->unhash(*sk);\n\t}\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(inet_ctl_sock_create);\n\nu64 snmp_get_cpu_field(void __percpu *mib, int cpu, int offt)\n{\n\treturn  *(((unsigned long *)per_cpu_ptr(mib, cpu)) + offt);\n}\nEXPORT_SYMBOL_GPL(snmp_get_cpu_field);\n\nunsigned long snmp_fold_field(void __percpu *mib, int offt)\n{\n\tunsigned long res = 0;\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tres += snmp_get_cpu_field(mib, i, offt);\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field);\n\n#if BITS_PER_LONG==32\n\nu64 snmp_get_cpu_field64(void __percpu *mib, int cpu, int offt,\n\t\t\t size_t syncp_offset)\n{\n\tvoid *bhptr;\n\tstruct u64_stats_sync *syncp;\n\tu64 v;\n\tunsigned int start;\n\n\tbhptr = per_cpu_ptr(mib, cpu);\n\tsyncp = (struct u64_stats_sync *)(bhptr + syncp_offset);\n\tdo {\n\t\tstart = u64_stats_fetch_begin_irq(syncp);\n\t\tv = *(((u64 *)bhptr) + offt);\n\t} while (u64_stats_fetch_retry_irq(syncp, start));\n\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(snmp_get_cpu_field64);\n\nu64 snmp_fold_field64(void __percpu *mib, int offt, size_t syncp_offset)\n{\n\tu64 res = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tres += snmp_get_cpu_field64(mib, cpu, offt, syncp_offset);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field64);\n#endif\n\n#ifdef CONFIG_IP_MULTICAST\nstatic const struct net_protocol igmp_protocol = {\n\t.handler =\tigmp_rcv,\n\t.netns_ok =\t1,\n};\n#endif\n\nstatic const struct net_protocol tcp_protocol = {\n\t.early_demux\t=\ttcp_v4_early_demux,\n\t.handler\t=\ttcp_v4_rcv,\n\t.err_handler\t=\ttcp_v4_err,\n\t.no_policy\t=\t1,\n\t.netns_ok\t=\t1,\n\t.icmp_strict_tag_validation = 1,\n};\n\nstatic const struct net_protocol udp_protocol = {\n\t.early_demux =\tudp_v4_early_demux,\n\t.handler =\tudp_rcv,\n\t.err_handler =\tudp_err,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic const struct net_protocol icmp_protocol = {\n\t.handler =\ticmp_rcv,\n\t.err_handler =\ticmp_err,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic __net_init int ipv4_mib_init_net(struct net *net)\n{\n\tint i;\n\n\tnet->mib.tcp_statistics = alloc_percpu(struct tcp_mib);\n\tif (!net->mib.tcp_statistics)\n\t\tgoto err_tcp_mib;\n\tnet->mib.ip_statistics = alloc_percpu(struct ipstats_mib);\n\tif (!net->mib.ip_statistics)\n\t\tgoto err_ip_mib;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct ipstats_mib *af_inet_stats;\n\t\taf_inet_stats = per_cpu_ptr(net->mib.ip_statistics, i);\n\t\tu64_stats_init(&af_inet_stats->syncp);\n\t}\n\n\tnet->mib.net_statistics = alloc_percpu(struct linux_mib);\n\tif (!net->mib.net_statistics)\n\t\tgoto err_net_mib;\n\tnet->mib.udp_statistics = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udp_statistics)\n\t\tgoto err_udp_mib;\n\tnet->mib.udplite_statistics = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udplite_statistics)\n\t\tgoto err_udplite_mib;\n\tnet->mib.icmp_statistics = alloc_percpu(struct icmp_mib);\n\tif (!net->mib.icmp_statistics)\n\t\tgoto err_icmp_mib;\n\tnet->mib.icmpmsg_statistics = kzalloc(sizeof(struct icmpmsg_mib),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!net->mib.icmpmsg_statistics)\n\t\tgoto err_icmpmsg_mib;\n\n\ttcp_mib_init(net);\n\treturn 0;\n\nerr_icmpmsg_mib:\n\tfree_percpu(net->mib.icmp_statistics);\nerr_icmp_mib:\n\tfree_percpu(net->mib.udplite_statistics);\nerr_udplite_mib:\n\tfree_percpu(net->mib.udp_statistics);\nerr_udp_mib:\n\tfree_percpu(net->mib.net_statistics);\nerr_net_mib:\n\tfree_percpu(net->mib.ip_statistics);\nerr_ip_mib:\n\tfree_percpu(net->mib.tcp_statistics);\nerr_tcp_mib:\n\treturn -ENOMEM;\n}\n\nstatic __net_exit void ipv4_mib_exit_net(struct net *net)\n{\n\tkfree(net->mib.icmpmsg_statistics);\n\tfree_percpu(net->mib.icmp_statistics);\n\tfree_percpu(net->mib.udplite_statistics);\n\tfree_percpu(net->mib.udp_statistics);\n\tfree_percpu(net->mib.net_statistics);\n\tfree_percpu(net->mib.ip_statistics);\n\tfree_percpu(net->mib.tcp_statistics);\n}\n\nstatic __net_initdata struct pernet_operations ipv4_mib_ops = {\n\t.init = ipv4_mib_init_net,\n\t.exit = ipv4_mib_exit_net,\n};\n\nstatic int __init init_ipv4_mibs(void)\n{\n\treturn register_pernet_subsys(&ipv4_mib_ops);\n}\n\nstatic __net_init int inet_init_net(struct net *net)\n{\n\t/*\n\t * Set defaults for local port range\n\t */\n\tseqlock_init(&net->ipv4.ip_local_ports.lock);\n\tnet->ipv4.ip_local_ports.range[0] =  32768;\n\tnet->ipv4.ip_local_ports.range[1] =  60999;\n\n\tseqlock_init(&net->ipv4.ping_group_range.lock);\n\t/*\n\t * Sane defaults - nobody may create ping sockets.\n\t * Boot scripts should set this to distro-specific group.\n\t */\n\tnet->ipv4.ping_group_range.range[0] = make_kgid(&init_user_ns, 1);\n\tnet->ipv4.ping_group_range.range[1] = make_kgid(&init_user_ns, 0);\n\treturn 0;\n}\n\nstatic __net_exit void inet_exit_net(struct net *net)\n{\n}\n\nstatic __net_initdata struct pernet_operations af_inet_ops = {\n\t.init = inet_init_net,\n\t.exit = inet_exit_net,\n};\n\nstatic int __init init_inet_pernet_ops(void)\n{\n\treturn register_pernet_subsys(&af_inet_ops);\n}\n\nstatic int ipv4_proc_init(void);\n\n/*\n *\tIP protocol layer initialiser\n */\n\nstatic struct packet_offload ip_packet_offload __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.callbacks = {\n\t\t.gso_segment = inet_gso_segment,\n\t\t.gro_receive = inet_gro_receive,\n\t\t.gro_complete = inet_gro_complete,\n\t},\n};\n\nstatic const struct net_offload ipip_offload = {\n\t.callbacks = {\n\t\t.gso_segment\t= inet_gso_segment,\n\t\t.gro_receive\t= ipip_gro_receive,\n\t\t.gro_complete\t= ipip_gro_complete,\n\t},\n};\n\nstatic int __init ipv4_offload_init(void)\n{\n\t/*\n\t * Add offloads\n\t */\n\tif (udpv4_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add UDP protocol offload\\n\", __func__);\n\tif (tcpv4_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add TCP protocol offload\\n\", __func__);\n\n\tdev_add_offload(&ip_packet_offload);\n\tinet_add_offload(&ipip_offload, IPPROTO_IPIP);\n\treturn 0;\n}\n\nfs_initcall(ipv4_offload_init);\n\nstatic struct packet_type ip_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.func = ip_rcv,\n};\n\nstatic int __init inet_init(void)\n{\n\tstruct inet_protosw *q;\n\tstruct list_head *r;\n\tint rc = -EINVAL;\n\n\tsock_skb_cb_check_size(sizeof(struct inet_skb_parm));\n\n\trc = proto_register(&tcp_prot, 1);\n\tif (rc)\n\t\tgoto out;\n\n\trc = proto_register(&udp_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_tcp_proto;\n\n\trc = proto_register(&raw_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_udp_proto;\n\n\trc = proto_register(&ping_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_raw_proto;\n\n\t/*\n\t *\tTell SOCKET that we are alive...\n\t */\n\n\t(void)sock_register(&inet_family_ops);\n\n#ifdef CONFIG_SYSCTL\n\tip_static_sysctl_init();\n#endif\n\n\t/*\n\t *\tAdd all the base protocols.\n\t */\n\n\tif (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)\n\t\tpr_crit(\"%s: Cannot add ICMP protocol\\n\", __func__);\n\tif (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)\n\t\tpr_crit(\"%s: Cannot add UDP protocol\\n\", __func__);\n\tif (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) < 0)\n\t\tpr_crit(\"%s: Cannot add TCP protocol\\n\", __func__);\n#ifdef CONFIG_IP_MULTICAST\n\tif (inet_add_protocol(&igmp_protocol, IPPROTO_IGMP) < 0)\n\t\tpr_crit(\"%s: Cannot add IGMP protocol\\n\", __func__);\n#endif\n\n\t/* Register the socket-side information for inet_create. */\n\tfor (r = &inetsw[0]; r < &inetsw[SOCK_MAX]; ++r)\n\t\tINIT_LIST_HEAD(r);\n\n\tfor (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)\n\t\tinet_register_protosw(q);\n\n\t/*\n\t *\tSet the ARP module up\n\t */\n\n\tarp_init();\n\n\t/*\n\t *\tSet the IP module up\n\t */\n\n\tip_init();\n\n\ttcp_v4_init();\n\n\t/* Setup TCP slab cache for open requests. */\n\ttcp_init();\n\n\t/* Setup UDP memory threshold */\n\tudp_init();\n\n\t/* Add UDP-Lite (RFC 3828) */\n\tudplite4_register();\n\n\tping_init();\n\n\t/*\n\t *\tSet the ICMP layer up\n\t */\n\n\tif (icmp_init() < 0)\n\t\tpanic(\"Failed to create the ICMP control socket.\\n\");\n\n\t/*\n\t *\tInitialise the multicast router\n\t */\n#if defined(CONFIG_IP_MROUTE)\n\tif (ip_mr_init())\n\t\tpr_crit(\"%s: Cannot init ipv4 mroute\\n\", __func__);\n#endif\n\n\tif (init_inet_pernet_ops())\n\t\tpr_crit(\"%s: Cannot init ipv4 inet pernet ops\\n\", __func__);\n\t/*\n\t *\tInitialise per-cpu ipv4 mibs\n\t */\n\n\tif (init_ipv4_mibs())\n\t\tpr_crit(\"%s: Cannot init ipv4 mibs\\n\", __func__);\n\n\tipv4_proc_init();\n\n\tipfrag_init();\n\n\tdev_add_pack(&ip_packet_type);\n\n\tip_tunnel_core_init();\n\n\trc = 0;\nout:\n\treturn rc;\nout_unregister_raw_proto:\n\tproto_unregister(&raw_prot);\nout_unregister_udp_proto:\n\tproto_unregister(&udp_prot);\nout_unregister_tcp_proto:\n\tproto_unregister(&tcp_prot);\n\tgoto out;\n}\n\nfs_initcall(inet_init);\n\n/* ------------------------------------------------------------------------ */\n\n#ifdef CONFIG_PROC_FS\nstatic int __init ipv4_proc_init(void)\n{\n\tint rc = 0;\n\n\tif (raw_proc_init())\n\t\tgoto out_raw;\n\tif (tcp4_proc_init())\n\t\tgoto out_tcp;\n\tif (udp4_proc_init())\n\t\tgoto out_udp;\n\tif (ping_proc_init())\n\t\tgoto out_ping;\n\tif (ip_misc_proc_init())\n\t\tgoto out_misc;\nout:\n\treturn rc;\nout_misc:\n\tping_proc_exit();\nout_ping:\n\tudp4_proc_exit();\nout_udp:\n\ttcp4_proc_exit();\nout_tcp:\n\traw_proc_exit();\nout_raw:\n\trc = -ENOMEM;\n\tgoto out;\n}\n\n#else /* CONFIG_PROC_FS */\nstatic int __init ipv4_proc_init(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_PROC_FS */\n\nMODULE_ALIAS_NETPROTO(PF_INET);\n\n", "/*\n *\tIPV4 GSO/GRO offload support\n *\tLinux INET implementation\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *\tGRE GSO support\n */\n\n#include <linux/skbuff.h>\n#include <linux/init.h>\n#include <net/protocol.h>\n#include <net/gre.h>\n\nstatic struct sk_buff *gre_gso_segment(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\tint tnl_hlen = skb_inner_mac_header(skb) - skb_transport_header(skb);\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tu16 mac_offset = skb->mac_header;\n\t__be16 protocol = skb->protocol;\n\tu16 mac_len = skb->mac_len;\n\tint gre_offset, outer_hlen;\n\tbool need_csum, ufo;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t\t\t~(SKB_GSO_TCPV4 |\n\t\t\t\t  SKB_GSO_TCPV6 |\n\t\t\t\t  SKB_GSO_UDP |\n\t\t\t\t  SKB_GSO_DODGY |\n\t\t\t\t  SKB_GSO_TCP_ECN |\n\t\t\t\t  SKB_GSO_GRE |\n\t\t\t\t  SKB_GSO_GRE_CSUM |\n\t\t\t\t  SKB_GSO_IPIP |\n\t\t\t\t  SKB_GSO_SIT)))\n\t\tgoto out;\n\n\tif (!skb->encapsulation)\n\t\tgoto out;\n\n\tif (unlikely(tnl_hlen < sizeof(struct gre_base_hdr)))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, tnl_hlen)))\n\t\tgoto out;\n\n\t/* setup inner skb. */\n\tskb->encapsulation = 0;\n\t__skb_pull(skb, tnl_hlen);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, skb_inner_network_offset(skb));\n\tskb->mac_len = skb_inner_network_offset(skb);\n\tskb->protocol = skb->inner_protocol;\n\n\tneed_csum = !!(skb_shinfo(skb)->gso_type & SKB_GSO_GRE_CSUM);\n\tskb->encap_hdr_csum = need_csum;\n\n\tufo = !!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP);\n\n\tfeatures &= skb->dev->hw_enc_features;\n\n\t/* The only checksum offload we care about from here on out is the\n\t * outer one so strip the existing checksum feature flags based\n\t * on the fact that we will be computing our checksum in software.\n\t */\n\tif (ufo) {\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\t\tif (!need_csum)\n\t\t\tfeatures |= NETIF_F_HW_CSUM;\n\t}\n\n\t/* segment inner packet. */\n\tsegs = skb_mac_gso_segment(skb, features);\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tskb_gso_error_unwind(skb, protocol, tnl_hlen, mac_offset,\n\t\t\t\t     mac_len);\n\t\tgoto out;\n\t}\n\n\touter_hlen = skb_tnl_header_len(skb);\n\tgre_offset = outer_hlen - tnl_hlen;\n\tskb = segs;\n\tdo {\n\t\tstruct gre_base_hdr *greh;\n\t\t__be32 *pcsum;\n\n\t\t/* Set up inner headers if we are offloading inner checksum */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_reset_inner_headers(skb);\n\t\t\tskb->encapsulation = 1;\n\t\t}\n\n\t\tskb->mac_len = mac_len;\n\t\tskb->protocol = protocol;\n\n\t\t__skb_push(skb, outer_hlen);\n\t\tskb_reset_mac_header(skb);\n\t\tskb_set_network_header(skb, mac_len);\n\t\tskb_set_transport_header(skb, gre_offset);\n\n\t\tif (!need_csum)\n\t\t\tcontinue;\n\n\t\tgreh = (struct gre_base_hdr *)skb_transport_header(skb);\n\t\tpcsum = (__be32 *)(greh + 1);\n\n\t\t*pcsum = 0;\n\t\t*(__sum16 *)pcsum = gso_make_checksum(skb, 0);\n\t} while ((skb = skb->next));\nout:\n\treturn segs;\n}\n\nstatic struct sk_buff **gre_gro_receive(struct sk_buff **head,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct gre_base_hdr *greh;\n\tunsigned int hlen, grehlen;\n\tunsigned int off;\n\tint flush = 1;\n\tstruct packet_offload *ptype;\n\t__be16 type;\n\n\tif (NAPI_GRO_CB(skb)->encap_mark)\n\t\tgoto out;\n\n\tNAPI_GRO_CB(skb)->encap_mark = 1;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*greh);\n\tgreh = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out;\n\t}\n\n\t/* Only support version 0 and K (key), C (csum) flags. Note that\n\t * although the support for the S (seq#) flag can be added easily\n\t * for GRO, this is problematic for GSO hence can not be enabled\n\t * here because a GRO pkt may end up in the forwarding path, thus\n\t * requiring GSO support to break it up correctly.\n\t */\n\tif ((greh->flags & ~(GRE_KEY|GRE_CSUM)) != 0)\n\t\tgoto out;\n\n\ttype = greh->protocol;\n\n\trcu_read_lock();\n\tptype = gro_find_receive_by_type(type);\n\tif (!ptype)\n\t\tgoto out_unlock;\n\n\tgrehlen = GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_KEY)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_CSUM)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\thlen = off + grehlen;\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Don't bother verifying checksum if we're going to flush anyway. */\n\tif ((greh->flags & GRE_CSUM) && !NAPI_GRO_CB(skb)->flush) {\n\t\tif (skb_gro_checksum_simple_validate(skb))\n\t\t\tgoto out_unlock;\n\n\t\tskb_gro_checksum_try_convert(skb, IPPROTO_GRE, 0,\n\t\t\t\t\t     null_compute_pseudo);\n\t}\n\n\tfor (p = *head; p; p = p->next) {\n\t\tconst struct gre_base_hdr *greh2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\t/* The following checks are needed to ensure only pkts\n\t\t * from the same tunnel are considered for aggregation.\n\t\t * The criteria for \"the same tunnel\" includes:\n\t\t * 1) same version (we only support version 0 here)\n\t\t * 2) same protocol (we only support ETH_P_IP for now)\n\t\t * 3) same set of flags\n\t\t * 4) same key if the key field is present.\n\t\t */\n\t\tgreh2 = (struct gre_base_hdr *)(p->data + off);\n\n\t\tif (greh2->flags != greh->flags ||\n\t\t    greh2->protocol != greh->protocol) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (greh->flags & GRE_KEY) {\n\t\t\t/* compare keys */\n\t\t\tif (*(__be32 *)(greh2+1) != *(__be32 *)(greh+1)) {\n\t\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, grehlen);\n\n\t/* Adjusted NAPI_GRO_CB(skb)->csum after skb_gro_pull()*/\n\tskb_gro_postpull_rcsum(skb, greh, grehlen);\n\n\tpp = ptype->callbacks.gro_receive(head, skb);\n\tflush = 0;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic int gre_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tstruct gre_base_hdr *greh = (struct gre_base_hdr *)(skb->data + nhoff);\n\tstruct packet_offload *ptype;\n\tunsigned int grehlen = sizeof(*greh);\n\tint err = -ENOENT;\n\t__be16 type;\n\n\tskb->encapsulation = 1;\n\tskb_shinfo(skb)->gso_type = SKB_GSO_GRE;\n\n\ttype = greh->protocol;\n\tif (greh->flags & GRE_KEY)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_CSUM)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\trcu_read_lock();\n\tptype = gro_find_complete_by_type(type);\n\tif (ptype)\n\t\terr = ptype->callbacks.gro_complete(skb, nhoff + grehlen);\n\n\trcu_read_unlock();\n\n\tskb_set_inner_mac_header(skb, nhoff + grehlen);\n\n\treturn err;\n}\n\nstatic const struct net_offload gre_offload = {\n\t.callbacks = {\n\t\t.gso_segment = gre_gso_segment,\n\t\t.gro_receive = gre_gro_receive,\n\t\t.gro_complete = gre_gro_complete,\n\t},\n};\n\nstatic int __init gre_offload_init(void)\n{\n\treturn inet_add_offload(&gre_offload, IPPROTO_GRE);\n}\ndevice_initcall(gre_offload_init);\n", "/*\n *\tIPV4 GSO/GRO offload support\n *\tLinux INET implementation\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *\tUDPv4 GSO support\n */\n\n#include <linux/skbuff.h>\n#include <net/udp.h>\n#include <net/protocol.h>\n\nstatic DEFINE_SPINLOCK(udp_offload_lock);\nstatic struct udp_offload_priv __rcu *udp_offload_base __read_mostly;\n\n#define udp_deref_protected(X) rcu_dereference_protected(X, lockdep_is_held(&udp_offload_lock))\n\nstruct udp_offload_priv {\n\tstruct udp_offload\t*offload;\n\tpossible_net_t\tnet;\n\tstruct rcu_head\t\trcu;\n\tstruct udp_offload_priv __rcu *next;\n};\n\nstatic struct sk_buff *__skb_udp_tunnel_segment(struct sk_buff *skb,\n\tnetdev_features_t features,\n\tstruct sk_buff *(*gso_inner_segment)(struct sk_buff *skb,\n\t\t\t\t\t     netdev_features_t features),\n\t__be16 new_protocol, bool is_ipv6)\n{\n\tint tnl_hlen = skb_inner_mac_header(skb) - skb_transport_header(skb);\n\tbool remcsum, need_csum, offload_csum, ufo;\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct udphdr *uh = udp_hdr(skb);\n\tu16 mac_offset = skb->mac_header;\n\t__be16 protocol = skb->protocol;\n\tu16 mac_len = skb->mac_len;\n\tint udp_offset, outer_hlen;\n\t__wsum partial;\n\n\tif (unlikely(!pskb_may_pull(skb, tnl_hlen)))\n\t\tgoto out;\n\n\t/* Adjust partial header checksum to negate old length.\n\t * We cannot rely on the value contained in uh->len as it is\n\t * possible that the actual value exceeds the boundaries of the\n\t * 16 bit length field due to the header being added outside of an\n\t * IP or IPv6 frame that was already limited to 64K - 1.\n\t */\n\tpartial = csum_sub(csum_unfold(uh->check),\n\t\t\t   (__force __wsum)htonl(skb->len));\n\n\t/* setup inner skb. */\n\tskb->encapsulation = 0;\n\t__skb_pull(skb, tnl_hlen);\n\tskb_reset_mac_header(skb);\n\tskb_set_network_header(skb, skb_inner_network_offset(skb));\n\tskb->mac_len = skb_inner_network_offset(skb);\n\tskb->protocol = new_protocol;\n\n\tneed_csum = !!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM);\n\tskb->encap_hdr_csum = need_csum;\n\n\tremcsum = !!(skb_shinfo(skb)->gso_type & SKB_GSO_TUNNEL_REMCSUM);\n\tskb->remcsum_offload = remcsum;\n\n\tufo = !!(skb_shinfo(skb)->gso_type & SKB_GSO_UDP);\n\n\t/* Try to offload checksum if possible */\n\toffload_csum = !!(need_csum &&\n\t\t\t  (skb->dev->features &\n\t\t\t   (is_ipv6 ? (NETIF_F_HW_CSUM | NETIF_F_IPV6_CSUM) :\n\t\t\t\t      (NETIF_F_HW_CSUM | NETIF_F_IP_CSUM))));\n\n\tfeatures &= skb->dev->hw_enc_features;\n\n\t/* The only checksum offload we care about from here on out is the\n\t * outer one so strip the existing checksum feature flags and\n\t * instead set the flag based on our outer checksum offload value.\n\t */\n\tif (remcsum || ufo) {\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\t\tif (!need_csum || offload_csum)\n\t\t\tfeatures |= NETIF_F_HW_CSUM;\n\t}\n\n\t/* segment inner packet. */\n\tsegs = gso_inner_segment(skb, features);\n\tif (IS_ERR_OR_NULL(segs)) {\n\t\tskb_gso_error_unwind(skb, protocol, tnl_hlen, mac_offset,\n\t\t\t\t     mac_len);\n\t\tgoto out;\n\t}\n\n\touter_hlen = skb_tnl_header_len(skb);\n\tudp_offset = outer_hlen - tnl_hlen;\n\tskb = segs;\n\tdo {\n\t\t__be16 len;\n\n\t\tif (remcsum)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t/* Set up inner headers if we are offloading inner checksum */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_reset_inner_headers(skb);\n\t\t\tskb->encapsulation = 1;\n\t\t}\n\n\t\tskb->mac_len = mac_len;\n\t\tskb->protocol = protocol;\n\n\t\t__skb_push(skb, outer_hlen);\n\t\tskb_reset_mac_header(skb);\n\t\tskb_set_network_header(skb, mac_len);\n\t\tskb_set_transport_header(skb, udp_offset);\n\t\tlen = htons(skb->len - udp_offset);\n\t\tuh = udp_hdr(skb);\n\t\tuh->len = len;\n\n\t\tif (!need_csum)\n\t\t\tcontinue;\n\n\t\tuh->check = ~csum_fold(csum_add(partial, (__force __wsum)len));\n\n\t\tif (skb->encapsulation || !offload_csum) {\n\t\t\tuh->check = gso_make_checksum(skb, ~uh->check);\n\t\t\tif (uh->check == 0)\n\t\t\t\tuh->check = CSUM_MANGLED_0;\n\t\t} else {\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\t}\n\t} while ((skb = skb->next));\nout:\n\treturn segs;\n}\n\nstruct sk_buff *skb_udp_tunnel_segment(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features,\n\t\t\t\t       bool is_ipv6)\n{\n\t__be16 protocol = skb->protocol;\n\tconst struct net_offload **offloads;\n\tconst struct net_offload *ops;\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct sk_buff *(*gso_inner_segment)(struct sk_buff *skb,\n\t\t\t\t\t     netdev_features_t features);\n\n\trcu_read_lock();\n\n\tswitch (skb->inner_protocol_type) {\n\tcase ENCAP_TYPE_ETHER:\n\t\tprotocol = skb->inner_protocol;\n\t\tgso_inner_segment = skb_mac_gso_segment;\n\t\tbreak;\n\tcase ENCAP_TYPE_IPPROTO:\n\t\toffloads = is_ipv6 ? inet6_offloads : inet_offloads;\n\t\tops = rcu_dereference(offloads[skb->inner_ipproto]);\n\t\tif (!ops || !ops->callbacks.gso_segment)\n\t\t\tgoto out_unlock;\n\t\tgso_inner_segment = ops->callbacks.gso_segment;\n\t\tbreak;\n\tdefault:\n\t\tgoto out_unlock;\n\t}\n\n\tsegs = __skb_udp_tunnel_segment(skb, features, gso_inner_segment,\n\t\t\t\t\tprotocol, is_ipv6);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn segs;\n}\n\nstatic struct sk_buff *udp4_ufo_fragment(struct sk_buff *skb,\n\t\t\t\t\t netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\t__wsum csum;\n\tstruct udphdr *uh;\n\tstruct iphdr *iph;\n\n\tif (skb->encapsulation &&\n\t    (skb_shinfo(skb)->gso_type &\n\t     (SKB_GSO_UDP_TUNNEL|SKB_GSO_UDP_TUNNEL_CSUM))) {\n\t\tsegs = skb_udp_tunnel_segment(skb, features, false);\n\t\tgoto out;\n\t}\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto out;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP | SKB_GSO_DODGY |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL |\n\t\t\t\t      SKB_GSO_UDP_TUNNEL_CSUM |\n\t\t\t\t      SKB_GSO_TUNNEL_REMCSUM |\n\t\t\t\t      SKB_GSO_IPIP |\n\t\t\t\t      SKB_GSO_GRE | SKB_GSO_GRE_CSUM) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\t/* Do software UFO. Complete and fill in the UDP checksum as\n\t * HW cannot do checksum of UDP packets sent as multiple\n\t * IP fragments.\n\t */\n\n\tuh = udp_hdr(skb);\n\tiph = ip_hdr(skb);\n\n\tuh->check = 0;\n\tcsum = skb_checksum(skb, 0, skb->len, 0);\n\tuh->check = udp_v4_check(skb->len, iph->saddr, iph->daddr, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t/* If there is no outer header we can fake a checksum offload\n\t * due to the fact that we have already done the checksum in\n\t * software prior to segmenting the frame.\n\t */\n\tif (!skb->encap_hdr_csum)\n\t\tfeatures |= NETIF_F_HW_CSUM;\n\n\t/* Fragment the skb. IP headers of the fragments are updated in\n\t * inet_gso_segment()\n\t */\n\tsegs = skb_segment(skb, features);\nout:\n\treturn segs;\n}\n\nint udp_add_offload(struct net *net, struct udp_offload *uo)\n{\n\tstruct udp_offload_priv *new_offload = kzalloc(sizeof(*new_offload), GFP_ATOMIC);\n\n\tif (!new_offload)\n\t\treturn -ENOMEM;\n\n\twrite_pnet(&new_offload->net, net);\n\tnew_offload->offload = uo;\n\n\tspin_lock(&udp_offload_lock);\n\tnew_offload->next = udp_offload_base;\n\trcu_assign_pointer(udp_offload_base, new_offload);\n\tspin_unlock(&udp_offload_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_add_offload);\n\nstatic void udp_offload_free_routine(struct rcu_head *head)\n{\n\tstruct udp_offload_priv *ou_priv = container_of(head, struct udp_offload_priv, rcu);\n\tkfree(ou_priv);\n}\n\nvoid udp_del_offload(struct udp_offload *uo)\n{\n\tstruct udp_offload_priv __rcu **head = &udp_offload_base;\n\tstruct udp_offload_priv *uo_priv;\n\n\tspin_lock(&udp_offload_lock);\n\n\tuo_priv = udp_deref_protected(*head);\n\tfor (; uo_priv != NULL;\n\t     uo_priv = udp_deref_protected(*head)) {\n\t\tif (uo_priv->offload == uo) {\n\t\t\trcu_assign_pointer(*head,\n\t\t\t\t\t   udp_deref_protected(uo_priv->next));\n\t\t\tgoto unlock;\n\t\t}\n\t\thead = &uo_priv->next;\n\t}\n\tpr_warn(\"udp_del_offload: didn't find offload for port %d\\n\", ntohs(uo->port));\nunlock:\n\tspin_unlock(&udp_offload_lock);\n\tif (uo_priv)\n\t\tcall_rcu(&uo_priv->rcu, udp_offload_free_routine);\n}\nEXPORT_SYMBOL(udp_del_offload);\n\nstruct sk_buff **udp_gro_receive(struct sk_buff **head, struct sk_buff *skb,\n\t\t\t\t struct udphdr *uh)\n{\n\tstruct udp_offload_priv *uo_priv;\n\tstruct sk_buff *p, **pp = NULL;\n\tstruct udphdr *uh2;\n\tunsigned int off = skb_gro_offset(skb);\n\tint flush = 1;\n\n\tif (NAPI_GRO_CB(skb)->encap_mark ||\n\t    (skb->ip_summed != CHECKSUM_PARTIAL &&\n\t     NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t     !NAPI_GRO_CB(skb)->csum_valid))\n\t\tgoto out;\n\n\t/* mark that this skb passed once through the tunnel gro layer */\n\tNAPI_GRO_CB(skb)->encap_mark = 1;\n\n\trcu_read_lock();\n\tuo_priv = rcu_dereference(udp_offload_base);\n\tfor (; uo_priv != NULL; uo_priv = rcu_dereference(uo_priv->next)) {\n\t\tif (net_eq(read_pnet(&uo_priv->net), dev_net(skb->dev)) &&\n\t\t    uo_priv->offload->port == uh->dest &&\n\t\t    uo_priv->offload->callbacks.gro_receive)\n\t\t\tgoto unflush;\n\t}\n\tgoto out_unlock;\n\nunflush:\n\tflush = 0;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tuh2 = (struct udphdr   *)(p->data + off);\n\n\t\t/* Match ports and either checksums are either both zero\n\t\t * or nonzero.\n\t\t */\n\t\tif ((*(u32 *)&uh->source != *(u32 *)&uh2->source) ||\n\t\t    (!uh->check ^ !uh2->check)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, sizeof(struct udphdr)); /* pull encapsulating udp header */\n\tskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\n\tNAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;\n\tpp = uo_priv->offload->callbacks.gro_receive(head, skb,\n\t\t\t\t\t\t     uo_priv->offload);\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\treturn pp;\n}\n\nstatic struct sk_buff **udp4_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct udphdr *uh = udp_gro_udphdr(skb);\n\n\tif (unlikely(!uh))\n\t\tgoto flush;\n\n\t/* Don't bother verifying checksum if we're going to flush anyway. */\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto skip;\n\n\tif (skb_gro_checksum_validate_zero_check(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t inet_gro_compute_pseudo))\n\t\tgoto flush;\n\telse if (uh->check)\n\t\tskb_gro_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t     inet_gro_compute_pseudo);\nskip:\n\tNAPI_GRO_CB(skb)->is_ipv6 = 0;\n\treturn udp_gro_receive(head, skb, uh);\n\nflush:\n\tNAPI_GRO_CB(skb)->flush = 1;\n\treturn NULL;\n}\n\nint udp_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tstruct udp_offload_priv *uo_priv;\n\t__be16 newlen = htons(skb->len - nhoff);\n\tstruct udphdr *uh = (struct udphdr *)(skb->data + nhoff);\n\tint err = -ENOSYS;\n\n\tuh->len = newlen;\n\n\trcu_read_lock();\n\n\tuo_priv = rcu_dereference(udp_offload_base);\n\tfor (; uo_priv != NULL; uo_priv = rcu_dereference(uo_priv->next)) {\n\t\tif (net_eq(read_pnet(&uo_priv->net), dev_net(skb->dev)) &&\n\t\t    uo_priv->offload->port == uh->dest &&\n\t\t    uo_priv->offload->callbacks.gro_complete)\n\t\t\tbreak;\n\t}\n\n\tif (uo_priv) {\n\t\tNAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;\n\t\terr = uo_priv->offload->callbacks.gro_complete(skb,\n\t\t\t\tnhoff + sizeof(struct udphdr),\n\t\t\t\tuo_priv->offload);\n\t}\n\n\trcu_read_unlock();\n\n\tif (skb->remcsum_offload)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TUNNEL_REMCSUM;\n\n\tskb->encapsulation = 1;\n\tskb_set_inner_mac_header(skb, nhoff + sizeof(struct udphdr));\n\n\treturn err;\n}\n\nstatic int udp4_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct udphdr *uh = (struct udphdr *)(skb->data + nhoff);\n\n\tif (uh->check) {\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_UDP_TUNNEL_CSUM;\n\t\tuh->check = ~udp_v4_check(skb->len - nhoff, iph->saddr,\n\t\t\t\t\t  iph->daddr, 0);\n\t} else {\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_UDP_TUNNEL;\n\t}\n\n\treturn udp_gro_complete(skb, nhoff);\n}\n\nstatic const struct net_offload udpv4_offload = {\n\t.callbacks = {\n\t\t.gso_segment = udp4_ufo_fragment,\n\t\t.gro_receive  =\tudp4_gro_receive,\n\t\t.gro_complete =\tudp4_gro_complete,\n\t},\n};\n\nint __init udpv4_offload_init(void)\n{\n\treturn inet_add_offload(&udpv4_offload, IPPROTO_UDP);\n}\n", "/*\n *\tIPV6 GSO/GRO offload support\n *\tLinux INET6 implementation\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/kernel.h>\n#include <linux/socket.h>\n#include <linux/netdevice.h>\n#include <linux/skbuff.h>\n#include <linux/printk.h>\n\n#include <net/protocol.h>\n#include <net/ipv6.h>\n\n#include \"ip6_offload.h\"\n\nstatic int ipv6_gso_pull_exthdrs(struct sk_buff *skb, int proto)\n{\n\tconst struct net_offload *ops = NULL;\n\n\tfor (;;) {\n\t\tstruct ipv6_opt_hdr *opth;\n\t\tint len;\n\n\t\tif (proto != NEXTHDR_HOP) {\n\t\t\tops = rcu_dereference(inet6_offloads[proto]);\n\n\t\t\tif (unlikely(!ops))\n\t\t\t\tbreak;\n\n\t\t\tif (!(ops->flags & INET6_PROTO_GSO_EXTHDR))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!pskb_may_pull(skb, 8)))\n\t\t\tbreak;\n\n\t\topth = (void *)skb->data;\n\t\tlen = ipv6_optlen(opth);\n\n\t\tif (unlikely(!pskb_may_pull(skb, len)))\n\t\t\tbreak;\n\n\t\topth = (void *)skb->data;\n\t\tproto = opth->nexthdr;\n\t\t__skb_pull(skb, len);\n\t}\n\n\treturn proto;\n}\n\nstatic struct sk_buff *ipv6_gso_segment(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct ipv6hdr *ipv6h;\n\tconst struct net_offload *ops;\n\tint proto;\n\tstruct frag_hdr *fptr;\n\tunsigned int unfrag_ip6hlen;\n\tu8 *prevhdr;\n\tint offset = 0;\n\tbool encap, udpfrag;\n\tint nhoff;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t     ~(SKB_GSO_TCPV4 |\n\t\t       SKB_GSO_UDP |\n\t\t       SKB_GSO_DODGY |\n\t\t       SKB_GSO_TCP_ECN |\n\t\t       SKB_GSO_GRE |\n\t\t       SKB_GSO_GRE_CSUM |\n\t\t       SKB_GSO_IPIP |\n\t\t       SKB_GSO_SIT |\n\t\t       SKB_GSO_UDP_TUNNEL |\n\t\t       SKB_GSO_UDP_TUNNEL_CSUM |\n\t\t       SKB_GSO_TUNNEL_REMCSUM |\n\t\t       SKB_GSO_TCPV6 |\n\t\t       0)))\n\t\tgoto out;\n\n\tskb_reset_network_header(skb);\n\tnhoff = skb_network_header(skb) - skb_mac_header(skb);\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*ipv6h))))\n\t\tgoto out;\n\n\tencap = SKB_GSO_CB(skb)->encap_level > 0;\n\tif (encap)\n\t\tfeatures &= skb->dev->hw_enc_features;\n\tSKB_GSO_CB(skb)->encap_level += sizeof(*ipv6h);\n\n\tipv6h = ipv6_hdr(skb);\n\t__skb_pull(skb, sizeof(*ipv6h));\n\tsegs = ERR_PTR(-EPROTONOSUPPORT);\n\n\tproto = ipv6_gso_pull_exthdrs(skb, ipv6h->nexthdr);\n\n\tif (skb->encapsulation &&\n\t    skb_shinfo(skb)->gso_type & (SKB_GSO_SIT|SKB_GSO_IPIP))\n\t\tudpfrag = proto == IPPROTO_UDP && encap;\n\telse\n\t\tudpfrag = proto == IPPROTO_UDP && !skb->encapsulation;\n\n\tops = rcu_dereference(inet6_offloads[proto]);\n\tif (likely(ops && ops->callbacks.gso_segment)) {\n\t\tskb_reset_transport_header(skb);\n\t\tsegs = ops->callbacks.gso_segment(skb, features);\n\t}\n\n\tif (IS_ERR(segs))\n\t\tgoto out;\n\n\tfor (skb = segs; skb; skb = skb->next) {\n\t\tipv6h = (struct ipv6hdr *)(skb_mac_header(skb) + nhoff);\n\t\tipv6h->payload_len = htons(skb->len - nhoff - sizeof(*ipv6h));\n\t\tskb->network_header = (u8 *)ipv6h - skb->head;\n\n\t\tif (udpfrag) {\n\t\t\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\t\t\tfptr = (struct frag_hdr *)((u8 *)ipv6h + unfrag_ip6hlen);\n\t\t\tfptr->frag_off = htons(offset);\n\t\t\tif (skb->next)\n\t\t\t\tfptr->frag_off |= htons(IP6_MF);\n\t\t\toffset += (ntohs(ipv6h->payload_len) -\n\t\t\t\t   sizeof(struct frag_hdr));\n\t\t}\n\t\tif (encap)\n\t\t\tskb_reset_inner_headers(skb);\n\t}\n\nout:\n\treturn segs;\n}\n\n/* Return the total length of all the extension hdrs, following the same\n * logic in ipv6_gso_pull_exthdrs() when parsing ext-hdrs.\n */\nstatic int ipv6_exthdrs_len(struct ipv6hdr *iph,\n\t\t\t    const struct net_offload **opps)\n{\n\tstruct ipv6_opt_hdr *opth = (void *)iph;\n\tint len = 0, proto, optlen = sizeof(*iph);\n\n\tproto = iph->nexthdr;\n\tfor (;;) {\n\t\tif (proto != NEXTHDR_HOP) {\n\t\t\t*opps = rcu_dereference(inet6_offloads[proto]);\n\t\t\tif (unlikely(!(*opps)))\n\t\t\t\tbreak;\n\t\t\tif (!((*opps)->flags & INET6_PROTO_GSO_EXTHDR))\n\t\t\t\tbreak;\n\t\t}\n\t\topth = (void *)opth + optlen;\n\t\toptlen = ipv6_optlen(opth);\n\t\tlen += optlen;\n\t\tproto = opth->nexthdr;\n\t}\n\treturn len;\n}\n\nstatic struct sk_buff **ipv6_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct net_offload *ops;\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tstruct ipv6hdr *iph;\n\tunsigned int nlen;\n\tunsigned int hlen;\n\tunsigned int off;\n\tu16 flush = 1;\n\tint proto;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*iph);\n\tiph = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tiph = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!iph))\n\t\t\tgoto out;\n\t}\n\n\tskb_set_network_header(skb, off);\n\tskb_gro_pull(skb, sizeof(*iph));\n\tskb_set_transport_header(skb, skb_gro_offset(skb));\n\n\tflush += ntohs(iph->payload_len) != skb_gro_len(skb);\n\n\trcu_read_lock();\n\tproto = iph->nexthdr;\n\tops = rcu_dereference(inet6_offloads[proto]);\n\tif (!ops || !ops->callbacks.gro_receive) {\n\t\t__pskb_pull(skb, skb_gro_offset(skb));\n\t\tproto = ipv6_gso_pull_exthdrs(skb, proto);\n\t\tskb_gro_pull(skb, -skb_transport_offset(skb));\n\t\tskb_reset_transport_header(skb);\n\t\t__skb_push(skb, skb_gro_offset(skb));\n\n\t\tops = rcu_dereference(inet6_offloads[proto]);\n\t\tif (!ops || !ops->callbacks.gro_receive)\n\t\t\tgoto out_unlock;\n\n\t\tiph = ipv6_hdr(skb);\n\t}\n\n\tNAPI_GRO_CB(skb)->proto = proto;\n\n\tflush--;\n\tnlen = skb_network_header_len(skb);\n\n\tfor (p = *head; p; p = p->next) {\n\t\tconst struct ipv6hdr *iph2;\n\t\t__be32 first_word; /* <Version:4><Traffic_Class:8><Flow_Label:20> */\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tiph2 = (struct ipv6hdr *)(p->data + off);\n\t\tfirst_word = *(__be32 *)iph ^ *(__be32 *)iph2;\n\n\t\t/* All fields must match except length and Traffic Class.\n\t\t * XXX skbs on the gro_list have all been parsed and pulled\n\t\t * already so we don't need to compare nlen\n\t\t * (nlen != (sizeof(*iph2) + ipv6_exthdrs_len(iph2, &ops)))\n\t\t * memcmp() alone below is suffcient, right?\n\t\t */\n\t\t if ((first_word & htonl(0xF00FFFFF)) ||\n\t\t    memcmp(&iph->nexthdr, &iph2->nexthdr,\n\t\t\t   nlen - offsetof(struct ipv6hdr, nexthdr))) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t\t/* flush if Traffic Class fields are different */\n\t\tNAPI_GRO_CB(p)->flush |= !!(first_word & htonl(0x0FF00000));\n\t\tNAPI_GRO_CB(p)->flush |= flush;\n\n\t\t/* Clear flush_id, there's really no concept of ID in IPv6. */\n\t\tNAPI_GRO_CB(p)->flush_id = 0;\n\t}\n\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\tskb_gro_postpull_rcsum(skb, iph, nlen);\n\n\tpp = ops->callbacks.gro_receive(head, skb);\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic struct sk_buff **sit_gro_receive(struct sk_buff **head,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->encap_mark) {\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\tNAPI_GRO_CB(skb)->encap_mark = 1;\n\n\treturn ipv6_gro_receive(head, skb);\n}\n\nstatic int ipv6_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tconst struct net_offload *ops;\n\tstruct ipv6hdr *iph = (struct ipv6hdr *)(skb->data + nhoff);\n\tint err = -ENOSYS;\n\n\tif (skb->encapsulation)\n\t\tskb_set_inner_network_header(skb, nhoff);\n\n\tiph->payload_len = htons(skb->len - nhoff - sizeof(*iph));\n\n\trcu_read_lock();\n\n\tnhoff += sizeof(*iph) + ipv6_exthdrs_len(iph, &ops);\n\tif (WARN_ON(!ops || !ops->callbacks.gro_complete))\n\t\tgoto out_unlock;\n\n\terr = ops->callbacks.gro_complete(skb, nhoff);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic int sit_gro_complete(struct sk_buff *skb, int nhoff)\n{\n\tskb->encapsulation = 1;\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_SIT;\n\treturn ipv6_gro_complete(skb, nhoff);\n}\n\nstatic struct packet_offload ipv6_packet_offload __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IPV6),\n\t.callbacks = {\n\t\t.gso_segment = ipv6_gso_segment,\n\t\t.gro_receive = ipv6_gro_receive,\n\t\t.gro_complete = ipv6_gro_complete,\n\t},\n};\n\nstatic const struct net_offload sit_offload = {\n\t.callbacks = {\n\t\t.gso_segment\t= ipv6_gso_segment,\n\t\t.gro_receive    = sit_gro_receive,\n\t\t.gro_complete   = sit_gro_complete,\n\t},\n};\n\nstatic int __init ipv6_offload_init(void)\n{\n\n\tif (tcpv6_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add TCP protocol offload\\n\", __func__);\n\tif (udp_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add UDP protocol offload\\n\", __func__);\n\tif (ipv6_exthdrs_offload_init() < 0)\n\t\tpr_crit(\"%s: Cannot add EXTHDRS protocol offload\\n\", __func__);\n\n\tdev_add_offload(&ipv6_packet_offload);\n\n\tinet_add_offload(&sit_offload, IPPROTO_IPV6);\n\n\treturn 0;\n}\n\nfs_initcall(ipv6_offload_init);\n"], "filenames": ["include/linux/netdevice.h", "net/core/dev.c", "net/ipv4/af_inet.c", "net/ipv4/gre_offload.c", "net/ipv4/udp_offload.c", "net/ipv6/ip6_offload.c"], "buggy_code_start_loc": [2099, 4441, 1382, 127, 314, 260], "buggy_code_end_loc": [2101, 4442, 1686, 127, 322, 306], "fixing_code_start_loc": [2099, 4441, 1383, 128, 314, 261], "fixing_code_end_loc": [2101, 4442, 1699, 133, 322, 319], "type": "CWE-400", "message": "The IP stack in the Linux kernel before 4.6 allows remote attackers to cause a denial of service (stack consumption and panic) or possibly have unspecified other impact by triggering use of the GRO path for packets with tunnel stacking, as demonstrated by interleaved IPv4 headers and GRE headers, a related issue to CVE-2016-7039.", "other": {"cve": {"id": "CVE-2016-8666", "sourceIdentifier": "meissner@suse.de", "published": "2016-10-16T21:59:15.523", "lastModified": "2023-01-17T21:36:11.550", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The IP stack in the Linux kernel before 4.6 allows remote attackers to cause a denial of service (stack consumption and panic) or possibly have unspecified other impact by triggering use of the GRO path for packets with tunnel stacking, as demonstrated by interleaved IPv4 headers and GRE headers, a related issue to CVE-2016-7039."}, {"lang": "es", "value": "La pila IP en el kernel de Linux en versiones anteriores a 4.6 permite a atacantes remotos provocar una denegaci\u00f3n de servicio (consumo de pila y p\u00e1nico) o tener otro posible impacto no especificado desencadenando uso de la ruta GRO para paquetes con apilamiento en t\u00fanel, como se demuestra por cabeceras IPv4 y cabeceras GRE intercaladas, un problema relacionado con CVE-2016-7039."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.8}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.14", "versionEndExcluding": "3.16.35", "matchCriteriaId": "94939292-97B9-46BE-BF06-57D0DB7A8904"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.47", "matchCriteriaId": "B1A82714-1C53-498D-94AA-DE9F6B577522"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.38", "matchCriteriaId": "755C626E-7669-4E6E-BC91-2656E4740E66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.29", "matchCriteriaId": "D23F7205-D265-429A-ACA9-F0FDAA8615A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.6", "matchCriteriaId": "628AFDA5-6C82-4DB8-8280-D1D7C58BBFE7"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=fac8e0f579695a3ecbc4d3cac369139d7f819971", "source": "meissner@suse.de", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2047.html", "source": "meissner@suse.de", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2107.html", "source": "meissner@suse.de", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2016-2110.html", "source": "meissner@suse.de", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2017-0004.html", "source": "meissner@suse.de", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/10/13/11", "source": "meissner@suse.de", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/93562", "source": "meissner@suse.de", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:0372", "source": "meissner@suse.de", "tags": ["Third Party Advisory"]}, {"url": "https://bto.bluecoat.com/security-advisory/sa134", "source": "meissner@suse.de", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1384991", "source": "meissner@suse.de", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://bugzilla.suse.com/show_bug.cgi?id=1001486", "source": "meissner@suse.de", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/fac8e0f579695a3ecbc4d3cac369139d7f819971", "source": "meissner@suse.de", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/fac8e0f579695a3ecbc4d3cac369139d7f819971"}}