{"buggy_code": ["<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Transformer XL\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, so we won't accept any new PRs changing its code. This model was deprecated due to security issues linked to `pickle.load`.\n\nWe recommend switching to more recent models for improved security.\n\nIn case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub:\n\n```\nfrom transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n\ncheckpoint = 'transfo-xl-wt103'\nrevision = '40a186da79458c9f9de846edfaea79c412137f97'\n\ntokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\nmodel = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n```\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.35.0.\nYou can do so by running the following command: `pip install -U transformers==4.35.0`.\n\n</Tip>\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=transfo-xl\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-transfo--xl-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/transfo-xl-wt103\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\nSalakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinuso\u00efdal) embeddings which can\nreuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax\ninputs and outputs (tied).\n\nThe abstract from the paper is the following:\n\n*Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the\nsetting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency\nbeyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a\nnovel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the\ncontext fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450%\nlonger than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+\ntimes faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of\nbpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn\nTreebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably\ncoherent, novel text articles with thousands of tokens.*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\n\n## Usage tips\n\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done on the left or on the right. The\n  original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.\n- Transformer-XL is one of the few models that has no sequence length limit.\n- Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive segments (similar to a regular RNNs with two consecutive inputs). In this context, a segment is a number of consecutive tokens (for instance 512) that may span across multiple documents, and segments are fed in order to the model.\n- Basically, the hidden states of the previous segment are concatenated to the current input to compute the attention scores. This allows the model to pay attention to information that was in the previous segment as well as the current one. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.\n- This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.\n\n\n<Tip warning={true}>\n\nTransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)\n\n</Tip>\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## TransfoXLConfig\n\n[[autodoc]] TransfoXLConfig\n\n## TransfoXLTokenizer\n\n[[autodoc]] TransfoXLTokenizer\n    - save_vocabulary\n\n## TransfoXL specific outputs\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput\n\n<frameworkcontent>\n<pt>\n\n## TransfoXLModel\n\n[[autodoc]] TransfoXLModel\n    - forward\n\n## TransfoXLLMHeadModel\n\n[[autodoc]] TransfoXLLMHeadModel\n    - forward\n\n## TransfoXLForSequenceClassification\n\n[[autodoc]] TransfoXLForSequenceClassification\n    - forward\n\n</pt>\n<tf>\n\n## TFTransfoXLModel\n\n[[autodoc]] TFTransfoXLModel\n    - call\n\n## TFTransfoXLLMHeadModel\n\n[[autodoc]] TFTransfoXLLMHeadModel\n    - call\n\n## TFTransfoXLForSequenceClassification\n\n[[autodoc]] TFTransfoXLForSequenceClassification\n    - call\n\n</tf>\n</frameworkcontent>\n\n## Internal Layers\n\n[[autodoc]] AdaptiveEmbedding\n\n[[autodoc]] TFAdaptiveEmbedding\n", "# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\n Tokenization classes for Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl.\n\"\"\"\n\n\nimport glob\nimport os\nimport pickle\nimport re\nfrom collections import Counter, OrderedDict\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\n\nfrom ....tokenization_utils import PreTrainedTokenizer\nfrom ....utils import (\n    cached_file,\n    is_sacremoses_available,\n    is_torch_available,\n    logging,\n    requires_backends,\n    torch_only_method,\n)\n\n\nif is_sacremoses_available():\n    import sacremoses as sm\n\n\nif is_torch_available():\n    import torch\n\n\nlogger = logging.get_logger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \"pretrained_vocab_file\": \"vocab.pkl\",\n    \"pretrained_vocab_file_torch\": \"vocab.bin\",\n    \"vocab_file\": \"vocab.txt\",\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"pretrained_vocab_file\": {\n        \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/vocab.pkl\",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"transfo-xl-wt103\": None,\n}\n\nPRETRAINED_CORPUS_ARCHIVE_MAP = {\n    \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/corpus.bin\",\n}\nCORPUS_NAME = \"corpus.bin\"\n\nMATCH_NUMBERS = r\"(?<=\\d)[,.](?=\\d)\", r\" @\\g<0>@ \"\nDETOKENIZE_NUMBERS = [(r\" @\\,@ \", r\",\"), (r\" @\\.@ \", r\".\")]\n\n\ndef tokenize_numbers(text_array: List[str]) -> List[str]:\n    \"\"\"\n    Splits large comma-separated numbers and floating point values. This is done by replacing commas with ' @,@ ' and\n    dots with ' @.@ '.\n\n    Args:\n        text_array: An already tokenized text as list.\n\n    Returns:\n        A list of strings with tokenized numbers.\n\n    Example:\n\n    ```python\n    >>> tokenize_numbers([\"$\", \"5,000\", \"1.73\", \"m\"])\n    ['$', '5', '@,@', '000', '1', '@.@', '73', 'm']\n    ```\"\"\"\n    tokenized = []\n    for i in range(len(text_array)):\n        reg, sub = MATCH_NUMBERS\n        replaced = re.sub(reg, sub, text_array[i]).split()\n        tokenized.extend(replaced)\n\n    return tokenized\n\n\ndef detokenize_numbers(text: str) -> str:\n    \"\"\"\n    Inverts the operation of *tokenize_numbers*. This is replacing ' @,@ ' and ' @.@' by ',' and '.'.\n\n    Args:\n        text: A string where the number should be detokenized.\n\n    Returns:\n        A detokenized string.\n\n    Example:\n\n    ```python\n    >>> detokenize_numbers(\"$ 5 @,@ 000 1 @.@ 73 m\")\n    '$ 5,000 1.73 m'\n    ```\"\"\"\n    for reg, sub in DETOKENIZE_NUMBERS:\n        text = re.sub(reg, sub, text)\n    return text\n\n\nclass TransfoXLTokenizer(PreTrainedTokenizer):\n    \"\"\"\n    Construct a Transformer-XL tokenizer adapted from Vocab class in [the original\n    code](https://github.com/kimiyoung/transformer-xl). The Transformer-XL tokenizer is a word-level tokenizer (no\n    sub-word tokenization).\n\n    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n    this superclass for more information regarding those methods.\n\n    Args:\n        special (`List[str]`, *optional*):\n            A list of special tokens (to be treated by the original implementation of this tokenizer).\n        min_freq (`int`, *optional*, defaults to 0):\n            The minimum number of times a token has to be present in order to be kept in the vocabulary (otherwise it\n            will be mapped to `unk_token`).\n        max_size (`int`, *optional*):\n            The maximum size of the vocabulary. If left unset, it will default to the size of the vocabulary found\n            after excluding the tokens according to the `min_freq` rule.\n        lower_case (`bool`, *optional*, defaults to `False`):\n            Whether or not to lowercase the input when tokenizing.\n        delimiter (`str`, *optional*):\n            The delimiter used between tokens.\n        vocab_file (`str`, *optional*):\n            File containing the vocabulary (from the original implementation).\n        pretrained_vocab_file (`str`, *optional*):\n            File containing the vocabulary as saved with the `save_pretrained()` method.\n        never_split (`List[str]`, *optional*):\n            List of tokens that should never be split. If no list is specified, will simply use the existing special\n            tokens.\n        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n            token instead.\n        eos_token (`str`, *optional*, defaults to `\"<eos>\"`):\n            The end of sequence token.\n        additional_special_tokens (`List[str]`, *optional*, defaults to `['<formula>']`):\n            A list of additional special tokens (for the HuggingFace functionality).\n        language (`str`, *optional*, defaults to `\"en\"`):\n            The language of this tokenizer (used for mose preprocessing).\n    \"\"\"\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n    model_input_names = [\"input_ids\"]\n\n    def __init__(\n        self,\n        special=None,\n        min_freq=0,\n        max_size=None,\n        lower_case=False,\n        delimiter=None,\n        vocab_file=None,\n        pretrained_vocab_file: str = None,\n        never_split=None,\n        unk_token=\"<unk>\",\n        eos_token=\"<eos>\",\n        additional_special_tokens=[\"<formula>\"],\n        language=\"en\",\n        **kwargs,\n    ):\n        logger.error(\n            \"`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. \"\n            \"See more details on this model's documentation page: \"\n            \"`https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\"\n        )\n\n        requires_backends(self, \"sacremoses\")\n        if special is None:\n            special = []\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n        self.punctuation_symbols = '!\"#$%&()*+,-./\\\\:;<=>?@[\\\\]^_`{|}~'\n        self.punction_without_space_before_pattern = re.compile(rf\"[^\\s][{self.punctuation_symbols}]\")\n        self.punctuation_with_space_around_pattern = self._compile_space_around_punctuation_pattern()\n        self.language = language\n        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)\n        self.moses_tokenizer = sm.MosesTokenizer(language)\n        self.moses_detokenizer = sm.MosesDetokenizer(language)\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n        # This try... catch... is not beautiful but honestly this tokenizer was not made to be used\n        # in a library like ours, at all.\n        try:\n            vocab_dict = None\n            if pretrained_vocab_file is not None:\n                # Priority on pickle files (support PyTorch and TF)\n                with open(pretrained_vocab_file, \"rb\") as f:\n                    vocab_dict = pickle.load(f)\n\n                # Loading a torch-saved transfo-xl vocab dict with pickle results in an integer\n                # Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.\n                # We therefore load it with torch, if it's available.\n                if isinstance(vocab_dict, int):\n                    if not is_torch_available():\n                        raise ImportError(\n                            \"Not trying to load dict with PyTorch as you need to install pytorch to load \"\n                            \"from a PyTorch pretrained vocabulary, \"\n                            \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"\n                        )\n                    vocab_dict = torch.load(pretrained_vocab_file)\n\n            if vocab_dict is not None:\n                for key, value in vocab_dict.items():\n                    if key not in self.__dict__ or key in [\"sym2idx\", \"idx2sym\"]:\n                        self.__dict__[key] = value\n            elif vocab_file is not None:\n                self.build_vocab()\n\n        except Exception as e:\n            raise ValueError(\n                f\"Unable to parse file {pretrained_vocab_file}. Unknown format. \"\n                \"If you tried to load a model saved through TransfoXLTokenizerFast, \"\n                \"please note they are not compatible.\"\n            ) from e\n\n        if vocab_file is not None:\n            self.build_vocab()\n\n        super().__init__(\n            special=special,\n            min_freq=min_freq,\n            max_size=max_size,\n            lower_case=lower_case,\n            delimiter=delimiter,\n            vocab_file=vocab_file,\n            pretrained_vocab_file=pretrained_vocab_file,\n            never_split=never_split,\n            unk_token=unk_token,\n            eos_token=eos_token,\n            additional_special_tokens=additional_special_tokens,\n            language=language,\n            **kwargs,\n        )\n\n        # these are not required to initialize the parent class as only used when tokenizing.\n        if never_split is None:\n            never_split = self.all_special_tokens\n        self.never_split = never_split\n\n    @property\n    def do_lower_case(self):\n        return self.lower_case\n\n    def _compile_space_around_punctuation_pattern(self):\n        look_ahead_for_special_token = f\"(?=[{self.punctuation_symbols}])\"\n        look_ahead_to_match_all_except_space = r\"(?=[^\\s])\"\n        return re.compile(r\"\" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)\n\n    def count_file(self, path, verbose=False, add_eos=False):\n        if verbose:\n            logger.info(f\"counting file {path} ...\")\n        assert os.path.exists(path), f\"Input file {path} not found\"\n\n        sents = []\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    logger.info(f\"    line {idx}\")\n                symbols = self.tokenize(line, add_eos=add_eos)\n                self.counter.update(symbols)\n                sents.append(symbols)\n\n        return sents\n\n    def count_sents(self, sents, verbose=False):\n        \"\"\"\n        sents : a list of sentences, each a list of tokenized symbols\n        \"\"\"\n        if verbose:\n            logger.info(f\"counting {len(sents)} sents ...\")\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                logger.info(f\"    line {idx}\")\n            self.counter.update(symbols)\n\n    def _build_from_file(self, vocab_file):\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n\n        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                symb = line.strip().split()[0]\n                self.add_symbol(symb)\n        if \"<UNK>\" in self.sym2idx:\n            self.unk_idx = self.sym2idx[\"<UNK>\"]\n        elif \"<unk>\" in self.sym2idx:\n            self.unk_idx = self.sym2idx[\"<unk>\"]\n        else:\n            raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")\n\n    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n        if os.path.isdir(save_directory):\n            vocab_file = os.path.join(\n                save_directory,\n                (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"pretrained_vocab_file\"],\n            )\n        else:\n            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n        with open(vocab_file, \"wb\") as f:\n            pickle.dump(self.__dict__, f)\n        return (vocab_file,)\n\n    def build_vocab(self):\n        if self.vocab_file:\n            logger.info(f\"building vocab from {self.vocab_file}\")\n            self._build_from_file(self.vocab_file)\n            logger.info(f\"Final vocab size {len(self.sym2idx)}\")\n        else:\n            logger.info(f\"building vocab with min_freq={self.min_freq}, max_size={self.max_size}\")\n            self.idx2sym = []\n            self.sym2idx = OrderedDict()\n\n            for sym in self.special:\n                self.add_special(sym)\n\n            for sym, cnt in self.counter.most_common(self.max_size):\n                if cnt < self.min_freq:\n                    break\n                self.add_symbol(sym)\n\n            logger.info(f\"Final vocab size {len(self.sym2idx)} from {len(self.counter)} unique tokens\")\n\n    @torch_only_method\n    def encode_file(self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False):\n        if verbose:\n            logger.info(f\"encoding file {path} ...\")\n        assert os.path.exists(path), f\"Output file {path} not found\"\n        encoded = []\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    logger.info(f\"    line {idx}\")\n                symbols = self.tokenize(line, add_eos=add_eos, add_double_eos=add_double_eos)\n                encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    @torch_only_method\n    def encode_sents(self, sents, ordered=False, verbose=False):\n        if verbose:\n            logger.info(f\"encoding {len(sents)} sents ...\")\n        encoded = []\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                logger.info(f\"    line {idx}\")\n            encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def add_special(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n            setattr(self, f\"{sym.strip('<>')}_idx\", self.sym2idx[sym])\n\n    def add_symbol(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n\n    def move_added_token(self, token: str, target_idx: int):\n        \"\"\"\n        Moves an added token to a specific position in the vocab. This method should be used when resizing an embedding\n        layer other than the last one in the `AdaptiveEmbedding` in order to move the token in the tokenizer from the\n        default position (at the very end) to the desired one.\n\n        Args:\n            token: The token to move to a specific position in the vocab.\n            target_idx: The position where the token should be moved to.\n        \"\"\"\n        assert token in self.added_tokens_encoder, \"Token which should be moved has to be an added token\"\n        assert token not in self.idx2sym, \"Token which should be moved is already in vocab\"\n\n        # Insert sym into vocab\n        self.idx2sym.insert(target_idx, token)\n        self.sym2idx[token] = target_idx\n\n        # Shift following indices in sym2idx\n        for idx in range(target_idx + 1, len(self.idx2sym)):\n            current_sym = self.idx2sym[idx]\n            self.sym2idx[current_sym] = idx\n\n        # Delete token from added_tokens\n        old_index = self._added_tokens_encoder.pop(token)\n        self._added_tokens_decoder.pop(old_index)\n\n    def moses_punct_norm(self, text):\n        return self.moses_punct_normalizer.normalize(text)\n\n    def moses_tokenize(self, text):\n        return self.moses_tokenizer.tokenize(\n            text, aggressive_dash_splits=True, return_str=False, escape=False, protected_patterns=self.never_split\n        )\n\n    def moses_pipeline(self, text: str) -> List[str]:\n        \"\"\"\n        Does basic tokenization using [`sacremoses.MosesPunctNormalizer`] and [`sacremoses.MosesTokenizer`] with\n        *aggressive_dash_splits=True* (see [`sacremoses.tokenize.MosesTokenizer.tokenize`]). Additionally, large\n        comma-separated numbers and floating point values are split. E.g. \"23,000 people are 1.80m tall\" -> \"23 @,@ 000\n        people are 1 @.@ 80m tall\"\n\n        Args:\n            text: Text to be tokenize\n\n        Returns:\n            A list of tokenized string\n\n        Example:\n\n        ```python\n        >>> tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")\n        >>> tokenizer.moses_pipeline(\"23,000 people are 1.80 m tall\")\n        ['23', '@,@', '000', 'people', 'are', '1', '@.@', '80', 'm', 'tall']\n        ```\"\"\"\n        text = self.moses_punct_norm(text)\n        text = self.moses_tokenize(text)\n        text = tokenize_numbers(text)\n        return text\n\n    def _convert_id_to_token(self, idx):\n        \"\"\"Converts an id in a token (BPE) using the vocab.\"\"\"\n        assert 0 <= idx < len(self), f\"Index {idx} out of vocabulary range\"\n        return self.idx2sym[idx]\n\n    def _convert_token_to_id(self, sym):\n        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n        if sym in self.sym2idx:\n            return self.sym2idx[sym]\n        else:\n            # logger.info(f'encounter unk {sym}')\n            # assert '<eos>' not in sym\n            if hasattr(self, \"unk_idx\"):\n                return self.sym2idx.get(sym, self.unk_idx)\n            # Backward compatibility with pre-trained models\n            elif \"<unk>\" in self.sym2idx:\n                return self.sym2idx[\"<unk>\"]\n            elif \"<UNK>\" in self.sym2idx:\n                return self.sym2idx[\"<UNK>\"]\n            else:\n                raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\"\n        Converts a sequence of tokens (string) in a single string. Additionally, the split numbers are converted back\n        into it's original form.\n        \"\"\"\n        out_string = self.moses_detokenizer.detokenize(tokens)\n        return detokenize_numbers(out_string).strip()\n\n    @torch_only_method\n    def convert_to_tensor(self, symbols):\n        return torch.LongTensor(self.convert_tokens_to_ids(symbols))\n\n    @property\n    def vocab_size(self):\n        return len(self.idx2sym)\n\n    def get_vocab(self):\n        vocab = self.sym2idx.copy()\n        vocab.update(self.added_tokens_encoder)\n        return vocab\n\n    def _tokenize(self, line, add_eos=False, add_double_eos=False):\n        line = line.strip()\n        # convert to lower case\n        if self.lower_case:\n            line = line.lower()\n\n        # empty delimiter '' will evaluate False\n        if self.delimiter == \"\":\n            symbols = line\n        else:\n            symbols = self.moses_pipeline(line)\n\n        if add_double_eos:  # lm1b\n            return [\"<S>\"] + symbols + [\"<S>\"]\n        elif add_eos:\n            return symbols + [\"<eos>\"]\n        else:\n            return symbols\n\n\nclass LMOrderedIterator(object):\n    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None):\n        \"\"\"\n        data -- LongTensor -- the LongTensor is strictly ordered\n        \"\"\"\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        self.n_step = data.size(0) // bsz\n\n        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n        data = data.narrow(0, 0, self.n_step * bsz)\n\n        # Evenly divide the data across the bsz batches.\n        self.data = data.view(bsz, -1).t().contiguous().to(device)\n\n        # Number of mini-batches\n        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n\n    def get_batch(self, i, bptt=None):\n        if bptt is None:\n            bptt = self.bptt\n        seq_len = min(bptt, self.data.size(0) - 1 - i)\n\n        end_idx = i + seq_len\n        beg_idx = max(0, i - self.ext_len)\n\n        data = self.data[beg_idx:end_idx]\n        target = self.data[i + 1 : i + 1 + seq_len]\n\n        data_out = data.transpose(0, 1).contiguous().to(self.device)\n        target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n        return data_out, target_out, seq_len\n\n    def get_fixlen_iter(self, start=0):\n        for i in range(start, self.data.size(0) - 1, self.bptt):\n            yield self.get_batch(i)\n\n    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n        max_len = self.bptt + max_deviation * std\n        i = start\n        while True:\n            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.0\n            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n            data, target, seq_len = self.get_batch(i, bptt)\n            i += seq_len\n            yield data, target, seq_len\n            if i >= self.data.size(0) - 2:\n                break\n\n    def __iter__(self):\n        return self.get_fixlen_iter()\n\n\nclass LMShuffledIterator(object):\n    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):\n        \"\"\"\n        data -- list[LongTensor] -- there is no order among the LongTensors\n        \"\"\"\n        self.data = data\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self):\n        # index iterator\n        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle else np.array(range(len(self.data)))\n\n        # sentence iterator\n        for idx in epoch_indices:\n            yield self.data[idx]\n\n    @torch_only_method\n    def stream_iterator(self, sent_stream):\n        # streams for each data in the batch\n        streams = [None] * self.bsz\n\n        data = torch.LongTensor(self.bptt, self.bsz)\n        target = torch.LongTensor(self.bptt, self.bsz)\n\n        n_retain = 0\n\n        while True:\n            # data   : [n_retain+bptt x bsz]\n            # target : [bptt x bsz]\n            data[n_retain:].fill_(-1)\n            target.fill_(-1)\n\n            valid_batch = True\n\n            for i in range(self.bsz):\n                n_filled = 0\n                try:\n                    while n_filled < self.bptt:\n                        if streams[i] is None or len(streams[i]) <= 1:\n                            streams[i] = next(sent_stream)\n                        # number of new tokens to fill in\n                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)\n                        # first n_retain tokens are retained from last batch\n                        data[n_retain + n_filled : n_retain + n_filled + n_new, i] = streams[i][:n_new]\n                        target[n_filled : n_filled + n_new, i] = streams[i][1 : n_new + 1]\n                        streams[i] = streams[i][n_new:]\n                        n_filled += n_new\n                except StopIteration:\n                    valid_batch = False\n                    break\n\n            if not valid_batch:\n                return\n\n            data_out = data.transpose(0, 1).contiguous().to(self.device)\n            target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n            yield data_out, target_out, self.bptt\n\n            n_retain = min(data.size(0), self.ext_len)\n            if n_retain > 0:\n                data[:n_retain] = data[-n_retain:]\n            data.resize_(n_retain + self.bptt, data.size(1))\n\n    def __iter__(self):\n        # sent_stream is an iterator\n        sent_stream = self.get_sent_stream()\n\n        for batch in self.stream_iterator(sent_stream):\n            yield batch\n\n\nclass LMMultiFileIterator(LMShuffledIterator):\n    def __init__(self, paths, vocab, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):\n        self.paths = paths\n        self.vocab = vocab\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self, path):\n        sents = self.vocab.encode_file(path, add_double_eos=True)\n        if self.shuffle:\n            np.random.shuffle(sents)\n        sent_stream = iter(sents)\n\n        return sent_stream\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.paths)\n\n        for path in self.paths:\n            # sent_stream is an iterator\n            sent_stream = self.get_sent_stream(path)\n            for batch in self.stream_iterator(sent_stream):\n                yield batch\n\n\nclass TransfoXLCorpus(object):\n    @classmethod\n    @torch_only_method\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        \"\"\"\n        Instantiate a pre-processed corpus.\n        \"\"\"\n        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        is_local = os.path.isdir(pretrained_model_name_or_path)\n        # redirect to the cache, if necessary\n        try:\n            resolved_corpus_file = cached_file(pretrained_model_name_or_path, CORPUS_NAME, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                f\"Corpus '{pretrained_model_name_or_path}' was not found in corpus list\"\n                f\" ({', '.join(PRETRAINED_CORPUS_ARCHIVE_MAP.keys())}. We assumed '{pretrained_model_name_or_path}'\"\n                f\" was a path or url but couldn't find files {CORPUS_NAME} at this path or url.\"\n            )\n            return None\n        if is_local:\n            logger.info(f\"loading corpus file {resolved_corpus_file}\")\n        else:\n            logger.info(f\"loading corpus file {CORPUS_NAME} from cache at {resolved_corpus_file}\")\n\n        # Instantiate tokenizer.\n        corpus = cls(*inputs, **kwargs)\n        corpus_dict = torch.load(resolved_corpus_file)\n        for key, value in corpus_dict.items():\n            corpus.__dict__[key] = value\n        corpus.vocab = vocab\n        if corpus.train is not None:\n            corpus.train = torch.tensor(corpus.train, dtype=torch.long)\n        if corpus.valid is not None:\n            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)\n        if corpus.test is not None:\n            corpus.test = torch.tensor(corpus.test, dtype=torch.long)\n        return corpus\n\n    def __init__(self, *args, **kwargs):\n        self.vocab = TransfoXLTokenizer(*args, **kwargs)\n        self.dataset = None\n        self.train = None\n        self.valid = None\n        self.test = None\n\n    def build_corpus(self, path, dataset):\n        self.dataset = dataset\n\n        if self.dataset in [\"ptb\", \"wt2\", \"enwik8\", \"text8\"]:\n            self.vocab.count_file(os.path.join(path, \"train.txt\"))\n            self.vocab.count_file(os.path.join(path, \"valid.txt\"))\n            self.vocab.count_file(os.path.join(path, \"test.txt\"))\n        elif self.dataset == \"wt103\":\n            self.vocab.count_file(os.path.join(path, \"train.txt\"))\n        elif self.dataset == \"lm1b\":\n            train_path_pattern = os.path.join(\n                path,\n                \"1-billion-word-language-modeling-benchmark-r13output\",\n                \"training-monolingual.tokenized.shuffled\",\n                \"news.en-*\",\n            )\n            train_paths = glob.glob(train_path_pattern)\n            # the vocab will load from file when build_vocab() is called\n\n        self.vocab.build_vocab()\n\n        if self.dataset in [\"ptb\", \"wt2\", \"wt103\"]:\n            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True)\n            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True)\n            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True)\n        elif self.dataset in [\"enwik8\", \"text8\"]:\n            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True, add_eos=False)\n            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True, add_eos=False)\n            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True, add_eos=False)\n        elif self.dataset == \"lm1b\":\n            self.train = train_paths\n            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=False, add_double_eos=True)\n            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=False, add_double_eos=True)\n\n    def get_iterator(self, split, *args, **kwargs):\n        if split == \"train\":\n            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:\n                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n            elif self.dataset == \"lm1b\":\n                kwargs[\"shuffle\"] = True\n                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n        elif split in [\"valid\", \"test\"]:\n            data = self.valid if split == \"valid\" else self.test\n            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:\n                data_iter = LMOrderedIterator(data, *args, **kwargs)\n            elif self.dataset == \"lm1b\":\n                data_iter = LMShuffledIterator(data, *args, **kwargs)\n        else:\n            data_iter = None\n            raise ValueError(f\"Split not recognized: {split}\")\n\n        return data_iter\n\n\n@torch_only_method\ndef get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \"cache.pt\")\n    fn_pickle = os.path.join(datadir, \"cache.pkl\")\n    if os.path.exists(fn):\n        logger.info(\"Loading cached dataset...\")\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        logger.info(\"Loading cached dataset from pickle...\")\n        with open(fn, \"rb\") as fp:\n            corpus = pickle.load(fp)\n    else:\n        logger.info(f\"Producing dataset {dataset}...\")\n        kwargs = {}\n        if dataset in [\"wt103\", \"wt2\"]:\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = False\n        elif dataset == \"ptb\":\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = True\n        elif dataset == \"lm1b\":\n            kwargs[\"special\"] = []\n            kwargs[\"lower_case\"] = False\n            kwargs[\"vocab_file\"] = os.path.join(datadir, \"1b_word_vocab.txt\")\n        elif dataset in [\"enwik8\", \"text8\"]:\n            pass\n\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n\n    return corpus\n", "# coding=utf-8\n# Copyright 2020, The RAG Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"RAG Retriever model implementation.\"\"\"\n\nimport os\nimport pickle\nimport time\nfrom typing import Iterable, List, Optional, Tuple\n\nimport numpy as np\n\nfrom ...tokenization_utils import PreTrainedTokenizer\nfrom ...tokenization_utils_base import BatchEncoding\nfrom ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends\nfrom .configuration_rag import RagConfig\nfrom .tokenization_rag import RagTokenizer\n\n\nif is_datasets_available():\n    from datasets import Dataset, load_dataset, load_from_disk\n\nif is_faiss_available():\n    import faiss\n\n\nlogger = logging.get_logger(__name__)\n\n\nLEGACY_INDEX_PATH = \"https://storage.googleapis.com/huggingface-nlp/datasets/wiki_dpr/\"\n\n\nclass Index:\n    \"\"\"\n    A base class for the Indices encapsulated by the [`RagRetriever`].\n    \"\"\"\n\n    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:\n        \"\"\"\n        Returns a list of dictionaries, containing titles and text of the retrieved documents.\n\n        Args:\n            doc_ids (`np.ndarray` of shape `(batch_size, n_docs)`):\n                A tensor of document indices.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        For each query in the batch, retrieves `n_docs` documents.\n\n        Args:\n            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):\n                An array of query vectors.\n            n_docs (`int`):\n                The number of docs retrieved per query.\n\n        Returns:\n            `np.ndarray` of shape `(batch_size, n_docs)`: A tensor of indices of retrieved documents. `np.ndarray` of\n            shape `(batch_size, vector_size)`: A tensor of vector representations of retrieved documents.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_initialized(self):\n        \"\"\"\n        Returns `True` if index is already initialized.\n        \"\"\"\n        raise NotImplementedError\n\n    def init_index(self):\n        \"\"\"\n        A function responsible for loading the index into memory. Should be called only once per training run of a RAG\n        model. E.g. if the model is trained on multiple GPUs in a distributed setup, only one of the workers will load\n        the index.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass LegacyIndex(Index):\n    \"\"\"\n    An index which can be deserialized from the files built using https://github.com/facebookresearch/DPR. We use\n    default faiss index parameters as specified in that repository.\n\n    Args:\n        vector_size (`int`):\n            The dimension of indexed vectors.\n        index_path (`str`):\n            A path to a *directory* containing index files compatible with [`~models.rag.retrieval_rag.LegacyIndex`]\n    \"\"\"\n\n    INDEX_FILENAME = \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\"\n    PASSAGE_FILENAME = \"psgs_w100.tsv.pkl\"\n\n    def __init__(self, vector_size, index_path):\n        self.index_id_to_db_id = []\n        self.index_path = index_path\n        self.passages = self._load_passages()\n        self.vector_size = vector_size\n        self.index = None\n        self._index_initialized = False\n\n    def _resolve_path(self, index_path, filename):\n        is_local = os.path.isdir(index_path)\n        try:\n            # Load from URL or cache if already cached\n            resolved_archive_file = cached_file(index_path, filename)\n        except EnvironmentError:\n            msg = (\n                f\"Can't load '{filename}'. Make sure that:\\n\\n\"\n                f\"- '{index_path}' is a correct remote path to a directory containing a file named {filename}\\n\\n\"\n                f\"- or '{index_path}' is the correct path to a directory containing a file named {filename}.\\n\\n\"\n            )\n            raise EnvironmentError(msg)\n        if is_local:\n            logger.info(f\"loading file {resolved_archive_file}\")\n        else:\n            logger.info(f\"loading file {filename} from cache at {resolved_archive_file}\")\n        return resolved_archive_file\n\n    def _load_passages(self):\n        logger.info(f\"Loading passages from {self.index_path}\")\n        passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)\n        with open(passages_path, \"rb\") as passages_file:\n            passages = pickle.load(passages_file)\n        return passages\n\n    def _deserialize_index(self):\n        logger.info(f\"Loading index from {self.index_path}\")\n        resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")\n        self.index = faiss.read_index(resolved_index_path)\n        resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")\n        with open(resolved_meta_path, \"rb\") as metadata_file:\n            self.index_id_to_db_id = pickle.load(metadata_file)\n        assert (\n            len(self.index_id_to_db_id) == self.index.ntotal\n        ), \"Deserialized index_id_to_db_id should match faiss index size\"\n\n    def is_initialized(self):\n        return self._index_initialized\n\n    def init_index(self):\n        index = faiss.IndexHNSWFlat(self.vector_size + 1, 512)\n        index.hnsw.efSearch = 128\n        index.hnsw.efConstruction = 200\n        self.index = index\n        self._deserialize_index()\n        self._index_initialized = True\n\n    def get_doc_dicts(self, doc_ids: np.array):\n        doc_list = []\n        for doc_ids_i in doc_ids:\n            ids = [str(int(doc_id)) for doc_id in doc_ids_i]\n            docs = [self.passages[doc_id] for doc_id in ids]\n            doc_list.append(docs)\n        doc_dicts = []\n        for docs in doc_list:\n            doc_dict = {}\n            doc_dict[\"title\"] = [doc[1] for doc in docs]\n            doc_dict[\"text\"] = [doc[0] for doc in docs]\n            doc_dicts.append(doc_dict)\n        return doc_dicts\n\n    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:\n        aux_dim = np.zeros(len(question_hidden_states), dtype=\"float32\").reshape(-1, 1)\n        query_nhsw_vectors = np.hstack((question_hidden_states, aux_dim))\n        _, docs_ids = self.index.search(query_nhsw_vectors, n_docs)\n        vectors = [[self.index.reconstruct(int(doc_id))[:-1] for doc_id in doc_ids] for doc_ids in docs_ids]\n        ids = [[int(self.index_id_to_db_id[doc_id]) for doc_id in doc_ids] for doc_ids in docs_ids]\n        return np.array(ids), np.array(vectors)\n\n\nclass HFIndexBase(Index):\n    def __init__(self, vector_size, dataset, index_initialized=False):\n        self.vector_size = vector_size\n        self.dataset = dataset\n        self._index_initialized = index_initialized\n        self._check_dataset_format(with_index=index_initialized)\n        dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True, dtype=\"float32\")\n\n    def _check_dataset_format(self, with_index: bool):\n        if not isinstance(self.dataset, Dataset):\n            raise ValueError(f\"Dataset should be a datasets.Dataset object, but got {type(self.dataset)}\")\n        if len({\"title\", \"text\", \"embeddings\"} - set(self.dataset.column_names)) > 0:\n            raise ValueError(\n                \"Dataset should be a dataset with the following columns: \"\n                \"title (str), text (str) and embeddings (arrays of dimension vector_size), \"\n                f\"but got columns {self.dataset.column_names}\"\n            )\n        if with_index and \"embeddings\" not in self.dataset.list_indexes():\n            raise ValueError(\n                \"Missing faiss index in the dataset. Make sure you called `dataset.add_faiss_index` to compute it \"\n                \"or `dataset.load_faiss_index` to load one from the disk.\"\n            )\n\n    def init_index(self):\n        raise NotImplementedError()\n\n    def is_initialized(self):\n        return self._index_initialized\n\n    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:\n        return [self.dataset[doc_ids[i].tolist()] for i in range(doc_ids.shape[0])]\n\n    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:\n        _, ids = self.dataset.search_batch(\"embeddings\", question_hidden_states, n_docs)\n        docs = [self.dataset[[i for i in indices if i >= 0]] for indices in ids]\n        vectors = [doc[\"embeddings\"] for doc in docs]\n        for i in range(len(vectors)):\n            if len(vectors[i]) < n_docs:\n                vectors[i] = np.vstack([vectors[i], np.zeros((n_docs - len(vectors[i]), self.vector_size))])\n        return np.array(ids), np.array(vectors)  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)\n\n\nclass CanonicalHFIndex(HFIndexBase):\n    \"\"\"\n    A wrapper around an instance of [`~datasets.Datasets`]. If `index_path` is set to `None`, we load the pre-computed\n    index available with the [`~datasets.arrow_dataset.Dataset`], otherwise, we load the index from the indicated path\n    on disk.\n\n    Args:\n        vector_size (`int`): the dimension of the passages embeddings used by the index\n        dataset_name (`str`, optional, defaults to `wiki_dpr`):\n            A dataset identifier of the indexed dataset on HuggingFace AWS bucket (list all available datasets and ids\n            with `datasets.list_datasets()`).\n        dataset_split (`str`, optional, defaults to `train`)\n            Which split of the `dataset` to load.\n        index_name (`str`, optional, defaults to `train`)\n            The index_name of the index associated with the `dataset`. The index loaded from `index_path` will be saved\n            under this name.\n        index_path (`str`, optional, defaults to `None`)\n            The path to the serialized faiss index on disk.\n        use_dummy_dataset (`bool`, optional, defaults to `False`):\n            If True, use the dummy configuration of the dataset for tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_size: int,\n        dataset_name: str = \"wiki_dpr\",\n        dataset_split: str = \"train\",\n        index_name: Optional[str] = None,\n        index_path: Optional[str] = None,\n        use_dummy_dataset=False,\n    ):\n        if int(index_path is None) + int(index_name is None) != 1:\n            raise ValueError(\"Please provide `index_name` or `index_path`.\")\n        self.dataset_name = dataset_name\n        self.dataset_split = dataset_split\n        self.index_name = index_name\n        self.index_path = index_path\n        self.use_dummy_dataset = use_dummy_dataset\n        logger.info(f\"Loading passages from {self.dataset_name}\")\n        dataset = load_dataset(\n            self.dataset_name, with_index=False, split=self.dataset_split, dummy=self.use_dummy_dataset\n        )\n        super().__init__(vector_size, dataset, index_initialized=False)\n\n    def init_index(self):\n        if self.index_path is not None:\n            logger.info(f\"Loading index from {self.index_path}\")\n            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)\n        else:\n            logger.info(f\"Loading index from {self.dataset_name} with index name {self.index_name}\")\n            self.dataset = load_dataset(\n                self.dataset_name,\n                with_embeddings=True,\n                with_index=True,\n                split=self.dataset_split,\n                index_name=self.index_name,\n                dummy=self.use_dummy_dataset,\n            )\n            self.dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True)\n        self._index_initialized = True\n\n\nclass CustomHFIndex(HFIndexBase):\n    \"\"\"\n    A wrapper around an instance of [`~datasets.Datasets`]. The dataset and the index are both loaded from the\n    indicated paths on disk.\n\n    Args:\n        vector_size (`int`): the dimension of the passages embeddings used by the index\n        dataset_path (`str`):\n            The path to the serialized dataset on disk. The dataset should have 3 columns: title (str), text (str) and\n            embeddings (arrays of dimension vector_size)\n        index_path (`str`)\n            The path to the serialized faiss index on disk.\n    \"\"\"\n\n    def __init__(self, vector_size: int, dataset, index_path=None):\n        super().__init__(vector_size, dataset, index_initialized=index_path is None)\n        self.index_path = index_path\n\n    @classmethod\n    def load_from_disk(cls, vector_size, dataset_path, index_path):\n        logger.info(f\"Loading passages from {dataset_path}\")\n        if dataset_path is None or index_path is None:\n            raise ValueError(\n                \"Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \"\n                \"and `dataset.get_index('embeddings').save(index_path)`.\"\n            )\n        dataset = load_from_disk(dataset_path)\n        return cls(vector_size=vector_size, dataset=dataset, index_path=index_path)\n\n    def init_index(self):\n        if not self.is_initialized():\n            logger.info(f\"Loading index from {self.index_path}\")\n            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)\n            self._index_initialized = True\n\n\nclass RagRetriever:\n    \"\"\"\n    Retriever used to get documents from vector queries. It retrieves the documents embeddings as well as the documents\n    contents, and it formats them to be used with a RagModel.\n\n    Args:\n        config ([`RagConfig`]):\n            The configuration of the RAG model this Retriever is used with. Contains parameters indicating which\n            `Index` to build. You can load your own custom dataset with `config.index_name=\"custom\"` or use a canonical\n            one (default) from the datasets library with `config.index_name=\"wiki_dpr\"` for example.\n        question_encoder_tokenizer ([`PreTrainedTokenizer`]):\n            The tokenizer that was used to tokenize the question. It is used to decode the question and then use the\n            generator_tokenizer.\n        generator_tokenizer ([`PreTrainedTokenizer`]):\n            The tokenizer used for the generator part of the RagModel.\n        index ([`~models.rag.retrieval_rag.Index`], optional, defaults to the one defined by the configuration):\n            If specified, use this index instead of the one built using the configuration\n\n    Examples:\n\n    ```python\n    >>> # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')\n    >>> from transformers import RagRetriever\n\n    >>> retriever = RagRetriever.from_pretrained(\n    ...     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n    ... )\n\n    >>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py\n    >>> from transformers import RagRetriever\n\n    >>> dataset = (\n    ...     ...\n    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index\n    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n\n    >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n    >>> from transformers import RagRetriever\n\n    >>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n    >>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n    >>> retriever = RagRetriever.from_pretrained(\n    ...     \"facebook/dpr-ctx_encoder-single-nq-base\",\n    ...     index_name=\"custom\",\n    ...     passages_path=dataset_path,\n    ...     index_path=index_path,\n    ... )\n\n    >>> # To load the legacy index built originally for Rag's paper\n    >>> from transformers import RagRetriever\n\n    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")\n    ```\"\"\"\n\n    def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):\n        self._init_retrieval = init_retrieval\n        requires_backends(self, [\"datasets\", \"faiss\"])\n        super().__init__()\n        self.index = index or self._build_index(config)\n        self.generator_tokenizer = generator_tokenizer\n        self.question_encoder_tokenizer = question_encoder_tokenizer\n\n        self.n_docs = config.n_docs\n        self.batch_size = config.retrieval_batch_size\n\n        self.config = config\n        if self._init_retrieval:\n            self.init_retrieval()\n\n        self.ctx_encoder_tokenizer = None\n        self.return_tokenized_docs = False\n\n    @staticmethod\n    def _build_index(config):\n        if config.index_name == \"legacy\":\n            return LegacyIndex(\n                config.retrieval_vector_size,\n                config.index_path or LEGACY_INDEX_PATH,\n            )\n        elif config.index_name == \"custom\":\n            return CustomHFIndex.load_from_disk(\n                vector_size=config.retrieval_vector_size,\n                dataset_path=config.passages_path,\n                index_path=config.index_path,\n            )\n        else:\n            return CanonicalHFIndex(\n                vector_size=config.retrieval_vector_size,\n                dataset_name=config.dataset,\n                dataset_split=config.dataset_split,\n                index_name=config.index_name,\n                index_path=config.index_path,\n                use_dummy_dataset=config.use_dummy_dataset,\n            )\n\n    @classmethod\n    def from_pretrained(cls, retriever_name_or_path, indexed_dataset=None, **kwargs):\n        requires_backends(cls, [\"datasets\", \"faiss\"])\n        config = kwargs.pop(\"config\", None) or RagConfig.from_pretrained(retriever_name_or_path, **kwargs)\n        rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)\n        question_encoder_tokenizer = rag_tokenizer.question_encoder\n        generator_tokenizer = rag_tokenizer.generator\n        if indexed_dataset is not None:\n            config.index_name = \"custom\"\n            index = CustomHFIndex(config.retrieval_vector_size, indexed_dataset)\n        else:\n            index = cls._build_index(config)\n        return cls(\n            config,\n            question_encoder_tokenizer=question_encoder_tokenizer,\n            generator_tokenizer=generator_tokenizer,\n            index=index,\n        )\n\n    def save_pretrained(self, save_directory):\n        if isinstance(self.index, CustomHFIndex):\n            if self.config.index_path is None:\n                index_path = os.path.join(save_directory, \"hf_dataset_index.faiss\")\n                self.index.dataset.get_index(\"embeddings\").save(index_path)\n                self.config.index_path = index_path\n            if self.config.passages_path is None:\n                passages_path = os.path.join(save_directory, \"hf_dataset\")\n                # datasets don't support save_to_disk with indexes right now\n                faiss_index = self.index.dataset._indexes.pop(\"embeddings\")\n                self.index.dataset.save_to_disk(passages_path)\n                self.index.dataset._indexes[\"embeddings\"] = faiss_index\n                self.config.passages_path = passages_path\n        self.config.save_pretrained(save_directory)\n        rag_tokenizer = RagTokenizer(\n            question_encoder=self.question_encoder_tokenizer,\n            generator=self.generator_tokenizer,\n        )\n        rag_tokenizer.save_pretrained(save_directory)\n\n    def init_retrieval(self):\n        \"\"\"\n        Retriever initialization function. It loads the index into memory.\n        \"\"\"\n\n        logger.info(\"initializing retrieval\")\n        self.index.init_index()\n\n    def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):\n        r\"\"\"\n        Postprocessing retrieved `docs` and combining them with `input_strings`.\n\n        Args:\n            docs  (`dict`):\n                Retrieved documents.\n            input_strings (`str`):\n                Input strings decoded by `preprocess_query`.\n            prefix (`str`):\n                Prefix added at the beginning of each input, typically used with T5-based models.\n\n        Return:\n            `tuple(tensors)`: a tuple consisting of two elements: contextualized `input_ids` and a compatible\n            `attention_mask`.\n        \"\"\"\n\n        def cat_input_and_doc(doc_title, doc_text, input_string, prefix):\n            # TODO(Patrick): if we train more RAG models, I want to put the input first to take advantage of effortless truncation\n            # TODO(piktus): better handling of truncation\n            if doc_title.startswith('\"'):\n                doc_title = doc_title[1:]\n            if doc_title.endswith('\"'):\n                doc_title = doc_title[:-1]\n            if prefix is None:\n                prefix = \"\"\n            out = (prefix + doc_title + self.config.title_sep + doc_text + self.config.doc_sep + input_string).replace(\n                \"  \", \" \"\n            )\n            return out\n\n        rag_input_strings = [\n            cat_input_and_doc(\n                docs[i][\"title\"][j],\n                docs[i][\"text\"][j],\n                input_strings[i],\n                prefix,\n            )\n            for i in range(len(docs))\n            for j in range(n_docs)\n        ]\n\n        contextualized_inputs = self.generator_tokenizer.batch_encode_plus(\n            rag_input_strings,\n            max_length=self.config.max_combined_length,\n            return_tensors=return_tensors,\n            padding=\"max_length\",\n            truncation=True,\n        )\n\n        return contextualized_inputs[\"input_ids\"], contextualized_inputs[\"attention_mask\"]\n\n    def _chunk_tensor(self, t: Iterable, chunk_size: int) -> List[Iterable]:\n        return [t[i : i + chunk_size] for i in range(0, len(t), chunk_size)]\n\n    def _main_retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, np.ndarray]:\n        question_hidden_states_batched = self._chunk_tensor(question_hidden_states, self.batch_size)\n        ids_batched = []\n        vectors_batched = []\n        for question_hidden_states in question_hidden_states_batched:\n            start_time = time.time()\n            ids, vectors = self.index.get_top_docs(question_hidden_states, n_docs)\n            logger.debug(\n                f\"index search time: {time.time() - start_time} sec, batch size {question_hidden_states.shape}\"\n            )\n            ids_batched.extend(ids)\n            vectors_batched.extend(vectors)\n        return (\n            np.array(ids_batched),\n            np.array(vectors_batched),\n        )  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)\n\n    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:\n        \"\"\"\n        Retrieves documents for specified `question_hidden_states`.\n\n        Args:\n            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):\n                A batch of query vectors to retrieve with.\n            n_docs (`int`):\n                The number of docs retrieved per query.\n\n        Return:\n            `Tuple[np.ndarray, np.ndarray, List[dict]]`: A tuple with the following objects:\n\n            - **retrieved_doc_embeds** (`np.ndarray` of shape `(batch_size, n_docs, dim)`) -- The retrieval embeddings\n              of the retrieved docs per query.\n            - **doc_ids** (`np.ndarray` of shape `(batch_size, n_docs)`) -- The ids of the documents in the index\n            - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.\n        \"\"\"\n\n        doc_ids, retrieved_doc_embeds = self._main_retrieve(question_hidden_states, n_docs)\n        return retrieved_doc_embeds, doc_ids, self.index.get_doc_dicts(doc_ids)\n\n    def set_ctx_encoder_tokenizer(self, ctx_encoder_tokenizer: PreTrainedTokenizer):\n        # used in end2end retriever training\n        self.ctx_encoder_tokenizer = ctx_encoder_tokenizer\n        self.return_tokenized_docs = True\n\n    def __call__(\n        self,\n        question_input_ids: List[List[int]],\n        question_hidden_states: np.ndarray,\n        prefix=None,\n        n_docs=None,\n        return_tensors=None,\n    ) -> BatchEncoding:\n        \"\"\"\n        Retrieves documents for specified `question_hidden_states`.\n\n        Args:\n            question_input_ids (`List[List[int]]`) batch of input ids\n            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`:\n                A batch of query vectors to retrieve with.\n            prefix (`str`, *optional*):\n                The prefix used by the generator's tokenizer.\n            n_docs (`int`, *optional*):\n                The number of docs retrieved per query.\n            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to \"pt\"):\n                If set, will return tensors instead of list of python integers. Acceptable values are:\n\n                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                - `'np'`: Return Numpy `np.ndarray` objects.\n\n        Returns: [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n\n            - **context_input_ids** -- List of token ids to be fed to a model.\n\n              [What are input IDs?](../glossary#input-ids)\n\n            - **context_attention_mask** -- List of indices specifying which tokens should be attended to by the model\n            (when `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n\n              [What are attention masks?](../glossary#attention-mask)\n\n            - **retrieved_doc_embeds** -- List of embeddings of the retrieved documents\n            - **doc_ids** -- List of ids of the retrieved documents\n        \"\"\"\n\n        n_docs = n_docs if n_docs is not None else self.n_docs\n        prefix = prefix if prefix is not None else self.config.generator.prefix\n        retrieved_doc_embeds, doc_ids, docs = self.retrieve(question_hidden_states, n_docs)\n\n        input_strings = self.question_encoder_tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)\n        context_input_ids, context_attention_mask = self.postprocess_docs(\n            docs, input_strings, prefix, n_docs, return_tensors=return_tensors\n        )\n\n        if self.return_tokenized_docs:\n            retrieved_doc_text = []\n            retrieved_doc_title = []\n\n            for b_idx in range(len(docs)):\n                for doc_idx in range(n_docs):\n                    retrieved_doc_text.append(docs[b_idx][\"text\"][doc_idx])\n                    retrieved_doc_title.append(docs[b_idx][\"title\"][doc_idx])\n\n            tokenized_docs = self.ctx_encoder_tokenizer(\n                retrieved_doc_title,\n                retrieved_doc_text,\n                truncation=True,\n                padding=\"longest\",\n                return_tensors=return_tensors,\n            )\n\n            return BatchEncoding(\n                {\n                    \"context_input_ids\": context_input_ids,\n                    \"context_attention_mask\": context_attention_mask,\n                    \"retrieved_doc_embeds\": retrieved_doc_embeds,\n                    \"doc_ids\": doc_ids,\n                    \"tokenized_doc_ids\": tokenized_docs[\"input_ids\"],\n                    \"tokenized_doc_attention_mask\": tokenized_docs[\"attention_mask\"],\n                },\n                tensor_type=return_tensors,\n            )\n\n        else:\n            return BatchEncoding(\n                {\n                    \"context_input_ids\": context_input_ids,\n                    \"context_attention_mask\": context_attention_mask,\n                    \"retrieved_doc_embeds\": retrieved_doc_embeds,\n                    \"doc_ids\": doc_ids,\n                },\n                tensor_type=return_tensors,\n            )\n", "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport pickle\nimport shutil\nimport tempfile\nfrom unittest import TestCase\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom datasets import Dataset\n\nfrom transformers import is_faiss_available\nfrom transformers.models.bart.configuration_bart import BartConfig\nfrom transformers.models.bart.tokenization_bart import BartTokenizer\nfrom transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES as DPR_VOCAB_FILES_NAMES\nfrom transformers.models.dpr.configuration_dpr import DPRConfig\nfrom transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\nfrom transformers.models.rag.configuration_rag import RagConfig\nfrom transformers.models.rag.retrieval_rag import CustomHFIndex, RagRetriever\nfrom transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES as BART_VOCAB_FILES_NAMES\nfrom transformers.testing_utils import require_faiss, require_sentencepiece, require_tokenizers, require_torch\n\n\nif is_faiss_available():\n    import faiss\n\n\n@require_faiss\nclass RagRetrieverTest(TestCase):\n    def setUp(self):\n        self.tmpdirname = tempfile.mkdtemp()\n        self.retrieval_vector_size = 8\n\n        # DPR tok\n        vocab_tokens = [\n            \"[UNK]\",\n            \"[CLS]\",\n            \"[SEP]\",\n            \"[PAD]\",\n            \"[MASK]\",\n            \"want\",\n            \"##want\",\n            \"##ed\",\n            \"wa\",\n            \"un\",\n            \"runn\",\n            \"##ing\",\n            \",\",\n            \"low\",\n            \"lowest\",\n        ]\n        dpr_tokenizer_path = os.path.join(self.tmpdirname, \"dpr_tokenizer\")\n        os.makedirs(dpr_tokenizer_path, exist_ok=True)\n        self.vocab_file = os.path.join(dpr_tokenizer_path, DPR_VOCAB_FILES_NAMES[\"vocab_file\"])\n        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n\n        # BART tok\n        vocab = [\n            \"l\",\n            \"o\",\n            \"w\",\n            \"e\",\n            \"r\",\n            \"s\",\n            \"t\",\n            \"i\",\n            \"d\",\n            \"n\",\n            \"\\u0120\",\n            \"\\u0120l\",\n            \"\\u0120n\",\n            \"\\u0120lo\",\n            \"\\u0120low\",\n            \"er\",\n            \"\\u0120lowest\",\n            \"\\u0120newer\",\n            \"\\u0120wider\",\n            \"<unk>\",\n        ]\n        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n        merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n\n        bart_tokenizer_path = os.path.join(self.tmpdirname, \"bart_tokenizer\")\n        os.makedirs(bart_tokenizer_path, exist_ok=True)\n        self.vocab_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"vocab_file\"])\n        self.merges_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"merges_file\"])\n        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps(vocab_tokens) + \"\\n\")\n        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(\"\\n\".join(merges))\n\n    def get_dpr_tokenizer(self) -> DPRQuestionEncoderTokenizer:\n        return DPRQuestionEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n\n    def get_dpr_ctx_encoder_tokenizer(self) -> DPRContextEncoderTokenizer:\n        return DPRContextEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n\n    def get_bart_tokenizer(self) -> BartTokenizer:\n        return BartTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"bart_tokenizer\"))\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpdirname)\n\n    def get_dummy_dataset(self):\n        dataset = Dataset.from_dict(\n            {\n                \"id\": [\"0\", \"1\"],\n                \"text\": [\"foo\", \"bar\"],\n                \"title\": [\"Foo\", \"Bar\"],\n                \"embeddings\": [np.ones(self.retrieval_vector_size), 2 * np.ones(self.retrieval_vector_size)],\n            }\n        )\n        dataset.add_faiss_index(\"embeddings\", string_factory=\"Flat\", metric_type=faiss.METRIC_INNER_PRODUCT)\n        return dataset\n\n    def get_dummy_canonical_hf_index_retriever(self):\n        dataset = self.get_dummy_dataset()\n        config = RagConfig(\n            retrieval_vector_size=self.retrieval_vector_size,\n            question_encoder=DPRConfig().to_dict(),\n            generator=BartConfig().to_dict(),\n        )\n        with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:\n            mock_load_dataset.return_value = dataset\n            retriever = RagRetriever(\n                config,\n                question_encoder_tokenizer=self.get_dpr_tokenizer(),\n                generator_tokenizer=self.get_bart_tokenizer(),\n            )\n        return retriever\n\n    def get_dummy_custom_hf_index_retriever(self, from_disk: bool):\n        dataset = self.get_dummy_dataset()\n        config = RagConfig(\n            retrieval_vector_size=self.retrieval_vector_size,\n            question_encoder=DPRConfig().to_dict(),\n            generator=BartConfig().to_dict(),\n            index_name=\"custom\",\n        )\n        if from_disk:\n            config.passages_path = os.path.join(self.tmpdirname, \"dataset\")\n            config.index_path = os.path.join(self.tmpdirname, \"index.faiss\")\n            dataset.get_index(\"embeddings\").save(os.path.join(self.tmpdirname, \"index.faiss\"))\n            dataset.drop_index(\"embeddings\")\n            dataset.save_to_disk(os.path.join(self.tmpdirname, \"dataset\"))\n            del dataset\n            retriever = RagRetriever(\n                config,\n                question_encoder_tokenizer=self.get_dpr_tokenizer(),\n                generator_tokenizer=self.get_bart_tokenizer(),\n            )\n        else:\n            retriever = RagRetriever(\n                config,\n                question_encoder_tokenizer=self.get_dpr_tokenizer(),\n                generator_tokenizer=self.get_bart_tokenizer(),\n                index=CustomHFIndex(config.retrieval_vector_size, dataset),\n            )\n        return retriever\n\n    def get_dummy_legacy_index_retriever(self):\n        dataset = Dataset.from_dict(\n            {\n                \"id\": [\"0\", \"1\"],\n                \"text\": [\"foo\", \"bar\"],\n                \"title\": [\"Foo\", \"Bar\"],\n                \"embeddings\": [np.ones(self.retrieval_vector_size + 1), 2 * np.ones(self.retrieval_vector_size + 1)],\n            }\n        )\n        dataset.add_faiss_index(\"embeddings\", string_factory=\"Flat\", metric_type=faiss.METRIC_INNER_PRODUCT)\n\n        index_file_name = os.path.join(self.tmpdirname, \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\")\n        dataset.save_faiss_index(\"embeddings\", index_file_name + \".index.dpr\")\n        pickle.dump(dataset[\"id\"], open(index_file_name + \".index_meta.dpr\", \"wb\"))\n\n        passages_file_name = os.path.join(self.tmpdirname, \"psgs_w100.tsv.pkl\")\n        passages = {sample[\"id\"]: [sample[\"text\"], sample[\"title\"]] for sample in dataset}\n        pickle.dump(passages, open(passages_file_name, \"wb\"))\n\n        config = RagConfig(\n            retrieval_vector_size=self.retrieval_vector_size,\n            question_encoder=DPRConfig().to_dict(),\n            generator=BartConfig().to_dict(),\n            index_name=\"legacy\",\n            index_path=self.tmpdirname,\n        )\n        retriever = RagRetriever(\n            config, question_encoder_tokenizer=self.get_dpr_tokenizer(), generator_tokenizer=self.get_bart_tokenizer()\n        )\n        return retriever\n\n    def test_canonical_hf_index_retriever_retrieve(self):\n        n_docs = 1\n        retriever = self.get_dummy_canonical_hf_index_retriever()\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"embeddings\", \"id\", \"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"id\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"id\"][0], \"1\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"id\"][0], \"0\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_canonical_hf_index_retriever_save_and_from_pretrained(self):\n        retriever = self.get_dummy_canonical_hf_index_retriever()\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:\n                mock_load_dataset.return_value = self.get_dummy_dataset()\n                retriever.save_pretrained(tmp_dirname)\n                retriever = RagRetriever.from_pretrained(tmp_dirname)\n                self.assertIsInstance(retriever, RagRetriever)\n                hidden_states = np.array(\n                    [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n                )\n                out = retriever.retrieve(hidden_states, n_docs=1)\n                self.assertTrue(out is not None)\n\n    def test_custom_hf_index_retriever_retrieve(self):\n        n_docs = 1\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=False)\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"embeddings\", \"id\", \"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"id\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"id\"][0], \"1\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"id\"][0], \"0\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_custom_hf_index_retriever_save_and_from_pretrained(self):\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=False)\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            retriever.save_pretrained(tmp_dirname)\n            retriever = RagRetriever.from_pretrained(tmp_dirname)\n            self.assertIsInstance(retriever, RagRetriever)\n            hidden_states = np.array(\n                [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n            )\n            out = retriever.retrieve(hidden_states, n_docs=1)\n            self.assertTrue(out is not None)\n\n    def test_custom_hf_index_retriever_retrieve_from_disk(self):\n        n_docs = 1\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=True)\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"embeddings\", \"id\", \"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"id\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"id\"][0], \"1\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"id\"][0], \"0\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_custom_hf_index_retriever_save_and_from_pretrained_from_disk(self):\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=True)\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            retriever.save_pretrained(tmp_dirname)\n            retriever = RagRetriever.from_pretrained(tmp_dirname)\n            self.assertIsInstance(retriever, RagRetriever)\n            hidden_states = np.array(\n                [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n            )\n            out = retriever.retrieve(hidden_states, n_docs=1)\n            self.assertTrue(out is not None)\n\n    def test_legacy_index_retriever_retrieve(self):\n        n_docs = 1\n        retriever = self.get_dummy_legacy_index_retriever()\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"text\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"text\"][0], \"bar\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"text\"][0], \"foo\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_legacy_hf_index_retriever_save_and_from_pretrained(self):\n        retriever = self.get_dummy_legacy_index_retriever()\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            retriever.save_pretrained(tmp_dirname)\n            retriever = RagRetriever.from_pretrained(tmp_dirname)\n            self.assertIsInstance(retriever, RagRetriever)\n            hidden_states = np.array(\n                [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n            )\n            out = retriever.retrieve(hidden_states, n_docs=1)\n            self.assertTrue(out is not None)\n\n    @require_torch\n    @require_tokenizers\n    @require_sentencepiece\n    def test_hf_index_retriever_call(self):\n        import torch\n\n        n_docs = 1\n        retriever = self.get_dummy_canonical_hf_index_retriever()\n        question_input_ids = [[5, 7], [10, 11]]\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        out = retriever(question_input_ids, hidden_states, prefix=retriever.config.generator.prefix, n_docs=n_docs)\n        context_input_ids, context_attention_mask, retrieved_doc_embeds = (\n            out[\"context_input_ids\"],\n            out[\"context_attention_mask\"],\n            out[\"retrieved_doc_embeds\"],\n        )\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertIsInstance(context_input_ids, list)\n        self.assertIsInstance(context_attention_mask, list)\n        self.assertIsInstance(retrieved_doc_embeds, np.ndarray)\n\n        out = retriever(\n            question_input_ids,\n            hidden_states,\n            prefix=retriever.config.generator.prefix,\n            n_docs=n_docs,\n            return_tensors=\"pt\",\n        )\n        context_input_ids, context_attention_mask, retrieved_doc_embeds, doc_ids = (  # noqa: F841\n            out[\"context_input_ids\"],\n            out[\"context_attention_mask\"],\n            out[\"retrieved_doc_embeds\"],\n            out[\"doc_ids\"],\n        )\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertIsInstance(context_input_ids, torch.Tensor)\n        self.assertIsInstance(context_attention_mask, torch.Tensor)\n        self.assertIsInstance(retrieved_doc_embeds, torch.Tensor)\n\n    @require_torch\n    @require_tokenizers\n    @require_sentencepiece\n    def test_custom_hf_index_end2end_retriever_call(self):\n        context_encoder_tokenizer = self.get_dpr_ctx_encoder_tokenizer()\n        n_docs = 1\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=False)\n        retriever.set_ctx_encoder_tokenizer(context_encoder_tokenizer)\n\n        question_input_ids = [[5, 7], [10, 11]]\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        out = retriever(question_input_ids, hidden_states, prefix=retriever.config.generator.prefix, n_docs=n_docs)\n\n        self.assertEqual(\n            len(out), 6\n        )  # check whether the retriever output consist of 6 attributes including tokenized docs\n        self.assertEqual(\n            all(k in out for k in (\"tokenized_doc_ids\", \"tokenized_doc_attention_mask\")), True\n        )  # check for doc token related keys in dictionary.\n"], "fixing_code": ["<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\n\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# Transformer XL\n\n<Tip warning={true}>\n\nThis model is in maintenance mode only, so we won't accept any new PRs changing its code. This model was deprecated due to security issues linked to `pickle.load`.\n\nWe recommend switching to more recent models for improved security.\n\nIn case you would still like to use `TransfoXL` in your experiments, we recommend using the [Hub checkpoint](https://huggingface.co/transfo-xl-wt103) with a specific revision to ensure you are downloading safe files from the Hub.\n\nYou will need to set the environment variable `TRUST_REMOTE_CODE` to `True` in order to allow the\nusage of `pickle.load()`:\n\n```python\nimport os\nfrom transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n\nos.environ[\"TRUST_REMOTE_CODE\"] = \"True\"\n\ncheckpoint = 'transfo-xl-wt103'\nrevision = '40a186da79458c9f9de846edfaea79c412137f97'\n\ntokenizer = TransfoXLTokenizer.from_pretrained(checkpoint, revision=revision)\nmodel = TransfoXLLMHeadModel.from_pretrained(checkpoint, revision=revision)\n```\n\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.35.0.\nYou can do so by running the following command: `pip install -U transformers==4.35.0`.\n\n</Tip>\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=transfo-xl\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-transfo--xl-blueviolet\">\n</a>\n<a href=\"https://huggingface.co/spaces/docs-demos/transfo-xl-wt103\">\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\n</a>\n</div>\n\n## Overview\n\nThe Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\nSalakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinuso\u00efdal) embeddings which can\nreuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax\ninputs and outputs (tied).\n\nThe abstract from the paper is the following:\n\n*Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the\nsetting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency\nbeyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a\nnovel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the\ncontext fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450%\nlonger than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+\ntimes faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of\nbpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn\nTreebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably\ncoherent, novel text articles with thousands of tokens.*\n\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\n\n## Usage tips\n\n- Transformer-XL uses relative sinusoidal positional embeddings. Padding can be done on the left or on the right. The\n  original implementation trains on SQuAD with padding on the left, therefore the padding defaults are set to left.\n- Transformer-XL is one of the few models that has no sequence length limit.\n- Same as a regular GPT model, but introduces a recurrence mechanism for two consecutive segments (similar to a regular RNNs with two consecutive inputs). In this context, a segment is a number of consecutive tokens (for instance 512) that may span across multiple documents, and segments are fed in order to the model.\n- Basically, the hidden states of the previous segment are concatenated to the current input to compute the attention scores. This allows the model to pay attention to information that was in the previous segment as well as the current one. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments.\n- This changes the positional embeddings to positional relative embeddings (as the regular positional embeddings would give the same results in the current input and the current hidden state at a given position) and needs to make some adjustments in the way attention scores are computed.\n\n\n<Tip warning={true}>\n\nTransformerXL does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035)\n\n</Tip>\n\n## Resources\n\n- [Text classification task guide](../tasks/sequence_classification)\n- [Causal language modeling task guide](../tasks/language_modeling)\n\n## TransfoXLConfig\n\n[[autodoc]] TransfoXLConfig\n\n## TransfoXLTokenizer\n\n[[autodoc]] TransfoXLTokenizer\n    - save_vocabulary\n\n## TransfoXL specific outputs\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_transfo_xl.TransfoXLLMHeadModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLModelOutput\n\n[[autodoc]] models.deprecated.transfo_xl.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput\n\n<frameworkcontent>\n<pt>\n\n## TransfoXLModel\n\n[[autodoc]] TransfoXLModel\n    - forward\n\n## TransfoXLLMHeadModel\n\n[[autodoc]] TransfoXLLMHeadModel\n    - forward\n\n## TransfoXLForSequenceClassification\n\n[[autodoc]] TransfoXLForSequenceClassification\n    - forward\n\n</pt>\n<tf>\n\n## TFTransfoXLModel\n\n[[autodoc]] TFTransfoXLModel\n    - call\n\n## TFTransfoXLLMHeadModel\n\n[[autodoc]] TFTransfoXLLMHeadModel\n    - call\n\n## TFTransfoXLForSequenceClassification\n\n[[autodoc]] TFTransfoXLForSequenceClassification\n    - call\n\n</tf>\n</frameworkcontent>\n\n## Internal Layers\n\n[[autodoc]] AdaptiveEmbedding\n\n[[autodoc]] TFAdaptiveEmbedding\n", "# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\n Tokenization classes for Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl.\n\"\"\"\n\n\nimport glob\nimport os\nimport pickle\nimport re\nfrom collections import Counter, OrderedDict\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\n\nfrom ....tokenization_utils import PreTrainedTokenizer\nfrom ....utils import (\n    cached_file,\n    is_sacremoses_available,\n    is_torch_available,\n    logging,\n    requires_backends,\n    strtobool,\n    torch_only_method,\n)\n\n\nif is_sacremoses_available():\n    import sacremoses as sm\n\n\nif is_torch_available():\n    import torch\n\n\nlogger = logging.get_logger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \"pretrained_vocab_file\": \"vocab.pkl\",\n    \"pretrained_vocab_file_torch\": \"vocab.bin\",\n    \"vocab_file\": \"vocab.txt\",\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"pretrained_vocab_file\": {\n        \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/vocab.pkl\",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"transfo-xl-wt103\": None,\n}\n\nPRETRAINED_CORPUS_ARCHIVE_MAP = {\n    \"transfo-xl-wt103\": \"https://huggingface.co/transfo-xl-wt103/resolve/main/corpus.bin\",\n}\nCORPUS_NAME = \"corpus.bin\"\n\nMATCH_NUMBERS = r\"(?<=\\d)[,.](?=\\d)\", r\" @\\g<0>@ \"\nDETOKENIZE_NUMBERS = [(r\" @\\,@ \", r\",\"), (r\" @\\.@ \", r\".\")]\n\n\ndef tokenize_numbers(text_array: List[str]) -> List[str]:\n    \"\"\"\n    Splits large comma-separated numbers and floating point values. This is done by replacing commas with ' @,@ ' and\n    dots with ' @.@ '.\n\n    Args:\n        text_array: An already tokenized text as list.\n\n    Returns:\n        A list of strings with tokenized numbers.\n\n    Example:\n\n    ```python\n    >>> tokenize_numbers([\"$\", \"5,000\", \"1.73\", \"m\"])\n    ['$', '5', '@,@', '000', '1', '@.@', '73', 'm']\n    ```\"\"\"\n    tokenized = []\n    for i in range(len(text_array)):\n        reg, sub = MATCH_NUMBERS\n        replaced = re.sub(reg, sub, text_array[i]).split()\n        tokenized.extend(replaced)\n\n    return tokenized\n\n\ndef detokenize_numbers(text: str) -> str:\n    \"\"\"\n    Inverts the operation of *tokenize_numbers*. This is replacing ' @,@ ' and ' @.@' by ',' and '.'.\n\n    Args:\n        text: A string where the number should be detokenized.\n\n    Returns:\n        A detokenized string.\n\n    Example:\n\n    ```python\n    >>> detokenize_numbers(\"$ 5 @,@ 000 1 @.@ 73 m\")\n    '$ 5,000 1.73 m'\n    ```\"\"\"\n    for reg, sub in DETOKENIZE_NUMBERS:\n        text = re.sub(reg, sub, text)\n    return text\n\n\nclass TransfoXLTokenizer(PreTrainedTokenizer):\n    \"\"\"\n    Construct a Transformer-XL tokenizer adapted from Vocab class in [the original\n    code](https://github.com/kimiyoung/transformer-xl). The Transformer-XL tokenizer is a word-level tokenizer (no\n    sub-word tokenization).\n\n    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n    this superclass for more information regarding those methods.\n\n    Args:\n        special (`List[str]`, *optional*):\n            A list of special tokens (to be treated by the original implementation of this tokenizer).\n        min_freq (`int`, *optional*, defaults to 0):\n            The minimum number of times a token has to be present in order to be kept in the vocabulary (otherwise it\n            will be mapped to `unk_token`).\n        max_size (`int`, *optional*):\n            The maximum size of the vocabulary. If left unset, it will default to the size of the vocabulary found\n            after excluding the tokens according to the `min_freq` rule.\n        lower_case (`bool`, *optional*, defaults to `False`):\n            Whether or not to lowercase the input when tokenizing.\n        delimiter (`str`, *optional*):\n            The delimiter used between tokens.\n        vocab_file (`str`, *optional*):\n            File containing the vocabulary (from the original implementation).\n        pretrained_vocab_file (`str`, *optional*):\n            File containing the vocabulary as saved with the `save_pretrained()` method.\n        never_split (`List[str]`, *optional*):\n            List of tokens that should never be split. If no list is specified, will simply use the existing special\n            tokens.\n        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n            token instead.\n        eos_token (`str`, *optional*, defaults to `\"<eos>\"`):\n            The end of sequence token.\n        additional_special_tokens (`List[str]`, *optional*, defaults to `['<formula>']`):\n            A list of additional special tokens (for the HuggingFace functionality).\n        language (`str`, *optional*, defaults to `\"en\"`):\n            The language of this tokenizer (used for mose preprocessing).\n    \"\"\"\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n    model_input_names = [\"input_ids\"]\n\n    def __init__(\n        self,\n        special=None,\n        min_freq=0,\n        max_size=None,\n        lower_case=False,\n        delimiter=None,\n        vocab_file=None,\n        pretrained_vocab_file: str = None,\n        never_split=None,\n        unk_token=\"<unk>\",\n        eos_token=\"<eos>\",\n        additional_special_tokens=[\"<formula>\"],\n        language=\"en\",\n        **kwargs,\n    ):\n        logger.error(\n            \"`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. \"\n            \"See more details on this model's documentation page: \"\n            \"`https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\"\n        )\n\n        requires_backends(self, \"sacremoses\")\n        if special is None:\n            special = []\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n        self.punctuation_symbols = '!\"#$%&()*+,-./\\\\:;<=>?@[\\\\]^_`{|}~'\n        self.punction_without_space_before_pattern = re.compile(rf\"[^\\s][{self.punctuation_symbols}]\")\n        self.punctuation_with_space_around_pattern = self._compile_space_around_punctuation_pattern()\n        self.language = language\n        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)\n        self.moses_tokenizer = sm.MosesTokenizer(language)\n        self.moses_detokenizer = sm.MosesDetokenizer(language)\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n        # This try... catch... is not beautiful but honestly this tokenizer was not made to be used\n        # in a library like ours, at all.\n        try:\n            vocab_dict = None\n            if pretrained_vocab_file is not None:\n                # Priority on pickle files (support PyTorch and TF)\n                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n                    raise ValueError(\n                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is \"\n                        \"potentially malicious. It's recommended to never unpickle data that could have come from an \"\n                        \"untrusted source, or that could have been tampered with. If you already verified the pickle \"\n                        \"data and decided to use it, you can set the environment variable \"\n                        \"`TRUST_REMOTE_CODE` to `True` to allow it.\"\n                    )\n                with open(pretrained_vocab_file, \"rb\") as f:\n                    vocab_dict = pickle.load(f)\n\n                # Loading a torch-saved transfo-xl vocab dict with pickle results in an integer\n                # Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.\n                # We therefore load it with torch, if it's available.\n                if isinstance(vocab_dict, int):\n                    if not is_torch_available():\n                        raise ImportError(\n                            \"Not trying to load dict with PyTorch as you need to install pytorch to load \"\n                            \"from a PyTorch pretrained vocabulary, \"\n                            \"or activate it with environment variables USE_TORCH=1 and USE_TF=0.\"\n                        )\n                    vocab_dict = torch.load(pretrained_vocab_file)\n\n            if vocab_dict is not None:\n                for key, value in vocab_dict.items():\n                    if key not in self.__dict__ or key in [\"sym2idx\", \"idx2sym\"]:\n                        self.__dict__[key] = value\n            elif vocab_file is not None:\n                self.build_vocab()\n\n        except Exception as e:\n            raise ValueError(\n                f\"Unable to parse file {pretrained_vocab_file}. Unknown format. \"\n                \"If you tried to load a model saved through TransfoXLTokenizerFast, \"\n                \"please note they are not compatible.\"\n            ) from e\n\n        if vocab_file is not None:\n            self.build_vocab()\n\n        super().__init__(\n            special=special,\n            min_freq=min_freq,\n            max_size=max_size,\n            lower_case=lower_case,\n            delimiter=delimiter,\n            vocab_file=vocab_file,\n            pretrained_vocab_file=pretrained_vocab_file,\n            never_split=never_split,\n            unk_token=unk_token,\n            eos_token=eos_token,\n            additional_special_tokens=additional_special_tokens,\n            language=language,\n            **kwargs,\n        )\n\n        # these are not required to initialize the parent class as only used when tokenizing.\n        if never_split is None:\n            never_split = self.all_special_tokens\n        self.never_split = never_split\n\n    @property\n    def do_lower_case(self):\n        return self.lower_case\n\n    def _compile_space_around_punctuation_pattern(self):\n        look_ahead_for_special_token = f\"(?=[{self.punctuation_symbols}])\"\n        look_ahead_to_match_all_except_space = r\"(?=[^\\s])\"\n        return re.compile(r\"\" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)\n\n    def count_file(self, path, verbose=False, add_eos=False):\n        if verbose:\n            logger.info(f\"counting file {path} ...\")\n        assert os.path.exists(path), f\"Input file {path} not found\"\n\n        sents = []\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    logger.info(f\"    line {idx}\")\n                symbols = self.tokenize(line, add_eos=add_eos)\n                self.counter.update(symbols)\n                sents.append(symbols)\n\n        return sents\n\n    def count_sents(self, sents, verbose=False):\n        \"\"\"\n        sents : a list of sentences, each a list of tokenized symbols\n        \"\"\"\n        if verbose:\n            logger.info(f\"counting {len(sents)} sents ...\")\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                logger.info(f\"    line {idx}\")\n            self.counter.update(symbols)\n\n    def _build_from_file(self, vocab_file):\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n\n        with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                symb = line.strip().split()[0]\n                self.add_symbol(symb)\n        if \"<UNK>\" in self.sym2idx:\n            self.unk_idx = self.sym2idx[\"<UNK>\"]\n        elif \"<unk>\" in self.sym2idx:\n            self.unk_idx = self.sym2idx[\"<unk>\"]\n        else:\n            raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")\n\n    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n        if os.path.isdir(save_directory):\n            vocab_file = os.path.join(\n                save_directory,\n                (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"pretrained_vocab_file\"],\n            )\n        else:\n            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n        with open(vocab_file, \"wb\") as f:\n            pickle.dump(self.__dict__, f)\n        return (vocab_file,)\n\n    def build_vocab(self):\n        if self.vocab_file:\n            logger.info(f\"building vocab from {self.vocab_file}\")\n            self._build_from_file(self.vocab_file)\n            logger.info(f\"Final vocab size {len(self.sym2idx)}\")\n        else:\n            logger.info(f\"building vocab with min_freq={self.min_freq}, max_size={self.max_size}\")\n            self.idx2sym = []\n            self.sym2idx = OrderedDict()\n\n            for sym in self.special:\n                self.add_special(sym)\n\n            for sym, cnt in self.counter.most_common(self.max_size):\n                if cnt < self.min_freq:\n                    break\n                self.add_symbol(sym)\n\n            logger.info(f\"Final vocab size {len(self.sym2idx)} from {len(self.counter)} unique tokens\")\n\n    @torch_only_method\n    def encode_file(self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False):\n        if verbose:\n            logger.info(f\"encoding file {path} ...\")\n        assert os.path.exists(path), f\"Output file {path} not found\"\n        encoded = []\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    logger.info(f\"    line {idx}\")\n                symbols = self.tokenize(line, add_eos=add_eos, add_double_eos=add_double_eos)\n                encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    @torch_only_method\n    def encode_sents(self, sents, ordered=False, verbose=False):\n        if verbose:\n            logger.info(f\"encoding {len(sents)} sents ...\")\n        encoded = []\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                logger.info(f\"    line {idx}\")\n            encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def add_special(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n            setattr(self, f\"{sym.strip('<>')}_idx\", self.sym2idx[sym])\n\n    def add_symbol(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n\n    def move_added_token(self, token: str, target_idx: int):\n        \"\"\"\n        Moves an added token to a specific position in the vocab. This method should be used when resizing an embedding\n        layer other than the last one in the `AdaptiveEmbedding` in order to move the token in the tokenizer from the\n        default position (at the very end) to the desired one.\n\n        Args:\n            token: The token to move to a specific position in the vocab.\n            target_idx: The position where the token should be moved to.\n        \"\"\"\n        assert token in self.added_tokens_encoder, \"Token which should be moved has to be an added token\"\n        assert token not in self.idx2sym, \"Token which should be moved is already in vocab\"\n\n        # Insert sym into vocab\n        self.idx2sym.insert(target_idx, token)\n        self.sym2idx[token] = target_idx\n\n        # Shift following indices in sym2idx\n        for idx in range(target_idx + 1, len(self.idx2sym)):\n            current_sym = self.idx2sym[idx]\n            self.sym2idx[current_sym] = idx\n\n        # Delete token from added_tokens\n        old_index = self._added_tokens_encoder.pop(token)\n        self._added_tokens_decoder.pop(old_index)\n\n    def moses_punct_norm(self, text):\n        return self.moses_punct_normalizer.normalize(text)\n\n    def moses_tokenize(self, text):\n        return self.moses_tokenizer.tokenize(\n            text, aggressive_dash_splits=True, return_str=False, escape=False, protected_patterns=self.never_split\n        )\n\n    def moses_pipeline(self, text: str) -> List[str]:\n        \"\"\"\n        Does basic tokenization using [`sacremoses.MosesPunctNormalizer`] and [`sacremoses.MosesTokenizer`] with\n        *aggressive_dash_splits=True* (see [`sacremoses.tokenize.MosesTokenizer.tokenize`]). Additionally, large\n        comma-separated numbers and floating point values are split. E.g. \"23,000 people are 1.80m tall\" -> \"23 @,@ 000\n        people are 1 @.@ 80m tall\"\n\n        Args:\n            text: Text to be tokenize\n\n        Returns:\n            A list of tokenized string\n\n        Example:\n\n        ```python\n        >>> tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")\n        >>> tokenizer.moses_pipeline(\"23,000 people are 1.80 m tall\")\n        ['23', '@,@', '000', 'people', 'are', '1', '@.@', '80', 'm', 'tall']\n        ```\"\"\"\n        text = self.moses_punct_norm(text)\n        text = self.moses_tokenize(text)\n        text = tokenize_numbers(text)\n        return text\n\n    def _convert_id_to_token(self, idx):\n        \"\"\"Converts an id in a token (BPE) using the vocab.\"\"\"\n        assert 0 <= idx < len(self), f\"Index {idx} out of vocabulary range\"\n        return self.idx2sym[idx]\n\n    def _convert_token_to_id(self, sym):\n        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n        if sym in self.sym2idx:\n            return self.sym2idx[sym]\n        else:\n            # logger.info(f'encounter unk {sym}')\n            # assert '<eos>' not in sym\n            if hasattr(self, \"unk_idx\"):\n                return self.sym2idx.get(sym, self.unk_idx)\n            # Backward compatibility with pre-trained models\n            elif \"<unk>\" in self.sym2idx:\n                return self.sym2idx[\"<unk>\"]\n            elif \"<UNK>\" in self.sym2idx:\n                return self.sym2idx[\"<UNK>\"]\n            else:\n                raise ValueError(\"Token not in vocabulary and no <unk> token in vocabulary for replacement.\")\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\"\n        Converts a sequence of tokens (string) in a single string. Additionally, the split numbers are converted back\n        into it's original form.\n        \"\"\"\n        out_string = self.moses_detokenizer.detokenize(tokens)\n        return detokenize_numbers(out_string).strip()\n\n    @torch_only_method\n    def convert_to_tensor(self, symbols):\n        return torch.LongTensor(self.convert_tokens_to_ids(symbols))\n\n    @property\n    def vocab_size(self):\n        return len(self.idx2sym)\n\n    def get_vocab(self):\n        vocab = self.sym2idx.copy()\n        vocab.update(self.added_tokens_encoder)\n        return vocab\n\n    def _tokenize(self, line, add_eos=False, add_double_eos=False):\n        line = line.strip()\n        # convert to lower case\n        if self.lower_case:\n            line = line.lower()\n\n        # empty delimiter '' will evaluate False\n        if self.delimiter == \"\":\n            symbols = line\n        else:\n            symbols = self.moses_pipeline(line)\n\n        if add_double_eos:  # lm1b\n            return [\"<S>\"] + symbols + [\"<S>\"]\n        elif add_eos:\n            return symbols + [\"<eos>\"]\n        else:\n            return symbols\n\n\nclass LMOrderedIterator(object):\n    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None):\n        \"\"\"\n        data -- LongTensor -- the LongTensor is strictly ordered\n        \"\"\"\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        self.n_step = data.size(0) // bsz\n\n        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n        data = data.narrow(0, 0, self.n_step * bsz)\n\n        # Evenly divide the data across the bsz batches.\n        self.data = data.view(bsz, -1).t().contiguous().to(device)\n\n        # Number of mini-batches\n        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n\n    def get_batch(self, i, bptt=None):\n        if bptt is None:\n            bptt = self.bptt\n        seq_len = min(bptt, self.data.size(0) - 1 - i)\n\n        end_idx = i + seq_len\n        beg_idx = max(0, i - self.ext_len)\n\n        data = self.data[beg_idx:end_idx]\n        target = self.data[i + 1 : i + 1 + seq_len]\n\n        data_out = data.transpose(0, 1).contiguous().to(self.device)\n        target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n        return data_out, target_out, seq_len\n\n    def get_fixlen_iter(self, start=0):\n        for i in range(start, self.data.size(0) - 1, self.bptt):\n            yield self.get_batch(i)\n\n    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n        max_len = self.bptt + max_deviation * std\n        i = start\n        while True:\n            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.0\n            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n            data, target, seq_len = self.get_batch(i, bptt)\n            i += seq_len\n            yield data, target, seq_len\n            if i >= self.data.size(0) - 2:\n                break\n\n    def __iter__(self):\n        return self.get_fixlen_iter()\n\n\nclass LMShuffledIterator(object):\n    def __init__(self, data, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):\n        \"\"\"\n        data -- list[LongTensor] -- there is no order among the LongTensors\n        \"\"\"\n        self.data = data\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self):\n        # index iterator\n        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle else np.array(range(len(self.data)))\n\n        # sentence iterator\n        for idx in epoch_indices:\n            yield self.data[idx]\n\n    @torch_only_method\n    def stream_iterator(self, sent_stream):\n        # streams for each data in the batch\n        streams = [None] * self.bsz\n\n        data = torch.LongTensor(self.bptt, self.bsz)\n        target = torch.LongTensor(self.bptt, self.bsz)\n\n        n_retain = 0\n\n        while True:\n            # data   : [n_retain+bptt x bsz]\n            # target : [bptt x bsz]\n            data[n_retain:].fill_(-1)\n            target.fill_(-1)\n\n            valid_batch = True\n\n            for i in range(self.bsz):\n                n_filled = 0\n                try:\n                    while n_filled < self.bptt:\n                        if streams[i] is None or len(streams[i]) <= 1:\n                            streams[i] = next(sent_stream)\n                        # number of new tokens to fill in\n                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)\n                        # first n_retain tokens are retained from last batch\n                        data[n_retain + n_filled : n_retain + n_filled + n_new, i] = streams[i][:n_new]\n                        target[n_filled : n_filled + n_new, i] = streams[i][1 : n_new + 1]\n                        streams[i] = streams[i][n_new:]\n                        n_filled += n_new\n                except StopIteration:\n                    valid_batch = False\n                    break\n\n            if not valid_batch:\n                return\n\n            data_out = data.transpose(0, 1).contiguous().to(self.device)\n            target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n            yield data_out, target_out, self.bptt\n\n            n_retain = min(data.size(0), self.ext_len)\n            if n_retain > 0:\n                data[:n_retain] = data[-n_retain:]\n            data.resize_(n_retain + self.bptt, data.size(1))\n\n    def __iter__(self):\n        # sent_stream is an iterator\n        sent_stream = self.get_sent_stream()\n\n        for batch in self.stream_iterator(sent_stream):\n            yield batch\n\n\nclass LMMultiFileIterator(LMShuffledIterator):\n    def __init__(self, paths, vocab, bsz, bptt, device=\"cpu\", ext_len=None, shuffle=False):\n        self.paths = paths\n        self.vocab = vocab\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self, path):\n        sents = self.vocab.encode_file(path, add_double_eos=True)\n        if self.shuffle:\n            np.random.shuffle(sents)\n        sent_stream = iter(sents)\n\n        return sent_stream\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.paths)\n\n        for path in self.paths:\n            # sent_stream is an iterator\n            sent_stream = self.get_sent_stream(path)\n            for batch in self.stream_iterator(sent_stream):\n                yield batch\n\n\nclass TransfoXLCorpus(object):\n    @classmethod\n    @torch_only_method\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        \"\"\"\n        Instantiate a pre-processed corpus.\n        \"\"\"\n        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        is_local = os.path.isdir(pretrained_model_name_or_path)\n        # redirect to the cache, if necessary\n        try:\n            resolved_corpus_file = cached_file(pretrained_model_name_or_path, CORPUS_NAME, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                f\"Corpus '{pretrained_model_name_or_path}' was not found in corpus list\"\n                f\" ({', '.join(PRETRAINED_CORPUS_ARCHIVE_MAP.keys())}. We assumed '{pretrained_model_name_or_path}'\"\n                f\" was a path or url but couldn't find files {CORPUS_NAME} at this path or url.\"\n            )\n            return None\n        if is_local:\n            logger.info(f\"loading corpus file {resolved_corpus_file}\")\n        else:\n            logger.info(f\"loading corpus file {CORPUS_NAME} from cache at {resolved_corpus_file}\")\n\n        # Instantiate tokenizer.\n        corpus = cls(*inputs, **kwargs)\n        corpus_dict = torch.load(resolved_corpus_file)\n        for key, value in corpus_dict.items():\n            corpus.__dict__[key] = value\n        corpus.vocab = vocab\n        if corpus.train is not None:\n            corpus.train = torch.tensor(corpus.train, dtype=torch.long)\n        if corpus.valid is not None:\n            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)\n        if corpus.test is not None:\n            corpus.test = torch.tensor(corpus.test, dtype=torch.long)\n        return corpus\n\n    def __init__(self, *args, **kwargs):\n        self.vocab = TransfoXLTokenizer(*args, **kwargs)\n        self.dataset = None\n        self.train = None\n        self.valid = None\n        self.test = None\n\n    def build_corpus(self, path, dataset):\n        self.dataset = dataset\n\n        if self.dataset in [\"ptb\", \"wt2\", \"enwik8\", \"text8\"]:\n            self.vocab.count_file(os.path.join(path, \"train.txt\"))\n            self.vocab.count_file(os.path.join(path, \"valid.txt\"))\n            self.vocab.count_file(os.path.join(path, \"test.txt\"))\n        elif self.dataset == \"wt103\":\n            self.vocab.count_file(os.path.join(path, \"train.txt\"))\n        elif self.dataset == \"lm1b\":\n            train_path_pattern = os.path.join(\n                path,\n                \"1-billion-word-language-modeling-benchmark-r13output\",\n                \"training-monolingual.tokenized.shuffled\",\n                \"news.en-*\",\n            )\n            train_paths = glob.glob(train_path_pattern)\n            # the vocab will load from file when build_vocab() is called\n\n        self.vocab.build_vocab()\n\n        if self.dataset in [\"ptb\", \"wt2\", \"wt103\"]:\n            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True)\n            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True)\n            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True)\n        elif self.dataset in [\"enwik8\", \"text8\"]:\n            self.train = self.vocab.encode_file(os.path.join(path, \"train.txt\"), ordered=True, add_eos=False)\n            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=True, add_eos=False)\n            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=True, add_eos=False)\n        elif self.dataset == \"lm1b\":\n            self.train = train_paths\n            self.valid = self.vocab.encode_file(os.path.join(path, \"valid.txt\"), ordered=False, add_double_eos=True)\n            self.test = self.vocab.encode_file(os.path.join(path, \"test.txt\"), ordered=False, add_double_eos=True)\n\n    def get_iterator(self, split, *args, **kwargs):\n        if split == \"train\":\n            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:\n                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n            elif self.dataset == \"lm1b\":\n                kwargs[\"shuffle\"] = True\n                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n        elif split in [\"valid\", \"test\"]:\n            data = self.valid if split == \"valid\" else self.test\n            if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:\n                data_iter = LMOrderedIterator(data, *args, **kwargs)\n            elif self.dataset == \"lm1b\":\n                data_iter = LMShuffledIterator(data, *args, **kwargs)\n        else:\n            data_iter = None\n            raise ValueError(f\"Split not recognized: {split}\")\n\n        return data_iter\n\n\n@torch_only_method\ndef get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \"cache.pt\")\n    fn_pickle = os.path.join(datadir, \"cache.pkl\")\n    if os.path.exists(fn):\n        logger.info(\"Loading cached dataset...\")\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        logger.info(\"Loading cached dataset from pickle...\")\n        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n            raise ValueError(\n                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n            )\n        with open(fn, \"rb\") as fp:\n            corpus = pickle.load(fp)\n    else:\n        logger.info(f\"Producing dataset {dataset}...\")\n        kwargs = {}\n        if dataset in [\"wt103\", \"wt2\"]:\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = False\n        elif dataset == \"ptb\":\n            kwargs[\"special\"] = [\"<eos>\"]\n            kwargs[\"lower_case\"] = True\n        elif dataset == \"lm1b\":\n            kwargs[\"special\"] = []\n            kwargs[\"lower_case\"] = False\n            kwargs[\"vocab_file\"] = os.path.join(datadir, \"1b_word_vocab.txt\")\n        elif dataset in [\"enwik8\", \"text8\"]:\n            pass\n\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n\n    return corpus\n", "# coding=utf-8\n# Copyright 2020, The RAG Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"RAG Retriever model implementation.\"\"\"\n\nimport os\nimport pickle\nimport time\nfrom typing import Iterable, List, Optional, Tuple\n\nimport numpy as np\n\nfrom ...tokenization_utils import PreTrainedTokenizer\nfrom ...tokenization_utils_base import BatchEncoding\nfrom ...utils import cached_file, is_datasets_available, is_faiss_available, logging, requires_backends, strtobool\nfrom .configuration_rag import RagConfig\nfrom .tokenization_rag import RagTokenizer\n\n\nif is_datasets_available():\n    from datasets import Dataset, load_dataset, load_from_disk\n\nif is_faiss_available():\n    import faiss\n\n\nlogger = logging.get_logger(__name__)\n\n\nLEGACY_INDEX_PATH = \"https://storage.googleapis.com/huggingface-nlp/datasets/wiki_dpr/\"\n\n\nclass Index:\n    \"\"\"\n    A base class for the Indices encapsulated by the [`RagRetriever`].\n    \"\"\"\n\n    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:\n        \"\"\"\n        Returns a list of dictionaries, containing titles and text of the retrieved documents.\n\n        Args:\n            doc_ids (`np.ndarray` of shape `(batch_size, n_docs)`):\n                A tensor of document indices.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        For each query in the batch, retrieves `n_docs` documents.\n\n        Args:\n            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):\n                An array of query vectors.\n            n_docs (`int`):\n                The number of docs retrieved per query.\n\n        Returns:\n            `np.ndarray` of shape `(batch_size, n_docs)`: A tensor of indices of retrieved documents. `np.ndarray` of\n            shape `(batch_size, vector_size)`: A tensor of vector representations of retrieved documents.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_initialized(self):\n        \"\"\"\n        Returns `True` if index is already initialized.\n        \"\"\"\n        raise NotImplementedError\n\n    def init_index(self):\n        \"\"\"\n        A function responsible for loading the index into memory. Should be called only once per training run of a RAG\n        model. E.g. if the model is trained on multiple GPUs in a distributed setup, only one of the workers will load\n        the index.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass LegacyIndex(Index):\n    \"\"\"\n    An index which can be deserialized from the files built using https://github.com/facebookresearch/DPR. We use\n    default faiss index parameters as specified in that repository.\n\n    Args:\n        vector_size (`int`):\n            The dimension of indexed vectors.\n        index_path (`str`):\n            A path to a *directory* containing index files compatible with [`~models.rag.retrieval_rag.LegacyIndex`]\n    \"\"\"\n\n    INDEX_FILENAME = \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\"\n    PASSAGE_FILENAME = \"psgs_w100.tsv.pkl\"\n\n    def __init__(self, vector_size, index_path):\n        self.index_id_to_db_id = []\n        self.index_path = index_path\n        self.passages = self._load_passages()\n        self.vector_size = vector_size\n        self.index = None\n        self._index_initialized = False\n\n    def _resolve_path(self, index_path, filename):\n        is_local = os.path.isdir(index_path)\n        try:\n            # Load from URL or cache if already cached\n            resolved_archive_file = cached_file(index_path, filename)\n        except EnvironmentError:\n            msg = (\n                f\"Can't load '{filename}'. Make sure that:\\n\\n\"\n                f\"- '{index_path}' is a correct remote path to a directory containing a file named {filename}\\n\\n\"\n                f\"- or '{index_path}' is the correct path to a directory containing a file named {filename}.\\n\\n\"\n            )\n            raise EnvironmentError(msg)\n        if is_local:\n            logger.info(f\"loading file {resolved_archive_file}\")\n        else:\n            logger.info(f\"loading file {filename} from cache at {resolved_archive_file}\")\n        return resolved_archive_file\n\n    def _load_passages(self):\n        logger.info(f\"Loading passages from {self.index_path}\")\n        passages_path = self._resolve_path(self.index_path, self.PASSAGE_FILENAME)\n        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n            raise ValueError(\n                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n            )\n        with open(passages_path, \"rb\") as passages_file:\n            passages = pickle.load(passages_file)\n        return passages\n\n    def _deserialize_index(self):\n        logger.info(f\"Loading index from {self.index_path}\")\n        resolved_index_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index.dpr\")\n        self.index = faiss.read_index(resolved_index_path)\n        resolved_meta_path = self._resolve_path(self.index_path, self.INDEX_FILENAME + \".index_meta.dpr\")\n        if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n            raise ValueError(\n                \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n                \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n                \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n                \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n            )\n        with open(resolved_meta_path, \"rb\") as metadata_file:\n            self.index_id_to_db_id = pickle.load(metadata_file)\n        assert (\n            len(self.index_id_to_db_id) == self.index.ntotal\n        ), \"Deserialized index_id_to_db_id should match faiss index size\"\n\n    def is_initialized(self):\n        return self._index_initialized\n\n    def init_index(self):\n        index = faiss.IndexHNSWFlat(self.vector_size + 1, 512)\n        index.hnsw.efSearch = 128\n        index.hnsw.efConstruction = 200\n        self.index = index\n        self._deserialize_index()\n        self._index_initialized = True\n\n    def get_doc_dicts(self, doc_ids: np.array):\n        doc_list = []\n        for doc_ids_i in doc_ids:\n            ids = [str(int(doc_id)) for doc_id in doc_ids_i]\n            docs = [self.passages[doc_id] for doc_id in ids]\n            doc_list.append(docs)\n        doc_dicts = []\n        for docs in doc_list:\n            doc_dict = {}\n            doc_dict[\"title\"] = [doc[1] for doc in docs]\n            doc_dict[\"text\"] = [doc[0] for doc in docs]\n            doc_dicts.append(doc_dict)\n        return doc_dicts\n\n    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:\n        aux_dim = np.zeros(len(question_hidden_states), dtype=\"float32\").reshape(-1, 1)\n        query_nhsw_vectors = np.hstack((question_hidden_states, aux_dim))\n        _, docs_ids = self.index.search(query_nhsw_vectors, n_docs)\n        vectors = [[self.index.reconstruct(int(doc_id))[:-1] for doc_id in doc_ids] for doc_ids in docs_ids]\n        ids = [[int(self.index_id_to_db_id[doc_id]) for doc_id in doc_ids] for doc_ids in docs_ids]\n        return np.array(ids), np.array(vectors)\n\n\nclass HFIndexBase(Index):\n    def __init__(self, vector_size, dataset, index_initialized=False):\n        self.vector_size = vector_size\n        self.dataset = dataset\n        self._index_initialized = index_initialized\n        self._check_dataset_format(with_index=index_initialized)\n        dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True, dtype=\"float32\")\n\n    def _check_dataset_format(self, with_index: bool):\n        if not isinstance(self.dataset, Dataset):\n            raise ValueError(f\"Dataset should be a datasets.Dataset object, but got {type(self.dataset)}\")\n        if len({\"title\", \"text\", \"embeddings\"} - set(self.dataset.column_names)) > 0:\n            raise ValueError(\n                \"Dataset should be a dataset with the following columns: \"\n                \"title (str), text (str) and embeddings (arrays of dimension vector_size), \"\n                f\"but got columns {self.dataset.column_names}\"\n            )\n        if with_index and \"embeddings\" not in self.dataset.list_indexes():\n            raise ValueError(\n                \"Missing faiss index in the dataset. Make sure you called `dataset.add_faiss_index` to compute it \"\n                \"or `dataset.load_faiss_index` to load one from the disk.\"\n            )\n\n    def init_index(self):\n        raise NotImplementedError()\n\n    def is_initialized(self):\n        return self._index_initialized\n\n    def get_doc_dicts(self, doc_ids: np.ndarray) -> List[dict]:\n        return [self.dataset[doc_ids[i].tolist()] for i in range(doc_ids.shape[0])]\n\n    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np.ndarray, np.ndarray]:\n        _, ids = self.dataset.search_batch(\"embeddings\", question_hidden_states, n_docs)\n        docs = [self.dataset[[i for i in indices if i >= 0]] for indices in ids]\n        vectors = [doc[\"embeddings\"] for doc in docs]\n        for i in range(len(vectors)):\n            if len(vectors[i]) < n_docs:\n                vectors[i] = np.vstack([vectors[i], np.zeros((n_docs - len(vectors[i]), self.vector_size))])\n        return np.array(ids), np.array(vectors)  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)\n\n\nclass CanonicalHFIndex(HFIndexBase):\n    \"\"\"\n    A wrapper around an instance of [`~datasets.Datasets`]. If `index_path` is set to `None`, we load the pre-computed\n    index available with the [`~datasets.arrow_dataset.Dataset`], otherwise, we load the index from the indicated path\n    on disk.\n\n    Args:\n        vector_size (`int`): the dimension of the passages embeddings used by the index\n        dataset_name (`str`, optional, defaults to `wiki_dpr`):\n            A dataset identifier of the indexed dataset on HuggingFace AWS bucket (list all available datasets and ids\n            with `datasets.list_datasets()`).\n        dataset_split (`str`, optional, defaults to `train`)\n            Which split of the `dataset` to load.\n        index_name (`str`, optional, defaults to `train`)\n            The index_name of the index associated with the `dataset`. The index loaded from `index_path` will be saved\n            under this name.\n        index_path (`str`, optional, defaults to `None`)\n            The path to the serialized faiss index on disk.\n        use_dummy_dataset (`bool`, optional, defaults to `False`):\n            If True, use the dummy configuration of the dataset for tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_size: int,\n        dataset_name: str = \"wiki_dpr\",\n        dataset_split: str = \"train\",\n        index_name: Optional[str] = None,\n        index_path: Optional[str] = None,\n        use_dummy_dataset=False,\n    ):\n        if int(index_path is None) + int(index_name is None) != 1:\n            raise ValueError(\"Please provide `index_name` or `index_path`.\")\n        self.dataset_name = dataset_name\n        self.dataset_split = dataset_split\n        self.index_name = index_name\n        self.index_path = index_path\n        self.use_dummy_dataset = use_dummy_dataset\n        logger.info(f\"Loading passages from {self.dataset_name}\")\n        dataset = load_dataset(\n            self.dataset_name, with_index=False, split=self.dataset_split, dummy=self.use_dummy_dataset\n        )\n        super().__init__(vector_size, dataset, index_initialized=False)\n\n    def init_index(self):\n        if self.index_path is not None:\n            logger.info(f\"Loading index from {self.index_path}\")\n            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)\n        else:\n            logger.info(f\"Loading index from {self.dataset_name} with index name {self.index_name}\")\n            self.dataset = load_dataset(\n                self.dataset_name,\n                with_embeddings=True,\n                with_index=True,\n                split=self.dataset_split,\n                index_name=self.index_name,\n                dummy=self.use_dummy_dataset,\n            )\n            self.dataset.set_format(\"numpy\", columns=[\"embeddings\"], output_all_columns=True)\n        self._index_initialized = True\n\n\nclass CustomHFIndex(HFIndexBase):\n    \"\"\"\n    A wrapper around an instance of [`~datasets.Datasets`]. The dataset and the index are both loaded from the\n    indicated paths on disk.\n\n    Args:\n        vector_size (`int`): the dimension of the passages embeddings used by the index\n        dataset_path (`str`):\n            The path to the serialized dataset on disk. The dataset should have 3 columns: title (str), text (str) and\n            embeddings (arrays of dimension vector_size)\n        index_path (`str`)\n            The path to the serialized faiss index on disk.\n    \"\"\"\n\n    def __init__(self, vector_size: int, dataset, index_path=None):\n        super().__init__(vector_size, dataset, index_initialized=index_path is None)\n        self.index_path = index_path\n\n    @classmethod\n    def load_from_disk(cls, vector_size, dataset_path, index_path):\n        logger.info(f\"Loading passages from {dataset_path}\")\n        if dataset_path is None or index_path is None:\n            raise ValueError(\n                \"Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \"\n                \"and `dataset.get_index('embeddings').save(index_path)`.\"\n            )\n        dataset = load_from_disk(dataset_path)\n        return cls(vector_size=vector_size, dataset=dataset, index_path=index_path)\n\n    def init_index(self):\n        if not self.is_initialized():\n            logger.info(f\"Loading index from {self.index_path}\")\n            self.dataset.load_faiss_index(\"embeddings\", file=self.index_path)\n            self._index_initialized = True\n\n\nclass RagRetriever:\n    \"\"\"\n    Retriever used to get documents from vector queries. It retrieves the documents embeddings as well as the documents\n    contents, and it formats them to be used with a RagModel.\n\n    Args:\n        config ([`RagConfig`]):\n            The configuration of the RAG model this Retriever is used with. Contains parameters indicating which\n            `Index` to build. You can load your own custom dataset with `config.index_name=\"custom\"` or use a canonical\n            one (default) from the datasets library with `config.index_name=\"wiki_dpr\"` for example.\n        question_encoder_tokenizer ([`PreTrainedTokenizer`]):\n            The tokenizer that was used to tokenize the question. It is used to decode the question and then use the\n            generator_tokenizer.\n        generator_tokenizer ([`PreTrainedTokenizer`]):\n            The tokenizer used for the generator part of the RagModel.\n        index ([`~models.rag.retrieval_rag.Index`], optional, defaults to the one defined by the configuration):\n            If specified, use this index instead of the one built using the configuration\n\n    Examples:\n\n    ```python\n    >>> # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')\n    >>> from transformers import RagRetriever\n\n    >>> retriever = RagRetriever.from_pretrained(\n    ...     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n    ... )\n\n    >>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py\n    >>> from transformers import RagRetriever\n\n    >>> dataset = (\n    ...     ...\n    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index\n    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n\n    >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n    >>> from transformers import RagRetriever\n\n    >>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n    >>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n    >>> retriever = RagRetriever.from_pretrained(\n    ...     \"facebook/dpr-ctx_encoder-single-nq-base\",\n    ...     index_name=\"custom\",\n    ...     passages_path=dataset_path,\n    ...     index_path=index_path,\n    ... )\n\n    >>> # To load the legacy index built originally for Rag's paper\n    >>> from transformers import RagRetriever\n\n    >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")\n    ```\"\"\"\n\n    def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):\n        self._init_retrieval = init_retrieval\n        requires_backends(self, [\"datasets\", \"faiss\"])\n        super().__init__()\n        self.index = index or self._build_index(config)\n        self.generator_tokenizer = generator_tokenizer\n        self.question_encoder_tokenizer = question_encoder_tokenizer\n\n        self.n_docs = config.n_docs\n        self.batch_size = config.retrieval_batch_size\n\n        self.config = config\n        if self._init_retrieval:\n            self.init_retrieval()\n\n        self.ctx_encoder_tokenizer = None\n        self.return_tokenized_docs = False\n\n    @staticmethod\n    def _build_index(config):\n        if config.index_name == \"legacy\":\n            return LegacyIndex(\n                config.retrieval_vector_size,\n                config.index_path or LEGACY_INDEX_PATH,\n            )\n        elif config.index_name == \"custom\":\n            return CustomHFIndex.load_from_disk(\n                vector_size=config.retrieval_vector_size,\n                dataset_path=config.passages_path,\n                index_path=config.index_path,\n            )\n        else:\n            return CanonicalHFIndex(\n                vector_size=config.retrieval_vector_size,\n                dataset_name=config.dataset,\n                dataset_split=config.dataset_split,\n                index_name=config.index_name,\n                index_path=config.index_path,\n                use_dummy_dataset=config.use_dummy_dataset,\n            )\n\n    @classmethod\n    def from_pretrained(cls, retriever_name_or_path, indexed_dataset=None, **kwargs):\n        requires_backends(cls, [\"datasets\", \"faiss\"])\n        config = kwargs.pop(\"config\", None) or RagConfig.from_pretrained(retriever_name_or_path, **kwargs)\n        rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)\n        question_encoder_tokenizer = rag_tokenizer.question_encoder\n        generator_tokenizer = rag_tokenizer.generator\n        if indexed_dataset is not None:\n            config.index_name = \"custom\"\n            index = CustomHFIndex(config.retrieval_vector_size, indexed_dataset)\n        else:\n            index = cls._build_index(config)\n        return cls(\n            config,\n            question_encoder_tokenizer=question_encoder_tokenizer,\n            generator_tokenizer=generator_tokenizer,\n            index=index,\n        )\n\n    def save_pretrained(self, save_directory):\n        if isinstance(self.index, CustomHFIndex):\n            if self.config.index_path is None:\n                index_path = os.path.join(save_directory, \"hf_dataset_index.faiss\")\n                self.index.dataset.get_index(\"embeddings\").save(index_path)\n                self.config.index_path = index_path\n            if self.config.passages_path is None:\n                passages_path = os.path.join(save_directory, \"hf_dataset\")\n                # datasets don't support save_to_disk with indexes right now\n                faiss_index = self.index.dataset._indexes.pop(\"embeddings\")\n                self.index.dataset.save_to_disk(passages_path)\n                self.index.dataset._indexes[\"embeddings\"] = faiss_index\n                self.config.passages_path = passages_path\n        self.config.save_pretrained(save_directory)\n        rag_tokenizer = RagTokenizer(\n            question_encoder=self.question_encoder_tokenizer,\n            generator=self.generator_tokenizer,\n        )\n        rag_tokenizer.save_pretrained(save_directory)\n\n    def init_retrieval(self):\n        \"\"\"\n        Retriever initialization function. It loads the index into memory.\n        \"\"\"\n\n        logger.info(\"initializing retrieval\")\n        self.index.init_index()\n\n    def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):\n        r\"\"\"\n        Postprocessing retrieved `docs` and combining them with `input_strings`.\n\n        Args:\n            docs  (`dict`):\n                Retrieved documents.\n            input_strings (`str`):\n                Input strings decoded by `preprocess_query`.\n            prefix (`str`):\n                Prefix added at the beginning of each input, typically used with T5-based models.\n\n        Return:\n            `tuple(tensors)`: a tuple consisting of two elements: contextualized `input_ids` and a compatible\n            `attention_mask`.\n        \"\"\"\n\n        def cat_input_and_doc(doc_title, doc_text, input_string, prefix):\n            # TODO(Patrick): if we train more RAG models, I want to put the input first to take advantage of effortless truncation\n            # TODO(piktus): better handling of truncation\n            if doc_title.startswith('\"'):\n                doc_title = doc_title[1:]\n            if doc_title.endswith('\"'):\n                doc_title = doc_title[:-1]\n            if prefix is None:\n                prefix = \"\"\n            out = (prefix + doc_title + self.config.title_sep + doc_text + self.config.doc_sep + input_string).replace(\n                \"  \", \" \"\n            )\n            return out\n\n        rag_input_strings = [\n            cat_input_and_doc(\n                docs[i][\"title\"][j],\n                docs[i][\"text\"][j],\n                input_strings[i],\n                prefix,\n            )\n            for i in range(len(docs))\n            for j in range(n_docs)\n        ]\n\n        contextualized_inputs = self.generator_tokenizer.batch_encode_plus(\n            rag_input_strings,\n            max_length=self.config.max_combined_length,\n            return_tensors=return_tensors,\n            padding=\"max_length\",\n            truncation=True,\n        )\n\n        return contextualized_inputs[\"input_ids\"], contextualized_inputs[\"attention_mask\"]\n\n    def _chunk_tensor(self, t: Iterable, chunk_size: int) -> List[Iterable]:\n        return [t[i : i + chunk_size] for i in range(0, len(t), chunk_size)]\n\n    def _main_retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, np.ndarray]:\n        question_hidden_states_batched = self._chunk_tensor(question_hidden_states, self.batch_size)\n        ids_batched = []\n        vectors_batched = []\n        for question_hidden_states in question_hidden_states_batched:\n            start_time = time.time()\n            ids, vectors = self.index.get_top_docs(question_hidden_states, n_docs)\n            logger.debug(\n                f\"index search time: {time.time() - start_time} sec, batch size {question_hidden_states.shape}\"\n            )\n            ids_batched.extend(ids)\n            vectors_batched.extend(vectors)\n        return (\n            np.array(ids_batched),\n            np.array(vectors_batched),\n        )  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)\n\n    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:\n        \"\"\"\n        Retrieves documents for specified `question_hidden_states`.\n\n        Args:\n            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`):\n                A batch of query vectors to retrieve with.\n            n_docs (`int`):\n                The number of docs retrieved per query.\n\n        Return:\n            `Tuple[np.ndarray, np.ndarray, List[dict]]`: A tuple with the following objects:\n\n            - **retrieved_doc_embeds** (`np.ndarray` of shape `(batch_size, n_docs, dim)`) -- The retrieval embeddings\n              of the retrieved docs per query.\n            - **doc_ids** (`np.ndarray` of shape `(batch_size, n_docs)`) -- The ids of the documents in the index\n            - **doc_dicts** (`List[dict]`): The `retrieved_doc_embeds` examples per query.\n        \"\"\"\n\n        doc_ids, retrieved_doc_embeds = self._main_retrieve(question_hidden_states, n_docs)\n        return retrieved_doc_embeds, doc_ids, self.index.get_doc_dicts(doc_ids)\n\n    def set_ctx_encoder_tokenizer(self, ctx_encoder_tokenizer: PreTrainedTokenizer):\n        # used in end2end retriever training\n        self.ctx_encoder_tokenizer = ctx_encoder_tokenizer\n        self.return_tokenized_docs = True\n\n    def __call__(\n        self,\n        question_input_ids: List[List[int]],\n        question_hidden_states: np.ndarray,\n        prefix=None,\n        n_docs=None,\n        return_tensors=None,\n    ) -> BatchEncoding:\n        \"\"\"\n        Retrieves documents for specified `question_hidden_states`.\n\n        Args:\n            question_input_ids (`List[List[int]]`) batch of input ids\n            question_hidden_states (`np.ndarray` of shape `(batch_size, vector_size)`:\n                A batch of query vectors to retrieve with.\n            prefix (`str`, *optional*):\n                The prefix used by the generator's tokenizer.\n            n_docs (`int`, *optional*):\n                The number of docs retrieved per query.\n            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to \"pt\"):\n                If set, will return tensors instead of list of python integers. Acceptable values are:\n\n                - `'tf'`: Return TensorFlow `tf.constant` objects.\n                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                - `'np'`: Return Numpy `np.ndarray` objects.\n\n        Returns: [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n\n            - **context_input_ids** -- List of token ids to be fed to a model.\n\n              [What are input IDs?](../glossary#input-ids)\n\n            - **context_attention_mask** -- List of indices specifying which tokens should be attended to by the model\n            (when `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n\n              [What are attention masks?](../glossary#attention-mask)\n\n            - **retrieved_doc_embeds** -- List of embeddings of the retrieved documents\n            - **doc_ids** -- List of ids of the retrieved documents\n        \"\"\"\n\n        n_docs = n_docs if n_docs is not None else self.n_docs\n        prefix = prefix if prefix is not None else self.config.generator.prefix\n        retrieved_doc_embeds, doc_ids, docs = self.retrieve(question_hidden_states, n_docs)\n\n        input_strings = self.question_encoder_tokenizer.batch_decode(question_input_ids, skip_special_tokens=True)\n        context_input_ids, context_attention_mask = self.postprocess_docs(\n            docs, input_strings, prefix, n_docs, return_tensors=return_tensors\n        )\n\n        if self.return_tokenized_docs:\n            retrieved_doc_text = []\n            retrieved_doc_title = []\n\n            for b_idx in range(len(docs)):\n                for doc_idx in range(n_docs):\n                    retrieved_doc_text.append(docs[b_idx][\"text\"][doc_idx])\n                    retrieved_doc_title.append(docs[b_idx][\"title\"][doc_idx])\n\n            tokenized_docs = self.ctx_encoder_tokenizer(\n                retrieved_doc_title,\n                retrieved_doc_text,\n                truncation=True,\n                padding=\"longest\",\n                return_tensors=return_tensors,\n            )\n\n            return BatchEncoding(\n                {\n                    \"context_input_ids\": context_input_ids,\n                    \"context_attention_mask\": context_attention_mask,\n                    \"retrieved_doc_embeds\": retrieved_doc_embeds,\n                    \"doc_ids\": doc_ids,\n                    \"tokenized_doc_ids\": tokenized_docs[\"input_ids\"],\n                    \"tokenized_doc_attention_mask\": tokenized_docs[\"attention_mask\"],\n                },\n                tensor_type=return_tensors,\n            )\n\n        else:\n            return BatchEncoding(\n                {\n                    \"context_input_ids\": context_input_ids,\n                    \"context_attention_mask\": context_attention_mask,\n                    \"retrieved_doc_embeds\": retrieved_doc_embeds,\n                    \"doc_ids\": doc_ids,\n                },\n                tensor_type=return_tensors,\n            )\n", "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport shutil\nimport tempfile\nfrom unittest import TestCase\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom datasets import Dataset\n\nfrom transformers import is_faiss_available\nfrom transformers.models.bart.configuration_bart import BartConfig\nfrom transformers.models.bart.tokenization_bart import BartTokenizer\nfrom transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES as DPR_VOCAB_FILES_NAMES\nfrom transformers.models.dpr.configuration_dpr import DPRConfig\nfrom transformers.models.dpr.tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer\nfrom transformers.models.rag.configuration_rag import RagConfig\nfrom transformers.models.rag.retrieval_rag import CustomHFIndex, RagRetriever\nfrom transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES as BART_VOCAB_FILES_NAMES\nfrom transformers.testing_utils import require_faiss, require_sentencepiece, require_tokenizers, require_torch\n\n\nif is_faiss_available():\n    import faiss\n\n\n@require_faiss\nclass RagRetrieverTest(TestCase):\n    def setUp(self):\n        self.tmpdirname = tempfile.mkdtemp()\n        self.retrieval_vector_size = 8\n\n        # DPR tok\n        vocab_tokens = [\n            \"[UNK]\",\n            \"[CLS]\",\n            \"[SEP]\",\n            \"[PAD]\",\n            \"[MASK]\",\n            \"want\",\n            \"##want\",\n            \"##ed\",\n            \"wa\",\n            \"un\",\n            \"runn\",\n            \"##ing\",\n            \",\",\n            \"low\",\n            \"lowest\",\n        ]\n        dpr_tokenizer_path = os.path.join(self.tmpdirname, \"dpr_tokenizer\")\n        os.makedirs(dpr_tokenizer_path, exist_ok=True)\n        self.vocab_file = os.path.join(dpr_tokenizer_path, DPR_VOCAB_FILES_NAMES[\"vocab_file\"])\n        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n\n        # BART tok\n        vocab = [\n            \"l\",\n            \"o\",\n            \"w\",\n            \"e\",\n            \"r\",\n            \"s\",\n            \"t\",\n            \"i\",\n            \"d\",\n            \"n\",\n            \"\\u0120\",\n            \"\\u0120l\",\n            \"\\u0120n\",\n            \"\\u0120lo\",\n            \"\\u0120low\",\n            \"er\",\n            \"\\u0120lowest\",\n            \"\\u0120newer\",\n            \"\\u0120wider\",\n            \"<unk>\",\n        ]\n        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n        merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n\n        bart_tokenizer_path = os.path.join(self.tmpdirname, \"bart_tokenizer\")\n        os.makedirs(bart_tokenizer_path, exist_ok=True)\n        self.vocab_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"vocab_file\"])\n        self.merges_file = os.path.join(bart_tokenizer_path, BART_VOCAB_FILES_NAMES[\"merges_file\"])\n        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps(vocab_tokens) + \"\\n\")\n        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n            fp.write(\"\\n\".join(merges))\n\n    def get_dpr_tokenizer(self) -> DPRQuestionEncoderTokenizer:\n        return DPRQuestionEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n\n    def get_dpr_ctx_encoder_tokenizer(self) -> DPRContextEncoderTokenizer:\n        return DPRContextEncoderTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"dpr_tokenizer\"))\n\n    def get_bart_tokenizer(self) -> BartTokenizer:\n        return BartTokenizer.from_pretrained(os.path.join(self.tmpdirname, \"bart_tokenizer\"))\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpdirname)\n\n    def get_dummy_dataset(self):\n        dataset = Dataset.from_dict(\n            {\n                \"id\": [\"0\", \"1\"],\n                \"text\": [\"foo\", \"bar\"],\n                \"title\": [\"Foo\", \"Bar\"],\n                \"embeddings\": [np.ones(self.retrieval_vector_size), 2 * np.ones(self.retrieval_vector_size)],\n            }\n        )\n        dataset.add_faiss_index(\"embeddings\", string_factory=\"Flat\", metric_type=faiss.METRIC_INNER_PRODUCT)\n        return dataset\n\n    def get_dummy_canonical_hf_index_retriever(self):\n        dataset = self.get_dummy_dataset()\n        config = RagConfig(\n            retrieval_vector_size=self.retrieval_vector_size,\n            question_encoder=DPRConfig().to_dict(),\n            generator=BartConfig().to_dict(),\n        )\n        with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:\n            mock_load_dataset.return_value = dataset\n            retriever = RagRetriever(\n                config,\n                question_encoder_tokenizer=self.get_dpr_tokenizer(),\n                generator_tokenizer=self.get_bart_tokenizer(),\n            )\n        return retriever\n\n    def get_dummy_custom_hf_index_retriever(self, from_disk: bool):\n        dataset = self.get_dummy_dataset()\n        config = RagConfig(\n            retrieval_vector_size=self.retrieval_vector_size,\n            question_encoder=DPRConfig().to_dict(),\n            generator=BartConfig().to_dict(),\n            index_name=\"custom\",\n        )\n        if from_disk:\n            config.passages_path = os.path.join(self.tmpdirname, \"dataset\")\n            config.index_path = os.path.join(self.tmpdirname, \"index.faiss\")\n            dataset.get_index(\"embeddings\").save(os.path.join(self.tmpdirname, \"index.faiss\"))\n            dataset.drop_index(\"embeddings\")\n            dataset.save_to_disk(os.path.join(self.tmpdirname, \"dataset\"))\n            del dataset\n            retriever = RagRetriever(\n                config,\n                question_encoder_tokenizer=self.get_dpr_tokenizer(),\n                generator_tokenizer=self.get_bart_tokenizer(),\n            )\n        else:\n            retriever = RagRetriever(\n                config,\n                question_encoder_tokenizer=self.get_dpr_tokenizer(),\n                generator_tokenizer=self.get_bart_tokenizer(),\n                index=CustomHFIndex(config.retrieval_vector_size, dataset),\n            )\n        return retriever\n\n    def test_canonical_hf_index_retriever_retrieve(self):\n        n_docs = 1\n        retriever = self.get_dummy_canonical_hf_index_retriever()\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"embeddings\", \"id\", \"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"id\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"id\"][0], \"1\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"id\"][0], \"0\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_canonical_hf_index_retriever_save_and_from_pretrained(self):\n        retriever = self.get_dummy_canonical_hf_index_retriever()\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:\n                mock_load_dataset.return_value = self.get_dummy_dataset()\n                retriever.save_pretrained(tmp_dirname)\n                retriever = RagRetriever.from_pretrained(tmp_dirname)\n                self.assertIsInstance(retriever, RagRetriever)\n                hidden_states = np.array(\n                    [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n                )\n                out = retriever.retrieve(hidden_states, n_docs=1)\n                self.assertTrue(out is not None)\n\n    def test_custom_hf_index_retriever_retrieve(self):\n        n_docs = 1\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=False)\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"embeddings\", \"id\", \"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"id\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"id\"][0], \"1\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"id\"][0], \"0\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_custom_hf_index_retriever_save_and_from_pretrained(self):\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=False)\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            retriever.save_pretrained(tmp_dirname)\n            retriever = RagRetriever.from_pretrained(tmp_dirname)\n            self.assertIsInstance(retriever, RagRetriever)\n            hidden_states = np.array(\n                [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n            )\n            out = retriever.retrieve(hidden_states, n_docs=1)\n            self.assertTrue(out is not None)\n\n    def test_custom_hf_index_retriever_retrieve_from_disk(self):\n        n_docs = 1\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=True)\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertEqual(len(doc_dicts), 2)\n        self.assertEqual(sorted(doc_dicts[0]), [\"embeddings\", \"id\", \"text\", \"title\"])\n        self.assertEqual(len(doc_dicts[0][\"id\"]), n_docs)\n        self.assertEqual(doc_dicts[0][\"id\"][0], \"1\")  # max inner product is reached with second doc\n        self.assertEqual(doc_dicts[1][\"id\"][0], \"0\")  # max inner product is reached with first doc\n        self.assertListEqual(doc_ids.tolist(), [[1], [0]])\n\n    def test_custom_hf_index_retriever_save_and_from_pretrained_from_disk(self):\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=True)\n        with tempfile.TemporaryDirectory() as tmp_dirname:\n            retriever.save_pretrained(tmp_dirname)\n            retriever = RagRetriever.from_pretrained(tmp_dirname)\n            self.assertIsInstance(retriever, RagRetriever)\n            hidden_states = np.array(\n                [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n            )\n            out = retriever.retrieve(hidden_states, n_docs=1)\n            self.assertTrue(out is not None)\n\n    @require_torch\n    @require_tokenizers\n    @require_sentencepiece\n    def test_hf_index_retriever_call(self):\n        import torch\n\n        n_docs = 1\n        retriever = self.get_dummy_canonical_hf_index_retriever()\n        question_input_ids = [[5, 7], [10, 11]]\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        out = retriever(question_input_ids, hidden_states, prefix=retriever.config.generator.prefix, n_docs=n_docs)\n        context_input_ids, context_attention_mask, retrieved_doc_embeds = (\n            out[\"context_input_ids\"],\n            out[\"context_attention_mask\"],\n            out[\"retrieved_doc_embeds\"],\n        )\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertIsInstance(context_input_ids, list)\n        self.assertIsInstance(context_attention_mask, list)\n        self.assertIsInstance(retrieved_doc_embeds, np.ndarray)\n\n        out = retriever(\n            question_input_ids,\n            hidden_states,\n            prefix=retriever.config.generator.prefix,\n            n_docs=n_docs,\n            return_tensors=\"pt\",\n        )\n        context_input_ids, context_attention_mask, retrieved_doc_embeds, doc_ids = (  # noqa: F841\n            out[\"context_input_ids\"],\n            out[\"context_attention_mask\"],\n            out[\"retrieved_doc_embeds\"],\n            out[\"doc_ids\"],\n        )\n        self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))\n        self.assertIsInstance(context_input_ids, torch.Tensor)\n        self.assertIsInstance(context_attention_mask, torch.Tensor)\n        self.assertIsInstance(retrieved_doc_embeds, torch.Tensor)\n\n    @require_torch\n    @require_tokenizers\n    @require_sentencepiece\n    def test_custom_hf_index_end2end_retriever_call(self):\n        context_encoder_tokenizer = self.get_dpr_ctx_encoder_tokenizer()\n        n_docs = 1\n        retriever = self.get_dummy_custom_hf_index_retriever(from_disk=False)\n        retriever.set_ctx_encoder_tokenizer(context_encoder_tokenizer)\n\n        question_input_ids = [[5, 7], [10, 11]]\n        hidden_states = np.array(\n            [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32\n        )\n        out = retriever(question_input_ids, hidden_states, prefix=retriever.config.generator.prefix, n_docs=n_docs)\n\n        self.assertEqual(\n            len(out), 6\n        )  # check whether the retriever output consist of 6 attributes including tokenized docs\n        self.assertEqual(\n            all(k in out for k in (\"tokenized_doc_ids\", \"tokenized_doc_attention_mask\")), True\n        )  # check for doc token related keys in dictionary.\n"], "filenames": ["docs/source/en/model_doc/transfo-xl.md", "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py", "src/transformers/models/rag/retrieval_rag.py", "tests/models/rag/test_retrieval_rag.py"], "buggy_code_start_loc": [25, 36, 26, 17], "buggy_code_end_loc": [28, 792, 142, 318], "fixing_code_start_loc": [25, 37, 26, 16], "fixing_code_end_loc": [35, 809, 157, 258], "type": "CWE-502", "message": "Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36.", "other": {"cve": {"id": "CVE-2023-7018", "sourceIdentifier": "security@huntr.dev", "published": "2023-12-20T17:15:08.823", "lastModified": "2023-12-30T03:13:12.367", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36."}, {"lang": "es", "value": "Deserializaci\u00f3n de datos que no son de confianza en el repositorio de GitHub huggingface/transformers anteriores a 4.36."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.6, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 2.8, "impactScore": 6.0}]}, "weaknesses": [{"source": "security@huntr.dev", "type": "Primary", "description": [{"lang": "en", "value": "CWE-502"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:huggingface:transformers:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.36.0", "matchCriteriaId": "A7A810D1-9219-4534-83E2-F3FC5402E521"}]}]}], "references": [{"url": "https://github.com/huggingface/transformers/commit/1d63b0ec361e7a38f1339385e8a5a855085532ce", "source": "security@huntr.dev", "tags": ["Patch"]}, {"url": "https://huntr.com/bounties/e1a3e548-e53a-48df-b708-9ee62140963c", "source": "security@huntr.dev", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/huggingface/transformers/commit/1d63b0ec361e7a38f1339385e8a5a855085532ce"}}