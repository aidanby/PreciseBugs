{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Implements convolution operations with image transformations (resize and\n// mirror padding) baked into the processing, to optimize latency and memory\n// usage.\n\n#define EIGEN_USE_THREADS\n\n#include <string>\n#include <vector>\n\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/kernels/conv_ops.h\"\n#include \"tensorflow/core/kernels/gemm_functors.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/util/image_resizer_state.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\nnamespace {\n\n// We don't want to allocate a buffer to hold all the patches if the size is\n// going to be extremely large, so break it into chunks if it's bigger than\n// a limit. Each chunk will be processed serially, so we can refill the\n// buffer for the next chunk and reuse it, keeping maximum memory size down.\n// In this case, we've picked 16 megabytes as a reasonable limit for Android and\n// other platforms using Eigen, and 1MB for iOS devices, from experimentation.\n#if defined(__APPLE__) && defined(IS_MOBILE_PLATFORM)\nconst size_t kMaxChunkSize = (1 * 1024 * 1024);\n#else\nconst size_t kMaxChunkSize = (16 * 1024 * 1024);\n#endif\nconst size_t kResizeCacheSize = (8 * 1024 * 1024);\n\n// Lookup method used when resizing.\nenum SamplingMode {\n  BILINEAR = 0,\n  NEAREST = 1,\n};\n\n// Simple utility function used by FusedConv to multithread basic workloads. To\n// use it, pass begin and end values for the full workload and a std::function\n// that receives a subset of that through the begin and end values for each\n// worker's task. The division of the full workload into worker tasks is handled\n// by the multithreading logic. Here's an example of how to use it:\n// std::vector<float> my_vector(100);\n// ...\n// FusedConvParallelFor(context, 0, 100,\n//   [&my_vector](int64 task_begin, int64 task_end) {\n//     for (int64 current = task_begin; current != task_end; ++current) {\n//       my_vector[current] *= 10.0f;\n//     }\n// });\nvoid FusedConvParallelFor(\n    OpKernelContext* context, int64_t begin, int64_t end,\n    const std::function<void(int64_t, int64_t)>& task_function) {\n// On iOS, the thread management imposes a very big performance penalty, so\n// just call the function directly with no multithreading.\n#if defined(__APPLE__) && defined(IS_MOBILE_PLATFORM)\n  task_function(begin, end);\n#else\n  auto& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n  thread::ThreadPool* thread_pool = worker_threads.workers;\n  const int64_t total_elements = end - begin;\n  // This is a bit of an arbitrary number, but was found to work well for\n  // typical models we've been profiling on various devices.\n  const int64_t element_cost = 10000000;\n  thread_pool->ParallelFor(\n      total_elements, element_cost,\n      [begin, task_function](int64_t begin_offset, int64_t end_offset) {\n        const int64_t task_begin = begin + begin_offset;\n        const int64_t task_end = begin + end_offset;\n        task_function(task_begin, task_end);\n      });\n#endif\n}\n\n// Holds the state needed for the resizing subtasks.\ntemplate <class T1>\nstruct ResizeTaskParameters {\n  ResizeTaskParameters() : st(false, false) {}\n\n  int cache_height;\n  T1* resize_cache;\n  int cache_line_width;\n  int input_width;\n  int input_depth;\n  int top_padding;\n  int pad_offset;\n  int64_t resized_height;\n  ImageResizerState st;\n  const T1* input_batch_start;\n  int64_t cache_start_x;\n  int64_t cache_end_x;\n  int left_padding;\n  int64_t resized_width;\n  int64_t padded_width;\n  int64_t padded_height;\n};\n\ntemplate <class T1>\nstruct PerCacheLineParameters {\n  PerCacheLineParameters() {}\n  PerCacheLineParameters(const PerCacheLineParameters<T1>& other)\n      : cache_line_start(other.cache_line_start),\n        input_top_row_start(other.input_top_row_start),\n        input_bottom_row_start(other.input_bottom_row_start),\n        y_lerp(other.y_lerp) {}\n\n  T1* cache_line_start;\n  const T1* input_top_row_start;\n  const T1* input_bottom_row_start;\n  T1 y_lerp;\n};\n\n// Helper class to simplify bilinear filtering\ntemplate <class T1>\nstruct SampleRect {\n  EIGEN_ALWAYS_INLINE SampleRect(const T1* in_top_left, const T1* in_top_right,\n                                 const T1* in_bottom_left,\n                                 const T1* in_bottom_right)\n      : top_left(in_top_left),\n        top_right(in_top_right),\n        bottom_left(in_bottom_left),\n        bottom_right(in_bottom_right) {}\n\n  EIGEN_ALWAYS_INLINE T1 BilinearSample(int channel, T1 x_lerp,\n                                        T1 y_lerp) const {\n    const T1 top =\n        top_left[channel] + (top_right[channel] - top_left[channel]) * x_lerp;\n    const T1 bottom = bottom_left[channel] +\n                      (bottom_right[channel] - bottom_left[channel]) * x_lerp;\n    return top + (bottom - top) * y_lerp;\n  }\n\n  const T1* top_left;\n  const T1* top_right;\n  const T1* bottom_left;\n  const T1* bottom_right;\n};\n\n// Calculates parameters which remain constant through a resize cache row.\ntemplate <class T1>\nEIGEN_ALWAYS_INLINE PerCacheLineParameters<T1> CalculatePerCacheLineParameters(\n    int64_t cache_height, int64_t cache_y, T1* resize_cache,\n    int64_t cache_line_width, int64_t input_width, int64_t input_depth,\n    int64_t top_padding, int64_t pad_offset, int64_t resized_height,\n    const ImageResizerState& st, const T1* input_batch_start) {\n  PerCacheLineParameters<T1> result;\n  // The cache is organized so that the real y values of the resized image map\n  // onto the actual cache values through a modulo scheme. This means that as we\n  // progress downwards through the image, we keep reusing a small cache and so\n  // keep memory usage down.\n  int64_t cache_index_y;\n  if (cache_y < 0) {\n    cache_index_y = cache_height + (cache_y % cache_height);\n  } else {\n    cache_index_y = cache_y % cache_height;\n  }\n  result.cache_line_start =\n      resize_cache + (cache_index_y * cache_line_width * input_depth);\n  // This part is implementing the mirror padding that happens before resizing.\n  float in_y = (cache_y - top_padding);\n  if (in_y < 0) {\n    in_y = -(in_y + 1.0f - pad_offset);\n  } else if (in_y >= resized_height) {\n    in_y = (resized_height * 2.0f) - (in_y + 1.0f + pad_offset);\n  }\n  // Here's where to do the actual resize.\n  in_y *= st.height_scale;\n  const int64_t top_y_index = static_cast<int64_t>(std::floor(in_y));\n  const int64_t bottom_y_index =\n      std::min(static_cast<int64_t>(std::ceil(in_y)), (st.in_height - 1));\n  // Lerp is used for bilinear filtering when that's needed.\n  result.y_lerp = static_cast<T1>(in_y - top_y_index);\n  // Which rows of the original input image to pull the values from.\n  result.input_top_row_start =\n      input_batch_start + (top_y_index * input_width * input_depth);\n  result.input_bottom_row_start =\n      input_batch_start + (bottom_y_index * input_width * input_depth);\n  return result;\n}\n\ntemplate <class T1>\nstruct PerCachePixelParameters {\n  PerCachePixelParameters() {}\n  PerCachePixelParameters(const PerCachePixelParameters<T1>& other)\n      : cache_line_pixel(other.cache_line_pixel),\n        left_x_index(other.left_x_index),\n        right_x_index(other.right_x_index),\n        x_lerp(other.x_lerp) {}\n\n  T1* cache_line_pixel;\n  int64_t left_x_index;\n  int64_t right_x_index;\n  T1 x_lerp;\n};\n\n// Pulls out common parameters used for every resized pixel.\ntemplate <class T1>\nEIGEN_ALWAYS_INLINE PerCachePixelParameters<T1>\nCalculatePerCachePixelParameters(int64_t cache_x, int64_t cache_start_x,\n                                 T1* cache_line_start, int64_t input_depth,\n                                 int64_t left_padding, int64_t pad_offset,\n                                 int64_t resized_width,\n                                 const ImageResizerState& st) {\n  PerCachePixelParameters<T1> result;\n  // Figure out where we're going to store the results of our transform.\n  const int cache_index_x = cache_x - cache_start_x;\n  result.cache_line_pixel = cache_line_start + (cache_index_x * input_depth);\n  // Implement mirror padding by flipping in_x if it's off the edge.\n  float in_x = (cache_x - left_padding);\n  if (in_x < 0) {\n    in_x = -(in_x + 1.0f - pad_offset);\n  } else if (in_x >= resized_width) {\n    in_x = (resized_width * 2.0f) - (in_x + 1.0f + pad_offset);\n  }\n  // Resize the x parameters.\n  in_x *= st.width_scale;\n  // Get the x coordinates for the left and right pixels to pull from.\n  result.left_x_index = static_cast<int64_t>(std::floor(in_x));\n  result.right_x_index =\n      std::min(static_cast<int64_t>(std::ceil(in_x)), (st.in_width - 1));\n  // This x_lerp is used to blend pixels in bilinear filtering.\n  result.x_lerp = static_cast<T1>(in_x - result.left_x_index);\n  return result;\n}\n\n// Combines bilinear resizing and mirror padding into the im2col transformation\n// stage of convolution.\ntemplate <class T1, class T2, class T3, class TGemmFunctor,\n          SamplingMode SampleMode>\nclass FusedResizeAndPadConvFunctor {\n public:\n  void operator()(OpKernelContext* context, const Tensor& input,\n                  int input_batches, int resized_height, int resized_width,\n                  int padded_height, int padded_width, int input_depth,\n                  const T2* filter_data, int filter_height, int filter_width,\n                  int filter_count, int stride_rows, int stride_cols,\n                  Padding padding, T3* output_data, int output_height,\n                  int output_width, const ImageResizerState& st,\n                  int top_padding, int bottom_padding, int left_padding,\n                  int right_padding, int pad_offset) {\n    if ((input_batches <= 0) || (padded_width <= 0) || (padded_height <= 0) ||\n        (input_depth <= 0)) {\n      LOG(WARNING) << \"Conv2D was called with bad input dimensions: \"\n                   << input_batches << \", \" << padded_height << \", \"\n                   << padded_width << \", \" << input_depth;\n      return;\n    }\n    if ((filter_width <= 0) || (filter_height <= 0) || (filter_count <= 0)) {\n      LOG(WARNING) << \"Conv2D was called with bad filter dimensions: \"\n                   << filter_width << \", \" << filter_height << \", \"\n                   << filter_count;\n      return;\n    }\n    if ((output_width <= 0) || (output_height <= 0)) {\n      LOG(WARNING) << \"Conv2D was called with bad output width or height: \"\n                   << output_width << \", \" << output_height;\n      return;\n    }\n    OP_REQUIRES(\n        context, ((SampleMode == NEAREST) || (SampleMode == BILINEAR)),\n        errors::InvalidArgument(\"Bad sample mode passed in\", SampleMode));\n\n    // These calculations define how the patches will be positioned within the\n    // input image. The actual definitions are quite complex, and rely on the\n    // previously-calculated output size.\n    int filter_left_offset;\n    int filter_top_offset;\n    if (padding == VALID) {\n      filter_left_offset =\n          ((output_width - 1) * stride_cols + filter_width - padded_width + 1) /\n          2;\n      filter_top_offset = ((output_height - 1) * stride_rows + filter_height -\n                           padded_height + 1) /\n                          2;\n    } else {\n      filter_left_offset =\n          ((output_width - 1) * stride_cols + filter_width - padded_width) / 2;\n      filter_top_offset =\n          ((output_height - 1) * stride_rows + filter_height - padded_height) /\n          2;\n    }\n\n    ResizeTaskParameters<T1> task_params;\n    task_params.input_depth = input_depth;\n    task_params.top_padding = top_padding;\n    task_params.pad_offset = pad_offset;\n    task_params.resized_height = resized_height;\n    task_params.st = st;\n    task_params.left_padding = left_padding;\n    task_params.resized_width = resized_width;\n    task_params.padded_width = padded_width;\n    task_params.padded_height = padded_height;\n\n    // The im2col buffer has # of patches rows, and # of filters cols.\n    // It's laid out like this, in row major order in memory:\n    //        < filter value count >\n    //   ^   +---------------------+\n    // patch |                     |\n    // count |                     |\n    //   v   +---------------------+\n    // Each patch row contains a filter_width x filter_height patch of the\n    // input, with the depth channel as the most contiguous in memory, followed\n    // by the width, then the height. This is the standard memory order in the\n    // image world if it helps to visualize it.\n    const int filter_value_count = filter_width * filter_height * input_depth;\n\n    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= kMaxChunkSize,\n                errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n    const size_t patches_per_chunk =\n        kMaxChunkSize / (filter_value_count * sizeof(T1));\n    // Because memory allocation is very expensive on mobile platforms, try to\n    // allocate a persistent buffer that will be kept around between calls. We\n    // use TensorFlow's resource management to ensure that the memory will be\n    // released when the session is over.\n    Im2ColBufferResource<T1, kMaxChunkSize>* im2col_buffer_resource;\n    std::function<Status(Im2ColBufferResource<T1, kMaxChunkSize>**)> creator =\n        [](Im2ColBufferResource<T1, kMaxChunkSize>** resource) {\n          *resource = new Im2ColBufferResource<T1, kMaxChunkSize>();\n          return OkStatus();\n        };\n    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate(\n                                \"Conv2d\", \"im2col_buffer\",\n                                &im2col_buffer_resource, creator));\n\n    // Create a resize cache memory buffer that will hold the rows of\n    // transformed and mirror padded input pixels, ready to be copied\n    // into filter patches by im2col.\n    // It's laid out like this, in row major order in memory:\n    //         < cache line width >\n    //   ^    +--------------------+\n    // cache  |                    |\n    // height |                    |\n    //   v    +--------------------+\n    // Each cache row contains a cache_line_width number of resized pixels,\n    // each with input_depth channels. The cache height is typically less than\n    // the full height the resized image would be, so it's filled up\n    // incrementally as we progress downwards through the input creating im2col\n    // patches.\n    task_params.cache_start_x = -filter_left_offset;\n    task_params.cache_end_x =\n        (((output_width - 1) * stride_cols) - filter_left_offset) +\n        filter_width;\n    task_params.cache_line_width =\n        task_params.cache_end_x - task_params.cache_start_x;\n    task_params.cache_height =\n        kResizeCacheSize / (task_params.cache_line_width * input_depth);\n    const int needed_resize_cache_count =\n        filter_height * task_params.cache_line_width * input_depth;\n    OP_REQUIRES(context,\n                (needed_resize_cache_count * sizeof(T1)) <= kResizeCacheSize,\n                errors::InvalidArgument(\"Input too large for resize cache\"));\n    Im2ColBufferResource<T1, kResizeCacheSize>* resize_cache_resource;\n    std::function<Status(Im2ColBufferResource<T1, kResizeCacheSize>**)>\n        resize_creator =\n            [](Im2ColBufferResource<T1, kResizeCacheSize>** resource) {\n              *resource = new Im2ColBufferResource<T1, kResizeCacheSize>();\n              return OkStatus();\n            };\n    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate(\n                                \"Conv2d\", \"resize_cache\",\n                                &resize_cache_resource, resize_creator));\n\n    // This means that multiple ops can't be run simultaneously on different\n    // threads, because we have a single shared resource. The platforms this is\n    // aimed at have intra-op parallelism as their focus though, so it shouldn't\n    // be an issue.\n    mutex_lock lock_buffer(im2col_buffer_resource->mu);\n    core::ScopedUnref unref_buffer(im2col_buffer_resource);\n    T1* im2col_buffer = im2col_buffer_resource->data;\n\n    // This buffer is used as a fairly heavy-weight cache for the resized and\n    // mirrored inputs to the im2col operation. The problem is that we want to\n    // keep the memory usage down by not rendering the fully resized and padded\n    // input tensor to the convolution into an entire buffer. The first approach\n    // to avoid this was to fold the bilinear filtering and padding spatial\n    // transformations into the im2col lookup itself. This successfully reduced\n    // memory usage, but because im2col can access an individual pixel for many\n    // different patches, the extra overhead of doing the same bilinear lookups\n    // repeatedly became too expensive.\n    // The resize cache is designed to avoid this problem by keeping a\n    // horizontal slice of the resized and padded input to the im2col\n    // precalculated, so that repeated accesses to the same pixel from different\n    // filter patches can just be copied from this cache. It's organized as a\n    // horizontal slice stretching across the whole virtual image, and as high\n    // as the filter window, so that as the patch processing moves across all\n    // the pixels are present, and before a new row of patches is started any\n    // previously calculated rows that are needed are maintained, with new rows\n    // calculated as required.\n    mutex_lock resize_lock_buffer(resize_cache_resource->mu);\n    core::ScopedUnref unref_resized_cache(resize_cache_resource);\n    task_params.resize_cache = resize_cache_resource->data;\n\n    const T1* input_data = input.flat<T1>().data();\n    const int64_t input_height = input.shape().dim_sizes()[1];\n    task_params.input_width = input.shape().dim_sizes()[2];\n\n    int end_cached_lines = std::numeric_limits<int>::min();\n\n    for (int batch = 0; batch < input_batches; ++batch) {\n      task_params.input_batch_start =\n          input_data +\n          (batch * input_height * task_params.input_width * input_depth);\n      const int in_y_end =\n          ((output_height * stride_rows) - filter_top_offset) + filter_height;\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        const int in_y_origin = (out_y * stride_rows) - filter_top_offset;\n        const int cache_start_y = std::max(in_y_origin, end_cached_lines);\n        const int cache_end_y = std::min(\n            in_y_end, std::max((in_y_origin + task_params.cache_height),\n                               end_cached_lines));\n        if (end_cached_lines < (in_y_origin + filter_height)) {\n          // This call breaks up the work required for calculating the mirror\n          // padding and resizing across multiple threads.\n          FusedConvParallelFor(\n              context, cache_start_y, cache_end_y,\n              [task_params](int64_t task_cache_start_y,\n                            int64_t task_cache_end_y) {\n                // This is a long and confusing function, but it's been laid out\n                // this way to help with performance on some intensive models.\n                // What it's doing is populating a cache of the original input\n                // image, after it's been bilinear resized and had its edges\n                // mirrored. This allows the following im2col code to access the\n                // transformed pixels from this cache, without having to\n                // repeatedly apply the expensive bilinear calculations as the\n                // same pixels are accessed by different patches.\n                // This is most effective when the stride is small and the\n                // filter size is large, since that's when pixels are reused\n                // most frequently as patches overlap.\n                for (int cache_y = task_cache_start_y;\n                     cache_y < task_cache_end_y; ++cache_y) {\n                  // We organize the cache as a series of rows, each containing\n                  // all the transformed pixels for a given line in the image.\n                  // This cache is big enough to hold at least a filter's height\n                  // worth of rows, but typically more, limited by the size of\n                  // the cache buffer.\n                  // We don't allocate an entire image's worth of rows though,\n                  // because we're trying to keep memory usage down, so as we\n                  // progress downwards through the im2col we periodically\n                  // refresh the cache so that the next lines that are needed\n                  // for that operation are always present.\n                  // Work out the parameters that remain constant across the\n                  // row we're calculating.\n                  PerCacheLineParameters<T1> line_params(\n                      CalculatePerCacheLineParameters<T1>(\n                          task_params.cache_height, cache_y,\n                          task_params.resize_cache,\n                          task_params.cache_line_width, task_params.input_width,\n                          task_params.input_depth, task_params.top_padding,\n                          task_params.pad_offset, task_params.resized_height,\n                          task_params.st, task_params.input_batch_start));\n                  // Iterate through the resize cache row we're filling in.\n                  for (int cache_x = task_params.cache_start_x;\n                       cache_x < task_params.cache_end_x; ++cache_x) {\n                    // Figure out what we need for the cache pixel we're\n                    // populating.\n                    PerCachePixelParameters<T1> pixel_params(\n                        CalculatePerCachePixelParameters<T1>(\n                            cache_x, task_params.cache_start_x,\n                            line_params.cache_line_start,\n                            task_params.input_depth, task_params.left_padding,\n                            task_params.pad_offset, task_params.resized_width,\n                            task_params.st));\n                    // If the access is off the left, right, top, or bottom of\n                    // the resized image, the conv padding means we should set\n                    // it to zero.\n                    if ((cache_x < 0) ||\n                        (cache_x >= task_params.padded_width) ||\n                        (cache_y < 0) ||\n                        (cache_y >= task_params.padded_height)) {\n                      std::fill_n(pixel_params.cache_line_pixel,\n                                  task_params.input_depth, T1(0));\n                    } else {\n                      // There are two different sampling strategies for\n                      // resizing. When using nearest, we can just do a\n                      // straight copy of the pixel closest to our sample point,\n                      // but bilinear requires a more complex calculation.\n                      if (SampleMode == NEAREST) {\n                        const T1* input_top_left_pixel =\n                            line_params.input_top_row_start +\n                            (pixel_params.left_x_index *\n                             task_params.input_depth);\n\n                        std::copy_n(input_top_left_pixel,\n                                    task_params.input_depth,\n                                    pixel_params.cache_line_pixel);\n                      } else {\n                        const SampleRect<T1> rect(\n                            line_params.input_top_row_start +\n                                (pixel_params.left_x_index *\n                                 task_params.input_depth),\n                            line_params.input_top_row_start +\n                                (pixel_params.right_x_index *\n                                 task_params.input_depth),\n                            line_params.input_bottom_row_start +\n                                (pixel_params.left_x_index *\n                                 task_params.input_depth),\n                            line_params.input_bottom_row_start +\n                                (pixel_params.right_x_index *\n                                 task_params.input_depth));\n                        for (int in_channel = 0;\n                             in_channel < task_params.input_depth;\n                             ++in_channel) {\n                          pixel_params.cache_line_pixel[in_channel] =\n                              rect.BilinearSample(in_channel,\n                                                  pixel_params.x_lerp,\n                                                  line_params.y_lerp);\n                        }\n                      }\n                    }\n                  }\n                }\n              });\n          end_cached_lines = cache_end_y;\n        }\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin = (out_x * stride_cols) - filter_left_offset;\n          const int patch_index = (batch * output_width * output_height) +\n                                  (out_y * output_width) + out_x;\n          const int patch_index_within_chunk = patch_index % patches_per_chunk;\n          T1* im2col_patch_start =\n              im2col_buffer + (patch_index_within_chunk * filter_value_count);\n          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n            T1* im2col_row_start =\n                im2col_patch_start +\n                (filter_y * filter_width * task_params.input_depth);\n            const int conv_in_y = in_y_origin + filter_y;\n            int cache_index_y;\n            if (conv_in_y < 0) {\n              cache_index_y = task_params.cache_height +\n                              (conv_in_y % task_params.cache_height);\n            } else {\n              cache_index_y = conv_in_y % task_params.cache_height;\n            }\n            T1* cache_line_start =\n                task_params.resize_cache +\n                (cache_index_y * task_params.cache_line_width *\n                 task_params.input_depth);\n            T1* cache_filter_row_start =\n                cache_line_start + ((in_x_origin - task_params.cache_start_x) *\n                                    task_params.input_depth);\n            std::copy_n(cache_filter_row_start,\n                        (filter_width * task_params.input_depth),\n                        im2col_row_start);\n          }\n          const bool is_last_in_chunk =\n              (patch_index_within_chunk == (patches_per_chunk - 1));\n          const bool is_last_overall =\n              ((batch == (input_batches - 1)) &&\n               (out_y == (output_height - 1)) && (out_x == (output_width - 1)));\n          if (is_last_in_chunk || is_last_overall) {\n            // Now we've assembled a set of image patches into a matrix, apply\n            // a GEMM matrix multiply of the patches as rows, times the filter\n            // weights in columns, to get partial results in the output\n            // matrix.\n            const int how_many_patches = patch_index_within_chunk + 1;\n            const int m = how_many_patches;\n            const int n = filter_count;\n            const int k = filter_value_count;\n            const int lda = filter_value_count;\n            const int ldb = filter_count;\n            const int ldc = filter_count;\n            const size_t start_patch_index =\n                patch_index - (how_many_patches - 1);\n            T3* chunk_output_data =\n                output_data + (start_patch_index * filter_count);\n            TGemmFunctor gemm_functor;\n            gemm_functor(context, m, n, k, im2col_buffer, lda, filter_data, ldb,\n                         chunk_output_data, ldc);\n          }\n        }\n      }\n    }\n  }\n};\n\n}  // namespace\n\n// Implements a version of convolution with bilinear resizing and mirror padding\n// included.\ntemplate <class T, class TConvFunctor, bool DoResize>\nclass FusedResizeConv2DUsingGemmOp : public OpKernel {\n public:\n  explicit FusedResizeConv2DUsingGemmOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    if (DoResize) {\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"resize_align_corners\", &align_corners_));\n    }\n    MirrorPadMode mode;\n    OP_REQUIRES_OK(context, context->GetAttr(\"mode\", &mode));\n\n    switch (mode) {\n      case MirrorPadMode::SYMMETRIC: {\n        offset_ = 0;\n        break;\n      }\n      case MirrorPadMode::REFLECT: {\n        offset_ = 1;\n        break;\n      }\n      default:\n        OP_REQUIRES(context, false,\n                    errors::InvalidArgument(\n                        \"mode must be either REFLECT or SYMMETRIC.\"));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));\n    OP_REQUIRES(context, strides_.size() == 4,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\"));\n    const int64_t stride_n = GetTensorDim(strides_, FORMAT_NHWC, 'N');\n    const int64_t stride_c = GetTensorDim(strides_, FORMAT_NHWC, 'C');\n    OP_REQUIRES(\n        context, stride_n == 1 && stride_c == 1,\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_rows, in_cols, in_depth ]\n    const Tensor& input = context->input(0);\n    OP_REQUIRES(context, (input.shape().num_elements() > 0),\n                errors::InvalidArgument(\"Input tensor can't be empty\"));\n\n    ImageResizerState st(false, false);\n    if (DoResize) {\n      st = ImageResizerState(align_corners_, false);\n      st.ValidateAndCalculateOutputSize(context);\n      if (!context->status().ok()) return;\n    } else {\n      // Set up the resize parameters to do no scaling at all.\n      st.batch_size = input.dim_size(0);\n      st.out_height = input.dim_size(1);\n      st.out_width = input.dim_size(2);\n      st.in_height = input.dim_size(1);\n      st.in_width = input.dim_size(2);\n      st.channels = input.dim_size(3);\n      st.height_scale = 1.0f;\n      st.width_scale = 1.0f;\n    }\n    TensorShape resized_shape(\n        {input.dim_size(0), st.out_height, st.out_width, input.dim_size(3)});\n    int paddings_index;\n    int filter_index;\n    if (DoResize) {\n      paddings_index = 2;\n      filter_index = 3;\n    } else {\n      paddings_index = 1;\n      filter_index = 2;\n    }\n    const Tensor& paddings = context->input(paddings_index);\n\n    const int dims = resized_shape.dims();\n    OP_REQUIRES(\n        context,\n        TensorShapeUtils::IsMatrix(paddings.shape()) &&\n            paddings.dim_size(1) == 2,\n        errors::InvalidArgument(\"paddings must be a matrix with 2 columns: \",\n                                paddings.shape().DebugString()));\n    OP_REQUIRES(\n        context, dims == paddings.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs: \",\n            dims, \" \", paddings.shape().DebugString(), \" \",\n            resized_shape.DebugString()));\n    OP_REQUIRES(\n        context, dims == paddings.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs: \",\n            dims, \" \", paddings.shape().DebugString(), \" \",\n            resized_shape.DebugString()));\n\n    OP_REQUIRES(\n        context, dims == 4,\n        errors::InvalidArgument(\n            \"Fused mirror padding only supports four-dimensional inputs, but \",\n            dims, \" requested\"));\n\n    // Compute the shape of the output tensor, and allocate it.\n    TensorShape padded_shape;\n    TTypes<int32>::ConstMatrix paddings_matrix = paddings.matrix<int32>();\n    for (int d = 0; d < dims; ++d) {\n      const int32_t before =\n          paddings_matrix(d, 0);  // Pad before existing elements.\n      const int32_t after =\n          paddings_matrix(d, 1);  // Pad after existing elements.\n      OP_REQUIRES(context, before >= 0 && after >= 0,\n                  errors::InvalidArgument(\n                      \"paddings must be non-negative: \", before, \" \", after));\n      if (offset_ == 0) {  // SYMMETRIC mode.\n        OP_REQUIRES(\n            context,\n            before <= resized_shape.dim_size(d) &&\n                after <= resized_shape.dim_size(d),\n            errors::InvalidArgument(\"paddings must be no greater \"\n                                    \"than the dimension size: \",\n                                    before, \", \", after, \" greater than \",\n                                    resized_shape.dim_size(d)));\n      } else if (offset_ == 1) {  // REFLECT mode.\n        OP_REQUIRES(\n            context,\n            before < resized_shape.dim_size(d) &&\n                after < resized_shape.dim_size(d),\n            errors::InvalidArgument(\"paddings must be less than\"\n                                    \" the dimension size: \",\n                                    before, \", \", after, \" not less than \",\n                                    resized_shape.dim_size(d)));\n      }\n      padded_shape.AddDim(before + resized_shape.dim_size(d) + after);\n    }\n\n    OP_REQUIRES(\n        context, ((paddings_matrix(0, 0) == 0) && (paddings_matrix(0, 1) == 0)),\n        errors::InvalidArgument(\n            \"Fused mirror padding only support spatial padding, not batches: \",\n            paddings.DebugString()));\n    OP_REQUIRES(\n        context, ((paddings_matrix(3, 0) == 0) && (paddings_matrix(3, 1) == 0)),\n        errors::InvalidArgument(\n            \"Fused mirror padding only support spatial padding, not channels: \",\n            paddings.DebugString()));\n    const int32_t top_padding = paddings_matrix(1, 0);\n    const int32_t bottom_padding = paddings_matrix(1, 1);\n    const int32_t left_padding = paddings_matrix(2, 0);\n    const int32_t right_padding = paddings_matrix(2, 1);\n\n    // Input filter is of the following dimensions:\n    // [ filter_rows, filter_cols, in_depth, out_depth]\n    const Tensor& filter = context->input(filter_index);\n\n    // For 2D convolution, there should be 4 dimensions.\n    OP_REQUIRES(context, padded_shape.dims() == 4,\n                errors::InvalidArgument(\"input must be 4-dimensional\",\n                                        padded_shape.DebugString()));\n    OP_REQUIRES(context, filter.dims() == 4,\n                errors::InvalidArgument(\"filter must be 4-dimensional: \",\n                                        filter.shape().DebugString()));\n\n    // We only check the first three dims, since the depth is accessed as an\n    // int64 below.\n    for (int i = 0; i < 3; i++) {\n      OP_REQUIRES(\n          context,\n          FastBoundsCheck(filter.dim_size(i), std::numeric_limits<int>::max()),\n          errors::InvalidArgument(\"filter too large\"));\n    }\n\n    // The last dimension for input is in_depth. It must be the same as the\n    // filter's in_depth.\n    const int64_t in_depth = padded_shape.dim_size(3);\n    OP_REQUIRES(context, in_depth == filter.dim_size(2),\n                errors::InvalidArgument(\n                    \"input and filter must have the same depth: \", in_depth,\n                    \" vs \", filter.dim_size(2)));\n\n    // The last dimension for filter is out_depth.\n    const int out_depth = static_cast<int>(filter.dim_size(3));\n\n    // The second dimension for input is rows/height.\n    // The first dimension for filter is rows/height.\n    const int64_t padded_rows_raw = padded_shape.dim_size(1);\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(padded_rows_raw, std::numeric_limits<int>::max()),\n        errors::InvalidArgument(\"Input rows too large\"));\n    const int padded_rows = static_cast<int>(padded_rows_raw);\n    const int filter_rows = static_cast<int>(filter.dim_size(0));\n    const int resized_rows = static_cast<int>(resized_shape.dim_size(1));\n\n    // The third dimension for input is columns/width.\n    // The second dimension for filter is columns/width.\n    const int64_t padded_cols_raw = padded_shape.dim_size(2);\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(padded_cols_raw, std::numeric_limits<int>::max()),\n        errors::InvalidArgument(\"Input cols too large\"));\n    const int padded_cols = static_cast<int>(padded_cols_raw);\n    const int filter_cols = static_cast<int>(filter.dim_size(1));\n    const int resized_cols = static_cast<int>(resized_shape.dim_size(2));\n\n    // The first dimension for input is batch.\n    const int64_t batch_raw = padded_shape.dim_size(0);\n    OP_REQUIRES(context,\n                FastBoundsCheck(batch_raw, std::numeric_limits<int>::max()),\n                errors::InvalidArgument(\"batch is too large\"));\n    const int batch = static_cast<int>(batch_raw);\n\n    // For now we take the stride from the second and third dimensions only (we\n    // do not support striding on the batch or depth dimension).\n    const int stride_rows = GetTensorDim(strides_, FORMAT_NHWC, 'H');\n    const int stride_cols = GetTensorDim(strides_, FORMAT_NHWC, 'W');\n\n    int64_t out_rows = 0, out_cols = 0, pad_rows = 0, pad_cols = 0;\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(padded_rows, filter_rows, stride_rows,\n                                         padding_, &out_rows, &pad_rows));\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(padded_cols, filter_cols, stride_cols,\n                                         padding_, &out_cols, &pad_cols));\n    TensorShape out_shape =\n        ShapeFromFormat(FORMAT_NHWC, batch, out_rows, out_cols, out_depth);\n    OP_REQUIRES(context, (out_shape.num_elements() > 0),\n                errors::InvalidArgument(\"Output tensor can't be empty\"));\n\n    // Output tensor is of the following dimensions:\n    // [ in_batch, out_rows, out_cols, out_depth ]\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    VLOG(2) << \"FusedConv2D: \" << name() << \", in_depth = \" << in_depth\n            << \", padded_cols = \" << padded_cols\n            << \", resized_cols = \" << resized_cols\n            << \", filter_cols = \" << filter_cols\n            << \", padded_rows = \" << padded_rows\n            << \", resized_rows = \" << resized_rows\n            << \", filter_rows = \" << filter_rows\n            << \", stride_rows = \" << stride_rows\n            << \", stride_cols = \" << stride_cols\n            << \", out_depth = \" << out_depth << \", DoResize=\" << DoResize;\n\n    // If there is nothing to compute, return.\n    if (out_shape.num_elements() == 0) {\n      return;\n    }\n    TConvFunctor conv_functor;\n    conv_functor(context, input, batch, resized_rows, resized_cols, padded_rows,\n                 padded_cols, in_depth, filter.flat<T>().data(), filter_rows,\n                 filter_cols, out_depth, stride_rows, stride_cols, padding_,\n                 output->flat<T>().data(), out_rows, out_cols, st, top_padding,\n                 bottom_padding, left_padding, right_padding, offset_);\n  }\n\n private:\n  std::vector<int32> strides_;\n  Padding padding_;\n  bool align_corners_;\n  int offset_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(FusedResizeConv2DUsingGemmOp);\n};\n\n#define REGISTER_FUSED(T)                                                 \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"FusedResizeAndPadConv2D\")                                     \\\n          .Device(DEVICE_CPU)                                             \\\n          .TypeConstraint<T>(\"T\"),                                        \\\n      FusedResizeConv2DUsingGemmOp<                                       \\\n          T,                                                              \\\n          FusedResizeAndPadConvFunctor<T, T, T, FastGemmFunctor<T, T, T>, \\\n                                       BILINEAR>,                         \\\n          true>);\n\nTF_CALL_half(REGISTER_FUSED);\nTF_CALL_float(REGISTER_FUSED);\nTF_CALL_double(REGISTER_FUSED);\n\n#define REGISTER_PAD_ONLY_FUSED(T)                                        \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"FusedPadConv2D\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"),   \\\n      FusedResizeConv2DUsingGemmOp<                                       \\\n          T,                                                              \\\n          FusedResizeAndPadConvFunctor<T, T, T, FastGemmFunctor<T, T, T>, \\\n                                       NEAREST>,                          \\\n          false>);\n\nTF_CALL_half(REGISTER_PAD_ONLY_FUSED);\nTF_CALL_float(REGISTER_PAD_ONLY_FUSED);\nTF_CALL_double(REGISTER_PAD_ONLY_FUSED);\n\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <cmath>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/lib/math/math_util.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nnamespace {\n\nStatus FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolShape);\n\nREGISTER_OP(\"AvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchNormWithGlobalNormalization\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"beta: T\")\n    .Input(\"gamma: T\")\n    .Output(\"result: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BatchNormWithGlobalNormalizationGrad\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"gamma: T\")\n    .Input(\"backprop: T\")\n    .Output(\"dx: T\")\n    .Output(\"dm: T\")\n    .Output(\"dv: T\")\n    .Output(\"db: T\")\n    .Output(\"dg: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(input, c->input(4), &input));  // with backprop\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 4; ++i) {  // covers m, v, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle dx;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &dx));\n      c->set_output(0, dx);\n\n      ShapeHandle vector_shape = c->Vector(last_dim);\n      c->set_output(1, vector_shape);\n      c->set_output(2, vector_shape);\n      c->set_output(3, vector_shape);\n      c->set_output(4, vector_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV3\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {bfloat16, float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormV3Shape);\n\nREGISTER_OP(\"_FusedBatchNormEx\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"side_input: num_side_inputs * T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, float, bfloat16}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNorm operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n\nREGISTER_OP(\"FusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV3\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_FusedBatchNormGradEx\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Input(\"offset: float\")\n    .Input(\"y: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Output(\"side_input_backprop: num_side_inputs * T\")\n    .Attr(\"T: {half, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNormGrad operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAdd\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddGrad\")\n    .Attr(\"T: numbertype\")\n    .Input(\"out_backprop: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddGradShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddV1\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding);\n\nREGISTER_OP(\"Conv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropInputShape);\n\n// TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n// more general string attribute ('kernel_impl'?) that can be used to\n// select among several possible implementations.\nREGISTER_OP(\"Conv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: TArgs\")\n    .Input(\"host_args : num_host_args * float\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, int8, qint8}\")\n    .Attr(\"TArgs: list(type)\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"num_host_args: int >= 0 =0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: { 'NHWC', 'NCHW', 'NCHW_VECT_C' } = 'NHWC'\")\n    .Attr(\"filter_format: {'HWIO', 'OIHW', 'OIHW_VECT_I'} = 'HWIO'\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nPerforms a convolution followed by a specified series of operations.\n\nThe inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.\n)doc\");\n\nnamespace {\n\nStatus CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  ShapeHandle resized = input;\n  int paddings_index = 1;\n  int filter_index = 2;\n  if (has_resize) {\n    paddings_index = 2;\n    filter_index = 3;\n\n    ShapeHandle unused_size;\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->Vector(2), &unused_size));\n\n    const Tensor* size = c->input_tensor(1);\n    DimensionHandle new_height = c->UnknownDim();\n    DimensionHandle new_width = c->UnknownDim();\n    if (size != nullptr) {\n      new_height = c->MakeDim(size->flat<int32>()(0));\n      new_width = c->MakeDim(size->flat<int32>()(1));\n    }\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n  }\n\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(paddings_index), 2, &paddings));\n  TF_RETURN_IF_ERROR(\n      c->WithRank(resized, c->Value(c->Dim(paddings, 0)), &resized));\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings, c->Matrix(c->Rank(resized), 2), &paddings));\n\n  const Tensor* paddings_t = c->input_tensor(paddings_index);\n  ShapeHandle padded;\n  if (paddings_t != nullptr) {\n    std::vector<DimensionHandle> output_dims;\n    for (int i = 0; i < 4; ++i) {\n      DimensionHandle dim = c->Dim(resized, i);\n      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n      if (p0 < 0 || p1 < 0) {\n        return errors::InvalidArgument(\"Paddings must be non-negative\");\n      }\n\n      TF_RETURN_IF_ERROR(c->Add(dim, p0 + p1, &dim));\n      output_dims.push_back(dim);\n    }\n    padded = c->MakeShape(output_dims);\n  } else {\n    padded = c->UnknownShapeOfRank(4);\n  }\n\n  // Work out the convolution's effect with 'padded' as the input.\n  ShapeHandle filter;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n  std::vector<int32> strides;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n  if (strides.size() != 4) {\n    return errors::InvalidArgument(\n        \"Operation requires the stride attribute to contain 4 values, but \",\n        \"got: \", strides.size());\n  }\n\n  int32_t stride_rows = strides[1];\n  int32_t stride_cols = strides[2];\n\n  DimensionHandle batch_size_dim = c->Dim(padded, 0);\n  DimensionHandle in_rows_dim = c->Dim(padded, 1);\n  DimensionHandle in_cols_dim = c->Dim(padded, 2);\n  DimensionHandle filter_rows_dim = c->Dim(filter, 0);\n  DimensionHandle filter_cols_dim = c->Dim(filter, 1);\n  DimensionHandle output_depth_dim = c->Dim(filter, 3);\n\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->Dim(padded, 3), c->Dim(filter, 2), &unused));\n\n  Padding padding;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n  DimensionHandle output_rows, output_cols;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_rows_dim, filter_rows_dim, stride_rows, padding, &output_rows));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_cols_dim, filter_cols_dim, stride_cols, padding, &output_cols));\n\n  ShapeHandle output_shape = c->MakeShape(\n      {batch_size_dim, output_rows, output_cols, output_depth_dim});\n  c->set_output(0, output_shape);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"DataFormatDimMap\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"DataFormatVecPermute\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FusedResizeAndPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"size: int32\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"resize_align_corners: bool = false\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, true /* has_resize */);\n    });\n\nREGISTER_OP(\"FusedPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, false /* has_resize */);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"DepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape);\n\nREGISTER_OP(\"Conv3DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropInputV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 5);\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropFilterV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 5, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"AvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"MaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape);\n\nREGISTER_OP(\"MaxPool3DGradGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5 \")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"L2Loss\")\n    .Input(\"t: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LRN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"LRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::MaxPoolShapeWithExplicitPadding);\n\nREGISTER_OP(\"MaxPoolV2\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 3));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\nREGISTER_OP(\"MaxPoolGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\n// TODO(b/150813181): Implement explicit padding.\nREGISTER_OP(\"MaxPoolGradGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 5));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"Targmax: {int32, int64} = DT_INT64\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Output(\"argmax: Targmax\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      c->set_output(1, c->output(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"MaxPoolGradGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &unused));\n      // Validate 'argmax' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Dilation2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n      ShapeHandle filter_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the stride attribute to contain 4 values, but \"\n            \"got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the rates attribute to contain 4 values, but \"\n            \"got: \",\n            rates.size());\n      }\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle filter_rows_dim = c->Dim(filter_shape, 0);\n      DimensionHandle filter_cols_dim = c->Dim(filter_shape, 1);\n      DimensionHandle output_depth_dim = c->Dim(filter_shape, 2);\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim) ||\n          !c->ValueKnown(filter_rows_dim) || !c->ValueKnown(filter_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(input_shape, 3), output_depth_dim, &unused));\n\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n      auto filter_rows = c->Value(filter_rows_dim);\n      auto filter_cols = c->Value(filter_cols_dim);\n      auto filter_rows_eff = filter_rows + (filter_rows - 1) * (rate_rows - 1);\n      auto filter_cols_eff = filter_cols + (filter_cols - 1) * (rate_cols - 1);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, filter_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, filter_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Dilation2DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"in_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Dilation2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"filter_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Relu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {realnumbertype, qint8}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Relu6\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Relu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"LeakyRelu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"LeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Elu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"EluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Selu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SeluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softplus\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftplusGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softsign\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftsignGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Softmax\")\n    .Input(\"logits: T\")\n    .Output(\"softmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogSoftmax\")\n    .Input(\"logits: T\")\n    .Output(\"logsoftmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"SoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: T\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      if (c->WithRank(c->input(0), 2, &input) == OkStatus() &&\n          c->Merge(input, c->input(1), &input) == OkStatus()) {\n        DimensionHandle batch_size = c->Dim(input, 0);\n        c->set_output(0, c->Vector(batch_size));\n        c->set_output(1, input);\n        return OkStatus();\n      }\n      TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFn(c, 1));\n\n      if (!c->RankKnown(c->output(1))) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank is unknown.\");\n      }\n\n      if (c->Rank(c->output(1)) != 2) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank \",\n            c->Rank(c->output(1)));\n      }\n      DimensionHandle batch_size = c->Dim(c->output(1), 0);\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SparseSoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: Tlabels\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"Tlabels: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle features;\n      ShapeHandle labels;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &features));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &labels));\n\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(features, 0), c->Dim(labels, 0), &batch_size));\n      TF_RETURN_IF_ERROR(c->ReplaceDim(features, 0, batch_size, &features));\n\n      c->set_output(0, c->Vector(batch_size));\n      c->set_output(1, features);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"InTopK\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"k: int\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\n// This is the same as `InTopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"InTopKV2\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Input(\"k: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus TopKShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n  // Get the k value, either from input tensor or attribute.\n  DimensionHandle k_dim;\n  if (c->num_inputs() >= 2) {\n    TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &k_dim));\n  } else {\n    int32_t k;\n    TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n    if (k < 0) {\n      return errors::InvalidArgument(\"Need k >= 0, got \", k);\n    }\n    k_dim = c->MakeDim(k);\n  }\n\n  DimensionHandle last_dim = c->Dim(input, -1);\n  if (c->ValueKnown(last_dim) && c->ValueKnown(k_dim) &&\n      c->Value(last_dim) < c->Value(k_dim)) {\n    return errors::InvalidArgument(\n        \"input must have last dimension >= k = \", c->Value(k_dim), \" but is \",\n        c->Value(last_dim));\n  }\n\n  // Replace last_dim with k_dim.\n  ShapeHandle s;\n  TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n  TF_RETURN_IF_ERROR(c->Concatenate(s, c->Vector(k_dim), &s));\n  c->set_output(0, s);\n  c->set_output(1, s);\n  return OkStatus();\n}\n\n// Utility functions for ApproxTopKShape.\n// It is not easy to link xla/client/lib into the tensorflow core lib, so we\n// have to replicate the logic.\n// LINT.IfChange\ninline uint32_t log2_floor(uint64_t value) {\n  return value == 0 ? 0 : Log2Floor(value);\n}\n\ninline uint32_t log2_ceil(uint64_t value) {\n  return value == 0 ? 0 : Log2Ceiling(value);\n}\n\nStatus ApproxTopKShape(shape_inference::InferenceContext* c) {\n  int64_t k;\n  int64_t reduction_dimension;\n  float recall_target;\n  int64_t reduction_input_size_override;\n  bool aggregate_to_topk;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_dimension\", &reduction_dimension));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"recall_target\", &recall_target));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_input_size_override\",\n                                &reduction_input_size_override));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"aggregate_to_topk\", &aggregate_to_topk));\n  ShapeHandle input_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input_shape));\n  if (reduction_dimension < 0) {\n    // Reverse index\n    reduction_dimension += c->Rank(input_shape);\n  }\n  int64_t reduction_dim_value =\n      c->Value(c->Dim(input_shape, reduction_dimension));\n\n  if (reduction_dim_value < k) {\n    return errors::InvalidArgument(\"input must have last dimension >= k = \", k,\n                                   \" but was \", reduction_dim_value);\n  }\n\n  int64_t output_dim_value = [&] {\n    if (aggregate_to_topk) {\n      return k;\n    }\n    int64_t tpu_tiling = c->Rank(input_shape) == 1 ? 1024 : 128;\n    if (reduction_dim_value <= tpu_tiling || recall_target == 1.0) {\n      return reduction_dim_value;\n    }\n    if (k == 1) {\n      return tpu_tiling;\n    }\n    uint64_t logical_input_size = reduction_input_size_override >= 0\n                                      ? reduction_input_size_override\n                                      : reduction_dim_value;\n    uint64_t m = std::min<uint64_t>(\n        std::max<uint64_t>(\n            static_cast<uint64_t>((1.0 - k) /\n                                  std::log(static_cast<double>(recall_target))),\n            tpu_tiling),\n        reduction_dim_value);\n    uint32_t log2_reduction = log2_floor(logical_input_size / m);\n    if (log2_reduction == 0) {\n      return reduction_dim_value;\n    }\n    log2_reduction = std::min<uint32_t>(\n        log2_reduction, log2_ceil(reduction_dim_value / tpu_tiling));\n    return tensorflow::MathUtil::CeilOfRatio<int64_t>(\n               tensorflow::MathUtil::CeilOfRatio<int64_t>(reduction_dim_value,\n                                                          tpu_tiling),\n               (1 << log2_reduction)) *\n           tpu_tiling;\n  }();\n\n  auto output_dim = c->MakeDim(output_dim_value);\n\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->ReplaceDim(input_shape, reduction_dimension, output_dim,\n                                   &output_shape));\n  c->set_output(0, output_shape);\n  c->set_output(1, output_shape);\n  return OkStatus();\n}\n// LINT.ThenChange(//tensorflow/compiler/xla/client/lib/approx_topk_shape.cc)\n\n}  // namespace\n\nREGISTER_OP(\"TopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .Deprecated(7, \"Use TopKV2 instead\")\n    .SetShapeFn(TopKShapeFn);\n\n// This is the same as `TopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"TopKV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(TopKShapeFn);\n\nREGISTER_OP(\"ApproxTopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"reduction_dimension: int = -1\")\n    .Attr(\"recall_target: float = 0.95\")\n    .Attr(\"is_max_k: bool = true\")\n    .Attr(\"reduction_input_size_override: int = -1\")\n    .Attr(\"aggregate_to_topk: bool = true\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(ApproxTopKShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"NthElement\")\n    .Input(\"input: T\")\n    .Input(\"n: int32\")\n    .Output(\"values: T\")\n    .Attr(\"reverse: bool = false\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n      // Get the n value from input tensor, and make sure which is a scalar.\n      DimensionHandle n_dim;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &n_dim));\n\n      // The last dimension of input tensor must be greater than N.\n      DimensionHandle last_dim = c->Dim(input, -1);\n      if (c->ValueKnown(last_dim) && c->ValueKnown(n_dim) &&\n          c->Value(last_dim) <= c->Value(n_dim)) {\n        return errors::InvalidArgument(\n            \"Input must have last dimension > n = \", c->Value(n_dim),\n            \" but is \", c->Value(last_dim));\n      }\n\n      // Reduce last_dim for output tensor\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalMaxPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalMaxPoolGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRank(c, 4);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalAvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalAvgPoolGrad\")\n    .Input(\"orig_input_tensor_shape: int64\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      if (c->input_tensor(0) != nullptr) {\n        ShapeHandle out;\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n        c->set_output(0, out);\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(4));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedAvgPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::QuantizedAvgPoolShape);\n\nREGISTER_OP(\"QuantizedBiasAdd\")\n    .Input(\"input: T1\")\n    .Input(\"bias: T2\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_bias: float\")\n    .Input(\"max_bias: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BiasAddShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::QuantizedConv2DShape);\n\nREGISTER_OP(\"QuantizedMaxPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu6\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReluX\")\n    .Input(\"features: Tinput\")\n    .Input(\"max_value: float\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedBatchNormWithGlobalNormalization\")\n    .Input(\"t: Tinput\")\n    .Input(\"t_min: float\")\n    .Input(\"t_max: float\")\n    .Input(\"m: Tinput\")\n    .Input(\"m_min: float\")\n    .Input(\"m_max: float\")\n    .Input(\"v: Tinput\")\n    .Input(\"v_min: float\")\n    .Input(\"v_max: float\")\n    .Input(\"beta: Tinput\")\n    .Input(\"beta_min: float\")\n    .Input(\"beta_max: float\")\n    .Input(\"gamma: Tinput\")\n    .Input(\"gamma_min: float\")\n    .Input(\"gamma_max: float\")\n    .Output(\"result: out_type\")\n    .Output(\"result_min: float\")\n    .Output(\"result_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i * 3), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"_MklConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D operator. Uses MKL DNN APIs to perform 2D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\n    MKL version of Conv2D operator for Eager mode. Uses MKL DNN APIs to perform 2D convolution.\n\n    NOTE Do not invoke this operator directly in Python. Eager Op rewrite is\n    expected to invoke these operators.\n    )doc\");\n\nREGISTER_OP(\"__MklDummyConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2D and BiasAdd operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Conv2D and BiasAdd.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_bias: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D and BiasAdd operator. Uses MKL DNN APIs to perform\n2D convolution and add Bias to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nDummy node that enables fusing Pad and Conv2D operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Pad and Conv2D.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_paddings: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nMKL version of Pad and Conv2D operator. Uses MKL DNN APIs to perform\nPad and 2D convolution to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Eager Op rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      // Fetch the data_format attribute, which may not exist.\n      string data_format;\n      Status s = c->GetAttr(\"data_format\", &data_format);\n\n      if (s.ok() && data_format == \"NCHW\") {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -3)));\n      } else {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -1)));\n      }\n      ShapeHandle sh;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &sh));\n      TF_RETURN_IF_ERROR(c->WithRank(sh, 4, &sh));\n      c->set_output(0, sh);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2DBackpropFilter and BiasAddGrad operator\nfor MKL. This node does not perform anything. It is just created as an\nintermediate output of merging Conv2DBackpropFilter and BiasAddGrad.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_bias_grad: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropFilterWithBiasShape)\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilterWithBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#ifdef INTEL_MKL_ML_ONLY\nREGISTER_OP(\"_MklConv2DWithBiasBackpropBias\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the bias.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"_MklConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Eager op rewrite is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape)\n    .Doc(R\"doc(\nMKL version of Conv3D operator. Uses MKL DNN APIs to perform 3D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution3D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv3DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu operator. Uses MKL DNN APIs to implement Relu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu6 operator. Uses MKL DNN APIs to implement Relu6 operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of Relu6Grad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu6 operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of LeakyRelu operator. Uses MKL DNN APIs to implement\nLeakyRelu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of LeakyReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for LeakyReluGrad operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklElu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Elu operator. Uses MKL DNN APIs to implement Elu operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklEluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of EluGrad operator. Uses MKL DNN APIs to compute Elu\ngradients for Elu operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklSoftmax\")\n    .Input(\"logits: T\")\n    .Input(\"mkl_logits: uint8\")\n    .Output(\"softmax: T\")\n    .Output(\"mkl_softmax: uint8\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    })\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n)doc\");\n\nREGISTER_OP(\"_MklTanh\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Tanh operator. Uses MKL DNN APIs to implement Tanh operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklTanhGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of TanhGrad operator. Uses MKL DNN APIs to compute tanh\ngradients for Tanh operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"workspace_enabled: bool = false\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Output(\"workspace: T\")\n#else\n    .Output(\"workspace: uint8\")\n#endif\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolShape)\n    .Doc(R\"doc(\nMKL version of MaxPool operator. Uses MKL DNN APIs to perform max pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPoolGrad\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Input(\"workspace: T\")\n#else\n    .Input(\"workspace: uint8\")\n#endif\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPoolGrad. Uses oneDNN APIs to compute gradients of\nMaxPool operator.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolShape)\n    .Doc(R\"doc(\nMKL version of AvgPool operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPoolGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3D\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of AvgPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3D\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of MaxPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof MaxPool3D function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRN\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    })\n    .Doc(R\"doc(\nMKL version of LRN operator. Uses MKL DNN APIs to perform local response\nnormalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_input_grads: uint8\")\n    .Input(\"mkl_input_image: uint8\")\n    .Input(\"mkl_output_image: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of LRNGrad operator. Uses MKL DNN APIs to compute gradient for\nlocal response normalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNorm operator. Uses oneDNN APIs to perform fused\nbatch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNormGrad operator. Uses oneDNN APIs to compute\ngradients for fused batch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"_MklFusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_MklToTf\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, bfloat16, qint8, quint8, qint32}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to convert a tensor from MKL layout to TensorFlow layout.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklInputConversion\")\n    .Input(\"input_0: T\")\n    .Input(\"input_1: T\")\n    .Input(\"mkl_input_0: uint8\")\n    .Input(\"mkl_input_1: uint8\")\n    .Output(\"output_0: T\")\n    .Output(\"output_1: T\")\n    .Output(\"mkl_output_0: uint8\")\n    .Output(\"mkl_output_1: uint8\")\n    // All datatypes supported by element-wise ops\n    .Attr(\n        \"T: {half, float, bfloat16, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to process the inputs to an elementwise MKL op. Both inputs\nneed to be either in TF or in MKL format. This op is added before every\nelement-wise MKL op.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#endif  // INTEL_MKL\nREGISTER_OP(\"QuantizedConv2DAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and BiasAdd.\nREGISTER_OP(\"QuantizedConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and Relu.\nREGISTER_OP(\"QuantizedConv2DAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Relu, and Requantize.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Sum, and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      // Since activations are not requantized per channel, `min_output`\n      // and `max_output` are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized MatMul and BiasAdd.\nREGISTER_OP(\"QuantizedMatMulWithBias\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRelu\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: float\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndReluAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndDequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: {float}\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DPerChannel\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedDepthwiseConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"IsotonicRegression\")\n    .Input(\"input: T\")\n    .Output(\"output: output_dtype\")\n    .Output(\"segments: int32\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"output_dtype: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {\n      context->set_output(0, context->input(0));\n      context->set_output(1, context->input(0));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for convolutional operations.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\n\n\ndef GetShrunkInceptionShapes(shrink=10):\n  \"\"\"Iterator for smaller versions of convolution shapes in 2015 Inception.\n\n  Relative to inception, each depth value is `depth // shrink`.\n\n  Args:\n    shrink: Factor to shrink each depth value by relative to Inception.\n\n  Yields:\n    Tuple (input_size, filter_size, out_size, stride, padding), the convolution\n    parameters of Inception layers.\n  \"\"\"\n  input_sizes = [[4, 5, 5, 1248], [4, 8, 8, 384], [4, 8, 8, 384],\n                 [4, 8, 8, 2048], [4, 8, 8, 448], [4, 8, 8, 2048],\n                 [4, 8, 8, 2048], [4, 8, 8, 2048], [4, 8, 8, 1760],\n                 [4, 8, 8, 1760], [4, 8, 8, 1760], [4, 8, 8, 1760],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1248],\n                 [4, 17, 17, 128], [4, 17, 17, 1248], [4, 17, 17, 224],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1216],\n                 [4, 17, 17, 1216], [4, 17, 17, 224], [4, 17, 17, 192],\n                 [4, 17, 17, 192], [4, 17, 17, 1152], [4, 17, 17, 1152],\n                 [4, 17, 17, 192], [4, 17, 17, 160], [4, 17, 17, 1152],\n                 [4, 17, 17, 1024], [4, 17, 17, 128], [4, 17, 17, 1024],\n                 [4, 17, 17, 128], [4, 17, 17, 1024], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 128], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 768], [4, 35, 35, 96],\n                 [4, 35, 35, 288], [4, 35, 35, 64], [4, 35, 35, 288],\n                 [4, 35, 35, 256], [4, 35, 35, 48], [4, 35, 35, 256],\n                 [4, 35, 35, 96], [4, 35, 35, 192], [4, 35, 35, 192],\n                 [4, 35, 35, 192], [4, 73, 73, 64], [4, 73, 73, 64],\n                 [4, 147, 147, 24]]\n  filter_sizes = [[1, 1, 1248, 128], [1, 3, 384, 384], [3, 1, 384, 384],\n                  [1, 1, 2048, 192], [3, 3, 448, 384], [1, 1, 2048, 320],\n                  [1, 1, 2048, 448], [1, 1, 2048, 384], [1, 1, 1760, 384],\n                  [1, 1, 1760, 192], [1, 1, 1760, 448], [1, 1, 1760, 320],\n                  [3, 3, 192, 192], [3, 3, 192, 192], [1, 1, 1248, 192],\n                  [3, 3, 128, 320], [1, 1, 1248, 128], [1, 3, 224, 224],\n                  [3, 1, 192, 256], [1, 3, 192, 256], [1, 1, 1216, 192],\n                  [1, 1, 1216, 96], [3, 1, 224, 224], [3, 3, 192, 224],\n                  [1, 3, 192, 192], [1, 1, 1152, 192], [1, 1, 1152, 128],\n                  [3, 1, 192, 192], [3, 3, 160, 192], [1, 1, 1152, 160],\n                  [1, 1, 1024, 128], [1, 3, 128, 192], [1, 1, 1024, 160],\n                  [3, 1, 128, 192], [1, 1, 1024, 256], [3, 1, 128, 128],\n                  [1, 1, 768, 192], [1, 3, 128, 128], [3, 3, 128, 128],\n                  [1, 1, 768, 128], [1, 1, 768, 320], [3, 3, 96, 96],\n                  [3, 3, 288, 384], [3, 3, 64, 96], [1, 1, 288, 64],\n                  [1, 1, 256, 64], [5, 5, 48, 64], [1, 1, 256, 48],\n                  [3, 3, 96, 96], [1, 1, 192, 32], [1, 1, 192, 64],\n                  [1, 1, 192, 48], [3, 3, 64, 192], [1, 1, 64, 64],\n                  [1, 1, 24, 64]]\n  out_sizes = [[4, 5, 5, 128], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 384], [4, 8, 8, 320],\n               [4, 8, 8, 448], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 448], [4, 8, 8, 320],\n               [4, 8, 8, 192], [4, 17, 17, 192], [4, 17, 17, 192],\n               [4, 8, 8, 320], [4, 17, 17, 128], [4, 17, 17, 224],\n               [4, 17, 17, 256], [4, 17, 17, 256], [4, 17, 17, 192],\n               [4, 17, 17, 96], [4, 17, 17, 224], [4, 17, 17, 224],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 128], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 192], [4, 17, 17, 256], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 128], [4, 17, 17, 128],\n               [4, 17, 17, 128], [4, 17, 17, 320], [4, 17, 17, 96],\n               [4, 17, 17, 384], [4, 35, 35, 96], [4, 35, 35, 64],\n               [4, 35, 35, 64], [4, 35, 35, 64], [4, 35, 35, 48],\n               [4, 35, 35, 96], [4, 35, 35, 32], [4, 35, 35, 64],\n               [4, 35, 35, 48], [4, 71, 71, 192], [4, 73, 73, 64],\n               [4, 147, 147, 64]]\n  strides = [\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1\n  ]\n  # Shrink sizes to make the test faster\n  for i in input_sizes:\n    i[3] //= shrink\n  for f in filter_sizes:\n    f[2] //= shrink\n    f[3] //= shrink\n  for o in out_sizes:\n    o[3] //= shrink\n  # pylint: disable=invalid-name\n  VALID = \"VALID\"\n  SAME = \"SAME\"\n  # pylint: enable=invalid-name\n  paddings = [\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      VALID, SAME, SAME, VALID, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, VALID, VALID, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, VALID, VALID, VALID\n  ]\n  for i, f, o, s, p in zip(input_sizes, filter_sizes, out_sizes, strides,\n                           paddings):\n    yield i, f, o, s, p\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NHWC\", False), (\"NHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is only supported on CUDA.\n    test_configs += [(\"NCHW\", True)]\n  return test_configs\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass Conv2DTest(test.TestCase):\n\n  def _DtypesToTest(self, use_gpu):\n    if test_util.IsMklEnabled():\n      return [dtypes.float32]\n\n    if use_gpu:\n      # It is important that float32 comes first, since we are using its\n      # gradients as a reference for fp16 gradients.\n      out = [dtypes.float32]\n      if test_util.GpuSupportsHalfMatMulAndConv():\n        out.append(dtypes.float16)\n      if not test.is_built_with_rocm():\n        out.append(dtypes.float64)\n      return out\n\n    return [dtypes.float32, dtypes.float64, dtypes.float16, dtypes.bfloat16]\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = 1\n    for s in shape:\n      total_size *= s\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _SetupValuesForDevice(self, tensor_in_sizes, filter_in_sizes, dilations,\n                            strides, padding, data_format, dtype, use_gpu):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n      use_gpu: True if the operations should be run on GPU\n    Returns:\n      Symbolic tensor value that can be used to execute the computation\n    \"\"\"\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes, dtype=dtype)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW(padding)\n      conv = nn_ops.conv2d(\n          t1,\n          t2,\n          dilations=dilations,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      self.assertEqual(conv.dtype, dtype)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n\n      return conv\n\n  def _CompareFwdValues(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that CPU and GPU produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    def _SetupVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n        t2 = constant_op.constant(x2, shape=filter_in_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t1 = test_util.NHWCToNCHW(t1)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d(\n            t1, t2, strides=strides, padding=padding, data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        return conv\n\n    tensors = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      tensors.append(_SetupVal(data_format, use_gpu))\n    values = self.evaluate(tensors)\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-3, atol=1e-3)\n\n  def _ComputeReferenceDilatedConv(self, tensor_in_sizes, filter_in_sizes,\n                                   stride, dilation, padding, data_format,\n                                   use_gpu):\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      if isinstance(stride, collections_abc.Iterable):\n        strides = list(stride)\n      else:\n        strides = [stride, stride]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        full_strides = [1, 1] + strides\n        full_dilation = [1, 1] + dilation\n      else:\n        full_strides = [1] + strides + [1]\n        full_dilation = [1] + dilation + [1]\n      expected = nn_ops.convolution(\n          t1,\n          t2,\n          padding=padding,\n          strides=strides,\n          dilation_rate=dilation,\n          data_format=data_format)\n      computed = nn_ops.conv2d(\n          t1,\n          t2,\n          strides=full_strides,\n          dilations=full_dilation,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCHW\":\n        expected = test_util.NCHWToNHWC(expected)\n        computed = test_util.NCHWToNHWC(computed)\n    return expected, computed\n\n  def _VerifyDilatedConvValues(self, tensor_in_sizes, filter_in_sizes, strides,\n                               padding, dilations, rtol=1e-4):\n    expected_results = []\n    computed_results = []\n    for data_format, use_gpu in GetTestConfigs():\n      expected, computed = self._ComputeReferenceDilatedConv(\n          tensor_in_sizes, filter_in_sizes, strides, dilations, padding,\n          data_format, use_gpu)\n      expected_results.append(expected)\n      computed_results.append(computed)\n    tolerance = 1e-2 if use_gpu else 1e-5\n    expected_values = self.evaluate(expected_results)\n    computed_values = self.evaluate(computed_results)\n    for e_value, c_value in zip(expected_values, computed_values):\n      tf_logging.debug(\"expected = %s\", e_value)\n      tf_logging.debug(\"actual = %s\", c_value)\n      self.assertAllClose(\n          e_value.flatten(), c_value.flatten(), atol=tolerance, rtol=rtol)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    filter_in_sizes,\n                    strides,\n                    padding,\n                    expected,\n                    dilations=(1, 1),\n                    gpu_only=False,\n                    test_grappler_layout_optimizer=False,\n                    tol=1e-5):\n    if gpu_only and not test.is_gpu_available(cuda_only=True):\n      return\n    tensors = []\n    dilations = list(dilations)\n    for (data_format, use_gpu) in GetTestConfigs():\n      if gpu_only and not use_gpu:\n        continue\n      dtypes_to_test = self._DtypesToTest(use_gpu)\n      if not test_grappler_layout_optimizer and data_format == \"NHWC\":\n        dtypes_to_test.append(dtypes.int32)\n      for dtype in dtypes_to_test:\n        result = self._SetupValuesForDevice(\n            tensor_in_sizes,\n            filter_in_sizes,\n            dilations,\n            strides,\n            padding,\n            data_format,\n            dtype,\n            use_gpu=use_gpu)\n        if test_grappler_layout_optimizer and data_format == \"NHWC\" and use_gpu:\n          # Grappler's layout optimizer will not optimize a fetch node, so\n          # this identity allows Grappler to optimize the Conv2D node.\n          result = array_ops.identity(result)\n        tensors.append(result)\n      values = self.evaluate(tensors)\n      for i in range(len(tensors)):\n        conv = tensors[i]\n        value = values[i]\n        tf_logging.debug(\"expected = %s\", expected)\n        tf_logging.debug(\"actual = %s\", value)\n        if np.issubdtype(value.dtype, np.integer):\n          self.assertAllEqual(np.rint(expected), np.ravel(value))\n        else:\n          self.assertAllCloseAccordingToType(\n              expected, np.ravel(value), atol=tol, rtol=tol)\n        self.assertShapeEqual(value, conv)\n        self.assertEqual(value.dtype, conv.dtype.as_numpy_dtype)\n\n  def _VerifyExplicitPaddings(self,\n                              tensor_in_sizes,\n                              filter_in_sizes,\n                              strides,\n                              padding,\n                              dilations=(1, 1),\n                              test_grappler_layout_optimizer=False,\n                              tol=1e-5):\n    \"\"\"Verifies Conv2D with explicit padding generates correct values.\n\n    It does this by comparing with Conv2D without explicit padding. This\n    function assumes Conv2D without explicit padding works correctly.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      strides: [row_stride, col_stride] for the convolution;\n      padding: Explicit padding amounts.\n      dilations: Dilation values\n      test_grappler_layout_optimizer: If True, allow the Grappler layout\n        optimizer to run, which turns NHWC Conv2Ds on the GPU to NCHW Conv2Ds.\n      tol: The absolute and relative tolerance.\n    \"\"\"\n    input_tensor = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_tensor = self._CreateNumpyTensor(filter_in_sizes)\n    input_tensor = array_ops.pad(input_tensor, [(0, 0)] + padding + [(0, 0)])\n    dilations = list(dilations)\n    conv2d_result = nn_ops.conv2d(\n        input_tensor,\n        filter_tensor, [1] + list(strides) + [1],\n        \"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(conv2d_result, [-1])))\n    self._VerifyValues(\n        tensor_in_sizes,\n        filter_in_sizes,\n        strides,\n        padding,\n        expected,\n        dilations,\n        test_grappler_layout_optimizer=test_grappler_layout_optimizer,\n        tol=tol)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x1Filter(self):\n    expected_output = [\n        30.0, 36.0, 42.0, 66.0, 81.0, 96.0, 102.0, 126.0, 150.0, 138.0, 171.0,\n        204.0, 174.0, 216.0, 258.0, 210.0, 261.0, 312.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.conv2d(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.conv2d(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionClass2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    convolver1 = nn_ops.Convolution(\n        input_shape=x1.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver1.num_batch_dims, 1)\n    convolver2 = nn_ops.Convolution(\n        input_shape=x2.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver2.num_batch_dims, 2)\n    conv1 = convolver1(x1, filter_in)\n    conv2 = convolver2(x2, filter_in)\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionWith2SpatialDimensionsAndExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.convolution(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.convolution(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter2x1Dilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmpty(self):\n    expected_output = []\n    self._VerifyValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [2271.0, 2367.0, 2463.0, 2901.0, 3033.0, 3165.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[1, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [\n        231.0, 252.0, 273.0, 384.0, 423.0, 462.0, 690.0, 765.0, 840.0, 843.0,\n        936.0, 1029.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2(self):\n    expected_output = [2271.0, 2367.0, 2463.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2Same(self):\n    expected_output = [2271.0, 2367.0, 2463.0, 1230.0, 1305.0, 1380.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride1x2(self):\n    expected_output = [58.0, 78.0, 98.0, 118.0, 138.0, 158.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 6, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideValid(self):\n    expected_output = [65, 95, 275, 305]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 7, 7, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideSame(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 9, 11])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"SAME\",\n        expected=[44, 28, 41, 16])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSize(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=[50, 60])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        dilations=[2, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D0x0Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[0, 0], [0, 0]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[3, 4, 3, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 2],\n        padding=[[0, 0], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D1x1Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[1, 1, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[2, 1, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 2], [2, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 1],\n        padding=[[2, 2], [2, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyBottomPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[0, 3], [0, 0]], tol=2e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[2, 2, 4, 3],\n        filter_in_sizes=[1, 2, 3, 2],\n        strides=[2, 2],\n        padding=[[0, 3], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyTopRightPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 2]],\n        tol=5e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 4, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 3],\n        padding=[[1, 0], [0, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DLotsPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 1, 1, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[3, 4], [4, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 1],\n        filter_in_sizes=[2, 2, 1, 3],\n        strides=[2, 1],\n        padding=[[3, 4], [4, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DExplicitPaddingWithDilations(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2dOnlyPaddingReturnsZeros(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 0, 2, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]],\n        expected=[0, 0, 0, 0, 0, 0, 0, 0])\n\n  def testConv2DExplicitPaddingWithLayoutOptimizer(self):\n    # Test with Grappler's layout optimizer, to ensure the layout optimizer\n    # handles explicit padding correctly.\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1],\n        test_grappler_layout_optimizer=True)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3],\n        test_grappler_layout_optimizer=True)\n\n  def _VerifyGroupConvFwd(self, tensor_in_sizes, filter_in_sizes, dilations,\n                          strides, padding, data_format, dtype):\n    \"\"\"Verify the output of group convolution is equal to a for-loop implementation.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n    \"\"\"\n    tensor_in = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    num_groups = tensor_in_sizes[3] // filter_in_sizes[2]\n    assert num_groups > 1 and \\\n        filter_in_sizes[2] * num_groups == tensor_in_sizes[3]\n    with test_util.device(True):\n      t1 = constant_op.constant(tensor_in, dtype=dtype)\n      t2 = constant_op.constant(filter_in, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        t1_splits = array_ops.split(t1, num_groups, axis=1)\n      else:\n        t1_splits = array_ops.split(t1, num_groups, axis=3)\n      t2_splits = array_ops.split(t2, num_groups, axis=3)\n\n      def MakeConv2d(inputs, filters):\n        return nn_ops.conv2d(\n            inputs,\n            filters,\n            strides,\n            padding,\n            dilations=dilations,\n            data_format=data_format)\n\n      group_conv = MakeConv2d(t1, t2)\n      group_conv_loop = array_ops.concat(\n          [MakeConv2d(t1s, t2s) for t1s, t2s in zip(t1_splits, t2_splits)],\n          axis=1 if data_format == \"NCHW\" else 3)\n\n      results = self.evaluate([group_conv, group_conv_loop])\n      tol_to_use = 1e-5\n      self.assertAllClose(\n          results[0], results[1], atol=tol_to_use, rtol=tol_to_use)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DGroupConvFwd(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      data_formats = [\"NHWC\", \"NCHW\"]\n    else:\n      data_formats = [\"NHWC\"]\n    for data_format in data_formats:\n      for dilation in [1, 2]:\n        for stride in [1, 2]:\n          for filter_dims in [[3, 3, 4, 8], [1, 1, 2, 16]]:\n            self._VerifyGroupConvFwd([10, 32, 32, 16], filter_dims,\n                                     dilations=[dilation, dilation],\n                                     strides=[stride, stride],\n                                     padding=\"SAME\",\n                                     data_format=data_format,\n                                     dtype=dtypes.float32)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testInputGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testFilterGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n  # TODO(yzhwang): this currently fails.\n  # self._VerifyValues(tensor_in_sizes=[1, 8, 8, 1],\n  #                   filter_in_sizes=[2, 2, 1, 1],\n  #                   strides=[4, 4], padding=\"SAME\",\n  #                   expected=[72, 112, 392, 432])\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInput(self,\n                                 input_sizes,\n                                 filter_sizes,\n                                 output_sizes,\n                                 strides,\n                                 padding,\n                                 expected,\n                                 data_format,\n                                 use_gpu,\n                                 err,\n                                 dilations=(1, 1)):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    with test_util.device(use_gpu):\n      if len(input_sizes) == 4:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n      t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])\n      t1 = constant_op.constant(x1, shape=filter_sizes)\n      t2 = constant_op.constant(x2, shape=output_sizes)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t2 = test_util.NHWCToNCHW(t2)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW((padding))\n      conv = nn_ops.conv2d_backprop_input(\n          t0,\n          t1,\n          t2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          dilations=dilations)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n      # \"values\" consists of two tensors for two backprops\n      value = self.evaluate(conv)\n      self.assertShapeEqual(value, conv)\n    tf_logging.debug(\"expected = %s\", expected)\n    tf_logging.debug(\"actual = %s\", value)\n    self.assertAllCloseAccordingToType(expected, value.flatten(), atol=1e-5)\n\n  def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,\n                            conv_strides, padding):\n    x1 = np.random.rand(*filter_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        if data_format == \"NCHW\":\n          new_input_sizes = test_util.NHWCToNCHW(input_sizes)\n        else:\n          new_input_sizes = input_sizes\n        t0 = constant_op.constant(new_input_sizes, shape=[len(new_input_sizes)])\n        t1 = constant_op.constant(x1, shape=filter_sizes)\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_input(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-2, atol=1e-2)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropInput(self):\n    expected_output = [1.0, 4.0, 4.0, 3.0, 10.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropInput(self):\n    expected_output = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInput(self):\n    expected_output = [\n        14.0, 32.0, 50.0, 100.0, 163.0, 226.0, 167.0, 212.0, 257.0, 122.0,\n        140.0, 158.0, 478.0, 541.0, 604.0, 437.0, 482.0, 527.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      # The GPU version of this test is not very stable. So adjusting the\n      # error threshold to 1e-4.\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInputStride1x2(self):\n    expected_output = [\n        1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 7.0, 12.0, 11.0, 18.0, 15.0, 24.0, 12.0,\n        16.0, 15.0, 20.0, 18.0, 24.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropInput(self):\n    expected_output = [\n        1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 4.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"XLA requires input_sizes to be a 4D shape.\")\n  def testConv2DInputSizesContainsOnlySpatialDimensionsBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[2, 2],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"b/239598470\")\n  def testConv2DBackpropInputDegenerateBackpropInput(self):\n    input_sizes = [3, 1, 1, 2]\n    expected_output = np.zeros(input_sizes).flatten()\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=input_sizes,\n          filter_sizes=[1, 3, 2, 3],\n          output_sizes=[3, 1, 0, 3],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilter(self,\n                                  input_sizes,\n                                  filter_sizes,\n                                  output_sizes,\n                                  strides,\n                                  padding,\n                                  expected,\n                                  data_format,\n                                  use_gpu,\n                                  dilations=(1, 1),\n                                  err=1e-5):\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    explicit_strides = [1] + strides + [1]\n    new_padding = padding\n    new_dilations = [1] + dilations + [1]\n    if isinstance(new_padding, (list, tuple)):\n      new_padding = [(0, 0)] + new_padding + [(0, 0)]\n    if data_format == \"NCHW\":\n      explicit_strides = test_util.NHWCToNCHW(explicit_strides)\n      new_dilations = test_util.NHWCToNCHW(new_dilations)\n      if isinstance(padding, (list, tuple)):\n        new_padding = test_util.NHWCToNCHW(new_padding)\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=explicit_strides,\n            padding=new_padding,\n            dilations=new_dilations,\n            data_format=data_format)\n        value = self.evaluate(conv)\n        self.assertShapeEqual(value, conv)\n      tf_logging.debug(\"expected = %s\", expected)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertAllCloseAccordingToType(expected, value.flatten(), err)\n\n  def _CompareBackFilter(self, input_sizes, filter_sizes, output_sizes,\n                         conv_strides, padding):\n    x0 = np.random.rand(*input_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-4, atol=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropFilter(self):\n    expected = [5.0, 8.0, 14.0, 17.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropFilter(self):\n    expected = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 0],\n          output_sizes=[1, 1, 2, 0],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DBackpropFilterWithEmptyInput(self):\n    expected = [0, 0, 0, 0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilter(self):\n    expected = [\n        17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0, 32.0, 43.0, 54.0,\n        37.0, 50.0, 63.0, 42.0, 57.0, 72.0, 62.0, 85.0, 108.0, 67.0, 92.0,\n        117.0, 72.0, 99.0, 126.0, 77.0, 106.0, 135.0, 82.0, 113.0, 144.0, 87.0,\n        120.0, 153.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x2(self):\n    expected = [161.0, 182.0, 287.0, 308.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropFilter(self):\n    expected_output = [78.]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilter(self):\n    expected_output = [1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInputDilation(self, input_sizes, filter_sizes,\n                                         output_sizes, strides, dilations,\n                                         padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu):\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t1)[0]\n        conv_2 = gradients_impl.gradients(conv_forward_2, t1)[0]\n        # \"values\" consists of two tensors for two backprops\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilterDilation(self, input_sizes, filter_sizes,\n                                          output_sizes, strides, dilations,\n                                          padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu):\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t2)[0]\n        conv_2 = gradients_impl.gradients(conv_forward, t2)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 0],\n            output_sizes=[1, 1, 2, 0],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 4, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[0, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[0, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputDilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        # The GPU version of this test is not very stable. So adjusting the\n        # error threshold to 1e-4.\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 2, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-4)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropInputDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  def _RunAndVerifyBackpropInputExplicitPadding(self,\n                                                input_sizes,\n                                                filter_sizes,\n                                                output_sizes,\n                                                strides,\n                                                padding,\n                                                data_format,\n                                                use_gpu,\n                                                dilations=(1, 1),\n                                                err=2e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    padded_input_sizes = input_sizes[:]\n    padded_input_sizes[1] += padding[0][0] + padding[0][1]\n    padded_input_sizes[2] += padding[1][0] + padding[1][1]\n    c = nn_ops.conv2d_backprop_input(\n        padded_input_sizes,\n        x1,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    c = c[:, padding[0][0]:(c.shape[1] - padding[0][1]), padding[1][0]:(\n        c.shape[2] - padding[1][1]), :]\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropInput(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        err=err,\n        dilations=dilations)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          dilations=[2, 2], use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 3],\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          err=5e-5,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 1],\n          use_gpu=use_gpu)\n\n  def _RunAndVerifyBackpropFilterExplicitPadding(self,\n                                                 input_sizes,\n                                                 filter_sizes,\n                                                 output_sizes,\n                                                 strides,\n                                                 padding,\n                                                 data_format,\n                                                 use_gpu,\n                                                 dilations=(1, 1),\n                                                 err=1e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n\n    x0 = np.pad(x0, [(0, 0)] + padding + [(0, 0)], \"constant\")\n    c = nn_ops.conv2d_backprop_filter(\n        x0,\n        filter_sizes,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropFilter(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        dilations=dilations,\n        err=err)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 2])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 1])\n\n  # Gradient checkers\n  def ConstructAndTestGradient(self,\n                               batch,\n                               input_rows,\n                               input_cols,\n                               filter_rows,\n                               filter_cols,\n                               in_depth,\n                               out_depth,\n                               stride_rows,\n                               stride_cols,\n                               padding,\n                               test_input,\n                               data_format,\n                               use_gpu,\n                               num_groups=1,\n                               max_err=0.003):\n    assert in_depth % num_groups == 0 and out_depth % num_groups == 0\n    input_shape = [batch, input_rows, input_cols, in_depth]\n    filter_shape = [filter_rows, filter_cols, in_depth // num_groups, out_depth]\n    # TODO(yangke): re-factor the computation of output shape.\n    if padding == \"VALID\":\n      output_rows = (input_rows - filter_rows + stride_rows) // stride_rows\n      output_cols = (input_cols - filter_cols + stride_cols) // stride_cols\n    elif padding == \"SAME\":\n      output_rows = (input_rows + stride_rows - 1) // stride_rows\n      output_cols = (input_cols + stride_cols - 1) // stride_cols\n    else:\n      self.assertIsInstance(padding, (list, tuple))\n      output_rows = (input_rows + padding[1][0] + padding[1][1] - filter_rows +\n                     stride_rows) // stride_rows\n      output_cols = (input_cols + padding[2][0] + padding[2][1] - filter_cols +\n                     stride_cols) // stride_cols\n    output_shape = [batch, output_rows, output_cols, out_depth]\n    input_size = 1\n    for x in input_shape:\n      input_size *= x\n    filter_size = 1\n    for x in filter_shape:\n      filter_size *= x\n    input_data = [x * 1.0 / input_size for x in range(0, input_size)]\n    filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]\n    # Conv2DGrad functions are not compiled for double due to\n    # a problem in the way Eigen's Conv2DGrad works for double.\n    # So we disable the DOUBLE path.  We should re-enable this\n    # when double support returns for CPU and/or GPU.\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with self.cached_session(use_gpu=use_gpu):\n        input_tensor = constant_op.constant(\n            input_data, shape=input_shape, dtype=dtype, name=\"input\")\n        filter_tensor = constant_op.constant(\n            filter_data, shape=filter_shape, dtype=dtype, name=\"filter\")\n        strides = [1, stride_rows, stride_cols, 1]\n        new_padding = padding\n        if data_format == \"NCHW\":\n          new_input_tensor = test_util.NHWCToNCHW(input_tensor)\n          strides = test_util.NHWCToNCHW(strides)\n          if isinstance(padding, (list, tuple)):\n            new_padding = test_util.NHWCToNCHW(padding)\n        else:\n          new_input_tensor = input_tensor\n        conv = nn_ops.conv2d(\n            new_input_tensor,\n            filter_tensor,\n            strides,\n            new_padding,\n            data_format=data_format,\n            name=\"conv\")\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        self.assertEqual(output_shape, conv.get_shape())\n        if test_input:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(input_tensor,\n                                                               input_shape,\n                                                               conv,\n                                                               output_shape)\n        else:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(filter_tensor,\n                                                               filter_shape,\n                                                               conv,\n                                                               output_shape)\n        if dtype == dtypes.float32:\n          reference_jacob_t = jacob_t\n          err = np.fabs(jacob_t - jacob_n).max()\n        else:\n          # Compare fp16/bf16 theoretical gradients to fp32 gradients,\n          # since fp16/bf16 numerical gradients are too imprecise.\n          err = np.fabs(jacob_t - reference_jacob_t).max()\n\n        tf_logging.debug(\"conv_2d gradient error = %s\", err)\n        self.assertLess(err, max_err)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=3,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.0025)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.003)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testShapeFunctionEdgeCases(self):\n    # All shapes unknown.\n    c1 = nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.float32),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n    self.assertEqual([None, None, None, None], c1.get_shape().as_list())\n\n    # Incorrect input shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Incorrect filter shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Depth mismatch.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[32, 20, 20, 3]),\n          array_ops.placeholder(\n              dtypes.float32, shape=[4, 4, 2, 2]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Input depth divisible by filter depth (group convolution).\n    # No exceptions should appear.\n    nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 8]),\n        array_ops.placeholder(dtypes.float32, shape=[4, 4, 2, 16]),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n\n    # Negative padding.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, -1], [1, 2], [0, 0]])\n\n    # Nonzero padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[1, 0], [0, 0], [0, 0], [0, 0]])\n\n    # Nonzero NCHW padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 1], [0, 0], [0, 0]],\n          data_format=\"NCHW\")\n\n    # Wrong amount of padding\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 0], [0, 0]])\n\n    # Only specify one padding amount per dimension\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0], [0], [0], [0]])\n\n    # Explicit padding elements are not lists\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[0, 0, 0, 0])\n\n  def testOpEdgeCases(self):\n    # Illegal strides.\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[2, 1, 1, 1], padding=\"SAME\"))\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 2], padding=\"SAME\"))\n\n    # TODO(b/195689143): Will enable when fixed for V2 behavior\n    # # Filter larger than input.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([20, 21, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([21, 20, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    #\n    # # Filter larger than input + padding.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    # filter_val = np.ones([24, 25, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val,\n    #           filter_val,\n    #           strides=[1, 1, 1, 1],\n    #           padding=[[0, 0], [2, 2], [2, 2], [0, 0]]))\n\n    # Filter dimensions must be greater than 0.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError, \"filter must not have zero elements\"\n        \"|has a non-positive dimension\"):\n      input_val = np.ones([1, 1, 1, 1])\n      filter_val = np.ones([1, 0, 1, 1])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 1], padding=\"SAME\"))\n\n    # Negative padding during backprop.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      filter_val = np.ones([18, 18, 3, 2])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_input([32, 20, 20, 3],\n                                       filter_val,\n                                       out_backprop_val,\n                                       strides=[1, 1, 1, 1],\n                                       padding=[[0, 0], [-1, 0], [0, 0], [0,\n                                                                          0]]))\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      input_val = np.ones([32, 20, 20, 3])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_filter(\n              input_val, [18, 18, 3, 2],\n              out_backprop_val,\n              strides=[1, 1, 1, 1],\n              padding=[[0, 0], [-1, 0], [0, 0], [0, 0]]))\n\n  def testConv2DBackpropInputInvalidOutBackpropRaiseError(self):\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      with self.cached_session():\n        input_sizes = constant_op.constant([65534, 65534],\n                                           shape=[2],\n                                           dtype=dtypes.int32)\n        filters = constant_op.constant(\n            0.159749106, shape=[3, 3, 2, 2], dtype=dtypes.float32)\n        out_backprop = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n        t = gen_nn_ops.conv2d_backprop_input(\n            input_sizes=input_sizes,\n            filter=filters,\n            out_backprop=out_backprop,\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\",\n            use_cudnn_on_gpu=True,\n            explicit_paddings=[],\n            data_format=\"NHWC\",\n            dilations=[1, 1, 1, 1])\n        self.evaluate(t)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DepthwiseConv2DTest(test.TestCase):\n\n  def _VerifyValues(self, tensor_in_sizes, filter_in_sizes, stride, padding,\n                    expected):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [filter_rows, filter_cols,\n        input_depth, depth_multiplier].\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n    \"\"\"\n    total_size_1 = 1\n    total_size_2 = 1\n    for s in tensor_in_sizes:\n      total_size_1 *= s\n    for s in filter_in_sizes:\n      total_size_2 *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n    with self.cached_session():\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t1.set_shape(tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      conv = nn_impl.depthwise_conv2d(\n          t1, t2, strides=[1, stride, stride, 1], padding=padding)\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 1e-5)\n    self.assertShapeEqual(value, conv)\n\n  def testConv2D2x2Filter(self):\n    # The inputs look like this (it's a 3 x 2 matrix, each of depth 2):\n    #\n    # [ (1.0, 2.0), (3.0,  4.0), ( 5.0,  6.0) ]\n    # [ (7.0, 8.0), (9.0, 10.0), (11.0, 12.0) ]\n    #  We can view this as two inputs\n    #\n    #  input depth 0:\n    #\n    #  [ 1.0,  3.0,  5.0 ]\n    #  [ 7.0,  9.0, 11.0 ]\n    #\n    #  input depth 1:\n    #\n    #  [ 2.0,  4.0,  6.0 ]\n    #  [ 8.0, 10.0, 12.0 ]\n    #\n    # The filter looks like this (it has two 2 x 2 patches, each generating 2\n    # depths):\n    #\n    #  filter #0:\n    #\n    #  [ (1.0,  3.0), ( 5.0,  7.0)]\n    #  [ (9.0, 11.0), (13.0, 15.0)]\n    #\n    #  filter #1:\n    #\n    #  [ ( 2.0,  4.0), ( 6.0,  8.0)]\n    #  [ (10.0, 12.0), (14.0, 16.0)]\n    #\n    # So the outputs are:\n    #\n    # (position 0, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  1.0 * 1.0 + 7.0 * 9.0 + 3.0 * 5.0 + 9.0 * 13.0 = 196\n    # (position 0, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  1.0 * 2.0 + 7.0 * 10.0 + 3.0 * 6.0 + 9.0 * 14.0 = 216\n    # (position 0, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  2.0 * 3.0 + 8.0 * 11.0 + 4.0 * 7.0 + 10.0 * 15.0 = 272\n    # (position 0, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  2.0 * 4.0 + 8.0 * 12.0 + 4.0 * 8.0 + 10.0 * 16.0 = 296\n    #\n    # (position 1, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  3.0 * 1.0 + 9.0 * 9.0 + 5.0 * 5.0 + 11.0 * 13.0 = 252\n    # (position 1, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  3.0 * 2.0 + 9.0 * 10.0 + 5.0 * 6.0 + 11.0 * 14.0 = 280\n    # (position 1, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  4.0 * 3.0 + 10.0 * 11.0 + 6.0 * 7.0 + 12.0 * 15.0 = 344\n    # (position 1, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  4.0 * 4.0 + 10.0 * 12.0 + 6.0 * 8.0 + 12.0 * 16.0 = 376\n    expected_output = [196, 216, 272, 296, 252, 280, 344, 376]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        stride=1,\n        padding=\"VALID\",\n        expected=expected_output)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass SeparableConv2DTest(test.TestCase):\n\n  def _InitValues(self, sizes):\n    \"\"\"Initializes values for input tensors.\n\n    Args:\n      sizes: Tensor dimensions.\n\n    Returns:\n      Tensor initialized to values.\n    \"\"\"\n    total_size = 1\n    for s in sizes:\n      total_size *= s\n    x = [f * 0.5 for f in range(1, total_size + 1)]\n    return constant_op.constant(x, shape=sizes)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    depthwise_filter_in_sizes,\n                    pointwise_filter_in_sizes,\n                    stride,\n                    padding,\n                    expected,\n                    data_format=\"NHWC\"):\n    \"\"\"Verifies the output values of the separable convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions.\n      depthwise_filter_in_sizes: Depthwise filter tensor dimensions.\n      pointwise_filter_in_sizes: Pointwise filter tensor dimensions.\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n      data_format: string data format for input tensor.\n    \"\"\"\n    with self.cached_session():\n      t1 = self._InitValues(tensor_in_sizes)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n\n      real_t1 = t1\n      strides = [1, stride, stride, 1]\n      if data_format == \"NCHW\":\n        real_t1 = array_ops.transpose(t1, [0, 3, 1, 2])\n        strides = [1, 1, stride, stride]\n        if isinstance(padding, list):\n          padding = [padding[0], padding[3], padding[1], padding[2]]\n\n      conv = nn_impl.separable_conv2d(\n          real_t1,\n          f1,\n          f2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n\n      if data_format == \"NCHW\":\n        conv = array_ops.transpose(conv, [0, 2, 3, 1])\n\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 2e-3)\n    self.assertShapeEqual(value, conv)\n\n  def _testSeparableConv2D(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 2, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 7].\n    # Complexity is O(2*3*2*2 + 6*7*1*1) as opposed to O(2*7*2*2).\n    expected_output = [\n        6644.5, 6971.5, 7298.5, 7625.5, 7952.5, 8279.5, 8606.5, 8154.5, 8556.5,\n        8958.5, 9360.5, 9762.5, 10164.5, 10566.5, 9664.5, 10141.5, 10618.5,\n        11095.5, 11572.5, 12049.5, 12526.5, 4145.5, 4346.5, 4547.5, 4748.5,\n        4949.5, 5150.5, 5351.5, 12684.5, 13311.5, 13938.5, 14565.5, 15192.5,\n        15819.5, 16446.5, 14194.5, 14896.5, 15598.5, 16300.5, 17002.5, 17704.5,\n        18406.5, 15704.5, 16481.5, 17258.5, 18035.5, 18812.5, 19589.5, 20366.5,\n        6499.5, 6814.5, 7129.5, 7444.5, 7759.5, 8074.5, 8389.5, 18724.5,\n        19651.5, 20578.5, 21505.5, 22432.5, 23359.5, 24286.5, 20234.5, 21236.5,\n        22238.5, 23240.5, 24242.5, 25244.5, 26246.5, 21744.5, 22821.5, 23898.5,\n        24975.5, 26052.5, 27129.5, 28206.5, 8853.5, 9282.5, 9711.5, 10140.5,\n        10569.5, 10998.5, 11427.5, 5746.75, 6010.75, 6274.75, 6538.75, 6802.75,\n        7066.75, 7330.75, 6168.75, 6452.25, 6735.75, 7019.25, 7302.75, 7586.25,\n        7869.75, 6590.75, 6893.75, 7196.75, 7499.75, 7802.75, 8105.75, 8408.75,\n        2036.25, 2119.5, 2202.75, 2286.0, 2369.25, 2452.5, 2535.75\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 7],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  def testSeparableConv2D(self):\n    self._testSeparableConv2D(\"NHWC\")\n\n  def disabledtestSeparableConv2DNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2D(\"NCHW\")\n\n  def _testSeparableConv2DEqualInputOutputDepth(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 3, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 6].\n    # Complexity is O(2*3*2*2 + 6*6*1*1) as opposed to O(2*6*2*2).\n    expected_output = [\n        5742.0, 6069.0, 6396.0, 6723.0, 7050.0, 7377.0, 7047.0, 7449.0, 7851.0,\n        8253.0, 8655.0, 9057.0, 8352.0, 8829.0, 9306.0, 9783.0, 10260.0,\n        10737.0, 3582.0, 3783.0, 3984.0, 4185.0, 4386.0, 4587.0, 10962.0,\n        11589.0, 12216.0, 12843.0, 13470.0, 14097.0, 12267.0, 12969.0, 13671.0,\n        14373.0, 15075.0, 15777.0, 13572.0, 14349.0, 15126.0, 15903.0, 16680.0,\n        17457.0, 5616.0, 5931.0, 6246.0, 6561.0, 6876.0, 7191.0, 16182.0,\n        17109.0, 18036.0, 18963.0, 19890.0, 20817.0, 17487.0, 18489.0, 19491.0,\n        20493.0, 21495.0, 22497.0, 18792.0, 19869.0, 20946.0, 22023.0, 23100.0,\n        24177.0, 7650.0, 8079.0, 8508.0, 8937.0, 9366.0, 9795.0, 4963.5, 5227.5,\n        5491.5, 5755.5, 6019.5, 6283.5, 5328.0, 5611.5, 5895.0, 6178.5, 6462.0,\n        6745.5, 5692.5, 5995.5, 6298.5, 6601.5, 6904.5, 7207.5, 1757.25, 1840.5,\n        1923.75, 2007.0, 2090.25, 2173.5\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 6],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  @test_util.deprecated_graph_mode_only\n  def testSeparableConv2DEqualInputOutputDepth(self):\n    self._testSeparableConv2DEqualInputOutputDepth(\"NHWC\")\n\n  def testSeparableConv2DEqualInputOutputDepthNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2DEqualInputOutputDepth(\"NCHW\")\n\n  def _testSeparableConv2dExplicitPadding(self, data_format):\n    tensor_in_sizes = [1, 4, 4, 2]\n    depthwise_filter_in_sizes = [2, 2, 2, 3]\n    pointwise_filter_in_sizes = [1, 1, 6, 7]\n    padding = [[0, 0], [1, 2], [3, 4], [0, 0]]\n    with self.cached_session():\n      # Compute the 'expected' values by manually padding before calling\n      # separable_conv2d\n      t1 = self._InitValues(tensor_in_sizes)\n      t1 = array_ops.pad(t1, padding)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n      conv = nn_impl.separable_conv2d(\n          t1,\n          f1,\n          f2,\n          strides=[1, 1, 1, 1],\n          padding=\"VALID\",\n          data_format=\"NHWC\")\n      expected = self.evaluate(conv)\n      expected = np.ravel(expected)\n    self._VerifyValues(\n        tensor_in_sizes=tensor_in_sizes,\n        depthwise_filter_in_sizes=depthwise_filter_in_sizes,\n        pointwise_filter_in_sizes=pointwise_filter_in_sizes,\n        stride=1,\n        padding=padding,\n        expected=expected,\n        data_format=data_format)\n\n  def testSeparableConv2dExplicitPadding(self):\n    self._testSeparableConv2dExplicitPadding(\"NHWC\")\n\n  def testSeparableConv2dExplicitPaddingNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2dExplicitPadding(\"NCHW\")\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DeepConv2DTest(test.TestCase):\n\n  def _CompareFwdConv2D(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that DeepConv2D and Conv2D produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    with self.cached_session(use_gpu=False):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      strides = [1] + conv_strides + [1]\n\n      conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"0\"\n      values_expect = self.evaluate([conv])\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"1\"\n      values_test = self.evaluate([conv])\n\n      self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\n\n  def _RunTestCases(self, conv_strides, padding):\n    input_sizes = [[5, 5, 5, 1248], [3, 17, 17, 192], [2, 35, 35, 288],\n                   [2, 6, 8, 517], [2, 7, 4, 81], [3, 11, 3, 77]]\n    filter_sizes = [[3, 3, 1248, 128], [3, 3, 192, 192], [3, 3, 288, 384],\n                    [3, 3, 517, 64], [3, 3, 81, 77], [3, 3, 77, 181]]\n    for input_shape, filter_shape in zip(input_sizes, filter_sizes):\n      self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\n\n  def testConv2D3x3FilterStride1x1Valid(self):\n    self._RunTestCases([1, 1], \"VALID\")\n\n  def testConv2D3x3FilterStride1x1Same(self):\n    self._RunTestCases([1, 1], \"SAME\")\n\n\nclass Conv2DBenchmark(test.Benchmark):\n\n  def benchmarkGPUConvStackFirst(self):\n    # Benchmark the first iteration of a conv-net with many identical conv\n    # operations.\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default(), session_lib.Session() as session:\n      batch_size = 1\n      timesteps = 600\n      features = 1\n\n      inputs = random_ops.random_uniform(\n          [batch_size, 1, timesteps, features], seed=1234)\n      num_outputs_list = [512] * 40 + [1]\n      kernel_w = 3\n      x = inputs\n      for num_outputs in num_outputs_list:\n        x = convolutional.conv2d(x, num_outputs, [1, kernel_w])\n      outputs = x\n\n      self.evaluate(variables.global_variables_initializer())\n      num_iterations = 4\n      for iter_index in range(num_iterations):\n        start = time.time()\n        session.run(outputs)\n        wall_time = time.time() - start\n        self.report_benchmark(\n            name=\"conv_stack_iter_%d\" % iter_index, wall_time=wall_time)\n        tf_logging.info(\"conv_stack_iter_%d: %.4f\" % (iter_index, wall_time))\n\n  def _bench_op(self, name, op, burn_iters, num_iters):\n    config = config_pb2.ConfigProto()\n    # Prevent Grappler from optimizing away the entire graph.\n    config.graph_options.rewrite_options.dependency_optimization = (\n        rewriter_config_pb2.RewriterConfig.OFF)\n    with session_lib.Session(config=config) as session:\n      self.evaluate(variables.global_variables_initializer())\n      self.run_op_benchmark(\n          session, op, burn_iters=burn_iters, min_iters=num_iters, name=name)\n\n  def benchmarkExplicitVsManualPadding(self):\n    \"\"\"Compare performance of EXPLICIT padding and calling tf.pad.\n\n    A Conv2D op with EXPLICIT padding is benchmarked, and a tf.pad with the same\n    padding followed by an equivalent Conv2D op is benchmarked.\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_iters = 300\n      batch_size = 64\n      # The input and filter correspond to the first layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              3,\n              224,\n              224\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([7, 7, 3, 64]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 2, 2]\n      padding = [(0, 0), (0, 0), (3, 3), (3, 3)]\n      output_explicit_pad = nn_ops.conv2d(\n          input, filter, strides, padding=padding, data_format=\"NCHW\")\n      input_padded = array_ops.pad(input, padding)\n      output_manual_pad = nn_ops.conv2d(\n          input_padded, filter, strides, padding=\"VALID\", data_format=\"NCHW\")\n      # Benchmark just the forward pass.\n      self._bench_op(\"explicit_pad_forward\", output_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"manual_pad_forward\", output_manual_pad.op, burn_iters,\n                     num_iters)\n\n      # Benchmark both the forward and backwards passes.\n      input_grad_explicit_pad, filter_grad_explicit_pad = (\n          gradients_impl.gradients(output_explicit_pad, [input, filter]))\n      self._bench_op(\n          \"explicit_pad_backward\",\n          control_flow_ops.group(input_grad_explicit_pad,\n                                 filter_grad_explicit_pad), burn_iters,\n          num_iters)\n      input_grad_manual_pad, filter_grad_manual_pad = gradients_impl.gradients(\n          output_manual_pad, [input, filter])\n      self._bench_op(\n          \"manual_pad_backward\",\n          control_flow_ops.group(input_grad_manual_pad, filter_grad_manual_pad),\n          burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingGraph(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in graph mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. The purpose is to ensure EXPLICIT padding is just as\n    efficient as the SAME case\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n\n      for _ in range(num_convs):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n      grad_explicit_pad, = gradients_impl.gradients(output_explicit_pad, filter)\n      grad_same_pad, = gradients_impl.gradients(output_same_pad, filter)\n      self._bench_op(\"graph_explicit_pad\", grad_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"graph_same_pad\", grad_same_pad.op, burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingEager(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in eager mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. Currently, EXPLICIT padding is slightly slower, due to the\n    fact the Python padding list must be checked and processed before the Conv2D\n    op can run.\n    \"\"\"\n    # TODO(reedwm): Make EXPLICIT padding as fast as SAME padding.\n    if not test.is_gpu_available():\n      return\n\n    with context.eager_mode():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n      for _ in range(burn_iters):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_explicit_pad = nn_ops.conv2d(\n                output_explicit_pad,\n                filter,\n                strides,\n                padding=padding,\n                data_format=\"NCHW\")\n          tape.gradient(output_explicit_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_explicit_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_same_pad = nn_ops.conv2d(\n                output_same_pad,\n                filter,\n                strides,\n                padding=\"SAME\",\n                data_format=\"NCHW\")\n          tape.gradient(output_same_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_same_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n\ndef GetInceptionFwdTest(input_size, filter_size, stride, padding,\n                        gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionFwd %s\", (input_size, filter_size,\n                                                   stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionFwd %s\", (input_size, filter_size, stride,\n                                                padding))\n    self._CompareFwdValues(input_size, filter_size, [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionFwdDilatedConvTest(input_size, filter_size, stride, padding):\n\n  def Test(self):\n    if stride == 1:\n      tf_logging.info(\"Testing InceptionFwd with dilations %s\",\n                      (input_size, filter_size, stride, padding))\n      self._VerifyDilatedConvValues(\n          tensor_in_sizes=input_size,\n          filter_in_sizes=filter_size,\n          strides=[stride, stride],\n          dilations=[2, 2],\n          padding=padding,\n          rtol=5e-4)\n\n  return Test\n\n\ndef GetInceptionBackInputTest(input_size, filter_size, output_size, stride,\n                              padding,\n                              gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackInput %s\",\n                      (input_size, filter_size, output_size, stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackInput %s\",\n                    (input_size, filter_size, output_size, stride, padding))\n    self._CompareBackpropInput(input_size, filter_size, output_size,\n                               [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionBackFilterTest(input_size, filter_size, output_size, strides,\n                               padding, gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackFilter %s\",\n                      (input_size, filter_size, output_size, strides, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackFilter %s\",\n                    (input_size, filter_size, output_size, strides, padding))\n    self._CompareBackFilter(input_size, filter_size, output_size, strides,\n                            padding)\n\n  return Test\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass FusedConv2DTest(test.TestCase):\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = np.prod(shape)\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _CreateConv2D(self,\n                    input_values,\n                    filters,\n                    strides=[1, 1],\n                    padding=\"SAME\"):\n    return nn_ops.convolution(\n        input_values, filters, strides=strides, padding=padding)\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 1.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountOne(self):\n    expected_output = [\n        113377, 125570, 77305, 86738, 19433, 22226, 60681, 70722, 36291, 43718,\n        7143, 9206, 9785, 12098, 4783, 6366, 779, 1134\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has a total refcount of 2, and Add is its last consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddLast(self):\n    expected_output = [\n        1.907175e+06, 2.253505e+06, 7.809210e+05, 9.537180e+05, 1.184170e+05,\n        1.523070e+05, 5.367010e+05, 6.803700e+05, 1.867090e+05, 2.529460e+05,\n        2.362300e+04, 3.522600e+04, 5.121700e+04, 7.168300e+04, 1.494300e+04,\n        2.347400e+04, 1.558000e+03, 2.903000e+03\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv2, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2 and Add (in the fused Conv2D op) is its first consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddFirst(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu = nn_ops.relu(add)\n    output = math_ops.add_n([relu, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2, and there is no dependency between its two consumers.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndNoDependence(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu1 = nn_ops.relu(add)\n    relu2 = nn_ops.relu(conv2)\n    output = math_ops.add_n([relu1, relu2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add is the same as the input to the fused Conv2D op and needs a tensor\n  # buffer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithSameSrcAndAddTensorBuffer(self):\n    expected_output = [\n        57157, 63298, 39249, 44026, 9971, 11402, 31193, 36306, 19126, 22948,\n        3970, 5060, 5135, 6350, 2666, 3524, 461, 674\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n\n    conv1 = self._CreateConv2D(x, filter_in)\n\n    conv = self._CreateConv2D(conv1, filter_in)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n\nif __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n\n  # TODO(b/35359731)\n  # Fwd, BckInput, and BackFilter to test that for certain input parameter\n  # set, winograd nonfused algorithm will be excluded from conv autotune. If\n  # in such case, winograd nonfused algorithm is added as one option of the\n  # conv autotune, and cuDNN version is smaller than 7, the following tests\n  # will fail.\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Implements convolution operations with image transformations (resize and\n// mirror padding) baked into the processing, to optimize latency and memory\n// usage.\n\n#define EIGEN_USE_THREADS\n\n#include <string>\n#include <vector>\n\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_slice.h\"\n#include \"tensorflow/core/kernels/conv_2d.h\"\n#include \"tensorflow/core/kernels/conv_ops.h\"\n#include \"tensorflow/core/kernels/gemm_functors.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/util/image_resizer_state.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\nnamespace {\n\n// We don't want to allocate a buffer to hold all the patches if the size is\n// going to be extremely large, so break it into chunks if it's bigger than\n// a limit. Each chunk will be processed serially, so we can refill the\n// buffer for the next chunk and reuse it, keeping maximum memory size down.\n// In this case, we've picked 16 megabytes as a reasonable limit for Android and\n// other platforms using Eigen, and 1MB for iOS devices, from experimentation.\n#if defined(__APPLE__) && defined(IS_MOBILE_PLATFORM)\nconst size_t kMaxChunkSize = (1 * 1024 * 1024);\n#else\nconst size_t kMaxChunkSize = (16 * 1024 * 1024);\n#endif\nconst size_t kResizeCacheSize = (8 * 1024 * 1024);\n\n// Lookup method used when resizing.\nenum SamplingMode {\n  BILINEAR = 0,\n  NEAREST = 1,\n};\n\n// Simple utility function used by FusedConv to multithread basic workloads. To\n// use it, pass begin and end values for the full workload and a std::function\n// that receives a subset of that through the begin and end values for each\n// worker's task. The division of the full workload into worker tasks is handled\n// by the multithreading logic. Here's an example of how to use it:\n// std::vector<float> my_vector(100);\n// ...\n// FusedConvParallelFor(context, 0, 100,\n//   [&my_vector](int64 task_begin, int64 task_end) {\n//     for (int64 current = task_begin; current != task_end; ++current) {\n//       my_vector[current] *= 10.0f;\n//     }\n// });\nvoid FusedConvParallelFor(\n    OpKernelContext* context, int64_t begin, int64_t end,\n    const std::function<void(int64_t, int64_t)>& task_function) {\n// On iOS, the thread management imposes a very big performance penalty, so\n// just call the function directly with no multithreading.\n#if defined(__APPLE__) && defined(IS_MOBILE_PLATFORM)\n  task_function(begin, end);\n#else\n  auto& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n  thread::ThreadPool* thread_pool = worker_threads.workers;\n  const int64_t total_elements = end - begin;\n  // This is a bit of an arbitrary number, but was found to work well for\n  // typical models we've been profiling on various devices.\n  const int64_t element_cost = 10000000;\n  thread_pool->ParallelFor(\n      total_elements, element_cost,\n      [begin, task_function](int64_t begin_offset, int64_t end_offset) {\n        const int64_t task_begin = begin + begin_offset;\n        const int64_t task_end = begin + end_offset;\n        task_function(task_begin, task_end);\n      });\n#endif\n}\n\n// Holds the state needed for the resizing subtasks.\ntemplate <class T1>\nstruct ResizeTaskParameters {\n  ResizeTaskParameters() : st(false, false) {}\n\n  int cache_height;\n  T1* resize_cache;\n  int cache_line_width;\n  int input_width;\n  int input_depth;\n  int top_padding;\n  int pad_offset;\n  int64_t resized_height;\n  ImageResizerState st;\n  const T1* input_batch_start;\n  int64_t cache_start_x;\n  int64_t cache_end_x;\n  int left_padding;\n  int64_t resized_width;\n  int64_t padded_width;\n  int64_t padded_height;\n};\n\ntemplate <class T1>\nstruct PerCacheLineParameters {\n  PerCacheLineParameters() {}\n  PerCacheLineParameters(const PerCacheLineParameters<T1>& other)\n      : cache_line_start(other.cache_line_start),\n        input_top_row_start(other.input_top_row_start),\n        input_bottom_row_start(other.input_bottom_row_start),\n        y_lerp(other.y_lerp) {}\n\n  T1* cache_line_start;\n  const T1* input_top_row_start;\n  const T1* input_bottom_row_start;\n  T1 y_lerp;\n};\n\n// Helper class to simplify bilinear filtering\ntemplate <class T1>\nstruct SampleRect {\n  EIGEN_ALWAYS_INLINE SampleRect(const T1* in_top_left, const T1* in_top_right,\n                                 const T1* in_bottom_left,\n                                 const T1* in_bottom_right)\n      : top_left(in_top_left),\n        top_right(in_top_right),\n        bottom_left(in_bottom_left),\n        bottom_right(in_bottom_right) {}\n\n  EIGEN_ALWAYS_INLINE T1 BilinearSample(int channel, T1 x_lerp,\n                                        T1 y_lerp) const {\n    const T1 top =\n        top_left[channel] + (top_right[channel] - top_left[channel]) * x_lerp;\n    const T1 bottom = bottom_left[channel] +\n                      (bottom_right[channel] - bottom_left[channel]) * x_lerp;\n    return top + (bottom - top) * y_lerp;\n  }\n\n  const T1* top_left;\n  const T1* top_right;\n  const T1* bottom_left;\n  const T1* bottom_right;\n};\n\n// Calculates parameters which remain constant through a resize cache row.\ntemplate <class T1>\nEIGEN_ALWAYS_INLINE PerCacheLineParameters<T1> CalculatePerCacheLineParameters(\n    int64_t cache_height, int64_t cache_y, T1* resize_cache,\n    int64_t cache_line_width, int64_t input_width, int64_t input_depth,\n    int64_t top_padding, int64_t pad_offset, int64_t resized_height,\n    const ImageResizerState& st, const T1* input_batch_start) {\n  PerCacheLineParameters<T1> result;\n  // The cache is organized so that the real y values of the resized image map\n  // onto the actual cache values through a modulo scheme. This means that as we\n  // progress downwards through the image, we keep reusing a small cache and so\n  // keep memory usage down.\n  int64_t cache_index_y;\n  if (cache_y < 0) {\n    cache_index_y = cache_height + (cache_y % cache_height);\n  } else {\n    cache_index_y = cache_y % cache_height;\n  }\n  result.cache_line_start =\n      resize_cache + (cache_index_y * cache_line_width * input_depth);\n  // This part is implementing the mirror padding that happens before resizing.\n  float in_y = (cache_y - top_padding);\n  if (in_y < 0) {\n    in_y = -(in_y + 1.0f - pad_offset);\n  } else if (in_y >= resized_height) {\n    in_y = (resized_height * 2.0f) - (in_y + 1.0f + pad_offset);\n  }\n  // Here's where to do the actual resize.\n  in_y *= st.height_scale;\n  const int64_t top_y_index = static_cast<int64_t>(std::floor(in_y));\n  const int64_t bottom_y_index =\n      std::min(static_cast<int64_t>(std::ceil(in_y)), (st.in_height - 1));\n  // Lerp is used for bilinear filtering when that's needed.\n  result.y_lerp = static_cast<T1>(in_y - top_y_index);\n  // Which rows of the original input image to pull the values from.\n  result.input_top_row_start =\n      input_batch_start + (top_y_index * input_width * input_depth);\n  result.input_bottom_row_start =\n      input_batch_start + (bottom_y_index * input_width * input_depth);\n  return result;\n}\n\ntemplate <class T1>\nstruct PerCachePixelParameters {\n  PerCachePixelParameters() {}\n  PerCachePixelParameters(const PerCachePixelParameters<T1>& other)\n      : cache_line_pixel(other.cache_line_pixel),\n        left_x_index(other.left_x_index),\n        right_x_index(other.right_x_index),\n        x_lerp(other.x_lerp) {}\n\n  T1* cache_line_pixel;\n  int64_t left_x_index;\n  int64_t right_x_index;\n  T1 x_lerp;\n};\n\n// Pulls out common parameters used for every resized pixel.\ntemplate <class T1>\nEIGEN_ALWAYS_INLINE PerCachePixelParameters<T1>\nCalculatePerCachePixelParameters(int64_t cache_x, int64_t cache_start_x,\n                                 T1* cache_line_start, int64_t input_depth,\n                                 int64_t left_padding, int64_t pad_offset,\n                                 int64_t resized_width,\n                                 const ImageResizerState& st) {\n  PerCachePixelParameters<T1> result;\n  // Figure out where we're going to store the results of our transform.\n  const int cache_index_x = cache_x - cache_start_x;\n  result.cache_line_pixel = cache_line_start + (cache_index_x * input_depth);\n  // Implement mirror padding by flipping in_x if it's off the edge.\n  float in_x = (cache_x - left_padding);\n  if (in_x < 0) {\n    in_x = -(in_x + 1.0f - pad_offset);\n  } else if (in_x >= resized_width) {\n    in_x = (resized_width * 2.0f) - (in_x + 1.0f + pad_offset);\n  }\n  // Resize the x parameters.\n  in_x *= st.width_scale;\n  // Get the x coordinates for the left and right pixels to pull from.\n  result.left_x_index = static_cast<int64_t>(std::floor(in_x));\n  result.right_x_index =\n      std::min(static_cast<int64_t>(std::ceil(in_x)), (st.in_width - 1));\n  // This x_lerp is used to blend pixels in bilinear filtering.\n  result.x_lerp = static_cast<T1>(in_x - result.left_x_index);\n  return result;\n}\n\n// Combines bilinear resizing and mirror padding into the im2col transformation\n// stage of convolution.\ntemplate <class T1, class T2, class T3, class TGemmFunctor,\n          SamplingMode SampleMode>\nclass FusedResizeAndPadConvFunctor {\n public:\n  void operator()(OpKernelContext* context, const Tensor& input,\n                  int input_batches, int resized_height, int resized_width,\n                  int padded_height, int padded_width, int input_depth,\n                  const T2* filter_data, int filter_height, int filter_width,\n                  int filter_count, int stride_rows, int stride_cols,\n                  Padding padding, T3* output_data, int output_height,\n                  int output_width, const ImageResizerState& st,\n                  int top_padding, int bottom_padding, int left_padding,\n                  int right_padding, int pad_offset) {\n    if ((input_batches <= 0) || (padded_width <= 0) || (padded_height <= 0) ||\n        (input_depth <= 0)) {\n      LOG(WARNING) << \"Conv2D was called with bad input dimensions: \"\n                   << input_batches << \", \" << padded_height << \", \"\n                   << padded_width << \", \" << input_depth;\n      return;\n    }\n    if ((filter_width <= 0) || (filter_height <= 0) || (filter_count <= 0)) {\n      LOG(WARNING) << \"Conv2D was called with bad filter dimensions: \"\n                   << filter_width << \", \" << filter_height << \", \"\n                   << filter_count;\n      return;\n    }\n    if ((output_width <= 0) || (output_height <= 0)) {\n      LOG(WARNING) << \"Conv2D was called with bad output width or height: \"\n                   << output_width << \", \" << output_height;\n      return;\n    }\n    OP_REQUIRES(\n        context, ((SampleMode == NEAREST) || (SampleMode == BILINEAR)),\n        errors::InvalidArgument(\"Bad sample mode passed in\", SampleMode));\n\n    // These calculations define how the patches will be positioned within the\n    // input image. The actual definitions are quite complex, and rely on the\n    // previously-calculated output size.\n    int filter_left_offset;\n    int filter_top_offset;\n    if (padding == VALID) {\n      filter_left_offset =\n          ((output_width - 1) * stride_cols + filter_width - padded_width + 1) /\n          2;\n      filter_top_offset = ((output_height - 1) * stride_rows + filter_height -\n                           padded_height + 1) /\n                          2;\n    } else {\n      filter_left_offset =\n          ((output_width - 1) * stride_cols + filter_width - padded_width) / 2;\n      filter_top_offset =\n          ((output_height - 1) * stride_rows + filter_height - padded_height) /\n          2;\n    }\n\n    ResizeTaskParameters<T1> task_params;\n    task_params.input_depth = input_depth;\n    task_params.top_padding = top_padding;\n    task_params.pad_offset = pad_offset;\n    task_params.resized_height = resized_height;\n    task_params.st = st;\n    task_params.left_padding = left_padding;\n    task_params.resized_width = resized_width;\n    task_params.padded_width = padded_width;\n    task_params.padded_height = padded_height;\n\n    // The im2col buffer has # of patches rows, and # of filters cols.\n    // It's laid out like this, in row major order in memory:\n    //        < filter value count >\n    //   ^   +---------------------+\n    // patch |                     |\n    // count |                     |\n    //   v   +---------------------+\n    // Each patch row contains a filter_width x filter_height patch of the\n    // input, with the depth channel as the most contiguous in memory, followed\n    // by the width, then the height. This is the standard memory order in the\n    // image world if it helps to visualize it.\n    const int filter_value_count = filter_width * filter_height * input_depth;\n\n    OP_REQUIRES(context, (filter_value_count * sizeof(T1)) <= kMaxChunkSize,\n                errors::InvalidArgument(\"Im2Col patch too large for buffer\"));\n    const size_t patches_per_chunk =\n        kMaxChunkSize / (filter_value_count * sizeof(T1));\n    // Because memory allocation is very expensive on mobile platforms, try to\n    // allocate a persistent buffer that will be kept around between calls. We\n    // use TensorFlow's resource management to ensure that the memory will be\n    // released when the session is over.\n    Im2ColBufferResource<T1, kMaxChunkSize>* im2col_buffer_resource;\n    std::function<Status(Im2ColBufferResource<T1, kMaxChunkSize>**)> creator =\n        [](Im2ColBufferResource<T1, kMaxChunkSize>** resource) {\n          *resource = new Im2ColBufferResource<T1, kMaxChunkSize>();\n          return OkStatus();\n        };\n    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate(\n                                \"Conv2d\", \"im2col_buffer\",\n                                &im2col_buffer_resource, creator));\n\n    // Create a resize cache memory buffer that will hold the rows of\n    // transformed and mirror padded input pixels, ready to be copied\n    // into filter patches by im2col.\n    // It's laid out like this, in row major order in memory:\n    //         < cache line width >\n    //   ^    +--------------------+\n    // cache  |                    |\n    // height |                    |\n    //   v    +--------------------+\n    // Each cache row contains a cache_line_width number of resized pixels,\n    // each with input_depth channels. The cache height is typically less than\n    // the full height the resized image would be, so it's filled up\n    // incrementally as we progress downwards through the input creating im2col\n    // patches.\n    task_params.cache_start_x = -filter_left_offset;\n    task_params.cache_end_x =\n        (((output_width - 1) * stride_cols) - filter_left_offset) +\n        filter_width;\n    task_params.cache_line_width =\n        task_params.cache_end_x - task_params.cache_start_x;\n    task_params.cache_height =\n        kResizeCacheSize / (task_params.cache_line_width * input_depth);\n    const int needed_resize_cache_count =\n        filter_height * task_params.cache_line_width * input_depth;\n    OP_REQUIRES(context,\n                (needed_resize_cache_count * sizeof(T1)) <= kResizeCacheSize,\n                errors::InvalidArgument(\"Input too large for resize cache\"));\n    Im2ColBufferResource<T1, kResizeCacheSize>* resize_cache_resource;\n    std::function<Status(Im2ColBufferResource<T1, kResizeCacheSize>**)>\n        resize_creator =\n            [](Im2ColBufferResource<T1, kResizeCacheSize>** resource) {\n              *resource = new Im2ColBufferResource<T1, kResizeCacheSize>();\n              return OkStatus();\n            };\n    OP_REQUIRES_OK(context, context->resource_manager()->LookupOrCreate(\n                                \"Conv2d\", \"resize_cache\",\n                                &resize_cache_resource, resize_creator));\n\n    // This means that multiple ops can't be run simultaneously on different\n    // threads, because we have a single shared resource. The platforms this is\n    // aimed at have intra-op parallelism as their focus though, so it shouldn't\n    // be an issue.\n    mutex_lock lock_buffer(im2col_buffer_resource->mu);\n    core::ScopedUnref unref_buffer(im2col_buffer_resource);\n    T1* im2col_buffer = im2col_buffer_resource->data;\n\n    // This buffer is used as a fairly heavy-weight cache for the resized and\n    // mirrored inputs to the im2col operation. The problem is that we want to\n    // keep the memory usage down by not rendering the fully resized and padded\n    // input tensor to the convolution into an entire buffer. The first approach\n    // to avoid this was to fold the bilinear filtering and padding spatial\n    // transformations into the im2col lookup itself. This successfully reduced\n    // memory usage, but because im2col can access an individual pixel for many\n    // different patches, the extra overhead of doing the same bilinear lookups\n    // repeatedly became too expensive.\n    // The resize cache is designed to avoid this problem by keeping a\n    // horizontal slice of the resized and padded input to the im2col\n    // precalculated, so that repeated accesses to the same pixel from different\n    // filter patches can just be copied from this cache. It's organized as a\n    // horizontal slice stretching across the whole virtual image, and as high\n    // as the filter window, so that as the patch processing moves across all\n    // the pixels are present, and before a new row of patches is started any\n    // previously calculated rows that are needed are maintained, with new rows\n    // calculated as required.\n    mutex_lock resize_lock_buffer(resize_cache_resource->mu);\n    core::ScopedUnref unref_resized_cache(resize_cache_resource);\n    task_params.resize_cache = resize_cache_resource->data;\n\n    const T1* input_data = input.flat<T1>().data();\n    const int64_t input_height = input.shape().dim_sizes()[1];\n    task_params.input_width = input.shape().dim_sizes()[2];\n\n    int end_cached_lines = std::numeric_limits<int>::min();\n\n    for (int batch = 0; batch < input_batches; ++batch) {\n      task_params.input_batch_start =\n          input_data +\n          (batch * input_height * task_params.input_width * input_depth);\n      const int in_y_end =\n          ((output_height * stride_rows) - filter_top_offset) + filter_height;\n      for (int out_y = 0; out_y < output_height; ++out_y) {\n        const int in_y_origin = (out_y * stride_rows) - filter_top_offset;\n        const int cache_start_y = std::max(in_y_origin, end_cached_lines);\n        const int cache_end_y = std::min(\n            in_y_end, std::max((in_y_origin + task_params.cache_height),\n                               end_cached_lines));\n        if (end_cached_lines < (in_y_origin + filter_height)) {\n          // This call breaks up the work required for calculating the mirror\n          // padding and resizing across multiple threads.\n          FusedConvParallelFor(\n              context, cache_start_y, cache_end_y,\n              [task_params](int64_t task_cache_start_y,\n                            int64_t task_cache_end_y) {\n                // This is a long and confusing function, but it's been laid out\n                // this way to help with performance on some intensive models.\n                // What it's doing is populating a cache of the original input\n                // image, after it's been bilinear resized and had its edges\n                // mirrored. This allows the following im2col code to access the\n                // transformed pixels from this cache, without having to\n                // repeatedly apply the expensive bilinear calculations as the\n                // same pixels are accessed by different patches.\n                // This is most effective when the stride is small and the\n                // filter size is large, since that's when pixels are reused\n                // most frequently as patches overlap.\n                for (int cache_y = task_cache_start_y;\n                     cache_y < task_cache_end_y; ++cache_y) {\n                  // We organize the cache as a series of rows, each containing\n                  // all the transformed pixels for a given line in the image.\n                  // This cache is big enough to hold at least a filter's height\n                  // worth of rows, but typically more, limited by the size of\n                  // the cache buffer.\n                  // We don't allocate an entire image's worth of rows though,\n                  // because we're trying to keep memory usage down, so as we\n                  // progress downwards through the im2col we periodically\n                  // refresh the cache so that the next lines that are needed\n                  // for that operation are always present.\n                  // Work out the parameters that remain constant across the\n                  // row we're calculating.\n                  PerCacheLineParameters<T1> line_params(\n                      CalculatePerCacheLineParameters<T1>(\n                          task_params.cache_height, cache_y,\n                          task_params.resize_cache,\n                          task_params.cache_line_width, task_params.input_width,\n                          task_params.input_depth, task_params.top_padding,\n                          task_params.pad_offset, task_params.resized_height,\n                          task_params.st, task_params.input_batch_start));\n                  // Iterate through the resize cache row we're filling in.\n                  for (int cache_x = task_params.cache_start_x;\n                       cache_x < task_params.cache_end_x; ++cache_x) {\n                    // Figure out what we need for the cache pixel we're\n                    // populating.\n                    PerCachePixelParameters<T1> pixel_params(\n                        CalculatePerCachePixelParameters<T1>(\n                            cache_x, task_params.cache_start_x,\n                            line_params.cache_line_start,\n                            task_params.input_depth, task_params.left_padding,\n                            task_params.pad_offset, task_params.resized_width,\n                            task_params.st));\n                    // If the access is off the left, right, top, or bottom of\n                    // the resized image, the conv padding means we should set\n                    // it to zero.\n                    if ((cache_x < 0) ||\n                        (cache_x >= task_params.padded_width) ||\n                        (cache_y < 0) ||\n                        (cache_y >= task_params.padded_height)) {\n                      std::fill_n(pixel_params.cache_line_pixel,\n                                  task_params.input_depth, T1(0));\n                    } else {\n                      // There are two different sampling strategies for\n                      // resizing. When using nearest, we can just do a\n                      // straight copy of the pixel closest to our sample point,\n                      // but bilinear requires a more complex calculation.\n                      if (SampleMode == NEAREST) {\n                        const T1* input_top_left_pixel =\n                            line_params.input_top_row_start +\n                            (pixel_params.left_x_index *\n                             task_params.input_depth);\n\n                        std::copy_n(input_top_left_pixel,\n                                    task_params.input_depth,\n                                    pixel_params.cache_line_pixel);\n                      } else {\n                        const SampleRect<T1> rect(\n                            line_params.input_top_row_start +\n                                (pixel_params.left_x_index *\n                                 task_params.input_depth),\n                            line_params.input_top_row_start +\n                                (pixel_params.right_x_index *\n                                 task_params.input_depth),\n                            line_params.input_bottom_row_start +\n                                (pixel_params.left_x_index *\n                                 task_params.input_depth),\n                            line_params.input_bottom_row_start +\n                                (pixel_params.right_x_index *\n                                 task_params.input_depth));\n                        for (int in_channel = 0;\n                             in_channel < task_params.input_depth;\n                             ++in_channel) {\n                          pixel_params.cache_line_pixel[in_channel] =\n                              rect.BilinearSample(in_channel,\n                                                  pixel_params.x_lerp,\n                                                  line_params.y_lerp);\n                        }\n                      }\n                    }\n                  }\n                }\n              });\n          end_cached_lines = cache_end_y;\n        }\n        for (int out_x = 0; out_x < output_width; ++out_x) {\n          const int in_x_origin = (out_x * stride_cols) - filter_left_offset;\n          const int patch_index = (batch * output_width * output_height) +\n                                  (out_y * output_width) + out_x;\n          const int patch_index_within_chunk = patch_index % patches_per_chunk;\n          T1* im2col_patch_start =\n              im2col_buffer + (patch_index_within_chunk * filter_value_count);\n          for (int filter_y = 0; filter_y < filter_height; ++filter_y) {\n            T1* im2col_row_start =\n                im2col_patch_start +\n                (filter_y * filter_width * task_params.input_depth);\n            const int conv_in_y = in_y_origin + filter_y;\n            int cache_index_y;\n            if (conv_in_y < 0) {\n              cache_index_y = task_params.cache_height +\n                              (conv_in_y % task_params.cache_height);\n            } else {\n              cache_index_y = conv_in_y % task_params.cache_height;\n            }\n            T1* cache_line_start =\n                task_params.resize_cache +\n                (cache_index_y * task_params.cache_line_width *\n                 task_params.input_depth);\n            T1* cache_filter_row_start =\n                cache_line_start + ((in_x_origin - task_params.cache_start_x) *\n                                    task_params.input_depth);\n            std::copy_n(cache_filter_row_start,\n                        (filter_width * task_params.input_depth),\n                        im2col_row_start);\n          }\n          const bool is_last_in_chunk =\n              (patch_index_within_chunk == (patches_per_chunk - 1));\n          const bool is_last_overall =\n              ((batch == (input_batches - 1)) &&\n               (out_y == (output_height - 1)) && (out_x == (output_width - 1)));\n          if (is_last_in_chunk || is_last_overall) {\n            // Now we've assembled a set of image patches into a matrix, apply\n            // a GEMM matrix multiply of the patches as rows, times the filter\n            // weights in columns, to get partial results in the output\n            // matrix.\n            const int how_many_patches = patch_index_within_chunk + 1;\n            const int m = how_many_patches;\n            const int n = filter_count;\n            const int k = filter_value_count;\n            const int lda = filter_value_count;\n            const int ldb = filter_count;\n            const int ldc = filter_count;\n            const size_t start_patch_index =\n                patch_index - (how_many_patches - 1);\n            T3* chunk_output_data =\n                output_data + (start_patch_index * filter_count);\n            TGemmFunctor gemm_functor;\n            gemm_functor(context, m, n, k, im2col_buffer, lda, filter_data, ldb,\n                         chunk_output_data, ldc);\n          }\n        }\n      }\n    }\n  }\n};\n\n}  // namespace\n\n// Implements a version of convolution with bilinear resizing and mirror padding\n// included.\ntemplate <class T, class TConvFunctor, bool DoResize>\nclass FusedResizeConv2DUsingGemmOp : public OpKernel {\n public:\n  explicit FusedResizeConv2DUsingGemmOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    if (DoResize) {\n      OP_REQUIRES_OK(context,\n                     context->GetAttr(\"resize_align_corners\", &align_corners_));\n    }\n    MirrorPadMode mode;\n    OP_REQUIRES_OK(context, context->GetAttr(\"mode\", &mode));\n\n    switch (mode) {\n      case MirrorPadMode::SYMMETRIC: {\n        offset_ = 0;\n        break;\n      }\n      case MirrorPadMode::REFLECT: {\n        offset_ = 1;\n        break;\n      }\n      default:\n        OP_REQUIRES(context, false,\n                    errors::InvalidArgument(\n                        \"mode must be either REFLECT or SYMMETRIC.\"));\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &strides_));\n    OP_REQUIRES(context, strides_.size() == 4,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\"));\n    const int64_t stride_n = GetTensorDim(strides_, FORMAT_NHWC, 'N');\n    const int64_t stride_c = GetTensorDim(strides_, FORMAT_NHWC, 'C');\n    OP_REQUIRES(\n        context, stride_n == 1 && stride_c == 1,\n        errors::InvalidArgument(\"Current implementation does not yet support \"\n                                \"strides in the batch and depth dimensions.\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // Input tensor is of the following dimensions:\n    // [ batch, in_rows, in_cols, in_depth ]\n    const Tensor& input = context->input(0);\n    OP_REQUIRES(context, (input.shape().num_elements() > 0),\n                errors::InvalidArgument(\"Input tensor can't be empty\"));\n\n    ImageResizerState st(false, false);\n    if (DoResize) {\n      st = ImageResizerState(align_corners_, false);\n      st.ValidateAndCalculateOutputSize(context);\n      if (!context->status().ok()) return;\n    } else {\n      // Set up the resize parameters to do no scaling at all.\n      st.batch_size = input.dim_size(0);\n      st.out_height = input.dim_size(1);\n      st.out_width = input.dim_size(2);\n      st.in_height = input.dim_size(1);\n      st.in_width = input.dim_size(2);\n      st.channels = input.dim_size(3);\n      st.height_scale = 1.0f;\n      st.width_scale = 1.0f;\n    }\n    TensorShape resized_shape;\n    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape(\n                                {input.dim_size(0), st.out_height, st.out_width,\n                                 input.dim_size(3)},\n                                &resized_shape));\n    int paddings_index;\n    int filter_index;\n    if (DoResize) {\n      paddings_index = 2;\n      filter_index = 3;\n    } else {\n      paddings_index = 1;\n      filter_index = 2;\n    }\n    const Tensor& paddings = context->input(paddings_index);\n\n    const int dims = resized_shape.dims();\n    OP_REQUIRES(\n        context,\n        TensorShapeUtils::IsMatrix(paddings.shape()) &&\n            paddings.dim_size(1) == 2,\n        errors::InvalidArgument(\"paddings must be a matrix with 2 columns: \",\n                                paddings.shape().DebugString()));\n    OP_REQUIRES(\n        context, dims == paddings.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs: \",\n            dims, \" \", paddings.shape().DebugString(), \" \",\n            resized_shape.DebugString()));\n    OP_REQUIRES(\n        context, dims == paddings.dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of paddings must be the rank of inputs: \",\n            dims, \" \", paddings.shape().DebugString(), \" \",\n            resized_shape.DebugString()));\n\n    OP_REQUIRES(\n        context, dims == 4,\n        errors::InvalidArgument(\n            \"Fused mirror padding only supports four-dimensional inputs, but \",\n            dims, \" requested\"));\n\n    // Compute the shape of the output tensor, and allocate it.\n    TensorShape padded_shape;\n    TTypes<int32>::ConstMatrix paddings_matrix = paddings.matrix<int32>();\n    for (int d = 0; d < dims; ++d) {\n      const int32_t before =\n          paddings_matrix(d, 0);  // Pad before existing elements.\n      const int32_t after =\n          paddings_matrix(d, 1);  // Pad after existing elements.\n      OP_REQUIRES(context, before >= 0 && after >= 0,\n                  errors::InvalidArgument(\n                      \"paddings must be non-negative: \", before, \" \", after));\n      if (offset_ == 0) {  // SYMMETRIC mode.\n        OP_REQUIRES(\n            context,\n            before <= resized_shape.dim_size(d) &&\n                after <= resized_shape.dim_size(d),\n            errors::InvalidArgument(\"paddings must be no greater \"\n                                    \"than the dimension size: \",\n                                    before, \", \", after, \" greater than \",\n                                    resized_shape.dim_size(d)));\n      } else if (offset_ == 1) {  // REFLECT mode.\n        OP_REQUIRES(\n            context,\n            before < resized_shape.dim_size(d) &&\n                after < resized_shape.dim_size(d),\n            errors::InvalidArgument(\"paddings must be less than\"\n                                    \" the dimension size: \",\n                                    before, \", \", after, \" not less than \",\n                                    resized_shape.dim_size(d)));\n      }\n      padded_shape.AddDim(before + resized_shape.dim_size(d) + after);\n    }\n\n    OP_REQUIRES(\n        context, ((paddings_matrix(0, 0) == 0) && (paddings_matrix(0, 1) == 0)),\n        errors::InvalidArgument(\n            \"Fused mirror padding only support spatial padding, not batches: \",\n            paddings.DebugString()));\n    OP_REQUIRES(\n        context, ((paddings_matrix(3, 0) == 0) && (paddings_matrix(3, 1) == 0)),\n        errors::InvalidArgument(\n            \"Fused mirror padding only support spatial padding, not channels: \",\n            paddings.DebugString()));\n    const int32_t top_padding = paddings_matrix(1, 0);\n    const int32_t bottom_padding = paddings_matrix(1, 1);\n    const int32_t left_padding = paddings_matrix(2, 0);\n    const int32_t right_padding = paddings_matrix(2, 1);\n\n    // Input filter is of the following dimensions:\n    // [ filter_rows, filter_cols, in_depth, out_depth]\n    const Tensor& filter = context->input(filter_index);\n\n    // For 2D convolution, there should be 4 dimensions.\n    OP_REQUIRES(context, padded_shape.dims() == 4,\n                errors::InvalidArgument(\"input must be 4-dimensional\",\n                                        padded_shape.DebugString()));\n    OP_REQUIRES(context, filter.dims() == 4,\n                errors::InvalidArgument(\"filter must be 4-dimensional: \",\n                                        filter.shape().DebugString()));\n\n    // We only check the first three dims, since the depth is accessed as an\n    // int64 below.\n    for (int i = 0; i < 3; i++) {\n      OP_REQUIRES(\n          context,\n          FastBoundsCheck(filter.dim_size(i), std::numeric_limits<int>::max()),\n          errors::InvalidArgument(\"filter too large\"));\n    }\n\n    // The last dimension for input is in_depth. It must be the same as the\n    // filter's in_depth.\n    const int64_t in_depth = padded_shape.dim_size(3);\n    OP_REQUIRES(context, in_depth == filter.dim_size(2),\n                errors::InvalidArgument(\n                    \"input and filter must have the same depth: \", in_depth,\n                    \" vs \", filter.dim_size(2)));\n\n    // The last dimension for filter is out_depth.\n    const int out_depth = static_cast<int>(filter.dim_size(3));\n\n    // The second dimension for input is rows/height.\n    // The first dimension for filter is rows/height.\n    const int64_t padded_rows_raw = padded_shape.dim_size(1);\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(padded_rows_raw, std::numeric_limits<int>::max()),\n        errors::InvalidArgument(\"Input rows too large\"));\n    const int padded_rows = static_cast<int>(padded_rows_raw);\n    const int filter_rows = static_cast<int>(filter.dim_size(0));\n    const int resized_rows = static_cast<int>(resized_shape.dim_size(1));\n\n    // The third dimension for input is columns/width.\n    // The second dimension for filter is columns/width.\n    const int64_t padded_cols_raw = padded_shape.dim_size(2);\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(padded_cols_raw, std::numeric_limits<int>::max()),\n        errors::InvalidArgument(\"Input cols too large\"));\n    const int padded_cols = static_cast<int>(padded_cols_raw);\n    const int filter_cols = static_cast<int>(filter.dim_size(1));\n    const int resized_cols = static_cast<int>(resized_shape.dim_size(2));\n\n    // The first dimension for input is batch.\n    const int64_t batch_raw = padded_shape.dim_size(0);\n    OP_REQUIRES(context,\n                FastBoundsCheck(batch_raw, std::numeric_limits<int>::max()),\n                errors::InvalidArgument(\"batch is too large\"));\n    const int batch = static_cast<int>(batch_raw);\n\n    // For now we take the stride from the second and third dimensions only (we\n    // do not support striding on the batch or depth dimension).\n    const int stride_rows = GetTensorDim(strides_, FORMAT_NHWC, 'H');\n    const int stride_cols = GetTensorDim(strides_, FORMAT_NHWC, 'W');\n\n    int64_t out_rows = 0, out_cols = 0, pad_rows = 0, pad_cols = 0;\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(padded_rows, filter_rows, stride_rows,\n                                         padding_, &out_rows, &pad_rows));\n    OP_REQUIRES_OK(context,\n                   GetWindowedOutputSize(padded_cols, filter_cols, stride_cols,\n                                         padding_, &out_cols, &pad_cols));\n    TensorShape out_shape =\n        ShapeFromFormat(FORMAT_NHWC, batch, out_rows, out_cols, out_depth);\n    OP_REQUIRES(context, (out_shape.num_elements() > 0),\n                errors::InvalidArgument(\"Output tensor can't be empty\"));\n\n    // Output tensor is of the following dimensions:\n    // [ in_batch, out_rows, out_cols, out_depth ]\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\n    VLOG(2) << \"FusedConv2D: \" << name() << \", in_depth = \" << in_depth\n            << \", padded_cols = \" << padded_cols\n            << \", resized_cols = \" << resized_cols\n            << \", filter_cols = \" << filter_cols\n            << \", padded_rows = \" << padded_rows\n            << \", resized_rows = \" << resized_rows\n            << \", filter_rows = \" << filter_rows\n            << \", stride_rows = \" << stride_rows\n            << \", stride_cols = \" << stride_cols\n            << \", out_depth = \" << out_depth << \", DoResize=\" << DoResize;\n\n    // If there is nothing to compute, return.\n    if (out_shape.num_elements() == 0) {\n      return;\n    }\n    TConvFunctor conv_functor;\n    conv_functor(context, input, batch, resized_rows, resized_cols, padded_rows,\n                 padded_cols, in_depth, filter.flat<T>().data(), filter_rows,\n                 filter_cols, out_depth, stride_rows, stride_cols, padding_,\n                 output->flat<T>().data(), out_rows, out_cols, st, top_padding,\n                 bottom_padding, left_padding, right_padding, offset_);\n  }\n\n private:\n  std::vector<int32> strides_;\n  Padding padding_;\n  bool align_corners_;\n  int offset_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(FusedResizeConv2DUsingGemmOp);\n};\n\n#define REGISTER_FUSED(T)                                                 \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"FusedResizeAndPadConv2D\")                                     \\\n          .Device(DEVICE_CPU)                                             \\\n          .TypeConstraint<T>(\"T\"),                                        \\\n      FusedResizeConv2DUsingGemmOp<                                       \\\n          T,                                                              \\\n          FusedResizeAndPadConvFunctor<T, T, T, FastGemmFunctor<T, T, T>, \\\n                                       BILINEAR>,                         \\\n          true>);\n\nTF_CALL_half(REGISTER_FUSED);\nTF_CALL_float(REGISTER_FUSED);\nTF_CALL_double(REGISTER_FUSED);\n\n#define REGISTER_PAD_ONLY_FUSED(T)                                        \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"FusedPadConv2D\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"),   \\\n      FusedResizeConv2DUsingGemmOp<                                       \\\n          T,                                                              \\\n          FusedResizeAndPadConvFunctor<T, T, T, FastGemmFunctor<T, T, T>, \\\n                                       NEAREST>,                          \\\n          false>);\n\nTF_CALL_half(REGISTER_PAD_ONLY_FUSED);\nTF_CALL_float(REGISTER_PAD_ONLY_FUSED);\nTF_CALL_double(REGISTER_PAD_ONLY_FUSED);\n\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <algorithm>\n#include <cmath>\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/kernel_shape_util.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/lib/math/math_util.h\"\n#include \"tensorflow/core/util/mirror_pad_mode.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nnamespace {\n\nStatus FractionalPoolShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  std::vector<float> pooling_ratio;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"pooling_ratio\", &pooling_ratio));\n  if (pooling_ratio.size() != 4) {\n    return errors::InvalidArgument(\n        \"pooling_ratio field must specify 4 dimensions\");\n  }\n  std::vector<DimensionHandle> output_dims;\n  for (int i = 0; i < 4; ++i) {\n    DimensionHandle d = c->Dim(input, i);\n    if (c->ValueKnown(d)) {\n      // This must match the same logic in the kernel function in\n      // core/kernels/fractional_max_pool_op.cc.\n      auto val =\n          static_cast<int64_t>(std::floor(c->Value(d) / pooling_ratio[i]));\n      if (val < 0) {\n        return errors::InvalidArgument(\"Size computed for dim \", i,\n                                       \" is negative: \", val);\n      }\n      output_dims.push_back(c->MakeDim(val));\n    } else {\n      output_dims.push_back(c->UnknownDim());\n    }\n  }\n\n  c->set_output(0, c->MakeShape(output_dims));\n  c->set_output(1, c->Vector(output_dims[1]));\n  c->set_output(2, c->Vector(output_dims[2]));\n  return OkStatus();\n}\n\n}  // namespace\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolShape);\n\nREGISTER_OP(\"AvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchNormWithGlobalNormalization\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"beta: T\")\n    .Input(\"gamma: T\")\n    .Output(\"result: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"BatchNormWithGlobalNormalizationGrad\")\n    .Input(\"t: T\")\n    .Input(\"m: T\")\n    .Input(\"v: T\")\n    .Input(\"gamma: T\")\n    .Input(\"backprop: T\")\n    .Output(\"dx: T\")\n    .Output(\"dm: T\")\n    .Output(\"dv: T\")\n    .Output(\"db: T\")\n    .Output(\"dg: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .Deprecated(9, \"Use tf.nn.batch_normalization()\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n      TF_RETURN_IF_ERROR(\n          c->Merge(input, c->input(4), &input));  // with backprop\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 4; ++i) {  // covers m, v, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle dx;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &dx));\n      c->set_output(0, dx);\n\n      ShapeHandle vector_shape = c->Vector(last_dim);\n      c->set_output(1, vector_shape);\n      c->set_output(2, vector_shape);\n      c->set_output(3, vector_shape);\n      c->set_output(4, vector_shape);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"FusedBatchNormV3\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {bfloat16, float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormV3Shape);\n\nREGISTER_OP(\"_FusedBatchNormEx\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"side_input: num_side_inputs * T\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"reserve_space_3: U\")\n    .Attr(\"T: {half, float, bfloat16}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNorm operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n\nREGISTER_OP(\"FusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Attr(\"T: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"FusedBatchNormGradV3\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_FusedBatchNormGradEx\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"reserve_space_3: U\")\n    .Input(\"offset: float\")\n    .Input(\"y: T\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"reserve_space_5: U\")\n    .Output(\"side_input_backprop: num_side_inputs * T\")\n    .Attr(\"T: {half, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"num_side_inputs: int >= 0 = 0\")\n    .Attr(\"activation_mode: string = \\\"Identity\\\"\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradExShape)\n    .Doc(R\"doc(\nInternal FusedBatchNormGrad operation: reserved for internal use.\n\nDo not invoke this operator directly in Python. A fusion optimization is\nexpected to create these operators.\n)doc\");\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAdd\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddGrad\")\n    .Attr(\"T: numbertype\")\n    .Input(\"out_backprop: T\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddGradShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BiasAddV1\")\n    .Attr(\"T: numbertype\")\n    .Input(\"value: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BiasAddShape);\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding);\n\nREGISTER_OP(\"Conv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double, int32}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropInputShape);\n\n// TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n// more general string attribute ('kernel_impl'?) that can be used to\n// select among several possible implementations.\nREGISTER_OP(\"Conv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: TArgs\")\n    .Input(\"host_args : num_host_args * float\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, int8, qint8}\")\n    .Attr(\"TArgs: list(type)\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"num_host_args: int >= 0 =0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: { 'NHWC', 'NCHW', 'NCHW_VECT_C' } = 'NHWC'\")\n    .Attr(\"filter_format: {'HWIO', 'OIHW', 'OIHW_VECT_I'} = 'HWIO'\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nPerforms a convolution followed by a specified series of operations.\n\nThe inputs to the convolution are `input` and `filter`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [X] and [X,A], where X is one of\n{\"BiasAdd\",\"FusedBatchNorm\"} and A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to op X is the Conv2D result, and the additional input(s) to X\nare specified by `args`.\n* If there is an op A specified, the output of op X is the input to op A, and op\nA produces the _FusedConv2D output. Otherwise, op X produces the _FusedConv2D\noutput.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is expected to\ncreate these operators.\n)doc\");\n\nnamespace {\n\nStatus CommonFusedConvCalculations(InferenceContext* c, bool has_resize) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n  ShapeHandle resized = input;\n  int paddings_index = 1;\n  int filter_index = 2;\n  if (has_resize) {\n    paddings_index = 2;\n    filter_index = 3;\n\n    ShapeHandle unused_size;\n    TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->Vector(2), &unused_size));\n\n    const Tensor* size = c->input_tensor(1);\n    DimensionHandle new_height = c->UnknownDim();\n    DimensionHandle new_width = c->UnknownDim();\n    if (size != nullptr) {\n      new_height = c->MakeDim(size->flat<int32>()(0));\n      new_width = c->MakeDim(size->flat<int32>()(1));\n    }\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 1, new_height, &resized));\n    TF_RETURN_IF_ERROR(c->ReplaceDim(resized, 2, new_width, &resized));\n  }\n\n  ShapeHandle paddings;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(paddings_index), 2, &paddings));\n  TF_RETURN_IF_ERROR(\n      c->WithRank(resized, c->Value(c->Dim(paddings, 0)), &resized));\n  TF_RETURN_IF_ERROR(\n      c->Merge(paddings, c->Matrix(c->Rank(resized), 2), &paddings));\n\n  const Tensor* paddings_t = c->input_tensor(paddings_index);\n  ShapeHandle padded;\n  if (paddings_t != nullptr) {\n    std::vector<DimensionHandle> output_dims;\n    for (int i = 0; i < 4; ++i) {\n      DimensionHandle dim = c->Dim(resized, i);\n      int64_t p0 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 0));\n      int64_t p1 = static_cast<int64_t>(paddings_t->matrix<int32>()(i, 1));\n      if (p0 < 0 || p1 < 0) {\n        return errors::InvalidArgument(\"Paddings must be non-negative\");\n      }\n\n      TF_RETURN_IF_ERROR(c->Add(dim, p0 + p1, &dim));\n      output_dims.push_back(dim);\n    }\n    padded = c->MakeShape(output_dims);\n  } else {\n    padded = c->UnknownShapeOfRank(4);\n  }\n\n  // Work out the convolution's effect with 'padded' as the input.\n  ShapeHandle filter;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(filter_index), 4, &filter));\n  std::vector<int32> strides;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n  if (strides.size() != 4) {\n    return errors::InvalidArgument(\n        \"Operation requires the stride attribute to contain 4 values, but \",\n        \"got: \", strides.size());\n  }\n\n  int32_t stride_rows = strides[1];\n  int32_t stride_cols = strides[2];\n\n  DimensionHandle batch_size_dim = c->Dim(padded, 0);\n  DimensionHandle in_rows_dim = c->Dim(padded, 1);\n  DimensionHandle in_cols_dim = c->Dim(padded, 2);\n  DimensionHandle filter_rows_dim = c->Dim(filter, 0);\n  DimensionHandle filter_cols_dim = c->Dim(filter, 1);\n  DimensionHandle output_depth_dim = c->Dim(filter, 3);\n\n  DimensionHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->Dim(padded, 3), c->Dim(filter, 2), &unused));\n\n  Padding padding;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n  DimensionHandle output_rows, output_cols;\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_rows_dim, filter_rows_dim, stride_rows, padding, &output_rows));\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\n      c, in_cols_dim, filter_cols_dim, stride_cols, padding, &output_cols));\n\n  ShapeHandle output_shape = c->MakeShape(\n      {batch_size_dim, output_rows, output_cols, output_depth_dim});\n  c->set_output(0, output_shape);\n  return OkStatus();\n}\n\n}  // namespace\n\nREGISTER_OP(\"DataFormatDimMap\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"DataFormatVecPermute\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .Attr(\"src_format: string = 'NHWC'\")\n    .Attr(\"dst_format: string = 'NCHW'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"FusedResizeAndPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"size: int32\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"resize_align_corners: bool = false\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/true);\n    });\n\nREGISTER_OP(\"FusedPadConv2D\")\n    .Input(\"input: T\")\n    .Input(\"paddings: int32\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(GetMirrorPadModeAttrString())\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      return CommonFusedConvCalculations(c, /*has_resize=*/false);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"DepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"DepthwiseConv2dNativeBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"_FusedDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ------------------------------------ //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ----------------------------------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // ---------------------------------------------------------------------- //\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Conv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape);\n\nREGISTER_OP(\"Conv3DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropInputV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 5);\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Deprecated(10, \"Use Conv3DBackpropFilterV2\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 5, &out));\n      c->set_output(0, out);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Conv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"AvgPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"AvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool3D\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(shape_inference::Pool3DShape);\n\nREGISTER_OP(\"MaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape);\n\nREGISTER_OP(\"MaxPool3DGradGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"ksize: list(int) >= 5 \")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Pool3DShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"L2Loss\")\n    .Input(\"t: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LRN\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"LRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Output(\"output: T\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MaxPool\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::MaxPoolShapeWithExplicitPadding);\n\nREGISTER_OP(\"MaxPoolV2\")\n    .Attr(\n        \"T: {half, bfloat16, float, double, int32, int64, uint8, int16, int8, \"\n        \"uint16, qint8} = DT_FLOAT\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"data_format: {'NHWC', 'NCHW', 'NCHW_VECT_C'} = 'NHWC'\")\n    .Input(\"input: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 3));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\nREGISTER_OP(\"MaxPoolGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape);\n\n// TODO(b/150813181): Implement explicit padding.\nREGISTER_OP(\"MaxPoolGradGrad\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradGradV2\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n    .Input(\"ksize: int32\")\n    .Input(\"strides: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolV2Shape(c, 5));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(2), &unused));\n      // Validate 'orig_output' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(1), c->output(0), &unused));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"Targmax: {int32, int64} = DT_INT64\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Output(\"argmax: Targmax\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      c->set_output(1, c->output(0));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"MaxPoolGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    });\n\nREGISTER_OP(\"MaxPoolGradGradWithArgmax\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"include_batch_in_index: bool = false\")\n    .Attr(\"Targmax: {int32, int64}\")\n    .Input(\"input: T\")\n    .Input(\"grad: T\")\n    .Input(\"argmax: Targmax\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      // Validate 'orig_input' is the same shape as 'grad'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(0), c->input(1), &unused));\n      // Validate 'argmax' is same shape as 'output'\n      TF_RETURN_IF_ERROR(c->Merge(c->input(2), c->output(0), &unused));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Dilation2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n      ShapeHandle filter_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &filter_shape));\n\n      std::vector<int32> strides;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"strides\", &strides));\n      if (strides.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the stride attribute to contain 4 values, but \"\n            \"got: \",\n            strides.size());\n      }\n\n      std::vector<int32> rates;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rates\", &rates));\n      if (rates.size() != 4) {\n        return errors::InvalidArgument(\n            \"Dilation2D requires the rates attribute to contain 4 values, but \"\n            \"got: \",\n            rates.size());\n      }\n\n      int32_t stride_rows = strides[1];\n      int32_t stride_cols = strides[2];\n\n      int32_t rate_rows = rates[1];\n      int32_t rate_cols = rates[2];\n\n      DimensionHandle batch_size_dim = c->Dim(input_shape, 0);\n      DimensionHandle in_rows_dim = c->Dim(input_shape, 1);\n      DimensionHandle in_cols_dim = c->Dim(input_shape, 2);\n      DimensionHandle filter_rows_dim = c->Dim(filter_shape, 0);\n      DimensionHandle filter_cols_dim = c->Dim(filter_shape, 1);\n      DimensionHandle output_depth_dim = c->Dim(filter_shape, 2);\n\n      if (!c->ValueKnown(in_rows_dim) || !c->ValueKnown(in_cols_dim) ||\n          !c->ValueKnown(filter_rows_dim) || !c->ValueKnown(filter_cols_dim)) {\n        ShapeHandle output_shape =\n            c->MakeShape({batch_size_dim, InferenceContext::kUnknownDim,\n                          InferenceContext::kUnknownDim, output_depth_dim});\n        c->set_output(0, output_shape);\n        return OkStatus();\n      }\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(input_shape, 3), output_depth_dim, &unused));\n\n      auto in_rows = c->Value(in_rows_dim);\n      auto in_cols = c->Value(in_cols_dim);\n      auto filter_rows = c->Value(filter_rows_dim);\n      auto filter_cols = c->Value(filter_cols_dim);\n      auto filter_rows_eff = filter_rows + (filter_rows - 1) * (rate_rows - 1);\n      auto filter_cols_eff = filter_cols + (filter_cols - 1) * (rate_cols - 1);\n\n      Padding padding;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"padding\", &padding));\n\n      int64_t output_rows, output_cols;\n      int64_t padding_before, padding_after;\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_rows, filter_rows_eff, stride_rows, padding, &output_rows,\n          &padding_before, &padding_after));\n      TF_RETURN_IF_ERROR(GetWindowedOutputSizeVerbose(\n          in_cols, filter_cols_eff, stride_cols, padding, &output_cols,\n          &padding_before, &padding_after));\n\n      ShapeHandle output_shape = c->MakeShape(\n          {batch_size_dim, output_rows, output_cols, output_depth_dim});\n      c->set_output(0, output_shape);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"Dilation2DBackpropInput\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"in_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Dilation2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"filter_backprop: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"rates: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(1));\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Relu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {realnumbertype, qint8}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Relu6\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Relu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"LeakyRelu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"LeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"alpha: float = 0.2\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Elu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"EluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Selu\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SeluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"outputs: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softplus\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftplusGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\nREGISTER_OP(\"Softsign\")\n    .Input(\"features: T\")\n    .Output(\"activations: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"SoftsignGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Output(\"backprops: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Softmax\")\n    .Input(\"logits: T\")\n    .Output(\"softmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogSoftmax\")\n    .Input(\"logits: T\")\n    .Output(\"logsoftmax: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"SoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: T\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      if (c->WithRank(c->input(0), 2, &input) == OkStatus() &&\n          c->Merge(input, c->input(1), &input) == OkStatus()) {\n        DimensionHandle batch_size = c->Dim(input, 0);\n        c->set_output(0, c->Vector(batch_size));\n        c->set_output(1, input);\n        return OkStatus();\n      }\n      TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFn(c, 1));\n\n      if (!c->RankKnown(c->output(1))) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank is unknown.\");\n      }\n\n      if (c->Rank(c->output(1)) != 2) {\n        return errors::InvalidArgument(\n            \"Shape must be broadcasted with rank 2, but is rank \",\n            c->Rank(c->output(1)));\n      }\n      DimensionHandle batch_size = c->Dim(c->output(1), 0);\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nREGISTER_OP(\"SparseSoftmaxCrossEntropyWithLogits\")\n    .Input(\"features: T\")\n    .Input(\"labels: Tlabels\")\n    .Output(\"loss: T\")\n    .Output(\"backprop: T\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"Tlabels: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle features;\n      ShapeHandle labels;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &features));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &labels));\n\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(features, 0), c->Dim(labels, 0), &batch_size));\n      TF_RETURN_IF_ERROR(c->ReplaceDim(features, 0, batch_size, &features));\n\n      c->set_output(0, c->Vector(batch_size));\n      c->set_output(1, features);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"InTopK\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"k: int\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\n// This is the same as `InTopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"InTopKV2\")\n    .Input(\"predictions: float\")\n    .Input(\"targets: T\")\n    .Input(\"k: T\")\n    .Output(\"precision: bool\")\n    .Attr(\"T: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle predictions;\n      ShapeHandle targets;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &predictions));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &targets));\n      DimensionHandle batch_size;\n      TF_RETURN_IF_ERROR(\n          c->Merge(c->Dim(predictions, 0), c->Dim(targets, 0), &batch_size));\n      c->set_output(0, c->Vector(batch_size));\n      return OkStatus();\n    });\n\nnamespace {\n\nStatus TopKShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n  // Get the k value, either from input tensor or attribute.\n  DimensionHandle k_dim;\n  if (c->num_inputs() >= 2) {\n    TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &k_dim));\n  } else {\n    int32_t k;\n    TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n    if (k < 0) {\n      return errors::InvalidArgument(\"Need k >= 0, got \", k);\n    }\n    k_dim = c->MakeDim(k);\n  }\n\n  DimensionHandle last_dim = c->Dim(input, -1);\n  if (c->ValueKnown(last_dim) && c->ValueKnown(k_dim) &&\n      c->Value(last_dim) < c->Value(k_dim)) {\n    return errors::InvalidArgument(\n        \"input must have last dimension >= k = \", c->Value(k_dim), \" but is \",\n        c->Value(last_dim));\n  }\n\n  // Replace last_dim with k_dim.\n  ShapeHandle s;\n  TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n  TF_RETURN_IF_ERROR(c->Concatenate(s, c->Vector(k_dim), &s));\n  c->set_output(0, s);\n  c->set_output(1, s);\n  return OkStatus();\n}\n\n// Utility functions for ApproxTopKShape.\n// It is not easy to link xla/client/lib into the tensorflow core lib, so we\n// have to replicate the logic.\n// LINT.IfChange\ninline uint32_t log2_floor(uint64_t value) {\n  return value == 0 ? 0 : Log2Floor(value);\n}\n\ninline uint32_t log2_ceil(uint64_t value) {\n  return value == 0 ? 0 : Log2Ceiling(value);\n}\n\nStatus ApproxTopKShape(shape_inference::InferenceContext* c) {\n  int64_t k;\n  int64_t reduction_dimension;\n  float recall_target;\n  int64_t reduction_input_size_override;\n  bool aggregate_to_topk;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"k\", &k));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_dimension\", &reduction_dimension));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"recall_target\", &recall_target));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"reduction_input_size_override\",\n                                &reduction_input_size_override));\n  TF_RETURN_IF_ERROR(c->GetAttr(\"aggregate_to_topk\", &aggregate_to_topk));\n  ShapeHandle input_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input_shape));\n  if (reduction_dimension < 0) {\n    // Reverse index\n    reduction_dimension += c->Rank(input_shape);\n  }\n  int64_t reduction_dim_value =\n      c->Value(c->Dim(input_shape, reduction_dimension));\n\n  if (reduction_dim_value < k) {\n    return errors::InvalidArgument(\"input must have last dimension >= k = \", k,\n                                   \" but was \", reduction_dim_value);\n  }\n\n  int64_t output_dim_value = [&] {\n    if (aggregate_to_topk) {\n      return k;\n    }\n    int64_t tpu_tiling = c->Rank(input_shape) == 1 ? 1024 : 128;\n    if (reduction_dim_value <= tpu_tiling || recall_target == 1.0) {\n      return reduction_dim_value;\n    }\n    if (k == 1) {\n      return tpu_tiling;\n    }\n    uint64_t logical_input_size = reduction_input_size_override >= 0\n                                      ? reduction_input_size_override\n                                      : reduction_dim_value;\n    uint64_t m = std::min<uint64_t>(\n        std::max<uint64_t>(\n            static_cast<uint64_t>((1.0 - k) /\n                                  std::log(static_cast<double>(recall_target))),\n            tpu_tiling),\n        reduction_dim_value);\n    uint32_t log2_reduction = log2_floor(logical_input_size / m);\n    if (log2_reduction == 0) {\n      return reduction_dim_value;\n    }\n    log2_reduction = std::min<uint32_t>(\n        log2_reduction, log2_ceil(reduction_dim_value / tpu_tiling));\n    return tensorflow::MathUtil::CeilOfRatio<int64_t>(\n               tensorflow::MathUtil::CeilOfRatio<int64_t>(reduction_dim_value,\n                                                          tpu_tiling),\n               (1 << log2_reduction)) *\n           tpu_tiling;\n  }();\n\n  auto output_dim = c->MakeDim(output_dim_value);\n\n  ShapeHandle output_shape;\n  TF_RETURN_IF_ERROR(c->ReplaceDim(input_shape, reduction_dimension, output_dim,\n                                   &output_shape));\n  c->set_output(0, output_shape);\n  c->set_output(1, output_shape);\n  return OkStatus();\n}\n// LINT.ThenChange(//tensorflow/compiler/xla/client/lib/approx_topk_shape.cc)\n\n}  // namespace\n\nREGISTER_OP(\"TopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .Deprecated(7, \"Use TopKV2 instead\")\n    .SetShapeFn(TopKShapeFn);\n\n// This is the same as `TopK`, but takes `k` as in input rather than an attr.\nREGISTER_OP(\"TopKV2\")\n    .Input(\"input: T\")\n    .Input(\"k: int32\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"sorted: bool = true\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(TopKShapeFn);\n\nREGISTER_OP(\"ApproxTopK\")\n    .Input(\"input: T\")\n    .Output(\"values: T\")\n    .Output(\"indices: int32\")\n    .Attr(\"k: int >= 0\")\n    .Attr(\"reduction_dimension: int = -1\")\n    .Attr(\"recall_target: float = 0.95\")\n    .Attr(\"is_max_k: bool = true\")\n    .Attr(\"reduction_input_size_override: int = -1\")\n    .Attr(\"aggregate_to_topk: bool = true\")\n    .Attr(\"T: {half, bfloat16, float}\")\n    .SetShapeFn(ApproxTopKShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"NthElement\")\n    .Input(\"input: T\")\n    .Input(\"n: int32\")\n    .Output(\"values: T\")\n    .Attr(\"reverse: bool = false\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &input));\n\n      // Get the n value from input tensor, and make sure which is a scalar.\n      DimensionHandle n_dim;\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &n_dim));\n\n      // The last dimension of input tensor must be greater than N.\n      DimensionHandle last_dim = c->Dim(input, -1);\n      if (c->ValueKnown(last_dim) && c->ValueKnown(n_dim) &&\n          c->Value(last_dim) <= c->Value(n_dim)) {\n        return errors::InvalidArgument(\n            \"Input must have last dimension > n = \", c->Value(n_dim),\n            \" but is \", c->Value(last_dim));\n      }\n\n      // Reduce last_dim for output tensor\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->Subshape(input, 0, -1, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalMaxPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalMaxPoolGrad\")\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRank(c, 4);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"FractionalAvgPool\")\n    .Input(\"value: T\")\n    .Output(\"output: T\")\n    .Output(\"row_pooling_sequence: int64\")\n    .Output(\"col_pooling_sequence: int64\")\n    .Attr(\"pooling_ratio: list(float) >=4\")\n    .Attr(\"pseudo_random: bool = false\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"deterministic: bool = false\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn(FractionalPoolShapeFn);\n\nREGISTER_OP(\"FractionalAvgPoolGrad\")\n    .Input(\"orig_input_tensor_shape: int64\")\n    .Input(\"out_backprop: T\")\n    .Input(\"row_pooling_sequence: int64\")\n    .Input(\"col_pooling_sequence: int64\")\n    .Output(\"output: T\")\n    .Attr(\"overlapping: bool = false\")\n    .Attr(\"T: {float, double, int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      if (c->input_tensor(0) != nullptr) {\n        ShapeHandle out;\n        TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &out));\n        c->set_output(0, out);\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(4));\n      }\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedAvgPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn(shape_inference::QuantizedAvgPoolShape);\n\nREGISTER_OP(\"QuantizedBiasAdd\")\n    .Input(\"input: T1\")\n    .Input(\"bias: T2\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_bias: float\")\n    .Input(\"max_bias: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BiasAddShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::QuantizedConv2DShape);\n\nREGISTER_OP(\"QuantizedMaxPool\")\n    .Input(\"input: T\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Output(\"output: T\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"T: quantizedtype\")\n    .Attr(\"ksize: list(int)\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MaxPoolShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedRelu6\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedReluX\")\n    .Input(\"features: Tinput\")\n    .Input(\"max_value: float\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedBatchNormWithGlobalNormalization\")\n    .Input(\"t: Tinput\")\n    .Input(\"t_min: float\")\n    .Input(\"t_max: float\")\n    .Input(\"m: Tinput\")\n    .Input(\"m_min: float\")\n    .Input(\"m_max: float\")\n    .Input(\"v: Tinput\")\n    .Input(\"v_min: float\")\n    .Input(\"v_max: float\")\n    .Input(\"beta: Tinput\")\n    .Input(\"beta_min: float\")\n    .Input(\"beta_max: float\")\n    .Input(\"gamma: Tinput\")\n    .Input(\"gamma_min: float\")\n    .Input(\"gamma_max: float\")\n    .Output(\"result: out_type\")\n    .Output(\"result_min: float\")\n    .Output(\"result_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i * 3), 1, &vec));\n        TF_RETURN_IF_ERROR(c->Merge(last_dim, c->Dim(vec, 0), &last_dim));\n      }\n\n      ShapeHandle out;\n      TF_RETURN_IF_ERROR(c->ReplaceDim(input, 3, last_dim, &out));\n      c->set_output(0, out);\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n\n      return OkStatus();\n    });\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklDepthwiseConv2dNative\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {half, bfloat16, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShapeWithExplicitPadding);\n\nREGISTER_OP(\"_MklConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D operator. Uses MKL DNN APIs to perform 2D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\n    MKL version of Conv2D operator for Eager mode. Uses MKL DNN APIs to perform 2D convolution.\n\n    NOTE Do not invoke this operator directly in Python. Eager Op rewrite is\n    expected to invoke these operators.\n    )doc\");\n\nREGISTER_OP(\"__MklDummyConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2D and BiasAdd operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Conv2D and BiasAdd.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"bias: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_bias: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DShapeWithExplicitPadding)\n    .Doc(R\"doc(\nMKL version of Conv2D and BiasAdd operator. Uses MKL DNN APIs to perform\n2D convolution and add Bias to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nDummy node that enables fusing Pad and Conv2D operator for MKL. This node\ndoes not perform anything. It is just created as an intermediate output of\nmerging Pad and Conv2D.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklPadWithConv2D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"paddings: Tpaddings\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_paddings: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"Tpaddings: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::Conv2DShape)\n    .Doc(R\"doc(\nMKL version of Pad and Conv2D operator. Uses MKL DNN APIs to perform\nPad and 2D convolution to the output of convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropFilter\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilter for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Eager Op rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"__MklDummyConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input_shape;\n      // Fetch the data_format attribute, which may not exist.\n      string data_format;\n      Status s = c->GetAttr(\"data_format\", &data_format);\n\n      if (s.ok() && data_format == \"NCHW\") {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -3)));\n      } else {\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input_shape));\n        c->set_output(1, c->Vector(c->Dim(input_shape, -1)));\n      }\n      ShapeHandle sh;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &sh));\n      TF_RETURN_IF_ERROR(c->WithRank(sh, 4, &sh));\n      c->set_output(0, sh);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nDummy node that enables fusing Conv2DBackpropFilter and BiasAddGrad operator\nfor MKL. This node does not perform anything. It is just created as an\nintermediate output of merging Conv2DBackpropFilter and BiasAddGrad.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv2DBackpropFilterWithBias\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"bias_grad: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_bias_grad: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv2DBackpropFilterWithBiasShape)\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropFilterWithBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#ifdef INTEL_MKL_ML_ONLY\nREGISTER_OP(\"_MklConv2DWithBiasBackpropBias\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {half, float, double}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Doc(R\"doc(\nMKL version of Conv2DBackpropBias. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the bias.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n#endif\n\nREGISTER_OP(\"_MklConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklNativeConv2DBackpropInput\")\n    .Input(\"input_sizes: int32\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(\"use_cudnn_on_gpu: bool = true\")\n    .Attr(GetPaddingAttrStringWithExplicit())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 4, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution2D backward input for Eager mode. Uses MKL DNN APIs\nto compute the gradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Eager op rewrite is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3D\")\n    .Input(\"input: T\")\n    .Input(\"filter: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Output(\"output: T\")\n    .Output(\"filter_output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_filter_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"is_filter_const: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::Conv3DShape)\n    .Doc(R\"doc(\nMKL version of Conv3D operator. Uses MKL DNN APIs to perform 3D convolution.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropInputV2\")\n    .Input(\"input_sizes: Tshape\")\n    .Input(\"filter: T\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input_sizes: uint8\")\n    .Input(\"mkl_filter: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .Attr(\"Tshape: {int32, int64} = DT_INT32\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(0, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Convolution3D backward input. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklConv3DBackpropFilterV2\")\n    .Input(\"input: T\")\n    .Input(\"filter_sizes: int32\")\n    .Input(\"out_backprop: T\")\n    .Input(\"mkl_input: uint8\")\n    .Input(\"mkl_filter_size: uint8\")\n    .Input(\"mkl_out_backprop: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->MakeShapeFromShapeTensor(1, &s));\n      TF_RETURN_IF_ERROR(c->WithRank(s, 5, &s));\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of Conv3DBackpropFilter. Uses MKL DNN APIs to compute the\ngradients of convolution with respect to the filter.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu operator. Uses MKL DNN APIs to implement Relu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Relu6 operator. Uses MKL DNN APIs to implement Relu6 operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklRelu6Grad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of Relu6Grad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu6 operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyRelu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of LeakyRelu operator. Uses MKL DNN APIs to implement\nLeakyRelu operator.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLeakyReluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"alpha: float = 0.2\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of LeakyReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for LeakyReluGrad operation.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklElu\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Elu operator. Uses MKL DNN APIs to implement Elu operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklEluGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of EluGrad operator. Uses MKL DNN APIs to compute Elu\ngradients for Elu operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklSoftmax\")\n    .Input(\"logits: T\")\n    .Input(\"mkl_logits: uint8\")\n    .Output(\"softmax: T\")\n    .Output(\"mkl_softmax: uint8\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 1);\n    })\n    .Doc(R\"doc(\nMKL version of ReluGrad operator. Uses MKL DNN APIs to compute rectified\nlinear gradients for Relu operation.\n)doc\");\n\nREGISTER_OP(\"_MklTanh\")\n    .Input(\"features: T\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"activations: T\")\n    .Output(\"mkl_activations: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nMKL version of Tanh operator. Uses MKL DNN APIs to implement Tanh operator.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklTanhGrad\")\n    .Input(\"gradients: T\")\n    .Input(\"features: T\")\n    .Input(\"mkl_gradients: uint8\")\n    .Input(\"mkl_features: uint8\")\n    .Output(\"backprops: T\")\n    .Output(\"mkl_backprops: uint8\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn(shape_inference::MergeBothInputsShapeFn)\n    .Doc(R\"doc(\nMKL version of TanhGrad operator. Uses MKL DNN APIs to compute tanh\ngradients for Tanh operation.\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Attr(\"workspace_enabled: bool = false\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Output(\"workspace: T\")\n#else\n    .Output(\"workspace: uint8\")\n#endif\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolShape)\n    .Doc(R\"doc(\nMKL version of MaxPool operator. Uses MKL DNN APIs to perform max pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPoolGrad\")\n    .Attr(\"T: {float, half, bfloat16} = DT_FLOAT\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(GetExplicitPaddingsAttrString())\n    .Input(\"orig_input: T\")\n    .Input(\"orig_output: T\")\n    .Input(\"grad: T\")\n#ifdef INTEL_MKL_ML_ONLY\n    .Input(\"workspace: T\")\n#else\n    .Input(\"workspace: uint8\")\n#endif\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .SetShapeFn(shape_inference::MaxPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPoolGrad. Uses oneDNN APIs to compute gradients of\nMaxPool operator.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolShape)\n    .Doc(R\"doc(\nMKL version of AvgPool operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPoolGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 4\")\n    .Attr(\"strides: list(int) >= 4\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPoolGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPoolGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3D\")\n    .Input(\"value: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of AvgPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklAvgPool3DGrad\")\n    .Input(\"orig_input_shape: int32\")\n    .Input(\"grad: T\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {float, half, double, bfloat16}\")\n    .SetShapeFn(shape_inference::AvgPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of AvgPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof AvgPool function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3D\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float}\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::Pool3DShape)\n    .Doc(R\"doc(\nMKL version of MaxPool3D operator. Uses MKL DNN APIs to perform average pooling\non the input.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklMaxPool3DGrad\")\n    .Input(\"orig_input: TInput\")\n    .Input(\"orig_output: TInput\")\n    .Input(\"grad: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_orig_input: uint8\")\n    .Input(\"mkl_orig_output: uint8\")\n    .Input(\"mkl_grad: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"ksize: list(int) >= 5\")\n    .Attr(\"strides: list(int) >= 5\")\n    .Attr(GetPaddingAttrString())\n    .Attr(GetConvnet3dDataFormatAttrString())\n    .Attr(\"T: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"TInput: {half, bfloat16, float} = DT_FLOAT\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .SetShapeFn(shape_inference::MaxPool3DGradShape)\n    .Doc(R\"doc(\noneDNN version of MaxPool3DGrad operator. Uses oneDNN APIs to compute gradients\nof MaxPool3D function.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRN\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Output(\"workspace: uint8\")\n    .Output(\"mkl_output: uint8\")\n    .Output(\"mkl_workspace: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      return UnchangedShapeWithRank(c, 4);\n    })\n    .Doc(R\"doc(\nMKL version of LRN operator. Uses MKL DNN APIs to perform local response\nnormalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklLRNGrad\")\n    .Input(\"input_grads: T\")\n    .Input(\"input_image: T\")\n    .Input(\"output_image: T\")\n    .Input(\"workspace: uint8\")\n    .Input(\"mkl_input_grads: uint8\")\n    .Input(\"mkl_input_image: uint8\")\n    .Input(\"mkl_output_image: uint8\")\n    .Input(\"mkl_workspace: uint8\")\n    .Output(\"output: T\")\n    .Output(\"mkl_output: uint8\")\n    .Attr(\"depth_radius: int = 5\")\n    .Attr(\"bias: float = 1.0\")\n    .Attr(\"alpha: float = 1.0\")\n    .Attr(\"beta: float = 0.5\")\n    .Attr(\"workspace_enabled: bool = false\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle s;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &s));  // input_grads\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(1), &s));     // input_image\n      TF_RETURN_IF_ERROR(c->Merge(s, c->input(2), &s));     // output_image\n      c->set_output(0, s);\n      return OkStatus();\n    })\n    .Doc(R\"doc(\nMKL version of LRNGrad operator. Uses MKL DNN APIs to compute gradient for\nlocal response normalization.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNorm\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"offset: T\")\n    .Input(\"mean: T\")\n    .Input(\"variance: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: T\")\n    .Output(\"batch_variance: T\")\n    .Output(\"reserve_space_1: T\")\n    .Output(\"reserve_space_2: T\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNorm operator. Uses oneDNN APIs to perform fused\nbatch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormGrad\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: T\")\n    .Input(\"reserve_space_1: T\")\n    .Input(\"reserve_space_2: T\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: T\")\n    .Output(\"offset_backprop: T\")\n    .Output(\"reserve_space_3: T\")\n    .Output(\"reserve_space_4: T\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(\"data_format: string = 'NHWC'\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape)\n    .Doc(R\"doc(\noneDNN version of FusedBatchNormGrad operator. Uses oneDNN APIs to compute\ngradients for fused batch normalization.\n\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklFusedBatchNormV2\")\n    .Input(\"x: T\")\n    .Input(\"scale: U\")\n    .Input(\"offset: U\")\n    .Input(\"mean: U\")\n    .Input(\"variance: U\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_offset: uint8\")\n    .Input(\"mkl_mean: uint8\")\n    .Input(\"mkl_variance: uint8\")\n    .Output(\"y: T\")\n    .Output(\"batch_mean: U\")\n    .Output(\"batch_variance: U\")\n    .Output(\"reserve_space_1: U\")\n    .Output(\"reserve_space_2: U\")\n    .Output(\"mkl_y: uint8\")\n    .Output(\"mkl_batch_mean: uint8\")\n    .Output(\"mkl_batch_variance: uint8\")\n    .Output(\"mkl_reserve_space_1: uint8\")\n    .Output(\"mkl_reserve_space_2: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"exponential_avg_factor: float = 1.0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormShape);\n\nREGISTER_OP(\"_MklFusedBatchNormGradV2\")\n    .Input(\"y_backprop: T\")\n    .Input(\"x: T\")\n    .Input(\"scale: float\")\n    .Input(\"reserve_space_1: U\")\n    .Input(\"reserve_space_2: U\")\n    .Input(\"mkl_y_backprop: uint8\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_scale: uint8\")\n    .Input(\"mkl_reserve_space_1: uint8\")\n    .Input(\"mkl_reserve_space_2: uint8\")\n    .Output(\"x_backprop: T\")\n    .Output(\"scale_backprop: U\")\n    .Output(\"offset_backprop: U\")\n    .Output(\"reserve_space_3: U\")\n    .Output(\"reserve_space_4: U\")\n    .Output(\"mkl_x_backprop: uint8\")\n    .Output(\"mkl_scale_backprop: uint8\")\n    .Output(\"mkl_offset_backprop: uint8\")\n    .Output(\"mkl_reserve_space_3: uint8\")\n    .Output(\"mkl_reserve_space_4: uint8\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"U: {float}\")\n    .Attr(\"epsilon: float = 0.0001\")\n    .Attr(GetConvnetDataFormatAttrString())\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn(shape_inference::FusedBatchNormGradShape);\n\nREGISTER_OP(\"_MklToTf\")\n    .Input(\"input: T\")\n    .Input(\"mkl_input: uint8\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, float, double, bfloat16, qint8, quint8, qint32}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to convert a tensor from MKL layout to TensorFlow layout.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\nREGISTER_OP(\"_MklInputConversion\")\n    .Input(\"input_0: T\")\n    .Input(\"input_1: T\")\n    .Input(\"mkl_input_0: uint8\")\n    .Input(\"mkl_input_1: uint8\")\n    .Output(\"output_0: T\")\n    .Output(\"output_1: T\")\n    .Output(\"mkl_output_0: uint8\")\n    .Output(\"mkl_output_1: uint8\")\n    // All datatypes supported by element-wise ops\n    .Attr(\n        \"T: {half, float, bfloat16, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128}\")\n    .Attr(GetConvnetDataFormat2D3DAttrString())\n    .SetShapeFn(shape_inference::UnknownShape)\n    .Doc(R\"doc(\nMKL operator to process the inputs to an elementwise MKL op. Both inputs\nneed to be either in TF or in MKL format. This op is added before every\nelement-wise MKL op.\n\nNOTE Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to invoke these operators.\n)doc\");\n\n#endif  // INTEL_MKL\nREGISTER_OP(\"QuantizedConv2DAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and BiasAdd.\nREGISTER_OP(\"QuantizedConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D and Relu.\nREGISTER_OP(\"QuantizedConv2DAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Relu, and Requantize.\nREGISTER_OP(\"QuantizedConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized Conv2D, BiasAdd, Sum, and Relu.\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Input(\"summand: Tsummand\")\n    .Input(\"min_summand: float\")\n    .Input(\"max_summand: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Tsummand: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(6), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      // Since activations are not requantized per channel, `min_output`\n      // and `max_output` are scalars.\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\n// Fusion of Quantized MatMul and BiasAdd.\nREGISTER_OP(\"QuantizedMatMulWithBias\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRelu\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: float\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndReluAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndDequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: {float}\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedMatMulWithBiasAndRequantize\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"Toutput: quantizedtype = DT_QUINT8\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"input_quant_mode: {'MIN_FIRST', 'SCALED'} = 'MIN_FIRST'\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(6), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(7), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(8), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedConv2DPerChannel\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::Conv2DShape(c));\n      ShapeHandle unused, channel;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &channel));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(5), 1, &channel));\n      c->set_output(1, channel);\n      c->set_output(2, channel);\n      return OkStatus();\n    });\n\nREGISTER_OP(\"QuantizedDepthwiseConv2D\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBias\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndRelu\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: float\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QINT32\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"filter: Tfilter\")\n    .Input(\"bias: Tbias\")\n    .Input(\"min_input: float\")\n    .Input(\"max_input: float\")\n    .Input(\"min_filter: float\")\n    .Input(\"max_filter: float\")\n    .Input(\"min_freezed_output: float\")\n    .Input(\"max_freezed_output: float\")\n    .Output(\"output: out_type\")\n    .Output(\"min_output: float\")\n    .Output(\"max_output: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"Tfilter: quantizedtype\")\n    .Attr(\"Tbias: {float, qint32}\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .Attr(\"strides: list(int)\")\n    .Attr(GetPaddingAttrString())\n    .Attr(\"dilations: list(int) = [1, 1, 1, 1]\")\n    .Attr(\"padding_list: list(int) = []\")\n    .SetShapeFn(shape_inference::DepthwiseConv2DNativeShape);\n\nREGISTER_OP(\"IsotonicRegression\")\n    .Input(\"input: T\")\n    .Output(\"output: output_dtype\")\n    .Output(\"segments: int32\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"output_dtype: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* context) {\n      context->set_output(0, context->input(0));\n      context->set_output(1, context->input(0));\n      return OkStatus();\n    });\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for convolutional operations.\"\"\"\n\nimport os\nimport time\n\nimport numpy as np\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\n\n\ndef GetShrunkInceptionShapes(shrink=10):\n  \"\"\"Iterator for smaller versions of convolution shapes in 2015 Inception.\n\n  Relative to inception, each depth value is `depth // shrink`.\n\n  Args:\n    shrink: Factor to shrink each depth value by relative to Inception.\n\n  Yields:\n    Tuple (input_size, filter_size, out_size, stride, padding), the convolution\n    parameters of Inception layers.\n  \"\"\"\n  input_sizes = [[4, 5, 5, 1248], [4, 8, 8, 384], [4, 8, 8, 384],\n                 [4, 8, 8, 2048], [4, 8, 8, 448], [4, 8, 8, 2048],\n                 [4, 8, 8, 2048], [4, 8, 8, 2048], [4, 8, 8, 1760],\n                 [4, 8, 8, 1760], [4, 8, 8, 1760], [4, 8, 8, 1760],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1248],\n                 [4, 17, 17, 128], [4, 17, 17, 1248], [4, 17, 17, 224],\n                 [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 1216],\n                 [4, 17, 17, 1216], [4, 17, 17, 224], [4, 17, 17, 192],\n                 [4, 17, 17, 192], [4, 17, 17, 1152], [4, 17, 17, 1152],\n                 [4, 17, 17, 192], [4, 17, 17, 160], [4, 17, 17, 1152],\n                 [4, 17, 17, 1024], [4, 17, 17, 128], [4, 17, 17, 1024],\n                 [4, 17, 17, 128], [4, 17, 17, 1024], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 128], [4, 17, 17, 128],\n                 [4, 17, 17, 768], [4, 17, 17, 768], [4, 35, 35, 96],\n                 [4, 35, 35, 288], [4, 35, 35, 64], [4, 35, 35, 288],\n                 [4, 35, 35, 256], [4, 35, 35, 48], [4, 35, 35, 256],\n                 [4, 35, 35, 96], [4, 35, 35, 192], [4, 35, 35, 192],\n                 [4, 35, 35, 192], [4, 73, 73, 64], [4, 73, 73, 64],\n                 [4, 147, 147, 24]]\n  filter_sizes = [[1, 1, 1248, 128], [1, 3, 384, 384], [3, 1, 384, 384],\n                  [1, 1, 2048, 192], [3, 3, 448, 384], [1, 1, 2048, 320],\n                  [1, 1, 2048, 448], [1, 1, 2048, 384], [1, 1, 1760, 384],\n                  [1, 1, 1760, 192], [1, 1, 1760, 448], [1, 1, 1760, 320],\n                  [3, 3, 192, 192], [3, 3, 192, 192], [1, 1, 1248, 192],\n                  [3, 3, 128, 320], [1, 1, 1248, 128], [1, 3, 224, 224],\n                  [3, 1, 192, 256], [1, 3, 192, 256], [1, 1, 1216, 192],\n                  [1, 1, 1216, 96], [3, 1, 224, 224], [3, 3, 192, 224],\n                  [1, 3, 192, 192], [1, 1, 1152, 192], [1, 1, 1152, 128],\n                  [3, 1, 192, 192], [3, 3, 160, 192], [1, 1, 1152, 160],\n                  [1, 1, 1024, 128], [1, 3, 128, 192], [1, 1, 1024, 160],\n                  [3, 1, 128, 192], [1, 1, 1024, 256], [3, 1, 128, 128],\n                  [1, 1, 768, 192], [1, 3, 128, 128], [3, 3, 128, 128],\n                  [1, 1, 768, 128], [1, 1, 768, 320], [3, 3, 96, 96],\n                  [3, 3, 288, 384], [3, 3, 64, 96], [1, 1, 288, 64],\n                  [1, 1, 256, 64], [5, 5, 48, 64], [1, 1, 256, 48],\n                  [3, 3, 96, 96], [1, 1, 192, 32], [1, 1, 192, 64],\n                  [1, 1, 192, 48], [3, 3, 64, 192], [1, 1, 64, 64],\n                  [1, 1, 24, 64]]\n  out_sizes = [[4, 5, 5, 128], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 384], [4, 8, 8, 320],\n               [4, 8, 8, 448], [4, 8, 8, 384], [4, 8, 8, 384],\n               [4, 8, 8, 192], [4, 8, 8, 448], [4, 8, 8, 320],\n               [4, 8, 8, 192], [4, 17, 17, 192], [4, 17, 17, 192],\n               [4, 8, 8, 320], [4, 17, 17, 128], [4, 17, 17, 224],\n               [4, 17, 17, 256], [4, 17, 17, 256], [4, 17, 17, 192],\n               [4, 17, 17, 96], [4, 17, 17, 224], [4, 17, 17, 224],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 128], [4, 17, 17, 192], [4, 17, 17, 160],\n               [4, 17, 17, 192], [4, 17, 17, 256], [4, 17, 17, 128],\n               [4, 17, 17, 192], [4, 17, 17, 128], [4, 17, 17, 128],\n               [4, 17, 17, 128], [4, 17, 17, 320], [4, 17, 17, 96],\n               [4, 17, 17, 384], [4, 35, 35, 96], [4, 35, 35, 64],\n               [4, 35, 35, 64], [4, 35, 35, 64], [4, 35, 35, 48],\n               [4, 35, 35, 96], [4, 35, 35, 32], [4, 35, 35, 64],\n               [4, 35, 35, 48], [4, 71, 71, 192], [4, 73, 73, 64],\n               [4, 147, 147, 64]]\n  strides = [\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n      1, 1, 1, 1, 1\n  ]\n  # Shrink sizes to make the test faster\n  for i in input_sizes:\n    i[3] //= shrink\n  for f in filter_sizes:\n    f[2] //= shrink\n    f[3] //= shrink\n  for o in out_sizes:\n    o[3] //= shrink\n  # pylint: disable=invalid-name\n  VALID = \"VALID\"\n  SAME = \"SAME\"\n  # pylint: enable=invalid-name\n  paddings = [\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      VALID, SAME, SAME, VALID, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, SAME, VALID, VALID, SAME, SAME, SAME, SAME, SAME,\n      SAME, SAME, SAME, SAME, VALID, VALID, VALID\n  ]\n  for i, f, o, s, p in zip(input_sizes, filter_sizes, out_sizes, strides,\n                           paddings):\n    yield i, f, o, s, p\n\n\ndef GetTestConfigs():\n  \"\"\"Get all the valid tests configs to run.\n\n  Returns:\n    all the valid test configs as tuples of data_format and use_gpu.\n  \"\"\"\n  test_configs = [(\"NHWC\", False), (\"NHWC\", True)]\n  if test.is_gpu_available(cuda_only=True):\n    # \"NCHW\" format is only supported on CUDA.\n    test_configs += [(\"NCHW\", True)]\n  return test_configs\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass Conv2DTest(test.TestCase):\n\n  def _DtypesToTest(self, use_gpu):\n    if test_util.IsMklEnabled():\n      return [dtypes.float32]\n\n    if use_gpu:\n      # It is important that float32 comes first, since we are using its\n      # gradients as a reference for fp16 gradients.\n      out = [dtypes.float32]\n      if test_util.GpuSupportsHalfMatMulAndConv():\n        out.append(dtypes.float16)\n      if not test.is_built_with_rocm():\n        out.append(dtypes.float64)\n      return out\n\n    return [dtypes.float32, dtypes.float64, dtypes.float16, dtypes.bfloat16]\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = 1\n    for s in shape:\n      total_size *= s\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _SetupValuesForDevice(self, tensor_in_sizes, filter_in_sizes, dilations,\n                            strides, padding, data_format, dtype, use_gpu):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n      use_gpu: True if the operations should be run on GPU\n    Returns:\n      Symbolic tensor value that can be used to execute the computation\n    \"\"\"\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes, dtype=dtype)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW(padding)\n      conv = nn_ops.conv2d(\n          t1,\n          t2,\n          dilations=dilations,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n      self.assertEqual(conv.dtype, dtype)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n\n      return conv\n\n  def _CompareFwdValues(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that CPU and GPU produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    def _SetupVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n        t2 = constant_op.constant(x2, shape=filter_in_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t1 = test_util.NHWCToNCHW(t1)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d(\n            t1, t2, strides=strides, padding=padding, data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        return conv\n\n    tensors = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      tensors.append(_SetupVal(data_format, use_gpu))\n    values = self.evaluate(tensors)\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-3, atol=1e-3)\n\n  def _ComputeReferenceDilatedConv(self, tensor_in_sizes, filter_in_sizes,\n                                   stride, dilation, padding, data_format,\n                                   use_gpu):\n    x1 = self._CreateNumpyTensor(tensor_in_sizes)\n    x2 = self._CreateNumpyTensor(filter_in_sizes)\n    with test_util.device(use_gpu):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      if isinstance(stride, collections_abc.Iterable):\n        strides = list(stride)\n      else:\n        strides = [stride, stride]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        full_strides = [1, 1] + strides\n        full_dilation = [1, 1] + dilation\n      else:\n        full_strides = [1] + strides + [1]\n        full_dilation = [1] + dilation + [1]\n      expected = nn_ops.convolution(\n          t1,\n          t2,\n          padding=padding,\n          strides=strides,\n          dilation_rate=dilation,\n          data_format=data_format)\n      computed = nn_ops.conv2d(\n          t1,\n          t2,\n          strides=full_strides,\n          dilations=full_dilation,\n          padding=padding,\n          data_format=data_format)\n      if data_format == \"NCHW\":\n        expected = test_util.NCHWToNHWC(expected)\n        computed = test_util.NCHWToNHWC(computed)\n    return expected, computed\n\n  def _VerifyDilatedConvValues(self, tensor_in_sizes, filter_in_sizes, strides,\n                               padding, dilations, rtol=1e-4):\n    expected_results = []\n    computed_results = []\n    for data_format, use_gpu in GetTestConfigs():\n      expected, computed = self._ComputeReferenceDilatedConv(\n          tensor_in_sizes, filter_in_sizes, strides, dilations, padding,\n          data_format, use_gpu)\n      expected_results.append(expected)\n      computed_results.append(computed)\n    tolerance = 1e-2 if use_gpu else 1e-5\n    expected_values = self.evaluate(expected_results)\n    computed_values = self.evaluate(computed_results)\n    for e_value, c_value in zip(expected_values, computed_values):\n      tf_logging.debug(\"expected = %s\", e_value)\n      tf_logging.debug(\"actual = %s\", c_value)\n      self.assertAllClose(\n          e_value.flatten(), c_value.flatten(), atol=tolerance, rtol=rtol)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    filter_in_sizes,\n                    strides,\n                    padding,\n                    expected,\n                    dilations=(1, 1),\n                    gpu_only=False,\n                    test_grappler_layout_optimizer=False,\n                    tol=1e-5):\n    if gpu_only and not test.is_gpu_available(cuda_only=True):\n      return\n    tensors = []\n    dilations = list(dilations)\n    for (data_format, use_gpu) in GetTestConfigs():\n      if gpu_only and not use_gpu:\n        continue\n      dtypes_to_test = self._DtypesToTest(use_gpu)\n      if not test_grappler_layout_optimizer and data_format == \"NHWC\":\n        dtypes_to_test.append(dtypes.int32)\n      for dtype in dtypes_to_test:\n        result = self._SetupValuesForDevice(\n            tensor_in_sizes,\n            filter_in_sizes,\n            dilations,\n            strides,\n            padding,\n            data_format,\n            dtype,\n            use_gpu=use_gpu)\n        if test_grappler_layout_optimizer and data_format == \"NHWC\" and use_gpu:\n          # Grappler's layout optimizer will not optimize a fetch node, so\n          # this identity allows Grappler to optimize the Conv2D node.\n          result = array_ops.identity(result)\n        tensors.append(result)\n      values = self.evaluate(tensors)\n      for i in range(len(tensors)):\n        conv = tensors[i]\n        value = values[i]\n        tf_logging.debug(\"expected = %s\", expected)\n        tf_logging.debug(\"actual = %s\", value)\n        if np.issubdtype(value.dtype, np.integer):\n          self.assertAllEqual(np.rint(expected), np.ravel(value))\n        else:\n          self.assertAllCloseAccordingToType(\n              expected, np.ravel(value), atol=tol, rtol=tol)\n        self.assertShapeEqual(value, conv)\n        self.assertEqual(value.dtype, conv.dtype.as_numpy_dtype)\n\n  def _VerifyExplicitPaddings(self,\n                              tensor_in_sizes,\n                              filter_in_sizes,\n                              strides,\n                              padding,\n                              dilations=(1, 1),\n                              test_grappler_layout_optimizer=False,\n                              tol=1e-5):\n    \"\"\"Verifies Conv2D with explicit padding generates correct values.\n\n    It does this by comparing with Conv2D without explicit padding. This\n    function assumes Conv2D without explicit padding works correctly.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      strides: [row_stride, col_stride] for the convolution;\n      padding: Explicit padding amounts.\n      dilations: Dilation values\n      test_grappler_layout_optimizer: If True, allow the Grappler layout\n        optimizer to run, which turns NHWC Conv2Ds on the GPU to NCHW Conv2Ds.\n      tol: The absolute and relative tolerance.\n    \"\"\"\n    input_tensor = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_tensor = self._CreateNumpyTensor(filter_in_sizes)\n    input_tensor = array_ops.pad(input_tensor, [(0, 0)] + padding + [(0, 0)])\n    dilations = list(dilations)\n    conv2d_result = nn_ops.conv2d(\n        input_tensor,\n        filter_tensor, [1] + list(strides) + [1],\n        \"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(conv2d_result, [-1])))\n    self._VerifyValues(\n        tensor_in_sizes,\n        filter_in_sizes,\n        strides,\n        padding,\n        expected,\n        dilations,\n        test_grappler_layout_optimizer=test_grappler_layout_optimizer,\n        tol=tol)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x1Filter(self):\n    expected_output = [\n        30.0, 36.0, 42.0, 66.0, 81.0, 96.0, 102.0, 126.0, 150.0, 138.0, 171.0,\n        204.0, 174.0, 216.0, 258.0, 210.0, 261.0, 312.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.conv2d(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.conv2d(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionClass2DExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    convolver1 = nn_ops.Convolution(\n        input_shape=x1.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver1.num_batch_dims, 1)\n    convolver2 = nn_ops.Convolution(\n        input_shape=x2.shape,\n        filter_shape=filter_in.shape,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(convolver2.num_batch_dims, 2)\n    conv1 = convolver1(x1, filter_in)\n    conv2 = convolver2(x2, filter_in)\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvolutionWith2SpatialDimensionsAndExpandedBatch(self):\n    tensor_in_sizes_batch = [10, 2, 3, 3]\n    tensor_in_sizes_expanded_batch = [2, 5, 2, 3, 3]\n    filter_in_sizes = [1, 1, 3, 3]\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    x1 = self._CreateNumpyTensor(tensor_in_sizes_batch)\n    x2 = x1.reshape(tensor_in_sizes_expanded_batch)\n    conv1 = nn_ops.convolution(\n        x1,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    conv2 = nn_ops.convolution(\n        x2,\n        filter_in,\n        strides=[1, 1],\n        padding=\"VALID\")\n    self.assertEqual(conv1.shape, tensor_in_sizes_batch)\n    self.assertEqual(conv2.shape, tensor_in_sizes_expanded_batch)\n    self.assertAllEqual(\n        conv1,\n        self.evaluate(conv2).reshape(conv1.shape))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter2x1Dilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmpty(self):\n    expected_output = []\n    self._VerifyValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[0, 2, 3, 3],\n        filter_in_sizes=[1, 1, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [2271.0, 2367.0, 2463.0, 2901.0, 3033.0, 3165.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[1, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2Filter(self):\n    # The outputs are computed using third_party/py/IPython/notebook.\n    expected_output = [\n        231.0, 252.0, 273.0, 384.0, 423.0, 462.0, 690.0, 765.0, 840.0, 843.0,\n        936.0, 1029.0\n    ]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D1x2FilterDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[1, 2, 3, 3],\n        strides=[1, 1],\n        dilations=[2, 1],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2(self):\n    expected_output = [2271.0, 2367.0, 2463.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride2Same(self):\n    expected_output = [2271.0, 2367.0, 2463.0, 1230.0, 1305.0, 1380.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2FilterStride1x2(self):\n    expected_output = [58.0, 78.0, 98.0, 118.0, 138.0, 158.0]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 6, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[1, 2],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideValid(self):\n    expected_output = [65, 95, 275, 305]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 7, 7, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"VALID\",\n        expected=expected_output)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSmallerThanStrideSame(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 7, 9])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[2, 2],\n        padding=\"SAME\",\n        expected=[1, 3, 9, 11])\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 1],\n        filter_in_sizes=[2, 2, 1, 1],\n        strides=[3, 3],\n        padding=\"SAME\",\n        expected=[44, 28, 41, 16])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSize(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        padding=\"VALID\",\n        expected=[50, 60])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeDilation(self):\n    self._VerifyDilatedConvValues(\n        tensor_in_sizes=[1, 3, 3, 1],\n        filter_in_sizes=[2, 2, 1, 2],\n        strides=[1, 1],\n        dilations=[2, 2],\n        padding=\"VALID\")\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D0x0Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[0, 0], [0, 0]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[3, 4, 3, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 2],\n        padding=[[0, 0], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D1x1Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 2, 1],\n        filter_in_sizes=[1, 1, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Padding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[2, 1, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 2], [2, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 2],\n        filter_in_sizes=[1, 1, 2, 1],\n        strides=[2, 1],\n        padding=[[2, 2], [2, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyBottomPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[0, 3], [0, 0]], tol=2e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[2, 2, 4, 3],\n        filter_in_sizes=[1, 2, 3, 2],\n        strides=[2, 2],\n        padding=[[0, 3], [0, 0]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DOnlyTopRightPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 3],\n        filter_in_sizes=[2, 2, 3, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 2]],\n        tol=5e-5)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 4, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        strides=[1, 3],\n        padding=[[1, 0], [0, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DLotsPadding(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 1, 1, 3],\n        filter_in_sizes=[2, 2, 3, 3],\n        strides=[1, 1],\n        padding=[[3, 4], [4, 2]])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 1, 1],\n        filter_in_sizes=[2, 2, 1, 3],\n        strides=[2, 1],\n        padding=[[3, 4], [4, 2]])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2DExplicitPaddingWithDilations(self):\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1])\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2dOnlyPaddingReturnsZeros(self):\n    self._VerifyValues(\n        tensor_in_sizes=[1, 0, 2, 1],\n        filter_in_sizes=[1, 1, 1, 1],\n        strides=[1, 1],\n        padding=[[1, 1], [1, 1]],\n        expected=[0, 0, 0, 0, 0, 0, 0, 0])\n\n  def testConv2DExplicitPaddingWithLayoutOptimizer(self):\n    # Test with Grappler's layout optimizer, to ensure the layout optimizer\n    # handles explicit padding correctly.\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 3, 2, 1],\n        filter_in_sizes=[1, 2, 1, 2],\n        strides=[1, 1],\n        padding=[[1, 0], [0, 1]],\n        dilations=[2, 1],\n        test_grappler_layout_optimizer=True)\n\n    self._VerifyExplicitPaddings(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[3, 2, 2, 1],\n        strides=[1, 1],\n        padding=[[2, 1], [1, 2]],\n        dilations=[2, 3],\n        test_grappler_layout_optimizer=True)\n\n  def _VerifyGroupConvFwd(self, tensor_in_sizes, filter_in_sizes, dilations,\n                          strides, padding, data_format, dtype):\n    \"\"\"Verify the output of group convolution is equal to a for-loop implementation.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [kernel_rows, kernel_cols,\n        input_depth, output_depth].\n      dilations: Dilated rate: [col_dilation, row_dilation]\n      strides: Stride: [col_stride, row_stride]\n      padding: Padding type.\n      data_format: Format of the data tensors.\n      dtype: Data type for inputs and outputs.\n    \"\"\"\n    tensor_in = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    num_groups = tensor_in_sizes[3] // filter_in_sizes[2]\n    assert num_groups > 1 and \\\n        filter_in_sizes[2] * num_groups == tensor_in_sizes[3]\n    with test_util.device(True):\n      t1 = constant_op.constant(tensor_in, dtype=dtype)\n      t2 = constant_op.constant(filter_in, dtype=dtype)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if data_format == \"NCHW\":\n        t1 = test_util.NHWCToNCHW(t1)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        t1_splits = array_ops.split(t1, num_groups, axis=1)\n      else:\n        t1_splits = array_ops.split(t1, num_groups, axis=3)\n      t2_splits = array_ops.split(t2, num_groups, axis=3)\n\n      def MakeConv2d(inputs, filters):\n        return nn_ops.conv2d(\n            inputs,\n            filters,\n            strides,\n            padding,\n            dilations=dilations,\n            data_format=data_format)\n\n      group_conv = MakeConv2d(t1, t2)\n      group_conv_loop = array_ops.concat(\n          [MakeConv2d(t1s, t2s) for t1s, t2s in zip(t1_splits, t2_splits)],\n          axis=1 if data_format == \"NCHW\" else 3)\n\n      results = self.evaluate([group_conv, group_conv_loop])\n      tol_to_use = 1e-5\n      self.assertAllClose(\n          results[0], results[1], atol=tol_to_use, rtol=tol_to_use)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DGroupConvFwd(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      data_formats = [\"NHWC\", \"NCHW\"]\n    else:\n      data_formats = [\"NHWC\"]\n    for data_format in data_formats:\n      for dilation in [1, 2]:\n        for stride in [1, 2]:\n          for filter_dims in [[3, 3, 4, 8], [1, 1, 2, 16]]:\n            self._VerifyGroupConvFwd([10, 32, 32, 16], filter_dims,\n                                     dilations=[dilation, dilation],\n                                     strides=[stride, stride],\n                                     padding=\"SAME\",\n                                     data_format=data_format,\n                                     dtype=dtypes.float32)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testInputGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  @test_util.run_cuda_only\n  def testFilterGradientGroupConv(self):\n    for data_format in [\"NCHW\", \"NHWC\"]:\n      for test_input in [True, False]:\n        self.ConstructAndTestGradient(\n            batch=2,\n            input_rows=5,\n            input_cols=4,\n            filter_rows=3,\n            filter_cols=3,\n            num_groups=2,\n            padding=\"VALID\",\n            in_depth=4,\n            out_depth=6,\n            stride_rows=1,\n            stride_cols=1,\n            test_input=test_input,\n            data_format=data_format,\n            use_gpu=True,\n            max_err=0.005)\n  # TODO(yzhwang): this currently fails.\n  # self._VerifyValues(tensor_in_sizes=[1, 8, 8, 1],\n  #                   filter_in_sizes=[2, 2, 1, 1],\n  #                   strides=[4, 4], padding=\"SAME\",\n  #                   expected=[72, 112, 392, 432])\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInput(self,\n                                 input_sizes,\n                                 filter_sizes,\n                                 output_sizes,\n                                 strides,\n                                 padding,\n                                 expected,\n                                 data_format,\n                                 use_gpu,\n                                 err,\n                                 dilations=(1, 1)):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    with test_util.device(use_gpu):\n      if len(input_sizes) == 4:\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n      t0 = constant_op.constant(input_sizes, shape=[len(input_sizes)])\n      t1 = constant_op.constant(x1, shape=filter_sizes)\n      t2 = constant_op.constant(x2, shape=output_sizes)\n      strides = [1] + strides + [1]\n      dilations = [1] + dilations + [1]\n      if isinstance(padding, (list, tuple)):\n        padding = [(0, 0)] + padding + [(0, 0)]\n      if data_format == \"NCHW\":\n        t2 = test_util.NHWCToNCHW(t2)\n        strides = test_util.NHWCToNCHW(strides)\n        dilations = test_util.NHWCToNCHW(dilations)\n        if isinstance(padding, (list, tuple)):\n          padding = test_util.NHWCToNCHW((padding))\n      conv = nn_ops.conv2d_backprop_input(\n          t0,\n          t1,\n          t2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format,\n          dilations=dilations)\n      if data_format == \"NCHW\":\n        conv = test_util.NCHWToNHWC(conv)\n      # \"values\" consists of two tensors for two backprops\n      value = self.evaluate(conv)\n      self.assertShapeEqual(value, conv)\n    tf_logging.debug(\"expected = %s\", expected)\n    tf_logging.debug(\"actual = %s\", value)\n    self.assertAllCloseAccordingToType(expected, value.flatten(), atol=1e-5)\n\n  def _CompareBackpropInput(self, input_sizes, filter_sizes, output_sizes,\n                            conv_strides, padding):\n    x1 = np.random.rand(*filter_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        if data_format == \"NCHW\":\n          new_input_sizes = test_util.NHWCToNCHW(input_sizes)\n        else:\n          new_input_sizes = input_sizes\n        t0 = constant_op.constant(new_input_sizes, shape=[len(new_input_sizes)])\n        t1 = constant_op.constant(x1, shape=filter_sizes)\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_input(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-2, atol=1e-2)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropInput(self):\n    expected_output = [1.0, 4.0, 4.0, 3.0, 10.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropInput(self):\n    expected_output = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInput(self):\n    expected_output = [\n        14.0, 32.0, 50.0, 100.0, 163.0, 226.0, 167.0, 212.0, 257.0, 122.0,\n        140.0, 158.0, 478.0, 541.0, 604.0, 437.0, 482.0, 527.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      # The GPU version of this test is not very stable. So adjusting the\n      # error threshold to 1e-4.\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropInputStride1x2(self):\n    expected_output = [\n        1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 7.0, 12.0, 11.0, 18.0, 15.0, 24.0, 12.0,\n        16.0, 15.0, 20.0, 18.0, 24.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropInput(self):\n    expected_output = [\n        1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 4.0, 0.0, 0.0, 0.0,\n        0.0, 0.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"XLA requires input_sizes to be a 4D shape.\")\n  def testConv2DInputSizesContainsOnlySpatialDimensionsBackpropInput(self):\n    expected_output = [5.0, 11.0, 17.0, 23.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=[2, 2],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  @test_util.run_in_graph_and_eager_modes\n  @test_util.disable_xla(\"b/239598470\")\n  def testConv2DBackpropInputDegenerateBackpropInput(self):\n    input_sizes = [3, 1, 1, 2]\n    expected_output = np.zeros(input_sizes).flatten()\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInput(\n          input_sizes=input_sizes,\n          filter_sizes=[1, 3, 2, 3],\n          output_sizes=[3, 1, 0, 3],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-5)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilter(self,\n                                  input_sizes,\n                                  filter_sizes,\n                                  output_sizes,\n                                  strides,\n                                  padding,\n                                  expected,\n                                  data_format,\n                                  use_gpu,\n                                  dilations=(1, 1),\n                                  err=1e-5):\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    explicit_strides = [1] + strides + [1]\n    new_padding = padding\n    new_dilations = [1] + dilations + [1]\n    if isinstance(new_padding, (list, tuple)):\n      new_padding = [(0, 0)] + new_padding + [(0, 0)]\n    if data_format == \"NCHW\":\n      explicit_strides = test_util.NHWCToNCHW(explicit_strides)\n      new_dilations = test_util.NHWCToNCHW(new_dilations)\n      if isinstance(padding, (list, tuple)):\n        new_padding = test_util.NHWCToNCHW(new_padding)\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes, dtype=dtype)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes, dtype=dtype)\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=explicit_strides,\n            padding=new_padding,\n            dilations=new_dilations,\n            data_format=data_format)\n        value = self.evaluate(conv)\n        self.assertShapeEqual(value, conv)\n      tf_logging.debug(\"expected = %s\", expected)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertAllCloseAccordingToType(expected, value.flatten(), err)\n\n  def _CompareBackFilter(self, input_sizes, filter_sizes, output_sizes,\n                         conv_strides, padding):\n    x0 = np.random.rand(*input_sizes).astype(np.float32)\n    x2 = np.random.rand(*output_sizes).astype(np.float32)\n\n    def _GetVal(data_format, use_gpu):\n      with test_util.device(use_gpu):\n        t0 = constant_op.constant(x0, shape=input_sizes)\n        t1 = constant_op.constant(filter_sizes, shape=[len(filter_sizes)])\n        t2 = constant_op.constant(x2, shape=output_sizes)\n        strides = [1] + conv_strides + [1]\n        if data_format == \"NCHW\":\n          t0 = test_util.NHWCToNCHW(t0)\n          t2 = test_util.NHWCToNCHW(t2)\n          strides = test_util.NHWCToNCHW(strides)\n        conv = nn_ops.conv2d_backprop_filter(\n            t0,\n            t1,\n            t2,\n            strides=strides,\n            padding=padding,\n            data_format=data_format)\n        ret = self.evaluate(conv)\n        self.assertShapeEqual(ret, conv)\n        return ret\n\n    values = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      values.append(_GetVal(data_format, use_gpu))\n    for i in range(1, len(values)):\n      self.assertAllClose(values[0], values[i], rtol=1e-4, atol=1e-4)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth1ValidBackpropFilter(self):\n    expected = [5.0, 8.0, 14.0, 17.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DEmptyBackpropFilter(self):\n    expected = []\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 0],\n          output_sizes=[1, 1, 2, 0],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DBackpropFilterWithEmptyInput(self):\n    expected = [0, 0, 0, 0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[0, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[0, 1, 2, 1],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilter(self):\n    expected = [\n        17.0, 22.0, 27.0, 22.0, 29.0, 36.0, 27.0, 36.0, 45.0, 32.0, 43.0, 54.0,\n        37.0, 50.0, 63.0, 42.0, 57.0, 72.0, 62.0, 85.0, 108.0, 67.0, 92.0,\n        117.0, 72.0, 99.0, 126.0, 77.0, 106.0, 135.0, 82.0, 113.0, 144.0, 87.0,\n        120.0, 153.0\n    ]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 3, 3],\n          filter_sizes=[2, 2, 3, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x2(self):\n    expected = [161.0, 182.0, 287.0, 308.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 2, 3, 1],\n          strides=[1, 2],\n          padding=\"VALID\",\n          expected=expected,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DStrideTwoFilterOneSameBackpropFilter(self):\n    expected_output = [78.]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 4, 4, 1],\n          filter_sizes=[1, 1, 1, 1],\n          output_sizes=[1, 2, 2, 1],\n          strides=[2, 2],\n          padding=\"SAME\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilter(self):\n    expected_output = [1.0, 2.0, 2.0, 4.0, 3.0, 6.0, 4.0, 8.0]\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilter(\n          input_sizes=[1, 2, 2, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 1, 1, 2],\n          strides=[1, 1],\n          padding=\"VALID\",\n          expected=expected_output,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropInputDilation(self, input_sizes, filter_sizes,\n                                         output_sizes, strides, dilations,\n                                         padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu):\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t1)[0]\n        conv_2 = gradients_impl.gradients(conv_forward_2, t1)[0]\n        # \"values\" consists of two tensors for two backprops\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  # Testing for backprops\n  def _RunAndVerifyBackpropFilterDilation(self, input_sizes, filter_sizes,\n                                          output_sizes, strides, dilations,\n                                          padding, data_format, use_gpu, err):\n    x1 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(filter_sizes)\n    default_dilations = (dilations[0] == 1 and dilations[1] == 1)\n    if default_dilations or use_gpu:\n      with self.cached_session(use_gpu=use_gpu):\n        if data_format == \"NCHW\":\n          input_sizes = test_util.NHWCToNCHW(input_sizes)\n        t1 = constant_op.constant(x1, shape=input_sizes)\n        t2 = constant_op.constant(x2, shape=filter_sizes)\n        full_strides = [1] + strides + [1]\n        full_dilations = [1] + dilations + [1]\n        if data_format == \"NCHW\":\n          full_strides = test_util.NHWCToNCHW(full_strides)\n          full_dilations = test_util.NHWCToNCHW(full_dilations)\n        conv_forward = nn_ops.conv2d(\n            t1,\n            t2,\n            strides=full_strides,\n            dilations=full_dilations,\n            padding=padding,\n            data_format=data_format)\n        conv_forward_2 = nn_ops.convolution(\n            t1,\n            t2,\n            padding=padding,\n            strides=strides,\n            dilation_rate=dilations,\n            data_format=data_format)\n        if data_format == \"NCHW\":\n          conv_forward = test_util.NCHWToNHWC(conv_forward)\n          conv_forward_2 = test_util.NCHWToNHWC(conv_forward_2)\n        conv = gradients_impl.gradients(conv_forward, t2)[0]\n        conv_2 = gradients_impl.gradients(conv_forward, t2)[0]\n        value = self.evaluate(conv)\n        value_2 = self.evaluate(conv_2)\n        self.assertShapeEqual(value, conv)\n        self.assertShapeEqual(value_2, conv_2)\n      tf_logging.debug(\"expected = %s\", value_2)\n      tf_logging.debug(\"actual = %s\", value)\n      self.assertArrayNear(value_2.flatten(), value.flatten(), err)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropFilterDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 0],\n            output_sizes=[1, 1, 2, 0],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 4, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropFilterDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropFilterDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputStride1x1Dilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 6, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 5, 1],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth1ValidBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[1, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DEmptyBackpropInputDilation1x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[0, 2, 3, 1],\n            filter_sizes=[2, 2, 1, 1],\n            output_sizes=[0, 1, 2, 1],\n            strides=[1, 1],\n            dilations=[1, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2D2x2Depth3ValidBackpropInputDilation2x1(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        # The GPU version of this test is not very stable. So adjusting the\n        # error threshold to 1e-4.\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 2, 3],\n            filter_sizes=[2, 2, 3, 3],\n            output_sizes=[1, 1, 2, 3],\n            strides=[1, 1],\n            dilations=[2, 1],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-4)\n\n  @test_util.deprecated_graph_mode_only\n  def testConv2DKernelSizeMatchesInputSizeBackpropInputDilation2x2(self):\n    if test.is_gpu_available(cuda_only=True) or test_util.IsMklEnabled():\n      for (data_format, use_gpu) in GetTestConfigs():\n        self._RunAndVerifyBackpropInputDilation(\n            input_sizes=[1, 3, 3, 1],\n            filter_sizes=[2, 2, 1, 2],\n            output_sizes=[1, 1, 1, 2],\n            strides=[1, 1],\n            dilations=[2, 2],\n            padding=\"VALID\",\n            data_format=data_format,\n            use_gpu=use_gpu,\n            err=1e-5)\n\n  def _RunAndVerifyBackpropInputExplicitPadding(self,\n                                                input_sizes,\n                                                filter_sizes,\n                                                output_sizes,\n                                                strides,\n                                                padding,\n                                                data_format,\n                                                use_gpu,\n                                                dilations=(1, 1),\n                                                err=2e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x1 = self._CreateNumpyTensor(filter_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n    padded_input_sizes = input_sizes[:]\n    padded_input_sizes[1] += padding[0][0] + padding[0][1]\n    padded_input_sizes[2] += padding[1][0] + padding[1][1]\n    c = nn_ops.conv2d_backprop_input(\n        padded_input_sizes,\n        x1,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    c = c[:, padding[0][0]:(c.shape[1] - padding[0][1]), padding[1][0]:(\n        c.shape[2] - padding[1][1]), :]\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropInput(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        err=err,\n        dilations=dilations)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          dilations=[2, 2], use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 3],\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropInput(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          err=5e-5,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropInputExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          dilations=[2, 1],\n          use_gpu=use_gpu)\n\n  def _RunAndVerifyBackpropFilterExplicitPadding(self,\n                                                 input_sizes,\n                                                 filter_sizes,\n                                                 output_sizes,\n                                                 strides,\n                                                 padding,\n                                                 data_format,\n                                                 use_gpu,\n                                                 dilations=(1, 1),\n                                                 err=1e-5):\n    if use_gpu and not test.is_gpu_available(cuda_only=True):\n      return\n    if not use_gpu and dilations != (1, 1):\n      return  # Non-default dilations is currently not supported on the CPU.\n\n    x0 = self._CreateNumpyTensor(input_sizes)\n    x2 = self._CreateNumpyTensor(output_sizes)\n    dilations = list(dilations)\n\n    x0 = np.pad(x0, [(0, 0)] + padding + [(0, 0)], \"constant\")\n    c = nn_ops.conv2d_backprop_filter(\n        x0,\n        filter_sizes,\n        x2,\n        strides=[1] + strides + [1],\n        padding=\"VALID\",\n        dilations=[1] + dilations + [1])\n    expected = list(self.evaluate(array_ops.reshape(c, [-1])))\n    self._RunAndVerifyBackpropFilter(\n        input_sizes,\n        filter_sizes,\n        output_sizes,\n        strides,\n        padding,\n        expected,\n        data_format,\n        use_gpu=use_gpu,\n        dilations=dilations,\n        err=err)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding0x0BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 1, 2, 1],\n          strides=[1, 1],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 4, 2],\n          filter_sizes=[2, 2, 2, 3],\n          output_sizes=[1, 1, 2, 3],\n          strides=[2, 2],\n          padding=[[0, 0], [0, 0]],\n          data_format=data_format, use_gpu=use_gpu)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding1x1BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 2],\n          output_sizes=[1, 3, 4, 2],\n          strides=[1, 1],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=5e-5)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 2],\n          filter_sizes=[1, 1, 2, 1],\n          output_sizes=[1, 4, 3, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 4, 2, 1],\n          strides=[1, 2],\n          padding=[[1, 1], [1, 1]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 2])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding2x2BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[2, 3, 1, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[2, 2, 5, 1],\n          strides=[3, 1],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 6, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 3, 4, 1],\n          strides=[1, 2],\n          padding=[[2, 2], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 3])\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_1_8_4_1_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 2, 3, 1],\n          filter_sizes=[2, 2, 1, 1],\n          output_sizes=[1, 10, 8, 1],\n          strides=[1, 1],\n          padding=[[1, 8], [4, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 5, 3, 1],\n          filter_sizes=[3, 2, 1, 1],\n          output_sizes=[1, 4, 8, 1],\n          strides=[3, 1],\n          padding=[[1, 8], [4, 2]],\n          use_gpu=use_gpu,\n          data_format=data_format)\n\n  @test_util.run_in_graph_and_eager_modes()\n  def testConv2D2x2Depth1Padding_5_0_2_2_BackpropFilter(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 3, 3, 1],\n          filter_sizes=[2, 1, 1, 1],\n          output_sizes=[1, 7, 7, 1],\n          strides=[1, 1],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          err=1e-4)\n\n      self._RunAndVerifyBackpropFilterExplicitPadding(\n          input_sizes=[1, 4, 2, 1],\n          filter_sizes=[3, 3, 1, 1],\n          output_sizes=[1, 5, 2, 1],\n          strides=[1, 2],\n          padding=[[5, 0], [2, 2]],\n          data_format=data_format,\n          use_gpu=use_gpu,\n          dilations=[2, 1])\n\n  # Gradient checkers\n  def ConstructAndTestGradient(self,\n                               batch,\n                               input_rows,\n                               input_cols,\n                               filter_rows,\n                               filter_cols,\n                               in_depth,\n                               out_depth,\n                               stride_rows,\n                               stride_cols,\n                               padding,\n                               test_input,\n                               data_format,\n                               use_gpu,\n                               num_groups=1,\n                               max_err=0.003):\n    assert in_depth % num_groups == 0 and out_depth % num_groups == 0\n    input_shape = [batch, input_rows, input_cols, in_depth]\n    filter_shape = [filter_rows, filter_cols, in_depth // num_groups, out_depth]\n    # TODO(yangke): re-factor the computation of output shape.\n    if padding == \"VALID\":\n      output_rows = (input_rows - filter_rows + stride_rows) // stride_rows\n      output_cols = (input_cols - filter_cols + stride_cols) // stride_cols\n    elif padding == \"SAME\":\n      output_rows = (input_rows + stride_rows - 1) // stride_rows\n      output_cols = (input_cols + stride_cols - 1) // stride_cols\n    else:\n      self.assertIsInstance(padding, (list, tuple))\n      output_rows = (input_rows + padding[1][0] + padding[1][1] - filter_rows +\n                     stride_rows) // stride_rows\n      output_cols = (input_cols + padding[2][0] + padding[2][1] - filter_cols +\n                     stride_cols) // stride_cols\n    output_shape = [batch, output_rows, output_cols, out_depth]\n    input_size = 1\n    for x in input_shape:\n      input_size *= x\n    filter_size = 1\n    for x in filter_shape:\n      filter_size *= x\n    input_data = [x * 1.0 / input_size for x in range(0, input_size)]\n    filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]\n    # Conv2DGrad functions are not compiled for double due to\n    # a problem in the way Eigen's Conv2DGrad works for double.\n    # So we disable the DOUBLE path.  We should re-enable this\n    # when double support returns for CPU and/or GPU.\n    for dtype in self._DtypesToTest(use_gpu=use_gpu):\n      with self.cached_session(use_gpu=use_gpu):\n        input_tensor = constant_op.constant(\n            input_data, shape=input_shape, dtype=dtype, name=\"input\")\n        filter_tensor = constant_op.constant(\n            filter_data, shape=filter_shape, dtype=dtype, name=\"filter\")\n        strides = [1, stride_rows, stride_cols, 1]\n        new_padding = padding\n        if data_format == \"NCHW\":\n          new_input_tensor = test_util.NHWCToNCHW(input_tensor)\n          strides = test_util.NHWCToNCHW(strides)\n          if isinstance(padding, (list, tuple)):\n            new_padding = test_util.NHWCToNCHW(padding)\n        else:\n          new_input_tensor = input_tensor\n        conv = nn_ops.conv2d(\n            new_input_tensor,\n            filter_tensor,\n            strides,\n            new_padding,\n            data_format=data_format,\n            name=\"conv\")\n        if data_format == \"NCHW\":\n          conv = test_util.NCHWToNHWC(conv)\n        self.assertEqual(output_shape, conv.get_shape())\n        if test_input:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(input_tensor,\n                                                               input_shape,\n                                                               conv,\n                                                               output_shape)\n        else:\n          jacob_t, jacob_n = gradient_checker.compute_gradient(filter_tensor,\n                                                               filter_shape,\n                                                               conv,\n                                                               output_shape)\n        if dtype == dtypes.float32:\n          reference_jacob_t = jacob_t\n          err = np.fabs(jacob_t - jacob_n).max()\n        else:\n          # Compare fp16/bf16 theoretical gradients to fp32 gradients,\n          # since fp16/bf16 numerical gradients are too imprecise.\n          err = np.fabs(jacob_t - reference_jacob_t).max()\n\n        tf_logging.debug(\"conv_2d gradient error = %s\", err)\n        self.assertLess(err, max_err)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientValidPaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=3,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=4,\n          input_rows=6,\n          input_cols=5,\n          filter_rows=2,\n          filter_cols=2,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=7,\n          input_cols=6,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=4,\n          out_depth=5,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStrideThree(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=3,\n          stride_cols=3,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientSamePaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=7,\n          filter_rows=4,\n          filter_cols=4,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=1,\n          padding=\"SAME\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradientKernelSizeMatchesInputSize(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=3,\n          filter_rows=4,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=\"VALID\",\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.0025)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1x1PaddingStrideTwo(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=4,\n          input_cols=5,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=2,\n          stride_cols=2,\n          padding=[[0, 0], [1, 1], [1, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.003)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient2x2PaddingStrideOne(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=5,\n          input_cols=4,\n          filter_rows=3,\n          filter_cols=3,\n          in_depth=2,\n          out_depth=3,\n          stride_rows=1,\n          stride_cols=1,\n          padding=[[0, 0], [2, 2], [2, 2], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu,\n          max_err=0.005)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient1_2_3_4PaddingStride3x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=8,\n          input_cols=5,\n          filter_rows=4,\n          filter_cols=2,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=3,\n          stride_cols=2,\n          padding=[[0, 0], [1, 2], [3, 4], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient4_3_2_1PaddingStride2x1(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=3,\n          input_rows=5,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=2,\n          in_depth=1,\n          out_depth=2,\n          stride_rows=2,\n          stride_cols=1,\n          padding=[[0, 0], [4, 3], [2, 1], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testInputGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=True,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testFilterGradient0_0_0_5PaddingStride1x2(self):\n    for (data_format, use_gpu) in GetTestConfigs():\n      self.ConstructAndTestGradient(\n          batch=2,\n          input_rows=6,\n          input_cols=7,\n          filter_rows=3,\n          filter_cols=4,\n          in_depth=3,\n          out_depth=2,\n          stride_rows=1,\n          stride_cols=2,\n          padding=[[0, 0], [0, 0], [0, 5], [0, 0]],\n          test_input=False,\n          data_format=data_format,\n          use_gpu=use_gpu)\n\n  @test_util.deprecated_graph_mode_only\n  def testShapeFunctionEdgeCases(self):\n    # All shapes unknown.\n    c1 = nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.float32),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n    self.assertEqual([None, None, None, None], c1.get_shape().as_list())\n\n    # Incorrect input shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Incorrect filter shape.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(\n              dtypes.float32, shape=[1, 3]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Depth mismatch.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(\n              dtypes.float32, shape=[32, 20, 20, 3]),\n          array_ops.placeholder(\n              dtypes.float32, shape=[4, 4, 2, 2]),\n          strides=[1, 1, 1, 1],\n          padding=\"SAME\")\n\n    # Input depth divisible by filter depth (group convolution).\n    # No exceptions should appear.\n    nn_ops.conv2d(\n        array_ops.placeholder(dtypes.float32, shape=[32, 20, 20, 8]),\n        array_ops.placeholder(dtypes.float32, shape=[4, 4, 2, 16]),\n        strides=[1, 1, 1, 1],\n        padding=\"SAME\")\n\n    # Negative padding.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, -1], [1, 2], [0, 0]])\n\n    # Nonzero padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[1, 0], [0, 0], [0, 0], [0, 0]])\n\n    # Nonzero NCHW padding in nonspatial dimension.\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 1], [0, 0], [0, 0]],\n          data_format=\"NCHW\")\n\n    # Wrong amount of padding\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0, 0], [0, 0], [0, 0]])\n\n    # Only specify one padding amount per dimension\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[[0], [0], [0], [0]])\n\n    # Explicit padding elements are not lists\n    with self.assertRaises(ValueError):\n      nn_ops.conv2d(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.float32),\n          strides=[1, 1, 1, 1],\n          padding=[0, 0, 0, 0])\n\n  def testOpEdgeCases(self):\n    # Illegal strides.\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[2, 1, 1, 1], padding=\"SAME\"))\n    with self.assertRaisesRegex((ValueError, errors_impl.UnimplementedError),\n                                \"strides in the batch and depth\"):\n      input_val = np.ones([2, 4, 10, 10])\n      filter_val = np.ones([2, 4, 10, 10])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 2], padding=\"SAME\"))\n\n    # TODO(b/195689143): Will enable when fixed for V2 behavior\n    # # Filter larger than input.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([20, 21, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    #   filter_val = np.ones([21, 20, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val, filter_val, strides=[1, 1, 1, 1], padding=\"VALID\"))\n    #\n    # # Filter larger than input + padding.\n    # with self.assertRaisesRegex(ValueError, \"Negative dimension size\"):\n    #   input_val = np.ones([32, 20, 20, 3])\n    # filter_val = np.ones([24, 25, 3, 2])\n    #   self.evaluate(\n    #       nn_ops.conv2d(\n    #           input_val,\n    #           filter_val,\n    #           strides=[1, 1, 1, 1],\n    #           padding=[[0, 0], [2, 2], [2, 2], [0, 0]]))\n\n    # Filter dimensions must be greater than 0.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError, \"filter must not have zero elements\"\n        \"|has a non-positive dimension\"):\n      input_val = np.ones([1, 1, 1, 1])\n      filter_val = np.ones([1, 0, 1, 1])\n      self.evaluate(\n          nn_ops.conv2d(\n              input_val, filter_val, strides=[1, 1, 1, 1], padding=\"SAME\"))\n\n    # Negative padding during backprop.\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      filter_val = np.ones([18, 18, 3, 2])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_input([32, 20, 20, 3],\n                                       filter_val,\n                                       out_backprop_val,\n                                       strides=[1, 1, 1, 1],\n                                       padding=[[0, 0], [-1, 0], [0, 0], [0,\n                                                                          0]]))\n    with self.assertRaisesRegex(\n        errors_impl.InvalidArgumentError,\n        \"All elements of explicit_paddings must be nonnegative\"):\n      input_val = np.ones([32, 20, 20, 3])\n      out_backprop_val = np.ones([32, 3, 2, 2])\n      self.evaluate(\n          nn_ops.conv2d_backprop_filter(\n              input_val, [18, 18, 3, 2],\n              out_backprop_val,\n              strides=[1, 1, 1, 1],\n              padding=[[0, 0], [-1, 0], [0, 0], [0, 0]]))\n\n  def testConv2DBackpropInputInvalidOutBackpropRaiseError(self):\n    with self.assertRaises((ValueError, errors_impl.InvalidArgumentError)):\n      with self.cached_session():\n        input_sizes = constant_op.constant([65534, 65534],\n                                           shape=[2],\n                                           dtype=dtypes.int32)\n        filters = constant_op.constant(\n            0.159749106, shape=[3, 3, 2, 2], dtype=dtypes.float32)\n        out_backprop = constant_op.constant(0, shape=[], dtype=dtypes.float32)\n        t = gen_nn_ops.conv2d_backprop_input(\n            input_sizes=input_sizes,\n            filter=filters,\n            out_backprop=out_backprop,\n            strides=[1, 1, 1, 1],\n            padding=\"SAME\",\n            use_cudnn_on_gpu=True,\n            explicit_paddings=[],\n            data_format=\"NHWC\",\n            dilations=[1, 1, 1, 1])\n        self.evaluate(t)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DepthwiseConv2DTest(test.TestCase):\n\n  def _VerifyValues(self, tensor_in_sizes, filter_in_sizes, stride, padding,\n                    expected):\n    \"\"\"Verifies the output values of the convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in [batch, input_rows,\n        input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in [filter_rows, filter_cols,\n        input_depth, depth_multiplier].\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n    \"\"\"\n    total_size_1 = 1\n    total_size_2 = 1\n    for s in tensor_in_sizes:\n      total_size_1 *= s\n    for s in filter_in_sizes:\n      total_size_2 *= s\n    # Initializes the input tensor with array containing incrementing\n    # numbers from 1.\n    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n    with self.cached_session():\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t1.set_shape(tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      conv = nn_impl.depthwise_conv2d(\n          t1, t2, strides=[1, stride, stride, 1], padding=padding)\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 1e-5)\n    self.assertShapeEqual(value, conv)\n\n  def testConv2D2x2Filter(self):\n    # The inputs look like this (it's a 3 x 2 matrix, each of depth 2):\n    #\n    # [ (1.0, 2.0), (3.0,  4.0), ( 5.0,  6.0) ]\n    # [ (7.0, 8.0), (9.0, 10.0), (11.0, 12.0) ]\n    #  We can view this as two inputs\n    #\n    #  input depth 0:\n    #\n    #  [ 1.0,  3.0,  5.0 ]\n    #  [ 7.0,  9.0, 11.0 ]\n    #\n    #  input depth 1:\n    #\n    #  [ 2.0,  4.0,  6.0 ]\n    #  [ 8.0, 10.0, 12.0 ]\n    #\n    # The filter looks like this (it has two 2 x 2 patches, each generating 2\n    # depths):\n    #\n    #  filter #0:\n    #\n    #  [ (1.0,  3.0), ( 5.0,  7.0)]\n    #  [ (9.0, 11.0), (13.0, 15.0)]\n    #\n    #  filter #1:\n    #\n    #  [ ( 2.0,  4.0), ( 6.0,  8.0)]\n    #  [ (10.0, 12.0), (14.0, 16.0)]\n    #\n    # So the outputs are:\n    #\n    # (position 0, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  1.0 * 1.0 + 7.0 * 9.0 + 3.0 * 5.0 + 9.0 * 13.0 = 196\n    # (position 0, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  1.0 * 2.0 + 7.0 * 10.0 + 3.0 * 6.0 + 9.0 * 14.0 = 216\n    # (position 0, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  2.0 * 3.0 + 8.0 * 11.0 + 4.0 * 7.0 + 10.0 * 15.0 = 272\n    # (position 0, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  2.0 * 4.0 + 8.0 * 12.0 + 4.0 * 8.0 + 10.0 * 16.0 = 296\n    #\n    # (position 1, 0: in_depth 0, output_depth 0 -- using filter #0)\n    #  3.0 * 1.0 + 9.0 * 9.0 + 5.0 * 5.0 + 11.0 * 13.0 = 252\n    # (position 1, 0: in_depth 0, output_depth 1 -- using filter #1)\n    #  3.0 * 2.0 + 9.0 * 10.0 + 5.0 * 6.0 + 11.0 * 14.0 = 280\n    # (position 1, 0: in_depth 1, output_depth 2 -- using filter #0)\n    #  4.0 * 3.0 + 10.0 * 11.0 + 6.0 * 7.0 + 12.0 * 15.0 = 344\n    # (position 1, 0: in_depth 1, output_depth 3 -- using filter #1)\n    #  4.0 * 4.0 + 10.0 * 12.0 + 6.0 * 8.0 + 12.0 * 16.0 = 376\n    expected_output = [196, 216, 272, 296, 252, 280, 344, 376]\n    self._VerifyValues(\n        tensor_in_sizes=[1, 2, 3, 2],\n        filter_in_sizes=[2, 2, 2, 2],\n        stride=1,\n        padding=\"VALID\",\n        expected=expected_output)\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass SeparableConv2DTest(test.TestCase):\n\n  def _InitValues(self, sizes):\n    \"\"\"Initializes values for input tensors.\n\n    Args:\n      sizes: Tensor dimensions.\n\n    Returns:\n      Tensor initialized to values.\n    \"\"\"\n    total_size = 1\n    for s in sizes:\n      total_size *= s\n    x = [f * 0.5 for f in range(1, total_size + 1)]\n    return constant_op.constant(x, shape=sizes)\n\n  def _VerifyValues(self,\n                    tensor_in_sizes,\n                    depthwise_filter_in_sizes,\n                    pointwise_filter_in_sizes,\n                    stride,\n                    padding,\n                    expected,\n                    data_format=\"NHWC\"):\n    \"\"\"Verifies the output values of the separable convolution function.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions.\n      depthwise_filter_in_sizes: Depthwise filter tensor dimensions.\n      pointwise_filter_in_sizes: Pointwise filter tensor dimensions.\n      stride: Stride.\n      padding: Padding type.\n      expected: An array containing the expected operation outputs.\n      data_format: string data format for input tensor.\n    \"\"\"\n    with self.cached_session():\n      t1 = self._InitValues(tensor_in_sizes)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n\n      real_t1 = t1\n      strides = [1, stride, stride, 1]\n      if data_format == \"NCHW\":\n        real_t1 = array_ops.transpose(t1, [0, 3, 1, 2])\n        strides = [1, 1, stride, stride]\n        if isinstance(padding, list):\n          padding = [padding[0], padding[3], padding[1], padding[2]]\n\n      conv = nn_impl.separable_conv2d(\n          real_t1,\n          f1,\n          f2,\n          strides=strides,\n          padding=padding,\n          data_format=data_format)\n\n      if data_format == \"NCHW\":\n        conv = array_ops.transpose(conv, [0, 2, 3, 1])\n\n      value = self.evaluate(conv)\n    tf_logging.debug(\"value = %s\", value)\n    self.assertArrayNear(expected, np.ravel(value), 2e-3)\n    self.assertShapeEqual(value, conv)\n\n  def _testSeparableConv2D(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 2, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 7].\n    # Complexity is O(2*3*2*2 + 6*7*1*1) as opposed to O(2*7*2*2).\n    expected_output = [\n        6644.5, 6971.5, 7298.5, 7625.5, 7952.5, 8279.5, 8606.5, 8154.5, 8556.5,\n        8958.5, 9360.5, 9762.5, 10164.5, 10566.5, 9664.5, 10141.5, 10618.5,\n        11095.5, 11572.5, 12049.5, 12526.5, 4145.5, 4346.5, 4547.5, 4748.5,\n        4949.5, 5150.5, 5351.5, 12684.5, 13311.5, 13938.5, 14565.5, 15192.5,\n        15819.5, 16446.5, 14194.5, 14896.5, 15598.5, 16300.5, 17002.5, 17704.5,\n        18406.5, 15704.5, 16481.5, 17258.5, 18035.5, 18812.5, 19589.5, 20366.5,\n        6499.5, 6814.5, 7129.5, 7444.5, 7759.5, 8074.5, 8389.5, 18724.5,\n        19651.5, 20578.5, 21505.5, 22432.5, 23359.5, 24286.5, 20234.5, 21236.5,\n        22238.5, 23240.5, 24242.5, 25244.5, 26246.5, 21744.5, 22821.5, 23898.5,\n        24975.5, 26052.5, 27129.5, 28206.5, 8853.5, 9282.5, 9711.5, 10140.5,\n        10569.5, 10998.5, 11427.5, 5746.75, 6010.75, 6274.75, 6538.75, 6802.75,\n        7066.75, 7330.75, 6168.75, 6452.25, 6735.75, 7019.25, 7302.75, 7586.25,\n        7869.75, 6590.75, 6893.75, 7196.75, 7499.75, 7802.75, 8105.75, 8408.75,\n        2036.25, 2119.5, 2202.75, 2286.0, 2369.25, 2452.5, 2535.75\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 7],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  def testSeparableConv2D(self):\n    self._testSeparableConv2D(\"NHWC\")\n\n  def disabledtestSeparableConv2DNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2D(\"NCHW\")\n\n  def _testSeparableConv2DEqualInputOutputDepth(self, data_format):\n    # The output is the result of two convolutions:\n    # First with tensor_in[1, 4, 4, 2] * filter1[2, 2, 3, 3].\n    # Second with intermediate_out[1, 4, 4, 6] * filter2[1, 1, 6, 6].\n    # Complexity is O(2*3*2*2 + 6*6*1*1) as opposed to O(2*6*2*2).\n    expected_output = [\n        5742.0, 6069.0, 6396.0, 6723.0, 7050.0, 7377.0, 7047.0, 7449.0, 7851.0,\n        8253.0, 8655.0, 9057.0, 8352.0, 8829.0, 9306.0, 9783.0, 10260.0,\n        10737.0, 3582.0, 3783.0, 3984.0, 4185.0, 4386.0, 4587.0, 10962.0,\n        11589.0, 12216.0, 12843.0, 13470.0, 14097.0, 12267.0, 12969.0, 13671.0,\n        14373.0, 15075.0, 15777.0, 13572.0, 14349.0, 15126.0, 15903.0, 16680.0,\n        17457.0, 5616.0, 5931.0, 6246.0, 6561.0, 6876.0, 7191.0, 16182.0,\n        17109.0, 18036.0, 18963.0, 19890.0, 20817.0, 17487.0, 18489.0, 19491.0,\n        20493.0, 21495.0, 22497.0, 18792.0, 19869.0, 20946.0, 22023.0, 23100.0,\n        24177.0, 7650.0, 8079.0, 8508.0, 8937.0, 9366.0, 9795.0, 4963.5, 5227.5,\n        5491.5, 5755.5, 6019.5, 6283.5, 5328.0, 5611.5, 5895.0, 6178.5, 6462.0,\n        6745.5, 5692.5, 5995.5, 6298.5, 6601.5, 6904.5, 7207.5, 1757.25, 1840.5,\n        1923.75, 2007.0, 2090.25, 2173.5\n    ]\n\n    self._VerifyValues(\n        tensor_in_sizes=[1, 4, 4, 2],\n        depthwise_filter_in_sizes=[2, 2, 2, 3],\n        pointwise_filter_in_sizes=[1, 1, 6, 6],\n        stride=1,\n        padding=\"SAME\",\n        expected=expected_output,\n        data_format=data_format)\n\n  @test_util.deprecated_graph_mode_only\n  def testSeparableConv2DEqualInputOutputDepth(self):\n    self._testSeparableConv2DEqualInputOutputDepth(\"NHWC\")\n\n  def testSeparableConv2DEqualInputOutputDepthNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2DEqualInputOutputDepth(\"NCHW\")\n\n  def _testSeparableConv2dExplicitPadding(self, data_format):\n    tensor_in_sizes = [1, 4, 4, 2]\n    depthwise_filter_in_sizes = [2, 2, 2, 3]\n    pointwise_filter_in_sizes = [1, 1, 6, 7]\n    padding = [[0, 0], [1, 2], [3, 4], [0, 0]]\n    with self.cached_session():\n      # Compute the 'expected' values by manually padding before calling\n      # separable_conv2d\n      t1 = self._InitValues(tensor_in_sizes)\n      t1 = array_ops.pad(t1, padding)\n      f1 = self._InitValues(depthwise_filter_in_sizes)\n      f1.set_shape(depthwise_filter_in_sizes)\n      f2 = self._InitValues(pointwise_filter_in_sizes)\n      conv = nn_impl.separable_conv2d(\n          t1,\n          f1,\n          f2,\n          strides=[1, 1, 1, 1],\n          padding=\"VALID\",\n          data_format=\"NHWC\")\n      expected = self.evaluate(conv)\n      expected = np.ravel(expected)\n    self._VerifyValues(\n        tensor_in_sizes=tensor_in_sizes,\n        depthwise_filter_in_sizes=depthwise_filter_in_sizes,\n        pointwise_filter_in_sizes=pointwise_filter_in_sizes,\n        stride=1,\n        padding=padding,\n        expected=expected,\n        data_format=data_format)\n\n  def testSeparableConv2dExplicitPadding(self):\n    self._testSeparableConv2dExplicitPadding(\"NHWC\")\n\n  def testSeparableConv2dExplicitPaddingNCHW(self):\n    if not test.is_gpu_available():\n      return\n    self._testSeparableConv2dExplicitPadding(\"NCHW\")\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass DeepConv2DTest(test.TestCase):\n\n  def _CompareFwdConv2D(self, tensor_in_sizes, filter_in_sizes, conv_strides,\n                        padding):\n    \"\"\"Verifies that DeepConv2D and Conv2D produce the same values.\n\n    Args:\n      tensor_in_sizes: Input tensor dimensions in\n        [batch, input_rows, input_cols, input_depth].\n      filter_in_sizes: Filter tensor dimensions in\n        [kernel_rows, kernel_cols, input_depth, output_depth].\n      conv_strides: [row_stride, col_stride] for the convolution;\n      padding: Padding type.\n    \"\"\"\n    x1 = np.random.rand(*tensor_in_sizes).astype(np.float32)\n    x2 = np.random.rand(*filter_in_sizes).astype(np.float32)\n\n    with self.cached_session(use_gpu=False):\n      t1 = constant_op.constant(x1, shape=tensor_in_sizes)\n      t2 = constant_op.constant(x2, shape=filter_in_sizes)\n      strides = [1] + conv_strides + [1]\n\n      conv = nn_ops.conv2d(t1, t2, strides=strides, padding=padding)\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"0\"\n      values_expect = self.evaluate([conv])\n\n      os.environ[\"TF_USE_DEEP_CONV2D\"] = \"1\"\n      values_test = self.evaluate([conv])\n\n      self.assertAllClose(values_expect, values_test, rtol=1e-5, atol=1e-5)\n\n  def _RunTestCases(self, conv_strides, padding):\n    input_sizes = [[5, 5, 5, 1248], [3, 17, 17, 192], [2, 35, 35, 288],\n                   [2, 6, 8, 517], [2, 7, 4, 81], [3, 11, 3, 77]]\n    filter_sizes = [[3, 3, 1248, 128], [3, 3, 192, 192], [3, 3, 288, 384],\n                    [3, 3, 517, 64], [3, 3, 81, 77], [3, 3, 77, 181]]\n    for input_shape, filter_shape in zip(input_sizes, filter_sizes):\n      self._CompareFwdConv2D(input_shape, filter_shape, conv_strides, padding)\n\n  def testConv2D3x3FilterStride1x1Valid(self):\n    self._RunTestCases([1, 1], \"VALID\")\n\n  def testConv2D3x3FilterStride1x1Same(self):\n    self._RunTestCases([1, 1], \"SAME\")\n\n\nclass Conv2DBenchmark(test.Benchmark):\n\n  def benchmarkGPUConvStackFirst(self):\n    # Benchmark the first iteration of a conv-net with many identical conv\n    # operations.\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default(), session_lib.Session() as session:\n      batch_size = 1\n      timesteps = 600\n      features = 1\n\n      inputs = random_ops.random_uniform(\n          [batch_size, 1, timesteps, features], seed=1234)\n      num_outputs_list = [512] * 40 + [1]\n      kernel_w = 3\n      x = inputs\n      for num_outputs in num_outputs_list:\n        x = convolutional.conv2d(x, num_outputs, [1, kernel_w])\n      outputs = x\n\n      self.evaluate(variables.global_variables_initializer())\n      num_iterations = 4\n      for iter_index in range(num_iterations):\n        start = time.time()\n        session.run(outputs)\n        wall_time = time.time() - start\n        self.report_benchmark(\n            name=\"conv_stack_iter_%d\" % iter_index, wall_time=wall_time)\n        tf_logging.info(\"conv_stack_iter_%d: %.4f\" % (iter_index, wall_time))\n\n  def _bench_op(self, name, op, burn_iters, num_iters):\n    config = config_pb2.ConfigProto()\n    # Prevent Grappler from optimizing away the entire graph.\n    config.graph_options.rewrite_options.dependency_optimization = (\n        rewriter_config_pb2.RewriterConfig.OFF)\n    with session_lib.Session(config=config) as session:\n      self.evaluate(variables.global_variables_initializer())\n      self.run_op_benchmark(\n          session, op, burn_iters=burn_iters, min_iters=num_iters, name=name)\n\n  def benchmarkExplicitVsManualPadding(self):\n    \"\"\"Compare performance of EXPLICIT padding and calling tf.pad.\n\n    A Conv2D op with EXPLICIT padding is benchmarked, and a tf.pad with the same\n    padding followed by an equivalent Conv2D op is benchmarked.\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_iters = 300\n      batch_size = 64\n      # The input and filter correspond to the first layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              3,\n              224,\n              224\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([7, 7, 3, 64]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 2, 2]\n      padding = [(0, 0), (0, 0), (3, 3), (3, 3)]\n      output_explicit_pad = nn_ops.conv2d(\n          input, filter, strides, padding=padding, data_format=\"NCHW\")\n      input_padded = array_ops.pad(input, padding)\n      output_manual_pad = nn_ops.conv2d(\n          input_padded, filter, strides, padding=\"VALID\", data_format=\"NCHW\")\n      # Benchmark just the forward pass.\n      self._bench_op(\"explicit_pad_forward\", output_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"manual_pad_forward\", output_manual_pad.op, burn_iters,\n                     num_iters)\n\n      # Benchmark both the forward and backwards passes.\n      input_grad_explicit_pad, filter_grad_explicit_pad = (\n          gradients_impl.gradients(output_explicit_pad, [input, filter]))\n      self._bench_op(\n          \"explicit_pad_backward\",\n          control_flow_ops.group(input_grad_explicit_pad,\n                                 filter_grad_explicit_pad), burn_iters,\n          num_iters)\n      input_grad_manual_pad, filter_grad_manual_pad = gradients_impl.gradients(\n          output_manual_pad, [input, filter])\n      self._bench_op(\n          \"manual_pad_backward\",\n          control_flow_ops.group(input_grad_manual_pad, filter_grad_manual_pad),\n          burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingGraph(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in graph mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. The purpose is to ensure EXPLICIT padding is just as\n    efficient as the SAME case\n    \"\"\"\n    if not test.is_gpu_available():\n      return\n\n    with ops.Graph().as_default():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n\n      for _ in range(num_convs):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n      grad_explicit_pad, = gradients_impl.gradients(output_explicit_pad, filter)\n      grad_same_pad, = gradients_impl.gradients(output_same_pad, filter)\n      self._bench_op(\"graph_explicit_pad\", grad_explicit_pad.op, burn_iters,\n                     num_iters)\n      self._bench_op(\"graph_same_pad\", grad_same_pad.op, burn_iters, num_iters)\n\n  def benchmarkExplicitVsSamePaddingEager(self):\n    \"\"\"Compare performance of EXPLICIT and SAME padding in eager mode.\n\n    A Conv2D op with SAME padding is benchmarked, and an equivalent Conv2D op\n    with explicit padding is benchmarked, where the padding is the same as in\n    the SAME case. Currently, EXPLICIT padding is slightly slower, due to the\n    fact the Python padding list must be checked and processed before the Conv2D\n    op can run.\n    \"\"\"\n    # TODO(reedwm): Make EXPLICIT padding as fast as SAME padding.\n    if not test.is_gpu_available():\n      return\n\n    with context.eager_mode():\n      burn_iters = 15\n      num_convs = 20\n      num_iters = 50\n      batch_size = 64\n      # The input and filter correspond to a middle layer of Resnet50.\n      input = variables.Variable(  # pylint: disable=redefined-builtin\n          random_ops.random_uniform([\n              batch_size,\n              256,\n              14,\n              14\n          ]))\n      filter = variables.Variable(random_ops.random_uniform([3, 3, 256, 256]))  # pylint: disable=redefined-builtin\n      strides = [1, 1, 1, 1]\n      padding = [(0, 0), (0, 0), (1, 1), (1, 1)]\n      output_explicit_pad = input\n      output_same_pad = input\n      for _ in range(burn_iters):\n        output_explicit_pad = nn_ops.conv2d(\n            output_explicit_pad,\n            filter,\n            strides,\n            padding=padding,\n            data_format=\"NCHW\")\n        output_same_pad = nn_ops.conv2d(\n            output_same_pad,\n            filter,\n            strides,\n            padding=\"SAME\",\n            data_format=\"NCHW\")\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_explicit_pad = nn_ops.conv2d(\n                output_explicit_pad,\n                filter,\n                strides,\n                padding=padding,\n                data_format=\"NCHW\")\n          tape.gradient(output_explicit_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_explicit_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n      start = time.time()\n      for _ in range(num_iters):\n        with backprop.GradientTape() as tape:\n          for _ in range(num_convs):\n            output_same_pad = nn_ops.conv2d(\n                output_same_pad,\n                filter,\n                strides,\n                padding=\"SAME\",\n                data_format=\"NCHW\")\n          tape.gradient(output_same_pad, filter)\n      end = time.time()\n      self.report_benchmark(\n          name=\"eager_same_pad\",\n          wall_time=(end - start) / num_iters,\n          iters=num_iters)\n\n\ndef GetInceptionFwdTest(input_size, filter_size, stride, padding,\n                        gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionFwd %s\", (input_size, filter_size,\n                                                   stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionFwd %s\", (input_size, filter_size, stride,\n                                                padding))\n    self._CompareFwdValues(input_size, filter_size, [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionFwdDilatedConvTest(input_size, filter_size, stride, padding):\n\n  def Test(self):\n    if stride == 1:\n      tf_logging.info(\"Testing InceptionFwd with dilations %s\",\n                      (input_size, filter_size, stride, padding))\n      self._VerifyDilatedConvValues(\n          tensor_in_sizes=input_size,\n          filter_in_sizes=filter_size,\n          strides=[stride, stride],\n          dilations=[2, 2],\n          padding=padding,\n          rtol=5e-4)\n\n  return Test\n\n\ndef GetInceptionBackInputTest(input_size, filter_size, output_size, stride,\n                              padding,\n                              gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackInput %s\",\n                      (input_size, filter_size, output_size, stride, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackInput %s\",\n                    (input_size, filter_size, output_size, stride, padding))\n    self._CompareBackpropInput(input_size, filter_size, output_size,\n                               [stride, stride], padding)\n\n  return Test\n\n\ndef GetInceptionBackFilterTest(input_size, filter_size, output_size, strides,\n                               padding, gpu_only=False):\n\n  def Test(self):\n    if gpu_only and not test.is_gpu_available():\n      tf_logging.info(\"Skipping InceptionBackFilter %s\",\n                      (input_size, filter_size, output_size, strides, padding))\n      return\n    tf_logging.info(\"Testing InceptionBackFilter %s\",\n                    (input_size, filter_size, output_size, strides, padding))\n    self._CompareBackFilter(input_size, filter_size, output_size, strides,\n                            padding)\n\n  return Test\n\n\n@test_util.run_all_without_tensor_float_32(\"Avoid TF32 conv on GPU\")\nclass FusedConv2DTest(test.TestCase):\n\n  def _CreateNumpyTensor(self, shape):\n    total_size = np.prod(shape)\n    return np.arange(1, total_size + 1, dtype=np.float32).reshape(shape)\n\n  def _CreateConv2D(self,\n                    input_values,\n                    filters,\n                    strides=[1, 1],\n                    padding=\"SAME\"):\n    return nn_ops.convolution(\n        input_values, filters, strides=strides, padding=padding)\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 1.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountOne(self):\n    expected_output = [\n        113377, 125570, 77305, 86738, 19433, 22226, 60681, 70722, 36291, 43718,\n        7143, 9206, 9785, 12098, 4783, 6366, 779, 1134\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has a total refcount of 2, and Add is its last consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddLast(self):\n    expected_output = [\n        1.907175e+06, 2.253505e+06, 7.809210e+05, 9.537180e+05, 1.184170e+05,\n        1.523070e+05, 5.367010e+05, 6.803700e+05, 1.867090e+05, 2.529460e+05,\n        2.362300e+04, 3.522600e+04, 5.121700e+04, 7.168300e+04, 1.494300e+04,\n        2.347400e+04, 1.558000e+03, 2.903000e+03\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv2, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2 and Add (in the fused Conv2D op) is its first consumer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndRunAddFirst(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu = nn_ops.relu(add)\n    output = math_ops.add_n([relu, conv2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add has refcount 2, and there is no dependency between its two consumers.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithRefCountTwoAndNoDependence(self):\n    expected_output = [\n        176161, 194450, 120673, 134822, 30545, 34734, 96041, 111102, 58149,\n        69289, 11745, 14839, 15833, 19302, 7965, 10339, 1345, 1877\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n    # To get different weights for filter\n    offset = 1\n\n    conv1 = self._CreateConv2D(x, filter_in)\n    conv2 = self._CreateConv2D(conv1, filter_in + offset)\n\n    conv = self._CreateConv2D(conv1, filter_in - offset)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv2])\n\n    relu1 = nn_ops.relu(add)\n    relu2 = nn_ops.relu(conv2)\n    output = math_ops.add_n([relu1, relu2])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(output).reshape(-1))\n\n  # Tests tensor forwarding of a fused Conv2D+BiasAdd+Add op when the input to\n  # Add is the same as the input to the fused Conv2D op and needs a tensor\n  # buffer.\n  @test_util.run_in_graph_and_eager_modes(use_gpu=False)\n  def testAddWithSameSrcAndAddTensorBuffer(self):\n    expected_output = [\n        57157, 63298, 39249, 44026, 9971, 11402, 31193, 36306, 19126, 22948,\n        3970, 5060, 5135, 6350, 2666, 3524, 461, 674\n    ]\n    tensor_in_sizes = [1, 3, 3, 2]\n    filter_in_sizes = [2, 2, 2, 2]\n    bias_in_sizes = [2]\n\n    x = self._CreateNumpyTensor(tensor_in_sizes)\n    filter_in = self._CreateNumpyTensor(filter_in_sizes)\n    bias_in = self._CreateNumpyTensor(bias_in_sizes)\n\n    conv1 = self._CreateConv2D(x, filter_in)\n\n    conv = self._CreateConv2D(conv1, filter_in)\n    bias_add = nn_ops.bias_add(conv, bias_in)\n    add = math_ops.add_n([bias_add, conv1])\n\n    self.assertAllEqual(\n        np.rint(expected_output),\n        self.evaluate(add).reshape(-1))\n\n  # Fused resize and pad conv.\n  @test_util.run_in_graph_and_eager_modes()\n  def testResizeAndPadLargeResize(self):\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                \"Encountered overflow\"):\n      mode = \"REFLECT\"\n      strides = [1, 1, 1, 1]\n      padding = \"SAME\"\n      resize_align_corners = False\n      tensor = constant_op.constant(\n          147, shape=[3, 3, 1, 4], dtype=dtypes.float32)\n      size = constant_op.constant([1879048192, 1879048192], dtype=dtypes.int32)\n      paddings = constant_op.constant([[0, 0], [0, 0], [0, 0], [0, 0]],\n                                      dtype=dtypes.int32)\n      kernel = constant_op.constant(\n          123, shape=[1, 3, 4, 1], dtype=dtypes.float32)\n      self.evaluate(\n          gen_nn_ops.fused_resize_and_pad_conv2d(\n              input=tensor,\n              size=size,\n              paddings=paddings,\n              filter=kernel,\n              mode=mode,\n              strides=strides,\n              padding=padding,\n              resize_align_corners=resize_align_corners))\n\n\nif __name__ == \"__main__\":\n  for index, (input_size_, filter_size_, output_size_, stride_,\n              padding_) in enumerate(GetShrunkInceptionShapes()):\n    setattr(Conv2DTest, \"testInceptionFwd_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionFwdTest(input_size_, filter_size_, stride_,\n                                    padding_)))\n    setattr(\n        Conv2DTest, \"testInceptionFwdDilatedConv_\" + str(index),\n        test_util.run_in_graph_and_eager_modes(GetInceptionFwdDilatedConvTest(\n            input_size_, filter_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackInput_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackInputTest(input_size_, filter_size_,\n                                          output_size_, stride_, padding_)))\n    setattr(Conv2DTest, \"testInceptionBackFilter_\" + str(index),\n            test_util.run_in_graph_and_eager_modes(\n                GetInceptionBackFilterTest(input_size_, filter_size_,\n                                           output_size_, [stride_, stride_],\n                                           padding_)))\n\n  # TODO(b/35359731)\n  # Fwd, BckInput, and BackFilter to test that for certain input parameter\n  # set, winograd nonfused algorithm will be excluded from conv autotune. If\n  # in such case, winograd nonfused algorithm is added as one option of the\n  # conv autotune, and cuDNN version is smaller than 7, the following tests\n  # will fail.\n  ishape = [1, 400, 400, 1]\n  fshape = [1, 1, 1, 256]\n  oshape = [1, 400, 400, 256]\n  setattr(Conv2DTest, \"testInceptionFwd_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdTest(ishape, fshape, 1, \"SAME\", gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionFwdDilatedConv_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionFwdDilatedConvTest(ishape, fshape, 1, \"SAME\")))\n  setattr(Conv2DTest, \"testInceptionBackInput_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackInputTest(ishape, fshape, oshape, 1, \"SAME\",\n                                        gpu_only=True)))\n  setattr(Conv2DTest, \"testInceptionBackFilter_No_Winograd_Nonfused\",\n          test_util.run_in_graph_and_eager_modes(\n              GetInceptionBackFilterTest(ishape, fshape, oshape, [1, 1], \"SAME\",\n                                         gpu_only=True)))\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/conv_ops_fused_image_transform.cc", "tensorflow/core/ops/nn_ops.cc", "tensorflow/python/kernel_tests/nn_ops/conv_ops_test.py"], "buggy_code_start_loc": [670, 584, 3431], "buggy_code_end_loc": [672, 598, 3431], "fixing_code_start_loc": [670, 584, 3432], "fixing_code_end_loc": [675, 598, 3459], "type": "CWE-131", "message": "TensorFlow is an open source platform for machine learning. When `tf.raw_ops.FusedResizeAndPadConv2D` is given a large tensor shape, it overflows. We have patched the issue in GitHub commit d66e1d568275e6a2947de97dca7a102a211e01ce. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-41885", "sourceIdentifier": "security-advisories@github.com", "published": "2022-11-18T22:15:14.147", "lastModified": "2022-11-23T17:44:07.997", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. When `tf.raw_ops.FusedResizeAndPadConv2D` is given a large tensor shape, it overflows. We have patched the issue in GitHub commit d66e1d568275e6a2947de97dca7a102a211e01ce. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-131"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-131"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.4", "matchCriteriaId": "7EBE1EC2-A67A-4885-B1BB-A2BB5D18459D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "B5F5D78E-DBBA-4CC7-ADB1-454F86700280"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "EF6375A0-9871-4072-95F0-4266620F4713"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "534F3684-3E31-4A0A-9821-70EEFA8AB258"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "E1D5EAED-B494-4E30-AB79-99BD1876B5FA"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_fused_image_transform.cc", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/d66e1d568275e6a2947de97dca7a102a211e01ce", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-762h-vpvw-3rcx", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/d66e1d568275e6a2947de97dca7a102a211e01ce"}}