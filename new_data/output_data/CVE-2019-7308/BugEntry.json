{"buggy_code": ["/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of version 2 of the GNU General Public\n * License as published by the Free Software Foundation.\n */\n#ifndef _LINUX_BPF_VERIFIER_H\n#define _LINUX_BPF_VERIFIER_H 1\n\n#include <linux/bpf.h> /* for enum bpf_reg_type */\n#include <linux/filter.h> /* for MAX_BPF_STACK */\n#include <linux/tnum.h>\n\n/* Maximum variable offset umax_value permitted when resolving memory accesses.\n * In practice this is far bigger than any realistic pointer offset; this limit\n * ensures that umax_value + (int)off + (int)size cannot overflow a u64.\n */\n#define BPF_MAX_VAR_OFF\t(1 << 29)\n/* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures\n * that converting umax_value to int cannot overflow.\n */\n#define BPF_MAX_VAR_SIZ\t(1 << 29)\n\n/* Liveness marks, used for registers and spilled-regs (in stack slots).\n * Read marks propagate upwards until they find a write mark; they record that\n * \"one of this state's descendants read this reg\" (and therefore the reg is\n * relevant for states_equal() checks).\n * Write marks collect downwards and do not propagate; they record that \"the\n * straight-line code that reached this state (from its parent) wrote this reg\"\n * (and therefore that reads propagated from this state or its descendants\n * should not propagate to its parent).\n * A state with a write mark can receive read marks; it just won't propagate\n * them to its parent, since the write mark is a property, not of the state,\n * but of the link between it and its parent.  See mark_reg_read() and\n * mark_stack_slot_read() in kernel/bpf/verifier.c.\n */\nenum bpf_reg_liveness {\n\tREG_LIVE_NONE = 0, /* reg hasn't been read or written this branch */\n\tREG_LIVE_READ, /* reg was read, so we're sensitive to initial value */\n\tREG_LIVE_WRITTEN, /* reg was written first, screening off later reads */\n\tREG_LIVE_DONE = 4, /* liveness won't be updating this register anymore */\n};\n\nstruct bpf_reg_state {\n\t/* Ordering of fields matters.  See states_equal() */\n\tenum bpf_reg_type type;\n\tunion {\n\t\t/* valid when type == PTR_TO_PACKET */\n\t\tu16 range;\n\n\t\t/* valid when type == CONST_PTR_TO_MAP | PTR_TO_MAP_VALUE |\n\t\t *   PTR_TO_MAP_VALUE_OR_NULL\n\t\t */\n\t\tstruct bpf_map *map_ptr;\n\n\t\t/* Max size from any of the above. */\n\t\tunsigned long raw;\n\t};\n\t/* Fixed part of pointer offset, pointer types only */\n\ts32 off;\n\t/* For PTR_TO_PACKET, used to find other pointers with the same variable\n\t * offset, so they can share range knowledge.\n\t * For PTR_TO_MAP_VALUE_OR_NULL this is used to share which map value we\n\t * came from, when one is tested for != NULL.\n\t * For PTR_TO_SOCKET this is used to share which pointers retain the\n\t * same reference to the socket, to determine proper reference freeing.\n\t */\n\tu32 id;\n\t/* For scalar types (SCALAR_VALUE), this represents our knowledge of\n\t * the actual value.\n\t * For pointer types, this represents the variable part of the offset\n\t * from the pointed-to object, and is shared with all bpf_reg_states\n\t * with the same id as us.\n\t */\n\tstruct tnum var_off;\n\t/* Used to determine if any memory access using this register will\n\t * result in a bad access.\n\t * These refer to the same value as var_off, not necessarily the actual\n\t * contents of the register.\n\t */\n\ts64 smin_value; /* minimum possible (s64)value */\n\ts64 smax_value; /* maximum possible (s64)value */\n\tu64 umin_value; /* minimum possible (u64)value */\n\tu64 umax_value; /* maximum possible (u64)value */\n\t/* parentage chain for liveness checking */\n\tstruct bpf_reg_state *parent;\n\t/* Inside the callee two registers can be both PTR_TO_STACK like\n\t * R1=fp-8 and R2=fp-8, but one of them points to this function stack\n\t * while another to the caller's stack. To differentiate them 'frameno'\n\t * is used which is an index in bpf_verifier_state->frame[] array\n\t * pointing to bpf_func_state.\n\t */\n\tu32 frameno;\n\tenum bpf_reg_liveness live;\n};\n\nenum bpf_stack_slot_type {\n\tSTACK_INVALID,    /* nothing was stored in this stack slot */\n\tSTACK_SPILL,      /* register spilled into stack */\n\tSTACK_MISC,\t  /* BPF program wrote some data into this slot */\n\tSTACK_ZERO,\t  /* BPF program wrote constant zero */\n};\n\n#define BPF_REG_SIZE 8\t/* size of eBPF register in bytes */\n\nstruct bpf_stack_state {\n\tstruct bpf_reg_state spilled_ptr;\n\tu8 slot_type[BPF_REG_SIZE];\n};\n\nstruct bpf_reference_state {\n\t/* Track each reference created with a unique id, even if the same\n\t * instruction creates the reference multiple times (eg, via CALL).\n\t */\n\tint id;\n\t/* Instruction where the allocation of this reference occurred. This\n\t * is used purely to inform the user of a reference leak.\n\t */\n\tint insn_idx;\n};\n\n/* state of the program:\n * type of all registers and stack info\n */\nstruct bpf_func_state {\n\tstruct bpf_reg_state regs[MAX_BPF_REG];\n\t/* index of call instruction that called into this func */\n\tint callsite;\n\t/* stack frame number of this function state from pov of\n\t * enclosing bpf_verifier_state.\n\t * 0 = main function, 1 = first callee.\n\t */\n\tu32 frameno;\n\t/* subprog number == index within subprog_stack_depth\n\t * zero == main subprog\n\t */\n\tu32 subprogno;\n\n\t/* The following fields should be last. See copy_func_state() */\n\tint acquired_refs;\n\tstruct bpf_reference_state *refs;\n\tint allocated_stack;\n\tstruct bpf_stack_state *stack;\n};\n\n#define MAX_CALL_FRAMES 8\nstruct bpf_verifier_state {\n\t/* call stack tracking */\n\tstruct bpf_func_state *frame[MAX_CALL_FRAMES];\n\tu32 curframe;\n};\n\n#define bpf_get_spilled_reg(slot, frame)\t\t\t\t\\\n\t(((slot < frame->allocated_stack / BPF_REG_SIZE) &&\t\t\\\n\t  (frame->stack[slot].slot_type[0] == STACK_SPILL))\t\t\\\n\t ? &frame->stack[slot].spilled_ptr : NULL)\n\n/* Iterate over 'frame', setting 'reg' to either NULL or a spilled register. */\n#define bpf_for_each_spilled_reg(iter, frame, reg)\t\t\t\\\n\tfor (iter = 0, reg = bpf_get_spilled_reg(iter, frame);\t\t\\\n\t     iter < frame->allocated_stack / BPF_REG_SIZE;\t\t\\\n\t     iter++, reg = bpf_get_spilled_reg(iter, frame))\n\n/* linked list of verifier states used to prune search */\nstruct bpf_verifier_state_list {\n\tstruct bpf_verifier_state state;\n\tstruct bpf_verifier_state_list *next;\n};\n\nstruct bpf_insn_aux_data {\n\tunion {\n\t\tenum bpf_reg_type ptr_type;\t/* pointer type for load/store insns */\n\t\tunsigned long map_state;\t/* pointer/poison value for maps */\n\t\ts32 call_imm;\t\t\t/* saved imm field of call insn */\n\t};\n\tint ctx_field_size; /* the ctx field size for load insn, maybe 0 */\n\tint sanitize_stack_off; /* stack slot to be cleared */\n\tbool seen; /* this insn was processed by the verifier */\n};\n\n#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */\n\n#define BPF_VERIFIER_TMP_LOG_SIZE\t1024\n\nstruct bpf_verifier_log {\n\tu32 level;\n\tchar kbuf[BPF_VERIFIER_TMP_LOG_SIZE];\n\tchar __user *ubuf;\n\tu32 len_used;\n\tu32 len_total;\n};\n\nstatic inline bool bpf_verifier_log_full(const struct bpf_verifier_log *log)\n{\n\treturn log->len_used >= log->len_total - 1;\n}\n\nstatic inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)\n{\n\treturn log->level && log->ubuf && !bpf_verifier_log_full(log);\n}\n\n#define BPF_MAX_SUBPROGS 256\n\nstruct bpf_subprog_info {\n\tu32 start; /* insn idx of function entry point */\n\tu32 linfo_idx; /* The idx to the main_prog->aux->linfo */\n\tu16 stack_depth; /* max. stack depth used by this function */\n};\n\n/* single container for all structs\n * one verifier_env per bpf_check() call\n */\nstruct bpf_verifier_env {\n\tu32 insn_idx;\n\tu32 prev_insn_idx;\n\tstruct bpf_prog *prog;\t\t/* eBPF program being verified */\n\tconst struct bpf_verifier_ops *ops;\n\tstruct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */\n\tint stack_size;\t\t\t/* number of states to be processed */\n\tbool strict_alignment;\t\t/* perform strict pointer alignment checks */\n\tstruct bpf_verifier_state *cur_state; /* current verifier state */\n\tstruct bpf_verifier_state_list **explored_states; /* search pruning optimization */\n\tstruct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */\n\tu32 used_map_cnt;\t\t/* number of used maps */\n\tu32 id_gen;\t\t\t/* used to generate unique reg IDs */\n\tbool allow_ptr_leaks;\n\tbool seen_direct_write;\n\tstruct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */\n\tconst struct bpf_line_info *prev_linfo;\n\tstruct bpf_verifier_log log;\n\tstruct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];\n\tu32 subprog_cnt;\n};\n\n__printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,\n\t\t\t\t      const char *fmt, va_list args);\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...);\n\nstatic inline struct bpf_func_state *cur_func(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[cur->curframe];\n}\n\nstatic inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)\n{\n\treturn cur_func(env)->regs;\n}\n\nint bpf_prog_offload_verifier_prep(struct bpf_prog *prog);\nint bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,\n\t\t\t\t int insn_idx, int prev_insn_idx);\nint bpf_prog_offload_finalize(struct bpf_verifier_env *env);\n\n#endif /* _LINUX_BPF_VERIFIER_H */\n", "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n * Copyright (c) 2016 Facebook\n * Copyright (c) 2018 Covalent IO, Inc. http://covalent.io\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of version 2 of the GNU General Public\n * License as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n */\n#include <uapi/linux/btf.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n#include <linux/bsearch.h>\n#include <linux/sort.h>\n#include <linux/perf_event.h>\n#include <linux/ctype.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n};\n\n/* bpf_check() is a static code analyzer that walks eBPF program\n * instruction by instruction and updates register/stack state.\n * All paths of conditional branches are analyzed until 'bpf_exit' insn.\n *\n * The first pass is depth-first-search to check that the program is a DAG.\n * It rejects the following programs:\n * - larger than BPF_MAXINSNS insns\n * - if loop is present (detected via back-edge)\n * - unreachable insns exist (shouldn't be a forest. program = one function)\n * - out of bounds or malformed jumps\n * The second pass is all possible path descent from the 1st insn.\n * Since it's analyzing all pathes through the program, the length of the\n * analysis is limited to 64k insn, which may be hit even if total number of\n * insn is less then 4K, but there are too many branches that change stack/regs.\n * Number of 'branches to be analyzed' is limited to 1k\n *\n * On entry to each instruction, each register has a type, and the instruction\n * changes the types of the registers depending on instruction semantics.\n * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is\n * copied to R1.\n *\n * All registers are 64-bit.\n * R0 - return register\n * R1-R5 argument passing registers\n * R6-R9 callee saved registers\n * R10 - frame pointer read-only\n *\n * At the start of BPF program the register R1 contains a pointer to bpf_context\n * and has type PTR_TO_CTX.\n *\n * Verifier tracks arithmetic operations on pointers in case:\n *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),\n * 1st insn copies R10 (which has FRAME_PTR) type into R1\n * and 2nd arithmetic instruction is pattern matched to recognize\n * that it wants to construct a pointer to some element within stack.\n * So after 2nd insn, the register R1 has type PTR_TO_STACK\n * (and -20 constant is saved for further stack bounds checking).\n * Meaning that this reg is a pointer to stack plus known immediate constant.\n *\n * Most of the time the registers have SCALAR_VALUE type, which\n * means the register has some value, but it's not a valid pointer.\n * (like pointer plus pointer becomes SCALAR_VALUE type)\n *\n * When verifier sees load or store instructions the type of base register\n * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK, PTR_TO_SOCKET. These are\n * four pointer types recognized by check_mem_access() function.\n *\n * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'\n * and the range of [ptr, ptr + map's value_size) is accessible.\n *\n * registers used to pass values to function calls are checked against\n * function argument constraints.\n *\n * ARG_PTR_TO_MAP_KEY is one of such argument constraints.\n * It means that the register type passed to this function must be\n * PTR_TO_STACK and it will be used inside the function as\n * 'pointer to map element key'\n *\n * For example the argument constraints for bpf_map_lookup_elem():\n *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,\n *   .arg1_type = ARG_CONST_MAP_PTR,\n *   .arg2_type = ARG_PTR_TO_MAP_KEY,\n *\n * ret_type says that this function returns 'pointer to map elem value or null'\n * function expects 1st argument to be a const pointer to 'struct bpf_map' and\n * 2nd argument should be a pointer to stack, which will be used inside\n * the helper function as a pointer to map element key.\n *\n * On the kernel side the helper function looks like:\n * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n * {\n *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;\n *    void *key = (void *) (unsigned long) r2;\n *    void *value;\n *\n *    here kernel can access 'key' and 'map' pointers safely, knowing that\n *    [key, key + map->key_size) bytes are valid and were initialized on\n *    the stack of eBPF program.\n * }\n *\n * Corresponding eBPF program may look like:\n *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK\n *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP\n *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n * here verifier looks at prototype of map_lookup_elem() and sees:\n * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,\n * Now verifier knows that this map has key of R1->map_ptr->key_size bytes\n *\n * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,\n * Now verifier checks that [R2, R2 + map's key_size) are within stack limits\n * and were initialized prior to this call.\n * If it's ok, then verifier allows this BPF_CALL insn and looks at\n * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets\n * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function\n * returns ether pointer to map value or NULL.\n *\n * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'\n * insn, the register holding that pointer in the true branch changes state to\n * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false\n * branch. See check_cond_jmp_op().\n *\n * After the call R0 is set to return type of the function and registers R1-R5\n * are set to NOT_INIT to indicate that they are no longer readable.\n *\n * The following reference types represent a potential reference to a kernel\n * resource which, after first being allocated, must be checked and freed by\n * the BPF program:\n * - PTR_TO_SOCKET_OR_NULL, PTR_TO_SOCKET\n *\n * When the verifier sees a helper call return a reference type, it allocates a\n * pointer id for the reference and stores it in the current function state.\n * Similar to the way that PTR_TO_MAP_VALUE_OR_NULL is converted into\n * PTR_TO_MAP_VALUE, PTR_TO_SOCKET_OR_NULL becomes PTR_TO_SOCKET when the type\n * passes through a NULL-check conditional. For the branch wherein the state is\n * changed to CONST_IMM, the verifier releases the reference.\n *\n * For each helper function that allocates a reference, such as\n * bpf_sk_lookup_tcp(), there is a corresponding release function, such as\n * bpf_sk_release(). When a reference type passes into the release function,\n * the verifier also releases the reference. If any unchecked or unreleased\n * reference remains at the end of the program, the verifier rejects it.\n */\n\n/* verifier_state + insn_idx are pushed to stack when branch is encountered */\nstruct bpf_verifier_stack_elem {\n\t/* verifer state is 'st'\n\t * before processing instruction 'insn_idx'\n\t * and after processing instruction 'prev_insn_idx'\n\t */\n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n};\n\n#define BPF_COMPLEXITY_LIMIT_INSNS\t131072\n#define BPF_COMPLEXITY_LIMIT_STACK\t1024\n#define BPF_COMPLEXITY_LIMIT_STATES\t64\n\n#define BPF_MAP_PTR_UNPRIV\t1UL\n#define BPF_MAP_PTR_POISON\t((void *)((0xeB9FUL << 1) +\t\\\n\t\t\t\t\t  POISON_POINTER_DELTA))\n#define BPF_MAP_PTR(X)\t\t((struct bpf_map *)((X) & ~BPF_MAP_PTR_UNPRIV))\n\nstatic bool bpf_map_ptr_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn BPF_MAP_PTR(aux->map_state) == BPF_MAP_PTR_POISON;\n}\n\nstatic bool bpf_map_ptr_unpriv(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_state & BPF_MAP_PTR_UNPRIV;\n}\n\nstatic void bpf_map_ptr_store(struct bpf_insn_aux_data *aux,\n\t\t\t      const struct bpf_map *map, bool unpriv)\n{\n\tBUILD_BUG_ON((unsigned long)BPF_MAP_PTR_POISON & BPF_MAP_PTR_UNPRIV);\n\tunpriv |= bpf_map_ptr_unpriv(aux);\n\taux->map_state = (unsigned long)map |\n\t\t\t (unpriv ? BPF_MAP_PTR_UNPRIV : 0UL);\n}\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tint regno;\n\tint access_size;\n\ts64 msize_smax_value;\n\tu64 msize_umax_value;\n\tint ptr_id;\n};\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\nstatic const struct bpf_line_info *\nfind_linfo(const struct bpf_verifier_env *env, u32 insn_off)\n{\n\tconst struct bpf_line_info *linfo;\n\tconst struct bpf_prog *prog;\n\tu32 i, nr_linfo;\n\n\tprog = env->prog;\n\tnr_linfo = prog->aux->nr_linfo;\n\n\tif (!nr_linfo || insn_off >= prog->len)\n\t\treturn NULL;\n\n\tlinfo = prog->aux->linfo;\n\tfor (i = 1; i < nr_linfo; i++)\n\t\tif (insn_off < linfo[i].insn_off)\n\t\t\tbreak;\n\n\treturn &linfo[i - 1];\n}\n\nvoid bpf_verifier_vlog(struct bpf_verifier_log *log, const char *fmt,\n\t\t       va_list args)\n{\n\tunsigned int n;\n\n\tn = vscnprintf(log->kbuf, BPF_VERIFIER_TMP_LOG_SIZE, fmt, args);\n\n\tWARN_ONCE(n >= BPF_VERIFIER_TMP_LOG_SIZE - 1,\n\t\t  \"verifier log line truncated - local buffer too short\\n\");\n\n\tn = min(log->len_total - log->len_used - 1, n);\n\tlog->kbuf[n] = '\\0';\n\n\tif (!copy_to_user(log->ubuf + log->len_used, log->kbuf, n + 1))\n\t\tlog->len_used += n;\n\telse\n\t\tlog->ubuf = NULL;\n}\n\n/* log_level controls verbosity level of eBPF verifier.\n * bpf_verifier_log_write() is used to dump the verification trace to the log,\n * so the user can figure out what's wrong with the program\n */\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...)\n{\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\nEXPORT_SYMBOL_GPL(bpf_verifier_log_write);\n\n__printf(2, 3) static void verbose(void *private_data, const char *fmt, ...)\n{\n\tstruct bpf_verifier_env *env = private_data;\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\n\nstatic const char *ltrim(const char *s)\n{\n\twhile (isspace(*s))\n\t\ts++;\n\n\treturn s;\n}\n\n__printf(3, 4) static void verbose_linfo(struct bpf_verifier_env *env,\n\t\t\t\t\t u32 insn_off,\n\t\t\t\t\t const char *prefix_fmt, ...)\n{\n\tconst struct bpf_line_info *linfo;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tlinfo = find_linfo(env, insn_off);\n\tif (!linfo || linfo == env->prev_linfo)\n\t\treturn;\n\n\tif (prefix_fmt) {\n\t\tva_list args;\n\n\t\tva_start(args, prefix_fmt);\n\t\tbpf_verifier_vlog(&env->log, prefix_fmt, args);\n\t\tva_end(args);\n\t}\n\n\tverbose(env, \"%s\\n\",\n\t\tltrim(btf_name_by_offset(env->prog->aux->btf,\n\t\t\t\t\t linfo->line_off)));\n\n\tenv->prev_linfo = linfo;\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\nstatic bool reg_type_may_be_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_MAP_VALUE_OR_NULL ||\n\t       type == PTR_TO_SOCKET_OR_NULL;\n}\n\nstatic bool type_is_refcounted(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET;\n}\n\nstatic bool type_is_refcounted_or_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET || type == PTR_TO_SOCKET_OR_NULL;\n}\n\nstatic bool reg_is_refcounted(const struct bpf_reg_state *reg)\n{\n\treturn type_is_refcounted(reg->type);\n}\n\nstatic bool reg_is_refcounted_or_null(const struct bpf_reg_state *reg)\n{\n\treturn type_is_refcounted_or_null(reg->type);\n}\n\nstatic bool arg_type_is_refcounted(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_SOCKET;\n}\n\n/* Determine whether the function releases some resources allocated by another\n * function call. The first reference type argument will be assumed to be\n * released by release_reference().\n */\nstatic bool is_release_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_release;\n}\n\n/* string representation of 'enum bpf_reg_type' */\nstatic const char * const reg_type_str[] = {\n\t[NOT_INIT]\t\t= \"?\",\n\t[SCALAR_VALUE]\t\t= \"inv\",\n\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t[PTR_TO_MAP_VALUE_OR_NULL] = \"map_value_or_null\",\n\t[PTR_TO_STACK]\t\t= \"fp\",\n\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n\t[PTR_TO_FLOW_KEYS]\t= \"flow_keys\",\n\t[PTR_TO_SOCKET]\t\t= \"sock\",\n\t[PTR_TO_SOCKET_OR_NULL] = \"sock_or_null\",\n};\n\nstatic char slot_type_char[] = {\n\t[STACK_INVALID]\t= '?',\n\t[STACK_SPILL]\t= 'r',\n\t[STACK_MISC]\t= 'm',\n\t[STACK_ZERO]\t= '0',\n};\n\nstatic void print_liveness(struct bpf_verifier_env *env,\n\t\t\t   enum bpf_reg_liveness live)\n{\n\tif (live & (REG_LIVE_READ | REG_LIVE_WRITTEN | REG_LIVE_DONE))\n\t    verbose(env, \"_\");\n\tif (live & REG_LIVE_READ)\n\t\tverbose(env, \"r\");\n\tif (live & REG_LIVE_WRITTEN)\n\t\tverbose(env, \"w\");\n\tif (live & REG_LIVE_DONE)\n\t\tverbose(env, \"D\");\n}\n\nstatic struct bpf_func_state *func(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[reg->frameno];\n}\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t\tif (t == PTR_TO_STACK)\n\t\t\t\tverbose(env, \",call_%d\", func(env, reg)->callsite);\n\t\t} else {\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (state->stack[i].slot_type[0] == STACK_SPILL)\n\t\t\tverbose(env, \"=%s\",\n\t\t\t\treg_type_str[state->stack[i].spilled_ptr.type]);\n\t\telse\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tverbose(env, \"\\n\");\n}\n\n#define COPY_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\t\\\nstatic int copy_##NAME##_state(struct bpf_func_state *dst,\t\t\\\n\t\t\t       const struct bpf_func_state *src)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (!src->FIELD)\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\tif (WARN_ON_ONCE(dst->COUNT < src->COUNT)) {\t\t\t\\\n\t\t/* internal bug, make state invalid to reject the program */ \\\n\t\tmemset(dst, 0, sizeof(*dst));\t\t\t\t\\\n\t\treturn -EFAULT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tmemcpy(dst->FIELD, src->FIELD,\t\t\t\t\t\\\n\t       sizeof(*src->FIELD) * (src->COUNT / SIZE));\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* copy_reference_state() */\nCOPY_STATE_FN(reference, acquired_refs, refs, 1)\n/* copy_stack_state() */\nCOPY_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef COPY_STATE_FN\n\n#define REALLOC_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\\\nstatic int realloc_##NAME##_state(struct bpf_func_state *state, int size, \\\n\t\t\t\t  bool copy_old)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu32 old_size = state->COUNT;\t\t\t\t\t\\\n\tstruct bpf_##NAME##_state *new_##FIELD;\t\t\t\t\\\n\tint slot = size / SIZE;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (size <= old_size || !size) {\t\t\t\t\\\n\t\tif (copy_old)\t\t\t\t\t\t\\\n\t\t\treturn 0;\t\t\t\t\t\\\n\t\tstate->COUNT = slot * SIZE;\t\t\t\t\\\n\t\tif (!size && old_size) {\t\t\t\t\\\n\t\t\tkfree(state->FIELD);\t\t\t\t\\\n\t\t\tstate->FIELD = NULL;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tnew_##FIELD = kmalloc_array(slot, sizeof(struct bpf_##NAME##_state), \\\n\t\t\t\t    GFP_KERNEL);\t\t\t\\\n\tif (!new_##FIELD)\t\t\t\t\t\t\\\n\t\treturn -ENOMEM;\t\t\t\t\t\t\\\n\tif (copy_old) {\t\t\t\t\t\t\t\\\n\t\tif (state->FIELD)\t\t\t\t\t\\\n\t\t\tmemcpy(new_##FIELD, state->FIELD,\t\t\\\n\t\t\t       sizeof(*new_##FIELD) * (old_size / SIZE)); \\\n\t\tmemset(new_##FIELD + old_size / SIZE, 0,\t\t\\\n\t\t       sizeof(*new_##FIELD) * (size - old_size) / SIZE); \\\n\t}\t\t\t\t\t\t\t\t\\\n\tstate->COUNT = slot * SIZE;\t\t\t\t\t\\\n\tkfree(state->FIELD);\t\t\t\t\t\t\\\n\tstate->FIELD = new_##FIELD;\t\t\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* realloc_reference_state() */\nREALLOC_STATE_FN(reference, acquired_refs, refs, 1)\n/* realloc_stack_state() */\nREALLOC_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef REALLOC_STATE_FN\n\n/* do_check() starts with zero-sized stack in struct bpf_verifier_state to\n * make it consume minimal amount of memory. check_stack_write() access from\n * the program calls into realloc_func_state() to grow the stack size.\n * Note there is a non-zero 'parent' pointer inside bpf_verifier_state\n * which realloc_stack_state() copies over. It points to previous\n * bpf_verifier_state which is never reallocated.\n */\nstatic int realloc_func_state(struct bpf_func_state *state, int stack_size,\n\t\t\t      int refs_size, bool copy_old)\n{\n\tint err = realloc_reference_state(state, refs_size, copy_old);\n\tif (err)\n\t\treturn err;\n\treturn realloc_stack_state(state, stack_size, copy_old);\n}\n\n/* Acquire a pointer id from the env and update the state->refs to include\n * this new pointer reference.\n * On success, returns a valid pointer id to associate with the register\n * On failure, returns a negative errno.\n */\nstatic int acquire_reference_state(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint new_ofs = state->acquired_refs;\n\tint id, err;\n\n\terr = realloc_reference_state(state, state->acquired_refs + 1, true);\n\tif (err)\n\t\treturn err;\n\tid = ++env->id_gen;\n\tstate->refs[new_ofs].id = id;\n\tstate->refs[new_ofs].insn_idx = insn_idx;\n\n\treturn id;\n}\n\n/* release function corresponding to acquire_reference_state(). Idempotent. */\nstatic int __release_reference_state(struct bpf_func_state *state, int ptr_id)\n{\n\tint i, last_idx;\n\n\tif (!ptr_id)\n\t\treturn -EFAULT;\n\n\tlast_idx = state->acquired_refs - 1;\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->refs[i].id == ptr_id) {\n\t\t\tif (last_idx && i != last_idx)\n\t\t\t\tmemcpy(&state->refs[i], &state->refs[last_idx],\n\t\t\t\t       sizeof(*state->refs));\n\t\t\tmemset(&state->refs[last_idx], 0, sizeof(*state->refs));\n\t\t\tstate->acquired_refs--;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EFAULT;\n}\n\n/* variation on the above for cases where we expect that there must be an\n * outstanding reference for the specified ptr_id.\n */\nstatic int release_reference_state(struct bpf_verifier_env *env, int ptr_id)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint err;\n\n\terr = __release_reference_state(state, ptr_id);\n\tif (WARN_ON_ONCE(err != 0))\n\t\tverbose(env, \"verifier internal error: can't release reference\\n\");\n\treturn err;\n}\n\nstatic int transfer_reference_state(struct bpf_func_state *dst,\n\t\t\t\t    struct bpf_func_state *src)\n{\n\tint err = realloc_reference_state(dst, src->acquired_refs, false);\n\tif (err)\n\t\treturn err;\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic void free_func_state(struct bpf_func_state *state)\n{\n\tif (!state)\n\t\treturn;\n\tkfree(state->refs);\n\tkfree(state->stack);\n\tkfree(state);\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tint i;\n\n\tfor (i = 0; i <= state->curframe; i++) {\n\t\tfree_func_state(state->frame[i]);\n\t\tstate->frame[i] = NULL;\n\t}\n\tif (free_self)\n\t\tkfree(state);\n}\n\n/* copy verifier state from src to dst growing dst stack space\n * when necessary to accommodate larger src stack\n */\nstatic int copy_func_state(struct bpf_func_state *dst,\n\t\t\t   const struct bpf_func_state *src)\n{\n\tint err;\n\n\terr = realloc_func_state(dst, src->allocated_stack, src->acquired_refs,\n\t\t\t\t false);\n\tif (err)\n\t\treturn err;\n\tmemcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tint i, err;\n\n\t/* if dst has more stack frames then src frame, free them */\n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->curframe = src->curframe;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {\n\t\tverbose(env, \"BPF program is too complex\\n\");\n\t\tgoto err;\n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg);\n\n/* Mark the unknown part of a register (variable offset or scalar value) as\n * known to have the value @imm.\n */\nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\t/* Clear id, off, and union(map_ptr, range) */\n\tmemset(((u8 *)reg) + sizeof(reg->type), 0,\n\t       offsetof(struct bpf_reg_state, var_off) - sizeof(reg->type));\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n}\n\n/* Mark the 'variable offset' part of a register as zero.  This should be\n * used only on registers holding a pointer type.\n */\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void __mark_reg_const_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n\treg->type = SCALAR_VALUE;\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\n/* Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */\nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t/* The register can already have a range from prior markings.\n\t * This is fine as long as it hasn't been advanced from its\n\t * origin.\n\t */\n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n/* Attempts to improve min/max values based on var_off information */\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\n/* Uses signed min/max values to inform unsigned, and vice-versa */\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s64)reg->umax_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\n/* Attempts to improve var_off based on unsigned min/max information */\nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_intersect(reg->var_off,\n\t\t\t\t      tnum_range(reg->umin_value,\n\t\t\t\t\t\t reg->umax_value));\n}\n\n/* Reset the min/max bounds of a register */\nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\n/* Mark a register as having a completely unknown (scalar) value. */\nstatic void __mark_reg_unknown(struct bpf_reg_state *reg)\n{\n\t/*\n\t * Clear type, id, off, and union(map_ptr, range) and\n\t * padding between 'type' and union\n\t */\n\tmemset(reg, 0, offsetof(struct bpf_reg_state, var_off));\n\treg->type = SCALAR_VALUE;\n\treg->var_off = tnum_unknown;\n\treg->frameno = 0;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(regs + regno);\n}\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(regs + regno);\n}\n\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t\tregs[i].parent = NULL;\n\t}\n\n\t/* frame pointer */\n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\tregs[BPF_REG_FP].frameno = state->frameno;\n\n\t/* 1st arg to a function */\n\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\tmark_reg_known_zero(env, regs, BPF_REG_1);\n}\n\n#define BPF_MAIN_FUNC (-1)\nstatic void init_func_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *state,\n\t\t\t    int callsite, int frameno, int subprogno)\n{\n\tstate->callsite = callsite;\n\tstate->frameno = frameno;\n\tstate->subprogno = subprogno;\n\tinit_reg_state(env, state);\n}\n\nenum reg_arg_type {\n\tSRC_OP,\t\t/* register is used as source operand */\n\tDST_OP,\t\t/* register is used as destination operand */\n\tDST_OP_NO_MARK\t/* same as above, check only, don't mark */\n};\n\nstatic int cmp_subprogs(const void *a, const void *b)\n{\n\treturn ((struct bpf_subprog_info *)a)->start -\n\t       ((struct bpf_subprog_info *)b)->start;\n}\n\nstatic int find_subprog(struct bpf_verifier_env *env, int off)\n{\n\tstruct bpf_subprog_info *p;\n\n\tp = bsearch(&off, env->subprog_info, env->subprog_cnt,\n\t\t    sizeof(env->subprog_info[0]), cmp_subprogs);\n\tif (!p)\n\t\treturn -ENOENT;\n\treturn p - env->subprog_info;\n\n}\n\nstatic int add_subprog(struct bpf_verifier_env *env, int off)\n{\n\tint insn_cnt = env->prog->len;\n\tint ret;\n\n\tif (off >= insn_cnt || off < 0) {\n\t\tverbose(env, \"call to invalid destination\\n\");\n\t\treturn -EINVAL;\n\t}\n\tret = find_subprog(env, off);\n\tif (ret >= 0)\n\t\treturn 0;\n\tif (env->subprog_cnt >= BPF_MAX_SUBPROGS) {\n\t\tverbose(env, \"too many subprograms\\n\");\n\t\treturn -E2BIG;\n\t}\n\tenv->subprog_info[env->subprog_cnt++].start = off;\n\tsort(env->subprog_info, env->subprog_cnt,\n\t     sizeof(env->subprog_info[0]), cmp_subprogs, NULL);\n\treturn 0;\n}\n\nstatic int check_subprogs(struct bpf_verifier_env *env)\n{\n\tint i, ret, subprog_start, subprog_end, off, cur_subprog = 0;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\n\t/* Add entry function. */\n\tret = add_subprog(env, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* determine subprog starts. The end is one before the next starts */\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (!env->allow_ptr_leaks) {\n\t\t\tverbose(env, \"function calls to other bpf functions are allowed for root only\\n\");\n\t\t\treturn -EPERM;\n\t\t}\n\t\tret = add_subprog(env, i + insn[i].imm + 1);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t/* Add a fake 'exit' subprog which could simplify subprog iteration\n\t * logic. 'subprog_cnt' should not be increased.\n\t */\n\tsubprog[env->subprog_cnt].start = insn_cnt;\n\n\tif (env->log.level > 1)\n\t\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\t\tverbose(env, \"func#%d @%d\\n\", i, subprog[i].start);\n\n\t/* now check that all jumps are within the same subprog */\n\tsubprog_start = subprog[cur_subprog].start;\n\tsubprog_end = subprog[cur_subprog + 1].start;\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tu8 code = insn[i].code;\n\n\t\tif (BPF_CLASS(code) != BPF_JMP)\n\t\t\tgoto next;\n\t\tif (BPF_OP(code) == BPF_EXIT || BPF_OP(code) == BPF_CALL)\n\t\t\tgoto next;\n\t\toff = i + insn[i].off + 1;\n\t\tif (off < subprog_start || off >= subprog_end) {\n\t\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", i, off);\n\t\t\treturn -EINVAL;\n\t\t}\nnext:\n\t\tif (i == subprog_end - 1) {\n\t\t\t/* to avoid fall-through from one subprog into another\n\t\t\t * the last insn of the subprog should be either exit\n\t\t\t * or unconditional jump back\n\t\t\t */\n\t\t\tif (code != (BPF_JMP | BPF_EXIT) &&\n\t\t\t    code != (BPF_JMP | BPF_JA)) {\n\t\t\t\tverbose(env, \"last insn is not an exit or jmp\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsubprog_start = subprog_end;\n\t\t\tcur_subprog++;\n\t\t\tif (cur_subprog < env->subprog_cnt)\n\t\t\t\tsubprog_end = subprog[cur_subprog + 1].start;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Parentage chain of this register (or stack slot) should take care of all\n * issues like callee-saved registers, stack slot allocation time, etc.\n */\nstatic int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str[parent->type],\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* ... then we depend on parent's value */\n\t\tparent->live |= REG_LIVE_READ;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t}\n\treturn 0;\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\tif (t == SRC_OP) {\n\t\t/* check whether register used as source operand can be read */\n\t\tif (regs[regno].type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't need to worry about FP liveness because it's read-only */\n\t\tif (regno != BPF_REG_FP)\n\t\t\treturn mark_reg_read(env, &regs[regno],\n\t\t\t\t\t     regs[regno].parent);\n\t} else {\n\t\t/* check whether register used as dest operand can be written to */\n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tregs[regno].live |= REG_LIVE_WRITTEN;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Does this register contain a constant zero? */\nstatic bool register_is_null(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_equals_const(reg->var_off, 0);\n}\n\n/* check_stack_read/write functions track spill/fill of registers,\n * stack boundary and alignment are checked in check_mem_access()\n */\nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *state, /* func where register points to */\n\t\t\t     int off, int size, int value_regno, int insn_idx)\n{\n\tstruct bpf_func_state *cur; /* state of the current function */\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\tenum bpf_reg_type type;\n\n\terr = realloc_func_state(state, round_up(slot + 1, BPF_REG_SIZE),\n\t\t\t\t state->acquired_refs, true);\n\tif (err)\n\t\treturn err;\n\t/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,\n\t * so it's aligned access and [off, off + size) are within stack limits\n\t */\n\tif (!env->allow_ptr_leaks &&\n\t    state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tcur = env->cur_state->frame[env->cur_state->curframe];\n\tif (value_regno >= 0 &&\n\t    is_spillable_regtype((type = cur->regs[value_regno].type))) {\n\n\t\t/* register containing pointer is being spilled into stack */\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (state != cur && type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"cannot spill pointers to stack into stack frame of the caller\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* save register state */\n\t\tstate->stack[spi].spilled_ptr = cur->regs[value_regno];\n\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\tfor (i = 0; i < BPF_REG_SIZE; i++) {\n\t\t\tif (state->stack[spi].slot_type[i] == STACK_MISC &&\n\t\t\t    !env->allow_ptr_leaks) {\n\t\t\t\tint *poff = &env->insn_aux_data[insn_idx].sanitize_stack_off;\n\t\t\t\tint soff = (-spi - 1) * BPF_REG_SIZE;\n\n\t\t\t\t/* detected reuse of integer stack slot with a pointer\n\t\t\t\t * which means either llvm is reusing stack slot or\n\t\t\t\t * an attacker is trying to exploit CVE-2018-3639\n\t\t\t\t * (speculative store bypass)\n\t\t\t\t * Have to sanitize that slot with preemptive\n\t\t\t\t * store of zero.\n\t\t\t\t */\n\t\t\t\tif (*poff && *poff != soff) {\n\t\t\t\t\t/* disallow programs where single insn stores\n\t\t\t\t\t * into two different stack slots, since verifier\n\t\t\t\t\t * cannot sanitize them\n\t\t\t\t\t */\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"insn %d cannot access two stack slots fp%d and fp%d\",\n\t\t\t\t\t\tinsn_idx, *poff, soff);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\t*poff = soff;\n\t\t\t}\n\t\t\tstate->stack[spi].slot_type[i] = STACK_SPILL;\n\t\t}\n\t} else {\n\t\tu8 type = STACK_MISC;\n\n\t\t/* regular write of data into stack destroys any spilled ptr */\n\t\tstate->stack[spi].spilled_ptr.type = NOT_INIT;\n\t\t/* Mark slots as STACK_MISC if they belonged to spilled ptr. */\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL)\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tstate->stack[spi].slot_type[i] = STACK_MISC;\n\n\t\t/* only mark the slot as written if all 8 bytes were written\n\t\t * otherwise read propagation may incorrectly stop too soon\n\t\t * when stack slots are partially written.\n\t\t * This heuristic means that read propagation will be\n\t\t * conservative, since it will add reg_live_read marks\n\t\t * to stack slots all the way to first state when programs\n\t\t * writes+reads less than 8 bytes\n\t\t */\n\t\tif (size == BPF_REG_SIZE)\n\t\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\t/* when we zero initialize stack slots mark them as such */\n\t\tif (value_regno >= 0 &&\n\t\t    register_is_null(&cur->regs[value_regno]))\n\t\t\ttype = STACK_ZERO;\n\n\t\t/* Mark slots affected by this stack write. */\n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\ttype;\n\t}\n\treturn 0;\n}\n\nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *reg_state /* func where register points to */,\n\t\t\t    int off, int size, int value_regno)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tu8 *stype;\n\n\tif (reg_state->allocated_stack <= slot) {\n\t\tverbose(env, \"invalid read from stack off %d+0 size %d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\tstype = reg_state->stack[spi].slot_type;\n\n\tif (stype[0] == STACK_SPILL) {\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfor (i = 1; i < BPF_REG_SIZE; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_SPILL) {\n\t\t\t\tverbose(env, \"corrupted spill memory\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\n\t\tif (value_regno >= 0) {\n\t\t\t/* restore register state from stack */\n\t\t\tstate->regs[value_regno] = reg_state->stack[spi].spilled_ptr;\n\t\t\t/* mark reg as written since spilled pointer state likely\n\t\t\t * has its liveness marks cleared by is_state_visited()\n\t\t\t * which resets stack/reg liveness for state transitions\n\t\t\t */\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t}\n\t\tmark_reg_read(env, &reg_state->stack[spi].spilled_ptr,\n\t\t\t      reg_state->stack[spi].spilled_ptr.parent);\n\t\treturn 0;\n\t} else {\n\t\tint zeros = 0;\n\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_MISC)\n\t\t\t\tcontinue;\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_ZERO) {\n\t\t\t\tzeros++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, &reg_state->stack[spi].spilled_ptr,\n\t\t\t      reg_state->stack[spi].spilled_ptr.parent);\n\t\tif (value_regno >= 0) {\n\t\t\tif (zeros == size) {\n\t\t\t\t/* any size read into register is zero extended,\n\t\t\t\t * so the whole register == const_zero\n\t\t\t\t */\n\t\t\t\t__mark_reg_const_zero(&state->regs[value_regno]);\n\t\t\t} else {\n\t\t\t\t/* have read misc data from the stack */\n\t\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\t\t}\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t}\n\t\treturn 0;\n\t}\n}\n\nstatic int check_stack_access(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_reg_state *reg,\n\t\t\t      int off, int size)\n{\n\t/* Stack accesses must be at a fixed offset, so that we\n\t * can determine what type of data were returned. See\n\t * check_stack_read().\n\t */\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable stack access var_off=%s off=%d size=%d\",\n\t\t\ttn_buf, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\tverbose(env, \"invalid stack off=%d size=%d\\n\", off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n/* check read/write into map element returned by bpf_map_lookup_elem() */\nstatic int __check_map_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t      int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    off + size > map->value_size) {\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n/* check read/write into a map element with possible variable offset */\nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t/* We may have adjusted the register to this map value, so we\n\t * need to try adding each of min_value and max_value to off\n\t * to make sure our theoretical access will be safe.\n\t */\n\tif (env->log.level)\n\t\tprint_verifier_state(env, state);\n\n\t/* The minimum value is only important with signed\n\t * comparisons where we can't assume the floor of a\n\t * value is 0.  If we are using signed variables for our\n\t * index'es we need to make sure that whatever we use\n\t * will have a set floor within our range.\n\t */\n\tif (reg->smin_value < 0 &&\n\t    (reg->smin_value == S64_MIN ||\n\t     (off + reg->smin_value != (s64)(s32)(off + reg->smin_value)) ||\n\t      reg->smin_value + off < 0)) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the array range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t/* If we haven't set a max value then we need to bail since we can't be\n\t * sure we won't do bad things.\n\t * If reg->umax_value + off could overflow, treat that as unbounded too.\n\t */\n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any array access into a map\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err)\n\t\tverbose(env, \"R%d max value is outside of the array range\\n\",\n\t\t\tregno);\n\treturn err;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tswitch (env->prog->type) {\n\t/* Program types only with direct read access go here! */\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_SEG6LOCAL:\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\t/* fallthrough */\n\n\t/* Program types with direct read + write access go here! */\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int __check_packet_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    (u64)off + size > reg->range) {\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, reg->off, reg->range);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_packet_access(env, regno, off, size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\n\t/* __check_packet_access has made sure \"off + size - 1\" is within u16.\n\t * reg->umax_value can't be bigger than MAX_PACKET_OFF which is 0xffff,\n\t * otherwise find_good_pkt_pointers would have refused to set range info\n\t * that __check_packet_access would have rejected this pkt access.\n\t * Therefore, \"off + reg->umax_value + size - 1\" won't overflow u32.\n\t */\n\tenv->prog->aux->max_pkt_offset =\n\t\tmax_t(u32, env->prog->aux->max_pkt_offset,\n\t\t      off + reg->umax_value + size - 1);\n\n\treturn err;\n}\n\n/* check access to 'struct bpf_context' fields.  Supports fixed offsets only */\nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic int check_flow_keys_access(struct bpf_verifier_env *env, int off,\n\t\t\t\t  int size)\n{\n\tif (size < 0 || off < 0 ||\n\t    (u64)off + size > sizeof(struct bpf_flow_keys)) {\n\t\tverbose(env, \"invalid access to flow keys off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_sock_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t     int size, enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tif (!bpf_sock_is_valid_access(off, size, t, &info)) {\n\t\tverbose(env, \"invalid bpf_sock access off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\nstatic struct bpf_reg_state *reg_state(struct bpf_verifier_env *env, int regno)\n{\n\treturn cur_regs(env) + regno;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, reg_state(env, regno));\n}\n\nstatic bool is_ctx_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn reg->type == PTR_TO_CTX ||\n\t       reg->type == PTR_TO_SOCKET;\n}\n\nstatic bool is_pkt_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool is_flow_key_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\t/* Separate to is_ctx_reg() since we still want to allow BPF_ST here. */\n\treturn reg->type == PTR_TO_FLOW_KEYS;\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t/* For platforms that do not have a Kconfig enabling\n\t * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of\n\t * NET_IP_ALIGN is universally set to '2'.  And on platforms\n\t * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get\n\t * to this code only in strict mode where we want to emulate\n\t * the NET_IP_ALIGN==2 checking.  Therefore use an\n\t * unconditional IP align value of '2'.\n\t */\n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg, int off,\n\t\t\t       int size, bool strict_alignment_once)\n{\n\tbool strict = env->strict_alignment || strict_alignment_once;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t/* Special case, because of NET_IP_ALIGN. Given metadata sits\n\t\t * right in front, treat it the very same way.\n\t\t */\n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_FLOW_KEYS:\n\t\tpointer_desc = \"flow keys \";\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\t/* The stack spill tracking logic in check_stack_write()\n\t\t * and check_stack_read() relies on stack accesses being\n\t\t * aligned.\n\t\t */\n\t\tstrict = true;\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tpointer_desc = \"sock \";\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\nstatic int update_stack_depth(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_func_state *func,\n\t\t\t      int off)\n{\n\tu16 stack = env->subprog_info[func->subprogno].stack_depth;\n\n\tif (stack >= -off)\n\t\treturn 0;\n\n\t/* update known max for given subprogram */\n\tenv->subprog_info[func->subprogno].stack_depth = -off;\n\treturn 0;\n}\n\n/* starting from main bpf function walk all instructions of the function\n * and recursively walk all callees that given function can call.\n * Ignore jump and exit insns.\n * Since recursion is prevented by check_cfg() this algorithm\n * only needs a local stack of MAX_CALL_FRAMES to remember callsites\n */\nstatic int check_max_stack_depth(struct bpf_verifier_env *env)\n{\n\tint depth = 0, frame = 0, idx = 0, i = 0, subprog_end;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint ret_insn[MAX_CALL_FRAMES];\n\tint ret_prog[MAX_CALL_FRAMES];\n\nprocess_func:\n\t/* round up to 32-bytes, since this is granularity\n\t * of interpreter stack size\n\t */\n\tdepth += round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tif (depth > MAX_BPF_STACK) {\n\t\tverbose(env, \"combined stack size of %d calls is %d. Too large\\n\",\n\t\t\tframe + 1, depth);\n\t\treturn -EACCES;\n\t}\ncontinue_func:\n\tsubprog_end = subprog[idx + 1].start;\n\tfor (; i < subprog_end; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* remember insn and function to return to */\n\t\tret_insn[frame] = i + 1;\n\t\tret_prog[frame] = idx;\n\n\t\t/* find the callee */\n\t\ti = i + insn[i].imm + 1;\n\t\tidx = find_subprog(env, i);\n\t\tif (idx < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tframe++;\n\t\tif (frame >= MAX_CALL_FRAMES) {\n\t\t\tWARN_ONCE(1, \"verifier bug. Call stack is too deep\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tgoto process_func;\n\t}\n\t/* end of for() loop means the last insn of the 'subprog'\n\t * was reached. Doesn't matter whether it was JA or EXIT\n\t */\n\tif (frame == 0)\n\t\treturn 0;\n\tdepth -= round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tframe--;\n\ti = ret_insn[frame];\n\tidx = ret_prog[frame];\n\tgoto continue_func;\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\nstatic int get_callee_stack_depth(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_insn *insn, int idx)\n{\n\tint start = idx + insn->imm + 1, subprog;\n\n\tsubprog = find_subprog(env, start);\n\tif (subprog < 0) {\n\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t  start);\n\t\treturn -EFAULT;\n\t}\n\treturn env->subprog_info[subprog].stack_depth;\n}\n#endif\n\nstatic int check_ctx_reg(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *reg, int regno)\n{\n\t/* Access to ctx or passing it to a helper is only allowed in\n\t * its original, unmodified form.\n\t */\n\n\tif (reg->off) {\n\t\tverbose(env, \"dereference of modified ctx ptr R%d off=%d disallowed\\n\",\n\t\t\tregno, reg->off);\n\t\treturn -EACCES;\n\t}\n\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable ctx access var_off=%s disallowed\\n\", tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n/* truncate register to smaller size (in bytes)\n * must be called with size < BPF_REG_SIZE\n */\nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t/* clear high bits in bit representation */\n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t/* fix arithmetic bounds */\n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n}\n\n/* check whether memory at (regno + off) is accessible for t = (read | write)\n * if t==write, value_regno is a register which value is stored into memory\n * if t==read, value_regno is a register which will receive the value from memory\n * if t==write && value_regno==-1, some unknown value is stored into memory\n * if t==read && value_regno==-1, don't care what we read from memory\n */\nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\telse\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\toff += reg->var_off.value;\n\t\terr = check_stack_access(env, reg, off, size);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstate = func(env, reg);\n\t\terr = update_stack_depth(env, state, off);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_WRITE)\n\t\t\terr = check_stack_write(env, state, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t\telse\n\t\t\terr = check_stack_read(env, state, off, size,\n\t\t\t\t\t       value_regno);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_SOCKET) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"cannot write into socket\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint err;\n\n\tif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\n\t    insn->imm != 0) {\n\t\tverbose(env, \"BPF_XADD uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check src1 operand */\n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\tif (is_ctx_reg(env, insn->dst_reg) ||\n\t    is_pkt_reg(env, insn->dst_reg) ||\n\t    is_flow_key_reg(env, insn->dst_reg)) {\n\t\tverbose(env, \"BPF_XADD stores into R%d %s is not allowed\\n\",\n\t\t\tinsn->dst_reg,\n\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\treturn -EACCES;\n\t}\n\n\t/* check whether atomic_add can read the memory */\n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1, true);\n\tif (err)\n\t\treturn err;\n\n\t/* check whether atomic_add can write into the same memory */\n\treturn check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\tBPF_SIZE(insn->code), BPF_WRITE, -1, true);\n}\n\n/* when register 'regno' is passed into function that will read 'access_size'\n * bytes from that pointer, make sure that it's within stack boundary\n * and all elements of stack are initialized.\n * Unlike most pointer bounds-checking functions, this one doesn't take an\n * 'off' argument, so it has to add in reg->off itself.\n */\nstatic int check_stack_boundary(struct bpf_verifier_env *env, int regno,\n\t\t\t\tint access_size, bool zero_size_allowed,\n\t\t\t\tstruct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint off, i, slot, spi;\n\n\tif (reg->type != PTR_TO_STACK) {\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n\n\t/* Only allow fixed-offset stack reads */\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"invalid variable stack read R%d var_off=%s\\n\",\n\t\t\tregno, tn_buf);\n\t\treturn -EACCES;\n\t}\n\toff = reg->off + reg->var_off.value;\n\tif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\n\t    access_size < 0 || (access_size == 0 && !zero_size_allowed)) {\n\t\tverbose(env, \"invalid stack type R%d off=%d access_size=%d\\n\",\n\t\t\tregno, off, access_size);\n\t\treturn -EACCES;\n\t}\n\n\tif (meta && meta->raw_mode) {\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < access_size; i++) {\n\t\tu8 *stype;\n\n\t\tslot = -(off + i) - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot)\n\t\t\tgoto err;\n\t\tstype = &state->stack[spi].slot_type[slot % BPF_REG_SIZE];\n\t\tif (*stype == STACK_MISC)\n\t\t\tgoto mark;\n\t\tif (*stype == STACK_ZERO) {\n\t\t\t/* helper can write anything into the stack */\n\t\t\t*stype = STACK_MISC;\n\t\t\tgoto mark;\n\t\t}\nerr:\n\t\tverbose(env, \"invalid indirect read from stack off %d+%d size %d\\n\",\n\t\t\toff, i, access_size);\n\t\treturn -EACCES;\nmark:\n\t\t/* reading any byte out of 8-byte 'spill_slot' will cause\n\t\t * the whole slot to be marked as 'read'\n\t\t */\n\t\tmark_reg_read(env, &state->stack[spi].spilled_ptr,\n\t\t\t      state->stack[spi].spilled_ptr.parent);\n\t}\n\treturn update_stack_depth(env, state, off);\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_VALUE:\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tdefault: /* scalar_value|ptr_to_stack or invalid ptr */\n\t\treturn check_stack_boundary(env, regno, access_size,\n\t\t\t\t\t    zero_size_allowed, meta);\n\t}\n}\n\nstatic bool arg_type_is_mem_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_MEM ||\n\t       type == ARG_PTR_TO_MEM_OR_NULL ||\n\t       type == ARG_PTR_TO_UNINIT_MEM;\n}\n\nstatic bool arg_type_is_mem_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_SIZE ||\n\t       type == ARG_CONST_SIZE_OR_ZERO;\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected_type, type = reg->type;\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (arg_type == ARG_PTR_TO_MAP_KEY ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t    arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (!type_is_pkt_pointer(type) && type != PTR_TO_MAP_VALUE &&\n\t\t    type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO) {\n\t\texpected_type = SCALAR_VALUE;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_MAP_PTR) {\n\t\texpected_type = CONST_PTR_TO_MAP;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_CTX) {\n\t\texpected_type = PTR_TO_CTX;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else if (arg_type == ARG_PTR_TO_SOCKET) {\n\t\texpected_type = PTR_TO_SOCKET;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t\tif (meta->ptr_id || !reg->id) {\n\t\t\tverbose(env, \"verifier internal error: mismatched references meta=%d, reg=%d\\n\",\n\t\t\t\tmeta->ptr_id, reg->id);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tmeta->ptr_id = reg->id;\n\t} else if (arg_type_is_mem_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_STACK;\n\t\t/* One exception here. In case function allows for NULL to be\n\t\t * passed in as argument, it's a SCALAR_VALUE type. Final test\n\t\t * happens during stack boundary checking.\n\t\t */\n\t\tif (register_is_null(reg) &&\n\t\t    arg_type == ARG_PTR_TO_MEM_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t\tmeta->raw_mode = arg_type == ARG_PTR_TO_UNINIT_MEM;\n\t} else {\n\t\tverbose(env, \"unsupported arg_type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tif (arg_type == ARG_CONST_MAP_PTR) {\n\t\t/* bpf_map_xxx(map_ptr) call: remember that map_ptr */\n\t\tmeta->map_ptr = reg->map_ptr;\n\t} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., key) call:\n\t\t * check that [key, key + map->key_size) are within\n\t\t * stack limits and initialized\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* in function declaration map_ptr must come before\n\t\t\t * map_key, so that it's verified and known before\n\t\t\t * we have to check map_key here. Otherwise it means\n\t\t\t * that kernel subsystem misconfigured verifier\n\t\t\t */\n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->key_size, false,\n\t\t\t\t\t      NULL);\n\t} else if (arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t\t   arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., value) call:\n\t\t * check [value, value + map->value_size) validity\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->raw_mode = (arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE);\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->value_size, false,\n\t\t\t\t\t      meta);\n\t} else if (arg_type_is_mem_size(arg_type)) {\n\t\tbool zero_size_allowed = (arg_type == ARG_CONST_SIZE_OR_ZERO);\n\n\t\t/* remember the mem_size which may be used later\n\t\t * to refine return values.\n\t\t */\n\t\tmeta->msize_smax_value = reg->smax_value;\n\t\tmeta->msize_umax_value = reg->umax_value;\n\n\t\t/* The register is SCALAR_VALUE; the access check\n\t\t * happens using its boundaries.\n\t\t */\n\t\tif (!tnum_is_const(reg->var_off))\n\t\t\t/* For unprivileged variable accesses, disable raw\n\t\t\t * mode so that the program is required to\n\t\t\t * initialize all the memory that the helper could\n\t\t\t * just partially fill up.\n\t\t\t */\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smin_value < 0) {\n\t\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->umin_value == 0) {\n\t\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t\t      meta);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t\t      reg->umax_value,\n\t\t\t\t\t      zero_size_allowed, meta);\n\t}\n\n\treturn err;\nerr_type:\n\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\treg_type_str[type], reg_type_str[expected_type]);\n\treturn -EACCES;\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t/* We need a two way check, first is from map perspective ... */\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_STORAGE:\n\tcase BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:\n\t\tif (func_id != BPF_FUNC_get_local_storage)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* devmap returns a pointer to a live net_device ifindex that we cannot\n\t * allow to be modified from bpf side. So do not allow lookup elements\n\t * for now.\n\t */\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* Restrict bpf side of cpumap and xskmap, open when use-cases\n\t * appear.\n\t */\n\tcase BPF_MAP_TYPE_CPUMAP:\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\tif (func_id != BPF_FUNC_sk_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sock_hash_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_hash)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:\n\t\tif (func_id != BPF_FUNC_sk_select_reuseport)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_QUEUE:\n\tcase BPF_MAP_TYPE_STACK:\n\t\tif (func_id != BPF_FUNC_map_peek_elem &&\n\t\t    func_id != BPF_FUNC_map_pop_elem &&\n\t\t    func_id != BPF_FUNC_map_push_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* ... and second from the function itself. */\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tif (env->subprog_cnt > 1) {\n\t\t\tverbose(env, \"tail_calls are not allowed in programs with bpf-to-bpf calls\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_XSKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\tcase BPF_FUNC_msg_redirect_map:\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_hash:\n\tcase BPF_FUNC_msg_redirect_hash:\n\tcase BPF_FUNC_sock_hash_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&\n\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_select_reuseport:\n\t\tif (map->map_type != BPF_MAP_TYPE_REUSEPORT_SOCKARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_peek_elem:\n\tcase BPF_FUNC_map_pop_elem:\n\tcase BPF_FUNC_map_push_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_QUEUE &&\n\t\t    map->map_type != BPF_MAP_TYPE_STACK)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic bool check_raw_mode_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\t/* We only support one arg being in raw mode at the moment,\n\t * which is sufficient for the helper functions we have\n\t * right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic bool check_args_pair_invalid(enum bpf_arg_type arg_curr,\n\t\t\t\t    enum bpf_arg_type arg_next)\n{\n\treturn (arg_type_is_mem_ptr(arg_curr) &&\n\t        !arg_type_is_mem_size(arg_next)) ||\n\t       (!arg_type_is_mem_ptr(arg_curr) &&\n\t\targ_type_is_mem_size(arg_next));\n}\n\nstatic bool check_arg_pair_ok(const struct bpf_func_proto *fn)\n{\n\t/* bpf_xxx(..., buf, len) call will access 'len'\n\t * bytes from memory 'buf'. Both arg types need\n\t * to be paired, so make sure there's no buggy\n\t * helper function specification.\n\t */\n\tif (arg_type_is_mem_size(fn->arg1_type) ||\n\t    arg_type_is_mem_ptr(fn->arg5_type)  ||\n\t    check_args_pair_invalid(fn->arg1_type, fn->arg2_type) ||\n\t    check_args_pair_invalid(fn->arg2_type, fn->arg3_type) ||\n\t    check_args_pair_invalid(fn->arg3_type, fn->arg4_type) ||\n\t    check_args_pair_invalid(fn->arg4_type, fn->arg5_type))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool check_refcount_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (arg_type_is_refcounted(fn->arg1_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg2_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg3_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg4_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg5_type))\n\t\tcount++;\n\n\t/* We only support one arg being unreferenced at the moment,\n\t * which is sufficient for the helper functions we have right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic int check_func_proto(const struct bpf_func_proto *fn)\n{\n\treturn check_raw_mode_ok(fn) &&\n\t       check_arg_pair_ok(fn) &&\n\t       check_refcount_ok(fn) ? 0 : -EINVAL;\n}\n\n/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]\n * are now invalid, so turn them into unknown SCALAR_VALUE.\n */\nstatic void __clear_all_pkt_pointers(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}\n\nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint i;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__clear_all_pkt_pointers(env, vstate->frame[i]);\n}\n\nstatic void release_reg_references(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *state, int id)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].id == id)\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg_is_refcounted(reg) && reg->id == id)\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}\n\n/* The pointer with the specified id has released its reference to kernel\n * resources. Identify all copies of the same pointer and clear the reference.\n */\nstatic int release_reference(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint i;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\trelease_reg_references(env, vstate->frame[i], meta->ptr_id);\n\n\treturn release_reference_state(env, meta->ptr_id);\n}\n\nstatic int check_func_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t   int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tint i, err, subprog, target_insn;\n\n\tif (state->curframe + 1 >= MAX_CALL_FRAMES) {\n\t\tverbose(env, \"the call stack of %d frames is too deep\\n\",\n\t\t\tstate->curframe + 2);\n\t\treturn -E2BIG;\n\t}\n\n\ttarget_insn = *insn_idx + insn->imm;\n\tsubprog = find_subprog(env, target_insn + 1);\n\tif (subprog < 0) {\n\t\tverbose(env, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\ttarget_insn + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tcaller = state->frame[state->curframe];\n\tif (state->frame[state->curframe + 1]) {\n\t\tverbose(env, \"verifier bug. Frame %d already allocated\\n\",\n\t\t\tstate->curframe + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tcallee = kzalloc(sizeof(*callee), GFP_KERNEL);\n\tif (!callee)\n\t\treturn -ENOMEM;\n\tstate->frame[state->curframe + 1] = callee;\n\n\t/* callee cannot access r0, r6 - r9 for reading and has to write\n\t * into its own stack before reading from it.\n\t * callee can read/write into caller's stack\n\t */\n\tinit_func_state(env, callee,\n\t\t\t/* remember the callsite, it will be used by bpf_exit */\n\t\t\t*insn_idx /* callsite */,\n\t\t\tstate->curframe + 1 /* frameno within this callchain */,\n\t\t\tsubprog /* subprog number within this prog */);\n\n\t/* Transfer references to the callee */\n\terr = transfer_reference_state(callee, caller);\n\tif (err)\n\t\treturn err;\n\n\t/* copy r1 - r5 args that callee can access.  The copy includes parent\n\t * pointers, which connects us up to the liveness chain\n\t */\n\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++)\n\t\tcallee->regs[i] = caller->regs[i];\n\n\t/* after the call registers r0 - r5 were scratched */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, caller->regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* only increment it after check_reg_arg() finished */\n\tstate->curframe++;\n\n\t/* and go analyze first insn of the callee */\n\t*insn_idx = target_insn;\n\n\tif (env->log.level) {\n\t\tverbose(env, \"caller:\\n\");\n\t\tprint_verifier_state(env, caller);\n\t\tverbose(env, \"callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t}\n\treturn 0;\n}\n\nstatic int prepare_func_exit(struct bpf_verifier_env *env, int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tstruct bpf_reg_state *r0;\n\tint err;\n\n\tcallee = state->frame[state->curframe];\n\tr0 = &callee->regs[BPF_REG_0];\n\tif (r0->type == PTR_TO_STACK) {\n\t\t/* technically it's ok to return caller's stack pointer\n\t\t * (or caller's caller's pointer) back to the caller,\n\t\t * since these pointers are valid. Only current stack\n\t\t * pointer will be invalid as soon as function exits,\n\t\t * but let's be conservative\n\t\t */\n\t\tverbose(env, \"cannot return stack pointer to the caller\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tstate->curframe--;\n\tcaller = state->frame[state->curframe];\n\t/* return to the caller whatever r0 had in the callee */\n\tcaller->regs[BPF_REG_0] = *r0;\n\n\t/* Transfer references to the caller */\n\terr = transfer_reference_state(caller, callee);\n\tif (err)\n\t\treturn err;\n\n\t*insn_idx = callee->callsite + 1;\n\tif (env->log.level) {\n\t\tverbose(env, \"returning from callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t\tverbose(env, \"to caller at %d:\\n\", *insn_idx);\n\t\tprint_verifier_state(env, caller);\n\t}\n\t/* clear everything in the callee */\n\tfree_func_state(callee);\n\tstate->frame[state->curframe + 1] = NULL;\n\treturn 0;\n}\n\nstatic void do_refine_retval_range(struct bpf_reg_state *regs, int ret_type,\n\t\t\t\t   int func_id,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *ret_reg = &regs[BPF_REG_0];\n\n\tif (ret_type != RET_INTEGER ||\n\t    (func_id != BPF_FUNC_get_stack &&\n\t     func_id != BPF_FUNC_probe_read_str))\n\t\treturn;\n\n\tret_reg->smax_value = meta->msize_smax_value;\n\tret_reg->umax_value = meta->msize_umax_value;\n\t__reg_deduce_bounds(ret_reg);\n\t__reg_bound_offset(ret_reg);\n}\n\nstatic int\nrecord_func_map(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\n\tif (func_id != BPF_FUNC_tail_call &&\n\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t    func_id != BPF_FUNC_map_update_elem &&\n\t    func_id != BPF_FUNC_map_delete_elem &&\n\t    func_id != BPF_FUNC_map_push_elem &&\n\t    func_id != BPF_FUNC_map_pop_elem &&\n\t    func_id != BPF_FUNC_map_peek_elem)\n\t\treturn 0;\n\n\tif (meta->map_ptr == NULL) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!BPF_MAP_PTR(aux->map_state))\n\t\tbpf_map_ptr_store(aux, meta->map_ptr,\n\t\t\t\t  meta->map_ptr->unpriv_array);\n\telse if (BPF_MAP_PTR(aux->map_state) != meta->map_ptr)\n\t\tbpf_map_ptr_store(aux, BPF_MAP_PTR_POISON,\n\t\t\t\t  meta->map_ptr->unpriv_array);\n\treturn 0;\n}\n\nstatic int check_reference_leak(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint i;\n\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tverbose(env, \"Unreleased reference id=%d alloc_insn=%d\\n\",\n\t\t\tstate->refs[i].id, state->refs[i].insn_idx);\n\t}\n\treturn state->acquired_refs ? -EINVAL : 0;\n}\n\nstatic int check_helper_call(struct bpf_verifier_env *env, int func_id, int insn_idx)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tbool changes_data;\n\tint i, err;\n\n\t/* find function prototype */\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\t/* check args */\n\terr = check_func_arg(env, BPF_REG_1, fn->arg1_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_2, fn->arg2_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_3, fn->arg3_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_4, fn->arg4_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_5, fn->arg5_type, &meta);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (func_id == BPF_FUNC_tail_call) {\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t} else if (is_release_function(func_id)) {\n\t\terr = release_reference(env, &meta);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tregs = cur_regs(env);\n\n\t/* check that flags argument in get_local_storage(map, flags) is 0,\n\t * this is required because get_local_storage() can't return an error.\n\t */\n\tif (func_id == BPF_FUNC_get_local_storage &&\n\t    !register_is_null(&regs[BPF_REG_2])) {\n\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* update return register (already marked as written above) */\n\tif (fn->ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (fn->ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL ||\n\t\t   fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tif (fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t} else if (fn->ret_type == RET_PTR_TO_SOCKET_OR_NULL) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET_OR_NULL;\n\t\tregs[BPF_REG_0].id = id;\n\t} else {\n\t\tverbose(env, \"unknown return type %d of func %s#%d\\n\",\n\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif (func_id == BPF_FUNC_get_stack && !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s64 a, s64 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t/* liveness must not touch this register anymore */\n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t/* since the register is unused, clear its state\n\t\t\t * to make further comparison simpler\n\t\t\t */\n\t\t\t__mark_reg_not_init(&st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t/* liveness must not touch this stack slot anymore */\n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(&st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t/* all regs in this state in all frames were already marked */\n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n/* the parentage chains form a tree.\n * the verifier states are added to state lists at given insn and\n * pushed into state stack for future exploration.\n * when the verifier reaches bpf_exit insn some of the verifer states\n * stored in the state lists have their final liveness state already,\n * but a lot of states will get revised from liveness point of view when\n * the verifier explores other branches.\n * Example:\n * 1: r0 = 1\n * 2: if r1 == 100 goto pc+1\n * 3: r0 = 2\n * 4: exit\n * when the verifier reaches exit insn the register r0 in the state list of\n * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch\n * of insn 2 and goes exploring further. At the insn 4 it will walk the\n * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.\n *\n * Since the verifier pushes the branch states as it sees them while exploring\n * the program the condition of walking the branch instruction for the second\n * time means that all states below this branch were already explored and\n * their final liveness markes are already propagated.\n * Hence when the verifier completes the search of state list in is_state_visited()\n * we can call this clean_live_states() function to mark all liveness states\n * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'\n * will not be used.\n * This function also clears the registers and stack for states that !READ\n * to simplify state merging.\n *\n * Important note here that walking the same branch instruction in the callee\n * doesn't meant that the states are DONE. The verifier has to compare\n * the callsites\n */\nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = env->explored_states[insn];\n\tif (!sl)\n\t\treturn;\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t/* explored state didn't use this */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\t/* explored stack has more populated slots than current stack\n\t\t * and these slots were used\n\t\t */\n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t/* if old state was safe with misc data in the stack\n\t\t * it will be safe with zero-initialized stack.\n\t\t * The opposite is not true\n\t\t */\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur)\n{\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\treturn !memcmp(old->refs, cur->refs,\n\t\t       sizeof(*old->refs) * old->acquired_refs);\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool func_states_equal(struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\n\tif (!refsafe(old, cur))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}\n\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\t/* for states to be equal callsites have to be the same\n\t * and all frame states need to be equivalent\n\t */\n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n/* A write screens off any subsequent reads; but write marks come from the\n * straight-line code between a state and its parent.  When we arrive at an\n * equivalent state (jump target or such) we didn't arrive by the straight-line\n * code, so read marks in the state must propagate to the parent regardless\n * of the state's write marks. That's what 'parent == state->parent' comparison\n * in mark_reg_read() is for.\n */\nstatic int propagate_liveness(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_verifier_state *vstate,\n\t\t\t      struct bpf_verifier_state *vparent)\n{\n\tint i, frame, err = 0;\n\tstruct bpf_func_state *state, *parent;\n\n\tif (vparent->curframe != vstate->curframe) {\n\t\tWARN(1, \"propagate_live: parent frame %d current frame %d\\n\",\n\t\t     vparent->curframe, vstate->curframe);\n\t\treturn -EFAULT;\n\t}\n\t/* Propagate read liveness of registers... */\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\t/* We don't need to worry about FP liveness because it's read-only */\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tif (vparent->frame[vparent->curframe]->regs[i].live & REG_LIVE_READ)\n\t\t\tcontinue;\n\t\tif (vstate->frame[vstate->curframe]->regs[i].live & REG_LIVE_READ) {\n\t\t\terr = mark_reg_read(env, &vstate->frame[vstate->curframe]->regs[i],\n\t\t\t\t\t    &vparent->frame[vstate->curframe]->regs[i]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\t/* ... and stack slots */\n\tfor (frame = 0; frame <= vstate->curframe; frame++) {\n\t\tstate = vstate->frame[frame];\n\t\tparent = vparent->frame[frame];\n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tif (parent->stack[i].spilled_ptr.live & REG_LIVE_READ)\n\t\t\t\tcontinue;\n\t\t\tif (state->stack[i].spilled_ptr.live & REG_LIVE_READ)\n\t\t\t\tmark_reg_read(env, &state->stack[i].spilled_ptr,\n\t\t\t\t\t      &parent->stack[i].spilled_ptr);\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl;\n\tstruct bpf_verifier_state *cur = env->cur_state, *new;\n\tint i, j, err, states_cnt = 0;\n\n\tsl = env->explored_states[insn_idx];\n\tif (!sl)\n\t\t/* this 'insn_idx' instruction wasn't marked, so we will not\n\t\t * be doing state search here\n\t\t */\n\t\treturn 0;\n\n\tclean_live_states(env, insn_idx, cur);\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\t/* reached equivalent register/stack state,\n\t\t\t * prune the search.\n\t\t\t * Registers read by the continuation are read by us.\n\t\t\t * If we have any write marks in env->cur_state, they\n\t\t\t * will prevent corresponding reads in the continuation\n\t\t\t * from reaching our parent (an explored_state).  Our\n\t\t\t * own state will get the read marks recorded, but\n\t\t\t * they'll be immediately forgotten as we're pruning\n\t\t\t * this state and will pop a new one.\n\t\t\t */\n\t\t\terr = propagate_liveness(env, &sl->state, cur);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn 1;\n\t\t}\n\t\tsl = sl->next;\n\t\tstates_cnt++;\n\t}\n\n\tif (!env->allow_ptr_leaks && states_cnt > BPF_COMPLEXITY_LIMIT_STATES)\n\t\treturn 0;\n\n\t/* there were no equivalent states, remember current one.\n\t * technically the current state is not proven to be safe yet,\n\t * but it will either reach outer most bpf_exit (which means it's safe)\n\t * or it will be rejected. Since there are no loops, we won't be\n\t * seeing this tuple (frame[0].callsite, frame[1].callsite, .. insn_idx)\n\t * again on the way to bpf_exit\n\t */\n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\n\t/* add new state to the head of linked list */\n\tnew = &new_sl->state;\n\terr = copy_verifier_state(new, cur);\n\tif (err) {\n\t\tfree_verifier_state(new, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew_sl->next = env->explored_states[insn_idx];\n\tenv->explored_states[insn_idx] = new_sl;\n\t/* connect new state to parentage chain. Current frame needs all\n\t * registers connected. Only r6 - r9 of the callers are alive (pushed\n\t * to the stack implicitly by JITs) so in callers' frames connect just\n\t * r6 - r9 as an optimization. Callers will have r1 - r5 connected to\n\t * the state of the call instruction (with WRITTEN set), and r0 comes\n\t * from callee with its full parentage chain, anyway.\n\t */\n\tfor (j = 0; j <= cur->curframe; j++)\n\t\tfor (i = j < cur->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].parent = &new->frame[j]->regs[i];\n\t/* clear write marks in current state: the writes we did are not writes\n\t * our child did, so they don't screen off its reads from us.\n\t * (There are no read marks in current state, because reads always mark\n\t * their parent and current state never has children yet.  Only\n\t * explored_states can get read marks.)\n\t */\n\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\tcur->frame[cur->curframe]->regs[i].live = REG_LIVE_NONE;\n\n\t/* all stack frames are accessible from callee, clear them all */\n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tstruct bpf_func_state *frame = cur->frame[j];\n\t\tstruct bpf_func_state *newframe = new->frame[j];\n\n\t\tfor (i = 0; i < frame->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tframe->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\t\t\tframe->stack[i].spilled_ptr.parent =\n\t\t\t\t\t\t&newframe->stack[i].spilled_ptr;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Return true if it's OK to have the same insn return a different type. */\nstatic bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n/* If an instruction was previously used with particular pointer types, then we\n * need to be careful to avoid cases such as the below, where it may be ok\n * for one branch accessing the pointer, but not ok for the other branch:\n *\n * R1 = sock_ptr\n * goto X;\n * ...\n * R1 = some_other_valid_ptr;\n * goto X;\n * ...\n * R2 = *(u32 *)(R1 + 0);\n */\nstatic bool reg_type_mismatch(enum bpf_reg_type src, enum bpf_reg_type prev)\n{\n\treturn src != prev && (!reg_type_mismatch_ok(src) ||\n\t\t\t       !reg_type_mismatch_ok(prev));\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len, i;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tenv->prev_linfo = NULL;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\t0 /* subprogno, zero == main subprog */);\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx);\n\t\t\tprint_verifier_state(env, state->frame[state->curframe]);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[env->insn_idx].seen = true;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn->imm, env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\tenv->prev_insn_idx = env->insn_idx;\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\terr = pop_stack(env, &env->prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tenv->insn_aux_data[env->insn_idx].seen = true;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\tverbose(env, \"processed %d insns (limit %d), stack depth \",\n\t\tinsn_processed, BPF_COMPLEXITY_LIMIT_INSNS);\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tu32 depth = env->subprog_info[i].stack_depth;\n\n\t\tverbose(env, \"%d\", depth);\n\t\tif (i + 1 < env->subprog_cnt)\n\t\t\tverbose(env, \"+\");\n\t}\n\tverbose(env, \"\\n\");\n\tenv->prog->aux->stack_depth = env->subprog_info[0].stack_depth;\n\treturn 0;\n}\n\nstatic int check_map_prealloc(struct bpf_map *map)\n{\n\treturn (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_HASH_OF_MAPS) ||\n\t\t!(map->map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\t/* Make sure that BPF_PROG_TYPE_PERF_EVENT programs only use\n\t * preallocated hash maps, since doing memory allocation\n\t * in overflow_handler can crash depending on where nmi got\n\t * triggered.\n\t */\n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\tif (!check_map_prealloc(map)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map->inner_map_meta &&\n\t\t    !check_map_prealloc(map->inner_map_meta)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated inner hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif ((bpf_prog_is_dev_bound(prog->aux) || bpf_map_is_dev_bound(map)) &&\n\t    !bpf_offload_prog_map_match(prog, map)) {\n\t\tverbose(env, \"offload device mismatch between prog and map\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool bpf_map_is_cgroup_storage(struct bpf_map *map)\n{\n\treturn (map->map_type == BPF_MAP_TYPE_CGROUP_STORAGE ||\n\t\tmap->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE);\n}\n\n/* look for pseudo eBPF instructions that access map FDs and\n * replace them with actual map pointers\n */\nstatic int replace_map_fd_with_map_ptr(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_STX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM &&\n\t\t      BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn->src_reg == 0)\n\t\t\t\t/* valid generic load 64-bit imm */\n\t\t\t\tgoto next_insn;\n\n\t\t\tif (insn->src_reg != BPF_PSEUDO_MAP_FD) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf = fdget(insn->imm);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn->imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* store map pointer inside BPF_LD_IMM64 instruction */\n\t\t\tinsn[0].imm = (u32) (unsigned long) map;\n\t\t\tinsn[1].imm = ((u64) (unsigned long) map) >> 32;\n\n\t\t\t/* check whether we recorded this map already */\n\t\t\tfor (j = 0; j < env->used_map_cnt; j++)\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t/* hold the map. If the program is rejected by verifier,\n\t\t\t * the map will be released by release_maps() or it\n\t\t\t * will be used by the valid program until it's unloaded\n\t\t\t * and all maps are released in free_used_maps()\n\t\t\t */\n\t\t\tmap = bpf_map_inc(map, false);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tif (bpf_map_is_cgroup_storage(map) &&\n\t\t\t    bpf_cgroup_storage_assign(env->prog, map)) {\n\t\t\t\tverbose(env, \"only one cgroup storage of each type is allowed\\n\");\n\t\t\t\tfdput(f);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Basic sanity check before we invest more work here. */\n\t\tif (!bpf_opcode_in_insntable(insn->code)) {\n\t\t\tverbose(env, \"unknown opcode %02x\\n\", insn->code);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* now all pseudo BPF_LD_IMM64 instructions load valid\n\t * 'struct bpf_map *' into a register instead of user map_fd.\n\t * These pointers will be used later by verifier to validate map access.\n\t */\n\treturn 0;\n}\n\n/* drop refcnt of maps used by the rejected program */\nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\tenum bpf_cgroup_storage_type stype;\n\tint i;\n\n\tfor_each_cgroup_storage_type(stype) {\n\t\tif (!env->prog->aux->cgroup_storage[stype])\n\t\t\tcontinue;\n\t\tbpf_cgroup_storage_release(env->prog,\n\t\t\tenv->prog->aux->cgroup_storage[stype]);\n\t}\n\n\tfor (i = 0; i < env->used_map_cnt; i++)\n\t\tbpf_map_put(env->used_maps[i]);\n}\n\n/* convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */\nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++)\n\t\tif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tinsn->src_reg = 0;\n}\n\n/* single env->prog->insni[off] instruction was replaced with the range\n * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying\n * [0, off) and [off, end) to new locations, so the patched range stays zero\n */\nstatic int adjust_insn_aux_data(struct bpf_verifier_env *env, u32 prog_len,\n\t\t\t\tu32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;\n\tint i;\n\n\tif (cnt == 1)\n\t\treturn 0;\n\tnew_data = vzalloc(array_size(prog_len,\n\t\t\t\t      sizeof(struct bpf_insn_aux_data)));\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++)\n\t\tnew_data[i].seen = true;\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n\treturn 0;\n}\n\nstatic void adjust_subprog_starts(struct bpf_verifier_env *env, u32 off, u32 len)\n{\n\tint i;\n\n\tif (len == 1)\n\t\treturn;\n\t/* NOTE: fake 'exit' subprog should be updated as well. */\n\tfor (i = 0; i <= env->subprog_cnt; i++) {\n\t\tif (env->subprog_info[i].start <= off)\n\t\t\tcontinue;\n\t\tenv->subprog_info[i].start += len - 1;\n\t}\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (!new_prog)\n\t\treturn NULL;\n\tif (adjust_insn_aux_data(env, new_prog->len, off, len))\n\t\treturn NULL;\n\tadjust_subprog_starts(env, off, len);\n\treturn new_prog;\n}\n\n/* The verifier does more data flow analysis than llvm and will not\n * explore branches that are dead at run time. Malicious programs can\n * have dead code too. Therefore replace all dead at-run-time code\n * with 'ja -1'.\n *\n * Just nops are not optimal, e.g. if they would sit at the end of the\n * program and through another bug we would manage to jump there, then\n * we'd execute beyond program memory otherwise. Returning exception\n * code also wouldn't work since we can have subprogs where the dead\n * code could be located.\n */\nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn trap = BPF_JMP_IMM(BPF_JA, 0, 0, -1);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &trap, sizeof(trap));\n\t}\n}\n\n/* convert load instructions that access fields of a context type into a\n * sequence of instructions that access fields of the underlying structure:\n *     struct __sk_buff    -> struct sk_buff\n *     struct bpf_sock_ops -> struct sock\n */\nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tu32 target_size, size_default, off;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\n\tif (ops->gen_prologue || env->seen_direct_write) {\n\t\tif (!ops->gen_prologue) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tbpf_convert_ctx_access_t convert_ctx_access;\n\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (type == BPF_WRITE &&\n\t\t    env->insn_aux_data[i + delta].sanitize_stack_off) {\n\t\t\tstruct bpf_insn patch[] = {\n\t\t\t\t/* Sanitize suspicious stack slot with zero.\n\t\t\t\t * There are no memory dependencies for this store,\n\t\t\t\t * since it's only using frame pointer and immediate\n\t\t\t\t * constant of zero\n\t\t\t\t */\n\t\t\t\tBPF_ST_MEM(BPF_DW, BPF_REG_FP,\n\t\t\t\t\t   env->insn_aux_data[i + delta].sanitize_stack_off,\n\t\t\t\t\t   0),\n\t\t\t\t/* the original STX instruction will immediately\n\t\t\t\t * overwrite the same stack slot with appropriate value\n\t\t\t\t */\n\t\t\t\t*insn,\n\t\t\t};\n\n\t\t\tcnt = ARRAY_SIZE(patch);\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patch, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (env->insn_aux_data[i + delta].ptr_type) {\n\t\tcase PTR_TO_CTX:\n\t\t\tif (!ops->convert_ctx_access)\n\t\t\t\tcontinue;\n\t\t\tconvert_ctx_access = ops->convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_SOCKET:\n\t\t\tconvert_ctx_access = bpf_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcontinue;\n\t\t}\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tsize_default = bpf_ctx_off_adjust_machine(ctx_field_size);\n\t\toff = insn->off;\n\t\tif (is_narrower_load) {\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(size_default - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tu8 shift = (off & (size_default - 1)) * 8;\n\n\t\t\tif (ctx_field_size <= 4) {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t} else {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t}\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\nstatic int jit_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog, **func, *tmp;\n\tint i, j, subprog_start, subprog_end = 0, len, subprog;\n\tstruct bpf_insn *insn;\n\tvoid *old_bpf_func;\n\tint err;\n\n\tif (env->subprog_cnt <= 1)\n\t\treturn 0;\n\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* Upon error here we cannot fall back to interpreter but\n\t\t * need a hard reject of the program. Thus -EFAULT is\n\t\t * propagated in any case.\n\t\t */\n\t\tsubprog = find_subprog(env, i + insn->imm + 1);\n\t\tif (subprog < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i + insn->imm + 1);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* temporarily remember subprog id inside insn instead of\n\t\t * aux_data, since next loop will split up all insns into funcs\n\t\t */\n\t\tinsn->off = subprog;\n\t\t/* remember original imm in case JIT fails and fallback\n\t\t * to interpreter will be needed\n\t\t */\n\t\tenv->insn_aux_data[i].call_imm = insn->imm;\n\t\t/* point imm to __bpf_call_base+1 from JITs point of view */\n\t\tinsn->imm = 1;\n\t}\n\n\terr = bpf_prog_alloc_jited_linfo(prog);\n\tif (err)\n\t\tgoto out_undo_insn;\n\n\terr = -ENOMEM;\n\tfunc = kcalloc(env->subprog_cnt, sizeof(prog), GFP_KERNEL);\n\tif (!func)\n\t\tgoto out_undo_insn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tsubprog_start = subprog_end;\n\t\tsubprog_end = env->subprog_info[i + 1].start;\n\n\t\tlen = subprog_end - subprog_start;\n\t\tfunc[i] = bpf_prog_alloc(bpf_prog_size(len), GFP_USER);\n\t\tif (!func[i])\n\t\t\tgoto out_free;\n\t\tmemcpy(func[i]->insnsi, &prog->insnsi[subprog_start],\n\t\t       len * sizeof(struct bpf_insn));\n\t\tfunc[i]->type = prog->type;\n\t\tfunc[i]->len = len;\n\t\tif (bpf_prog_calc_tag(func[i]))\n\t\t\tgoto out_free;\n\t\tfunc[i]->is_func = 1;\n\t\tfunc[i]->aux->func_idx = i;\n\t\t/* the btf and func_info will be freed only at prog->aux */\n\t\tfunc[i]->aux->btf = prog->aux->btf;\n\t\tfunc[i]->aux->func_info = prog->aux->func_info;\n\n\t\t/* Use bpf_prog_F_tag to indicate functions in stack traces.\n\t\t * Long term would need debug info to populate names\n\t\t */\n\t\tfunc[i]->aux->name[0] = 'F';\n\t\tfunc[i]->aux->stack_depth = env->subprog_info[i].stack_depth;\n\t\tfunc[i]->jit_requested = 1;\n\t\tfunc[i]->aux->linfo = prog->aux->linfo;\n\t\tfunc[i]->aux->nr_linfo = prog->aux->nr_linfo;\n\t\tfunc[i]->aux->jited_linfo = prog->aux->jited_linfo;\n\t\tfunc[i]->aux->linfo_idx = env->subprog_info[i].linfo_idx;\n\t\tfunc[i] = bpf_int_jit_compile(func[i]);\n\t\tif (!func[i]->jited) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\t/* at this point all bpf functions were successfully JITed\n\t * now populate all bpf_calls with correct addresses and\n\t * run last pass of JIT\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\t\tcontinue;\n\t\t\tsubprog = insn->off;\n\t\t\tinsn->imm = (u64 (*)(u64, u64, u64, u64, u64))\n\t\t\t\tfunc[subprog]->bpf_func -\n\t\t\t\t__bpf_call_base;\n\t\t}\n\n\t\t/* we use the aux data to keep a list of the start addresses\n\t\t * of the JITed images for each function in the program\n\t\t *\n\t\t * for some architectures, such as powerpc64, the imm field\n\t\t * might not be large enough to hold the offset of the start\n\t\t * address of the callee's JITed image from __bpf_call_base\n\t\t *\n\t\t * in such cases, we can lookup the start address of a callee\n\t\t * by using its subprog id, available from the off field of\n\t\t * the call instruction, as an index for this list\n\t\t */\n\t\tfunc[i]->aux->func = func;\n\t\tfunc[i]->aux->func_cnt = env->subprog_cnt;\n\t}\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\told_bpf_func = func[i]->bpf_func;\n\t\ttmp = bpf_int_jit_compile(func[i]);\n\t\tif (tmp != func[i] || func[i]->bpf_func != old_bpf_func) {\n\t\t\tverbose(env, \"JIT doesn't support bpf-to-bpf calls\\n\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t/* finally lock prog and jit images for all functions and\n\t * populate kallsysm\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tbpf_prog_lock_ro(func[i]);\n\t\tbpf_prog_kallsyms_add(func[i]);\n\t}\n\n\t/* Last step: make now unused interpreter insns from main\n\t * prog consistent for later dump requests, so they can\n\t * later look the same as if they were interpreted only.\n\t */\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = env->insn_aux_data[i].call_imm;\n\t\tsubprog = find_subprog(env, i + insn->off + 1);\n\t\tinsn->imm = subprog;\n\t}\n\n\tprog->jited = 1;\n\tprog->bpf_func = func[0]->bpf_func;\n\tprog->aux->func = func;\n\tprog->aux->func_cnt = env->subprog_cnt;\n\tbpf_prog_free_unused_jited_linfo(prog);\n\treturn 0;\nout_free:\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (func[i])\n\t\t\tbpf_jit_free(func[i]);\n\tkfree(func);\nout_undo_insn:\n\t/* cleanup main prog to be interpreted */\n\tprog->jit_requested = 0;\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = 0;\n\t\tinsn->imm = env->insn_aux_data[i].call_imm;\n\t}\n\tbpf_prog_free_jited_linfo(prog);\n\treturn err;\n}\n\nstatic int fixup_call_args(struct bpf_verifier_env *env)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tint i, depth;\n#endif\n\tint err = 0;\n\n\tif (env->prog->jit_requested &&\n\t    !bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\terr = jit_subprogs(env);\n\t\tif (err == 0)\n\t\t\treturn 0;\n\t\tif (err == -EFAULT)\n\t\t\treturn err;\n\t}\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tfor (i = 0; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tdepth = get_callee_stack_depth(env, insn, i);\n\t\tif (depth < 0)\n\t\t\treturn depth;\n\t\tbpf_patch_call_args(insn, depth);\n\t}\n\terr = 0;\n#endif\n\treturn err;\n}\n\n/* fixup insn->imm field of bpf_call instructions\n * and inline eligible helpers as explicit sequence of BPF instructions\n *\n * this function is called after eBPF program passed verification\n */\nstatic int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tenv->prog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tenv->prog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tsl = env->explored_states[i];\n\n\t\tif (sl)\n\t\t\twhile (sl != STATE_LIST_MARK) {\n\t\t\t\tsln = sl->next;\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tsl = sln;\n\t\t\t}\n\t}\n\n\tkfree(env->explored_states);\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr,\n\t      union bpf_attr __user *uattr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifier_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data =\n\t\tvzalloc(array_size(sizeof(struct bpf_insn_aux_data),\n\t\t\t\t   (*prog)->len));\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\tif (attr->prog_flags & BPF_F_ANY_ALIGNMENT)\n\t\tenv->strict_alignment = false;\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\tret = bpf_prog_offload_verifier_prep(env->prog);\n\t\tif (ret)\n\t\t\tgoto skip_full_check;\n\t}\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = check_subprogs(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_btf_info(env, attr, uattr);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\n\tif (ret == 0 && bpf_prog_is_dev_bound(env->prog->aux))\n\t\tret = bpf_prog_offload_finalize(env);\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\tret = check_max_stack_depth(env);\n\n\t/* instruction rewrites happen after this point */\n\tif (ret == 0)\n\t\tsanitize_dead_code(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (ret == 0)\n\t\tret = fixup_call_args(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\n\tif (ret == 0)\n\t\tadjust_btf_func(env);\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_used_maps() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n"], "fixing_code": ["/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of version 2 of the GNU General Public\n * License as published by the Free Software Foundation.\n */\n#ifndef _LINUX_BPF_VERIFIER_H\n#define _LINUX_BPF_VERIFIER_H 1\n\n#include <linux/bpf.h> /* for enum bpf_reg_type */\n#include <linux/filter.h> /* for MAX_BPF_STACK */\n#include <linux/tnum.h>\n\n/* Maximum variable offset umax_value permitted when resolving memory accesses.\n * In practice this is far bigger than any realistic pointer offset; this limit\n * ensures that umax_value + (int)off + (int)size cannot overflow a u64.\n */\n#define BPF_MAX_VAR_OFF\t(1 << 29)\n/* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures\n * that converting umax_value to int cannot overflow.\n */\n#define BPF_MAX_VAR_SIZ\t(1 << 29)\n\n/* Liveness marks, used for registers and spilled-regs (in stack slots).\n * Read marks propagate upwards until they find a write mark; they record that\n * \"one of this state's descendants read this reg\" (and therefore the reg is\n * relevant for states_equal() checks).\n * Write marks collect downwards and do not propagate; they record that \"the\n * straight-line code that reached this state (from its parent) wrote this reg\"\n * (and therefore that reads propagated from this state or its descendants\n * should not propagate to its parent).\n * A state with a write mark can receive read marks; it just won't propagate\n * them to its parent, since the write mark is a property, not of the state,\n * but of the link between it and its parent.  See mark_reg_read() and\n * mark_stack_slot_read() in kernel/bpf/verifier.c.\n */\nenum bpf_reg_liveness {\n\tREG_LIVE_NONE = 0, /* reg hasn't been read or written this branch */\n\tREG_LIVE_READ, /* reg was read, so we're sensitive to initial value */\n\tREG_LIVE_WRITTEN, /* reg was written first, screening off later reads */\n\tREG_LIVE_DONE = 4, /* liveness won't be updating this register anymore */\n};\n\nstruct bpf_reg_state {\n\t/* Ordering of fields matters.  See states_equal() */\n\tenum bpf_reg_type type;\n\tunion {\n\t\t/* valid when type == PTR_TO_PACKET */\n\t\tu16 range;\n\n\t\t/* valid when type == CONST_PTR_TO_MAP | PTR_TO_MAP_VALUE |\n\t\t *   PTR_TO_MAP_VALUE_OR_NULL\n\t\t */\n\t\tstruct bpf_map *map_ptr;\n\n\t\t/* Max size from any of the above. */\n\t\tunsigned long raw;\n\t};\n\t/* Fixed part of pointer offset, pointer types only */\n\ts32 off;\n\t/* For PTR_TO_PACKET, used to find other pointers with the same variable\n\t * offset, so they can share range knowledge.\n\t * For PTR_TO_MAP_VALUE_OR_NULL this is used to share which map value we\n\t * came from, when one is tested for != NULL.\n\t * For PTR_TO_SOCKET this is used to share which pointers retain the\n\t * same reference to the socket, to determine proper reference freeing.\n\t */\n\tu32 id;\n\t/* For scalar types (SCALAR_VALUE), this represents our knowledge of\n\t * the actual value.\n\t * For pointer types, this represents the variable part of the offset\n\t * from the pointed-to object, and is shared with all bpf_reg_states\n\t * with the same id as us.\n\t */\n\tstruct tnum var_off;\n\t/* Used to determine if any memory access using this register will\n\t * result in a bad access.\n\t * These refer to the same value as var_off, not necessarily the actual\n\t * contents of the register.\n\t */\n\ts64 smin_value; /* minimum possible (s64)value */\n\ts64 smax_value; /* maximum possible (s64)value */\n\tu64 umin_value; /* minimum possible (u64)value */\n\tu64 umax_value; /* maximum possible (u64)value */\n\t/* parentage chain for liveness checking */\n\tstruct bpf_reg_state *parent;\n\t/* Inside the callee two registers can be both PTR_TO_STACK like\n\t * R1=fp-8 and R2=fp-8, but one of them points to this function stack\n\t * while another to the caller's stack. To differentiate them 'frameno'\n\t * is used which is an index in bpf_verifier_state->frame[] array\n\t * pointing to bpf_func_state.\n\t */\n\tu32 frameno;\n\tenum bpf_reg_liveness live;\n};\n\nenum bpf_stack_slot_type {\n\tSTACK_INVALID,    /* nothing was stored in this stack slot */\n\tSTACK_SPILL,      /* register spilled into stack */\n\tSTACK_MISC,\t  /* BPF program wrote some data into this slot */\n\tSTACK_ZERO,\t  /* BPF program wrote constant zero */\n};\n\n#define BPF_REG_SIZE 8\t/* size of eBPF register in bytes */\n\nstruct bpf_stack_state {\n\tstruct bpf_reg_state spilled_ptr;\n\tu8 slot_type[BPF_REG_SIZE];\n};\n\nstruct bpf_reference_state {\n\t/* Track each reference created with a unique id, even if the same\n\t * instruction creates the reference multiple times (eg, via CALL).\n\t */\n\tint id;\n\t/* Instruction where the allocation of this reference occurred. This\n\t * is used purely to inform the user of a reference leak.\n\t */\n\tint insn_idx;\n};\n\n/* state of the program:\n * type of all registers and stack info\n */\nstruct bpf_func_state {\n\tstruct bpf_reg_state regs[MAX_BPF_REG];\n\t/* index of call instruction that called into this func */\n\tint callsite;\n\t/* stack frame number of this function state from pov of\n\t * enclosing bpf_verifier_state.\n\t * 0 = main function, 1 = first callee.\n\t */\n\tu32 frameno;\n\t/* subprog number == index within subprog_stack_depth\n\t * zero == main subprog\n\t */\n\tu32 subprogno;\n\n\t/* The following fields should be last. See copy_func_state() */\n\tint acquired_refs;\n\tstruct bpf_reference_state *refs;\n\tint allocated_stack;\n\tstruct bpf_stack_state *stack;\n};\n\n#define MAX_CALL_FRAMES 8\nstruct bpf_verifier_state {\n\t/* call stack tracking */\n\tstruct bpf_func_state *frame[MAX_CALL_FRAMES];\n\tu32 curframe;\n\tbool speculative;\n};\n\n#define bpf_get_spilled_reg(slot, frame)\t\t\t\t\\\n\t(((slot < frame->allocated_stack / BPF_REG_SIZE) &&\t\t\\\n\t  (frame->stack[slot].slot_type[0] == STACK_SPILL))\t\t\\\n\t ? &frame->stack[slot].spilled_ptr : NULL)\n\n/* Iterate over 'frame', setting 'reg' to either NULL or a spilled register. */\n#define bpf_for_each_spilled_reg(iter, frame, reg)\t\t\t\\\n\tfor (iter = 0, reg = bpf_get_spilled_reg(iter, frame);\t\t\\\n\t     iter < frame->allocated_stack / BPF_REG_SIZE;\t\t\\\n\t     iter++, reg = bpf_get_spilled_reg(iter, frame))\n\n/* linked list of verifier states used to prune search */\nstruct bpf_verifier_state_list {\n\tstruct bpf_verifier_state state;\n\tstruct bpf_verifier_state_list *next;\n};\n\n/* Possible states for alu_state member. */\n#define BPF_ALU_SANITIZE_SRC\t\t1U\n#define BPF_ALU_SANITIZE_DST\t\t2U\n#define BPF_ALU_NEG_VALUE\t\t(1U << 2)\n#define BPF_ALU_SANITIZE\t\t(BPF_ALU_SANITIZE_SRC | \\\n\t\t\t\t\t BPF_ALU_SANITIZE_DST)\n\nstruct bpf_insn_aux_data {\n\tunion {\n\t\tenum bpf_reg_type ptr_type;\t/* pointer type for load/store insns */\n\t\tunsigned long map_state;\t/* pointer/poison value for maps */\n\t\ts32 call_imm;\t\t\t/* saved imm field of call insn */\n\t\tu32 alu_limit;\t\t\t/* limit for add/sub register with pointer */\n\t};\n\tint ctx_field_size; /* the ctx field size for load insn, maybe 0 */\n\tint sanitize_stack_off; /* stack slot to be cleared */\n\tbool seen; /* this insn was processed by the verifier */\n\tu8 alu_state; /* used in combination with alu_limit */\n};\n\n#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */\n\n#define BPF_VERIFIER_TMP_LOG_SIZE\t1024\n\nstruct bpf_verifier_log {\n\tu32 level;\n\tchar kbuf[BPF_VERIFIER_TMP_LOG_SIZE];\n\tchar __user *ubuf;\n\tu32 len_used;\n\tu32 len_total;\n};\n\nstatic inline bool bpf_verifier_log_full(const struct bpf_verifier_log *log)\n{\n\treturn log->len_used >= log->len_total - 1;\n}\n\nstatic inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)\n{\n\treturn log->level && log->ubuf && !bpf_verifier_log_full(log);\n}\n\n#define BPF_MAX_SUBPROGS 256\n\nstruct bpf_subprog_info {\n\tu32 start; /* insn idx of function entry point */\n\tu32 linfo_idx; /* The idx to the main_prog->aux->linfo */\n\tu16 stack_depth; /* max. stack depth used by this function */\n};\n\n/* single container for all structs\n * one verifier_env per bpf_check() call\n */\nstruct bpf_verifier_env {\n\tu32 insn_idx;\n\tu32 prev_insn_idx;\n\tstruct bpf_prog *prog;\t\t/* eBPF program being verified */\n\tconst struct bpf_verifier_ops *ops;\n\tstruct bpf_verifier_stack_elem *head; /* stack of verifier states to be processed */\n\tint stack_size;\t\t\t/* number of states to be processed */\n\tbool strict_alignment;\t\t/* perform strict pointer alignment checks */\n\tstruct bpf_verifier_state *cur_state; /* current verifier state */\n\tstruct bpf_verifier_state_list **explored_states; /* search pruning optimization */\n\tstruct bpf_map *used_maps[MAX_USED_MAPS]; /* array of map's used by eBPF program */\n\tu32 used_map_cnt;\t\t/* number of used maps */\n\tu32 id_gen;\t\t\t/* used to generate unique reg IDs */\n\tbool allow_ptr_leaks;\n\tbool seen_direct_write;\n\tstruct bpf_insn_aux_data *insn_aux_data; /* array of per-insn state */\n\tconst struct bpf_line_info *prev_linfo;\n\tstruct bpf_verifier_log log;\n\tstruct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];\n\tu32 subprog_cnt;\n};\n\n__printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,\n\t\t\t\t      const char *fmt, va_list args);\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...);\n\nstatic inline struct bpf_func_state *cur_func(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[cur->curframe];\n}\n\nstatic inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)\n{\n\treturn cur_func(env)->regs;\n}\n\nint bpf_prog_offload_verifier_prep(struct bpf_prog *prog);\nint bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,\n\t\t\t\t int insn_idx, int prev_insn_idx);\nint bpf_prog_offload_finalize(struct bpf_verifier_env *env);\n\n#endif /* _LINUX_BPF_VERIFIER_H */\n", "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n * Copyright (c) 2016 Facebook\n * Copyright (c) 2018 Covalent IO, Inc. http://covalent.io\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of version 2 of the GNU General Public\n * License as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n * General Public License for more details.\n */\n#include <uapi/linux/btf.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n#include <linux/bsearch.h>\n#include <linux/sort.h>\n#include <linux/perf_event.h>\n#include <linux/ctype.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n};\n\n/* bpf_check() is a static code analyzer that walks eBPF program\n * instruction by instruction and updates register/stack state.\n * All paths of conditional branches are analyzed until 'bpf_exit' insn.\n *\n * The first pass is depth-first-search to check that the program is a DAG.\n * It rejects the following programs:\n * - larger than BPF_MAXINSNS insns\n * - if loop is present (detected via back-edge)\n * - unreachable insns exist (shouldn't be a forest. program = one function)\n * - out of bounds or malformed jumps\n * The second pass is all possible path descent from the 1st insn.\n * Since it's analyzing all pathes through the program, the length of the\n * analysis is limited to 64k insn, which may be hit even if total number of\n * insn is less then 4K, but there are too many branches that change stack/regs.\n * Number of 'branches to be analyzed' is limited to 1k\n *\n * On entry to each instruction, each register has a type, and the instruction\n * changes the types of the registers depending on instruction semantics.\n * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is\n * copied to R1.\n *\n * All registers are 64-bit.\n * R0 - return register\n * R1-R5 argument passing registers\n * R6-R9 callee saved registers\n * R10 - frame pointer read-only\n *\n * At the start of BPF program the register R1 contains a pointer to bpf_context\n * and has type PTR_TO_CTX.\n *\n * Verifier tracks arithmetic operations on pointers in case:\n *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),\n * 1st insn copies R10 (which has FRAME_PTR) type into R1\n * and 2nd arithmetic instruction is pattern matched to recognize\n * that it wants to construct a pointer to some element within stack.\n * So after 2nd insn, the register R1 has type PTR_TO_STACK\n * (and -20 constant is saved for further stack bounds checking).\n * Meaning that this reg is a pointer to stack plus known immediate constant.\n *\n * Most of the time the registers have SCALAR_VALUE type, which\n * means the register has some value, but it's not a valid pointer.\n * (like pointer plus pointer becomes SCALAR_VALUE type)\n *\n * When verifier sees load or store instructions the type of base register\n * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK, PTR_TO_SOCKET. These are\n * four pointer types recognized by check_mem_access() function.\n *\n * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'\n * and the range of [ptr, ptr + map's value_size) is accessible.\n *\n * registers used to pass values to function calls are checked against\n * function argument constraints.\n *\n * ARG_PTR_TO_MAP_KEY is one of such argument constraints.\n * It means that the register type passed to this function must be\n * PTR_TO_STACK and it will be used inside the function as\n * 'pointer to map element key'\n *\n * For example the argument constraints for bpf_map_lookup_elem():\n *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,\n *   .arg1_type = ARG_CONST_MAP_PTR,\n *   .arg2_type = ARG_PTR_TO_MAP_KEY,\n *\n * ret_type says that this function returns 'pointer to map elem value or null'\n * function expects 1st argument to be a const pointer to 'struct bpf_map' and\n * 2nd argument should be a pointer to stack, which will be used inside\n * the helper function as a pointer to map element key.\n *\n * On the kernel side the helper function looks like:\n * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n * {\n *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;\n *    void *key = (void *) (unsigned long) r2;\n *    void *value;\n *\n *    here kernel can access 'key' and 'map' pointers safely, knowing that\n *    [key, key + map->key_size) bytes are valid and were initialized on\n *    the stack of eBPF program.\n * }\n *\n * Corresponding eBPF program may look like:\n *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK\n *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP\n *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n * here verifier looks at prototype of map_lookup_elem() and sees:\n * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,\n * Now verifier knows that this map has key of R1->map_ptr->key_size bytes\n *\n * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,\n * Now verifier checks that [R2, R2 + map's key_size) are within stack limits\n * and were initialized prior to this call.\n * If it's ok, then verifier allows this BPF_CALL insn and looks at\n * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets\n * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function\n * returns ether pointer to map value or NULL.\n *\n * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'\n * insn, the register holding that pointer in the true branch changes state to\n * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false\n * branch. See check_cond_jmp_op().\n *\n * After the call R0 is set to return type of the function and registers R1-R5\n * are set to NOT_INIT to indicate that they are no longer readable.\n *\n * The following reference types represent a potential reference to a kernel\n * resource which, after first being allocated, must be checked and freed by\n * the BPF program:\n * - PTR_TO_SOCKET_OR_NULL, PTR_TO_SOCKET\n *\n * When the verifier sees a helper call return a reference type, it allocates a\n * pointer id for the reference and stores it in the current function state.\n * Similar to the way that PTR_TO_MAP_VALUE_OR_NULL is converted into\n * PTR_TO_MAP_VALUE, PTR_TO_SOCKET_OR_NULL becomes PTR_TO_SOCKET when the type\n * passes through a NULL-check conditional. For the branch wherein the state is\n * changed to CONST_IMM, the verifier releases the reference.\n *\n * For each helper function that allocates a reference, such as\n * bpf_sk_lookup_tcp(), there is a corresponding release function, such as\n * bpf_sk_release(). When a reference type passes into the release function,\n * the verifier also releases the reference. If any unchecked or unreleased\n * reference remains at the end of the program, the verifier rejects it.\n */\n\n/* verifier_state + insn_idx are pushed to stack when branch is encountered */\nstruct bpf_verifier_stack_elem {\n\t/* verifer state is 'st'\n\t * before processing instruction 'insn_idx'\n\t * and after processing instruction 'prev_insn_idx'\n\t */\n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n};\n\n#define BPF_COMPLEXITY_LIMIT_INSNS\t131072\n#define BPF_COMPLEXITY_LIMIT_STACK\t1024\n#define BPF_COMPLEXITY_LIMIT_STATES\t64\n\n#define BPF_MAP_PTR_UNPRIV\t1UL\n#define BPF_MAP_PTR_POISON\t((void *)((0xeB9FUL << 1) +\t\\\n\t\t\t\t\t  POISON_POINTER_DELTA))\n#define BPF_MAP_PTR(X)\t\t((struct bpf_map *)((X) & ~BPF_MAP_PTR_UNPRIV))\n\nstatic bool bpf_map_ptr_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn BPF_MAP_PTR(aux->map_state) == BPF_MAP_PTR_POISON;\n}\n\nstatic bool bpf_map_ptr_unpriv(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_state & BPF_MAP_PTR_UNPRIV;\n}\n\nstatic void bpf_map_ptr_store(struct bpf_insn_aux_data *aux,\n\t\t\t      const struct bpf_map *map, bool unpriv)\n{\n\tBUILD_BUG_ON((unsigned long)BPF_MAP_PTR_POISON & BPF_MAP_PTR_UNPRIV);\n\tunpriv |= bpf_map_ptr_unpriv(aux);\n\taux->map_state = (unsigned long)map |\n\t\t\t (unpriv ? BPF_MAP_PTR_UNPRIV : 0UL);\n}\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tint regno;\n\tint access_size;\n\ts64 msize_smax_value;\n\tu64 msize_umax_value;\n\tint ptr_id;\n};\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\nstatic const struct bpf_line_info *\nfind_linfo(const struct bpf_verifier_env *env, u32 insn_off)\n{\n\tconst struct bpf_line_info *linfo;\n\tconst struct bpf_prog *prog;\n\tu32 i, nr_linfo;\n\n\tprog = env->prog;\n\tnr_linfo = prog->aux->nr_linfo;\n\n\tif (!nr_linfo || insn_off >= prog->len)\n\t\treturn NULL;\n\n\tlinfo = prog->aux->linfo;\n\tfor (i = 1; i < nr_linfo; i++)\n\t\tif (insn_off < linfo[i].insn_off)\n\t\t\tbreak;\n\n\treturn &linfo[i - 1];\n}\n\nvoid bpf_verifier_vlog(struct bpf_verifier_log *log, const char *fmt,\n\t\t       va_list args)\n{\n\tunsigned int n;\n\n\tn = vscnprintf(log->kbuf, BPF_VERIFIER_TMP_LOG_SIZE, fmt, args);\n\n\tWARN_ONCE(n >= BPF_VERIFIER_TMP_LOG_SIZE - 1,\n\t\t  \"verifier log line truncated - local buffer too short\\n\");\n\n\tn = min(log->len_total - log->len_used - 1, n);\n\tlog->kbuf[n] = '\\0';\n\n\tif (!copy_to_user(log->ubuf + log->len_used, log->kbuf, n + 1))\n\t\tlog->len_used += n;\n\telse\n\t\tlog->ubuf = NULL;\n}\n\n/* log_level controls verbosity level of eBPF verifier.\n * bpf_verifier_log_write() is used to dump the verification trace to the log,\n * so the user can figure out what's wrong with the program\n */\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...)\n{\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\nEXPORT_SYMBOL_GPL(bpf_verifier_log_write);\n\n__printf(2, 3) static void verbose(void *private_data, const char *fmt, ...)\n{\n\tstruct bpf_verifier_env *env = private_data;\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\n\nstatic const char *ltrim(const char *s)\n{\n\twhile (isspace(*s))\n\t\ts++;\n\n\treturn s;\n}\n\n__printf(3, 4) static void verbose_linfo(struct bpf_verifier_env *env,\n\t\t\t\t\t u32 insn_off,\n\t\t\t\t\t const char *prefix_fmt, ...)\n{\n\tconst struct bpf_line_info *linfo;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tlinfo = find_linfo(env, insn_off);\n\tif (!linfo || linfo == env->prev_linfo)\n\t\treturn;\n\n\tif (prefix_fmt) {\n\t\tva_list args;\n\n\t\tva_start(args, prefix_fmt);\n\t\tbpf_verifier_vlog(&env->log, prefix_fmt, args);\n\t\tva_end(args);\n\t}\n\n\tverbose(env, \"%s\\n\",\n\t\tltrim(btf_name_by_offset(env->prog->aux->btf,\n\t\t\t\t\t linfo->line_off)));\n\n\tenv->prev_linfo = linfo;\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\nstatic bool reg_type_may_be_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_MAP_VALUE_OR_NULL ||\n\t       type == PTR_TO_SOCKET_OR_NULL;\n}\n\nstatic bool type_is_refcounted(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET;\n}\n\nstatic bool type_is_refcounted_or_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET || type == PTR_TO_SOCKET_OR_NULL;\n}\n\nstatic bool reg_is_refcounted(const struct bpf_reg_state *reg)\n{\n\treturn type_is_refcounted(reg->type);\n}\n\nstatic bool reg_is_refcounted_or_null(const struct bpf_reg_state *reg)\n{\n\treturn type_is_refcounted_or_null(reg->type);\n}\n\nstatic bool arg_type_is_refcounted(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_SOCKET;\n}\n\n/* Determine whether the function releases some resources allocated by another\n * function call. The first reference type argument will be assumed to be\n * released by release_reference().\n */\nstatic bool is_release_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_release;\n}\n\n/* string representation of 'enum bpf_reg_type' */\nstatic const char * const reg_type_str[] = {\n\t[NOT_INIT]\t\t= \"?\",\n\t[SCALAR_VALUE]\t\t= \"inv\",\n\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t[PTR_TO_MAP_VALUE_OR_NULL] = \"map_value_or_null\",\n\t[PTR_TO_STACK]\t\t= \"fp\",\n\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n\t[PTR_TO_FLOW_KEYS]\t= \"flow_keys\",\n\t[PTR_TO_SOCKET]\t\t= \"sock\",\n\t[PTR_TO_SOCKET_OR_NULL] = \"sock_or_null\",\n};\n\nstatic char slot_type_char[] = {\n\t[STACK_INVALID]\t= '?',\n\t[STACK_SPILL]\t= 'r',\n\t[STACK_MISC]\t= 'm',\n\t[STACK_ZERO]\t= '0',\n};\n\nstatic void print_liveness(struct bpf_verifier_env *env,\n\t\t\t   enum bpf_reg_liveness live)\n{\n\tif (live & (REG_LIVE_READ | REG_LIVE_WRITTEN | REG_LIVE_DONE))\n\t    verbose(env, \"_\");\n\tif (live & REG_LIVE_READ)\n\t\tverbose(env, \"r\");\n\tif (live & REG_LIVE_WRITTEN)\n\t\tverbose(env, \"w\");\n\tif (live & REG_LIVE_DONE)\n\t\tverbose(env, \"D\");\n}\n\nstatic struct bpf_func_state *func(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[reg->frameno];\n}\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t\tif (t == PTR_TO_STACK)\n\t\t\t\tverbose(env, \",call_%d\", func(env, reg)->callsite);\n\t\t} else {\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (state->stack[i].slot_type[0] == STACK_SPILL)\n\t\t\tverbose(env, \"=%s\",\n\t\t\t\treg_type_str[state->stack[i].spilled_ptr.type]);\n\t\telse\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tverbose(env, \"\\n\");\n}\n\n#define COPY_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\t\\\nstatic int copy_##NAME##_state(struct bpf_func_state *dst,\t\t\\\n\t\t\t       const struct bpf_func_state *src)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (!src->FIELD)\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\tif (WARN_ON_ONCE(dst->COUNT < src->COUNT)) {\t\t\t\\\n\t\t/* internal bug, make state invalid to reject the program */ \\\n\t\tmemset(dst, 0, sizeof(*dst));\t\t\t\t\\\n\t\treturn -EFAULT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tmemcpy(dst->FIELD, src->FIELD,\t\t\t\t\t\\\n\t       sizeof(*src->FIELD) * (src->COUNT / SIZE));\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* copy_reference_state() */\nCOPY_STATE_FN(reference, acquired_refs, refs, 1)\n/* copy_stack_state() */\nCOPY_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef COPY_STATE_FN\n\n#define REALLOC_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\\\nstatic int realloc_##NAME##_state(struct bpf_func_state *state, int size, \\\n\t\t\t\t  bool copy_old)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu32 old_size = state->COUNT;\t\t\t\t\t\\\n\tstruct bpf_##NAME##_state *new_##FIELD;\t\t\t\t\\\n\tint slot = size / SIZE;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (size <= old_size || !size) {\t\t\t\t\\\n\t\tif (copy_old)\t\t\t\t\t\t\\\n\t\t\treturn 0;\t\t\t\t\t\\\n\t\tstate->COUNT = slot * SIZE;\t\t\t\t\\\n\t\tif (!size && old_size) {\t\t\t\t\\\n\t\t\tkfree(state->FIELD);\t\t\t\t\\\n\t\t\tstate->FIELD = NULL;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tnew_##FIELD = kmalloc_array(slot, sizeof(struct bpf_##NAME##_state), \\\n\t\t\t\t    GFP_KERNEL);\t\t\t\\\n\tif (!new_##FIELD)\t\t\t\t\t\t\\\n\t\treturn -ENOMEM;\t\t\t\t\t\t\\\n\tif (copy_old) {\t\t\t\t\t\t\t\\\n\t\tif (state->FIELD)\t\t\t\t\t\\\n\t\t\tmemcpy(new_##FIELD, state->FIELD,\t\t\\\n\t\t\t       sizeof(*new_##FIELD) * (old_size / SIZE)); \\\n\t\tmemset(new_##FIELD + old_size / SIZE, 0,\t\t\\\n\t\t       sizeof(*new_##FIELD) * (size - old_size) / SIZE); \\\n\t}\t\t\t\t\t\t\t\t\\\n\tstate->COUNT = slot * SIZE;\t\t\t\t\t\\\n\tkfree(state->FIELD);\t\t\t\t\t\t\\\n\tstate->FIELD = new_##FIELD;\t\t\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* realloc_reference_state() */\nREALLOC_STATE_FN(reference, acquired_refs, refs, 1)\n/* realloc_stack_state() */\nREALLOC_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef REALLOC_STATE_FN\n\n/* do_check() starts with zero-sized stack in struct bpf_verifier_state to\n * make it consume minimal amount of memory. check_stack_write() access from\n * the program calls into realloc_func_state() to grow the stack size.\n * Note there is a non-zero 'parent' pointer inside bpf_verifier_state\n * which realloc_stack_state() copies over. It points to previous\n * bpf_verifier_state which is never reallocated.\n */\nstatic int realloc_func_state(struct bpf_func_state *state, int stack_size,\n\t\t\t      int refs_size, bool copy_old)\n{\n\tint err = realloc_reference_state(state, refs_size, copy_old);\n\tif (err)\n\t\treturn err;\n\treturn realloc_stack_state(state, stack_size, copy_old);\n}\n\n/* Acquire a pointer id from the env and update the state->refs to include\n * this new pointer reference.\n * On success, returns a valid pointer id to associate with the register\n * On failure, returns a negative errno.\n */\nstatic int acquire_reference_state(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint new_ofs = state->acquired_refs;\n\tint id, err;\n\n\terr = realloc_reference_state(state, state->acquired_refs + 1, true);\n\tif (err)\n\t\treturn err;\n\tid = ++env->id_gen;\n\tstate->refs[new_ofs].id = id;\n\tstate->refs[new_ofs].insn_idx = insn_idx;\n\n\treturn id;\n}\n\n/* release function corresponding to acquire_reference_state(). Idempotent. */\nstatic int __release_reference_state(struct bpf_func_state *state, int ptr_id)\n{\n\tint i, last_idx;\n\n\tif (!ptr_id)\n\t\treturn -EFAULT;\n\n\tlast_idx = state->acquired_refs - 1;\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->refs[i].id == ptr_id) {\n\t\t\tif (last_idx && i != last_idx)\n\t\t\t\tmemcpy(&state->refs[i], &state->refs[last_idx],\n\t\t\t\t       sizeof(*state->refs));\n\t\t\tmemset(&state->refs[last_idx], 0, sizeof(*state->refs));\n\t\t\tstate->acquired_refs--;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EFAULT;\n}\n\n/* variation on the above for cases where we expect that there must be an\n * outstanding reference for the specified ptr_id.\n */\nstatic int release_reference_state(struct bpf_verifier_env *env, int ptr_id)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint err;\n\n\terr = __release_reference_state(state, ptr_id);\n\tif (WARN_ON_ONCE(err != 0))\n\t\tverbose(env, \"verifier internal error: can't release reference\\n\");\n\treturn err;\n}\n\nstatic int transfer_reference_state(struct bpf_func_state *dst,\n\t\t\t\t    struct bpf_func_state *src)\n{\n\tint err = realloc_reference_state(dst, src->acquired_refs, false);\n\tif (err)\n\t\treturn err;\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic void free_func_state(struct bpf_func_state *state)\n{\n\tif (!state)\n\t\treturn;\n\tkfree(state->refs);\n\tkfree(state->stack);\n\tkfree(state);\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tint i;\n\n\tfor (i = 0; i <= state->curframe; i++) {\n\t\tfree_func_state(state->frame[i]);\n\t\tstate->frame[i] = NULL;\n\t}\n\tif (free_self)\n\t\tkfree(state);\n}\n\n/* copy verifier state from src to dst growing dst stack space\n * when necessary to accommodate larger src stack\n */\nstatic int copy_func_state(struct bpf_func_state *dst,\n\t\t\t   const struct bpf_func_state *src)\n{\n\tint err;\n\n\terr = realloc_func_state(dst, src->allocated_stack, src->acquired_refs,\n\t\t\t\t false);\n\tif (err)\n\t\treturn err;\n\tmemcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tint i, err;\n\n\t/* if dst has more stack frames then src frame, free them */\n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->speculative = src->speculative;\n\tdst_state->curframe = src->curframe;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx,\n\t\t\t\t\t     bool speculative)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\telem->st.speculative |= speculative;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {\n\t\tverbose(env, \"BPF program is too complex\\n\");\n\t\tgoto err;\n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg);\n\n/* Mark the unknown part of a register (variable offset or scalar value) as\n * known to have the value @imm.\n */\nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\t/* Clear id, off, and union(map_ptr, range) */\n\tmemset(((u8 *)reg) + sizeof(reg->type), 0,\n\t       offsetof(struct bpf_reg_state, var_off) - sizeof(reg->type));\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n}\n\n/* Mark the 'variable offset' part of a register as zero.  This should be\n * used only on registers holding a pointer type.\n */\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void __mark_reg_const_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n\treg->type = SCALAR_VALUE;\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\n/* Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */\nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t/* The register can already have a range from prior markings.\n\t * This is fine as long as it hasn't been advanced from its\n\t * origin.\n\t */\n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n/* Attempts to improve min/max values based on var_off information */\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\n/* Uses signed min/max values to inform unsigned, and vice-versa */\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s64)reg->umax_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\n/* Attempts to improve var_off based on unsigned min/max information */\nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_intersect(reg->var_off,\n\t\t\t\t      tnum_range(reg->umin_value,\n\t\t\t\t\t\t reg->umax_value));\n}\n\n/* Reset the min/max bounds of a register */\nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\n/* Mark a register as having a completely unknown (scalar) value. */\nstatic void __mark_reg_unknown(struct bpf_reg_state *reg)\n{\n\t/*\n\t * Clear type, id, off, and union(map_ptr, range) and\n\t * padding between 'type' and union\n\t */\n\tmemset(reg, 0, offsetof(struct bpf_reg_state, var_off));\n\treg->type = SCALAR_VALUE;\n\treg->var_off = tnum_unknown;\n\treg->frameno = 0;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(regs + regno);\n}\n\nstatic void __mark_reg_not_init(struct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(regs + regno);\n}\n\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t\tregs[i].parent = NULL;\n\t}\n\n\t/* frame pointer */\n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\tregs[BPF_REG_FP].frameno = state->frameno;\n\n\t/* 1st arg to a function */\n\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\tmark_reg_known_zero(env, regs, BPF_REG_1);\n}\n\n#define BPF_MAIN_FUNC (-1)\nstatic void init_func_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *state,\n\t\t\t    int callsite, int frameno, int subprogno)\n{\n\tstate->callsite = callsite;\n\tstate->frameno = frameno;\n\tstate->subprogno = subprogno;\n\tinit_reg_state(env, state);\n}\n\nenum reg_arg_type {\n\tSRC_OP,\t\t/* register is used as source operand */\n\tDST_OP,\t\t/* register is used as destination operand */\n\tDST_OP_NO_MARK\t/* same as above, check only, don't mark */\n};\n\nstatic int cmp_subprogs(const void *a, const void *b)\n{\n\treturn ((struct bpf_subprog_info *)a)->start -\n\t       ((struct bpf_subprog_info *)b)->start;\n}\n\nstatic int find_subprog(struct bpf_verifier_env *env, int off)\n{\n\tstruct bpf_subprog_info *p;\n\n\tp = bsearch(&off, env->subprog_info, env->subprog_cnt,\n\t\t    sizeof(env->subprog_info[0]), cmp_subprogs);\n\tif (!p)\n\t\treturn -ENOENT;\n\treturn p - env->subprog_info;\n\n}\n\nstatic int add_subprog(struct bpf_verifier_env *env, int off)\n{\n\tint insn_cnt = env->prog->len;\n\tint ret;\n\n\tif (off >= insn_cnt || off < 0) {\n\t\tverbose(env, \"call to invalid destination\\n\");\n\t\treturn -EINVAL;\n\t}\n\tret = find_subprog(env, off);\n\tif (ret >= 0)\n\t\treturn 0;\n\tif (env->subprog_cnt >= BPF_MAX_SUBPROGS) {\n\t\tverbose(env, \"too many subprograms\\n\");\n\t\treturn -E2BIG;\n\t}\n\tenv->subprog_info[env->subprog_cnt++].start = off;\n\tsort(env->subprog_info, env->subprog_cnt,\n\t     sizeof(env->subprog_info[0]), cmp_subprogs, NULL);\n\treturn 0;\n}\n\nstatic int check_subprogs(struct bpf_verifier_env *env)\n{\n\tint i, ret, subprog_start, subprog_end, off, cur_subprog = 0;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\n\t/* Add entry function. */\n\tret = add_subprog(env, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* determine subprog starts. The end is one before the next starts */\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (!env->allow_ptr_leaks) {\n\t\t\tverbose(env, \"function calls to other bpf functions are allowed for root only\\n\");\n\t\t\treturn -EPERM;\n\t\t}\n\t\tret = add_subprog(env, i + insn[i].imm + 1);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t/* Add a fake 'exit' subprog which could simplify subprog iteration\n\t * logic. 'subprog_cnt' should not be increased.\n\t */\n\tsubprog[env->subprog_cnt].start = insn_cnt;\n\n\tif (env->log.level > 1)\n\t\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\t\tverbose(env, \"func#%d @%d\\n\", i, subprog[i].start);\n\n\t/* now check that all jumps are within the same subprog */\n\tsubprog_start = subprog[cur_subprog].start;\n\tsubprog_end = subprog[cur_subprog + 1].start;\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tu8 code = insn[i].code;\n\n\t\tif (BPF_CLASS(code) != BPF_JMP)\n\t\t\tgoto next;\n\t\tif (BPF_OP(code) == BPF_EXIT || BPF_OP(code) == BPF_CALL)\n\t\t\tgoto next;\n\t\toff = i + insn[i].off + 1;\n\t\tif (off < subprog_start || off >= subprog_end) {\n\t\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", i, off);\n\t\t\treturn -EINVAL;\n\t\t}\nnext:\n\t\tif (i == subprog_end - 1) {\n\t\t\t/* to avoid fall-through from one subprog into another\n\t\t\t * the last insn of the subprog should be either exit\n\t\t\t * or unconditional jump back\n\t\t\t */\n\t\t\tif (code != (BPF_JMP | BPF_EXIT) &&\n\t\t\t    code != (BPF_JMP | BPF_JA)) {\n\t\t\t\tverbose(env, \"last insn is not an exit or jmp\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsubprog_start = subprog_end;\n\t\t\tcur_subprog++;\n\t\t\tif (cur_subprog < env->subprog_cnt)\n\t\t\t\tsubprog_end = subprog[cur_subprog + 1].start;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Parentage chain of this register (or stack slot) should take care of all\n * issues like callee-saved registers, stack slot allocation time, etc.\n */\nstatic int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str[parent->type],\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* ... then we depend on parent's value */\n\t\tparent->live |= REG_LIVE_READ;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t}\n\treturn 0;\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\tif (t == SRC_OP) {\n\t\t/* check whether register used as source operand can be read */\n\t\tif (regs[regno].type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't need to worry about FP liveness because it's read-only */\n\t\tif (regno != BPF_REG_FP)\n\t\t\treturn mark_reg_read(env, &regs[regno],\n\t\t\t\t\t     regs[regno].parent);\n\t} else {\n\t\t/* check whether register used as dest operand can be written to */\n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tregs[regno].live |= REG_LIVE_WRITTEN;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Does this register contain a constant zero? */\nstatic bool register_is_null(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_equals_const(reg->var_off, 0);\n}\n\n/* check_stack_read/write functions track spill/fill of registers,\n * stack boundary and alignment are checked in check_mem_access()\n */\nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *state, /* func where register points to */\n\t\t\t     int off, int size, int value_regno, int insn_idx)\n{\n\tstruct bpf_func_state *cur; /* state of the current function */\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\tenum bpf_reg_type type;\n\n\terr = realloc_func_state(state, round_up(slot + 1, BPF_REG_SIZE),\n\t\t\t\t state->acquired_refs, true);\n\tif (err)\n\t\treturn err;\n\t/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,\n\t * so it's aligned access and [off, off + size) are within stack limits\n\t */\n\tif (!env->allow_ptr_leaks &&\n\t    state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tcur = env->cur_state->frame[env->cur_state->curframe];\n\tif (value_regno >= 0 &&\n\t    is_spillable_regtype((type = cur->regs[value_regno].type))) {\n\n\t\t/* register containing pointer is being spilled into stack */\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (state != cur && type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"cannot spill pointers to stack into stack frame of the caller\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* save register state */\n\t\tstate->stack[spi].spilled_ptr = cur->regs[value_regno];\n\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\tfor (i = 0; i < BPF_REG_SIZE; i++) {\n\t\t\tif (state->stack[spi].slot_type[i] == STACK_MISC &&\n\t\t\t    !env->allow_ptr_leaks) {\n\t\t\t\tint *poff = &env->insn_aux_data[insn_idx].sanitize_stack_off;\n\t\t\t\tint soff = (-spi - 1) * BPF_REG_SIZE;\n\n\t\t\t\t/* detected reuse of integer stack slot with a pointer\n\t\t\t\t * which means either llvm is reusing stack slot or\n\t\t\t\t * an attacker is trying to exploit CVE-2018-3639\n\t\t\t\t * (speculative store bypass)\n\t\t\t\t * Have to sanitize that slot with preemptive\n\t\t\t\t * store of zero.\n\t\t\t\t */\n\t\t\t\tif (*poff && *poff != soff) {\n\t\t\t\t\t/* disallow programs where single insn stores\n\t\t\t\t\t * into two different stack slots, since verifier\n\t\t\t\t\t * cannot sanitize them\n\t\t\t\t\t */\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"insn %d cannot access two stack slots fp%d and fp%d\",\n\t\t\t\t\t\tinsn_idx, *poff, soff);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\t*poff = soff;\n\t\t\t}\n\t\t\tstate->stack[spi].slot_type[i] = STACK_SPILL;\n\t\t}\n\t} else {\n\t\tu8 type = STACK_MISC;\n\n\t\t/* regular write of data into stack destroys any spilled ptr */\n\t\tstate->stack[spi].spilled_ptr.type = NOT_INIT;\n\t\t/* Mark slots as STACK_MISC if they belonged to spilled ptr. */\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL)\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tstate->stack[spi].slot_type[i] = STACK_MISC;\n\n\t\t/* only mark the slot as written if all 8 bytes were written\n\t\t * otherwise read propagation may incorrectly stop too soon\n\t\t * when stack slots are partially written.\n\t\t * This heuristic means that read propagation will be\n\t\t * conservative, since it will add reg_live_read marks\n\t\t * to stack slots all the way to first state when programs\n\t\t * writes+reads less than 8 bytes\n\t\t */\n\t\tif (size == BPF_REG_SIZE)\n\t\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\t/* when we zero initialize stack slots mark them as such */\n\t\tif (value_regno >= 0 &&\n\t\t    register_is_null(&cur->regs[value_regno]))\n\t\t\ttype = STACK_ZERO;\n\n\t\t/* Mark slots affected by this stack write. */\n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\ttype;\n\t}\n\treturn 0;\n}\n\nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *reg_state /* func where register points to */,\n\t\t\t    int off, int size, int value_regno)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tu8 *stype;\n\n\tif (reg_state->allocated_stack <= slot) {\n\t\tverbose(env, \"invalid read from stack off %d+0 size %d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\tstype = reg_state->stack[spi].slot_type;\n\n\tif (stype[0] == STACK_SPILL) {\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfor (i = 1; i < BPF_REG_SIZE; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_SPILL) {\n\t\t\t\tverbose(env, \"corrupted spill memory\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\n\t\tif (value_regno >= 0) {\n\t\t\t/* restore register state from stack */\n\t\t\tstate->regs[value_regno] = reg_state->stack[spi].spilled_ptr;\n\t\t\t/* mark reg as written since spilled pointer state likely\n\t\t\t * has its liveness marks cleared by is_state_visited()\n\t\t\t * which resets stack/reg liveness for state transitions\n\t\t\t */\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t}\n\t\tmark_reg_read(env, &reg_state->stack[spi].spilled_ptr,\n\t\t\t      reg_state->stack[spi].spilled_ptr.parent);\n\t\treturn 0;\n\t} else {\n\t\tint zeros = 0;\n\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_MISC)\n\t\t\t\tcontinue;\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_ZERO) {\n\t\t\t\tzeros++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, &reg_state->stack[spi].spilled_ptr,\n\t\t\t      reg_state->stack[spi].spilled_ptr.parent);\n\t\tif (value_regno >= 0) {\n\t\t\tif (zeros == size) {\n\t\t\t\t/* any size read into register is zero extended,\n\t\t\t\t * so the whole register == const_zero\n\t\t\t\t */\n\t\t\t\t__mark_reg_const_zero(&state->regs[value_regno]);\n\t\t\t} else {\n\t\t\t\t/* have read misc data from the stack */\n\t\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\t\t}\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t}\n\t\treturn 0;\n\t}\n}\n\nstatic int check_stack_access(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_reg_state *reg,\n\t\t\t      int off, int size)\n{\n\t/* Stack accesses must be at a fixed offset, so that we\n\t * can determine what type of data were returned. See\n\t * check_stack_read().\n\t */\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable stack access var_off=%s off=%d size=%d\",\n\t\t\ttn_buf, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\tverbose(env, \"invalid stack off=%d size=%d\\n\", off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n/* check read/write into map element returned by bpf_map_lookup_elem() */\nstatic int __check_map_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t      int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    off + size > map->value_size) {\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n/* check read/write into a map element with possible variable offset */\nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t/* We may have adjusted the register to this map value, so we\n\t * need to try adding each of min_value and max_value to off\n\t * to make sure our theoretical access will be safe.\n\t */\n\tif (env->log.level)\n\t\tprint_verifier_state(env, state);\n\n\t/* The minimum value is only important with signed\n\t * comparisons where we can't assume the floor of a\n\t * value is 0.  If we are using signed variables for our\n\t * index'es we need to make sure that whatever we use\n\t * will have a set floor within our range.\n\t */\n\tif (reg->smin_value < 0 &&\n\t    (reg->smin_value == S64_MIN ||\n\t     (off + reg->smin_value != (s64)(s32)(off + reg->smin_value)) ||\n\t      reg->smin_value + off < 0)) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the array range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t/* If we haven't set a max value then we need to bail since we can't be\n\t * sure we won't do bad things.\n\t * If reg->umax_value + off could overflow, treat that as unbounded too.\n\t */\n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any array access into a map\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_map_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t zero_size_allowed);\n\tif (err)\n\t\tverbose(env, \"R%d max value is outside of the array range\\n\",\n\t\t\tregno);\n\treturn err;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tswitch (env->prog->type) {\n\t/* Program types only with direct read access go here! */\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_SEG6LOCAL:\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\t/* fallthrough */\n\n\t/* Program types with direct read + write access go here! */\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int __check_packet_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\n\tif (off < 0 || size < 0 || (size == 0 && !zero_size_allowed) ||\n\t    (u64)off + size > reg->range) {\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, reg->off, reg->range);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_packet_access(env, regno, off, size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\n\t/* __check_packet_access has made sure \"off + size - 1\" is within u16.\n\t * reg->umax_value can't be bigger than MAX_PACKET_OFF which is 0xffff,\n\t * otherwise find_good_pkt_pointers would have refused to set range info\n\t * that __check_packet_access would have rejected this pkt access.\n\t * Therefore, \"off + reg->umax_value + size - 1\" won't overflow u32.\n\t */\n\tenv->prog->aux->max_pkt_offset =\n\t\tmax_t(u32, env->prog->aux->max_pkt_offset,\n\t\t      off + reg->umax_value + size - 1);\n\n\treturn err;\n}\n\n/* check access to 'struct bpf_context' fields.  Supports fixed offsets only */\nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic int check_flow_keys_access(struct bpf_verifier_env *env, int off,\n\t\t\t\t  int size)\n{\n\tif (size < 0 || off < 0 ||\n\t    (u64)off + size > sizeof(struct bpf_flow_keys)) {\n\t\tverbose(env, \"invalid access to flow keys off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_sock_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t     int size, enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tif (!bpf_sock_is_valid_access(off, size, t, &info)) {\n\t\tverbose(env, \"invalid bpf_sock access off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\nstatic struct bpf_reg_state *reg_state(struct bpf_verifier_env *env, int regno)\n{\n\treturn cur_regs(env) + regno;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, reg_state(env, regno));\n}\n\nstatic bool is_ctx_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn reg->type == PTR_TO_CTX ||\n\t       reg->type == PTR_TO_SOCKET;\n}\n\nstatic bool is_pkt_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool is_flow_key_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\t/* Separate to is_ctx_reg() since we still want to allow BPF_ST here. */\n\treturn reg->type == PTR_TO_FLOW_KEYS;\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t/* For platforms that do not have a Kconfig enabling\n\t * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of\n\t * NET_IP_ALIGN is universally set to '2'.  And on platforms\n\t * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get\n\t * to this code only in strict mode where we want to emulate\n\t * the NET_IP_ALIGN==2 checking.  Therefore use an\n\t * unconditional IP align value of '2'.\n\t */\n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg, int off,\n\t\t\t       int size, bool strict_alignment_once)\n{\n\tbool strict = env->strict_alignment || strict_alignment_once;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t/* Special case, because of NET_IP_ALIGN. Given metadata sits\n\t\t * right in front, treat it the very same way.\n\t\t */\n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_FLOW_KEYS:\n\t\tpointer_desc = \"flow keys \";\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\t/* The stack spill tracking logic in check_stack_write()\n\t\t * and check_stack_read() relies on stack accesses being\n\t\t * aligned.\n\t\t */\n\t\tstrict = true;\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tpointer_desc = \"sock \";\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\nstatic int update_stack_depth(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_func_state *func,\n\t\t\t      int off)\n{\n\tu16 stack = env->subprog_info[func->subprogno].stack_depth;\n\n\tif (stack >= -off)\n\t\treturn 0;\n\n\t/* update known max for given subprogram */\n\tenv->subprog_info[func->subprogno].stack_depth = -off;\n\treturn 0;\n}\n\n/* starting from main bpf function walk all instructions of the function\n * and recursively walk all callees that given function can call.\n * Ignore jump and exit insns.\n * Since recursion is prevented by check_cfg() this algorithm\n * only needs a local stack of MAX_CALL_FRAMES to remember callsites\n */\nstatic int check_max_stack_depth(struct bpf_verifier_env *env)\n{\n\tint depth = 0, frame = 0, idx = 0, i = 0, subprog_end;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint ret_insn[MAX_CALL_FRAMES];\n\tint ret_prog[MAX_CALL_FRAMES];\n\nprocess_func:\n\t/* round up to 32-bytes, since this is granularity\n\t * of interpreter stack size\n\t */\n\tdepth += round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tif (depth > MAX_BPF_STACK) {\n\t\tverbose(env, \"combined stack size of %d calls is %d. Too large\\n\",\n\t\t\tframe + 1, depth);\n\t\treturn -EACCES;\n\t}\ncontinue_func:\n\tsubprog_end = subprog[idx + 1].start;\n\tfor (; i < subprog_end; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* remember insn and function to return to */\n\t\tret_insn[frame] = i + 1;\n\t\tret_prog[frame] = idx;\n\n\t\t/* find the callee */\n\t\ti = i + insn[i].imm + 1;\n\t\tidx = find_subprog(env, i);\n\t\tif (idx < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tframe++;\n\t\tif (frame >= MAX_CALL_FRAMES) {\n\t\t\tWARN_ONCE(1, \"verifier bug. Call stack is too deep\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tgoto process_func;\n\t}\n\t/* end of for() loop means the last insn of the 'subprog'\n\t * was reached. Doesn't matter whether it was JA or EXIT\n\t */\n\tif (frame == 0)\n\t\treturn 0;\n\tdepth -= round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tframe--;\n\ti = ret_insn[frame];\n\tidx = ret_prog[frame];\n\tgoto continue_func;\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\nstatic int get_callee_stack_depth(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_insn *insn, int idx)\n{\n\tint start = idx + insn->imm + 1, subprog;\n\n\tsubprog = find_subprog(env, start);\n\tif (subprog < 0) {\n\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t  start);\n\t\treturn -EFAULT;\n\t}\n\treturn env->subprog_info[subprog].stack_depth;\n}\n#endif\n\nstatic int check_ctx_reg(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *reg, int regno)\n{\n\t/* Access to ctx or passing it to a helper is only allowed in\n\t * its original, unmodified form.\n\t */\n\n\tif (reg->off) {\n\t\tverbose(env, \"dereference of modified ctx ptr R%d off=%d disallowed\\n\",\n\t\t\tregno, reg->off);\n\t\treturn -EACCES;\n\t}\n\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable ctx access var_off=%s disallowed\\n\", tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n/* truncate register to smaller size (in bytes)\n * must be called with size < BPF_REG_SIZE\n */\nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t/* clear high bits in bit representation */\n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t/* fix arithmetic bounds */\n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n}\n\n/* check whether memory at (regno + off) is accessible for t = (read | write)\n * if t==write, value_regno is a register which value is stored into memory\n * if t==read, value_regno is a register which will receive the value from memory\n * if t==write && value_regno==-1, some unknown value is stored into memory\n * if t==read && value_regno==-1, don't care what we read from memory\n */\nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\telse\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\toff += reg->var_off.value;\n\t\terr = check_stack_access(env, reg, off, size);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstate = func(env, reg);\n\t\terr = update_stack_depth(env, state, off);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_WRITE)\n\t\t\terr = check_stack_write(env, state, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t\telse\n\t\t\terr = check_stack_read(env, state, off, size,\n\t\t\t\t\t       value_regno);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_SOCKET) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"cannot write into socket\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint err;\n\n\tif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\n\t    insn->imm != 0) {\n\t\tverbose(env, \"BPF_XADD uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check src1 operand */\n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\tif (is_ctx_reg(env, insn->dst_reg) ||\n\t    is_pkt_reg(env, insn->dst_reg) ||\n\t    is_flow_key_reg(env, insn->dst_reg)) {\n\t\tverbose(env, \"BPF_XADD stores into R%d %s is not allowed\\n\",\n\t\t\tinsn->dst_reg,\n\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\treturn -EACCES;\n\t}\n\n\t/* check whether atomic_add can read the memory */\n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1, true);\n\tif (err)\n\t\treturn err;\n\n\t/* check whether atomic_add can write into the same memory */\n\treturn check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\tBPF_SIZE(insn->code), BPF_WRITE, -1, true);\n}\n\n/* when register 'regno' is passed into function that will read 'access_size'\n * bytes from that pointer, make sure that it's within stack boundary\n * and all elements of stack are initialized.\n * Unlike most pointer bounds-checking functions, this one doesn't take an\n * 'off' argument, so it has to add in reg->off itself.\n */\nstatic int check_stack_boundary(struct bpf_verifier_env *env, int regno,\n\t\t\t\tint access_size, bool zero_size_allowed,\n\t\t\t\tstruct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint off, i, slot, spi;\n\n\tif (reg->type != PTR_TO_STACK) {\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n\n\t/* Only allow fixed-offset stack reads */\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"invalid variable stack read R%d var_off=%s\\n\",\n\t\t\tregno, tn_buf);\n\t\treturn -EACCES;\n\t}\n\toff = reg->off + reg->var_off.value;\n\tif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\n\t    access_size < 0 || (access_size == 0 && !zero_size_allowed)) {\n\t\tverbose(env, \"invalid stack type R%d off=%d access_size=%d\\n\",\n\t\t\tregno, off, access_size);\n\t\treturn -EACCES;\n\t}\n\n\tif (meta && meta->raw_mode) {\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < access_size; i++) {\n\t\tu8 *stype;\n\n\t\tslot = -(off + i) - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot)\n\t\t\tgoto err;\n\t\tstype = &state->stack[spi].slot_type[slot % BPF_REG_SIZE];\n\t\tif (*stype == STACK_MISC)\n\t\t\tgoto mark;\n\t\tif (*stype == STACK_ZERO) {\n\t\t\t/* helper can write anything into the stack */\n\t\t\t*stype = STACK_MISC;\n\t\t\tgoto mark;\n\t\t}\nerr:\n\t\tverbose(env, \"invalid indirect read from stack off %d+%d size %d\\n\",\n\t\t\toff, i, access_size);\n\t\treturn -EACCES;\nmark:\n\t\t/* reading any byte out of 8-byte 'spill_slot' will cause\n\t\t * the whole slot to be marked as 'read'\n\t\t */\n\t\tmark_reg_read(env, &state->stack[spi].spilled_ptr,\n\t\t\t      state->stack[spi].spilled_ptr.parent);\n\t}\n\treturn update_stack_depth(env, state, off);\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_VALUE:\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tdefault: /* scalar_value|ptr_to_stack or invalid ptr */\n\t\treturn check_stack_boundary(env, regno, access_size,\n\t\t\t\t\t    zero_size_allowed, meta);\n\t}\n}\n\nstatic bool arg_type_is_mem_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_MEM ||\n\t       type == ARG_PTR_TO_MEM_OR_NULL ||\n\t       type == ARG_PTR_TO_UNINIT_MEM;\n}\n\nstatic bool arg_type_is_mem_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_SIZE ||\n\t       type == ARG_CONST_SIZE_OR_ZERO;\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected_type, type = reg->type;\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (arg_type == ARG_PTR_TO_MAP_KEY ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t    arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (!type_is_pkt_pointer(type) && type != PTR_TO_MAP_VALUE &&\n\t\t    type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO) {\n\t\texpected_type = SCALAR_VALUE;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_MAP_PTR) {\n\t\texpected_type = CONST_PTR_TO_MAP;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_CTX) {\n\t\texpected_type = PTR_TO_CTX;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else if (arg_type == ARG_PTR_TO_SOCKET) {\n\t\texpected_type = PTR_TO_SOCKET;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t\tif (meta->ptr_id || !reg->id) {\n\t\t\tverbose(env, \"verifier internal error: mismatched references meta=%d, reg=%d\\n\",\n\t\t\t\tmeta->ptr_id, reg->id);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tmeta->ptr_id = reg->id;\n\t} else if (arg_type_is_mem_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_STACK;\n\t\t/* One exception here. In case function allows for NULL to be\n\t\t * passed in as argument, it's a SCALAR_VALUE type. Final test\n\t\t * happens during stack boundary checking.\n\t\t */\n\t\tif (register_is_null(reg) &&\n\t\t    arg_type == ARG_PTR_TO_MEM_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t\tmeta->raw_mode = arg_type == ARG_PTR_TO_UNINIT_MEM;\n\t} else {\n\t\tverbose(env, \"unsupported arg_type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tif (arg_type == ARG_CONST_MAP_PTR) {\n\t\t/* bpf_map_xxx(map_ptr) call: remember that map_ptr */\n\t\tmeta->map_ptr = reg->map_ptr;\n\t} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., key) call:\n\t\t * check that [key, key + map->key_size) are within\n\t\t * stack limits and initialized\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* in function declaration map_ptr must come before\n\t\t\t * map_key, so that it's verified and known before\n\t\t\t * we have to check map_key here. Otherwise it means\n\t\t\t * that kernel subsystem misconfigured verifier\n\t\t\t */\n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->key_size, false,\n\t\t\t\t\t      NULL);\n\t} else if (arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t\t   arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., value) call:\n\t\t * check [value, value + map->value_size) validity\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->raw_mode = (arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE);\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->value_size, false,\n\t\t\t\t\t      meta);\n\t} else if (arg_type_is_mem_size(arg_type)) {\n\t\tbool zero_size_allowed = (arg_type == ARG_CONST_SIZE_OR_ZERO);\n\n\t\t/* remember the mem_size which may be used later\n\t\t * to refine return values.\n\t\t */\n\t\tmeta->msize_smax_value = reg->smax_value;\n\t\tmeta->msize_umax_value = reg->umax_value;\n\n\t\t/* The register is SCALAR_VALUE; the access check\n\t\t * happens using its boundaries.\n\t\t */\n\t\tif (!tnum_is_const(reg->var_off))\n\t\t\t/* For unprivileged variable accesses, disable raw\n\t\t\t * mode so that the program is required to\n\t\t\t * initialize all the memory that the helper could\n\t\t\t * just partially fill up.\n\t\t\t */\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smin_value < 0) {\n\t\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->umin_value == 0) {\n\t\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t\t      meta);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t\t      reg->umax_value,\n\t\t\t\t\t      zero_size_allowed, meta);\n\t}\n\n\treturn err;\nerr_type:\n\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\treg_type_str[type], reg_type_str[expected_type]);\n\treturn -EACCES;\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t/* We need a two way check, first is from map perspective ... */\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_STORAGE:\n\tcase BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:\n\t\tif (func_id != BPF_FUNC_get_local_storage)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* devmap returns a pointer to a live net_device ifindex that we cannot\n\t * allow to be modified from bpf side. So do not allow lookup elements\n\t * for now.\n\t */\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* Restrict bpf side of cpumap and xskmap, open when use-cases\n\t * appear.\n\t */\n\tcase BPF_MAP_TYPE_CPUMAP:\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\tif (func_id != BPF_FUNC_sk_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sock_hash_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_hash)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:\n\t\tif (func_id != BPF_FUNC_sk_select_reuseport)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_QUEUE:\n\tcase BPF_MAP_TYPE_STACK:\n\t\tif (func_id != BPF_FUNC_map_peek_elem &&\n\t\t    func_id != BPF_FUNC_map_pop_elem &&\n\t\t    func_id != BPF_FUNC_map_push_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* ... and second from the function itself. */\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tif (env->subprog_cnt > 1) {\n\t\t\tverbose(env, \"tail_calls are not allowed in programs with bpf-to-bpf calls\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_XSKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\tcase BPF_FUNC_msg_redirect_map:\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_hash:\n\tcase BPF_FUNC_msg_redirect_hash:\n\tcase BPF_FUNC_sock_hash_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&\n\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_select_reuseport:\n\t\tif (map->map_type != BPF_MAP_TYPE_REUSEPORT_SOCKARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_peek_elem:\n\tcase BPF_FUNC_map_pop_elem:\n\tcase BPF_FUNC_map_push_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_QUEUE &&\n\t\t    map->map_type != BPF_MAP_TYPE_STACK)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic bool check_raw_mode_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\t/* We only support one arg being in raw mode at the moment,\n\t * which is sufficient for the helper functions we have\n\t * right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic bool check_args_pair_invalid(enum bpf_arg_type arg_curr,\n\t\t\t\t    enum bpf_arg_type arg_next)\n{\n\treturn (arg_type_is_mem_ptr(arg_curr) &&\n\t        !arg_type_is_mem_size(arg_next)) ||\n\t       (!arg_type_is_mem_ptr(arg_curr) &&\n\t\targ_type_is_mem_size(arg_next));\n}\n\nstatic bool check_arg_pair_ok(const struct bpf_func_proto *fn)\n{\n\t/* bpf_xxx(..., buf, len) call will access 'len'\n\t * bytes from memory 'buf'. Both arg types need\n\t * to be paired, so make sure there's no buggy\n\t * helper function specification.\n\t */\n\tif (arg_type_is_mem_size(fn->arg1_type) ||\n\t    arg_type_is_mem_ptr(fn->arg5_type)  ||\n\t    check_args_pair_invalid(fn->arg1_type, fn->arg2_type) ||\n\t    check_args_pair_invalid(fn->arg2_type, fn->arg3_type) ||\n\t    check_args_pair_invalid(fn->arg3_type, fn->arg4_type) ||\n\t    check_args_pair_invalid(fn->arg4_type, fn->arg5_type))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool check_refcount_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (arg_type_is_refcounted(fn->arg1_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg2_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg3_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg4_type))\n\t\tcount++;\n\tif (arg_type_is_refcounted(fn->arg5_type))\n\t\tcount++;\n\n\t/* We only support one arg being unreferenced at the moment,\n\t * which is sufficient for the helper functions we have right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic int check_func_proto(const struct bpf_func_proto *fn)\n{\n\treturn check_raw_mode_ok(fn) &&\n\t       check_arg_pair_ok(fn) &&\n\t       check_refcount_ok(fn) ? 0 : -EINVAL;\n}\n\n/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]\n * are now invalid, so turn them into unknown SCALAR_VALUE.\n */\nstatic void __clear_all_pkt_pointers(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}\n\nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint i;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__clear_all_pkt_pointers(env, vstate->frame[i]);\n}\n\nstatic void release_reg_references(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *state, int id)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].id == id)\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg_is_refcounted(reg) && reg->id == id)\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}\n\n/* The pointer with the specified id has released its reference to kernel\n * resources. Identify all copies of the same pointer and clear the reference.\n */\nstatic int release_reference(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint i;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\trelease_reg_references(env, vstate->frame[i], meta->ptr_id);\n\n\treturn release_reference_state(env, meta->ptr_id);\n}\n\nstatic int check_func_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t   int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tint i, err, subprog, target_insn;\n\n\tif (state->curframe + 1 >= MAX_CALL_FRAMES) {\n\t\tverbose(env, \"the call stack of %d frames is too deep\\n\",\n\t\t\tstate->curframe + 2);\n\t\treturn -E2BIG;\n\t}\n\n\ttarget_insn = *insn_idx + insn->imm;\n\tsubprog = find_subprog(env, target_insn + 1);\n\tif (subprog < 0) {\n\t\tverbose(env, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\ttarget_insn + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tcaller = state->frame[state->curframe];\n\tif (state->frame[state->curframe + 1]) {\n\t\tverbose(env, \"verifier bug. Frame %d already allocated\\n\",\n\t\t\tstate->curframe + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tcallee = kzalloc(sizeof(*callee), GFP_KERNEL);\n\tif (!callee)\n\t\treturn -ENOMEM;\n\tstate->frame[state->curframe + 1] = callee;\n\n\t/* callee cannot access r0, r6 - r9 for reading and has to write\n\t * into its own stack before reading from it.\n\t * callee can read/write into caller's stack\n\t */\n\tinit_func_state(env, callee,\n\t\t\t/* remember the callsite, it will be used by bpf_exit */\n\t\t\t*insn_idx /* callsite */,\n\t\t\tstate->curframe + 1 /* frameno within this callchain */,\n\t\t\tsubprog /* subprog number within this prog */);\n\n\t/* Transfer references to the callee */\n\terr = transfer_reference_state(callee, caller);\n\tif (err)\n\t\treturn err;\n\n\t/* copy r1 - r5 args that callee can access.  The copy includes parent\n\t * pointers, which connects us up to the liveness chain\n\t */\n\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++)\n\t\tcallee->regs[i] = caller->regs[i];\n\n\t/* after the call registers r0 - r5 were scratched */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, caller->regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* only increment it after check_reg_arg() finished */\n\tstate->curframe++;\n\n\t/* and go analyze first insn of the callee */\n\t*insn_idx = target_insn;\n\n\tif (env->log.level) {\n\t\tverbose(env, \"caller:\\n\");\n\t\tprint_verifier_state(env, caller);\n\t\tverbose(env, \"callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t}\n\treturn 0;\n}\n\nstatic int prepare_func_exit(struct bpf_verifier_env *env, int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tstruct bpf_reg_state *r0;\n\tint err;\n\n\tcallee = state->frame[state->curframe];\n\tr0 = &callee->regs[BPF_REG_0];\n\tif (r0->type == PTR_TO_STACK) {\n\t\t/* technically it's ok to return caller's stack pointer\n\t\t * (or caller's caller's pointer) back to the caller,\n\t\t * since these pointers are valid. Only current stack\n\t\t * pointer will be invalid as soon as function exits,\n\t\t * but let's be conservative\n\t\t */\n\t\tverbose(env, \"cannot return stack pointer to the caller\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tstate->curframe--;\n\tcaller = state->frame[state->curframe];\n\t/* return to the caller whatever r0 had in the callee */\n\tcaller->regs[BPF_REG_0] = *r0;\n\n\t/* Transfer references to the caller */\n\terr = transfer_reference_state(caller, callee);\n\tif (err)\n\t\treturn err;\n\n\t*insn_idx = callee->callsite + 1;\n\tif (env->log.level) {\n\t\tverbose(env, \"returning from callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t\tverbose(env, \"to caller at %d:\\n\", *insn_idx);\n\t\tprint_verifier_state(env, caller);\n\t}\n\t/* clear everything in the callee */\n\tfree_func_state(callee);\n\tstate->frame[state->curframe + 1] = NULL;\n\treturn 0;\n}\n\nstatic void do_refine_retval_range(struct bpf_reg_state *regs, int ret_type,\n\t\t\t\t   int func_id,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *ret_reg = &regs[BPF_REG_0];\n\n\tif (ret_type != RET_INTEGER ||\n\t    (func_id != BPF_FUNC_get_stack &&\n\t     func_id != BPF_FUNC_probe_read_str))\n\t\treturn;\n\n\tret_reg->smax_value = meta->msize_smax_value;\n\tret_reg->umax_value = meta->msize_umax_value;\n\t__reg_deduce_bounds(ret_reg);\n\t__reg_bound_offset(ret_reg);\n}\n\nstatic int\nrecord_func_map(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\n\tif (func_id != BPF_FUNC_tail_call &&\n\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t    func_id != BPF_FUNC_map_update_elem &&\n\t    func_id != BPF_FUNC_map_delete_elem &&\n\t    func_id != BPF_FUNC_map_push_elem &&\n\t    func_id != BPF_FUNC_map_pop_elem &&\n\t    func_id != BPF_FUNC_map_peek_elem)\n\t\treturn 0;\n\n\tif (meta->map_ptr == NULL) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!BPF_MAP_PTR(aux->map_state))\n\t\tbpf_map_ptr_store(aux, meta->map_ptr,\n\t\t\t\t  meta->map_ptr->unpriv_array);\n\telse if (BPF_MAP_PTR(aux->map_state) != meta->map_ptr)\n\t\tbpf_map_ptr_store(aux, BPF_MAP_PTR_POISON,\n\t\t\t\t  meta->map_ptr->unpriv_array);\n\treturn 0;\n}\n\nstatic int check_reference_leak(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint i;\n\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tverbose(env, \"Unreleased reference id=%d alloc_insn=%d\\n\",\n\t\t\tstate->refs[i].id, state->refs[i].insn_idx);\n\t}\n\treturn state->acquired_refs ? -EINVAL : 0;\n}\n\nstatic int check_helper_call(struct bpf_verifier_env *env, int func_id, int insn_idx)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tbool changes_data;\n\tint i, err;\n\n\t/* find function prototype */\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\t/* check args */\n\terr = check_func_arg(env, BPF_REG_1, fn->arg1_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_2, fn->arg2_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_3, fn->arg3_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_4, fn->arg4_type, &meta);\n\tif (err)\n\t\treturn err;\n\terr = check_func_arg(env, BPF_REG_5, fn->arg5_type, &meta);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (func_id == BPF_FUNC_tail_call) {\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t} else if (is_release_function(func_id)) {\n\t\terr = release_reference(env, &meta);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tregs = cur_regs(env);\n\n\t/* check that flags argument in get_local_storage(map, flags) is 0,\n\t * this is required because get_local_storage() can't return an error.\n\t */\n\tif (func_id == BPF_FUNC_get_local_storage &&\n\t    !register_is_null(&regs[BPF_REG_2])) {\n\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* update return register (already marked as written above) */\n\tif (fn->ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (fn->ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL ||\n\t\t   fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tif (fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t} else if (fn->ret_type == RET_PTR_TO_SOCKET_OR_NULL) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET_OR_NULL;\n\t\tregs[BPF_REG_0].id = id;\n\t} else {\n\t\tverbose(env, \"unknown return type %d of func %s#%d\\n\",\n\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif (func_id == BPF_FUNC_get_stack && !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s64 a, s64 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic struct bpf_insn_aux_data *cur_aux(struct bpf_verifier_env *env)\n{\n\treturn &env->insn_aux_data[env->insn_idx];\n}\n\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\n\t/* If we arrived here from different branches with different\n\t * limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* fall-through */\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->allow_ptr_leaks) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known, dst_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\n\tif (insn_bitness == 32) {\n\t\t/* Relevant for 32-bit RSH: Information can propagate towards\n\t\t * LSB, so it isn't sufficient to only truncate the output to\n\t\t * 32 bits.\n\t\t */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\tcoerce_reg_to_size(&src_reg, 4);\n\t}\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\tsrc_known = tnum_is_const(src_reg.var_off);\n\tdst_known = tnum_is_const(dst_reg->var_off);\n\n\tif ((src_known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(dst_reg);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value += smin_val;\n\t\t\tdst_reg->smax_value += smax_val;\n\t\t}\n\t\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value += umin_val;\n\t\t\tdst_reg->umax_value += umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value -= smax_val;\n\t\t\tdst_reg->smax_value -= smin_val;\n\t\t}\n\t\tif (dst_reg->umin_value < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value -= umax_val;\n\t\t\tdst_reg->umax_value -= umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t\t/* Ain't nobody got time to multiply that sign */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* Both values are positive, so we can work with unsigned and\n\t\t * copy the result to signed (unless it exceeds S64_MAX).\n\t\t */\n\t\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t\t/* Potential overflow, we know nothing */\n\t\t\t__mark_reg_unbounded(dst_reg);\n\t\t\t/* (except what we can learn from the var_off) */\n\t\t\t__update_reg_bounds(dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tdst_reg->umin_value *= umin_val;\n\t\tdst_reg->umax_value *= umax_val;\n\t\tif (dst_reg->umax_value > S64_MAX) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our minimum from the var_off, since that's inherently\n\t\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t\t */\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = dst_reg->var_off.value;\n\t\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ANDing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tif (src_known && dst_known) {\n\t\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t\t  src_reg.var_off.value);\n\t\t\tbreak;\n\t\t}\n\t\t/* We get our maximum from the var_off, and our minimum is the\n\t\t * maximum of the operands' minima\n\t\t */\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\t\tdst_reg->umax_value = dst_reg->var_off.value |\n\t\t\t\t      dst_reg->var_off.mask;\n\t\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t\t * ain't nobody got time for that.\n\t\t\t */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\t/* ORing two positives gives a positive, so safe to\n\t\t\t * cast result into s64.\n\t\t\t */\n\t\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t\t}\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* We lose all sign bit information (except what we can pick\n\t\t * up from var_off)\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\t/* If we might shift our top bit out, then we know nothing */\n\t\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value <<= umin_val;\n\t\t\tdst_reg->umax_value <<= umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t\t * be negative, then either:\n\t\t * 1) src_reg might be zero, so the sign bit of the result is\n\t\t *    unknown, so we lose our signed bounds\n\t\t * 2) it's known negative, thus the unsigned bounds capture the\n\t\t *    signed bounds\n\t\t * 3) the signed bounds cross zero, so they tell us nothing\n\t\t *    about the result\n\t\t * If the value in dst_reg is known nonnegative, then again the\n\t\t * unsigned bounts capture the signed bounds.\n\t\t * Thus, in all cases it suffices to blow away our signed bounds\n\t\t * and rely on inferring new ones from the unsigned bounds and\n\t\t * var_off of the result.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\t\tdst_reg->umin_value >>= umax_val;\n\t\tdst_reg->umax_value >>= umin_val;\n\t\t/* We may learn something more from the var_off */\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Upon reaching here, src_known is true and\n\t\t * umax_val is equal to umin_val.\n\t\t */\n\t\tdst_reg->smin_value >>= umin_val;\n\t\tdst_reg->smax_value >>= umin_val;\n\t\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val);\n\n\t\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t\t * dst_reg var_off to refine the result.\n\t\t */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t\t__update_reg_bounds(dst_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tcoerce_reg_to_size(dst_reg, 4);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu16 new_range;\n\tint i, j;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].type == type && regs[i].id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\tregs[i].range = max(regs[i].range, new_range);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t\treg->range = max(reg->range, new_range);\n\t\t}\n\t}\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\tif (__is_pointer_value(false, reg))\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= (s64)val)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > (s64)val)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg, u64 val,\n\t\t\t    u8 opcode)\n{\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode)\n{\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t/* If this is false then we know nothing Jon Snow, but if it is\n\t\t * true then we know for sure.\n\t\t */\n\t\t__mark_reg_known(true_reg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t/* If this is true we know nothing Jon Snow, but if it is false\n\t\t * we know the value for sure;\n\t\t */\n\t\t__mark_reg_known(false_reg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGT:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val - 1);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val);\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val - 1);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val);\n\t\tbreak;\n\tcase BPF_JLT:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val + 1);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val);\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val + 1);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val);\n\t\tbreak;\n\tcase BPF_JGE:\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, val);\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\ttrue_reg->smax_value = min_t(s64, true_reg->smax_value, val);\n\t\tfalse_reg->smin_value = max_t(s64, false_reg->smin_value, val + 1);\n\t\tbreak;\n\tcase BPF_JLE:\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, val);\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, val - 1);\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\ttrue_reg->smin_value = max_t(s64, true_reg->smin_value, val);\n\t\tfalse_reg->smax_value = min_t(s64, false_reg->smax_value, val - 1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tif (reg->map_ptr->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = reg->map_ptr->inner_map_meta;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t}\n\t\tif (is_null || !reg_is_refcounted(reg)) {\n\t\t\t/* We don't need id from this point onwards anymore,\n\t\t\t * thus we should better reset it, so that state\n\t\t\t * pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tu32 id = regs[regno].id;\n\tint i, j;\n\n\tif (reg_is_refcounted_or_null(&regs[regno]) && is_null)\n\t\t__release_reference_state(state, id);\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &regs[i], id, is_null);\n\n\tfor (j = 0; j <= vstate->curframe; j++) {\n\t\tstate = vstate->frame[j];\n\t\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\t\tif (!reg)\n\t\t\t\tcontinue;\n\t\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t\t}\n\t}\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tint pred = is_branch_taken(dst_reg, insn->imm, opcode);\n\n\t\tif (pred == 1) {\n\t\t\t /* only follow the goto, ignore fall-through */\n\t\t\t*insn_idx += insn->off;\n\t\t\treturn 0;\n\t\t} else if (pred == 0) {\n\t\t\t/* only follow fall-through branch, since\n\t\t\t * that's where the program will go\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    regs[insn->src_reg].type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(regs[insn->src_reg].var_off))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg, regs[insn->src_reg].var_off.value,\n\t\t\t\t\t\topcode);\n\t\t\telse if (tnum_is_const(dst_reg->var_off))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    dst_reg->var_off.value, opcode);\n\t\t\telse if (opcode == BPF_JEQ || opcode == BPF_JNE)\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    &regs[insn->src_reg],\n\t\t\t\t\t\t    &regs[insn->dst_reg], opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, opcode);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem() */\n\tif (BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* return the map pointer stored inside BPF_LD_IMM64 instruction */\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\n{\n\tu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\n\n\treturn (struct bpf_map *) (unsigned long) imm64;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t/* replace_map_fd_with_map_ptr() should have caught bad ld_imm64 */\n\tBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\n\n\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\tregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, BPF_REG_6, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (regs[BPF_REG_6].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\tverbose(env, \" should have been 0 or 1\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\n#define STATE_LIST_MARK ((struct bpf_verifier_state_list *) -1L)\n\nstatic int *insn_stack;\t/* stack of insns to process */\nstatic int cur_stack;\t/* current stack index */\nstatic int *insn_state;\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tenv->explored_states[w] = STATE_LIST_MARK;\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tcur_stack = 1;\n\npeek_stack:\n\tif (cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tenv->explored_states[t + 1] = STATE_LIST_MARK;\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tenv->explored_states[t] = STATE_LIST_MARK;\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkfree(insn_state);\n\tkfree(insn_stack);\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size, prev_offset;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tint ret = 0;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || BTF_INFO_KIND(type->info) != BTF_KIND_FUNC) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (!env->prog->aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tenv->prog->aux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t/* liveness must not touch this register anymore */\n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t/* since the register is unused, clear its state\n\t\t\t * to make further comparison simpler\n\t\t\t */\n\t\t\t__mark_reg_not_init(&st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t/* liveness must not touch this stack slot anymore */\n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(&st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t/* all regs in this state in all frames were already marked */\n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n/* the parentage chains form a tree.\n * the verifier states are added to state lists at given insn and\n * pushed into state stack for future exploration.\n * when the verifier reaches bpf_exit insn some of the verifer states\n * stored in the state lists have their final liveness state already,\n * but a lot of states will get revised from liveness point of view when\n * the verifier explores other branches.\n * Example:\n * 1: r0 = 1\n * 2: if r1 == 100 goto pc+1\n * 3: r0 = 2\n * 4: exit\n * when the verifier reaches exit insn the register r0 in the state list of\n * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch\n * of insn 2 and goes exploring further. At the insn 4 it will walk the\n * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.\n *\n * Since the verifier pushes the branch states as it sees them while exploring\n * the program the condition of walking the branch instruction for the second\n * time means that all states below this branch were already explored and\n * their final liveness markes are already propagated.\n * Hence when the verifier completes the search of state list in is_state_visited()\n * we can call this clean_live_states() function to mark all liveness states\n * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'\n * will not be used.\n * This function also clears the registers and stack for states that !READ\n * to simplify state merging.\n *\n * Important note here that walking the same branch instruction in the callee\n * doesn't meant that the states are DONE. The verifier has to compare\n * the callsites\n */\nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = env->explored_states[insn];\n\tif (!sl)\n\t\treturn;\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * We don't care about the 'id' value, because nothing\n\t\t * uses it for PTR_TO_MAP_VALUE (only for ..._OR_NULL)\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t/* explored state didn't use this */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\t/* explored stack has more populated slots than current stack\n\t\t * and these slots were used\n\t\t */\n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t/* if old state was safe with misc data in the stack\n\t\t * it will be safe with zero-initialized stack.\n\t\t * The opposite is not true\n\t\t */\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur)\n{\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\treturn !memcmp(old->refs, cur->refs,\n\t\t       sizeof(*old->refs) * old->acquired_refs);\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool func_states_equal(struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\n\tif (!refsafe(old, cur))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}\n\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\t/* Verification state from speculative execution simulation\n\t * must never prune a non-speculative execution one.\n\t */\n\tif (old->speculative && !cur->speculative)\n\t\treturn false;\n\n\t/* for states to be equal callsites have to be the same\n\t * and all frame states need to be equivalent\n\t */\n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n/* A write screens off any subsequent reads; but write marks come from the\n * straight-line code between a state and its parent.  When we arrive at an\n * equivalent state (jump target or such) we didn't arrive by the straight-line\n * code, so read marks in the state must propagate to the parent regardless\n * of the state's write marks. That's what 'parent == state->parent' comparison\n * in mark_reg_read() is for.\n */\nstatic int propagate_liveness(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_verifier_state *vstate,\n\t\t\t      struct bpf_verifier_state *vparent)\n{\n\tint i, frame, err = 0;\n\tstruct bpf_func_state *state, *parent;\n\n\tif (vparent->curframe != vstate->curframe) {\n\t\tWARN(1, \"propagate_live: parent frame %d current frame %d\\n\",\n\t\t     vparent->curframe, vstate->curframe);\n\t\treturn -EFAULT;\n\t}\n\t/* Propagate read liveness of registers... */\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\t/* We don't need to worry about FP liveness because it's read-only */\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tif (vparent->frame[vparent->curframe]->regs[i].live & REG_LIVE_READ)\n\t\t\tcontinue;\n\t\tif (vstate->frame[vstate->curframe]->regs[i].live & REG_LIVE_READ) {\n\t\t\terr = mark_reg_read(env, &vstate->frame[vstate->curframe]->regs[i],\n\t\t\t\t\t    &vparent->frame[vstate->curframe]->regs[i]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\t/* ... and stack slots */\n\tfor (frame = 0; frame <= vstate->curframe; frame++) {\n\t\tstate = vstate->frame[frame];\n\t\tparent = vparent->frame[frame];\n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tif (parent->stack[i].spilled_ptr.live & REG_LIVE_READ)\n\t\t\t\tcontinue;\n\t\t\tif (state->stack[i].spilled_ptr.live & REG_LIVE_READ)\n\t\t\t\tmark_reg_read(env, &state->stack[i].spilled_ptr,\n\t\t\t\t\t      &parent->stack[i].spilled_ptr);\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl;\n\tstruct bpf_verifier_state *cur = env->cur_state, *new;\n\tint i, j, err, states_cnt = 0;\n\n\tsl = env->explored_states[insn_idx];\n\tif (!sl)\n\t\t/* this 'insn_idx' instruction wasn't marked, so we will not\n\t\t * be doing state search here\n\t\t */\n\t\treturn 0;\n\n\tclean_live_states(env, insn_idx, cur);\n\n\twhile (sl != STATE_LIST_MARK) {\n\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\t/* reached equivalent register/stack state,\n\t\t\t * prune the search.\n\t\t\t * Registers read by the continuation are read by us.\n\t\t\t * If we have any write marks in env->cur_state, they\n\t\t\t * will prevent corresponding reads in the continuation\n\t\t\t * from reaching our parent (an explored_state).  Our\n\t\t\t * own state will get the read marks recorded, but\n\t\t\t * they'll be immediately forgotten as we're pruning\n\t\t\t * this state and will pop a new one.\n\t\t\t */\n\t\t\terr = propagate_liveness(env, &sl->state, cur);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn 1;\n\t\t}\n\t\tsl = sl->next;\n\t\tstates_cnt++;\n\t}\n\n\tif (!env->allow_ptr_leaks && states_cnt > BPF_COMPLEXITY_LIMIT_STATES)\n\t\treturn 0;\n\n\t/* there were no equivalent states, remember current one.\n\t * technically the current state is not proven to be safe yet,\n\t * but it will either reach outer most bpf_exit (which means it's safe)\n\t * or it will be rejected. Since there are no loops, we won't be\n\t * seeing this tuple (frame[0].callsite, frame[1].callsite, .. insn_idx)\n\t * again on the way to bpf_exit\n\t */\n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\n\t/* add new state to the head of linked list */\n\tnew = &new_sl->state;\n\terr = copy_verifier_state(new, cur);\n\tif (err) {\n\t\tfree_verifier_state(new, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew_sl->next = env->explored_states[insn_idx];\n\tenv->explored_states[insn_idx] = new_sl;\n\t/* connect new state to parentage chain. Current frame needs all\n\t * registers connected. Only r6 - r9 of the callers are alive (pushed\n\t * to the stack implicitly by JITs) so in callers' frames connect just\n\t * r6 - r9 as an optimization. Callers will have r1 - r5 connected to\n\t * the state of the call instruction (with WRITTEN set), and r0 comes\n\t * from callee with its full parentage chain, anyway.\n\t */\n\tfor (j = 0; j <= cur->curframe; j++)\n\t\tfor (i = j < cur->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].parent = &new->frame[j]->regs[i];\n\t/* clear write marks in current state: the writes we did are not writes\n\t * our child did, so they don't screen off its reads from us.\n\t * (There are no read marks in current state, because reads always mark\n\t * their parent and current state never has children yet.  Only\n\t * explored_states can get read marks.)\n\t */\n\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\tcur->frame[cur->curframe]->regs[i].live = REG_LIVE_NONE;\n\n\t/* all stack frames are accessible from callee, clear them all */\n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tstruct bpf_func_state *frame = cur->frame[j];\n\t\tstruct bpf_func_state *newframe = new->frame[j];\n\n\t\tfor (i = 0; i < frame->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tframe->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\t\t\tframe->stack[i].spilled_ptr.parent =\n\t\t\t\t\t\t&newframe->stack[i].spilled_ptr;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Return true if it's OK to have the same insn return a different type. */\nstatic bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n/* If an instruction was previously used with particular pointer types, then we\n * need to be careful to avoid cases such as the below, where it may be ok\n * for one branch accessing the pointer, but not ok for the other branch:\n *\n * R1 = sock_ptr\n * goto X;\n * ...\n * R1 = some_other_valid_ptr;\n * goto X;\n * ...\n * R2 = *(u32 *)(R1 + 0);\n */\nstatic bool reg_type_mismatch(enum bpf_reg_type src, enum bpf_reg_type prev)\n{\n\treturn src != prev && (!reg_type_mismatch_ok(src) ||\n\t\t\t       !reg_type_mismatch_ok(prev));\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len, i;\n\tint insn_processed = 0;\n\tbool do_print_state = false;\n\n\tenv->prev_linfo = NULL;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\t0 /* subprogno, zero == main subprog */);\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tinsn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level > 1 || (env->log.level && do_print_state)) {\n\t\t\tif (env->log.level > 1)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe]);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[env->insn_idx].seen = true;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn->imm, env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\tenv->prev_insn_idx = env->insn_idx;\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\t/* eBPF calling convetion is such that R0 is used\n\t\t\t\t * to return the value from eBPF program.\n\t\t\t\t * Make sure that it's readable at this time\n\t\t\t\t * of bpf_exit, which means that program wrote\n\t\t\t\t * something into it earlier\n\t\t\t\t */\n\t\t\t\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\t\t\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\terr = pop_stack(env, &env->prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tenv->insn_aux_data[env->insn_idx].seen = true;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\tverbose(env, \"processed %d insns (limit %d), stack depth \",\n\t\tinsn_processed, BPF_COMPLEXITY_LIMIT_INSNS);\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tu32 depth = env->subprog_info[i].stack_depth;\n\n\t\tverbose(env, \"%d\", depth);\n\t\tif (i + 1 < env->subprog_cnt)\n\t\t\tverbose(env, \"+\");\n\t}\n\tverbose(env, \"\\n\");\n\tenv->prog->aux->stack_depth = env->subprog_info[0].stack_depth;\n\treturn 0;\n}\n\nstatic int check_map_prealloc(struct bpf_map *map)\n{\n\treturn (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_HASH_OF_MAPS) ||\n\t\t!(map->map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\t/* Make sure that BPF_PROG_TYPE_PERF_EVENT programs only use\n\t * preallocated hash maps, since doing memory allocation\n\t * in overflow_handler can crash depending on where nmi got\n\t * triggered.\n\t */\n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\tif (!check_map_prealloc(map)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map->inner_map_meta &&\n\t\t    !check_map_prealloc(map->inner_map_meta)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated inner hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif ((bpf_prog_is_dev_bound(prog->aux) || bpf_map_is_dev_bound(map)) &&\n\t    !bpf_offload_prog_map_match(prog, map)) {\n\t\tverbose(env, \"offload device mismatch between prog and map\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool bpf_map_is_cgroup_storage(struct bpf_map *map)\n{\n\treturn (map->map_type == BPF_MAP_TYPE_CGROUP_STORAGE ||\n\t\tmap->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE);\n}\n\n/* look for pseudo eBPF instructions that access map FDs and\n * replace them with actual map pointers\n */\nstatic int replace_map_fd_with_map_ptr(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_STX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM &&\n\t\t      BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn->src_reg == 0)\n\t\t\t\t/* valid generic load 64-bit imm */\n\t\t\t\tgoto next_insn;\n\n\t\t\tif (insn->src_reg != BPF_PSEUDO_MAP_FD) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf = fdget(insn->imm);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn->imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* store map pointer inside BPF_LD_IMM64 instruction */\n\t\t\tinsn[0].imm = (u32) (unsigned long) map;\n\t\t\tinsn[1].imm = ((u64) (unsigned long) map) >> 32;\n\n\t\t\t/* check whether we recorded this map already */\n\t\t\tfor (j = 0; j < env->used_map_cnt; j++)\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t/* hold the map. If the program is rejected by verifier,\n\t\t\t * the map will be released by release_maps() or it\n\t\t\t * will be used by the valid program until it's unloaded\n\t\t\t * and all maps are released in free_used_maps()\n\t\t\t */\n\t\t\tmap = bpf_map_inc(map, false);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tif (bpf_map_is_cgroup_storage(map) &&\n\t\t\t    bpf_cgroup_storage_assign(env->prog, map)) {\n\t\t\t\tverbose(env, \"only one cgroup storage of each type is allowed\\n\");\n\t\t\t\tfdput(f);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Basic sanity check before we invest more work here. */\n\t\tif (!bpf_opcode_in_insntable(insn->code)) {\n\t\t\tverbose(env, \"unknown opcode %02x\\n\", insn->code);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* now all pseudo BPF_LD_IMM64 instructions load valid\n\t * 'struct bpf_map *' into a register instead of user map_fd.\n\t * These pointers will be used later by verifier to validate map access.\n\t */\n\treturn 0;\n}\n\n/* drop refcnt of maps used by the rejected program */\nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\tenum bpf_cgroup_storage_type stype;\n\tint i;\n\n\tfor_each_cgroup_storage_type(stype) {\n\t\tif (!env->prog->aux->cgroup_storage[stype])\n\t\t\tcontinue;\n\t\tbpf_cgroup_storage_release(env->prog,\n\t\t\tenv->prog->aux->cgroup_storage[stype]);\n\t}\n\n\tfor (i = 0; i < env->used_map_cnt; i++)\n\t\tbpf_map_put(env->used_maps[i]);\n}\n\n/* convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */\nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++)\n\t\tif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tinsn->src_reg = 0;\n}\n\n/* single env->prog->insni[off] instruction was replaced with the range\n * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying\n * [0, off) and [off, end) to new locations, so the patched range stays zero\n */\nstatic int adjust_insn_aux_data(struct bpf_verifier_env *env, u32 prog_len,\n\t\t\t\tu32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;\n\tint i;\n\n\tif (cnt == 1)\n\t\treturn 0;\n\tnew_data = vzalloc(array_size(prog_len,\n\t\t\t\t      sizeof(struct bpf_insn_aux_data)));\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++)\n\t\tnew_data[i].seen = true;\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n\treturn 0;\n}\n\nstatic void adjust_subprog_starts(struct bpf_verifier_env *env, u32 off, u32 len)\n{\n\tint i;\n\n\tif (len == 1)\n\t\treturn;\n\t/* NOTE: fake 'exit' subprog should be updated as well. */\n\tfor (i = 0; i <= env->subprog_cnt; i++) {\n\t\tif (env->subprog_info[i].start <= off)\n\t\t\tcontinue;\n\t\tenv->subprog_info[i].start += len - 1;\n\t}\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (!new_prog)\n\t\treturn NULL;\n\tif (adjust_insn_aux_data(env, new_prog->len, off, len))\n\t\treturn NULL;\n\tadjust_subprog_starts(env, off, len);\n\treturn new_prog;\n}\n\n/* The verifier does more data flow analysis than llvm and will not\n * explore branches that are dead at run time. Malicious programs can\n * have dead code too. Therefore replace all dead at-run-time code\n * with 'ja -1'.\n *\n * Just nops are not optimal, e.g. if they would sit at the end of the\n * program and through another bug we would manage to jump there, then\n * we'd execute beyond program memory otherwise. Returning exception\n * code also wouldn't work since we can have subprogs where the dead\n * code could be located.\n */\nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn trap = BPF_JMP_IMM(BPF_JA, 0, 0, -1);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &trap, sizeof(trap));\n\t}\n}\n\n/* convert load instructions that access fields of a context type into a\n * sequence of instructions that access fields of the underlying structure:\n *     struct __sk_buff    -> struct sk_buff\n *     struct bpf_sock_ops -> struct sock\n */\nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tu32 target_size, size_default, off;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\n\tif (ops->gen_prologue || env->seen_direct_write) {\n\t\tif (!ops->gen_prologue) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tbpf_convert_ctx_access_t convert_ctx_access;\n\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (type == BPF_WRITE &&\n\t\t    env->insn_aux_data[i + delta].sanitize_stack_off) {\n\t\t\tstruct bpf_insn patch[] = {\n\t\t\t\t/* Sanitize suspicious stack slot with zero.\n\t\t\t\t * There are no memory dependencies for this store,\n\t\t\t\t * since it's only using frame pointer and immediate\n\t\t\t\t * constant of zero\n\t\t\t\t */\n\t\t\t\tBPF_ST_MEM(BPF_DW, BPF_REG_FP,\n\t\t\t\t\t   env->insn_aux_data[i + delta].sanitize_stack_off,\n\t\t\t\t\t   0),\n\t\t\t\t/* the original STX instruction will immediately\n\t\t\t\t * overwrite the same stack slot with appropriate value\n\t\t\t\t */\n\t\t\t\t*insn,\n\t\t\t};\n\n\t\t\tcnt = ARRAY_SIZE(patch);\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patch, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (env->insn_aux_data[i + delta].ptr_type) {\n\t\tcase PTR_TO_CTX:\n\t\t\tif (!ops->convert_ctx_access)\n\t\t\t\tcontinue;\n\t\t\tconvert_ctx_access = ops->convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_SOCKET:\n\t\t\tconvert_ctx_access = bpf_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcontinue;\n\t\t}\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tsize_default = bpf_ctx_off_adjust_machine(ctx_field_size);\n\t\toff = insn->off;\n\t\tif (is_narrower_load) {\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(size_default - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tu8 shift = (off & (size_default - 1)) * 8;\n\n\t\t\tif (ctx_field_size <= 4) {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t} else {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t}\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\nstatic int jit_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog, **func, *tmp;\n\tint i, j, subprog_start, subprog_end = 0, len, subprog;\n\tstruct bpf_insn *insn;\n\tvoid *old_bpf_func;\n\tint err;\n\n\tif (env->subprog_cnt <= 1)\n\t\treturn 0;\n\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* Upon error here we cannot fall back to interpreter but\n\t\t * need a hard reject of the program. Thus -EFAULT is\n\t\t * propagated in any case.\n\t\t */\n\t\tsubprog = find_subprog(env, i + insn->imm + 1);\n\t\tif (subprog < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i + insn->imm + 1);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* temporarily remember subprog id inside insn instead of\n\t\t * aux_data, since next loop will split up all insns into funcs\n\t\t */\n\t\tinsn->off = subprog;\n\t\t/* remember original imm in case JIT fails and fallback\n\t\t * to interpreter will be needed\n\t\t */\n\t\tenv->insn_aux_data[i].call_imm = insn->imm;\n\t\t/* point imm to __bpf_call_base+1 from JITs point of view */\n\t\tinsn->imm = 1;\n\t}\n\n\terr = bpf_prog_alloc_jited_linfo(prog);\n\tif (err)\n\t\tgoto out_undo_insn;\n\n\terr = -ENOMEM;\n\tfunc = kcalloc(env->subprog_cnt, sizeof(prog), GFP_KERNEL);\n\tif (!func)\n\t\tgoto out_undo_insn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tsubprog_start = subprog_end;\n\t\tsubprog_end = env->subprog_info[i + 1].start;\n\n\t\tlen = subprog_end - subprog_start;\n\t\tfunc[i] = bpf_prog_alloc(bpf_prog_size(len), GFP_USER);\n\t\tif (!func[i])\n\t\t\tgoto out_free;\n\t\tmemcpy(func[i]->insnsi, &prog->insnsi[subprog_start],\n\t\t       len * sizeof(struct bpf_insn));\n\t\tfunc[i]->type = prog->type;\n\t\tfunc[i]->len = len;\n\t\tif (bpf_prog_calc_tag(func[i]))\n\t\t\tgoto out_free;\n\t\tfunc[i]->is_func = 1;\n\t\tfunc[i]->aux->func_idx = i;\n\t\t/* the btf and func_info will be freed only at prog->aux */\n\t\tfunc[i]->aux->btf = prog->aux->btf;\n\t\tfunc[i]->aux->func_info = prog->aux->func_info;\n\n\t\t/* Use bpf_prog_F_tag to indicate functions in stack traces.\n\t\t * Long term would need debug info to populate names\n\t\t */\n\t\tfunc[i]->aux->name[0] = 'F';\n\t\tfunc[i]->aux->stack_depth = env->subprog_info[i].stack_depth;\n\t\tfunc[i]->jit_requested = 1;\n\t\tfunc[i]->aux->linfo = prog->aux->linfo;\n\t\tfunc[i]->aux->nr_linfo = prog->aux->nr_linfo;\n\t\tfunc[i]->aux->jited_linfo = prog->aux->jited_linfo;\n\t\tfunc[i]->aux->linfo_idx = env->subprog_info[i].linfo_idx;\n\t\tfunc[i] = bpf_int_jit_compile(func[i]);\n\t\tif (!func[i]->jited) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\t/* at this point all bpf functions were successfully JITed\n\t * now populate all bpf_calls with correct addresses and\n\t * run last pass of JIT\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\t\tcontinue;\n\t\t\tsubprog = insn->off;\n\t\t\tinsn->imm = (u64 (*)(u64, u64, u64, u64, u64))\n\t\t\t\tfunc[subprog]->bpf_func -\n\t\t\t\t__bpf_call_base;\n\t\t}\n\n\t\t/* we use the aux data to keep a list of the start addresses\n\t\t * of the JITed images for each function in the program\n\t\t *\n\t\t * for some architectures, such as powerpc64, the imm field\n\t\t * might not be large enough to hold the offset of the start\n\t\t * address of the callee's JITed image from __bpf_call_base\n\t\t *\n\t\t * in such cases, we can lookup the start address of a callee\n\t\t * by using its subprog id, available from the off field of\n\t\t * the call instruction, as an index for this list\n\t\t */\n\t\tfunc[i]->aux->func = func;\n\t\tfunc[i]->aux->func_cnt = env->subprog_cnt;\n\t}\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\told_bpf_func = func[i]->bpf_func;\n\t\ttmp = bpf_int_jit_compile(func[i]);\n\t\tif (tmp != func[i] || func[i]->bpf_func != old_bpf_func) {\n\t\t\tverbose(env, \"JIT doesn't support bpf-to-bpf calls\\n\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t/* finally lock prog and jit images for all functions and\n\t * populate kallsysm\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tbpf_prog_lock_ro(func[i]);\n\t\tbpf_prog_kallsyms_add(func[i]);\n\t}\n\n\t/* Last step: make now unused interpreter insns from main\n\t * prog consistent for later dump requests, so they can\n\t * later look the same as if they were interpreted only.\n\t */\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = env->insn_aux_data[i].call_imm;\n\t\tsubprog = find_subprog(env, i + insn->off + 1);\n\t\tinsn->imm = subprog;\n\t}\n\n\tprog->jited = 1;\n\tprog->bpf_func = func[0]->bpf_func;\n\tprog->aux->func = func;\n\tprog->aux->func_cnt = env->subprog_cnt;\n\tbpf_prog_free_unused_jited_linfo(prog);\n\treturn 0;\nout_free:\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (func[i])\n\t\t\tbpf_jit_free(func[i]);\n\tkfree(func);\nout_undo_insn:\n\t/* cleanup main prog to be interpreted */\n\tprog->jit_requested = 0;\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = 0;\n\t\tinsn->imm = env->insn_aux_data[i].call_imm;\n\t}\n\tbpf_prog_free_jited_linfo(prog);\n\treturn err;\n}\n\nstatic int fixup_call_args(struct bpf_verifier_env *env)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tint i, depth;\n#endif\n\tint err = 0;\n\n\tif (env->prog->jit_requested &&\n\t    !bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\terr = jit_subprogs(env);\n\t\tif (err == 0)\n\t\t\treturn 0;\n\t\tif (err == -EFAULT)\n\t\t\treturn err;\n\t}\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tfor (i = 0; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tdepth = get_callee_stack_depth(env, insn, i);\n\t\tif (depth < 0)\n\t\t\treturn depth;\n\t\tbpf_patch_call_args(insn, depth);\n\t}\n\terr = 0;\n#endif\n\treturn err;\n}\n\n/* fixup insn->imm field of bpf_call instructions\n * and inline eligible helpers as explicit sequence of BPF instructions\n *\n * this function is called after eBPF program passed verification\n */\nstatic int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn insn_buf[16];\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit - 1);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\tif (issrc) {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX,\n\t\t\t\t\t\t\t off_reg);\n\t\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\t} else {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, off_reg,\n\t\t\t\t\t\t\t BPF_REG_AX);\n\t\t\t}\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tenv->prog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tenv->prog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tsl = env->explored_states[i];\n\n\t\tif (sl)\n\t\t\twhile (sl != STATE_LIST_MARK) {\n\t\t\t\tsln = sl->next;\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tsl = sln;\n\t\t\t}\n\t}\n\n\tkfree(env->explored_states);\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr,\n\t      union bpf_attr __user *uattr)\n{\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifier_log *log;\n\tint ret = -EINVAL;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tenv->insn_aux_data =\n\t\tvzalloc(array_size(sizeof(struct bpf_insn_aux_data),\n\t\t\t\t   (*prog)->len));\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 8 ||\n\t\t    !log->level || !log->ubuf)\n\t\t\tgoto err_unlock;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\tif (attr->prog_flags & BPF_F_ANY_ALIGNMENT)\n\t\tenv->strict_alignment = false;\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\tret = bpf_prog_offload_verifier_prep(env->prog);\n\t\tif (ret)\n\t\t\tgoto skip_full_check;\n\t}\n\n\tenv->explored_states = kcalloc(env->prog->len,\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tenv->allow_ptr_leaks = capable(CAP_SYS_ADMIN);\n\n\tret = check_subprogs(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_btf_info(env, attr, uattr);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = do_check(env);\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\n\tif (ret == 0 && bpf_prog_is_dev_bound(env->prog->aux))\n\t\tret = bpf_prog_offload_finalize(env);\n\nskip_full_check:\n\twhile (!pop_stack(env, NULL, NULL));\n\tfree_states(env);\n\n\tif (ret == 0)\n\t\tret = check_max_stack_depth(env);\n\n\t/* instruction rewrites happen after this point */\n\tif (ret == 0)\n\t\tsanitize_dead_code(env);\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\tif (ret == 0)\n\t\tret = fixup_call_args(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\n\tif (ret == 0)\n\t\tadjust_btf_func(env);\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_used_maps() will release them.\n\t\t */\n\t\trelease_maps(env);\n\t*prog = env->prog;\nerr_unlock:\n\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n"], "filenames": ["include/linux/bpf_verifier.h", "kernel/bpf/verifier.c"], "buggy_code_start_loc": [150, 712], "buggy_code_end_loc": [178, 6741], "fixing_code_start_loc": [151, 713], "fixing_code_end_loc": [189, 6915], "type": "CWE-189", "message": "kernel/bpf/verifier.c in the Linux kernel before 4.20.6 performs undesirable out-of-bounds speculation on pointer arithmetic in various cases, including cases of different branches with different state or limits to sanitize, leading to side-channel attacks.", "other": {"cve": {"id": "CVE-2019-7308", "sourceIdentifier": "cve@mitre.org", "published": "2019-02-01T22:29:00.283", "lastModified": "2019-09-24T20:15:12.153", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "kernel/bpf/verifier.c in the Linux kernel before 4.20.6 performs undesirable out-of-bounds speculation on pointer arithmetic in various cases, including cases of different branches with different state or limits to sanitize, leading to side-channel attacks."}, {"lang": "es", "value": "En el kernel de Linux, en versiones anteriores a la 4.20.6, \"kernel/bpf/verifier.c\" realiza especulaciones fuera de l\u00edmites no deseables en la aritm\u00e9tica de punteros en varias ocasiones, incluyendo casos de diferentes ramas con distintos estados o l\u00edmites que hay que sanear, conduciendo a ataques de canal lateral."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:C/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.6, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.1, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.19.19", "matchCriteriaId": "628049F8-5F0F-499C-BDD3-30487768D043"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20.0", "versionEndExcluding": "4.20.6", "matchCriteriaId": "53904B23-3E55-4CEA-849C-3238A624E9D1"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.10:*:*:*:*:*:*:*", "matchCriteriaId": "07C312A0-CD2C-4B9C-B064-6409B25C278F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.0:*:*:*:*:*:*:*", "matchCriteriaId": "F1E78106-58E6-4D59-990F-75DA575BFAD9"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=979d63d50c0c0f7bc537bf821e056cc9fe5abd38", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d3bd7413e0ca40b60cf60d4003246d067cafdeda", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-04/msg00052.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/106827", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1711", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.20.6", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/979d63d50c0c0f7bc537bf821e056cc9fe5abd38", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/d3bd7413e0ca40b60cf60d4003246d067cafdeda", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://support.f5.com/csp/article/K43030517", "source": "cve@mitre.org"}, {"url": "https://support.f5.com/csp/article/K43030517?utm_source=f5support&amp;utm_medium=RSS", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3930-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3930-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3931-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3931-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/979d63d50c0c0f7bc537bf821e056cc9fe5abd38"}}