{"buggy_code": ["/*\n * linux/fs/lockd/clntproc.c\n *\n * RPC procedures for the client side NLM implementation\n *\n * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/nfs_fs.h>\n#include <linux/utsname.h>\n#include <linux/freezer.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/svc.h>\n#include <linux/lockd/lockd.h>\n\n#define NLMDBG_FACILITY\t\tNLMDBG_CLIENT\n#define NLMCLNT_GRACE_WAIT\t(5*HZ)\n#define NLMCLNT_POLL_TIMEOUT\t(30*HZ)\n#define NLMCLNT_MAX_RETRIES\t3\n\nstatic int\tnlmclnt_test(struct nlm_rqst *, struct file_lock *);\nstatic int\tnlmclnt_lock(struct nlm_rqst *, struct file_lock *);\nstatic int\tnlmclnt_unlock(struct nlm_rqst *, struct file_lock *);\nstatic int\tnlm_stat_to_errno(__be32 stat);\nstatic void\tnlmclnt_locks_init_private(struct file_lock *fl, struct nlm_host *host);\nstatic int\tnlmclnt_cancel(struct nlm_host *, int , struct file_lock *);\n\nstatic const struct rpc_call_ops nlmclnt_unlock_ops;\nstatic const struct rpc_call_ops nlmclnt_cancel_ops;\n\n/*\n * Cookie counter for NLM requests\n */\nstatic atomic_t\tnlm_cookie = ATOMIC_INIT(0x1234);\n\nvoid nlmclnt_next_cookie(struct nlm_cookie *c)\n{\n\tu32\tcookie = atomic_inc_return(&nlm_cookie);\n\n\tmemcpy(c->data, &cookie, 4);\n\tc->len=4;\n}\n\nstatic struct nlm_lockowner *nlm_get_lockowner(struct nlm_lockowner *lockowner)\n{\n\tatomic_inc(&lockowner->count);\n\treturn lockowner;\n}\n\nstatic void nlm_put_lockowner(struct nlm_lockowner *lockowner)\n{\n\tif (!atomic_dec_and_lock(&lockowner->count, &lockowner->host->h_lock))\n\t\treturn;\n\tlist_del(&lockowner->list);\n\tspin_unlock(&lockowner->host->h_lock);\n\tnlmclnt_release_host(lockowner->host);\n\tkfree(lockowner);\n}\n\nstatic inline int nlm_pidbusy(struct nlm_host *host, uint32_t pid)\n{\n\tstruct nlm_lockowner *lockowner;\n\tlist_for_each_entry(lockowner, &host->h_lockowners, list) {\n\t\tif (lockowner->pid == pid)\n\t\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\n\nstatic inline uint32_t __nlm_alloc_pid(struct nlm_host *host)\n{\n\tuint32_t res;\n\tdo {\n\t\tres = host->h_pidcount++;\n\t} while (nlm_pidbusy(host, res) < 0);\n\treturn res;\n}\n\nstatic struct nlm_lockowner *__nlm_find_lockowner(struct nlm_host *host, fl_owner_t owner)\n{\n\tstruct nlm_lockowner *lockowner;\n\tlist_for_each_entry(lockowner, &host->h_lockowners, list) {\n\t\tif (lockowner->owner != owner)\n\t\t\tcontinue;\n\t\treturn nlm_get_lockowner(lockowner);\n\t}\n\treturn NULL;\n}\n\nstatic struct nlm_lockowner *nlm_find_lockowner(struct nlm_host *host, fl_owner_t owner)\n{\n\tstruct nlm_lockowner *res, *new = NULL;\n\n\tspin_lock(&host->h_lock);\n\tres = __nlm_find_lockowner(host, owner);\n\tif (res == NULL) {\n\t\tspin_unlock(&host->h_lock);\n\t\tnew = kmalloc(sizeof(*new), GFP_KERNEL);\n\t\tspin_lock(&host->h_lock);\n\t\tres = __nlm_find_lockowner(host, owner);\n\t\tif (res == NULL && new != NULL) {\n\t\t\tres = new;\n\t\t\tatomic_set(&new->count, 1);\n\t\t\tnew->owner = owner;\n\t\t\tnew->pid = __nlm_alloc_pid(host);\n\t\t\tnew->host = nlm_get_host(host);\n\t\t\tlist_add(&new->list, &host->h_lockowners);\n\t\t\tnew = NULL;\n\t\t}\n\t}\n\tspin_unlock(&host->h_lock);\n\tkfree(new);\n\treturn res;\n}\n\n/*\n * Initialize arguments for TEST/LOCK/UNLOCK/CANCEL calls\n */\nstatic void nlmclnt_setlockargs(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tstruct nlm_args\t*argp = &req->a_args;\n\tstruct nlm_lock\t*lock = &argp->lock;\n\n\tnlmclnt_next_cookie(&argp->cookie);\n\tmemcpy(&lock->fh, NFS_FH(fl->fl_file->f_path.dentry->d_inode), sizeof(struct nfs_fh));\n\tlock->caller  = utsname()->nodename;\n\tlock->oh.data = req->a_owner;\n\tlock->oh.len  = snprintf(req->a_owner, sizeof(req->a_owner), \"%u@%s\",\n\t\t\t\t(unsigned int)fl->fl_u.nfs_fl.owner->pid,\n\t\t\t\tutsname()->nodename);\n\tlock->svid = fl->fl_u.nfs_fl.owner->pid;\n\tlock->fl.fl_start = fl->fl_start;\n\tlock->fl.fl_end = fl->fl_end;\n\tlock->fl.fl_type = fl->fl_type;\n}\n\nstatic void nlmclnt_release_lockargs(struct nlm_rqst *req)\n{\n\tBUG_ON(req->a_args.lock.fl.fl_ops != NULL);\n}\n\n/**\n * nlmclnt_proc - Perform a single client-side lock request\n * @host: address of a valid nlm_host context representing the NLM server\n * @cmd: fcntl-style file lock operation to perform\n * @fl: address of arguments for the lock operation\n *\n */\nint nlmclnt_proc(struct nlm_host *host, int cmd, struct file_lock *fl)\n{\n\tstruct nlm_rqst\t\t*call;\n\tint\t\t\tstatus;\n\n\tnlm_get_host(host);\n\tcall = nlm_alloc_call(host);\n\tif (call == NULL)\n\t\treturn -ENOMEM;\n\n\tnlmclnt_locks_init_private(fl, host);\n\t/* Set up the argument struct */\n\tnlmclnt_setlockargs(call, fl);\n\n\tif (IS_SETLK(cmd) || IS_SETLKW(cmd)) {\n\t\tif (fl->fl_type != F_UNLCK) {\n\t\t\tcall->a_args.block = IS_SETLKW(cmd) ? 1 : 0;\n\t\t\tstatus = nlmclnt_lock(call, fl);\n\t\t} else\n\t\t\tstatus = nlmclnt_unlock(call, fl);\n\t} else if (IS_GETLK(cmd))\n\t\tstatus = nlmclnt_test(call, fl);\n\telse\n\t\tstatus = -EINVAL;\n\tfl->fl_ops->fl_release_private(fl);\n\tfl->fl_ops = NULL;\n\n\tdprintk(\"lockd: clnt proc returns %d\\n\", status);\n\treturn status;\n}\nEXPORT_SYMBOL_GPL(nlmclnt_proc);\n\n/*\n * Allocate an NLM RPC call struct\n *\n * Note: the caller must hold a reference to host. In case of failure,\n * this reference will be released.\n */\nstruct nlm_rqst *nlm_alloc_call(struct nlm_host *host)\n{\n\tstruct nlm_rqst\t*call;\n\n\tfor(;;) {\n\t\tcall = kzalloc(sizeof(*call), GFP_KERNEL);\n\t\tif (call != NULL) {\n\t\t\tatomic_set(&call->a_count, 1);\n\t\t\tlocks_init_lock(&call->a_args.lock.fl);\n\t\t\tlocks_init_lock(&call->a_res.lock.fl);\n\t\t\tcall->a_host = host;\n\t\t\treturn call;\n\t\t}\n\t\tif (signalled())\n\t\t\tbreak;\n\t\tprintk(\"nlm_alloc_call: failed, waiting for memory\\n\");\n\t\tschedule_timeout_interruptible(5*HZ);\n\t}\n\tnlmclnt_release_host(host);\n\treturn NULL;\n}\n\nvoid nlmclnt_release_call(struct nlm_rqst *call)\n{\n\tif (!atomic_dec_and_test(&call->a_count))\n\t\treturn;\n\tnlmclnt_release_host(call->a_host);\n\tnlmclnt_release_lockargs(call);\n\tkfree(call);\n}\n\nstatic void nlmclnt_rpc_release(void *data)\n{\n\tnlmclnt_release_call(data);\n}\n\nstatic int nlm_wait_on_grace(wait_queue_head_t *queue)\n{\n\tDEFINE_WAIT(wait);\n\tint status = -EINTR;\n\n\tprepare_to_wait(queue, &wait, TASK_INTERRUPTIBLE);\n\tif (!signalled ()) {\n\t\tschedule_timeout(NLMCLNT_GRACE_WAIT);\n\t\ttry_to_freeze();\n\t\tif (!signalled ())\n\t\t\tstatus = 0;\n\t}\n\tfinish_wait(queue, &wait);\n\treturn status;\n}\n\n/*\n * Generic NLM call\n */\nstatic int\nnlmclnt_call(struct rpc_cred *cred, struct nlm_rqst *req, u32 proc)\n{\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct rpc_clnt\t*clnt;\n\tstruct nlm_args\t*argp = &req->a_args;\n\tstruct nlm_res\t*resp = &req->a_res;\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= argp,\n\t\t.rpc_resp\t= resp,\n\t\t.rpc_cred\t= cred,\n\t};\n\tint\t\tstatus;\n\n\tdprintk(\"lockd: call procedure %d on %s\\n\",\n\t\t\t(int)proc, host->h_name);\n\n\tdo {\n\t\tif (host->h_reclaiming && !argp->reclaim)\n\t\t\tgoto in_grace_period;\n\n\t\t/* If we have no RPC client yet, create one. */\n\t\tif ((clnt = nlm_bind_host(host)) == NULL)\n\t\t\treturn -ENOLCK;\n\t\tmsg.rpc_proc = &clnt->cl_procinfo[proc];\n\n\t\t/* Perform the RPC call. If an error occurs, try again */\n\t\tif ((status = rpc_call_sync(clnt, &msg, 0)) < 0) {\n\t\t\tdprintk(\"lockd: rpc_call returned error %d\\n\", -status);\n\t\t\tswitch (status) {\n\t\t\tcase -EPROTONOSUPPORT:\n\t\t\t\tstatus = -EINVAL;\n\t\t\t\tbreak;\n\t\t\tcase -ECONNREFUSED:\n\t\t\tcase -ETIMEDOUT:\n\t\t\tcase -ENOTCONN:\n\t\t\t\tnlm_rebind_host(host);\n\t\t\t\tstatus = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\tcase -ERESTARTSYS:\n\t\t\t\treturn signalled () ? -EINTR : status;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\t} else\n\t\tif (resp->status == nlm_lck_denied_grace_period) {\n\t\t\tdprintk(\"lockd: server in grace period\\n\");\n\t\t\tif (argp->reclaim) {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t     \"lockd: spurious grace period reject?!\\n\");\n\t\t\t\treturn -ENOLCK;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!argp->reclaim) {\n\t\t\t\t/* We appear to be out of the grace period */\n\t\t\t\twake_up_all(&host->h_gracewait);\n\t\t\t}\n\t\t\tdprintk(\"lockd: server returns status %d\\n\", resp->status);\n\t\t\treturn 0;\t/* Okay, call complete */\n\t\t}\n\nin_grace_period:\n\t\t/*\n\t\t * The server has rebooted and appears to be in the grace\n\t\t * period during which locks are only allowed to be\n\t\t * reclaimed.\n\t\t * We can only back off and try again later.\n\t\t */\n\t\tstatus = nlm_wait_on_grace(&host->h_gracewait);\n\t} while (status == 0);\n\n\treturn status;\n}\n\n/*\n * Generic NLM call, async version.\n */\nstatic struct rpc_task *__nlm_async_call(struct nlm_rqst *req, u32 proc, struct rpc_message *msg, const struct rpc_call_ops *tk_ops)\n{\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct rpc_clnt\t*clnt;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_message = msg,\n\t\t.callback_ops = tk_ops,\n\t\t.callback_data = req,\n\t\t.flags = RPC_TASK_ASYNC,\n\t};\n\n\tdprintk(\"lockd: call procedure %d on %s (async)\\n\",\n\t\t\t(int)proc, host->h_name);\n\n\t/* If we have no RPC client yet, create one. */\n\tclnt = nlm_bind_host(host);\n\tif (clnt == NULL)\n\t\tgoto out_err;\n\tmsg->rpc_proc = &clnt->cl_procinfo[proc];\n\ttask_setup_data.rpc_client = clnt;\n\n        /* bootstrap and kick off the async RPC call */\n\treturn rpc_run_task(&task_setup_data);\nout_err:\n\ttk_ops->rpc_release(req);\n\treturn ERR_PTR(-ENOLCK);\n}\n\nstatic int nlm_do_async_call(struct nlm_rqst *req, u32 proc, struct rpc_message *msg, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_task *task;\n\n\ttask = __nlm_async_call(req, proc, msg, tk_ops);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\trpc_put_task(task);\n\treturn 0;\n}\n\n/*\n * NLM asynchronous call.\n */\nint nlm_async_call(struct nlm_rqst *req, u32 proc, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= &req->a_args,\n\t\t.rpc_resp\t= &req->a_res,\n\t};\n\treturn nlm_do_async_call(req, proc, &msg, tk_ops);\n}\n\nint nlm_async_reply(struct nlm_rqst *req, u32 proc, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= &req->a_res,\n\t};\n\treturn nlm_do_async_call(req, proc, &msg, tk_ops);\n}\n\n/*\n * NLM client asynchronous call.\n *\n * Note that although the calls are asynchronous, and are therefore\n *      guaranteed to complete, we still always attempt to wait for\n *      completion in order to be able to correctly track the lock\n *      state.\n */\nstatic int nlmclnt_async_call(struct rpc_cred *cred, struct nlm_rqst *req, u32 proc, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= &req->a_args,\n\t\t.rpc_resp\t= &req->a_res,\n\t\t.rpc_cred\t= cred,\n\t};\n\tstruct rpc_task *task;\n\tint err;\n\n\ttask = __nlm_async_call(req, proc, &msg, tk_ops);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\terr = rpc_wait_for_completion_task(task);\n\trpc_put_task(task);\n\treturn err;\n}\n\n/*\n * TEST for the presence of a conflicting lock\n */\nstatic int\nnlmclnt_test(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tint\tstatus;\n\n\tstatus = nlmclnt_call(nfs_file_cred(fl->fl_file), req, NLMPROC_TEST);\n\tif (status < 0)\n\t\tgoto out;\n\n\tswitch (req->a_res.status) {\n\t\tcase nlm_granted:\n\t\t\tfl->fl_type = F_UNLCK;\n\t\t\tbreak;\n\t\tcase nlm_lck_denied:\n\t\t\t/*\n\t\t\t * Report the conflicting lock back to the application.\n\t\t\t */\n\t\t\tfl->fl_start = req->a_res.lock.fl.fl_start;\n\t\t\tfl->fl_end = req->a_res.lock.fl.fl_end;\n\t\t\tfl->fl_type = req->a_res.lock.fl.fl_type;\n\t\t\tfl->fl_pid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = nlm_stat_to_errno(req->a_res.status);\n\t}\nout:\n\tnlmclnt_release_call(req);\n\treturn status;\n}\n\nstatic void nlmclnt_locks_copy_lock(struct file_lock *new, struct file_lock *fl)\n{\n\tspin_lock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n\tnew->fl_u.nfs_fl.state = fl->fl_u.nfs_fl.state;\n\tnew->fl_u.nfs_fl.owner = nlm_get_lockowner(fl->fl_u.nfs_fl.owner);\n\tlist_add_tail(&new->fl_u.nfs_fl.list, &fl->fl_u.nfs_fl.owner->host->h_granted);\n\tspin_unlock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n}\n\nstatic void nlmclnt_locks_release_private(struct file_lock *fl)\n{\n\tspin_lock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n\tlist_del(&fl->fl_u.nfs_fl.list);\n\tspin_unlock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n\tnlm_put_lockowner(fl->fl_u.nfs_fl.owner);\n}\n\nstatic const struct file_lock_operations nlmclnt_lock_ops = {\n\t.fl_copy_lock = nlmclnt_locks_copy_lock,\n\t.fl_release_private = nlmclnt_locks_release_private,\n};\n\nstatic void nlmclnt_locks_init_private(struct file_lock *fl, struct nlm_host *host)\n{\n\tBUG_ON(fl->fl_ops != NULL);\n\tfl->fl_u.nfs_fl.state = 0;\n\tfl->fl_u.nfs_fl.owner = nlm_find_lockowner(host, fl->fl_owner);\n\tINIT_LIST_HEAD(&fl->fl_u.nfs_fl.list);\n\tfl->fl_ops = &nlmclnt_lock_ops;\n}\n\nstatic int do_vfs_lock(struct file_lock *fl)\n{\n\tint res = 0;\n\tswitch (fl->fl_flags & (FL_POSIX|FL_FLOCK)) {\n\t\tcase FL_POSIX:\n\t\t\tres = posix_lock_file_wait(fl->fl_file, fl);\n\t\t\tbreak;\n\t\tcase FL_FLOCK:\n\t\t\tres = flock_lock_file_wait(fl->fl_file, fl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t}\n\treturn res;\n}\n\n/*\n * LOCK: Try to create a lock\n *\n *\t\t\tProgrammer Harassment Alert\n *\n * When given a blocking lock request in a sync RPC call, the HPUX lockd\n * will faithfully return LCK_BLOCKED but never cares to notify us when\n * the lock could be granted. This way, our local process could hang\n * around forever waiting for the callback.\n *\n *  Solution A:\tImplement busy-waiting\n *  Solution B: Use the async version of the call (NLM_LOCK_{MSG,RES})\n *\n * For now I am implementing solution A, because I hate the idea of\n * re-implementing lockd for a third time in two months. The async\n * calls shouldn't be too hard to do, however.\n *\n * This is one of the lovely things about standards in the NFS area:\n * they're so soft and squishy you can't really blame HP for doing this.\n */\nstatic int\nnlmclnt_lock(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tstruct rpc_cred *cred = nfs_file_cred(fl->fl_file);\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct nlm_res\t*resp = &req->a_res;\n\tstruct nlm_wait *block = NULL;\n\tunsigned char fl_flags = fl->fl_flags;\n\tunsigned char fl_type;\n\tint status = -ENOLCK;\n\n\tif (nsm_monitor(host) < 0)\n\t\tgoto out;\n\treq->a_args.state = nsm_local_state;\n\n\tfl->fl_flags |= FL_ACCESS;\n\tstatus = do_vfs_lock(fl);\n\tfl->fl_flags = fl_flags;\n\tif (status < 0)\n\t\tgoto out;\n\n\tblock = nlmclnt_prepare_block(host, fl);\nagain:\n\t/*\n\t * Initialise resp->status to a valid non-zero value,\n\t * since 0 == nlm_lck_granted\n\t */\n\tresp->status = nlm_lck_blocked;\n\tfor(;;) {\n\t\t/* Reboot protection */\n\t\tfl->fl_u.nfs_fl.state = host->h_state;\n\t\tstatus = nlmclnt_call(cred, req, NLMPROC_LOCK);\n\t\tif (status < 0)\n\t\t\tbreak;\n\t\t/* Did a reclaimer thread notify us of a server reboot? */\n\t\tif (resp->status ==  nlm_lck_denied_grace_period)\n\t\t\tcontinue;\n\t\tif (resp->status != nlm_lck_blocked)\n\t\t\tbreak;\n\t\t/* Wait on an NLM blocking lock */\n\t\tstatus = nlmclnt_block(block, req, NLMCLNT_POLL_TIMEOUT);\n\t\tif (status < 0)\n\t\t\tbreak;\n\t\tif (resp->status != nlm_lck_blocked)\n\t\t\tbreak;\n\t}\n\n\t/* if we were interrupted while blocking, then cancel the lock request\n\t * and exit\n\t */\n\tif (resp->status == nlm_lck_blocked) {\n\t\tif (!req->a_args.block)\n\t\t\tgoto out_unlock;\n\t\tif (nlmclnt_cancel(host, req->a_args.block, fl) == 0)\n\t\t\tgoto out_unblock;\n\t}\n\n\tif (resp->status == nlm_granted) {\n\t\tdown_read(&host->h_rwsem);\n\t\t/* Check whether or not the server has rebooted */\n\t\tif (fl->fl_u.nfs_fl.state != host->h_state) {\n\t\t\tup_read(&host->h_rwsem);\n\t\t\tgoto again;\n\t\t}\n\t\t/* Ensure the resulting lock will get added to granted list */\n\t\tfl->fl_flags |= FL_SLEEP;\n\t\tif (do_vfs_lock(fl) < 0)\n\t\t\tprintk(KERN_WARNING \"%s: VFS is out of sync with lock manager!\\n\", __func__);\n\t\tup_read(&host->h_rwsem);\n\t\tfl->fl_flags = fl_flags;\n\t\tstatus = 0;\n\t}\n\tif (status < 0)\n\t\tgoto out_unlock;\n\t/*\n\t * EAGAIN doesn't make sense for sleeping locks, and in some\n\t * cases NLM_LCK_DENIED is returned for a permanent error.  So\n\t * turn it into an ENOLCK.\n\t */\n\tif (resp->status == nlm_lck_denied && (fl_flags & FL_SLEEP))\n\t\tstatus = -ENOLCK;\n\telse\n\t\tstatus = nlm_stat_to_errno(resp->status);\nout_unblock:\n\tnlmclnt_finish_block(block);\nout:\n\tnlmclnt_release_call(req);\n\treturn status;\nout_unlock:\n\t/* Fatal error: ensure that we remove the lock altogether */\n\tdprintk(\"lockd: lock attempt ended in fatal error.\\n\"\n\t\t\"       Attempting to unlock.\\n\");\n\tnlmclnt_finish_block(block);\n\tfl_type = fl->fl_type;\n\tfl->fl_type = F_UNLCK;\n\tdown_read(&host->h_rwsem);\n\tdo_vfs_lock(fl);\n\tup_read(&host->h_rwsem);\n\tfl->fl_type = fl_type;\n\tfl->fl_flags = fl_flags;\n\tnlmclnt_async_call(cred, req, NLMPROC_UNLOCK, &nlmclnt_unlock_ops);\n\treturn status;\n}\n\n/*\n * RECLAIM: Try to reclaim a lock\n */\nint\nnlmclnt_reclaim(struct nlm_host *host, struct file_lock *fl)\n{\n\tstruct nlm_rqst reqst, *req;\n\tint\t\tstatus;\n\n\treq = &reqst;\n\tmemset(req, 0, sizeof(*req));\n\tlocks_init_lock(&req->a_args.lock.fl);\n\tlocks_init_lock(&req->a_res.lock.fl);\n\treq->a_host  = host;\n\treq->a_flags = 0;\n\n\t/* Set up the argument struct */\n\tnlmclnt_setlockargs(req, fl);\n\treq->a_args.reclaim = 1;\n\n\tstatus = nlmclnt_call(nfs_file_cred(fl->fl_file), req, NLMPROC_LOCK);\n\tif (status >= 0 && req->a_res.status == nlm_granted)\n\t\treturn 0;\n\n\tprintk(KERN_WARNING \"lockd: failed to reclaim lock for pid %d \"\n\t\t\t\t\"(errno %d, status %d)\\n\", fl->fl_pid,\n\t\t\t\tstatus, ntohl(req->a_res.status));\n\n\t/*\n\t * FIXME: This is a serious failure. We can\n\t *\n\t *  a.\tIgnore the problem\n\t *  b.\tSend the owning process some signal (Linux doesn't have\n\t *\tSIGLOST, though...)\n\t *  c.\tRetry the operation\n\t *\n\t * Until someone comes up with a simple implementation\n\t * for b or c, I'll choose option a.\n\t */\n\n\treturn -ENOLCK;\n}\n\n/*\n * UNLOCK: remove an existing lock\n */\nstatic int\nnlmclnt_unlock(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct nlm_res\t*resp = &req->a_res;\n\tint status;\n\tunsigned char fl_flags = fl->fl_flags;\n\n\t/*\n\t * Note: the server is supposed to either grant us the unlock\n\t * request, or to deny it with NLM_LCK_DENIED_GRACE_PERIOD. In either\n\t * case, we want to unlock.\n\t */\n\tfl->fl_flags |= FL_EXISTS;\n\tdown_read(&host->h_rwsem);\n\tstatus = do_vfs_lock(fl);\n\tup_read(&host->h_rwsem);\n\tfl->fl_flags = fl_flags;\n\tif (status == -ENOENT) {\n\t\tstatus = 0;\n\t\tgoto out;\n\t}\n\n\tatomic_inc(&req->a_count);\n\tstatus = nlmclnt_async_call(nfs_file_cred(fl->fl_file), req,\n\t\t\tNLMPROC_UNLOCK, &nlmclnt_unlock_ops);\n\tif (status < 0)\n\t\tgoto out;\n\n\tif (resp->status == nlm_granted)\n\t\tgoto out;\n\n\tif (resp->status != nlm_lck_denied_nolocks)\n\t\tprintk(\"lockd: unexpected unlock status: %d\\n\", resp->status);\n\t/* What to do now? I'm out of my depth... */\n\tstatus = -ENOLCK;\nout:\n\tnlmclnt_release_call(req);\n\treturn status;\n}\n\nstatic void nlmclnt_unlock_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t*req = data;\n\tu32 status = ntohl(req->a_res.status);\n\n\tif (RPC_ASSASSINATED(task))\n\t\tgoto die;\n\n\tif (task->tk_status < 0) {\n\t\tdprintk(\"lockd: unlock failed (err = %d)\\n\", -task->tk_status);\n\t\tgoto retry_rebind;\n\t}\n\tif (status == NLM_LCK_DENIED_GRACE_PERIOD) {\n\t\trpc_delay(task, NLMCLNT_GRACE_WAIT);\n\t\tgoto retry_unlock;\n\t}\n\tif (status != NLM_LCK_GRANTED)\n\t\tprintk(KERN_WARNING \"lockd: unexpected unlock status: %d\\n\", status);\ndie:\n\treturn;\n retry_rebind:\n\tnlm_rebind_host(req->a_host);\n retry_unlock:\n\trpc_restart_call(task);\n}\n\nstatic const struct rpc_call_ops nlmclnt_unlock_ops = {\n\t.rpc_call_done = nlmclnt_unlock_callback,\n\t.rpc_release = nlmclnt_rpc_release,\n};\n\n/*\n * Cancel a blocked lock request.\n * We always use an async RPC call for this in order not to hang a\n * process that has been Ctrl-C'ed.\n */\nstatic int nlmclnt_cancel(struct nlm_host *host, int block, struct file_lock *fl)\n{\n\tstruct nlm_rqst\t*req;\n\tint status;\n\n\tdprintk(\"lockd: blocking lock attempt was interrupted by a signal.\\n\"\n\t\t\"       Attempting to cancel lock.\\n\");\n\n\treq = nlm_alloc_call(nlm_get_host(host));\n\tif (!req)\n\t\treturn -ENOMEM;\n\treq->a_flags = RPC_TASK_ASYNC;\n\n\tnlmclnt_setlockargs(req, fl);\n\treq->a_args.block = block;\n\n\tatomic_inc(&req->a_count);\n\tstatus = nlmclnt_async_call(nfs_file_cred(fl->fl_file), req,\n\t\t\tNLMPROC_CANCEL, &nlmclnt_cancel_ops);\n\tif (status == 0 && req->a_res.status == nlm_lck_denied)\n\t\tstatus = -ENOLCK;\n\tnlmclnt_release_call(req);\n\treturn status;\n}\n\nstatic void nlmclnt_cancel_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t*req = data;\n\tu32 status = ntohl(req->a_res.status);\n\n\tif (RPC_ASSASSINATED(task))\n\t\tgoto die;\n\n\tif (task->tk_status < 0) {\n\t\tdprintk(\"lockd: CANCEL call error %d, retrying.\\n\",\n\t\t\t\t\ttask->tk_status);\n\t\tgoto retry_cancel;\n\t}\n\n\tdprintk(\"lockd: cancel status %u (task %u)\\n\",\n\t\t\tstatus, task->tk_pid);\n\n\tswitch (status) {\n\tcase NLM_LCK_GRANTED:\n\tcase NLM_LCK_DENIED_GRACE_PERIOD:\n\tcase NLM_LCK_DENIED:\n\t\t/* Everything's good */\n\t\tbreak;\n\tcase NLM_LCK_DENIED_NOLOCKS:\n\t\tdprintk(\"lockd: CANCEL failed (server has no locks)\\n\");\n\t\tgoto retry_cancel;\n\tdefault:\n\t\tprintk(KERN_NOTICE \"lockd: weird return %d for CANCEL call\\n\",\n\t\t\tstatus);\n\t}\n\ndie:\n\treturn;\n\nretry_cancel:\n\t/* Don't ever retry more than 3 times */\n\tif (req->a_retries++ >= NLMCLNT_MAX_RETRIES)\n\t\tgoto die;\n\tnlm_rebind_host(req->a_host);\n\trpc_restart_call(task);\n\trpc_delay(task, 30 * HZ);\n}\n\nstatic const struct rpc_call_ops nlmclnt_cancel_ops = {\n\t.rpc_call_done = nlmclnt_cancel_callback,\n\t.rpc_release = nlmclnt_rpc_release,\n};\n\n/*\n * Convert an NLM status code to a generic kernel errno\n */\nstatic int\nnlm_stat_to_errno(__be32 status)\n{\n\tswitch(ntohl(status)) {\n\tcase NLM_LCK_GRANTED:\n\t\treturn 0;\n\tcase NLM_LCK_DENIED:\n\t\treturn -EAGAIN;\n\tcase NLM_LCK_DENIED_NOLOCKS:\n\tcase NLM_LCK_DENIED_GRACE_PERIOD:\n\t\treturn -ENOLCK;\n\tcase NLM_LCK_BLOCKED:\n\t\tprintk(KERN_NOTICE \"lockd: unexpected status NLM_BLOCKED\\n\");\n\t\treturn -ENOLCK;\n#ifdef CONFIG_LOCKD_V4\n\tcase NLM_DEADLCK:\n\t\treturn -EDEADLK;\n\tcase NLM_ROFS:\n\t\treturn -EROFS;\n\tcase NLM_STALE_FH:\n\t\treturn -ESTALE;\n\tcase NLM_FBIG:\n\t\treturn -EOVERFLOW;\n\tcase NLM_FAILED:\n\t\treturn -ENOLCK;\n#endif\n\t}\n\tprintk(KERN_NOTICE \"lockd: unexpected server status %d\\n\", status);\n\treturn -ENOLCK;\n}\n", "/*\n * linux/include/linux/sunrpc/sched.h\n *\n * Scheduling primitives for kernel Sun RPC.\n *\n * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>\n */\n\n#ifndef _LINUX_SUNRPC_SCHED_H_\n#define _LINUX_SUNRPC_SCHED_H_\n\n#include <linux/timer.h>\n#include <linux/ktime.h>\n#include <linux/sunrpc/types.h>\n#include <linux/spinlock.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <linux/sunrpc/xdr.h>\n\n/*\n * This is the actual RPC procedure call info.\n */\nstruct rpc_procinfo;\nstruct rpc_message {\n\tstruct rpc_procinfo *\trpc_proc;\t/* Procedure information */\n\tvoid *\t\t\trpc_argp;\t/* Arguments */\n\tvoid *\t\t\trpc_resp;\t/* Result */\n\tstruct rpc_cred *\trpc_cred;\t/* Credentials */\n};\n\nstruct rpc_call_ops;\nstruct rpc_wait_queue;\nstruct rpc_wait {\n\tstruct list_head\tlist;\t\t/* wait queue links */\n\tstruct list_head\tlinks;\t\t/* Links to related tasks */\n\tstruct list_head\ttimer_list;\t/* Timer list */\n\tunsigned long\t\texpires;\n};\n\n/*\n * This is the RPC task struct\n */\nstruct rpc_task {\n\tatomic_t\t\ttk_count;\t/* Reference count */\n\tstruct list_head\ttk_task;\t/* global list of tasks */\n\tstruct rpc_clnt *\ttk_client;\t/* RPC client */\n\tstruct rpc_rqst *\ttk_rqstp;\t/* RPC request */\n\n\t/*\n\t * RPC call state\n\t */\n\tstruct rpc_message\ttk_msg;\t\t/* RPC call info */\n\n\t/*\n\t * callback\tto be executed after waking up\n\t * action\tnext procedure for async tasks\n\t * tk_ops\tcaller callbacks\n\t */\n\tvoid\t\t\t(*tk_callback)(struct rpc_task *);\n\tvoid\t\t\t(*tk_action)(struct rpc_task *);\n\tconst struct rpc_call_ops *tk_ops;\n\tvoid *\t\t\ttk_calldata;\n\n\tunsigned long\t\ttk_timeout;\t/* timeout for rpc_sleep() */\n\tunsigned long\t\ttk_runstate;\t/* Task run status */\n\tstruct workqueue_struct\t*tk_workqueue;\t/* Normally rpciod, but could\n\t\t\t\t\t\t * be any workqueue\n\t\t\t\t\t\t */\n\tstruct rpc_wait_queue \t*tk_waitqueue;\t/* RPC wait queue we're on */\n\tunion {\n\t\tstruct work_struct\ttk_work;\t/* Async task work queue */\n\t\tstruct rpc_wait\t\ttk_wait;\t/* RPC wait */\n\t} u;\n\n\tktime_t\t\t\ttk_start;\t/* RPC task init timestamp */\n\n\tpid_t\t\t\ttk_owner;\t/* Process id for batching tasks */\n\tint\t\t\ttk_status;\t/* result of last operation */\n\tunsigned short\t\ttk_flags;\t/* misc flags */\n\tunsigned short\t\ttk_timeouts;\t/* maj timeouts */\n\n#ifdef RPC_DEBUG\n\tunsigned short\t\ttk_pid;\t\t/* debugging aid */\n#endif\n\tunsigned char\t\ttk_priority : 2,/* Task priority */\n\t\t\t\ttk_garb_retry : 2,\n\t\t\t\ttk_cred_retry : 2;\n};\n#define tk_xprt\t\t\ttk_client->cl_xprt\n\n/* support walking a list of tasks on a wait queue */\n#define\ttask_for_each(task, pos, head) \\\n\tlist_for_each(pos, head) \\\n\t\tif ((task=list_entry(pos, struct rpc_task, u.tk_wait.list)),1)\n\n#define\ttask_for_first(task, head) \\\n\tif (!list_empty(head) &&  \\\n\t    ((task=list_entry((head)->next, struct rpc_task, u.tk_wait.list)),1))\n\ntypedef void\t\t\t(*rpc_action)(struct rpc_task *);\n\nstruct rpc_call_ops {\n\tvoid (*rpc_call_prepare)(struct rpc_task *, void *);\n\tvoid (*rpc_call_done)(struct rpc_task *, void *);\n\tvoid (*rpc_release)(void *);\n};\n\nstruct rpc_task_setup {\n\tstruct rpc_task *task;\n\tstruct rpc_clnt *rpc_client;\n\tconst struct rpc_message *rpc_message;\n\tconst struct rpc_call_ops *callback_ops;\n\tvoid *callback_data;\n\tstruct workqueue_struct *workqueue;\n\tunsigned short flags;\n\tsigned char priority;\n};\n\n/*\n * RPC task flags\n */\n#define RPC_TASK_ASYNC\t\t0x0001\t\t/* is an async task */\n#define RPC_TASK_SWAPPER\t0x0002\t\t/* is swapping in/out */\n#define RPC_CALL_MAJORSEEN\t0x0020\t\t/* major timeout seen */\n#define RPC_TASK_ROOTCREDS\t0x0040\t\t/* force root creds */\n#define RPC_TASK_DYNAMIC\t0x0080\t\t/* task was kmalloc'ed */\n#define RPC_TASK_KILLED\t\t0x0100\t\t/* task was killed */\n#define RPC_TASK_SOFT\t\t0x0200\t\t/* Use soft timeouts */\n#define RPC_TASK_SOFTCONN\t0x0400\t\t/* Fail if can't connect */\n#define RPC_TASK_SENT\t\t0x0800\t\t/* message was sent */\n#define RPC_TASK_TIMEOUT\t0x1000\t\t/* fail with ETIMEDOUT on timeout */\n\n#define RPC_IS_ASYNC(t)\t\t((t)->tk_flags & RPC_TASK_ASYNC)\n#define RPC_IS_SWAPPER(t)\t((t)->tk_flags & RPC_TASK_SWAPPER)\n#define RPC_DO_ROOTOVERRIDE(t)\t((t)->tk_flags & RPC_TASK_ROOTCREDS)\n#define RPC_ASSASSINATED(t)\t((t)->tk_flags & RPC_TASK_KILLED)\n#define RPC_IS_SOFT(t)\t\t((t)->tk_flags & (RPC_TASK_SOFT|RPC_TASK_TIMEOUT))\n#define RPC_IS_SOFTCONN(t)\t((t)->tk_flags & RPC_TASK_SOFTCONN)\n#define RPC_WAS_SENT(t)\t\t((t)->tk_flags & RPC_TASK_SENT)\n\n#define RPC_TASK_RUNNING\t0\n#define RPC_TASK_QUEUED\t\t1\n#define RPC_TASK_ACTIVE\t\t2\n\n#define RPC_IS_RUNNING(t)\ttest_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)\n#define rpc_set_running(t)\tset_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)\n#define rpc_test_and_set_running(t) \\\n\t\t\t\ttest_and_set_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)\n#define rpc_clear_running(t)\t\\\n\tdo { \\\n\t\tsmp_mb__before_clear_bit(); \\\n\t\tclear_bit(RPC_TASK_RUNNING, &(t)->tk_runstate); \\\n\t\tsmp_mb__after_clear_bit(); \\\n\t} while (0)\n\n#define RPC_IS_QUEUED(t)\ttest_bit(RPC_TASK_QUEUED, &(t)->tk_runstate)\n#define rpc_set_queued(t)\tset_bit(RPC_TASK_QUEUED, &(t)->tk_runstate)\n#define rpc_clear_queued(t)\t\\\n\tdo { \\\n\t\tsmp_mb__before_clear_bit(); \\\n\t\tclear_bit(RPC_TASK_QUEUED, &(t)->tk_runstate); \\\n\t\tsmp_mb__after_clear_bit(); \\\n\t} while (0)\n\n#define RPC_IS_ACTIVATED(t)\ttest_bit(RPC_TASK_ACTIVE, &(t)->tk_runstate)\n\n/*\n * Task priorities.\n * Note: if you change these, you must also change\n * the task initialization definitions below.\n */\n#define RPC_PRIORITY_LOW\t(-1)\n#define RPC_PRIORITY_NORMAL\t(0)\n#define RPC_PRIORITY_HIGH\t(1)\n#define RPC_PRIORITY_PRIVILEGED\t(2)\n#define RPC_NR_PRIORITY\t\t(1 + RPC_PRIORITY_PRIVILEGED - RPC_PRIORITY_LOW)\n\nstruct rpc_timer {\n\tstruct timer_list timer;\n\tstruct list_head list;\n\tunsigned long expires;\n};\n\n/*\n * RPC synchronization objects\n */\nstruct rpc_wait_queue {\n\tspinlock_t\t\tlock;\n\tstruct list_head\ttasks[RPC_NR_PRIORITY];\t/* task queue for each priority level */\n\tpid_t\t\t\towner;\t\t\t/* process id of last task serviced */\n\tunsigned char\t\tmaxpriority;\t\t/* maximum priority (0 if queue is not a priority queue) */\n\tunsigned char\t\tpriority;\t\t/* current priority */\n\tunsigned char\t\tcount;\t\t\t/* # task groups remaining serviced so far */\n\tunsigned char\t\tnr;\t\t\t/* # tasks remaining for cookie */\n\tunsigned short\t\tqlen;\t\t\t/* total # tasks waiting in queue */\n\tstruct rpc_timer\ttimer_list;\n#ifdef RPC_DEBUG\n\tconst char *\t\tname;\n#endif\n};\n\n/*\n * This is the # requests to send consecutively\n * from a single cookie.  The aim is to improve\n * performance of NFS operations such as read/write.\n */\n#define RPC_BATCH_COUNT\t\t\t16\n#define RPC_IS_PRIORITY(q)\t\t((q)->maxpriority > 0)\n\n/*\n * Function prototypes\n */\nstruct rpc_task *rpc_new_task(const struct rpc_task_setup *);\nstruct rpc_task *rpc_run_task(const struct rpc_task_setup *);\nstruct rpc_task *rpc_run_bc_task(struct rpc_rqst *req,\n\t\t\t\tconst struct rpc_call_ops *ops);\nvoid\t\trpc_put_task(struct rpc_task *);\nvoid\t\trpc_put_task_async(struct rpc_task *);\nvoid\t\trpc_exit_task(struct rpc_task *);\nvoid\t\trpc_exit(struct rpc_task *, int);\nvoid\t\trpc_release_calldata(const struct rpc_call_ops *, void *);\nvoid\t\trpc_killall_tasks(struct rpc_clnt *);\nvoid\t\trpc_execute(struct rpc_task *);\nvoid\t\trpc_init_priority_wait_queue(struct rpc_wait_queue *, const char *);\nvoid\t\trpc_init_wait_queue(struct rpc_wait_queue *, const char *);\nvoid\t\trpc_destroy_wait_queue(struct rpc_wait_queue *);\nvoid\t\trpc_sleep_on(struct rpc_wait_queue *, struct rpc_task *,\n\t\t\t\t\trpc_action action);\nvoid\t\trpc_wake_up_queued_task(struct rpc_wait_queue *,\n\t\t\t\t\tstruct rpc_task *);\nvoid\t\trpc_wake_up(struct rpc_wait_queue *);\nstruct rpc_task *rpc_wake_up_next(struct rpc_wait_queue *);\nvoid\t\trpc_wake_up_status(struct rpc_wait_queue *, int);\nint\t\trpc_queue_empty(struct rpc_wait_queue *);\nvoid\t\trpc_delay(struct rpc_task *, unsigned long);\nvoid *\t\trpc_malloc(struct rpc_task *, size_t);\nvoid\t\trpc_free(void *);\nint\t\trpciod_up(void);\nvoid\t\trpciod_down(void);\nint\t\t__rpc_wait_for_completion_task(struct rpc_task *task, int (*)(void *));\n#ifdef RPC_DEBUG\nvoid\t\trpc_show_tasks(void);\n#endif\nint\t\trpc_init_mempool(void);\nvoid\t\trpc_destroy_mempool(void);\nextern struct workqueue_struct *rpciod_workqueue;\nvoid\t\trpc_prepare_task(struct rpc_task *task);\n\nstatic inline int rpc_wait_for_completion_task(struct rpc_task *task)\n{\n\treturn __rpc_wait_for_completion_task(task, NULL);\n}\n\nstatic inline void rpc_task_set_priority(struct rpc_task *task, unsigned char prio)\n{\n\ttask->tk_priority = prio - RPC_PRIORITY_LOW;\n}\n\nstatic inline int rpc_task_has_priority(struct rpc_task *task, unsigned char prio)\n{\n\treturn (task->tk_priority + RPC_PRIORITY_LOW == prio);\n}\n\n#ifdef RPC_DEBUG\nstatic inline const char * rpc_qname(struct rpc_wait_queue *q)\n{\n\treturn ((q && q->name) ? q->name : \"unknown\");\n}\n#endif\n\n#endif /* _LINUX_SUNRPC_SCHED_H_ */\n", "/*\n *  linux/net/sunrpc/clnt.c\n *\n *  This file contains the high-level RPC interface.\n *  It is modeled as a finite state machine to support both synchronous\n *  and asynchronous requests.\n *\n *  -\tRPC header generation and argument serialization.\n *  -\tCredential refresh.\n *  -\tTCP connect handling.\n *  -\tRetry of operation when it is suspected the operation failed because\n *\tof uid squashing on the server, or when the credentials were stale\n *\tand need to be refreshed, or when a packet was damaged in transit.\n *\tThis may be have to be moved to the VFS layer.\n *\n *  Copyright (C) 1992,1993 Rick Sladkey <jrs@world.std.com>\n *  Copyright (C) 1995,1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <asm/system.h>\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kallsyms.h>\n#include <linux/mm.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/slab.h>\n#include <linux/utsname.h>\n#include <linux/workqueue.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/un.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/rpc_pipe_fs.h>\n#include <linux/sunrpc/metrics.h>\n#include <linux/sunrpc/bc_xprt.h>\n\n#include \"sunrpc.h\"\n\n#ifdef RPC_DEBUG\n# define RPCDBG_FACILITY\tRPCDBG_CALL\n#endif\n\n#define dprint_status(t)\t\t\t\t\t\\\n\tdprintk(\"RPC: %5u %s (status %d)\\n\", t->tk_pid,\t\t\\\n\t\t\t__func__, t->tk_status)\n\n/*\n * All RPC clients are linked into this list\n */\nstatic LIST_HEAD(all_clients);\nstatic DEFINE_SPINLOCK(rpc_client_lock);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(destroy_wait);\n\n\nstatic void\tcall_start(struct rpc_task *task);\nstatic void\tcall_reserve(struct rpc_task *task);\nstatic void\tcall_reserveresult(struct rpc_task *task);\nstatic void\tcall_allocate(struct rpc_task *task);\nstatic void\tcall_decode(struct rpc_task *task);\nstatic void\tcall_bind(struct rpc_task *task);\nstatic void\tcall_bind_status(struct rpc_task *task);\nstatic void\tcall_transmit(struct rpc_task *task);\n#if defined(CONFIG_NFS_V4_1)\nstatic void\tcall_bc_transmit(struct rpc_task *task);\n#endif /* CONFIG_NFS_V4_1 */\nstatic void\tcall_status(struct rpc_task *task);\nstatic void\tcall_transmit_status(struct rpc_task *task);\nstatic void\tcall_refresh(struct rpc_task *task);\nstatic void\tcall_refreshresult(struct rpc_task *task);\nstatic void\tcall_timeout(struct rpc_task *task);\nstatic void\tcall_connect(struct rpc_task *task);\nstatic void\tcall_connect_status(struct rpc_task *task);\n\nstatic __be32\t*rpc_encode_header(struct rpc_task *task);\nstatic __be32\t*rpc_verify_header(struct rpc_task *task);\nstatic int\trpc_ping(struct rpc_clnt *clnt);\n\nstatic void rpc_register_client(struct rpc_clnt *clnt)\n{\n\tspin_lock(&rpc_client_lock);\n\tlist_add(&clnt->cl_clients, &all_clients);\n\tspin_unlock(&rpc_client_lock);\n}\n\nstatic void rpc_unregister_client(struct rpc_clnt *clnt)\n{\n\tspin_lock(&rpc_client_lock);\n\tlist_del(&clnt->cl_clients);\n\tspin_unlock(&rpc_client_lock);\n}\n\nstatic int\nrpc_setup_pipedir(struct rpc_clnt *clnt, char *dir_name)\n{\n\tstatic uint32_t clntid;\n\tstruct nameidata nd;\n\tstruct path path;\n\tchar name[15];\n\tstruct qstr q = {\n\t\t.name = name,\n\t};\n\tint error;\n\n\tclnt->cl_path.mnt = ERR_PTR(-ENOENT);\n\tclnt->cl_path.dentry = ERR_PTR(-ENOENT);\n\tif (dir_name == NULL)\n\t\treturn 0;\n\n\tpath.mnt = rpc_get_mount();\n\tif (IS_ERR(path.mnt))\n\t\treturn PTR_ERR(path.mnt);\n\terror = vfs_path_lookup(path.mnt->mnt_root, path.mnt, dir_name, 0, &nd);\n\tif (error)\n\t\tgoto err;\n\n\tfor (;;) {\n\t\tq.len = snprintf(name, sizeof(name), \"clnt%x\", (unsigned int)clntid++);\n\t\tname[sizeof(name) - 1] = '\\0';\n\t\tq.hash = full_name_hash(q.name, q.len);\n\t\tpath.dentry = rpc_create_client_dir(nd.path.dentry, &q, clnt);\n\t\tif (!IS_ERR(path.dentry))\n\t\t\tbreak;\n\t\terror = PTR_ERR(path.dentry);\n\t\tif (error != -EEXIST) {\n\t\t\tprintk(KERN_INFO \"RPC: Couldn't create pipefs entry\"\n\t\t\t\t\t\" %s/%s, error %d\\n\",\n\t\t\t\t\tdir_name, name, error);\n\t\t\tgoto err_path_put;\n\t\t}\n\t}\n\tpath_put(&nd.path);\n\tclnt->cl_path = path;\n\treturn 0;\nerr_path_put:\n\tpath_put(&nd.path);\nerr:\n\trpc_put_mount();\n\treturn error;\n}\n\nstatic struct rpc_clnt * rpc_new_client(const struct rpc_create_args *args, struct rpc_xprt *xprt)\n{\n\tstruct rpc_program\t*program = args->program;\n\tstruct rpc_version\t*version;\n\tstruct rpc_clnt\t\t*clnt = NULL;\n\tstruct rpc_auth\t\t*auth;\n\tint err;\n\tsize_t len;\n\n\t/* sanity check the name before trying to print it */\n\terr = -EINVAL;\n\tlen = strlen(args->servername);\n\tif (len > RPC_MAXNETNAMELEN)\n\t\tgoto out_no_rpciod;\n\tlen++;\n\n\tdprintk(\"RPC:       creating %s client for %s (xprt %p)\\n\",\n\t\t\tprogram->name, args->servername, xprt);\n\n\terr = rpciod_up();\n\tif (err)\n\t\tgoto out_no_rpciod;\n\terr = -EINVAL;\n\tif (!xprt)\n\t\tgoto out_no_xprt;\n\n\tif (args->version >= program->nrvers)\n\t\tgoto out_err;\n\tversion = program->version[args->version];\n\tif (version == NULL)\n\t\tgoto out_err;\n\n\terr = -ENOMEM;\n\tclnt = kzalloc(sizeof(*clnt), GFP_KERNEL);\n\tif (!clnt)\n\t\tgoto out_err;\n\tclnt->cl_parent = clnt;\n\n\tclnt->cl_server = clnt->cl_inline_name;\n\tif (len > sizeof(clnt->cl_inline_name)) {\n\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n\t\tif (buf != NULL)\n\t\t\tclnt->cl_server = buf;\n\t\telse\n\t\t\tlen = sizeof(clnt->cl_inline_name);\n\t}\n\tstrlcpy(clnt->cl_server, args->servername, len);\n\n\tclnt->cl_xprt     = xprt;\n\tclnt->cl_procinfo = version->procs;\n\tclnt->cl_maxproc  = version->nrprocs;\n\tclnt->cl_protname = program->name;\n\tclnt->cl_prog     = args->prognumber ? : program->number;\n\tclnt->cl_vers     = version->number;\n\tclnt->cl_stats    = program->stats;\n\tclnt->cl_metrics  = rpc_alloc_iostats(clnt);\n\terr = -ENOMEM;\n\tif (clnt->cl_metrics == NULL)\n\t\tgoto out_no_stats;\n\tclnt->cl_program  = program;\n\tINIT_LIST_HEAD(&clnt->cl_tasks);\n\tspin_lock_init(&clnt->cl_lock);\n\n\tif (!xprt_bound(clnt->cl_xprt))\n\t\tclnt->cl_autobind = 1;\n\n\tclnt->cl_timeout = xprt->timeout;\n\tif (args->timeout != NULL) {\n\t\tmemcpy(&clnt->cl_timeout_default, args->timeout,\n\t\t\t\tsizeof(clnt->cl_timeout_default));\n\t\tclnt->cl_timeout = &clnt->cl_timeout_default;\n\t}\n\n\tclnt->cl_rtt = &clnt->cl_rtt_default;\n\trpc_init_rtt(&clnt->cl_rtt_default, clnt->cl_timeout->to_initval);\n\tclnt->cl_principal = NULL;\n\tif (args->client_name) {\n\t\tclnt->cl_principal = kstrdup(args->client_name, GFP_KERNEL);\n\t\tif (!clnt->cl_principal)\n\t\t\tgoto out_no_principal;\n\t}\n\n\tatomic_set(&clnt->cl_count, 1);\n\n\terr = rpc_setup_pipedir(clnt, program->pipe_dir_name);\n\tif (err < 0)\n\t\tgoto out_no_path;\n\n\tauth = rpcauth_create(args->authflavor, clnt);\n\tif (IS_ERR(auth)) {\n\t\tprintk(KERN_INFO \"RPC: Couldn't create auth handle (flavor %u)\\n\",\n\t\t\t\targs->authflavor);\n\t\terr = PTR_ERR(auth);\n\t\tgoto out_no_auth;\n\t}\n\n\t/* save the nodename */\n\tclnt->cl_nodelen = strlen(init_utsname()->nodename);\n\tif (clnt->cl_nodelen > UNX_MAXNODENAME)\n\t\tclnt->cl_nodelen = UNX_MAXNODENAME;\n\tmemcpy(clnt->cl_nodename, init_utsname()->nodename, clnt->cl_nodelen);\n\trpc_register_client(clnt);\n\treturn clnt;\n\nout_no_auth:\n\tif (!IS_ERR(clnt->cl_path.dentry)) {\n\t\trpc_remove_client_dir(clnt->cl_path.dentry);\n\t\trpc_put_mount();\n\t}\nout_no_path:\n\tkfree(clnt->cl_principal);\nout_no_principal:\n\trpc_free_iostats(clnt->cl_metrics);\nout_no_stats:\n\tif (clnt->cl_server != clnt->cl_inline_name)\n\t\tkfree(clnt->cl_server);\n\tkfree(clnt);\nout_err:\n\txprt_put(xprt);\nout_no_xprt:\n\trpciod_down();\nout_no_rpciod:\n\treturn ERR_PTR(err);\n}\n\n/*\n * rpc_create - create an RPC client and transport with one call\n * @args: rpc_clnt create argument structure\n *\n * Creates and initializes an RPC transport and an RPC client.\n *\n * It can ping the server in order to determine if it is up, and to see if\n * it supports this program and version.  RPC_CLNT_CREATE_NOPING disables\n * this behavior so asynchronous tasks can also use rpc_create.\n */\nstruct rpc_clnt *rpc_create(struct rpc_create_args *args)\n{\n\tstruct rpc_xprt *xprt;\n\tstruct rpc_clnt *clnt;\n\tstruct xprt_create xprtargs = {\n\t\t.net = args->net,\n\t\t.ident = args->protocol,\n\t\t.srcaddr = args->saddress,\n\t\t.dstaddr = args->address,\n\t\t.addrlen = args->addrsize,\n\t\t.bc_xprt = args->bc_xprt,\n\t};\n\tchar servername[48];\n\n\t/*\n\t * If the caller chooses not to specify a hostname, whip\n\t * up a string representation of the passed-in address.\n\t */\n\tif (args->servername == NULL) {\n\t\tstruct sockaddr_un *sun =\n\t\t\t\t(struct sockaddr_un *)args->address;\n\t\tstruct sockaddr_in *sin =\n\t\t\t\t(struct sockaddr_in *)args->address;\n\t\tstruct sockaddr_in6 *sin6 =\n\t\t\t\t(struct sockaddr_in6 *)args->address;\n\n\t\tservername[0] = '\\0';\n\t\tswitch (args->address->sa_family) {\n\t\tcase AF_LOCAL:\n\t\t\tsnprintf(servername, sizeof(servername), \"%s\",\n\t\t\t\t sun->sun_path);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tsnprintf(servername, sizeof(servername), \"%pI4\",\n\t\t\t\t &sin->sin_addr.s_addr);\n\t\t\tbreak;\n\t\tcase AF_INET6:\n\t\t\tsnprintf(servername, sizeof(servername), \"%pI6\",\n\t\t\t\t &sin6->sin6_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* caller wants default server name, but\n\t\t\t * address family isn't recognized. */\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\targs->servername = servername;\n\t}\n\n\txprt = xprt_create_transport(&xprtargs);\n\tif (IS_ERR(xprt))\n\t\treturn (struct rpc_clnt *)xprt;\n\n\t/*\n\t * By default, kernel RPC client connects from a reserved port.\n\t * CAP_NET_BIND_SERVICE will not be set for unprivileged requesters,\n\t * but it is always enabled for rpciod, which handles the connect\n\t * operation.\n\t */\n\txprt->resvport = 1;\n\tif (args->flags & RPC_CLNT_CREATE_NONPRIVPORT)\n\t\txprt->resvport = 0;\n\n\tclnt = rpc_new_client(args, xprt);\n\tif (IS_ERR(clnt))\n\t\treturn clnt;\n\n\tif (!(args->flags & RPC_CLNT_CREATE_NOPING)) {\n\t\tint err = rpc_ping(clnt);\n\t\tif (err != 0) {\n\t\t\trpc_shutdown_client(clnt);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\tclnt->cl_softrtry = 1;\n\tif (args->flags & RPC_CLNT_CREATE_HARDRTRY)\n\t\tclnt->cl_softrtry = 0;\n\n\tif (args->flags & RPC_CLNT_CREATE_AUTOBIND)\n\t\tclnt->cl_autobind = 1;\n\tif (args->flags & RPC_CLNT_CREATE_DISCRTRY)\n\t\tclnt->cl_discrtry = 1;\n\tif (!(args->flags & RPC_CLNT_CREATE_QUIET))\n\t\tclnt->cl_chatty = 1;\n\n\treturn clnt;\n}\nEXPORT_SYMBOL_GPL(rpc_create);\n\n/*\n * This function clones the RPC client structure. It allows us to share the\n * same transport while varying parameters such as the authentication\n * flavour.\n */\nstruct rpc_clnt *\nrpc_clone_client(struct rpc_clnt *clnt)\n{\n\tstruct rpc_clnt *new;\n\tint err = -ENOMEM;\n\n\tnew = kmemdup(clnt, sizeof(*new), GFP_KERNEL);\n\tif (!new)\n\t\tgoto out_no_clnt;\n\tnew->cl_parent = clnt;\n\t/* Turn off autobind on clones */\n\tnew->cl_autobind = 0;\n\tINIT_LIST_HEAD(&new->cl_tasks);\n\tspin_lock_init(&new->cl_lock);\n\trpc_init_rtt(&new->cl_rtt_default, clnt->cl_timeout->to_initval);\n\tnew->cl_metrics = rpc_alloc_iostats(clnt);\n\tif (new->cl_metrics == NULL)\n\t\tgoto out_no_stats;\n\tif (clnt->cl_principal) {\n\t\tnew->cl_principal = kstrdup(clnt->cl_principal, GFP_KERNEL);\n\t\tif (new->cl_principal == NULL)\n\t\t\tgoto out_no_principal;\n\t}\n\tatomic_set(&new->cl_count, 1);\n\terr = rpc_setup_pipedir(new, clnt->cl_program->pipe_dir_name);\n\tif (err != 0)\n\t\tgoto out_no_path;\n\tif (new->cl_auth)\n\t\tatomic_inc(&new->cl_auth->au_count);\n\txprt_get(clnt->cl_xprt);\n\tatomic_inc(&clnt->cl_count);\n\trpc_register_client(new);\n\trpciod_up();\n\treturn new;\nout_no_path:\n\tkfree(new->cl_principal);\nout_no_principal:\n\trpc_free_iostats(new->cl_metrics);\nout_no_stats:\n\tkfree(new);\nout_no_clnt:\n\tdprintk(\"RPC:       %s: returned error %d\\n\", __func__, err);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(rpc_clone_client);\n\n/*\n * Kill all tasks for the given client.\n * XXX: kill their descendants as well?\n */\nvoid rpc_killall_tasks(struct rpc_clnt *clnt)\n{\n\tstruct rpc_task\t*rovr;\n\n\n\tif (list_empty(&clnt->cl_tasks))\n\t\treturn;\n\tdprintk(\"RPC:       killing all tasks for client %p\\n\", clnt);\n\t/*\n\t * Spin lock all_tasks to prevent changes...\n\t */\n\tspin_lock(&clnt->cl_lock);\n\tlist_for_each_entry(rovr, &clnt->cl_tasks, tk_task) {\n\t\tif (!RPC_IS_ACTIVATED(rovr))\n\t\t\tcontinue;\n\t\tif (!(rovr->tk_flags & RPC_TASK_KILLED)) {\n\t\t\trovr->tk_flags |= RPC_TASK_KILLED;\n\t\t\trpc_exit(rovr, -EIO);\n\t\t\tif (RPC_IS_QUEUED(rovr))\n\t\t\t\trpc_wake_up_queued_task(rovr->tk_waitqueue,\n\t\t\t\t\t\t\trovr);\n\t\t}\n\t}\n\tspin_unlock(&clnt->cl_lock);\n}\nEXPORT_SYMBOL_GPL(rpc_killall_tasks);\n\n/*\n * Properly shut down an RPC client, terminating all outstanding\n * requests.\n */\nvoid rpc_shutdown_client(struct rpc_clnt *clnt)\n{\n\tdprintk(\"RPC:       shutting down %s client for %s\\n\",\n\t\t\tclnt->cl_protname, clnt->cl_server);\n\n\twhile (!list_empty(&clnt->cl_tasks)) {\n\t\trpc_killall_tasks(clnt);\n\t\twait_event_timeout(destroy_wait,\n\t\t\tlist_empty(&clnt->cl_tasks), 1*HZ);\n\t}\n\n\trpc_release_client(clnt);\n}\nEXPORT_SYMBOL_GPL(rpc_shutdown_client);\n\n/*\n * Free an RPC client\n */\nstatic void\nrpc_free_client(struct rpc_clnt *clnt)\n{\n\tdprintk(\"RPC:       destroying %s client for %s\\n\",\n\t\t\tclnt->cl_protname, clnt->cl_server);\n\tif (!IS_ERR(clnt->cl_path.dentry)) {\n\t\trpc_remove_client_dir(clnt->cl_path.dentry);\n\t\trpc_put_mount();\n\t}\n\tif (clnt->cl_parent != clnt) {\n\t\trpc_release_client(clnt->cl_parent);\n\t\tgoto out_free;\n\t}\n\tif (clnt->cl_server != clnt->cl_inline_name)\n\t\tkfree(clnt->cl_server);\nout_free:\n\trpc_unregister_client(clnt);\n\trpc_free_iostats(clnt->cl_metrics);\n\tkfree(clnt->cl_principal);\n\tclnt->cl_metrics = NULL;\n\txprt_put(clnt->cl_xprt);\n\trpciod_down();\n\tkfree(clnt);\n}\n\n/*\n * Free an RPC client\n */\nstatic void\nrpc_free_auth(struct rpc_clnt *clnt)\n{\n\tif (clnt->cl_auth == NULL) {\n\t\trpc_free_client(clnt);\n\t\treturn;\n\t}\n\n\t/*\n\t * Note: RPCSEC_GSS may need to send NULL RPC calls in order to\n\t *       release remaining GSS contexts. This mechanism ensures\n\t *       that it can do so safely.\n\t */\n\tatomic_inc(&clnt->cl_count);\n\trpcauth_release(clnt->cl_auth);\n\tclnt->cl_auth = NULL;\n\tif (atomic_dec_and_test(&clnt->cl_count))\n\t\trpc_free_client(clnt);\n}\n\n/*\n * Release reference to the RPC client\n */\nvoid\nrpc_release_client(struct rpc_clnt *clnt)\n{\n\tdprintk(\"RPC:       rpc_release_client(%p)\\n\", clnt);\n\n\tif (list_empty(&clnt->cl_tasks))\n\t\twake_up(&destroy_wait);\n\tif (atomic_dec_and_test(&clnt->cl_count))\n\t\trpc_free_auth(clnt);\n}\n\n/**\n * rpc_bind_new_program - bind a new RPC program to an existing client\n * @old: old rpc_client\n * @program: rpc program to set\n * @vers: rpc program version\n *\n * Clones the rpc client and sets up a new RPC program. This is mainly\n * of use for enabling different RPC programs to share the same transport.\n * The Sun NFSv2/v3 ACL protocol can do this.\n */\nstruct rpc_clnt *rpc_bind_new_program(struct rpc_clnt *old,\n\t\t\t\t      struct rpc_program *program,\n\t\t\t\t      u32 vers)\n{\n\tstruct rpc_clnt *clnt;\n\tstruct rpc_version *version;\n\tint err;\n\n\tBUG_ON(vers >= program->nrvers || !program->version[vers]);\n\tversion = program->version[vers];\n\tclnt = rpc_clone_client(old);\n\tif (IS_ERR(clnt))\n\t\tgoto out;\n\tclnt->cl_procinfo = version->procs;\n\tclnt->cl_maxproc  = version->nrprocs;\n\tclnt->cl_protname = program->name;\n\tclnt->cl_prog     = program->number;\n\tclnt->cl_vers     = version->number;\n\tclnt->cl_stats    = program->stats;\n\terr = rpc_ping(clnt);\n\tif (err != 0) {\n\t\trpc_shutdown_client(clnt);\n\t\tclnt = ERR_PTR(err);\n\t}\nout:\n\treturn clnt;\n}\nEXPORT_SYMBOL_GPL(rpc_bind_new_program);\n\nvoid rpc_task_release_client(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\n\tif (clnt != NULL) {\n\t\t/* Remove from client task list */\n\t\tspin_lock(&clnt->cl_lock);\n\t\tlist_del(&task->tk_task);\n\t\tspin_unlock(&clnt->cl_lock);\n\t\ttask->tk_client = NULL;\n\n\t\trpc_release_client(clnt);\n\t}\n}\n\nstatic\nvoid rpc_task_set_client(struct rpc_task *task, struct rpc_clnt *clnt)\n{\n\tif (clnt != NULL) {\n\t\trpc_task_release_client(task);\n\t\ttask->tk_client = clnt;\n\t\tatomic_inc(&clnt->cl_count);\n\t\tif (clnt->cl_softrtry)\n\t\t\ttask->tk_flags |= RPC_TASK_SOFT;\n\t\t/* Add to the client's list of all tasks */\n\t\tspin_lock(&clnt->cl_lock);\n\t\tlist_add_tail(&task->tk_task, &clnt->cl_tasks);\n\t\tspin_unlock(&clnt->cl_lock);\n\t}\n}\n\nvoid rpc_task_reset_client(struct rpc_task *task, struct rpc_clnt *clnt)\n{\n\trpc_task_release_client(task);\n\trpc_task_set_client(task, clnt);\n}\nEXPORT_SYMBOL_GPL(rpc_task_reset_client);\n\n\nstatic void\nrpc_task_set_rpc_message(struct rpc_task *task, const struct rpc_message *msg)\n{\n\tif (msg != NULL) {\n\t\ttask->tk_msg.rpc_proc = msg->rpc_proc;\n\t\ttask->tk_msg.rpc_argp = msg->rpc_argp;\n\t\ttask->tk_msg.rpc_resp = msg->rpc_resp;\n\t\tif (msg->rpc_cred != NULL)\n\t\t\ttask->tk_msg.rpc_cred = get_rpccred(msg->rpc_cred);\n\t}\n}\n\n/*\n * Default callback for async RPC calls\n */\nstatic void\nrpc_default_callback(struct rpc_task *task, void *data)\n{\n}\n\nstatic const struct rpc_call_ops rpc_default_ops = {\n\t.rpc_call_done = rpc_default_callback,\n};\n\n/**\n * rpc_run_task - Allocate a new RPC task, then run rpc_execute against it\n * @task_setup_data: pointer to task initialisation data\n */\nstruct rpc_task *rpc_run_task(const struct rpc_task_setup *task_setup_data)\n{\n\tstruct rpc_task *task;\n\n\ttask = rpc_new_task(task_setup_data);\n\tif (IS_ERR(task))\n\t\tgoto out;\n\n\trpc_task_set_client(task, task_setup_data->rpc_client);\n\trpc_task_set_rpc_message(task, task_setup_data->rpc_message);\n\n\tif (task->tk_action == NULL)\n\t\trpc_call_start(task);\n\n\tatomic_inc(&task->tk_count);\n\trpc_execute(task);\nout:\n\treturn task;\n}\nEXPORT_SYMBOL_GPL(rpc_run_task);\n\n/**\n * rpc_call_sync - Perform a synchronous RPC call\n * @clnt: pointer to RPC client\n * @msg: RPC call parameters\n * @flags: RPC call flags\n */\nint rpc_call_sync(struct rpc_clnt *clnt, const struct rpc_message *msg, int flags)\n{\n\tstruct rpc_task\t*task;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = msg,\n\t\t.callback_ops = &rpc_default_ops,\n\t\t.flags = flags,\n\t};\n\tint status;\n\n\tBUG_ON(flags & RPC_TASK_ASYNC);\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\tstatus = task->tk_status;\n\trpc_put_task(task);\n\treturn status;\n}\nEXPORT_SYMBOL_GPL(rpc_call_sync);\n\n/**\n * rpc_call_async - Perform an asynchronous RPC call\n * @clnt: pointer to RPC client\n * @msg: RPC call parameters\n * @flags: RPC call flags\n * @tk_ops: RPC call ops\n * @data: user call data\n */\nint\nrpc_call_async(struct rpc_clnt *clnt, const struct rpc_message *msg, int flags,\n\t       const struct rpc_call_ops *tk_ops, void *data)\n{\n\tstruct rpc_task\t*task;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = msg,\n\t\t.callback_ops = tk_ops,\n\t\t.callback_data = data,\n\t\t.flags = flags|RPC_TASK_ASYNC,\n\t};\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\trpc_put_task(task);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rpc_call_async);\n\n#if defined(CONFIG_NFS_V4_1)\n/**\n * rpc_run_bc_task - Allocate a new RPC task for backchannel use, then run\n * rpc_execute against it\n * @req: RPC request\n * @tk_ops: RPC call ops\n */\nstruct rpc_task *rpc_run_bc_task(struct rpc_rqst *req,\n\t\t\t\tconst struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_task *task;\n\tstruct xdr_buf *xbufp = &req->rq_snd_buf;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.callback_ops = tk_ops,\n\t};\n\n\tdprintk(\"RPC: rpc_run_bc_task req= %p\\n\", req);\n\t/*\n\t * Create an rpc_task to send the data\n\t */\n\ttask = rpc_new_task(&task_setup_data);\n\tif (IS_ERR(task)) {\n\t\txprt_free_bc_request(req);\n\t\tgoto out;\n\t}\n\ttask->tk_rqstp = req;\n\n\t/*\n\t * Set up the xdr_buf length.\n\t * This also indicates that the buffer is XDR encoded already.\n\t */\n\txbufp->len = xbufp->head[0].iov_len + xbufp->page_len +\n\t\t\txbufp->tail[0].iov_len;\n\n\ttask->tk_action = call_bc_transmit;\n\tatomic_inc(&task->tk_count);\n\tBUG_ON(atomic_read(&task->tk_count) != 2);\n\trpc_execute(task);\n\nout:\n\tdprintk(\"RPC: rpc_run_bc_task: task= %p\\n\", task);\n\treturn task;\n}\n#endif /* CONFIG_NFS_V4_1 */\n\nvoid\nrpc_call_start(struct rpc_task *task)\n{\n\ttask->tk_action = call_start;\n}\nEXPORT_SYMBOL_GPL(rpc_call_start);\n\n/**\n * rpc_peeraddr - extract remote peer address from clnt's xprt\n * @clnt: RPC client structure\n * @buf: target buffer\n * @bufsize: length of target buffer\n *\n * Returns the number of bytes that are actually in the stored address.\n */\nsize_t rpc_peeraddr(struct rpc_clnt *clnt, struct sockaddr *buf, size_t bufsize)\n{\n\tsize_t bytes;\n\tstruct rpc_xprt *xprt = clnt->cl_xprt;\n\n\tbytes = sizeof(xprt->addr);\n\tif (bytes > bufsize)\n\t\tbytes = bufsize;\n\tmemcpy(buf, &clnt->cl_xprt->addr, bytes);\n\treturn xprt->addrlen;\n}\nEXPORT_SYMBOL_GPL(rpc_peeraddr);\n\n/**\n * rpc_peeraddr2str - return remote peer address in printable format\n * @clnt: RPC client structure\n * @format: address format\n *\n */\nconst char *rpc_peeraddr2str(struct rpc_clnt *clnt,\n\t\t\t     enum rpc_display_format_t format)\n{\n\tstruct rpc_xprt *xprt = clnt->cl_xprt;\n\n\tif (xprt->address_strings[format] != NULL)\n\t\treturn xprt->address_strings[format];\n\telse\n\t\treturn \"unprintable\";\n}\nEXPORT_SYMBOL_GPL(rpc_peeraddr2str);\n\nvoid\nrpc_setbufsize(struct rpc_clnt *clnt, unsigned int sndsize, unsigned int rcvsize)\n{\n\tstruct rpc_xprt *xprt = clnt->cl_xprt;\n\tif (xprt->ops->set_buffer_size)\n\t\txprt->ops->set_buffer_size(xprt, sndsize, rcvsize);\n}\nEXPORT_SYMBOL_GPL(rpc_setbufsize);\n\n/*\n * Return size of largest payload RPC client can support, in bytes\n *\n * For stream transports, this is one RPC record fragment (see RFC\n * 1831), as we don't support multi-record requests yet.  For datagram\n * transports, this is the size of an IP packet minus the IP, UDP, and\n * RPC header sizes.\n */\nsize_t rpc_max_payload(struct rpc_clnt *clnt)\n{\n\treturn clnt->cl_xprt->max_payload;\n}\nEXPORT_SYMBOL_GPL(rpc_max_payload);\n\n/**\n * rpc_force_rebind - force transport to check that remote port is unchanged\n * @clnt: client to rebind\n *\n */\nvoid rpc_force_rebind(struct rpc_clnt *clnt)\n{\n\tif (clnt->cl_autobind)\n\t\txprt_clear_bound(clnt->cl_xprt);\n}\nEXPORT_SYMBOL_GPL(rpc_force_rebind);\n\n/*\n * Restart an (async) RPC call from the call_prepare state.\n * Usually called from within the exit handler.\n */\nint\nrpc_restart_call_prepare(struct rpc_task *task)\n{\n\tif (RPC_ASSASSINATED(task))\n\t\treturn 0;\n\ttask->tk_action = rpc_prepare_task;\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(rpc_restart_call_prepare);\n\n/*\n * Restart an (async) RPC call. Usually called from within the\n * exit handler.\n */\nint\nrpc_restart_call(struct rpc_task *task)\n{\n\tif (RPC_ASSASSINATED(task))\n\t\treturn 0;\n\ttask->tk_action = call_start;\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(rpc_restart_call);\n\n#ifdef RPC_DEBUG\nstatic const char *rpc_proc_name(const struct rpc_task *task)\n{\n\tconst struct rpc_procinfo *proc = task->tk_msg.rpc_proc;\n\n\tif (proc) {\n\t\tif (proc->p_name)\n\t\t\treturn proc->p_name;\n\t\telse\n\t\t\treturn \"NULL\";\n\t} else\n\t\treturn \"no proc\";\n}\n#endif\n\n/*\n * 0.  Initial state\n *\n *     Other FSM states can be visited zero or more times, but\n *     this state is visited exactly once for each RPC.\n */\nstatic void\ncall_start(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\n\tdprintk(\"RPC: %5u call_start %s%d proc %s (%s)\\n\", task->tk_pid,\n\t\t\tclnt->cl_protname, clnt->cl_vers,\n\t\t\trpc_proc_name(task),\n\t\t\t(RPC_IS_ASYNC(task) ? \"async\" : \"sync\"));\n\n\t/* Increment call count */\n\ttask->tk_msg.rpc_proc->p_count++;\n\tclnt->cl_stats->rpccnt++;\n\ttask->tk_action = call_reserve;\n}\n\n/*\n * 1.\tReserve an RPC call slot\n */\nstatic void\ncall_reserve(struct rpc_task *task)\n{\n\tdprint_status(task);\n\n\ttask->tk_status  = 0;\n\ttask->tk_action  = call_reserveresult;\n\txprt_reserve(task);\n}\n\n/*\n * 1b.\tGrok the result of xprt_reserve()\n */\nstatic void\ncall_reserveresult(struct rpc_task *task)\n{\n\tint status = task->tk_status;\n\n\tdprint_status(task);\n\n\t/*\n\t * After a call to xprt_reserve(), we must have either\n\t * a request slot or else an error status.\n\t */\n\ttask->tk_status = 0;\n\tif (status >= 0) {\n\t\tif (task->tk_rqstp) {\n\t\t\ttask->tk_action = call_refresh;\n\t\t\treturn;\n\t\t}\n\n\t\tprintk(KERN_ERR \"%s: status=%d, but no request slot, exiting\\n\",\n\t\t\t\t__func__, status);\n\t\trpc_exit(task, -EIO);\n\t\treturn;\n\t}\n\n\t/*\n\t * Even though there was an error, we may have acquired\n\t * a request slot somehow.  Make sure not to leak it.\n\t */\n\tif (task->tk_rqstp) {\n\t\tprintk(KERN_ERR \"%s: status=%d, request allocated anyway\\n\",\n\t\t\t\t__func__, status);\n\t\txprt_release(task);\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\t/* woken up; retry */\n\t\ttask->tk_action = call_reserve;\n\t\treturn;\n\tcase -EIO:\t/* probably a shutdown */\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"%s: unrecognized error %d, exiting\\n\",\n\t\t\t\t__func__, status);\n\t\tbreak;\n\t}\n\trpc_exit(task, status);\n}\n\n/*\n * 2.\tBind and/or refresh the credentials\n */\nstatic void\ncall_refresh(struct rpc_task *task)\n{\n\tdprint_status(task);\n\n\ttask->tk_action = call_refreshresult;\n\ttask->tk_status = 0;\n\ttask->tk_client->cl_stats->rpcauthrefresh++;\n\trpcauth_refreshcred(task);\n}\n\n/*\n * 2a.\tProcess the results of a credential refresh\n */\nstatic void\ncall_refreshresult(struct rpc_task *task)\n{\n\tint status = task->tk_status;\n\n\tdprint_status(task);\n\n\ttask->tk_status = 0;\n\ttask->tk_action = call_refresh;\n\tswitch (status) {\n\tcase 0:\n\t\tif (rpcauth_uptodatecred(task))\n\t\t\ttask->tk_action = call_allocate;\n\t\treturn;\n\tcase -ETIMEDOUT:\n\t\trpc_delay(task, 3*HZ);\n\tcase -EAGAIN:\n\t\tstatus = -EACCES;\n\t\tif (!task->tk_cred_retry)\n\t\t\tbreak;\n\t\ttask->tk_cred_retry--;\n\t\tdprintk(\"RPC: %5u %s: retry refresh creds\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\treturn;\n\t}\n\tdprintk(\"RPC: %5u %s: refresh creds failed with error %d\\n\",\n\t\t\t\ttask->tk_pid, __func__, status);\n\trpc_exit(task, status);\n}\n\n/*\n * 2b.\tAllocate the buffer. For details, see sched.c:rpc_malloc.\n *\t(Note: buffer memory is freed in xprt_release).\n */\nstatic void\ncall_allocate(struct rpc_task *task)\n{\n\tunsigned int slack = task->tk_rqstp->rq_cred->cr_auth->au_cslack;\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\tstruct rpc_procinfo *proc = task->tk_msg.rpc_proc;\n\n\tdprint_status(task);\n\n\ttask->tk_status = 0;\n\ttask->tk_action = call_bind;\n\n\tif (req->rq_buffer)\n\t\treturn;\n\n\tif (proc->p_proc != 0) {\n\t\tBUG_ON(proc->p_arglen == 0);\n\t\tif (proc->p_decode != NULL)\n\t\t\tBUG_ON(proc->p_replen == 0);\n\t}\n\n\t/*\n\t * Calculate the size (in quads) of the RPC call\n\t * and reply headers, and convert both values\n\t * to byte sizes.\n\t */\n\treq->rq_callsize = RPC_CALLHDRSIZE + (slack << 1) + proc->p_arglen;\n\treq->rq_callsize <<= 2;\n\treq->rq_rcvsize = RPC_REPHDRSIZE + slack + proc->p_replen;\n\treq->rq_rcvsize <<= 2;\n\n\treq->rq_buffer = xprt->ops->buf_alloc(task,\n\t\t\t\t\treq->rq_callsize + req->rq_rcvsize);\n\tif (req->rq_buffer != NULL)\n\t\treturn;\n\n\tdprintk(\"RPC: %5u rpc_buffer allocation failed\\n\", task->tk_pid);\n\n\tif (RPC_IS_ASYNC(task) || !signalled()) {\n\t\ttask->tk_action = call_allocate;\n\t\trpc_delay(task, HZ>>4);\n\t\treturn;\n\t}\n\n\trpc_exit(task, -ERESTARTSYS);\n}\n\nstatic inline int\nrpc_task_need_encode(struct rpc_task *task)\n{\n\treturn task->tk_rqstp->rq_snd_buf.len == 0;\n}\n\nstatic inline void\nrpc_task_force_reencode(struct rpc_task *task)\n{\n\ttask->tk_rqstp->rq_snd_buf.len = 0;\n\ttask->tk_rqstp->rq_bytes_sent = 0;\n}\n\nstatic inline void\nrpc_xdr_buf_init(struct xdr_buf *buf, void *start, size_t len)\n{\n\tbuf->head[0].iov_base = start;\n\tbuf->head[0].iov_len = len;\n\tbuf->tail[0].iov_len = 0;\n\tbuf->page_len = 0;\n\tbuf->flags = 0;\n\tbuf->len = 0;\n\tbuf->buflen = len;\n}\n\n/*\n * 3.\tEncode arguments of an RPC call\n */\nstatic void\nrpc_xdr_encode(struct rpc_task *task)\n{\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tkxdreproc_t\tencode;\n\t__be32\t\t*p;\n\n\tdprint_status(task);\n\n\trpc_xdr_buf_init(&req->rq_snd_buf,\n\t\t\t req->rq_buffer,\n\t\t\t req->rq_callsize);\n\trpc_xdr_buf_init(&req->rq_rcv_buf,\n\t\t\t (char *)req->rq_buffer + req->rq_callsize,\n\t\t\t req->rq_rcvsize);\n\n\tp = rpc_encode_header(task);\n\tif (p == NULL) {\n\t\tprintk(KERN_INFO \"RPC: couldn't encode RPC header, exit EIO\\n\");\n\t\trpc_exit(task, -EIO);\n\t\treturn;\n\t}\n\n\tencode = task->tk_msg.rpc_proc->p_encode;\n\tif (encode == NULL)\n\t\treturn;\n\n\ttask->tk_status = rpcauth_wrap_req(task, encode, req, p,\n\t\t\ttask->tk_msg.rpc_argp);\n}\n\n/*\n * 4.\tGet the server port number if not yet set\n */\nstatic void\ncall_bind(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\tdprint_status(task);\n\n\ttask->tk_action = call_connect;\n\tif (!xprt_bound(xprt)) {\n\t\ttask->tk_action = call_bind_status;\n\t\ttask->tk_timeout = xprt->bind_timeout;\n\t\txprt->ops->rpcbind(task);\n\t}\n}\n\n/*\n * 4a.\tSort out bind result\n */\nstatic void\ncall_bind_status(struct rpc_task *task)\n{\n\tint status = -EIO;\n\n\tif (task->tk_status >= 0) {\n\t\tdprint_status(task);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_connect;\n\t\treturn;\n\t}\n\n\tswitch (task->tk_status) {\n\tcase -ENOMEM:\n\t\tdprintk(\"RPC: %5u rpcbind out of memory\\n\", task->tk_pid);\n\t\trpc_delay(task, HZ >> 2);\n\t\tgoto retry_timeout;\n\tcase -EACCES:\n\t\tdprintk(\"RPC: %5u remote rpcbind: RPC program/version \"\n\t\t\t\t\"unavailable\\n\", task->tk_pid);\n\t\t/* fail immediately if this is an RPC ping */\n\t\tif (task->tk_msg.rpc_proc->p_proc == 0) {\n\t\t\tstatus = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\trpc_delay(task, 3*HZ);\n\t\tgoto retry_timeout;\n\tcase -ETIMEDOUT:\n\t\tdprintk(\"RPC: %5u rpcbind request timed out\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tgoto retry_timeout;\n\tcase -EPFNOSUPPORT:\n\t\t/* server doesn't support any rpcbind version we know of */\n\t\tdprintk(\"RPC: %5u unrecognized remote rpcbind service\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tbreak;\n\tcase -EPROTONOSUPPORT:\n\t\tdprintk(\"RPC: %5u remote rpcbind version unavailable, retrying\\n\",\n\t\t\t\ttask->tk_pid);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_bind;\n\t\treturn;\n\tcase -ECONNREFUSED:\t\t/* connection problems */\n\tcase -ECONNRESET:\n\tcase -ENOTCONN:\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\tcase -EPIPE:\n\t\tdprintk(\"RPC: %5u remote rpcbind unreachable: %d\\n\",\n\t\t\t\ttask->tk_pid, task->tk_status);\n\t\tif (!RPC_IS_SOFTCONN(task)) {\n\t\t\trpc_delay(task, 5*HZ);\n\t\t\tgoto retry_timeout;\n\t\t}\n\t\tstatus = task->tk_status;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC: %5u unrecognized rpcbind error (%d)\\n\",\n\t\t\t\ttask->tk_pid, -task->tk_status);\n\t}\n\n\trpc_exit(task, status);\n\treturn;\n\nretry_timeout:\n\ttask->tk_action = call_timeout;\n}\n\n/*\n * 4b.\tConnect to the RPC server\n */\nstatic void\ncall_connect(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\tdprintk(\"RPC: %5u call_connect xprt %p %s connected\\n\",\n\t\t\ttask->tk_pid, xprt,\n\t\t\t(xprt_connected(xprt) ? \"is\" : \"is not\"));\n\n\ttask->tk_action = call_transmit;\n\tif (!xprt_connected(xprt)) {\n\t\ttask->tk_action = call_connect_status;\n\t\tif (task->tk_status < 0)\n\t\t\treturn;\n\t\txprt_connect(task);\n\t}\n}\n\n/*\n * 4c.\tSort out connect result\n */\nstatic void\ncall_connect_status(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\tint status = task->tk_status;\n\n\tdprint_status(task);\n\n\ttask->tk_status = 0;\n\tif (status >= 0 || status == -EAGAIN) {\n\t\tclnt->cl_stats->netreconn++;\n\t\ttask->tk_action = call_transmit;\n\t\treturn;\n\t}\n\n\tswitch (status) {\n\t\t/* if soft mounted, test if we've timed out */\n\tcase -ETIMEDOUT:\n\t\ttask->tk_action = call_timeout;\n\t\tbreak;\n\tdefault:\n\t\trpc_exit(task, -EIO);\n\t}\n}\n\n/*\n * 5.\tTransmit the RPC request, and wait for reply\n */\nstatic void\ncall_transmit(struct rpc_task *task)\n{\n\tdprint_status(task);\n\n\ttask->tk_action = call_status;\n\tif (task->tk_status < 0)\n\t\treturn;\n\ttask->tk_status = xprt_prepare_transmit(task);\n\tif (task->tk_status != 0)\n\t\treturn;\n\ttask->tk_action = call_transmit_status;\n\t/* Encode here so that rpcsec_gss can use correct sequence number. */\n\tif (rpc_task_need_encode(task)) {\n\t\tBUG_ON(task->tk_rqstp->rq_bytes_sent != 0);\n\t\trpc_xdr_encode(task);\n\t\t/* Did the encode result in an error condition? */\n\t\tif (task->tk_status != 0) {\n\t\t\t/* Was the error nonfatal? */\n\t\t\tif (task->tk_status == -EAGAIN)\n\t\t\t\trpc_delay(task, HZ >> 4);\n\t\t\telse\n\t\t\t\trpc_exit(task, task->tk_status);\n\t\t\treturn;\n\t\t}\n\t}\n\txprt_transmit(task);\n\tif (task->tk_status < 0)\n\t\treturn;\n\t/*\n\t * On success, ensure that we call xprt_end_transmit() before sleeping\n\t * in order to allow access to the socket to other RPC requests.\n\t */\n\tcall_transmit_status(task);\n\tif (rpc_reply_expected(task))\n\t\treturn;\n\ttask->tk_action = rpc_exit_task;\n\trpc_wake_up_queued_task(&task->tk_xprt->pending, task);\n}\n\n/*\n * 5a.\tHandle cleanup after a transmission\n */\nstatic void\ncall_transmit_status(struct rpc_task *task)\n{\n\ttask->tk_action = call_status;\n\n\t/*\n\t * Common case: success.  Force the compiler to put this\n\t * test first.\n\t */\n\tif (task->tk_status == 0) {\n\t\txprt_end_transmit(task);\n\t\trpc_task_force_reencode(task);\n\t\treturn;\n\t}\n\n\tswitch (task->tk_status) {\n\tcase -EAGAIN:\n\t\tbreak;\n\tdefault:\n\t\tdprint_status(task);\n\t\txprt_end_transmit(task);\n\t\trpc_task_force_reencode(task);\n\t\tbreak;\n\t\t/*\n\t\t * Special cases: if we've been waiting on the\n\t\t * socket's write_space() callback, or if the\n\t\t * socket just returned a connection error,\n\t\t * then hold onto the transport lock.\n\t\t */\n\tcase -ECONNREFUSED:\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\t\tif (RPC_IS_SOFTCONN(task)) {\n\t\t\txprt_end_transmit(task);\n\t\t\trpc_exit(task, task->tk_status);\n\t\t\tbreak;\n\t\t}\n\tcase -ECONNRESET:\n\tcase -ENOTCONN:\n\tcase -EPIPE:\n\t\trpc_task_force_reencode(task);\n\t}\n}\n\n#if defined(CONFIG_NFS_V4_1)\n/*\n * 5b.\tSend the backchannel RPC reply.  On error, drop the reply.  In\n * addition, disconnect on connectivity errors.\n */\nstatic void\ncall_bc_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tBUG_ON(task->tk_status != 0);\n\ttask->tk_status = xprt_prepare_transmit(task);\n\tif (task->tk_status == -EAGAIN) {\n\t\t/*\n\t\t * Could not reserve the transport. Try again after the\n\t\t * transport is released.\n\t\t */\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_bc_transmit;\n\t\treturn;\n\t}\n\n\ttask->tk_action = rpc_exit_task;\n\tif (task->tk_status < 0) {\n\t\tprintk(KERN_NOTICE \"RPC: Could not send backchannel reply \"\n\t\t\t\"error: %d\\n\", task->tk_status);\n\t\treturn;\n\t}\n\n\txprt_transmit(task);\n\txprt_end_transmit(task);\n\tdprint_status(task);\n\tswitch (task->tk_status) {\n\tcase 0:\n\t\t/* Success */\n\t\tbreak;\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\tcase -ETIMEDOUT:\n\t\t/*\n\t\t * Problem reaching the server.  Disconnect and let the\n\t\t * forechannel reestablish the connection.  The server will\n\t\t * have to retransmit the backchannel request and we'll\n\t\t * reprocess it.  Since these ops are idempotent, there's no\n\t\t * need to cache our reply at this time.\n\t\t */\n\t\tprintk(KERN_NOTICE \"RPC: Could not send backchannel reply \"\n\t\t\t\"error: %d\\n\", task->tk_status);\n\t\txprt_conditional_disconnect(task->tk_xprt,\n\t\t\treq->rq_connect_cookie);\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * We were unable to reply and will have to drop the\n\t\t * request.  The server should reconnect and retransmit.\n\t\t */\n\t\tBUG_ON(task->tk_status == -EAGAIN);\n\t\tprintk(KERN_NOTICE \"RPC: Could not send backchannel reply \"\n\t\t\t\"error: %d\\n\", task->tk_status);\n\t\tbreak;\n\t}\n\trpc_wake_up_queued_task(&req->rq_xprt->pending, task);\n}\n#endif /* CONFIG_NFS_V4_1 */\n\n/*\n * 6.\tSort out the RPC call status\n */\nstatic void\ncall_status(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tint\t\tstatus;\n\n\tif (req->rq_reply_bytes_recvd > 0 && !req->rq_bytes_sent)\n\t\ttask->tk_status = req->rq_reply_bytes_recvd;\n\n\tdprint_status(task);\n\n\tstatus = task->tk_status;\n\tif (status >= 0) {\n\t\ttask->tk_action = call_decode;\n\t\treturn;\n\t}\n\n\ttask->tk_status = 0;\n\tswitch(status) {\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\t\t/*\n\t\t * Delay any retries for 3 seconds, then handle as if it\n\t\t * were a timeout.\n\t\t */\n\t\trpc_delay(task, 3*HZ);\n\tcase -ETIMEDOUT:\n\t\ttask->tk_action = call_timeout;\n\t\tif (task->tk_client->cl_discrtry)\n\t\t\txprt_conditional_disconnect(task->tk_xprt,\n\t\t\t\t\treq->rq_connect_cookie);\n\t\tbreak;\n\tcase -ECONNRESET:\n\tcase -ECONNREFUSED:\n\t\trpc_force_rebind(clnt);\n\t\trpc_delay(task, 3*HZ);\n\tcase -EPIPE:\n\tcase -ENOTCONN:\n\t\ttask->tk_action = call_bind;\n\t\tbreak;\n\tcase -EAGAIN:\n\t\ttask->tk_action = call_transmit;\n\t\tbreak;\n\tcase -EIO:\n\t\t/* shutdown or soft timeout */\n\t\trpc_exit(task, status);\n\t\tbreak;\n\tdefault:\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(\"%s: RPC call returned error %d\\n\",\n\t\t\t       clnt->cl_protname, -status);\n\t\trpc_exit(task, status);\n\t}\n}\n\n/*\n * 6a.\tHandle RPC timeout\n * \tWe do not release the request slot, so we keep using the\n *\tsame XID for all retransmits.\n */\nstatic void\ncall_timeout(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\n\tif (xprt_adjust_timeout(task->tk_rqstp) == 0) {\n\t\tdprintk(\"RPC: %5u call_timeout (minor)\\n\", task->tk_pid);\n\t\tgoto retry;\n\t}\n\n\tdprintk(\"RPC: %5u call_timeout (major)\\n\", task->tk_pid);\n\ttask->tk_timeouts++;\n\n\tif (RPC_IS_SOFTCONN(task)) {\n\t\trpc_exit(task, -ETIMEDOUT);\n\t\treturn;\n\t}\n\tif (RPC_IS_SOFT(task)) {\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(KERN_NOTICE \"%s: server %s not responding, timed out\\n\",\n\t\t\t\tclnt->cl_protname, clnt->cl_server);\n\t\tif (task->tk_flags & RPC_TASK_TIMEOUT)\n\t\t\trpc_exit(task, -ETIMEDOUT);\n\t\telse\n\t\t\trpc_exit(task, -EIO);\n\t\treturn;\n\t}\n\n\tif (!(task->tk_flags & RPC_CALL_MAJORSEEN)) {\n\t\ttask->tk_flags |= RPC_CALL_MAJORSEEN;\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(KERN_NOTICE \"%s: server %s not responding, still trying\\n\",\n\t\t\tclnt->cl_protname, clnt->cl_server);\n\t}\n\trpc_force_rebind(clnt);\n\t/*\n\t * Did our request time out due to an RPCSEC_GSS out-of-sequence\n\t * event? RFC2203 requires the server to drop all such requests.\n\t */\n\trpcauth_invalcred(task);\n\nretry:\n\tclnt->cl_stats->rpcretrans++;\n\ttask->tk_action = call_bind;\n\ttask->tk_status = 0;\n}\n\n/*\n * 7.\tDecode the RPC reply\n */\nstatic void\ncall_decode(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tkxdrdproc_t\tdecode = task->tk_msg.rpc_proc->p_decode;\n\t__be32\t\t*p;\n\n\tdprintk(\"RPC: %5u call_decode (status %d)\\n\",\n\t\t\ttask->tk_pid, task->tk_status);\n\n\tif (task->tk_flags & RPC_CALL_MAJORSEEN) {\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(KERN_NOTICE \"%s: server %s OK\\n\",\n\t\t\t\tclnt->cl_protname, clnt->cl_server);\n\t\ttask->tk_flags &= ~RPC_CALL_MAJORSEEN;\n\t}\n\n\t/*\n\t * Ensure that we see all writes made by xprt_complete_rqst()\n\t * before it changed req->rq_reply_bytes_recvd.\n\t */\n\tsmp_rmb();\n\treq->rq_rcv_buf.len = req->rq_private_buf.len;\n\n\t/* Check that the softirq receive buffer is valid */\n\tWARN_ON(memcmp(&req->rq_rcv_buf, &req->rq_private_buf,\n\t\t\t\tsizeof(req->rq_rcv_buf)) != 0);\n\n\tif (req->rq_rcv_buf.len < 12) {\n\t\tif (!RPC_IS_SOFT(task)) {\n\t\t\ttask->tk_action = call_bind;\n\t\t\tclnt->cl_stats->rpcretrans++;\n\t\t\tgoto out_retry;\n\t\t}\n\t\tdprintk(\"RPC:       %s: too small RPC reply size (%d bytes)\\n\",\n\t\t\t\tclnt->cl_protname, task->tk_status);\n\t\ttask->tk_action = call_timeout;\n\t\tgoto out_retry;\n\t}\n\n\tp = rpc_verify_header(task);\n\tif (IS_ERR(p)) {\n\t\tif (p == ERR_PTR(-EAGAIN))\n\t\t\tgoto out_retry;\n\t\treturn;\n\t}\n\n\ttask->tk_action = rpc_exit_task;\n\n\tif (decode) {\n\t\ttask->tk_status = rpcauth_unwrap_resp(task, decode, req, p,\n\t\t\t\t\t\t      task->tk_msg.rpc_resp);\n\t}\n\tdprintk(\"RPC: %5u call_decode result %d\\n\", task->tk_pid,\n\t\t\ttask->tk_status);\n\treturn;\nout_retry:\n\ttask->tk_status = 0;\n\t/* Note: rpc_verify_header() may have freed the RPC slot */\n\tif (task->tk_rqstp == req) {\n\t\treq->rq_reply_bytes_recvd = req->rq_rcv_buf.len = 0;\n\t\tif (task->tk_client->cl_discrtry)\n\t\t\txprt_conditional_disconnect(task->tk_xprt,\n\t\t\t\t\treq->rq_connect_cookie);\n\t}\n}\n\nstatic __be32 *\nrpc_encode_header(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\t__be32\t\t*p = req->rq_svec[0].iov_base;\n\n\t/* FIXME: check buffer size? */\n\n\tp = xprt_skip_transport_header(task->tk_xprt, p);\n\t*p++ = req->rq_xid;\t\t/* XID */\n\t*p++ = htonl(RPC_CALL);\t\t/* CALL */\n\t*p++ = htonl(RPC_VERSION);\t/* RPC version */\n\t*p++ = htonl(clnt->cl_prog);\t/* program number */\n\t*p++ = htonl(clnt->cl_vers);\t/* program version */\n\t*p++ = htonl(task->tk_msg.rpc_proc->p_proc);\t/* procedure */\n\tp = rpcauth_marshcred(task, p);\n\treq->rq_slen = xdr_adjust_iovec(&req->rq_svec[0], p);\n\treturn p;\n}\n\nstatic __be32 *\nrpc_verify_header(struct rpc_task *task)\n{\n\tstruct kvec *iov = &task->tk_rqstp->rq_rcv_buf.head[0];\n\tint len = task->tk_rqstp->rq_rcv_buf.len >> 2;\n\t__be32\t*p = iov->iov_base;\n\tu32 n;\n\tint error = -EACCES;\n\n\tif ((task->tk_rqstp->rq_rcv_buf.len & 3) != 0) {\n\t\t/* RFC-1014 says that the representation of XDR data must be a\n\t\t * multiple of four bytes\n\t\t * - if it isn't pointer subtraction in the NFS client may give\n\t\t *   undefined results\n\t\t */\n\t\tdprintk(\"RPC: %5u %s: XDR representation not a multiple of\"\n\t\t       \" 4 bytes: 0x%x\\n\", task->tk_pid, __func__,\n\t\t       task->tk_rqstp->rq_rcv_buf.len);\n\t\tgoto out_eio;\n\t}\n\tif ((len -= 3) < 0)\n\t\tgoto out_overflow;\n\n\tp += 1; /* skip XID */\n\tif ((n = ntohl(*p++)) != RPC_REPLY) {\n\t\tdprintk(\"RPC: %5u %s: not an RPC reply: %x\\n\",\n\t\t\ttask->tk_pid, __func__, n);\n\t\tgoto out_garbage;\n\t}\n\n\tif ((n = ntohl(*p++)) != RPC_MSG_ACCEPTED) {\n\t\tif (--len < 0)\n\t\t\tgoto out_overflow;\n\t\tswitch ((n = ntohl(*p++))) {\n\t\t\tcase RPC_AUTH_ERROR:\n\t\t\t\tbreak;\n\t\t\tcase RPC_MISMATCH:\n\t\t\t\tdprintk(\"RPC: %5u %s: RPC call version \"\n\t\t\t\t\t\t\"mismatch!\\n\",\n\t\t\t\t\t\ttask->tk_pid, __func__);\n\t\t\t\terror = -EPROTONOSUPPORT;\n\t\t\t\tgoto out_err;\n\t\t\tdefault:\n\t\t\t\tdprintk(\"RPC: %5u %s: RPC call rejected, \"\n\t\t\t\t\t\t\"unknown error: %x\\n\",\n\t\t\t\t\t\ttask->tk_pid, __func__, n);\n\t\t\t\tgoto out_eio;\n\t\t}\n\t\tif (--len < 0)\n\t\t\tgoto out_overflow;\n\t\tswitch ((n = ntohl(*p++))) {\n\t\tcase RPC_AUTH_REJECTEDCRED:\n\t\tcase RPC_AUTH_REJECTEDVERF:\n\t\tcase RPCSEC_GSS_CREDPROBLEM:\n\t\tcase RPCSEC_GSS_CTXPROBLEM:\n\t\t\tif (!task->tk_cred_retry)\n\t\t\t\tbreak;\n\t\t\ttask->tk_cred_retry--;\n\t\t\tdprintk(\"RPC: %5u %s: retry stale creds\\n\",\n\t\t\t\t\ttask->tk_pid, __func__);\n\t\t\trpcauth_invalcred(task);\n\t\t\t/* Ensure we obtain a new XID! */\n\t\t\txprt_release(task);\n\t\t\ttask->tk_action = call_reserve;\n\t\t\tgoto out_retry;\n\t\tcase RPC_AUTH_BADCRED:\n\t\tcase RPC_AUTH_BADVERF:\n\t\t\t/* possibly garbled cred/verf? */\n\t\t\tif (!task->tk_garb_retry)\n\t\t\t\tbreak;\n\t\t\ttask->tk_garb_retry--;\n\t\t\tdprintk(\"RPC: %5u %s: retry garbled creds\\n\",\n\t\t\t\t\ttask->tk_pid, __func__);\n\t\t\ttask->tk_action = call_bind;\n\t\t\tgoto out_retry;\n\t\tcase RPC_AUTH_TOOWEAK:\n\t\t\tprintk(KERN_NOTICE \"RPC: server %s requires stronger \"\n\t\t\t       \"authentication.\\n\", task->tk_client->cl_server);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdprintk(\"RPC: %5u %s: unknown auth error: %x\\n\",\n\t\t\t\t\ttask->tk_pid, __func__, n);\n\t\t\terror = -EIO;\n\t\t}\n\t\tdprintk(\"RPC: %5u %s: call rejected %d\\n\",\n\t\t\t\ttask->tk_pid, __func__, n);\n\t\tgoto out_err;\n\t}\n\tif (!(p = rpcauth_checkverf(task, p))) {\n\t\tdprintk(\"RPC: %5u %s: auth check failed\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\tgoto out_garbage;\t\t/* bad verifier, retry */\n\t}\n\tlen = p - (__be32 *)iov->iov_base - 1;\n\tif (len < 0)\n\t\tgoto out_overflow;\n\tswitch ((n = ntohl(*p++))) {\n\tcase RPC_SUCCESS:\n\t\treturn p;\n\tcase RPC_PROG_UNAVAIL:\n\t\tdprintk(\"RPC: %5u %s: program %u is unsupported by server %s\\n\",\n\t\t\t\ttask->tk_pid, __func__,\n\t\t\t\t(unsigned int)task->tk_client->cl_prog,\n\t\t\t\ttask->tk_client->cl_server);\n\t\terror = -EPFNOSUPPORT;\n\t\tgoto out_err;\n\tcase RPC_PROG_MISMATCH:\n\t\tdprintk(\"RPC: %5u %s: program %u, version %u unsupported by \"\n\t\t\t\t\"server %s\\n\", task->tk_pid, __func__,\n\t\t\t\t(unsigned int)task->tk_client->cl_prog,\n\t\t\t\t(unsigned int)task->tk_client->cl_vers,\n\t\t\t\ttask->tk_client->cl_server);\n\t\terror = -EPROTONOSUPPORT;\n\t\tgoto out_err;\n\tcase RPC_PROC_UNAVAIL:\n\t\tdprintk(\"RPC: %5u %s: proc %s unsupported by program %u, \"\n\t\t\t\t\"version %u on server %s\\n\",\n\t\t\t\ttask->tk_pid, __func__,\n\t\t\t\trpc_proc_name(task),\n\t\t\t\ttask->tk_client->cl_prog,\n\t\t\t\ttask->tk_client->cl_vers,\n\t\t\t\ttask->tk_client->cl_server);\n\t\terror = -EOPNOTSUPP;\n\t\tgoto out_err;\n\tcase RPC_GARBAGE_ARGS:\n\t\tdprintk(\"RPC: %5u %s: server saw garbage\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\tbreak;\t\t\t/* retry */\n\tdefault:\n\t\tdprintk(\"RPC: %5u %s: server accept status: %x\\n\",\n\t\t\t\ttask->tk_pid, __func__, n);\n\t\t/* Also retry */\n\t}\n\nout_garbage:\n\ttask->tk_client->cl_stats->rpcgarbage++;\n\tif (task->tk_garb_retry) {\n\t\ttask->tk_garb_retry--;\n\t\tdprintk(\"RPC: %5u %s: retrying\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\ttask->tk_action = call_bind;\nout_retry:\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\nout_eio:\n\terror = -EIO;\nout_err:\n\trpc_exit(task, error);\n\tdprintk(\"RPC: %5u %s: call failed with error %d\\n\", task->tk_pid,\n\t\t\t__func__, error);\n\treturn ERR_PTR(error);\nout_overflow:\n\tdprintk(\"RPC: %5u %s: server reply was truncated.\\n\", task->tk_pid,\n\t\t\t__func__);\n\tgoto out_garbage;\n}\n\nstatic void rpcproc_encode_null(void *rqstp, struct xdr_stream *xdr, void *obj)\n{\n}\n\nstatic int rpcproc_decode_null(void *rqstp, struct xdr_stream *xdr, void *obj)\n{\n\treturn 0;\n}\n\nstatic struct rpc_procinfo rpcproc_null = {\n\t.p_encode = rpcproc_encode_null,\n\t.p_decode = rpcproc_decode_null,\n};\n\nstatic int rpc_ping(struct rpc_clnt *clnt)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &rpcproc_null,\n\t};\n\tint err;\n\tmsg.rpc_cred = authnull_ops.lookup_cred(NULL, NULL, 0);\n\terr = rpc_call_sync(clnt, &msg, RPC_TASK_SOFT | RPC_TASK_SOFTCONN);\n\tput_rpccred(msg.rpc_cred);\n\treturn err;\n}\n\nstruct rpc_task *rpc_call_null(struct rpc_clnt *clnt, struct rpc_cred *cred, int flags)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &rpcproc_null,\n\t\t.rpc_cred = cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = &rpc_default_ops,\n\t\t.flags = flags,\n\t};\n\treturn rpc_run_task(&task_setup_data);\n}\nEXPORT_SYMBOL_GPL(rpc_call_null);\n\n#ifdef RPC_DEBUG\nstatic void rpc_show_header(void)\n{\n\tprintk(KERN_INFO \"-pid- flgs status -client- --rqstp- \"\n\t\t\"-timeout ---ops--\\n\");\n}\n\nstatic void rpc_show_task(const struct rpc_clnt *clnt,\n\t\t\t  const struct rpc_task *task)\n{\n\tconst char *rpc_waitq = \"none\";\n\n\tif (RPC_IS_QUEUED(task))\n\t\trpc_waitq = rpc_qname(task->tk_waitqueue);\n\n\tprintk(KERN_INFO \"%5u %04x %6d %8p %8p %8ld %8p %sv%u %s a:%ps q:%s\\n\",\n\t\ttask->tk_pid, task->tk_flags, task->tk_status,\n\t\tclnt, task->tk_rqstp, task->tk_timeout, task->tk_ops,\n\t\tclnt->cl_protname, clnt->cl_vers, rpc_proc_name(task),\n\t\ttask->tk_action, rpc_waitq);\n}\n\nvoid rpc_show_tasks(void)\n{\n\tstruct rpc_clnt *clnt;\n\tstruct rpc_task *task;\n\tint header = 0;\n\n\tspin_lock(&rpc_client_lock);\n\tlist_for_each_entry(clnt, &all_clients, cl_clients) {\n\t\tspin_lock(&clnt->cl_lock);\n\t\tlist_for_each_entry(task, &clnt->cl_tasks, tk_task) {\n\t\t\tif (!header) {\n\t\t\t\trpc_show_header();\n\t\t\t\theader++;\n\t\t\t}\n\t\t\trpc_show_task(clnt, task);\n\t\t}\n\t\tspin_unlock(&clnt->cl_lock);\n\t}\n\tspin_unlock(&rpc_client_lock);\n}\n#endif\n", "/*\n * linux/net/sunrpc/sched.c\n *\n * Scheduling for synchronous and asynchronous RPC requests.\n *\n * Copyright (C) 1996 Olaf Kirch, <okir@monad.swb.de>\n *\n * TCP NFS related read + write fixes\n * (C) 1999 Dave Airlie, University of Limerick, Ireland <airlied@linux.ie>\n */\n\n#include <linux/module.h>\n\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/mempool.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/mutex.h>\n\n#include <linux/sunrpc/clnt.h>\n\n#include \"sunrpc.h\"\n\n#ifdef RPC_DEBUG\n#define RPCDBG_FACILITY\t\tRPCDBG_SCHED\n#endif\n\n/*\n * RPC slabs and memory pools\n */\n#define RPC_BUFFER_MAXSIZE\t(2048)\n#define RPC_BUFFER_POOLSIZE\t(8)\n#define RPC_TASK_POOLSIZE\t(8)\nstatic struct kmem_cache\t*rpc_task_slabp __read_mostly;\nstatic struct kmem_cache\t*rpc_buffer_slabp __read_mostly;\nstatic mempool_t\t*rpc_task_mempool __read_mostly;\nstatic mempool_t\t*rpc_buffer_mempool __read_mostly;\n\nstatic void\t\t\trpc_async_schedule(struct work_struct *);\nstatic void\t\t\t rpc_release_task(struct rpc_task *task);\nstatic void __rpc_queue_timer_fn(unsigned long ptr);\n\n/*\n * RPC tasks sit here while waiting for conditions to improve.\n */\nstatic struct rpc_wait_queue delay_queue;\n\n/*\n * rpciod-related stuff\n */\nstruct workqueue_struct *rpciod_workqueue;\n\n/*\n * Disable the timer for a given RPC task. Should be called with\n * queue->lock and bh_disabled in order to avoid races within\n * rpc_run_timer().\n */\nstatic void\n__rpc_disable_timer(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (task->tk_timeout == 0)\n\t\treturn;\n\tdprintk(\"RPC: %5u disabling timer\\n\", task->tk_pid);\n\ttask->tk_timeout = 0;\n\tlist_del(&task->u.tk_wait.timer_list);\n\tif (list_empty(&queue->timer_list.list))\n\t\tdel_timer(&queue->timer_list.timer);\n}\n\nstatic void\nrpc_set_queue_timer(struct rpc_wait_queue *queue, unsigned long expires)\n{\n\tqueue->timer_list.expires = expires;\n\tmod_timer(&queue->timer_list.timer, expires);\n}\n\n/*\n * Set up a timer for the current task.\n */\nstatic void\n__rpc_add_timer(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (!task->tk_timeout)\n\t\treturn;\n\n\tdprintk(\"RPC: %5u setting alarm for %lu ms\\n\",\n\t\t\ttask->tk_pid, task->tk_timeout * 1000 / HZ);\n\n\ttask->u.tk_wait.expires = jiffies + task->tk_timeout;\n\tif (list_empty(&queue->timer_list.list) || time_before(task->u.tk_wait.expires, queue->timer_list.expires))\n\t\trpc_set_queue_timer(queue, task->u.tk_wait.expires);\n\tlist_add(&task->u.tk_wait.timer_list, &queue->timer_list.list);\n}\n\n/*\n * Add new request to a priority queue.\n */\nstatic void __rpc_add_wait_queue_priority(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tstruct list_head *q;\n\tstruct rpc_task *t;\n\n\tINIT_LIST_HEAD(&task->u.tk_wait.links);\n\tq = &queue->tasks[task->tk_priority];\n\tif (unlikely(task->tk_priority > queue->maxpriority))\n\t\tq = &queue->tasks[queue->maxpriority];\n\tlist_for_each_entry(t, q, u.tk_wait.list) {\n\t\tif (t->tk_owner == task->tk_owner) {\n\t\t\tlist_add_tail(&task->u.tk_wait.list, &t->u.tk_wait.links);\n\t\t\treturn;\n\t\t}\n\t}\n\tlist_add_tail(&task->u.tk_wait.list, q);\n}\n\n/*\n * Add new request to wait queue.\n *\n * Swapper tasks always get inserted at the head of the queue.\n * This should avoid many nasty memory deadlocks and hopefully\n * improve overall performance.\n * Everyone else gets appended to the queue to ensure proper FIFO behavior.\n */\nstatic void __rpc_add_wait_queue(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tBUG_ON (RPC_IS_QUEUED(task));\n\n\tif (RPC_IS_PRIORITY(queue))\n\t\t__rpc_add_wait_queue_priority(queue, task);\n\telse if (RPC_IS_SWAPPER(task))\n\t\tlist_add(&task->u.tk_wait.list, &queue->tasks[0]);\n\telse\n\t\tlist_add_tail(&task->u.tk_wait.list, &queue->tasks[0]);\n\ttask->tk_waitqueue = queue;\n\tqueue->qlen++;\n\trpc_set_queued(task);\n\n\tdprintk(\"RPC: %5u added to queue %p \\\"%s\\\"\\n\",\n\t\t\ttask->tk_pid, queue, rpc_qname(queue));\n}\n\n/*\n * Remove request from a priority queue.\n */\nstatic void __rpc_remove_wait_queue_priority(struct rpc_task *task)\n{\n\tstruct rpc_task *t;\n\n\tif (!list_empty(&task->u.tk_wait.links)) {\n\t\tt = list_entry(task->u.tk_wait.links.next, struct rpc_task, u.tk_wait.list);\n\t\tlist_move(&t->u.tk_wait.list, &task->u.tk_wait.list);\n\t\tlist_splice_init(&task->u.tk_wait.links, &t->u.tk_wait.links);\n\t}\n}\n\n/*\n * Remove request from queue.\n * Note: must be called with spin lock held.\n */\nstatic void __rpc_remove_wait_queue(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\t__rpc_disable_timer(queue, task);\n\tif (RPC_IS_PRIORITY(queue))\n\t\t__rpc_remove_wait_queue_priority(task);\n\tlist_del(&task->u.tk_wait.list);\n\tqueue->qlen--;\n\tdprintk(\"RPC: %5u removed from queue %p \\\"%s\\\"\\n\",\n\t\t\ttask->tk_pid, queue, rpc_qname(queue));\n}\n\nstatic inline void rpc_set_waitqueue_priority(struct rpc_wait_queue *queue, int priority)\n{\n\tqueue->priority = priority;\n\tqueue->count = 1 << (priority * 2);\n}\n\nstatic inline void rpc_set_waitqueue_owner(struct rpc_wait_queue *queue, pid_t pid)\n{\n\tqueue->owner = pid;\n\tqueue->nr = RPC_BATCH_COUNT;\n}\n\nstatic inline void rpc_reset_waitqueue_priority(struct rpc_wait_queue *queue)\n{\n\trpc_set_waitqueue_priority(queue, queue->maxpriority);\n\trpc_set_waitqueue_owner(queue, 0);\n}\n\nstatic void __rpc_init_priority_wait_queue(struct rpc_wait_queue *queue, const char *qname, unsigned char nr_queues)\n{\n\tint i;\n\n\tspin_lock_init(&queue->lock);\n\tfor (i = 0; i < ARRAY_SIZE(queue->tasks); i++)\n\t\tINIT_LIST_HEAD(&queue->tasks[i]);\n\tqueue->maxpriority = nr_queues - 1;\n\trpc_reset_waitqueue_priority(queue);\n\tqueue->qlen = 0;\n\tsetup_timer(&queue->timer_list.timer, __rpc_queue_timer_fn, (unsigned long)queue);\n\tINIT_LIST_HEAD(&queue->timer_list.list);\n#ifdef RPC_DEBUG\n\tqueue->name = qname;\n#endif\n}\n\nvoid rpc_init_priority_wait_queue(struct rpc_wait_queue *queue, const char *qname)\n{\n\t__rpc_init_priority_wait_queue(queue, qname, RPC_NR_PRIORITY);\n}\nEXPORT_SYMBOL_GPL(rpc_init_priority_wait_queue);\n\nvoid rpc_init_wait_queue(struct rpc_wait_queue *queue, const char *qname)\n{\n\t__rpc_init_priority_wait_queue(queue, qname, 1);\n}\nEXPORT_SYMBOL_GPL(rpc_init_wait_queue);\n\nvoid rpc_destroy_wait_queue(struct rpc_wait_queue *queue)\n{\n\tdel_timer_sync(&queue->timer_list.timer);\n}\nEXPORT_SYMBOL_GPL(rpc_destroy_wait_queue);\n\nstatic int rpc_wait_bit_killable(void *word)\n{\n\tif (fatal_signal_pending(current))\n\t\treturn -ERESTARTSYS;\n\tschedule();\n\treturn 0;\n}\n\n#ifdef RPC_DEBUG\nstatic void rpc_task_set_debuginfo(struct rpc_task *task)\n{\n\tstatic atomic_t rpc_pid;\n\n\ttask->tk_pid = atomic_inc_return(&rpc_pid);\n}\n#else\nstatic inline void rpc_task_set_debuginfo(struct rpc_task *task)\n{\n}\n#endif\n\nstatic void rpc_set_active(struct rpc_task *task)\n{\n\trpc_task_set_debuginfo(task);\n\tset_bit(RPC_TASK_ACTIVE, &task->tk_runstate);\n}\n\n/*\n * Mark an RPC call as having completed by clearing the 'active' bit\n * and then waking up all tasks that were sleeping.\n */\nstatic int rpc_complete_task(struct rpc_task *task)\n{\n\tvoid *m = &task->tk_runstate;\n\twait_queue_head_t *wq = bit_waitqueue(m, RPC_TASK_ACTIVE);\n\tstruct wait_bit_key k = __WAIT_BIT_KEY_INITIALIZER(m, RPC_TASK_ACTIVE);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&wq->lock, flags);\n\tclear_bit(RPC_TASK_ACTIVE, &task->tk_runstate);\n\tret = atomic_dec_and_test(&task->tk_count);\n\tif (waitqueue_active(wq))\n\t\t__wake_up_locked_key(wq, TASK_NORMAL, &k);\n\tspin_unlock_irqrestore(&wq->lock, flags);\n\treturn ret;\n}\n\n/*\n * Allow callers to wait for completion of an RPC call\n *\n * Note the use of out_of_line_wait_on_bit() rather than wait_on_bit()\n * to enforce taking of the wq->lock and hence avoid races with\n * rpc_complete_task().\n */\nint __rpc_wait_for_completion_task(struct rpc_task *task, int (*action)(void *))\n{\n\tif (action == NULL)\n\t\taction = rpc_wait_bit_killable;\n\treturn out_of_line_wait_on_bit(&task->tk_runstate, RPC_TASK_ACTIVE,\n\t\t\taction, TASK_KILLABLE);\n}\nEXPORT_SYMBOL_GPL(__rpc_wait_for_completion_task);\n\n/*\n * Make an RPC task runnable.\n *\n * Note: If the task is ASYNC, this must be called with\n * the spinlock held to protect the wait queue operation.\n */\nstatic void rpc_make_runnable(struct rpc_task *task)\n{\n\trpc_clear_queued(task);\n\tif (rpc_test_and_set_running(task))\n\t\treturn;\n\tif (RPC_IS_ASYNC(task)) {\n\t\tINIT_WORK(&task->u.tk_work, rpc_async_schedule);\n\t\tqueue_work(rpciod_workqueue, &task->u.tk_work);\n\t} else\n\t\twake_up_bit(&task->tk_runstate, RPC_TASK_QUEUED);\n}\n\n/*\n * Prepare for sleeping on a wait queue.\n * By always appending tasks to the list we ensure FIFO behavior.\n * NB: An RPC task will only receive interrupt-driven events as long\n * as it's on a wait queue.\n */\nstatic void __rpc_sleep_on(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\t\trpc_action action)\n{\n\tdprintk(\"RPC: %5u sleep_on(queue \\\"%s\\\" time %lu)\\n\",\n\t\t\ttask->tk_pid, rpc_qname(q), jiffies);\n\n\t__rpc_add_wait_queue(q, task);\n\n\tBUG_ON(task->tk_callback != NULL);\n\ttask->tk_callback = action;\n\t__rpc_add_timer(q, task);\n}\n\nvoid rpc_sleep_on(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\t\t\trpc_action action)\n{\n\t/* We shouldn't ever put an inactive task to sleep */\n\tBUG_ON(!RPC_IS_ACTIVATED(task));\n\n\t/*\n\t * Protect the queue operations.\n\t */\n\tspin_lock_bh(&q->lock);\n\t__rpc_sleep_on(q, task, action);\n\tspin_unlock_bh(&q->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_sleep_on);\n\n/**\n * __rpc_do_wake_up_task - wake up a single rpc_task\n * @queue: wait queue\n * @task: task to be woken up\n *\n * Caller must hold queue->lock, and have cleared the task queued flag.\n */\nstatic void __rpc_do_wake_up_task(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tdprintk(\"RPC: %5u __rpc_wake_up_task (now %lu)\\n\",\n\t\t\ttask->tk_pid, jiffies);\n\n\t/* Has the task been executed yet? If not, we cannot wake it up! */\n\tif (!RPC_IS_ACTIVATED(task)) {\n\t\tprintk(KERN_ERR \"RPC: Inactive task (%p) being woken up!\\n\", task);\n\t\treturn;\n\t}\n\n\t__rpc_remove_wait_queue(queue, task);\n\n\trpc_make_runnable(task);\n\n\tdprintk(\"RPC:       __rpc_wake_up_task done\\n\");\n}\n\n/*\n * Wake up a queued task while the queue lock is being held\n */\nstatic void rpc_wake_up_task_queue_locked(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (RPC_IS_QUEUED(task) && task->tk_waitqueue == queue)\n\t\t__rpc_do_wake_up_task(queue, task);\n}\n\n/*\n * Tests whether rpc queue is empty\n */\nint rpc_queue_empty(struct rpc_wait_queue *queue)\n{\n\tint res;\n\n\tspin_lock_bh(&queue->lock);\n\tres = queue->qlen;\n\tspin_unlock_bh(&queue->lock);\n\treturn res == 0;\n}\nEXPORT_SYMBOL_GPL(rpc_queue_empty);\n\n/*\n * Wake up a task on a specific queue\n */\nvoid rpc_wake_up_queued_task(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tspin_lock_bh(&queue->lock);\n\trpc_wake_up_task_queue_locked(queue, task);\n\tspin_unlock_bh(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_queued_task);\n\n/*\n * Wake up the next task on a priority queue.\n */\nstatic struct rpc_task * __rpc_wake_up_next_priority(struct rpc_wait_queue *queue)\n{\n\tstruct list_head *q;\n\tstruct rpc_task *task;\n\n\t/*\n\t * Service a batch of tasks from a single owner.\n\t */\n\tq = &queue->tasks[queue->priority];\n\tif (!list_empty(q)) {\n\t\ttask = list_entry(q->next, struct rpc_task, u.tk_wait.list);\n\t\tif (queue->owner == task->tk_owner) {\n\t\t\tif (--queue->nr)\n\t\t\t\tgoto out;\n\t\t\tlist_move_tail(&task->u.tk_wait.list, q);\n\t\t}\n\t\t/*\n\t\t * Check if we need to switch queues.\n\t\t */\n\t\tif (--queue->count)\n\t\t\tgoto new_owner;\n\t}\n\n\t/*\n\t * Service the next queue.\n\t */\n\tdo {\n\t\tif (q == &queue->tasks[0])\n\t\t\tq = &queue->tasks[queue->maxpriority];\n\t\telse\n\t\t\tq = q - 1;\n\t\tif (!list_empty(q)) {\n\t\t\ttask = list_entry(q->next, struct rpc_task, u.tk_wait.list);\n\t\t\tgoto new_queue;\n\t\t}\n\t} while (q != &queue->tasks[queue->priority]);\n\n\trpc_reset_waitqueue_priority(queue);\n\treturn NULL;\n\nnew_queue:\n\trpc_set_waitqueue_priority(queue, (unsigned int)(q - &queue->tasks[0]));\nnew_owner:\n\trpc_set_waitqueue_owner(queue, task->tk_owner);\nout:\n\trpc_wake_up_task_queue_locked(queue, task);\n\treturn task;\n}\n\n/*\n * Wake up the next task on the wait queue.\n */\nstruct rpc_task * rpc_wake_up_next(struct rpc_wait_queue *queue)\n{\n\tstruct rpc_task\t*task = NULL;\n\n\tdprintk(\"RPC:       wake_up_next(%p \\\"%s\\\")\\n\",\n\t\t\tqueue, rpc_qname(queue));\n\tspin_lock_bh(&queue->lock);\n\tif (RPC_IS_PRIORITY(queue))\n\t\ttask = __rpc_wake_up_next_priority(queue);\n\telse {\n\t\ttask_for_first(task, &queue->tasks[0])\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t}\n\tspin_unlock_bh(&queue->lock);\n\n\treturn task;\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_next);\n\n/**\n * rpc_wake_up - wake up all rpc_tasks\n * @queue: rpc_wait_queue on which the tasks are sleeping\n *\n * Grabs queue->lock\n */\nvoid rpc_wake_up(struct rpc_wait_queue *queue)\n{\n\tstruct rpc_task *task, *next;\n\tstruct list_head *head;\n\n\tspin_lock_bh(&queue->lock);\n\thead = &queue->tasks[queue->maxpriority];\n\tfor (;;) {\n\t\tlist_for_each_entry_safe(task, next, head, u.tk_wait.list)\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\tif (head == &queue->tasks[0])\n\t\t\tbreak;\n\t\thead--;\n\t}\n\tspin_unlock_bh(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up);\n\n/**\n * rpc_wake_up_status - wake up all rpc_tasks and set their status value.\n * @queue: rpc_wait_queue on which the tasks are sleeping\n * @status: status value to set\n *\n * Grabs queue->lock\n */\nvoid rpc_wake_up_status(struct rpc_wait_queue *queue, int status)\n{\n\tstruct rpc_task *task, *next;\n\tstruct list_head *head;\n\n\tspin_lock_bh(&queue->lock);\n\thead = &queue->tasks[queue->maxpriority];\n\tfor (;;) {\n\t\tlist_for_each_entry_safe(task, next, head, u.tk_wait.list) {\n\t\t\ttask->tk_status = status;\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\t}\n\t\tif (head == &queue->tasks[0])\n\t\t\tbreak;\n\t\thead--;\n\t}\n\tspin_unlock_bh(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_status);\n\nstatic void __rpc_queue_timer_fn(unsigned long ptr)\n{\n\tstruct rpc_wait_queue *queue = (struct rpc_wait_queue *)ptr;\n\tstruct rpc_task *task, *n;\n\tunsigned long expires, now, timeo;\n\n\tspin_lock(&queue->lock);\n\texpires = now = jiffies;\n\tlist_for_each_entry_safe(task, n, &queue->timer_list.list, u.tk_wait.timer_list) {\n\t\ttimeo = task->u.tk_wait.expires;\n\t\tif (time_after_eq(now, timeo)) {\n\t\t\tdprintk(\"RPC: %5u timeout\\n\", task->tk_pid);\n\t\t\ttask->tk_status = -ETIMEDOUT;\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\t\tcontinue;\n\t\t}\n\t\tif (expires == now || time_after(expires, timeo))\n\t\t\texpires = timeo;\n\t}\n\tif (!list_empty(&queue->timer_list.list))\n\t\trpc_set_queue_timer(queue, expires);\n\tspin_unlock(&queue->lock);\n}\n\nstatic void __rpc_atrun(struct rpc_task *task)\n{\n\ttask->tk_status = 0;\n}\n\n/*\n * Run a task at a later time\n */\nvoid rpc_delay(struct rpc_task *task, unsigned long delay)\n{\n\ttask->tk_timeout = delay;\n\trpc_sleep_on(&delay_queue, task, __rpc_atrun);\n}\nEXPORT_SYMBOL_GPL(rpc_delay);\n\n/*\n * Helper to call task->tk_ops->rpc_call_prepare\n */\nvoid rpc_prepare_task(struct rpc_task *task)\n{\n\ttask->tk_ops->rpc_call_prepare(task, task->tk_calldata);\n}\n\n/*\n * Helper that calls task->tk_ops->rpc_call_done if it exists\n */\nvoid rpc_exit_task(struct rpc_task *task)\n{\n\ttask->tk_action = NULL;\n\tif (task->tk_ops->rpc_call_done != NULL) {\n\t\ttask->tk_ops->rpc_call_done(task, task->tk_calldata);\n\t\tif (task->tk_action != NULL) {\n\t\t\tWARN_ON(RPC_ASSASSINATED(task));\n\t\t\t/* Always release the RPC slot and buffer memory */\n\t\t\txprt_release(task);\n\t\t}\n\t}\n}\n\nvoid rpc_exit(struct rpc_task *task, int status)\n{\n\ttask->tk_status = status;\n\ttask->tk_action = rpc_exit_task;\n\tif (RPC_IS_QUEUED(task))\n\t\trpc_wake_up_queued_task(task->tk_waitqueue, task);\n}\nEXPORT_SYMBOL_GPL(rpc_exit);\n\nvoid rpc_release_calldata(const struct rpc_call_ops *ops, void *calldata)\n{\n\tif (ops->rpc_release != NULL)\n\t\tops->rpc_release(calldata);\n}\n\n/*\n * This is the RPC `scheduler' (or rather, the finite state machine).\n */\nstatic void __rpc_execute(struct rpc_task *task)\n{\n\tstruct rpc_wait_queue *queue;\n\tint task_is_async = RPC_IS_ASYNC(task);\n\tint status = 0;\n\n\tdprintk(\"RPC: %5u __rpc_execute flags=0x%x\\n\",\n\t\t\ttask->tk_pid, task->tk_flags);\n\n\tBUG_ON(RPC_IS_QUEUED(task));\n\n\tfor (;;) {\n\n\t\t/*\n\t\t * Execute any pending callback.\n\t\t */\n\t\tif (task->tk_callback) {\n\t\t\tvoid (*save_callback)(struct rpc_task *);\n\n\t\t\t/*\n\t\t\t * We set tk_callback to NULL before calling it,\n\t\t\t * in case it sets the tk_callback field itself:\n\t\t\t */\n\t\t\tsave_callback = task->tk_callback;\n\t\t\ttask->tk_callback = NULL;\n\t\t\tsave_callback(task);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Perform the next FSM step.\n\t\t\t * tk_action may be NULL when the task has been killed\n\t\t\t * by someone else.\n\t\t\t */\n\t\t\tif (task->tk_action == NULL)\n\t\t\t\tbreak;\n\t\t\ttask->tk_action(task);\n\t\t}\n\n\t\t/*\n\t\t * Lockless check for whether task is sleeping or not.\n\t\t */\n\t\tif (!RPC_IS_QUEUED(task))\n\t\t\tcontinue;\n\t\t/*\n\t\t * The queue->lock protects against races with\n\t\t * rpc_make_runnable().\n\t\t *\n\t\t * Note that once we clear RPC_TASK_RUNNING on an asynchronous\n\t\t * rpc_task, rpc_make_runnable() can assign it to a\n\t\t * different workqueue. We therefore cannot assume that the\n\t\t * rpc_task pointer may still be dereferenced.\n\t\t */\n\t\tqueue = task->tk_waitqueue;\n\t\tspin_lock_bh(&queue->lock);\n\t\tif (!RPC_IS_QUEUED(task)) {\n\t\t\tspin_unlock_bh(&queue->lock);\n\t\t\tcontinue;\n\t\t}\n\t\trpc_clear_running(task);\n\t\tspin_unlock_bh(&queue->lock);\n\t\tif (task_is_async)\n\t\t\treturn;\n\n\t\t/* sync task: sleep here */\n\t\tdprintk(\"RPC: %5u sync task going to sleep\\n\", task->tk_pid);\n\t\tstatus = out_of_line_wait_on_bit(&task->tk_runstate,\n\t\t\t\tRPC_TASK_QUEUED, rpc_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE);\n\t\tif (status == -ERESTARTSYS) {\n\t\t\t/*\n\t\t\t * When a sync task receives a signal, it exits with\n\t\t\t * -ERESTARTSYS. In order to catch any callbacks that\n\t\t\t * clean up after sleeping on some queue, we don't\n\t\t\t * break the loop here, but go around once more.\n\t\t\t */\n\t\t\tdprintk(\"RPC: %5u got signal\\n\", task->tk_pid);\n\t\t\ttask->tk_flags |= RPC_TASK_KILLED;\n\t\t\trpc_exit(task, -ERESTARTSYS);\n\t\t}\n\t\trpc_set_running(task);\n\t\tdprintk(\"RPC: %5u sync task resuming\\n\", task->tk_pid);\n\t}\n\n\tdprintk(\"RPC: %5u return %d, status %d\\n\", task->tk_pid, status,\n\t\t\ttask->tk_status);\n\t/* Release all resources associated with the task */\n\trpc_release_task(task);\n}\n\n/*\n * User-visible entry point to the scheduler.\n *\n * This may be called recursively if e.g. an async NFS task updates\n * the attributes and finds that dirty pages must be flushed.\n * NOTE: Upon exit of this function the task is guaranteed to be\n *\t released. In particular note that tk_release() will have\n *\t been called, so your task memory may have been freed.\n */\nvoid rpc_execute(struct rpc_task *task)\n{\n\trpc_set_active(task);\n\trpc_make_runnable(task);\n\tif (!RPC_IS_ASYNC(task))\n\t\t__rpc_execute(task);\n}\n\nstatic void rpc_async_schedule(struct work_struct *work)\n{\n\t__rpc_execute(container_of(work, struct rpc_task, u.tk_work));\n}\n\n/**\n * rpc_malloc - allocate an RPC buffer\n * @task: RPC task that will use this buffer\n * @size: requested byte size\n *\n * To prevent rpciod from hanging, this allocator never sleeps,\n * returning NULL if the request cannot be serviced immediately.\n * The caller can arrange to sleep in a way that is safe for rpciod.\n *\n * Most requests are 'small' (under 2KiB) and can be serviced from a\n * mempool, ensuring that NFS reads and writes can always proceed,\n * and that there is good locality of reference for these buffers.\n *\n * In order to avoid memory starvation triggering more writebacks of\n * NFS requests, we avoid using GFP_KERNEL.\n */\nvoid *rpc_malloc(struct rpc_task *task, size_t size)\n{\n\tstruct rpc_buffer *buf;\n\tgfp_t gfp = RPC_IS_SWAPPER(task) ? GFP_ATOMIC : GFP_NOWAIT;\n\n\tsize += sizeof(struct rpc_buffer);\n\tif (size <= RPC_BUFFER_MAXSIZE)\n\t\tbuf = mempool_alloc(rpc_buffer_mempool, gfp);\n\telse\n\t\tbuf = kmalloc(size, gfp);\n\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->len = size;\n\tdprintk(\"RPC: %5u allocated buffer of size %zu at %p\\n\",\n\t\t\ttask->tk_pid, size, buf);\n\treturn &buf->data;\n}\nEXPORT_SYMBOL_GPL(rpc_malloc);\n\n/**\n * rpc_free - free buffer allocated via rpc_malloc\n * @buffer: buffer to free\n *\n */\nvoid rpc_free(void *buffer)\n{\n\tsize_t size;\n\tstruct rpc_buffer *buf;\n\n\tif (!buffer)\n\t\treturn;\n\n\tbuf = container_of(buffer, struct rpc_buffer, data);\n\tsize = buf->len;\n\n\tdprintk(\"RPC:       freeing buffer of size %zu at %p\\n\",\n\t\t\tsize, buf);\n\n\tif (size <= RPC_BUFFER_MAXSIZE)\n\t\tmempool_free(buf, rpc_buffer_mempool);\n\telse\n\t\tkfree(buf);\n}\nEXPORT_SYMBOL_GPL(rpc_free);\n\n/*\n * Creation and deletion of RPC task structures\n */\nstatic void rpc_init_task(struct rpc_task *task, const struct rpc_task_setup *task_setup_data)\n{\n\tmemset(task, 0, sizeof(*task));\n\tatomic_set(&task->tk_count, 1);\n\ttask->tk_flags  = task_setup_data->flags;\n\ttask->tk_ops = task_setup_data->callback_ops;\n\ttask->tk_calldata = task_setup_data->callback_data;\n\tINIT_LIST_HEAD(&task->tk_task);\n\n\t/* Initialize retry counters */\n\ttask->tk_garb_retry = 2;\n\ttask->tk_cred_retry = 2;\n\n\ttask->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;\n\ttask->tk_owner = current->tgid;\n\n\t/* Initialize workqueue for async tasks */\n\ttask->tk_workqueue = task_setup_data->workqueue;\n\n\tif (task->tk_ops->rpc_call_prepare != NULL)\n\t\ttask->tk_action = rpc_prepare_task;\n\n\t/* starting timestamp */\n\ttask->tk_start = ktime_get();\n\n\tdprintk(\"RPC:       new task initialized, procpid %u\\n\",\n\t\t\t\ttask_pid_nr(current));\n}\n\nstatic struct rpc_task *\nrpc_alloc_task(void)\n{\n\treturn (struct rpc_task *)mempool_alloc(rpc_task_mempool, GFP_NOFS);\n}\n\n/*\n * Create a new task for the specified client.\n */\nstruct rpc_task *rpc_new_task(const struct rpc_task_setup *setup_data)\n{\n\tstruct rpc_task\t*task = setup_data->task;\n\tunsigned short flags = 0;\n\n\tif (task == NULL) {\n\t\ttask = rpc_alloc_task();\n\t\tif (task == NULL) {\n\t\t\trpc_release_calldata(setup_data->callback_ops,\n\t\t\t\t\tsetup_data->callback_data);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tflags = RPC_TASK_DYNAMIC;\n\t}\n\n\trpc_init_task(task, setup_data);\n\ttask->tk_flags |= flags;\n\tdprintk(\"RPC:       allocated task %p\\n\", task);\n\treturn task;\n}\n\nstatic void rpc_free_task(struct rpc_task *task)\n{\n\tconst struct rpc_call_ops *tk_ops = task->tk_ops;\n\tvoid *calldata = task->tk_calldata;\n\n\tif (task->tk_flags & RPC_TASK_DYNAMIC) {\n\t\tdprintk(\"RPC: %5u freeing task\\n\", task->tk_pid);\n\t\tmempool_free(task, rpc_task_mempool);\n\t}\n\trpc_release_calldata(tk_ops, calldata);\n}\n\nstatic void rpc_async_release(struct work_struct *work)\n{\n\trpc_free_task(container_of(work, struct rpc_task, u.tk_work));\n}\n\nstatic void rpc_release_resources_task(struct rpc_task *task)\n{\n\tif (task->tk_rqstp)\n\t\txprt_release(task);\n\tif (task->tk_msg.rpc_cred) {\n\t\tput_rpccred(task->tk_msg.rpc_cred);\n\t\ttask->tk_msg.rpc_cred = NULL;\n\t}\n\trpc_task_release_client(task);\n}\n\nstatic void rpc_final_put_task(struct rpc_task *task,\n\t\tstruct workqueue_struct *q)\n{\n\tif (q != NULL) {\n\t\tINIT_WORK(&task->u.tk_work, rpc_async_release);\n\t\tqueue_work(q, &task->u.tk_work);\n\t} else\n\t\trpc_free_task(task);\n}\n\nstatic void rpc_do_put_task(struct rpc_task *task, struct workqueue_struct *q)\n{\n\tif (atomic_dec_and_test(&task->tk_count)) {\n\t\trpc_release_resources_task(task);\n\t\trpc_final_put_task(task, q);\n\t}\n}\n\nvoid rpc_put_task(struct rpc_task *task)\n{\n\trpc_do_put_task(task, NULL);\n}\nEXPORT_SYMBOL_GPL(rpc_put_task);\n\nvoid rpc_put_task_async(struct rpc_task *task)\n{\n\trpc_do_put_task(task, task->tk_workqueue);\n}\nEXPORT_SYMBOL_GPL(rpc_put_task_async);\n\nstatic void rpc_release_task(struct rpc_task *task)\n{\n\tdprintk(\"RPC: %5u release task\\n\", task->tk_pid);\n\n\tBUG_ON (RPC_IS_QUEUED(task));\n\n\trpc_release_resources_task(task);\n\n\t/*\n\t * Note: at this point we have been removed from rpc_clnt->cl_tasks,\n\t * so it should be safe to use task->tk_count as a test for whether\n\t * or not any other processes still hold references to our rpc_task.\n\t */\n\tif (atomic_read(&task->tk_count) != 1 + !RPC_IS_ASYNC(task)) {\n\t\t/* Wake up anyone who may be waiting for task completion */\n\t\tif (!rpc_complete_task(task))\n\t\t\treturn;\n\t} else {\n\t\tif (!atomic_dec_and_test(&task->tk_count))\n\t\t\treturn;\n\t}\n\trpc_final_put_task(task, task->tk_workqueue);\n}\n\nint rpciod_up(void)\n{\n\treturn try_module_get(THIS_MODULE) ? 0 : -EINVAL;\n}\n\nvoid rpciod_down(void)\n{\n\tmodule_put(THIS_MODULE);\n}\n\n/*\n * Start up the rpciod workqueue.\n */\nstatic int rpciod_start(void)\n{\n\tstruct workqueue_struct *wq;\n\n\t/*\n\t * Create the rpciod thread and wait for it to start.\n\t */\n\tdprintk(\"RPC:       creating workqueue rpciod\\n\");\n\twq = alloc_workqueue(\"rpciod\", WQ_MEM_RECLAIM, 0);\n\trpciod_workqueue = wq;\n\treturn rpciod_workqueue != NULL;\n}\n\nstatic void rpciod_stop(void)\n{\n\tstruct workqueue_struct *wq = NULL;\n\n\tif (rpciod_workqueue == NULL)\n\t\treturn;\n\tdprintk(\"RPC:       destroying workqueue rpciod\\n\");\n\n\twq = rpciod_workqueue;\n\trpciod_workqueue = NULL;\n\tdestroy_workqueue(wq);\n}\n\nvoid\nrpc_destroy_mempool(void)\n{\n\trpciod_stop();\n\tif (rpc_buffer_mempool)\n\t\tmempool_destroy(rpc_buffer_mempool);\n\tif (rpc_task_mempool)\n\t\tmempool_destroy(rpc_task_mempool);\n\tif (rpc_task_slabp)\n\t\tkmem_cache_destroy(rpc_task_slabp);\n\tif (rpc_buffer_slabp)\n\t\tkmem_cache_destroy(rpc_buffer_slabp);\n\trpc_destroy_wait_queue(&delay_queue);\n}\n\nint\nrpc_init_mempool(void)\n{\n\t/*\n\t * The following is not strictly a mempool initialisation,\n\t * but there is no harm in doing it here\n\t */\n\trpc_init_wait_queue(&delay_queue, \"delayq\");\n\tif (!rpciod_start())\n\t\tgoto err_nomem;\n\n\trpc_task_slabp = kmem_cache_create(\"rpc_tasks\",\n\t\t\t\t\t     sizeof(struct rpc_task),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (!rpc_task_slabp)\n\t\tgoto err_nomem;\n\trpc_buffer_slabp = kmem_cache_create(\"rpc_buffers\",\n\t\t\t\t\t     RPC_BUFFER_MAXSIZE,\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (!rpc_buffer_slabp)\n\t\tgoto err_nomem;\n\trpc_task_mempool = mempool_create_slab_pool(RPC_TASK_POOLSIZE,\n\t\t\t\t\t\t    rpc_task_slabp);\n\tif (!rpc_task_mempool)\n\t\tgoto err_nomem;\n\trpc_buffer_mempool = mempool_create_slab_pool(RPC_BUFFER_POOLSIZE,\n\t\t\t\t\t\t      rpc_buffer_slabp);\n\tif (!rpc_buffer_mempool)\n\t\tgoto err_nomem;\n\treturn 0;\nerr_nomem:\n\trpc_destroy_mempool();\n\treturn -ENOMEM;\n}\n"], "fixing_code": ["/*\n * linux/fs/lockd/clntproc.c\n *\n * RPC procedures for the client side NLM implementation\n *\n * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/nfs_fs.h>\n#include <linux/utsname.h>\n#include <linux/freezer.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/svc.h>\n#include <linux/lockd/lockd.h>\n\n#define NLMDBG_FACILITY\t\tNLMDBG_CLIENT\n#define NLMCLNT_GRACE_WAIT\t(5*HZ)\n#define NLMCLNT_POLL_TIMEOUT\t(30*HZ)\n#define NLMCLNT_MAX_RETRIES\t3\n\nstatic int\tnlmclnt_test(struct nlm_rqst *, struct file_lock *);\nstatic int\tnlmclnt_lock(struct nlm_rqst *, struct file_lock *);\nstatic int\tnlmclnt_unlock(struct nlm_rqst *, struct file_lock *);\nstatic int\tnlm_stat_to_errno(__be32 stat);\nstatic void\tnlmclnt_locks_init_private(struct file_lock *fl, struct nlm_host *host);\nstatic int\tnlmclnt_cancel(struct nlm_host *, int , struct file_lock *);\n\nstatic const struct rpc_call_ops nlmclnt_unlock_ops;\nstatic const struct rpc_call_ops nlmclnt_cancel_ops;\n\n/*\n * Cookie counter for NLM requests\n */\nstatic atomic_t\tnlm_cookie = ATOMIC_INIT(0x1234);\n\nvoid nlmclnt_next_cookie(struct nlm_cookie *c)\n{\n\tu32\tcookie = atomic_inc_return(&nlm_cookie);\n\n\tmemcpy(c->data, &cookie, 4);\n\tc->len=4;\n}\n\nstatic struct nlm_lockowner *nlm_get_lockowner(struct nlm_lockowner *lockowner)\n{\n\tatomic_inc(&lockowner->count);\n\treturn lockowner;\n}\n\nstatic void nlm_put_lockowner(struct nlm_lockowner *lockowner)\n{\n\tif (!atomic_dec_and_lock(&lockowner->count, &lockowner->host->h_lock))\n\t\treturn;\n\tlist_del(&lockowner->list);\n\tspin_unlock(&lockowner->host->h_lock);\n\tnlmclnt_release_host(lockowner->host);\n\tkfree(lockowner);\n}\n\nstatic inline int nlm_pidbusy(struct nlm_host *host, uint32_t pid)\n{\n\tstruct nlm_lockowner *lockowner;\n\tlist_for_each_entry(lockowner, &host->h_lockowners, list) {\n\t\tif (lockowner->pid == pid)\n\t\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\n\nstatic inline uint32_t __nlm_alloc_pid(struct nlm_host *host)\n{\n\tuint32_t res;\n\tdo {\n\t\tres = host->h_pidcount++;\n\t} while (nlm_pidbusy(host, res) < 0);\n\treturn res;\n}\n\nstatic struct nlm_lockowner *__nlm_find_lockowner(struct nlm_host *host, fl_owner_t owner)\n{\n\tstruct nlm_lockowner *lockowner;\n\tlist_for_each_entry(lockowner, &host->h_lockowners, list) {\n\t\tif (lockowner->owner != owner)\n\t\t\tcontinue;\n\t\treturn nlm_get_lockowner(lockowner);\n\t}\n\treturn NULL;\n}\n\nstatic struct nlm_lockowner *nlm_find_lockowner(struct nlm_host *host, fl_owner_t owner)\n{\n\tstruct nlm_lockowner *res, *new = NULL;\n\n\tspin_lock(&host->h_lock);\n\tres = __nlm_find_lockowner(host, owner);\n\tif (res == NULL) {\n\t\tspin_unlock(&host->h_lock);\n\t\tnew = kmalloc(sizeof(*new), GFP_KERNEL);\n\t\tspin_lock(&host->h_lock);\n\t\tres = __nlm_find_lockowner(host, owner);\n\t\tif (res == NULL && new != NULL) {\n\t\t\tres = new;\n\t\t\tatomic_set(&new->count, 1);\n\t\t\tnew->owner = owner;\n\t\t\tnew->pid = __nlm_alloc_pid(host);\n\t\t\tnew->host = nlm_get_host(host);\n\t\t\tlist_add(&new->list, &host->h_lockowners);\n\t\t\tnew = NULL;\n\t\t}\n\t}\n\tspin_unlock(&host->h_lock);\n\tkfree(new);\n\treturn res;\n}\n\n/*\n * Initialize arguments for TEST/LOCK/UNLOCK/CANCEL calls\n */\nstatic void nlmclnt_setlockargs(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tstruct nlm_args\t*argp = &req->a_args;\n\tstruct nlm_lock\t*lock = &argp->lock;\n\n\tnlmclnt_next_cookie(&argp->cookie);\n\tmemcpy(&lock->fh, NFS_FH(fl->fl_file->f_path.dentry->d_inode), sizeof(struct nfs_fh));\n\tlock->caller  = utsname()->nodename;\n\tlock->oh.data = req->a_owner;\n\tlock->oh.len  = snprintf(req->a_owner, sizeof(req->a_owner), \"%u@%s\",\n\t\t\t\t(unsigned int)fl->fl_u.nfs_fl.owner->pid,\n\t\t\t\tutsname()->nodename);\n\tlock->svid = fl->fl_u.nfs_fl.owner->pid;\n\tlock->fl.fl_start = fl->fl_start;\n\tlock->fl.fl_end = fl->fl_end;\n\tlock->fl.fl_type = fl->fl_type;\n}\n\nstatic void nlmclnt_release_lockargs(struct nlm_rqst *req)\n{\n\tBUG_ON(req->a_args.lock.fl.fl_ops != NULL);\n}\n\n/**\n * nlmclnt_proc - Perform a single client-side lock request\n * @host: address of a valid nlm_host context representing the NLM server\n * @cmd: fcntl-style file lock operation to perform\n * @fl: address of arguments for the lock operation\n *\n */\nint nlmclnt_proc(struct nlm_host *host, int cmd, struct file_lock *fl)\n{\n\tstruct nlm_rqst\t\t*call;\n\tint\t\t\tstatus;\n\n\tnlm_get_host(host);\n\tcall = nlm_alloc_call(host);\n\tif (call == NULL)\n\t\treturn -ENOMEM;\n\n\tnlmclnt_locks_init_private(fl, host);\n\t/* Set up the argument struct */\n\tnlmclnt_setlockargs(call, fl);\n\n\tif (IS_SETLK(cmd) || IS_SETLKW(cmd)) {\n\t\tif (fl->fl_type != F_UNLCK) {\n\t\t\tcall->a_args.block = IS_SETLKW(cmd) ? 1 : 0;\n\t\t\tstatus = nlmclnt_lock(call, fl);\n\t\t} else\n\t\t\tstatus = nlmclnt_unlock(call, fl);\n\t} else if (IS_GETLK(cmd))\n\t\tstatus = nlmclnt_test(call, fl);\n\telse\n\t\tstatus = -EINVAL;\n\tfl->fl_ops->fl_release_private(fl);\n\tfl->fl_ops = NULL;\n\n\tdprintk(\"lockd: clnt proc returns %d\\n\", status);\n\treturn status;\n}\nEXPORT_SYMBOL_GPL(nlmclnt_proc);\n\n/*\n * Allocate an NLM RPC call struct\n *\n * Note: the caller must hold a reference to host. In case of failure,\n * this reference will be released.\n */\nstruct nlm_rqst *nlm_alloc_call(struct nlm_host *host)\n{\n\tstruct nlm_rqst\t*call;\n\n\tfor(;;) {\n\t\tcall = kzalloc(sizeof(*call), GFP_KERNEL);\n\t\tif (call != NULL) {\n\t\t\tatomic_set(&call->a_count, 1);\n\t\t\tlocks_init_lock(&call->a_args.lock.fl);\n\t\t\tlocks_init_lock(&call->a_res.lock.fl);\n\t\t\tcall->a_host = host;\n\t\t\treturn call;\n\t\t}\n\t\tif (signalled())\n\t\t\tbreak;\n\t\tprintk(\"nlm_alloc_call: failed, waiting for memory\\n\");\n\t\tschedule_timeout_interruptible(5*HZ);\n\t}\n\tnlmclnt_release_host(host);\n\treturn NULL;\n}\n\nvoid nlmclnt_release_call(struct nlm_rqst *call)\n{\n\tif (!atomic_dec_and_test(&call->a_count))\n\t\treturn;\n\tnlmclnt_release_host(call->a_host);\n\tnlmclnt_release_lockargs(call);\n\tkfree(call);\n}\n\nstatic void nlmclnt_rpc_release(void *data)\n{\n\tnlmclnt_release_call(data);\n}\n\nstatic int nlm_wait_on_grace(wait_queue_head_t *queue)\n{\n\tDEFINE_WAIT(wait);\n\tint status = -EINTR;\n\n\tprepare_to_wait(queue, &wait, TASK_INTERRUPTIBLE);\n\tif (!signalled ()) {\n\t\tschedule_timeout(NLMCLNT_GRACE_WAIT);\n\t\ttry_to_freeze();\n\t\tif (!signalled ())\n\t\t\tstatus = 0;\n\t}\n\tfinish_wait(queue, &wait);\n\treturn status;\n}\n\n/*\n * Generic NLM call\n */\nstatic int\nnlmclnt_call(struct rpc_cred *cred, struct nlm_rqst *req, u32 proc)\n{\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct rpc_clnt\t*clnt;\n\tstruct nlm_args\t*argp = &req->a_args;\n\tstruct nlm_res\t*resp = &req->a_res;\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= argp,\n\t\t.rpc_resp\t= resp,\n\t\t.rpc_cred\t= cred,\n\t};\n\tint\t\tstatus;\n\n\tdprintk(\"lockd: call procedure %d on %s\\n\",\n\t\t\t(int)proc, host->h_name);\n\n\tdo {\n\t\tif (host->h_reclaiming && !argp->reclaim)\n\t\t\tgoto in_grace_period;\n\n\t\t/* If we have no RPC client yet, create one. */\n\t\tif ((clnt = nlm_bind_host(host)) == NULL)\n\t\t\treturn -ENOLCK;\n\t\tmsg.rpc_proc = &clnt->cl_procinfo[proc];\n\n\t\t/* Perform the RPC call. If an error occurs, try again */\n\t\tif ((status = rpc_call_sync(clnt, &msg, 0)) < 0) {\n\t\t\tdprintk(\"lockd: rpc_call returned error %d\\n\", -status);\n\t\t\tswitch (status) {\n\t\t\tcase -EPROTONOSUPPORT:\n\t\t\t\tstatus = -EINVAL;\n\t\t\t\tbreak;\n\t\t\tcase -ECONNREFUSED:\n\t\t\tcase -ETIMEDOUT:\n\t\t\tcase -ENOTCONN:\n\t\t\t\tnlm_rebind_host(host);\n\t\t\t\tstatus = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\tcase -ERESTARTSYS:\n\t\t\t\treturn signalled () ? -EINTR : status;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\t} else\n\t\tif (resp->status == nlm_lck_denied_grace_period) {\n\t\t\tdprintk(\"lockd: server in grace period\\n\");\n\t\t\tif (argp->reclaim) {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t     \"lockd: spurious grace period reject?!\\n\");\n\t\t\t\treturn -ENOLCK;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!argp->reclaim) {\n\t\t\t\t/* We appear to be out of the grace period */\n\t\t\t\twake_up_all(&host->h_gracewait);\n\t\t\t}\n\t\t\tdprintk(\"lockd: server returns status %d\\n\", resp->status);\n\t\t\treturn 0;\t/* Okay, call complete */\n\t\t}\n\nin_grace_period:\n\t\t/*\n\t\t * The server has rebooted and appears to be in the grace\n\t\t * period during which locks are only allowed to be\n\t\t * reclaimed.\n\t\t * We can only back off and try again later.\n\t\t */\n\t\tstatus = nlm_wait_on_grace(&host->h_gracewait);\n\t} while (status == 0);\n\n\treturn status;\n}\n\n/*\n * Generic NLM call, async version.\n */\nstatic struct rpc_task *__nlm_async_call(struct nlm_rqst *req, u32 proc, struct rpc_message *msg, const struct rpc_call_ops *tk_ops)\n{\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct rpc_clnt\t*clnt;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_message = msg,\n\t\t.callback_ops = tk_ops,\n\t\t.callback_data = req,\n\t\t.flags = RPC_TASK_ASYNC,\n\t};\n\n\tdprintk(\"lockd: call procedure %d on %s (async)\\n\",\n\t\t\t(int)proc, host->h_name);\n\n\t/* If we have no RPC client yet, create one. */\n\tclnt = nlm_bind_host(host);\n\tif (clnt == NULL)\n\t\tgoto out_err;\n\tmsg->rpc_proc = &clnt->cl_procinfo[proc];\n\ttask_setup_data.rpc_client = clnt;\n\n        /* bootstrap and kick off the async RPC call */\n\treturn rpc_run_task(&task_setup_data);\nout_err:\n\ttk_ops->rpc_release(req);\n\treturn ERR_PTR(-ENOLCK);\n}\n\nstatic int nlm_do_async_call(struct nlm_rqst *req, u32 proc, struct rpc_message *msg, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_task *task;\n\n\ttask = __nlm_async_call(req, proc, msg, tk_ops);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\trpc_put_task(task);\n\treturn 0;\n}\n\n/*\n * NLM asynchronous call.\n */\nint nlm_async_call(struct nlm_rqst *req, u32 proc, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= &req->a_args,\n\t\t.rpc_resp\t= &req->a_res,\n\t};\n\treturn nlm_do_async_call(req, proc, &msg, tk_ops);\n}\n\nint nlm_async_reply(struct nlm_rqst *req, u32 proc, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= &req->a_res,\n\t};\n\treturn nlm_do_async_call(req, proc, &msg, tk_ops);\n}\n\n/*\n * NLM client asynchronous call.\n *\n * Note that although the calls are asynchronous, and are therefore\n *      guaranteed to complete, we still always attempt to wait for\n *      completion in order to be able to correctly track the lock\n *      state.\n */\nstatic int nlmclnt_async_call(struct rpc_cred *cred, struct nlm_rqst *req, u32 proc, const struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_argp\t= &req->a_args,\n\t\t.rpc_resp\t= &req->a_res,\n\t\t.rpc_cred\t= cred,\n\t};\n\tstruct rpc_task *task;\n\tint err;\n\n\ttask = __nlm_async_call(req, proc, &msg, tk_ops);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\terr = rpc_wait_for_completion_task(task);\n\trpc_put_task(task);\n\treturn err;\n}\n\n/*\n * TEST for the presence of a conflicting lock\n */\nstatic int\nnlmclnt_test(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tint\tstatus;\n\n\tstatus = nlmclnt_call(nfs_file_cred(fl->fl_file), req, NLMPROC_TEST);\n\tif (status < 0)\n\t\tgoto out;\n\n\tswitch (req->a_res.status) {\n\t\tcase nlm_granted:\n\t\t\tfl->fl_type = F_UNLCK;\n\t\t\tbreak;\n\t\tcase nlm_lck_denied:\n\t\t\t/*\n\t\t\t * Report the conflicting lock back to the application.\n\t\t\t */\n\t\t\tfl->fl_start = req->a_res.lock.fl.fl_start;\n\t\t\tfl->fl_end = req->a_res.lock.fl.fl_end;\n\t\t\tfl->fl_type = req->a_res.lock.fl.fl_type;\n\t\t\tfl->fl_pid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = nlm_stat_to_errno(req->a_res.status);\n\t}\nout:\n\tnlmclnt_release_call(req);\n\treturn status;\n}\n\nstatic void nlmclnt_locks_copy_lock(struct file_lock *new, struct file_lock *fl)\n{\n\tspin_lock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n\tnew->fl_u.nfs_fl.state = fl->fl_u.nfs_fl.state;\n\tnew->fl_u.nfs_fl.owner = nlm_get_lockowner(fl->fl_u.nfs_fl.owner);\n\tlist_add_tail(&new->fl_u.nfs_fl.list, &fl->fl_u.nfs_fl.owner->host->h_granted);\n\tspin_unlock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n}\n\nstatic void nlmclnt_locks_release_private(struct file_lock *fl)\n{\n\tspin_lock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n\tlist_del(&fl->fl_u.nfs_fl.list);\n\tspin_unlock(&fl->fl_u.nfs_fl.owner->host->h_lock);\n\tnlm_put_lockowner(fl->fl_u.nfs_fl.owner);\n}\n\nstatic const struct file_lock_operations nlmclnt_lock_ops = {\n\t.fl_copy_lock = nlmclnt_locks_copy_lock,\n\t.fl_release_private = nlmclnt_locks_release_private,\n};\n\nstatic void nlmclnt_locks_init_private(struct file_lock *fl, struct nlm_host *host)\n{\n\tBUG_ON(fl->fl_ops != NULL);\n\tfl->fl_u.nfs_fl.state = 0;\n\tfl->fl_u.nfs_fl.owner = nlm_find_lockowner(host, fl->fl_owner);\n\tINIT_LIST_HEAD(&fl->fl_u.nfs_fl.list);\n\tfl->fl_ops = &nlmclnt_lock_ops;\n}\n\nstatic int do_vfs_lock(struct file_lock *fl)\n{\n\tint res = 0;\n\tswitch (fl->fl_flags & (FL_POSIX|FL_FLOCK)) {\n\t\tcase FL_POSIX:\n\t\t\tres = posix_lock_file_wait(fl->fl_file, fl);\n\t\t\tbreak;\n\t\tcase FL_FLOCK:\n\t\t\tres = flock_lock_file_wait(fl->fl_file, fl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t}\n\treturn res;\n}\n\n/*\n * LOCK: Try to create a lock\n *\n *\t\t\tProgrammer Harassment Alert\n *\n * When given a blocking lock request in a sync RPC call, the HPUX lockd\n * will faithfully return LCK_BLOCKED but never cares to notify us when\n * the lock could be granted. This way, our local process could hang\n * around forever waiting for the callback.\n *\n *  Solution A:\tImplement busy-waiting\n *  Solution B: Use the async version of the call (NLM_LOCK_{MSG,RES})\n *\n * For now I am implementing solution A, because I hate the idea of\n * re-implementing lockd for a third time in two months. The async\n * calls shouldn't be too hard to do, however.\n *\n * This is one of the lovely things about standards in the NFS area:\n * they're so soft and squishy you can't really blame HP for doing this.\n */\nstatic int\nnlmclnt_lock(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tstruct rpc_cred *cred = nfs_file_cred(fl->fl_file);\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct nlm_res\t*resp = &req->a_res;\n\tstruct nlm_wait *block = NULL;\n\tunsigned char fl_flags = fl->fl_flags;\n\tunsigned char fl_type;\n\tint status = -ENOLCK;\n\n\tif (nsm_monitor(host) < 0)\n\t\tgoto out;\n\treq->a_args.state = nsm_local_state;\n\n\tfl->fl_flags |= FL_ACCESS;\n\tstatus = do_vfs_lock(fl);\n\tfl->fl_flags = fl_flags;\n\tif (status < 0)\n\t\tgoto out;\n\n\tblock = nlmclnt_prepare_block(host, fl);\nagain:\n\t/*\n\t * Initialise resp->status to a valid non-zero value,\n\t * since 0 == nlm_lck_granted\n\t */\n\tresp->status = nlm_lck_blocked;\n\tfor(;;) {\n\t\t/* Reboot protection */\n\t\tfl->fl_u.nfs_fl.state = host->h_state;\n\t\tstatus = nlmclnt_call(cred, req, NLMPROC_LOCK);\n\t\tif (status < 0)\n\t\t\tbreak;\n\t\t/* Did a reclaimer thread notify us of a server reboot? */\n\t\tif (resp->status ==  nlm_lck_denied_grace_period)\n\t\t\tcontinue;\n\t\tif (resp->status != nlm_lck_blocked)\n\t\t\tbreak;\n\t\t/* Wait on an NLM blocking lock */\n\t\tstatus = nlmclnt_block(block, req, NLMCLNT_POLL_TIMEOUT);\n\t\tif (status < 0)\n\t\t\tbreak;\n\t\tif (resp->status != nlm_lck_blocked)\n\t\t\tbreak;\n\t}\n\n\t/* if we were interrupted while blocking, then cancel the lock request\n\t * and exit\n\t */\n\tif (resp->status == nlm_lck_blocked) {\n\t\tif (!req->a_args.block)\n\t\t\tgoto out_unlock;\n\t\tif (nlmclnt_cancel(host, req->a_args.block, fl) == 0)\n\t\t\tgoto out_unblock;\n\t}\n\n\tif (resp->status == nlm_granted) {\n\t\tdown_read(&host->h_rwsem);\n\t\t/* Check whether or not the server has rebooted */\n\t\tif (fl->fl_u.nfs_fl.state != host->h_state) {\n\t\t\tup_read(&host->h_rwsem);\n\t\t\tgoto again;\n\t\t}\n\t\t/* Ensure the resulting lock will get added to granted list */\n\t\tfl->fl_flags |= FL_SLEEP;\n\t\tif (do_vfs_lock(fl) < 0)\n\t\t\tprintk(KERN_WARNING \"%s: VFS is out of sync with lock manager!\\n\", __func__);\n\t\tup_read(&host->h_rwsem);\n\t\tfl->fl_flags = fl_flags;\n\t\tstatus = 0;\n\t}\n\tif (status < 0)\n\t\tgoto out_unlock;\n\t/*\n\t * EAGAIN doesn't make sense for sleeping locks, and in some\n\t * cases NLM_LCK_DENIED is returned for a permanent error.  So\n\t * turn it into an ENOLCK.\n\t */\n\tif (resp->status == nlm_lck_denied && (fl_flags & FL_SLEEP))\n\t\tstatus = -ENOLCK;\n\telse\n\t\tstatus = nlm_stat_to_errno(resp->status);\nout_unblock:\n\tnlmclnt_finish_block(block);\nout:\n\tnlmclnt_release_call(req);\n\treturn status;\nout_unlock:\n\t/* Fatal error: ensure that we remove the lock altogether */\n\tdprintk(\"lockd: lock attempt ended in fatal error.\\n\"\n\t\t\"       Attempting to unlock.\\n\");\n\tnlmclnt_finish_block(block);\n\tfl_type = fl->fl_type;\n\tfl->fl_type = F_UNLCK;\n\tdown_read(&host->h_rwsem);\n\tdo_vfs_lock(fl);\n\tup_read(&host->h_rwsem);\n\tfl->fl_type = fl_type;\n\tfl->fl_flags = fl_flags;\n\tnlmclnt_async_call(cred, req, NLMPROC_UNLOCK, &nlmclnt_unlock_ops);\n\treturn status;\n}\n\n/*\n * RECLAIM: Try to reclaim a lock\n */\nint\nnlmclnt_reclaim(struct nlm_host *host, struct file_lock *fl)\n{\n\tstruct nlm_rqst reqst, *req;\n\tint\t\tstatus;\n\n\treq = &reqst;\n\tmemset(req, 0, sizeof(*req));\n\tlocks_init_lock(&req->a_args.lock.fl);\n\tlocks_init_lock(&req->a_res.lock.fl);\n\treq->a_host  = host;\n\treq->a_flags = 0;\n\n\t/* Set up the argument struct */\n\tnlmclnt_setlockargs(req, fl);\n\treq->a_args.reclaim = 1;\n\n\tstatus = nlmclnt_call(nfs_file_cred(fl->fl_file), req, NLMPROC_LOCK);\n\tif (status >= 0 && req->a_res.status == nlm_granted)\n\t\treturn 0;\n\n\tprintk(KERN_WARNING \"lockd: failed to reclaim lock for pid %d \"\n\t\t\t\t\"(errno %d, status %d)\\n\", fl->fl_pid,\n\t\t\t\tstatus, ntohl(req->a_res.status));\n\n\t/*\n\t * FIXME: This is a serious failure. We can\n\t *\n\t *  a.\tIgnore the problem\n\t *  b.\tSend the owning process some signal (Linux doesn't have\n\t *\tSIGLOST, though...)\n\t *  c.\tRetry the operation\n\t *\n\t * Until someone comes up with a simple implementation\n\t * for b or c, I'll choose option a.\n\t */\n\n\treturn -ENOLCK;\n}\n\n/*\n * UNLOCK: remove an existing lock\n */\nstatic int\nnlmclnt_unlock(struct nlm_rqst *req, struct file_lock *fl)\n{\n\tstruct nlm_host\t*host = req->a_host;\n\tstruct nlm_res\t*resp = &req->a_res;\n\tint status;\n\tunsigned char fl_flags = fl->fl_flags;\n\n\t/*\n\t * Note: the server is supposed to either grant us the unlock\n\t * request, or to deny it with NLM_LCK_DENIED_GRACE_PERIOD. In either\n\t * case, we want to unlock.\n\t */\n\tfl->fl_flags |= FL_EXISTS;\n\tdown_read(&host->h_rwsem);\n\tstatus = do_vfs_lock(fl);\n\tup_read(&host->h_rwsem);\n\tfl->fl_flags = fl_flags;\n\tif (status == -ENOENT) {\n\t\tstatus = 0;\n\t\tgoto out;\n\t}\n\n\tatomic_inc(&req->a_count);\n\tstatus = nlmclnt_async_call(nfs_file_cred(fl->fl_file), req,\n\t\t\tNLMPROC_UNLOCK, &nlmclnt_unlock_ops);\n\tif (status < 0)\n\t\tgoto out;\n\n\tif (resp->status == nlm_granted)\n\t\tgoto out;\n\n\tif (resp->status != nlm_lck_denied_nolocks)\n\t\tprintk(\"lockd: unexpected unlock status: %d\\n\", resp->status);\n\t/* What to do now? I'm out of my depth... */\n\tstatus = -ENOLCK;\nout:\n\tnlmclnt_release_call(req);\n\treturn status;\n}\n\nstatic void nlmclnt_unlock_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t*req = data;\n\tu32 status = ntohl(req->a_res.status);\n\n\tif (RPC_ASSASSINATED(task))\n\t\tgoto die;\n\n\tif (task->tk_status < 0) {\n\t\tdprintk(\"lockd: unlock failed (err = %d)\\n\", -task->tk_status);\n\t\tswitch (task->tk_status) {\n\t\tcase -EACCES:\n\t\tcase -EIO:\n\t\t\tgoto die;\n\t\tdefault:\n\t\t\tgoto retry_rebind;\n\t\t}\n\t}\n\tif (status == NLM_LCK_DENIED_GRACE_PERIOD) {\n\t\trpc_delay(task, NLMCLNT_GRACE_WAIT);\n\t\tgoto retry_unlock;\n\t}\n\tif (status != NLM_LCK_GRANTED)\n\t\tprintk(KERN_WARNING \"lockd: unexpected unlock status: %d\\n\", status);\ndie:\n\treturn;\n retry_rebind:\n\tnlm_rebind_host(req->a_host);\n retry_unlock:\n\trpc_restart_call(task);\n}\n\nstatic const struct rpc_call_ops nlmclnt_unlock_ops = {\n\t.rpc_call_done = nlmclnt_unlock_callback,\n\t.rpc_release = nlmclnt_rpc_release,\n};\n\n/*\n * Cancel a blocked lock request.\n * We always use an async RPC call for this in order not to hang a\n * process that has been Ctrl-C'ed.\n */\nstatic int nlmclnt_cancel(struct nlm_host *host, int block, struct file_lock *fl)\n{\n\tstruct nlm_rqst\t*req;\n\tint status;\n\n\tdprintk(\"lockd: blocking lock attempt was interrupted by a signal.\\n\"\n\t\t\"       Attempting to cancel lock.\\n\");\n\n\treq = nlm_alloc_call(nlm_get_host(host));\n\tif (!req)\n\t\treturn -ENOMEM;\n\treq->a_flags = RPC_TASK_ASYNC;\n\n\tnlmclnt_setlockargs(req, fl);\n\treq->a_args.block = block;\n\n\tatomic_inc(&req->a_count);\n\tstatus = nlmclnt_async_call(nfs_file_cred(fl->fl_file), req,\n\t\t\tNLMPROC_CANCEL, &nlmclnt_cancel_ops);\n\tif (status == 0 && req->a_res.status == nlm_lck_denied)\n\t\tstatus = -ENOLCK;\n\tnlmclnt_release_call(req);\n\treturn status;\n}\n\nstatic void nlmclnt_cancel_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t*req = data;\n\tu32 status = ntohl(req->a_res.status);\n\n\tif (RPC_ASSASSINATED(task))\n\t\tgoto die;\n\n\tif (task->tk_status < 0) {\n\t\tdprintk(\"lockd: CANCEL call error %d, retrying.\\n\",\n\t\t\t\t\ttask->tk_status);\n\t\tgoto retry_cancel;\n\t}\n\n\tdprintk(\"lockd: cancel status %u (task %u)\\n\",\n\t\t\tstatus, task->tk_pid);\n\n\tswitch (status) {\n\tcase NLM_LCK_GRANTED:\n\tcase NLM_LCK_DENIED_GRACE_PERIOD:\n\tcase NLM_LCK_DENIED:\n\t\t/* Everything's good */\n\t\tbreak;\n\tcase NLM_LCK_DENIED_NOLOCKS:\n\t\tdprintk(\"lockd: CANCEL failed (server has no locks)\\n\");\n\t\tgoto retry_cancel;\n\tdefault:\n\t\tprintk(KERN_NOTICE \"lockd: weird return %d for CANCEL call\\n\",\n\t\t\tstatus);\n\t}\n\ndie:\n\treturn;\n\nretry_cancel:\n\t/* Don't ever retry more than 3 times */\n\tif (req->a_retries++ >= NLMCLNT_MAX_RETRIES)\n\t\tgoto die;\n\tnlm_rebind_host(req->a_host);\n\trpc_restart_call(task);\n\trpc_delay(task, 30 * HZ);\n}\n\nstatic const struct rpc_call_ops nlmclnt_cancel_ops = {\n\t.rpc_call_done = nlmclnt_cancel_callback,\n\t.rpc_release = nlmclnt_rpc_release,\n};\n\n/*\n * Convert an NLM status code to a generic kernel errno\n */\nstatic int\nnlm_stat_to_errno(__be32 status)\n{\n\tswitch(ntohl(status)) {\n\tcase NLM_LCK_GRANTED:\n\t\treturn 0;\n\tcase NLM_LCK_DENIED:\n\t\treturn -EAGAIN;\n\tcase NLM_LCK_DENIED_NOLOCKS:\n\tcase NLM_LCK_DENIED_GRACE_PERIOD:\n\t\treturn -ENOLCK;\n\tcase NLM_LCK_BLOCKED:\n\t\tprintk(KERN_NOTICE \"lockd: unexpected status NLM_BLOCKED\\n\");\n\t\treturn -ENOLCK;\n#ifdef CONFIG_LOCKD_V4\n\tcase NLM_DEADLCK:\n\t\treturn -EDEADLK;\n\tcase NLM_ROFS:\n\t\treturn -EROFS;\n\tcase NLM_STALE_FH:\n\t\treturn -ESTALE;\n\tcase NLM_FBIG:\n\t\treturn -EOVERFLOW;\n\tcase NLM_FAILED:\n\t\treturn -ENOLCK;\n#endif\n\t}\n\tprintk(KERN_NOTICE \"lockd: unexpected server status %d\\n\", status);\n\treturn -ENOLCK;\n}\n", "/*\n * linux/include/linux/sunrpc/sched.h\n *\n * Scheduling primitives for kernel Sun RPC.\n *\n * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>\n */\n\n#ifndef _LINUX_SUNRPC_SCHED_H_\n#define _LINUX_SUNRPC_SCHED_H_\n\n#include <linux/timer.h>\n#include <linux/ktime.h>\n#include <linux/sunrpc/types.h>\n#include <linux/spinlock.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <linux/sunrpc/xdr.h>\n\n/*\n * This is the actual RPC procedure call info.\n */\nstruct rpc_procinfo;\nstruct rpc_message {\n\tstruct rpc_procinfo *\trpc_proc;\t/* Procedure information */\n\tvoid *\t\t\trpc_argp;\t/* Arguments */\n\tvoid *\t\t\trpc_resp;\t/* Result */\n\tstruct rpc_cred *\trpc_cred;\t/* Credentials */\n};\n\nstruct rpc_call_ops;\nstruct rpc_wait_queue;\nstruct rpc_wait {\n\tstruct list_head\tlist;\t\t/* wait queue links */\n\tstruct list_head\tlinks;\t\t/* Links to related tasks */\n\tstruct list_head\ttimer_list;\t/* Timer list */\n\tunsigned long\t\texpires;\n};\n\n/*\n * This is the RPC task struct\n */\nstruct rpc_task {\n\tatomic_t\t\ttk_count;\t/* Reference count */\n\tstruct list_head\ttk_task;\t/* global list of tasks */\n\tstruct rpc_clnt *\ttk_client;\t/* RPC client */\n\tstruct rpc_rqst *\ttk_rqstp;\t/* RPC request */\n\n\t/*\n\t * RPC call state\n\t */\n\tstruct rpc_message\ttk_msg;\t\t/* RPC call info */\n\n\t/*\n\t * callback\tto be executed after waking up\n\t * action\tnext procedure for async tasks\n\t * tk_ops\tcaller callbacks\n\t */\n\tvoid\t\t\t(*tk_callback)(struct rpc_task *);\n\tvoid\t\t\t(*tk_action)(struct rpc_task *);\n\tconst struct rpc_call_ops *tk_ops;\n\tvoid *\t\t\ttk_calldata;\n\n\tunsigned long\t\ttk_timeout;\t/* timeout for rpc_sleep() */\n\tunsigned long\t\ttk_runstate;\t/* Task run status */\n\tstruct workqueue_struct\t*tk_workqueue;\t/* Normally rpciod, but could\n\t\t\t\t\t\t * be any workqueue\n\t\t\t\t\t\t */\n\tstruct rpc_wait_queue \t*tk_waitqueue;\t/* RPC wait queue we're on */\n\tunion {\n\t\tstruct work_struct\ttk_work;\t/* Async task work queue */\n\t\tstruct rpc_wait\t\ttk_wait;\t/* RPC wait */\n\t} u;\n\n\tktime_t\t\t\ttk_start;\t/* RPC task init timestamp */\n\n\tpid_t\t\t\ttk_owner;\t/* Process id for batching tasks */\n\tint\t\t\ttk_status;\t/* result of last operation */\n\tunsigned short\t\ttk_flags;\t/* misc flags */\n\tunsigned short\t\ttk_timeouts;\t/* maj timeouts */\n\n#ifdef RPC_DEBUG\n\tunsigned short\t\ttk_pid;\t\t/* debugging aid */\n#endif\n\tunsigned char\t\ttk_priority : 2,/* Task priority */\n\t\t\t\ttk_garb_retry : 2,\n\t\t\t\ttk_cred_retry : 2,\n\t\t\t\ttk_rebind_retry : 2;\n};\n#define tk_xprt\t\t\ttk_client->cl_xprt\n\n/* support walking a list of tasks on a wait queue */\n#define\ttask_for_each(task, pos, head) \\\n\tlist_for_each(pos, head) \\\n\t\tif ((task=list_entry(pos, struct rpc_task, u.tk_wait.list)),1)\n\n#define\ttask_for_first(task, head) \\\n\tif (!list_empty(head) &&  \\\n\t    ((task=list_entry((head)->next, struct rpc_task, u.tk_wait.list)),1))\n\ntypedef void\t\t\t(*rpc_action)(struct rpc_task *);\n\nstruct rpc_call_ops {\n\tvoid (*rpc_call_prepare)(struct rpc_task *, void *);\n\tvoid (*rpc_call_done)(struct rpc_task *, void *);\n\tvoid (*rpc_release)(void *);\n};\n\nstruct rpc_task_setup {\n\tstruct rpc_task *task;\n\tstruct rpc_clnt *rpc_client;\n\tconst struct rpc_message *rpc_message;\n\tconst struct rpc_call_ops *callback_ops;\n\tvoid *callback_data;\n\tstruct workqueue_struct *workqueue;\n\tunsigned short flags;\n\tsigned char priority;\n};\n\n/*\n * RPC task flags\n */\n#define RPC_TASK_ASYNC\t\t0x0001\t\t/* is an async task */\n#define RPC_TASK_SWAPPER\t0x0002\t\t/* is swapping in/out */\n#define RPC_CALL_MAJORSEEN\t0x0020\t\t/* major timeout seen */\n#define RPC_TASK_ROOTCREDS\t0x0040\t\t/* force root creds */\n#define RPC_TASK_DYNAMIC\t0x0080\t\t/* task was kmalloc'ed */\n#define RPC_TASK_KILLED\t\t0x0100\t\t/* task was killed */\n#define RPC_TASK_SOFT\t\t0x0200\t\t/* Use soft timeouts */\n#define RPC_TASK_SOFTCONN\t0x0400\t\t/* Fail if can't connect */\n#define RPC_TASK_SENT\t\t0x0800\t\t/* message was sent */\n#define RPC_TASK_TIMEOUT\t0x1000\t\t/* fail with ETIMEDOUT on timeout */\n\n#define RPC_IS_ASYNC(t)\t\t((t)->tk_flags & RPC_TASK_ASYNC)\n#define RPC_IS_SWAPPER(t)\t((t)->tk_flags & RPC_TASK_SWAPPER)\n#define RPC_DO_ROOTOVERRIDE(t)\t((t)->tk_flags & RPC_TASK_ROOTCREDS)\n#define RPC_ASSASSINATED(t)\t((t)->tk_flags & RPC_TASK_KILLED)\n#define RPC_IS_SOFT(t)\t\t((t)->tk_flags & (RPC_TASK_SOFT|RPC_TASK_TIMEOUT))\n#define RPC_IS_SOFTCONN(t)\t((t)->tk_flags & RPC_TASK_SOFTCONN)\n#define RPC_WAS_SENT(t)\t\t((t)->tk_flags & RPC_TASK_SENT)\n\n#define RPC_TASK_RUNNING\t0\n#define RPC_TASK_QUEUED\t\t1\n#define RPC_TASK_ACTIVE\t\t2\n\n#define RPC_IS_RUNNING(t)\ttest_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)\n#define rpc_set_running(t)\tset_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)\n#define rpc_test_and_set_running(t) \\\n\t\t\t\ttest_and_set_bit(RPC_TASK_RUNNING, &(t)->tk_runstate)\n#define rpc_clear_running(t)\t\\\n\tdo { \\\n\t\tsmp_mb__before_clear_bit(); \\\n\t\tclear_bit(RPC_TASK_RUNNING, &(t)->tk_runstate); \\\n\t\tsmp_mb__after_clear_bit(); \\\n\t} while (0)\n\n#define RPC_IS_QUEUED(t)\ttest_bit(RPC_TASK_QUEUED, &(t)->tk_runstate)\n#define rpc_set_queued(t)\tset_bit(RPC_TASK_QUEUED, &(t)->tk_runstate)\n#define rpc_clear_queued(t)\t\\\n\tdo { \\\n\t\tsmp_mb__before_clear_bit(); \\\n\t\tclear_bit(RPC_TASK_QUEUED, &(t)->tk_runstate); \\\n\t\tsmp_mb__after_clear_bit(); \\\n\t} while (0)\n\n#define RPC_IS_ACTIVATED(t)\ttest_bit(RPC_TASK_ACTIVE, &(t)->tk_runstate)\n\n/*\n * Task priorities.\n * Note: if you change these, you must also change\n * the task initialization definitions below.\n */\n#define RPC_PRIORITY_LOW\t(-1)\n#define RPC_PRIORITY_NORMAL\t(0)\n#define RPC_PRIORITY_HIGH\t(1)\n#define RPC_PRIORITY_PRIVILEGED\t(2)\n#define RPC_NR_PRIORITY\t\t(1 + RPC_PRIORITY_PRIVILEGED - RPC_PRIORITY_LOW)\n\nstruct rpc_timer {\n\tstruct timer_list timer;\n\tstruct list_head list;\n\tunsigned long expires;\n};\n\n/*\n * RPC synchronization objects\n */\nstruct rpc_wait_queue {\n\tspinlock_t\t\tlock;\n\tstruct list_head\ttasks[RPC_NR_PRIORITY];\t/* task queue for each priority level */\n\tpid_t\t\t\towner;\t\t\t/* process id of last task serviced */\n\tunsigned char\t\tmaxpriority;\t\t/* maximum priority (0 if queue is not a priority queue) */\n\tunsigned char\t\tpriority;\t\t/* current priority */\n\tunsigned char\t\tcount;\t\t\t/* # task groups remaining serviced so far */\n\tunsigned char\t\tnr;\t\t\t/* # tasks remaining for cookie */\n\tunsigned short\t\tqlen;\t\t\t/* total # tasks waiting in queue */\n\tstruct rpc_timer\ttimer_list;\n#ifdef RPC_DEBUG\n\tconst char *\t\tname;\n#endif\n};\n\n/*\n * This is the # requests to send consecutively\n * from a single cookie.  The aim is to improve\n * performance of NFS operations such as read/write.\n */\n#define RPC_BATCH_COUNT\t\t\t16\n#define RPC_IS_PRIORITY(q)\t\t((q)->maxpriority > 0)\n\n/*\n * Function prototypes\n */\nstruct rpc_task *rpc_new_task(const struct rpc_task_setup *);\nstruct rpc_task *rpc_run_task(const struct rpc_task_setup *);\nstruct rpc_task *rpc_run_bc_task(struct rpc_rqst *req,\n\t\t\t\tconst struct rpc_call_ops *ops);\nvoid\t\trpc_put_task(struct rpc_task *);\nvoid\t\trpc_put_task_async(struct rpc_task *);\nvoid\t\trpc_exit_task(struct rpc_task *);\nvoid\t\trpc_exit(struct rpc_task *, int);\nvoid\t\trpc_release_calldata(const struct rpc_call_ops *, void *);\nvoid\t\trpc_killall_tasks(struct rpc_clnt *);\nvoid\t\trpc_execute(struct rpc_task *);\nvoid\t\trpc_init_priority_wait_queue(struct rpc_wait_queue *, const char *);\nvoid\t\trpc_init_wait_queue(struct rpc_wait_queue *, const char *);\nvoid\t\trpc_destroy_wait_queue(struct rpc_wait_queue *);\nvoid\t\trpc_sleep_on(struct rpc_wait_queue *, struct rpc_task *,\n\t\t\t\t\trpc_action action);\nvoid\t\trpc_wake_up_queued_task(struct rpc_wait_queue *,\n\t\t\t\t\tstruct rpc_task *);\nvoid\t\trpc_wake_up(struct rpc_wait_queue *);\nstruct rpc_task *rpc_wake_up_next(struct rpc_wait_queue *);\nvoid\t\trpc_wake_up_status(struct rpc_wait_queue *, int);\nint\t\trpc_queue_empty(struct rpc_wait_queue *);\nvoid\t\trpc_delay(struct rpc_task *, unsigned long);\nvoid *\t\trpc_malloc(struct rpc_task *, size_t);\nvoid\t\trpc_free(void *);\nint\t\trpciod_up(void);\nvoid\t\trpciod_down(void);\nint\t\t__rpc_wait_for_completion_task(struct rpc_task *task, int (*)(void *));\n#ifdef RPC_DEBUG\nvoid\t\trpc_show_tasks(void);\n#endif\nint\t\trpc_init_mempool(void);\nvoid\t\trpc_destroy_mempool(void);\nextern struct workqueue_struct *rpciod_workqueue;\nvoid\t\trpc_prepare_task(struct rpc_task *task);\n\nstatic inline int rpc_wait_for_completion_task(struct rpc_task *task)\n{\n\treturn __rpc_wait_for_completion_task(task, NULL);\n}\n\nstatic inline void rpc_task_set_priority(struct rpc_task *task, unsigned char prio)\n{\n\ttask->tk_priority = prio - RPC_PRIORITY_LOW;\n}\n\nstatic inline int rpc_task_has_priority(struct rpc_task *task, unsigned char prio)\n{\n\treturn (task->tk_priority + RPC_PRIORITY_LOW == prio);\n}\n\n#ifdef RPC_DEBUG\nstatic inline const char * rpc_qname(struct rpc_wait_queue *q)\n{\n\treturn ((q && q->name) ? q->name : \"unknown\");\n}\n#endif\n\n#endif /* _LINUX_SUNRPC_SCHED_H_ */\n", "/*\n *  linux/net/sunrpc/clnt.c\n *\n *  This file contains the high-level RPC interface.\n *  It is modeled as a finite state machine to support both synchronous\n *  and asynchronous requests.\n *\n *  -\tRPC header generation and argument serialization.\n *  -\tCredential refresh.\n *  -\tTCP connect handling.\n *  -\tRetry of operation when it is suspected the operation failed because\n *\tof uid squashing on the server, or when the credentials were stale\n *\tand need to be refreshed, or when a packet was damaged in transit.\n *\tThis may be have to be moved to the VFS layer.\n *\n *  Copyright (C) 1992,1993 Rick Sladkey <jrs@world.std.com>\n *  Copyright (C) 1995,1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <asm/system.h>\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kallsyms.h>\n#include <linux/mm.h>\n#include <linux/namei.h>\n#include <linux/mount.h>\n#include <linux/slab.h>\n#include <linux/utsname.h>\n#include <linux/workqueue.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/un.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/rpc_pipe_fs.h>\n#include <linux/sunrpc/metrics.h>\n#include <linux/sunrpc/bc_xprt.h>\n\n#include \"sunrpc.h\"\n\n#ifdef RPC_DEBUG\n# define RPCDBG_FACILITY\tRPCDBG_CALL\n#endif\n\n#define dprint_status(t)\t\t\t\t\t\\\n\tdprintk(\"RPC: %5u %s (status %d)\\n\", t->tk_pid,\t\t\\\n\t\t\t__func__, t->tk_status)\n\n/*\n * All RPC clients are linked into this list\n */\nstatic LIST_HEAD(all_clients);\nstatic DEFINE_SPINLOCK(rpc_client_lock);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(destroy_wait);\n\n\nstatic void\tcall_start(struct rpc_task *task);\nstatic void\tcall_reserve(struct rpc_task *task);\nstatic void\tcall_reserveresult(struct rpc_task *task);\nstatic void\tcall_allocate(struct rpc_task *task);\nstatic void\tcall_decode(struct rpc_task *task);\nstatic void\tcall_bind(struct rpc_task *task);\nstatic void\tcall_bind_status(struct rpc_task *task);\nstatic void\tcall_transmit(struct rpc_task *task);\n#if defined(CONFIG_NFS_V4_1)\nstatic void\tcall_bc_transmit(struct rpc_task *task);\n#endif /* CONFIG_NFS_V4_1 */\nstatic void\tcall_status(struct rpc_task *task);\nstatic void\tcall_transmit_status(struct rpc_task *task);\nstatic void\tcall_refresh(struct rpc_task *task);\nstatic void\tcall_refreshresult(struct rpc_task *task);\nstatic void\tcall_timeout(struct rpc_task *task);\nstatic void\tcall_connect(struct rpc_task *task);\nstatic void\tcall_connect_status(struct rpc_task *task);\n\nstatic __be32\t*rpc_encode_header(struct rpc_task *task);\nstatic __be32\t*rpc_verify_header(struct rpc_task *task);\nstatic int\trpc_ping(struct rpc_clnt *clnt);\n\nstatic void rpc_register_client(struct rpc_clnt *clnt)\n{\n\tspin_lock(&rpc_client_lock);\n\tlist_add(&clnt->cl_clients, &all_clients);\n\tspin_unlock(&rpc_client_lock);\n}\n\nstatic void rpc_unregister_client(struct rpc_clnt *clnt)\n{\n\tspin_lock(&rpc_client_lock);\n\tlist_del(&clnt->cl_clients);\n\tspin_unlock(&rpc_client_lock);\n}\n\nstatic int\nrpc_setup_pipedir(struct rpc_clnt *clnt, char *dir_name)\n{\n\tstatic uint32_t clntid;\n\tstruct nameidata nd;\n\tstruct path path;\n\tchar name[15];\n\tstruct qstr q = {\n\t\t.name = name,\n\t};\n\tint error;\n\n\tclnt->cl_path.mnt = ERR_PTR(-ENOENT);\n\tclnt->cl_path.dentry = ERR_PTR(-ENOENT);\n\tif (dir_name == NULL)\n\t\treturn 0;\n\n\tpath.mnt = rpc_get_mount();\n\tif (IS_ERR(path.mnt))\n\t\treturn PTR_ERR(path.mnt);\n\terror = vfs_path_lookup(path.mnt->mnt_root, path.mnt, dir_name, 0, &nd);\n\tif (error)\n\t\tgoto err;\n\n\tfor (;;) {\n\t\tq.len = snprintf(name, sizeof(name), \"clnt%x\", (unsigned int)clntid++);\n\t\tname[sizeof(name) - 1] = '\\0';\n\t\tq.hash = full_name_hash(q.name, q.len);\n\t\tpath.dentry = rpc_create_client_dir(nd.path.dentry, &q, clnt);\n\t\tif (!IS_ERR(path.dentry))\n\t\t\tbreak;\n\t\terror = PTR_ERR(path.dentry);\n\t\tif (error != -EEXIST) {\n\t\t\tprintk(KERN_INFO \"RPC: Couldn't create pipefs entry\"\n\t\t\t\t\t\" %s/%s, error %d\\n\",\n\t\t\t\t\tdir_name, name, error);\n\t\t\tgoto err_path_put;\n\t\t}\n\t}\n\tpath_put(&nd.path);\n\tclnt->cl_path = path;\n\treturn 0;\nerr_path_put:\n\tpath_put(&nd.path);\nerr:\n\trpc_put_mount();\n\treturn error;\n}\n\nstatic struct rpc_clnt * rpc_new_client(const struct rpc_create_args *args, struct rpc_xprt *xprt)\n{\n\tstruct rpc_program\t*program = args->program;\n\tstruct rpc_version\t*version;\n\tstruct rpc_clnt\t\t*clnt = NULL;\n\tstruct rpc_auth\t\t*auth;\n\tint err;\n\tsize_t len;\n\n\t/* sanity check the name before trying to print it */\n\terr = -EINVAL;\n\tlen = strlen(args->servername);\n\tif (len > RPC_MAXNETNAMELEN)\n\t\tgoto out_no_rpciod;\n\tlen++;\n\n\tdprintk(\"RPC:       creating %s client for %s (xprt %p)\\n\",\n\t\t\tprogram->name, args->servername, xprt);\n\n\terr = rpciod_up();\n\tif (err)\n\t\tgoto out_no_rpciod;\n\terr = -EINVAL;\n\tif (!xprt)\n\t\tgoto out_no_xprt;\n\n\tif (args->version >= program->nrvers)\n\t\tgoto out_err;\n\tversion = program->version[args->version];\n\tif (version == NULL)\n\t\tgoto out_err;\n\n\terr = -ENOMEM;\n\tclnt = kzalloc(sizeof(*clnt), GFP_KERNEL);\n\tif (!clnt)\n\t\tgoto out_err;\n\tclnt->cl_parent = clnt;\n\n\tclnt->cl_server = clnt->cl_inline_name;\n\tif (len > sizeof(clnt->cl_inline_name)) {\n\t\tchar *buf = kmalloc(len, GFP_KERNEL);\n\t\tif (buf != NULL)\n\t\t\tclnt->cl_server = buf;\n\t\telse\n\t\t\tlen = sizeof(clnt->cl_inline_name);\n\t}\n\tstrlcpy(clnt->cl_server, args->servername, len);\n\n\tclnt->cl_xprt     = xprt;\n\tclnt->cl_procinfo = version->procs;\n\tclnt->cl_maxproc  = version->nrprocs;\n\tclnt->cl_protname = program->name;\n\tclnt->cl_prog     = args->prognumber ? : program->number;\n\tclnt->cl_vers     = version->number;\n\tclnt->cl_stats    = program->stats;\n\tclnt->cl_metrics  = rpc_alloc_iostats(clnt);\n\terr = -ENOMEM;\n\tif (clnt->cl_metrics == NULL)\n\t\tgoto out_no_stats;\n\tclnt->cl_program  = program;\n\tINIT_LIST_HEAD(&clnt->cl_tasks);\n\tspin_lock_init(&clnt->cl_lock);\n\n\tif (!xprt_bound(clnt->cl_xprt))\n\t\tclnt->cl_autobind = 1;\n\n\tclnt->cl_timeout = xprt->timeout;\n\tif (args->timeout != NULL) {\n\t\tmemcpy(&clnt->cl_timeout_default, args->timeout,\n\t\t\t\tsizeof(clnt->cl_timeout_default));\n\t\tclnt->cl_timeout = &clnt->cl_timeout_default;\n\t}\n\n\tclnt->cl_rtt = &clnt->cl_rtt_default;\n\trpc_init_rtt(&clnt->cl_rtt_default, clnt->cl_timeout->to_initval);\n\tclnt->cl_principal = NULL;\n\tif (args->client_name) {\n\t\tclnt->cl_principal = kstrdup(args->client_name, GFP_KERNEL);\n\t\tif (!clnt->cl_principal)\n\t\t\tgoto out_no_principal;\n\t}\n\n\tatomic_set(&clnt->cl_count, 1);\n\n\terr = rpc_setup_pipedir(clnt, program->pipe_dir_name);\n\tif (err < 0)\n\t\tgoto out_no_path;\n\n\tauth = rpcauth_create(args->authflavor, clnt);\n\tif (IS_ERR(auth)) {\n\t\tprintk(KERN_INFO \"RPC: Couldn't create auth handle (flavor %u)\\n\",\n\t\t\t\targs->authflavor);\n\t\terr = PTR_ERR(auth);\n\t\tgoto out_no_auth;\n\t}\n\n\t/* save the nodename */\n\tclnt->cl_nodelen = strlen(init_utsname()->nodename);\n\tif (clnt->cl_nodelen > UNX_MAXNODENAME)\n\t\tclnt->cl_nodelen = UNX_MAXNODENAME;\n\tmemcpy(clnt->cl_nodename, init_utsname()->nodename, clnt->cl_nodelen);\n\trpc_register_client(clnt);\n\treturn clnt;\n\nout_no_auth:\n\tif (!IS_ERR(clnt->cl_path.dentry)) {\n\t\trpc_remove_client_dir(clnt->cl_path.dentry);\n\t\trpc_put_mount();\n\t}\nout_no_path:\n\tkfree(clnt->cl_principal);\nout_no_principal:\n\trpc_free_iostats(clnt->cl_metrics);\nout_no_stats:\n\tif (clnt->cl_server != clnt->cl_inline_name)\n\t\tkfree(clnt->cl_server);\n\tkfree(clnt);\nout_err:\n\txprt_put(xprt);\nout_no_xprt:\n\trpciod_down();\nout_no_rpciod:\n\treturn ERR_PTR(err);\n}\n\n/*\n * rpc_create - create an RPC client and transport with one call\n * @args: rpc_clnt create argument structure\n *\n * Creates and initializes an RPC transport and an RPC client.\n *\n * It can ping the server in order to determine if it is up, and to see if\n * it supports this program and version.  RPC_CLNT_CREATE_NOPING disables\n * this behavior so asynchronous tasks can also use rpc_create.\n */\nstruct rpc_clnt *rpc_create(struct rpc_create_args *args)\n{\n\tstruct rpc_xprt *xprt;\n\tstruct rpc_clnt *clnt;\n\tstruct xprt_create xprtargs = {\n\t\t.net = args->net,\n\t\t.ident = args->protocol,\n\t\t.srcaddr = args->saddress,\n\t\t.dstaddr = args->address,\n\t\t.addrlen = args->addrsize,\n\t\t.bc_xprt = args->bc_xprt,\n\t};\n\tchar servername[48];\n\n\t/*\n\t * If the caller chooses not to specify a hostname, whip\n\t * up a string representation of the passed-in address.\n\t */\n\tif (args->servername == NULL) {\n\t\tstruct sockaddr_un *sun =\n\t\t\t\t(struct sockaddr_un *)args->address;\n\t\tstruct sockaddr_in *sin =\n\t\t\t\t(struct sockaddr_in *)args->address;\n\t\tstruct sockaddr_in6 *sin6 =\n\t\t\t\t(struct sockaddr_in6 *)args->address;\n\n\t\tservername[0] = '\\0';\n\t\tswitch (args->address->sa_family) {\n\t\tcase AF_LOCAL:\n\t\t\tsnprintf(servername, sizeof(servername), \"%s\",\n\t\t\t\t sun->sun_path);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tsnprintf(servername, sizeof(servername), \"%pI4\",\n\t\t\t\t &sin->sin_addr.s_addr);\n\t\t\tbreak;\n\t\tcase AF_INET6:\n\t\t\tsnprintf(servername, sizeof(servername), \"%pI6\",\n\t\t\t\t &sin6->sin6_addr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* caller wants default server name, but\n\t\t\t * address family isn't recognized. */\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\targs->servername = servername;\n\t}\n\n\txprt = xprt_create_transport(&xprtargs);\n\tif (IS_ERR(xprt))\n\t\treturn (struct rpc_clnt *)xprt;\n\n\t/*\n\t * By default, kernel RPC client connects from a reserved port.\n\t * CAP_NET_BIND_SERVICE will not be set for unprivileged requesters,\n\t * but it is always enabled for rpciod, which handles the connect\n\t * operation.\n\t */\n\txprt->resvport = 1;\n\tif (args->flags & RPC_CLNT_CREATE_NONPRIVPORT)\n\t\txprt->resvport = 0;\n\n\tclnt = rpc_new_client(args, xprt);\n\tif (IS_ERR(clnt))\n\t\treturn clnt;\n\n\tif (!(args->flags & RPC_CLNT_CREATE_NOPING)) {\n\t\tint err = rpc_ping(clnt);\n\t\tif (err != 0) {\n\t\t\trpc_shutdown_client(clnt);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\tclnt->cl_softrtry = 1;\n\tif (args->flags & RPC_CLNT_CREATE_HARDRTRY)\n\t\tclnt->cl_softrtry = 0;\n\n\tif (args->flags & RPC_CLNT_CREATE_AUTOBIND)\n\t\tclnt->cl_autobind = 1;\n\tif (args->flags & RPC_CLNT_CREATE_DISCRTRY)\n\t\tclnt->cl_discrtry = 1;\n\tif (!(args->flags & RPC_CLNT_CREATE_QUIET))\n\t\tclnt->cl_chatty = 1;\n\n\treturn clnt;\n}\nEXPORT_SYMBOL_GPL(rpc_create);\n\n/*\n * This function clones the RPC client structure. It allows us to share the\n * same transport while varying parameters such as the authentication\n * flavour.\n */\nstruct rpc_clnt *\nrpc_clone_client(struct rpc_clnt *clnt)\n{\n\tstruct rpc_clnt *new;\n\tint err = -ENOMEM;\n\n\tnew = kmemdup(clnt, sizeof(*new), GFP_KERNEL);\n\tif (!new)\n\t\tgoto out_no_clnt;\n\tnew->cl_parent = clnt;\n\t/* Turn off autobind on clones */\n\tnew->cl_autobind = 0;\n\tINIT_LIST_HEAD(&new->cl_tasks);\n\tspin_lock_init(&new->cl_lock);\n\trpc_init_rtt(&new->cl_rtt_default, clnt->cl_timeout->to_initval);\n\tnew->cl_metrics = rpc_alloc_iostats(clnt);\n\tif (new->cl_metrics == NULL)\n\t\tgoto out_no_stats;\n\tif (clnt->cl_principal) {\n\t\tnew->cl_principal = kstrdup(clnt->cl_principal, GFP_KERNEL);\n\t\tif (new->cl_principal == NULL)\n\t\t\tgoto out_no_principal;\n\t}\n\tatomic_set(&new->cl_count, 1);\n\terr = rpc_setup_pipedir(new, clnt->cl_program->pipe_dir_name);\n\tif (err != 0)\n\t\tgoto out_no_path;\n\tif (new->cl_auth)\n\t\tatomic_inc(&new->cl_auth->au_count);\n\txprt_get(clnt->cl_xprt);\n\tatomic_inc(&clnt->cl_count);\n\trpc_register_client(new);\n\trpciod_up();\n\treturn new;\nout_no_path:\n\tkfree(new->cl_principal);\nout_no_principal:\n\trpc_free_iostats(new->cl_metrics);\nout_no_stats:\n\tkfree(new);\nout_no_clnt:\n\tdprintk(\"RPC:       %s: returned error %d\\n\", __func__, err);\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(rpc_clone_client);\n\n/*\n * Kill all tasks for the given client.\n * XXX: kill their descendants as well?\n */\nvoid rpc_killall_tasks(struct rpc_clnt *clnt)\n{\n\tstruct rpc_task\t*rovr;\n\n\n\tif (list_empty(&clnt->cl_tasks))\n\t\treturn;\n\tdprintk(\"RPC:       killing all tasks for client %p\\n\", clnt);\n\t/*\n\t * Spin lock all_tasks to prevent changes...\n\t */\n\tspin_lock(&clnt->cl_lock);\n\tlist_for_each_entry(rovr, &clnt->cl_tasks, tk_task) {\n\t\tif (!RPC_IS_ACTIVATED(rovr))\n\t\t\tcontinue;\n\t\tif (!(rovr->tk_flags & RPC_TASK_KILLED)) {\n\t\t\trovr->tk_flags |= RPC_TASK_KILLED;\n\t\t\trpc_exit(rovr, -EIO);\n\t\t\tif (RPC_IS_QUEUED(rovr))\n\t\t\t\trpc_wake_up_queued_task(rovr->tk_waitqueue,\n\t\t\t\t\t\t\trovr);\n\t\t}\n\t}\n\tspin_unlock(&clnt->cl_lock);\n}\nEXPORT_SYMBOL_GPL(rpc_killall_tasks);\n\n/*\n * Properly shut down an RPC client, terminating all outstanding\n * requests.\n */\nvoid rpc_shutdown_client(struct rpc_clnt *clnt)\n{\n\tdprintk(\"RPC:       shutting down %s client for %s\\n\",\n\t\t\tclnt->cl_protname, clnt->cl_server);\n\n\twhile (!list_empty(&clnt->cl_tasks)) {\n\t\trpc_killall_tasks(clnt);\n\t\twait_event_timeout(destroy_wait,\n\t\t\tlist_empty(&clnt->cl_tasks), 1*HZ);\n\t}\n\n\trpc_release_client(clnt);\n}\nEXPORT_SYMBOL_GPL(rpc_shutdown_client);\n\n/*\n * Free an RPC client\n */\nstatic void\nrpc_free_client(struct rpc_clnt *clnt)\n{\n\tdprintk(\"RPC:       destroying %s client for %s\\n\",\n\t\t\tclnt->cl_protname, clnt->cl_server);\n\tif (!IS_ERR(clnt->cl_path.dentry)) {\n\t\trpc_remove_client_dir(clnt->cl_path.dentry);\n\t\trpc_put_mount();\n\t}\n\tif (clnt->cl_parent != clnt) {\n\t\trpc_release_client(clnt->cl_parent);\n\t\tgoto out_free;\n\t}\n\tif (clnt->cl_server != clnt->cl_inline_name)\n\t\tkfree(clnt->cl_server);\nout_free:\n\trpc_unregister_client(clnt);\n\trpc_free_iostats(clnt->cl_metrics);\n\tkfree(clnt->cl_principal);\n\tclnt->cl_metrics = NULL;\n\txprt_put(clnt->cl_xprt);\n\trpciod_down();\n\tkfree(clnt);\n}\n\n/*\n * Free an RPC client\n */\nstatic void\nrpc_free_auth(struct rpc_clnt *clnt)\n{\n\tif (clnt->cl_auth == NULL) {\n\t\trpc_free_client(clnt);\n\t\treturn;\n\t}\n\n\t/*\n\t * Note: RPCSEC_GSS may need to send NULL RPC calls in order to\n\t *       release remaining GSS contexts. This mechanism ensures\n\t *       that it can do so safely.\n\t */\n\tatomic_inc(&clnt->cl_count);\n\trpcauth_release(clnt->cl_auth);\n\tclnt->cl_auth = NULL;\n\tif (atomic_dec_and_test(&clnt->cl_count))\n\t\trpc_free_client(clnt);\n}\n\n/*\n * Release reference to the RPC client\n */\nvoid\nrpc_release_client(struct rpc_clnt *clnt)\n{\n\tdprintk(\"RPC:       rpc_release_client(%p)\\n\", clnt);\n\n\tif (list_empty(&clnt->cl_tasks))\n\t\twake_up(&destroy_wait);\n\tif (atomic_dec_and_test(&clnt->cl_count))\n\t\trpc_free_auth(clnt);\n}\n\n/**\n * rpc_bind_new_program - bind a new RPC program to an existing client\n * @old: old rpc_client\n * @program: rpc program to set\n * @vers: rpc program version\n *\n * Clones the rpc client and sets up a new RPC program. This is mainly\n * of use for enabling different RPC programs to share the same transport.\n * The Sun NFSv2/v3 ACL protocol can do this.\n */\nstruct rpc_clnt *rpc_bind_new_program(struct rpc_clnt *old,\n\t\t\t\t      struct rpc_program *program,\n\t\t\t\t      u32 vers)\n{\n\tstruct rpc_clnt *clnt;\n\tstruct rpc_version *version;\n\tint err;\n\n\tBUG_ON(vers >= program->nrvers || !program->version[vers]);\n\tversion = program->version[vers];\n\tclnt = rpc_clone_client(old);\n\tif (IS_ERR(clnt))\n\t\tgoto out;\n\tclnt->cl_procinfo = version->procs;\n\tclnt->cl_maxproc  = version->nrprocs;\n\tclnt->cl_protname = program->name;\n\tclnt->cl_prog     = program->number;\n\tclnt->cl_vers     = version->number;\n\tclnt->cl_stats    = program->stats;\n\terr = rpc_ping(clnt);\n\tif (err != 0) {\n\t\trpc_shutdown_client(clnt);\n\t\tclnt = ERR_PTR(err);\n\t}\nout:\n\treturn clnt;\n}\nEXPORT_SYMBOL_GPL(rpc_bind_new_program);\n\nvoid rpc_task_release_client(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\n\tif (clnt != NULL) {\n\t\t/* Remove from client task list */\n\t\tspin_lock(&clnt->cl_lock);\n\t\tlist_del(&task->tk_task);\n\t\tspin_unlock(&clnt->cl_lock);\n\t\ttask->tk_client = NULL;\n\n\t\trpc_release_client(clnt);\n\t}\n}\n\nstatic\nvoid rpc_task_set_client(struct rpc_task *task, struct rpc_clnt *clnt)\n{\n\tif (clnt != NULL) {\n\t\trpc_task_release_client(task);\n\t\ttask->tk_client = clnt;\n\t\tatomic_inc(&clnt->cl_count);\n\t\tif (clnt->cl_softrtry)\n\t\t\ttask->tk_flags |= RPC_TASK_SOFT;\n\t\t/* Add to the client's list of all tasks */\n\t\tspin_lock(&clnt->cl_lock);\n\t\tlist_add_tail(&task->tk_task, &clnt->cl_tasks);\n\t\tspin_unlock(&clnt->cl_lock);\n\t}\n}\n\nvoid rpc_task_reset_client(struct rpc_task *task, struct rpc_clnt *clnt)\n{\n\trpc_task_release_client(task);\n\trpc_task_set_client(task, clnt);\n}\nEXPORT_SYMBOL_GPL(rpc_task_reset_client);\n\n\nstatic void\nrpc_task_set_rpc_message(struct rpc_task *task, const struct rpc_message *msg)\n{\n\tif (msg != NULL) {\n\t\ttask->tk_msg.rpc_proc = msg->rpc_proc;\n\t\ttask->tk_msg.rpc_argp = msg->rpc_argp;\n\t\ttask->tk_msg.rpc_resp = msg->rpc_resp;\n\t\tif (msg->rpc_cred != NULL)\n\t\t\ttask->tk_msg.rpc_cred = get_rpccred(msg->rpc_cred);\n\t}\n}\n\n/*\n * Default callback for async RPC calls\n */\nstatic void\nrpc_default_callback(struct rpc_task *task, void *data)\n{\n}\n\nstatic const struct rpc_call_ops rpc_default_ops = {\n\t.rpc_call_done = rpc_default_callback,\n};\n\n/**\n * rpc_run_task - Allocate a new RPC task, then run rpc_execute against it\n * @task_setup_data: pointer to task initialisation data\n */\nstruct rpc_task *rpc_run_task(const struct rpc_task_setup *task_setup_data)\n{\n\tstruct rpc_task *task;\n\n\ttask = rpc_new_task(task_setup_data);\n\tif (IS_ERR(task))\n\t\tgoto out;\n\n\trpc_task_set_client(task, task_setup_data->rpc_client);\n\trpc_task_set_rpc_message(task, task_setup_data->rpc_message);\n\n\tif (task->tk_action == NULL)\n\t\trpc_call_start(task);\n\n\tatomic_inc(&task->tk_count);\n\trpc_execute(task);\nout:\n\treturn task;\n}\nEXPORT_SYMBOL_GPL(rpc_run_task);\n\n/**\n * rpc_call_sync - Perform a synchronous RPC call\n * @clnt: pointer to RPC client\n * @msg: RPC call parameters\n * @flags: RPC call flags\n */\nint rpc_call_sync(struct rpc_clnt *clnt, const struct rpc_message *msg, int flags)\n{\n\tstruct rpc_task\t*task;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = msg,\n\t\t.callback_ops = &rpc_default_ops,\n\t\t.flags = flags,\n\t};\n\tint status;\n\n\tBUG_ON(flags & RPC_TASK_ASYNC);\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\tstatus = task->tk_status;\n\trpc_put_task(task);\n\treturn status;\n}\nEXPORT_SYMBOL_GPL(rpc_call_sync);\n\n/**\n * rpc_call_async - Perform an asynchronous RPC call\n * @clnt: pointer to RPC client\n * @msg: RPC call parameters\n * @flags: RPC call flags\n * @tk_ops: RPC call ops\n * @data: user call data\n */\nint\nrpc_call_async(struct rpc_clnt *clnt, const struct rpc_message *msg, int flags,\n\t       const struct rpc_call_ops *tk_ops, void *data)\n{\n\tstruct rpc_task\t*task;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = msg,\n\t\t.callback_ops = tk_ops,\n\t\t.callback_data = data,\n\t\t.flags = flags|RPC_TASK_ASYNC,\n\t};\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\trpc_put_task(task);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rpc_call_async);\n\n#if defined(CONFIG_NFS_V4_1)\n/**\n * rpc_run_bc_task - Allocate a new RPC task for backchannel use, then run\n * rpc_execute against it\n * @req: RPC request\n * @tk_ops: RPC call ops\n */\nstruct rpc_task *rpc_run_bc_task(struct rpc_rqst *req,\n\t\t\t\tconst struct rpc_call_ops *tk_ops)\n{\n\tstruct rpc_task *task;\n\tstruct xdr_buf *xbufp = &req->rq_snd_buf;\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.callback_ops = tk_ops,\n\t};\n\n\tdprintk(\"RPC: rpc_run_bc_task req= %p\\n\", req);\n\t/*\n\t * Create an rpc_task to send the data\n\t */\n\ttask = rpc_new_task(&task_setup_data);\n\tif (IS_ERR(task)) {\n\t\txprt_free_bc_request(req);\n\t\tgoto out;\n\t}\n\ttask->tk_rqstp = req;\n\n\t/*\n\t * Set up the xdr_buf length.\n\t * This also indicates that the buffer is XDR encoded already.\n\t */\n\txbufp->len = xbufp->head[0].iov_len + xbufp->page_len +\n\t\t\txbufp->tail[0].iov_len;\n\n\ttask->tk_action = call_bc_transmit;\n\tatomic_inc(&task->tk_count);\n\tBUG_ON(atomic_read(&task->tk_count) != 2);\n\trpc_execute(task);\n\nout:\n\tdprintk(\"RPC: rpc_run_bc_task: task= %p\\n\", task);\n\treturn task;\n}\n#endif /* CONFIG_NFS_V4_1 */\n\nvoid\nrpc_call_start(struct rpc_task *task)\n{\n\ttask->tk_action = call_start;\n}\nEXPORT_SYMBOL_GPL(rpc_call_start);\n\n/**\n * rpc_peeraddr - extract remote peer address from clnt's xprt\n * @clnt: RPC client structure\n * @buf: target buffer\n * @bufsize: length of target buffer\n *\n * Returns the number of bytes that are actually in the stored address.\n */\nsize_t rpc_peeraddr(struct rpc_clnt *clnt, struct sockaddr *buf, size_t bufsize)\n{\n\tsize_t bytes;\n\tstruct rpc_xprt *xprt = clnt->cl_xprt;\n\n\tbytes = sizeof(xprt->addr);\n\tif (bytes > bufsize)\n\t\tbytes = bufsize;\n\tmemcpy(buf, &clnt->cl_xprt->addr, bytes);\n\treturn xprt->addrlen;\n}\nEXPORT_SYMBOL_GPL(rpc_peeraddr);\n\n/**\n * rpc_peeraddr2str - return remote peer address in printable format\n * @clnt: RPC client structure\n * @format: address format\n *\n */\nconst char *rpc_peeraddr2str(struct rpc_clnt *clnt,\n\t\t\t     enum rpc_display_format_t format)\n{\n\tstruct rpc_xprt *xprt = clnt->cl_xprt;\n\n\tif (xprt->address_strings[format] != NULL)\n\t\treturn xprt->address_strings[format];\n\telse\n\t\treturn \"unprintable\";\n}\nEXPORT_SYMBOL_GPL(rpc_peeraddr2str);\n\nvoid\nrpc_setbufsize(struct rpc_clnt *clnt, unsigned int sndsize, unsigned int rcvsize)\n{\n\tstruct rpc_xprt *xprt = clnt->cl_xprt;\n\tif (xprt->ops->set_buffer_size)\n\t\txprt->ops->set_buffer_size(xprt, sndsize, rcvsize);\n}\nEXPORT_SYMBOL_GPL(rpc_setbufsize);\n\n/*\n * Return size of largest payload RPC client can support, in bytes\n *\n * For stream transports, this is one RPC record fragment (see RFC\n * 1831), as we don't support multi-record requests yet.  For datagram\n * transports, this is the size of an IP packet minus the IP, UDP, and\n * RPC header sizes.\n */\nsize_t rpc_max_payload(struct rpc_clnt *clnt)\n{\n\treturn clnt->cl_xprt->max_payload;\n}\nEXPORT_SYMBOL_GPL(rpc_max_payload);\n\n/**\n * rpc_force_rebind - force transport to check that remote port is unchanged\n * @clnt: client to rebind\n *\n */\nvoid rpc_force_rebind(struct rpc_clnt *clnt)\n{\n\tif (clnt->cl_autobind)\n\t\txprt_clear_bound(clnt->cl_xprt);\n}\nEXPORT_SYMBOL_GPL(rpc_force_rebind);\n\n/*\n * Restart an (async) RPC call from the call_prepare state.\n * Usually called from within the exit handler.\n */\nint\nrpc_restart_call_prepare(struct rpc_task *task)\n{\n\tif (RPC_ASSASSINATED(task))\n\t\treturn 0;\n\ttask->tk_action = rpc_prepare_task;\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(rpc_restart_call_prepare);\n\n/*\n * Restart an (async) RPC call. Usually called from within the\n * exit handler.\n */\nint\nrpc_restart_call(struct rpc_task *task)\n{\n\tif (RPC_ASSASSINATED(task))\n\t\treturn 0;\n\ttask->tk_action = call_start;\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(rpc_restart_call);\n\n#ifdef RPC_DEBUG\nstatic const char *rpc_proc_name(const struct rpc_task *task)\n{\n\tconst struct rpc_procinfo *proc = task->tk_msg.rpc_proc;\n\n\tif (proc) {\n\t\tif (proc->p_name)\n\t\t\treturn proc->p_name;\n\t\telse\n\t\t\treturn \"NULL\";\n\t} else\n\t\treturn \"no proc\";\n}\n#endif\n\n/*\n * 0.  Initial state\n *\n *     Other FSM states can be visited zero or more times, but\n *     this state is visited exactly once for each RPC.\n */\nstatic void\ncall_start(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\n\tdprintk(\"RPC: %5u call_start %s%d proc %s (%s)\\n\", task->tk_pid,\n\t\t\tclnt->cl_protname, clnt->cl_vers,\n\t\t\trpc_proc_name(task),\n\t\t\t(RPC_IS_ASYNC(task) ? \"async\" : \"sync\"));\n\n\t/* Increment call count */\n\ttask->tk_msg.rpc_proc->p_count++;\n\tclnt->cl_stats->rpccnt++;\n\ttask->tk_action = call_reserve;\n}\n\n/*\n * 1.\tReserve an RPC call slot\n */\nstatic void\ncall_reserve(struct rpc_task *task)\n{\n\tdprint_status(task);\n\n\ttask->tk_status  = 0;\n\ttask->tk_action  = call_reserveresult;\n\txprt_reserve(task);\n}\n\n/*\n * 1b.\tGrok the result of xprt_reserve()\n */\nstatic void\ncall_reserveresult(struct rpc_task *task)\n{\n\tint status = task->tk_status;\n\n\tdprint_status(task);\n\n\t/*\n\t * After a call to xprt_reserve(), we must have either\n\t * a request slot or else an error status.\n\t */\n\ttask->tk_status = 0;\n\tif (status >= 0) {\n\t\tif (task->tk_rqstp) {\n\t\t\ttask->tk_action = call_refresh;\n\t\t\treturn;\n\t\t}\n\n\t\tprintk(KERN_ERR \"%s: status=%d, but no request slot, exiting\\n\",\n\t\t\t\t__func__, status);\n\t\trpc_exit(task, -EIO);\n\t\treturn;\n\t}\n\n\t/*\n\t * Even though there was an error, we may have acquired\n\t * a request slot somehow.  Make sure not to leak it.\n\t */\n\tif (task->tk_rqstp) {\n\t\tprintk(KERN_ERR \"%s: status=%d, request allocated anyway\\n\",\n\t\t\t\t__func__, status);\n\t\txprt_release(task);\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\t/* woken up; retry */\n\t\ttask->tk_action = call_reserve;\n\t\treturn;\n\tcase -EIO:\t/* probably a shutdown */\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"%s: unrecognized error %d, exiting\\n\",\n\t\t\t\t__func__, status);\n\t\tbreak;\n\t}\n\trpc_exit(task, status);\n}\n\n/*\n * 2.\tBind and/or refresh the credentials\n */\nstatic void\ncall_refresh(struct rpc_task *task)\n{\n\tdprint_status(task);\n\n\ttask->tk_action = call_refreshresult;\n\ttask->tk_status = 0;\n\ttask->tk_client->cl_stats->rpcauthrefresh++;\n\trpcauth_refreshcred(task);\n}\n\n/*\n * 2a.\tProcess the results of a credential refresh\n */\nstatic void\ncall_refreshresult(struct rpc_task *task)\n{\n\tint status = task->tk_status;\n\n\tdprint_status(task);\n\n\ttask->tk_status = 0;\n\ttask->tk_action = call_refresh;\n\tswitch (status) {\n\tcase 0:\n\t\tif (rpcauth_uptodatecred(task))\n\t\t\ttask->tk_action = call_allocate;\n\t\treturn;\n\tcase -ETIMEDOUT:\n\t\trpc_delay(task, 3*HZ);\n\tcase -EAGAIN:\n\t\tstatus = -EACCES;\n\t\tif (!task->tk_cred_retry)\n\t\t\tbreak;\n\t\ttask->tk_cred_retry--;\n\t\tdprintk(\"RPC: %5u %s: retry refresh creds\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\treturn;\n\t}\n\tdprintk(\"RPC: %5u %s: refresh creds failed with error %d\\n\",\n\t\t\t\ttask->tk_pid, __func__, status);\n\trpc_exit(task, status);\n}\n\n/*\n * 2b.\tAllocate the buffer. For details, see sched.c:rpc_malloc.\n *\t(Note: buffer memory is freed in xprt_release).\n */\nstatic void\ncall_allocate(struct rpc_task *task)\n{\n\tunsigned int slack = task->tk_rqstp->rq_cred->cr_auth->au_cslack;\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\tstruct rpc_procinfo *proc = task->tk_msg.rpc_proc;\n\n\tdprint_status(task);\n\n\ttask->tk_status = 0;\n\ttask->tk_action = call_bind;\n\n\tif (req->rq_buffer)\n\t\treturn;\n\n\tif (proc->p_proc != 0) {\n\t\tBUG_ON(proc->p_arglen == 0);\n\t\tif (proc->p_decode != NULL)\n\t\t\tBUG_ON(proc->p_replen == 0);\n\t}\n\n\t/*\n\t * Calculate the size (in quads) of the RPC call\n\t * and reply headers, and convert both values\n\t * to byte sizes.\n\t */\n\treq->rq_callsize = RPC_CALLHDRSIZE + (slack << 1) + proc->p_arglen;\n\treq->rq_callsize <<= 2;\n\treq->rq_rcvsize = RPC_REPHDRSIZE + slack + proc->p_replen;\n\treq->rq_rcvsize <<= 2;\n\n\treq->rq_buffer = xprt->ops->buf_alloc(task,\n\t\t\t\t\treq->rq_callsize + req->rq_rcvsize);\n\tif (req->rq_buffer != NULL)\n\t\treturn;\n\n\tdprintk(\"RPC: %5u rpc_buffer allocation failed\\n\", task->tk_pid);\n\n\tif (RPC_IS_ASYNC(task) || !signalled()) {\n\t\ttask->tk_action = call_allocate;\n\t\trpc_delay(task, HZ>>4);\n\t\treturn;\n\t}\n\n\trpc_exit(task, -ERESTARTSYS);\n}\n\nstatic inline int\nrpc_task_need_encode(struct rpc_task *task)\n{\n\treturn task->tk_rqstp->rq_snd_buf.len == 0;\n}\n\nstatic inline void\nrpc_task_force_reencode(struct rpc_task *task)\n{\n\ttask->tk_rqstp->rq_snd_buf.len = 0;\n\ttask->tk_rqstp->rq_bytes_sent = 0;\n}\n\nstatic inline void\nrpc_xdr_buf_init(struct xdr_buf *buf, void *start, size_t len)\n{\n\tbuf->head[0].iov_base = start;\n\tbuf->head[0].iov_len = len;\n\tbuf->tail[0].iov_len = 0;\n\tbuf->page_len = 0;\n\tbuf->flags = 0;\n\tbuf->len = 0;\n\tbuf->buflen = len;\n}\n\n/*\n * 3.\tEncode arguments of an RPC call\n */\nstatic void\nrpc_xdr_encode(struct rpc_task *task)\n{\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tkxdreproc_t\tencode;\n\t__be32\t\t*p;\n\n\tdprint_status(task);\n\n\trpc_xdr_buf_init(&req->rq_snd_buf,\n\t\t\t req->rq_buffer,\n\t\t\t req->rq_callsize);\n\trpc_xdr_buf_init(&req->rq_rcv_buf,\n\t\t\t (char *)req->rq_buffer + req->rq_callsize,\n\t\t\t req->rq_rcvsize);\n\n\tp = rpc_encode_header(task);\n\tif (p == NULL) {\n\t\tprintk(KERN_INFO \"RPC: couldn't encode RPC header, exit EIO\\n\");\n\t\trpc_exit(task, -EIO);\n\t\treturn;\n\t}\n\n\tencode = task->tk_msg.rpc_proc->p_encode;\n\tif (encode == NULL)\n\t\treturn;\n\n\ttask->tk_status = rpcauth_wrap_req(task, encode, req, p,\n\t\t\ttask->tk_msg.rpc_argp);\n}\n\n/*\n * 4.\tGet the server port number if not yet set\n */\nstatic void\ncall_bind(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\tdprint_status(task);\n\n\ttask->tk_action = call_connect;\n\tif (!xprt_bound(xprt)) {\n\t\ttask->tk_action = call_bind_status;\n\t\ttask->tk_timeout = xprt->bind_timeout;\n\t\txprt->ops->rpcbind(task);\n\t}\n}\n\n/*\n * 4a.\tSort out bind result\n */\nstatic void\ncall_bind_status(struct rpc_task *task)\n{\n\tint status = -EIO;\n\n\tif (task->tk_status >= 0) {\n\t\tdprint_status(task);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_connect;\n\t\treturn;\n\t}\n\n\tswitch (task->tk_status) {\n\tcase -ENOMEM:\n\t\tdprintk(\"RPC: %5u rpcbind out of memory\\n\", task->tk_pid);\n\t\trpc_delay(task, HZ >> 2);\n\t\tgoto retry_timeout;\n\tcase -EACCES:\n\t\tdprintk(\"RPC: %5u remote rpcbind: RPC program/version \"\n\t\t\t\t\"unavailable\\n\", task->tk_pid);\n\t\t/* fail immediately if this is an RPC ping */\n\t\tif (task->tk_msg.rpc_proc->p_proc == 0) {\n\t\t\tstatus = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (task->tk_rebind_retry == 0)\n\t\t\tbreak;\n\t\ttask->tk_rebind_retry--;\n\t\trpc_delay(task, 3*HZ);\n\t\tgoto retry_timeout;\n\tcase -ETIMEDOUT:\n\t\tdprintk(\"RPC: %5u rpcbind request timed out\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tgoto retry_timeout;\n\tcase -EPFNOSUPPORT:\n\t\t/* server doesn't support any rpcbind version we know of */\n\t\tdprintk(\"RPC: %5u unrecognized remote rpcbind service\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tbreak;\n\tcase -EPROTONOSUPPORT:\n\t\tdprintk(\"RPC: %5u remote rpcbind version unavailable, retrying\\n\",\n\t\t\t\ttask->tk_pid);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_bind;\n\t\treturn;\n\tcase -ECONNREFUSED:\t\t/* connection problems */\n\tcase -ECONNRESET:\n\tcase -ENOTCONN:\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\tcase -EPIPE:\n\t\tdprintk(\"RPC: %5u remote rpcbind unreachable: %d\\n\",\n\t\t\t\ttask->tk_pid, task->tk_status);\n\t\tif (!RPC_IS_SOFTCONN(task)) {\n\t\t\trpc_delay(task, 5*HZ);\n\t\t\tgoto retry_timeout;\n\t\t}\n\t\tstatus = task->tk_status;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC: %5u unrecognized rpcbind error (%d)\\n\",\n\t\t\t\ttask->tk_pid, -task->tk_status);\n\t}\n\n\trpc_exit(task, status);\n\treturn;\n\nretry_timeout:\n\ttask->tk_action = call_timeout;\n}\n\n/*\n * 4b.\tConnect to the RPC server\n */\nstatic void\ncall_connect(struct rpc_task *task)\n{\n\tstruct rpc_xprt *xprt = task->tk_xprt;\n\n\tdprintk(\"RPC: %5u call_connect xprt %p %s connected\\n\",\n\t\t\ttask->tk_pid, xprt,\n\t\t\t(xprt_connected(xprt) ? \"is\" : \"is not\"));\n\n\ttask->tk_action = call_transmit;\n\tif (!xprt_connected(xprt)) {\n\t\ttask->tk_action = call_connect_status;\n\t\tif (task->tk_status < 0)\n\t\t\treturn;\n\t\txprt_connect(task);\n\t}\n}\n\n/*\n * 4c.\tSort out connect result\n */\nstatic void\ncall_connect_status(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\tint status = task->tk_status;\n\n\tdprint_status(task);\n\n\ttask->tk_status = 0;\n\tif (status >= 0 || status == -EAGAIN) {\n\t\tclnt->cl_stats->netreconn++;\n\t\ttask->tk_action = call_transmit;\n\t\treturn;\n\t}\n\n\tswitch (status) {\n\t\t/* if soft mounted, test if we've timed out */\n\tcase -ETIMEDOUT:\n\t\ttask->tk_action = call_timeout;\n\t\tbreak;\n\tdefault:\n\t\trpc_exit(task, -EIO);\n\t}\n}\n\n/*\n * 5.\tTransmit the RPC request, and wait for reply\n */\nstatic void\ncall_transmit(struct rpc_task *task)\n{\n\tdprint_status(task);\n\n\ttask->tk_action = call_status;\n\tif (task->tk_status < 0)\n\t\treturn;\n\ttask->tk_status = xprt_prepare_transmit(task);\n\tif (task->tk_status != 0)\n\t\treturn;\n\ttask->tk_action = call_transmit_status;\n\t/* Encode here so that rpcsec_gss can use correct sequence number. */\n\tif (rpc_task_need_encode(task)) {\n\t\tBUG_ON(task->tk_rqstp->rq_bytes_sent != 0);\n\t\trpc_xdr_encode(task);\n\t\t/* Did the encode result in an error condition? */\n\t\tif (task->tk_status != 0) {\n\t\t\t/* Was the error nonfatal? */\n\t\t\tif (task->tk_status == -EAGAIN)\n\t\t\t\trpc_delay(task, HZ >> 4);\n\t\t\telse\n\t\t\t\trpc_exit(task, task->tk_status);\n\t\t\treturn;\n\t\t}\n\t}\n\txprt_transmit(task);\n\tif (task->tk_status < 0)\n\t\treturn;\n\t/*\n\t * On success, ensure that we call xprt_end_transmit() before sleeping\n\t * in order to allow access to the socket to other RPC requests.\n\t */\n\tcall_transmit_status(task);\n\tif (rpc_reply_expected(task))\n\t\treturn;\n\ttask->tk_action = rpc_exit_task;\n\trpc_wake_up_queued_task(&task->tk_xprt->pending, task);\n}\n\n/*\n * 5a.\tHandle cleanup after a transmission\n */\nstatic void\ncall_transmit_status(struct rpc_task *task)\n{\n\ttask->tk_action = call_status;\n\n\t/*\n\t * Common case: success.  Force the compiler to put this\n\t * test first.\n\t */\n\tif (task->tk_status == 0) {\n\t\txprt_end_transmit(task);\n\t\trpc_task_force_reencode(task);\n\t\treturn;\n\t}\n\n\tswitch (task->tk_status) {\n\tcase -EAGAIN:\n\t\tbreak;\n\tdefault:\n\t\tdprint_status(task);\n\t\txprt_end_transmit(task);\n\t\trpc_task_force_reencode(task);\n\t\tbreak;\n\t\t/*\n\t\t * Special cases: if we've been waiting on the\n\t\t * socket's write_space() callback, or if the\n\t\t * socket just returned a connection error,\n\t\t * then hold onto the transport lock.\n\t\t */\n\tcase -ECONNREFUSED:\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\t\tif (RPC_IS_SOFTCONN(task)) {\n\t\t\txprt_end_transmit(task);\n\t\t\trpc_exit(task, task->tk_status);\n\t\t\tbreak;\n\t\t}\n\tcase -ECONNRESET:\n\tcase -ENOTCONN:\n\tcase -EPIPE:\n\t\trpc_task_force_reencode(task);\n\t}\n}\n\n#if defined(CONFIG_NFS_V4_1)\n/*\n * 5b.\tSend the backchannel RPC reply.  On error, drop the reply.  In\n * addition, disconnect on connectivity errors.\n */\nstatic void\ncall_bc_transmit(struct rpc_task *task)\n{\n\tstruct rpc_rqst *req = task->tk_rqstp;\n\n\tBUG_ON(task->tk_status != 0);\n\ttask->tk_status = xprt_prepare_transmit(task);\n\tif (task->tk_status == -EAGAIN) {\n\t\t/*\n\t\t * Could not reserve the transport. Try again after the\n\t\t * transport is released.\n\t\t */\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_bc_transmit;\n\t\treturn;\n\t}\n\n\ttask->tk_action = rpc_exit_task;\n\tif (task->tk_status < 0) {\n\t\tprintk(KERN_NOTICE \"RPC: Could not send backchannel reply \"\n\t\t\t\"error: %d\\n\", task->tk_status);\n\t\treturn;\n\t}\n\n\txprt_transmit(task);\n\txprt_end_transmit(task);\n\tdprint_status(task);\n\tswitch (task->tk_status) {\n\tcase 0:\n\t\t/* Success */\n\t\tbreak;\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\tcase -ETIMEDOUT:\n\t\t/*\n\t\t * Problem reaching the server.  Disconnect and let the\n\t\t * forechannel reestablish the connection.  The server will\n\t\t * have to retransmit the backchannel request and we'll\n\t\t * reprocess it.  Since these ops are idempotent, there's no\n\t\t * need to cache our reply at this time.\n\t\t */\n\t\tprintk(KERN_NOTICE \"RPC: Could not send backchannel reply \"\n\t\t\t\"error: %d\\n\", task->tk_status);\n\t\txprt_conditional_disconnect(task->tk_xprt,\n\t\t\treq->rq_connect_cookie);\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * We were unable to reply and will have to drop the\n\t\t * request.  The server should reconnect and retransmit.\n\t\t */\n\t\tBUG_ON(task->tk_status == -EAGAIN);\n\t\tprintk(KERN_NOTICE \"RPC: Could not send backchannel reply \"\n\t\t\t\"error: %d\\n\", task->tk_status);\n\t\tbreak;\n\t}\n\trpc_wake_up_queued_task(&req->rq_xprt->pending, task);\n}\n#endif /* CONFIG_NFS_V4_1 */\n\n/*\n * 6.\tSort out the RPC call status\n */\nstatic void\ncall_status(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tint\t\tstatus;\n\n\tif (req->rq_reply_bytes_recvd > 0 && !req->rq_bytes_sent)\n\t\ttask->tk_status = req->rq_reply_bytes_recvd;\n\n\tdprint_status(task);\n\n\tstatus = task->tk_status;\n\tif (status >= 0) {\n\t\ttask->tk_action = call_decode;\n\t\treturn;\n\t}\n\n\ttask->tk_status = 0;\n\tswitch(status) {\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\t\t/*\n\t\t * Delay any retries for 3 seconds, then handle as if it\n\t\t * were a timeout.\n\t\t */\n\t\trpc_delay(task, 3*HZ);\n\tcase -ETIMEDOUT:\n\t\ttask->tk_action = call_timeout;\n\t\tif (task->tk_client->cl_discrtry)\n\t\t\txprt_conditional_disconnect(task->tk_xprt,\n\t\t\t\t\treq->rq_connect_cookie);\n\t\tbreak;\n\tcase -ECONNRESET:\n\tcase -ECONNREFUSED:\n\t\trpc_force_rebind(clnt);\n\t\trpc_delay(task, 3*HZ);\n\tcase -EPIPE:\n\tcase -ENOTCONN:\n\t\ttask->tk_action = call_bind;\n\t\tbreak;\n\tcase -EAGAIN:\n\t\ttask->tk_action = call_transmit;\n\t\tbreak;\n\tcase -EIO:\n\t\t/* shutdown or soft timeout */\n\t\trpc_exit(task, status);\n\t\tbreak;\n\tdefault:\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(\"%s: RPC call returned error %d\\n\",\n\t\t\t       clnt->cl_protname, -status);\n\t\trpc_exit(task, status);\n\t}\n}\n\n/*\n * 6a.\tHandle RPC timeout\n * \tWe do not release the request slot, so we keep using the\n *\tsame XID for all retransmits.\n */\nstatic void\ncall_timeout(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\n\tif (xprt_adjust_timeout(task->tk_rqstp) == 0) {\n\t\tdprintk(\"RPC: %5u call_timeout (minor)\\n\", task->tk_pid);\n\t\tgoto retry;\n\t}\n\n\tdprintk(\"RPC: %5u call_timeout (major)\\n\", task->tk_pid);\n\ttask->tk_timeouts++;\n\n\tif (RPC_IS_SOFTCONN(task)) {\n\t\trpc_exit(task, -ETIMEDOUT);\n\t\treturn;\n\t}\n\tif (RPC_IS_SOFT(task)) {\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(KERN_NOTICE \"%s: server %s not responding, timed out\\n\",\n\t\t\t\tclnt->cl_protname, clnt->cl_server);\n\t\tif (task->tk_flags & RPC_TASK_TIMEOUT)\n\t\t\trpc_exit(task, -ETIMEDOUT);\n\t\telse\n\t\t\trpc_exit(task, -EIO);\n\t\treturn;\n\t}\n\n\tif (!(task->tk_flags & RPC_CALL_MAJORSEEN)) {\n\t\ttask->tk_flags |= RPC_CALL_MAJORSEEN;\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(KERN_NOTICE \"%s: server %s not responding, still trying\\n\",\n\t\t\tclnt->cl_protname, clnt->cl_server);\n\t}\n\trpc_force_rebind(clnt);\n\t/*\n\t * Did our request time out due to an RPCSEC_GSS out-of-sequence\n\t * event? RFC2203 requires the server to drop all such requests.\n\t */\n\trpcauth_invalcred(task);\n\nretry:\n\tclnt->cl_stats->rpcretrans++;\n\ttask->tk_action = call_bind;\n\ttask->tk_status = 0;\n}\n\n/*\n * 7.\tDecode the RPC reply\n */\nstatic void\ncall_decode(struct rpc_task *task)\n{\n\tstruct rpc_clnt\t*clnt = task->tk_client;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\tkxdrdproc_t\tdecode = task->tk_msg.rpc_proc->p_decode;\n\t__be32\t\t*p;\n\n\tdprintk(\"RPC: %5u call_decode (status %d)\\n\",\n\t\t\ttask->tk_pid, task->tk_status);\n\n\tif (task->tk_flags & RPC_CALL_MAJORSEEN) {\n\t\tif (clnt->cl_chatty)\n\t\t\tprintk(KERN_NOTICE \"%s: server %s OK\\n\",\n\t\t\t\tclnt->cl_protname, clnt->cl_server);\n\t\ttask->tk_flags &= ~RPC_CALL_MAJORSEEN;\n\t}\n\n\t/*\n\t * Ensure that we see all writes made by xprt_complete_rqst()\n\t * before it changed req->rq_reply_bytes_recvd.\n\t */\n\tsmp_rmb();\n\treq->rq_rcv_buf.len = req->rq_private_buf.len;\n\n\t/* Check that the softirq receive buffer is valid */\n\tWARN_ON(memcmp(&req->rq_rcv_buf, &req->rq_private_buf,\n\t\t\t\tsizeof(req->rq_rcv_buf)) != 0);\n\n\tif (req->rq_rcv_buf.len < 12) {\n\t\tif (!RPC_IS_SOFT(task)) {\n\t\t\ttask->tk_action = call_bind;\n\t\t\tclnt->cl_stats->rpcretrans++;\n\t\t\tgoto out_retry;\n\t\t}\n\t\tdprintk(\"RPC:       %s: too small RPC reply size (%d bytes)\\n\",\n\t\t\t\tclnt->cl_protname, task->tk_status);\n\t\ttask->tk_action = call_timeout;\n\t\tgoto out_retry;\n\t}\n\n\tp = rpc_verify_header(task);\n\tif (IS_ERR(p)) {\n\t\tif (p == ERR_PTR(-EAGAIN))\n\t\t\tgoto out_retry;\n\t\treturn;\n\t}\n\n\ttask->tk_action = rpc_exit_task;\n\n\tif (decode) {\n\t\ttask->tk_status = rpcauth_unwrap_resp(task, decode, req, p,\n\t\t\t\t\t\t      task->tk_msg.rpc_resp);\n\t}\n\tdprintk(\"RPC: %5u call_decode result %d\\n\", task->tk_pid,\n\t\t\ttask->tk_status);\n\treturn;\nout_retry:\n\ttask->tk_status = 0;\n\t/* Note: rpc_verify_header() may have freed the RPC slot */\n\tif (task->tk_rqstp == req) {\n\t\treq->rq_reply_bytes_recvd = req->rq_rcv_buf.len = 0;\n\t\tif (task->tk_client->cl_discrtry)\n\t\t\txprt_conditional_disconnect(task->tk_xprt,\n\t\t\t\t\treq->rq_connect_cookie);\n\t}\n}\n\nstatic __be32 *\nrpc_encode_header(struct rpc_task *task)\n{\n\tstruct rpc_clnt *clnt = task->tk_client;\n\tstruct rpc_rqst\t*req = task->tk_rqstp;\n\t__be32\t\t*p = req->rq_svec[0].iov_base;\n\n\t/* FIXME: check buffer size? */\n\n\tp = xprt_skip_transport_header(task->tk_xprt, p);\n\t*p++ = req->rq_xid;\t\t/* XID */\n\t*p++ = htonl(RPC_CALL);\t\t/* CALL */\n\t*p++ = htonl(RPC_VERSION);\t/* RPC version */\n\t*p++ = htonl(clnt->cl_prog);\t/* program number */\n\t*p++ = htonl(clnt->cl_vers);\t/* program version */\n\t*p++ = htonl(task->tk_msg.rpc_proc->p_proc);\t/* procedure */\n\tp = rpcauth_marshcred(task, p);\n\treq->rq_slen = xdr_adjust_iovec(&req->rq_svec[0], p);\n\treturn p;\n}\n\nstatic __be32 *\nrpc_verify_header(struct rpc_task *task)\n{\n\tstruct kvec *iov = &task->tk_rqstp->rq_rcv_buf.head[0];\n\tint len = task->tk_rqstp->rq_rcv_buf.len >> 2;\n\t__be32\t*p = iov->iov_base;\n\tu32 n;\n\tint error = -EACCES;\n\n\tif ((task->tk_rqstp->rq_rcv_buf.len & 3) != 0) {\n\t\t/* RFC-1014 says that the representation of XDR data must be a\n\t\t * multiple of four bytes\n\t\t * - if it isn't pointer subtraction in the NFS client may give\n\t\t *   undefined results\n\t\t */\n\t\tdprintk(\"RPC: %5u %s: XDR representation not a multiple of\"\n\t\t       \" 4 bytes: 0x%x\\n\", task->tk_pid, __func__,\n\t\t       task->tk_rqstp->rq_rcv_buf.len);\n\t\tgoto out_eio;\n\t}\n\tif ((len -= 3) < 0)\n\t\tgoto out_overflow;\n\n\tp += 1; /* skip XID */\n\tif ((n = ntohl(*p++)) != RPC_REPLY) {\n\t\tdprintk(\"RPC: %5u %s: not an RPC reply: %x\\n\",\n\t\t\ttask->tk_pid, __func__, n);\n\t\tgoto out_garbage;\n\t}\n\n\tif ((n = ntohl(*p++)) != RPC_MSG_ACCEPTED) {\n\t\tif (--len < 0)\n\t\t\tgoto out_overflow;\n\t\tswitch ((n = ntohl(*p++))) {\n\t\t\tcase RPC_AUTH_ERROR:\n\t\t\t\tbreak;\n\t\t\tcase RPC_MISMATCH:\n\t\t\t\tdprintk(\"RPC: %5u %s: RPC call version \"\n\t\t\t\t\t\t\"mismatch!\\n\",\n\t\t\t\t\t\ttask->tk_pid, __func__);\n\t\t\t\terror = -EPROTONOSUPPORT;\n\t\t\t\tgoto out_err;\n\t\t\tdefault:\n\t\t\t\tdprintk(\"RPC: %5u %s: RPC call rejected, \"\n\t\t\t\t\t\t\"unknown error: %x\\n\",\n\t\t\t\t\t\ttask->tk_pid, __func__, n);\n\t\t\t\tgoto out_eio;\n\t\t}\n\t\tif (--len < 0)\n\t\t\tgoto out_overflow;\n\t\tswitch ((n = ntohl(*p++))) {\n\t\tcase RPC_AUTH_REJECTEDCRED:\n\t\tcase RPC_AUTH_REJECTEDVERF:\n\t\tcase RPCSEC_GSS_CREDPROBLEM:\n\t\tcase RPCSEC_GSS_CTXPROBLEM:\n\t\t\tif (!task->tk_cred_retry)\n\t\t\t\tbreak;\n\t\t\ttask->tk_cred_retry--;\n\t\t\tdprintk(\"RPC: %5u %s: retry stale creds\\n\",\n\t\t\t\t\ttask->tk_pid, __func__);\n\t\t\trpcauth_invalcred(task);\n\t\t\t/* Ensure we obtain a new XID! */\n\t\t\txprt_release(task);\n\t\t\ttask->tk_action = call_reserve;\n\t\t\tgoto out_retry;\n\t\tcase RPC_AUTH_BADCRED:\n\t\tcase RPC_AUTH_BADVERF:\n\t\t\t/* possibly garbled cred/verf? */\n\t\t\tif (!task->tk_garb_retry)\n\t\t\t\tbreak;\n\t\t\ttask->tk_garb_retry--;\n\t\t\tdprintk(\"RPC: %5u %s: retry garbled creds\\n\",\n\t\t\t\t\ttask->tk_pid, __func__);\n\t\t\ttask->tk_action = call_bind;\n\t\t\tgoto out_retry;\n\t\tcase RPC_AUTH_TOOWEAK:\n\t\t\tprintk(KERN_NOTICE \"RPC: server %s requires stronger \"\n\t\t\t       \"authentication.\\n\", task->tk_client->cl_server);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdprintk(\"RPC: %5u %s: unknown auth error: %x\\n\",\n\t\t\t\t\ttask->tk_pid, __func__, n);\n\t\t\terror = -EIO;\n\t\t}\n\t\tdprintk(\"RPC: %5u %s: call rejected %d\\n\",\n\t\t\t\ttask->tk_pid, __func__, n);\n\t\tgoto out_err;\n\t}\n\tif (!(p = rpcauth_checkverf(task, p))) {\n\t\tdprintk(\"RPC: %5u %s: auth check failed\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\tgoto out_garbage;\t\t/* bad verifier, retry */\n\t}\n\tlen = p - (__be32 *)iov->iov_base - 1;\n\tif (len < 0)\n\t\tgoto out_overflow;\n\tswitch ((n = ntohl(*p++))) {\n\tcase RPC_SUCCESS:\n\t\treturn p;\n\tcase RPC_PROG_UNAVAIL:\n\t\tdprintk(\"RPC: %5u %s: program %u is unsupported by server %s\\n\",\n\t\t\t\ttask->tk_pid, __func__,\n\t\t\t\t(unsigned int)task->tk_client->cl_prog,\n\t\t\t\ttask->tk_client->cl_server);\n\t\terror = -EPFNOSUPPORT;\n\t\tgoto out_err;\n\tcase RPC_PROG_MISMATCH:\n\t\tdprintk(\"RPC: %5u %s: program %u, version %u unsupported by \"\n\t\t\t\t\"server %s\\n\", task->tk_pid, __func__,\n\t\t\t\t(unsigned int)task->tk_client->cl_prog,\n\t\t\t\t(unsigned int)task->tk_client->cl_vers,\n\t\t\t\ttask->tk_client->cl_server);\n\t\terror = -EPROTONOSUPPORT;\n\t\tgoto out_err;\n\tcase RPC_PROC_UNAVAIL:\n\t\tdprintk(\"RPC: %5u %s: proc %s unsupported by program %u, \"\n\t\t\t\t\"version %u on server %s\\n\",\n\t\t\t\ttask->tk_pid, __func__,\n\t\t\t\trpc_proc_name(task),\n\t\t\t\ttask->tk_client->cl_prog,\n\t\t\t\ttask->tk_client->cl_vers,\n\t\t\t\ttask->tk_client->cl_server);\n\t\terror = -EOPNOTSUPP;\n\t\tgoto out_err;\n\tcase RPC_GARBAGE_ARGS:\n\t\tdprintk(\"RPC: %5u %s: server saw garbage\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\tbreak;\t\t\t/* retry */\n\tdefault:\n\t\tdprintk(\"RPC: %5u %s: server accept status: %x\\n\",\n\t\t\t\ttask->tk_pid, __func__, n);\n\t\t/* Also retry */\n\t}\n\nout_garbage:\n\ttask->tk_client->cl_stats->rpcgarbage++;\n\tif (task->tk_garb_retry) {\n\t\ttask->tk_garb_retry--;\n\t\tdprintk(\"RPC: %5u %s: retrying\\n\",\n\t\t\t\ttask->tk_pid, __func__);\n\t\ttask->tk_action = call_bind;\nout_retry:\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\nout_eio:\n\terror = -EIO;\nout_err:\n\trpc_exit(task, error);\n\tdprintk(\"RPC: %5u %s: call failed with error %d\\n\", task->tk_pid,\n\t\t\t__func__, error);\n\treturn ERR_PTR(error);\nout_overflow:\n\tdprintk(\"RPC: %5u %s: server reply was truncated.\\n\", task->tk_pid,\n\t\t\t__func__);\n\tgoto out_garbage;\n}\n\nstatic void rpcproc_encode_null(void *rqstp, struct xdr_stream *xdr, void *obj)\n{\n}\n\nstatic int rpcproc_decode_null(void *rqstp, struct xdr_stream *xdr, void *obj)\n{\n\treturn 0;\n}\n\nstatic struct rpc_procinfo rpcproc_null = {\n\t.p_encode = rpcproc_encode_null,\n\t.p_decode = rpcproc_decode_null,\n};\n\nstatic int rpc_ping(struct rpc_clnt *clnt)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &rpcproc_null,\n\t};\n\tint err;\n\tmsg.rpc_cred = authnull_ops.lookup_cred(NULL, NULL, 0);\n\terr = rpc_call_sync(clnt, &msg, RPC_TASK_SOFT | RPC_TASK_SOFTCONN);\n\tput_rpccred(msg.rpc_cred);\n\treturn err;\n}\n\nstruct rpc_task *rpc_call_null(struct rpc_clnt *clnt, struct rpc_cred *cred, int flags)\n{\n\tstruct rpc_message msg = {\n\t\t.rpc_proc = &rpcproc_null,\n\t\t.rpc_cred = cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = &rpc_default_ops,\n\t\t.flags = flags,\n\t};\n\treturn rpc_run_task(&task_setup_data);\n}\nEXPORT_SYMBOL_GPL(rpc_call_null);\n\n#ifdef RPC_DEBUG\nstatic void rpc_show_header(void)\n{\n\tprintk(KERN_INFO \"-pid- flgs status -client- --rqstp- \"\n\t\t\"-timeout ---ops--\\n\");\n}\n\nstatic void rpc_show_task(const struct rpc_clnt *clnt,\n\t\t\t  const struct rpc_task *task)\n{\n\tconst char *rpc_waitq = \"none\";\n\n\tif (RPC_IS_QUEUED(task))\n\t\trpc_waitq = rpc_qname(task->tk_waitqueue);\n\n\tprintk(KERN_INFO \"%5u %04x %6d %8p %8p %8ld %8p %sv%u %s a:%ps q:%s\\n\",\n\t\ttask->tk_pid, task->tk_flags, task->tk_status,\n\t\tclnt, task->tk_rqstp, task->tk_timeout, task->tk_ops,\n\t\tclnt->cl_protname, clnt->cl_vers, rpc_proc_name(task),\n\t\ttask->tk_action, rpc_waitq);\n}\n\nvoid rpc_show_tasks(void)\n{\n\tstruct rpc_clnt *clnt;\n\tstruct rpc_task *task;\n\tint header = 0;\n\n\tspin_lock(&rpc_client_lock);\n\tlist_for_each_entry(clnt, &all_clients, cl_clients) {\n\t\tspin_lock(&clnt->cl_lock);\n\t\tlist_for_each_entry(task, &clnt->cl_tasks, tk_task) {\n\t\t\tif (!header) {\n\t\t\t\trpc_show_header();\n\t\t\t\theader++;\n\t\t\t}\n\t\t\trpc_show_task(clnt, task);\n\t\t}\n\t\tspin_unlock(&clnt->cl_lock);\n\t}\n\tspin_unlock(&rpc_client_lock);\n}\n#endif\n", "/*\n * linux/net/sunrpc/sched.c\n *\n * Scheduling for synchronous and asynchronous RPC requests.\n *\n * Copyright (C) 1996 Olaf Kirch, <okir@monad.swb.de>\n *\n * TCP NFS related read + write fixes\n * (C) 1999 Dave Airlie, University of Limerick, Ireland <airlied@linux.ie>\n */\n\n#include <linux/module.h>\n\n#include <linux/sched.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/mempool.h>\n#include <linux/smp.h>\n#include <linux/spinlock.h>\n#include <linux/mutex.h>\n\n#include <linux/sunrpc/clnt.h>\n\n#include \"sunrpc.h\"\n\n#ifdef RPC_DEBUG\n#define RPCDBG_FACILITY\t\tRPCDBG_SCHED\n#endif\n\n/*\n * RPC slabs and memory pools\n */\n#define RPC_BUFFER_MAXSIZE\t(2048)\n#define RPC_BUFFER_POOLSIZE\t(8)\n#define RPC_TASK_POOLSIZE\t(8)\nstatic struct kmem_cache\t*rpc_task_slabp __read_mostly;\nstatic struct kmem_cache\t*rpc_buffer_slabp __read_mostly;\nstatic mempool_t\t*rpc_task_mempool __read_mostly;\nstatic mempool_t\t*rpc_buffer_mempool __read_mostly;\n\nstatic void\t\t\trpc_async_schedule(struct work_struct *);\nstatic void\t\t\t rpc_release_task(struct rpc_task *task);\nstatic void __rpc_queue_timer_fn(unsigned long ptr);\n\n/*\n * RPC tasks sit here while waiting for conditions to improve.\n */\nstatic struct rpc_wait_queue delay_queue;\n\n/*\n * rpciod-related stuff\n */\nstruct workqueue_struct *rpciod_workqueue;\n\n/*\n * Disable the timer for a given RPC task. Should be called with\n * queue->lock and bh_disabled in order to avoid races within\n * rpc_run_timer().\n */\nstatic void\n__rpc_disable_timer(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (task->tk_timeout == 0)\n\t\treturn;\n\tdprintk(\"RPC: %5u disabling timer\\n\", task->tk_pid);\n\ttask->tk_timeout = 0;\n\tlist_del(&task->u.tk_wait.timer_list);\n\tif (list_empty(&queue->timer_list.list))\n\t\tdel_timer(&queue->timer_list.timer);\n}\n\nstatic void\nrpc_set_queue_timer(struct rpc_wait_queue *queue, unsigned long expires)\n{\n\tqueue->timer_list.expires = expires;\n\tmod_timer(&queue->timer_list.timer, expires);\n}\n\n/*\n * Set up a timer for the current task.\n */\nstatic void\n__rpc_add_timer(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (!task->tk_timeout)\n\t\treturn;\n\n\tdprintk(\"RPC: %5u setting alarm for %lu ms\\n\",\n\t\t\ttask->tk_pid, task->tk_timeout * 1000 / HZ);\n\n\ttask->u.tk_wait.expires = jiffies + task->tk_timeout;\n\tif (list_empty(&queue->timer_list.list) || time_before(task->u.tk_wait.expires, queue->timer_list.expires))\n\t\trpc_set_queue_timer(queue, task->u.tk_wait.expires);\n\tlist_add(&task->u.tk_wait.timer_list, &queue->timer_list.list);\n}\n\n/*\n * Add new request to a priority queue.\n */\nstatic void __rpc_add_wait_queue_priority(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tstruct list_head *q;\n\tstruct rpc_task *t;\n\n\tINIT_LIST_HEAD(&task->u.tk_wait.links);\n\tq = &queue->tasks[task->tk_priority];\n\tif (unlikely(task->tk_priority > queue->maxpriority))\n\t\tq = &queue->tasks[queue->maxpriority];\n\tlist_for_each_entry(t, q, u.tk_wait.list) {\n\t\tif (t->tk_owner == task->tk_owner) {\n\t\t\tlist_add_tail(&task->u.tk_wait.list, &t->u.tk_wait.links);\n\t\t\treturn;\n\t\t}\n\t}\n\tlist_add_tail(&task->u.tk_wait.list, q);\n}\n\n/*\n * Add new request to wait queue.\n *\n * Swapper tasks always get inserted at the head of the queue.\n * This should avoid many nasty memory deadlocks and hopefully\n * improve overall performance.\n * Everyone else gets appended to the queue to ensure proper FIFO behavior.\n */\nstatic void __rpc_add_wait_queue(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tBUG_ON (RPC_IS_QUEUED(task));\n\n\tif (RPC_IS_PRIORITY(queue))\n\t\t__rpc_add_wait_queue_priority(queue, task);\n\telse if (RPC_IS_SWAPPER(task))\n\t\tlist_add(&task->u.tk_wait.list, &queue->tasks[0]);\n\telse\n\t\tlist_add_tail(&task->u.tk_wait.list, &queue->tasks[0]);\n\ttask->tk_waitqueue = queue;\n\tqueue->qlen++;\n\trpc_set_queued(task);\n\n\tdprintk(\"RPC: %5u added to queue %p \\\"%s\\\"\\n\",\n\t\t\ttask->tk_pid, queue, rpc_qname(queue));\n}\n\n/*\n * Remove request from a priority queue.\n */\nstatic void __rpc_remove_wait_queue_priority(struct rpc_task *task)\n{\n\tstruct rpc_task *t;\n\n\tif (!list_empty(&task->u.tk_wait.links)) {\n\t\tt = list_entry(task->u.tk_wait.links.next, struct rpc_task, u.tk_wait.list);\n\t\tlist_move(&t->u.tk_wait.list, &task->u.tk_wait.list);\n\t\tlist_splice_init(&task->u.tk_wait.links, &t->u.tk_wait.links);\n\t}\n}\n\n/*\n * Remove request from queue.\n * Note: must be called with spin lock held.\n */\nstatic void __rpc_remove_wait_queue(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\t__rpc_disable_timer(queue, task);\n\tif (RPC_IS_PRIORITY(queue))\n\t\t__rpc_remove_wait_queue_priority(task);\n\tlist_del(&task->u.tk_wait.list);\n\tqueue->qlen--;\n\tdprintk(\"RPC: %5u removed from queue %p \\\"%s\\\"\\n\",\n\t\t\ttask->tk_pid, queue, rpc_qname(queue));\n}\n\nstatic inline void rpc_set_waitqueue_priority(struct rpc_wait_queue *queue, int priority)\n{\n\tqueue->priority = priority;\n\tqueue->count = 1 << (priority * 2);\n}\n\nstatic inline void rpc_set_waitqueue_owner(struct rpc_wait_queue *queue, pid_t pid)\n{\n\tqueue->owner = pid;\n\tqueue->nr = RPC_BATCH_COUNT;\n}\n\nstatic inline void rpc_reset_waitqueue_priority(struct rpc_wait_queue *queue)\n{\n\trpc_set_waitqueue_priority(queue, queue->maxpriority);\n\trpc_set_waitqueue_owner(queue, 0);\n}\n\nstatic void __rpc_init_priority_wait_queue(struct rpc_wait_queue *queue, const char *qname, unsigned char nr_queues)\n{\n\tint i;\n\n\tspin_lock_init(&queue->lock);\n\tfor (i = 0; i < ARRAY_SIZE(queue->tasks); i++)\n\t\tINIT_LIST_HEAD(&queue->tasks[i]);\n\tqueue->maxpriority = nr_queues - 1;\n\trpc_reset_waitqueue_priority(queue);\n\tqueue->qlen = 0;\n\tsetup_timer(&queue->timer_list.timer, __rpc_queue_timer_fn, (unsigned long)queue);\n\tINIT_LIST_HEAD(&queue->timer_list.list);\n#ifdef RPC_DEBUG\n\tqueue->name = qname;\n#endif\n}\n\nvoid rpc_init_priority_wait_queue(struct rpc_wait_queue *queue, const char *qname)\n{\n\t__rpc_init_priority_wait_queue(queue, qname, RPC_NR_PRIORITY);\n}\nEXPORT_SYMBOL_GPL(rpc_init_priority_wait_queue);\n\nvoid rpc_init_wait_queue(struct rpc_wait_queue *queue, const char *qname)\n{\n\t__rpc_init_priority_wait_queue(queue, qname, 1);\n}\nEXPORT_SYMBOL_GPL(rpc_init_wait_queue);\n\nvoid rpc_destroy_wait_queue(struct rpc_wait_queue *queue)\n{\n\tdel_timer_sync(&queue->timer_list.timer);\n}\nEXPORT_SYMBOL_GPL(rpc_destroy_wait_queue);\n\nstatic int rpc_wait_bit_killable(void *word)\n{\n\tif (fatal_signal_pending(current))\n\t\treturn -ERESTARTSYS;\n\tschedule();\n\treturn 0;\n}\n\n#ifdef RPC_DEBUG\nstatic void rpc_task_set_debuginfo(struct rpc_task *task)\n{\n\tstatic atomic_t rpc_pid;\n\n\ttask->tk_pid = atomic_inc_return(&rpc_pid);\n}\n#else\nstatic inline void rpc_task_set_debuginfo(struct rpc_task *task)\n{\n}\n#endif\n\nstatic void rpc_set_active(struct rpc_task *task)\n{\n\trpc_task_set_debuginfo(task);\n\tset_bit(RPC_TASK_ACTIVE, &task->tk_runstate);\n}\n\n/*\n * Mark an RPC call as having completed by clearing the 'active' bit\n * and then waking up all tasks that were sleeping.\n */\nstatic int rpc_complete_task(struct rpc_task *task)\n{\n\tvoid *m = &task->tk_runstate;\n\twait_queue_head_t *wq = bit_waitqueue(m, RPC_TASK_ACTIVE);\n\tstruct wait_bit_key k = __WAIT_BIT_KEY_INITIALIZER(m, RPC_TASK_ACTIVE);\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&wq->lock, flags);\n\tclear_bit(RPC_TASK_ACTIVE, &task->tk_runstate);\n\tret = atomic_dec_and_test(&task->tk_count);\n\tif (waitqueue_active(wq))\n\t\t__wake_up_locked_key(wq, TASK_NORMAL, &k);\n\tspin_unlock_irqrestore(&wq->lock, flags);\n\treturn ret;\n}\n\n/*\n * Allow callers to wait for completion of an RPC call\n *\n * Note the use of out_of_line_wait_on_bit() rather than wait_on_bit()\n * to enforce taking of the wq->lock and hence avoid races with\n * rpc_complete_task().\n */\nint __rpc_wait_for_completion_task(struct rpc_task *task, int (*action)(void *))\n{\n\tif (action == NULL)\n\t\taction = rpc_wait_bit_killable;\n\treturn out_of_line_wait_on_bit(&task->tk_runstate, RPC_TASK_ACTIVE,\n\t\t\taction, TASK_KILLABLE);\n}\nEXPORT_SYMBOL_GPL(__rpc_wait_for_completion_task);\n\n/*\n * Make an RPC task runnable.\n *\n * Note: If the task is ASYNC, this must be called with\n * the spinlock held to protect the wait queue operation.\n */\nstatic void rpc_make_runnable(struct rpc_task *task)\n{\n\trpc_clear_queued(task);\n\tif (rpc_test_and_set_running(task))\n\t\treturn;\n\tif (RPC_IS_ASYNC(task)) {\n\t\tINIT_WORK(&task->u.tk_work, rpc_async_schedule);\n\t\tqueue_work(rpciod_workqueue, &task->u.tk_work);\n\t} else\n\t\twake_up_bit(&task->tk_runstate, RPC_TASK_QUEUED);\n}\n\n/*\n * Prepare for sleeping on a wait queue.\n * By always appending tasks to the list we ensure FIFO behavior.\n * NB: An RPC task will only receive interrupt-driven events as long\n * as it's on a wait queue.\n */\nstatic void __rpc_sleep_on(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\t\trpc_action action)\n{\n\tdprintk(\"RPC: %5u sleep_on(queue \\\"%s\\\" time %lu)\\n\",\n\t\t\ttask->tk_pid, rpc_qname(q), jiffies);\n\n\t__rpc_add_wait_queue(q, task);\n\n\tBUG_ON(task->tk_callback != NULL);\n\ttask->tk_callback = action;\n\t__rpc_add_timer(q, task);\n}\n\nvoid rpc_sleep_on(struct rpc_wait_queue *q, struct rpc_task *task,\n\t\t\t\trpc_action action)\n{\n\t/* We shouldn't ever put an inactive task to sleep */\n\tBUG_ON(!RPC_IS_ACTIVATED(task));\n\n\t/*\n\t * Protect the queue operations.\n\t */\n\tspin_lock_bh(&q->lock);\n\t__rpc_sleep_on(q, task, action);\n\tspin_unlock_bh(&q->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_sleep_on);\n\n/**\n * __rpc_do_wake_up_task - wake up a single rpc_task\n * @queue: wait queue\n * @task: task to be woken up\n *\n * Caller must hold queue->lock, and have cleared the task queued flag.\n */\nstatic void __rpc_do_wake_up_task(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tdprintk(\"RPC: %5u __rpc_wake_up_task (now %lu)\\n\",\n\t\t\ttask->tk_pid, jiffies);\n\n\t/* Has the task been executed yet? If not, we cannot wake it up! */\n\tif (!RPC_IS_ACTIVATED(task)) {\n\t\tprintk(KERN_ERR \"RPC: Inactive task (%p) being woken up!\\n\", task);\n\t\treturn;\n\t}\n\n\t__rpc_remove_wait_queue(queue, task);\n\n\trpc_make_runnable(task);\n\n\tdprintk(\"RPC:       __rpc_wake_up_task done\\n\");\n}\n\n/*\n * Wake up a queued task while the queue lock is being held\n */\nstatic void rpc_wake_up_task_queue_locked(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tif (RPC_IS_QUEUED(task) && task->tk_waitqueue == queue)\n\t\t__rpc_do_wake_up_task(queue, task);\n}\n\n/*\n * Tests whether rpc queue is empty\n */\nint rpc_queue_empty(struct rpc_wait_queue *queue)\n{\n\tint res;\n\n\tspin_lock_bh(&queue->lock);\n\tres = queue->qlen;\n\tspin_unlock_bh(&queue->lock);\n\treturn res == 0;\n}\nEXPORT_SYMBOL_GPL(rpc_queue_empty);\n\n/*\n * Wake up a task on a specific queue\n */\nvoid rpc_wake_up_queued_task(struct rpc_wait_queue *queue, struct rpc_task *task)\n{\n\tspin_lock_bh(&queue->lock);\n\trpc_wake_up_task_queue_locked(queue, task);\n\tspin_unlock_bh(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_queued_task);\n\n/*\n * Wake up the next task on a priority queue.\n */\nstatic struct rpc_task * __rpc_wake_up_next_priority(struct rpc_wait_queue *queue)\n{\n\tstruct list_head *q;\n\tstruct rpc_task *task;\n\n\t/*\n\t * Service a batch of tasks from a single owner.\n\t */\n\tq = &queue->tasks[queue->priority];\n\tif (!list_empty(q)) {\n\t\ttask = list_entry(q->next, struct rpc_task, u.tk_wait.list);\n\t\tif (queue->owner == task->tk_owner) {\n\t\t\tif (--queue->nr)\n\t\t\t\tgoto out;\n\t\t\tlist_move_tail(&task->u.tk_wait.list, q);\n\t\t}\n\t\t/*\n\t\t * Check if we need to switch queues.\n\t\t */\n\t\tif (--queue->count)\n\t\t\tgoto new_owner;\n\t}\n\n\t/*\n\t * Service the next queue.\n\t */\n\tdo {\n\t\tif (q == &queue->tasks[0])\n\t\t\tq = &queue->tasks[queue->maxpriority];\n\t\telse\n\t\t\tq = q - 1;\n\t\tif (!list_empty(q)) {\n\t\t\ttask = list_entry(q->next, struct rpc_task, u.tk_wait.list);\n\t\t\tgoto new_queue;\n\t\t}\n\t} while (q != &queue->tasks[queue->priority]);\n\n\trpc_reset_waitqueue_priority(queue);\n\treturn NULL;\n\nnew_queue:\n\trpc_set_waitqueue_priority(queue, (unsigned int)(q - &queue->tasks[0]));\nnew_owner:\n\trpc_set_waitqueue_owner(queue, task->tk_owner);\nout:\n\trpc_wake_up_task_queue_locked(queue, task);\n\treturn task;\n}\n\n/*\n * Wake up the next task on the wait queue.\n */\nstruct rpc_task * rpc_wake_up_next(struct rpc_wait_queue *queue)\n{\n\tstruct rpc_task\t*task = NULL;\n\n\tdprintk(\"RPC:       wake_up_next(%p \\\"%s\\\")\\n\",\n\t\t\tqueue, rpc_qname(queue));\n\tspin_lock_bh(&queue->lock);\n\tif (RPC_IS_PRIORITY(queue))\n\t\ttask = __rpc_wake_up_next_priority(queue);\n\telse {\n\t\ttask_for_first(task, &queue->tasks[0])\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t}\n\tspin_unlock_bh(&queue->lock);\n\n\treturn task;\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_next);\n\n/**\n * rpc_wake_up - wake up all rpc_tasks\n * @queue: rpc_wait_queue on which the tasks are sleeping\n *\n * Grabs queue->lock\n */\nvoid rpc_wake_up(struct rpc_wait_queue *queue)\n{\n\tstruct rpc_task *task, *next;\n\tstruct list_head *head;\n\n\tspin_lock_bh(&queue->lock);\n\thead = &queue->tasks[queue->maxpriority];\n\tfor (;;) {\n\t\tlist_for_each_entry_safe(task, next, head, u.tk_wait.list)\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\tif (head == &queue->tasks[0])\n\t\t\tbreak;\n\t\thead--;\n\t}\n\tspin_unlock_bh(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up);\n\n/**\n * rpc_wake_up_status - wake up all rpc_tasks and set their status value.\n * @queue: rpc_wait_queue on which the tasks are sleeping\n * @status: status value to set\n *\n * Grabs queue->lock\n */\nvoid rpc_wake_up_status(struct rpc_wait_queue *queue, int status)\n{\n\tstruct rpc_task *task, *next;\n\tstruct list_head *head;\n\n\tspin_lock_bh(&queue->lock);\n\thead = &queue->tasks[queue->maxpriority];\n\tfor (;;) {\n\t\tlist_for_each_entry_safe(task, next, head, u.tk_wait.list) {\n\t\t\ttask->tk_status = status;\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\t}\n\t\tif (head == &queue->tasks[0])\n\t\t\tbreak;\n\t\thead--;\n\t}\n\tspin_unlock_bh(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(rpc_wake_up_status);\n\nstatic void __rpc_queue_timer_fn(unsigned long ptr)\n{\n\tstruct rpc_wait_queue *queue = (struct rpc_wait_queue *)ptr;\n\tstruct rpc_task *task, *n;\n\tunsigned long expires, now, timeo;\n\n\tspin_lock(&queue->lock);\n\texpires = now = jiffies;\n\tlist_for_each_entry_safe(task, n, &queue->timer_list.list, u.tk_wait.timer_list) {\n\t\ttimeo = task->u.tk_wait.expires;\n\t\tif (time_after_eq(now, timeo)) {\n\t\t\tdprintk(\"RPC: %5u timeout\\n\", task->tk_pid);\n\t\t\ttask->tk_status = -ETIMEDOUT;\n\t\t\trpc_wake_up_task_queue_locked(queue, task);\n\t\t\tcontinue;\n\t\t}\n\t\tif (expires == now || time_after(expires, timeo))\n\t\t\texpires = timeo;\n\t}\n\tif (!list_empty(&queue->timer_list.list))\n\t\trpc_set_queue_timer(queue, expires);\n\tspin_unlock(&queue->lock);\n}\n\nstatic void __rpc_atrun(struct rpc_task *task)\n{\n\ttask->tk_status = 0;\n}\n\n/*\n * Run a task at a later time\n */\nvoid rpc_delay(struct rpc_task *task, unsigned long delay)\n{\n\ttask->tk_timeout = delay;\n\trpc_sleep_on(&delay_queue, task, __rpc_atrun);\n}\nEXPORT_SYMBOL_GPL(rpc_delay);\n\n/*\n * Helper to call task->tk_ops->rpc_call_prepare\n */\nvoid rpc_prepare_task(struct rpc_task *task)\n{\n\ttask->tk_ops->rpc_call_prepare(task, task->tk_calldata);\n}\n\n/*\n * Helper that calls task->tk_ops->rpc_call_done if it exists\n */\nvoid rpc_exit_task(struct rpc_task *task)\n{\n\ttask->tk_action = NULL;\n\tif (task->tk_ops->rpc_call_done != NULL) {\n\t\ttask->tk_ops->rpc_call_done(task, task->tk_calldata);\n\t\tif (task->tk_action != NULL) {\n\t\t\tWARN_ON(RPC_ASSASSINATED(task));\n\t\t\t/* Always release the RPC slot and buffer memory */\n\t\t\txprt_release(task);\n\t\t}\n\t}\n}\n\nvoid rpc_exit(struct rpc_task *task, int status)\n{\n\ttask->tk_status = status;\n\ttask->tk_action = rpc_exit_task;\n\tif (RPC_IS_QUEUED(task))\n\t\trpc_wake_up_queued_task(task->tk_waitqueue, task);\n}\nEXPORT_SYMBOL_GPL(rpc_exit);\n\nvoid rpc_release_calldata(const struct rpc_call_ops *ops, void *calldata)\n{\n\tif (ops->rpc_release != NULL)\n\t\tops->rpc_release(calldata);\n}\n\n/*\n * This is the RPC `scheduler' (or rather, the finite state machine).\n */\nstatic void __rpc_execute(struct rpc_task *task)\n{\n\tstruct rpc_wait_queue *queue;\n\tint task_is_async = RPC_IS_ASYNC(task);\n\tint status = 0;\n\n\tdprintk(\"RPC: %5u __rpc_execute flags=0x%x\\n\",\n\t\t\ttask->tk_pid, task->tk_flags);\n\n\tBUG_ON(RPC_IS_QUEUED(task));\n\n\tfor (;;) {\n\n\t\t/*\n\t\t * Execute any pending callback.\n\t\t */\n\t\tif (task->tk_callback) {\n\t\t\tvoid (*save_callback)(struct rpc_task *);\n\n\t\t\t/*\n\t\t\t * We set tk_callback to NULL before calling it,\n\t\t\t * in case it sets the tk_callback field itself:\n\t\t\t */\n\t\t\tsave_callback = task->tk_callback;\n\t\t\ttask->tk_callback = NULL;\n\t\t\tsave_callback(task);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Perform the next FSM step.\n\t\t\t * tk_action may be NULL when the task has been killed\n\t\t\t * by someone else.\n\t\t\t */\n\t\t\tif (task->tk_action == NULL)\n\t\t\t\tbreak;\n\t\t\ttask->tk_action(task);\n\t\t}\n\n\t\t/*\n\t\t * Lockless check for whether task is sleeping or not.\n\t\t */\n\t\tif (!RPC_IS_QUEUED(task))\n\t\t\tcontinue;\n\t\t/*\n\t\t * The queue->lock protects against races with\n\t\t * rpc_make_runnable().\n\t\t *\n\t\t * Note that once we clear RPC_TASK_RUNNING on an asynchronous\n\t\t * rpc_task, rpc_make_runnable() can assign it to a\n\t\t * different workqueue. We therefore cannot assume that the\n\t\t * rpc_task pointer may still be dereferenced.\n\t\t */\n\t\tqueue = task->tk_waitqueue;\n\t\tspin_lock_bh(&queue->lock);\n\t\tif (!RPC_IS_QUEUED(task)) {\n\t\t\tspin_unlock_bh(&queue->lock);\n\t\t\tcontinue;\n\t\t}\n\t\trpc_clear_running(task);\n\t\tspin_unlock_bh(&queue->lock);\n\t\tif (task_is_async)\n\t\t\treturn;\n\n\t\t/* sync task: sleep here */\n\t\tdprintk(\"RPC: %5u sync task going to sleep\\n\", task->tk_pid);\n\t\tstatus = out_of_line_wait_on_bit(&task->tk_runstate,\n\t\t\t\tRPC_TASK_QUEUED, rpc_wait_bit_killable,\n\t\t\t\tTASK_KILLABLE);\n\t\tif (status == -ERESTARTSYS) {\n\t\t\t/*\n\t\t\t * When a sync task receives a signal, it exits with\n\t\t\t * -ERESTARTSYS. In order to catch any callbacks that\n\t\t\t * clean up after sleeping on some queue, we don't\n\t\t\t * break the loop here, but go around once more.\n\t\t\t */\n\t\t\tdprintk(\"RPC: %5u got signal\\n\", task->tk_pid);\n\t\t\ttask->tk_flags |= RPC_TASK_KILLED;\n\t\t\trpc_exit(task, -ERESTARTSYS);\n\t\t}\n\t\trpc_set_running(task);\n\t\tdprintk(\"RPC: %5u sync task resuming\\n\", task->tk_pid);\n\t}\n\n\tdprintk(\"RPC: %5u return %d, status %d\\n\", task->tk_pid, status,\n\t\t\ttask->tk_status);\n\t/* Release all resources associated with the task */\n\trpc_release_task(task);\n}\n\n/*\n * User-visible entry point to the scheduler.\n *\n * This may be called recursively if e.g. an async NFS task updates\n * the attributes and finds that dirty pages must be flushed.\n * NOTE: Upon exit of this function the task is guaranteed to be\n *\t released. In particular note that tk_release() will have\n *\t been called, so your task memory may have been freed.\n */\nvoid rpc_execute(struct rpc_task *task)\n{\n\trpc_set_active(task);\n\trpc_make_runnable(task);\n\tif (!RPC_IS_ASYNC(task))\n\t\t__rpc_execute(task);\n}\n\nstatic void rpc_async_schedule(struct work_struct *work)\n{\n\t__rpc_execute(container_of(work, struct rpc_task, u.tk_work));\n}\n\n/**\n * rpc_malloc - allocate an RPC buffer\n * @task: RPC task that will use this buffer\n * @size: requested byte size\n *\n * To prevent rpciod from hanging, this allocator never sleeps,\n * returning NULL if the request cannot be serviced immediately.\n * The caller can arrange to sleep in a way that is safe for rpciod.\n *\n * Most requests are 'small' (under 2KiB) and can be serviced from a\n * mempool, ensuring that NFS reads and writes can always proceed,\n * and that there is good locality of reference for these buffers.\n *\n * In order to avoid memory starvation triggering more writebacks of\n * NFS requests, we avoid using GFP_KERNEL.\n */\nvoid *rpc_malloc(struct rpc_task *task, size_t size)\n{\n\tstruct rpc_buffer *buf;\n\tgfp_t gfp = RPC_IS_SWAPPER(task) ? GFP_ATOMIC : GFP_NOWAIT;\n\n\tsize += sizeof(struct rpc_buffer);\n\tif (size <= RPC_BUFFER_MAXSIZE)\n\t\tbuf = mempool_alloc(rpc_buffer_mempool, gfp);\n\telse\n\t\tbuf = kmalloc(size, gfp);\n\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->len = size;\n\tdprintk(\"RPC: %5u allocated buffer of size %zu at %p\\n\",\n\t\t\ttask->tk_pid, size, buf);\n\treturn &buf->data;\n}\nEXPORT_SYMBOL_GPL(rpc_malloc);\n\n/**\n * rpc_free - free buffer allocated via rpc_malloc\n * @buffer: buffer to free\n *\n */\nvoid rpc_free(void *buffer)\n{\n\tsize_t size;\n\tstruct rpc_buffer *buf;\n\n\tif (!buffer)\n\t\treturn;\n\n\tbuf = container_of(buffer, struct rpc_buffer, data);\n\tsize = buf->len;\n\n\tdprintk(\"RPC:       freeing buffer of size %zu at %p\\n\",\n\t\t\tsize, buf);\n\n\tif (size <= RPC_BUFFER_MAXSIZE)\n\t\tmempool_free(buf, rpc_buffer_mempool);\n\telse\n\t\tkfree(buf);\n}\nEXPORT_SYMBOL_GPL(rpc_free);\n\n/*\n * Creation and deletion of RPC task structures\n */\nstatic void rpc_init_task(struct rpc_task *task, const struct rpc_task_setup *task_setup_data)\n{\n\tmemset(task, 0, sizeof(*task));\n\tatomic_set(&task->tk_count, 1);\n\ttask->tk_flags  = task_setup_data->flags;\n\ttask->tk_ops = task_setup_data->callback_ops;\n\ttask->tk_calldata = task_setup_data->callback_data;\n\tINIT_LIST_HEAD(&task->tk_task);\n\n\t/* Initialize retry counters */\n\ttask->tk_garb_retry = 2;\n\ttask->tk_cred_retry = 2;\n\ttask->tk_rebind_retry = 2;\n\n\ttask->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;\n\ttask->tk_owner = current->tgid;\n\n\t/* Initialize workqueue for async tasks */\n\ttask->tk_workqueue = task_setup_data->workqueue;\n\n\tif (task->tk_ops->rpc_call_prepare != NULL)\n\t\ttask->tk_action = rpc_prepare_task;\n\n\t/* starting timestamp */\n\ttask->tk_start = ktime_get();\n\n\tdprintk(\"RPC:       new task initialized, procpid %u\\n\",\n\t\t\t\ttask_pid_nr(current));\n}\n\nstatic struct rpc_task *\nrpc_alloc_task(void)\n{\n\treturn (struct rpc_task *)mempool_alloc(rpc_task_mempool, GFP_NOFS);\n}\n\n/*\n * Create a new task for the specified client.\n */\nstruct rpc_task *rpc_new_task(const struct rpc_task_setup *setup_data)\n{\n\tstruct rpc_task\t*task = setup_data->task;\n\tunsigned short flags = 0;\n\n\tif (task == NULL) {\n\t\ttask = rpc_alloc_task();\n\t\tif (task == NULL) {\n\t\t\trpc_release_calldata(setup_data->callback_ops,\n\t\t\t\t\tsetup_data->callback_data);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tflags = RPC_TASK_DYNAMIC;\n\t}\n\n\trpc_init_task(task, setup_data);\n\ttask->tk_flags |= flags;\n\tdprintk(\"RPC:       allocated task %p\\n\", task);\n\treturn task;\n}\n\nstatic void rpc_free_task(struct rpc_task *task)\n{\n\tconst struct rpc_call_ops *tk_ops = task->tk_ops;\n\tvoid *calldata = task->tk_calldata;\n\n\tif (task->tk_flags & RPC_TASK_DYNAMIC) {\n\t\tdprintk(\"RPC: %5u freeing task\\n\", task->tk_pid);\n\t\tmempool_free(task, rpc_task_mempool);\n\t}\n\trpc_release_calldata(tk_ops, calldata);\n}\n\nstatic void rpc_async_release(struct work_struct *work)\n{\n\trpc_free_task(container_of(work, struct rpc_task, u.tk_work));\n}\n\nstatic void rpc_release_resources_task(struct rpc_task *task)\n{\n\tif (task->tk_rqstp)\n\t\txprt_release(task);\n\tif (task->tk_msg.rpc_cred) {\n\t\tput_rpccred(task->tk_msg.rpc_cred);\n\t\ttask->tk_msg.rpc_cred = NULL;\n\t}\n\trpc_task_release_client(task);\n}\n\nstatic void rpc_final_put_task(struct rpc_task *task,\n\t\tstruct workqueue_struct *q)\n{\n\tif (q != NULL) {\n\t\tINIT_WORK(&task->u.tk_work, rpc_async_release);\n\t\tqueue_work(q, &task->u.tk_work);\n\t} else\n\t\trpc_free_task(task);\n}\n\nstatic void rpc_do_put_task(struct rpc_task *task, struct workqueue_struct *q)\n{\n\tif (atomic_dec_and_test(&task->tk_count)) {\n\t\trpc_release_resources_task(task);\n\t\trpc_final_put_task(task, q);\n\t}\n}\n\nvoid rpc_put_task(struct rpc_task *task)\n{\n\trpc_do_put_task(task, NULL);\n}\nEXPORT_SYMBOL_GPL(rpc_put_task);\n\nvoid rpc_put_task_async(struct rpc_task *task)\n{\n\trpc_do_put_task(task, task->tk_workqueue);\n}\nEXPORT_SYMBOL_GPL(rpc_put_task_async);\n\nstatic void rpc_release_task(struct rpc_task *task)\n{\n\tdprintk(\"RPC: %5u release task\\n\", task->tk_pid);\n\n\tBUG_ON (RPC_IS_QUEUED(task));\n\n\trpc_release_resources_task(task);\n\n\t/*\n\t * Note: at this point we have been removed from rpc_clnt->cl_tasks,\n\t * so it should be safe to use task->tk_count as a test for whether\n\t * or not any other processes still hold references to our rpc_task.\n\t */\n\tif (atomic_read(&task->tk_count) != 1 + !RPC_IS_ASYNC(task)) {\n\t\t/* Wake up anyone who may be waiting for task completion */\n\t\tif (!rpc_complete_task(task))\n\t\t\treturn;\n\t} else {\n\t\tif (!atomic_dec_and_test(&task->tk_count))\n\t\t\treturn;\n\t}\n\trpc_final_put_task(task, task->tk_workqueue);\n}\n\nint rpciod_up(void)\n{\n\treturn try_module_get(THIS_MODULE) ? 0 : -EINVAL;\n}\n\nvoid rpciod_down(void)\n{\n\tmodule_put(THIS_MODULE);\n}\n\n/*\n * Start up the rpciod workqueue.\n */\nstatic int rpciod_start(void)\n{\n\tstruct workqueue_struct *wq;\n\n\t/*\n\t * Create the rpciod thread and wait for it to start.\n\t */\n\tdprintk(\"RPC:       creating workqueue rpciod\\n\");\n\twq = alloc_workqueue(\"rpciod\", WQ_MEM_RECLAIM, 0);\n\trpciod_workqueue = wq;\n\treturn rpciod_workqueue != NULL;\n}\n\nstatic void rpciod_stop(void)\n{\n\tstruct workqueue_struct *wq = NULL;\n\n\tif (rpciod_workqueue == NULL)\n\t\treturn;\n\tdprintk(\"RPC:       destroying workqueue rpciod\\n\");\n\n\twq = rpciod_workqueue;\n\trpciod_workqueue = NULL;\n\tdestroy_workqueue(wq);\n}\n\nvoid\nrpc_destroy_mempool(void)\n{\n\trpciod_stop();\n\tif (rpc_buffer_mempool)\n\t\tmempool_destroy(rpc_buffer_mempool);\n\tif (rpc_task_mempool)\n\t\tmempool_destroy(rpc_task_mempool);\n\tif (rpc_task_slabp)\n\t\tkmem_cache_destroy(rpc_task_slabp);\n\tif (rpc_buffer_slabp)\n\t\tkmem_cache_destroy(rpc_buffer_slabp);\n\trpc_destroy_wait_queue(&delay_queue);\n}\n\nint\nrpc_init_mempool(void)\n{\n\t/*\n\t * The following is not strictly a mempool initialisation,\n\t * but there is no harm in doing it here\n\t */\n\trpc_init_wait_queue(&delay_queue, \"delayq\");\n\tif (!rpciod_start())\n\t\tgoto err_nomem;\n\n\trpc_task_slabp = kmem_cache_create(\"rpc_tasks\",\n\t\t\t\t\t     sizeof(struct rpc_task),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (!rpc_task_slabp)\n\t\tgoto err_nomem;\n\trpc_buffer_slabp = kmem_cache_create(\"rpc_buffers\",\n\t\t\t\t\t     RPC_BUFFER_MAXSIZE,\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (!rpc_buffer_slabp)\n\t\tgoto err_nomem;\n\trpc_task_mempool = mempool_create_slab_pool(RPC_TASK_POOLSIZE,\n\t\t\t\t\t\t    rpc_task_slabp);\n\tif (!rpc_task_mempool)\n\t\tgoto err_nomem;\n\trpc_buffer_mempool = mempool_create_slab_pool(RPC_BUFFER_POOLSIZE,\n\t\t\t\t\t\t      rpc_buffer_slabp);\n\tif (!rpc_buffer_mempool)\n\t\tgoto err_nomem;\n\treturn 0;\nerr_nomem:\n\trpc_destroy_mempool();\n\treturn -ENOMEM;\n}\n"], "filenames": ["fs/lockd/clntproc.c", "include/linux/sunrpc/sched.h", "net/sunrpc/clnt.c", "net/sunrpc/sched.c"], "buggy_code_start_loc": [711, 87, 1177, 794], "buggy_code_end_loc": [712, 88, 1177, 794], "fixing_code_start_loc": [711, 87, 1178, 795], "fixing_code_end_loc": [718, 89, 1181, 796], "type": "CWE-400", "message": "The Network Lock Manager (NLM) protocol implementation in the NFS client functionality in the Linux kernel before 3.0 allows local users to cause a denial of service (system hang) via a LOCK_UN flock system call.", "other": {"cve": {"id": "CVE-2011-2491", "sourceIdentifier": "secalert@redhat.com", "published": "2013-03-01T12:37:53.757", "lastModified": "2023-02-13T04:31:05.793", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Network Lock Manager (NLM) protocol implementation in the NFS client functionality in the Linux kernel before 3.0 allows local users to cause a denial of service (system hang) via a LOCK_UN flock system call."}, {"lang": "es", "value": "La implementaci\u00f3n del protocolo Network Lock Manager (NLM) en la funcionalidad de cliente NFS en el kernel de Linux anteriores a v3.0 permite a usuarios locales provocar una denegaci\u00f3n de servicio (ca\u00edda del sistema) a trav\u00e9s de una llamada de sistema flock LOCK_UN."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.0", "matchCriteriaId": "E0135A6D-9FB7-4E1B-B471-914E37494942"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:5.0:*:*:*:*:*:*:*", "matchCriteriaId": "133AAFA7-AF42-4D7B-8822-AA2E85611BF5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:5.0:*:*:*:*:*:*:*", "matchCriteriaId": "54D669D4-6D7E-449D-80C1-28FA44F06FFE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D0AC5CD5-6E58-433C-9EB3-6DFE5656463E"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v3.0/ChangeLog-3.0", "source": "secalert@redhat.com", "tags": ["Broken Link"]}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=0b760113a3a155269a3fba93a409c640031dd68f", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2011-1212.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2011/06/23/6", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=709393", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/0b760113a3a155269a3fba93a409c640031dd68f", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0b760113a3a155269a3fba93a409c640031dd68f"}}