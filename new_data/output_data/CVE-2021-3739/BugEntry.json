{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/raid/pq.h>\n#include <linux/semaphore.h>\n#include <linux/uuid.h>\n#include <linux/list_sort.h>\n#include \"misc.h\"\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"async-thread.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n#include \"tree-checker.h\"\n#include \"space-info.h\"\n#include \"block-group.h\"\n#include \"discard.h\"\n#include \"zoned.h\"\n\nconst struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES] = {\n\t[BTRFS_RAID_RAID10] = {\n\t\t.sub_stripes\t= 2,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\t/* 0 == as many as possible */\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid10\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID10,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID10_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 2,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1C3] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 3,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 3,\n\t\t.ncopies\t= 3,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1c3\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1C3,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1C3_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1C4] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 4,\n\t\t.devs_min\t= 4,\n\t\t.tolerated_failures = 3,\n\t\t.devs_increment\t= 4,\n\t\t.ncopies\t= 4,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1c4\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1C4,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1C4_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_DUP] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 2,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"dup\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_DUP,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID0] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid0\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_SINGLE] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"single\",\n\t\t.bg_flag\t= 0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID5] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 1,\n\t\t.raid_name\t= \"raid5\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID5,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID5_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID6] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 2,\n\t\t.raid_name\t= \"raid6\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID6,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID6_MIN_NOT_MET,\n\t},\n};\n\n/*\n * Convert block group flags (BTRFS_BLOCK_GROUP_*) to btrfs_raid_types, which\n * can be used as index to access btrfs_raid_array[].\n */\nenum btrfs_raid_types __attribute_const__ btrfs_bg_flags_to_raid_index(u64 flags)\n{\n\tif (flags & BTRFS_BLOCK_GROUP_RAID10)\n\t\treturn BTRFS_RAID_RAID10;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1)\n\t\treturn BTRFS_RAID_RAID1;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1C3)\n\t\treturn BTRFS_RAID_RAID1C3;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1C4)\n\t\treturn BTRFS_RAID_RAID1C4;\n\telse if (flags & BTRFS_BLOCK_GROUP_DUP)\n\t\treturn BTRFS_RAID_DUP;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID0)\n\t\treturn BTRFS_RAID_RAID0;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID5)\n\t\treturn BTRFS_RAID_RAID5;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID6)\n\t\treturn BTRFS_RAID_RAID6;\n\n\treturn BTRFS_RAID_SINGLE; /* BTRFS_BLOCK_GROUP_SINGLE */\n}\n\nconst char *btrfs_bg_type_to_raid_name(u64 flags)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(flags);\n\n\tif (index >= BTRFS_NR_RAID_TYPES)\n\t\treturn NULL;\n\n\treturn btrfs_raid_array[index].raid_name;\n}\n\n/*\n * Fill @buf with textual description of @bg_flags, no more than @size_buf\n * bytes including terminating null byte.\n */\nvoid btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n{\n\tint i;\n\tint ret;\n\tchar *bp = buf;\n\tu64 flags = bg_flags;\n\tu32 size_bp = size_buf;\n\n\tif (!flags) {\n\t\tstrcpy(bp, \"NONE\");\n\t\treturn;\n\t}\n\n#define DESCRIBE_FLAG(flag, desc)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (flags & (flag)) {\t\t\t\t\t\\\n\t\t\tret = snprintf(bp, size_bp, \"%s|\", (desc));\t\\\n\t\t\tif (ret < 0 || ret >= size_bp)\t\t\t\\\n\t\t\t\tgoto out_overflow;\t\t\t\\\n\t\t\tsize_bp -= ret;\t\t\t\t\t\\\n\t\t\tbp += ret;\t\t\t\t\t\\\n\t\t\tflags &= ~(flag);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n\n\tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++)\n\t\tDESCRIBE_FLAG(btrfs_raid_array[i].bg_flag,\n\t\t\t      btrfs_raid_array[i].raid_name);\n#undef DESCRIBE_FLAG\n\n\tif (flags) {\n\t\tret = snprintf(bp, size_bp, \"0x%llx|\", flags);\n\t\tsize_bp -= ret;\n\t}\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last | */\n\n\t/*\n\t * The text is trimmed, it's up to the caller to provide sufficiently\n\t * large buffer\n\t */\nout_overflow:;\n}\n\nstatic int init_first_rw_device(struct btrfs_trans_handle *trans);\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info);\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev);\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *device);\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map);\n\n/*\n * Device locking\n * ==============\n *\n * There are several mutexes that protect manipulation of devices and low-level\n * structures like chunks but not block groups, extents or files\n *\n * uuid_mutex (global lock)\n * ------------------------\n * protects the fs_uuids list that tracks all per-fs fs_devices, resulting from\n * the SCAN_DEV ioctl registration or from mount either implicitly (the first\n * device) or requested by the device= mount option\n *\n * the mutex can be very coarse and can cover long-running operations\n *\n * protects: updates to fs_devices counters like missing devices, rw devices,\n * seeding, structure cloning, opening/closing devices at mount/umount time\n *\n * global::fs_devs - add, remove, updates to the global list\n *\n * does not protect: manipulation of the fs_devices::devices list in general\n * but in mount context it could be used to exclude list modifications by eg.\n * scan ioctl\n *\n * btrfs_device::name - renames (write side), read is RCU\n *\n * fs_devices::device_list_mutex (per-fs, with RCU)\n * ------------------------------------------------\n * protects updates to fs_devices::devices, ie. adding and deleting\n *\n * simple list traversal with read-only actions can be done with RCU protection\n *\n * may be used to exclude some operations from running concurrently without any\n * modifications to the list (see write_all_supers)\n *\n * Is not required at mount and close times, because our device list is\n * protected by the uuid_mutex at that point.\n *\n * balance_mutex\n * -------------\n * protects balance structures (status, state) and context accessed from\n * several places (internally, ioctl)\n *\n * chunk_mutex\n * -----------\n * protects chunks, adding or removing during allocation, trim or when a new\n * device is added/removed. Additionally it also protects post_commit_list of\n * individual devices, since they can be added to the transaction's\n * post_commit_list only with chunk_mutex held.\n *\n * cleaner_mutex\n * -------------\n * a big lock that is held by the cleaner thread and prevents running subvolume\n * cleaning together with relocation or delayed iputs\n *\n *\n * Lock nesting\n * ============\n *\n * uuid_mutex\n *   device_list_mutex\n *     chunk_mutex\n *   balance_mutex\n *\n *\n * Exclusive operations\n * ====================\n *\n * Maintains the exclusivity of the following operations that apply to the\n * whole filesystem and cannot run in parallel.\n *\n * - Balance (*)\n * - Device add\n * - Device remove\n * - Device replace (*)\n * - Resize\n *\n * The device operations (as above) can be in one of the following states:\n *\n * - Running state\n * - Paused state\n * - Completed state\n *\n * Only device operations marked with (*) can go into the Paused state for the\n * following reasons:\n *\n * - ioctl (only Balance can be Paused through ioctl)\n * - filesystem remounted as read-only\n * - filesystem unmounted and mounted as read-only\n * - system power-cycle and filesystem mounted as read-only\n * - filesystem or device errors leading to forced read-only\n *\n * The status of exclusive operation is set and cleared atomically.\n * During the course of Paused state, fs_info::exclusive_operation remains set.\n * A device operation in Paused or Running state can be canceled or resumed\n * either by ioctl (Balance only) or when remounted as read-write.\n * The exclusive status is cleared when the device operation is canceled or\n * completed.\n */\n\nDEFINE_MUTEX(uuid_mutex);\nstatic LIST_HEAD(fs_uuids);\nstruct list_head * __attribute_const__ btrfs_get_fs_uuids(void)\n{\n\treturn &fs_uuids;\n}\n\n/*\n * alloc_fs_devices - allocate struct btrfs_fs_devices\n * @fsid:\t\tif not NULL, copy the UUID to fs_devices::fsid\n * @metadata_fsid:\tif not NULL, copy the UUID to fs_devices::metadata_fsid\n *\n * Return a pointer to a new struct btrfs_fs_devices on success, or ERR_PTR().\n * The returned struct is not linked onto any lists and can be destroyed with\n * kfree() right away.\n */\nstatic struct btrfs_fs_devices *alloc_fs_devices(const u8 *fsid,\n\t\t\t\t\t\t const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devs;\n\n\tfs_devs = kzalloc(sizeof(*fs_devs), GFP_KERNEL);\n\tif (!fs_devs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&fs_devs->device_list_mutex);\n\n\tINIT_LIST_HEAD(&fs_devs->devices);\n\tINIT_LIST_HEAD(&fs_devs->alloc_list);\n\tINIT_LIST_HEAD(&fs_devs->fs_list);\n\tINIT_LIST_HEAD(&fs_devs->seed_list);\n\tif (fsid)\n\t\tmemcpy(fs_devs->fsid, fsid, BTRFS_FSID_SIZE);\n\n\tif (metadata_fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, metadata_fsid, BTRFS_FSID_SIZE);\n\telse if (fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, fsid, BTRFS_FSID_SIZE);\n\n\treturn fs_devs;\n}\n\nvoid btrfs_free_device(struct btrfs_device *device)\n{\n\tWARN_ON(!list_empty(&device->post_commit_list));\n\trcu_string_free(device->name);\n\textent_io_tree_release(&device->alloc_state);\n\tbio_put(device->flush_bio);\n\tbtrfs_destroy_dev_zone_info(device);\n\tkfree(device);\n}\n\nstatic void free_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device;\n\tWARN_ON(fs_devices->opened);\n\twhile (!list_empty(&fs_devices->devices)) {\n\t\tdevice = list_entry(fs_devices->devices.next,\n\t\t\t\t    struct btrfs_device, dev_list);\n\t\tlist_del(&device->dev_list);\n\t\tbtrfs_free_device(device);\n\t}\n\tkfree(fs_devices);\n}\n\nvoid __exit btrfs_cleanup_fs_uuids(void)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\twhile (!list_empty(&fs_uuids)) {\n\t\tfs_devices = list_entry(fs_uuids.next,\n\t\t\t\t\tstruct btrfs_fs_devices, fs_list);\n\t\tlist_del(&fs_devices->fs_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\nstatic noinline struct btrfs_fs_devices *find_fsid(\n\t\tconst u8 *fsid, const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tASSERT(fsid);\n\n\t/* Handle non-split brain cases */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (metadata_fsid) {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0\n\t\t\t    && memcmp(metadata_fsid, fs_devices->metadata_uuid,\n\t\t\t\t      BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t} else {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct btrfs_fs_devices *find_fsid_with_metadata_uuid(\n\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handle scanned device having completed its fsid change but\n\t * belonging to a fs_devices that was created by first scanning\n\t * a device which didn't have its fsid/metadata_uuid changed\n\t * at all and the CHANGING_FSID_V2 flag set.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (fs_devices->fsid_change &&\n\t\t    memcmp(disk_super->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\t/*\n\t * Handle scanned device having completed its fsid change but\n\t * belonging to a fs_devices that was created by a device that\n\t * has an outdated pair of fsid/metadata_uuid and\n\t * CHANGING_FSID_V2 flag set.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (fs_devices->fsid_change &&\n\t\t    memcmp(fs_devices->metadata_uuid,\n\t\t\t   fs_devices->fsid, BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(disk_super->metadata_uuid, fs_devices->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn find_fsid(disk_super->fsid, disk_super->metadata_uuid);\n}\n\n\nstatic int\nbtrfs_get_bdev_and_sb(const char *device_path, fmode_t flags, void *holder,\n\t\t      int flush, struct block_device **bdev,\n\t\t      struct btrfs_super_block **disk_super)\n{\n\tint ret;\n\n\t*bdev = blkdev_get_by_path(device_path, flags, holder);\n\n\tif (IS_ERR(*bdev)) {\n\t\tret = PTR_ERR(*bdev);\n\t\tgoto error;\n\t}\n\n\tif (flush)\n\t\tfilemap_write_and_wait((*bdev)->bd_inode->i_mapping);\n\tret = set_blocksize(*bdev, BTRFS_BDEV_BLOCKSIZE);\n\tif (ret) {\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\tinvalidate_bdev(*bdev);\n\t*disk_super = btrfs_read_dev_super(*bdev);\n\tif (IS_ERR(*disk_super)) {\n\t\tret = PTR_ERR(*disk_super);\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\n\treturn 0;\n\nerror:\n\t*bdev = NULL;\n\treturn ret;\n}\n\nstatic bool device_path_matched(const char *path, struct btrfs_device *device)\n{\n\tint found;\n\n\trcu_read_lock();\n\tfound = strcmp(rcu_str_deref(device->name), path);\n\trcu_read_unlock();\n\n\treturn found == 0;\n}\n\n/*\n *  Search and remove all stale (devices which are not mounted) devices.\n *  When both inputs are NULL, it will search and release all stale devices.\n *  path:\tOptional. When provided will it release all unmounted devices\n *\t\tmatching this path only.\n *  skip_dev:\tOptional. Will skip this device when searching for the stale\n *\t\tdevices.\n *  Return:\t0 for success or if @path is NULL.\n * \t\t-EBUSY if @path is a mounted device.\n * \t\t-ENOENT if @path does not match any device in the list.\n */\nstatic int btrfs_free_stale_devices(const char *path,\n\t\t\t\t     struct btrfs_device *skip_device)\n{\n\tstruct btrfs_fs_devices *fs_devices, *tmp_fs_devices;\n\tstruct btrfs_device *device, *tmp_device;\n\tint ret = 0;\n\n\tif (path)\n\t\tret = -ENOENT;\n\n\tlist_for_each_entry_safe(fs_devices, tmp_fs_devices, &fs_uuids, fs_list) {\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry_safe(device, tmp_device,\n\t\t\t\t\t &fs_devices->devices, dev_list) {\n\t\t\tif (skip_device && skip_device == device)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device->name)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device_path_matched(path, device))\n\t\t\t\tcontinue;\n\t\t\tif (fs_devices->opened) {\n\t\t\t\t/* for an already deleted device return 0 */\n\t\t\t\tif (path && ret != 0)\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* delete the stale device */\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\n\t\t\tret = 0;\n\t\t}\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tif (fs_devices->num_devices == 0) {\n\t\t\tbtrfs_sysfs_remove_fsid(fs_devices);\n\t\t\tlist_del(&fs_devices->fs_list);\n\t\t\tfree_fs_devices(fs_devices);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * This is only used on mount, and we are protected from competing things\n * messing with our fs_devices by the uuid_mutex, thus we do not need the\n * fs_devices->device_list_mutex here.\n */\nstatic int btrfs_open_one_device(struct btrfs_fs_devices *fs_devices,\n\t\t\tstruct btrfs_device *device, fmode_t flags,\n\t\t\tvoid *holder)\n{\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tint ret;\n\n\tif (device->bdev)\n\t\treturn -EINVAL;\n\tif (!device->name)\n\t\treturn -EINVAL;\n\n\tret = btrfs_get_bdev_and_sb(device->name->str, flags, holder, 1,\n\t\t\t\t    &bdev, &disk_super);\n\tif (ret)\n\t\treturn ret;\n\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tif (devid != device->devid)\n\t\tgoto error_free_page;\n\n\tif (memcmp(device->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE))\n\t\tgoto error_free_page;\n\n\tdevice->generation = btrfs_super_generation(disk_super);\n\n\tif (btrfs_super_flags(disk_super) & BTRFS_SUPER_FLAG_SEEDING) {\n\t\tif (btrfs_super_incompat_flags(disk_super) &\n\t\t    BTRFS_FEATURE_INCOMPAT_METADATA_UUID) {\n\t\t\tpr_err(\n\t\t\"BTRFS: Invalid seeding and uuid-changed device detected\\n\");\n\t\t\tgoto error_free_page;\n\t\t}\n\n\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\tfs_devices->seeding = true;\n\t} else {\n\t\tif (bdev_read_only(bdev))\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\telse\n\t\t\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = true;\n\n\tdevice->bdev = bdev;\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tdevice->mode = flags;\n\n\tfs_devices->open_devices++;\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tfs_devices->rw_devices++;\n\t\tlist_add_tail(&device->dev_alloc_list, &fs_devices->alloc_list);\n\t}\n\tbtrfs_release_disk_super(disk_super);\n\n\treturn 0;\n\nerror_free_page:\n\tbtrfs_release_disk_super(disk_super);\n\tblkdev_put(bdev, flags);\n\n\treturn -EINVAL;\n}\n\n/*\n * Handle scanned device having its CHANGING_FSID_V2 flag set and the fs_devices\n * being created with a disk that has already completed its fsid change. Such\n * disk can belong to an fs which has its FSID changed or to one which doesn't.\n * Handle both cases here.\n */\nstatic struct btrfs_fs_devices *find_fsid_inprogress(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 && !fs_devices->fsid_change) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn find_fsid(disk_super->fsid, NULL);\n}\n\n\nstatic struct btrfs_fs_devices *find_fsid_changed(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handles the case where scanned device is part of an fs that had\n\t * multiple successful changes of FSID but currently device didn't\n\t * observe it. Meaning our fsid will be different than theirs. We need\n\t * to handle two subcases :\n\t *  1 - The fs still continues to have different METADATA/FSID uuids.\n\t *  2 - The fs is switched back to its original FSID (METADATA/FSID\n\t *  are equal).\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t/* Changed UUIDs */\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0)\n\t\t\treturn fs_devices;\n\n\t\t/* Unchanged UUIDs */\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0)\n\t\t\treturn fs_devices;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct btrfs_fs_devices *find_fsid_reverted_metadata(\n\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handle the case where the scanned device is part of an fs whose last\n\t * metadata UUID change reverted it to the original FSID. At the same\n\t * time * fs_devices was first created by another constitutent device\n\t * which didn't fully observe the operation. This results in an\n\t * btrfs_fs_devices created with metadata/fsid different AND\n\t * btrfs_fs_devices::fsid_change set AND the metadata_uuid of the\n\t * fs_devices equal to the FSID of the disk.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    fs_devices->fsid_change)\n\t\t\treturn fs_devices;\n\t}\n\n\treturn NULL;\n}\n/*\n * Add new device to list of registered devices\n *\n * Returns:\n * device pointer which was just added or updated when successful\n * error pointer when failed\n */\nstatic noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid)\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\telse\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid_with_metadata_uuid(disk_super);\n\t} else {\n\t\tfs_devices = find_fsid_reverted_metadata(disk_super);\n\t\tif (!fs_devices)\n\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\n\t\t\tif (has_metadata_uuid)\n\t\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t       disk_super->metadata_uuid,\n\t\t\t\t       BTRFS_FSID_SIZE);\n\t\t\telse\n\t\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t       disk_super->fsid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\n\t\"BTRFS: device label %s devid %llu transid %llu %s scanned by %s (%d)\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path,\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\t\telse\n\t\t\tpr_info(\n\t\"BTRFS: device fsid %pU devid %llu transid %llu %s scanned by %s (%d)\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path,\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tint error;\n\t\t\tdev_t path_dev;\n\n\t\t\terror = lookup_bdev(path, &path_dev);\n\t\t\tif (error) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_PTR(error);\n\t\t\t}\n\n\t\t\tif (device->bdev->bd_dev != path_dev) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\t/*\n\t\t\t\t * device->fs_info may not be reliable here, so\n\t\t\t\t * pass in a NULL instead. This avoids a\n\t\t\t\t * possible use-after-free when the fs_info and\n\t\t\t\t * fs_info->sb are already torn down.\n\t\t\t\t */\n\t\t\t\tbtrfs_warn_in_rcu(NULL,\n\t\"duplicate device %s devid %llu generation %llu scanned by %s (%d)\",\n\t\t\t\t\t\t  path, devid, found_transid,\n\t\t\t\t\t\t  current->comm,\n\t\t\t\t\t\t  task_pid_nr(current));\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\"devid %llu device path %s changed to %s scanned by %s (%d)\",\n\t\t\t\t\t  devid, rcu_str_deref(device->name),\n\t\t\t\t\t  path, current->comm,\n\t\t\t\t\t  task_pid_nr(current));\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}\n\nstatic struct btrfs_fs_devices *clone_fs_devices(struct btrfs_fs_devices *orig)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *orig_dev;\n\tint ret = 0;\n\n\tfs_devices = alloc_fs_devices(orig->fsid, NULL);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tmutex_lock(&orig->device_list_mutex);\n\tfs_devices->total_devices = orig->total_devices;\n\n\tlist_for_each_entry(orig_dev, &orig->devices, dev_list) {\n\t\tstruct rcu_string *name;\n\n\t\tdevice = btrfs_alloc_device(NULL, &orig_dev->devid,\n\t\t\t\t\t    orig_dev->uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tret = PTR_ERR(device);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * This is ok to do without rcu read locked because we hold the\n\t\t * uuid mutex so nothing we touch in here is going to disappear.\n\t\t */\n\t\tif (orig_dev->name) {\n\t\t\tname = rcu_string_strdup(orig_dev->name->str,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!name) {\n\t\t\t\tbtrfs_free_device(device);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\trcu_assign_pointer(device->name, name);\n\t\t}\n\n\t\tlist_add(&device->dev_list, &fs_devices->devices);\n\t\tdevice->fs_devices = fs_devices;\n\t\tfs_devices->num_devices++;\n\t}\n\tmutex_unlock(&orig->device_list_mutex);\n\treturn fs_devices;\nerror:\n\tmutex_unlock(&orig->device_list_mutex);\n\tfree_fs_devices(fs_devices);\n\treturn ERR_PTR(ret);\n}\n\nstatic void __btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      struct btrfs_device **latest_dev)\n{\n\tstruct btrfs_device *device, *next;\n\n\t/* This is the initialized path, it is safe to release the devices. */\n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state)) {\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t      &device->dev_state) &&\n\t\t\t    !test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t      &device->dev_state) &&\n\t\t\t    (!*latest_dev ||\n\t\t\t     device->generation > (*latest_dev)->generation)) {\n\t\t\t\t*latest_dev = device;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We have already validated the presence of BTRFS_DEV_REPLACE_DEVID,\n\t\t * in btrfs_init_dev_replace() so just continue.\n\t\t */\n\t\tif (device->devid == BTRFS_DEV_REPLACE_DEVID)\n\t\t\tcontinue;\n\n\t\tif (device->bdev) {\n\t\t\tblkdev_put(device->bdev, device->mode);\n\t\t\tdevice->bdev = NULL;\n\t\t\tfs_devices->open_devices--;\n\t\t}\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tlist_del_init(&device->dev_alloc_list);\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\t\tfs_devices->rw_devices--;\n\t\t}\n\t\tlist_del_init(&device->dev_list);\n\t\tfs_devices->num_devices--;\n\t\tbtrfs_free_device(device);\n\t}\n\n}\n\n/*\n * After we have read the system tree and know devids belonging to this\n * filesystem, remove the device which does not belong there.\n */\nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *latest_dev = NULL;\n\tstruct btrfs_fs_devices *seed_dev;\n\n\tmutex_lock(&uuid_mutex);\n\t__btrfs_free_extra_devids(fs_devices, &latest_dev);\n\n\tlist_for_each_entry(seed_dev, &fs_devices->seed_list, seed_list)\n\t\t__btrfs_free_extra_devids(seed_dev, &latest_dev);\n\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic void btrfs_close_bdev(struct btrfs_device *device)\n{\n\tif (!device->bdev)\n\t\treturn;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tsync_blockdev(device->bdev);\n\t\tinvalidate_bdev(device->bdev);\n\t}\n\n\tblkdev_put(device->bdev, device->mode);\n}\n\nstatic void btrfs_close_one_device(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_devices *fs_devices = device->fs_devices;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tfs_devices->rw_devices--;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tbtrfs_close_bdev(device);\n\tif (device->bdev) {\n\t\tfs_devices->open_devices--;\n\t\tdevice->bdev = NULL;\n\t}\n\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tbtrfs_destroy_dev_zone_info(device);\n\n\tdevice->fs_info = NULL;\n\tatomic_set(&device->dev_stats_ccnt, 0);\n\textent_io_tree_release(&device->alloc_state);\n\n\t/* Verify the device is back in a pristine state  */\n\tASSERT(!test_bit(BTRFS_DEV_STATE_FLUSH_SENT, &device->dev_state));\n\tASSERT(!test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state));\n\tASSERT(list_empty(&device->dev_alloc_list));\n\tASSERT(list_empty(&device->post_commit_list));\n\tASSERT(atomic_read(&device->reada_in_flight) == 0);\n}\n\nstatic void close_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device, *tmp;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tif (--fs_devices->opened > 0)\n\t\treturn;\n\n\tlist_for_each_entry_safe(device, tmp, &fs_devices->devices, dev_list)\n\t\tbtrfs_close_one_device(device);\n\n\tWARN_ON(fs_devices->open_devices);\n\tWARN_ON(fs_devices->rw_devices);\n\tfs_devices->opened = 0;\n\tfs_devices->seeding = false;\n\tfs_devices->fs_info = NULL;\n}\n\nvoid btrfs_close_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_devices *tmp;\n\n\tmutex_lock(&uuid_mutex);\n\tclose_fs_devices(fs_devices);\n\tif (!fs_devices->opened)\n\t\tlist_splice_init(&fs_devices->seed_list, &list);\n\n\tlist_for_each_entry_safe(fs_devices, tmp, &list, seed_list) {\n\t\tclose_fs_devices(fs_devices);\n\t\tlist_del(&fs_devices->seed_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic int open_fs_devices(struct btrfs_fs_devices *fs_devices,\n\t\t\t\tfmode_t flags, void *holder)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *latest_dev = NULL;\n\tstruct btrfs_device *tmp_device;\n\n\tflags |= FMODE_EXCL;\n\n\tlist_for_each_entry_safe(device, tmp_device, &fs_devices->devices,\n\t\t\t\t dev_list) {\n\t\tint ret;\n\n\t\tret = btrfs_open_one_device(fs_devices, device, flags, holder);\n\t\tif (ret == 0 &&\n\t\t    (!latest_dev || device->generation > latest_dev->generation)) {\n\t\t\tlatest_dev = device;\n\t\t} else if (ret == -ENODATA) {\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\t\t}\n\t}\n\tif (fs_devices->open_devices == 0)\n\t\treturn -EINVAL;\n\n\tfs_devices->opened = 1;\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\tfs_devices->total_rw_bytes = 0;\n\tfs_devices->chunk_alloc_policy = BTRFS_CHUNK_ALLOC_REGULAR;\n\tfs_devices->read_policy = BTRFS_READ_POLICY_PID;\n\n\treturn 0;\n}\n\nstatic int devid_cmp(void *priv, const struct list_head *a,\n\t\t     const struct list_head *b)\n{\n\tconst struct btrfs_device *dev1, *dev2;\n\n\tdev1 = list_entry(a, struct btrfs_device, dev_list);\n\tdev2 = list_entry(b, struct btrfs_device, dev_list);\n\n\tif (dev1->devid < dev2->devid)\n\t\treturn -1;\n\telse if (dev1->devid > dev2->devid)\n\t\treturn 1;\n\treturn 0;\n}\n\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       fmode_t flags, void *holder)\n{\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\t/*\n\t * The device_list_mutex cannot be taken here in case opening the\n\t * underlying device takes further locks like open_mutex.\n\t *\n\t * We also don't need the lock here as this is called during mount and\n\t * exclusion is provided by uuid_mutex\n\t */\n\n\tif (fs_devices->opened) {\n\t\tfs_devices->opened++;\n\t\tret = 0;\n\t} else {\n\t\tlist_sort(NULL, &fs_devices->devices, devid_cmp);\n\t\tret = open_fs_devices(fs_devices, flags, holder);\n\t}\n\n\treturn ret;\n}\n\nvoid btrfs_release_disk_super(struct btrfs_super_block *super)\n{\n\tstruct page *page = virt_to_page(super);\n\n\tput_page(page);\n}\n\nstatic struct btrfs_super_block *btrfs_read_disk_super(struct block_device *bdev,\n\t\t\t\t\t\t       u64 bytenr, u64 bytenr_orig)\n{\n\tstruct btrfs_super_block *disk_super;\n\tstruct page *page;\n\tvoid *p;\n\tpgoff_t index;\n\n\t/* make sure our super fits in the device */\n\tif (bytenr + PAGE_SIZE >= i_size_read(bdev->bd_inode))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* make sure our super fits in the page */\n\tif (sizeof(*disk_super) > PAGE_SIZE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* make sure our super doesn't straddle pages on disk */\n\tindex = bytenr >> PAGE_SHIFT;\n\tif ((bytenr + sizeof(*disk_super) - 1) >> PAGE_SHIFT != index)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* pull in the page with our super */\n\tpage = read_cache_page_gfp(bdev->bd_inode->i_mapping, index, GFP_KERNEL);\n\n\tif (IS_ERR(page))\n\t\treturn ERR_CAST(page);\n\n\tp = page_address(page);\n\n\t/* align our pointer to the offset of the super block */\n\tdisk_super = p + offset_in_page(bytenr);\n\n\tif (btrfs_super_bytenr(disk_super) != bytenr_orig ||\n\t    btrfs_super_magic(disk_super) != BTRFS_MAGIC) {\n\t\tbtrfs_release_disk_super(p);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (disk_super->label[0] && disk_super->label[BTRFS_LABEL_SIZE - 1])\n\t\tdisk_super->label[BTRFS_LABEL_SIZE - 1] = 0;\n\n\treturn disk_super;\n}\n\nint btrfs_forget_devices(const char *path)\n{\n\tint ret;\n\n\tmutex_lock(&uuid_mutex);\n\tret = btrfs_free_stale_devices(strlen(path) ? path : NULL, NULL);\n\tmutex_unlock(&uuid_mutex);\n\n\treturn ret;\n}\n\n/*\n * Look for a btrfs signature on a device. This may be called out of the mount path\n * and we are not allowed to call set_blocksize during the scan. The superblock\n * is read via pagecache\n */\nstruct btrfs_device *btrfs_scan_one_device(const char *path, fmode_t flags,\n\t\t\t\t\t   void *holder)\n{\n\tstruct btrfs_super_block *disk_super;\n\tbool new_device_added = false;\n\tstruct btrfs_device *device = NULL;\n\tstruct block_device *bdev;\n\tu64 bytenr, bytenr_orig;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\t/*\n\t * we would like to check all the supers, but that would make\n\t * a btrfs mount succeed after a mkfs from a different FS.\n\t * So, we need to add a special mount option to scan for\n\t * later supers, using BTRFS_SUPER_MIRROR_MAX instead\n\t */\n\tflags |= FMODE_EXCL;\n\n\tbdev = blkdev_get_by_path(path, flags, holder);\n\tif (IS_ERR(bdev))\n\t\treturn ERR_CAST(bdev);\n\n\tbytenr_orig = btrfs_sb_offset(0);\n\tret = btrfs_sb_log_location_bdev(bdev, 0, READ, &bytenr);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdisk_super = btrfs_read_disk_super(bdev, bytenr, bytenr_orig);\n\tif (IS_ERR(disk_super)) {\n\t\tdevice = ERR_CAST(disk_super);\n\t\tgoto error_bdev_put;\n\t}\n\n\tdevice = device_list_add(path, disk_super, &new_device_added);\n\tif (!IS_ERR(device)) {\n\t\tif (new_device_added)\n\t\t\tbtrfs_free_stale_devices(path, device);\n\t}\n\n\tbtrfs_release_disk_super(disk_super);\n\nerror_bdev_put:\n\tblkdev_put(bdev, flags);\n\n\treturn device;\n}\n\n/*\n * Try to find a chunk that intersects [start, start + len] range and when one\n * such is found, record the end of it in *start\n */\nstatic bool contains_pending_extent(struct btrfs_device *device, u64 *start,\n\t\t\t\t    u64 len)\n{\n\tu64 physical_start, physical_end;\n\n\tlockdep_assert_held(&device->fs_info->chunk_mutex);\n\n\tif (!find_first_extent_bit(&device->alloc_state, *start,\n\t\t\t\t   &physical_start, &physical_end,\n\t\t\t\t   CHUNK_ALLOCATED, NULL)) {\n\n\t\tif (in_range(physical_start, *start, len) ||\n\t\t    in_range(*start, physical_start,\n\t\t\t     physical_end - physical_start)) {\n\t\t\t*start = physical_end + 1;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic u64 dev_extent_search_start(struct btrfs_device *device, u64 start)\n{\n\tswitch (device->fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\t/*\n\t\t * We don't want to overwrite the superblock on the drive nor\n\t\t * any area used by the boot loader (grub for example), so we\n\t\t * make sure to start at an offset of at least 1MB.\n\t\t */\n\t\treturn max_t(u64, start, SZ_1M);\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\t/*\n\t\t * We don't care about the starting region like regular\n\t\t * allocator, because we anyway use/reserve the first two zones\n\t\t * for superblock logging.\n\t\t */\n\t\treturn ALIGN(start, device->zone_info->zone_size);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic bool dev_extent_hole_check_zoned(struct btrfs_device *device,\n\t\t\t\t\tu64 *hole_start, u64 *hole_size,\n\t\t\t\t\tu64 num_bytes)\n{\n\tu64 zone_size = device->zone_info->zone_size;\n\tu64 pos;\n\tint ret;\n\tbool changed = false;\n\n\tASSERT(IS_ALIGNED(*hole_start, zone_size));\n\n\twhile (*hole_size > 0) {\n\t\tpos = btrfs_find_allocatable_zones(device, *hole_start,\n\t\t\t\t\t\t   *hole_start + *hole_size,\n\t\t\t\t\t\t   num_bytes);\n\t\tif (pos != *hole_start) {\n\t\t\t*hole_size = *hole_start + *hole_size - pos;\n\t\t\t*hole_start = pos;\n\t\t\tchanged = true;\n\t\t\tif (*hole_size < num_bytes)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tret = btrfs_ensure_empty_zones(device, pos, num_bytes);\n\n\t\t/* Range is ensured to be empty */\n\t\tif (!ret)\n\t\t\treturn changed;\n\n\t\t/* Given hole range was invalid (outside of device) */\n\t\tif (ret == -ERANGE) {\n\t\t\t*hole_start += *hole_size;\n\t\t\t*hole_size = 0;\n\t\t\treturn true;\n\t\t}\n\n\t\t*hole_start += zone_size;\n\t\t*hole_size -= zone_size;\n\t\tchanged = true;\n\t}\n\n\treturn changed;\n}\n\n/**\n * dev_extent_hole_check - check if specified hole is suitable for allocation\n * @device:\tthe device which we have the hole\n * @hole_start: starting position of the hole\n * @hole_size:\tthe size of the hole\n * @num_bytes:\tthe size of the free space that we need\n *\n * This function may modify @hole_start and @hole_size to reflect the suitable\n * position for allocation. Returns 1 if hole position is updated, 0 otherwise.\n */\nstatic bool dev_extent_hole_check(struct btrfs_device *device, u64 *hole_start,\n\t\t\t\t  u64 *hole_size, u64 num_bytes)\n{\n\tbool changed = false;\n\tu64 hole_end = *hole_start + *hole_size;\n\n\tfor (;;) {\n\t\t/*\n\t\t * Check before we set max_hole_start, otherwise we could end up\n\t\t * sending back this offset anyway.\n\t\t */\n\t\tif (contains_pending_extent(device, hole_start, *hole_size)) {\n\t\t\tif (hole_end >= *hole_start)\n\t\t\t\t*hole_size = hole_end - *hole_start;\n\t\t\telse\n\t\t\t\t*hole_size = 0;\n\t\t\tchanged = true;\n\t\t}\n\n\t\tswitch (device->fs_devices->chunk_alloc_policy) {\n\t\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\t\t/* No extra check */\n\t\t\tbreak;\n\t\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\t\tif (dev_extent_hole_check_zoned(device, hole_start,\n\t\t\t\t\t\t\thole_size, num_bytes)) {\n\t\t\t\tchanged = true;\n\t\t\t\t/*\n\t\t\t\t * The changed hole can contain pending extent.\n\t\t\t\t * Loop again to check that.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tbreak;\n\t}\n\n\treturn changed;\n}\n\n/*\n * find_free_dev_extent_start - find free space in the specified device\n * @device:\t  the device which we search the free space in\n * @num_bytes:\t  the size of the free space that we need\n * @search_start: the position from which to begin the search\n * @start:\t  store the start of the free space.\n * @len:\t  the size of the free space. that we find, or the size\n *\t\t  of the max free space if we don't find suitable free space\n *\n * this uses a pretty simple search, the expectation is that it is\n * called very infrequently and that a given device has a small number\n * of extents\n *\n * @start is used to store the start of the free space if we find. But if we\n * don't find suitable free space, it will be used to store the start position\n * of the max free space.\n *\n * @len is used to store the size of the free space that we find.\n * But if we don't find suitable free space, it is used to store the size of\n * the max free space.\n *\n * NOTE: This function will search *commit* root of device tree, and does extra\n * check to ensure dev extents are not double allocated.\n * This makes the function safe to allocate dev extents but may not report\n * correct usable device space, as device extent freed in current transaction\n * is not reported as available.\n */\nstatic int find_free_dev_extent_start(struct btrfs_device *device,\n\t\t\t\tu64 num_bytes, u64 search_start, u64 *start,\n\t\t\t\tu64 *len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_dev_extent *dev_extent;\n\tstruct btrfs_path *path;\n\tu64 hole_size;\n\tu64 max_hole_start;\n\tu64 max_hole_size;\n\tu64 extent_end;\n\tu64 search_end = device->total_bytes;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\n\tsearch_start = dev_extent_search_start(device, search_start);\n\n\tWARN_ON(device->zone_info &&\n\t\t!IS_ALIGNED(num_bytes, device->zone_info->zone_size));\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmax_hole_start = search_start;\n\tmax_hole_size = 0;\n\nagain:\n\tif (search_start >= search_end ||\n\t\ttest_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = device->devid;\n\tkey.offset = search_start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tret = btrfs_search_backwards(root, &key, path);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twhile (1) {\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (key.objectid < device->devid)\n\t\t\tgoto next;\n\n\t\tif (key.objectid > device->devid)\n\t\t\tbreak;\n\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tgoto next;\n\n\t\tif (key.offset > search_start) {\n\t\t\thole_size = key.offset - search_start;\n\t\t\tdev_extent_hole_check(device, &search_start, &hole_size,\n\t\t\t\t\t      num_bytes);\n\n\t\t\tif (hole_size > max_hole_size) {\n\t\t\t\tmax_hole_start = search_start;\n\t\t\t\tmax_hole_size = hole_size;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If this free space is greater than which we need,\n\t\t\t * it must be the max free space that we have found\n\t\t\t * until now, so max_hole_start must point to the start\n\t\t\t * of this free space and the length of this free space\n\t\t\t * is stored in max_hole_size. Thus, we return\n\t\t\t * max_hole_start and max_hole_size and go back to the\n\t\t\t * caller.\n\t\t\t */\n\t\t\tif (hole_size >= num_bytes) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\textent_end = key.offset + btrfs_dev_extent_length(l,\n\t\t\t\t\t\t\t\t  dev_extent);\n\t\tif (extent_end > search_start)\n\t\t\tsearch_start = extent_end;\nnext:\n\t\tpath->slots[0]++;\n\t\tcond_resched();\n\t}\n\n\t/*\n\t * At this point, search_start should be the end of\n\t * allocated dev extents, and when shrinking the device,\n\t * search_end may be smaller than search_start.\n\t */\n\tif (search_end > search_start) {\n\t\thole_size = search_end - search_start;\n\t\tif (dev_extent_hole_check(device, &search_start, &hole_size,\n\t\t\t\t\t  num_bytes)) {\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (hole_size > max_hole_size) {\n\t\t\tmax_hole_start = search_start;\n\t\t\tmax_hole_size = hole_size;\n\t\t}\n\t}\n\n\t/* See above. */\n\tif (max_hole_size < num_bytes)\n\t\tret = -ENOSPC;\n\telse\n\t\tret = 0;\n\nout:\n\tbtrfs_free_path(path);\n\t*start = max_hole_start;\n\tif (len)\n\t\t*len = max_hole_size;\n\treturn ret;\n}\n\nint find_free_dev_extent(struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 *start, u64 *len)\n{\n\t/* FIXME use last free of some kind */\n\treturn find_free_dev_extent_start(device, num_bytes, 0, start, len);\n}\n\nstatic int btrfs_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_device *device,\n\t\t\t  u64 start, u64 *dev_extent_len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf = NULL;\n\tstruct btrfs_dev_extent *extent = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid,\n\t\t\t\t\t  BTRFS_DEV_EXTENT_KEY);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t\tBUG_ON(found_key.offset > start || found_key.offset +\n\t\t       btrfs_dev_extent_length(leaf, extent) < start);\n\t\tkey = found_key;\n\t\tbtrfs_release_path(path);\n\t\tgoto again;\n\t} else if (ret == 0) {\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t} else {\n\t\tgoto out;\n\t}\n\n\t*dev_extent_len = btrfs_dev_extent_length(leaf, extent);\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret == 0)\n\t\tset_bit(BTRFS_TRANS_HAVE_FREE_BGS, &trans->transaction->flags);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic u64 find_next_chunk(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *n;\n\tu64 ret = 0;\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tn = rb_last(&em_tree->map.rb_root);\n\tif (n) {\n\t\tem = rb_entry(n, struct extent_map, rb_node);\n\t\tret = em->start + em->len;\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn ret;\n}\n\nstatic noinline int find_next_devid(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 *devid_ret)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, fs_info->chunk_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\n\tif (ret == 0) {\n\t\t/* Corruption */\n\t\tbtrfs_err(fs_info, \"corrupted chunk tree devid -1 matched\");\n\t\tret = -EUCLEAN;\n\t\tgoto error;\n\t}\n\n\tret = btrfs_previous_item(fs_info->chunk_root, path,\n\t\t\t\t  BTRFS_DEV_ITEMS_OBJECTID,\n\t\t\t\t  BTRFS_DEV_ITEM_KEY);\n\tif (ret) {\n\t\t*devid_ret = 1;\n\t} else {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &found_key,\n\t\t\t\t      path->slots[0]);\n\t\t*devid_ret = found_key.offset + 1;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * the device information is stored in the chunk root\n * the btrfs_device struct should be fully filled in\n */\nstatic int btrfs_add_dev_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tunsigned long ptr;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_insert_empty_item(trans, trans->fs_info->chunk_root, path,\n\t\t\t\t      &key, sizeof(*dev_item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_generation(leaf, dev_item, 0);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_set_device_group(leaf, dev_item, 0);\n\tbtrfs_set_device_seek_speed(leaf, dev_item, 0);\n\tbtrfs_set_device_bandwidth(leaf, dev_item, 0);\n\tbtrfs_set_device_start_offset(leaf, dev_item, 0);\n\n\tptr = btrfs_device_uuid(dev_item);\n\twrite_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n\tptr = btrfs_device_fsid(dev_item);\n\twrite_extent_buffer(leaf, trans->fs_info->fs_devices->metadata_uuid,\n\t\t\t    ptr, BTRFS_FSID_SIZE);\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Function to update ctime/mtime for a given device path.\n * Mainly used for ctime/mtime based probe like libblkid.\n */\nstatic void update_dev_time(const char *path_name)\n{\n\tstruct file *filp;\n\n\tfilp = filp_open(path_name, O_RDWR, 0);\n\tif (IS_ERR(filp))\n\t\treturn;\n\tfile_update_time(filp);\n\tfilp_close(filp, NULL);\n}\n\nstatic int btrfs_rm_dev_item(struct btrfs_device *device)\n{\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_trans_handle *trans;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (!ret)\n\t\tret = btrfs_commit_transaction(trans);\n\treturn ret;\n}\n\n/*\n * Verify that @num_devices satisfies the RAID profile constraints in the whole\n * filesystem. It's up to the caller to adjust that number regarding eg. device\n * replace.\n */\nstatic int btrfs_check_raid_min_devices(struct btrfs_fs_info *fs_info,\n\t\tu64 num_devices)\n{\n\tu64 all_avail;\n\tunsigned seq;\n\tint i;\n\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tall_avail = fs_info->avail_data_alloc_bits |\n\t\t\t    fs_info->avail_system_alloc_bits |\n\t\t\t    fs_info->avail_metadata_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {\n\t\tif (!(all_avail & btrfs_raid_array[i].bg_flag))\n\t\t\tcontinue;\n\n\t\tif (num_devices < btrfs_raid_array[i].devs_min)\n\t\t\treturn btrfs_raid_array[i].mindev_error;\n\t}\n\n\treturn 0;\n}\n\nstatic struct btrfs_device * btrfs_find_next_active_device(\n\t\tstruct btrfs_fs_devices *fs_devs, struct btrfs_device *device)\n{\n\tstruct btrfs_device *next_device;\n\n\tlist_for_each_entry(next_device, &fs_devs->devices, dev_list) {\n\t\tif (next_device != device &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &next_device->dev_state)\n\t\t    && next_device->bdev)\n\t\t\treturn next_device;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Helper function to check if the given device is part of s_bdev / latest_bdev\n * and replace it with the provided or the next active device, in the context\n * where this function called, there should be always be another device (or\n * this_dev) which is active.\n */\nvoid __cold btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t\t    struct btrfs_device *next_device)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\n\tif (!next_device)\n\t\tnext_device = btrfs_find_next_active_device(fs_info->fs_devices,\n\t\t\t\t\t\t\t    device);\n\tASSERT(next_device);\n\n\tif (fs_info->sb->s_bdev &&\n\t\t\t(fs_info->sb->s_bdev == device->bdev))\n\t\tfs_info->sb->s_bdev = next_device->bdev;\n\n\tif (fs_info->fs_devices->latest_bdev == device->bdev)\n\t\tfs_info->fs_devices->latest_bdev = next_device->bdev;\n}\n\n/*\n * Return btrfs_fs_devices::num_devices excluding the device that's being\n * currently replaced.\n */\nstatic u64 btrfs_num_devices(struct btrfs_fs_info *fs_info)\n{\n\tu64 num_devices = fs_info->fs_devices->num_devices;\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace)) {\n\t\tASSERT(num_devices > 1);\n\t\tnum_devices--;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn num_devices;\n}\n\nvoid btrfs_scratch_superblocks(struct btrfs_fs_info *fs_info,\n\t\t\t       struct block_device *bdev,\n\t\t\t       const char *device_path)\n{\n\tstruct btrfs_super_block *disk_super;\n\tint copy_num;\n\n\tif (!bdev)\n\t\treturn;\n\n\tfor (copy_num = 0; copy_num < BTRFS_SUPER_MIRROR_MAX; copy_num++) {\n\t\tstruct page *page;\n\t\tint ret;\n\n\t\tdisk_super = btrfs_read_dev_one_super(bdev, copy_num);\n\t\tif (IS_ERR(disk_super))\n\t\t\tcontinue;\n\n\t\tif (bdev_is_zoned(bdev)) {\n\t\t\tbtrfs_reset_sb_log_zones(bdev, copy_num);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemset(&disk_super->magic, 0, sizeof(disk_super->magic));\n\n\t\tpage = virt_to_page(disk_super);\n\t\tset_page_dirty(page);\n\t\tlock_page(page);\n\t\t/* write_on_page() unlocks the page */\n\t\tret = write_one_page(page);\n\t\tif (ret)\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t\"error clearing superblock number %d (%d)\",\n\t\t\t\tcopy_num, ret);\n\t\tbtrfs_release_disk_super(disk_super);\n\n\t}\n\n\t/* Notify udev that device has changed */\n\tbtrfs_kobject_uevent(bdev, KOBJ_CHANGE);\n\n\t/* Update ctime/mtime for device path for libblkid */\n\tupdate_dev_time(device_path);\n}\n\nint btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}\n\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlockdep_assert_held(&srcdev->fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * in case of fs with no seed, srcdev->fs_devices will point\n\t * to fs_devices of fs_info. However when the dev being replaced is\n\t * a seed dev it will point to the seed's local fs_devices. In short\n\t * srcdev will have its correct fs_devices in both the cases.\n\t */\n\tfs_devices = srcdev->fs_devices;\n\n\tlist_del_rcu(&srcdev->dev_list);\n\tlist_del(&srcdev->dev_alloc_list);\n\tfs_devices->num_devices--;\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &srcdev->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state))\n\t\tfs_devices->rw_devices--;\n\n\tif (srcdev->bdev)\n\t\tfs_devices->open_devices--;\n}\n\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = srcdev->fs_devices;\n\n\tmutex_lock(&uuid_mutex);\n\n\tbtrfs_close_bdev(srcdev);\n\tsynchronize_rcu();\n\tbtrfs_free_device(srcdev);\n\n\t/* if this is no devs we rather delete the fs_devices */\n\tif (!fs_devices->num_devices) {\n\t\t/*\n\t\t * On a mounted FS, num_devices can't be zero unless it's a\n\t\t * seed. In case of a seed device being replaced, the replace\n\t\t * target added to the sprout FS, so there will be no more\n\t\t * device left under the seed FS.\n\t\t */\n\t\tASSERT(fs_devices->seeding);\n\n\t\tlist_del_init(&fs_devices->seed_list);\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\tmutex_unlock(&uuid_mutex);\n}\n\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = tgtdev->fs_info->fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\n\tbtrfs_sysfs_remove_device(tgtdev);\n\n\tif (tgtdev->bdev)\n\t\tfs_devices->open_devices--;\n\n\tfs_devices->num_devices--;\n\n\tbtrfs_assign_next_active_device(tgtdev, NULL);\n\n\tlist_del_rcu(&tgtdev->dev_list);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * The update_dev_time() with in btrfs_scratch_superblocks()\n\t * may lead to a call to btrfs_show_devname() which will try\n\t * to hold device_list_mutex. And here this device\n\t * is already out of device list, so we don't have to hold\n\t * the device_list_mutex lock.\n\t */\n\tbtrfs_scratch_superblocks(tgtdev->fs_info, tgtdev->bdev,\n\t\t\t\t  tgtdev->name->str);\n\n\tbtrfs_close_bdev(tgtdev);\n\tsynchronize_rcu();\n\tbtrfs_free_device(tgtdev);\n}\n\nstatic struct btrfs_device *btrfs_find_device_by_path(\n\t\tstruct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tint ret = 0;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tu8 *dev_uuid;\n\tstruct block_device *bdev;\n\tstruct btrfs_device *device;\n\n\tret = btrfs_get_bdev_and_sb(device_path, FMODE_READ,\n\t\t\t\t    fs_info->bdev_holder, 0, &bdev, &disk_super);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_uuid = disk_super->dev_item.uuid;\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->metadata_uuid);\n\telse\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->fsid);\n\n\tbtrfs_release_disk_super(disk_super);\n\tif (!device)\n\t\tdevice = ERR_PTR(-ENOENT);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn device;\n}\n\n/*\n * Lookup a device given by device id, or the path if the id is 0.\n */\nstruct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tstruct btrfs_device *device;\n\n\tif (devid) {\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL,\n\t\t\t\t\t   NULL);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tif (!device_path || !device_path[0])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strcmp(device_path, \"missing\") == 0) {\n\t\t/* Find first missing device */\n\t\tlist_for_each_entry(device, &fs_info->fs_devices->devices,\n\t\t\t\t    dev_list) {\n\t\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t     &device->dev_state) && !device->bdev)\n\t\t\t\treturn device;\n\t\t}\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn btrfs_find_device_by_path(fs_info, device_path);\n}\n\n/*\n * does all the dirty work required for changing file system's UUID.\n */\nstatic int btrfs_prepare_sprout(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_fs_devices *old_devices;\n\tstruct btrfs_fs_devices *seed_devices;\n\tstruct btrfs_super_block *disk_super = fs_info->super_copy;\n\tstruct btrfs_device *device;\n\tu64 super_flags;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tif (!fs_devices->seeding)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Private copy of the seed devices, anchored at\n\t * fs_info->fs_devices->seed_list\n\t */\n\tseed_devices = alloc_fs_devices(NULL, NULL);\n\tif (IS_ERR(seed_devices))\n\t\treturn PTR_ERR(seed_devices);\n\n\t/*\n\t * It's necessary to retain a copy of the original seed fs_devices in\n\t * fs_uuids so that filesystems which have been seeded can successfully\n\t * reference the seed device from open_seed_devices. This also supports\n\t * multiple fs seed.\n\t */\n\told_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(old_devices)) {\n\t\tkfree(seed_devices);\n\t\treturn PTR_ERR(old_devices);\n\t}\n\n\tlist_add(&old_devices->fs_list, &fs_uuids);\n\n\tmemcpy(seed_devices, fs_devices, sizeof(*seed_devices));\n\tseed_devices->opened = 1;\n\tINIT_LIST_HEAD(&seed_devices->devices);\n\tINIT_LIST_HEAD(&seed_devices->alloc_list);\n\tmutex_init(&seed_devices->device_list_mutex);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_splice_init_rcu(&fs_devices->devices, &seed_devices->devices,\n\t\t\t      synchronize_rcu);\n\tlist_for_each_entry(device, &seed_devices->devices, dev_list)\n\t\tdevice->fs_devices = seed_devices;\n\n\tfs_devices->seeding = false;\n\tfs_devices->num_devices = 0;\n\tfs_devices->open_devices = 0;\n\tfs_devices->missing_devices = 0;\n\tfs_devices->rotating = false;\n\tlist_add(&seed_devices->seed_list, &fs_devices->seed_list);\n\n\tgenerate_random_uuid(fs_devices->fsid);\n\tmemcpy(fs_devices->metadata_uuid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmemcpy(disk_super->fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tsuper_flags = btrfs_super_flags(disk_super) &\n\t\t      ~BTRFS_SUPER_FLAG_SEEDING;\n\tbtrfs_set_super_flags(disk_super, super_flags);\n\n\treturn 0;\n}\n\n/*\n * Store the expected generation for seed devices in device items.\n */\nstatic int btrfs_finish_sprout(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct request_queue *q;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct super_block *sb = fs_info->sb;\n\tstruct rcu_string *name;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 orig_super_total_bytes;\n\tu64 orig_super_num_devices;\n\tint seeding_dev = 0;\n\tint ret = 0;\n\tbool locked = false;\n\n\tif (sb_rdonly(sb) && !fs_devices->seeding)\n\t\treturn -EROFS;\n\n\tbdev = blkdev_get_by_path(device_path, FMODE_WRITE | FMODE_EXCL,\n\t\t\t\t  fs_info->bdev_holder);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tif (!btrfs_check_device_zone_type(fs_info, bdev)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (fs_devices->seeding) {\n\t\tseeding_dev = 1;\n\t\tdown_write(&sb->s_umount);\n\t\tmutex_lock(&uuid_mutex);\n\t\tlocked = true;\n\t}\n\n\tsync_blockdev(bdev);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_devices->devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tret = -EEXIST;\n\t\t\trcu_read_unlock();\n\t\t\tgoto error;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tdevice = btrfs_alloc_device(fs_info, NULL, NULL);\n\tif (IS_ERR(device)) {\n\t\t/* we can safely leave the fs_devices entry around */\n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tname = rcu_string_strdup(device_path, GFP_KERNEL);\n\tif (!name) {\n\t\tret = -ENOMEM;\n\t\tgoto error_free_device;\n\t}\n\trcu_assign_pointer(device->name, name);\n\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\n\tret = btrfs_get_dev_zone_info(device);\n\tif (ret)\n\t\tgoto error_free_device;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto error_free_zone;\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = trans->transid;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes = round_down(i_size_read(bdev->bd_inode),\n\t\t\t\t\t fs_info->sectorsize);\n\tdevice->disk_total_bytes = device->total_bytes;\n\tdevice->commit_total_bytes = device->total_bytes;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->mode = FMODE_EXCL;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\n\tif (seeding_dev) {\n\t\tbtrfs_clear_sb_rdonly(sb);\n\t\tret = btrfs_prepare_sprout(fs_info);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_trans;\n\t\t}\n\t}\n\n\tdevice->fs_devices = fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\tlist_add(&device->dev_alloc_list, &fs_devices->alloc_list);\n\tfs_devices->num_devices++;\n\tfs_devices->open_devices++;\n\tfs_devices->rw_devices++;\n\tfs_devices->total_devices++;\n\tfs_devices->total_rw_bytes += device->total_bytes;\n\n\tatomic64_add(device->total_bytes, &fs_info->free_chunk_space);\n\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = true;\n\n\torig_super_total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\tround_down(orig_super_total_bytes + device->total_bytes,\n\t\t\t   fs_info->sectorsize));\n\n\torig_super_num_devices = btrfs_super_num_devices(fs_info->super_copy);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices + 1);\n\n\t/*\n\t * we've got more storage, clear any full flags on the space\n\t * infos\n\t */\n\tbtrfs_clear_space_info_full(fs_info);\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/* Add sysfs device entry */\n\tbtrfs_sysfs_add_device(device);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (seeding_dev) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tret = init_first_rw_device(trans);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\t}\n\n\tret = btrfs_add_dev_item(trans, device);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto error_sysfs;\n\t}\n\n\tif (seeding_dev) {\n\t\tret = btrfs_finish_sprout(trans);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\n\t\t/*\n\t\t * fs_devices now represents the newly sprouted filesystem and\n\t\t * its fsid has been changed by btrfs_prepare_sprout\n\t\t */\n\t\tbtrfs_sysfs_update_sprout_fsid(fs_devices);\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\n\tif (seeding_dev) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t\tlocked = false;\n\n\t\tif (ret) /* transaction commit */\n\t\t\treturn ret;\n\n\t\tret = btrfs_relocate_sys_chunks(fs_info);\n\t\tif (ret < 0)\n\t\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t    \"Failed to relocate sys chunks after device initialization. This can be fixed using the \\\"btrfs balance\\\" command.\");\n\t\ttrans = btrfs_attach_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tif (PTR_ERR(trans) == -ENOENT)\n\t\t\t\treturn 0;\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto error_sysfs;\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\n\t/*\n\t * Now that we have written a new super block to this device, check all\n\t * other fs_devices list if device_path alienates any other scanned\n\t * device.\n\t * We can ignore the return value as it typically returns -EINVAL and\n\t * only succeeds if the device was an alien.\n\t */\n\tbtrfs_forget_devices(device_path);\n\n\t/* Update ctime/mtime for blkid or udev */\n\tupdate_dev_time(device_path);\n\n\treturn ret;\n\nerror_sysfs:\n\tbtrfs_sysfs_remove_device(device);\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_del_rcu(&device->dev_list);\n\tlist_del(&device->dev_alloc_list);\n\tfs_info->fs_devices->num_devices--;\n\tfs_info->fs_devices->open_devices--;\n\tfs_info->fs_devices->rw_devices--;\n\tfs_info->fs_devices->total_devices--;\n\tfs_info->fs_devices->total_rw_bytes -= device->total_bytes;\n\tatomic64_sub(device->total_bytes, &fs_info->free_chunk_space);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\t\t\t    orig_super_total_bytes);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\nerror_trans:\n\tif (seeding_dev)\n\t\tbtrfs_set_sb_rdonly(sb);\n\tif (trans)\n\t\tbtrfs_end_transaction(trans);\nerror_free_zone:\n\tbtrfs_destroy_dev_zone_info(device);\nerror_free_device:\n\tbtrfs_free_device(device);\nerror:\n\tblkdev_put(bdev, FMODE_EXCL);\n\tif (locked) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t}\n\treturn ret;\n}\n\nstatic noinline int btrfs_update_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_mark_buffer_dirty(leaf);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total;\n\tu64 diff;\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\treturn -EACCES;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\told_total = btrfs_super_total_bytes(super_copy);\n\tdiff = round_down(new_size - device->total_bytes, fs_info->sectorsize);\n\n\tif (new_size <= device->total_bytes ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total + diff, fs_info->sectorsize));\n\tdevice->fs_devices->total_rw_bytes += diff;\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tbtrfs_clear_space_info_full(device->fs_info);\n\tif (list_empty(&device->post_commit_list))\n\t\tlist_add_tail(&device->post_commit_list,\n\t\t\t      &trans->transaction->dev_update_list);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn btrfs_update_device(trans, device);\n}\n\nstatic int btrfs_free_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = chunk_offset;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\telse if (ret > 0) { /* Logic error or corruption */\n\t\tbtrfs_handle_fs_error(fs_info, -ENOENT,\n\t\t\t\t      \"Failed lookup while freeing chunk.\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret < 0)\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to delete chunk item.\");\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_del_sys_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *ptr;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur;\n\tstruct btrfs_key key;\n\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tptr = super_copy->sys_chunk_array;\n\tcur = 0;\n\n\twhile (cur < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)ptr;\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tlen = sizeof(*disk_key);\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)(ptr + len);\n\t\t\tnum_stripes = btrfs_stack_chunk_num_stripes(chunk);\n\t\t\tlen += btrfs_chunk_item_size(num_stripes);\n\t\t} else {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (key.objectid == BTRFS_FIRST_CHUNK_TREE_OBJECTID &&\n\t\t    key.offset == chunk_offset) {\n\t\t\tmemmove(ptr, ptr + len, array_size - (cur + len));\n\t\t\tarray_size -= len;\n\t\t\tbtrfs_set_super_sys_array_size(super_copy, array_size);\n\t\t} else {\n\t\t\tptr += len;\n\t\t\tcur += len;\n\t\t}\n\t}\n\treturn ret;\n}\n\n/*\n * btrfs_get_chunk_map() - Find the mapping containing the given logical extent.\n * @logical: Logical block offset in bytes.\n * @length: Length of extent in bytes.\n *\n * Return: Chunk mapping or ERR_PTR.\n */\nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, logical, length);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_crit(fs_info, \"unable to find logical %llu length %llu\",\n\t\t\t   logical, length);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (em->start > logical || em->start + em->len < logical) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"found a bad mapping, wanted %llu-%llu, found %llu-%llu\",\n\t\t\t   logical, length, em->start, em->start + em->len);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/* callers are responsible for dropping em's ref. */\n\treturn em;\n}\n\nstatic int remove_chunk_item(struct btrfs_trans_handle *trans,\n\t\t\t     struct map_lookup *map, u64 chunk_offset)\n{\n\tint i;\n\n\t/*\n\t * Removing chunk items and updating the device items in the chunks btree\n\t * requires holding the chunk_mutex.\n\t * See the comment at btrfs_chunk_alloc() for the details.\n\t */\n\tlockdep_assert_held(&trans->fs_info->chunk_mutex);\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tint ret;\n\n\t\tret = btrfs_update_device(trans, map->stripes[i].dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn btrfs_free_chunk(trans, chunk_offset);\n}\n\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 dev_extent_len = 0;\n\tint i, ret = 0;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em)) {\n\t\t/*\n\t\t * This is a logic error, but we don't want to just rely on the\n\t\t * user having built with ASSERT enabled, so if ASSERT doesn't\n\t\t * do anything we still error out.\n\t\t */\n\t\tASSERT(0);\n\t\treturn PTR_ERR(em);\n\t}\n\tmap = em->map_lookup;\n\n\t/*\n\t * First delete the device extent items from the devices btree.\n\t * We take the device_list_mutex to avoid racing with the finishing phase\n\t * of a device replace operation. See the comment below before acquiring\n\t * fs_info->chunk_mutex. Note that here we do not acquire the chunk_mutex\n\t * because that can result in a deadlock when deleting the device extent\n\t * items from the devices btree - COWing an extent buffer from the btree\n\t * may result in allocating a new metadata chunk, which would attempt to\n\t * lock again fs_info->chunk_mutex.\n\t */\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tret = btrfs_free_dev_extent(trans, device,\n\t\t\t\t\t    map->stripes[i].physical,\n\t\t\t\t\t    &dev_extent_len);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (device->bytes_used > 0) {\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tbtrfs_device_set_bytes_used(device,\n\t\t\t\t\tdevice->bytes_used - dev_extent_len);\n\t\t\tatomic64_add(dev_extent_len, &fs_info->free_chunk_space);\n\t\t\tbtrfs_clear_space_info_full(fs_info);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * We acquire fs_info->chunk_mutex for 2 reasons:\n\t *\n\t * 1) Just like with the first phase of the chunk allocation, we must\n\t *    reserve system space, do all chunk btree updates and deletions, and\n\t *    update the system chunk array in the superblock while holding this\n\t *    mutex. This is for similar reasons as explained on the comment at\n\t *    the top of btrfs_chunk_alloc();\n\t *\n\t * 2) Prevent races with the final phase of a device replace operation\n\t *    that replaces the device object associated with the map's stripes,\n\t *    because the device object's id can change at any time during that\n\t *    final phase of the device replace operation\n\t *    (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the\n\t *    replaced device and then see it with an ID of\n\t *    BTRFS_DEV_REPLACE_DEVID, which would cause a failure when updating\n\t *    the device item, which does not exists on the chunk btree.\n\t *    The finishing phase of device replace acquires both the\n\t *    device_list_mutex and the chunk_mutex, in that order, so we are\n\t *    safe by just acquiring the chunk_mutex.\n\t */\n\ttrans->removing_chunk = true;\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tcheck_system_chunk(trans, map->type);\n\n\tret = remove_chunk_item(trans, map, chunk_offset);\n\t/*\n\t * Normally we should not get -ENOSPC since we reserved space before\n\t * through the call to check_system_chunk().\n\t *\n\t * Despite our system space_info having enough free space, we may not\n\t * be able to allocate extents from its block groups, because all have\n\t * an incompatible profile, which will force us to allocate a new system\n\t * block group with the right profile, or right after we called\n\t * check_system_space() above, a scrub turned the only system block group\n\t * with enough free space into RO mode.\n\t * This is explained with more detail at do_chunk_alloc().\n\t *\n\t * So if we get -ENOSPC, allocate a new system chunk and retry once.\n\t */\n\tif (ret == -ENOSPC) {\n\t\tconst u64 sys_flags = btrfs_system_alloc_profile(fs_info);\n\t\tstruct btrfs_block_group *sys_bg;\n\n\t\tsys_bg = btrfs_alloc_chunk(trans, sys_flags);\n\t\tif (IS_ERR(sys_bg)) {\n\t\t\tret = PTR_ERR(sys_bg);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = btrfs_chunk_alloc_add_chunk_item(trans, sys_bg);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = remove_chunk_item(trans, map, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t} else if (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\ttrace_btrfs_chunk_free(fs_info, map, chunk_offset, em->len);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_del_sys_chunk(fs_info, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\ttrans->removing_chunk = false;\n\n\t/*\n\t * We are done with chunk btree updates and deletions, so release the\n\t * system space we previously reserved (with check_system_chunk()).\n\t */\n\tbtrfs_trans_release_chunk_metadata(trans);\n\n\tret = btrfs_remove_block_group(trans, chunk_offset, em);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\nout:\n\tif (trans->removing_chunk) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\ttrans->removing_chunk = false;\n\t}\n\t/* once for us */\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_block_group *block_group;\n\tu64 length;\n\tint ret;\n\n\t/*\n\t * Prevent races with automatic removal of unused block groups.\n\t * After we relocate and before we remove the chunk with offset\n\t * chunk_offset, automatic removal of the block group can kick in,\n\t * resulting in a failure when calling btrfs_remove_chunk() below.\n\t *\n\t * Make sure to acquire this mutex before doing a tree search (dev\n\t * or chunk trees) to find chunks. Otherwise the cleaner kthread might\n\t * call btrfs_remove_chunk() (through btrfs_delete_unused_bgs()) after\n\t * we release the path used to search the chunk/dev tree and before\n\t * the current task acquires this mutex and calls us.\n\t */\n\tlockdep_assert_held(&fs_info->reclaim_bgs_lock);\n\n\t/* step one, relocate all the extents inside this chunk */\n\tbtrfs_scrub_pause(fs_info);\n\tret = btrfs_relocate_block_group(fs_info, chunk_offset);\n\tbtrfs_scrub_continue(fs_info);\n\tif (ret)\n\t\treturn ret;\n\n\tblock_group = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tif (!block_group)\n\t\treturn -ENOENT;\n\tbtrfs_discard_cancel_work(&fs_info->discard_ctl, block_group);\n\tlength = block_group->length;\n\tbtrfs_put_block_group(block_group);\n\n\t/*\n\t * On a zoned file system, discard the whole block group, this will\n\t * trigger a REQ_OP_ZONE_RESET operation on the device zone. If\n\t * resetting the zone fails, don't treat it as a fatal problem from the\n\t * filesystem's point of view.\n\t */\n\tif (btrfs_is_zoned(fs_info)) {\n\t\tret = btrfs_discard_extent(fs_info, chunk_offset, length, NULL);\n\t\tif (ret)\n\t\t\tbtrfs_info(fs_info,\n\t\t\t\t\"failed to reset zone %llu after relocation\",\n\t\t\t\tchunk_offset);\n\t}\n\n\ttrans = btrfs_start_trans_remove_block_group(root->fs_info,\n\t\t\t\t\t\t     chunk_offset);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * step two, delete the device extents and the\n\t * chunk tree entries\n\t */\n\tret = btrfs_remove_chunk(trans, chunk_offset);\n\tbtrfs_end_transaction(trans);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 chunk_type;\n\tbool retried = false;\n\tint failed = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\nagain:\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto error;\n\t\t}\n\t\tBUG_ON(ret == 0); /* Corruption */\n\n\t\tret = btrfs_previous_item(chunk_root, path, key.objectid,\n\t\t\t\t\t  key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t\tif (ret > 0)\n\t\t\tbreak;\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tchunk = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t       struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\t\tbtrfs_release_path(path);\n\n\t\tif (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tfailed++;\n\t\t\telse\n\t\t\t\tBUG_ON(ret);\n\t\t}\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\tret = 0;\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (WARN_ON(failed && retried)) {\n\t\tret = -ENOSPC;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * return 1 : allocate a data chunk successfully,\n * return <0: errors during allocating a data chunk,\n * return 0 : no need to allocate a data chunk.\n */\nstatic int btrfs_may_alloc_data_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t      u64 chunk_offset)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 bytes_used;\n\tu64 chunk_type;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tASSERT(cache);\n\tchunk_type = cache->flags;\n\tbtrfs_put_block_group(cache);\n\n\tif (!(chunk_type & BTRFS_BLOCK_GROUP_DATA))\n\t\treturn 0;\n\n\tspin_lock(&fs_info->data_sinfo->lock);\n\tbytes_used = fs_info->data_sinfo->bytes_used;\n\tspin_unlock(&fs_info->data_sinfo->lock);\n\n\tif (!bytes_used) {\n\t\tstruct btrfs_trans_handle *trans;\n\t\tint ret;\n\n\t\ttrans =\tbtrfs_join_transaction(fs_info->tree_root);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_force_chunk_alloc(trans, BTRFS_BLOCK_GROUP_DATA);\n\t\tbtrfs_end_transaction(trans);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int insert_balance_item(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_balance_control *bctl)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tmemzero_extent_buffer(leaf, (unsigned long)item, sizeof(*item));\n\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->data);\n\tbtrfs_set_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->meta);\n\tbtrfs_set_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->sys);\n\tbtrfs_set_balance_sys(leaf, item, &disk_bargs);\n\n\tbtrfs_set_balance_flags(leaf, item, bctl->flags);\n\n\tbtrfs_mark_buffer_dirty(leaf);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int del_balance_item(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction_fallback_global_rsv(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * This is a heuristic used to reduce the number of chunks balanced on\n * resume after balance was interrupted.\n */\nstatic void update_balance_args(struct btrfs_balance_control *bctl)\n{\n\t/*\n\t * Turn on soft mode for chunk types that were being converted.\n\t */\n\tif (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\n\t/*\n\t * Turn on usage filter if is not already used.  The idea is\n\t * that chunks that we have already balanced should be\n\t * reasonably full.  Don't do it for chunks that are being\n\t * converted - that will keep us from relocating unconverted\n\t * (albeit full) chunks.\n\t */\n\tif (!(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->data.usage = 90;\n\t}\n\tif (!(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->sys.usage = 90;\n\t}\n\tif (!(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->meta.usage = 90;\n\t}\n}\n\n/*\n * Clear the balance status in fs_info and delete the balance item from disk.\n */\nstatic void reset_balance_state(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tint ret;\n\n\tBUG_ON(!fs_info->balance_ctl);\n\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = NULL;\n\tspin_unlock(&fs_info->balance_lock);\n\n\tkfree(bctl);\n\tret = del_balance_item(fs_info);\n\tif (ret)\n\t\tbtrfs_handle_fs_error(fs_info, ret, NULL);\n}\n\n/*\n * Balance filters.  Return 1 if chunk should be filtered out\n * (should not be balanced).\n */\nstatic int chunk_profiles_filter(u64 chunk_type,\n\t\t\t\t struct btrfs_balance_args *bargs)\n{\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->profiles & chunk_type)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_usage_range_filter(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 chunk_used;\n\tu64 user_thresh_min;\n\tu64 user_thresh_max;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = cache->used;\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh_min = 0;\n\telse\n\t\tuser_thresh_min = div_factor_fine(cache->length,\n\t\t\t\t\t\t  bargs->usage_min);\n\n\tif (bargs->usage_max == 0)\n\t\tuser_thresh_max = 1;\n\telse if (bargs->usage_max > 100)\n\t\tuser_thresh_max = cache->length;\n\telse\n\t\tuser_thresh_max = div_factor_fine(cache->length,\n\t\t\t\t\t\t  bargs->usage_max);\n\n\tif (user_thresh_min <= chunk_used && chunk_used < user_thresh_max)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_usage_filter(struct btrfs_fs_info *fs_info,\n\t\tu64 chunk_offset, struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 chunk_used, user_thresh;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = cache->used;\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh = 1;\n\telse if (bargs->usage > 100)\n\t\tuser_thresh = cache->length;\n\telse\n\t\tuser_thresh = div_factor_fine(cache->length, bargs->usage);\n\n\tif (chunk_used < user_thresh)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_devid_filter(struct extent_buffer *leaf,\n\t\t\t      struct btrfs_chunk *chunk,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tint i;\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) == bargs->devid)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic u64 calc_data_stripes(u64 type, int num_stripes)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(type);\n\tconst int ncopies = btrfs_raid_array[index].ncopies;\n\tconst int nparity = btrfs_raid_array[index].nparity;\n\n\treturn (num_stripes - nparity) / ncopies;\n}\n\n/* [pstart, pend) */\nstatic int chunk_drange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tu64 stripe_offset;\n\tu64 stripe_length;\n\tu64 type;\n\tint factor;\n\tint i;\n\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_DEVID))\n\t\treturn 0;\n\n\ttype = btrfs_chunk_type(leaf, chunk);\n\tfactor = calc_data_stripes(type, num_stripes);\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) != bargs->devid)\n\t\t\tcontinue;\n\n\t\tstripe_offset = btrfs_stripe_offset(leaf, stripe);\n\t\tstripe_length = btrfs_chunk_length(leaf, chunk);\n\t\tstripe_length = div_u64(stripe_length, factor);\n\n\t\tif (stripe_offset < bargs->pend &&\n\t\t    stripe_offset + stripe_length > bargs->pstart)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/* [vstart, vend) */\nstatic int chunk_vrange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       u64 chunk_offset,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tif (chunk_offset < bargs->vend &&\n\t    chunk_offset + btrfs_chunk_length(leaf, chunk) > bargs->vstart)\n\t\t/* at least part of the chunk is inside this vrange */\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_stripes_range_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tif (bargs->stripes_min <= num_stripes\n\t\t\t&& num_stripes <= bargs->stripes_max)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_soft_convert_filter(u64 chunk_type,\n\t\t\t\t     struct btrfs_balance_args *bargs)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn 0;\n\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->target == chunk_type)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int should_balance_chunk(struct extent_buffer *leaf,\n\t\t\t\tstruct btrfs_chunk *chunk, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_balance_args *bargs = NULL;\n\tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t/* type filter */\n\tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n\t      (bctl->flags & BTRFS_BALANCE_TYPE_MASK))) {\n\t\treturn 0;\n\t}\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\tbargs = &bctl->data;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tbargs = &bctl->sys;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\tbargs = &bctl->meta;\n\n\t/* profiles filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_PROFILES) &&\n\t    chunk_profiles_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* usage filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    chunk_usage_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    chunk_usage_range_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DEVID) &&\n\t    chunk_devid_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* drange filter, makes sense only with devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DRANGE) &&\n\t    chunk_drange_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* vrange filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_VRANGE) &&\n\t    chunk_vrange_filter(leaf, chunk, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* stripes filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE) &&\n\t    chunk_stripes_range_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* soft profile changing mode */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_SOFT) &&\n\t    chunk_soft_convert_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/*\n\t * limited by count, must be the last filter\n\t */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT)) {\n\t\tif (bargs->limit == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit--;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)) {\n\t\t/*\n\t\t * Same logic as the 'limit' filter; the minimum cannot be\n\t\t * determined here because we do not have the global information\n\t\t * about the count of all chunks that satisfy the filters.\n\t\t */\n\t\tif (bargs->limit_max == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit_max--;\n\t}\n\n\treturn 1;\n}\n\nstatic int __btrfs_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tu64 chunk_type;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tint ret;\n\tint enospc_errors = 0;\n\tbool counting = true;\n\t/* The single value limit and min/max limits use the same bytes in the */\n\tu64 limit_data = bctl->data.limit;\n\tu64 limit_meta = bctl->meta.limit;\n\tu64 limit_sys = bctl->sys.limit;\n\tu32 count_data = 0;\n\tu32 count_meta = 0;\n\tu32 count_sys = 0;\n\tint chunk_reserved = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\t/* zero out stat counters */\n\tspin_lock(&fs_info->balance_lock);\n\tmemset(&bctl->stat, 0, sizeof(bctl->stat));\n\tspin_unlock(&fs_info->balance_lock);\nagain:\n\tif (!counting) {\n\t\t/*\n\t\t * The single value limit and min/max limits use the same bytes\n\t\t * in the\n\t\t */\n\t\tbctl->data.limit = limit_data;\n\t\tbctl->meta.limit = limit_meta;\n\t\tbctl->sys.limit = limit_sys;\n\t}\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tif ((!counting && atomic_read(&fs_info->balance_pause_req)) ||\n\t\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto error;\n\t\t}\n\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * this shouldn't happen, it means the last relocate\n\t\t * failed\n\t\t */\n\t\tif (ret == 0)\n\t\t\tBUG(); /* FIXME break ? */\n\n\t\tret = btrfs_previous_item(chunk_root, path, 0,\n\t\t\t\t\t  BTRFS_CHUNK_ITEM_KEY);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t\tif (!counting) {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.considered++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\n\n\t\tret = should_balance_chunk(leaf, chunk, found_key.offset);\n\n\t\tbtrfs_release_path(path);\n\t\tif (!ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (counting) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.expected++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\t\t\tcount_data++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\t\tcount_sys++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\t\tcount_meta++;\n\n\t\t\tgoto loop;\n\t\t}\n\n\t\t/*\n\t\t * Apply limit_min filter, no need to check if the LIMITS\n\t\t * filter is used, limit_min is 0 by default\n\t\t */\n\t\tif (((chunk_type & BTRFS_BLOCK_GROUP_DATA) &&\n\t\t\t\t\tcount_data < bctl->data.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t\t\t\tcount_meta < bctl->meta.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t\t\t\t\tcount_sys < bctl->sys.limit_min)) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (!chunk_reserved) {\n\t\t\t/*\n\t\t\t * We may be relocating the only data chunk we have,\n\t\t\t * which could potentially end up with losing data's\n\t\t\t * raid profile, so lets allocate an empty one in\n\t\t\t * advance.\n\t\t\t */\n\t\t\tret = btrfs_may_alloc_data_chunk(fs_info,\n\t\t\t\t\t\t\t found_key.offset);\n\t\t\tif (ret < 0) {\n\t\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\t\tgoto error;\n\t\t\t} else if (ret == 1) {\n\t\t\t\tchunk_reserved = 1;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret == -ENOSPC) {\n\t\t\tenospc_errors++;\n\t\t} else if (ret == -ETXTBSY) {\n\t\t\tbtrfs_info(fs_info,\n\t   \"skipping relocation of block group %llu due to active swapfile\",\n\t\t\t\t   found_key.offset);\n\t\t\tret = 0;\n\t\t} else if (ret) {\n\t\t\tgoto error;\n\t\t} else {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.completed++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\nloop:\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\n\tif (counting) {\n\t\tbtrfs_release_path(path);\n\t\tcounting = false;\n\t\tgoto again;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\tif (enospc_errors) {\n\t\tbtrfs_info(fs_info, \"%d enospc errors during balance\",\n\t\t\t   enospc_errors);\n\t\tif (!ret)\n\t\t\tret = -ENOSPC;\n\t}\n\n\treturn ret;\n}\n\n/**\n * alloc_profile_is_valid - see if a given profile is valid and reduced\n * @flags: profile to validate\n * @extended: if true @flags is treated as an extended profile\n */\nstatic int alloc_profile_is_valid(u64 flags, int extended)\n{\n\tu64 mask = (extended ? BTRFS_EXTENDED_PROFILE_MASK :\n\t\t\t       BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\n\tflags &= ~BTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\t/* 1) check that all other bits are zeroed */\n\tif (flags & ~mask)\n\t\treturn 0;\n\n\t/* 2) see if profile is reduced */\n\tif (flags == 0)\n\t\treturn !extended; /* \"0\" is valid for usual profiles */\n\n\treturn has_single_bit_set(flags);\n}\n\nstatic inline int balance_need_close(struct btrfs_fs_info *fs_info)\n{\n\t/* cancel requested || normal exit path */\n\treturn atomic_read(&fs_info->balance_cancel_req) ||\n\t\t(atomic_read(&fs_info->balance_pause_req) == 0 &&\n\t\t atomic_read(&fs_info->balance_cancel_req) == 0);\n}\n\n/*\n * Validate target profile against allowed profiles and return true if it's OK.\n * Otherwise print the error message and return false.\n */\nstatic inline int validate_convert_profile(struct btrfs_fs_info *fs_info,\n\t\tconst struct btrfs_balance_args *bargs,\n\t\tu64 allowed, const char *type)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn true;\n\n\tif (fs_info->sectorsize < PAGE_SIZE &&\n\t\tbargs->target & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tbtrfs_err(fs_info,\n\t\t\"RAID56 is not yet supported for sectorsize %u with page size %lu\",\n\t\t\t  fs_info->sectorsize, PAGE_SIZE);\n\t\treturn false;\n\t}\n\t/* Profile is valid and does not have bits outside of the allowed set */\n\tif (alloc_profile_is_valid(bargs->target, 1) &&\n\t    (bargs->target & ~allowed) == 0)\n\t\treturn true;\n\n\tbtrfs_err(fs_info, \"balance: invalid convert %s profile %s\",\n\t\t\ttype, btrfs_bg_type_to_raid_name(bargs->target));\n\treturn false;\n}\n\n/*\n * Fill @buf with textual description of balance filter flags @bargs, up to\n * @size_buf including the terminating null. The output may be trimmed if it\n * does not fit into the provided buffer.\n */\nstatic void describe_balance_args(struct btrfs_balance_args *bargs, char *buf,\n\t\t\t\t u32 size_buf)\n{\n\tint ret;\n\tu32 size_bp = size_buf;\n\tchar *bp = buf;\n\tu64 flags = bargs->flags;\n\tchar tmp_buf[128] = {'\\0'};\n\n\tif (!flags)\n\t\treturn;\n\n#define CHECK_APPEND_NOARG(a)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_2ARG(a, v1, v2)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1), (v2));\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tCHECK_APPEND_1ARG(\"convert=%s,\",\n\t\t\t\t  btrfs_bg_type_to_raid_name(bargs->target));\n\n\tif (flags & BTRFS_BALANCE_ARGS_SOFT)\n\t\tCHECK_APPEND_NOARG(\"soft,\");\n\n\tif (flags & BTRFS_BALANCE_ARGS_PROFILES) {\n\t\tbtrfs_describe_block_groups(bargs->profiles, tmp_buf,\n\t\t\t\t\t    sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"profiles=%s,\", tmp_buf);\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE)\n\t\tCHECK_APPEND_1ARG(\"usage=%llu,\", bargs->usage);\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE_RANGE)\n\t\tCHECK_APPEND_2ARG(\"usage=%u..%u,\",\n\t\t\t\t  bargs->usage_min, bargs->usage_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DEVID)\n\t\tCHECK_APPEND_1ARG(\"devid=%llu,\", bargs->devid);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DRANGE)\n\t\tCHECK_APPEND_2ARG(\"drange=%llu..%llu,\",\n\t\t\t\t  bargs->pstart, bargs->pend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_VRANGE)\n\t\tCHECK_APPEND_2ARG(\"vrange=%llu..%llu,\",\n\t\t\t\t  bargs->vstart, bargs->vend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT)\n\t\tCHECK_APPEND_1ARG(\"limit=%llu,\", bargs->limit);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)\n\t\tCHECK_APPEND_2ARG(\"limit=%u..%u,\",\n\t\t\t\tbargs->limit_min, bargs->limit_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE)\n\t\tCHECK_APPEND_2ARG(\"stripes=%u..%u,\",\n\t\t\t\t  bargs->stripes_min, bargs->stripes_max);\n\n#undef CHECK_APPEND_2ARG\n#undef CHECK_APPEND_1ARG\n#undef CHECK_APPEND_NOARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last , */\n\telse\n\t\tbuf[0] = '\\0';\n}\n\nstatic void describe_balance_start_or_resume(struct btrfs_fs_info *fs_info)\n{\n\tu32 size_buf = 1024;\n\tchar tmp_buf[192] = {'\\0'};\n\tchar *buf;\n\tchar *bp;\n\tu32 size_bp = size_buf;\n\tint ret;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbuf = kzalloc(size_buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tbp = buf;\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (bctl->flags & BTRFS_BALANCE_FORCE)\n\t\tCHECK_APPEND_1ARG(\"%s\", \"-f \");\n\n\tif (bctl->flags & BTRFS_BALANCE_DATA) {\n\t\tdescribe_balance_args(&bctl->data, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-d%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_METADATA) {\n\t\tdescribe_balance_args(&bctl->meta, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-m%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_SYSTEM) {\n\t\tdescribe_balance_args(&bctl->sys, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-s%s \", tmp_buf);\n\t}\n\n#undef CHECK_APPEND_1ARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last \" \" */\n\tbtrfs_info(fs_info, \"balance: %s %s\",\n\t\t   (bctl->flags & BTRFS_BALANCE_RESUME) ?\n\t\t   \"resume\" : \"start\", buf);\n\n\tkfree(buf);\n}\n\n/*\n * Should be called with balance mutexe held\n */\nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs)\n{\n\tu64 meta_target, data_target;\n\tu64 allowed;\n\tint mixed = 0;\n\tint ret;\n\tu64 num_devices;\n\tunsigned seq;\n\tbool reducing_redundancy;\n\tint i;\n\n\tif (btrfs_fs_closing(fs_info) ||\n\t    atomic_read(&fs_info->balance_pause_req) ||\n\t    btrfs_should_cancel_balance(fs_info)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tallowed = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (allowed & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = 1;\n\n\t/*\n\t * In case of mixed groups both data and meta should be picked,\n\t * and identical options should be given for both of them.\n\t */\n\tallowed = BTRFS_BALANCE_DATA | BTRFS_BALANCE_METADATA;\n\tif (mixed && (bctl->flags & allowed)) {\n\t\tif (!(bctl->flags & BTRFS_BALANCE_DATA) ||\n\t\t    !(bctl->flags & BTRFS_BALANCE_METADATA) ||\n\t\t    memcmp(&bctl->data, &bctl->meta, sizeof(bctl->data))) {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: mixed groups data and metadata options must be the same\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * rw_devices will not change at the moment, device add/delete/replace\n\t * are exclusive\n\t */\n\tnum_devices = fs_info->fs_devices->rw_devices;\n\n\t/*\n\t * SINGLE profile on-disk has no profile bit, but in-memory we have a\n\t * special bit for it, to make it easier to distinguish.  Thus we need\n\t * to set it manually, or balance would refuse the profile.\n\t */\n\tallowed = BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n\tfor (i = 0; i < ARRAY_SIZE(btrfs_raid_array); i++)\n\t\tif (num_devices >= btrfs_raid_array[i].devs_min)\n\t\t\tallowed |= btrfs_raid_array[i].bg_flag;\n\n\tif (!validate_convert_profile(fs_info, &bctl->data, allowed, \"data\") ||\n\t    !validate_convert_profile(fs_info, &bctl->meta, allowed, \"metadata\") ||\n\t    !validate_convert_profile(fs_info, &bctl->sys,  allowed, \"system\")) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Allow to reduce metadata or system integrity only if force set for\n\t * profiles with redundancy (copies, parity)\n\t */\n\tallowed = 0;\n\tfor (i = 0; i < ARRAY_SIZE(btrfs_raid_array); i++) {\n\t\tif (btrfs_raid_array[i].ncopies >= 2 ||\n\t\t    btrfs_raid_array[i].tolerated_failures >= 1)\n\t\t\tallowed |= btrfs_raid_array[i].bg_flag;\n\t}\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tif (((bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_system_alloc_bits & allowed) &&\n\t\t     !(bctl->sys.target & allowed)) ||\n\t\t    ((bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_metadata_alloc_bits & allowed) &&\n\t\t     !(bctl->meta.target & allowed)))\n\t\t\treducing_redundancy = true;\n\t\telse\n\t\t\treducing_redundancy = false;\n\n\t\t/* if we're not converting, the target field is uninitialized */\n\t\tmeta_target = (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->meta.target : fs_info->avail_metadata_alloc_bits;\n\t\tdata_target = (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->data.target : fs_info->avail_data_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tif (reducing_redundancy) {\n\t\tif (bctl->flags & BTRFS_BALANCE_FORCE) {\n\t\t\tbtrfs_info(fs_info,\n\t\t\t   \"balance: force reducing metadata redundancy\");\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t\"balance: reduces metadata redundancy, use --force if you want this\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (btrfs_get_num_tolerated_disk_barrier_failures(meta_target) <\n\t\tbtrfs_get_num_tolerated_disk_barrier_failures(data_target)) {\n\t\tbtrfs_warn(fs_info,\n\t\"balance: metadata profile %s has lower redundancy than data profile %s\",\n\t\t\t\tbtrfs_bg_type_to_raid_name(meta_target),\n\t\t\t\tbtrfs_bg_type_to_raid_name(data_target));\n\t}\n\n\tret = insert_balance_item(fs_info, bctl);\n\tif (ret && ret != -EEXIST)\n\t\tgoto out;\n\n\tif (!(bctl->flags & BTRFS_BALANCE_RESUME)) {\n\t\tBUG_ON(ret == -EEXIST);\n\t\tBUG_ON(fs_info->balance_ctl);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tfs_info->balance_ctl = bctl;\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tBUG_ON(ret != -EEXIST);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tupdate_balance_args(bctl);\n\t\tspin_unlock(&fs_info->balance_lock);\n\t}\n\n\tASSERT(!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tset_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\tdescribe_balance_start_or_resume(fs_info);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tret = __btrfs_balance(fs_info);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (ret == -ECANCELED && atomic_read(&fs_info->balance_pause_req))\n\t\tbtrfs_info(fs_info, \"balance: paused\");\n\t/*\n\t * Balance can be canceled by:\n\t *\n\t * - Regular cancel request\n\t *   Then ret == -ECANCELED and balance_cancel_req > 0\n\t *\n\t * - Fatal signal to \"btrfs\" process\n\t *   Either the signal caught by wait_reserve_ticket() and callers\n\t *   got -EINTR, or caught by btrfs_should_cancel_balance() and\n\t *   got -ECANCELED.\n\t *   Either way, in this case balance_cancel_req = 0, and\n\t *   ret == -EINTR or ret == -ECANCELED.\n\t *\n\t * So here we only check the return value to catch canceled balance.\n\t */\n\telse if (ret == -ECANCELED || ret == -EINTR)\n\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\telse\n\t\tbtrfs_info(fs_info, \"balance: ended with status: %d\", ret);\n\n\tclear_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\n\tif (bargs) {\n\t\tmemset(bargs, 0, sizeof(*bargs));\n\t\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\t}\n\n\tif ((ret && ret != -ECANCELED && ret != -ENOSPC) ||\n\t    balance_need_close(fs_info)) {\n\t\treset_balance_state(fs_info);\n\t\tbtrfs_exclop_finish(fs_info);\n\t}\n\n\twake_up(&fs_info->balance_wait_q);\n\n\treturn ret;\nout:\n\tif (bctl->flags & BTRFS_BALANCE_RESUME)\n\t\treset_balance_state(fs_info);\n\telse\n\t\tkfree(bctl);\n\tbtrfs_exclop_finish(fs_info);\n\n\treturn ret;\n}\n\nstatic int balance_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl)\n\t\tret = btrfs_balance(fs_info, fs_info->balance_ctl, NULL);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\treturn ret;\n}\n\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *tsk;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn 0;\n\t}\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tif (btrfs_test_opt(fs_info, SKIP_BALANCE)) {\n\t\tbtrfs_info(fs_info, \"balance: resume skipped\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * A ro->rw remount sequence should continue with the paused balance\n\t * regardless of who pauses it, system or the user as of now, so set\n\t * the resume flag.\n\t */\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl->flags |= BTRFS_BALANCE_RESUME;\n\tspin_unlock(&fs_info->balance_lock);\n\n\ttsk = kthread_run(balance_kthread, fs_info, \"btrfs-balance\");\n\treturn PTR_ERR_OR_ZERO(tsk);\n}\n\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, fs_info->tree_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) { /* ret = -ENOENT; */\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tbctl->flags = btrfs_balance_flags(leaf, item);\n\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\n\tbtrfs_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->data, &disk_bargs);\n\tbtrfs_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->meta, &disk_bargs);\n\tbtrfs_balance_sys(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->sys, &disk_bargs);\n\n\t/*\n\t * This should never happen, as the paused balance state is recovered\n\t * during mount without any chance of other exclusive ops to collide.\n\t *\n\t * This gives the exclusive op status to balance and keeps in paused\n\t * state until user intervention (cancel or umount). If the ownership\n\t * cannot be assigned, show a message but do not fail. The balance\n\t * is in a paused state and must have fs_info::balance_ctl properly\n\t * set up.\n\t */\n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE))\n\t\tbtrfs_warn(fs_info,\n\t\"balance: cannot set exclusive op status, resume manually\");\n\n\tbtrfs_release_path(path);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tBUG_ON(fs_info->balance_ctl);\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = bctl;\n\tspin_unlock(&fs_info->balance_lock);\n\tmutex_unlock(&fs_info->balance_mutex);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info)\n{\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tatomic_inc(&fs_info->balance_pause_req);\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\t/* we are good with balance_ctl ripped off from under us */\n\t\tBUG_ON(test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tatomic_dec(&fs_info->balance_pause_req);\n\t} else {\n\t\tret = -ENOTCONN;\n\t}\n\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\t/*\n\t * A paused balance with the item stored on disk can be resumed at\n\t * mount time if the mount is read-write. Otherwise it's still paused\n\t * and we must not allow cancelling as it deletes the item.\n\t */\n\tif (sb_rdonly(fs_info->sb)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_inc(&fs_info->balance_cancel_req);\n\t/*\n\t * if we are running just wait and return, balance item is\n\t * deleted in btrfs_balance in this case\n\t */\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t} else {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t/*\n\t\t * Lock released to allow other waiters to continue, we'll\n\t\t * reexamine the status again.\n\t\t */\n\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\tif (fs_info->balance_ctl) {\n\t\t\treset_balance_state(fs_info);\n\t\t\tbtrfs_exclop_finish(fs_info);\n\t\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\t\t}\n\t}\n\n\tBUG_ON(fs_info->balance_ctl ||\n\t\ttest_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tatomic_dec(&fs_info->balance_cancel_req);\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn 0;\n}\n\nint btrfs_uuid_scan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tstruct btrfs_root_item root_item;\n\tu32 item_size;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tbool closing = false;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\tif (btrfs_fs_closing(fs_info)) {\n\t\t\tclosing = true;\n\t\t\tbreak;\n\t\t}\n\t\tret = btrfs_search_forward(root, &key, path,\n\t\t\t\tBTRFS_OLDEST_GENERATION);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (key.type != BTRFS_ROOT_ITEM_KEY ||\n\t\t    (key.objectid < BTRFS_FIRST_FREE_OBJECTID &&\n\t\t     key.objectid != BTRFS_FS_TREE_OBJECTID) ||\n\t\t    key.objectid > BTRFS_LAST_FREE_OBJECTID)\n\t\t\tgoto skip;\n\n\t\teb = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\titem_size = btrfs_item_size_nr(eb, slot);\n\t\tif (item_size < sizeof(root_item))\n\t\t\tgoto skip;\n\n\t\tread_extent_buffer(eb, &root_item,\n\t\t\t\t   btrfs_item_ptr_offset(eb, slot),\n\t\t\t\t   (int)sizeof(root_item));\n\t\tif (btrfs_root_refs(&root_item) == 0)\n\t\t\tgoto skip;\n\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid) ||\n\t\t    !btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tif (trans)\n\t\t\t\tgoto update_tree;\n\n\t\t\tbtrfs_release_path(path);\n\t\t\t/*\n\t\t\t * 1 - subvol uuid item\n\t\t\t * 1 - received_subvol uuid item\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(fs_info->uuid_root, 2);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tgoto skip;\n\t\t}\nupdate_tree:\n\t\tbtrfs_release_path(path);\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans, root_item.uuid,\n\t\t\t\t\t\t  BTRFS_UUID_KEY_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans,\n\t\t\t\t\t\t  root_item.received_uuid,\n\t\t\t\t\t\t BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nskip:\n\t\tbtrfs_release_path(path);\n\t\tif (trans) {\n\t\t\tret = btrfs_end_transaction(trans);\n\t\t\ttrans = NULL;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (key.offset < (u64)-1) {\n\t\t\tkey.offset++;\n\t\t} else if (key.type < BTRFS_ROOT_ITEM_KEY) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t} else if (key.objectid < (u64)-1) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t\tkey.objectid++;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (trans && !IS_ERR(trans))\n\t\tbtrfs_end_transaction(trans);\n\tif (ret)\n\t\tbtrfs_warn(fs_info, \"btrfs_uuid_scan_kthread failed %d\", ret);\n\telse if (!closing)\n\t\tset_bit(BTRFS_FS_UPDATE_UUID_TREE_GEN, &fs_info->flags);\n\tup(&fs_info->uuid_tree_rescan_sem);\n\treturn 0;\n}\n\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *uuid_root;\n\tstruct task_struct *task;\n\tint ret;\n\n\t/*\n\t * 1 - root node\n\t * 1 - root item\n\t */\n\ttrans = btrfs_start_transaction(tree_root, 2);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tuuid_root = btrfs_create_tree(trans, BTRFS_UUID_TREE_OBJECTID);\n\tif (IS_ERR(uuid_root)) {\n\t\tret = PTR_ERR(uuid_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\n\n\tfs_info->uuid_root = uuid_root;\n\n\tret = btrfs_commit_transaction(trans);\n\tif (ret)\n\t\treturn ret;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_scan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t/* fs_info->update_uuid_tree_gen remains 0 in all error case */\n\t\tbtrfs_warn(fs_info, \"failed to start uuid_scan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\n/*\n * shrinking a device means finding all of the device extents past\n * the new size, and then following the back refs to the chunks.\n * The chunk relocation code actually frees the device extent\n */\nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret;\n\tint slot;\n\tint failed = 0;\n\tbool retried = false;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total = btrfs_super_total_bytes(super_copy);\n\tu64 old_size = btrfs_device_get_total_bytes(device);\n\tu64 diff;\n\tu64 start;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\tstart = new_size;\n\tdiff = round_down(old_size - new_size, fs_info->sectorsize);\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\treturn -EINVAL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_BACK;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes -= diff;\n\t\tatomic64_sub(diff, &fs_info->free_chunk_space);\n\t}\n\n\t/*\n\t * Once the device's size has been set to the new size, ensure all\n\t * in-memory chunks are synced to disk so that the loop below sees them\n\t * and relocates them accordingly.\n\t */\n\tif (contains_pending_extent(device, &start, diff)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tret = btrfs_commit_transaction(trans);\n\t\tif (ret)\n\t\t\tgoto done;\n\t} else {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nagain:\n\tkey.objectid = device->devid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tdo {\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_previous_item(root, path, 0, key.type);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto done;\n\t\t\tret = 0;\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, path->slots[0]);\n\n\t\tif (key.objectid != device->devid) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (key.offset + length <= new_size) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * We may be relocating the only data chunk we have,\n\t\t * which could potentially end up with losing data's\n\t\t * raid profile, so lets allocate an empty one in\n\t\t * advance.\n\t\t */\n\t\tret = btrfs_may_alloc_data_chunk(fs_info, chunk_offset);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, chunk_offset);\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret == -ENOSPC) {\n\t\t\tfailed++;\n\t\t} else if (ret) {\n\t\t\tif (ret == -ETXTBSY) {\n\t\t\t\tbtrfs_warn(fs_info,\n\t\t   \"could not shrink block group %llu due to active swapfile\",\n\t\t\t\t\t   chunk_offset);\n\t\t\t}\n\t\t\tgoto done;\n\t\t}\n\t} while (key.offset-- > 0);\n\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (failed && retried) {\n\t\tret = -ENOSPC;\n\t\tgoto done;\n\t}\n\n\t/* Shrinking succeeded, else we would be at \"done\". */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto done;\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\t/* Clear all state bits beyond the shrunk device size */\n\tclear_extent_bits(&device->alloc_state, new_size, (u64)-1,\n\t\t\t  CHUNK_STATE_MASK);\n\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tif (list_empty(&device->post_commit_list))\n\t\tlist_add_tail(&device->post_commit_list,\n\t\t\t      &trans->transaction->dev_update_list);\n\n\tWARN_ON(diff > old_total);\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total - diff, fs_info->sectorsize));\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/* Now btrfs_update_device() will change the on-disk size. */\n\tret = btrfs_update_device(trans, device);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\ndone:\n\tbtrfs_free_path(path);\n\tif (ret) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tbtrfs_device_set_total_bytes(device, old_size);\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\t\tdevice->fs_devices->total_rw_bytes += diff;\n\t\tatomic64_add(diff, &fs_info->free_chunk_space);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_system_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_key *key,\n\t\t\t   struct btrfs_chunk *chunk, int item_size)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key disk_key;\n\tu32 array_size;\n\tu8 *ptr;\n\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\tif (array_size + item_size + sizeof(disk_key)\n\t\t\t> BTRFS_SYSTEM_CHUNK_ARRAY_SIZE)\n\t\treturn -EFBIG;\n\n\tptr = super_copy->sys_chunk_array + array_size;\n\tbtrfs_cpu_key_to_disk(&disk_key, key);\n\tmemcpy(ptr, &disk_key, sizeof(disk_key));\n\tptr += sizeof(disk_key);\n\tmemcpy(ptr, chunk, item_size);\n\titem_size += sizeof(disk_key);\n\tbtrfs_set_super_sys_array_size(super_copy, array_size + item_size);\n\n\treturn 0;\n}\n\n/*\n * sort the devices in descending order by max_avail, total_avail\n */\nstatic int btrfs_cmp_device_info(const void *a, const void *b)\n{\n\tconst struct btrfs_device_info *di_a = a;\n\tconst struct btrfs_device_info *di_b = b;\n\n\tif (di_a->max_avail > di_b->max_avail)\n\t\treturn -1;\n\tif (di_a->max_avail < di_b->max_avail)\n\t\treturn 1;\n\tif (di_a->total_avail > di_b->total_avail)\n\t\treturn -1;\n\tif (di_a->total_avail < di_b->total_avail)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void check_raid56_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID56);\n}\n\nstatic void check_raid1c34_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & (BTRFS_BLOCK_GROUP_RAID1C3 | BTRFS_BLOCK_GROUP_RAID1C4)))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID1C34);\n}\n\n/*\n * Structure used internally for __btrfs_alloc_chunk() function.\n * Wraps needed parameters.\n */\nstruct alloc_chunk_ctl {\n\tu64 start;\n\tu64 type;\n\t/* Total number of stripes to allocate */\n\tint num_stripes;\n\t/* sub_stripes info for map */\n\tint sub_stripes;\n\t/* Stripes per device */\n\tint dev_stripes;\n\t/* Maximum number of devices to use */\n\tint devs_max;\n\t/* Minimum number of devices to use */\n\tint devs_min;\n\t/* ndevs has to be a multiple of this */\n\tint devs_increment;\n\t/* Number of copies */\n\tint ncopies;\n\t/* Number of stripes worth of bytes to store parity information */\n\tint nparity;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 dev_extent_min;\n\tu64 stripe_size;\n\tu64 chunk_size;\n\tint ndevs;\n};\n\nstatic void init_alloc_chunk_ctl_policy_regular(\n\t\t\t\tstruct btrfs_fs_devices *fs_devices,\n\t\t\t\tstruct alloc_chunk_ctl *ctl)\n{\n\tu64 type = ctl->type;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tctl->max_stripe_size = SZ_1G;\n\t\tctl->max_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* For larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tctl->max_stripe_size = SZ_1G;\n\t\telse\n\t\t\tctl->max_stripe_size = SZ_256M;\n\t\tctl->max_chunk_size = ctl->max_stripe_size;\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tctl->max_stripe_size = SZ_32M;\n\t\tctl->max_chunk_size = 2 * ctl->max_stripe_size;\n\t\tctl->devs_max = min_t(int, ctl->devs_max,\n\t\t\t\t      BTRFS_MAX_DEVS_SYS_CHUNK);\n\t} else {\n\t\tBUG();\n\t}\n\n\t/* We don't want a chunk larger than 10% of writable space */\n\tctl->max_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t\t  ctl->max_chunk_size);\n\tctl->dev_extent_min = BTRFS_STRIPE_LEN * ctl->dev_stripes;\n}\n\nstatic void init_alloc_chunk_ctl_policy_zoned(\n\t\t\t\t      struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      struct alloc_chunk_ctl *ctl)\n{\n\tu64 zone_size = fs_devices->fs_info->zone_size;\n\tu64 limit;\n\tint min_num_stripes = ctl->devs_min * ctl->dev_stripes;\n\tint min_data_stripes = (min_num_stripes - ctl->nparity) / ctl->ncopies;\n\tu64 min_chunk_size = min_data_stripes * zone_size;\n\tu64 type = ctl->type;\n\n\tctl->max_stripe_size = zone_size;\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tctl->max_chunk_size = round_down(BTRFS_MAX_DATA_CHUNK_SIZE,\n\t\t\t\t\t\t zone_size);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\tctl->max_chunk_size = ctl->max_stripe_size;\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tctl->max_chunk_size = 2 * ctl->max_stripe_size;\n\t\tctl->devs_max = min_t(int, ctl->devs_max,\n\t\t\t\t      BTRFS_MAX_DEVS_SYS_CHUNK);\n\t} else {\n\t\tBUG();\n\t}\n\n\t/* We don't want a chunk larger than 10% of writable space */\n\tlimit = max(round_down(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t       zone_size),\n\t\t    min_chunk_size);\n\tctl->max_chunk_size = min(limit, ctl->max_chunk_size);\n\tctl->dev_extent_min = zone_size * ctl->dev_stripes;\n}\n\nstatic void init_alloc_chunk_ctl(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t struct alloc_chunk_ctl *ctl)\n{\n\tint index = btrfs_bg_flags_to_raid_index(ctl->type);\n\n\tctl->sub_stripes = btrfs_raid_array[index].sub_stripes;\n\tctl->dev_stripes = btrfs_raid_array[index].dev_stripes;\n\tctl->devs_max = btrfs_raid_array[index].devs_max;\n\tif (!ctl->devs_max)\n\t\tctl->devs_max = BTRFS_MAX_DEVS(fs_devices->fs_info);\n\tctl->devs_min = btrfs_raid_array[index].devs_min;\n\tctl->devs_increment = btrfs_raid_array[index].devs_increment;\n\tctl->ncopies = btrfs_raid_array[index].ncopies;\n\tctl->nparity = btrfs_raid_array[index].nparity;\n\tctl->ndevs = 0;\n\n\tswitch (fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\tinit_alloc_chunk_ctl_policy_regular(fs_devices, ctl);\n\t\tbreak;\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\tinit_alloc_chunk_ctl_policy_zoned(fs_devices, ctl);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int gather_device_info(struct btrfs_fs_devices *fs_devices,\n\t\t\t      struct alloc_chunk_ctl *ctl,\n\t\t\t      struct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = fs_devices->fs_info;\n\tstruct btrfs_device *device;\n\tu64 total_avail;\n\tu64 dev_extent_want = ctl->max_stripe_size * ctl->dev_stripes;\n\tint ret;\n\tint ndevs = 0;\n\tu64 max_avail;\n\tu64 dev_offset;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail < ctl->dev_extent_min)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(device, dev_extent_want, &dev_offset,\n\t\t\t\t\t   &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\treturn ret;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = dev_extent_want;\n\n\t\tif (max_avail < ctl->dev_extent_min) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%llu\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    ctl->dev_extent_min);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\tctl->ndevs = ndevs;\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size_regular(struct alloc_chunk_ctl *ctl,\n\t\t\t\t      struct btrfs_device_info *devices_info)\n{\n\t/* Number of stripes that count for block group size */\n\tint data_stripes;\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tctl->stripe_size = div_u64(devices_info[ctl->ndevs - 1].max_avail,\n\t\t\t\t   ctl->dev_stripes);\n\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\n\t/* This will have to be fixed for RAID1 and RAID10 over more drives */\n\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk is\n\t * really going to be in terms of logical address space, and compare\n\t * that answer with the max chunk size. If it's higher, we try to\n\t * reduce stripe_size.\n\t */\n\tif (ctl->stripe_size * data_stripes > ctl->max_chunk_size) {\n\t\t/*\n\t\t * Reduce stripe_size, round it up to a 16MB boundary again and\n\t\t * then use it, unless it ends up being even bigger than the\n\t\t * previous value we had already.\n\t\t */\n\t\tctl->stripe_size = min(round_up(div_u64(ctl->max_chunk_size,\n\t\t\t\t\t\t\tdata_stripes), SZ_16M),\n\t\t\t\t       ctl->stripe_size);\n\t}\n\n\t/* Align to BTRFS_STRIPE_LEN */\n\tctl->stripe_size = round_down(ctl->stripe_size, BTRFS_STRIPE_LEN);\n\tctl->chunk_size = ctl->stripe_size * data_stripes;\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size_zoned(struct alloc_chunk_ctl *ctl,\n\t\t\t\t    struct btrfs_device_info *devices_info)\n{\n\tu64 zone_size = devices_info[0].dev->zone_info->zone_size;\n\t/* Number of stripes that count for block group size */\n\tint data_stripes;\n\n\t/*\n\t * It should hold because:\n\t *    dev_extent_min == dev_extent_want == zone_size * dev_stripes\n\t */\n\tASSERT(devices_info[ctl->ndevs - 1].max_avail == ctl->dev_extent_min);\n\n\tctl->stripe_size = zone_size;\n\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\n\t/* stripe_size is fixed in zoned filesysmte. Reduce ndevs instead. */\n\tif (ctl->stripe_size * data_stripes > ctl->max_chunk_size) {\n\t\tctl->ndevs = div_u64(div_u64(ctl->max_chunk_size * ctl->ncopies,\n\t\t\t\t\t     ctl->stripe_size) + ctl->nparity,\n\t\t\t\t     ctl->dev_stripes);\n\t\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\t\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\t\tASSERT(ctl->stripe_size * data_stripes <= ctl->max_chunk_size);\n\t}\n\n\tctl->chunk_size = ctl->stripe_size * data_stripes;\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size(struct btrfs_fs_devices *fs_devices,\n\t\t\t      struct alloc_chunk_ctl *ctl,\n\t\t\t      struct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = fs_devices->fs_info;\n\n\t/*\n\t * Round down to number of usable stripes, devs_increment can be any\n\t * number so we can't use round_down() that requires power of 2, while\n\t * rounddown is safe.\n\t */\n\tctl->ndevs = rounddown(ctl->ndevs, ctl->devs_increment);\n\n\tif (ctl->ndevs < ctl->devs_min) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ctl->ndevs, ctl->devs_min);\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\n\tctl->ndevs = min(ctl->ndevs, ctl->devs_max);\n\n\tswitch (fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\treturn decide_stripe_size_regular(ctl, devices_info);\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\treturn decide_stripe_size_zoned(ctl, devices_info);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic struct btrfs_block_group *create_chunk(struct btrfs_trans_handle *trans,\n\t\t\tstruct alloc_chunk_ctl *ctl,\n\t\t\tstruct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct btrfs_block_group *block_group;\n\tstruct extent_map *em;\n\tu64 start = ctl->start;\n\tu64 type = ctl->type;\n\tint ret;\n\tint i;\n\tint j;\n\n\tmap = kmalloc(map_lookup_size(ctl->num_stripes), GFP_NOFS);\n\tif (!map)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmap->num_stripes = ctl->num_stripes;\n\n\tfor (i = 0; i < ctl->ndevs; ++i) {\n\t\tfor (j = 0; j < ctl->dev_stripes; ++j) {\n\t\t\tint s = i * ctl->dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * ctl->stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = ctl->sub_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, ctl->chunk_size);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = ctl->chunk_size;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = ctl->stripe_size;\n\n\tem_tree = &info->mapping_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(ret);\n\t}\n\twrite_unlock(&em_tree->lock);\n\n\tblock_group = btrfs_make_block_group(trans, 0, type, start, ctl->chunk_size);\n\tif (IS_ERR(block_group))\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\tbtrfs_device_set_bytes_used(dev,\n\t\t\t\t\t    dev->bytes_used + ctl->stripe_size);\n\t\tif (list_empty(&dev->post_commit_list))\n\t\t\tlist_add_tail(&dev->post_commit_list,\n\t\t\t\t      &trans->transaction->dev_update_list);\n\t}\n\n\tatomic64_sub(ctl->stripe_size * map->num_stripes,\n\t\t     &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\tcheck_raid1c34_incompat_flag(info, type);\n\n\treturn block_group;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\n\treturn block_group;\n}\n\nstruct btrfs_block_group *btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t\t\t    u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tstruct alloc_chunk_ctl ctl;\n\tstruct btrfs_block_group *block_group;\n\tint ret;\n\n\tlockdep_assert_held(&info->chunk_mutex);\n\n\tif (!alloc_profile_is_valid(type, 0)) {\n\t\tASSERT(0);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\tif (!(type & BTRFS_BLOCK_GROUP_TYPE_MASK)) {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\", type);\n\t\tASSERT(0);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctl.start = find_next_chunk(info);\n\tctl.type = type;\n\tinit_alloc_chunk_ctl(fs_devices, &ctl);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = gather_device_info(fs_devices, &ctl, devices_info);\n\tif (ret < 0) {\n\t\tblock_group = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tret = decide_stripe_size(fs_devices, &ctl, devices_info);\n\tif (ret < 0) {\n\t\tblock_group = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tblock_group = create_chunk(trans, &ctl, devices_info);\n\nout:\n\tkfree(devices_info);\n\treturn block_group;\n}\n\n/*\n * This function, btrfs_chunk_alloc_add_chunk_item(), typically belongs to the\n * phase 1 of chunk allocation. It belongs to phase 2 only when allocating system\n * chunks.\n *\n * See the comment at btrfs_chunk_alloc() for details about the chunk allocation\n * phases.\n */\nint btrfs_chunk_alloc_add_chunk_item(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *extent_root = fs_info->extent_root;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_stripe *stripe;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tsize_t item_size;\n\tint i;\n\tint ret;\n\n\t/*\n\t * We take the chunk_mutex for 2 reasons:\n\t *\n\t * 1) Updates and insertions in the chunk btree must be done while holding\n\t *    the chunk_mutex, as well as updating the system chunk array in the\n\t *    superblock. See the comment on top of btrfs_chunk_alloc() for the\n\t *    details;\n\t *\n\t * 2) To prevent races with the final phase of a device replace operation\n\t *    that replaces the device object associated with the map's stripes,\n\t *    because the device object's id can change at any time during that\n\t *    final phase of the device replace operation\n\t *    (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the\n\t *    replaced device and then see it with an ID of BTRFS_DEV_REPLACE_DEVID,\n\t *    which would cause a failure when updating the device item, which does\n\t *    not exists, or persisting a stripe of the chunk item with such ID.\n\t *    Here we can't use the device_list_mutex because our caller already\n\t *    has locked the chunk_mutex, and the final phase of device replace\n\t *    acquires both mutexes - first the device_list_mutex and then the\n\t *    chunk_mutex. Using any of those two mutexes protects us from a\n\t *    concurrent device replace.\n\t */\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tem = btrfs_get_chunk_map(fs_info, bg->start, bg->length);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\treturn ret;\n\t}\n\n\tmap = em->map_lookup;\n\titem_size = btrfs_chunk_item_size(map->num_stripes);\n\n\tchunk = kzalloc(item_size, GFP_NOFS);\n\tif (!chunk) {\n\t\tret = -ENOMEM;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tstripe = &chunk->stripe;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tconst u64 dev_offset = map->stripes[i].physical;\n\n\t\tbtrfs_set_stack_stripe_devid(stripe, device->devid);\n\t\tbtrfs_set_stack_stripe_offset(stripe, dev_offset);\n\t\tmemcpy(stripe->dev_uuid, device->uuid, BTRFS_UUID_SIZE);\n\t\tstripe++;\n\t}\n\n\tbtrfs_set_stack_chunk_length(chunk, bg->length);\n\tbtrfs_set_stack_chunk_owner(chunk, extent_root->root_key.objectid);\n\tbtrfs_set_stack_chunk_stripe_len(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_type(chunk, map->type);\n\tbtrfs_set_stack_chunk_num_stripes(chunk, map->num_stripes);\n\tbtrfs_set_stack_chunk_io_align(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_io_width(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_sector_size(chunk, fs_info->sectorsize);\n\tbtrfs_set_stack_chunk_sub_stripes(chunk, map->sub_stripes);\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\tkey.offset = bg->start;\n\n\tret = btrfs_insert_item(trans, chunk_root, &key, chunk, item_size);\n\tif (ret)\n\t\tgoto out;\n\n\tbg->chunk_item_inserted = 1;\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_add_system_chunk(fs_info, &key, chunk, item_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\tkfree(chunk);\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic noinline int init_first_rw_device(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tu64 alloc_profile;\n\tstruct btrfs_block_group *meta_bg;\n\tstruct btrfs_block_group *sys_bg;\n\n\t/*\n\t * When adding a new device for sprouting, the seed device is read-only\n\t * so we must first allocate a metadata and a system chunk. But before\n\t * adding the block group items to the extent, device and chunk btrees,\n\t * we must first:\n\t *\n\t * 1) Create both chunks without doing any changes to the btrees, as\n\t *    otherwise we would get -ENOSPC since the block groups from the\n\t *    seed device are read-only;\n\t *\n\t * 2) Add the device item for the new sprout device - finishing the setup\n\t *    of a new block group requires updating the device item in the chunk\n\t *    btree, so it must exist when we attempt to do it. The previous step\n\t *    ensures this does not fail with -ENOSPC.\n\t *\n\t * After that we can add the block group items to their btrees:\n\t * update existing device item in the chunk btree, add a new block group\n\t * item to the extent btree, add a new chunk item to the chunk btree and\n\t * finally add the new device extent items to the devices btree.\n\t */\n\n\talloc_profile = btrfs_metadata_alloc_profile(fs_info);\n\tmeta_bg = btrfs_alloc_chunk(trans, alloc_profile);\n\tif (IS_ERR(meta_bg))\n\t\treturn PTR_ERR(meta_bg);\n\n\talloc_profile = btrfs_system_alloc_profile(fs_info);\n\tsys_bg = btrfs_alloc_chunk(trans, alloc_profile);\n\tif (IS_ERR(sys_bg))\n\t\treturn PTR_ERR(sys_bg);\n\n\treturn 0;\n}\n\nstatic inline int btrfs_chunk_max_errors(struct map_lookup *map)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(map->type);\n\n\treturn btrfs_raid_array[index].tolerated_failures;\n}\n\nint btrfs_chunk_readonly(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint readonly = 0;\n\tint miss_ndevs = 0;\n\tint i;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em))\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\tmiss_ndevs++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\treadonly = 1;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\t/*\n\t * If the number of missing devices is larger than max errors,\n\t * we can not write the data into that chunk successfully, so\n\t * set it readonly.\n\t */\n\tif (miss_ndevs > btrfs_chunk_max_errors(map))\n\t\treadonly = 1;\nend:\n\tfree_extent_map(em);\n\treturn readonly;\n}\n\nvoid btrfs_mapping_tree_free(struct extent_map_tree *tree)\n{\n\tstruct extent_map *em;\n\n\twhile (1) {\n\t\twrite_lock(&tree->lock);\n\t\tem = lookup_extent_mapping(tree, 0, (u64)-1);\n\t\tif (em)\n\t\t\tremove_extent_mapping(tree, em);\n\t\twrite_unlock(&tree->lock);\n\t\tif (!em)\n\t\t\tbreak;\n\t\t/* once for us */\n\t\tfree_extent_map(em);\n\t\t/* once for the tree */\n\t\tfree_extent_map(em);\n\t}\n}\n\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\tif (IS_ERR(em))\n\t\t/*\n\t\t * We could return errors for these cases, but that could get\n\t\t * ugly and we'd probably do the same thing which is just not do\n\t\t * anything else and exit, so return 1 so the callers don't try\n\t\t * to use other copies.\n\t\t */\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tif (map->type & (BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1_MASK))\n\t\tret = map->num_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tret = map->sub_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tret = 2;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t/*\n\t\t * There could be two corrupted data stripes, we need\n\t\t * to loop retry in order to rebuild the correct data.\n\t\t *\n\t\t * Fail a stripe at a time on every retry except the\n\t\t * stripe under reconstruction.\n\t\t */\n\t\tret = map->num_stripes;\n\telse\n\t\tret = 1;\n\tfree_extent_map(em);\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace) &&\n\t    fs_info->dev_replace.tgtdev)\n\t\tret++;\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn ret;\n}\n\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tunsigned long len = fs_info->sectorsize;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif (!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tlen = map->stripe_len * nr_data_stripes(map);\n\t\tfree_extent_map(em);\n\t}\n\treturn len;\n}\n\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif(!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tret = 1;\n\t\tfree_extent_map(em);\n\t}\n\treturn ret;\n}\n\nstatic int find_live_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t    struct map_lookup *map, int first,\n\t\t\t    int dev_replace_is_ongoing)\n{\n\tint i;\n\tint num_stripes;\n\tint preferred_mirror;\n\tint tolerance;\n\tstruct btrfs_device *srcdev;\n\n\tASSERT((map->type &\n\t\t (BTRFS_BLOCK_GROUP_RAID1_MASK | BTRFS_BLOCK_GROUP_RAID10)));\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tnum_stripes = map->sub_stripes;\n\telse\n\t\tnum_stripes = map->num_stripes;\n\n\tswitch (fs_info->fs_devices->read_policy) {\n\tdefault:\n\t\t/* Shouldn't happen, just warn and use pid instead of failing */\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\t      \"unknown read_policy type %u, reset to pid\",\n\t\t\t      fs_info->fs_devices->read_policy);\n\t\tfs_info->fs_devices->read_policy = BTRFS_READ_POLICY_PID;\n\t\tfallthrough;\n\tcase BTRFS_READ_POLICY_PID:\n\t\tpreferred_mirror = first + (current->pid % num_stripes);\n\t\tbreak;\n\t}\n\n\tif (dev_replace_is_ongoing &&\n\t    fs_info->dev_replace.cont_reading_from_srcdev_mode ==\n\t     BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID)\n\t\tsrcdev = fs_info->dev_replace.srcdev;\n\telse\n\t\tsrcdev = NULL;\n\n\t/*\n\t * try to avoid the drive that is the source drive for a\n\t * dev-replace procedure, only choose it if no other non-missing\n\t * mirror is available\n\t */\n\tfor (tolerance = 0; tolerance < 2; tolerance++) {\n\t\tif (map->stripes[preferred_mirror].dev->bdev &&\n\t\t    (tolerance || map->stripes[preferred_mirror].dev != srcdev))\n\t\t\treturn preferred_mirror;\n\t\tfor (i = first; i < first + num_stripes; i++) {\n\t\t\tif (map->stripes[i].dev->bdev &&\n\t\t\t    (tolerance || map->stripes[i].dev != srcdev))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\t/* we couldn't find one that doesn't fail.  Just return something\n\t * and the io error handling code will clean up eventually\n\t */\n\treturn preferred_mirror;\n}\n\n/* Bubble-sort the stripe set to put the parity/syndrome stripes last */\nstatic void sort_parity_stripes(struct btrfs_bio *bbio, int num_stripes)\n{\n\tint i;\n\tint again = 1;\n\n\twhile (again) {\n\t\tagain = 0;\n\t\tfor (i = 0; i < num_stripes - 1; i++) {\n\t\t\t/* Swap if parity is on a smaller index */\n\t\t\tif (bbio->raid_map[i] > bbio->raid_map[i + 1]) {\n\t\t\t\tswap(bbio->stripes[i], bbio->stripes[i + 1]);\n\t\t\t\tswap(bbio->raid_map[i], bbio->raid_map[i + 1]);\n\t\t\t\tagain = 1;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic struct btrfs_bio *alloc_btrfs_bio(int total_stripes, int real_stripes)\n{\n\tstruct btrfs_bio *bbio = kzalloc(\n\t\t /* the size of the btrfs_bio */\n\t\tsizeof(struct btrfs_bio) +\n\t\t/* plus the variable array for the stripes */\n\t\tsizeof(struct btrfs_bio_stripe) * (total_stripes) +\n\t\t/* plus the variable array for the tgt dev */\n\t\tsizeof(int) * (real_stripes) +\n\t\t/*\n\t\t * plus the raid_map, which includes both the tgt dev\n\t\t * and the stripes\n\t\t */\n\t\tsizeof(u64) * (total_stripes),\n\t\tGFP_NOFS|__GFP_NOFAIL);\n\n\tatomic_set(&bbio->error, 0);\n\trefcount_set(&bbio->refs, 1);\n\n\tbbio->tgtdev_map = (int *)(bbio->stripes + total_stripes);\n\tbbio->raid_map = (u64 *)(bbio->tgtdev_map + real_stripes);\n\n\treturn bbio;\n}\n\nvoid btrfs_get_bbio(struct btrfs_bio *bbio)\n{\n\tWARN_ON(!refcount_read(&bbio->refs));\n\trefcount_inc(&bbio->refs);\n}\n\nvoid btrfs_put_bbio(struct btrfs_bio *bbio)\n{\n\tif (!bbio)\n\t\treturn;\n\tif (refcount_dec_and_test(&bbio->refs))\n\t\tkfree(bbio);\n}\n\n/* can REQ_OP_DISCARD be sent with other REQ like REQ_OP_WRITE? */\n/*\n * Please note that, discard won't be sent to target device of device\n * replace.\n */\nstatic int __btrfs_map_block_for_discard(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 *length_ret,\n\t\t\t\t\t struct btrfs_bio **bbio_ret)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_bio *bbio;\n\tu64 length = *length_ret;\n\tu64 offset;\n\tu64 stripe_nr;\n\tu64 stripe_nr_end;\n\tu64 stripe_end_offset;\n\tu64 stripe_cnt;\n\tu64 stripe_len;\n\tu64 stripe_offset;\n\tu64 num_stripes;\n\tu32 stripe_index;\n\tu32 factor = 0;\n\tu32 sub_stripes = 0;\n\tu64 stripes_per_dev = 0;\n\tu32 remaining_stripes = 0;\n\tu32 last_stripe = 0;\n\tint ret = 0;\n\tint i;\n\n\t/* discard always return a bbio */\n\tASSERT(bbio_ret);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\t/* we don't discard raid56 yet */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\toffset = logical - em->start;\n\tlength = min_t(u64, em->start + em->len - logical, length);\n\t*length_ret = length;\n\n\tstripe_len = map->stripe_len;\n\t/*\n\t * stripe_nr counts the total number of stripes we have to stride\n\t * to get to this block\n\t */\n\tstripe_nr = div64_u64(offset, stripe_len);\n\n\t/* stripe_offset is the offset of this block in its stripe */\n\tstripe_offset = offset - stripe_nr * stripe_len;\n\n\tstripe_nr_end = round_up(offset + length, map->stripe_len);\n\tstripe_nr_end = div64_u64(stripe_nr_end, map->stripe_len);\n\tstripe_cnt = stripe_nr_end - stripe_nr;\n\tstripe_end_offset = stripe_nr_end * map->stripe_len -\n\t\t\t    (offset + length);\n\t/*\n\t * after this, stripe_nr is the number of stripes on this\n\t * device we have to walk to find the data, and stripe_index is\n\t * the number of our device in the stripe array\n\t */\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\t\tsub_stripes = 1;\n\t\telse\n\t\t\tsub_stripes = map->sub_stripes;\n\n\t\tfactor = map->num_stripes / sub_stripes;\n\t\tnum_stripes = min_t(u64, map->num_stripes,\n\t\t\t\t    sub_stripes * stripe_cnt);\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= sub_stripes;\n\t\tstripes_per_dev = div_u64_rem(stripe_cnt, factor,\n\t\t\t\t\t      &remaining_stripes);\n\t\tdiv_u64_rem(stripe_nr_end - 1, factor, &last_stripe);\n\t\tlast_stripe *= sub_stripes;\n\t} else if (map->type & (BTRFS_BLOCK_GROUP_RAID1_MASK |\n\t\t\t\tBTRFS_BLOCK_GROUP_DUP)) {\n\t\tnum_stripes = map->num_stripes;\n\t} else {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t}\n\n\tbbio = alloc_btrfs_bio(num_stripes, 0);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset + stripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev = map->stripes[stripe_index].dev;\n\n\t\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\t\tbbio->stripes[i].length = stripes_per_dev *\n\t\t\t\tmap->stripe_len;\n\n\t\t\tif (i / sub_stripes < remaining_stripes)\n\t\t\t\tbbio->stripes[i].length +=\n\t\t\t\t\tmap->stripe_len;\n\n\t\t\t/*\n\t\t\t * Special for the first stripe and\n\t\t\t * the last stripe:\n\t\t\t *\n\t\t\t * |-------|...|-------|\n\t\t\t *     |----------|\n\t\t\t *    off     end_off\n\t\t\t */\n\t\t\tif (i < sub_stripes)\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_offset;\n\n\t\t\tif (stripe_index >= last_stripe &&\n\t\t\t    stripe_index <= (last_stripe +\n\t\t\t\t\t     sub_stripes - 1))\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_end_offset;\n\n\t\t\tif (i == sub_stripes - 1)\n\t\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\tbbio->stripes[i].length = length;\n\t\t}\n\n\t\tstripe_index++;\n\t\tif (stripe_index == map->num_stripes) {\n\t\t\tstripe_index = 0;\n\t\t\tstripe_nr++;\n\t\t}\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * In dev-replace case, for repair case (that's the only case where the mirror\n * is selected explicitly when calling btrfs_map_block), blocks left of the\n * left cursor can also be read from the target drive.\n *\n * For REQ_GET_READ_MIRRORS, the target drive is added as the last one to the\n * array of stripes.\n * For READ, it also needs to be supported using the same mirror number.\n *\n * If the requested block is not left of the left cursor, EIO is returned. This\n * can happen because btrfs_num_copies() returns one more in the dev-replace\n * case.\n */\nstatic int get_extra_mirror_from_replace(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 length,\n\t\t\t\t\t u64 srcdev_devid, int *mirror_num,\n\t\t\t\t\t u64 *physical)\n{\n\tstruct btrfs_bio *bbio = NULL;\n\tint num_stripes;\n\tint index_srcdev = 0;\n\tint found = 0;\n\tu64 physical_of_found = 0;\n\tint i;\n\tint ret = 0;\n\n\tret = __btrfs_map_block(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\tlogical, &length, &bbio, 0, 0);\n\tif (ret) {\n\t\tASSERT(bbio == NULL);\n\t\treturn ret;\n\t}\n\n\tnum_stripes = bbio->num_stripes;\n\tif (*mirror_num > num_stripes) {\n\t\t/*\n\t\t * BTRFS_MAP_GET_READ_MIRRORS does not contain this mirror,\n\t\t * that means that the requested area is not left of the left\n\t\t * cursor\n\t\t */\n\t\tbtrfs_put_bbio(bbio);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * process the rest of the function using the mirror_num of the source\n\t * drive. Therefore look it up first.  At the end, patch the device\n\t * pointer to the one of the target drive.\n\t */\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tif (bbio->stripes[i].dev->devid != srcdev_devid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * In case of DUP, in order to keep it simple, only add the\n\t\t * mirror with the lowest physical address\n\t\t */\n\t\tif (found &&\n\t\t    physical_of_found <= bbio->stripes[i].physical)\n\t\t\tcontinue;\n\n\t\tindex_srcdev = i;\n\t\tfound = 1;\n\t\tphysical_of_found = bbio->stripes[i].physical;\n\t}\n\n\tbtrfs_put_bbio(bbio);\n\n\tASSERT(found);\n\tif (!found)\n\t\treturn -EIO;\n\n\t*mirror_num = index_srcdev + 1;\n\t*physical = physical_of_found;\n\treturn ret;\n}\n\nstatic bool is_block_group_to_copy(struct btrfs_fs_info *fs_info, u64 logical)\n{\n\tstruct btrfs_block_group *cache;\n\tbool ret;\n\n\t/* Non zoned filesystem does not use \"to_copy\" flag */\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn false;\n\n\tcache = btrfs_lookup_block_group(fs_info, logical);\n\n\tspin_lock(&cache->lock);\n\tret = cache->to_copy;\n\tspin_unlock(&cache->lock);\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic void handle_ops_on_dev_replace(enum btrfs_map_op op,\n\t\t\t\t      struct btrfs_bio **bbio_ret,\n\t\t\t\t      struct btrfs_dev_replace *dev_replace,\n\t\t\t\t      u64 logical,\n\t\t\t\t      int *num_stripes_ret, int *max_errors_ret)\n{\n\tstruct btrfs_bio *bbio = *bbio_ret;\n\tu64 srcdev_devid = dev_replace->srcdev->devid;\n\tint tgtdev_indexes = 0;\n\tint num_stripes = *num_stripes_ret;\n\tint max_errors = *max_errors_ret;\n\tint i;\n\n\tif (op == BTRFS_MAP_WRITE) {\n\t\tint index_where_to_add;\n\n\t\t/*\n\t\t * A block group which have \"to_copy\" set will eventually\n\t\t * copied by dev-replace process. We can avoid cloning IO here.\n\t\t */\n\t\tif (is_block_group_to_copy(dev_replace->srcdev->fs_info, logical))\n\t\t\treturn;\n\n\t\t/*\n\t\t * duplicate the write operations while the dev replace\n\t\t * procedure is running. Since the copying of the old disk to\n\t\t * the new disk takes place at run time while the filesystem is\n\t\t * mounted writable, the regular write operations to the old\n\t\t * disk have to be duplicated to go to the new disk as well.\n\t\t *\n\t\t * Note that device->missing is handled by the caller, and that\n\t\t * the write to the old disk is already set up in the stripes\n\t\t * array.\n\t\t */\n\t\tindex_where_to_add = num_stripes;\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/* write to new disk, too */\n\t\t\t\tstruct btrfs_bio_stripe *new =\n\t\t\t\t\tbbio->stripes + index_where_to_add;\n\t\t\t\tstruct btrfs_bio_stripe *old =\n\t\t\t\t\tbbio->stripes + i;\n\n\t\t\t\tnew->physical = old->physical;\n\t\t\t\tnew->length = old->length;\n\t\t\t\tnew->dev = dev_replace->tgtdev;\n\t\t\t\tbbio->tgtdev_map[i] = index_where_to_add;\n\t\t\t\tindex_where_to_add++;\n\t\t\t\tmax_errors++;\n\t\t\t\ttgtdev_indexes++;\n\t\t\t}\n\t\t}\n\t\tnum_stripes = index_where_to_add;\n\t} else if (op == BTRFS_MAP_GET_READ_MIRRORS) {\n\t\tint index_srcdev = 0;\n\t\tint found = 0;\n\t\tu64 physical_of_found = 0;\n\n\t\t/*\n\t\t * During the dev-replace procedure, the target drive can also\n\t\t * be used to read data in case it is needed to repair a corrupt\n\t\t * block elsewhere. This is possible if the requested area is\n\t\t * left of the left cursor. In this area, the target drive is a\n\t\t * full copy of the source drive.\n\t\t */\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/*\n\t\t\t\t * In case of DUP, in order to keep it simple,\n\t\t\t\t * only add the mirror with the lowest physical\n\t\t\t\t * address\n\t\t\t\t */\n\t\t\t\tif (found &&\n\t\t\t\t    physical_of_found <=\n\t\t\t\t     bbio->stripes[i].physical)\n\t\t\t\t\tcontinue;\n\t\t\t\tindex_srcdev = i;\n\t\t\t\tfound = 1;\n\t\t\t\tphysical_of_found = bbio->stripes[i].physical;\n\t\t\t}\n\t\t}\n\t\tif (found) {\n\t\t\tstruct btrfs_bio_stripe *tgtdev_stripe =\n\t\t\t\tbbio->stripes + num_stripes;\n\n\t\t\ttgtdev_stripe->physical = physical_of_found;\n\t\t\ttgtdev_stripe->length =\n\t\t\t\tbbio->stripes[index_srcdev].length;\n\t\t\ttgtdev_stripe->dev = dev_replace->tgtdev;\n\t\t\tbbio->tgtdev_map[index_srcdev] = num_stripes;\n\n\t\t\ttgtdev_indexes++;\n\t\t\tnum_stripes++;\n\t\t}\n\t}\n\n\t*num_stripes_ret = num_stripes;\n\t*max_errors_ret = max_errors;\n\tbbio->num_tgtdevs = tgtdev_indexes;\n\t*bbio_ret = bbio;\n}\n\nstatic bool need_full_stripe(enum btrfs_map_op op)\n{\n\treturn (op == BTRFS_MAP_WRITE || op == BTRFS_MAP_GET_READ_MIRRORS);\n}\n\n/*\n * Calculate the geometry of a particular (address, len) tuple. This\n * information is used to calculate how big a particular bio can get before it\n * straddles a stripe.\n *\n * @fs_info: the filesystem\n * @em:      mapping containing the logical extent\n * @op:      type of operation - write or read\n * @logical: address that we want to figure out the geometry of\n * @io_geom: pointer used to return values\n *\n * Returns < 0 in case a chunk for the given logical address cannot be found,\n * usually shouldn't happen unless @logical is corrupted, 0 otherwise.\n */\nint btrfs_get_io_geometry(struct btrfs_fs_info *fs_info, struct extent_map *em,\n\t\t\t  enum btrfs_map_op op, u64 logical,\n\t\t\t  struct btrfs_io_geometry *io_geom)\n{\n\tstruct map_lookup *map;\n\tu64 len;\n\tu64 offset;\n\tu64 stripe_offset;\n\tu64 stripe_nr;\n\tu64 stripe_len;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\tint data_stripes;\n\n\tASSERT(op != BTRFS_MAP_DISCARD);\n\n\tmap = em->map_lookup;\n\t/* Offset of this logical address in the chunk */\n\toffset = logical - em->start;\n\t/* Len of a stripe in a chunk */\n\tstripe_len = map->stripe_len;\n\t/* Stripe where this block falls in */\n\tstripe_nr = div64_u64(offset, stripe_len);\n\t/* Offset of stripe in the chunk */\n\tstripe_offset = stripe_nr * stripe_len;\n\tif (offset < stripe_offset) {\n\t\tbtrfs_crit(fs_info,\n\"stripe math has gone wrong, stripe_offset=%llu offset=%llu start=%llu logical=%llu stripe_len=%llu\",\n\t\t\tstripe_offset, offset, em->start, logical, stripe_len);\n\t\treturn -EINVAL;\n\t}\n\n\t/* stripe_offset is the offset of this block in its stripe */\n\tstripe_offset = offset - stripe_offset;\n\tdata_stripes = nr_data_stripes(map);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\t\tu64 max_len = stripe_len - stripe_offset;\n\n\t\t/*\n\t\t * In case of raid56, we need to know the stripe aligned start\n\t\t */\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t\tunsigned long full_stripe_len = stripe_len * data_stripes;\n\t\t\traid56_full_stripe_start = offset;\n\n\t\t\t/*\n\t\t\t * Allow a write of a full stripe, but make sure we\n\t\t\t * don't allow straddling of stripes\n\t\t\t */\n\t\t\traid56_full_stripe_start = div64_u64(raid56_full_stripe_start,\n\t\t\t\t\tfull_stripe_len);\n\t\t\traid56_full_stripe_start *= full_stripe_len;\n\n\t\t\t/*\n\t\t\t * For writes to RAID[56], allow a full stripeset across\n\t\t\t * all disks. For other RAID types and for RAID[56]\n\t\t\t * reads, just allow a single stripe (on a single disk).\n\t\t\t */\n\t\t\tif (op == BTRFS_MAP_WRITE) {\n\t\t\t\tmax_len = stripe_len * data_stripes -\n\t\t\t\t\t  (offset - raid56_full_stripe_start);\n\t\t\t}\n\t\t}\n\t\tlen = min_t(u64, em->len - offset, max_len);\n\t} else {\n\t\tlen = em->len - offset;\n\t}\n\n\tio_geom->len = len;\n\tio_geom->offset = offset;\n\tio_geom->stripe_len = stripe_len;\n\tio_geom->stripe_nr = stripe_nr;\n\tio_geom->stripe_offset = stripe_offset;\n\tio_geom->raid56_stripe_offset = raid56_full_stripe_start;\n\n\treturn 0;\n}\n\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 stripe_offset;\n\tu64 stripe_nr;\n\tu64 stripe_len;\n\tu32 stripe_index;\n\tint data_stripes;\n\tint i;\n\tint ret = 0;\n\tint num_stripes;\n\tint max_errors = 0;\n\tint tgtdev_indexes = 0;\n\tstruct btrfs_bio *bbio = NULL;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint dev_replace_is_ongoing = 0;\n\tint num_alloc_stripes;\n\tint patch_the_first_stripe_for_dev_replace = 0;\n\tu64 physical_to_patch_in_first_stripe = 0;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\tstruct btrfs_io_geometry geom;\n\n\tASSERT(bbio_ret);\n\tASSERT(op != BTRFS_MAP_DISCARD);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, *length);\n\tASSERT(!IS_ERR(em));\n\n\tret = btrfs_get_io_geometry(fs_info, em, op, logical, &geom);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmap = em->map_lookup;\n\n\t*length = geom.len;\n\tstripe_len = geom.stripe_len;\n\tstripe_nr = geom.stripe_nr;\n\tstripe_offset = geom.stripe_offset;\n\traid56_full_stripe_start = geom.raid56_stripe_offset;\n\tdata_stripes = nr_data_stripes(map);\n\n\tdown_read(&dev_replace->rwsem);\n\tdev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace);\n\t/*\n\t * Hold the semaphore for read during the whole operation, write is\n\t * requested at commit time but must wait.\n\t */\n\tif (!dev_replace_is_ongoing)\n\t\tup_read(&dev_replace->rwsem);\n\n\tif (dev_replace_is_ongoing && mirror_num == map->num_stripes + 1 &&\n\t    !need_full_stripe(op) && dev_replace->tgtdev != NULL) {\n\t\tret = get_extra_mirror_from_replace(fs_info, logical, *length,\n\t\t\t\t\t\t    dev_replace->srcdev->devid,\n\t\t\t\t\t\t    &mirror_num,\n\t\t\t\t\t    &physical_to_patch_in_first_stripe);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\telse\n\t\t\tpatch_the_first_stripe_for_dev_replace = 1;\n\t} else if (mirror_num > map->num_stripes) {\n\t\tmirror_num = 0;\n\t}\n\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tif (!need_full_stripe(op))\n\t\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1_MASK) {\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->num_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index = mirror_num - 1;\n\t\telse {\n\t\t\tstripe_index = find_live_mirror(fs_info, map, 0,\n\t\t\t\t\t    dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tif (need_full_stripe(op)) {\n\t\t\tnum_stripes = map->num_stripes;\n\t\t} else if (mirror_num) {\n\t\t\tstripe_index = mirror_num - 1;\n\t\t} else {\n\t\t\tmirror_num = 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tu32 factor = map->num_stripes / map->sub_stripes;\n\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= map->sub_stripes;\n\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->sub_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index += mirror_num - 1;\n\t\telse {\n\t\t\tint old_stripe_index = stripe_index;\n\t\t\tstripe_index = find_live_mirror(fs_info, map,\n\t\t\t\t\t      stripe_index,\n\t\t\t\t\t      dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index - old_stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tif (need_raid_map && (need_full_stripe(op) || mirror_num > 1)) {\n\t\t\t/* push stripe_nr back to the start of the full stripe */\n\t\t\tstripe_nr = div64_u64(raid56_full_stripe_start,\n\t\t\t\t\tstripe_len * data_stripes);\n\n\t\t\t/* RAID[56] write or recovery. Return all stripes */\n\t\t\tnum_stripes = map->num_stripes;\n\t\t\tmax_errors = nr_parity_stripes(map);\n\n\t\t\t*length = map->stripe_len;\n\t\t\tstripe_index = 0;\n\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Mirror #0 or #1 means the original data block.\n\t\t\t * Mirror #2 is RAID5 parity block.\n\t\t\t * Mirror #3 is RAID6 Q block.\n\t\t\t */\n\t\t\tstripe_nr = div_u64_rem(stripe_nr,\n\t\t\t\t\tdata_stripes, &stripe_index);\n\t\t\tif (mirror_num > 1)\n\t\t\t\tstripe_index = data_stripes + mirror_num - 2;\n\n\t\t\t/* We distribute the parity blocks across stripes */\n\t\t\tdiv_u64_rem(stripe_nr + stripe_index, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t\t\tif (!need_full_stripe(op) && mirror_num <= 1)\n\t\t\t\tmirror_num = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * after this, stripe_nr is the number of stripes on this\n\t\t * device we have to walk to find the data, and stripe_index is\n\t\t * the number of our device in the stripe array\n\t\t */\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tmirror_num = stripe_index + 1;\n\t}\n\tif (stripe_index >= map->num_stripes) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u\",\n\t\t\t   stripe_index, map->num_stripes);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tnum_alloc_stripes = num_stripes;\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL) {\n\t\tif (op == BTRFS_MAP_WRITE)\n\t\t\tnum_alloc_stripes <<= 1;\n\t\tif (op == BTRFS_MAP_GET_READ_MIRRORS)\n\t\t\tnum_alloc_stripes++;\n\t\ttgtdev_indexes = num_stripes;\n\t}\n\n\tbbio = alloc_btrfs_bio(num_alloc_stripes, tgtdev_indexes);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical = map->stripes[stripe_index].physical +\n\t\t\tstripe_offset + stripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev = map->stripes[stripe_index].dev;\n\t\tstripe_index++;\n\t}\n\n\t/* build raid_map */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && need_raid_map &&\n\t    (need_full_stripe(op) || mirror_num > 1)) {\n\t\tu64 tmp;\n\t\tunsigned rot;\n\n\t\t/* Work out the disk rotation on this stripe-set */\n\t\tdiv_u64_rem(stripe_nr, num_stripes, &rot);\n\n\t\t/* Fill in the logical address of each stripe */\n\t\ttmp = stripe_nr * data_stripes;\n\t\tfor (i = 0; i < data_stripes; i++)\n\t\t\tbbio->raid_map[(i+rot) % num_stripes] =\n\t\t\t\tem->start + (tmp + i) * map->stripe_len;\n\n\t\tbbio->raid_map[(i+rot) % map->num_stripes] = RAID5_P_STRIPE;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t\tbbio->raid_map[(i+rot+1) % num_stripes] =\n\t\t\t\tRAID6_Q_STRIPE;\n\n\t\tsort_parity_stripes(bbio, num_stripes);\n\t}\n\n\tif (need_full_stripe(op))\n\t\tmax_errors = btrfs_chunk_max_errors(map);\n\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL &&\n\t    need_full_stripe(op)) {\n\t\thandle_ops_on_dev_replace(op, &bbio, dev_replace, logical,\n\t\t\t\t\t  &num_stripes, &max_errors);\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\n\tbbio->max_errors = max_errors;\n\tbbio->mirror_num = mirror_num;\n\n\t/*\n\t * this is the case that REQ_READ && dev_replace_is_ongoing &&\n\t * mirror_num == num_stripes + 1 && dev_replace target drive is\n\t * available as a mirror\n\t */\n\tif (patch_the_first_stripe_for_dev_replace && num_stripes > 0) {\n\t\tWARN_ON(num_stripes > 1);\n\t\tbbio->stripes[0].dev = dev_replace->tgtdev;\n\t\tbbio->stripes[0].physical = physical_to_patch_in_first_stripe;\n\t\tbbio->mirror_num = map->num_stripes + 1;\n\t}\nout:\n\tif (dev_replace_is_ongoing) {\n\t\tlockdep_assert_held(&dev_replace->rwsem);\n\t\t/* Unlock and let waiting writers proceed */\n\t\tup_read(&dev_replace->rwsem);\n\t}\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t      u64 logical, u64 *length,\n\t\t      struct btrfs_bio **bbio_ret, int mirror_num)\n{\n\tif (op == BTRFS_MAP_DISCARD)\n\t\treturn __btrfs_map_block_for_discard(fs_info, logical,\n\t\t\t\t\t\t     length, bbio_ret);\n\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret,\n\t\t\t\t mirror_num, 0);\n}\n\n/* For Scrub/replace */\nint btrfs_map_sblock(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t     u64 logical, u64 *length,\n\t\t     struct btrfs_bio **bbio_ret)\n{\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret, 0, 1);\n}\n\nstatic inline void btrfs_end_bbio(struct btrfs_bio *bbio, struct bio *bio)\n{\n\tbio->bi_private = bbio->private;\n\tbio->bi_end_io = bbio->end_io;\n\tbio_endio(bio);\n\n\tbtrfs_put_bbio(bbio);\n}\n\nstatic void btrfs_end_bio(struct bio *bio)\n{\n\tstruct btrfs_bio *bbio = bio->bi_private;\n\tint is_orig_bio = 0;\n\n\tif (bio->bi_status) {\n\t\tatomic_inc(&bbio->error);\n\t\tif (bio->bi_status == BLK_STS_IOERR ||\n\t\t    bio->bi_status == BLK_STS_TARGET) {\n\t\t\tstruct btrfs_device *dev = btrfs_io_bio(bio)->device;\n\n\t\t\tASSERT(dev->bdev);\n\t\t\tif (btrfs_op(bio) == BTRFS_MAP_WRITE)\n\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_WRITE_ERRS);\n\t\t\telse if (!(bio->bi_opf & REQ_RAHEAD))\n\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_READ_ERRS);\n\t\t\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_FLUSH_ERRS);\n\t\t}\n\t}\n\n\tif (bio == bbio->orig_bio)\n\t\tis_orig_bio = 1;\n\n\tbtrfs_bio_counter_dec(bbio->fs_info);\n\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\tif (!is_orig_bio) {\n\t\t\tbio_put(bio);\n\t\t\tbio = bbio->orig_bio;\n\t\t}\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\t/* only send an error to the higher layers if it is\n\t\t * beyond the tolerance of the btrfs bio\n\t\t */\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors) {\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t} else {\n\t\t\t/*\n\t\t\t * this bio is actually up to date, we didn't\n\t\t\t * go over the max number of errors\n\t\t\t */\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\t}\n\n\t\tbtrfs_end_bbio(bbio, bio);\n\t} else if (!is_orig_bio) {\n\t\tbio_put(bio);\n\t}\n}\n\nstatic void submit_stripe_bio(struct btrfs_bio *bbio, struct bio *bio,\n\t\t\t      u64 physical, struct btrfs_device *dev)\n{\n\tstruct btrfs_fs_info *fs_info = bbio->fs_info;\n\n\tbio->bi_private = bbio;\n\tbtrfs_io_bio(bio)->device = dev;\n\tbio->bi_end_io = btrfs_end_bio;\n\tbio->bi_iter.bi_sector = physical >> 9;\n\t/*\n\t * For zone append writing, bi_sector must point the beginning of the\n\t * zone\n\t */\n\tif (bio_op(bio) == REQ_OP_ZONE_APPEND) {\n\t\tif (btrfs_dev_is_sequential(dev, physical)) {\n\t\t\tu64 zone_start = round_down(physical, fs_info->zone_size);\n\n\t\t\tbio->bi_iter.bi_sector = zone_start >> SECTOR_SHIFT;\n\t\t} else {\n\t\t\tbio->bi_opf &= ~REQ_OP_ZONE_APPEND;\n\t\t\tbio->bi_opf |= REQ_OP_WRITE;\n\t\t}\n\t}\n\tbtrfs_debug_in_rcu(fs_info,\n\t\"btrfs_map_bio: rw %d 0x%x, sector=%llu, dev=%lu (%s id %llu), size=%u\",\n\t\tbio_op(bio), bio->bi_opf, bio->bi_iter.bi_sector,\n\t\t(unsigned long)dev->bdev->bd_dev, rcu_str_deref(dev->name),\n\t\tdev->devid, bio->bi_iter.bi_size);\n\tbio_set_dev(bio, dev->bdev);\n\n\tbtrfs_bio_counter_inc_noblocked(fs_info);\n\n\tbtrfsic_submit_bio(bio);\n}\n\nstatic void bbio_error(struct btrfs_bio *bbio, struct bio *bio, u64 logical)\n{\n\tatomic_inc(&bbio->error);\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\t/* Should be the original bio. */\n\t\tWARN_ON(bio != bbio->orig_bio);\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\tbio->bi_iter.bi_sector = logical >> 9;\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors)\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\tbtrfs_end_bbio(bbio, bio);\n\t}\n}\n\nblk_status_t btrfs_map_bio(struct btrfs_fs_info *fs_info, struct bio *bio,\n\t\t\t   int mirror_num)\n{\n\tstruct btrfs_device *dev;\n\tstruct bio *first_bio = bio;\n\tu64 logical = bio->bi_iter.bi_sector << 9;\n\tu64 length = 0;\n\tu64 map_length;\n\tint ret;\n\tint dev_nr;\n\tint total_devs;\n\tstruct btrfs_bio *bbio = NULL;\n\n\tlength = bio->bi_iter.bi_size;\n\tmap_length = length;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = __btrfs_map_block(fs_info, btrfs_op(bio), logical,\n\t\t\t\t&map_length, &bbio, mirror_num, 1);\n\tif (ret) {\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\ttotal_devs = bbio->num_stripes;\n\tbbio->orig_bio = first_bio;\n\tbbio->private = first_bio->bi_private;\n\tbbio->end_io = first_bio->bi_end_io;\n\tbbio->fs_info = fs_info;\n\tatomic_set(&bbio->stripes_pending, bbio->num_stripes);\n\n\tif ((bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) &&\n\t    ((btrfs_op(bio) == BTRFS_MAP_WRITE) || (mirror_num > 1))) {\n\t\t/* In this case, map_length has been set to the length of\n\t\t   a single stripe; not the whole write */\n\t\tif (btrfs_op(bio) == BTRFS_MAP_WRITE) {\n\t\t\tret = raid56_parity_write(fs_info, bio, bbio,\n\t\t\t\t\t\t  map_length);\n\t\t} else {\n\t\t\tret = raid56_parity_recover(fs_info, bio, bbio,\n\t\t\t\t\t\t    map_length, mirror_num, 1);\n\t\t}\n\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\tif (map_length < length) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"mapping failed logical %llu bio len %llu len %llu\",\n\t\t\t   logical, length, map_length);\n\t\tBUG();\n\t}\n\n\tfor (dev_nr = 0; dev_nr < total_devs; dev_nr++) {\n\t\tdev = bbio->stripes[dev_nr].dev;\n\t\tif (!dev || !dev->bdev || test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t   &dev->dev_state) ||\n\t\t    (btrfs_op(first_bio) == BTRFS_MAP_WRITE &&\n\t\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state))) {\n\t\t\tbbio_error(bbio, first_bio, logical);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dev_nr < total_devs - 1)\n\t\t\tbio = btrfs_bio_clone(first_bio);\n\t\telse\n\t\t\tbio = first_bio;\n\n\t\tsubmit_stripe_bio(bbio, bio, bbio->stripes[dev_nr].physical, dev);\n\t}\n\tbtrfs_bio_counter_dec(fs_info);\n\treturn BLK_STS_OK;\n}\n\n/*\n * Find a device specified by @devid or @uuid in the list of @fs_devices, or\n * return NULL.\n *\n * If devid and uuid are both specified, the match must be exact, otherwise\n * only devid is used.\n */\nstruct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       u64 devid, u8 *uuid, u8 *fsid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *seed_devs;\n\n\tif (!fsid || !memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {\n\t\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\t\tif (device->devid == devid &&\n\t\t\t    (!uuid || memcmp(device->uuid, uuid,\n\t\t\t\t\t     BTRFS_UUID_SIZE) == 0))\n\t\t\t\treturn device;\n\t\t}\n\t}\n\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tif (!fsid ||\n\t\t    !memcmp(seed_devs->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {\n\t\t\tlist_for_each_entry(device, &seed_devs->devices,\n\t\t\t\t\t    dev_list) {\n\t\t\t\tif (device->devid == devid &&\n\t\t\t\t    (!uuid || memcmp(device->uuid, uuid,\n\t\t\t\t\t\t     BTRFS_UUID_SIZE) == 0))\n\t\t\t\t\treturn device;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct btrfs_device *add_missing_dev(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t\t    u64 devid, u8 *dev_uuid)\n{\n\tstruct btrfs_device *device;\n\tunsigned int nofs_flag;\n\n\t/*\n\t * We call this under the chunk_mutex, so we want to use NOFS for this\n\t * allocation, however we don't want to change btrfs_alloc_device() to\n\t * always do NOFS because we use it in a lot of other GFP_KERNEL safe\n\t * places.\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tdevice = btrfs_alloc_device(NULL, &devid, dev_uuid);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(device))\n\t\treturn device;\n\n\tlist_add(&device->dev_list, &fs_devices->devices);\n\tdevice->fs_devices = fs_devices;\n\tfs_devices->num_devices++;\n\n\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\tfs_devices->missing_devices++;\n\n\treturn device;\n}\n\n/**\n * btrfs_alloc_device - allocate struct btrfs_device\n * @fs_info:\tused only for generating a new devid, can be NULL if\n *\t\tdevid is provided (i.e. @devid != NULL).\n * @devid:\ta pointer to devid for this device.  If NULL a new devid\n *\t\tis generated.\n * @uuid:\ta pointer to UUID for this device.  If NULL a new UUID\n *\t\tis generated.\n *\n * Return: a pointer to a new &struct btrfs_device on success; ERR_PTR()\n * on error.  Returned struct is not linked onto any lists and must be\n * destroyed with btrfs_free_device.\n */\nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid,\n\t\t\t\t\tconst u8 *uuid)\n{\n\tstruct btrfs_device *dev;\n\tu64 tmp;\n\n\tif (WARN_ON(!devid && !fs_info))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Preallocate a bio that's always going to be used for flushing device\n\t * barriers and matches the device lifespan\n\t */\n\tdev->flush_bio = bio_kmalloc(GFP_KERNEL, 0);\n\tif (!dev->flush_bio) {\n\t\tkfree(dev);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tINIT_LIST_HEAD(&dev->dev_list);\n\tINIT_LIST_HEAD(&dev->dev_alloc_list);\n\tINIT_LIST_HEAD(&dev->post_commit_list);\n\n\tatomic_set(&dev->reada_in_flight, 0);\n\tatomic_set(&dev->dev_stats_ccnt, 0);\n\tbtrfs_device_data_ordered_init(dev);\n\tINIT_RADIX_TREE(&dev->reada_zones, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\tINIT_RADIX_TREE(&dev->reada_extents, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\textent_io_tree_init(fs_info, &dev->alloc_state,\n\t\t\t    IO_TREE_DEVICE_ALLOC_STATE, NULL);\n\n\tif (devid)\n\t\ttmp = *devid;\n\telse {\n\t\tint ret;\n\n\t\tret = find_next_devid(fs_info, &tmp);\n\t\tif (ret) {\n\t\t\tbtrfs_free_device(dev);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tdev->devid = tmp;\n\n\tif (uuid)\n\t\tmemcpy(dev->uuid, uuid, BTRFS_UUID_SIZE);\n\telse\n\t\tgenerate_random_uuid(dev->uuid);\n\n\treturn dev;\n}\n\nstatic void btrfs_report_missing_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tu64 devid, u8 *uuid, bool error)\n{\n\tif (error)\n\t\tbtrfs_err_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n\telse\n\t\tbtrfs_warn_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n}\n\nstatic u64 calc_stripe_length(u64 type, u64 chunk_len, int num_stripes)\n{\n\tconst int data_stripes = calc_data_stripes(type, num_stripes);\n\n\treturn div_u64(chunk_len, data_stripes);\n}\n\n#if BITS_PER_LONG == 32\n/*\n * Due to page cache limit, metadata beyond BTRFS_32BIT_MAX_FILE_SIZE\n * can't be accessed on 32bit systems.\n *\n * This function do mount time check to reject the fs if it already has\n * metadata chunk beyond that limit.\n */\nstatic int check_32bit_meta_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t  u64 logical, u64 length, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_METADATA))\n\t\treturn 0;\n\n\tif (logical + length < MAX_LFS_FILESIZE)\n\t\treturn 0;\n\n\tbtrfs_err_32bit_limit(fs_info);\n\treturn -EOVERFLOW;\n}\n\n/*\n * This is to give early warning for any metadata chunk reaching\n * BTRFS_32BIT_EARLY_WARN_THRESHOLD.\n * Although we can still access the metadata, it's not going to be possible\n * once the limit is reached.\n */\nstatic void warn_32bit_meta_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t  u64 logical, u64 length, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_METADATA))\n\t\treturn;\n\n\tif (logical + length < BTRFS_32BIT_EARLY_WARN_THRESHOLD)\n\t\treturn;\n\n\tbtrfs_warn_32bit_limit(fs_info);\n}\n#endif\n\nstatic int read_one_chunk(struct btrfs_key *key, struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu64 type;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n#if BITS_PER_LONG == 32\n\tret = check_32bit_meta_chunk(fs_info, logical, length, type);\n\tif (ret < 0)\n\t\treturn ret;\n\twarn_32bit_meta_chunk(fs_info, logical, length, type);\n#endif\n\n\t/*\n\t * Only need to verify chunk item if we're reading from sys chunk array,\n\t * as chunk item in tree block is already verified by tree-checker.\n\t */\n\tif (leaf->start == BTRFS_SUPER_INFO_OFFSET) {\n\t\tret = btrfs_check_chunk_valid(leaf, chunk, logical);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, logical, 1);\n\tread_unlock(&map_tree->lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = type;\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tem->orig_block_len = calc_stripe_length(type, em->len,\n\t\t\t\t\t\tmap->num_stripes);\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->lock);\n\tret = add_extent_mapping(map_tree, em, 0);\n\twrite_unlock(&map_tree->lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic void fill_device_from_item(struct extent_buffer *leaf,\n\t\t\t\t struct btrfs_dev_item *dev_item,\n\t\t\t\t struct btrfs_device *device)\n{\n\tunsigned long ptr;\n\n\tdevice->devid = btrfs_device_id(leaf, dev_item);\n\tdevice->disk_total_bytes = btrfs_device_total_bytes(leaf, dev_item);\n\tdevice->total_bytes = device->disk_total_bytes;\n\tdevice->commit_total_bytes = device->disk_total_bytes;\n\tdevice->bytes_used = btrfs_device_bytes_used(leaf, dev_item);\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->type = btrfs_device_type(leaf, dev_item);\n\tdevice->io_align = btrfs_device_io_align(leaf, dev_item);\n\tdevice->io_width = btrfs_device_io_width(leaf, dev_item);\n\tdevice->sector_size = btrfs_device_sector_size(leaf, dev_item);\n\tWARN_ON(device->devid == BTRFS_DEV_REPLACE_DEVID);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\n\tptr = btrfs_device_uuid(dev_item);\n\tread_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n}\n\nstatic struct btrfs_fs_devices *open_seed_devices(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u8 *fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tASSERT(fsid);\n\n\t/* This will match only for multi-device seed fs */\n\tlist_for_each_entry(fs_devices, &fs_info->fs_devices->seed_list, seed_list)\n\t\tif (!memcmp(fs_devices->fsid, fsid, BTRFS_FSID_SIZE))\n\t\t\treturn fs_devices;\n\n\n\tfs_devices = find_fsid(fsid, NULL);\n\tif (!fs_devices) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED))\n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\tfs_devices = alloc_fs_devices(fsid, NULL);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices->seeding = true;\n\t\tfs_devices->opened = 1;\n\t\treturn fs_devices;\n\t}\n\n\t/*\n\t * Upon first call for a seed fs fsid, just create a private copy of the\n\t * respective fs_devices and anchor it at fs_info->fs_devices->seed_list\n\t */\n\tfs_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tret = open_fs_devices(fs_devices, FMODE_READ, fs_info->bdev_holder);\n\tif (ret) {\n\t\tfree_fs_devices(fs_devices);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tif (!fs_devices->seeding) {\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tlist_add(&fs_devices->seed_list, &fs_info->fs_devices->seed_list);\n\n\treturn fs_devices;\n}\n\nstatic int read_one_dev(struct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tif (device->bdev) {\n\t\tu64 max_total_bytes = i_size_read(device->bdev->bd_inode);\n\n\t\tif (device->total_bytes > max_total_bytes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"device total_bytes should be at most %llu but found %llu\",\n\t\t\t\t  max_total_bytes, device->total_bytes);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}\n\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct extent_buffer *sb;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *array_ptr;\n\tunsigned long sb_array_offset;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur_offset;\n\tu64 type;\n\tstruct btrfs_key key;\n\n\tASSERT(BTRFS_SUPER_INFO_SIZE <= fs_info->nodesize);\n\t/*\n\t * This will create extent buffer of nodesize, superblock size is\n\t * fixed to BTRFS_SUPER_INFO_SIZE. If nodesize > sb size, this will\n\t * overallocate but we can keep it as-is, only the first page is used.\n\t */\n\tsb = btrfs_find_create_tree_block(fs_info, BTRFS_SUPER_INFO_OFFSET,\n\t\t\t\t\t  root->root_key.objectid, 0);\n\tif (IS_ERR(sb))\n\t\treturn PTR_ERR(sb);\n\tset_extent_buffer_uptodate(sb);\n\t/*\n\t * The sb extent buffer is artificial and just used to read the system array.\n\t * set_extent_buffer_uptodate() call does not properly mark all it's\n\t * pages up-to-date when the page is larger: extent does not cover the\n\t * whole page and consequently check_page_uptodate does not find all\n\t * the page's extents up-to-date (the hole beyond sb),\n\t * write_extent_buffer then triggers a WARN_ON.\n\t *\n\t * Regular short extents go through mark_extent_buffer_dirty/writeback cycle,\n\t * but sb spans only this function. Add an explicit SetPageUptodate call\n\t * to silence the warning eg. on PowerPC 64.\n\t */\n\tif (PAGE_SIZE > BTRFS_SUPER_INFO_SIZE)\n\t\tSetPageUptodate(sb->pages[0]);\n\n\twrite_extent_buffer(sb, super_copy, 0, BTRFS_SUPER_INFO_SIZE);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tarray_ptr = super_copy->sys_chunk_array;\n\tsb_array_offset = offsetof(struct btrfs_super_block, sys_chunk_array);\n\tcur_offset = 0;\n\n\twhile (cur_offset < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)array_ptr;\n\t\tlen = sizeof(*disk_key);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\n\t\tif (key.type != BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"unexpected item type %u in sys_array at offset %u\",\n\t\t\t\t  (u32)key.type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = (struct btrfs_chunk *)sb_array_offset;\n\t\t/*\n\t\t * At least one btrfs_chunk with one stripe must be present,\n\t\t * exact stripe count check comes afterwards\n\t\t */\n\t\tlen = btrfs_chunk_item_size(1);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tnum_stripes = btrfs_chunk_num_stripes(sb, chunk);\n\t\tif (!num_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid number of stripes %u in sys_array at offset %u\",\n\t\t\t\t  num_stripes, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\ttype = btrfs_chunk_type(sb, chunk);\n\t\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) == 0) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid chunk type %llu in sys_array at offset %u\",\n\t\t\t\t  type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tlen = btrfs_chunk_item_size(num_stripes);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tret = read_one_chunk(&key, sb, chunk);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\t}\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn ret;\n\nout_short_read:\n\tbtrfs_err(fs_info, \"sys_array too short to read %u bytes at offset %u\",\n\t\t\tlen, cur_offset);\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn -EIO;\n}\n\n/*\n * Check if all chunks in the fs are OK for read-write degraded mount\n *\n * If the @failing_dev is specified, it's accounted as missing.\n *\n * Return true if all chunks meet the minimal RW mount requirements.\n * Return false if any chunk doesn't meet the minimal RW mount requirements.\n */\nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev)\n{\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tu64 next_start = 0;\n\tbool ret = true;\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, 0, (u64)-1);\n\tread_unlock(&map_tree->lock);\n\t/* No chunk at all? Return false anyway */\n\tif (!em) {\n\t\tret = false;\n\t\tgoto out;\n\t}\n\twhile (em) {\n\t\tstruct map_lookup *map;\n\t\tint missing = 0;\n\t\tint max_tolerated;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tmax_tolerated =\n\t\t\tbtrfs_get_num_tolerated_disk_barrier_failures(\n\t\t\t\t\tmap->type);\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\t\tif (!dev || !dev->bdev ||\n\t\t\t    test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) ||\n\t\t\t    dev->last_flush_error)\n\t\t\t\tmissing++;\n\t\t\telse if (failing_dev && failing_dev == dev)\n\t\t\t\tmissing++;\n\t\t}\n\t\tif (missing > max_tolerated) {\n\t\t\tif (!failing_dev)\n\t\t\t\tbtrfs_warn(fs_info,\n\t\"chunk %llu missing %d devices, max tolerance is %d for writable mount\",\n\t\t\t\t   em->start, missing, max_tolerated);\n\t\t\tfree_extent_map(em);\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t\tnext_start = extent_map_end(em);\n\t\tfree_extent_map(em);\n\n\t\tread_lock(&map_tree->lock);\n\t\tem = lookup_extent_mapping(map_tree, next_start,\n\t\t\t\t\t   (u64)(-1) - next_start);\n\t\tread_unlock(&map_tree->lock);\n\t}\nout:\n\treturn ret;\n}\n\nstatic void readahead_tree_node_children(struct extent_buffer *node)\n{\n\tint i;\n\tconst int nr_items = btrfs_header_nritems(node);\n\n\tfor (i = 0; i < nr_items; i++)\n\t\tbtrfs_readahead_node_child(node, i);\n}\n\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tint ret;\n\tint slot;\n\tu64 total_dev = 0;\n\tu64 last_ra_node = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * uuid_mutex is needed only if we are mounting a sprout FS\n\t * otherwise we don't need it.\n\t */\n\tmutex_lock(&uuid_mutex);\n\n\t/*\n\t * It is possible for mount and umount to race in such a way that\n\t * we execute this code path, but open_fs_devices failed to clear\n\t * total_rw_bytes. We certainly want it cleared before reading the\n\t * device items, so clear it here.\n\t */\n\tfs_info->fs_devices->total_rw_bytes = 0;\n\n\t/*\n\t * Read all device items, and then all the chunk items. All\n\t * device items are found before any chunk item (their object id\n\t * is smaller than the lowest possible object id for a chunk\n\t * item - BTRFS_FIRST_CHUNK_TREE_OBJECTID).\n\t */\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = 0;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\twhile (1) {\n\t\tstruct extent_buffer *node;\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * The nodes on level 1 are not locked but we don't need to do\n\t\t * that during mount time as nothing else can access the tree\n\t\t */\n\t\tnode = path->nodes[1];\n\t\tif (node) {\n\t\t\tif (last_ra_node != node->start) {\n\t\t\t\treadahead_tree_node_children(node);\n\t\t\t\tlast_ra_node = node->start;\n\t\t\t}\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\t\tif (found_key.type == BTRFS_DEV_ITEM_KEY) {\n\t\t\tstruct btrfs_dev_item *dev_item;\n\t\t\tdev_item = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\t  struct btrfs_dev_item);\n\t\t\tret = read_one_dev(leaf, dev_item);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t\ttotal_dev++;\n\t\t} else if (found_key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tstruct btrfs_chunk *chunk;\n\n\t\t\t/*\n\t\t\t * We are only called at mount time, so no need to take\n\t\t\t * fs_info->chunk_mutex. Plus, to avoid lockdep warnings,\n\t\t\t * we always lock first fs_info->chunk_mutex before\n\t\t\t * acquiring any locks on the chunk tree. This is a\n\t\t\t * requirement for chunk allocation, see the comment on\n\t\t\t * top of btrfs_chunk_alloc() for details.\n\t\t\t */\n\t\t\tASSERT(!test_bit(BTRFS_FS_OPEN, &fs_info->flags));\n\t\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\t\tret = read_one_chunk(&found_key, leaf, chunk);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\n\n\t/*\n\t * After loading chunk tree, we've got all device information,\n\t * do another round of validation checks.\n\t */\n\tif (total_dev != fs_info->fs_devices->total_devices) {\n\t\tbtrfs_err(fs_info,\n\t   \"super_num_devices %llu mismatch with num_devices %llu found here\",\n\t\t\t  btrfs_super_num_devices(fs_info->super_copy),\n\t\t\t  total_dev);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tif (btrfs_super_total_bytes(fs_info->super_copy) <\n\t    fs_info->fs_devices->total_rw_bytes) {\n\t\tbtrfs_err(fs_info,\n\t\"super_total_bytes %llu mismatch with fs_devices total_rw_bytes %llu\",\n\t\t\t  btrfs_super_total_bytes(fs_info->super_copy),\n\t\t\t  fs_info->fs_devices->total_rw_bytes);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = 0;\nerror:\n\tmutex_unlock(&uuid_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nvoid btrfs_init_devices_late(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices, *seed_devs;\n\tstruct btrfs_device *device;\n\n\tfs_devices->fs_info = fs_info;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list)\n\t\tdevice->fs_info = fs_info;\n\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list)\n\t\t\tdevice->fs_info = fs_info;\n\n\t\tseed_devs->fs_info = fs_info;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n}\n\nstatic u64 btrfs_dev_stats_value(const struct extent_buffer *eb,\n\t\t\t\t const struct btrfs_dev_stats_item *ptr,\n\t\t\t\t int index)\n{\n\tu64 val;\n\n\tread_extent_buffer(eb, &val,\n\t\t\t   offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t    ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t   sizeof(val));\n\treturn val;\n}\n\nstatic void btrfs_set_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dev_stats_item *ptr,\n\t\t\t\t      int index, u64 val)\n{\n\twrite_extent_buffer(eb, &val,\n\t\t\t    offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t     ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t    sizeof(val));\n}\n\nstatic int btrfs_device_init_dev_stats(struct btrfs_device *device,\n\t\t\t\t       struct btrfs_path *path)\n{\n\tstruct btrfs_dev_stats_item *ptr;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_key key;\n\tint item_size;\n\tint i, ret, slot;\n\n\tif (!device->fs_info->dev_root)\n\t\treturn 0;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\tret = btrfs_search_slot(NULL, device->fs_info->dev_root, &key, path, 0, 0);\n\tif (ret) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tbtrfs_dev_stat_set(device, i, 0);\n\t\tdevice->dev_stats_valid = 1;\n\t\tbtrfs_release_path(path);\n\t\treturn ret < 0 ? ret : 0;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_stats_item);\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\tif (item_size >= (1 + i) * sizeof(__le64))\n\t\t\tbtrfs_dev_stat_set(device, i,\n\t\t\t\t\t   btrfs_dev_stats_value(eb, ptr, i));\n\t\telse\n\t\t\tbtrfs_dev_stat_set(device, i, 0);\n\t}\n\n\tdevice->dev_stats_valid = 1;\n\tbtrfs_dev_stat_print_on_load(device);\n\tbtrfs_release_path(path);\n\n\treturn 0;\n}\n\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices, *seed_devs;\n\tstruct btrfs_device *device;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tret = btrfs_device_init_dev_stats(device, path);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list) {\n\t\t\tret = btrfs_device_init_dev_stats(device, path);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int update_dev_stat_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_stats_item *ptr;\n\tint ret;\n\tint i;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\"error %d while searching for dev_stats item for device %s\",\n\t\t\t      ret, rcu_str_deref(device->name));\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size_nr(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t/* need to delete old one and insert a new one */\n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"delete too small dev_stats item for device %s failed %d\",\n\t\t\t\t      rcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t/* need to insert a new item */\n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"insert dev_stats item for device %s failed %d\",\n\t\t\t\trcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0], struct btrfs_dev_stats_item);\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_set_dev_stats_value(eb, ptr, i,\n\t\t\t\t\t  btrfs_dev_stat_read(device, i));\n\tbtrfs_mark_buffer_dirty(eb);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * called from commit_transaction. Writes all changed device stats to disk.\n */\nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tint stats_cnt;\n\tint ret = 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tstats_cnt = atomic_read(&device->dev_stats_ccnt);\n\t\tif (!device->dev_stats_valid || stats_cnt == 0)\n\t\t\tcontinue;\n\n\n\t\t/*\n\t\t * There is a LOAD-LOAD control dependency between the value of\n\t\t * dev_stats_ccnt and updating the on-disk values which requires\n\t\t * reading the in-memory counters. Such control dependencies\n\t\t * require explicit read memory barriers.\n\t\t *\n\t\t * This memory barriers pairs with smp_mb__before_atomic in\n\t\t * btrfs_dev_stat_inc/btrfs_dev_stat_set and with the full\n\t\t * barrier implied by atomic_xchg in\n\t\t * btrfs_dev_stats_read_and_reset\n\t\t */\n\t\tsmp_rmb();\n\n\t\tret = update_dev_stat_item(trans, device);\n\t\tif (!ret)\n\t\t\tatomic_sub(stats_cnt, &device->dev_stats_ccnt);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index)\n{\n\tbtrfs_dev_stat_inc(dev, index);\n\tbtrfs_dev_stat_print_on_error(dev);\n}\n\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev)\n{\n\tif (!dev->dev_stats_valid)\n\t\treturn;\n\tbtrfs_err_rl_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t\t\t   rcu_str_deref(dev->name),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tif (btrfs_dev_stat_read(dev, i) != 0)\n\t\t\tbreak;\n\tif (i == BTRFS_DEV_STAT_VALUES_MAX)\n\t\treturn; /* all values == 0, suppress message */\n\n\tbtrfs_info_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t       rcu_str_deref(dev->name),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_set(dev, i, 0);\n\t\t}\n\t\tbtrfs_info(fs_info, \"device stats zeroed by %s (%d)\",\n\t\t\t   current->comm, task_pid_nr(current));\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}\n\n/*\n * Update the size and bytes used for each device where it changed.  This is\n * delayed since we would otherwise get errors while writing out the\n * superblocks.\n *\n * Must be invoked during transaction commit.\n */\nvoid btrfs_commit_device_sizes(struct btrfs_transaction *trans)\n{\n\tstruct btrfs_device *curr, *next;\n\n\tASSERT(trans->state == TRANS_STATE_COMMIT_DOING);\n\n\tif (list_empty(&trans->dev_update_list))\n\t\treturn;\n\n\t/*\n\t * We don't need the device_list_mutex here.  This list is owned by the\n\t * transaction and the transaction must complete before the device is\n\t * released.\n\t */\n\tmutex_lock(&trans->fs_info->chunk_mutex);\n\tlist_for_each_entry_safe(curr, next, &trans->dev_update_list,\n\t\t\t\t post_commit_list) {\n\t\tlist_del_init(&curr->post_commit_list);\n\t\tcurr->commit_total_bytes = curr->disk_total_bytes;\n\t\tcurr->commit_bytes_used = curr->bytes_used;\n\t}\n\tmutex_unlock(&trans->fs_info->chunk_mutex);\n}\n\n/*\n * Multiplicity factor for simple profiles: DUP, RAID1-like and RAID10.\n */\nint btrfs_bg_type_to_factor(u64 flags)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(flags);\n\n\treturn btrfs_raid_array[index].ncopies;\n}\n\n\n\nstatic int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device boundary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tif (dev->zone_info) {\n\t\tu64 zone_size = dev->zone_info->zone_size;\n\n\t\tif (!IS_ALIGNED(physical_offset, zone_size) ||\n\t\t    !IS_ALIGNED(physical_len, zone_size)) {\n\t\t\tbtrfs_err(fs_info,\n\"zoned: dev extent devid %llu physical offset %llu len %llu is not aligned to device zone\",\n\t\t\t\t  devid, physical_offset, physical_len);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int verify_chunk_dev_extent_mapping(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tread_lock(&em_tree->lock);\n\tfor (node = rb_first_cached(&em_tree->map); node; node = rb_next(node)) {\n\t\tem = rb_entry(node, struct extent_map, rb_node);\n\t\tif (em->map_lookup->num_stripes !=\n\t\t    em->map_lookup->verified_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"chunk %llu has missing dev extent, have %d expect %d\",\n\t\t\t\t  em->start, em->map_lookup->verified_stripes,\n\t\t\t\t  em->map_lookup->num_stripes);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tread_unlock(&em_tree->lock);\n\treturn ret;\n}\n\n/*\n * Ensure that all dev extents are mapped to correct chunk, otherwise\n * later chunk allocation/free would cause unexpected behavior.\n *\n * NOTE: This will iterate through the whole device tree, which should be of\n * the same size level as the chunk tree.  This slightly increases mount time.\n */\nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tu64 prev_devid = 0;\n\tu64 prev_dev_ext_end = 0;\n\tint ret = 0;\n\n\t/*\n\t * We don't have a dev_root because we mounted with ignorebadroots and\n\t * failed to load the root, so we want to skip the verification in this\n\t * case for sure.\n\t *\n\t * However if the dev root is fine, but the tree itself is corrupted\n\t * we'd still fail to mount.  This verification is only to make sure\n\t * writes can happen safely, so instead just bypass this check\n\t * completely in the case of IGNOREBADROOTS.\n\t */\n\tif (btrfs_test_opt(fs_info, IGNOREBADROOTS))\n\t\treturn 0;\n\n\tkey.objectid = 1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/* No dev extents at all? Not good */\n\t\tif (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tstruct btrfs_dev_extent *dext;\n\t\tint slot = path->slots[0];\n\t\tu64 chunk_offset;\n\t\tu64 physical_offset;\n\t\tu64 physical_len;\n\t\tu64 devid;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\t\tdevid = key.objectid;\n\t\tphysical_offset = key.offset;\n\n\t\tdext = btrfs_item_ptr(leaf, slot, struct btrfs_dev_extent);\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(leaf, dext);\n\t\tphysical_len = btrfs_dev_extent_length(leaf, dext);\n\n\t\t/* Check if this dev extent overlaps with the previous one */\n\t\tif (devid == prev_devid && physical_offset < prev_dev_ext_end) {\n\t\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu overlap with previous dev extent end %llu\",\n\t\t\t\t  devid, physical_offset, prev_dev_ext_end);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = verify_one_dev_extent(fs_info, chunk_offset, devid,\n\t\t\t\t\t    physical_offset, physical_len);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tprev_devid = devid;\n\t\tprev_dev_ext_end = physical_offset + physical_len;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ensure all chunks have corresponding dev extents */\n\tret = verify_chunk_dev_extent_mapping(fs_info);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Check whether the given block group or device is pinned by any inode being\n * used as a swapfile.\n */\nbool btrfs_pinned_by_swapfile(struct btrfs_fs_info *fs_info, void *ptr)\n{\n\tstruct btrfs_swapfile_pin *sp;\n\tstruct rb_node *node;\n\n\tspin_lock(&fs_info->swapfile_pins_lock);\n\tnode = fs_info->swapfile_pins.rb_node;\n\twhile (node) {\n\t\tsp = rb_entry(node, struct btrfs_swapfile_pin, node);\n\t\tif (ptr < sp->ptr)\n\t\t\tnode = node->rb_left;\n\t\telse if (ptr > sp->ptr)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tspin_unlock(&fs_info->swapfile_pins_lock);\n\treturn node != NULL;\n}\n\nstatic int relocating_repair_kthread(void *data)\n{\n\tstruct btrfs_block_group *cache = (struct btrfs_block_group *)data;\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tu64 target;\n\tint ret = 0;\n\n\ttarget = cache->start;\n\tbtrfs_put_block_group(cache);\n\n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"zoned: skip relocating block group %llu to repair: EBUSY\",\n\t\t\t   target);\n\t\treturn -EBUSY;\n\t}\n\n\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\n\t/* Ensure block group still exists */\n\tcache = btrfs_lookup_block_group(fs_info, target);\n\tif (!cache)\n\t\tgoto out;\n\n\tif (!cache->relocating_repair)\n\t\tgoto out;\n\n\tret = btrfs_may_alloc_data_chunk(fs_info, target);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tbtrfs_info(fs_info,\n\t\t   \"zoned: relocating block group %llu to repair IO failure\",\n\t\t   target);\n\tret = btrfs_relocate_chunk(fs_info, target);\n\nout:\n\tif (cache)\n\t\tbtrfs_put_block_group(cache);\n\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\tbtrfs_exclop_finish(fs_info);\n\n\treturn ret;\n}\n\nint btrfs_repair_one_zone(struct btrfs_fs_info *fs_info, u64 logical)\n{\n\tstruct btrfs_block_group *cache;\n\n\t/* Do not attempt to repair in degraded state */\n\tif (btrfs_test_opt(fs_info, DEGRADED))\n\t\treturn 0;\n\n\tcache = btrfs_lookup_block_group(fs_info, logical);\n\tif (!cache)\n\t\treturn 0;\n\n\tspin_lock(&cache->lock);\n\tif (cache->relocating_repair) {\n\t\tspin_unlock(&cache->lock);\n\t\tbtrfs_put_block_group(cache);\n\t\treturn 0;\n\t}\n\tcache->relocating_repair = 1;\n\tspin_unlock(&cache->lock);\n\n\tkthread_run(relocating_repair_kthread, cache,\n\t\t    \"btrfs-relocating-repair\");\n\n\treturn 0;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * Copyright (C) 2007 Oracle.  All rights reserved.\n */\n\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/raid/pq.h>\n#include <linux/semaphore.h>\n#include <linux/uuid.h>\n#include <linux/list_sort.h>\n#include \"misc.h\"\n#include \"ctree.h\"\n#include \"extent_map.h\"\n#include \"disk-io.h\"\n#include \"transaction.h\"\n#include \"print-tree.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"async-thread.h\"\n#include \"check-integrity.h\"\n#include \"rcu-string.h\"\n#include \"dev-replace.h\"\n#include \"sysfs.h\"\n#include \"tree-checker.h\"\n#include \"space-info.h\"\n#include \"block-group.h\"\n#include \"discard.h\"\n#include \"zoned.h\"\n\nconst struct btrfs_raid_attr btrfs_raid_array[BTRFS_NR_RAID_TYPES] = {\n\t[BTRFS_RAID_RAID10] = {\n\t\t.sub_stripes\t= 2,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\t/* 0 == as many as possible */\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid10\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID10,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID10_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 2,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 2,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1C3] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 3,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 3,\n\t\t.ncopies\t= 3,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1c3\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1C3,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1C3_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID1C4] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 4,\n\t\t.devs_min\t= 4,\n\t\t.tolerated_failures = 3,\n\t\t.devs_increment\t= 4,\n\t\t.ncopies\t= 4,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid1c4\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID1C4,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID1C4_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_DUP] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 2,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 2,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"dup\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_DUP,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID0] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"raid0\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_SINGLE] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 1,\n\t\t.devs_min\t= 1,\n\t\t.tolerated_failures = 0,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 0,\n\t\t.raid_name\t= \"single\",\n\t\t.bg_flag\t= 0,\n\t\t.mindev_error\t= 0,\n\t},\n\t[BTRFS_RAID_RAID5] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 2,\n\t\t.tolerated_failures = 1,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 1,\n\t\t.raid_name\t= \"raid5\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID5,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID5_MIN_NOT_MET,\n\t},\n\t[BTRFS_RAID_RAID6] = {\n\t\t.sub_stripes\t= 1,\n\t\t.dev_stripes\t= 1,\n\t\t.devs_max\t= 0,\n\t\t.devs_min\t= 3,\n\t\t.tolerated_failures = 2,\n\t\t.devs_increment\t= 1,\n\t\t.ncopies\t= 1,\n\t\t.nparity        = 2,\n\t\t.raid_name\t= \"raid6\",\n\t\t.bg_flag\t= BTRFS_BLOCK_GROUP_RAID6,\n\t\t.mindev_error\t= BTRFS_ERROR_DEV_RAID6_MIN_NOT_MET,\n\t},\n};\n\n/*\n * Convert block group flags (BTRFS_BLOCK_GROUP_*) to btrfs_raid_types, which\n * can be used as index to access btrfs_raid_array[].\n */\nenum btrfs_raid_types __attribute_const__ btrfs_bg_flags_to_raid_index(u64 flags)\n{\n\tif (flags & BTRFS_BLOCK_GROUP_RAID10)\n\t\treturn BTRFS_RAID_RAID10;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1)\n\t\treturn BTRFS_RAID_RAID1;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1C3)\n\t\treturn BTRFS_RAID_RAID1C3;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID1C4)\n\t\treturn BTRFS_RAID_RAID1C4;\n\telse if (flags & BTRFS_BLOCK_GROUP_DUP)\n\t\treturn BTRFS_RAID_DUP;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID0)\n\t\treturn BTRFS_RAID_RAID0;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID5)\n\t\treturn BTRFS_RAID_RAID5;\n\telse if (flags & BTRFS_BLOCK_GROUP_RAID6)\n\t\treturn BTRFS_RAID_RAID6;\n\n\treturn BTRFS_RAID_SINGLE; /* BTRFS_BLOCK_GROUP_SINGLE */\n}\n\nconst char *btrfs_bg_type_to_raid_name(u64 flags)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(flags);\n\n\tif (index >= BTRFS_NR_RAID_TYPES)\n\t\treturn NULL;\n\n\treturn btrfs_raid_array[index].raid_name;\n}\n\n/*\n * Fill @buf with textual description of @bg_flags, no more than @size_buf\n * bytes including terminating null byte.\n */\nvoid btrfs_describe_block_groups(u64 bg_flags, char *buf, u32 size_buf)\n{\n\tint i;\n\tint ret;\n\tchar *bp = buf;\n\tu64 flags = bg_flags;\n\tu32 size_bp = size_buf;\n\n\tif (!flags) {\n\t\tstrcpy(bp, \"NONE\");\n\t\treturn;\n\t}\n\n#define DESCRIBE_FLAG(flag, desc)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (flags & (flag)) {\t\t\t\t\t\\\n\t\t\tret = snprintf(bp, size_bp, \"%s|\", (desc));\t\\\n\t\t\tif (ret < 0 || ret >= size_bp)\t\t\t\\\n\t\t\t\tgoto out_overflow;\t\t\t\\\n\t\t\tsize_bp -= ret;\t\t\t\t\t\\\n\t\t\tbp += ret;\t\t\t\t\t\\\n\t\t\tflags &= ~(flag);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_DATA, \"data\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_SYSTEM, \"system\");\n\tDESCRIBE_FLAG(BTRFS_BLOCK_GROUP_METADATA, \"metadata\");\n\n\tDESCRIBE_FLAG(BTRFS_AVAIL_ALLOC_BIT_SINGLE, \"single\");\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++)\n\t\tDESCRIBE_FLAG(btrfs_raid_array[i].bg_flag,\n\t\t\t      btrfs_raid_array[i].raid_name);\n#undef DESCRIBE_FLAG\n\n\tif (flags) {\n\t\tret = snprintf(bp, size_bp, \"0x%llx|\", flags);\n\t\tsize_bp -= ret;\n\t}\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last | */\n\n\t/*\n\t * The text is trimmed, it's up to the caller to provide sufficiently\n\t * large buffer\n\t */\nout_overflow:;\n}\n\nstatic int init_first_rw_device(struct btrfs_trans_handle *trans);\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info);\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev);\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *device);\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map);\n\n/*\n * Device locking\n * ==============\n *\n * There are several mutexes that protect manipulation of devices and low-level\n * structures like chunks but not block groups, extents or files\n *\n * uuid_mutex (global lock)\n * ------------------------\n * protects the fs_uuids list that tracks all per-fs fs_devices, resulting from\n * the SCAN_DEV ioctl registration or from mount either implicitly (the first\n * device) or requested by the device= mount option\n *\n * the mutex can be very coarse and can cover long-running operations\n *\n * protects: updates to fs_devices counters like missing devices, rw devices,\n * seeding, structure cloning, opening/closing devices at mount/umount time\n *\n * global::fs_devs - add, remove, updates to the global list\n *\n * does not protect: manipulation of the fs_devices::devices list in general\n * but in mount context it could be used to exclude list modifications by eg.\n * scan ioctl\n *\n * btrfs_device::name - renames (write side), read is RCU\n *\n * fs_devices::device_list_mutex (per-fs, with RCU)\n * ------------------------------------------------\n * protects updates to fs_devices::devices, ie. adding and deleting\n *\n * simple list traversal with read-only actions can be done with RCU protection\n *\n * may be used to exclude some operations from running concurrently without any\n * modifications to the list (see write_all_supers)\n *\n * Is not required at mount and close times, because our device list is\n * protected by the uuid_mutex at that point.\n *\n * balance_mutex\n * -------------\n * protects balance structures (status, state) and context accessed from\n * several places (internally, ioctl)\n *\n * chunk_mutex\n * -----------\n * protects chunks, adding or removing during allocation, trim or when a new\n * device is added/removed. Additionally it also protects post_commit_list of\n * individual devices, since they can be added to the transaction's\n * post_commit_list only with chunk_mutex held.\n *\n * cleaner_mutex\n * -------------\n * a big lock that is held by the cleaner thread and prevents running subvolume\n * cleaning together with relocation or delayed iputs\n *\n *\n * Lock nesting\n * ============\n *\n * uuid_mutex\n *   device_list_mutex\n *     chunk_mutex\n *   balance_mutex\n *\n *\n * Exclusive operations\n * ====================\n *\n * Maintains the exclusivity of the following operations that apply to the\n * whole filesystem and cannot run in parallel.\n *\n * - Balance (*)\n * - Device add\n * - Device remove\n * - Device replace (*)\n * - Resize\n *\n * The device operations (as above) can be in one of the following states:\n *\n * - Running state\n * - Paused state\n * - Completed state\n *\n * Only device operations marked with (*) can go into the Paused state for the\n * following reasons:\n *\n * - ioctl (only Balance can be Paused through ioctl)\n * - filesystem remounted as read-only\n * - filesystem unmounted and mounted as read-only\n * - system power-cycle and filesystem mounted as read-only\n * - filesystem or device errors leading to forced read-only\n *\n * The status of exclusive operation is set and cleared atomically.\n * During the course of Paused state, fs_info::exclusive_operation remains set.\n * A device operation in Paused or Running state can be canceled or resumed\n * either by ioctl (Balance only) or when remounted as read-write.\n * The exclusive status is cleared when the device operation is canceled or\n * completed.\n */\n\nDEFINE_MUTEX(uuid_mutex);\nstatic LIST_HEAD(fs_uuids);\nstruct list_head * __attribute_const__ btrfs_get_fs_uuids(void)\n{\n\treturn &fs_uuids;\n}\n\n/*\n * alloc_fs_devices - allocate struct btrfs_fs_devices\n * @fsid:\t\tif not NULL, copy the UUID to fs_devices::fsid\n * @metadata_fsid:\tif not NULL, copy the UUID to fs_devices::metadata_fsid\n *\n * Return a pointer to a new struct btrfs_fs_devices on success, or ERR_PTR().\n * The returned struct is not linked onto any lists and can be destroyed with\n * kfree() right away.\n */\nstatic struct btrfs_fs_devices *alloc_fs_devices(const u8 *fsid,\n\t\t\t\t\t\t const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devs;\n\n\tfs_devs = kzalloc(sizeof(*fs_devs), GFP_KERNEL);\n\tif (!fs_devs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&fs_devs->device_list_mutex);\n\n\tINIT_LIST_HEAD(&fs_devs->devices);\n\tINIT_LIST_HEAD(&fs_devs->alloc_list);\n\tINIT_LIST_HEAD(&fs_devs->fs_list);\n\tINIT_LIST_HEAD(&fs_devs->seed_list);\n\tif (fsid)\n\t\tmemcpy(fs_devs->fsid, fsid, BTRFS_FSID_SIZE);\n\n\tif (metadata_fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, metadata_fsid, BTRFS_FSID_SIZE);\n\telse if (fsid)\n\t\tmemcpy(fs_devs->metadata_uuid, fsid, BTRFS_FSID_SIZE);\n\n\treturn fs_devs;\n}\n\nvoid btrfs_free_device(struct btrfs_device *device)\n{\n\tWARN_ON(!list_empty(&device->post_commit_list));\n\trcu_string_free(device->name);\n\textent_io_tree_release(&device->alloc_state);\n\tbio_put(device->flush_bio);\n\tbtrfs_destroy_dev_zone_info(device);\n\tkfree(device);\n}\n\nstatic void free_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device;\n\tWARN_ON(fs_devices->opened);\n\twhile (!list_empty(&fs_devices->devices)) {\n\t\tdevice = list_entry(fs_devices->devices.next,\n\t\t\t\t    struct btrfs_device, dev_list);\n\t\tlist_del(&device->dev_list);\n\t\tbtrfs_free_device(device);\n\t}\n\tkfree(fs_devices);\n}\n\nvoid __exit btrfs_cleanup_fs_uuids(void)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\twhile (!list_empty(&fs_uuids)) {\n\t\tfs_devices = list_entry(fs_uuids.next,\n\t\t\t\t\tstruct btrfs_fs_devices, fs_list);\n\t\tlist_del(&fs_devices->fs_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n}\n\nstatic noinline struct btrfs_fs_devices *find_fsid(\n\t\tconst u8 *fsid, const u8 *metadata_fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tASSERT(fsid);\n\n\t/* Handle non-split brain cases */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (metadata_fsid) {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0\n\t\t\t    && memcmp(metadata_fsid, fs_devices->metadata_uuid,\n\t\t\t\t      BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t} else {\n\t\t\tif (memcmp(fsid, fs_devices->fsid, BTRFS_FSID_SIZE) == 0)\n\t\t\t\treturn fs_devices;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct btrfs_fs_devices *find_fsid_with_metadata_uuid(\n\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handle scanned device having completed its fsid change but\n\t * belonging to a fs_devices that was created by first scanning\n\t * a device which didn't have its fsid/metadata_uuid changed\n\t * at all and the CHANGING_FSID_V2 flag set.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (fs_devices->fsid_change &&\n\t\t    memcmp(disk_super->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\t/*\n\t * Handle scanned device having completed its fsid change but\n\t * belonging to a fs_devices that was created by a device that\n\t * has an outdated pair of fsid/metadata_uuid and\n\t * CHANGING_FSID_V2 flag set.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (fs_devices->fsid_change &&\n\t\t    memcmp(fs_devices->metadata_uuid,\n\t\t\t   fs_devices->fsid, BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(disk_super->metadata_uuid, fs_devices->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn find_fsid(disk_super->fsid, disk_super->metadata_uuid);\n}\n\n\nstatic int\nbtrfs_get_bdev_and_sb(const char *device_path, fmode_t flags, void *holder,\n\t\t      int flush, struct block_device **bdev,\n\t\t      struct btrfs_super_block **disk_super)\n{\n\tint ret;\n\n\t*bdev = blkdev_get_by_path(device_path, flags, holder);\n\n\tif (IS_ERR(*bdev)) {\n\t\tret = PTR_ERR(*bdev);\n\t\tgoto error;\n\t}\n\n\tif (flush)\n\t\tfilemap_write_and_wait((*bdev)->bd_inode->i_mapping);\n\tret = set_blocksize(*bdev, BTRFS_BDEV_BLOCKSIZE);\n\tif (ret) {\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\tinvalidate_bdev(*bdev);\n\t*disk_super = btrfs_read_dev_super(*bdev);\n\tif (IS_ERR(*disk_super)) {\n\t\tret = PTR_ERR(*disk_super);\n\t\tblkdev_put(*bdev, flags);\n\t\tgoto error;\n\t}\n\n\treturn 0;\n\nerror:\n\t*bdev = NULL;\n\treturn ret;\n}\n\nstatic bool device_path_matched(const char *path, struct btrfs_device *device)\n{\n\tint found;\n\n\trcu_read_lock();\n\tfound = strcmp(rcu_str_deref(device->name), path);\n\trcu_read_unlock();\n\n\treturn found == 0;\n}\n\n/*\n *  Search and remove all stale (devices which are not mounted) devices.\n *  When both inputs are NULL, it will search and release all stale devices.\n *  path:\tOptional. When provided will it release all unmounted devices\n *\t\tmatching this path only.\n *  skip_dev:\tOptional. Will skip this device when searching for the stale\n *\t\tdevices.\n *  Return:\t0 for success or if @path is NULL.\n * \t\t-EBUSY if @path is a mounted device.\n * \t\t-ENOENT if @path does not match any device in the list.\n */\nstatic int btrfs_free_stale_devices(const char *path,\n\t\t\t\t     struct btrfs_device *skip_device)\n{\n\tstruct btrfs_fs_devices *fs_devices, *tmp_fs_devices;\n\tstruct btrfs_device *device, *tmp_device;\n\tint ret = 0;\n\n\tif (path)\n\t\tret = -ENOENT;\n\n\tlist_for_each_entry_safe(fs_devices, tmp_fs_devices, &fs_uuids, fs_list) {\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_for_each_entry_safe(device, tmp_device,\n\t\t\t\t\t &fs_devices->devices, dev_list) {\n\t\t\tif (skip_device && skip_device == device)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device->name)\n\t\t\t\tcontinue;\n\t\t\tif (path && !device_path_matched(path, device))\n\t\t\t\tcontinue;\n\t\t\tif (fs_devices->opened) {\n\t\t\t\t/* for an already deleted device return 0 */\n\t\t\t\tif (path && ret != 0)\n\t\t\t\t\tret = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* delete the stale device */\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\n\t\t\tret = 0;\n\t\t}\n\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t\tif (fs_devices->num_devices == 0) {\n\t\t\tbtrfs_sysfs_remove_fsid(fs_devices);\n\t\t\tlist_del(&fs_devices->fs_list);\n\t\t\tfree_fs_devices(fs_devices);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n/*\n * This is only used on mount, and we are protected from competing things\n * messing with our fs_devices by the uuid_mutex, thus we do not need the\n * fs_devices->device_list_mutex here.\n */\nstatic int btrfs_open_one_device(struct btrfs_fs_devices *fs_devices,\n\t\t\tstruct btrfs_device *device, fmode_t flags,\n\t\t\tvoid *holder)\n{\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tint ret;\n\n\tif (device->bdev)\n\t\treturn -EINVAL;\n\tif (!device->name)\n\t\treturn -EINVAL;\n\n\tret = btrfs_get_bdev_and_sb(device->name->str, flags, holder, 1,\n\t\t\t\t    &bdev, &disk_super);\n\tif (ret)\n\t\treturn ret;\n\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tif (devid != device->devid)\n\t\tgoto error_free_page;\n\n\tif (memcmp(device->uuid, disk_super->dev_item.uuid, BTRFS_UUID_SIZE))\n\t\tgoto error_free_page;\n\n\tdevice->generation = btrfs_super_generation(disk_super);\n\n\tif (btrfs_super_flags(disk_super) & BTRFS_SUPER_FLAG_SEEDING) {\n\t\tif (btrfs_super_incompat_flags(disk_super) &\n\t\t    BTRFS_FEATURE_INCOMPAT_METADATA_UUID) {\n\t\t\tpr_err(\n\t\t\"BTRFS: Invalid seeding and uuid-changed device detected\\n\");\n\t\t\tgoto error_free_page;\n\t\t}\n\n\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\tfs_devices->seeding = true;\n\t} else {\n\t\tif (bdev_read_only(bdev))\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\telse\n\t\t\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = true;\n\n\tdevice->bdev = bdev;\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tdevice->mode = flags;\n\n\tfs_devices->open_devices++;\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tfs_devices->rw_devices++;\n\t\tlist_add_tail(&device->dev_alloc_list, &fs_devices->alloc_list);\n\t}\n\tbtrfs_release_disk_super(disk_super);\n\n\treturn 0;\n\nerror_free_page:\n\tbtrfs_release_disk_super(disk_super);\n\tblkdev_put(bdev, flags);\n\n\treturn -EINVAL;\n}\n\n/*\n * Handle scanned device having its CHANGING_FSID_V2 flag set and the fs_devices\n * being created with a disk that has already completed its fsid change. Such\n * disk can belong to an fs which has its FSID changed or to one which doesn't.\n * Handle both cases here.\n */\nstatic struct btrfs_fs_devices *find_fsid_inprogress(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 && !fs_devices->fsid_change) {\n\t\t\treturn fs_devices;\n\t\t}\n\t}\n\n\treturn find_fsid(disk_super->fsid, NULL);\n}\n\n\nstatic struct btrfs_fs_devices *find_fsid_changed(\n\t\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handles the case where scanned device is part of an fs that had\n\t * multiple successful changes of FSID but currently device didn't\n\t * observe it. Meaning our fsid will be different than theirs. We need\n\t * to handle two subcases :\n\t *  1 - The fs still continues to have different METADATA/FSID uuids.\n\t *  2 - The fs is switched back to its original FSID (METADATA/FSID\n\t *  are equal).\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\t/* Changed UUIDs */\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) != 0)\n\t\t\treturn fs_devices;\n\n\t\t/* Unchanged UUIDs */\n\t\tif (memcmp(fs_devices->metadata_uuid, fs_devices->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    memcmp(fs_devices->fsid, disk_super->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) == 0)\n\t\t\treturn fs_devices;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct btrfs_fs_devices *find_fsid_reverted_metadata(\n\t\t\t\tstruct btrfs_super_block *disk_super)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\t/*\n\t * Handle the case where the scanned device is part of an fs whose last\n\t * metadata UUID change reverted it to the original FSID. At the same\n\t * time * fs_devices was first created by another constitutent device\n\t * which didn't fully observe the operation. This results in an\n\t * btrfs_fs_devices created with metadata/fsid different AND\n\t * btrfs_fs_devices::fsid_change set AND the metadata_uuid of the\n\t * fs_devices equal to the FSID of the disk.\n\t */\n\tlist_for_each_entry(fs_devices, &fs_uuids, fs_list) {\n\t\tif (memcmp(fs_devices->fsid, fs_devices->metadata_uuid,\n\t\t\t   BTRFS_FSID_SIZE) != 0 &&\n\t\t    memcmp(fs_devices->metadata_uuid, disk_super->fsid,\n\t\t\t   BTRFS_FSID_SIZE) == 0 &&\n\t\t    fs_devices->fsid_change)\n\t\t\treturn fs_devices;\n\t}\n\n\treturn NULL;\n}\n/*\n * Add new device to list of registered devices\n *\n * Returns:\n * device pointer which was just added or updated when successful\n * error pointer when failed\n */\nstatic noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid)\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\telse\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid_with_metadata_uuid(disk_super);\n\t} else {\n\t\tfs_devices = find_fsid_reverted_metadata(disk_super);\n\t\tif (!fs_devices)\n\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\n\t\t\tif (has_metadata_uuid)\n\t\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t       disk_super->metadata_uuid,\n\t\t\t\t       BTRFS_FSID_SIZE);\n\t\t\telse\n\t\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t       disk_super->fsid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\n\t\"BTRFS: device label %s devid %llu transid %llu %s scanned by %s (%d)\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path,\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\t\telse\n\t\t\tpr_info(\n\t\"BTRFS: device fsid %pU devid %llu transid %llu %s scanned by %s (%d)\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path,\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tint error;\n\t\t\tdev_t path_dev;\n\n\t\t\terror = lookup_bdev(path, &path_dev);\n\t\t\tif (error) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_PTR(error);\n\t\t\t}\n\n\t\t\tif (device->bdev->bd_dev != path_dev) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\t/*\n\t\t\t\t * device->fs_info may not be reliable here, so\n\t\t\t\t * pass in a NULL instead. This avoids a\n\t\t\t\t * possible use-after-free when the fs_info and\n\t\t\t\t * fs_info->sb are already torn down.\n\t\t\t\t */\n\t\t\t\tbtrfs_warn_in_rcu(NULL,\n\t\"duplicate device %s devid %llu generation %llu scanned by %s (%d)\",\n\t\t\t\t\t\t  path, devid, found_transid,\n\t\t\t\t\t\t  current->comm,\n\t\t\t\t\t\t  task_pid_nr(current));\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\"devid %llu device path %s changed to %s scanned by %s (%d)\",\n\t\t\t\t\t  devid, rcu_str_deref(device->name),\n\t\t\t\t\t  path, current->comm,\n\t\t\t\t\t  task_pid_nr(current));\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}\n\nstatic struct btrfs_fs_devices *clone_fs_devices(struct btrfs_fs_devices *orig)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *orig_dev;\n\tint ret = 0;\n\n\tfs_devices = alloc_fs_devices(orig->fsid, NULL);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tmutex_lock(&orig->device_list_mutex);\n\tfs_devices->total_devices = orig->total_devices;\n\n\tlist_for_each_entry(orig_dev, &orig->devices, dev_list) {\n\t\tstruct rcu_string *name;\n\n\t\tdevice = btrfs_alloc_device(NULL, &orig_dev->devid,\n\t\t\t\t\t    orig_dev->uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tret = PTR_ERR(device);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * This is ok to do without rcu read locked because we hold the\n\t\t * uuid mutex so nothing we touch in here is going to disappear.\n\t\t */\n\t\tif (orig_dev->name) {\n\t\t\tname = rcu_string_strdup(orig_dev->name->str,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!name) {\n\t\t\t\tbtrfs_free_device(device);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\trcu_assign_pointer(device->name, name);\n\t\t}\n\n\t\tlist_add(&device->dev_list, &fs_devices->devices);\n\t\tdevice->fs_devices = fs_devices;\n\t\tfs_devices->num_devices++;\n\t}\n\tmutex_unlock(&orig->device_list_mutex);\n\treturn fs_devices;\nerror:\n\tmutex_unlock(&orig->device_list_mutex);\n\tfree_fs_devices(fs_devices);\n\treturn ERR_PTR(ret);\n}\n\nstatic void __btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      struct btrfs_device **latest_dev)\n{\n\tstruct btrfs_device *device, *next;\n\n\t/* This is the initialized path, it is safe to release the devices. */\n\tlist_for_each_entry_safe(device, next, &fs_devices->devices, dev_list) {\n\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state)) {\n\t\t\tif (!test_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t      &device->dev_state) &&\n\t\t\t    !test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t      &device->dev_state) &&\n\t\t\t    (!*latest_dev ||\n\t\t\t     device->generation > (*latest_dev)->generation)) {\n\t\t\t\t*latest_dev = device;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We have already validated the presence of BTRFS_DEV_REPLACE_DEVID,\n\t\t * in btrfs_init_dev_replace() so just continue.\n\t\t */\n\t\tif (device->devid == BTRFS_DEV_REPLACE_DEVID)\n\t\t\tcontinue;\n\n\t\tif (device->bdev) {\n\t\t\tblkdev_put(device->bdev, device->mode);\n\t\t\tdevice->bdev = NULL;\n\t\t\tfs_devices->open_devices--;\n\t\t}\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tlist_del_init(&device->dev_alloc_list);\n\t\t\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\t\t\tfs_devices->rw_devices--;\n\t\t}\n\t\tlist_del_init(&device->dev_list);\n\t\tfs_devices->num_devices--;\n\t\tbtrfs_free_device(device);\n\t}\n\n}\n\n/*\n * After we have read the system tree and know devids belonging to this\n * filesystem, remove the device which does not belong there.\n */\nvoid btrfs_free_extra_devids(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *latest_dev = NULL;\n\tstruct btrfs_fs_devices *seed_dev;\n\n\tmutex_lock(&uuid_mutex);\n\t__btrfs_free_extra_devids(fs_devices, &latest_dev);\n\n\tlist_for_each_entry(seed_dev, &fs_devices->seed_list, seed_list)\n\t\t__btrfs_free_extra_devids(seed_dev, &latest_dev);\n\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic void btrfs_close_bdev(struct btrfs_device *device)\n{\n\tif (!device->bdev)\n\t\treturn;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tsync_blockdev(device->bdev);\n\t\tinvalidate_bdev(device->bdev);\n\t}\n\n\tblkdev_put(device->bdev, device->mode);\n}\n\nstatic void btrfs_close_one_device(struct btrfs_device *device)\n{\n\tstruct btrfs_fs_devices *fs_devices = device->fs_devices;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    device->devid != BTRFS_DEV_REPLACE_DEVID) {\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tfs_devices->rw_devices--;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tbtrfs_close_bdev(device);\n\tif (device->bdev) {\n\t\tfs_devices->open_devices--;\n\t\tdevice->bdev = NULL;\n\t}\n\tclear_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tbtrfs_destroy_dev_zone_info(device);\n\n\tdevice->fs_info = NULL;\n\tatomic_set(&device->dev_stats_ccnt, 0);\n\textent_io_tree_release(&device->alloc_state);\n\n\t/* Verify the device is back in a pristine state  */\n\tASSERT(!test_bit(BTRFS_DEV_STATE_FLUSH_SENT, &device->dev_state));\n\tASSERT(!test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state));\n\tASSERT(list_empty(&device->dev_alloc_list));\n\tASSERT(list_empty(&device->post_commit_list));\n\tASSERT(atomic_read(&device->reada_in_flight) == 0);\n}\n\nstatic void close_fs_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tstruct btrfs_device *device, *tmp;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\tif (--fs_devices->opened > 0)\n\t\treturn;\n\n\tlist_for_each_entry_safe(device, tmp, &fs_devices->devices, dev_list)\n\t\tbtrfs_close_one_device(device);\n\n\tWARN_ON(fs_devices->open_devices);\n\tWARN_ON(fs_devices->rw_devices);\n\tfs_devices->opened = 0;\n\tfs_devices->seeding = false;\n\tfs_devices->fs_info = NULL;\n}\n\nvoid btrfs_close_devices(struct btrfs_fs_devices *fs_devices)\n{\n\tLIST_HEAD(list);\n\tstruct btrfs_fs_devices *tmp;\n\n\tmutex_lock(&uuid_mutex);\n\tclose_fs_devices(fs_devices);\n\tif (!fs_devices->opened)\n\t\tlist_splice_init(&fs_devices->seed_list, &list);\n\n\tlist_for_each_entry_safe(fs_devices, tmp, &list, seed_list) {\n\t\tclose_fs_devices(fs_devices);\n\t\tlist_del(&fs_devices->seed_list);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\tmutex_unlock(&uuid_mutex);\n}\n\nstatic int open_fs_devices(struct btrfs_fs_devices *fs_devices,\n\t\t\t\tfmode_t flags, void *holder)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_device *latest_dev = NULL;\n\tstruct btrfs_device *tmp_device;\n\n\tflags |= FMODE_EXCL;\n\n\tlist_for_each_entry_safe(device, tmp_device, &fs_devices->devices,\n\t\t\t\t dev_list) {\n\t\tint ret;\n\n\t\tret = btrfs_open_one_device(fs_devices, device, flags, holder);\n\t\tif (ret == 0 &&\n\t\t    (!latest_dev || device->generation > latest_dev->generation)) {\n\t\t\tlatest_dev = device;\n\t\t} else if (ret == -ENODATA) {\n\t\t\tfs_devices->num_devices--;\n\t\t\tlist_del(&device->dev_list);\n\t\t\tbtrfs_free_device(device);\n\t\t}\n\t}\n\tif (fs_devices->open_devices == 0)\n\t\treturn -EINVAL;\n\n\tfs_devices->opened = 1;\n\tfs_devices->latest_bdev = latest_dev->bdev;\n\tfs_devices->total_rw_bytes = 0;\n\tfs_devices->chunk_alloc_policy = BTRFS_CHUNK_ALLOC_REGULAR;\n\tfs_devices->read_policy = BTRFS_READ_POLICY_PID;\n\n\treturn 0;\n}\n\nstatic int devid_cmp(void *priv, const struct list_head *a,\n\t\t     const struct list_head *b)\n{\n\tconst struct btrfs_device *dev1, *dev2;\n\n\tdev1 = list_entry(a, struct btrfs_device, dev_list);\n\tdev2 = list_entry(b, struct btrfs_device, dev_list);\n\n\tif (dev1->devid < dev2->devid)\n\t\treturn -1;\n\telse if (dev1->devid > dev2->devid)\n\t\treturn 1;\n\treturn 0;\n}\n\nint btrfs_open_devices(struct btrfs_fs_devices *fs_devices,\n\t\t       fmode_t flags, void *holder)\n{\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\t/*\n\t * The device_list_mutex cannot be taken here in case opening the\n\t * underlying device takes further locks like open_mutex.\n\t *\n\t * We also don't need the lock here as this is called during mount and\n\t * exclusion is provided by uuid_mutex\n\t */\n\n\tif (fs_devices->opened) {\n\t\tfs_devices->opened++;\n\t\tret = 0;\n\t} else {\n\t\tlist_sort(NULL, &fs_devices->devices, devid_cmp);\n\t\tret = open_fs_devices(fs_devices, flags, holder);\n\t}\n\n\treturn ret;\n}\n\nvoid btrfs_release_disk_super(struct btrfs_super_block *super)\n{\n\tstruct page *page = virt_to_page(super);\n\n\tput_page(page);\n}\n\nstatic struct btrfs_super_block *btrfs_read_disk_super(struct block_device *bdev,\n\t\t\t\t\t\t       u64 bytenr, u64 bytenr_orig)\n{\n\tstruct btrfs_super_block *disk_super;\n\tstruct page *page;\n\tvoid *p;\n\tpgoff_t index;\n\n\t/* make sure our super fits in the device */\n\tif (bytenr + PAGE_SIZE >= i_size_read(bdev->bd_inode))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* make sure our super fits in the page */\n\tif (sizeof(*disk_super) > PAGE_SIZE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* make sure our super doesn't straddle pages on disk */\n\tindex = bytenr >> PAGE_SHIFT;\n\tif ((bytenr + sizeof(*disk_super) - 1) >> PAGE_SHIFT != index)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* pull in the page with our super */\n\tpage = read_cache_page_gfp(bdev->bd_inode->i_mapping, index, GFP_KERNEL);\n\n\tif (IS_ERR(page))\n\t\treturn ERR_CAST(page);\n\n\tp = page_address(page);\n\n\t/* align our pointer to the offset of the super block */\n\tdisk_super = p + offset_in_page(bytenr);\n\n\tif (btrfs_super_bytenr(disk_super) != bytenr_orig ||\n\t    btrfs_super_magic(disk_super) != BTRFS_MAGIC) {\n\t\tbtrfs_release_disk_super(p);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (disk_super->label[0] && disk_super->label[BTRFS_LABEL_SIZE - 1])\n\t\tdisk_super->label[BTRFS_LABEL_SIZE - 1] = 0;\n\n\treturn disk_super;\n}\n\nint btrfs_forget_devices(const char *path)\n{\n\tint ret;\n\n\tmutex_lock(&uuid_mutex);\n\tret = btrfs_free_stale_devices(strlen(path) ? path : NULL, NULL);\n\tmutex_unlock(&uuid_mutex);\n\n\treturn ret;\n}\n\n/*\n * Look for a btrfs signature on a device. This may be called out of the mount path\n * and we are not allowed to call set_blocksize during the scan. The superblock\n * is read via pagecache\n */\nstruct btrfs_device *btrfs_scan_one_device(const char *path, fmode_t flags,\n\t\t\t\t\t   void *holder)\n{\n\tstruct btrfs_super_block *disk_super;\n\tbool new_device_added = false;\n\tstruct btrfs_device *device = NULL;\n\tstruct block_device *bdev;\n\tu64 bytenr, bytenr_orig;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\n\t/*\n\t * we would like to check all the supers, but that would make\n\t * a btrfs mount succeed after a mkfs from a different FS.\n\t * So, we need to add a special mount option to scan for\n\t * later supers, using BTRFS_SUPER_MIRROR_MAX instead\n\t */\n\tflags |= FMODE_EXCL;\n\n\tbdev = blkdev_get_by_path(path, flags, holder);\n\tif (IS_ERR(bdev))\n\t\treturn ERR_CAST(bdev);\n\n\tbytenr_orig = btrfs_sb_offset(0);\n\tret = btrfs_sb_log_location_bdev(bdev, 0, READ, &bytenr);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdisk_super = btrfs_read_disk_super(bdev, bytenr, bytenr_orig);\n\tif (IS_ERR(disk_super)) {\n\t\tdevice = ERR_CAST(disk_super);\n\t\tgoto error_bdev_put;\n\t}\n\n\tdevice = device_list_add(path, disk_super, &new_device_added);\n\tif (!IS_ERR(device)) {\n\t\tif (new_device_added)\n\t\t\tbtrfs_free_stale_devices(path, device);\n\t}\n\n\tbtrfs_release_disk_super(disk_super);\n\nerror_bdev_put:\n\tblkdev_put(bdev, flags);\n\n\treturn device;\n}\n\n/*\n * Try to find a chunk that intersects [start, start + len] range and when one\n * such is found, record the end of it in *start\n */\nstatic bool contains_pending_extent(struct btrfs_device *device, u64 *start,\n\t\t\t\t    u64 len)\n{\n\tu64 physical_start, physical_end;\n\n\tlockdep_assert_held(&device->fs_info->chunk_mutex);\n\n\tif (!find_first_extent_bit(&device->alloc_state, *start,\n\t\t\t\t   &physical_start, &physical_end,\n\t\t\t\t   CHUNK_ALLOCATED, NULL)) {\n\n\t\tif (in_range(physical_start, *start, len) ||\n\t\t    in_range(*start, physical_start,\n\t\t\t     physical_end - physical_start)) {\n\t\t\t*start = physical_end + 1;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic u64 dev_extent_search_start(struct btrfs_device *device, u64 start)\n{\n\tswitch (device->fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\t/*\n\t\t * We don't want to overwrite the superblock on the drive nor\n\t\t * any area used by the boot loader (grub for example), so we\n\t\t * make sure to start at an offset of at least 1MB.\n\t\t */\n\t\treturn max_t(u64, start, SZ_1M);\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\t/*\n\t\t * We don't care about the starting region like regular\n\t\t * allocator, because we anyway use/reserve the first two zones\n\t\t * for superblock logging.\n\t\t */\n\t\treturn ALIGN(start, device->zone_info->zone_size);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic bool dev_extent_hole_check_zoned(struct btrfs_device *device,\n\t\t\t\t\tu64 *hole_start, u64 *hole_size,\n\t\t\t\t\tu64 num_bytes)\n{\n\tu64 zone_size = device->zone_info->zone_size;\n\tu64 pos;\n\tint ret;\n\tbool changed = false;\n\n\tASSERT(IS_ALIGNED(*hole_start, zone_size));\n\n\twhile (*hole_size > 0) {\n\t\tpos = btrfs_find_allocatable_zones(device, *hole_start,\n\t\t\t\t\t\t   *hole_start + *hole_size,\n\t\t\t\t\t\t   num_bytes);\n\t\tif (pos != *hole_start) {\n\t\t\t*hole_size = *hole_start + *hole_size - pos;\n\t\t\t*hole_start = pos;\n\t\t\tchanged = true;\n\t\t\tif (*hole_size < num_bytes)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tret = btrfs_ensure_empty_zones(device, pos, num_bytes);\n\n\t\t/* Range is ensured to be empty */\n\t\tif (!ret)\n\t\t\treturn changed;\n\n\t\t/* Given hole range was invalid (outside of device) */\n\t\tif (ret == -ERANGE) {\n\t\t\t*hole_start += *hole_size;\n\t\t\t*hole_size = 0;\n\t\t\treturn true;\n\t\t}\n\n\t\t*hole_start += zone_size;\n\t\t*hole_size -= zone_size;\n\t\tchanged = true;\n\t}\n\n\treturn changed;\n}\n\n/**\n * dev_extent_hole_check - check if specified hole is suitable for allocation\n * @device:\tthe device which we have the hole\n * @hole_start: starting position of the hole\n * @hole_size:\tthe size of the hole\n * @num_bytes:\tthe size of the free space that we need\n *\n * This function may modify @hole_start and @hole_size to reflect the suitable\n * position for allocation. Returns 1 if hole position is updated, 0 otherwise.\n */\nstatic bool dev_extent_hole_check(struct btrfs_device *device, u64 *hole_start,\n\t\t\t\t  u64 *hole_size, u64 num_bytes)\n{\n\tbool changed = false;\n\tu64 hole_end = *hole_start + *hole_size;\n\n\tfor (;;) {\n\t\t/*\n\t\t * Check before we set max_hole_start, otherwise we could end up\n\t\t * sending back this offset anyway.\n\t\t */\n\t\tif (contains_pending_extent(device, hole_start, *hole_size)) {\n\t\t\tif (hole_end >= *hole_start)\n\t\t\t\t*hole_size = hole_end - *hole_start;\n\t\t\telse\n\t\t\t\t*hole_size = 0;\n\t\t\tchanged = true;\n\t\t}\n\n\t\tswitch (device->fs_devices->chunk_alloc_policy) {\n\t\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\t\t/* No extra check */\n\t\t\tbreak;\n\t\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\t\tif (dev_extent_hole_check_zoned(device, hole_start,\n\t\t\t\t\t\t\thole_size, num_bytes)) {\n\t\t\t\tchanged = true;\n\t\t\t\t/*\n\t\t\t\t * The changed hole can contain pending extent.\n\t\t\t\t * Loop again to check that.\n\t\t\t\t */\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tbreak;\n\t}\n\n\treturn changed;\n}\n\n/*\n * find_free_dev_extent_start - find free space in the specified device\n * @device:\t  the device which we search the free space in\n * @num_bytes:\t  the size of the free space that we need\n * @search_start: the position from which to begin the search\n * @start:\t  store the start of the free space.\n * @len:\t  the size of the free space. that we find, or the size\n *\t\t  of the max free space if we don't find suitable free space\n *\n * this uses a pretty simple search, the expectation is that it is\n * called very infrequently and that a given device has a small number\n * of extents\n *\n * @start is used to store the start of the free space if we find. But if we\n * don't find suitable free space, it will be used to store the start position\n * of the max free space.\n *\n * @len is used to store the size of the free space that we find.\n * But if we don't find suitable free space, it is used to store the size of\n * the max free space.\n *\n * NOTE: This function will search *commit* root of device tree, and does extra\n * check to ensure dev extents are not double allocated.\n * This makes the function safe to allocate dev extents but may not report\n * correct usable device space, as device extent freed in current transaction\n * is not reported as available.\n */\nstatic int find_free_dev_extent_start(struct btrfs_device *device,\n\t\t\t\tu64 num_bytes, u64 search_start, u64 *start,\n\t\t\t\tu64 *len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_dev_extent *dev_extent;\n\tstruct btrfs_path *path;\n\tu64 hole_size;\n\tu64 max_hole_start;\n\tu64 max_hole_size;\n\tu64 extent_end;\n\tu64 search_end = device->total_bytes;\n\tint ret;\n\tint slot;\n\tstruct extent_buffer *l;\n\n\tsearch_start = dev_extent_search_start(device, search_start);\n\n\tWARN_ON(device->zone_info &&\n\t\t!IS_ALIGNED(num_bytes, device->zone_info->zone_size));\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmax_hole_start = search_start;\n\tmax_hole_size = 0;\n\nagain:\n\tif (search_start >= search_end ||\n\t\ttest_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tpath->reada = READA_FORWARD;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\tkey.objectid = device->devid;\n\tkey.offset = search_start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tret = btrfs_search_backwards(root, &key, path);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twhile (1) {\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(l)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(l, &key, slot);\n\n\t\tif (key.objectid < device->devid)\n\t\t\tgoto next;\n\n\t\tif (key.objectid > device->devid)\n\t\t\tbreak;\n\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tgoto next;\n\n\t\tif (key.offset > search_start) {\n\t\t\thole_size = key.offset - search_start;\n\t\t\tdev_extent_hole_check(device, &search_start, &hole_size,\n\t\t\t\t\t      num_bytes);\n\n\t\t\tif (hole_size > max_hole_size) {\n\t\t\t\tmax_hole_start = search_start;\n\t\t\t\tmax_hole_size = hole_size;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If this free space is greater than which we need,\n\t\t\t * it must be the max free space that we have found\n\t\t\t * until now, so max_hole_start must point to the start\n\t\t\t * of this free space and the length of this free space\n\t\t\t * is stored in max_hole_size. Thus, we return\n\t\t\t * max_hole_start and max_hole_size and go back to the\n\t\t\t * caller.\n\t\t\t */\n\t\t\tif (hole_size >= num_bytes) {\n\t\t\t\tret = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\textent_end = key.offset + btrfs_dev_extent_length(l,\n\t\t\t\t\t\t\t\t  dev_extent);\n\t\tif (extent_end > search_start)\n\t\t\tsearch_start = extent_end;\nnext:\n\t\tpath->slots[0]++;\n\t\tcond_resched();\n\t}\n\n\t/*\n\t * At this point, search_start should be the end of\n\t * allocated dev extents, and when shrinking the device,\n\t * search_end may be smaller than search_start.\n\t */\n\tif (search_end > search_start) {\n\t\thole_size = search_end - search_start;\n\t\tif (dev_extent_hole_check(device, &search_start, &hole_size,\n\t\t\t\t\t  num_bytes)) {\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (hole_size > max_hole_size) {\n\t\t\tmax_hole_start = search_start;\n\t\t\tmax_hole_size = hole_size;\n\t\t}\n\t}\n\n\t/* See above. */\n\tif (max_hole_size < num_bytes)\n\t\tret = -ENOSPC;\n\telse\n\t\tret = 0;\n\nout:\n\tbtrfs_free_path(path);\n\t*start = max_hole_start;\n\tif (len)\n\t\t*len = max_hole_size;\n\treturn ret;\n}\n\nint find_free_dev_extent(struct btrfs_device *device, u64 num_bytes,\n\t\t\t u64 *start, u64 *len)\n{\n\t/* FIXME use last free of some kind */\n\treturn find_free_dev_extent_start(device, num_bytes, 0, start, len);\n}\n\nstatic int btrfs_free_dev_extent(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_device *device,\n\t\t\t  u64 start, u64 *dev_extent_len)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf = NULL;\n\tstruct btrfs_dev_extent *extent = NULL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = device->devid;\n\tkey.offset = start;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\nagain:\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret > 0) {\n\t\tret = btrfs_previous_item(root, path, key.objectid,\n\t\t\t\t\t  BTRFS_DEV_EXTENT_KEY);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t\tBUG_ON(found_key.offset > start || found_key.offset +\n\t\t       btrfs_dev_extent_length(leaf, extent) < start);\n\t\tkey = found_key;\n\t\tbtrfs_release_path(path);\n\t\tgoto again;\n\t} else if (ret == 0) {\n\t\tleaf = path->nodes[0];\n\t\textent = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\tstruct btrfs_dev_extent);\n\t} else {\n\t\tgoto out;\n\t}\n\n\t*dev_extent_len = btrfs_dev_extent_length(leaf, extent);\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret == 0)\n\t\tset_bit(BTRFS_TRANS_HAVE_FREE_BGS, &trans->transaction->flags);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic u64 find_next_chunk(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *n;\n\tu64 ret = 0;\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tn = rb_last(&em_tree->map.rb_root);\n\tif (n) {\n\t\tem = rb_entry(n, struct extent_map, rb_node);\n\t\tret = em->start + em->len;\n\t}\n\tread_unlock(&em_tree->lock);\n\n\treturn ret;\n}\n\nstatic noinline int find_next_devid(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 *devid_ret)\n{\n\tint ret;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct btrfs_path *path;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = (u64)-1;\n\n\tret = btrfs_search_slot(NULL, fs_info->chunk_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\n\tif (ret == 0) {\n\t\t/* Corruption */\n\t\tbtrfs_err(fs_info, \"corrupted chunk tree devid -1 matched\");\n\t\tret = -EUCLEAN;\n\t\tgoto error;\n\t}\n\n\tret = btrfs_previous_item(fs_info->chunk_root, path,\n\t\t\t\t  BTRFS_DEV_ITEMS_OBJECTID,\n\t\t\t\t  BTRFS_DEV_ITEM_KEY);\n\tif (ret) {\n\t\t*devid_ret = 1;\n\t} else {\n\t\tbtrfs_item_key_to_cpu(path->nodes[0], &found_key,\n\t\t\t\t      path->slots[0]);\n\t\t*devid_ret = found_key.offset + 1;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * the device information is stored in the chunk root\n * the btrfs_device struct should be fully filled in\n */\nstatic int btrfs_add_dev_item(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tunsigned long ptr;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_insert_empty_item(trans, trans->fs_info->chunk_root, path,\n\t\t\t\t      &key, sizeof(*dev_item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_generation(leaf, dev_item, 0);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_set_device_group(leaf, dev_item, 0);\n\tbtrfs_set_device_seek_speed(leaf, dev_item, 0);\n\tbtrfs_set_device_bandwidth(leaf, dev_item, 0);\n\tbtrfs_set_device_start_offset(leaf, dev_item, 0);\n\n\tptr = btrfs_device_uuid(dev_item);\n\twrite_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n\tptr = btrfs_device_fsid(dev_item);\n\twrite_extent_buffer(leaf, trans->fs_info->fs_devices->metadata_uuid,\n\t\t\t    ptr, BTRFS_FSID_SIZE);\n\tbtrfs_mark_buffer_dirty(leaf);\n\n\tret = 0;\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Function to update ctime/mtime for a given device path.\n * Mainly used for ctime/mtime based probe like libblkid.\n */\nstatic void update_dev_time(const char *path_name)\n{\n\tstruct file *filp;\n\n\tfilp = filp_open(path_name, O_RDWR, 0);\n\tif (IS_ERR(filp))\n\t\treturn;\n\tfile_update_time(filp);\n\tfilp_close(filp, NULL);\n}\n\nstatic int btrfs_rm_dev_item(struct btrfs_device *device)\n{\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct btrfs_trans_handle *trans;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = -ENOENT;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (!ret)\n\t\tret = btrfs_commit_transaction(trans);\n\treturn ret;\n}\n\n/*\n * Verify that @num_devices satisfies the RAID profile constraints in the whole\n * filesystem. It's up to the caller to adjust that number regarding eg. device\n * replace.\n */\nstatic int btrfs_check_raid_min_devices(struct btrfs_fs_info *fs_info,\n\t\tu64 num_devices)\n{\n\tu64 all_avail;\n\tunsigned seq;\n\tint i;\n\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tall_avail = fs_info->avail_data_alloc_bits |\n\t\t\t    fs_info->avail_system_alloc_bits |\n\t\t\t    fs_info->avail_metadata_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tfor (i = 0; i < BTRFS_NR_RAID_TYPES; i++) {\n\t\tif (!(all_avail & btrfs_raid_array[i].bg_flag))\n\t\t\tcontinue;\n\n\t\tif (num_devices < btrfs_raid_array[i].devs_min)\n\t\t\treturn btrfs_raid_array[i].mindev_error;\n\t}\n\n\treturn 0;\n}\n\nstatic struct btrfs_device * btrfs_find_next_active_device(\n\t\tstruct btrfs_fs_devices *fs_devs, struct btrfs_device *device)\n{\n\tstruct btrfs_device *next_device;\n\n\tlist_for_each_entry(next_device, &fs_devs->devices, dev_list) {\n\t\tif (next_device != device &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &next_device->dev_state)\n\t\t    && next_device->bdev)\n\t\t\treturn next_device;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Helper function to check if the given device is part of s_bdev / latest_bdev\n * and replace it with the provided or the next active device, in the context\n * where this function called, there should be always be another device (or\n * this_dev) which is active.\n */\nvoid __cold btrfs_assign_next_active_device(struct btrfs_device *device,\n\t\t\t\t\t    struct btrfs_device *next_device)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\n\tif (!next_device)\n\t\tnext_device = btrfs_find_next_active_device(fs_info->fs_devices,\n\t\t\t\t\t\t\t    device);\n\tASSERT(next_device);\n\n\tif (fs_info->sb->s_bdev &&\n\t\t\t(fs_info->sb->s_bdev == device->bdev))\n\t\tfs_info->sb->s_bdev = next_device->bdev;\n\n\tif (fs_info->fs_devices->latest_bdev == device->bdev)\n\t\tfs_info->fs_devices->latest_bdev = next_device->bdev;\n}\n\n/*\n * Return btrfs_fs_devices::num_devices excluding the device that's being\n * currently replaced.\n */\nstatic u64 btrfs_num_devices(struct btrfs_fs_info *fs_info)\n{\n\tu64 num_devices = fs_info->fs_devices->num_devices;\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace)) {\n\t\tASSERT(num_devices > 1);\n\t\tnum_devices--;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn num_devices;\n}\n\nvoid btrfs_scratch_superblocks(struct btrfs_fs_info *fs_info,\n\t\t\t       struct block_device *bdev,\n\t\t\t       const char *device_path)\n{\n\tstruct btrfs_super_block *disk_super;\n\tint copy_num;\n\n\tif (!bdev)\n\t\treturn;\n\n\tfor (copy_num = 0; copy_num < BTRFS_SUPER_MIRROR_MAX; copy_num++) {\n\t\tstruct page *page;\n\t\tint ret;\n\n\t\tdisk_super = btrfs_read_dev_one_super(bdev, copy_num);\n\t\tif (IS_ERR(disk_super))\n\t\t\tcontinue;\n\n\t\tif (bdev_is_zoned(bdev)) {\n\t\t\tbtrfs_reset_sb_log_zones(bdev, copy_num);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemset(&disk_super->magic, 0, sizeof(disk_super->magic));\n\n\t\tpage = virt_to_page(disk_super);\n\t\tset_page_dirty(page);\n\t\tlock_page(page);\n\t\t/* write_on_page() unlocks the page */\n\t\tret = write_one_page(page);\n\t\tif (ret)\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t\t\"error clearing superblock number %d (%d)\",\n\t\t\t\tcopy_num, ret);\n\t\tbtrfs_release_disk_super(disk_super);\n\n\t}\n\n\t/* Notify udev that device has changed */\n\tbtrfs_kobject_uevent(bdev, KOBJ_CHANGE);\n\n\t/* Update ctime/mtime for device path for libblkid */\n\tupdate_dev_time(device_path);\n}\n\nint btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}\n\nvoid btrfs_rm_dev_replace_remove_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\n\tlockdep_assert_held(&srcdev->fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * in case of fs with no seed, srcdev->fs_devices will point\n\t * to fs_devices of fs_info. However when the dev being replaced is\n\t * a seed dev it will point to the seed's local fs_devices. In short\n\t * srcdev will have its correct fs_devices in both the cases.\n\t */\n\tfs_devices = srcdev->fs_devices;\n\n\tlist_del_rcu(&srcdev->dev_list);\n\tlist_del(&srcdev->dev_alloc_list);\n\tfs_devices->num_devices--;\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &srcdev->dev_state))\n\t\tfs_devices->missing_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &srcdev->dev_state))\n\t\tfs_devices->rw_devices--;\n\n\tif (srcdev->bdev)\n\t\tfs_devices->open_devices--;\n}\n\nvoid btrfs_rm_dev_replace_free_srcdev(struct btrfs_device *srcdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = srcdev->fs_devices;\n\n\tmutex_lock(&uuid_mutex);\n\n\tbtrfs_close_bdev(srcdev);\n\tsynchronize_rcu();\n\tbtrfs_free_device(srcdev);\n\n\t/* if this is no devs we rather delete the fs_devices */\n\tif (!fs_devices->num_devices) {\n\t\t/*\n\t\t * On a mounted FS, num_devices can't be zero unless it's a\n\t\t * seed. In case of a seed device being replaced, the replace\n\t\t * target added to the sprout FS, so there will be no more\n\t\t * device left under the seed FS.\n\t\t */\n\t\tASSERT(fs_devices->seeding);\n\n\t\tlist_del_init(&fs_devices->seed_list);\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t}\n\tmutex_unlock(&uuid_mutex);\n}\n\nvoid btrfs_destroy_dev_replace_tgtdev(struct btrfs_device *tgtdev)\n{\n\tstruct btrfs_fs_devices *fs_devices = tgtdev->fs_info->fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\n\tbtrfs_sysfs_remove_device(tgtdev);\n\n\tif (tgtdev->bdev)\n\t\tfs_devices->open_devices--;\n\n\tfs_devices->num_devices--;\n\n\tbtrfs_assign_next_active_device(tgtdev, NULL);\n\n\tlist_del_rcu(&tgtdev->dev_list);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * The update_dev_time() with in btrfs_scratch_superblocks()\n\t * may lead to a call to btrfs_show_devname() which will try\n\t * to hold device_list_mutex. And here this device\n\t * is already out of device list, so we don't have to hold\n\t * the device_list_mutex lock.\n\t */\n\tbtrfs_scratch_superblocks(tgtdev->fs_info, tgtdev->bdev,\n\t\t\t\t  tgtdev->name->str);\n\n\tbtrfs_close_bdev(tgtdev);\n\tsynchronize_rcu();\n\tbtrfs_free_device(tgtdev);\n}\n\nstatic struct btrfs_device *btrfs_find_device_by_path(\n\t\tstruct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tint ret = 0;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tu8 *dev_uuid;\n\tstruct block_device *bdev;\n\tstruct btrfs_device *device;\n\n\tret = btrfs_get_bdev_and_sb(device_path, FMODE_READ,\n\t\t\t\t    fs_info->bdev_holder, 0, &bdev, &disk_super);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_uuid = disk_super->dev_item.uuid;\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->metadata_uuid);\n\telse\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->fsid);\n\n\tbtrfs_release_disk_super(disk_super);\n\tif (!device)\n\t\tdevice = ERR_PTR(-ENOENT);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn device;\n}\n\n/*\n * Lookup a device given by device id, or the path if the id is 0.\n */\nstruct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tstruct btrfs_device *device;\n\n\tif (devid) {\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL,\n\t\t\t\t\t   NULL);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tif (!device_path || !device_path[0])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strcmp(device_path, \"missing\") == 0) {\n\t\t/* Find first missing device */\n\t\tlist_for_each_entry(device, &fs_info->fs_devices->devices,\n\t\t\t\t    dev_list) {\n\t\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t     &device->dev_state) && !device->bdev)\n\t\t\t\treturn device;\n\t\t}\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn btrfs_find_device_by_path(fs_info, device_path);\n}\n\n/*\n * does all the dirty work required for changing file system's UUID.\n */\nstatic int btrfs_prepare_sprout(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_fs_devices *old_devices;\n\tstruct btrfs_fs_devices *seed_devices;\n\tstruct btrfs_super_block *disk_super = fs_info->super_copy;\n\tstruct btrfs_device *device;\n\tu64 super_flags;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tif (!fs_devices->seeding)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Private copy of the seed devices, anchored at\n\t * fs_info->fs_devices->seed_list\n\t */\n\tseed_devices = alloc_fs_devices(NULL, NULL);\n\tif (IS_ERR(seed_devices))\n\t\treturn PTR_ERR(seed_devices);\n\n\t/*\n\t * It's necessary to retain a copy of the original seed fs_devices in\n\t * fs_uuids so that filesystems which have been seeded can successfully\n\t * reference the seed device from open_seed_devices. This also supports\n\t * multiple fs seed.\n\t */\n\told_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(old_devices)) {\n\t\tkfree(seed_devices);\n\t\treturn PTR_ERR(old_devices);\n\t}\n\n\tlist_add(&old_devices->fs_list, &fs_uuids);\n\n\tmemcpy(seed_devices, fs_devices, sizeof(*seed_devices));\n\tseed_devices->opened = 1;\n\tINIT_LIST_HEAD(&seed_devices->devices);\n\tINIT_LIST_HEAD(&seed_devices->alloc_list);\n\tmutex_init(&seed_devices->device_list_mutex);\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_splice_init_rcu(&fs_devices->devices, &seed_devices->devices,\n\t\t\t      synchronize_rcu);\n\tlist_for_each_entry(device, &seed_devices->devices, dev_list)\n\t\tdevice->fs_devices = seed_devices;\n\n\tfs_devices->seeding = false;\n\tfs_devices->num_devices = 0;\n\tfs_devices->open_devices = 0;\n\tfs_devices->missing_devices = 0;\n\tfs_devices->rotating = false;\n\tlist_add(&seed_devices->seed_list, &fs_devices->seed_list);\n\n\tgenerate_random_uuid(fs_devices->fsid);\n\tmemcpy(fs_devices->metadata_uuid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmemcpy(disk_super->fsid, fs_devices->fsid, BTRFS_FSID_SIZE);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tsuper_flags = btrfs_super_flags(disk_super) &\n\t\t      ~BTRFS_SUPER_FLAG_SEEDING;\n\tbtrfs_set_super_flags(disk_super, super_flags);\n\n\treturn 0;\n}\n\n/*\n * Store the expected generation for seed devices in device items.\n */\nstatic int btrfs_finish_sprout(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct request_queue *q;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device;\n\tstruct block_device *bdev;\n\tstruct super_block *sb = fs_info->sb;\n\tstruct rcu_string *name;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 orig_super_total_bytes;\n\tu64 orig_super_num_devices;\n\tint seeding_dev = 0;\n\tint ret = 0;\n\tbool locked = false;\n\n\tif (sb_rdonly(sb) && !fs_devices->seeding)\n\t\treturn -EROFS;\n\n\tbdev = blkdev_get_by_path(device_path, FMODE_WRITE | FMODE_EXCL,\n\t\t\t\t  fs_info->bdev_holder);\n\tif (IS_ERR(bdev))\n\t\treturn PTR_ERR(bdev);\n\n\tif (!btrfs_check_device_zone_type(fs_info, bdev)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (fs_devices->seeding) {\n\t\tseeding_dev = 1;\n\t\tdown_write(&sb->s_umount);\n\t\tmutex_lock(&uuid_mutex);\n\t\tlocked = true;\n\t}\n\n\tsync_blockdev(bdev);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &fs_devices->devices, dev_list) {\n\t\tif (device->bdev == bdev) {\n\t\t\tret = -EEXIST;\n\t\t\trcu_read_unlock();\n\t\t\tgoto error;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tdevice = btrfs_alloc_device(fs_info, NULL, NULL);\n\tif (IS_ERR(device)) {\n\t\t/* we can safely leave the fs_devices entry around */\n\t\tret = PTR_ERR(device);\n\t\tgoto error;\n\t}\n\n\tname = rcu_string_strdup(device_path, GFP_KERNEL);\n\tif (!name) {\n\t\tret = -ENOMEM;\n\t\tgoto error_free_device;\n\t}\n\trcu_assign_pointer(device->name, name);\n\n\tdevice->fs_info = fs_info;\n\tdevice->bdev = bdev;\n\n\tret = btrfs_get_dev_zone_info(device);\n\tif (ret)\n\t\tgoto error_free_device;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto error_free_zone;\n\t}\n\n\tq = bdev_get_queue(bdev);\n\tset_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state);\n\tdevice->generation = trans->transid;\n\tdevice->io_width = fs_info->sectorsize;\n\tdevice->io_align = fs_info->sectorsize;\n\tdevice->sector_size = fs_info->sectorsize;\n\tdevice->total_bytes = round_down(i_size_read(bdev->bd_inode),\n\t\t\t\t\t fs_info->sectorsize);\n\tdevice->disk_total_bytes = device->total_bytes;\n\tdevice->commit_total_bytes = device->total_bytes;\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\tdevice->mode = FMODE_EXCL;\n\tdevice->dev_stats_valid = 1;\n\tset_blocksize(device->bdev, BTRFS_BDEV_BLOCKSIZE);\n\n\tif (seeding_dev) {\n\t\tbtrfs_clear_sb_rdonly(sb);\n\t\tret = btrfs_prepare_sprout(fs_info);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_trans;\n\t\t}\n\t}\n\n\tdevice->fs_devices = fs_devices;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\tlist_add(&device->dev_alloc_list, &fs_devices->alloc_list);\n\tfs_devices->num_devices++;\n\tfs_devices->open_devices++;\n\tfs_devices->rw_devices++;\n\tfs_devices->total_devices++;\n\tfs_devices->total_rw_bytes += device->total_bytes;\n\n\tatomic64_add(device->total_bytes, &fs_info->free_chunk_space);\n\n\tif (!blk_queue_nonrot(q))\n\t\tfs_devices->rotating = true;\n\n\torig_super_total_bytes = btrfs_super_total_bytes(fs_info->super_copy);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\tround_down(orig_super_total_bytes + device->total_bytes,\n\t\t\t   fs_info->sectorsize));\n\n\torig_super_num_devices = btrfs_super_num_devices(fs_info->super_copy);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices + 1);\n\n\t/*\n\t * we've got more storage, clear any full flags on the space\n\t * infos\n\t */\n\tbtrfs_clear_space_info_full(fs_info);\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/* Add sysfs device entry */\n\tbtrfs_sysfs_add_device(device);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (seeding_dev) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tret = init_first_rw_device(trans);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\t}\n\n\tret = btrfs_add_dev_item(trans, device);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto error_sysfs;\n\t}\n\n\tif (seeding_dev) {\n\t\tret = btrfs_finish_sprout(trans);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto error_sysfs;\n\t\t}\n\n\t\t/*\n\t\t * fs_devices now represents the newly sprouted filesystem and\n\t\t * its fsid has been changed by btrfs_prepare_sprout\n\t\t */\n\t\tbtrfs_sysfs_update_sprout_fsid(fs_devices);\n\t}\n\n\tret = btrfs_commit_transaction(trans);\n\n\tif (seeding_dev) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t\tlocked = false;\n\n\t\tif (ret) /* transaction commit */\n\t\t\treturn ret;\n\n\t\tret = btrfs_relocate_sys_chunks(fs_info);\n\t\tif (ret < 0)\n\t\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t    \"Failed to relocate sys chunks after device initialization. This can be fixed using the \\\"btrfs balance\\\" command.\");\n\t\ttrans = btrfs_attach_transaction(root);\n\t\tif (IS_ERR(trans)) {\n\t\t\tif (PTR_ERR(trans) == -ENOENT)\n\t\t\t\treturn 0;\n\t\t\tret = PTR_ERR(trans);\n\t\t\ttrans = NULL;\n\t\t\tgoto error_sysfs;\n\t\t}\n\t\tret = btrfs_commit_transaction(trans);\n\t}\n\n\t/*\n\t * Now that we have written a new super block to this device, check all\n\t * other fs_devices list if device_path alienates any other scanned\n\t * device.\n\t * We can ignore the return value as it typically returns -EINVAL and\n\t * only succeeds if the device was an alien.\n\t */\n\tbtrfs_forget_devices(device_path);\n\n\t/* Update ctime/mtime for blkid or udev */\n\tupdate_dev_time(device_path);\n\n\treturn ret;\n\nerror_sysfs:\n\tbtrfs_sysfs_remove_device(device);\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tmutex_lock(&fs_info->chunk_mutex);\n\tlist_del_rcu(&device->dev_list);\n\tlist_del(&device->dev_alloc_list);\n\tfs_info->fs_devices->num_devices--;\n\tfs_info->fs_devices->open_devices--;\n\tfs_info->fs_devices->rw_devices--;\n\tfs_info->fs_devices->total_devices--;\n\tfs_info->fs_devices->total_rw_bytes -= device->total_bytes;\n\tatomic64_sub(device->total_bytes, &fs_info->free_chunk_space);\n\tbtrfs_set_super_total_bytes(fs_info->super_copy,\n\t\t\t\t    orig_super_total_bytes);\n\tbtrfs_set_super_num_devices(fs_info->super_copy,\n\t\t\t\t    orig_super_num_devices);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\nerror_trans:\n\tif (seeding_dev)\n\t\tbtrfs_set_sb_rdonly(sb);\n\tif (trans)\n\t\tbtrfs_end_transaction(trans);\nerror_free_zone:\n\tbtrfs_destroy_dev_zone_info(device);\nerror_free_device:\n\tbtrfs_free_device(device);\nerror:\n\tblkdev_put(bdev, FMODE_EXCL);\n\tif (locked) {\n\t\tmutex_unlock(&uuid_mutex);\n\t\tup_write(&sb->s_umount);\n\t}\n\treturn ret;\n}\n\nstatic noinline int btrfs_update_device(struct btrfs_trans_handle *trans,\n\t\t\t\t\tstruct btrfs_device *device)\n{\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = device->fs_info->chunk_root;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\tdev_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dev_item);\n\n\tbtrfs_set_device_id(leaf, dev_item, device->devid);\n\tbtrfs_set_device_type(leaf, dev_item, device->type);\n\tbtrfs_set_device_io_align(leaf, dev_item, device->io_align);\n\tbtrfs_set_device_io_width(leaf, dev_item, device->io_width);\n\tbtrfs_set_device_sector_size(leaf, dev_item, device->sector_size);\n\tbtrfs_set_device_total_bytes(leaf, dev_item,\n\t\t\t\t     btrfs_device_get_disk_total_bytes(device));\n\tbtrfs_set_device_bytes_used(leaf, dev_item,\n\t\t\t\t    btrfs_device_get_bytes_used(device));\n\tbtrfs_mark_buffer_dirty(leaf);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_grow_device(struct btrfs_trans_handle *trans,\n\t\t      struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total;\n\tu64 diff;\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\treturn -EACCES;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\told_total = btrfs_super_total_bytes(super_copy);\n\tdiff = round_down(new_size - device->total_bytes, fs_info->sectorsize);\n\n\tif (new_size <= device->total_bytes ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total + diff, fs_info->sectorsize));\n\tdevice->fs_devices->total_rw_bytes += diff;\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tbtrfs_clear_space_info_full(device->fs_info);\n\tif (list_empty(&device->post_commit_list))\n\t\tlist_add_tail(&device->post_commit_list,\n\t\t\t      &trans->transaction->dev_update_list);\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\treturn btrfs_update_device(trans, device);\n}\n\nstatic int btrfs_free_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tint ret;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = chunk_offset;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\telse if (ret > 0) { /* Logic error or corruption */\n\t\tbtrfs_handle_fs_error(fs_info, -ENOENT,\n\t\t\t\t      \"Failed lookup while freeing chunk.\");\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\n\tif (ret < 0)\n\t\tbtrfs_handle_fs_error(fs_info, ret,\n\t\t\t\t      \"Failed to delete chunk item.\");\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int btrfs_del_sys_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *ptr;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur;\n\tstruct btrfs_key key;\n\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tptr = super_copy->sys_chunk_array;\n\tcur = 0;\n\n\twhile (cur < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)ptr;\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tlen = sizeof(*disk_key);\n\n\t\tif (key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tchunk = (struct btrfs_chunk *)(ptr + len);\n\t\t\tnum_stripes = btrfs_stack_chunk_num_stripes(chunk);\n\t\t\tlen += btrfs_chunk_item_size(num_stripes);\n\t\t} else {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tif (key.objectid == BTRFS_FIRST_CHUNK_TREE_OBJECTID &&\n\t\t    key.offset == chunk_offset) {\n\t\t\tmemmove(ptr, ptr + len, array_size - (cur + len));\n\t\t\tarray_size -= len;\n\t\t\tbtrfs_set_super_sys_array_size(super_copy, array_size);\n\t\t} else {\n\t\t\tptr += len;\n\t\t\tcur += len;\n\t\t}\n\t}\n\treturn ret;\n}\n\n/*\n * btrfs_get_chunk_map() - Find the mapping containing the given logical extent.\n * @logical: Logical block offset in bytes.\n * @length: Length of extent in bytes.\n *\n * Return: Chunk mapping or ERR_PTR.\n */\nstruct extent_map *btrfs_get_chunk_map(struct btrfs_fs_info *fs_info,\n\t\t\t\t       u64 logical, u64 length)\n{\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\n\tem_tree = &fs_info->mapping_tree;\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, logical, length);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_crit(fs_info, \"unable to find logical %llu length %llu\",\n\t\t\t   logical, length);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (em->start > logical || em->start + em->len < logical) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"found a bad mapping, wanted %llu-%llu, found %llu-%llu\",\n\t\t\t   logical, length, em->start, em->start + em->len);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/* callers are responsible for dropping em's ref. */\n\treturn em;\n}\n\nstatic int remove_chunk_item(struct btrfs_trans_handle *trans,\n\t\t\t     struct map_lookup *map, u64 chunk_offset)\n{\n\tint i;\n\n\t/*\n\t * Removing chunk items and updating the device items in the chunks btree\n\t * requires holding the chunk_mutex.\n\t * See the comment at btrfs_chunk_alloc() for the details.\n\t */\n\tlockdep_assert_held(&trans->fs_info->chunk_mutex);\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tint ret;\n\n\t\tret = btrfs_update_device(trans, map->stripes[i].dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn btrfs_free_chunk(trans, chunk_offset);\n}\n\nint btrfs_remove_chunk(struct btrfs_trans_handle *trans, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 dev_extent_len = 0;\n\tint i, ret = 0;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em)) {\n\t\t/*\n\t\t * This is a logic error, but we don't want to just rely on the\n\t\t * user having built with ASSERT enabled, so if ASSERT doesn't\n\t\t * do anything we still error out.\n\t\t */\n\t\tASSERT(0);\n\t\treturn PTR_ERR(em);\n\t}\n\tmap = em->map_lookup;\n\n\t/*\n\t * First delete the device extent items from the devices btree.\n\t * We take the device_list_mutex to avoid racing with the finishing phase\n\t * of a device replace operation. See the comment below before acquiring\n\t * fs_info->chunk_mutex. Note that here we do not acquire the chunk_mutex\n\t * because that can result in a deadlock when deleting the device extent\n\t * items from the devices btree - COWing an extent buffer from the btree\n\t * may result in allocating a new metadata chunk, which would attempt to\n\t * lock again fs_info->chunk_mutex.\n\t */\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tret = btrfs_free_dev_extent(trans, device,\n\t\t\t\t\t    map->stripes[i].physical,\n\t\t\t\t\t    &dev_extent_len);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (device->bytes_used > 0) {\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tbtrfs_device_set_bytes_used(device,\n\t\t\t\t\tdevice->bytes_used - dev_extent_len);\n\t\t\tatomic64_add(dev_extent_len, &fs_info->free_chunk_space);\n\t\t\tbtrfs_clear_space_info_full(fs_info);\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t}\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * We acquire fs_info->chunk_mutex for 2 reasons:\n\t *\n\t * 1) Just like with the first phase of the chunk allocation, we must\n\t *    reserve system space, do all chunk btree updates and deletions, and\n\t *    update the system chunk array in the superblock while holding this\n\t *    mutex. This is for similar reasons as explained on the comment at\n\t *    the top of btrfs_chunk_alloc();\n\t *\n\t * 2) Prevent races with the final phase of a device replace operation\n\t *    that replaces the device object associated with the map's stripes,\n\t *    because the device object's id can change at any time during that\n\t *    final phase of the device replace operation\n\t *    (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the\n\t *    replaced device and then see it with an ID of\n\t *    BTRFS_DEV_REPLACE_DEVID, which would cause a failure when updating\n\t *    the device item, which does not exists on the chunk btree.\n\t *    The finishing phase of device replace acquires both the\n\t *    device_list_mutex and the chunk_mutex, in that order, so we are\n\t *    safe by just acquiring the chunk_mutex.\n\t */\n\ttrans->removing_chunk = true;\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tcheck_system_chunk(trans, map->type);\n\n\tret = remove_chunk_item(trans, map, chunk_offset);\n\t/*\n\t * Normally we should not get -ENOSPC since we reserved space before\n\t * through the call to check_system_chunk().\n\t *\n\t * Despite our system space_info having enough free space, we may not\n\t * be able to allocate extents from its block groups, because all have\n\t * an incompatible profile, which will force us to allocate a new system\n\t * block group with the right profile, or right after we called\n\t * check_system_space() above, a scrub turned the only system block group\n\t * with enough free space into RO mode.\n\t * This is explained with more detail at do_chunk_alloc().\n\t *\n\t * So if we get -ENOSPC, allocate a new system chunk and retry once.\n\t */\n\tif (ret == -ENOSPC) {\n\t\tconst u64 sys_flags = btrfs_system_alloc_profile(fs_info);\n\t\tstruct btrfs_block_group *sys_bg;\n\n\t\tsys_bg = btrfs_alloc_chunk(trans, sys_flags);\n\t\tif (IS_ERR(sys_bg)) {\n\t\t\tret = PTR_ERR(sys_bg);\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = btrfs_chunk_alloc_add_chunk_item(trans, sys_bg);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = remove_chunk_item(trans, map, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t} else if (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\ttrace_btrfs_chunk_free(fs_info, map, chunk_offset, em->len);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_del_sys_chunk(fs_info, chunk_offset);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmutex_unlock(&fs_info->chunk_mutex);\n\ttrans->removing_chunk = false;\n\n\t/*\n\t * We are done with chunk btree updates and deletions, so release the\n\t * system space we previously reserved (with check_system_chunk()).\n\t */\n\tbtrfs_trans_release_chunk_metadata(trans);\n\n\tret = btrfs_remove_block_group(trans, chunk_offset, em);\n\tif (ret) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\nout:\n\tif (trans->removing_chunk) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\ttrans->removing_chunk = false;\n\t}\n\t/* once for us */\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_relocate_chunk(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_block_group *block_group;\n\tu64 length;\n\tint ret;\n\n\t/*\n\t * Prevent races with automatic removal of unused block groups.\n\t * After we relocate and before we remove the chunk with offset\n\t * chunk_offset, automatic removal of the block group can kick in,\n\t * resulting in a failure when calling btrfs_remove_chunk() below.\n\t *\n\t * Make sure to acquire this mutex before doing a tree search (dev\n\t * or chunk trees) to find chunks. Otherwise the cleaner kthread might\n\t * call btrfs_remove_chunk() (through btrfs_delete_unused_bgs()) after\n\t * we release the path used to search the chunk/dev tree and before\n\t * the current task acquires this mutex and calls us.\n\t */\n\tlockdep_assert_held(&fs_info->reclaim_bgs_lock);\n\n\t/* step one, relocate all the extents inside this chunk */\n\tbtrfs_scrub_pause(fs_info);\n\tret = btrfs_relocate_block_group(fs_info, chunk_offset);\n\tbtrfs_scrub_continue(fs_info);\n\tif (ret)\n\t\treturn ret;\n\n\tblock_group = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tif (!block_group)\n\t\treturn -ENOENT;\n\tbtrfs_discard_cancel_work(&fs_info->discard_ctl, block_group);\n\tlength = block_group->length;\n\tbtrfs_put_block_group(block_group);\n\n\t/*\n\t * On a zoned file system, discard the whole block group, this will\n\t * trigger a REQ_OP_ZONE_RESET operation on the device zone. If\n\t * resetting the zone fails, don't treat it as a fatal problem from the\n\t * filesystem's point of view.\n\t */\n\tif (btrfs_is_zoned(fs_info)) {\n\t\tret = btrfs_discard_extent(fs_info, chunk_offset, length, NULL);\n\t\tif (ret)\n\t\t\tbtrfs_info(fs_info,\n\t\t\t\t\"failed to reset zone %llu after relocation\",\n\t\t\t\tchunk_offset);\n\t}\n\n\ttrans = btrfs_start_trans_remove_block_group(root->fs_info,\n\t\t\t\t\t\t     chunk_offset);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tbtrfs_handle_fs_error(root->fs_info, ret, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * step two, delete the device extents and the\n\t * chunk tree entries\n\t */\n\tret = btrfs_remove_chunk(trans, chunk_offset);\n\tbtrfs_end_transaction(trans);\n\treturn ret;\n}\n\nstatic int btrfs_relocate_sys_chunks(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tu64 chunk_type;\n\tbool retried = false;\n\tint failed = 0;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\nagain:\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto error;\n\t\t}\n\t\tBUG_ON(ret == 0); /* Corruption */\n\n\t\tret = btrfs_previous_item(chunk_root, path, key.objectid,\n\t\t\t\t\t  key.type);\n\t\tif (ret)\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t\tif (ret > 0)\n\t\t\tbreak;\n\n\t\tleaf = path->nodes[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\n\t\tchunk = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t       struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\t\tbtrfs_release_path(path);\n\n\t\tif (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\t\tif (ret == -ENOSPC)\n\t\t\t\tfailed++;\n\t\t\telse\n\t\t\t\tBUG_ON(ret);\n\t\t}\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\tret = 0;\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (WARN_ON(failed && retried)) {\n\t\tret = -ENOSPC;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * return 1 : allocate a data chunk successfully,\n * return <0: errors during allocating a data chunk,\n * return 0 : no need to allocate a data chunk.\n */\nstatic int btrfs_may_alloc_data_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t      u64 chunk_offset)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 bytes_used;\n\tu64 chunk_type;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tASSERT(cache);\n\tchunk_type = cache->flags;\n\tbtrfs_put_block_group(cache);\n\n\tif (!(chunk_type & BTRFS_BLOCK_GROUP_DATA))\n\t\treturn 0;\n\n\tspin_lock(&fs_info->data_sinfo->lock);\n\tbytes_used = fs_info->data_sinfo->bytes_used;\n\tspin_unlock(&fs_info->data_sinfo->lock);\n\n\tif (!bytes_used) {\n\t\tstruct btrfs_trans_handle *trans;\n\t\tint ret;\n\n\t\ttrans =\tbtrfs_join_transaction(fs_info->tree_root);\n\t\tif (IS_ERR(trans))\n\t\t\treturn PTR_ERR(trans);\n\n\t\tret = btrfs_force_chunk_alloc(trans, BTRFS_BLOCK_GROUP_DATA);\n\t\tbtrfs_end_transaction(trans);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int insert_balance_item(struct btrfs_fs_info *fs_info,\n\t\t\t       struct btrfs_balance_control *bctl)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(*item));\n\tif (ret)\n\t\tgoto out;\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tmemzero_extent_buffer(leaf, (unsigned long)item, sizeof(*item));\n\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->data);\n\tbtrfs_set_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->meta);\n\tbtrfs_set_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_cpu_balance_args_to_disk(&disk_bargs, &bctl->sys);\n\tbtrfs_set_balance_sys(leaf, item, &disk_bargs);\n\n\tbtrfs_set_balance_flags(leaf, item, bctl->flags);\n\n\tbtrfs_mark_buffer_dirty(leaf);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\nstatic int del_balance_item(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret, err;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\ttrans = btrfs_start_transaction_fallback_global_rsv(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(trans, root, &key, path, -1, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = btrfs_del_item(trans, root, path);\nout:\n\tbtrfs_free_path(path);\n\terr = btrfs_commit_transaction(trans);\n\tif (err && !ret)\n\t\tret = err;\n\treturn ret;\n}\n\n/*\n * This is a heuristic used to reduce the number of chunks balanced on\n * resume after balance was interrupted.\n */\nstatic void update_balance_args(struct btrfs_balance_control *bctl)\n{\n\t/*\n\t * Turn on soft mode for chunk types that were being converted.\n\t */\n\tif (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\tif (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_SOFT;\n\n\t/*\n\t * Turn on usage filter if is not already used.  The idea is\n\t * that chunks that we have already balanced should be\n\t * reasonably full.  Don't do it for chunks that are being\n\t * converted - that will keep us from relocating unconverted\n\t * (albeit full) chunks.\n\t */\n\tif (!(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->data.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->data.usage = 90;\n\t}\n\tif (!(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->sys.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->sys.usage = 90;\n\t}\n\tif (!(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    !(bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT)) {\n\t\tbctl->meta.flags |= BTRFS_BALANCE_ARGS_USAGE;\n\t\tbctl->meta.usage = 90;\n\t}\n}\n\n/*\n * Clear the balance status in fs_info and delete the balance item from disk.\n */\nstatic void reset_balance_state(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tint ret;\n\n\tBUG_ON(!fs_info->balance_ctl);\n\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = NULL;\n\tspin_unlock(&fs_info->balance_lock);\n\n\tkfree(bctl);\n\tret = del_balance_item(fs_info);\n\tif (ret)\n\t\tbtrfs_handle_fs_error(fs_info, ret, NULL);\n}\n\n/*\n * Balance filters.  Return 1 if chunk should be filtered out\n * (should not be balanced).\n */\nstatic int chunk_profiles_filter(u64 chunk_type,\n\t\t\t\t struct btrfs_balance_args *bargs)\n{\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->profiles & chunk_type)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_usage_range_filter(struct btrfs_fs_info *fs_info, u64 chunk_offset,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 chunk_used;\n\tu64 user_thresh_min;\n\tu64 user_thresh_max;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = cache->used;\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh_min = 0;\n\telse\n\t\tuser_thresh_min = div_factor_fine(cache->length,\n\t\t\t\t\t\t  bargs->usage_min);\n\n\tif (bargs->usage_max == 0)\n\t\tuser_thresh_max = 1;\n\telse if (bargs->usage_max > 100)\n\t\tuser_thresh_max = cache->length;\n\telse\n\t\tuser_thresh_max = div_factor_fine(cache->length,\n\t\t\t\t\t\t  bargs->usage_max);\n\n\tif (user_thresh_min <= chunk_used && chunk_used < user_thresh_max)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_usage_filter(struct btrfs_fs_info *fs_info,\n\t\tu64 chunk_offset, struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_block_group *cache;\n\tu64 chunk_used, user_thresh;\n\tint ret = 1;\n\n\tcache = btrfs_lookup_block_group(fs_info, chunk_offset);\n\tchunk_used = cache->used;\n\n\tif (bargs->usage_min == 0)\n\t\tuser_thresh = 1;\n\telse if (bargs->usage > 100)\n\t\tuser_thresh = cache->length;\n\telse\n\t\tuser_thresh = div_factor_fine(cache->length, bargs->usage);\n\n\tif (chunk_used < user_thresh)\n\t\tret = 0;\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic int chunk_devid_filter(struct extent_buffer *leaf,\n\t\t\t      struct btrfs_chunk *chunk,\n\t\t\t      struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tint i;\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) == bargs->devid)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic u64 calc_data_stripes(u64 type, int num_stripes)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(type);\n\tconst int ncopies = btrfs_raid_array[index].ncopies;\n\tconst int nparity = btrfs_raid_array[index].nparity;\n\n\treturn (num_stripes - nparity) / ncopies;\n}\n\n/* [pstart, pend) */\nstatic int chunk_drange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tstruct btrfs_stripe *stripe;\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\tu64 stripe_offset;\n\tu64 stripe_length;\n\tu64 type;\n\tint factor;\n\tint i;\n\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_DEVID))\n\t\treturn 0;\n\n\ttype = btrfs_chunk_type(leaf, chunk);\n\tfactor = calc_data_stripes(type, num_stripes);\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tstripe = btrfs_stripe_nr(chunk, i);\n\t\tif (btrfs_stripe_devid(leaf, stripe) != bargs->devid)\n\t\t\tcontinue;\n\n\t\tstripe_offset = btrfs_stripe_offset(leaf, stripe);\n\t\tstripe_length = btrfs_chunk_length(leaf, chunk);\n\t\tstripe_length = div_u64(stripe_length, factor);\n\n\t\tif (stripe_offset < bargs->pend &&\n\t\t    stripe_offset + stripe_length > bargs->pstart)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/* [vstart, vend) */\nstatic int chunk_vrange_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       u64 chunk_offset,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tif (chunk_offset < bargs->vend &&\n\t    chunk_offset + btrfs_chunk_length(leaf, chunk) > bargs->vstart)\n\t\t/* at least part of the chunk is inside this vrange */\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_stripes_range_filter(struct extent_buffer *leaf,\n\t\t\t       struct btrfs_chunk *chunk,\n\t\t\t       struct btrfs_balance_args *bargs)\n{\n\tint num_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tif (bargs->stripes_min <= num_stripes\n\t\t\t&& num_stripes <= bargs->stripes_max)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic int chunk_soft_convert_filter(u64 chunk_type,\n\t\t\t\t     struct btrfs_balance_args *bargs)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn 0;\n\n\tchunk_type = chunk_to_extended(chunk_type) &\n\t\t\t\tBTRFS_EXTENDED_PROFILE_MASK;\n\n\tif (bargs->target == chunk_type)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int should_balance_chunk(struct extent_buffer *leaf,\n\t\t\t\tstruct btrfs_chunk *chunk, u64 chunk_offset)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_balance_args *bargs = NULL;\n\tu64 chunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t/* type filter */\n\tif (!((chunk_type & BTRFS_BLOCK_GROUP_TYPE_MASK) &\n\t      (bctl->flags & BTRFS_BALANCE_TYPE_MASK))) {\n\t\treturn 0;\n\t}\n\n\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\tbargs = &bctl->data;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\tbargs = &bctl->sys;\n\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\tbargs = &bctl->meta;\n\n\t/* profiles filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_PROFILES) &&\n\t    chunk_profiles_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* usage filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE) &&\n\t    chunk_usage_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_USAGE_RANGE) &&\n\t    chunk_usage_range_filter(fs_info, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DEVID) &&\n\t    chunk_devid_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* drange filter, makes sense only with devid filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_DRANGE) &&\n\t    chunk_drange_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* vrange filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_VRANGE) &&\n\t    chunk_vrange_filter(leaf, chunk, chunk_offset, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* stripes filter */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE) &&\n\t    chunk_stripes_range_filter(leaf, chunk, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/* soft profile changing mode */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_SOFT) &&\n\t    chunk_soft_convert_filter(chunk_type, bargs)) {\n\t\treturn 0;\n\t}\n\n\t/*\n\t * limited by count, must be the last filter\n\t */\n\tif ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT)) {\n\t\tif (bargs->limit == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit--;\n\t} else if ((bargs->flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)) {\n\t\t/*\n\t\t * Same logic as the 'limit' filter; the minimum cannot be\n\t\t * determined here because we do not have the global information\n\t\t * about the count of all chunks that satisfy the filters.\n\t\t */\n\t\tif (bargs->limit_max == 0)\n\t\t\treturn 0;\n\t\telse\n\t\t\tbargs->limit_max--;\n\t}\n\n\treturn 1;\n}\n\nstatic int __btrfs_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tu64 chunk_type;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\tint ret;\n\tint enospc_errors = 0;\n\tbool counting = true;\n\t/* The single value limit and min/max limits use the same bytes in the */\n\tu64 limit_data = bctl->data.limit;\n\tu64 limit_meta = bctl->meta.limit;\n\tu64 limit_sys = bctl->sys.limit;\n\tu32 count_data = 0;\n\tu32 count_meta = 0;\n\tu32 count_sys = 0;\n\tint chunk_reserved = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\t/* zero out stat counters */\n\tspin_lock(&fs_info->balance_lock);\n\tmemset(&bctl->stat, 0, sizeof(bctl->stat));\n\tspin_unlock(&fs_info->balance_lock);\nagain:\n\tif (!counting) {\n\t\t/*\n\t\t * The single value limit and min/max limits use the same bytes\n\t\t * in the\n\t\t */\n\t\tbctl->data.limit = limit_data;\n\t\tbctl->meta.limit = limit_meta;\n\t\tbctl->sys.limit = limit_sys;\n\t}\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\n\twhile (1) {\n\t\tif ((!counting && atomic_read(&fs_info->balance_pause_req)) ||\n\t\t    atomic_read(&fs_info->balance_cancel_req)) {\n\t\t\tret = -ECANCELED;\n\t\t\tgoto error;\n\t\t}\n\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, chunk_root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * this shouldn't happen, it means the last relocate\n\t\t * failed\n\t\t */\n\t\tif (ret == 0)\n\t\t\tBUG(); /* FIXME break ? */\n\n\t\tret = btrfs_previous_item(chunk_root, path, 0,\n\t\t\t\t\t  BTRFS_CHUNK_ITEM_KEY);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid != key.objectid) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\tchunk_type = btrfs_chunk_type(leaf, chunk);\n\n\t\tif (!counting) {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.considered++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\n\n\t\tret = should_balance_chunk(leaf, chunk, found_key.offset);\n\n\t\tbtrfs_release_path(path);\n\t\tif (!ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (counting) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.expected++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\n\t\t\tif (chunk_type & BTRFS_BLOCK_GROUP_DATA)\n\t\t\t\tcount_data++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_SYSTEM)\n\t\t\t\tcount_sys++;\n\t\t\telse if (chunk_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\t\t\tcount_meta++;\n\n\t\t\tgoto loop;\n\t\t}\n\n\t\t/*\n\t\t * Apply limit_min filter, no need to check if the LIMITS\n\t\t * filter is used, limit_min is 0 by default\n\t\t */\n\t\tif (((chunk_type & BTRFS_BLOCK_GROUP_DATA) &&\n\t\t\t\t\tcount_data < bctl->data.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_METADATA) &&\n\t\t\t\t\tcount_meta < bctl->meta.limit_min)\n\t\t\t\t|| ((chunk_type & BTRFS_BLOCK_GROUP_SYSTEM) &&\n\t\t\t\t\tcount_sys < bctl->sys.limit_min)) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto loop;\n\t\t}\n\n\t\tif (!chunk_reserved) {\n\t\t\t/*\n\t\t\t * We may be relocating the only data chunk we have,\n\t\t\t * which could potentially end up with losing data's\n\t\t\t * raid profile, so lets allocate an empty one in\n\t\t\t * advance.\n\t\t\t */\n\t\t\tret = btrfs_may_alloc_data_chunk(fs_info,\n\t\t\t\t\t\t\t found_key.offset);\n\t\t\tif (ret < 0) {\n\t\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\t\tgoto error;\n\t\t\t} else if (ret == 1) {\n\t\t\t\tchunk_reserved = 1;\n\t\t\t}\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, found_key.offset);\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret == -ENOSPC) {\n\t\t\tenospc_errors++;\n\t\t} else if (ret == -ETXTBSY) {\n\t\t\tbtrfs_info(fs_info,\n\t   \"skipping relocation of block group %llu due to active swapfile\",\n\t\t\t\t   found_key.offset);\n\t\t\tret = 0;\n\t\t} else if (ret) {\n\t\t\tgoto error;\n\t\t} else {\n\t\t\tspin_lock(&fs_info->balance_lock);\n\t\t\tbctl->stat.completed++;\n\t\t\tspin_unlock(&fs_info->balance_lock);\n\t\t}\nloop:\n\t\tif (found_key.offset == 0)\n\t\t\tbreak;\n\t\tkey.offset = found_key.offset - 1;\n\t}\n\n\tif (counting) {\n\t\tbtrfs_release_path(path);\n\t\tcounting = false;\n\t\tgoto again;\n\t}\nerror:\n\tbtrfs_free_path(path);\n\tif (enospc_errors) {\n\t\tbtrfs_info(fs_info, \"%d enospc errors during balance\",\n\t\t\t   enospc_errors);\n\t\tif (!ret)\n\t\t\tret = -ENOSPC;\n\t}\n\n\treturn ret;\n}\n\n/**\n * alloc_profile_is_valid - see if a given profile is valid and reduced\n * @flags: profile to validate\n * @extended: if true @flags is treated as an extended profile\n */\nstatic int alloc_profile_is_valid(u64 flags, int extended)\n{\n\tu64 mask = (extended ? BTRFS_EXTENDED_PROFILE_MASK :\n\t\t\t       BTRFS_BLOCK_GROUP_PROFILE_MASK);\n\n\tflags &= ~BTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\t/* 1) check that all other bits are zeroed */\n\tif (flags & ~mask)\n\t\treturn 0;\n\n\t/* 2) see if profile is reduced */\n\tif (flags == 0)\n\t\treturn !extended; /* \"0\" is valid for usual profiles */\n\n\treturn has_single_bit_set(flags);\n}\n\nstatic inline int balance_need_close(struct btrfs_fs_info *fs_info)\n{\n\t/* cancel requested || normal exit path */\n\treturn atomic_read(&fs_info->balance_cancel_req) ||\n\t\t(atomic_read(&fs_info->balance_pause_req) == 0 &&\n\t\t atomic_read(&fs_info->balance_cancel_req) == 0);\n}\n\n/*\n * Validate target profile against allowed profiles and return true if it's OK.\n * Otherwise print the error message and return false.\n */\nstatic inline int validate_convert_profile(struct btrfs_fs_info *fs_info,\n\t\tconst struct btrfs_balance_args *bargs,\n\t\tu64 allowed, const char *type)\n{\n\tif (!(bargs->flags & BTRFS_BALANCE_ARGS_CONVERT))\n\t\treturn true;\n\n\tif (fs_info->sectorsize < PAGE_SIZE &&\n\t\tbargs->target & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tbtrfs_err(fs_info,\n\t\t\"RAID56 is not yet supported for sectorsize %u with page size %lu\",\n\t\t\t  fs_info->sectorsize, PAGE_SIZE);\n\t\treturn false;\n\t}\n\t/* Profile is valid and does not have bits outside of the allowed set */\n\tif (alloc_profile_is_valid(bargs->target, 1) &&\n\t    (bargs->target & ~allowed) == 0)\n\t\treturn true;\n\n\tbtrfs_err(fs_info, \"balance: invalid convert %s profile %s\",\n\t\t\ttype, btrfs_bg_type_to_raid_name(bargs->target));\n\treturn false;\n}\n\n/*\n * Fill @buf with textual description of balance filter flags @bargs, up to\n * @size_buf including the terminating null. The output may be trimmed if it\n * does not fit into the provided buffer.\n */\nstatic void describe_balance_args(struct btrfs_balance_args *bargs, char *buf,\n\t\t\t\t u32 size_buf)\n{\n\tint ret;\n\tu32 size_bp = size_buf;\n\tchar *bp = buf;\n\tu64 flags = bargs->flags;\n\tchar tmp_buf[128] = {'\\0'};\n\n\tif (!flags)\n\t\treturn;\n\n#define CHECK_APPEND_NOARG(a)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n#define CHECK_APPEND_2ARG(a, v1, v2)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1), (v2));\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (flags & BTRFS_BALANCE_ARGS_CONVERT)\n\t\tCHECK_APPEND_1ARG(\"convert=%s,\",\n\t\t\t\t  btrfs_bg_type_to_raid_name(bargs->target));\n\n\tif (flags & BTRFS_BALANCE_ARGS_SOFT)\n\t\tCHECK_APPEND_NOARG(\"soft,\");\n\n\tif (flags & BTRFS_BALANCE_ARGS_PROFILES) {\n\t\tbtrfs_describe_block_groups(bargs->profiles, tmp_buf,\n\t\t\t\t\t    sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"profiles=%s,\", tmp_buf);\n\t}\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE)\n\t\tCHECK_APPEND_1ARG(\"usage=%llu,\", bargs->usage);\n\n\tif (flags & BTRFS_BALANCE_ARGS_USAGE_RANGE)\n\t\tCHECK_APPEND_2ARG(\"usage=%u..%u,\",\n\t\t\t\t  bargs->usage_min, bargs->usage_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DEVID)\n\t\tCHECK_APPEND_1ARG(\"devid=%llu,\", bargs->devid);\n\n\tif (flags & BTRFS_BALANCE_ARGS_DRANGE)\n\t\tCHECK_APPEND_2ARG(\"drange=%llu..%llu,\",\n\t\t\t\t  bargs->pstart, bargs->pend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_VRANGE)\n\t\tCHECK_APPEND_2ARG(\"vrange=%llu..%llu,\",\n\t\t\t\t  bargs->vstart, bargs->vend);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT)\n\t\tCHECK_APPEND_1ARG(\"limit=%llu,\", bargs->limit);\n\n\tif (flags & BTRFS_BALANCE_ARGS_LIMIT_RANGE)\n\t\tCHECK_APPEND_2ARG(\"limit=%u..%u,\",\n\t\t\t\tbargs->limit_min, bargs->limit_max);\n\n\tif (flags & BTRFS_BALANCE_ARGS_STRIPES_RANGE)\n\t\tCHECK_APPEND_2ARG(\"stripes=%u..%u,\",\n\t\t\t\t  bargs->stripes_min, bargs->stripes_max);\n\n#undef CHECK_APPEND_2ARG\n#undef CHECK_APPEND_1ARG\n#undef CHECK_APPEND_NOARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last , */\n\telse\n\t\tbuf[0] = '\\0';\n}\n\nstatic void describe_balance_start_or_resume(struct btrfs_fs_info *fs_info)\n{\n\tu32 size_buf = 1024;\n\tchar tmp_buf[192] = {'\\0'};\n\tchar *buf;\n\tchar *bp;\n\tu32 size_bp = size_buf;\n\tint ret;\n\tstruct btrfs_balance_control *bctl = fs_info->balance_ctl;\n\n\tbuf = kzalloc(size_buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn;\n\n\tbp = buf;\n\n#define CHECK_APPEND_1ARG(a, v1)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tret = snprintf(bp, size_bp, (a), (v1));\t\t\t\\\n\t\tif (ret < 0 || ret >= size_bp)\t\t\t\t\\\n\t\t\tgoto out_overflow;\t\t\t\t\\\n\t\tsize_bp -= ret;\t\t\t\t\t\t\\\n\t\tbp += ret;\t\t\t\t\t\t\\\n\t} while (0)\n\n\tif (bctl->flags & BTRFS_BALANCE_FORCE)\n\t\tCHECK_APPEND_1ARG(\"%s\", \"-f \");\n\n\tif (bctl->flags & BTRFS_BALANCE_DATA) {\n\t\tdescribe_balance_args(&bctl->data, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-d%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_METADATA) {\n\t\tdescribe_balance_args(&bctl->meta, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-m%s \", tmp_buf);\n\t}\n\n\tif (bctl->flags & BTRFS_BALANCE_SYSTEM) {\n\t\tdescribe_balance_args(&bctl->sys, tmp_buf, sizeof(tmp_buf));\n\t\tCHECK_APPEND_1ARG(\"-s%s \", tmp_buf);\n\t}\n\n#undef CHECK_APPEND_1ARG\n\nout_overflow:\n\n\tif (size_bp < size_buf)\n\t\tbuf[size_buf - size_bp - 1] = '\\0'; /* remove last \" \" */\n\tbtrfs_info(fs_info, \"balance: %s %s\",\n\t\t   (bctl->flags & BTRFS_BALANCE_RESUME) ?\n\t\t   \"resume\" : \"start\", buf);\n\n\tkfree(buf);\n}\n\n/*\n * Should be called with balance mutexe held\n */\nint btrfs_balance(struct btrfs_fs_info *fs_info,\n\t\t  struct btrfs_balance_control *bctl,\n\t\t  struct btrfs_ioctl_balance_args *bargs)\n{\n\tu64 meta_target, data_target;\n\tu64 allowed;\n\tint mixed = 0;\n\tint ret;\n\tu64 num_devices;\n\tunsigned seq;\n\tbool reducing_redundancy;\n\tint i;\n\n\tif (btrfs_fs_closing(fs_info) ||\n\t    atomic_read(&fs_info->balance_pause_req) ||\n\t    btrfs_should_cancel_balance(fs_info)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tallowed = btrfs_super_incompat_flags(fs_info->super_copy);\n\tif (allowed & BTRFS_FEATURE_INCOMPAT_MIXED_GROUPS)\n\t\tmixed = 1;\n\n\t/*\n\t * In case of mixed groups both data and meta should be picked,\n\t * and identical options should be given for both of them.\n\t */\n\tallowed = BTRFS_BALANCE_DATA | BTRFS_BALANCE_METADATA;\n\tif (mixed && (bctl->flags & allowed)) {\n\t\tif (!(bctl->flags & BTRFS_BALANCE_DATA) ||\n\t\t    !(bctl->flags & BTRFS_BALANCE_METADATA) ||\n\t\t    memcmp(&bctl->data, &bctl->meta, sizeof(bctl->data))) {\n\t\t\tbtrfs_err(fs_info,\n\t  \"balance: mixed groups data and metadata options must be the same\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * rw_devices will not change at the moment, device add/delete/replace\n\t * are exclusive\n\t */\n\tnum_devices = fs_info->fs_devices->rw_devices;\n\n\t/*\n\t * SINGLE profile on-disk has no profile bit, but in-memory we have a\n\t * special bit for it, to make it easier to distinguish.  Thus we need\n\t * to set it manually, or balance would refuse the profile.\n\t */\n\tallowed = BTRFS_AVAIL_ALLOC_BIT_SINGLE;\n\tfor (i = 0; i < ARRAY_SIZE(btrfs_raid_array); i++)\n\t\tif (num_devices >= btrfs_raid_array[i].devs_min)\n\t\t\tallowed |= btrfs_raid_array[i].bg_flag;\n\n\tif (!validate_convert_profile(fs_info, &bctl->data, allowed, \"data\") ||\n\t    !validate_convert_profile(fs_info, &bctl->meta, allowed, \"metadata\") ||\n\t    !validate_convert_profile(fs_info, &bctl->sys,  allowed, \"system\")) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Allow to reduce metadata or system integrity only if force set for\n\t * profiles with redundancy (copies, parity)\n\t */\n\tallowed = 0;\n\tfor (i = 0; i < ARRAY_SIZE(btrfs_raid_array); i++) {\n\t\tif (btrfs_raid_array[i].ncopies >= 2 ||\n\t\t    btrfs_raid_array[i].tolerated_failures >= 1)\n\t\t\tallowed |= btrfs_raid_array[i].bg_flag;\n\t}\n\tdo {\n\t\tseq = read_seqbegin(&fs_info->profiles_lock);\n\n\t\tif (((bctl->sys.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_system_alloc_bits & allowed) &&\n\t\t     !(bctl->sys.target & allowed)) ||\n\t\t    ((bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) &&\n\t\t     (fs_info->avail_metadata_alloc_bits & allowed) &&\n\t\t     !(bctl->meta.target & allowed)))\n\t\t\treducing_redundancy = true;\n\t\telse\n\t\t\treducing_redundancy = false;\n\n\t\t/* if we're not converting, the target field is uninitialized */\n\t\tmeta_target = (bctl->meta.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->meta.target : fs_info->avail_metadata_alloc_bits;\n\t\tdata_target = (bctl->data.flags & BTRFS_BALANCE_ARGS_CONVERT) ?\n\t\t\tbctl->data.target : fs_info->avail_data_alloc_bits;\n\t} while (read_seqretry(&fs_info->profiles_lock, seq));\n\n\tif (reducing_redundancy) {\n\t\tif (bctl->flags & BTRFS_BALANCE_FORCE) {\n\t\t\tbtrfs_info(fs_info,\n\t\t\t   \"balance: force reducing metadata redundancy\");\n\t\t} else {\n\t\t\tbtrfs_err(fs_info,\n\t\"balance: reduces metadata redundancy, use --force if you want this\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (btrfs_get_num_tolerated_disk_barrier_failures(meta_target) <\n\t\tbtrfs_get_num_tolerated_disk_barrier_failures(data_target)) {\n\t\tbtrfs_warn(fs_info,\n\t\"balance: metadata profile %s has lower redundancy than data profile %s\",\n\t\t\t\tbtrfs_bg_type_to_raid_name(meta_target),\n\t\t\t\tbtrfs_bg_type_to_raid_name(data_target));\n\t}\n\n\tret = insert_balance_item(fs_info, bctl);\n\tif (ret && ret != -EEXIST)\n\t\tgoto out;\n\n\tif (!(bctl->flags & BTRFS_BALANCE_RESUME)) {\n\t\tBUG_ON(ret == -EEXIST);\n\t\tBUG_ON(fs_info->balance_ctl);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tfs_info->balance_ctl = bctl;\n\t\tspin_unlock(&fs_info->balance_lock);\n\t} else {\n\t\tBUG_ON(ret != -EEXIST);\n\t\tspin_lock(&fs_info->balance_lock);\n\t\tupdate_balance_args(bctl);\n\t\tspin_unlock(&fs_info->balance_lock);\n\t}\n\n\tASSERT(!test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tset_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\tdescribe_balance_start_or_resume(fs_info);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tret = __btrfs_balance(fs_info);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (ret == -ECANCELED && atomic_read(&fs_info->balance_pause_req))\n\t\tbtrfs_info(fs_info, \"balance: paused\");\n\t/*\n\t * Balance can be canceled by:\n\t *\n\t * - Regular cancel request\n\t *   Then ret == -ECANCELED and balance_cancel_req > 0\n\t *\n\t * - Fatal signal to \"btrfs\" process\n\t *   Either the signal caught by wait_reserve_ticket() and callers\n\t *   got -EINTR, or caught by btrfs_should_cancel_balance() and\n\t *   got -ECANCELED.\n\t *   Either way, in this case balance_cancel_req = 0, and\n\t *   ret == -EINTR or ret == -ECANCELED.\n\t *\n\t * So here we only check the return value to catch canceled balance.\n\t */\n\telse if (ret == -ECANCELED || ret == -EINTR)\n\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\telse\n\t\tbtrfs_info(fs_info, \"balance: ended with status: %d\", ret);\n\n\tclear_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags);\n\n\tif (bargs) {\n\t\tmemset(bargs, 0, sizeof(*bargs));\n\t\tbtrfs_update_ioctl_balance_args(fs_info, bargs);\n\t}\n\n\tif ((ret && ret != -ECANCELED && ret != -ENOSPC) ||\n\t    balance_need_close(fs_info)) {\n\t\treset_balance_state(fs_info);\n\t\tbtrfs_exclop_finish(fs_info);\n\t}\n\n\twake_up(&fs_info->balance_wait_q);\n\n\treturn ret;\nout:\n\tif (bctl->flags & BTRFS_BALANCE_RESUME)\n\t\treset_balance_state(fs_info);\n\telse\n\t\tkfree(bctl);\n\tbtrfs_exclop_finish(fs_info);\n\n\treturn ret;\n}\n\nstatic int balance_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (fs_info->balance_ctl)\n\t\tret = btrfs_balance(fs_info, fs_info->balance_ctl, NULL);\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\treturn ret;\n}\n\nint btrfs_resume_balance_async(struct btrfs_fs_info *fs_info)\n{\n\tstruct task_struct *tsk;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn 0;\n\t}\n\tmutex_unlock(&fs_info->balance_mutex);\n\n\tif (btrfs_test_opt(fs_info, SKIP_BALANCE)) {\n\t\tbtrfs_info(fs_info, \"balance: resume skipped\");\n\t\treturn 0;\n\t}\n\n\t/*\n\t * A ro->rw remount sequence should continue with the paused balance\n\t * regardless of who pauses it, system or the user as of now, so set\n\t * the resume flag.\n\t */\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl->flags |= BTRFS_BALANCE_RESUME;\n\tspin_unlock(&fs_info->balance_lock);\n\n\ttsk = kthread_run(balance_kthread, fs_info, \"btrfs-balance\");\n\treturn PTR_ERR_OR_ZERO(tsk);\n}\n\nint btrfs_recover_balance(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_balance_control *bctl;\n\tstruct btrfs_balance_item *item;\n\tstruct btrfs_disk_balance_args disk_bargs;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_BALANCE_OBJECTID;\n\tkey.type = BTRFS_TEMPORARY_ITEM_KEY;\n\tkey.offset = 0;\n\n\tret = btrfs_search_slot(NULL, fs_info->tree_root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (ret > 0) { /* ret = -ENOENT; */\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tbctl = kzalloc(sizeof(*bctl), GFP_NOFS);\n\tif (!bctl) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_balance_item);\n\n\tbctl->flags = btrfs_balance_flags(leaf, item);\n\tbctl->flags |= BTRFS_BALANCE_RESUME;\n\n\tbtrfs_balance_data(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->data, &disk_bargs);\n\tbtrfs_balance_meta(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->meta, &disk_bargs);\n\tbtrfs_balance_sys(leaf, item, &disk_bargs);\n\tbtrfs_disk_balance_args_to_cpu(&bctl->sys, &disk_bargs);\n\n\t/*\n\t * This should never happen, as the paused balance state is recovered\n\t * during mount without any chance of other exclusive ops to collide.\n\t *\n\t * This gives the exclusive op status to balance and keeps in paused\n\t * state until user intervention (cancel or umount). If the ownership\n\t * cannot be assigned, show a message but do not fail. The balance\n\t * is in a paused state and must have fs_info::balance_ctl properly\n\t * set up.\n\t */\n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE))\n\t\tbtrfs_warn(fs_info,\n\t\"balance: cannot set exclusive op status, resume manually\");\n\n\tbtrfs_release_path(path);\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tBUG_ON(fs_info->balance_ctl);\n\tspin_lock(&fs_info->balance_lock);\n\tfs_info->balance_ctl = bctl;\n\tspin_unlock(&fs_info->balance_lock);\n\tmutex_unlock(&fs_info->balance_mutex);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_pause_balance(struct btrfs_fs_info *fs_info)\n{\n\tint ret = 0;\n\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tatomic_inc(&fs_info->balance_pause_req);\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t\t/* we are good with balance_ctl ripped off from under us */\n\t\tBUG_ON(test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tatomic_dec(&fs_info->balance_pause_req);\n\t} else {\n\t\tret = -ENOTCONN;\n\t}\n\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn ret;\n}\n\nint btrfs_cancel_balance(struct btrfs_fs_info *fs_info)\n{\n\tmutex_lock(&fs_info->balance_mutex);\n\tif (!fs_info->balance_ctl) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -ENOTCONN;\n\t}\n\n\t/*\n\t * A paused balance with the item stored on disk can be resumed at\n\t * mount time if the mount is read-write. Otherwise it's still paused\n\t * and we must not allow cancelling as it deletes the item.\n\t */\n\tif (sb_rdonly(fs_info->sb)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\treturn -EROFS;\n\t}\n\n\tatomic_inc(&fs_info->balance_cancel_req);\n\t/*\n\t * if we are running just wait and return, balance item is\n\t * deleted in btrfs_balance in this case\n\t */\n\tif (test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags)) {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\twait_event(fs_info->balance_wait_q,\n\t\t\t   !test_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\t\tmutex_lock(&fs_info->balance_mutex);\n\t} else {\n\t\tmutex_unlock(&fs_info->balance_mutex);\n\t\t/*\n\t\t * Lock released to allow other waiters to continue, we'll\n\t\t * reexamine the status again.\n\t\t */\n\t\tmutex_lock(&fs_info->balance_mutex);\n\n\t\tif (fs_info->balance_ctl) {\n\t\t\treset_balance_state(fs_info);\n\t\t\tbtrfs_exclop_finish(fs_info);\n\t\t\tbtrfs_info(fs_info, \"balance: canceled\");\n\t\t}\n\t}\n\n\tBUG_ON(fs_info->balance_ctl ||\n\t\ttest_bit(BTRFS_FS_BALANCE_RUNNING, &fs_info->flags));\n\tatomic_dec(&fs_info->balance_cancel_req);\n\tmutex_unlock(&fs_info->balance_mutex);\n\treturn 0;\n}\n\nint btrfs_uuid_scan_kthread(void *data)\n{\n\tstruct btrfs_fs_info *fs_info = data;\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tstruct btrfs_root_item root_item;\n\tu32 item_size;\n\tstruct btrfs_trans_handle *trans = NULL;\n\tbool closing = false;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\tkey.offset = 0;\n\n\twhile (1) {\n\t\tif (btrfs_fs_closing(fs_info)) {\n\t\t\tclosing = true;\n\t\t\tbreak;\n\t\t}\n\t\tret = btrfs_search_forward(root, &key, path,\n\t\t\t\tBTRFS_OLDEST_GENERATION);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (key.type != BTRFS_ROOT_ITEM_KEY ||\n\t\t    (key.objectid < BTRFS_FIRST_FREE_OBJECTID &&\n\t\t     key.objectid != BTRFS_FS_TREE_OBJECTID) ||\n\t\t    key.objectid > BTRFS_LAST_FREE_OBJECTID)\n\t\t\tgoto skip;\n\n\t\teb = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\titem_size = btrfs_item_size_nr(eb, slot);\n\t\tif (item_size < sizeof(root_item))\n\t\t\tgoto skip;\n\n\t\tread_extent_buffer(eb, &root_item,\n\t\t\t\t   btrfs_item_ptr_offset(eb, slot),\n\t\t\t\t   (int)sizeof(root_item));\n\t\tif (btrfs_root_refs(&root_item) == 0)\n\t\t\tgoto skip;\n\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid) ||\n\t\t    !btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tif (trans)\n\t\t\t\tgoto update_tree;\n\n\t\t\tbtrfs_release_path(path);\n\t\t\t/*\n\t\t\t * 1 - subvol uuid item\n\t\t\t * 1 - received_subvol uuid item\n\t\t\t */\n\t\t\ttrans = btrfs_start_transaction(fs_info->uuid_root, 2);\n\t\t\tif (IS_ERR(trans)) {\n\t\t\t\tret = PTR_ERR(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tgoto skip;\n\t\t}\nupdate_tree:\n\t\tbtrfs_release_path(path);\n\t\tif (!btrfs_is_empty_uuid(root_item.uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans, root_item.uuid,\n\t\t\t\t\t\t  BTRFS_UUID_KEY_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!btrfs_is_empty_uuid(root_item.received_uuid)) {\n\t\t\tret = btrfs_uuid_tree_add(trans,\n\t\t\t\t\t\t  root_item.received_uuid,\n\t\t\t\t\t\t BTRFS_UUID_KEY_RECEIVED_SUBVOL,\n\t\t\t\t\t\t  key.objectid);\n\t\t\tif (ret < 0) {\n\t\t\t\tbtrfs_warn(fs_info, \"uuid_tree_add failed %d\",\n\t\t\t\t\tret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nskip:\n\t\tbtrfs_release_path(path);\n\t\tif (trans) {\n\t\t\tret = btrfs_end_transaction(trans);\n\t\t\ttrans = NULL;\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (key.offset < (u64)-1) {\n\t\t\tkey.offset++;\n\t\t} else if (key.type < BTRFS_ROOT_ITEM_KEY) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t} else if (key.objectid < (u64)-1) {\n\t\t\tkey.offset = 0;\n\t\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\t\tkey.objectid++;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\tif (trans && !IS_ERR(trans))\n\t\tbtrfs_end_transaction(trans);\n\tif (ret)\n\t\tbtrfs_warn(fs_info, \"btrfs_uuid_scan_kthread failed %d\", ret);\n\telse if (!closing)\n\t\tset_bit(BTRFS_FS_UPDATE_UUID_TREE_GEN, &fs_info->flags);\n\tup(&fs_info->uuid_tree_rescan_sem);\n\treturn 0;\n}\n\nint btrfs_create_uuid_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_root *tree_root = fs_info->tree_root;\n\tstruct btrfs_root *uuid_root;\n\tstruct task_struct *task;\n\tint ret;\n\n\t/*\n\t * 1 - root node\n\t * 1 - root item\n\t */\n\ttrans = btrfs_start_transaction(tree_root, 2);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tuuid_root = btrfs_create_tree(trans, BTRFS_UUID_TREE_OBJECTID);\n\tif (IS_ERR(uuid_root)) {\n\t\tret = PTR_ERR(uuid_root);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t\treturn ret;\n\t}\n\n\tfs_info->uuid_root = uuid_root;\n\n\tret = btrfs_commit_transaction(trans);\n\tif (ret)\n\t\treturn ret;\n\n\tdown(&fs_info->uuid_tree_rescan_sem);\n\ttask = kthread_run(btrfs_uuid_scan_kthread, fs_info, \"btrfs-uuid\");\n\tif (IS_ERR(task)) {\n\t\t/* fs_info->update_uuid_tree_gen remains 0 in all error case */\n\t\tbtrfs_warn(fs_info, \"failed to start uuid_scan task\");\n\t\tup(&fs_info->uuid_tree_rescan_sem);\n\t\treturn PTR_ERR(task);\n\t}\n\n\treturn 0;\n}\n\n/*\n * shrinking a device means finding all of the device extents past\n * the new size, and then following the back refs to the chunks.\n * The chunk relocation code actually frees the device extent\n */\nint btrfs_shrink_device(struct btrfs_device *device, u64 new_size)\n{\n\tstruct btrfs_fs_info *fs_info = device->fs_info;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_dev_extent *dev_extent = NULL;\n\tstruct btrfs_path *path;\n\tu64 length;\n\tu64 chunk_offset;\n\tint ret;\n\tint slot;\n\tint failed = 0;\n\tbool retried = false;\n\tstruct extent_buffer *l;\n\tstruct btrfs_key key;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tu64 old_total = btrfs_super_total_bytes(super_copy);\n\tu64 old_size = btrfs_device_get_total_bytes(device);\n\tu64 diff;\n\tu64 start;\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\tstart = new_size;\n\tdiff = round_down(old_size - new_size, fs_info->sectorsize);\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\treturn -EINVAL;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_BACK;\n\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tbtrfs_free_path(path);\n\t\treturn PTR_ERR(trans);\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\n\tbtrfs_device_set_total_bytes(device, new_size);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes -= diff;\n\t\tatomic64_sub(diff, &fs_info->free_chunk_space);\n\t}\n\n\t/*\n\t * Once the device's size has been set to the new size, ensure all\n\t * in-memory chunks are synced to disk so that the loop below sees them\n\t * and relocates them accordingly.\n\t */\n\tif (contains_pending_extent(device, &start, diff)) {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tret = btrfs_commit_transaction(trans);\n\t\tif (ret)\n\t\t\tgoto done;\n\t} else {\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\tbtrfs_end_transaction(trans);\n\t}\n\nagain:\n\tkey.objectid = device->devid;\n\tkey.offset = (u64)-1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\n\tdo {\n\t\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\t\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_previous_item(root, path, 0, key.type);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto done;\n\t\t\tret = 0;\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tl = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tbtrfs_item_key_to_cpu(l, &key, path->slots[0]);\n\n\t\tif (key.objectid != device->devid) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tdev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\n\t\tlength = btrfs_dev_extent_length(l, dev_extent);\n\n\t\tif (key.offset + length <= new_size) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tbtrfs_release_path(path);\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\n\t\tbtrfs_release_path(path);\n\n\t\t/*\n\t\t * We may be relocating the only data chunk we have,\n\t\t * which could potentially end up with losing data's\n\t\t * raid profile, so lets allocate an empty one in\n\t\t * advance.\n\t\t */\n\t\tret = btrfs_may_alloc_data_chunk(fs_info, chunk_offset);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\t\tgoto done;\n\t\t}\n\n\t\tret = btrfs_relocate_chunk(fs_info, chunk_offset);\n\t\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\t\tif (ret == -ENOSPC) {\n\t\t\tfailed++;\n\t\t} else if (ret) {\n\t\t\tif (ret == -ETXTBSY) {\n\t\t\t\tbtrfs_warn(fs_info,\n\t\t   \"could not shrink block group %llu due to active swapfile\",\n\t\t\t\t\t   chunk_offset);\n\t\t\t}\n\t\t\tgoto done;\n\t\t}\n\t} while (key.offset-- > 0);\n\n\tif (failed && !retried) {\n\t\tfailed = 0;\n\t\tretried = true;\n\t\tgoto again;\n\t} else if (failed && retried) {\n\t\tret = -ENOSPC;\n\t\tgoto done;\n\t}\n\n\t/* Shrinking succeeded, else we would be at \"done\". */\n\ttrans = btrfs_start_transaction(root, 0);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto done;\n\t}\n\n\tmutex_lock(&fs_info->chunk_mutex);\n\t/* Clear all state bits beyond the shrunk device size */\n\tclear_extent_bits(&device->alloc_state, new_size, (u64)-1,\n\t\t\t  CHUNK_STATE_MASK);\n\n\tbtrfs_device_set_disk_total_bytes(device, new_size);\n\tif (list_empty(&device->post_commit_list))\n\t\tlist_add_tail(&device->post_commit_list,\n\t\t\t      &trans->transaction->dev_update_list);\n\n\tWARN_ON(diff > old_total);\n\tbtrfs_set_super_total_bytes(super_copy,\n\t\t\tround_down(old_total - diff, fs_info->sectorsize));\n\tmutex_unlock(&fs_info->chunk_mutex);\n\n\t/* Now btrfs_update_device() will change the on-disk size. */\n\tret = btrfs_update_device(trans, device);\n\tif (ret < 0) {\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tbtrfs_end_transaction(trans);\n\t} else {\n\t\tret = btrfs_commit_transaction(trans);\n\t}\ndone:\n\tbtrfs_free_path(path);\n\tif (ret) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tbtrfs_device_set_total_bytes(device, old_size);\n\t\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\t\tdevice->fs_devices->total_rw_bytes += diff;\n\t\tatomic64_add(diff, &fs_info->free_chunk_space);\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\treturn ret;\n}\n\nstatic int btrfs_add_system_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t   struct btrfs_key *key,\n\t\t\t   struct btrfs_chunk *chunk, int item_size)\n{\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct btrfs_disk_key disk_key;\n\tu32 array_size;\n\tu8 *ptr;\n\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\tif (array_size + item_size + sizeof(disk_key)\n\t\t\t> BTRFS_SYSTEM_CHUNK_ARRAY_SIZE)\n\t\treturn -EFBIG;\n\n\tptr = super_copy->sys_chunk_array + array_size;\n\tbtrfs_cpu_key_to_disk(&disk_key, key);\n\tmemcpy(ptr, &disk_key, sizeof(disk_key));\n\tptr += sizeof(disk_key);\n\tmemcpy(ptr, chunk, item_size);\n\titem_size += sizeof(disk_key);\n\tbtrfs_set_super_sys_array_size(super_copy, array_size + item_size);\n\n\treturn 0;\n}\n\n/*\n * sort the devices in descending order by max_avail, total_avail\n */\nstatic int btrfs_cmp_device_info(const void *a, const void *b)\n{\n\tconst struct btrfs_device_info *di_a = a;\n\tconst struct btrfs_device_info *di_b = b;\n\n\tif (di_a->max_avail > di_b->max_avail)\n\t\treturn -1;\n\tif (di_a->max_avail < di_b->max_avail)\n\t\treturn 1;\n\tif (di_a->total_avail > di_b->total_avail)\n\t\treturn -1;\n\tif (di_a->total_avail < di_b->total_avail)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void check_raid56_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_RAID56_MASK))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID56);\n}\n\nstatic void check_raid1c34_incompat_flag(struct btrfs_fs_info *info, u64 type)\n{\n\tif (!(type & (BTRFS_BLOCK_GROUP_RAID1C3 | BTRFS_BLOCK_GROUP_RAID1C4)))\n\t\treturn;\n\n\tbtrfs_set_fs_incompat(info, RAID1C34);\n}\n\n/*\n * Structure used internally for __btrfs_alloc_chunk() function.\n * Wraps needed parameters.\n */\nstruct alloc_chunk_ctl {\n\tu64 start;\n\tu64 type;\n\t/* Total number of stripes to allocate */\n\tint num_stripes;\n\t/* sub_stripes info for map */\n\tint sub_stripes;\n\t/* Stripes per device */\n\tint dev_stripes;\n\t/* Maximum number of devices to use */\n\tint devs_max;\n\t/* Minimum number of devices to use */\n\tint devs_min;\n\t/* ndevs has to be a multiple of this */\n\tint devs_increment;\n\t/* Number of copies */\n\tint ncopies;\n\t/* Number of stripes worth of bytes to store parity information */\n\tint nparity;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 dev_extent_min;\n\tu64 stripe_size;\n\tu64 chunk_size;\n\tint ndevs;\n};\n\nstatic void init_alloc_chunk_ctl_policy_regular(\n\t\t\t\tstruct btrfs_fs_devices *fs_devices,\n\t\t\t\tstruct alloc_chunk_ctl *ctl)\n{\n\tu64 type = ctl->type;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tctl->max_stripe_size = SZ_1G;\n\t\tctl->max_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* For larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tctl->max_stripe_size = SZ_1G;\n\t\telse\n\t\t\tctl->max_stripe_size = SZ_256M;\n\t\tctl->max_chunk_size = ctl->max_stripe_size;\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tctl->max_stripe_size = SZ_32M;\n\t\tctl->max_chunk_size = 2 * ctl->max_stripe_size;\n\t\tctl->devs_max = min_t(int, ctl->devs_max,\n\t\t\t\t      BTRFS_MAX_DEVS_SYS_CHUNK);\n\t} else {\n\t\tBUG();\n\t}\n\n\t/* We don't want a chunk larger than 10% of writable space */\n\tctl->max_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t\t  ctl->max_chunk_size);\n\tctl->dev_extent_min = BTRFS_STRIPE_LEN * ctl->dev_stripes;\n}\n\nstatic void init_alloc_chunk_ctl_policy_zoned(\n\t\t\t\t      struct btrfs_fs_devices *fs_devices,\n\t\t\t\t      struct alloc_chunk_ctl *ctl)\n{\n\tu64 zone_size = fs_devices->fs_info->zone_size;\n\tu64 limit;\n\tint min_num_stripes = ctl->devs_min * ctl->dev_stripes;\n\tint min_data_stripes = (min_num_stripes - ctl->nparity) / ctl->ncopies;\n\tu64 min_chunk_size = min_data_stripes * zone_size;\n\tu64 type = ctl->type;\n\n\tctl->max_stripe_size = zone_size;\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tctl->max_chunk_size = round_down(BTRFS_MAX_DATA_CHUNK_SIZE,\n\t\t\t\t\t\t zone_size);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\tctl->max_chunk_size = ctl->max_stripe_size;\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tctl->max_chunk_size = 2 * ctl->max_stripe_size;\n\t\tctl->devs_max = min_t(int, ctl->devs_max,\n\t\t\t\t      BTRFS_MAX_DEVS_SYS_CHUNK);\n\t} else {\n\t\tBUG();\n\t}\n\n\t/* We don't want a chunk larger than 10% of writable space */\n\tlimit = max(round_down(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t       zone_size),\n\t\t    min_chunk_size);\n\tctl->max_chunk_size = min(limit, ctl->max_chunk_size);\n\tctl->dev_extent_min = zone_size * ctl->dev_stripes;\n}\n\nstatic void init_alloc_chunk_ctl(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t struct alloc_chunk_ctl *ctl)\n{\n\tint index = btrfs_bg_flags_to_raid_index(ctl->type);\n\n\tctl->sub_stripes = btrfs_raid_array[index].sub_stripes;\n\tctl->dev_stripes = btrfs_raid_array[index].dev_stripes;\n\tctl->devs_max = btrfs_raid_array[index].devs_max;\n\tif (!ctl->devs_max)\n\t\tctl->devs_max = BTRFS_MAX_DEVS(fs_devices->fs_info);\n\tctl->devs_min = btrfs_raid_array[index].devs_min;\n\tctl->devs_increment = btrfs_raid_array[index].devs_increment;\n\tctl->ncopies = btrfs_raid_array[index].ncopies;\n\tctl->nparity = btrfs_raid_array[index].nparity;\n\tctl->ndevs = 0;\n\n\tswitch (fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\tinit_alloc_chunk_ctl_policy_regular(fs_devices, ctl);\n\t\tbreak;\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\tinit_alloc_chunk_ctl_policy_zoned(fs_devices, ctl);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int gather_device_info(struct btrfs_fs_devices *fs_devices,\n\t\t\t      struct alloc_chunk_ctl *ctl,\n\t\t\t      struct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = fs_devices->fs_info;\n\tstruct btrfs_device *device;\n\tu64 total_avail;\n\tu64 dev_extent_want = ctl->max_stripe_size * ctl->dev_stripes;\n\tint ret;\n\tint ndevs = 0;\n\tu64 max_avail;\n\tu64 dev_offset;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail < ctl->dev_extent_min)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(device, dev_extent_want, &dev_offset,\n\t\t\t\t\t   &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\treturn ret;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = dev_extent_want;\n\n\t\tif (max_avail < ctl->dev_extent_min) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%llu\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    ctl->dev_extent_min);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\tctl->ndevs = ndevs;\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size_regular(struct alloc_chunk_ctl *ctl,\n\t\t\t\t      struct btrfs_device_info *devices_info)\n{\n\t/* Number of stripes that count for block group size */\n\tint data_stripes;\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tctl->stripe_size = div_u64(devices_info[ctl->ndevs - 1].max_avail,\n\t\t\t\t   ctl->dev_stripes);\n\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\n\t/* This will have to be fixed for RAID1 and RAID10 over more drives */\n\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk is\n\t * really going to be in terms of logical address space, and compare\n\t * that answer with the max chunk size. If it's higher, we try to\n\t * reduce stripe_size.\n\t */\n\tif (ctl->stripe_size * data_stripes > ctl->max_chunk_size) {\n\t\t/*\n\t\t * Reduce stripe_size, round it up to a 16MB boundary again and\n\t\t * then use it, unless it ends up being even bigger than the\n\t\t * previous value we had already.\n\t\t */\n\t\tctl->stripe_size = min(round_up(div_u64(ctl->max_chunk_size,\n\t\t\t\t\t\t\tdata_stripes), SZ_16M),\n\t\t\t\t       ctl->stripe_size);\n\t}\n\n\t/* Align to BTRFS_STRIPE_LEN */\n\tctl->stripe_size = round_down(ctl->stripe_size, BTRFS_STRIPE_LEN);\n\tctl->chunk_size = ctl->stripe_size * data_stripes;\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size_zoned(struct alloc_chunk_ctl *ctl,\n\t\t\t\t    struct btrfs_device_info *devices_info)\n{\n\tu64 zone_size = devices_info[0].dev->zone_info->zone_size;\n\t/* Number of stripes that count for block group size */\n\tint data_stripes;\n\n\t/*\n\t * It should hold because:\n\t *    dev_extent_min == dev_extent_want == zone_size * dev_stripes\n\t */\n\tASSERT(devices_info[ctl->ndevs - 1].max_avail == ctl->dev_extent_min);\n\n\tctl->stripe_size = zone_size;\n\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\n\t/* stripe_size is fixed in zoned filesysmte. Reduce ndevs instead. */\n\tif (ctl->stripe_size * data_stripes > ctl->max_chunk_size) {\n\t\tctl->ndevs = div_u64(div_u64(ctl->max_chunk_size * ctl->ncopies,\n\t\t\t\t\t     ctl->stripe_size) + ctl->nparity,\n\t\t\t\t     ctl->dev_stripes);\n\t\tctl->num_stripes = ctl->ndevs * ctl->dev_stripes;\n\t\tdata_stripes = (ctl->num_stripes - ctl->nparity) / ctl->ncopies;\n\t\tASSERT(ctl->stripe_size * data_stripes <= ctl->max_chunk_size);\n\t}\n\n\tctl->chunk_size = ctl->stripe_size * data_stripes;\n\n\treturn 0;\n}\n\nstatic int decide_stripe_size(struct btrfs_fs_devices *fs_devices,\n\t\t\t      struct alloc_chunk_ctl *ctl,\n\t\t\t      struct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = fs_devices->fs_info;\n\n\t/*\n\t * Round down to number of usable stripes, devs_increment can be any\n\t * number so we can't use round_down() that requires power of 2, while\n\t * rounddown is safe.\n\t */\n\tctl->ndevs = rounddown(ctl->ndevs, ctl->devs_increment);\n\n\tif (ctl->ndevs < ctl->devs_min) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ctl->ndevs, ctl->devs_min);\n\t\t}\n\t\treturn -ENOSPC;\n\t}\n\n\tctl->ndevs = min(ctl->ndevs, ctl->devs_max);\n\n\tswitch (fs_devices->chunk_alloc_policy) {\n\tcase BTRFS_CHUNK_ALLOC_REGULAR:\n\t\treturn decide_stripe_size_regular(ctl, devices_info);\n\tcase BTRFS_CHUNK_ALLOC_ZONED:\n\t\treturn decide_stripe_size_zoned(ctl, devices_info);\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic struct btrfs_block_group *create_chunk(struct btrfs_trans_handle *trans,\n\t\t\tstruct alloc_chunk_ctl *ctl,\n\t\t\tstruct btrfs_device_info *devices_info)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct btrfs_block_group *block_group;\n\tstruct extent_map *em;\n\tu64 start = ctl->start;\n\tu64 type = ctl->type;\n\tint ret;\n\tint i;\n\tint j;\n\n\tmap = kmalloc(map_lookup_size(ctl->num_stripes), GFP_NOFS);\n\tif (!map)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmap->num_stripes = ctl->num_stripes;\n\n\tfor (i = 0; i < ctl->ndevs; ++i) {\n\t\tfor (j = 0; j < ctl->dev_stripes; ++j) {\n\t\t\tint s = i * ctl->dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * ctl->stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = ctl->sub_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, ctl->chunk_size);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = ctl->chunk_size;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = ctl->stripe_size;\n\n\tem_tree = &info->mapping_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(ret);\n\t}\n\twrite_unlock(&em_tree->lock);\n\n\tblock_group = btrfs_make_block_group(trans, 0, type, start, ctl->chunk_size);\n\tif (IS_ERR(block_group))\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\tbtrfs_device_set_bytes_used(dev,\n\t\t\t\t\t    dev->bytes_used + ctl->stripe_size);\n\t\tif (list_empty(&dev->post_commit_list))\n\t\t\tlist_add_tail(&dev->post_commit_list,\n\t\t\t\t      &trans->transaction->dev_update_list);\n\t}\n\n\tatomic64_sub(ctl->stripe_size * map->num_stripes,\n\t\t     &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\tcheck_raid1c34_incompat_flag(info, type);\n\n\treturn block_group;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\n\treturn block_group;\n}\n\nstruct btrfs_block_group *btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t\t\t    u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tstruct alloc_chunk_ctl ctl;\n\tstruct btrfs_block_group *block_group;\n\tint ret;\n\n\tlockdep_assert_held(&info->chunk_mutex);\n\n\tif (!alloc_profile_is_valid(type, 0)) {\n\t\tASSERT(0);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\tif (!(type & BTRFS_BLOCK_GROUP_TYPE_MASK)) {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\", type);\n\t\tASSERT(0);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctl.start = find_next_chunk(info);\n\tctl.type = type;\n\tinit_alloc_chunk_ctl(fs_devices, &ctl);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = gather_device_info(fs_devices, &ctl, devices_info);\n\tif (ret < 0) {\n\t\tblock_group = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tret = decide_stripe_size(fs_devices, &ctl, devices_info);\n\tif (ret < 0) {\n\t\tblock_group = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tblock_group = create_chunk(trans, &ctl, devices_info);\n\nout:\n\tkfree(devices_info);\n\treturn block_group;\n}\n\n/*\n * This function, btrfs_chunk_alloc_add_chunk_item(), typically belongs to the\n * phase 1 of chunk allocation. It belongs to phase 2 only when allocating system\n * chunks.\n *\n * See the comment at btrfs_chunk_alloc() for details about the chunk allocation\n * phases.\n */\nint btrfs_chunk_alloc_add_chunk_item(struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_block_group *bg)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *extent_root = fs_info->extent_root;\n\tstruct btrfs_root *chunk_root = fs_info->chunk_root;\n\tstruct btrfs_key key;\n\tstruct btrfs_chunk *chunk;\n\tstruct btrfs_stripe *stripe;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tsize_t item_size;\n\tint i;\n\tint ret;\n\n\t/*\n\t * We take the chunk_mutex for 2 reasons:\n\t *\n\t * 1) Updates and insertions in the chunk btree must be done while holding\n\t *    the chunk_mutex, as well as updating the system chunk array in the\n\t *    superblock. See the comment on top of btrfs_chunk_alloc() for the\n\t *    details;\n\t *\n\t * 2) To prevent races with the final phase of a device replace operation\n\t *    that replaces the device object associated with the map's stripes,\n\t *    because the device object's id can change at any time during that\n\t *    final phase of the device replace operation\n\t *    (dev-replace.c:btrfs_dev_replace_finishing()), so we could grab the\n\t *    replaced device and then see it with an ID of BTRFS_DEV_REPLACE_DEVID,\n\t *    which would cause a failure when updating the device item, which does\n\t *    not exists, or persisting a stripe of the chunk item with such ID.\n\t *    Here we can't use the device_list_mutex because our caller already\n\t *    has locked the chunk_mutex, and the final phase of device replace\n\t *    acquires both mutexes - first the device_list_mutex and then the\n\t *    chunk_mutex. Using any of those two mutexes protects us from a\n\t *    concurrent device replace.\n\t */\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tem = btrfs_get_chunk_map(fs_info, bg->start, bg->length);\n\tif (IS_ERR(em)) {\n\t\tret = PTR_ERR(em);\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\treturn ret;\n\t}\n\n\tmap = em->map_lookup;\n\titem_size = btrfs_chunk_item_size(map->num_stripes);\n\n\tchunk = kzalloc(item_size, GFP_NOFS);\n\tif (!chunk) {\n\t\tret = -ENOMEM;\n\t\tbtrfs_abort_transaction(trans, ret);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\n\t\tret = btrfs_update_device(trans, device);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tstripe = &chunk->stripe;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tstruct btrfs_device *device = map->stripes[i].dev;\n\t\tconst u64 dev_offset = map->stripes[i].physical;\n\n\t\tbtrfs_set_stack_stripe_devid(stripe, device->devid);\n\t\tbtrfs_set_stack_stripe_offset(stripe, dev_offset);\n\t\tmemcpy(stripe->dev_uuid, device->uuid, BTRFS_UUID_SIZE);\n\t\tstripe++;\n\t}\n\n\tbtrfs_set_stack_chunk_length(chunk, bg->length);\n\tbtrfs_set_stack_chunk_owner(chunk, extent_root->root_key.objectid);\n\tbtrfs_set_stack_chunk_stripe_len(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_type(chunk, map->type);\n\tbtrfs_set_stack_chunk_num_stripes(chunk, map->num_stripes);\n\tbtrfs_set_stack_chunk_io_align(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_io_width(chunk, map->stripe_len);\n\tbtrfs_set_stack_chunk_sector_size(chunk, fs_info->sectorsize);\n\tbtrfs_set_stack_chunk_sub_stripes(chunk, map->sub_stripes);\n\n\tkey.objectid = BTRFS_FIRST_CHUNK_TREE_OBJECTID;\n\tkey.type = BTRFS_CHUNK_ITEM_KEY;\n\tkey.offset = bg->start;\n\n\tret = btrfs_insert_item(trans, chunk_root, &key, chunk, item_size);\n\tif (ret)\n\t\tgoto out;\n\n\tbg->chunk_item_inserted = 1;\n\n\tif (map->type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tret = btrfs_add_system_chunk(fs_info, &key, chunk, item_size);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\tkfree(chunk);\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic noinline int init_first_rw_device(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tu64 alloc_profile;\n\tstruct btrfs_block_group *meta_bg;\n\tstruct btrfs_block_group *sys_bg;\n\n\t/*\n\t * When adding a new device for sprouting, the seed device is read-only\n\t * so we must first allocate a metadata and a system chunk. But before\n\t * adding the block group items to the extent, device and chunk btrees,\n\t * we must first:\n\t *\n\t * 1) Create both chunks without doing any changes to the btrees, as\n\t *    otherwise we would get -ENOSPC since the block groups from the\n\t *    seed device are read-only;\n\t *\n\t * 2) Add the device item for the new sprout device - finishing the setup\n\t *    of a new block group requires updating the device item in the chunk\n\t *    btree, so it must exist when we attempt to do it. The previous step\n\t *    ensures this does not fail with -ENOSPC.\n\t *\n\t * After that we can add the block group items to their btrees:\n\t * update existing device item in the chunk btree, add a new block group\n\t * item to the extent btree, add a new chunk item to the chunk btree and\n\t * finally add the new device extent items to the devices btree.\n\t */\n\n\talloc_profile = btrfs_metadata_alloc_profile(fs_info);\n\tmeta_bg = btrfs_alloc_chunk(trans, alloc_profile);\n\tif (IS_ERR(meta_bg))\n\t\treturn PTR_ERR(meta_bg);\n\n\talloc_profile = btrfs_system_alloc_profile(fs_info);\n\tsys_bg = btrfs_alloc_chunk(trans, alloc_profile);\n\tif (IS_ERR(sys_bg))\n\t\treturn PTR_ERR(sys_bg);\n\n\treturn 0;\n}\n\nstatic inline int btrfs_chunk_max_errors(struct map_lookup *map)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(map->type);\n\n\treturn btrfs_raid_array[index].tolerated_failures;\n}\n\nint btrfs_chunk_readonly(struct btrfs_fs_info *fs_info, u64 chunk_offset)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint readonly = 0;\n\tint miss_ndevs = 0;\n\tint i;\n\n\tem = btrfs_get_chunk_map(fs_info, chunk_offset, 1);\n\tif (IS_ERR(em))\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\tmiss_ndevs++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE,\n\t\t\t\t\t&map->stripes[i].dev->dev_state)) {\n\t\t\treadonly = 1;\n\t\t\tgoto end;\n\t\t}\n\t}\n\n\t/*\n\t * If the number of missing devices is larger than max errors,\n\t * we can not write the data into that chunk successfully, so\n\t * set it readonly.\n\t */\n\tif (miss_ndevs > btrfs_chunk_max_errors(map))\n\t\treadonly = 1;\nend:\n\tfree_extent_map(em);\n\treturn readonly;\n}\n\nvoid btrfs_mapping_tree_free(struct extent_map_tree *tree)\n{\n\tstruct extent_map *em;\n\n\twhile (1) {\n\t\twrite_lock(&tree->lock);\n\t\tem = lookup_extent_mapping(tree, 0, (u64)-1);\n\t\tif (em)\n\t\t\tremove_extent_mapping(tree, em);\n\t\twrite_unlock(&tree->lock);\n\t\tif (!em)\n\t\t\tbreak;\n\t\t/* once for us */\n\t\tfree_extent_map(em);\n\t\t/* once for the tree */\n\t\tfree_extent_map(em);\n\t}\n}\n\nint btrfs_num_copies(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\tif (IS_ERR(em))\n\t\t/*\n\t\t * We could return errors for these cases, but that could get\n\t\t * ugly and we'd probably do the same thing which is just not do\n\t\t * anything else and exit, so return 1 so the callers don't try\n\t\t * to use other copies.\n\t\t */\n\t\treturn 1;\n\n\tmap = em->map_lookup;\n\tif (map->type & (BTRFS_BLOCK_GROUP_DUP | BTRFS_BLOCK_GROUP_RAID1_MASK))\n\t\tret = map->num_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tret = map->sub_stripes;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tret = 2;\n\telse if (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t/*\n\t\t * There could be two corrupted data stripes, we need\n\t\t * to loop retry in order to rebuild the correct data.\n\t\t *\n\t\t * Fail a stripe at a time on every retry except the\n\t\t * stripe under reconstruction.\n\t\t */\n\t\tret = map->num_stripes;\n\telse\n\t\tret = 1;\n\tfree_extent_map(em);\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (btrfs_dev_replace_is_ongoing(&fs_info->dev_replace) &&\n\t    fs_info->dev_replace.tgtdev)\n\t\tret++;\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\treturn ret;\n}\n\nunsigned long btrfs_full_stripe_len(struct btrfs_fs_info *fs_info,\n\t\t\t\t    u64 logical)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tunsigned long len = fs_info->sectorsize;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif (!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tlen = map->stripe_len * nr_data_stripes(map);\n\t\tfree_extent_map(em);\n\t}\n\treturn len;\n}\n\nint btrfs_is_parity_mirror(struct btrfs_fs_info *fs_info, u64 logical, u64 len)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tint ret = 0;\n\n\tem = btrfs_get_chunk_map(fs_info, logical, len);\n\n\tif(!WARN_ON(IS_ERR(em))) {\n\t\tmap = em->map_lookup;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK)\n\t\t\tret = 1;\n\t\tfree_extent_map(em);\n\t}\n\treturn ret;\n}\n\nstatic int find_live_mirror(struct btrfs_fs_info *fs_info,\n\t\t\t    struct map_lookup *map, int first,\n\t\t\t    int dev_replace_is_ongoing)\n{\n\tint i;\n\tint num_stripes;\n\tint preferred_mirror;\n\tint tolerance;\n\tstruct btrfs_device *srcdev;\n\n\tASSERT((map->type &\n\t\t (BTRFS_BLOCK_GROUP_RAID1_MASK | BTRFS_BLOCK_GROUP_RAID10)));\n\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID10)\n\t\tnum_stripes = map->sub_stripes;\n\telse\n\t\tnum_stripes = map->num_stripes;\n\n\tswitch (fs_info->fs_devices->read_policy) {\n\tdefault:\n\t\t/* Shouldn't happen, just warn and use pid instead of failing */\n\t\tbtrfs_warn_rl(fs_info,\n\t\t\t      \"unknown read_policy type %u, reset to pid\",\n\t\t\t      fs_info->fs_devices->read_policy);\n\t\tfs_info->fs_devices->read_policy = BTRFS_READ_POLICY_PID;\n\t\tfallthrough;\n\tcase BTRFS_READ_POLICY_PID:\n\t\tpreferred_mirror = first + (current->pid % num_stripes);\n\t\tbreak;\n\t}\n\n\tif (dev_replace_is_ongoing &&\n\t    fs_info->dev_replace.cont_reading_from_srcdev_mode ==\n\t     BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_AVOID)\n\t\tsrcdev = fs_info->dev_replace.srcdev;\n\telse\n\t\tsrcdev = NULL;\n\n\t/*\n\t * try to avoid the drive that is the source drive for a\n\t * dev-replace procedure, only choose it if no other non-missing\n\t * mirror is available\n\t */\n\tfor (tolerance = 0; tolerance < 2; tolerance++) {\n\t\tif (map->stripes[preferred_mirror].dev->bdev &&\n\t\t    (tolerance || map->stripes[preferred_mirror].dev != srcdev))\n\t\t\treturn preferred_mirror;\n\t\tfor (i = first; i < first + num_stripes; i++) {\n\t\t\tif (map->stripes[i].dev->bdev &&\n\t\t\t    (tolerance || map->stripes[i].dev != srcdev))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\t/* we couldn't find one that doesn't fail.  Just return something\n\t * and the io error handling code will clean up eventually\n\t */\n\treturn preferred_mirror;\n}\n\n/* Bubble-sort the stripe set to put the parity/syndrome stripes last */\nstatic void sort_parity_stripes(struct btrfs_bio *bbio, int num_stripes)\n{\n\tint i;\n\tint again = 1;\n\n\twhile (again) {\n\t\tagain = 0;\n\t\tfor (i = 0; i < num_stripes - 1; i++) {\n\t\t\t/* Swap if parity is on a smaller index */\n\t\t\tif (bbio->raid_map[i] > bbio->raid_map[i + 1]) {\n\t\t\t\tswap(bbio->stripes[i], bbio->stripes[i + 1]);\n\t\t\t\tswap(bbio->raid_map[i], bbio->raid_map[i + 1]);\n\t\t\t\tagain = 1;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic struct btrfs_bio *alloc_btrfs_bio(int total_stripes, int real_stripes)\n{\n\tstruct btrfs_bio *bbio = kzalloc(\n\t\t /* the size of the btrfs_bio */\n\t\tsizeof(struct btrfs_bio) +\n\t\t/* plus the variable array for the stripes */\n\t\tsizeof(struct btrfs_bio_stripe) * (total_stripes) +\n\t\t/* plus the variable array for the tgt dev */\n\t\tsizeof(int) * (real_stripes) +\n\t\t/*\n\t\t * plus the raid_map, which includes both the tgt dev\n\t\t * and the stripes\n\t\t */\n\t\tsizeof(u64) * (total_stripes),\n\t\tGFP_NOFS|__GFP_NOFAIL);\n\n\tatomic_set(&bbio->error, 0);\n\trefcount_set(&bbio->refs, 1);\n\n\tbbio->tgtdev_map = (int *)(bbio->stripes + total_stripes);\n\tbbio->raid_map = (u64 *)(bbio->tgtdev_map + real_stripes);\n\n\treturn bbio;\n}\n\nvoid btrfs_get_bbio(struct btrfs_bio *bbio)\n{\n\tWARN_ON(!refcount_read(&bbio->refs));\n\trefcount_inc(&bbio->refs);\n}\n\nvoid btrfs_put_bbio(struct btrfs_bio *bbio)\n{\n\tif (!bbio)\n\t\treturn;\n\tif (refcount_dec_and_test(&bbio->refs))\n\t\tkfree(bbio);\n}\n\n/* can REQ_OP_DISCARD be sent with other REQ like REQ_OP_WRITE? */\n/*\n * Please note that, discard won't be sent to target device of device\n * replace.\n */\nstatic int __btrfs_map_block_for_discard(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 *length_ret,\n\t\t\t\t\t struct btrfs_bio **bbio_ret)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_bio *bbio;\n\tu64 length = *length_ret;\n\tu64 offset;\n\tu64 stripe_nr;\n\tu64 stripe_nr_end;\n\tu64 stripe_end_offset;\n\tu64 stripe_cnt;\n\tu64 stripe_len;\n\tu64 stripe_offset;\n\tu64 num_stripes;\n\tu32 stripe_index;\n\tu32 factor = 0;\n\tu32 sub_stripes = 0;\n\tu64 stripes_per_dev = 0;\n\tu32 remaining_stripes = 0;\n\tu32 last_stripe = 0;\n\tint ret = 0;\n\tint i;\n\n\t/* discard always return a bbio */\n\tASSERT(bbio_ret);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, length);\n\tif (IS_ERR(em))\n\t\treturn PTR_ERR(em);\n\n\tmap = em->map_lookup;\n\t/* we don't discard raid56 yet */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\toffset = logical - em->start;\n\tlength = min_t(u64, em->start + em->len - logical, length);\n\t*length_ret = length;\n\n\tstripe_len = map->stripe_len;\n\t/*\n\t * stripe_nr counts the total number of stripes we have to stride\n\t * to get to this block\n\t */\n\tstripe_nr = div64_u64(offset, stripe_len);\n\n\t/* stripe_offset is the offset of this block in its stripe */\n\tstripe_offset = offset - stripe_nr * stripe_len;\n\n\tstripe_nr_end = round_up(offset + length, map->stripe_len);\n\tstripe_nr_end = div64_u64(stripe_nr_end, map->stripe_len);\n\tstripe_cnt = stripe_nr_end - stripe_nr;\n\tstripe_end_offset = stripe_nr_end * map->stripe_len -\n\t\t\t    (offset + length);\n\t/*\n\t * after this, stripe_nr is the number of stripes on this\n\t * device we have to walk to find the data, and stripe_index is\n\t * the number of our device in the stripe array\n\t */\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID0)\n\t\t\tsub_stripes = 1;\n\t\telse\n\t\t\tsub_stripes = map->sub_stripes;\n\n\t\tfactor = map->num_stripes / sub_stripes;\n\t\tnum_stripes = min_t(u64, map->num_stripes,\n\t\t\t\t    sub_stripes * stripe_cnt);\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= sub_stripes;\n\t\tstripes_per_dev = div_u64_rem(stripe_cnt, factor,\n\t\t\t\t\t      &remaining_stripes);\n\t\tdiv_u64_rem(stripe_nr_end - 1, factor, &last_stripe);\n\t\tlast_stripe *= sub_stripes;\n\t} else if (map->type & (BTRFS_BLOCK_GROUP_RAID1_MASK |\n\t\t\t\tBTRFS_BLOCK_GROUP_DUP)) {\n\t\tnum_stripes = map->num_stripes;\n\t} else {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t}\n\n\tbbio = alloc_btrfs_bio(num_stripes, 0);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical =\n\t\t\tmap->stripes[stripe_index].physical +\n\t\t\tstripe_offset + stripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev = map->stripes[stripe_index].dev;\n\n\t\tif (map->type & (BTRFS_BLOCK_GROUP_RAID0 |\n\t\t\t\t BTRFS_BLOCK_GROUP_RAID10)) {\n\t\t\tbbio->stripes[i].length = stripes_per_dev *\n\t\t\t\tmap->stripe_len;\n\n\t\t\tif (i / sub_stripes < remaining_stripes)\n\t\t\t\tbbio->stripes[i].length +=\n\t\t\t\t\tmap->stripe_len;\n\n\t\t\t/*\n\t\t\t * Special for the first stripe and\n\t\t\t * the last stripe:\n\t\t\t *\n\t\t\t * |-------|...|-------|\n\t\t\t *     |----------|\n\t\t\t *    off     end_off\n\t\t\t */\n\t\t\tif (i < sub_stripes)\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_offset;\n\n\t\t\tif (stripe_index >= last_stripe &&\n\t\t\t    stripe_index <= (last_stripe +\n\t\t\t\t\t     sub_stripes - 1))\n\t\t\t\tbbio->stripes[i].length -=\n\t\t\t\t\tstripe_end_offset;\n\n\t\t\tif (i == sub_stripes - 1)\n\t\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\tbbio->stripes[i].length = length;\n\t\t}\n\n\t\tstripe_index++;\n\t\tif (stripe_index == map->num_stripes) {\n\t\t\tstripe_index = 0;\n\t\t\tstripe_nr++;\n\t\t}\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\n/*\n * In dev-replace case, for repair case (that's the only case where the mirror\n * is selected explicitly when calling btrfs_map_block), blocks left of the\n * left cursor can also be read from the target drive.\n *\n * For REQ_GET_READ_MIRRORS, the target drive is added as the last one to the\n * array of stripes.\n * For READ, it also needs to be supported using the same mirror number.\n *\n * If the requested block is not left of the left cursor, EIO is returned. This\n * can happen because btrfs_num_copies() returns one more in the dev-replace\n * case.\n */\nstatic int get_extra_mirror_from_replace(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t u64 logical, u64 length,\n\t\t\t\t\t u64 srcdev_devid, int *mirror_num,\n\t\t\t\t\t u64 *physical)\n{\n\tstruct btrfs_bio *bbio = NULL;\n\tint num_stripes;\n\tint index_srcdev = 0;\n\tint found = 0;\n\tu64 physical_of_found = 0;\n\tint i;\n\tint ret = 0;\n\n\tret = __btrfs_map_block(fs_info, BTRFS_MAP_GET_READ_MIRRORS,\n\t\t\t\tlogical, &length, &bbio, 0, 0);\n\tif (ret) {\n\t\tASSERT(bbio == NULL);\n\t\treturn ret;\n\t}\n\n\tnum_stripes = bbio->num_stripes;\n\tif (*mirror_num > num_stripes) {\n\t\t/*\n\t\t * BTRFS_MAP_GET_READ_MIRRORS does not contain this mirror,\n\t\t * that means that the requested area is not left of the left\n\t\t * cursor\n\t\t */\n\t\tbtrfs_put_bbio(bbio);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * process the rest of the function using the mirror_num of the source\n\t * drive. Therefore look it up first.  At the end, patch the device\n\t * pointer to the one of the target drive.\n\t */\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tif (bbio->stripes[i].dev->devid != srcdev_devid)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * In case of DUP, in order to keep it simple, only add the\n\t\t * mirror with the lowest physical address\n\t\t */\n\t\tif (found &&\n\t\t    physical_of_found <= bbio->stripes[i].physical)\n\t\t\tcontinue;\n\n\t\tindex_srcdev = i;\n\t\tfound = 1;\n\t\tphysical_of_found = bbio->stripes[i].physical;\n\t}\n\n\tbtrfs_put_bbio(bbio);\n\n\tASSERT(found);\n\tif (!found)\n\t\treturn -EIO;\n\n\t*mirror_num = index_srcdev + 1;\n\t*physical = physical_of_found;\n\treturn ret;\n}\n\nstatic bool is_block_group_to_copy(struct btrfs_fs_info *fs_info, u64 logical)\n{\n\tstruct btrfs_block_group *cache;\n\tbool ret;\n\n\t/* Non zoned filesystem does not use \"to_copy\" flag */\n\tif (!btrfs_is_zoned(fs_info))\n\t\treturn false;\n\n\tcache = btrfs_lookup_block_group(fs_info, logical);\n\n\tspin_lock(&cache->lock);\n\tret = cache->to_copy;\n\tspin_unlock(&cache->lock);\n\n\tbtrfs_put_block_group(cache);\n\treturn ret;\n}\n\nstatic void handle_ops_on_dev_replace(enum btrfs_map_op op,\n\t\t\t\t      struct btrfs_bio **bbio_ret,\n\t\t\t\t      struct btrfs_dev_replace *dev_replace,\n\t\t\t\t      u64 logical,\n\t\t\t\t      int *num_stripes_ret, int *max_errors_ret)\n{\n\tstruct btrfs_bio *bbio = *bbio_ret;\n\tu64 srcdev_devid = dev_replace->srcdev->devid;\n\tint tgtdev_indexes = 0;\n\tint num_stripes = *num_stripes_ret;\n\tint max_errors = *max_errors_ret;\n\tint i;\n\n\tif (op == BTRFS_MAP_WRITE) {\n\t\tint index_where_to_add;\n\n\t\t/*\n\t\t * A block group which have \"to_copy\" set will eventually\n\t\t * copied by dev-replace process. We can avoid cloning IO here.\n\t\t */\n\t\tif (is_block_group_to_copy(dev_replace->srcdev->fs_info, logical))\n\t\t\treturn;\n\n\t\t/*\n\t\t * duplicate the write operations while the dev replace\n\t\t * procedure is running. Since the copying of the old disk to\n\t\t * the new disk takes place at run time while the filesystem is\n\t\t * mounted writable, the regular write operations to the old\n\t\t * disk have to be duplicated to go to the new disk as well.\n\t\t *\n\t\t * Note that device->missing is handled by the caller, and that\n\t\t * the write to the old disk is already set up in the stripes\n\t\t * array.\n\t\t */\n\t\tindex_where_to_add = num_stripes;\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/* write to new disk, too */\n\t\t\t\tstruct btrfs_bio_stripe *new =\n\t\t\t\t\tbbio->stripes + index_where_to_add;\n\t\t\t\tstruct btrfs_bio_stripe *old =\n\t\t\t\t\tbbio->stripes + i;\n\n\t\t\t\tnew->physical = old->physical;\n\t\t\t\tnew->length = old->length;\n\t\t\t\tnew->dev = dev_replace->tgtdev;\n\t\t\t\tbbio->tgtdev_map[i] = index_where_to_add;\n\t\t\t\tindex_where_to_add++;\n\t\t\t\tmax_errors++;\n\t\t\t\ttgtdev_indexes++;\n\t\t\t}\n\t\t}\n\t\tnum_stripes = index_where_to_add;\n\t} else if (op == BTRFS_MAP_GET_READ_MIRRORS) {\n\t\tint index_srcdev = 0;\n\t\tint found = 0;\n\t\tu64 physical_of_found = 0;\n\n\t\t/*\n\t\t * During the dev-replace procedure, the target drive can also\n\t\t * be used to read data in case it is needed to repair a corrupt\n\t\t * block elsewhere. This is possible if the requested area is\n\t\t * left of the left cursor. In this area, the target drive is a\n\t\t * full copy of the source drive.\n\t\t */\n\t\tfor (i = 0; i < num_stripes; i++) {\n\t\t\tif (bbio->stripes[i].dev->devid == srcdev_devid) {\n\t\t\t\t/*\n\t\t\t\t * In case of DUP, in order to keep it simple,\n\t\t\t\t * only add the mirror with the lowest physical\n\t\t\t\t * address\n\t\t\t\t */\n\t\t\t\tif (found &&\n\t\t\t\t    physical_of_found <=\n\t\t\t\t     bbio->stripes[i].physical)\n\t\t\t\t\tcontinue;\n\t\t\t\tindex_srcdev = i;\n\t\t\t\tfound = 1;\n\t\t\t\tphysical_of_found = bbio->stripes[i].physical;\n\t\t\t}\n\t\t}\n\t\tif (found) {\n\t\t\tstruct btrfs_bio_stripe *tgtdev_stripe =\n\t\t\t\tbbio->stripes + num_stripes;\n\n\t\t\ttgtdev_stripe->physical = physical_of_found;\n\t\t\ttgtdev_stripe->length =\n\t\t\t\tbbio->stripes[index_srcdev].length;\n\t\t\ttgtdev_stripe->dev = dev_replace->tgtdev;\n\t\t\tbbio->tgtdev_map[index_srcdev] = num_stripes;\n\n\t\t\ttgtdev_indexes++;\n\t\t\tnum_stripes++;\n\t\t}\n\t}\n\n\t*num_stripes_ret = num_stripes;\n\t*max_errors_ret = max_errors;\n\tbbio->num_tgtdevs = tgtdev_indexes;\n\t*bbio_ret = bbio;\n}\n\nstatic bool need_full_stripe(enum btrfs_map_op op)\n{\n\treturn (op == BTRFS_MAP_WRITE || op == BTRFS_MAP_GET_READ_MIRRORS);\n}\n\n/*\n * Calculate the geometry of a particular (address, len) tuple. This\n * information is used to calculate how big a particular bio can get before it\n * straddles a stripe.\n *\n * @fs_info: the filesystem\n * @em:      mapping containing the logical extent\n * @op:      type of operation - write or read\n * @logical: address that we want to figure out the geometry of\n * @io_geom: pointer used to return values\n *\n * Returns < 0 in case a chunk for the given logical address cannot be found,\n * usually shouldn't happen unless @logical is corrupted, 0 otherwise.\n */\nint btrfs_get_io_geometry(struct btrfs_fs_info *fs_info, struct extent_map *em,\n\t\t\t  enum btrfs_map_op op, u64 logical,\n\t\t\t  struct btrfs_io_geometry *io_geom)\n{\n\tstruct map_lookup *map;\n\tu64 len;\n\tu64 offset;\n\tu64 stripe_offset;\n\tu64 stripe_nr;\n\tu64 stripe_len;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\tint data_stripes;\n\n\tASSERT(op != BTRFS_MAP_DISCARD);\n\n\tmap = em->map_lookup;\n\t/* Offset of this logical address in the chunk */\n\toffset = logical - em->start;\n\t/* Len of a stripe in a chunk */\n\tstripe_len = map->stripe_len;\n\t/* Stripe where this block falls in */\n\tstripe_nr = div64_u64(offset, stripe_len);\n\t/* Offset of stripe in the chunk */\n\tstripe_offset = stripe_nr * stripe_len;\n\tif (offset < stripe_offset) {\n\t\tbtrfs_crit(fs_info,\n\"stripe math has gone wrong, stripe_offset=%llu offset=%llu start=%llu logical=%llu stripe_len=%llu\",\n\t\t\tstripe_offset, offset, em->start, logical, stripe_len);\n\t\treturn -EINVAL;\n\t}\n\n\t/* stripe_offset is the offset of this block in its stripe */\n\tstripe_offset = offset - stripe_offset;\n\tdata_stripes = nr_data_stripes(map);\n\n\tif (map->type & BTRFS_BLOCK_GROUP_PROFILE_MASK) {\n\t\tu64 max_len = stripe_len - stripe_offset;\n\n\t\t/*\n\t\t * In case of raid56, we need to know the stripe aligned start\n\t\t */\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\t\tunsigned long full_stripe_len = stripe_len * data_stripes;\n\t\t\traid56_full_stripe_start = offset;\n\n\t\t\t/*\n\t\t\t * Allow a write of a full stripe, but make sure we\n\t\t\t * don't allow straddling of stripes\n\t\t\t */\n\t\t\traid56_full_stripe_start = div64_u64(raid56_full_stripe_start,\n\t\t\t\t\tfull_stripe_len);\n\t\t\traid56_full_stripe_start *= full_stripe_len;\n\n\t\t\t/*\n\t\t\t * For writes to RAID[56], allow a full stripeset across\n\t\t\t * all disks. For other RAID types and for RAID[56]\n\t\t\t * reads, just allow a single stripe (on a single disk).\n\t\t\t */\n\t\t\tif (op == BTRFS_MAP_WRITE) {\n\t\t\t\tmax_len = stripe_len * data_stripes -\n\t\t\t\t\t  (offset - raid56_full_stripe_start);\n\t\t\t}\n\t\t}\n\t\tlen = min_t(u64, em->len - offset, max_len);\n\t} else {\n\t\tlen = em->len - offset;\n\t}\n\n\tio_geom->len = len;\n\tio_geom->offset = offset;\n\tio_geom->stripe_len = stripe_len;\n\tio_geom->stripe_nr = stripe_nr;\n\tio_geom->stripe_offset = stripe_offset;\n\tio_geom->raid56_stripe_offset = raid56_full_stripe_start;\n\n\treturn 0;\n}\n\nstatic int __btrfs_map_block(struct btrfs_fs_info *fs_info,\n\t\t\t     enum btrfs_map_op op,\n\t\t\t     u64 logical, u64 *length,\n\t\t\t     struct btrfs_bio **bbio_ret,\n\t\t\t     int mirror_num, int need_raid_map)\n{\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tu64 stripe_offset;\n\tu64 stripe_nr;\n\tu64 stripe_len;\n\tu32 stripe_index;\n\tint data_stripes;\n\tint i;\n\tint ret = 0;\n\tint num_stripes;\n\tint max_errors = 0;\n\tint tgtdev_indexes = 0;\n\tstruct btrfs_bio *bbio = NULL;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tint dev_replace_is_ongoing = 0;\n\tint num_alloc_stripes;\n\tint patch_the_first_stripe_for_dev_replace = 0;\n\tu64 physical_to_patch_in_first_stripe = 0;\n\tu64 raid56_full_stripe_start = (u64)-1;\n\tstruct btrfs_io_geometry geom;\n\n\tASSERT(bbio_ret);\n\tASSERT(op != BTRFS_MAP_DISCARD);\n\n\tem = btrfs_get_chunk_map(fs_info, logical, *length);\n\tASSERT(!IS_ERR(em));\n\n\tret = btrfs_get_io_geometry(fs_info, em, op, logical, &geom);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmap = em->map_lookup;\n\n\t*length = geom.len;\n\tstripe_len = geom.stripe_len;\n\tstripe_nr = geom.stripe_nr;\n\tstripe_offset = geom.stripe_offset;\n\traid56_full_stripe_start = geom.raid56_stripe_offset;\n\tdata_stripes = nr_data_stripes(map);\n\n\tdown_read(&dev_replace->rwsem);\n\tdev_replace_is_ongoing = btrfs_dev_replace_is_ongoing(dev_replace);\n\t/*\n\t * Hold the semaphore for read during the whole operation, write is\n\t * requested at commit time but must wait.\n\t */\n\tif (!dev_replace_is_ongoing)\n\t\tup_read(&dev_replace->rwsem);\n\n\tif (dev_replace_is_ongoing && mirror_num == map->num_stripes + 1 &&\n\t    !need_full_stripe(op) && dev_replace->tgtdev != NULL) {\n\t\tret = get_extra_mirror_from_replace(fs_info, logical, *length,\n\t\t\t\t\t\t    dev_replace->srcdev->devid,\n\t\t\t\t\t\t    &mirror_num,\n\t\t\t\t\t    &physical_to_patch_in_first_stripe);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\telse\n\t\t\tpatch_the_first_stripe_for_dev_replace = 1;\n\t} else if (mirror_num > map->num_stripes) {\n\t\tmirror_num = 0;\n\t}\n\n\tnum_stripes = 1;\n\tstripe_index = 0;\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tif (!need_full_stripe(op))\n\t\t\tmirror_num = 1;\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID1_MASK) {\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->num_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index = mirror_num - 1;\n\t\telse {\n\t\t\tstripe_index = find_live_mirror(fs_info, map, 0,\n\t\t\t\t\t    dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\n\t\tif (need_full_stripe(op)) {\n\t\t\tnum_stripes = map->num_stripes;\n\t\t} else if (mirror_num) {\n\t\t\tstripe_index = mirror_num - 1;\n\t\t} else {\n\t\t\tmirror_num = 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\n\t\tu32 factor = map->num_stripes / map->sub_stripes;\n\n\t\tstripe_nr = div_u64_rem(stripe_nr, factor, &stripe_index);\n\t\tstripe_index *= map->sub_stripes;\n\n\t\tif (need_full_stripe(op))\n\t\t\tnum_stripes = map->sub_stripes;\n\t\telse if (mirror_num)\n\t\t\tstripe_index += mirror_num - 1;\n\t\telse {\n\t\t\tint old_stripe_index = stripe_index;\n\t\t\tstripe_index = find_live_mirror(fs_info, map,\n\t\t\t\t\t      stripe_index,\n\t\t\t\t\t      dev_replace_is_ongoing);\n\t\t\tmirror_num = stripe_index - old_stripe_index + 1;\n\t\t}\n\n\t} else if (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK) {\n\t\tif (need_raid_map && (need_full_stripe(op) || mirror_num > 1)) {\n\t\t\t/* push stripe_nr back to the start of the full stripe */\n\t\t\tstripe_nr = div64_u64(raid56_full_stripe_start,\n\t\t\t\t\tstripe_len * data_stripes);\n\n\t\t\t/* RAID[56] write or recovery. Return all stripes */\n\t\t\tnum_stripes = map->num_stripes;\n\t\t\tmax_errors = nr_parity_stripes(map);\n\n\t\t\t*length = map->stripe_len;\n\t\t\tstripe_index = 0;\n\t\t\tstripe_offset = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Mirror #0 or #1 means the original data block.\n\t\t\t * Mirror #2 is RAID5 parity block.\n\t\t\t * Mirror #3 is RAID6 Q block.\n\t\t\t */\n\t\t\tstripe_nr = div_u64_rem(stripe_nr,\n\t\t\t\t\tdata_stripes, &stripe_index);\n\t\t\tif (mirror_num > 1)\n\t\t\t\tstripe_index = data_stripes + mirror_num - 2;\n\n\t\t\t/* We distribute the parity blocks across stripes */\n\t\t\tdiv_u64_rem(stripe_nr + stripe_index, map->num_stripes,\n\t\t\t\t\t&stripe_index);\n\t\t\tif (!need_full_stripe(op) && mirror_num <= 1)\n\t\t\t\tmirror_num = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * after this, stripe_nr is the number of stripes on this\n\t\t * device we have to walk to find the data, and stripe_index is\n\t\t * the number of our device in the stripe array\n\t\t */\n\t\tstripe_nr = div_u64_rem(stripe_nr, map->num_stripes,\n\t\t\t\t&stripe_index);\n\t\tmirror_num = stripe_index + 1;\n\t}\n\tif (stripe_index >= map->num_stripes) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"stripe index math went horribly wrong, got stripe_index=%u, num_stripes=%u\",\n\t\t\t   stripe_index, map->num_stripes);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tnum_alloc_stripes = num_stripes;\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL) {\n\t\tif (op == BTRFS_MAP_WRITE)\n\t\t\tnum_alloc_stripes <<= 1;\n\t\tif (op == BTRFS_MAP_GET_READ_MIRRORS)\n\t\t\tnum_alloc_stripes++;\n\t\ttgtdev_indexes = num_stripes;\n\t}\n\n\tbbio = alloc_btrfs_bio(num_alloc_stripes, tgtdev_indexes);\n\tif (!bbio) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tbbio->stripes[i].physical = map->stripes[stripe_index].physical +\n\t\t\tstripe_offset + stripe_nr * map->stripe_len;\n\t\tbbio->stripes[i].dev = map->stripes[stripe_index].dev;\n\t\tstripe_index++;\n\t}\n\n\t/* build raid_map */\n\tif (map->type & BTRFS_BLOCK_GROUP_RAID56_MASK && need_raid_map &&\n\t    (need_full_stripe(op) || mirror_num > 1)) {\n\t\tu64 tmp;\n\t\tunsigned rot;\n\n\t\t/* Work out the disk rotation on this stripe-set */\n\t\tdiv_u64_rem(stripe_nr, num_stripes, &rot);\n\n\t\t/* Fill in the logical address of each stripe */\n\t\ttmp = stripe_nr * data_stripes;\n\t\tfor (i = 0; i < data_stripes; i++)\n\t\t\tbbio->raid_map[(i+rot) % num_stripes] =\n\t\t\t\tem->start + (tmp + i) * map->stripe_len;\n\n\t\tbbio->raid_map[(i+rot) % map->num_stripes] = RAID5_P_STRIPE;\n\t\tif (map->type & BTRFS_BLOCK_GROUP_RAID6)\n\t\t\tbbio->raid_map[(i+rot+1) % num_stripes] =\n\t\t\t\tRAID6_Q_STRIPE;\n\n\t\tsort_parity_stripes(bbio, num_stripes);\n\t}\n\n\tif (need_full_stripe(op))\n\t\tmax_errors = btrfs_chunk_max_errors(map);\n\n\tif (dev_replace_is_ongoing && dev_replace->tgtdev != NULL &&\n\t    need_full_stripe(op)) {\n\t\thandle_ops_on_dev_replace(op, &bbio, dev_replace, logical,\n\t\t\t\t\t  &num_stripes, &max_errors);\n\t}\n\n\t*bbio_ret = bbio;\n\tbbio->map_type = map->type;\n\tbbio->num_stripes = num_stripes;\n\tbbio->max_errors = max_errors;\n\tbbio->mirror_num = mirror_num;\n\n\t/*\n\t * this is the case that REQ_READ && dev_replace_is_ongoing &&\n\t * mirror_num == num_stripes + 1 && dev_replace target drive is\n\t * available as a mirror\n\t */\n\tif (patch_the_first_stripe_for_dev_replace && num_stripes > 0) {\n\t\tWARN_ON(num_stripes > 1);\n\t\tbbio->stripes[0].dev = dev_replace->tgtdev;\n\t\tbbio->stripes[0].physical = physical_to_patch_in_first_stripe;\n\t\tbbio->mirror_num = map->num_stripes + 1;\n\t}\nout:\n\tif (dev_replace_is_ongoing) {\n\t\tlockdep_assert_held(&dev_replace->rwsem);\n\t\t/* Unlock and let waiting writers proceed */\n\t\tup_read(&dev_replace->rwsem);\n\t}\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nint btrfs_map_block(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t      u64 logical, u64 *length,\n\t\t      struct btrfs_bio **bbio_ret, int mirror_num)\n{\n\tif (op == BTRFS_MAP_DISCARD)\n\t\treturn __btrfs_map_block_for_discard(fs_info, logical,\n\t\t\t\t\t\t     length, bbio_ret);\n\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret,\n\t\t\t\t mirror_num, 0);\n}\n\n/* For Scrub/replace */\nint btrfs_map_sblock(struct btrfs_fs_info *fs_info, enum btrfs_map_op op,\n\t\t     u64 logical, u64 *length,\n\t\t     struct btrfs_bio **bbio_ret)\n{\n\treturn __btrfs_map_block(fs_info, op, logical, length, bbio_ret, 0, 1);\n}\n\nstatic inline void btrfs_end_bbio(struct btrfs_bio *bbio, struct bio *bio)\n{\n\tbio->bi_private = bbio->private;\n\tbio->bi_end_io = bbio->end_io;\n\tbio_endio(bio);\n\n\tbtrfs_put_bbio(bbio);\n}\n\nstatic void btrfs_end_bio(struct bio *bio)\n{\n\tstruct btrfs_bio *bbio = bio->bi_private;\n\tint is_orig_bio = 0;\n\n\tif (bio->bi_status) {\n\t\tatomic_inc(&bbio->error);\n\t\tif (bio->bi_status == BLK_STS_IOERR ||\n\t\t    bio->bi_status == BLK_STS_TARGET) {\n\t\t\tstruct btrfs_device *dev = btrfs_io_bio(bio)->device;\n\n\t\t\tASSERT(dev->bdev);\n\t\t\tif (btrfs_op(bio) == BTRFS_MAP_WRITE)\n\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_WRITE_ERRS);\n\t\t\telse if (!(bio->bi_opf & REQ_RAHEAD))\n\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_READ_ERRS);\n\t\t\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\t\t\tbtrfs_dev_stat_inc_and_print(dev,\n\t\t\t\t\t\tBTRFS_DEV_STAT_FLUSH_ERRS);\n\t\t}\n\t}\n\n\tif (bio == bbio->orig_bio)\n\t\tis_orig_bio = 1;\n\n\tbtrfs_bio_counter_dec(bbio->fs_info);\n\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\tif (!is_orig_bio) {\n\t\t\tbio_put(bio);\n\t\t\tbio = bbio->orig_bio;\n\t\t}\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\t/* only send an error to the higher layers if it is\n\t\t * beyond the tolerance of the btrfs bio\n\t\t */\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors) {\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t} else {\n\t\t\t/*\n\t\t\t * this bio is actually up to date, we didn't\n\t\t\t * go over the max number of errors\n\t\t\t */\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\t}\n\n\t\tbtrfs_end_bbio(bbio, bio);\n\t} else if (!is_orig_bio) {\n\t\tbio_put(bio);\n\t}\n}\n\nstatic void submit_stripe_bio(struct btrfs_bio *bbio, struct bio *bio,\n\t\t\t      u64 physical, struct btrfs_device *dev)\n{\n\tstruct btrfs_fs_info *fs_info = bbio->fs_info;\n\n\tbio->bi_private = bbio;\n\tbtrfs_io_bio(bio)->device = dev;\n\tbio->bi_end_io = btrfs_end_bio;\n\tbio->bi_iter.bi_sector = physical >> 9;\n\t/*\n\t * For zone append writing, bi_sector must point the beginning of the\n\t * zone\n\t */\n\tif (bio_op(bio) == REQ_OP_ZONE_APPEND) {\n\t\tif (btrfs_dev_is_sequential(dev, physical)) {\n\t\t\tu64 zone_start = round_down(physical, fs_info->zone_size);\n\n\t\t\tbio->bi_iter.bi_sector = zone_start >> SECTOR_SHIFT;\n\t\t} else {\n\t\t\tbio->bi_opf &= ~REQ_OP_ZONE_APPEND;\n\t\t\tbio->bi_opf |= REQ_OP_WRITE;\n\t\t}\n\t}\n\tbtrfs_debug_in_rcu(fs_info,\n\t\"btrfs_map_bio: rw %d 0x%x, sector=%llu, dev=%lu (%s id %llu), size=%u\",\n\t\tbio_op(bio), bio->bi_opf, bio->bi_iter.bi_sector,\n\t\t(unsigned long)dev->bdev->bd_dev, rcu_str_deref(dev->name),\n\t\tdev->devid, bio->bi_iter.bi_size);\n\tbio_set_dev(bio, dev->bdev);\n\n\tbtrfs_bio_counter_inc_noblocked(fs_info);\n\n\tbtrfsic_submit_bio(bio);\n}\n\nstatic void bbio_error(struct btrfs_bio *bbio, struct bio *bio, u64 logical)\n{\n\tatomic_inc(&bbio->error);\n\tif (atomic_dec_and_test(&bbio->stripes_pending)) {\n\t\t/* Should be the original bio. */\n\t\tWARN_ON(bio != bbio->orig_bio);\n\n\t\tbtrfs_io_bio(bio)->mirror_num = bbio->mirror_num;\n\t\tbio->bi_iter.bi_sector = logical >> 9;\n\t\tif (atomic_read(&bbio->error) > bbio->max_errors)\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\tbtrfs_end_bbio(bbio, bio);\n\t}\n}\n\nblk_status_t btrfs_map_bio(struct btrfs_fs_info *fs_info, struct bio *bio,\n\t\t\t   int mirror_num)\n{\n\tstruct btrfs_device *dev;\n\tstruct bio *first_bio = bio;\n\tu64 logical = bio->bi_iter.bi_sector << 9;\n\tu64 length = 0;\n\tu64 map_length;\n\tint ret;\n\tint dev_nr;\n\tint total_devs;\n\tstruct btrfs_bio *bbio = NULL;\n\n\tlength = bio->bi_iter.bi_size;\n\tmap_length = length;\n\n\tbtrfs_bio_counter_inc_blocked(fs_info);\n\tret = __btrfs_map_block(fs_info, btrfs_op(bio), logical,\n\t\t\t\t&map_length, &bbio, mirror_num, 1);\n\tif (ret) {\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\ttotal_devs = bbio->num_stripes;\n\tbbio->orig_bio = first_bio;\n\tbbio->private = first_bio->bi_private;\n\tbbio->end_io = first_bio->bi_end_io;\n\tbbio->fs_info = fs_info;\n\tatomic_set(&bbio->stripes_pending, bbio->num_stripes);\n\n\tif ((bbio->map_type & BTRFS_BLOCK_GROUP_RAID56_MASK) &&\n\t    ((btrfs_op(bio) == BTRFS_MAP_WRITE) || (mirror_num > 1))) {\n\t\t/* In this case, map_length has been set to the length of\n\t\t   a single stripe; not the whole write */\n\t\tif (btrfs_op(bio) == BTRFS_MAP_WRITE) {\n\t\t\tret = raid56_parity_write(fs_info, bio, bbio,\n\t\t\t\t\t\t  map_length);\n\t\t} else {\n\t\t\tret = raid56_parity_recover(fs_info, bio, bbio,\n\t\t\t\t\t\t    map_length, mirror_num, 1);\n\t\t}\n\n\t\tbtrfs_bio_counter_dec(fs_info);\n\t\treturn errno_to_blk_status(ret);\n\t}\n\n\tif (map_length < length) {\n\t\tbtrfs_crit(fs_info,\n\t\t\t   \"mapping failed logical %llu bio len %llu len %llu\",\n\t\t\t   logical, length, map_length);\n\t\tBUG();\n\t}\n\n\tfor (dev_nr = 0; dev_nr < total_devs; dev_nr++) {\n\t\tdev = bbio->stripes[dev_nr].dev;\n\t\tif (!dev || !dev->bdev || test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t   &dev->dev_state) ||\n\t\t    (btrfs_op(first_bio) == BTRFS_MAP_WRITE &&\n\t\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state))) {\n\t\t\tbbio_error(bbio, first_bio, logical);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (dev_nr < total_devs - 1)\n\t\t\tbio = btrfs_bio_clone(first_bio);\n\t\telse\n\t\t\tbio = first_bio;\n\n\t\tsubmit_stripe_bio(bbio, bio, bbio->stripes[dev_nr].physical, dev);\n\t}\n\tbtrfs_bio_counter_dec(fs_info);\n\treturn BLK_STS_OK;\n}\n\n/*\n * Find a device specified by @devid or @uuid in the list of @fs_devices, or\n * return NULL.\n *\n * If devid and uuid are both specified, the match must be exact, otherwise\n * only devid is used.\n */\nstruct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t       u64 devid, u8 *uuid, u8 *fsid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *seed_devs;\n\n\tif (!fsid || !memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {\n\t\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\t\tif (device->devid == devid &&\n\t\t\t    (!uuid || memcmp(device->uuid, uuid,\n\t\t\t\t\t     BTRFS_UUID_SIZE) == 0))\n\t\t\t\treturn device;\n\t\t}\n\t}\n\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tif (!fsid ||\n\t\t    !memcmp(seed_devs->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {\n\t\t\tlist_for_each_entry(device, &seed_devs->devices,\n\t\t\t\t\t    dev_list) {\n\t\t\t\tif (device->devid == devid &&\n\t\t\t\t    (!uuid || memcmp(device->uuid, uuid,\n\t\t\t\t\t\t     BTRFS_UUID_SIZE) == 0))\n\t\t\t\t\treturn device;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct btrfs_device *add_missing_dev(struct btrfs_fs_devices *fs_devices,\n\t\t\t\t\t    u64 devid, u8 *dev_uuid)\n{\n\tstruct btrfs_device *device;\n\tunsigned int nofs_flag;\n\n\t/*\n\t * We call this under the chunk_mutex, so we want to use NOFS for this\n\t * allocation, however we don't want to change btrfs_alloc_device() to\n\t * always do NOFS because we use it in a lot of other GFP_KERNEL safe\n\t * places.\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tdevice = btrfs_alloc_device(NULL, &devid, dev_uuid);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(device))\n\t\treturn device;\n\n\tlist_add(&device->dev_list, &fs_devices->devices);\n\tdevice->fs_devices = fs_devices;\n\tfs_devices->num_devices++;\n\n\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\tfs_devices->missing_devices++;\n\n\treturn device;\n}\n\n/**\n * btrfs_alloc_device - allocate struct btrfs_device\n * @fs_info:\tused only for generating a new devid, can be NULL if\n *\t\tdevid is provided (i.e. @devid != NULL).\n * @devid:\ta pointer to devid for this device.  If NULL a new devid\n *\t\tis generated.\n * @uuid:\ta pointer to UUID for this device.  If NULL a new UUID\n *\t\tis generated.\n *\n * Return: a pointer to a new &struct btrfs_device on success; ERR_PTR()\n * on error.  Returned struct is not linked onto any lists and must be\n * destroyed with btrfs_free_device.\n */\nstruct btrfs_device *btrfs_alloc_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tconst u64 *devid,\n\t\t\t\t\tconst u8 *uuid)\n{\n\tstruct btrfs_device *dev;\n\tu64 tmp;\n\n\tif (WARN_ON(!devid && !fs_info))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * Preallocate a bio that's always going to be used for flushing device\n\t * barriers and matches the device lifespan\n\t */\n\tdev->flush_bio = bio_kmalloc(GFP_KERNEL, 0);\n\tif (!dev->flush_bio) {\n\t\tkfree(dev);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tINIT_LIST_HEAD(&dev->dev_list);\n\tINIT_LIST_HEAD(&dev->dev_alloc_list);\n\tINIT_LIST_HEAD(&dev->post_commit_list);\n\n\tatomic_set(&dev->reada_in_flight, 0);\n\tatomic_set(&dev->dev_stats_ccnt, 0);\n\tbtrfs_device_data_ordered_init(dev);\n\tINIT_RADIX_TREE(&dev->reada_zones, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\tINIT_RADIX_TREE(&dev->reada_extents, GFP_NOFS & ~__GFP_DIRECT_RECLAIM);\n\textent_io_tree_init(fs_info, &dev->alloc_state,\n\t\t\t    IO_TREE_DEVICE_ALLOC_STATE, NULL);\n\n\tif (devid)\n\t\ttmp = *devid;\n\telse {\n\t\tint ret;\n\n\t\tret = find_next_devid(fs_info, &tmp);\n\t\tif (ret) {\n\t\t\tbtrfs_free_device(dev);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tdev->devid = tmp;\n\n\tif (uuid)\n\t\tmemcpy(dev->uuid, uuid, BTRFS_UUID_SIZE);\n\telse\n\t\tgenerate_random_uuid(dev->uuid);\n\n\treturn dev;\n}\n\nstatic void btrfs_report_missing_device(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tu64 devid, u8 *uuid, bool error)\n{\n\tif (error)\n\t\tbtrfs_err_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n\telse\n\t\tbtrfs_warn_rl(fs_info, \"devid %llu uuid %pU is missing\",\n\t\t\t      devid, uuid);\n}\n\nstatic u64 calc_stripe_length(u64 type, u64 chunk_len, int num_stripes)\n{\n\tconst int data_stripes = calc_data_stripes(type, num_stripes);\n\n\treturn div_u64(chunk_len, data_stripes);\n}\n\n#if BITS_PER_LONG == 32\n/*\n * Due to page cache limit, metadata beyond BTRFS_32BIT_MAX_FILE_SIZE\n * can't be accessed on 32bit systems.\n *\n * This function do mount time check to reject the fs if it already has\n * metadata chunk beyond that limit.\n */\nstatic int check_32bit_meta_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t  u64 logical, u64 length, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_METADATA))\n\t\treturn 0;\n\n\tif (logical + length < MAX_LFS_FILESIZE)\n\t\treturn 0;\n\n\tbtrfs_err_32bit_limit(fs_info);\n\treturn -EOVERFLOW;\n}\n\n/*\n * This is to give early warning for any metadata chunk reaching\n * BTRFS_32BIT_EARLY_WARN_THRESHOLD.\n * Although we can still access the metadata, it's not going to be possible\n * once the limit is reached.\n */\nstatic void warn_32bit_meta_chunk(struct btrfs_fs_info *fs_info,\n\t\t\t\t  u64 logical, u64 length, u64 type)\n{\n\tif (!(type & BTRFS_BLOCK_GROUP_METADATA))\n\t\treturn;\n\n\tif (logical + length < BTRFS_32BIT_EARLY_WARN_THRESHOLD)\n\t\treturn;\n\n\tbtrfs_warn_32bit_limit(fs_info);\n}\n#endif\n\nstatic int read_one_chunk(struct btrfs_key *key, struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu64 type;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\ttype = btrfs_chunk_type(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n#if BITS_PER_LONG == 32\n\tret = check_32bit_meta_chunk(fs_info, logical, length, type);\n\tif (ret < 0)\n\t\treturn ret;\n\twarn_32bit_meta_chunk(fs_info, logical, length, type);\n#endif\n\n\t/*\n\t * Only need to verify chunk item if we're reading from sys chunk array,\n\t * as chunk item in tree block is already verified by tree-checker.\n\t */\n\tif (leaf->start == BTRFS_SUPER_INFO_OFFSET) {\n\t\tret = btrfs_check_chunk_valid(leaf, chunk, logical);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, logical, 1);\n\tread_unlock(&map_tree->lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = type;\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tem->orig_block_len = calc_stripe_length(type, em->len,\n\t\t\t\t\t\tmap->num_stripes);\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->lock);\n\tret = add_extent_mapping(map_tree, em, 0);\n\twrite_unlock(&map_tree->lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}\n\nstatic void fill_device_from_item(struct extent_buffer *leaf,\n\t\t\t\t struct btrfs_dev_item *dev_item,\n\t\t\t\t struct btrfs_device *device)\n{\n\tunsigned long ptr;\n\n\tdevice->devid = btrfs_device_id(leaf, dev_item);\n\tdevice->disk_total_bytes = btrfs_device_total_bytes(leaf, dev_item);\n\tdevice->total_bytes = device->disk_total_bytes;\n\tdevice->commit_total_bytes = device->disk_total_bytes;\n\tdevice->bytes_used = btrfs_device_bytes_used(leaf, dev_item);\n\tdevice->commit_bytes_used = device->bytes_used;\n\tdevice->type = btrfs_device_type(leaf, dev_item);\n\tdevice->io_align = btrfs_device_io_align(leaf, dev_item);\n\tdevice->io_width = btrfs_device_io_width(leaf, dev_item);\n\tdevice->sector_size = btrfs_device_sector_size(leaf, dev_item);\n\tWARN_ON(device->devid == BTRFS_DEV_REPLACE_DEVID);\n\tclear_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state);\n\n\tptr = btrfs_device_uuid(dev_item);\n\tread_extent_buffer(leaf, device->uuid, ptr, BTRFS_UUID_SIZE);\n}\n\nstatic struct btrfs_fs_devices *open_seed_devices(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t\t  u8 *fsid)\n{\n\tstruct btrfs_fs_devices *fs_devices;\n\tint ret;\n\n\tlockdep_assert_held(&uuid_mutex);\n\tASSERT(fsid);\n\n\t/* This will match only for multi-device seed fs */\n\tlist_for_each_entry(fs_devices, &fs_info->fs_devices->seed_list, seed_list)\n\t\tif (!memcmp(fs_devices->fsid, fsid, BTRFS_FSID_SIZE))\n\t\t\treturn fs_devices;\n\n\n\tfs_devices = find_fsid(fsid, NULL);\n\tif (!fs_devices) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED))\n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\tfs_devices = alloc_fs_devices(fsid, NULL);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn fs_devices;\n\n\t\tfs_devices->seeding = true;\n\t\tfs_devices->opened = 1;\n\t\treturn fs_devices;\n\t}\n\n\t/*\n\t * Upon first call for a seed fs fsid, just create a private copy of the\n\t * respective fs_devices and anchor it at fs_info->fs_devices->seed_list\n\t */\n\tfs_devices = clone_fs_devices(fs_devices);\n\tif (IS_ERR(fs_devices))\n\t\treturn fs_devices;\n\n\tret = open_fs_devices(fs_devices, FMODE_READ, fs_info->bdev_holder);\n\tif (ret) {\n\t\tfree_fs_devices(fs_devices);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tif (!fs_devices->seeding) {\n\t\tclose_fs_devices(fs_devices);\n\t\tfree_fs_devices(fs_devices);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tlist_add(&fs_devices->seed_list, &fs_info->fs_devices->seed_list);\n\n\treturn fs_devices;\n}\n\nstatic int read_one_dev(struct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tif (device->bdev) {\n\t\tu64 max_total_bytes = i_size_read(device->bdev->bd_inode);\n\n\t\tif (device->total_bytes > max_total_bytes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"device total_bytes should be at most %llu but found %llu\",\n\t\t\t\t  max_total_bytes, device->total_bytes);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}\n\nint btrfs_read_sys_array(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->tree_root;\n\tstruct btrfs_super_block *super_copy = fs_info->super_copy;\n\tstruct extent_buffer *sb;\n\tstruct btrfs_disk_key *disk_key;\n\tstruct btrfs_chunk *chunk;\n\tu8 *array_ptr;\n\tunsigned long sb_array_offset;\n\tint ret = 0;\n\tu32 num_stripes;\n\tu32 array_size;\n\tu32 len = 0;\n\tu32 cur_offset;\n\tu64 type;\n\tstruct btrfs_key key;\n\n\tASSERT(BTRFS_SUPER_INFO_SIZE <= fs_info->nodesize);\n\t/*\n\t * This will create extent buffer of nodesize, superblock size is\n\t * fixed to BTRFS_SUPER_INFO_SIZE. If nodesize > sb size, this will\n\t * overallocate but we can keep it as-is, only the first page is used.\n\t */\n\tsb = btrfs_find_create_tree_block(fs_info, BTRFS_SUPER_INFO_OFFSET,\n\t\t\t\t\t  root->root_key.objectid, 0);\n\tif (IS_ERR(sb))\n\t\treturn PTR_ERR(sb);\n\tset_extent_buffer_uptodate(sb);\n\t/*\n\t * The sb extent buffer is artificial and just used to read the system array.\n\t * set_extent_buffer_uptodate() call does not properly mark all it's\n\t * pages up-to-date when the page is larger: extent does not cover the\n\t * whole page and consequently check_page_uptodate does not find all\n\t * the page's extents up-to-date (the hole beyond sb),\n\t * write_extent_buffer then triggers a WARN_ON.\n\t *\n\t * Regular short extents go through mark_extent_buffer_dirty/writeback cycle,\n\t * but sb spans only this function. Add an explicit SetPageUptodate call\n\t * to silence the warning eg. on PowerPC 64.\n\t */\n\tif (PAGE_SIZE > BTRFS_SUPER_INFO_SIZE)\n\t\tSetPageUptodate(sb->pages[0]);\n\n\twrite_extent_buffer(sb, super_copy, 0, BTRFS_SUPER_INFO_SIZE);\n\tarray_size = btrfs_super_sys_array_size(super_copy);\n\n\tarray_ptr = super_copy->sys_chunk_array;\n\tsb_array_offset = offsetof(struct btrfs_super_block, sys_chunk_array);\n\tcur_offset = 0;\n\n\twhile (cur_offset < array_size) {\n\t\tdisk_key = (struct btrfs_disk_key *)array_ptr;\n\t\tlen = sizeof(*disk_key);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tbtrfs_disk_key_to_cpu(&key, disk_key);\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\n\t\tif (key.type != BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t    \"unexpected item type %u in sys_array at offset %u\",\n\t\t\t\t  (u32)key.type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tchunk = (struct btrfs_chunk *)sb_array_offset;\n\t\t/*\n\t\t * At least one btrfs_chunk with one stripe must be present,\n\t\t * exact stripe count check comes afterwards\n\t\t */\n\t\tlen = btrfs_chunk_item_size(1);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tnum_stripes = btrfs_chunk_num_stripes(sb, chunk);\n\t\tif (!num_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid number of stripes %u in sys_array at offset %u\",\n\t\t\t\t  num_stripes, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\ttype = btrfs_chunk_type(sb, chunk);\n\t\tif ((type & BTRFS_BLOCK_GROUP_SYSTEM) == 0) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"invalid chunk type %llu in sys_array at offset %u\",\n\t\t\t\t  type, cur_offset);\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\n\t\tlen = btrfs_chunk_item_size(num_stripes);\n\t\tif (cur_offset + len > array_size)\n\t\t\tgoto out_short_read;\n\n\t\tret = read_one_chunk(&key, sb, chunk);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tarray_ptr += len;\n\t\tsb_array_offset += len;\n\t\tcur_offset += len;\n\t}\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn ret;\n\nout_short_read:\n\tbtrfs_err(fs_info, \"sys_array too short to read %u bytes at offset %u\",\n\t\t\tlen, cur_offset);\n\tclear_extent_buffer_uptodate(sb);\n\tfree_extent_buffer_stale(sb);\n\treturn -EIO;\n}\n\n/*\n * Check if all chunks in the fs are OK for read-write degraded mount\n *\n * If the @failing_dev is specified, it's accounted as missing.\n *\n * Return true if all chunks meet the minimal RW mount requirements.\n * Return false if any chunk doesn't meet the minimal RW mount requirements.\n */\nbool btrfs_check_rw_degradable(struct btrfs_fs_info *fs_info,\n\t\t\t\t\tstruct btrfs_device *failing_dev)\n{\n\tstruct extent_map_tree *map_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tu64 next_start = 0;\n\tbool ret = true;\n\n\tread_lock(&map_tree->lock);\n\tem = lookup_extent_mapping(map_tree, 0, (u64)-1);\n\tread_unlock(&map_tree->lock);\n\t/* No chunk at all? Return false anyway */\n\tif (!em) {\n\t\tret = false;\n\t\tgoto out;\n\t}\n\twhile (em) {\n\t\tstruct map_lookup *map;\n\t\tint missing = 0;\n\t\tint max_tolerated;\n\t\tint i;\n\n\t\tmap = em->map_lookup;\n\t\tmax_tolerated =\n\t\t\tbtrfs_get_num_tolerated_disk_barrier_failures(\n\t\t\t\t\tmap->type);\n\t\tfor (i = 0; i < map->num_stripes; i++) {\n\t\t\tstruct btrfs_device *dev = map->stripes[i].dev;\n\n\t\t\tif (!dev || !dev->bdev ||\n\t\t\t    test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) ||\n\t\t\t    dev->last_flush_error)\n\t\t\t\tmissing++;\n\t\t\telse if (failing_dev && failing_dev == dev)\n\t\t\t\tmissing++;\n\t\t}\n\t\tif (missing > max_tolerated) {\n\t\t\tif (!failing_dev)\n\t\t\t\tbtrfs_warn(fs_info,\n\t\"chunk %llu missing %d devices, max tolerance is %d for writable mount\",\n\t\t\t\t   em->start, missing, max_tolerated);\n\t\t\tfree_extent_map(em);\n\t\t\tret = false;\n\t\t\tgoto out;\n\t\t}\n\t\tnext_start = extent_map_end(em);\n\t\tfree_extent_map(em);\n\n\t\tread_lock(&map_tree->lock);\n\t\tem = lookup_extent_mapping(map_tree, next_start,\n\t\t\t\t\t   (u64)(-1) - next_start);\n\t\tread_unlock(&map_tree->lock);\n\t}\nout:\n\treturn ret;\n}\n\nstatic void readahead_tree_node_children(struct extent_buffer *node)\n{\n\tint i;\n\tconst int nr_items = btrfs_header_nritems(node);\n\n\tfor (i = 0; i < nr_items; i++)\n\t\tbtrfs_readahead_node_child(node, i);\n}\n\nint btrfs_read_chunk_tree(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key key;\n\tstruct btrfs_key found_key;\n\tint ret;\n\tint slot;\n\tu64 total_dev = 0;\n\tu64 last_ra_node = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * uuid_mutex is needed only if we are mounting a sprout FS\n\t * otherwise we don't need it.\n\t */\n\tmutex_lock(&uuid_mutex);\n\n\t/*\n\t * It is possible for mount and umount to race in such a way that\n\t * we execute this code path, but open_fs_devices failed to clear\n\t * total_rw_bytes. We certainly want it cleared before reading the\n\t * device items, so clear it here.\n\t */\n\tfs_info->fs_devices->total_rw_bytes = 0;\n\n\t/*\n\t * Read all device items, and then all the chunk items. All\n\t * device items are found before any chunk item (their object id\n\t * is smaller than the lowest possible object id for a chunk\n\t * item - BTRFS_FIRST_CHUNK_TREE_OBJECTID).\n\t */\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = 0;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto error;\n\twhile (1) {\n\t\tstruct extent_buffer *node;\n\n\t\tleaf = path->nodes[0];\n\t\tslot = path->slots[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tbreak;\n\t\t}\n\t\t/*\n\t\t * The nodes on level 1 are not locked but we don't need to do\n\t\t * that during mount time as nothing else can access the tree\n\t\t */\n\t\tnode = path->nodes[1];\n\t\tif (node) {\n\t\t\tif (last_ra_node != node->start) {\n\t\t\t\treadahead_tree_node_children(node);\n\t\t\t\tlast_ra_node = node->start;\n\t\t\t}\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\t\tif (found_key.type == BTRFS_DEV_ITEM_KEY) {\n\t\t\tstruct btrfs_dev_item *dev_item;\n\t\t\tdev_item = btrfs_item_ptr(leaf, slot,\n\t\t\t\t\t\t  struct btrfs_dev_item);\n\t\t\tret = read_one_dev(leaf, dev_item);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t\ttotal_dev++;\n\t\t} else if (found_key.type == BTRFS_CHUNK_ITEM_KEY) {\n\t\t\tstruct btrfs_chunk *chunk;\n\n\t\t\t/*\n\t\t\t * We are only called at mount time, so no need to take\n\t\t\t * fs_info->chunk_mutex. Plus, to avoid lockdep warnings,\n\t\t\t * we always lock first fs_info->chunk_mutex before\n\t\t\t * acquiring any locks on the chunk tree. This is a\n\t\t\t * requirement for chunk allocation, see the comment on\n\t\t\t * top of btrfs_chunk_alloc() for details.\n\t\t\t */\n\t\t\tASSERT(!test_bit(BTRFS_FS_OPEN, &fs_info->flags));\n\t\t\tchunk = btrfs_item_ptr(leaf, slot, struct btrfs_chunk);\n\t\t\tret = read_one_chunk(&found_key, leaf, chunk);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\n\n\t/*\n\t * After loading chunk tree, we've got all device information,\n\t * do another round of validation checks.\n\t */\n\tif (total_dev != fs_info->fs_devices->total_devices) {\n\t\tbtrfs_err(fs_info,\n\t   \"super_num_devices %llu mismatch with num_devices %llu found here\",\n\t\t\t  btrfs_super_num_devices(fs_info->super_copy),\n\t\t\t  total_dev);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tif (btrfs_super_total_bytes(fs_info->super_copy) <\n\t    fs_info->fs_devices->total_rw_bytes) {\n\t\tbtrfs_err(fs_info,\n\t\"super_total_bytes %llu mismatch with fs_devices total_rw_bytes %llu\",\n\t\t\t  btrfs_super_total_bytes(fs_info->super_copy),\n\t\t\t  fs_info->fs_devices->total_rw_bytes);\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\tret = 0;\nerror:\n\tmutex_unlock(&uuid_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nvoid btrfs_init_devices_late(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices, *seed_devs;\n\tstruct btrfs_device *device;\n\n\tfs_devices->fs_info = fs_info;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list)\n\t\tdevice->fs_info = fs_info;\n\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list)\n\t\t\tdevice->fs_info = fs_info;\n\n\t\tseed_devs->fs_info = fs_info;\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n}\n\nstatic u64 btrfs_dev_stats_value(const struct extent_buffer *eb,\n\t\t\t\t const struct btrfs_dev_stats_item *ptr,\n\t\t\t\t int index)\n{\n\tu64 val;\n\n\tread_extent_buffer(eb, &val,\n\t\t\t   offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t    ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t   sizeof(val));\n\treturn val;\n}\n\nstatic void btrfs_set_dev_stats_value(struct extent_buffer *eb,\n\t\t\t\t      struct btrfs_dev_stats_item *ptr,\n\t\t\t\t      int index, u64 val)\n{\n\twrite_extent_buffer(eb, &val,\n\t\t\t    offsetof(struct btrfs_dev_stats_item, values) +\n\t\t\t     ((unsigned long)ptr) + (index * sizeof(u64)),\n\t\t\t    sizeof(val));\n}\n\nstatic int btrfs_device_init_dev_stats(struct btrfs_device *device,\n\t\t\t\t       struct btrfs_path *path)\n{\n\tstruct btrfs_dev_stats_item *ptr;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_key key;\n\tint item_size;\n\tint i, ret, slot;\n\n\tif (!device->fs_info->dev_root)\n\t\treturn 0;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\tret = btrfs_search_slot(NULL, device->fs_info->dev_root, &key, path, 0, 0);\n\tif (ret) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tbtrfs_dev_stat_set(device, i, 0);\n\t\tdevice->dev_stats_valid = 1;\n\t\tbtrfs_release_path(path);\n\t\treturn ret < 0 ? ret : 0;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_stats_item);\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\tif (item_size >= (1 + i) * sizeof(__le64))\n\t\t\tbtrfs_dev_stat_set(device, i,\n\t\t\t\t\t   btrfs_dev_stats_value(eb, ptr, i));\n\t\telse\n\t\t\tbtrfs_dev_stat_set(device, i, 0);\n\t}\n\n\tdevice->dev_stats_valid = 1;\n\tbtrfs_dev_stat_print_on_load(device);\n\tbtrfs_release_path(path);\n\n\treturn 0;\n}\n\nint btrfs_init_dev_stats(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices, *seed_devs;\n\tstruct btrfs_device *device;\n\tstruct btrfs_path *path = NULL;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tret = btrfs_device_init_dev_stats(device, path);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tlist_for_each_entry(seed_devs, &fs_devices->seed_list, seed_list) {\n\t\tlist_for_each_entry(device, &seed_devs->devices, dev_list) {\n\t\t\tret = btrfs_device_init_dev_stats(device, path);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nstatic int update_dev_stat_item(struct btrfs_trans_handle *trans,\n\t\t\t\tstruct btrfs_device *device)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tstruct extent_buffer *eb;\n\tstruct btrfs_dev_stats_item *ptr;\n\tint ret;\n\tint i;\n\n\tkey.objectid = BTRFS_DEV_STATS_OBJECTID;\n\tkey.type = BTRFS_PERSISTENT_ITEM_KEY;\n\tkey.offset = device->devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\tret = btrfs_search_slot(trans, dev_root, &key, path, -1, 1);\n\tif (ret < 0) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\"error %d while searching for dev_stats item for device %s\",\n\t\t\t      ret, rcu_str_deref(device->name));\n\t\tgoto out;\n\t}\n\n\tif (ret == 0 &&\n\t    btrfs_item_size_nr(path->nodes[0], path->slots[0]) < sizeof(*ptr)) {\n\t\t/* need to delete old one and insert a new one */\n\t\tret = btrfs_del_item(trans, dev_root, path);\n\t\tif (ret != 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"delete too small dev_stats item for device %s failed %d\",\n\t\t\t\t      rcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 1;\n\t}\n\n\tif (ret == 1) {\n\t\t/* need to insert a new item */\n\t\tbtrfs_release_path(path);\n\t\tret = btrfs_insert_empty_item(trans, dev_root, path,\n\t\t\t\t\t      &key, sizeof(*ptr));\n\t\tif (ret < 0) {\n\t\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t\t\t\"insert dev_stats item for device %s failed %d\",\n\t\t\t\trcu_str_deref(device->name), ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\teb = path->nodes[0];\n\tptr = btrfs_item_ptr(eb, path->slots[0], struct btrfs_dev_stats_item);\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tbtrfs_set_dev_stats_value(eb, ptr, i,\n\t\t\t\t\t  btrfs_dev_stat_read(device, i));\n\tbtrfs_mark_buffer_dirty(eb);\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * called from commit_transaction. Writes all changed device stats to disk.\n */\nint btrfs_run_dev_stats(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tint stats_cnt;\n\tint ret = 0;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_for_each_entry(device, &fs_devices->devices, dev_list) {\n\t\tstats_cnt = atomic_read(&device->dev_stats_ccnt);\n\t\tif (!device->dev_stats_valid || stats_cnt == 0)\n\t\t\tcontinue;\n\n\n\t\t/*\n\t\t * There is a LOAD-LOAD control dependency between the value of\n\t\t * dev_stats_ccnt and updating the on-disk values which requires\n\t\t * reading the in-memory counters. Such control dependencies\n\t\t * require explicit read memory barriers.\n\t\t *\n\t\t * This memory barriers pairs with smp_mb__before_atomic in\n\t\t * btrfs_dev_stat_inc/btrfs_dev_stat_set and with the full\n\t\t * barrier implied by atomic_xchg in\n\t\t * btrfs_dev_stats_read_and_reset\n\t\t */\n\t\tsmp_rmb();\n\n\t\tret = update_dev_stat_item(trans, device);\n\t\tif (!ret)\n\t\t\tatomic_sub(stats_cnt, &device->dev_stats_ccnt);\n\t}\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\treturn ret;\n}\n\nvoid btrfs_dev_stat_inc_and_print(struct btrfs_device *dev, int index)\n{\n\tbtrfs_dev_stat_inc(dev, index);\n\tbtrfs_dev_stat_print_on_error(dev);\n}\n\nstatic void btrfs_dev_stat_print_on_error(struct btrfs_device *dev)\n{\n\tif (!dev->dev_stats_valid)\n\t\treturn;\n\tbtrfs_err_rl_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t\t\t   rcu_str_deref(dev->name),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t\t\t   btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nstatic void btrfs_dev_stat_print_on_load(struct btrfs_device *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\tif (btrfs_dev_stat_read(dev, i) != 0)\n\t\t\tbreak;\n\tif (i == BTRFS_DEV_STAT_VALUES_MAX)\n\t\treturn; /* all values == 0, suppress message */\n\n\tbtrfs_info_in_rcu(dev->fs_info,\n\t\t\"bdev %s errs: wr %u, rd %u, flush %u, corrupt %u, gen %u\",\n\t       rcu_str_deref(dev->name),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_WRITE_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_READ_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_FLUSH_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_CORRUPTION_ERRS),\n\t       btrfs_dev_stat_read(dev, BTRFS_DEV_STAT_GENERATION_ERRS));\n}\n\nint btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_set(dev, i, 0);\n\t\t}\n\t\tbtrfs_info(fs_info, \"device stats zeroed by %s (%d)\",\n\t\t\t   current->comm, task_pid_nr(current));\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}\n\n/*\n * Update the size and bytes used for each device where it changed.  This is\n * delayed since we would otherwise get errors while writing out the\n * superblocks.\n *\n * Must be invoked during transaction commit.\n */\nvoid btrfs_commit_device_sizes(struct btrfs_transaction *trans)\n{\n\tstruct btrfs_device *curr, *next;\n\n\tASSERT(trans->state == TRANS_STATE_COMMIT_DOING);\n\n\tif (list_empty(&trans->dev_update_list))\n\t\treturn;\n\n\t/*\n\t * We don't need the device_list_mutex here.  This list is owned by the\n\t * transaction and the transaction must complete before the device is\n\t * released.\n\t */\n\tmutex_lock(&trans->fs_info->chunk_mutex);\n\tlist_for_each_entry_safe(curr, next, &trans->dev_update_list,\n\t\t\t\t post_commit_list) {\n\t\tlist_del_init(&curr->post_commit_list);\n\t\tcurr->commit_total_bytes = curr->disk_total_bytes;\n\t\tcurr->commit_bytes_used = curr->bytes_used;\n\t}\n\tmutex_unlock(&trans->fs_info->chunk_mutex);\n}\n\n/*\n * Multiplicity factor for simple profiles: DUP, RAID1-like and RAID10.\n */\nint btrfs_bg_type_to_factor(u64 flags)\n{\n\tconst int index = btrfs_bg_flags_to_raid_index(flags);\n\n\treturn btrfs_raid_array[index].ncopies;\n}\n\n\n\nstatic int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device boundary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tif (dev->zone_info) {\n\t\tu64 zone_size = dev->zone_info->zone_size;\n\n\t\tif (!IS_ALIGNED(physical_offset, zone_size) ||\n\t\t    !IS_ALIGNED(physical_len, zone_size)) {\n\t\t\tbtrfs_err(fs_info,\n\"zoned: dev extent devid %llu physical offset %llu len %llu is not aligned to device zone\",\n\t\t\t\t  devid, physical_offset, physical_len);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}\n\nstatic int verify_chunk_dev_extent_mapping(struct btrfs_fs_info *fs_info)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree;\n\tstruct extent_map *em;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tread_lock(&em_tree->lock);\n\tfor (node = rb_first_cached(&em_tree->map); node; node = rb_next(node)) {\n\t\tem = rb_entry(node, struct extent_map, rb_node);\n\t\tif (em->map_lookup->num_stripes !=\n\t\t    em->map_lookup->verified_stripes) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\"chunk %llu has missing dev extent, have %d expect %d\",\n\t\t\t\t  em->start, em->map_lookup->verified_stripes,\n\t\t\t\t  em->map_lookup->num_stripes);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tread_unlock(&em_tree->lock);\n\treturn ret;\n}\n\n/*\n * Ensure that all dev extents are mapped to correct chunk, otherwise\n * later chunk allocation/free would cause unexpected behavior.\n *\n * NOTE: This will iterate through the whole device tree, which should be of\n * the same size level as the chunk tree.  This slightly increases mount time.\n */\nint btrfs_verify_dev_extents(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_root *root = fs_info->dev_root;\n\tstruct btrfs_key key;\n\tu64 prev_devid = 0;\n\tu64 prev_dev_ext_end = 0;\n\tint ret = 0;\n\n\t/*\n\t * We don't have a dev_root because we mounted with ignorebadroots and\n\t * failed to load the root, so we want to skip the verification in this\n\t * case for sure.\n\t *\n\t * However if the dev root is fine, but the tree itself is corrupted\n\t * we'd still fail to mount.  This verification is only to make sure\n\t * writes can happen safely, so instead just bypass this check\n\t * completely in the case of IGNOREBADROOTS.\n\t */\n\tif (btrfs_test_opt(fs_info, IGNOREBADROOTS))\n\t\treturn 0;\n\n\tkey.objectid = 1;\n\tkey.type = BTRFS_DEV_EXTENT_KEY;\n\tkey.offset = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tpath->reada = READA_FORWARD;\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (path->slots[0] >= btrfs_header_nritems(path->nodes[0])) {\n\t\tret = btrfs_next_leaf(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/* No dev extents at all? Not good */\n\t\tif (ret > 0) {\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\twhile (1) {\n\t\tstruct extent_buffer *leaf = path->nodes[0];\n\t\tstruct btrfs_dev_extent *dext;\n\t\tint slot = path->slots[0];\n\t\tu64 chunk_offset;\n\t\tu64 physical_offset;\n\t\tu64 physical_len;\n\t\tu64 devid;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\t\tif (key.type != BTRFS_DEV_EXTENT_KEY)\n\t\t\tbreak;\n\t\tdevid = key.objectid;\n\t\tphysical_offset = key.offset;\n\n\t\tdext = btrfs_item_ptr(leaf, slot, struct btrfs_dev_extent);\n\t\tchunk_offset = btrfs_dev_extent_chunk_offset(leaf, dext);\n\t\tphysical_len = btrfs_dev_extent_length(leaf, dext);\n\n\t\t/* Check if this dev extent overlaps with the previous one */\n\t\tif (devid == prev_devid && physical_offset < prev_dev_ext_end) {\n\t\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu overlap with previous dev extent end %llu\",\n\t\t\t\t  devid, physical_offset, prev_dev_ext_end);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = verify_one_dev_extent(fs_info, chunk_offset, devid,\n\t\t\t\t\t    physical_offset, physical_len);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tprev_devid = devid;\n\t\tprev_dev_ext_end = physical_offset + physical_len;\n\n\t\tret = btrfs_next_item(root, path);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (ret > 0) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ensure all chunks have corresponding dev extents */\n\tret = verify_chunk_dev_extent_mapping(fs_info);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\n/*\n * Check whether the given block group or device is pinned by any inode being\n * used as a swapfile.\n */\nbool btrfs_pinned_by_swapfile(struct btrfs_fs_info *fs_info, void *ptr)\n{\n\tstruct btrfs_swapfile_pin *sp;\n\tstruct rb_node *node;\n\n\tspin_lock(&fs_info->swapfile_pins_lock);\n\tnode = fs_info->swapfile_pins.rb_node;\n\twhile (node) {\n\t\tsp = rb_entry(node, struct btrfs_swapfile_pin, node);\n\t\tif (ptr < sp->ptr)\n\t\t\tnode = node->rb_left;\n\t\telse if (ptr > sp->ptr)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\tbreak;\n\t}\n\tspin_unlock(&fs_info->swapfile_pins_lock);\n\treturn node != NULL;\n}\n\nstatic int relocating_repair_kthread(void *data)\n{\n\tstruct btrfs_block_group *cache = (struct btrfs_block_group *)data;\n\tstruct btrfs_fs_info *fs_info = cache->fs_info;\n\tu64 target;\n\tint ret = 0;\n\n\ttarget = cache->start;\n\tbtrfs_put_block_group(cache);\n\n\tif (!btrfs_exclop_start(fs_info, BTRFS_EXCLOP_BALANCE)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"zoned: skip relocating block group %llu to repair: EBUSY\",\n\t\t\t   target);\n\t\treturn -EBUSY;\n\t}\n\n\tmutex_lock(&fs_info->reclaim_bgs_lock);\n\n\t/* Ensure block group still exists */\n\tcache = btrfs_lookup_block_group(fs_info, target);\n\tif (!cache)\n\t\tgoto out;\n\n\tif (!cache->relocating_repair)\n\t\tgoto out;\n\n\tret = btrfs_may_alloc_data_chunk(fs_info, target);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tbtrfs_info(fs_info,\n\t\t   \"zoned: relocating block group %llu to repair IO failure\",\n\t\t   target);\n\tret = btrfs_relocate_chunk(fs_info, target);\n\nout:\n\tif (cache)\n\t\tbtrfs_put_block_group(cache);\n\tmutex_unlock(&fs_info->reclaim_bgs_lock);\n\tbtrfs_exclop_finish(fs_info);\n\n\treturn ret;\n}\n\nint btrfs_repair_one_zone(struct btrfs_fs_info *fs_info, u64 logical)\n{\n\tstruct btrfs_block_group *cache;\n\n\t/* Do not attempt to repair in degraded state */\n\tif (btrfs_test_opt(fs_info, DEGRADED))\n\t\treturn 0;\n\n\tcache = btrfs_lookup_block_group(fs_info, logical);\n\tif (!cache)\n\t\treturn 0;\n\n\tspin_lock(&cache->lock);\n\tif (cache->relocating_repair) {\n\t\tspin_unlock(&cache->lock);\n\t\tbtrfs_put_block_group(cache);\n\t\treturn 0;\n\t}\n\tcache->relocating_repair = 1;\n\tspin_unlock(&cache->lock);\n\n\tkthread_run(relocating_repair_kthread, cache,\n\t\t    \"btrfs-relocating-repair\");\n\n\treturn 0;\n}\n"], "filenames": ["fs/btrfs/volumes.c"], "buggy_code_start_loc": [2077], "buggy_code_end_loc": [2078], "fixing_code_start_loc": [2077], "fixing_code_end_loc": [2078], "type": "CWE-476", "message": "A NULL pointer dereference flaw was found in the btrfs_rm_device function in fs/btrfs/volumes.c in the Linux Kernel, where triggering the bug requires \u2018CAP_SYS_ADMIN\u2019. This flaw allows a local attacker to crash the system or leak kernel internal information. The highest threat from this vulnerability is to system availability.", "other": {"cve": {"id": "CVE-2021-3739", "sourceIdentifier": "secalert@redhat.com", "published": "2022-03-10T17:43:01.463", "lastModified": "2022-06-01T20:30:33.953", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A NULL pointer dereference flaw was found in the btrfs_rm_device function in fs/btrfs/volumes.c in the Linux Kernel, where triggering the bug requires \u2018CAP_SYS_ADMIN\u2019. This flaw allows a local attacker to crash the system or leak kernel internal information. The highest threat from this vulnerability is to system availability."}, {"lang": "es", "value": "Se ha encontrado un fallo de desreferencia de puntero NULL en la funci\u00f3n btrfs_rm_device en el archivo fs/btrfs/volumes.c en el Kernel de Linux, donde el desencadenamiento del bug requiere \"CAP_SYS_ADMIN\". Este fallo permite a un atacante local bloquear el sistema o filtrar informaci\u00f3n interna del kernel. La mayor amenaza de esta vulnerabilidad es la disponibilidad del sistema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "5.14.20", "matchCriteriaId": "BBD585C2-3F95-4E2A-BD16-33C3A6211557"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:34:*:*:*:*:*:*:*", "matchCriteriaId": "A930E247-0B43-43CB-98FF-6CE7B8189835"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "1746E116-3B82-4F65-B540-63D4058BD471"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "04FD1F9A-8F43-4509-9A49-714C54C4783C"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "BAFC49B0-FC63-4F06-A9DC-B853186003A9"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "504201E4-04CD-4224-9264-C1AEAD480E36"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "9BFAB819-0951-4D57-9B86-3CF590E50E7A"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "BDDA0D1D-3A1E-4CF5-BD6A-F05AE4E8CDDA"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "806D1842-64F5-4F4C-B728-AFD0B99CE6EB"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "266DDF39-2707-401F-88AF-3761D1045202"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "4ED4148B-73E9-48DA-BB54-F5A43B21FD56"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "B09AB46C-8B35-4085-AD86-56EC06F665CB"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "46BB915A-F2AE-4041-89F1-03547F819DFF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "A33DCDCD-58EC-495A-BD5E-BB612F5B8A39"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "88A6EDE4-DD97-45A9-8366-E999525AA68F"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "C2934495-6D4D-4C21-89E3-A2414ABDD5CE"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:baseboard_management_controller_h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "8665C148-3C9E-4EC9-A281-293D2352B8B9"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:baseboard_management_controller_h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "1A0CE664-32E5-4917-8319-7D2A31DCD72F"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1997958", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=e4571b8c5e9ffa1e85c0c671995bd4dcc5c75091", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/e4571b8c5e9ffa1e85c0c671995bd4dcc5c75091", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220407-0006/", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://ubuntu.com/security/CVE-2021-3739", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.openwall.com/lists/oss-security/2021/08/25/3", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/e4571b8c5e9ffa1e85c0c671995bd4dcc5c75091"}}