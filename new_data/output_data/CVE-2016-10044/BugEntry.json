{"buggy_code": ["/*\n *\tAn async IO implementation for Linux\n *\tWritten by Benjamin LaHaise <bcrl@kvack.org>\n *\n *\tImplements an efficient asynchronous io interface.\n *\n *\tCopyright 2000, 2001, 2002 Red Hat, Inc.  All Rights Reserved.\n *\n *\tSee ../COPYING for licensing terms.\n */\n#define pr_fmt(fmt) \"%s: \" fmt, __func__\n\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/time.h>\n#include <linux/aio_abi.h>\n#include <linux/export.h>\n#include <linux/syscalls.h>\n#include <linux/backing-dev.h>\n#include <linux/uio.h>\n\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/mmu_context.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/aio.h>\n#include <linux/highmem.h>\n#include <linux/workqueue.h>\n#include <linux/security.h>\n#include <linux/eventfd.h>\n#include <linux/blkdev.h>\n#include <linux/compat.h>\n#include <linux/migrate.h>\n#include <linux/ramfs.h>\n#include <linux/percpu-refcount.h>\n#include <linux/mount.h>\n\n#include <asm/kmap_types.h>\n#include <asm/uaccess.h>\n\n#include \"internal.h\"\n\n#define AIO_RING_MAGIC\t\t\t0xa10a10a1\n#define AIO_RING_COMPAT_FEATURES\t1\n#define AIO_RING_INCOMPAT_FEATURES\t0\nstruct aio_ring {\n\tunsigned\tid;\t/* kernel internal index number */\n\tunsigned\tnr;\t/* number of io_events */\n\tunsigned\thead;\t/* Written to by userland or under ring_lock\n\t\t\t\t * mutex by aio_read_events_ring(). */\n\tunsigned\ttail;\n\n\tunsigned\tmagic;\n\tunsigned\tcompat_features;\n\tunsigned\tincompat_features;\n\tunsigned\theader_length;\t/* size of aio_ring */\n\n\n\tstruct io_event\t\tio_events[0];\n}; /* 128 bytes + ring size */\n\n#define AIO_RING_PAGES\t8\n\nstruct kioctx_table {\n\tstruct rcu_head\trcu;\n\tunsigned\tnr;\n\tstruct kioctx\t*table[];\n};\n\nstruct kioctx_cpu {\n\tunsigned\t\treqs_available;\n};\n\nstruct ctx_rq_wait {\n\tstruct completion comp;\n\tatomic_t count;\n};\n\nstruct kioctx {\n\tstruct percpu_ref\tusers;\n\tatomic_t\t\tdead;\n\n\tstruct percpu_ref\treqs;\n\n\tunsigned long\t\tuser_id;\n\n\tstruct __percpu kioctx_cpu *cpu;\n\n\t/*\n\t * For percpu reqs_available, number of slots we move to/from global\n\t * counter at a time:\n\t */\n\tunsigned\t\treq_batch;\n\t/*\n\t * This is what userspace passed to io_setup(), it's not used for\n\t * anything but counting against the global max_reqs quota.\n\t *\n\t * The real limit is nr_events - 1, which will be larger (see\n\t * aio_setup_ring())\n\t */\n\tunsigned\t\tmax_reqs;\n\n\t/* Size of ringbuffer, in units of struct io_event */\n\tunsigned\t\tnr_events;\n\n\tunsigned long\t\tmmap_base;\n\tunsigned long\t\tmmap_size;\n\n\tstruct page\t\t**ring_pages;\n\tlong\t\t\tnr_pages;\n\n\tstruct work_struct\tfree_work;\n\n\t/*\n\t * signals when all in-flight requests are done\n\t */\n\tstruct ctx_rq_wait\t*rq_wait;\n\n\tstruct {\n\t\t/*\n\t\t * This counts the number of available slots in the ringbuffer,\n\t\t * so we avoid overflowing it: it's decremented (if positive)\n\t\t * when allocating a kiocb and incremented when the resulting\n\t\t * io_event is pulled off the ringbuffer.\n\t\t *\n\t\t * We batch accesses to it with a percpu version.\n\t\t */\n\t\tatomic_t\treqs_available;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\tctx_lock;\n\t\tstruct list_head active_reqs;\t/* used for cancellation */\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\tring_lock;\n\t\twait_queue_head_t wait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned\ttail;\n\t\tunsigned\tcompleted_events;\n\t\tspinlock_t\tcompletion_lock;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct page\t\t*internal_pages[AIO_RING_PAGES];\n\tstruct file\t\t*aio_ring_file;\n\n\tunsigned\t\tid;\n};\n\n/*\n * We use ki_cancel == KIOCB_CANCELLED to indicate that a kiocb has been either\n * cancelled or completed (this makes a certain amount of sense because\n * successful cancellation - io_cancel() - does deliver the completion to\n * userspace).\n *\n * And since most things don't implement kiocb cancellation and we'd really like\n * kiocb completion to be lockless when possible, we use ki_cancel to\n * synchronize cancellation and completion - we only set it to KIOCB_CANCELLED\n * with xchg() or cmpxchg(), see batch_complete_aio() and kiocb_cancel().\n */\n#define KIOCB_CANCELLED\t\t((void *) (~0ULL))\n\nstruct aio_kiocb {\n\tstruct kiocb\t\tcommon;\n\n\tstruct kioctx\t\t*ki_ctx;\n\tkiocb_cancel_fn\t\t*ki_cancel;\n\n\tstruct iocb __user\t*ki_user_iocb;\t/* user's aiocb */\n\t__u64\t\t\tki_user_data;\t/* user's data for completion */\n\n\tstruct list_head\tki_list;\t/* the aio core uses this\n\t\t\t\t\t\t * for cancellation */\n\n\t/*\n\t * If the aio_resfd field of the userspace iocb is not zero,\n\t * this is the underlying eventfd context to deliver events to.\n\t */\n\tstruct eventfd_ctx\t*ki_eventfd;\n};\n\n/*------ sysctl variables----*/\nstatic DEFINE_SPINLOCK(aio_nr_lock);\nunsigned long aio_nr;\t\t/* current system wide number of aio requests */\nunsigned long aio_max_nr = 0x10000; /* system wide maximum number of aio requests */\n/*----end sysctl variables---*/\n\nstatic struct kmem_cache\t*kiocb_cachep;\nstatic struct kmem_cache\t*kioctx_cachep;\n\nstatic struct vfsmount *aio_mnt;\n\nstatic const struct file_operations aio_ring_fops;\nstatic const struct address_space_operations aio_ctx_aops;\n\nstatic struct file *aio_private_file(struct kioctx *ctx, loff_t nr_pages)\n{\n\tstruct qstr this = QSTR_INIT(\"[aio]\", 5);\n\tstruct file *file;\n\tstruct path path;\n\tstruct inode *inode = alloc_anon_inode(aio_mnt->mnt_sb);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\n\tinode->i_mapping->a_ops = &aio_ctx_aops;\n\tinode->i_mapping->private_data = ctx;\n\tinode->i_size = PAGE_SIZE * nr_pages;\n\n\tpath.dentry = d_alloc_pseudo(aio_mnt->mnt_sb, &this);\n\tif (!path.dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tpath.mnt = mntget(aio_mnt);\n\n\td_instantiate(path.dentry, inode);\n\tfile = alloc_file(&path, FMODE_READ | FMODE_WRITE, &aio_ring_fops);\n\tif (IS_ERR(file)) {\n\t\tpath_put(&path);\n\t\treturn file;\n\t}\n\n\tfile->f_flags = O_RDWR;\n\treturn file;\n}\n\nstatic struct dentry *aio_mount(struct file_system_type *fs_type,\n\t\t\t\tint flags, const char *dev_name, void *data)\n{\n\tstatic const struct dentry_operations ops = {\n\t\t.d_dname\t= simple_dname,\n\t};\n\treturn mount_pseudo(fs_type, \"aio:\", NULL, &ops, AIO_RING_MAGIC);\n}\n\n/* aio_setup\n *\tCreates the slab caches used by the aio routines, panic on\n *\tfailure as this is done early during the boot sequence.\n */\nstatic int __init aio_setup(void)\n{\n\tstatic struct file_system_type aio_fs = {\n\t\t.name\t\t= \"aio\",\n\t\t.mount\t\t= aio_mount,\n\t\t.kill_sb\t= kill_anon_super,\n\t};\n\taio_mnt = kern_mount(&aio_fs);\n\tif (IS_ERR(aio_mnt))\n\t\tpanic(\"Failed to create aio fs mount.\");\n\n\tkiocb_cachep = KMEM_CACHE(aio_kiocb, SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\tkioctx_cachep = KMEM_CACHE(kioctx,SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\n\tpr_debug(\"sizeof(struct page) = %zu\\n\", sizeof(struct page));\n\n\treturn 0;\n}\n__initcall(aio_setup);\n\nstatic void put_aio_ring_file(struct kioctx *ctx)\n{\n\tstruct file *aio_ring_file = ctx->aio_ring_file;\n\tif (aio_ring_file) {\n\t\ttruncate_setsize(aio_ring_file->f_inode, 0);\n\n\t\t/* Prevent further access to the kioctx from migratepages */\n\t\tspin_lock(&aio_ring_file->f_inode->i_mapping->private_lock);\n\t\taio_ring_file->f_inode->i_mapping->private_data = NULL;\n\t\tctx->aio_ring_file = NULL;\n\t\tspin_unlock(&aio_ring_file->f_inode->i_mapping->private_lock);\n\n\t\tfput(aio_ring_file);\n\t}\n}\n\nstatic void aio_free_ring(struct kioctx *ctx)\n{\n\tint i;\n\n\t/* Disconnect the kiotx from the ring file.  This prevents future\n\t * accesses to the kioctx from page migration.\n\t */\n\tput_aio_ring_file(ctx);\n\n\tfor (i = 0; i < ctx->nr_pages; i++) {\n\t\tstruct page *page;\n\t\tpr_debug(\"pid(%d) [%d] page->count=%d\\n\", current->pid, i,\n\t\t\t\tpage_count(ctx->ring_pages[i]));\n\t\tpage = ctx->ring_pages[i];\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tctx->ring_pages[i] = NULL;\n\t\tput_page(page);\n\t}\n\n\tif (ctx->ring_pages && ctx->ring_pages != ctx->internal_pages) {\n\t\tkfree(ctx->ring_pages);\n\t\tctx->ring_pages = NULL;\n\t}\n}\n\nstatic int aio_ring_mremap(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct kioctx_table *table;\n\tint i, res = -EINVAL;\n\n\tspin_lock(&mm->ioctx_lock);\n\trcu_read_lock();\n\ttable = rcu_dereference(mm->ioctx_table);\n\tfor (i = 0; i < table->nr; i++) {\n\t\tstruct kioctx *ctx;\n\n\t\tctx = table->table[i];\n\t\tif (ctx && ctx->aio_ring_file == file) {\n\t\t\tif (!atomic_read(&ctx->dead)) {\n\t\t\t\tctx->user_id = ctx->mmap_base = vma->vm_start;\n\t\t\t\tres = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\tspin_unlock(&mm->ioctx_lock);\n\treturn res;\n}\n\nstatic const struct vm_operations_struct aio_ring_vm_ops = {\n\t.mremap\t\t= aio_ring_mremap,\n#if IS_ENABLED(CONFIG_MMU)\n\t.fault\t\t= filemap_fault,\n\t.map_pages\t= filemap_map_pages,\n\t.page_mkwrite\t= filemap_page_mkwrite,\n#endif\n};\n\nstatic int aio_ring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_flags |= VM_DONTEXPAND;\n\tvma->vm_ops = &aio_ring_vm_ops;\n\treturn 0;\n}\n\nstatic const struct file_operations aio_ring_fops = {\n\t.mmap = aio_ring_mmap,\n};\n\n#if IS_ENABLED(CONFIG_MIGRATION)\nstatic int aio_migratepage(struct address_space *mapping, struct page *new,\n\t\t\tstruct page *old, enum migrate_mode mode)\n{\n\tstruct kioctx *ctx;\n\tunsigned long flags;\n\tpgoff_t idx;\n\tint rc;\n\n\trc = 0;\n\n\t/* mapping->private_lock here protects against the kioctx teardown.  */\n\tspin_lock(&mapping->private_lock);\n\tctx = mapping->private_data;\n\tif (!ctx) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* The ring_lock mutex.  The prevents aio_read_events() from writing\n\t * to the ring's head, and prevents page migration from mucking in\n\t * a partially initialized kiotx.\n\t */\n\tif (!mutex_trylock(&ctx->ring_lock)) {\n\t\trc = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tidx = old->index;\n\tif (idx < (pgoff_t)ctx->nr_pages) {\n\t\t/* Make sure the old page hasn't already been changed */\n\t\tif (ctx->ring_pages[idx] != old)\n\t\t\trc = -EAGAIN;\n\t} else\n\t\trc = -EINVAL;\n\n\tif (rc != 0)\n\t\tgoto out_unlock;\n\n\t/* Writeback must be complete */\n\tBUG_ON(PageWriteback(old));\n\tget_page(new);\n\n\trc = migrate_page_move_mapping(mapping, new, old, NULL, mode, 1);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tput_page(new);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Take completion_lock to prevent other writes to the ring buffer\n\t * while the old page is copied to the new.  This prevents new\n\t * events from being lost.\n\t */\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tmigrate_page_copy(new, old);\n\tBUG_ON(ctx->ring_pages[idx] != old);\n\tctx->ring_pages[idx] = new;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\t/* The old page is no longer accessible. */\n\tput_page(old);\n\nout_unlock:\n\tmutex_unlock(&ctx->ring_lock);\nout:\n\tspin_unlock(&mapping->private_lock);\n\treturn rc;\n}\n#endif\n\nstatic const struct address_space_operations aio_ctx_aops = {\n\t.set_page_dirty = __set_page_dirty_no_writeback,\n#if IS_ENABLED(CONFIG_MIGRATION)\n\t.migratepage\t= aio_migratepage,\n#endif\n};\n\nstatic int aio_setup_ring(struct kioctx *ctx)\n{\n\tstruct aio_ring *ring;\n\tunsigned nr_events = ctx->max_reqs;\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long size, unused;\n\tint nr_pages;\n\tint i;\n\tstruct file *file;\n\n\t/* Compensate for the ring buffer's head/tail overlap entry */\n\tnr_events += 2;\t/* 1 is required, 2 for good luck */\n\n\tsize = sizeof(struct aio_ring);\n\tsize += sizeof(struct io_event) * nr_events;\n\n\tnr_pages = PFN_UP(size);\n\tif (nr_pages < 0)\n\t\treturn -EINVAL;\n\n\tfile = aio_private_file(ctx, nr_pages);\n\tif (IS_ERR(file)) {\n\t\tctx->aio_ring_file = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->aio_ring_file = file;\n\tnr_events = (PAGE_SIZE * nr_pages - sizeof(struct aio_ring))\n\t\t\t/ sizeof(struct io_event);\n\n\tctx->ring_pages = ctx->internal_pages;\n\tif (nr_pages > AIO_RING_PAGES) {\n\t\tctx->ring_pages = kcalloc(nr_pages, sizeof(struct page *),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!ctx->ring_pages) {\n\t\t\tput_aio_ring_file(ctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct page *page;\n\t\tpage = find_or_create_page(file->f_inode->i_mapping,\n\t\t\t\t\t   i, GFP_HIGHUSER | __GFP_ZERO);\n\t\tif (!page)\n\t\t\tbreak;\n\t\tpr_debug(\"pid(%d) page[%d]->count=%d\\n\",\n\t\t\t current->pid, i, page_count(page));\n\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\n\t\tctx->ring_pages[i] = page;\n\t}\n\tctx->nr_pages = i;\n\n\tif (unlikely(i != nr_pages)) {\n\t\taio_free_ring(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->mmap_size = nr_pages * PAGE_SIZE;\n\tpr_debug(\"attempting mmap of %lu bytes\\n\", ctx->mmap_size);\n\n\tif (down_write_killable(&mm->mmap_sem)) {\n\t\tctx->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -EINTR;\n\t}\n\n\tctx->mmap_base = do_mmap_pgoff(ctx->aio_ring_file, 0, ctx->mmap_size,\n\t\t\t\t       PROT_READ | PROT_WRITE,\n\t\t\t\t       MAP_SHARED, 0, &unused);\n\tup_write(&mm->mmap_sem);\n\tif (IS_ERR((void *)ctx->mmap_base)) {\n\t\tctx->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tpr_debug(\"mmap address: 0x%08lx\\n\", ctx->mmap_base);\n\n\tctx->user_id = ctx->mmap_base;\n\tctx->nr_events = nr_events; /* trusted copy */\n\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\tring->nr = nr_events;\t/* user copy */\n\tring->id = ~0U;\n\tring->head = ring->tail = 0;\n\tring->magic = AIO_RING_MAGIC;\n\tring->compat_features = AIO_RING_COMPAT_FEATURES;\n\tring->incompat_features = AIO_RING_INCOMPAT_FEATURES;\n\tring->header_length = sizeof(struct aio_ring);\n\tkunmap_atomic(ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\treturn 0;\n}\n\n#define AIO_EVENTS_PER_PAGE\t(PAGE_SIZE / sizeof(struct io_event))\n#define AIO_EVENTS_FIRST_PAGE\t((PAGE_SIZE - sizeof(struct aio_ring)) / sizeof(struct io_event))\n#define AIO_EVENTS_OFFSET\t(AIO_EVENTS_PER_PAGE - AIO_EVENTS_FIRST_PAGE)\n\nvoid kiocb_set_cancel_fn(struct kiocb *iocb, kiocb_cancel_fn *cancel)\n{\n\tstruct aio_kiocb *req = container_of(iocb, struct aio_kiocb, common);\n\tstruct kioctx *ctx = req->ki_ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\n\tif (!req->ki_list.next)\n\t\tlist_add(&req->ki_list, &ctx->active_reqs);\n\n\treq->ki_cancel = cancel;\n\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n}\nEXPORT_SYMBOL(kiocb_set_cancel_fn);\n\nstatic int kiocb_cancel(struct aio_kiocb *kiocb)\n{\n\tkiocb_cancel_fn *old, *cancel;\n\n\t/*\n\t * Don't want to set kiocb->ki_cancel = KIOCB_CANCELLED unless it\n\t * actually has a cancel function, hence the cmpxchg()\n\t */\n\n\tcancel = ACCESS_ONCE(kiocb->ki_cancel);\n\tdo {\n\t\tif (!cancel || cancel == KIOCB_CANCELLED)\n\t\t\treturn -EINVAL;\n\n\t\told = cancel;\n\t\tcancel = cmpxchg(&kiocb->ki_cancel, old, KIOCB_CANCELLED);\n\t} while (cancel != old);\n\n\treturn cancel(&kiocb->common);\n}\n\nstatic void free_ioctx(struct work_struct *work)\n{\n\tstruct kioctx *ctx = container_of(work, struct kioctx, free_work);\n\n\tpr_debug(\"freeing %p\\n\", ctx);\n\n\taio_free_ring(ctx);\n\tfree_percpu(ctx->cpu);\n\tpercpu_ref_exit(&ctx->reqs);\n\tpercpu_ref_exit(&ctx->users);\n\tkmem_cache_free(kioctx_cachep, ctx);\n}\n\nstatic void free_ioctx_reqs(struct percpu_ref *ref)\n{\n\tstruct kioctx *ctx = container_of(ref, struct kioctx, reqs);\n\n\t/* At this point we know that there are no any in-flight requests */\n\tif (ctx->rq_wait && atomic_dec_and_test(&ctx->rq_wait->count))\n\t\tcomplete(&ctx->rq_wait->comp);\n\n\tINIT_WORK(&ctx->free_work, free_ioctx);\n\tschedule_work(&ctx->free_work);\n}\n\n/*\n * When this function runs, the kioctx has been removed from the \"hash table\"\n * and ctx->users has dropped to 0, so we know no more kiocbs can be submitted -\n * now it's safe to cancel any that need to be.\n */\nstatic void free_ioctx_users(struct percpu_ref *ref)\n{\n\tstruct kioctx *ctx = container_of(ref, struct kioctx, users);\n\tstruct aio_kiocb *req;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\twhile (!list_empty(&ctx->active_reqs)) {\n\t\treq = list_first_entry(&ctx->active_reqs,\n\t\t\t\t       struct aio_kiocb, ki_list);\n\n\t\tlist_del_init(&req->ki_list);\n\t\tkiocb_cancel(req);\n\t}\n\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tpercpu_ref_kill(&ctx->reqs);\n\tpercpu_ref_put(&ctx->reqs);\n}\n\nstatic int ioctx_add_table(struct kioctx *ctx, struct mm_struct *mm)\n{\n\tunsigned i, new_nr;\n\tstruct kioctx_table *table, *old;\n\tstruct aio_ring *ring;\n\n\tspin_lock(&mm->ioctx_lock);\n\ttable = rcu_dereference_raw(mm->ioctx_table);\n\n\twhile (1) {\n\t\tif (table)\n\t\t\tfor (i = 0; i < table->nr; i++)\n\t\t\t\tif (!table->table[i]) {\n\t\t\t\t\tctx->id = i;\n\t\t\t\t\ttable->table[i] = ctx;\n\t\t\t\t\tspin_unlock(&mm->ioctx_lock);\n\n\t\t\t\t\t/* While kioctx setup is in progress,\n\t\t\t\t\t * we are protected from page migration\n\t\t\t\t\t * changes ring_pages by ->ring_lock.\n\t\t\t\t\t */\n\t\t\t\t\tring = kmap_atomic(ctx->ring_pages[0]);\n\t\t\t\t\tring->id = ctx->id;\n\t\t\t\t\tkunmap_atomic(ring);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\tnew_nr = (table ? table->nr : 1) * 4;\n\t\tspin_unlock(&mm->ioctx_lock);\n\n\t\ttable = kzalloc(sizeof(*table) + sizeof(struct kioctx *) *\n\t\t\t\tnew_nr, GFP_KERNEL);\n\t\tif (!table)\n\t\t\treturn -ENOMEM;\n\n\t\ttable->nr = new_nr;\n\n\t\tspin_lock(&mm->ioctx_lock);\n\t\told = rcu_dereference_raw(mm->ioctx_table);\n\n\t\tif (!old) {\n\t\t\trcu_assign_pointer(mm->ioctx_table, table);\n\t\t} else if (table->nr > old->nr) {\n\t\t\tmemcpy(table->table, old->table,\n\t\t\t       old->nr * sizeof(struct kioctx *));\n\n\t\t\trcu_assign_pointer(mm->ioctx_table, table);\n\t\t\tkfree_rcu(old, rcu);\n\t\t} else {\n\t\t\tkfree(table);\n\t\t\ttable = old;\n\t\t}\n\t}\n}\n\nstatic void aio_nr_sub(unsigned nr)\n{\n\tspin_lock(&aio_nr_lock);\n\tif (WARN_ON(aio_nr - nr > aio_nr))\n\t\taio_nr = 0;\n\telse\n\t\taio_nr -= nr;\n\tspin_unlock(&aio_nr_lock);\n}\n\n/* ioctx_alloc\n *\tAllocates and initializes an ioctx.  Returns an ERR_PTR if it failed.\n */\nstatic struct kioctx *ioctx_alloc(unsigned nr_events)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx;\n\tint err = -ENOMEM;\n\n\t/*\n\t * We keep track of the number of available ringbuffer slots, to prevent\n\t * overflow (reqs_available), and we also use percpu counters for this.\n\t *\n\t * So since up to half the slots might be on other cpu's percpu counters\n\t * and unavailable, double nr_events so userspace sees what they\n\t * expected: additionally, we move req_batch slots to/from percpu\n\t * counters at a time, so make sure that isn't 0:\n\t */\n\tnr_events = max(nr_events, num_possible_cpus() * 4);\n\tnr_events *= 2;\n\n\t/* Prevent overflows */\n\tif (nr_events > (0x10000000U / sizeof(struct io_event))) {\n\t\tpr_debug(\"ENOMEM: nr_events too high\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_events || (unsigned long)nr_events > (aio_max_nr * 2UL))\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tctx = kmem_cache_zalloc(kioctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx->max_reqs = nr_events;\n\n\tspin_lock_init(&ctx->ctx_lock);\n\tspin_lock_init(&ctx->completion_lock);\n\tmutex_init(&ctx->ring_lock);\n\t/* Protect against page migration throughout kiotx setup by keeping\n\t * the ring_lock mutex held until setup is complete. */\n\tmutex_lock(&ctx->ring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\n\tINIT_LIST_HEAD(&ctx->active_reqs);\n\n\tif (percpu_ref_init(&ctx->users, free_ioctx_users, 0, GFP_KERNEL))\n\t\tgoto err;\n\n\tif (percpu_ref_init(&ctx->reqs, free_ioctx_reqs, 0, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->cpu = alloc_percpu(struct kioctx_cpu);\n\tif (!ctx->cpu)\n\t\tgoto err;\n\n\terr = aio_setup_ring(ctx);\n\tif (err < 0)\n\t\tgoto err;\n\n\tatomic_set(&ctx->reqs_available, ctx->nr_events - 1);\n\tctx->req_batch = (ctx->nr_events - 1) / (num_possible_cpus() * 4);\n\tif (ctx->req_batch < 1)\n\t\tctx->req_batch = 1;\n\n\t/* limit the number of system wide aios */\n\tspin_lock(&aio_nr_lock);\n\tif (aio_nr + nr_events > (aio_max_nr * 2UL) ||\n\t    aio_nr + nr_events < aio_nr) {\n\t\tspin_unlock(&aio_nr_lock);\n\t\terr = -EAGAIN;\n\t\tgoto err_ctx;\n\t}\n\taio_nr += ctx->max_reqs;\n\tspin_unlock(&aio_nr_lock);\n\n\tpercpu_ref_get(&ctx->users);\t/* io_setup() will drop this ref */\n\tpercpu_ref_get(&ctx->reqs);\t/* free_ioctx_users() will drop this */\n\n\terr = ioctx_add_table(ctx, mm);\n\tif (err)\n\t\tgoto err_cleanup;\n\n\t/* Release the ring_lock mutex now that all setup is complete. */\n\tmutex_unlock(&ctx->ring_lock);\n\n\tpr_debug(\"allocated ioctx %p[%ld]: mm=%p mask=0x%x\\n\",\n\t\t ctx, ctx->user_id, mm, ctx->nr_events);\n\treturn ctx;\n\nerr_cleanup:\n\taio_nr_sub(ctx->max_reqs);\nerr_ctx:\n\tatomic_set(&ctx->dead, 1);\n\tif (ctx->mmap_size)\n\t\tvm_munmap(ctx->mmap_base, ctx->mmap_size);\n\taio_free_ring(ctx);\nerr:\n\tmutex_unlock(&ctx->ring_lock);\n\tfree_percpu(ctx->cpu);\n\tpercpu_ref_exit(&ctx->reqs);\n\tpercpu_ref_exit(&ctx->users);\n\tkmem_cache_free(kioctx_cachep, ctx);\n\tpr_debug(\"error allocating ioctx %d\\n\", err);\n\treturn ERR_PTR(err);\n}\n\n/* kill_ioctx\n *\tCancels all outstanding aio requests on an aio context.  Used\n *\twhen the processes owning a context have all exited to encourage\n *\tthe rapid destruction of the kioctx.\n */\nstatic int kill_ioctx(struct mm_struct *mm, struct kioctx *ctx,\n\t\t      struct ctx_rq_wait *wait)\n{\n\tstruct kioctx_table *table;\n\n\tspin_lock(&mm->ioctx_lock);\n\tif (atomic_xchg(&ctx->dead, 1)) {\n\t\tspin_unlock(&mm->ioctx_lock);\n\t\treturn -EINVAL;\n\t}\n\n\ttable = rcu_dereference_raw(mm->ioctx_table);\n\tWARN_ON(ctx != table->table[ctx->id]);\n\ttable->table[ctx->id] = NULL;\n\tspin_unlock(&mm->ioctx_lock);\n\n\t/* percpu_ref_kill() will do the necessary call_rcu() */\n\twake_up_all(&ctx->wait);\n\n\t/*\n\t * It'd be more correct to do this in free_ioctx(), after all\n\t * the outstanding kiocbs have finished - but by then io_destroy\n\t * has already returned, so io_setup() could potentially return\n\t * -EAGAIN with no ioctxs actually in use (as far as userspace\n\t *  could tell).\n\t */\n\taio_nr_sub(ctx->max_reqs);\n\n\tif (ctx->mmap_size)\n\t\tvm_munmap(ctx->mmap_base, ctx->mmap_size);\n\n\tctx->rq_wait = wait;\n\tpercpu_ref_kill(&ctx->users);\n\treturn 0;\n}\n\n/*\n * exit_aio: called when the last user of mm goes away.  At this point, there is\n * no way for any new requests to be submited or any of the io_* syscalls to be\n * called on the context.\n *\n * There may be outstanding kiocbs, but free_ioctx() will explicitly wait on\n * them.\n */\nvoid exit_aio(struct mm_struct *mm)\n{\n\tstruct kioctx_table *table = rcu_dereference_raw(mm->ioctx_table);\n\tstruct ctx_rq_wait wait;\n\tint i, skipped;\n\n\tif (!table)\n\t\treturn;\n\n\tatomic_set(&wait.count, table->nr);\n\tinit_completion(&wait.comp);\n\n\tskipped = 0;\n\tfor (i = 0; i < table->nr; ++i) {\n\t\tstruct kioctx *ctx = table->table[i];\n\n\t\tif (!ctx) {\n\t\t\tskipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We don't need to bother with munmap() here - exit_mmap(mm)\n\t\t * is coming and it'll unmap everything. And we simply can't,\n\t\t * this is not necessarily our ->mm.\n\t\t * Since kill_ioctx() uses non-zero ->mmap_size as indicator\n\t\t * that it needs to unmap the area, just set it to 0.\n\t\t */\n\t\tctx->mmap_size = 0;\n\t\tkill_ioctx(mm, ctx, &wait);\n\t}\n\n\tif (!atomic_sub_and_test(skipped, &wait.count)) {\n\t\t/* Wait until all IO for the context are done. */\n\t\twait_for_completion(&wait.comp);\n\t}\n\n\tRCU_INIT_POINTER(mm->ioctx_table, NULL);\n\tkfree(table);\n}\n\nstatic void put_reqs_available(struct kioctx *ctx, unsigned nr)\n{\n\tstruct kioctx_cpu *kcpu;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkcpu = this_cpu_ptr(ctx->cpu);\n\tkcpu->reqs_available += nr;\n\n\twhile (kcpu->reqs_available >= ctx->req_batch * 2) {\n\t\tkcpu->reqs_available -= ctx->req_batch;\n\t\tatomic_add(ctx->req_batch, &ctx->reqs_available);\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nstatic bool get_reqs_available(struct kioctx *ctx)\n{\n\tstruct kioctx_cpu *kcpu;\n\tbool ret = false;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkcpu = this_cpu_ptr(ctx->cpu);\n\tif (!kcpu->reqs_available) {\n\t\tint old, avail = atomic_read(&ctx->reqs_available);\n\n\t\tdo {\n\t\t\tif (avail < ctx->req_batch)\n\t\t\t\tgoto out;\n\n\t\t\told = avail;\n\t\t\tavail = atomic_cmpxchg(&ctx->reqs_available,\n\t\t\t\t\t       avail, avail - ctx->req_batch);\n\t\t} while (avail != old);\n\n\t\tkcpu->reqs_available += ctx->req_batch;\n\t}\n\n\tret = true;\n\tkcpu->reqs_available--;\nout:\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/* refill_reqs_available\n *\tUpdates the reqs_available reference counts used for tracking the\n *\tnumber of free slots in the completion ring.  This can be called\n *\tfrom aio_complete() (to optimistically update reqs_available) or\n *\tfrom aio_get_req() (the we're out of events case).  It must be\n *\tcalled holding ctx->completion_lock.\n */\nstatic void refill_reqs_available(struct kioctx *ctx, unsigned head,\n                                  unsigned tail)\n{\n\tunsigned events_in_ring, completed;\n\n\t/* Clamp head since userland can write to it. */\n\thead %= ctx->nr_events;\n\tif (head <= tail)\n\t\tevents_in_ring = tail - head;\n\telse\n\t\tevents_in_ring = ctx->nr_events - (head - tail);\n\n\tcompleted = ctx->completed_events;\n\tif (events_in_ring < completed)\n\t\tcompleted -= events_in_ring;\n\telse\n\t\tcompleted = 0;\n\n\tif (!completed)\n\t\treturn;\n\n\tctx->completed_events -= completed;\n\tput_reqs_available(ctx, completed);\n}\n\n/* user_refill_reqs_available\n *\tCalled to refill reqs_available when aio_get_req() encounters an\n *\tout of space in the completion ring.\n */\nstatic void user_refill_reqs_available(struct kioctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (ctx->completed_events) {\n\t\tstruct aio_ring *ring;\n\t\tunsigned head;\n\n\t\t/* Access of ring->head may race with aio_read_events_ring()\n\t\t * here, but that's okay since whether we read the old version\n\t\t * or the new version, and either will be valid.  The important\n\t\t * part is that head cannot pass tail since we prevent\n\t\t * aio_complete() from updating tail by holding\n\t\t * ctx->completion_lock.  Even if head is invalid, the check\n\t\t * against ctx->completed_events below will make sure we do the\n\t\t * safe/right thing.\n\t\t */\n\t\tring = kmap_atomic(ctx->ring_pages[0]);\n\t\thead = ring->head;\n\t\tkunmap_atomic(ring);\n\n\t\trefill_reqs_available(ctx, head, ctx->tail);\n\t}\n\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\n/* aio_get_req\n *\tAllocate a slot for an aio request.\n * Returns NULL if no requests are free.\n */\nstatic inline struct aio_kiocb *aio_get_req(struct kioctx *ctx)\n{\n\tstruct aio_kiocb *req;\n\n\tif (!get_reqs_available(ctx)) {\n\t\tuser_refill_reqs_available(ctx);\n\t\tif (!get_reqs_available(ctx))\n\t\t\treturn NULL;\n\t}\n\n\treq = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL|__GFP_ZERO);\n\tif (unlikely(!req))\n\t\tgoto out_put;\n\n\tpercpu_ref_get(&ctx->reqs);\n\n\treq->ki_ctx = ctx;\n\treturn req;\nout_put:\n\tput_reqs_available(ctx, 1);\n\treturn NULL;\n}\n\nstatic void kiocb_free(struct aio_kiocb *req)\n{\n\tif (req->common.ki_filp)\n\t\tfput(req->common.ki_filp);\n\tif (req->ki_eventfd != NULL)\n\t\teventfd_ctx_put(req->ki_eventfd);\n\tkmem_cache_free(kiocb_cachep, req);\n}\n\nstatic struct kioctx *lookup_ioctx(unsigned long ctx_id)\n{\n\tstruct aio_ring __user *ring  = (void __user *)ctx_id;\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx, *ret = NULL;\n\tstruct kioctx_table *table;\n\tunsigned id;\n\n\tif (get_user(id, &ring->id))\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttable = rcu_dereference(mm->ioctx_table);\n\n\tif (!table || id >= table->nr)\n\t\tgoto out;\n\n\tctx = table->table[id];\n\tif (ctx && ctx->user_id == ctx_id) {\n\t\tpercpu_ref_get(&ctx->users);\n\t\tret = ctx;\n\t}\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/* aio_complete\n *\tCalled when the io request on the given iocb is complete.\n */\nstatic void aio_complete(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, common);\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tstruct aio_ring\t*ring;\n\tstruct io_event\t*ev_page, *event;\n\tunsigned tail, pos, head;\n\tunsigned long\tflags;\n\n\t/*\n\t * Special case handling for sync iocbs:\n\t *  - events go directly into the iocb for fast handling\n\t *  - the sync task with the iocb in its stack holds the single iocb\n\t *    ref, no other paths have a way to get another ref\n\t *  - the sync task helpfully left a reference to itself in the iocb\n\t */\n\tBUG_ON(is_sync_kiocb(kiocb));\n\n\tif (iocb->ki_list.next) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\t\tlist_del(&iocb->ki_list);\n\t\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\t}\n\n\t/*\n\t * Add a completion event to the ring buffer. Must be done holding\n\t * ctx->completion_lock to prevent other code from messing with the tail\n\t * pointer since we might be called from irq context.\n\t */\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\n\ttail = ctx->tail;\n\tpos = tail + AIO_EVENTS_OFFSET;\n\n\tif (++tail >= ctx->nr_events)\n\t\ttail = 0;\n\n\tev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);\n\tevent = ev_page + pos % AIO_EVENTS_PER_PAGE;\n\n\tevent->obj = (u64)(unsigned long)iocb->ki_user_iocb;\n\tevent->data = iocb->ki_user_data;\n\tevent->res = res;\n\tevent->res2 = res2;\n\n\tkunmap_atomic(ev_page);\n\tflush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);\n\n\tpr_debug(\"%p[%u]: %p: %p %Lx %lx %lx\\n\",\n\t\t ctx, tail, iocb, iocb->ki_user_iocb, iocb->ki_user_data,\n\t\t res, res2);\n\n\t/* after flagging the request as done, we\n\t * must never even look at it again\n\t */\n\tsmp_wmb();\t/* make event visible before updating tail */\n\n\tctx->tail = tail;\n\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\thead = ring->head;\n\tring->tail = tail;\n\tkunmap_atomic(ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\tctx->completed_events++;\n\tif (ctx->completed_events > 1)\n\t\trefill_reqs_available(ctx, head, tail);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tpr_debug(\"added to ring %p at [%u]\\n\", iocb, tail);\n\n\t/*\n\t * Check if the user asked us to deliver the result through an\n\t * eventfd. The eventfd_signal() function is safe to be called\n\t * from IRQ context.\n\t */\n\tif (iocb->ki_eventfd != NULL)\n\t\teventfd_signal(iocb->ki_eventfd, 1);\n\n\t/* everything turned out well, dispose of the aiocb. */\n\tkiocb_free(iocb);\n\n\t/*\n\t * We have to order our ring_info tail store above and test\n\t * of the wait list below outside the wait lock.  This is\n\t * like in wake_up_bit() where clearing a bit has to be\n\t * ordered with the unlocked test.\n\t */\n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\n\tpercpu_ref_put(&ctx->reqs);\n}\n\n/* aio_read_events_ring\n *\tPull an event off of the ioctx's event ring.  Returns the number of\n *\tevents fetched\n */\nstatic long aio_read_events_ring(struct kioctx *ctx,\n\t\t\t\t struct io_event __user *event, long nr)\n{\n\tstruct aio_ring *ring;\n\tunsigned head, tail, pos;\n\tlong ret = 0;\n\tint copy_ret;\n\n\t/*\n\t * The mutex can block and wake us up and that will cause\n\t * wait_event_interruptible_hrtimeout() to schedule without sleeping\n\t * and repeat. This should be rare enough that it doesn't cause\n\t * peformance issues. See the comment in read_events() for more detail.\n\t */\n\tsched_annotate_sleep();\n\tmutex_lock(&ctx->ring_lock);\n\n\t/* Access to ->ring_pages here is protected by ctx->ring_lock. */\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\thead = ring->head;\n\ttail = ring->tail;\n\tkunmap_atomic(ring);\n\n\t/*\n\t * Ensure that once we've read the current tail pointer, that\n\t * we also see the events that were stored up to the tail.\n\t */\n\tsmp_rmb();\n\n\tpr_debug(\"h%u t%u m%u\\n\", head, tail, ctx->nr_events);\n\n\tif (head == tail)\n\t\tgoto out;\n\n\thead %= ctx->nr_events;\n\ttail %= ctx->nr_events;\n\n\twhile (ret < nr) {\n\t\tlong avail;\n\t\tstruct io_event *ev;\n\t\tstruct page *page;\n\n\t\tavail = (head <= tail ?  tail : ctx->nr_events) - head;\n\t\tif (head == tail)\n\t\t\tbreak;\n\n\t\tavail = min(avail, nr - ret);\n\t\tavail = min_t(long, avail, AIO_EVENTS_PER_PAGE -\n\t\t\t    ((head + AIO_EVENTS_OFFSET) % AIO_EVENTS_PER_PAGE));\n\n\t\tpos = head + AIO_EVENTS_OFFSET;\n\t\tpage = ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE];\n\t\tpos %= AIO_EVENTS_PER_PAGE;\n\n\t\tev = kmap(page);\n\t\tcopy_ret = copy_to_user(event + ret, ev + pos,\n\t\t\t\t\tsizeof(*ev) * avail);\n\t\tkunmap(page);\n\n\t\tif (unlikely(copy_ret)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret += avail;\n\t\thead += avail;\n\t\thead %= ctx->nr_events;\n\t}\n\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\tring->head = head;\n\tkunmap_atomic(ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\tpr_debug(\"%li  h%u t%u\\n\", ret, head, tail);\nout:\n\tmutex_unlock(&ctx->ring_lock);\n\n\treturn ret;\n}\n\nstatic bool aio_read_events(struct kioctx *ctx, long min_nr, long nr,\n\t\t\t    struct io_event __user *event, long *i)\n{\n\tlong ret = aio_read_events_ring(ctx, event + *i, nr - *i);\n\n\tif (ret > 0)\n\t\t*i += ret;\n\n\tif (unlikely(atomic_read(&ctx->dead)))\n\t\tret = -EINVAL;\n\n\tif (!*i)\n\t\t*i = ret;\n\n\treturn ret < 0 || *i >= min_nr;\n}\n\nstatic long read_events(struct kioctx *ctx, long min_nr, long nr,\n\t\t\tstruct io_event __user *event,\n\t\t\tstruct timespec __user *timeout)\n{\n\tktime_t until = { .tv64 = KTIME_MAX };\n\tlong ret = 0;\n\n\tif (timeout) {\n\t\tstruct timespec\tts;\n\n\t\tif (unlikely(copy_from_user(&ts, timeout, sizeof(ts))))\n\t\t\treturn -EFAULT;\n\n\t\tuntil = timespec_to_ktime(ts);\n\t}\n\n\t/*\n\t * Note that aio_read_events() is being called as the conditional - i.e.\n\t * we're calling it after prepare_to_wait() has set task state to\n\t * TASK_INTERRUPTIBLE.\n\t *\n\t * But aio_read_events() can block, and if it blocks it's going to flip\n\t * the task state back to TASK_RUNNING.\n\t *\n\t * This should be ok, provided it doesn't flip the state back to\n\t * TASK_RUNNING and return 0 too much - that causes us to spin. That\n\t * will only happen if the mutex_lock() call blocks, and we then find\n\t * the ringbuffer empty. So in practice we should be ok, but it's\n\t * something to be aware of when touching this code.\n\t */\n\tif (until.tv64 == 0)\n\t\taio_read_events(ctx, min_nr, nr, event, &ret);\n\telse\n\t\twait_event_interruptible_hrtimeout(ctx->wait,\n\t\t\t\taio_read_events(ctx, min_nr, nr, event, &ret),\n\t\t\t\tuntil);\n\n\tif (!ret && signal_pending(current))\n\t\tret = -EINTR;\n\n\treturn ret;\n}\n\n/* sys_io_setup:\n *\tCreate an aio_context capable of receiving at least nr_events.\n *\tctxp must not point to an aio_context that already exists, and\n *\tmust be initialized to 0 prior to the call.  On successful\n *\tcreation of the aio_context, *ctxp is filled in with the resulting \n *\thandle.  May fail with -EINVAL if *ctxp is not initialized,\n *\tif the specified nr_events exceeds internal limits.  May fail \n *\twith -EAGAIN if the specified nr_events exceeds the user's limit \n *\tof available events.  May fail with -ENOMEM if insufficient kernel\n *\tresources are available.  May fail with -EFAULT if an invalid\n *\tpointer is passed for ctxp.  Will fail with -ENOSYS if not\n *\timplemented.\n */\nSYSCALL_DEFINE2(io_setup, unsigned, nr_events, aio_context_t __user *, ctxp)\n{\n\tstruct kioctx *ioctx = NULL;\n\tunsigned long ctx;\n\tlong ret;\n\n\tret = get_user(ctx, ctxp);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (unlikely(ctx || nr_events == 0)) {\n\t\tpr_debug(\"EINVAL: ctx %lu nr_events %u\\n\",\n\t\t         ctx, nr_events);\n\t\tgoto out;\n\t}\n\n\tioctx = ioctx_alloc(nr_events);\n\tret = PTR_ERR(ioctx);\n\tif (!IS_ERR(ioctx)) {\n\t\tret = put_user(ioctx->user_id, ctxp);\n\t\tif (ret)\n\t\t\tkill_ioctx(current->mm, ioctx, NULL);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\nout:\n\treturn ret;\n}\n\n/* sys_io_destroy:\n *\tDestroy the aio_context specified.  May cancel any outstanding \n *\tAIOs and block on completion.  Will fail with -ENOSYS if not\n *\timplemented.  May fail with -EINVAL if the context pointed to\n *\tis invalid.\n */\nSYSCALL_DEFINE1(io_destroy, aio_context_t, ctx)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx);\n\tif (likely(NULL != ioctx)) {\n\t\tstruct ctx_rq_wait wait;\n\t\tint ret;\n\n\t\tinit_completion(&wait.comp);\n\t\tatomic_set(&wait.count, 1);\n\n\t\t/* Pass requests_done to kill_ioctx() where it can be set\n\t\t * in a thread-safe way. If we try to set it here then we have\n\t\t * a race condition if two io_destroy() called simultaneously.\n\t\t */\n\t\tret = kill_ioctx(current->mm, ioctx, &wait);\n\t\tpercpu_ref_put(&ioctx->users);\n\n\t\t/* Wait until all IO for the context are done. Otherwise kernel\n\t\t * keep using user-space buffers even if user thinks the context\n\t\t * is destroyed.\n\t\t */\n\t\tif (!ret)\n\t\t\twait_for_completion(&wait.comp);\n\n\t\treturn ret;\n\t}\n\tpr_debug(\"EINVAL: invalid context id\\n\");\n\treturn -EINVAL;\n}\n\ntypedef ssize_t (rw_iter_op)(struct kiocb *, struct iov_iter *);\n\nstatic int aio_setup_vectored_rw(int rw, char __user *buf, size_t len,\n\t\t\t\t struct iovec **iovec,\n\t\t\t\t bool compat,\n\t\t\t\t struct iov_iter *iter)\n{\n#ifdef CONFIG_COMPAT\n\tif (compat)\n\t\treturn compat_import_iovec(rw,\n\t\t\t\t(struct compat_iovec __user *)buf,\n\t\t\t\tlen, UIO_FASTIOV, iovec, iter);\n#endif\n\treturn import_iovec(rw, (struct iovec __user *)buf,\n\t\t\t\tlen, UIO_FASTIOV, iovec, iter);\n}\n\n/*\n * aio_run_iocb:\n *\tPerforms the initial checks and io submission.\n */\nstatic ssize_t aio_run_iocb(struct kiocb *req, unsigned opcode,\n\t\t\t    char __user *buf, size_t len, bool compat)\n{\n\tstruct file *file = req->ki_filp;\n\tssize_t ret;\n\tint rw;\n\tfmode_t mode;\n\trw_iter_op *iter_op;\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\n\tswitch (opcode) {\n\tcase IOCB_CMD_PREAD:\n\tcase IOCB_CMD_PREADV:\n\t\tmode\t= FMODE_READ;\n\t\trw\t= READ;\n\t\titer_op\t= file->f_op->read_iter;\n\t\tgoto rw_common;\n\n\tcase IOCB_CMD_PWRITE:\n\tcase IOCB_CMD_PWRITEV:\n\t\tmode\t= FMODE_WRITE;\n\t\trw\t= WRITE;\n\t\titer_op\t= file->f_op->write_iter;\n\t\tgoto rw_common;\nrw_common:\n\t\tif (unlikely(!(file->f_mode & mode)))\n\t\t\treturn -EBADF;\n\n\t\tif (!iter_op)\n\t\t\treturn -EINVAL;\n\n\t\tif (opcode == IOCB_CMD_PREADV || opcode == IOCB_CMD_PWRITEV)\n\t\t\tret = aio_setup_vectored_rw(rw, buf, len,\n\t\t\t\t\t\t&iovec, compat, &iter);\n\t\telse {\n\t\t\tret = import_single_range(rw, buf, len, iovec, &iter);\n\t\t\tiovec = NULL;\n\t\t}\n\t\tif (!ret)\n\t\t\tret = rw_verify_area(rw, file, &req->ki_pos,\n\t\t\t\t\t     iov_iter_count(&iter));\n\t\tif (ret < 0) {\n\t\t\tkfree(iovec);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (rw == WRITE)\n\t\t\tfile_start_write(file);\n\n\t\tret = iter_op(req, &iter);\n\n\t\tif (rw == WRITE)\n\t\t\tfile_end_write(file);\n\t\tkfree(iovec);\n\t\tbreak;\n\n\tcase IOCB_CMD_FDSYNC:\n\t\tif (!file->f_op->aio_fsync)\n\t\t\treturn -EINVAL;\n\n\t\tret = file->f_op->aio_fsync(req, 1);\n\t\tbreak;\n\n\tcase IOCB_CMD_FSYNC:\n\t\tif (!file->f_op->aio_fsync)\n\t\t\treturn -EINVAL;\n\n\t\tret = file->f_op->aio_fsync(req, 0);\n\t\tbreak;\n\n\tdefault:\n\t\tpr_debug(\"EINVAL: no operation provided\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ret != -EIOCBQUEUED) {\n\t\t/*\n\t\t * There's no easy way to restart the syscall since other AIO's\n\t\t * may be already running. Just fail this IO with EINTR.\n\t\t */\n\t\tif (unlikely(ret == -ERESTARTSYS || ret == -ERESTARTNOINTR ||\n\t\t\t     ret == -ERESTARTNOHAND ||\n\t\t\t     ret == -ERESTART_RESTARTBLOCK))\n\t\t\tret = -EINTR;\n\t\taio_complete(req, ret, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb,\n\t\t\t struct iocb *iocb, bool compat)\n{\n\tstruct aio_kiocb *req;\n\tssize_t ret;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(iocb->aio_reserved1 || iocb->aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* prevent overflows */\n\tif (unlikely(\n\t    (iocb->aio_buf != (unsigned long)iocb->aio_buf) ||\n\t    (iocb->aio_nbytes != (size_t)iocb->aio_nbytes) ||\n\t    ((ssize_t)iocb->aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treq = aio_get_req(ctx);\n\tif (unlikely(!req))\n\t\treturn -EAGAIN;\n\n\treq->common.ki_filp = fget(iocb->aio_fildes);\n\tif (unlikely(!req->common.ki_filp)) {\n\t\tret = -EBADF;\n\t\tgoto out_put_req;\n\t}\n\treq->common.ki_pos = iocb->aio_offset;\n\treq->common.ki_complete = aio_complete;\n\treq->common.ki_flags = iocb_flags(req->common.ki_filp);\n\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\t/*\n\t\t * If the IOCB_FLAG_RESFD flag of aio_flags is set, get an\n\t\t * instance of the file* now. The file descriptor must be\n\t\t * an eventfd() fd, and will be signaled for each completed\n\t\t * event using the eventfd_signal() function.\n\t\t */\n\t\treq->ki_eventfd = eventfd_ctx_fdget((int) iocb->aio_resfd);\n\t\tif (IS_ERR(req->ki_eventfd)) {\n\t\t\tret = PTR_ERR(req->ki_eventfd);\n\t\t\treq->ki_eventfd = NULL;\n\t\t\tgoto out_put_req;\n\t\t}\n\n\t\treq->common.ki_flags |= IOCB_EVENTFD;\n\t}\n\n\tret = put_user(KIOCB_KEY, &user_iocb->aio_key);\n\tif (unlikely(ret)) {\n\t\tpr_debug(\"EFAULT: aio_key\\n\");\n\t\tgoto out_put_req;\n\t}\n\n\treq->ki_user_iocb = user_iocb;\n\treq->ki_user_data = iocb->aio_data;\n\n\tret = aio_run_iocb(&req->common, iocb->aio_lio_opcode,\n\t\t\t   (char __user *)(unsigned long)iocb->aio_buf,\n\t\t\t   iocb->aio_nbytes,\n\t\t\t   compat);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\treturn 0;\nout_put_req:\n\tput_reqs_available(ctx, 1);\n\tpercpu_ref_put(&ctx->reqs);\n\tkiocb_free(req);\n\treturn ret;\n}\n\nlong do_io_submit(aio_context_t ctx_id, long nr,\n\t\t  struct iocb __user *__user *iocbpp, bool compat)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(nr > LONG_MAX/sizeof(*iocbpp)))\n\t\tnr = LONG_MAX/sizeof(*iocbpp);\n\n\tif (unlikely(!access_ok(VERIFY_READ, iocbpp, (nr*sizeof(*iocbpp)))))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tblk_start_plug(&plug);\n\n\t/*\n\t * AKPM: should this return a partial result if some of the IOs were\n\t * successfully submitted?\n\t */\n\tfor (i=0; i<nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\t\tstruct iocb tmp;\n\n\t\tif (unlikely(__get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, &tmp, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tblk_finish_plug(&plug);\n\n\tpercpu_ref_put(&ctx->users);\n\treturn i ? i : ret;\n}\n\n/* sys_io_submit:\n *\tQueue the nr iocbs pointed to by iocbpp for processing.  Returns\n *\tthe number of iocbs queued.  May return -EINVAL if the aio_context\n *\tspecified by ctx_id is invalid, if nr is < 0, if the iocb at\n *\t*iocbpp[0] is not properly initialized, if the operation specified\n *\tis invalid for the file descriptor in the iocb.  May fail with\n *\t-EFAULT if any of the data structures point to invalid data.  May\n *\tfail with -EBADF if the file descriptor specified in the first\n *\tiocb is invalid.  May fail with -EAGAIN if insufficient resources\n *\tare available to queue any iocbs.  Will return 0 if nr is 0.  Will\n *\tfail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,\n\t\tstruct iocb __user * __user *, iocbpp)\n{\n\treturn do_io_submit(ctx_id, nr, iocbpp, 0);\n}\n\n/* lookup_kiocb\n *\tFinds a given iocb for cancellation.\n */\nstatic struct aio_kiocb *\nlookup_kiocb(struct kioctx *ctx, struct iocb __user *iocb, u32 key)\n{\n\tstruct aio_kiocb *kiocb;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tif (key != KIOCB_KEY)\n\t\treturn NULL;\n\n\t/* TODO: use a hash or array, this sucks. */\n\tlist_for_each_entry(kiocb, &ctx->active_reqs, ki_list) {\n\t\tif (kiocb->ki_user_iocb == iocb)\n\t\t\treturn kiocb;\n\t}\n\treturn NULL;\n}\n\n/* sys_io_cancel:\n *\tAttempts to cancel an iocb previously passed to io_submit.  If\n *\tthe operation is successfully cancelled, the resulting event is\n *\tcopied into the memory pointed to by result without being placed\n *\tinto the completion queue and 0 is returned.  May fail with\n *\t-EFAULT if any of the data structures pointed to are invalid.\n *\tMay fail with -EINVAL if aio_context specified by ctx_id is\n *\tinvalid.  May fail with -EAGAIN if the iocb specified was not\n *\tcancelled.  Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_cancel, aio_context_t, ctx_id, struct iocb __user *, iocb,\n\t\tstruct io_event __user *, result)\n{\n\tstruct kioctx *ctx;\n\tstruct aio_kiocb *kiocb;\n\tu32 key;\n\tint ret;\n\n\tret = get_user(key, &iocb->aio_key);\n\tif (unlikely(ret))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\tkiocb = lookup_kiocb(ctx, iocb, key);\n\tif (kiocb)\n\t\tret = kiocb_cancel(kiocb);\n\telse\n\t\tret = -EINVAL;\n\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tif (!ret) {\n\t\t/*\n\t\t * The result argument is no longer used - the io_event is\n\t\t * always delivered via the ring buffer. -EINPROGRESS indicates\n\t\t * cancellation is progress:\n\t\t */\n\t\tret = -EINPROGRESS;\n\t}\n\n\tpercpu_ref_put(&ctx->users);\n\n\treturn ret;\n}\n\n/* io_getevents:\n *\tAttempts to read at least min_nr events and up to nr events from\n *\tthe completion queue for the aio_context specified by ctx_id. If\n *\tit succeeds, the number of read events is returned. May fail with\n *\t-EINVAL if ctx_id is invalid, if min_nr is out of range, if nr is\n *\tout of range, if timeout is out of range.  May fail with -EFAULT\n *\tif any of the memory specified is invalid.  May return 0 or\n *\t< min_nr if the timeout specified by timeout has elapsed\n *\tbefore sufficient events are available, where timeout == NULL\n *\tspecifies an infinite timeout. Note that the timeout pointed to by\n *\ttimeout is relative.  Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct timespec __user *, timeout)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx_id);\n\tlong ret = -EINVAL;\n\n\tif (likely(ioctx)) {\n\t\tif (likely(min_nr <= nr && min_nr >= 0))\n\t\t\tret = read_events(ioctx, min_nr, nr, events, timeout);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\treturn ret;\n}\n"], "fixing_code": ["/*\n *\tAn async IO implementation for Linux\n *\tWritten by Benjamin LaHaise <bcrl@kvack.org>\n *\n *\tImplements an efficient asynchronous io interface.\n *\n *\tCopyright 2000, 2001, 2002 Red Hat, Inc.  All Rights Reserved.\n *\n *\tSee ../COPYING for licensing terms.\n */\n#define pr_fmt(fmt) \"%s: \" fmt, __func__\n\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/time.h>\n#include <linux/aio_abi.h>\n#include <linux/export.h>\n#include <linux/syscalls.h>\n#include <linux/backing-dev.h>\n#include <linux/uio.h>\n\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/mmu_context.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/aio.h>\n#include <linux/highmem.h>\n#include <linux/workqueue.h>\n#include <linux/security.h>\n#include <linux/eventfd.h>\n#include <linux/blkdev.h>\n#include <linux/compat.h>\n#include <linux/migrate.h>\n#include <linux/ramfs.h>\n#include <linux/percpu-refcount.h>\n#include <linux/mount.h>\n\n#include <asm/kmap_types.h>\n#include <asm/uaccess.h>\n\n#include \"internal.h\"\n\n#define AIO_RING_MAGIC\t\t\t0xa10a10a1\n#define AIO_RING_COMPAT_FEATURES\t1\n#define AIO_RING_INCOMPAT_FEATURES\t0\nstruct aio_ring {\n\tunsigned\tid;\t/* kernel internal index number */\n\tunsigned\tnr;\t/* number of io_events */\n\tunsigned\thead;\t/* Written to by userland or under ring_lock\n\t\t\t\t * mutex by aio_read_events_ring(). */\n\tunsigned\ttail;\n\n\tunsigned\tmagic;\n\tunsigned\tcompat_features;\n\tunsigned\tincompat_features;\n\tunsigned\theader_length;\t/* size of aio_ring */\n\n\n\tstruct io_event\t\tio_events[0];\n}; /* 128 bytes + ring size */\n\n#define AIO_RING_PAGES\t8\n\nstruct kioctx_table {\n\tstruct rcu_head\trcu;\n\tunsigned\tnr;\n\tstruct kioctx\t*table[];\n};\n\nstruct kioctx_cpu {\n\tunsigned\t\treqs_available;\n};\n\nstruct ctx_rq_wait {\n\tstruct completion comp;\n\tatomic_t count;\n};\n\nstruct kioctx {\n\tstruct percpu_ref\tusers;\n\tatomic_t\t\tdead;\n\n\tstruct percpu_ref\treqs;\n\n\tunsigned long\t\tuser_id;\n\n\tstruct __percpu kioctx_cpu *cpu;\n\n\t/*\n\t * For percpu reqs_available, number of slots we move to/from global\n\t * counter at a time:\n\t */\n\tunsigned\t\treq_batch;\n\t/*\n\t * This is what userspace passed to io_setup(), it's not used for\n\t * anything but counting against the global max_reqs quota.\n\t *\n\t * The real limit is nr_events - 1, which will be larger (see\n\t * aio_setup_ring())\n\t */\n\tunsigned\t\tmax_reqs;\n\n\t/* Size of ringbuffer, in units of struct io_event */\n\tunsigned\t\tnr_events;\n\n\tunsigned long\t\tmmap_base;\n\tunsigned long\t\tmmap_size;\n\n\tstruct page\t\t**ring_pages;\n\tlong\t\t\tnr_pages;\n\n\tstruct work_struct\tfree_work;\n\n\t/*\n\t * signals when all in-flight requests are done\n\t */\n\tstruct ctx_rq_wait\t*rq_wait;\n\n\tstruct {\n\t\t/*\n\t\t * This counts the number of available slots in the ringbuffer,\n\t\t * so we avoid overflowing it: it's decremented (if positive)\n\t\t * when allocating a kiocb and incremented when the resulting\n\t\t * io_event is pulled off the ringbuffer.\n\t\t *\n\t\t * We batch accesses to it with a percpu version.\n\t\t */\n\t\tatomic_t\treqs_available;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\tctx_lock;\n\t\tstruct list_head active_reqs;\t/* used for cancellation */\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\tring_lock;\n\t\twait_queue_head_t wait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned\ttail;\n\t\tunsigned\tcompleted_events;\n\t\tspinlock_t\tcompletion_lock;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct page\t\t*internal_pages[AIO_RING_PAGES];\n\tstruct file\t\t*aio_ring_file;\n\n\tunsigned\t\tid;\n};\n\n/*\n * We use ki_cancel == KIOCB_CANCELLED to indicate that a kiocb has been either\n * cancelled or completed (this makes a certain amount of sense because\n * successful cancellation - io_cancel() - does deliver the completion to\n * userspace).\n *\n * And since most things don't implement kiocb cancellation and we'd really like\n * kiocb completion to be lockless when possible, we use ki_cancel to\n * synchronize cancellation and completion - we only set it to KIOCB_CANCELLED\n * with xchg() or cmpxchg(), see batch_complete_aio() and kiocb_cancel().\n */\n#define KIOCB_CANCELLED\t\t((void *) (~0ULL))\n\nstruct aio_kiocb {\n\tstruct kiocb\t\tcommon;\n\n\tstruct kioctx\t\t*ki_ctx;\n\tkiocb_cancel_fn\t\t*ki_cancel;\n\n\tstruct iocb __user\t*ki_user_iocb;\t/* user's aiocb */\n\t__u64\t\t\tki_user_data;\t/* user's data for completion */\n\n\tstruct list_head\tki_list;\t/* the aio core uses this\n\t\t\t\t\t\t * for cancellation */\n\n\t/*\n\t * If the aio_resfd field of the userspace iocb is not zero,\n\t * this is the underlying eventfd context to deliver events to.\n\t */\n\tstruct eventfd_ctx\t*ki_eventfd;\n};\n\n/*------ sysctl variables----*/\nstatic DEFINE_SPINLOCK(aio_nr_lock);\nunsigned long aio_nr;\t\t/* current system wide number of aio requests */\nunsigned long aio_max_nr = 0x10000; /* system wide maximum number of aio requests */\n/*----end sysctl variables---*/\n\nstatic struct kmem_cache\t*kiocb_cachep;\nstatic struct kmem_cache\t*kioctx_cachep;\n\nstatic struct vfsmount *aio_mnt;\n\nstatic const struct file_operations aio_ring_fops;\nstatic const struct address_space_operations aio_ctx_aops;\n\nstatic struct file *aio_private_file(struct kioctx *ctx, loff_t nr_pages)\n{\n\tstruct qstr this = QSTR_INIT(\"[aio]\", 5);\n\tstruct file *file;\n\tstruct path path;\n\tstruct inode *inode = alloc_anon_inode(aio_mnt->mnt_sb);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\n\tinode->i_mapping->a_ops = &aio_ctx_aops;\n\tinode->i_mapping->private_data = ctx;\n\tinode->i_size = PAGE_SIZE * nr_pages;\n\n\tpath.dentry = d_alloc_pseudo(aio_mnt->mnt_sb, &this);\n\tif (!path.dentry) {\n\t\tiput(inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tpath.mnt = mntget(aio_mnt);\n\n\td_instantiate(path.dentry, inode);\n\tfile = alloc_file(&path, FMODE_READ | FMODE_WRITE, &aio_ring_fops);\n\tif (IS_ERR(file)) {\n\t\tpath_put(&path);\n\t\treturn file;\n\t}\n\n\tfile->f_flags = O_RDWR;\n\treturn file;\n}\n\nstatic struct dentry *aio_mount(struct file_system_type *fs_type,\n\t\t\t\tint flags, const char *dev_name, void *data)\n{\n\tstatic const struct dentry_operations ops = {\n\t\t.d_dname\t= simple_dname,\n\t};\n\tstruct dentry *root = mount_pseudo(fs_type, \"aio:\", NULL, &ops,\n\t\t\t\t\t   AIO_RING_MAGIC);\n\n\tif (!IS_ERR(root))\n\t\troot->d_sb->s_iflags |= SB_I_NOEXEC;\n\treturn root;\n}\n\n/* aio_setup\n *\tCreates the slab caches used by the aio routines, panic on\n *\tfailure as this is done early during the boot sequence.\n */\nstatic int __init aio_setup(void)\n{\n\tstatic struct file_system_type aio_fs = {\n\t\t.name\t\t= \"aio\",\n\t\t.mount\t\t= aio_mount,\n\t\t.kill_sb\t= kill_anon_super,\n\t};\n\taio_mnt = kern_mount(&aio_fs);\n\tif (IS_ERR(aio_mnt))\n\t\tpanic(\"Failed to create aio fs mount.\");\n\n\tkiocb_cachep = KMEM_CACHE(aio_kiocb, SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\tkioctx_cachep = KMEM_CACHE(kioctx,SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\n\tpr_debug(\"sizeof(struct page) = %zu\\n\", sizeof(struct page));\n\n\treturn 0;\n}\n__initcall(aio_setup);\n\nstatic void put_aio_ring_file(struct kioctx *ctx)\n{\n\tstruct file *aio_ring_file = ctx->aio_ring_file;\n\tif (aio_ring_file) {\n\t\ttruncate_setsize(aio_ring_file->f_inode, 0);\n\n\t\t/* Prevent further access to the kioctx from migratepages */\n\t\tspin_lock(&aio_ring_file->f_inode->i_mapping->private_lock);\n\t\taio_ring_file->f_inode->i_mapping->private_data = NULL;\n\t\tctx->aio_ring_file = NULL;\n\t\tspin_unlock(&aio_ring_file->f_inode->i_mapping->private_lock);\n\n\t\tfput(aio_ring_file);\n\t}\n}\n\nstatic void aio_free_ring(struct kioctx *ctx)\n{\n\tint i;\n\n\t/* Disconnect the kiotx from the ring file.  This prevents future\n\t * accesses to the kioctx from page migration.\n\t */\n\tput_aio_ring_file(ctx);\n\n\tfor (i = 0; i < ctx->nr_pages; i++) {\n\t\tstruct page *page;\n\t\tpr_debug(\"pid(%d) [%d] page->count=%d\\n\", current->pid, i,\n\t\t\t\tpage_count(ctx->ring_pages[i]));\n\t\tpage = ctx->ring_pages[i];\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tctx->ring_pages[i] = NULL;\n\t\tput_page(page);\n\t}\n\n\tif (ctx->ring_pages && ctx->ring_pages != ctx->internal_pages) {\n\t\tkfree(ctx->ring_pages);\n\t\tctx->ring_pages = NULL;\n\t}\n}\n\nstatic int aio_ring_mremap(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct kioctx_table *table;\n\tint i, res = -EINVAL;\n\n\tspin_lock(&mm->ioctx_lock);\n\trcu_read_lock();\n\ttable = rcu_dereference(mm->ioctx_table);\n\tfor (i = 0; i < table->nr; i++) {\n\t\tstruct kioctx *ctx;\n\n\t\tctx = table->table[i];\n\t\tif (ctx && ctx->aio_ring_file == file) {\n\t\t\tif (!atomic_read(&ctx->dead)) {\n\t\t\t\tctx->user_id = ctx->mmap_base = vma->vm_start;\n\t\t\t\tres = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\tspin_unlock(&mm->ioctx_lock);\n\treturn res;\n}\n\nstatic const struct vm_operations_struct aio_ring_vm_ops = {\n\t.mremap\t\t= aio_ring_mremap,\n#if IS_ENABLED(CONFIG_MMU)\n\t.fault\t\t= filemap_fault,\n\t.map_pages\t= filemap_map_pages,\n\t.page_mkwrite\t= filemap_page_mkwrite,\n#endif\n};\n\nstatic int aio_ring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvma->vm_flags |= VM_DONTEXPAND;\n\tvma->vm_ops = &aio_ring_vm_ops;\n\treturn 0;\n}\n\nstatic const struct file_operations aio_ring_fops = {\n\t.mmap = aio_ring_mmap,\n};\n\n#if IS_ENABLED(CONFIG_MIGRATION)\nstatic int aio_migratepage(struct address_space *mapping, struct page *new,\n\t\t\tstruct page *old, enum migrate_mode mode)\n{\n\tstruct kioctx *ctx;\n\tunsigned long flags;\n\tpgoff_t idx;\n\tint rc;\n\n\trc = 0;\n\n\t/* mapping->private_lock here protects against the kioctx teardown.  */\n\tspin_lock(&mapping->private_lock);\n\tctx = mapping->private_data;\n\tif (!ctx) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* The ring_lock mutex.  The prevents aio_read_events() from writing\n\t * to the ring's head, and prevents page migration from mucking in\n\t * a partially initialized kiotx.\n\t */\n\tif (!mutex_trylock(&ctx->ring_lock)) {\n\t\trc = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tidx = old->index;\n\tif (idx < (pgoff_t)ctx->nr_pages) {\n\t\t/* Make sure the old page hasn't already been changed */\n\t\tif (ctx->ring_pages[idx] != old)\n\t\t\trc = -EAGAIN;\n\t} else\n\t\trc = -EINVAL;\n\n\tif (rc != 0)\n\t\tgoto out_unlock;\n\n\t/* Writeback must be complete */\n\tBUG_ON(PageWriteback(old));\n\tget_page(new);\n\n\trc = migrate_page_move_mapping(mapping, new, old, NULL, mode, 1);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tput_page(new);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Take completion_lock to prevent other writes to the ring buffer\n\t * while the old page is copied to the new.  This prevents new\n\t * events from being lost.\n\t */\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tmigrate_page_copy(new, old);\n\tBUG_ON(ctx->ring_pages[idx] != old);\n\tctx->ring_pages[idx] = new;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\t/* The old page is no longer accessible. */\n\tput_page(old);\n\nout_unlock:\n\tmutex_unlock(&ctx->ring_lock);\nout:\n\tspin_unlock(&mapping->private_lock);\n\treturn rc;\n}\n#endif\n\nstatic const struct address_space_operations aio_ctx_aops = {\n\t.set_page_dirty = __set_page_dirty_no_writeback,\n#if IS_ENABLED(CONFIG_MIGRATION)\n\t.migratepage\t= aio_migratepage,\n#endif\n};\n\nstatic int aio_setup_ring(struct kioctx *ctx)\n{\n\tstruct aio_ring *ring;\n\tunsigned nr_events = ctx->max_reqs;\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long size, unused;\n\tint nr_pages;\n\tint i;\n\tstruct file *file;\n\n\t/* Compensate for the ring buffer's head/tail overlap entry */\n\tnr_events += 2;\t/* 1 is required, 2 for good luck */\n\n\tsize = sizeof(struct aio_ring);\n\tsize += sizeof(struct io_event) * nr_events;\n\n\tnr_pages = PFN_UP(size);\n\tif (nr_pages < 0)\n\t\treturn -EINVAL;\n\n\tfile = aio_private_file(ctx, nr_pages);\n\tif (IS_ERR(file)) {\n\t\tctx->aio_ring_file = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->aio_ring_file = file;\n\tnr_events = (PAGE_SIZE * nr_pages - sizeof(struct aio_ring))\n\t\t\t/ sizeof(struct io_event);\n\n\tctx->ring_pages = ctx->internal_pages;\n\tif (nr_pages > AIO_RING_PAGES) {\n\t\tctx->ring_pages = kcalloc(nr_pages, sizeof(struct page *),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!ctx->ring_pages) {\n\t\t\tput_aio_ring_file(ctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct page *page;\n\t\tpage = find_or_create_page(file->f_inode->i_mapping,\n\t\t\t\t\t   i, GFP_HIGHUSER | __GFP_ZERO);\n\t\tif (!page)\n\t\t\tbreak;\n\t\tpr_debug(\"pid(%d) page[%d]->count=%d\\n\",\n\t\t\t current->pid, i, page_count(page));\n\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\n\t\tctx->ring_pages[i] = page;\n\t}\n\tctx->nr_pages = i;\n\n\tif (unlikely(i != nr_pages)) {\n\t\taio_free_ring(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->mmap_size = nr_pages * PAGE_SIZE;\n\tpr_debug(\"attempting mmap of %lu bytes\\n\", ctx->mmap_size);\n\n\tif (down_write_killable(&mm->mmap_sem)) {\n\t\tctx->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -EINTR;\n\t}\n\n\tctx->mmap_base = do_mmap_pgoff(ctx->aio_ring_file, 0, ctx->mmap_size,\n\t\t\t\t       PROT_READ | PROT_WRITE,\n\t\t\t\t       MAP_SHARED, 0, &unused);\n\tup_write(&mm->mmap_sem);\n\tif (IS_ERR((void *)ctx->mmap_base)) {\n\t\tctx->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tpr_debug(\"mmap address: 0x%08lx\\n\", ctx->mmap_base);\n\n\tctx->user_id = ctx->mmap_base;\n\tctx->nr_events = nr_events; /* trusted copy */\n\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\tring->nr = nr_events;\t/* user copy */\n\tring->id = ~0U;\n\tring->head = ring->tail = 0;\n\tring->magic = AIO_RING_MAGIC;\n\tring->compat_features = AIO_RING_COMPAT_FEATURES;\n\tring->incompat_features = AIO_RING_INCOMPAT_FEATURES;\n\tring->header_length = sizeof(struct aio_ring);\n\tkunmap_atomic(ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\treturn 0;\n}\n\n#define AIO_EVENTS_PER_PAGE\t(PAGE_SIZE / sizeof(struct io_event))\n#define AIO_EVENTS_FIRST_PAGE\t((PAGE_SIZE - sizeof(struct aio_ring)) / sizeof(struct io_event))\n#define AIO_EVENTS_OFFSET\t(AIO_EVENTS_PER_PAGE - AIO_EVENTS_FIRST_PAGE)\n\nvoid kiocb_set_cancel_fn(struct kiocb *iocb, kiocb_cancel_fn *cancel)\n{\n\tstruct aio_kiocb *req = container_of(iocb, struct aio_kiocb, common);\n\tstruct kioctx *ctx = req->ki_ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\n\tif (!req->ki_list.next)\n\t\tlist_add(&req->ki_list, &ctx->active_reqs);\n\n\treq->ki_cancel = cancel;\n\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n}\nEXPORT_SYMBOL(kiocb_set_cancel_fn);\n\nstatic int kiocb_cancel(struct aio_kiocb *kiocb)\n{\n\tkiocb_cancel_fn *old, *cancel;\n\n\t/*\n\t * Don't want to set kiocb->ki_cancel = KIOCB_CANCELLED unless it\n\t * actually has a cancel function, hence the cmpxchg()\n\t */\n\n\tcancel = ACCESS_ONCE(kiocb->ki_cancel);\n\tdo {\n\t\tif (!cancel || cancel == KIOCB_CANCELLED)\n\t\t\treturn -EINVAL;\n\n\t\told = cancel;\n\t\tcancel = cmpxchg(&kiocb->ki_cancel, old, KIOCB_CANCELLED);\n\t} while (cancel != old);\n\n\treturn cancel(&kiocb->common);\n}\n\nstatic void free_ioctx(struct work_struct *work)\n{\n\tstruct kioctx *ctx = container_of(work, struct kioctx, free_work);\n\n\tpr_debug(\"freeing %p\\n\", ctx);\n\n\taio_free_ring(ctx);\n\tfree_percpu(ctx->cpu);\n\tpercpu_ref_exit(&ctx->reqs);\n\tpercpu_ref_exit(&ctx->users);\n\tkmem_cache_free(kioctx_cachep, ctx);\n}\n\nstatic void free_ioctx_reqs(struct percpu_ref *ref)\n{\n\tstruct kioctx *ctx = container_of(ref, struct kioctx, reqs);\n\n\t/* At this point we know that there are no any in-flight requests */\n\tif (ctx->rq_wait && atomic_dec_and_test(&ctx->rq_wait->count))\n\t\tcomplete(&ctx->rq_wait->comp);\n\n\tINIT_WORK(&ctx->free_work, free_ioctx);\n\tschedule_work(&ctx->free_work);\n}\n\n/*\n * When this function runs, the kioctx has been removed from the \"hash table\"\n * and ctx->users has dropped to 0, so we know no more kiocbs can be submitted -\n * now it's safe to cancel any that need to be.\n */\nstatic void free_ioctx_users(struct percpu_ref *ref)\n{\n\tstruct kioctx *ctx = container_of(ref, struct kioctx, users);\n\tstruct aio_kiocb *req;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\twhile (!list_empty(&ctx->active_reqs)) {\n\t\treq = list_first_entry(&ctx->active_reqs,\n\t\t\t\t       struct aio_kiocb, ki_list);\n\n\t\tlist_del_init(&req->ki_list);\n\t\tkiocb_cancel(req);\n\t}\n\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tpercpu_ref_kill(&ctx->reqs);\n\tpercpu_ref_put(&ctx->reqs);\n}\n\nstatic int ioctx_add_table(struct kioctx *ctx, struct mm_struct *mm)\n{\n\tunsigned i, new_nr;\n\tstruct kioctx_table *table, *old;\n\tstruct aio_ring *ring;\n\n\tspin_lock(&mm->ioctx_lock);\n\ttable = rcu_dereference_raw(mm->ioctx_table);\n\n\twhile (1) {\n\t\tif (table)\n\t\t\tfor (i = 0; i < table->nr; i++)\n\t\t\t\tif (!table->table[i]) {\n\t\t\t\t\tctx->id = i;\n\t\t\t\t\ttable->table[i] = ctx;\n\t\t\t\t\tspin_unlock(&mm->ioctx_lock);\n\n\t\t\t\t\t/* While kioctx setup is in progress,\n\t\t\t\t\t * we are protected from page migration\n\t\t\t\t\t * changes ring_pages by ->ring_lock.\n\t\t\t\t\t */\n\t\t\t\t\tring = kmap_atomic(ctx->ring_pages[0]);\n\t\t\t\t\tring->id = ctx->id;\n\t\t\t\t\tkunmap_atomic(ring);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\tnew_nr = (table ? table->nr : 1) * 4;\n\t\tspin_unlock(&mm->ioctx_lock);\n\n\t\ttable = kzalloc(sizeof(*table) + sizeof(struct kioctx *) *\n\t\t\t\tnew_nr, GFP_KERNEL);\n\t\tif (!table)\n\t\t\treturn -ENOMEM;\n\n\t\ttable->nr = new_nr;\n\n\t\tspin_lock(&mm->ioctx_lock);\n\t\told = rcu_dereference_raw(mm->ioctx_table);\n\n\t\tif (!old) {\n\t\t\trcu_assign_pointer(mm->ioctx_table, table);\n\t\t} else if (table->nr > old->nr) {\n\t\t\tmemcpy(table->table, old->table,\n\t\t\t       old->nr * sizeof(struct kioctx *));\n\n\t\t\trcu_assign_pointer(mm->ioctx_table, table);\n\t\t\tkfree_rcu(old, rcu);\n\t\t} else {\n\t\t\tkfree(table);\n\t\t\ttable = old;\n\t\t}\n\t}\n}\n\nstatic void aio_nr_sub(unsigned nr)\n{\n\tspin_lock(&aio_nr_lock);\n\tif (WARN_ON(aio_nr - nr > aio_nr))\n\t\taio_nr = 0;\n\telse\n\t\taio_nr -= nr;\n\tspin_unlock(&aio_nr_lock);\n}\n\n/* ioctx_alloc\n *\tAllocates and initializes an ioctx.  Returns an ERR_PTR if it failed.\n */\nstatic struct kioctx *ioctx_alloc(unsigned nr_events)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx;\n\tint err = -ENOMEM;\n\n\t/*\n\t * We keep track of the number of available ringbuffer slots, to prevent\n\t * overflow (reqs_available), and we also use percpu counters for this.\n\t *\n\t * So since up to half the slots might be on other cpu's percpu counters\n\t * and unavailable, double nr_events so userspace sees what they\n\t * expected: additionally, we move req_batch slots to/from percpu\n\t * counters at a time, so make sure that isn't 0:\n\t */\n\tnr_events = max(nr_events, num_possible_cpus() * 4);\n\tnr_events *= 2;\n\n\t/* Prevent overflows */\n\tif (nr_events > (0x10000000U / sizeof(struct io_event))) {\n\t\tpr_debug(\"ENOMEM: nr_events too high\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_events || (unsigned long)nr_events > (aio_max_nr * 2UL))\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tctx = kmem_cache_zalloc(kioctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx->max_reqs = nr_events;\n\n\tspin_lock_init(&ctx->ctx_lock);\n\tspin_lock_init(&ctx->completion_lock);\n\tmutex_init(&ctx->ring_lock);\n\t/* Protect against page migration throughout kiotx setup by keeping\n\t * the ring_lock mutex held until setup is complete. */\n\tmutex_lock(&ctx->ring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\n\tINIT_LIST_HEAD(&ctx->active_reqs);\n\n\tif (percpu_ref_init(&ctx->users, free_ioctx_users, 0, GFP_KERNEL))\n\t\tgoto err;\n\n\tif (percpu_ref_init(&ctx->reqs, free_ioctx_reqs, 0, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->cpu = alloc_percpu(struct kioctx_cpu);\n\tif (!ctx->cpu)\n\t\tgoto err;\n\n\terr = aio_setup_ring(ctx);\n\tif (err < 0)\n\t\tgoto err;\n\n\tatomic_set(&ctx->reqs_available, ctx->nr_events - 1);\n\tctx->req_batch = (ctx->nr_events - 1) / (num_possible_cpus() * 4);\n\tif (ctx->req_batch < 1)\n\t\tctx->req_batch = 1;\n\n\t/* limit the number of system wide aios */\n\tspin_lock(&aio_nr_lock);\n\tif (aio_nr + nr_events > (aio_max_nr * 2UL) ||\n\t    aio_nr + nr_events < aio_nr) {\n\t\tspin_unlock(&aio_nr_lock);\n\t\terr = -EAGAIN;\n\t\tgoto err_ctx;\n\t}\n\taio_nr += ctx->max_reqs;\n\tspin_unlock(&aio_nr_lock);\n\n\tpercpu_ref_get(&ctx->users);\t/* io_setup() will drop this ref */\n\tpercpu_ref_get(&ctx->reqs);\t/* free_ioctx_users() will drop this */\n\n\terr = ioctx_add_table(ctx, mm);\n\tif (err)\n\t\tgoto err_cleanup;\n\n\t/* Release the ring_lock mutex now that all setup is complete. */\n\tmutex_unlock(&ctx->ring_lock);\n\n\tpr_debug(\"allocated ioctx %p[%ld]: mm=%p mask=0x%x\\n\",\n\t\t ctx, ctx->user_id, mm, ctx->nr_events);\n\treturn ctx;\n\nerr_cleanup:\n\taio_nr_sub(ctx->max_reqs);\nerr_ctx:\n\tatomic_set(&ctx->dead, 1);\n\tif (ctx->mmap_size)\n\t\tvm_munmap(ctx->mmap_base, ctx->mmap_size);\n\taio_free_ring(ctx);\nerr:\n\tmutex_unlock(&ctx->ring_lock);\n\tfree_percpu(ctx->cpu);\n\tpercpu_ref_exit(&ctx->reqs);\n\tpercpu_ref_exit(&ctx->users);\n\tkmem_cache_free(kioctx_cachep, ctx);\n\tpr_debug(\"error allocating ioctx %d\\n\", err);\n\treturn ERR_PTR(err);\n}\n\n/* kill_ioctx\n *\tCancels all outstanding aio requests on an aio context.  Used\n *\twhen the processes owning a context have all exited to encourage\n *\tthe rapid destruction of the kioctx.\n */\nstatic int kill_ioctx(struct mm_struct *mm, struct kioctx *ctx,\n\t\t      struct ctx_rq_wait *wait)\n{\n\tstruct kioctx_table *table;\n\n\tspin_lock(&mm->ioctx_lock);\n\tif (atomic_xchg(&ctx->dead, 1)) {\n\t\tspin_unlock(&mm->ioctx_lock);\n\t\treturn -EINVAL;\n\t}\n\n\ttable = rcu_dereference_raw(mm->ioctx_table);\n\tWARN_ON(ctx != table->table[ctx->id]);\n\ttable->table[ctx->id] = NULL;\n\tspin_unlock(&mm->ioctx_lock);\n\n\t/* percpu_ref_kill() will do the necessary call_rcu() */\n\twake_up_all(&ctx->wait);\n\n\t/*\n\t * It'd be more correct to do this in free_ioctx(), after all\n\t * the outstanding kiocbs have finished - but by then io_destroy\n\t * has already returned, so io_setup() could potentially return\n\t * -EAGAIN with no ioctxs actually in use (as far as userspace\n\t *  could tell).\n\t */\n\taio_nr_sub(ctx->max_reqs);\n\n\tif (ctx->mmap_size)\n\t\tvm_munmap(ctx->mmap_base, ctx->mmap_size);\n\n\tctx->rq_wait = wait;\n\tpercpu_ref_kill(&ctx->users);\n\treturn 0;\n}\n\n/*\n * exit_aio: called when the last user of mm goes away.  At this point, there is\n * no way for any new requests to be submited or any of the io_* syscalls to be\n * called on the context.\n *\n * There may be outstanding kiocbs, but free_ioctx() will explicitly wait on\n * them.\n */\nvoid exit_aio(struct mm_struct *mm)\n{\n\tstruct kioctx_table *table = rcu_dereference_raw(mm->ioctx_table);\n\tstruct ctx_rq_wait wait;\n\tint i, skipped;\n\n\tif (!table)\n\t\treturn;\n\n\tatomic_set(&wait.count, table->nr);\n\tinit_completion(&wait.comp);\n\n\tskipped = 0;\n\tfor (i = 0; i < table->nr; ++i) {\n\t\tstruct kioctx *ctx = table->table[i];\n\n\t\tif (!ctx) {\n\t\t\tskipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * We don't need to bother with munmap() here - exit_mmap(mm)\n\t\t * is coming and it'll unmap everything. And we simply can't,\n\t\t * this is not necessarily our ->mm.\n\t\t * Since kill_ioctx() uses non-zero ->mmap_size as indicator\n\t\t * that it needs to unmap the area, just set it to 0.\n\t\t */\n\t\tctx->mmap_size = 0;\n\t\tkill_ioctx(mm, ctx, &wait);\n\t}\n\n\tif (!atomic_sub_and_test(skipped, &wait.count)) {\n\t\t/* Wait until all IO for the context are done. */\n\t\twait_for_completion(&wait.comp);\n\t}\n\n\tRCU_INIT_POINTER(mm->ioctx_table, NULL);\n\tkfree(table);\n}\n\nstatic void put_reqs_available(struct kioctx *ctx, unsigned nr)\n{\n\tstruct kioctx_cpu *kcpu;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkcpu = this_cpu_ptr(ctx->cpu);\n\tkcpu->reqs_available += nr;\n\n\twhile (kcpu->reqs_available >= ctx->req_batch * 2) {\n\t\tkcpu->reqs_available -= ctx->req_batch;\n\t\tatomic_add(ctx->req_batch, &ctx->reqs_available);\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nstatic bool get_reqs_available(struct kioctx *ctx)\n{\n\tstruct kioctx_cpu *kcpu;\n\tbool ret = false;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkcpu = this_cpu_ptr(ctx->cpu);\n\tif (!kcpu->reqs_available) {\n\t\tint old, avail = atomic_read(&ctx->reqs_available);\n\n\t\tdo {\n\t\t\tif (avail < ctx->req_batch)\n\t\t\t\tgoto out;\n\n\t\t\told = avail;\n\t\t\tavail = atomic_cmpxchg(&ctx->reqs_available,\n\t\t\t\t\t       avail, avail - ctx->req_batch);\n\t\t} while (avail != old);\n\n\t\tkcpu->reqs_available += ctx->req_batch;\n\t}\n\n\tret = true;\n\tkcpu->reqs_available--;\nout:\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/* refill_reqs_available\n *\tUpdates the reqs_available reference counts used for tracking the\n *\tnumber of free slots in the completion ring.  This can be called\n *\tfrom aio_complete() (to optimistically update reqs_available) or\n *\tfrom aio_get_req() (the we're out of events case).  It must be\n *\tcalled holding ctx->completion_lock.\n */\nstatic void refill_reqs_available(struct kioctx *ctx, unsigned head,\n                                  unsigned tail)\n{\n\tunsigned events_in_ring, completed;\n\n\t/* Clamp head since userland can write to it. */\n\thead %= ctx->nr_events;\n\tif (head <= tail)\n\t\tevents_in_ring = tail - head;\n\telse\n\t\tevents_in_ring = ctx->nr_events - (head - tail);\n\n\tcompleted = ctx->completed_events;\n\tif (events_in_ring < completed)\n\t\tcompleted -= events_in_ring;\n\telse\n\t\tcompleted = 0;\n\n\tif (!completed)\n\t\treturn;\n\n\tctx->completed_events -= completed;\n\tput_reqs_available(ctx, completed);\n}\n\n/* user_refill_reqs_available\n *\tCalled to refill reqs_available when aio_get_req() encounters an\n *\tout of space in the completion ring.\n */\nstatic void user_refill_reqs_available(struct kioctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (ctx->completed_events) {\n\t\tstruct aio_ring *ring;\n\t\tunsigned head;\n\n\t\t/* Access of ring->head may race with aio_read_events_ring()\n\t\t * here, but that's okay since whether we read the old version\n\t\t * or the new version, and either will be valid.  The important\n\t\t * part is that head cannot pass tail since we prevent\n\t\t * aio_complete() from updating tail by holding\n\t\t * ctx->completion_lock.  Even if head is invalid, the check\n\t\t * against ctx->completed_events below will make sure we do the\n\t\t * safe/right thing.\n\t\t */\n\t\tring = kmap_atomic(ctx->ring_pages[0]);\n\t\thead = ring->head;\n\t\tkunmap_atomic(ring);\n\n\t\trefill_reqs_available(ctx, head, ctx->tail);\n\t}\n\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\n/* aio_get_req\n *\tAllocate a slot for an aio request.\n * Returns NULL if no requests are free.\n */\nstatic inline struct aio_kiocb *aio_get_req(struct kioctx *ctx)\n{\n\tstruct aio_kiocb *req;\n\n\tif (!get_reqs_available(ctx)) {\n\t\tuser_refill_reqs_available(ctx);\n\t\tif (!get_reqs_available(ctx))\n\t\t\treturn NULL;\n\t}\n\n\treq = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL|__GFP_ZERO);\n\tif (unlikely(!req))\n\t\tgoto out_put;\n\n\tpercpu_ref_get(&ctx->reqs);\n\n\treq->ki_ctx = ctx;\n\treturn req;\nout_put:\n\tput_reqs_available(ctx, 1);\n\treturn NULL;\n}\n\nstatic void kiocb_free(struct aio_kiocb *req)\n{\n\tif (req->common.ki_filp)\n\t\tfput(req->common.ki_filp);\n\tif (req->ki_eventfd != NULL)\n\t\teventfd_ctx_put(req->ki_eventfd);\n\tkmem_cache_free(kiocb_cachep, req);\n}\n\nstatic struct kioctx *lookup_ioctx(unsigned long ctx_id)\n{\n\tstruct aio_ring __user *ring  = (void __user *)ctx_id;\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx, *ret = NULL;\n\tstruct kioctx_table *table;\n\tunsigned id;\n\n\tif (get_user(id, &ring->id))\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttable = rcu_dereference(mm->ioctx_table);\n\n\tif (!table || id >= table->nr)\n\t\tgoto out;\n\n\tctx = table->table[id];\n\tif (ctx && ctx->user_id == ctx_id) {\n\t\tpercpu_ref_get(&ctx->users);\n\t\tret = ctx;\n\t}\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/* aio_complete\n *\tCalled when the io request on the given iocb is complete.\n */\nstatic void aio_complete(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, common);\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tstruct aio_ring\t*ring;\n\tstruct io_event\t*ev_page, *event;\n\tunsigned tail, pos, head;\n\tunsigned long\tflags;\n\n\t/*\n\t * Special case handling for sync iocbs:\n\t *  - events go directly into the iocb for fast handling\n\t *  - the sync task with the iocb in its stack holds the single iocb\n\t *    ref, no other paths have a way to get another ref\n\t *  - the sync task helpfully left a reference to itself in the iocb\n\t */\n\tBUG_ON(is_sync_kiocb(kiocb));\n\n\tif (iocb->ki_list.next) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\t\tlist_del(&iocb->ki_list);\n\t\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\t}\n\n\t/*\n\t * Add a completion event to the ring buffer. Must be done holding\n\t * ctx->completion_lock to prevent other code from messing with the tail\n\t * pointer since we might be called from irq context.\n\t */\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\n\ttail = ctx->tail;\n\tpos = tail + AIO_EVENTS_OFFSET;\n\n\tif (++tail >= ctx->nr_events)\n\t\ttail = 0;\n\n\tev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);\n\tevent = ev_page + pos % AIO_EVENTS_PER_PAGE;\n\n\tevent->obj = (u64)(unsigned long)iocb->ki_user_iocb;\n\tevent->data = iocb->ki_user_data;\n\tevent->res = res;\n\tevent->res2 = res2;\n\n\tkunmap_atomic(ev_page);\n\tflush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);\n\n\tpr_debug(\"%p[%u]: %p: %p %Lx %lx %lx\\n\",\n\t\t ctx, tail, iocb, iocb->ki_user_iocb, iocb->ki_user_data,\n\t\t res, res2);\n\n\t/* after flagging the request as done, we\n\t * must never even look at it again\n\t */\n\tsmp_wmb();\t/* make event visible before updating tail */\n\n\tctx->tail = tail;\n\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\thead = ring->head;\n\tring->tail = tail;\n\tkunmap_atomic(ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\tctx->completed_events++;\n\tif (ctx->completed_events > 1)\n\t\trefill_reqs_available(ctx, head, tail);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tpr_debug(\"added to ring %p at [%u]\\n\", iocb, tail);\n\n\t/*\n\t * Check if the user asked us to deliver the result through an\n\t * eventfd. The eventfd_signal() function is safe to be called\n\t * from IRQ context.\n\t */\n\tif (iocb->ki_eventfd != NULL)\n\t\teventfd_signal(iocb->ki_eventfd, 1);\n\n\t/* everything turned out well, dispose of the aiocb. */\n\tkiocb_free(iocb);\n\n\t/*\n\t * We have to order our ring_info tail store above and test\n\t * of the wait list below outside the wait lock.  This is\n\t * like in wake_up_bit() where clearing a bit has to be\n\t * ordered with the unlocked test.\n\t */\n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\n\tpercpu_ref_put(&ctx->reqs);\n}\n\n/* aio_read_events_ring\n *\tPull an event off of the ioctx's event ring.  Returns the number of\n *\tevents fetched\n */\nstatic long aio_read_events_ring(struct kioctx *ctx,\n\t\t\t\t struct io_event __user *event, long nr)\n{\n\tstruct aio_ring *ring;\n\tunsigned head, tail, pos;\n\tlong ret = 0;\n\tint copy_ret;\n\n\t/*\n\t * The mutex can block and wake us up and that will cause\n\t * wait_event_interruptible_hrtimeout() to schedule without sleeping\n\t * and repeat. This should be rare enough that it doesn't cause\n\t * peformance issues. See the comment in read_events() for more detail.\n\t */\n\tsched_annotate_sleep();\n\tmutex_lock(&ctx->ring_lock);\n\n\t/* Access to ->ring_pages here is protected by ctx->ring_lock. */\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\thead = ring->head;\n\ttail = ring->tail;\n\tkunmap_atomic(ring);\n\n\t/*\n\t * Ensure that once we've read the current tail pointer, that\n\t * we also see the events that were stored up to the tail.\n\t */\n\tsmp_rmb();\n\n\tpr_debug(\"h%u t%u m%u\\n\", head, tail, ctx->nr_events);\n\n\tif (head == tail)\n\t\tgoto out;\n\n\thead %= ctx->nr_events;\n\ttail %= ctx->nr_events;\n\n\twhile (ret < nr) {\n\t\tlong avail;\n\t\tstruct io_event *ev;\n\t\tstruct page *page;\n\n\t\tavail = (head <= tail ?  tail : ctx->nr_events) - head;\n\t\tif (head == tail)\n\t\t\tbreak;\n\n\t\tavail = min(avail, nr - ret);\n\t\tavail = min_t(long, avail, AIO_EVENTS_PER_PAGE -\n\t\t\t    ((head + AIO_EVENTS_OFFSET) % AIO_EVENTS_PER_PAGE));\n\n\t\tpos = head + AIO_EVENTS_OFFSET;\n\t\tpage = ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE];\n\t\tpos %= AIO_EVENTS_PER_PAGE;\n\n\t\tev = kmap(page);\n\t\tcopy_ret = copy_to_user(event + ret, ev + pos,\n\t\t\t\t\tsizeof(*ev) * avail);\n\t\tkunmap(page);\n\n\t\tif (unlikely(copy_ret)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret += avail;\n\t\thead += avail;\n\t\thead %= ctx->nr_events;\n\t}\n\n\tring = kmap_atomic(ctx->ring_pages[0]);\n\tring->head = head;\n\tkunmap_atomic(ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\tpr_debug(\"%li  h%u t%u\\n\", ret, head, tail);\nout:\n\tmutex_unlock(&ctx->ring_lock);\n\n\treturn ret;\n}\n\nstatic bool aio_read_events(struct kioctx *ctx, long min_nr, long nr,\n\t\t\t    struct io_event __user *event, long *i)\n{\n\tlong ret = aio_read_events_ring(ctx, event + *i, nr - *i);\n\n\tif (ret > 0)\n\t\t*i += ret;\n\n\tif (unlikely(atomic_read(&ctx->dead)))\n\t\tret = -EINVAL;\n\n\tif (!*i)\n\t\t*i = ret;\n\n\treturn ret < 0 || *i >= min_nr;\n}\n\nstatic long read_events(struct kioctx *ctx, long min_nr, long nr,\n\t\t\tstruct io_event __user *event,\n\t\t\tstruct timespec __user *timeout)\n{\n\tktime_t until = { .tv64 = KTIME_MAX };\n\tlong ret = 0;\n\n\tif (timeout) {\n\t\tstruct timespec\tts;\n\n\t\tif (unlikely(copy_from_user(&ts, timeout, sizeof(ts))))\n\t\t\treturn -EFAULT;\n\n\t\tuntil = timespec_to_ktime(ts);\n\t}\n\n\t/*\n\t * Note that aio_read_events() is being called as the conditional - i.e.\n\t * we're calling it after prepare_to_wait() has set task state to\n\t * TASK_INTERRUPTIBLE.\n\t *\n\t * But aio_read_events() can block, and if it blocks it's going to flip\n\t * the task state back to TASK_RUNNING.\n\t *\n\t * This should be ok, provided it doesn't flip the state back to\n\t * TASK_RUNNING and return 0 too much - that causes us to spin. That\n\t * will only happen if the mutex_lock() call blocks, and we then find\n\t * the ringbuffer empty. So in practice we should be ok, but it's\n\t * something to be aware of when touching this code.\n\t */\n\tif (until.tv64 == 0)\n\t\taio_read_events(ctx, min_nr, nr, event, &ret);\n\telse\n\t\twait_event_interruptible_hrtimeout(ctx->wait,\n\t\t\t\taio_read_events(ctx, min_nr, nr, event, &ret),\n\t\t\t\tuntil);\n\n\tif (!ret && signal_pending(current))\n\t\tret = -EINTR;\n\n\treturn ret;\n}\n\n/* sys_io_setup:\n *\tCreate an aio_context capable of receiving at least nr_events.\n *\tctxp must not point to an aio_context that already exists, and\n *\tmust be initialized to 0 prior to the call.  On successful\n *\tcreation of the aio_context, *ctxp is filled in with the resulting \n *\thandle.  May fail with -EINVAL if *ctxp is not initialized,\n *\tif the specified nr_events exceeds internal limits.  May fail \n *\twith -EAGAIN if the specified nr_events exceeds the user's limit \n *\tof available events.  May fail with -ENOMEM if insufficient kernel\n *\tresources are available.  May fail with -EFAULT if an invalid\n *\tpointer is passed for ctxp.  Will fail with -ENOSYS if not\n *\timplemented.\n */\nSYSCALL_DEFINE2(io_setup, unsigned, nr_events, aio_context_t __user *, ctxp)\n{\n\tstruct kioctx *ioctx = NULL;\n\tunsigned long ctx;\n\tlong ret;\n\n\tret = get_user(ctx, ctxp);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (unlikely(ctx || nr_events == 0)) {\n\t\tpr_debug(\"EINVAL: ctx %lu nr_events %u\\n\",\n\t\t         ctx, nr_events);\n\t\tgoto out;\n\t}\n\n\tioctx = ioctx_alloc(nr_events);\n\tret = PTR_ERR(ioctx);\n\tif (!IS_ERR(ioctx)) {\n\t\tret = put_user(ioctx->user_id, ctxp);\n\t\tif (ret)\n\t\t\tkill_ioctx(current->mm, ioctx, NULL);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\nout:\n\treturn ret;\n}\n\n/* sys_io_destroy:\n *\tDestroy the aio_context specified.  May cancel any outstanding \n *\tAIOs and block on completion.  Will fail with -ENOSYS if not\n *\timplemented.  May fail with -EINVAL if the context pointed to\n *\tis invalid.\n */\nSYSCALL_DEFINE1(io_destroy, aio_context_t, ctx)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx);\n\tif (likely(NULL != ioctx)) {\n\t\tstruct ctx_rq_wait wait;\n\t\tint ret;\n\n\t\tinit_completion(&wait.comp);\n\t\tatomic_set(&wait.count, 1);\n\n\t\t/* Pass requests_done to kill_ioctx() where it can be set\n\t\t * in a thread-safe way. If we try to set it here then we have\n\t\t * a race condition if two io_destroy() called simultaneously.\n\t\t */\n\t\tret = kill_ioctx(current->mm, ioctx, &wait);\n\t\tpercpu_ref_put(&ioctx->users);\n\n\t\t/* Wait until all IO for the context are done. Otherwise kernel\n\t\t * keep using user-space buffers even if user thinks the context\n\t\t * is destroyed.\n\t\t */\n\t\tif (!ret)\n\t\t\twait_for_completion(&wait.comp);\n\n\t\treturn ret;\n\t}\n\tpr_debug(\"EINVAL: invalid context id\\n\");\n\treturn -EINVAL;\n}\n\ntypedef ssize_t (rw_iter_op)(struct kiocb *, struct iov_iter *);\n\nstatic int aio_setup_vectored_rw(int rw, char __user *buf, size_t len,\n\t\t\t\t struct iovec **iovec,\n\t\t\t\t bool compat,\n\t\t\t\t struct iov_iter *iter)\n{\n#ifdef CONFIG_COMPAT\n\tif (compat)\n\t\treturn compat_import_iovec(rw,\n\t\t\t\t(struct compat_iovec __user *)buf,\n\t\t\t\tlen, UIO_FASTIOV, iovec, iter);\n#endif\n\treturn import_iovec(rw, (struct iovec __user *)buf,\n\t\t\t\tlen, UIO_FASTIOV, iovec, iter);\n}\n\n/*\n * aio_run_iocb:\n *\tPerforms the initial checks and io submission.\n */\nstatic ssize_t aio_run_iocb(struct kiocb *req, unsigned opcode,\n\t\t\t    char __user *buf, size_t len, bool compat)\n{\n\tstruct file *file = req->ki_filp;\n\tssize_t ret;\n\tint rw;\n\tfmode_t mode;\n\trw_iter_op *iter_op;\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\n\tswitch (opcode) {\n\tcase IOCB_CMD_PREAD:\n\tcase IOCB_CMD_PREADV:\n\t\tmode\t= FMODE_READ;\n\t\trw\t= READ;\n\t\titer_op\t= file->f_op->read_iter;\n\t\tgoto rw_common;\n\n\tcase IOCB_CMD_PWRITE:\n\tcase IOCB_CMD_PWRITEV:\n\t\tmode\t= FMODE_WRITE;\n\t\trw\t= WRITE;\n\t\titer_op\t= file->f_op->write_iter;\n\t\tgoto rw_common;\nrw_common:\n\t\tif (unlikely(!(file->f_mode & mode)))\n\t\t\treturn -EBADF;\n\n\t\tif (!iter_op)\n\t\t\treturn -EINVAL;\n\n\t\tif (opcode == IOCB_CMD_PREADV || opcode == IOCB_CMD_PWRITEV)\n\t\t\tret = aio_setup_vectored_rw(rw, buf, len,\n\t\t\t\t\t\t&iovec, compat, &iter);\n\t\telse {\n\t\t\tret = import_single_range(rw, buf, len, iovec, &iter);\n\t\t\tiovec = NULL;\n\t\t}\n\t\tif (!ret)\n\t\t\tret = rw_verify_area(rw, file, &req->ki_pos,\n\t\t\t\t\t     iov_iter_count(&iter));\n\t\tif (ret < 0) {\n\t\t\tkfree(iovec);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (rw == WRITE)\n\t\t\tfile_start_write(file);\n\n\t\tret = iter_op(req, &iter);\n\n\t\tif (rw == WRITE)\n\t\t\tfile_end_write(file);\n\t\tkfree(iovec);\n\t\tbreak;\n\n\tcase IOCB_CMD_FDSYNC:\n\t\tif (!file->f_op->aio_fsync)\n\t\t\treturn -EINVAL;\n\n\t\tret = file->f_op->aio_fsync(req, 1);\n\t\tbreak;\n\n\tcase IOCB_CMD_FSYNC:\n\t\tif (!file->f_op->aio_fsync)\n\t\t\treturn -EINVAL;\n\n\t\tret = file->f_op->aio_fsync(req, 0);\n\t\tbreak;\n\n\tdefault:\n\t\tpr_debug(\"EINVAL: no operation provided\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ret != -EIOCBQUEUED) {\n\t\t/*\n\t\t * There's no easy way to restart the syscall since other AIO's\n\t\t * may be already running. Just fail this IO with EINTR.\n\t\t */\n\t\tif (unlikely(ret == -ERESTARTSYS || ret == -ERESTARTNOINTR ||\n\t\t\t     ret == -ERESTARTNOHAND ||\n\t\t\t     ret == -ERESTART_RESTARTBLOCK))\n\t\t\tret = -EINTR;\n\t\taio_complete(req, ret, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb,\n\t\t\t struct iocb *iocb, bool compat)\n{\n\tstruct aio_kiocb *req;\n\tssize_t ret;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(iocb->aio_reserved1 || iocb->aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* prevent overflows */\n\tif (unlikely(\n\t    (iocb->aio_buf != (unsigned long)iocb->aio_buf) ||\n\t    (iocb->aio_nbytes != (size_t)iocb->aio_nbytes) ||\n\t    ((ssize_t)iocb->aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treq = aio_get_req(ctx);\n\tif (unlikely(!req))\n\t\treturn -EAGAIN;\n\n\treq->common.ki_filp = fget(iocb->aio_fildes);\n\tif (unlikely(!req->common.ki_filp)) {\n\t\tret = -EBADF;\n\t\tgoto out_put_req;\n\t}\n\treq->common.ki_pos = iocb->aio_offset;\n\treq->common.ki_complete = aio_complete;\n\treq->common.ki_flags = iocb_flags(req->common.ki_filp);\n\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\t/*\n\t\t * If the IOCB_FLAG_RESFD flag of aio_flags is set, get an\n\t\t * instance of the file* now. The file descriptor must be\n\t\t * an eventfd() fd, and will be signaled for each completed\n\t\t * event using the eventfd_signal() function.\n\t\t */\n\t\treq->ki_eventfd = eventfd_ctx_fdget((int) iocb->aio_resfd);\n\t\tif (IS_ERR(req->ki_eventfd)) {\n\t\t\tret = PTR_ERR(req->ki_eventfd);\n\t\t\treq->ki_eventfd = NULL;\n\t\t\tgoto out_put_req;\n\t\t}\n\n\t\treq->common.ki_flags |= IOCB_EVENTFD;\n\t}\n\n\tret = put_user(KIOCB_KEY, &user_iocb->aio_key);\n\tif (unlikely(ret)) {\n\t\tpr_debug(\"EFAULT: aio_key\\n\");\n\t\tgoto out_put_req;\n\t}\n\n\treq->ki_user_iocb = user_iocb;\n\treq->ki_user_data = iocb->aio_data;\n\n\tret = aio_run_iocb(&req->common, iocb->aio_lio_opcode,\n\t\t\t   (char __user *)(unsigned long)iocb->aio_buf,\n\t\t\t   iocb->aio_nbytes,\n\t\t\t   compat);\n\tif (ret)\n\t\tgoto out_put_req;\n\n\treturn 0;\nout_put_req:\n\tput_reqs_available(ctx, 1);\n\tpercpu_ref_put(&ctx->reqs);\n\tkiocb_free(req);\n\treturn ret;\n}\n\nlong do_io_submit(aio_context_t ctx_id, long nr,\n\t\t  struct iocb __user *__user *iocbpp, bool compat)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(nr > LONG_MAX/sizeof(*iocbpp)))\n\t\tnr = LONG_MAX/sizeof(*iocbpp);\n\n\tif (unlikely(!access_ok(VERIFY_READ, iocbpp, (nr*sizeof(*iocbpp)))))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tblk_start_plug(&plug);\n\n\t/*\n\t * AKPM: should this return a partial result if some of the IOs were\n\t * successfully submitted?\n\t */\n\tfor (i=0; i<nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\t\tstruct iocb tmp;\n\n\t\tif (unlikely(__get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, &tmp, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tblk_finish_plug(&plug);\n\n\tpercpu_ref_put(&ctx->users);\n\treturn i ? i : ret;\n}\n\n/* sys_io_submit:\n *\tQueue the nr iocbs pointed to by iocbpp for processing.  Returns\n *\tthe number of iocbs queued.  May return -EINVAL if the aio_context\n *\tspecified by ctx_id is invalid, if nr is < 0, if the iocb at\n *\t*iocbpp[0] is not properly initialized, if the operation specified\n *\tis invalid for the file descriptor in the iocb.  May fail with\n *\t-EFAULT if any of the data structures point to invalid data.  May\n *\tfail with -EBADF if the file descriptor specified in the first\n *\tiocb is invalid.  May fail with -EAGAIN if insufficient resources\n *\tare available to queue any iocbs.  Will return 0 if nr is 0.  Will\n *\tfail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,\n\t\tstruct iocb __user * __user *, iocbpp)\n{\n\treturn do_io_submit(ctx_id, nr, iocbpp, 0);\n}\n\n/* lookup_kiocb\n *\tFinds a given iocb for cancellation.\n */\nstatic struct aio_kiocb *\nlookup_kiocb(struct kioctx *ctx, struct iocb __user *iocb, u32 key)\n{\n\tstruct aio_kiocb *kiocb;\n\n\tassert_spin_locked(&ctx->ctx_lock);\n\n\tif (key != KIOCB_KEY)\n\t\treturn NULL;\n\n\t/* TODO: use a hash or array, this sucks. */\n\tlist_for_each_entry(kiocb, &ctx->active_reqs, ki_list) {\n\t\tif (kiocb->ki_user_iocb == iocb)\n\t\t\treturn kiocb;\n\t}\n\treturn NULL;\n}\n\n/* sys_io_cancel:\n *\tAttempts to cancel an iocb previously passed to io_submit.  If\n *\tthe operation is successfully cancelled, the resulting event is\n *\tcopied into the memory pointed to by result without being placed\n *\tinto the completion queue and 0 is returned.  May fail with\n *\t-EFAULT if any of the data structures pointed to are invalid.\n *\tMay fail with -EINVAL if aio_context specified by ctx_id is\n *\tinvalid.  May fail with -EAGAIN if the iocb specified was not\n *\tcancelled.  Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE3(io_cancel, aio_context_t, ctx_id, struct iocb __user *, iocb,\n\t\tstruct io_event __user *, result)\n{\n\tstruct kioctx *ctx;\n\tstruct aio_kiocb *kiocb;\n\tu32 key;\n\tint ret;\n\n\tret = get_user(key, &iocb->aio_key);\n\tif (unlikely(ret))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\tkiocb = lookup_kiocb(ctx, iocb, key);\n\tif (kiocb)\n\t\tret = kiocb_cancel(kiocb);\n\telse\n\t\tret = -EINVAL;\n\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tif (!ret) {\n\t\t/*\n\t\t * The result argument is no longer used - the io_event is\n\t\t * always delivered via the ring buffer. -EINPROGRESS indicates\n\t\t * cancellation is progress:\n\t\t */\n\t\tret = -EINPROGRESS;\n\t}\n\n\tpercpu_ref_put(&ctx->users);\n\n\treturn ret;\n}\n\n/* io_getevents:\n *\tAttempts to read at least min_nr events and up to nr events from\n *\tthe completion queue for the aio_context specified by ctx_id. If\n *\tit succeeds, the number of read events is returned. May fail with\n *\t-EINVAL if ctx_id is invalid, if min_nr is out of range, if nr is\n *\tout of range, if timeout is out of range.  May fail with -EFAULT\n *\tif any of the memory specified is invalid.  May return 0 or\n *\t< min_nr if the timeout specified by timeout has elapsed\n *\tbefore sufficient events are available, where timeout == NULL\n *\tspecifies an infinite timeout. Note that the timeout pointed to by\n *\ttimeout is relative.  Will fail with -ENOSYS if not implemented.\n */\nSYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct timespec __user *, timeout)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx_id);\n\tlong ret = -EINVAL;\n\n\tif (likely(ioctx)) {\n\t\tif (likely(min_nr <= nr && min_nr >= 0))\n\t\t\tret = read_events(ioctx, min_nr, nr, events, timeout);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\treturn ret;\n}\n"], "filenames": ["fs/aio.c"], "buggy_code_start_loc": [242], "buggy_code_end_loc": [243], "fixing_code_start_loc": [242], "fixing_code_end_loc": [248], "type": "CWE-264", "message": "The aio_mount function in fs/aio.c in the Linux kernel before 4.7.7 does not properly restrict execute access, which makes it easier for local users to bypass intended SELinux W^X policy restrictions, and consequently gain privileges, via an io_setup system call.", "other": {"cve": {"id": "CVE-2016-10044", "sourceIdentifier": "security@android.com", "published": "2017-02-07T07:59:00.293", "lastModified": "2023-01-17T21:40:37.717", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The aio_mount function in fs/aio.c in the Linux kernel before 4.7.7 does not properly restrict execute access, which makes it easier for local users to bypass intended SELinux W^X policy restrictions, and consequently gain privileges, via an io_setup system call."}, {"lang": "es", "value": "La funci\u00f3n aio_mount en fs/aio.c en el kernel de Linux en versiones anteriores a 4.7.7 no restringe adecuadamente el acceso de ejecuci\u00f3n, lo que facilita a usuarios locales eludir restricciones de pol\u00edtica destinadas SELinux W^X, y consecuentemente obtener privilegios, a trav\u00e9s de una llamada de sistema io_setup."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.16.43", "matchCriteriaId": "EE61EB40-EE9C-41E8-AC78-D38441DB9CF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "4.4.24", "matchCriteriaId": "21856BF4-4072-4055-BA57-93D5C4608C07"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.7.7", "matchCriteriaId": "004A2488-AD73-4B53-961A-EEA808A5183D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:google:android:*:*:*:*:*:*:*:*", "versionEndIncluding": "7.1.1", "matchCriteriaId": "0F11609D-D1B4-4DD6-8CC7-A224344E1E67"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=22f6b4d34fcf039c63a94e7670e0da24f8575a5a", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://source.android.com/security/bulletin/2017-02-01.html", "source": "security@android.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.7.7", "source": "security@android.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/96122", "source": "security@android.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1037798", "source": "security@android.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/22f6b4d34fcf039c63a94e7670e0da24f8575a5a", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/22f6b4d34fcf039c63a94e7670e0da24f8575a5a"}}