{"buggy_code": ["import os\nimport traceback\nimport yaml\nimport json\nimport csv\nimport validators\nimport requests\nimport logging\nimport metafinder.extractor as metadata_extractor\nimport whatportis\nimport subprocess\n\n\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium import webdriver\nfrom emailfinder.extractor import *\nfrom dotted_dict import DottedDict\nfrom celery import shared_task\nfrom discord_webhook import DiscordWebhook\nfrom reNgine.celery import app\nfrom startScan.models import *\nfrom targetApp.models import Domain\nfrom scanEngine.models import EngineType\nfrom django.conf import settings\nfrom django.shortcuts import get_object_or_404\n\nfrom celery import shared_task\nfrom datetime import datetime\nfrom degoogle import degoogle\n\nfrom django.conf import settings\nfrom django.utils import timezone, dateformat\nfrom django.shortcuts import get_object_or_404\nfrom django.core.exceptions import ObjectDoesNotExist\n\nfrom reNgine.celery import app\nfrom reNgine.definitions import *\n\nfrom startScan.models import *\nfrom targetApp.models import Domain\nfrom scanEngine.models import EngineType, Configuration, Wordlist\n\nfrom .common_func import *\n\n'''\ntask for background scan\n'''\n\n\n@app.task\ndef initiate_scan(\n        domain_id,\n        scan_history_id,\n        scan_type,\n        engine_type,\n        imported_subdomains=None,\n        out_of_scope_subdomains=None\n        ):\n    '''\n    scan_type = 0 -> immediate scan, need not create scan object\n    scan_type = 1 -> scheduled scan\n    '''\n    engine_object = EngineType.objects.get(pk=engine_type)\n    domain = Domain.objects.get(pk=domain_id)\n    if scan_type == 1:\n        task = ScanHistory()\n        task.scan_status = -1\n    elif scan_type == 0:\n        task = ScanHistory.objects.get(pk=scan_history_id)\n\n    # save the last scan date for domain model\n    domain.last_scan_date = timezone.now()\n    domain.save()\n\n    # once the celery task starts, change the task status to Started\n    task.scan_type = engine_object\n    task.celery_id = initiate_scan.request.id\n    task.domain = domain\n    task.scan_status = 1\n    task.start_scan_date = timezone.now()\n    task.subdomain_discovery = True if engine_object.subdomain_discovery else False\n    task.dir_file_search = True if engine_object.dir_file_search else False\n    task.port_scan = True if engine_object.port_scan else False\n    task.fetch_url = True if engine_object.fetch_url else False\n    task.osint = True if engine_object.osint else False\n    task.screenshot = True if engine_object.screenshot else False\n    task.vulnerability_scan = True if engine_object.vulnerability_scan else False\n    task.save()\n\n    activity_id = create_scan_activity(task, \"Scanning Started\", 2)\n    results_dir = '/usr/src/scan_results/'\n    os.chdir(results_dir)\n\n    notification = Notification.objects.all()\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has initiated recon for target {} with engine type {}'.format(domain.name, engine_object.engine_name))\n\n    try:\n        current_scan_dir = domain.name + '_' + \\\n            str(datetime.datetime.strftime(timezone.now(), '%Y_%m_%d_%H_%M_%S'))\n        os.mkdir(current_scan_dir)\n        task.results_dir = current_scan_dir\n        task.save()\n    except Exception as exception:\n        logger.error(exception)\n        scan_failed(task)\n\n    yaml_configuration = None\n    excluded_subdomains = ''\n\n    try:\n        yaml_configuration = yaml.load(\n            task.scan_type.yaml_configuration,\n            Loader=yaml.FullLoader)\n    except Exception as exception:\n        logger.error(exception)\n        # TODO: Put failed reason on db\n\n    '''\n    Add GF patterns name to db for dynamic URLs menu\n    '''\n    if engine_object.fetch_url and GF_PATTERNS in yaml_configuration[FETCH_URL]:\n        task.used_gf_patterns = ','.join(\n            pattern for pattern in yaml_configuration[FETCH_URL][GF_PATTERNS])\n        task.save()\n\n    results_dir = results_dir + current_scan_dir\n\n    # put all imported subdomains into txt file and also in Subdomain model\n    if imported_subdomains:\n        extract_imported_subdomain(\n            imported_subdomains, task, domain, results_dir)\n\n    if yaml_configuration:\n        '''\n        a target in itself is a subdomain, some tool give subdomains as\n        www.yogeshojha.com but url and everything else resolves to yogeshojha.com\n        In that case, we would already need to store target itself as subdomain\n        '''\n        initial_subdomain_file = '/target_domain.txt' if task.subdomain_discovery else '/sorted_subdomain_collection.txt'\n\n        subdomain_file = open(results_dir + initial_subdomain_file, \"w\")\n        subdomain_file.write(domain.name + \"\\n\")\n        subdomain_file.close()\n\n        if(task.subdomain_discovery):\n            activity_id = create_scan_activity(task, \"Subdomain Scanning\", 1)\n            subdomain_scan(\n                task,\n                domain,\n                yaml_configuration,\n                results_dir,\n                activity_id,\n                out_of_scope_subdomains\n                )\n        else:\n            skip_subdomain_scan(task, domain, results_dir)\n\n        update_last_activity(activity_id, 2)\n        activity_id = create_scan_activity(task, \"HTTP Crawler\", 1)\n        http_crawler(\n            task,\n            domain,\n            results_dir,\n            activity_id)\n        update_last_activity(activity_id, 2)\n\n        try:\n            if task.screenshot:\n                activity_id = create_scan_activity(\n                    task, \"Visual Recon - Screenshot\", 1)\n                grab_screenshot(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    current_scan_dir,\n                    activity_id)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if(task.port_scan):\n                activity_id = create_scan_activity(task, \"Port Scanning\", 1)\n                port_scanning(task, domain, yaml_configuration, results_dir)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if task.osint:\n                activity_id = create_scan_activity(task, \"OSINT Running\", 1)\n                perform_osint(task, domain, yaml_configuration, results_dir)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n\n        try:\n            if task.dir_file_search:\n                activity_id = create_scan_activity(task, \"Directory Search\", 1)\n                directory_brute(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    results_dir,\n                    activity_id\n                )\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if task.fetch_url:\n                activity_id = create_scan_activity(task, \"Fetching endpoints\", 1)\n                fetch_endpoints(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    results_dir,\n                    activity_id)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if task.vulnerability_scan:\n                activity_id = create_scan_activity(task, \"Vulnerability Scan\", 1)\n                vulnerability_scan(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    results_dir,\n                    activity_id)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n    activity_id = create_scan_activity(task, \"Scan Completed\", 2)\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('*Scan Completed*\\nreNgine has finished performing recon on target {}.'.format(domain.name))\n\n    '''\n    Once the scan is completed, save the status to successful\n    '''\n    if ScanActivity.objects.filter(scan_of=task).filter(status=0).all():\n        task.scan_status = 0\n    else:\n        task.scan_status = 2\n    task.stop_scan_date = timezone.now()\n    task.save()\n    # cleanup results\n    # delete_scan_data(results_dir)\n    return {\"status\": True}\n\n\ndef skip_subdomain_scan(task, domain, results_dir):\n    # store default target as subdomain\n    '''\n    If the imported subdomain already has target domain saved, we can skip this\n    '''\n    if not Subdomain.objects.filter(\n            scan_history=task,\n            name=domain.name).exists():\n\n        subdomain_dict = DottedDict({\n            'name': domain.name,\n            'scan_history': task,\n            'target_domain': domain\n        })\n        save_subdomain(subdomain_dict)\n\n    # Save target into target_domain.txt\n    with open('{}/target_domain.txt'.format(results_dir), 'w+') as file:\n        file.write(domain.name + '\\n')\n\n    file.close()\n\n    '''\n    We can have two conditions, either subdomain scan happens, or subdomain scan\n    does not happen, in either cases, because we are using import subdomain, we\n    need to collect and sort all the subdomains\n\n    Write target domain into subdomain_collection\n    '''\n\n    os.system(\n        'cat {0}/target_domain.txt > {0}/subdomain_collection.txt'.format(results_dir))\n\n    os.system(\n        'cat {0}/from_imported.txt > {0}/subdomain_collection.txt'.format(results_dir))\n\n    os.system('rm -f {}/from_imported.txt'.format(results_dir))\n\n    '''\n    Sort all Subdomains\n    '''\n    os.system(\n        'sort -u {0}/subdomain_collection.txt -o {0}/sorted_subdomain_collection.txt'.format(results_dir))\n\n    os.system('rm -f {}/subdomain_collection.txt'.format(results_dir))\n\n\ndef extract_imported_subdomain(imported_subdomains, task, domain, results_dir):\n    valid_imported_subdomains = [subdomain for subdomain in imported_subdomains if validators.domain(\n        subdomain) and domain.name == get_domain_from_subdomain(subdomain)]\n\n    # remove any duplicate\n    valid_imported_subdomains = list(set(valid_imported_subdomains))\n\n    with open('{}/from_imported.txt'.format(results_dir), 'w+') as file:\n        for subdomain_name in valid_imported_subdomains:\n            # save _subdomain to Subdomain model db\n            if not Subdomain.objects.filter(\n                    scan_history=task, name=subdomain_name).exists():\n\n                subdomain_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'name': subdomain_name,\n                    'is_imported_subdomain': True\n                })\n                save_subdomain(subdomain_dict)\n                # save subdomain to file\n                file.write('{}\\n'.format(subdomain_name))\n\n    file.close()\n\n\ndef subdomain_scan(task, domain, yaml_configuration, results_dir, activity_id, out_of_scope_subdomains=None):\n    '''\n    This function is responsible for performing subdomain enumeration\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Subdomain Gathering for target {} has been started'.format(domain.name))\n\n    subdomain_scan_results_file = results_dir + '/sorted_subdomain_collection.txt'\n\n    # check for all the tools and add them into string\n    # if tool selected is all then make string, no need for loop\n    if ALL in yaml_configuration[SUBDOMAIN_DISCOVERY][USES_TOOLS]:\n        tools = 'amass-active amass-passive assetfinder sublist3r subfinder oneforall'\n    else:\n        tools = ' '.join(\n            str(tool) for tool in yaml_configuration[SUBDOMAIN_DISCOVERY][USES_TOOLS])\n\n    logging.info(tools)\n\n    # check for THREADS, by default 10\n    threads = 10\n    if THREADS in yaml_configuration[SUBDOMAIN_DISCOVERY]:\n        _threads = yaml_configuration[SUBDOMAIN_DISCOVERY][THREADS]\n        if _threads > 0:\n            threads = _threads\n\n    if 'amass' in tools:\n        if 'amass-passive' in tools:\n            amass_command = 'amass enum -passive -d {} -o {}/from_amass.txt'.format(\n                    domain.name, results_dir)\n\n            if USE_AMASS_CONFIG in yaml_configuration[SUBDOMAIN_DISCOVERY] and yaml_configuration[SUBDOMAIN_DISCOVERY][USE_AMASS_CONFIG]:\n                amass_command += ' -config /root/.config/amass.ini'\n            # Run Amass Passive\n            logging.info(amass_command)\n            os.system(amass_command)\n\n        if 'amass-active' in tools:\n            amass_command = 'amass enum -active -d {} -o {}/from_amass_active.txt'.format(\n                    domain.name, results_dir)\n\n            if USE_AMASS_CONFIG in yaml_configuration[SUBDOMAIN_DISCOVERY] and yaml_configuration[SUBDOMAIN_DISCOVERY][USE_AMASS_CONFIG]:\n                amass_command += ' -config /root/.config/amass.ini'\n\n            if AMASS_WORDLIST in yaml_configuration[SUBDOMAIN_DISCOVERY]:\n                wordlist = yaml_configuration[SUBDOMAIN_DISCOVERY][AMASS_WORDLIST]\n                if wordlist == 'default':\n                    wordlist_path = settings.TOOL_LOCATION + AMASS_DEFAULT_WORDLIST_PATH\n                else:\n                    wordlist_path = settings.TOOL_LOCATION + 'wordlist/' + wordlist + '.txt'\n                    if not os.path.exists(wordlist_path):\n                        wordlist_path = settings.TOOL_LOCATION + AMASS_WORDLIST\n                amass_command = amass_command + \\\n                    ' -brute -w {}'.format(wordlist_path)\n            if amass_config_path:\n                amass_command = amass_command + \\\n                    ' -config {}'.format(settings.TOOL_LOCATION +\n                                         'scan_results/' + amass_config_path)\n\n            # Run Amass Active\n            logging.info(amass_command)\n            os.system(amass_command)\n\n    if 'assetfinder' in tools:\n        assetfinder_command = 'assetfinder --subs-only {} > {}/from_assetfinder.txt'.format(\n            domain.name, results_dir)\n\n        # Run Assetfinder\n        logging.info(assetfinder_command)\n        os.system(assetfinder_command)\n\n    if 'sublist3r' in tools:\n        sublist3r_command = 'python3 /usr/src/github/Sublist3r/sublist3r.py -d {} -t {} -o {}/from_sublister.txt'.format(\n            domain.name, threads, results_dir)\n\n        # Run sublist3r\n        logging.info(sublist3r_command)\n        os.system(sublist3r_command)\n\n    if 'subfinder' in tools:\n        subfinder_command = 'subfinder -d {} -t {} -o {}/from_subfinder.txt'.format(\n            domain.name, threads, results_dir)\n\n        if USE_SUBFINDER_CONFIG in yaml_configuration[SUBDOMAIN_DISCOVERY] and yaml_configuration[SUBDOMAIN_DISCOVERY][USE_SUBFINDER_CONFIG]:\n            subfinder_command += ' -config /root/.config/subfinder/config.yaml'\n\n        # Run Subfinder\n        logging.info(subfinder_command)\n        os.system(subfinder_command)\n\n    if 'oneforall' in tools:\n        oneforall_command = 'python3 /usr/src/github/OneForAll/oneforall.py --target {} run'.format(\n            domain.name, results_dir)\n\n        # Run OneForAll\n        logging.info(oneforall_command)\n        os.system(oneforall_command)\n\n        extract_subdomain = \"cut -d',' -f6 /usr/src/github/OneForAll/results/{}.csv >> {}/from_oneforall.txt\".format(\n            domain.name, results_dir)\n\n        os.system(extract_subdomain)\n\n        # remove the results from oneforall directory\n        os.system(\n            'rm -rf /usr/src/github/OneForAll/results/{}.*'.format(domain.name))\n\n    '''\n    All tools have gathered the list of subdomains with filename\n    initials as from_*\n    We will gather all the results in one single file, sort them and\n    remove the older results from_*\n    '''\n    os.system(\n        'cat {0}/*.txt > {0}/subdomain_collection.txt'.format(results_dir))\n\n    '''\n    Write target domain into subdomain_collection\n    '''\n    os.system(\n        'cat {0}/target_domain.txt >> {0}/subdomain_collection.txt'.format(results_dir))\n\n    '''\n    Remove all the from_* files\n    '''\n    os.system('rm -f {}/from*'.format(results_dir))\n\n    '''\n    Sort all Subdomains\n    '''\n    os.system(\n        'sort -u {0}/subdomain_collection.txt -o {0}/sorted_subdomain_collection.txt'.format(results_dir))\n\n    os.system('rm -f {}/subdomain_collection.txt'.format(results_dir))\n\n    '''\n    The final results will be stored in sorted_subdomain_collection.\n    '''\n    # parse the subdomain list file and store in db\n    with open(subdomain_scan_results_file) as subdomain_list:\n        for _subdomain in subdomain_list:\n            __subdomain = _subdomain.rstrip('\\n')\n            if not Subdomain.objects.filter(scan_history=task, name=__subdomain).exists(\n            ) and validators.domain(__subdomain) and __subdomain not in out_of_scope_subdomains:\n                subdomain_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'name': __subdomain,\n                })\n                save_subdomain(subdomain_dict)\n\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        subdomains_count = Subdomain.objects.filter(scan_history=task).count()\n        send_notification('Subdomain Gathering for target {} has been completed and has discovered *{}* subdomains.'.format(domain.name, subdomains_count))\n    if notification and notification[0].send_scan_output_file:\n        send_files_to_discord(results_dir + '/sorted_subdomain_collection.txt')\n\n    # check for any subdomain changes and send notif if any\n    if notification and notification[0].send_subdomain_changes_notif:\n        newly_added_subdomain = get_new_added_subdomain(task.id, domain.id)\n        if newly_added_subdomain:\n            message = \"**{} New Subdomains Discovered on domain {}**\".format(newly_added_subdomain.count(), domain.name)\n            for subdomain in newly_added_subdomain:\n                message += \"\\n\u2022 {}\".format(subdomain.name)\n            send_notification(message)\n\n        removed_subdomain = get_removed_subdomain(task.id, domain.id)\n        if removed_subdomain:\n            message = \"**{} Subdomains are no longer available on domain {}**\".format(removed_subdomain.count(), domain.name)\n            for subdomain in removed_subdomain:\n                message += \"\\n\u2022 {}\".format(subdomain.name)\n            send_notification(message)\n\n    # check for interesting subdomains and send notif if any\n    if notification and notification[0].send_interesting_notif:\n        interesting_subdomain = get_interesting_subdomains(task.id, domain.id)\n        print(interesting_subdomain)\n        if interesting_subdomain:\n            message = \"**{} Interesting Subdomains Found on domain {}**\".format(interesting_subdomain.count(), domain.name)\n            for subdomain in interesting_subdomain:\n                message += \"\\n\u2022 {}\".format(subdomain.name)\n            send_notification(message)\n\n\ndef get_new_added_subdomain(scan_id, domain_id):\n    scan_history = ScanHistory.objects.filter(\n        domain=domain_id).filter(\n            subdomain_discovery=True).filter(\n                id__lte=scan_id)\n    if scan_history.count() > 1:\n        last_scan = scan_history.order_by('-start_scan_date')[1]\n        scanned_host_q1 = Subdomain.objects.filter(\n            scan_history__id=scan_id).values('name')\n        scanned_host_q2 = Subdomain.objects.filter(\n            scan_history__id=last_scan.id).values('name')\n        added_subdomain = scanned_host_q1.difference(scanned_host_q2)\n\n        return Subdomain.objects.filter(\n            scan_history=scan_id).filter(\n                name__in=added_subdomain)\n\ndef get_removed_subdomain(scan_id, domain_id):\n    scan_history = ScanHistory.objects.filter(\n        domain=domain_id).filter(\n            subdomain_discovery=True).filter(\n                id__lte=scan_id)\n    if scan_history.count() > 1:\n        last_scan = scan_history.order_by('-start_scan_date')[1]\n        scanned_host_q1 = Subdomain.objects.filter(\n            scan_history__id=scan_id).values('name')\n        scanned_host_q2 = Subdomain.objects.filter(\n            scan_history__id=last_scan.id).values('name')\n        removed_subdomains = scanned_host_q2.difference(scanned_host_q1)\n\n        print()\n\n        return Subdomain.objects.filter(\n            scan_history=last_scan).filter(\n                name__in=removed_subdomains)\n\n\ndef http_crawler(task, domain, results_dir, activity_id):\n    '''\n    This function is runs right after subdomain gathering, and gathers important\n    like page title, http status, etc\n    HTTP Crawler runs by default\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('HTTP Crawler for target {} has been initiated.'.format(domain.name))\n\n    alive_file_location = results_dir + '/alive.txt'\n    httpx_results_file = results_dir + '/httpx.json'\n\n    subdomain_scan_results_file = results_dir + '/sorted_subdomain_collection.txt'\n    httpx_command = 'httpx -status-code -content-length -title -tech-detect -cdn -ip -follow-host-redirects -random-agent'\n\n    proxy = get_random_proxy()\n    if proxy:\n        httpx_command += ' --http-proxy {}'.format(proxy)\n\n    httpx_command += ' -json -o {}'.format(\n        httpx_results_file\n    )\n    httpx_command = 'cat {} | {}'.format(subdomain_scan_results_file, httpx_command)\n    print(httpx_command)\n    os.system(httpx_command)\n\n    # alive subdomains from httpx\n    alive_file = open(alive_file_location, 'w')\n\n    # writing httpx results\n    if os.path.isfile(httpx_results_file):\n        httpx_json_result = open(httpx_results_file, 'r')\n        lines = httpx_json_result.readlines()\n        for line in lines:\n            json_st = json.loads(line.strip())\n            try:\n                subdomain = Subdomain.objects.get(\n                    scan_history=task, name=json_st['url'].split(\"//\")[-1])\n                '''\n                Saving Default http urls to EndPoint\n                '''\n                endpoint = EndPoint()\n                endpoint.scan_history = task\n                endpoint.target_domain = domain\n                endpoint.subdomain = subdomain\n                if 'url' in json_st:\n                    endpoint.http_url = json_st['url']\n                    subdomain.http_url = json_st['url']\n                if 'status-code' in json_st:\n                    endpoint.http_status = json_st['status-code']\n                    subdomain.http_status = json_st['status-code']\n                if 'title' in json_st:\n                    endpoint.page_title = json_st['title']\n                    subdomain.page_title = json_st['title']\n                if 'content-length' in json_st:\n                    endpoint.content_length = json_st['content-length']\n                    subdomain.content_length = json_st['content-length']\n                if 'content-type' in json_st:\n                    endpoint.content_type = json_st['content-type']\n                    subdomain.content_type = json_st['content-type']\n                if 'webserver' in json_st:\n                    endpoint.webserver = json_st['webserver']\n                    subdomain.webserver = json_st['webserver']\n                if 'response-time' in json_st:\n                    response_time = float(\n                        ''.join(\n                            ch for ch in json_st['response-time'] if not ch.isalpha()))\n                    if json_st['response-time'][-2:] == 'ms':\n                        response_time = response_time / 1000\n                    endpoint.response_time = response_time\n                    subdomain.response_time = response_time\n                if 'cnames' in json_st:\n                    cname_list = ','.join(json_st['cnames'])\n                    subdomain.cname = cname_list\n                discovered_date = timezone.now()\n                endpoint.discovered_date = discovered_date\n                subdomain.discovered_date = discovered_date\n                endpoint.is_default = True\n                endpoint.save()\n                subdomain.save()\n                if 'technologies' in json_st:\n                    for _tech in json_st['technologies']:\n                        if Technology.objects.filter(name=_tech).exists():\n                            tech = Technology.objects.get(name=_tech)\n                        else:\n                            tech = Technology(name=_tech)\n                            tech.save()\n                        subdomain.technologies.add(tech)\n                        endpoint.technologies.add(tech)\n                if 'a' in json_st:\n                    for _ip in json_st['a']:\n                        if IpAddress.objects.filter(address=_ip).exists():\n                            ip = IpAddress.objects.get(address=_ip)\n                        else:\n                            ip = IpAddress(address=_ip)\n                            if 'cdn' in json_st:\n                                ip.is_cdn = json_st['cdn']\n                            ip.save()\n                        subdomain.ip_addresses.add(ip)\n                # see if to ignore 404 or 5xx\n                alive_file.write(json_st['url'] + '\\n')\n                subdomain.save()\n                endpoint.save()\n            except Exception as exception:\n                logging.error(exception)\n    alive_file.close()\n\n    if notification and notification[0].send_scan_status_notif:\n        alive_count = Subdomain.objects.filter(\n            scan_history__id=task.id).values('name').distinct().filter(\n            http_status__exact=200).count()\n        send_notification('HTTP Crawler for target {} has been completed.\\n\\n {} subdomains were alive (http status 200).'.format(domain.name, alive_count))\n\n\ndef grab_screenshot(task, domain, yaml_configuration, results_dir, activity_id):\n    '''\n    This function is responsible for taking screenshots\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine is currently gathering screenshots for {}'.format(domain.name))\n\n    output_screenshots_path = results_dir + '/screenshots'\n    result_csv_path = results_dir + '/screenshots/Requests.csv'\n    alive_subdomains_path = results_dir + '/alive.txt'\n\n    eyewitness_command = 'python3 /usr/src/github/EyeWitness/Python/EyeWitness.py'\n\n    eyewitness_command += ' -f {} -d {} --no-prompt'.format(\n        alive_subdomains_path,\n        output_screenshots_path\n    )\n\n    if EYEWITNESS in yaml_configuration \\\n        and TIMEOUT in yaml_configuration[EYEWITNESS] \\\n        and yaml_configuration[EYEWITNESS][TIMEOUT] > 0:\n        eyewitness_command += ' --timeout {}'.format(\n            yaml_configuration[EYEWITNESS][TIMEOUT]\n        )\n\n    if EYEWITNESS in yaml_configuration \\\n        and THREADS in yaml_configuration[EYEWITNESS] \\\n        and yaml_configuration[EYEWITNESS][THREADS] > 0:\n            eyewitness_command += ' --threads {}'.format(\n                yaml_configuration[EYEWITNESS][THREADS]\n            )\n\n    logger.info(eyewitness_command)\n\n    os.system(eyewitness_command)\n\n    if os.path.isfile(result_csv_path):\n        logger.info('Gathering Eyewitness results')\n        with open(result_csv_path, 'r') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                if row[3] == 'Successful' \\\n                    and Subdomain.objects.filter(\n                        scan_history__id=task.id).filter(name=row[2]).exists():\n                    subdomain = Subdomain.objects.get(\n                        scan_history__id=task.id,\n                        name=row[2]\n                    )\n                    subdomain.screenshot_path = row[4].replace(\n                        '/usr/src/scan_results/',\n                        ''\n                    )\n                    subdomain.save()\n\n    # remove all db, html extra files in screenshot results\n    os.system('rm -rf {0}/*.csv {0}/*.db {0}/*.js {0}/*.html {0}/*.css'.format(\n        output_screenshots_path,\n    ))\n    os.system('rm -rf {0}/source'.format(\n        output_screenshots_path,\n    ))\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has finished gathering screenshots for {}'.format(domain.name))\n\n\ndef port_scanning(task, domain, yaml_configuration, results_dir):\n    '''\n    This function is responsible for running the port scan\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Port Scan initiated for {}'.format(domain.name))\n\n    subdomain_scan_results_file = results_dir + '/sorted_subdomain_collection.txt'\n    port_results_file = results_dir + '/ports.json'\n\n    # check the yaml_configuration and choose the ports to be scanned\n\n    scan_ports = '-'  # default port scan everything\n    if PORTS in yaml_configuration[PORT_SCAN]:\n        # TODO:  legacy code, remove top-100 in future versions\n        all_ports = yaml_configuration[PORT_SCAN][PORTS]\n        if 'full' in all_ports:\n            naabu_command = 'cat {} | naabu -json -o {} -p {}'.format(\n                subdomain_scan_results_file, port_results_file, '-')\n        elif 'top-100' in all_ports:\n            naabu_command = 'cat {} | naabu -json -o {} -top-ports 100'.format(\n                subdomain_scan_results_file, port_results_file)\n        elif 'top-1000' in all_ports:\n            naabu_command = 'cat {} | naabu -json -o {} -top-ports 1000'.format(\n                subdomain_scan_results_file, port_results_file)\n        else:\n            scan_ports = ','.join(\n                str(port) for port in all_ports)\n            naabu_command = 'cat {} | naabu -json -o {} -p {}'.format(\n                subdomain_scan_results_file, port_results_file, scan_ports)\n\n    # check for exclude ports\n    if EXCLUDE_PORTS in yaml_configuration[PORT_SCAN] and yaml_configuration[PORT_SCAN][EXCLUDE_PORTS]:\n        exclude_ports = ','.join(\n            str(port) for port in yaml_configuration['port_scan']['exclude_ports'])\n        naabu_command = naabu_command + \\\n            ' -exclude-ports {}'.format(exclude_ports)\n\n    if NAABU_RATE in yaml_configuration[PORT_SCAN] and yaml_configuration[PORT_SCAN][NAABU_RATE] > 0:\n        naabu_command = naabu_command + \\\n            ' -rate {}'.format(\n                yaml_configuration[PORT_SCAN][NAABU_RATE])\n    else:\n        naabu_command = naabu_command + ' -t 10'\n\n    if USE_NAABU_CONFIG in yaml_configuration[PORT_SCAN] and yaml_configuration[PORT_SCAN][USE_NAABU_CONFIG]:\n        naabu_command += ' -config /root/.config/naabu/naabu.conf'\n\n    # run naabu\n    os.system(naabu_command)\n\n    # writing port results\n    try:\n        port_json_result = open(port_results_file, 'r')\n        lines = port_json_result.readlines()\n        for line in lines:\n            json_st = json.loads(line.strip())\n            port_number = json_st['port']\n            ip_address = json_st['ip']\n\n            # see if port already exists\n            if Port.objects.filter(number__exact=port_number).exists():\n                port = Port.objects.get(number=port_number)\n            else:\n                port = Port()\n                port.number = port_number\n            if port_number in UNCOMMON_WEB_PORTS:\n                port.is_uncommon = True\n            port_detail = whatportis.get_ports(str(port_number))\n            if len(port_detail):\n                port.service_name = port_detail[0].name\n                port.description = port_detail[0].description\n            port.save()\n            if IpAddress.objects.filter(address=json_st['ip']).exists():\n                ip = IpAddress.objects.get(address=json_st['ip'])\n                ip.ports.add(port)\n                ip.save()\n    except BaseException as exception:\n        logging.error(exception)\n        update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        port_count = Port.objects.filter(\n            ports__in=IpAddress.objects.filter(\n                ip_addresses__in=Subdomain.objects.filter(\n                    scan_history__id=task.id))).distinct().count()\n        send_notification('reNgine has finished Port Scanning on {} and has identified {} ports.'.format(domain.name, port_count))\n\n    if notification and notification[0].send_scan_output_file:\n        send_files_to_discord(results_dir + '/ports.json')\n\n\ndef check_waf():\n    '''\n    This function will check for the WAF being used in subdomains using wafw00f\n    '''\n    pass\n\n\ndef directory_brute(task, domain, yaml_configuration, results_dir, activity_id):\n    '''\n    This function is responsible for performing directory scan\n    '''\n    # scan directories for all the alive subdomain with http status >\n    # 200\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Directory Bruteforce has been initiated for {}.'.format(domain.name))\n\n    alive_subdomains = Subdomain.objects.filter(\n        scan_history__id=task.id).exclude(\n        http_url='')\n    dirs_results = results_dir + '/dirs.json'\n\n    # check the yaml settings\n    if EXTENSIONS in yaml_configuration[DIR_FILE_SEARCH]:\n        extensions = ','.join(\n            str(ext) for ext in yaml_configuration[DIR_FILE_SEARCH][EXTENSIONS])\n    else:\n        extensions = 'php,git,yaml,conf,db,mysql,bak,txt'\n\n    # Threads\n    if THREADS in yaml_configuration[DIR_FILE_SEARCH] \\\n        and yaml_configuration[DIR_FILE_SEARCH][THREADS] > 0:\n        threads = yaml_configuration[DIR_FILE_SEARCH][THREADS]\n    else:\n        threads = 10\n\n    for subdomain in alive_subdomains:\n        # delete any existing dirs.json\n        if os.path.isfile(dirs_results):\n            os.system('rm -rf {}'.format(dirs_results))\n        dirsearch_command = 'python3 /usr/src/github/dirsearch/dirsearch.py'\n\n        dirsearch_command += ' -u {}'.format(subdomain.http_url)\n\n        if (WORDLIST not in yaml_configuration[DIR_FILE_SEARCH] or\n            not yaml_configuration[DIR_FILE_SEARCH][WORDLIST] or\n                'default' in yaml_configuration[DIR_FILE_SEARCH][WORDLIST]):\n            wordlist_location = '/usr/src/github/dirsearch/db/dicc.txt'\n        else:\n            wordlist_location = settings.TOOL_LOCATION + 'wordlist/' + \\\n                yaml_configuration[DIR_FILE_SEARCH][WORDLIST] + '.txt'\n\n        dirsearch_command += ' -w {}'.format(wordlist_location)\n\n        dirsearch_command += ' --format json -o {}'.format(dirs_results)\n\n        dirsearch_command += ' -e {}'.format(extensions)\n\n        dirsearch_command += ' -t {}'.format(threads)\n\n        dirsearch_command += ' --random-agent'\n\n        if EXCLUDE_EXTENSIONS in yaml_configuration[DIR_FILE_SEARCH]:\n            exclude_extensions = ','.join(\n                str(ext) for ext in yaml_configuration[DIR_FILE_SEARCH][EXCLUDE_EXTENSIONS])\n            dirsearch_command += ' -X {}'.format(exclude_extensions)\n\n        # check if recursive strategy is set to on\n        if RECURSIVE in yaml_configuration[DIR_FILE_SEARCH] and yaml_configuration[DIR_FILE_SEARCH][RECURSIVE]:\n            dirsearch_command += ' -r'\n\n        if RECURSIVE_LEVEL in yaml_configuration[DIR_FILE_SEARCH]:\n            dirsearch_command += ' --recursion-depth {}'.format(yaml_configuration[DIR_FILE_SEARCH][RECURSIVE_LEVEL])\n\n        # proxy\n        proxy = get_random_proxy()\n        if proxy:\n            dirsearch_command += ' --proxy {}'.format(proxy)\n\n        print(dirsearch_command)\n        os.system(dirsearch_command)\n\n        try:\n            if os.path.isfile(dirs_results):\n                with open(dirs_results, \"r\") as json_file:\n                    json_string = json_file.read()\n                    scanned_host = Subdomain.objects.get(\n                        scan_history__id=task.id, http_url=subdomain.http_url)\n                    scanned_host.directory_json = json_string\n                    scanned_host.save()\n        except Exception as exception:\n            logging.error(exception)\n            update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Directory Bruteforce has been completed for {}.'.format(domain.name))\n\n\ndef fetch_endpoints(\n        task,\n        domain,\n        yaml_configuration,\n        results_dir,\n        activity_id):\n    '''\n    This function is responsible for fetching all the urls associated with target\n    and run HTTP probe\n    It first runs gau to gather all urls from wayback, then we will use hakrawler to identify more urls\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine is currently gathering endpoints for {}.'.format(domain.name))\n\n    # check yaml settings\n    if ALL in yaml_configuration[FETCH_URL][USES_TOOLS]:\n        tools = 'gauplus hakrawler waybackurls gospider'\n    else:\n        tools = ' '.join(\n            str(tool) for tool in yaml_configuration[FETCH_URL][USES_TOOLS])\n\n    if INTENSITY in yaml_configuration[FETCH_URL]:\n        scan_type = yaml_configuration[FETCH_URL][INTENSITY]\n    else:\n        scan_type = 'normal'\n\n    domain_regex = \"\\'https?://([a-z0-9]+[.])*{}.*\\'\".format(domain.name)\n\n    if 'deep' in scan_type:\n        # performs deep url gathering for all the subdomains present -\n        # RECOMMENDED\n        logger.info('Deep URLS Fetch')\n        os.system(settings.TOOL_LOCATION + 'get_urls.sh %s %s %s %s %s' %\n            (\"None\", results_dir, scan_type, domain_regex, tools))\n    else:\n        # perform url gathering only for main domain - USE only for quick scan\n        logger.info('Non Deep URLS Fetch')\n        os.system(\n            settings.TOOL_LOCATION +\n            'get_urls.sh %s %s %s %s %s' % (\n                domain.name,\n                results_dir,\n                scan_type,\n                domain_regex,\n                tools\n            ))\n\n    if IGNORE_FILE_EXTENSION in yaml_configuration[FETCH_URL]:\n        ignore_extension = '|'.join(\n            yaml_configuration[FETCH_URL][IGNORE_FILE_EXTENSION])\n        logger.info('Ignore extensions' + ignore_extension)\n        os.system(\n            'cat {0}/all_urls.txt | grep -Eiv \"\\\\.({1}).*\" > {0}/temp_urls.txt'.format(\n                results_dir, ignore_extension))\n        os.system(\n            'rm {0}/all_urls.txt && mv {0}/temp_urls.txt {0}/all_urls.txt'.format(results_dir))\n\n    '''\n    Store all the endpoints and then run the httpx\n    '''\n    try:\n        endpoint_final_url = results_dir + '/all_urls.txt'\n        if os.path.isfile(endpoint_final_url):\n            with open(endpoint_final_url) as endpoint_list:\n                for url in endpoint_list:\n                    http_url = url.rstrip('\\n')\n                    if not EndPoint.objects.filter(scan_history=task, http_url=http_url).exists():\n                        _subdomain = get_subdomain_from_url(http_url)\n                        if Subdomain.objects.filter(\n                                scan_history=task).filter(\n                                name=_subdomain).exists():\n                            subdomain = Subdomain.objects.get(\n                                scan_history=task, name=_subdomain)\n                        else:\n                            '''\n                            gau or gosppider can gather interesting endpoints which\n                            when parsed can give subdomains that were not existent from\n                            subdomain scan. so storing them\n                            '''\n                            logger.error(\n                                'Subdomain {} not found, adding...'.format(_subdomain))\n                            subdomain_dict = DottedDict({\n                                'scan_history': task,\n                                'target_domain': domain,\n                                'name': _subdomain,\n                            })\n                            subdomain = save_subdomain(subdomain_dict)\n                        endpoint_dict = DottedDict({\n                            'scan_history': task,\n                            'target_domain': domain,\n                            'subdomain': subdomain,\n                            'http_url': http_url,\n                        })\n                        save_endpoint(endpoint_dict)\n    except Exception as e:\n        logger.error(e)\n\n    if notification and notification[0].send_scan_output_file:\n        send_files_to_discord(results_dir + '/all_urls.txt')\n\n    '''\n    TODO:\n    Go spider & waybackurls accumulates a lot of urls, which is good but nuclei\n    takes forever to scan even a simple website, so we will do http probing\n    and filter HTTP status 404, this way we can reduce the number of Non Existent\n    URLS\n    '''\n    logger.info('HTTP Probing on collected endpoints')\n\n    httpx_command = 'httpx -l {0}/all_urls.txt -status-code -content-length -ip -cdn -title -tech-detect -json -follow-redirects -random-agent -o {0}/final_httpx_urls.json'.format(results_dir)\n\n    proxy = get_random_proxy()\n    if proxy:\n        httpx_command += ' --http-proxy {}'.format(proxy)\n\n    os.system(httpx_command)\n\n    url_results_file = results_dir + '/final_httpx_urls.json'\n    try:\n        urls_json_result = open(url_results_file, 'r')\n        lines = urls_json_result.readlines()\n        for line in lines:\n            json_st = json.loads(line.strip())\n            http_url = json_st['url']\n            _subdomain = get_subdomain_from_url(http_url)\n\n            if Subdomain.objects.filter(\n                    scan_history=task).filter(\n                    name=_subdomain).exists():\n                subdomain_obj = Subdomain.objects.get(\n                    scan_history=task, name=_subdomain)\n            else:\n                subdomain_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'name': _subdomain,\n                })\n                subdomain_obj = save_subdomain(subdomain_dict)\n\n            if EndPoint.objects.filter(\n                    scan_history=task).filter(\n                    http_url=http_url).exists():\n\n                endpoint = EndPoint.objects.get(\n                    scan_history=task, http_url=http_url)\n            else:\n                endpoint = EndPoint()\n                endpoint_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'http_url': http_url,\n                    'subdomain': subdomain_obj\n                })\n                endpoint = save_endpoint(endpoint_dict)\n\n            if 'title' in json_st:\n                endpoint.page_title = json_st['title']\n            if 'webserver' in json_st:\n                endpoint.webserver = json_st['webserver']\n            if 'content-length' in json_st:\n                endpoint.content_length = json_st['content-length']\n            if 'content-type' in json_st:\n                endpoint.content_type = json_st['content-type']\n            if 'status-code' in json_st:\n                endpoint.http_status = json_st['status-code']\n            if 'response-time' in json_st:\n                response_time = float(''.join(ch for ch in json_st['response-time'] if not ch.isalpha()))\n                if json_st['response-time'][-2:] == 'ms':\n                    response_time = response_time / 1000\n                endpoint.response_time = response_time\n            endpoint.save()\n            if 'technologies' in json_st:\n                for _tech in json_st['technologies']:\n                    if Technology.objects.filter(name=_tech).exists():\n                        tech = Technology.objects.get(name=_tech)\n                    else:\n                        tech = Technology(name=_tech)\n                        tech.save()\n                    endpoint.technologies.add(tech)\n                    # get subdomain object\n                    subdomain = Subdomain.objects.get(scan_history=task, name=_subdomain)\n                    subdomain.technologies.add(tech)\n                    subdomain.save()\n    except Exception as exception:\n        logging.error(exception)\n        update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        endpoint_count = EndPoint.objects.filter(\n            scan_history__id=task.id).values('http_url').distinct().count()\n        endpoint_alive_count = EndPoint.objects.filter(\n                scan_history__id=task.id, http_status__exact=200).values('http_url').distinct().count()\n        send_notification('reNgine has finished gathering endpoints for {} and has discovered *{}* unique endpoints.\\n\\n{} of those endpoints reported HTTP status 200.'.format(\n            domain.name,\n            endpoint_count,\n            endpoint_alive_count\n        ))\n\n\n    # once endpoint is saved, run gf patterns TODO: run threads\n    if GF_PATTERNS in yaml_configuration[FETCH_URL]:\n        for pattern in yaml_configuration[FETCH_URL][GF_PATTERNS]:\n            logger.info('Running GF for {}'.format(pattern))\n            gf_output_file_path = '{0}/gf_patterns_{1}.txt'.format(\n                results_dir, pattern)\n            gf_command = 'cat {0}/all_urls.txt | gf {1} >> {2}'.format(\n                results_dir, pattern, gf_output_file_path)\n            os.system(gf_command)\n            if os.path.exists(gf_output_file_path):\n                with open(gf_output_file_path) as gf_output:\n                    for line in gf_output:\n                        url = line.rstrip('\\n')\n                        try:\n                            endpoint = EndPoint.objects.get(\n                                scan_history=task, http_url=url)\n                            earlier_pattern = endpoint.matched_gf_patterns\n                            new_pattern = earlier_pattern + ',' + pattern if earlier_pattern else pattern\n                            endpoint.matched_gf_patterns = new_pattern\n                        except Exception as e:\n                            # add the url in db\n                            logger.error(e)\n                            logger.info('Adding URL' + url)\n                            endpoint = EndPoint()\n                            endpoint.http_url = url\n                            endpoint.target_domain = domain\n                            endpoint.scan_history = task\n                            try:\n                                _subdomain = Subdomain.objects.get(\n                                    scan_history=task, name=get_subdomain_from_url(url))\n                                endpoint.subdomain = _subdomain\n                            except Exception as e:\n                                continue\n                            endpoint.matched_gf_patterns = pattern\n                        finally:\n                            endpoint.save()\n\n\ndef vulnerability_scan(\n        task,\n        domain,\n        yaml_configuration,\n        results_dir,\n        activity_id):\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Vulnerability scan has been initiated for {}.'.format(domain.name))\n    '''\n    This function will run nuclei as a vulnerability scanner\n    ----\n    unfurl the urls to keep only domain and path, this will be sent to vuln scan\n    ignore certain file extensions\n    Thanks: https://github.com/six2dez/reconftw\n    '''\n    urls_path = '/alive.txt'\n    if task.scan_type.fetch_url:\n        os.system('cat {0}/all_urls.txt | grep -Eiv \"\\\\.(eot|jpg|jpeg|gif|css|tif|tiff|png|ttf|otf|woff|woff2|ico|pdf|svg|txt|js|doc|docx)$\" | unfurl -u format %s://%d%p >> {0}/unfurl_urls.txt'.format(results_dir))\n        os.system(\n            'sort -u {0}/unfurl_urls.txt -o {0}/unfurl_urls.txt'.format(results_dir))\n        urls_path = '/unfurl_urls.txt'\n\n    vulnerability_result_path = results_dir + '/vulnerability.json'\n\n    vulnerability_scan_input_file = results_dir + urls_path\n\n    nuclei_command = 'nuclei -json -l {} -o {}'.format(\n        vulnerability_scan_input_file, vulnerability_result_path)\n\n    # check nuclei config\n    if USE_NUCLEI_CONFIG in yaml_configuration[VULNERABILITY_SCAN] and yaml_configuration[VULNERABILITY_SCAN][USE_NUCLEI_CONFIG]:\n        nuclei_command += ' -config /root/.config/nuclei/config.yaml'\n\n    '''\n    Nuclei Templates\n    Either custom template has to be supplied or default template, if neither has\n    been supplied then use all templates including custom templates\n    '''\n\n    if CUSTOM_NUCLEI_TEMPLATE in yaml_configuration[\n            VULNERABILITY_SCAN] or NUCLEI_TEMPLATE in yaml_configuration[VULNERABILITY_SCAN]:\n        # check yaml settings for templates\n        if NUCLEI_TEMPLATE in yaml_configuration[VULNERABILITY_SCAN]:\n            if ALL in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_TEMPLATE]:\n                template = NUCLEI_TEMPLATES_PATH\n            else:\n                _template = ','.join([NUCLEI_TEMPLATES_PATH + str(element)\n                                      for element in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_TEMPLATE]])\n                template = _template.replace(',', ' -t ')\n\n            # Update nuclei command with templates\n            nuclei_command = nuclei_command + ' -t ' + template\n\n        if CUSTOM_NUCLEI_TEMPLATE in yaml_configuration[VULNERABILITY_SCAN]:\n            # add .yaml to the custom template extensions\n            _template = ','.join(\n                [str(element) + '.yaml' for element in yaml_configuration[VULNERABILITY_SCAN][CUSTOM_NUCLEI_TEMPLATE]])\n            template = _template.replace(',', ' -t ')\n            # Update nuclei command with templates\n            nuclei_command = nuclei_command + ' -t ' + template\n    else:\n        nuclei_command = nuclei_command + ' -t /root/nuclei-templates'\n\n    # check yaml settings for  concurrency\n    if NUCLEI_CONCURRENCY in yaml_configuration[VULNERABILITY_SCAN] and yaml_configuration[\n            VULNERABILITY_SCAN][NUCLEI_CONCURRENCY] > 0:\n        concurrency = yaml_configuration[VULNERABILITY_SCAN][NUCLEI_CONCURRENCY]\n        # Update nuclei command with concurrent\n        nuclei_command = nuclei_command + ' -c ' + str(concurrency)\n\n    # for severity\n    if NUCLEI_SEVERITY in yaml_configuration[VULNERABILITY_SCAN] and ALL not in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_SEVERITY]:\n        _severity = ','.join(\n            [str(element) for element in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_SEVERITY]])\n        severity = _severity.replace(\" \", \"\")\n    else:\n        severity = \"critical, high, medium, low, info\"\n\n    # update nuclei templates before running scan\n    os.system('nuclei -update-templates')\n\n    for _severity in severity.split(\",\"):\n        # delete any existing vulnerability.json file\n        if os.path.isfile(vulnerability_result_path):\n            os.system('rm {}'.format(vulnerability_result_path))\n        # run nuclei\n        final_nuclei_command = nuclei_command + ' -severity ' + _severity\n        proxy = get_random_proxy()\n        if proxy:\n            final_nuclei_command += ' --proxy-url {}'.format(proxy)\n\n        logger.info(final_nuclei_command)\n\n        os.system(final_nuclei_command)\n        try:\n            if os.path.isfile(vulnerability_result_path):\n                urls_json_result = open(vulnerability_result_path, 'r')\n                lines = urls_json_result.readlines()\n                for line in lines:\n                    json_st = json.loads(line.strip())\n                    host = json_st['host']\n                    _subdomain = get_subdomain_from_url(host)\n                    try:\n                        subdomain = Subdomain.objects.get(\n                            name=_subdomain, scan_history=task)\n                        vulnerability = Vulnerability()\n                        vulnerability.subdomain = subdomain\n                        vulnerability.scan_history = task\n                        vulnerability.target_domain = domain\n                        try:\n                            endpoint = EndPoint.objects.get(\n                                scan_history=task, target_domain=domain, http_url=host)\n                            vulnerability.endpoint = endpoint\n                        except Exception as exception:\n                            logger.error(exception)\n                        if 'name' in json_st['info']:\n                            vulnerability.name = json_st['info']['name']\n                        if 'severity' in json_st['info']:\n                            if json_st['info']['severity'] == 'info':\n                                severity = 0\n                            elif json_st['info']['severity'] == 'low':\n                                severity = 1\n                            elif json_st['info']['severity'] == 'medium':\n                                severity = 2\n                            elif json_st['info']['severity'] == 'high':\n                                severity = 3\n                            elif json_st['info']['severity'] == 'critical':\n                                severity = 4\n                            else:\n                                severity = 0\n                        else:\n                            severity = 0\n                        vulnerability.severity = severity\n                        if 'tags' in json_st['info']:\n                            vulnerability.tags = json_st['info']['tags']\n                        if 'description' in json_st['info']:\n                            vulnerability.description = json_st['info']['description']\n                        if 'reference' in json_st['info']:\n                            vulnerability.reference = json_st['info']['reference']\n                        if 'matched' in json_st:\n                            vulnerability.http_url = json_st['matched']\n                        if 'templateID' in json_st:\n                            vulnerability.template_used = json_st['templateID']\n                        if 'description' in json_st:\n                            vulnerability.description = json_st['description']\n                        if 'matcher_name' in json_st:\n                            vulnerability.matcher_name = json_st['matcher_name']\n                        if 'extracted_results' in json_st:\n                            vulnerability.extracted_results = json_st['extracted_results']\n                        vulnerability.discovered_date = timezone.now()\n                        vulnerability.open_status = True\n                        vulnerability.save()\n                        # send notification for all vulnerabilities except info\n                        if  severity != \"info\" and notification and notification[0].send_vuln_notif:\n                            message = \"*Alert: Vulnerability Identified*\"\n                            message += \"\\n\\n\"\n                            message += \"A *{}* severity vulnerability has been identified.\".format(json_st['info']['severity'])\n                            message += \"\\nVulnerability Name: {}\".format(json_st['info']['name'])\n                            message += \"\\nVulnerable URL: {}\".format(json_st['host'])\n                            send_notification(message)\n\n                        # send report to hackerone\n                        if Hackerone.objects.all().exists() and severity != 'info' and severity \\\n                            != 'low' and vulnerability.target_domain.h1_team_handle:\n                            hackerone = Hackerone.objects.all()[0]\n                            \n                            if hackerone.send_critical and severity == 'critical':\n                                send_hackerone_report(vulnerability.id)\n                            elif hackerone.send_high and severity == 'high':\n                                send_hackerone_report(vulnerability.id)\n                            elif hackerone.send_medium and severity == 'medium':\n                                send_hackerone_report(vulnerability.id)\n\n                    except ObjectDoesNotExist:\n                        logger.error('Object not found')\n                        continue\n\n        except Exception as exception:\n            logging.error(exception)\n            update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        info_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=0).count()\n        low_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=1).count()\n        medium_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=2).count()\n        high_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=3).count()\n        critical_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=4).count()\n        vulnerability_count = info + low_count + medium_count + high_count + critical_count\n\n        message = 'Vulnerability scan has been completed for {} and discovered {} vulnerabilities.'.format(\n            domain.name,\n            vulnerability_count\n        )\n        message += '\\n\\n*Vulnerability Stats:*'\n        message += '\\nCritical: {}'.format(critical_count)\n        message += '\\nHigh: {}'.format(high_count)\n        message += '\\nMedium: {}'.format(medium_count)\n        message += '\\nLow: {}'.format(low_count)\n        message += '\\nInfo: {}'.format(info_count)\n\n        send_notification(message)\n\n\ndef scan_failed(task):\n    task.scan_status = 0\n    task.stop_scan_date = timezone.now()\n    task.save()\n\n\ndef create_scan_activity(task, message, status):\n    scan_activity = ScanActivity()\n    scan_activity.scan_of = task\n    scan_activity.title = message\n    scan_activity.time = timezone.now()\n    scan_activity.status = status\n    scan_activity.save()\n    return scan_activity.id\n\n\ndef update_last_activity(id, activity_status):\n    ScanActivity.objects.filter(\n        id=id).update(\n        status=activity_status,\n        time=timezone.now())\n\n\ndef delete_scan_data(results_dir):\n    # remove all txt,html,json files\n    os.system('find {} -name \"*.txt\" -type f -delete'.format(results_dir))\n    os.system('find {} -name \"*.html\" -type f -delete'.format(results_dir))\n    os.system('find {} -name \"*.json\" -type f -delete'.format(results_dir))\n\n\ndef save_subdomain(subdomain_dict):\n    subdomain = Subdomain()\n    subdomain.discovered_date = timezone.now()\n    subdomain.target_domain = subdomain_dict.get('target_domain')\n    subdomain.scan_history = subdomain_dict.get('scan_history')\n    subdomain.name = subdomain_dict.get('name')\n    subdomain.http_url = subdomain_dict.get('http_url')\n    subdomain.screenshot_path = subdomain_dict.get('screenshot_path')\n    subdomain.http_header_path = subdomain_dict.get('http_header_path')\n    subdomain.cname = subdomain_dict.get('cname')\n    subdomain.is_cdn = subdomain_dict.get('is_cdn')\n    subdomain.content_type = subdomain_dict.get('content_type')\n    subdomain.webserver = subdomain_dict.get('webserver')\n    subdomain.page_title = subdomain_dict.get('page_title')\n\n    subdomain.is_imported_subdomain = subdomain_dict.get(\n        'is_imported_subdomain') if 'is_imported_subdomain' in subdomain_dict else False\n\n    if 'http_status' in subdomain_dict:\n        subdomain.http_status = subdomain_dict.get('http_status')\n\n    if 'response_time' in subdomain_dict:\n        subdomain.response_time = subdomain_dict.get('response_time')\n\n    if 'content_length' in subdomain_dict:\n        subdomain.content_length = subdomain_dict.get('content_length')\n\n    subdomain.save()\n    return subdomain\n\n\ndef save_endpoint(endpoint_dict):\n    endpoint = EndPoint()\n    endpoint.discovered_date = timezone.now()\n    endpoint.scan_history = endpoint_dict.get('scan_history')\n    endpoint.target_domain = endpoint_dict.get('target_domain') if 'target_domain' in endpoint_dict else None\n    endpoint.subdomain = endpoint_dict.get('subdomain') if 'target_domain' in endpoint_dict else None\n    endpoint.http_url = endpoint_dict.get('http_url')\n    endpoint.page_title = endpoint_dict.get('page_title') if 'page_title' in endpoint_dict else None\n    endpoint.content_type = endpoint_dict.get('content_type') if 'content_type' in endpoint_dict else None\n    endpoint.webserver = endpoint_dict.get('webserver') if 'webserver' in endpoint_dict else None\n    endpoint.response_time = endpoint_dict.get('response_time') if 'response_time' in endpoint_dict else 0\n    endpoint.http_status = endpoint_dict.get('http_status') if 'http_status' in endpoint_dict else 0\n    endpoint.content_length = endpoint_dict.get('content_length') if 'content_length' in endpoint_dict else 0\n    endpoint.is_default = endpoint_dict.get('is_default') if 'is_default' in endpoint_dict else False\n    endpoint.save()\n\n    return endpoint\n\ndef perform_osint(task, domain, yaml_configuration, results_dir):\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has initiated OSINT on target {}'.format(domain.name))\n\n    if 'discover' in yaml_configuration[OSINT]:\n        osint_discovery(task, domain, yaml_configuration, results_dir)\n\n    if 'dork' in yaml_configuration[OSINT]:\n        dorking(task, yaml_configuration)\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has completed performing OSINT on target {}'.format(domain.name))\n\ndef osint_discovery(task, domain, yaml_configuration, results_dir):\n    if ALL in yaml_configuration[OSINT][OSINT_DISCOVER]:\n        osint_lookup = 'emails metainfo employees'\n    else:\n        osint_lookup = ' '.join(\n            str(lookup) for lookup in yaml_configuration[OSINT][OSINT_DISCOVER])\n\n    if 'metainfo' in osint_lookup:\n        if INTENSITY in yaml_configuration[OSINT]:\n            osint_intensity = yaml_configuration[OSINT][INTENSITY]\n        else:\n            osint_intensity = 'normal'\n\n        if OSINT_DOCUMENTS_LIMIT in yaml_configuration[OSINT]:\n            documents_limit = yaml_configuration[OSINT][OSINT_DOCUMENTS_LIMIT]\n        else:\n            documents_limit = 50\n\n        if osint_intensity == 'normal':\n            meta_dict = DottedDict({\n                'osint_target': domain.name,\n                'domain': domain,\n                'scan_id': task,\n                'documents_limit': documents_limit\n            })\n            get_and_save_meta_info(meta_dict)\n        elif osint_intensity == 'deep':\n            # get all subdomains in scan_id\n            subdomains = Subdomain.objects.filter(scan_history=task)\n            for subdomain in subdomains:\n                meta_dict = DottedDict({\n                    'osint_target': subdomain.name,\n                    'domain': domain,\n                    'scan_id': task,\n                    'documents_limit': documents_limit\n                })\n                get_and_save_meta_info(meta_dict)\n\n    if 'emails' in osint_lookup:\n        get_and_save_emails(task, results_dir)\n        get_and_save_leaked_credentials(task, results_dir)\n\n    if 'employees' in osint_lookup:\n        get_and_save_employees(task, results_dir)\n\ndef dorking(scan_history, yaml_configuration):\n    # Some dork sources: https://github.com/six2dez/degoogle_hunter/blob/master/degoogle_hunter.sh\n    # look in stackoverflow\n    if ALL in yaml_configuration[OSINT][OSINT_DORK]:\n        dork_lookup = 'stackoverflow, 3rdparty, social_media, project_management, code_sharing, config_files, jenkins, cloud_buckets, php_error, exposed_documents, struts_rce, db_files, traefik, git_exposed'\n    else:\n        dork_lookup = ' '.join(\n            str(lookup) for lookup in yaml_configuration[OSINT][OSINT_DORK])\n\n    if 'stackoverflow' in dork_lookup:\n        dork = 'site:stackoverflow.com'\n        dork_type = 'stackoverflow'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if '3rdparty' in dork_lookup:\n        # look in 3rd party sitee\n        dork_type = '3rdparty'\n        lookup_websites = [\n            'gitter.im',\n            'papaly.com',\n            'productforums.google.com',\n            'coggle.it',\n            'replt.it',\n            'ycombinator.com',\n            'libraries.io',\n            'npm.runkit.com',\n            'npmjs.com',\n            'scribd.com',\n            'gitter.im'\n        ]\n        dork = ''\n        for website in lookup_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'social_media' in dork_lookup:\n        dork_type = 'Social Media'\n        social_websites = [\n            'tiktok.com',\n            'facebook.com',\n            'twitter.com',\n            'youtube.com',\n            'pinterest.com',\n            'tumblr.com',\n            'reddit.com'\n        ]\n        dork = ''\n        for website in social_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'project_management' in dork_lookup:\n        dork_type = 'Project Management'\n        project_websites = [\n            'trello.com',\n            '*.atlassian.net'\n        ]\n        dork = ''\n        for website in project_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'code_sharing' in dork_lookup:\n        dork_type = 'Code Sharing Sites'\n        code_websites = [\n            'github.com',\n            'gitlab.com',\n            'bitbucket.org'\n        ]\n        dork = ''\n        for website in code_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'config_files' in dork_lookup:\n        dork_type = 'Config Files'\n        config_file_ext = [\n            'env',\n            'xml',\n            'conf',\n            'cnf',\n            'inf',\n            'rdp',\n            'ora',\n            'txt',\n            'cfg',\n            'ini'\n        ]\n\n        dork = ''\n        for extension in config_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'jenkins' in dork_lookup:\n        dork_type = 'Jenkins'\n        dork = 'intitle:\\\"Dashboard [Jenkins]\\\"'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'wordpress_files' in dork_lookup:\n        dork_type = 'Wordpress Files'\n        inurl_lookup = [\n            'wp-content',\n            'wp-includes'\n        ]\n\n        dork = ''\n        for lookup in inurl_lookup:\n            dork = dork + ' | ' + 'inurl:' + lookup\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'cloud_buckets' in dork_lookup:\n        dork_type = 'Cloud Buckets'\n        cloud_websites = [\n            '.s3.amazonaws.com',\n            'storage.googleapis.com',\n            'amazonaws.com'\n        ]\n\n        dork = ''\n        for website in cloud_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'php_error' in dork_lookup:\n        dork_type = 'PHP Error'\n        error_words = [\n            '\\\"PHP Parse error\\\"',\n            '\\\"PHP Warning\\\"',\n            '\\\"PHP Error\\\"'\n        ]\n\n        dork = ''\n        for word in error_words:\n            dork = dork + ' | ' + word\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'exposed_documents' in dork_lookup:\n        dork_type = 'Exposed Documents'\n        docs_file_ext = [\n            'doc',\n            'docx',\n            'odt',\n            'pdf',\n            'rtf',\n            'sxw',\n            'psw',\n            'ppt',\n            'pptx',\n            'pps',\n            'csv'\n        ]\n\n        dork = ''\n        for extension in docs_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'struts_rce' in dork_lookup:\n        dork_type = 'Apache Struts RCE'\n        struts_file_ext = [\n            'action',\n            'struts',\n            'do'\n        ]\n\n        dork = ''\n        for extension in struts_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'db_files' in dork_lookup:\n        dork_type = 'Database Files'\n        db_file_ext = [\n            'sql',\n            'db',\n            'dbf',\n            'mdb'\n        ]\n\n        dork = ''\n        for extension in db_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'traefik' in dork_lookup:\n        dork = 'intitle:traefik inurl:8080/dashboard'\n        dork_type = 'Traefik'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'git_exposed' in dork_lookup:\n        dork = 'inurl:\\\"/.git\\\"'\n        dork_type = '.git Exposed'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n\ndef get_and_save_dork_results(dork, type, scan_history, in_target=False):\n    degoogle_obj = degoogle.dg()\n    proxy = get_random_proxy()\n    if proxy:\n        os.environ['https_proxy'] = proxy\n        os.environ['HTTPS_PROXY'] = proxy\n\n    if in_target:\n        query = dork + \" site:\" + scan_history.domain.name\n    else:\n        query = dork + \" \\\"{}\\\"\".format(scan_history.domain.name)\n    logger.info(query)\n    degoogle_obj.query = query\n    results = degoogle_obj.run()\n    logger.info(results)\n    for result in results:\n        dork, _ = Dork.objects.get_or_create(\n            type=type,\n            description=result['desc'],\n            url=result['url']\n        )\n        scan_history.dorks.add(dork)\n\ndef get_and_save_employees(scan_history, results_dir):\n    theHarvester_location = '/usr/src/github/theHarvester'\n\n    # update proxies.yaml\n    if Proxy.objects.all().exists():\n        proxy = Proxy.objects.all()[0]\n        if proxy.use_proxy:\n            proxy_list = proxy.proxies.splitlines()\n            yaml_data = {'http' : proxy_list}\n\n            with open(theHarvester_location + '/proxies.yaml', 'w') as file:\n                documents = yaml.dump(yaml_data, file)\n\n\n    os.system('cd {} && python3 theHarvester.py -d {} -b all -f {}/theHarvester.html'.format(\n        theHarvester_location,\n        scan_history.domain.name,\n        results_dir\n    ))\n\n    file_location = results_dir + '/theHarvester.html'\n    print(file_location)\n    # delete proxy environ var\n    del os.environ['https_proxy']\n    del os.environ['HTTPS_PROXY']\n\n    if os.path.isfile(file_location):\n        logger.info('Parsing theHarvester results')\n        options = FirefoxOptions()\n        options.add_argument(\"--headless\")\n        driver = webdriver.Firefox(options=options)\n        driver.get('file://'+file_location)\n        tabledata = driver.execute_script('return tabledata')\n        # save email addresses and linkedin employees\n        for data in tabledata:\n            if data['record'] == 'email':\n                _email = data['result']\n                email, _ = Email.objects.get_or_create(address=_email)\n                scan_history.emails.add(email)\n            elif data['record'] == 'people':\n                _employee = data['result']\n                split_val = _employee.split('-')\n                name = split_val[0]\n                if len(split_val) == 2:\n                    designation = split_val[1]\n                else:\n                    designation = \"\"\n                employee, _ = Employee.objects.get_or_create(name=name, designation=designation)\n                scan_history.employees.add(employee)\n        driver.quit()\n\n\n        print(tabledata)\n\n\ndef get_and_save_emails(scan_history, results_dir):\n    leak_target_path = '{}/creds_target.txt'.format(results_dir)\n\n    # get email address\n    proxy = get_random_proxy()\n    if proxy:\n        os.environ['https_proxy'] = proxy\n        os.environ['HTTPS_PROXY'] = proxy\n\n    emails = []\n\n    try:\n        logger.info('OSINT: Getting emails from Google')\n        email_from_google = get_emails_from_google(scan_history.domain.name)\n        logger.info('OSINT: Getting emails from Bing')\n        email_from_bing = get_emails_from_bing(scan_history.domain.name)\n        logger.info('OSINT: Getting emails from Baidu')\n        email_from_baidu = get_emails_from_baidu(scan_history.domain.name)\n        emails = list(set(email_from_google + email_from_bing + email_from_baidu))\n        logger.info(emails)\n    except Exception as e:\n        logger.error(e)\n\n    leak_target_file = open(leak_target_path, 'w')\n\n    for _email in emails:\n        email, _ = Email.objects.get_or_create(address=_email)\n        scan_history.emails.add(email)\n        leak_target_file.write('{}\\n'.format(_email))\n\n    # fill leak_target_file with possible email address\n    leak_target_file.write('%@{}\\n'.format(scan_history.domain.name))\n    leak_target_file.write('%@%.{}\\n'.format(scan_history.domain.name))\n\n    leak_target_file.write('%.%@{}\\n'.format(scan_history.domain.name))\n    leak_target_file.write('%.%@%.{}\\n'.format(scan_history.domain.name))\n\n    leak_target_file.write('%_%@{}\\n'.format(scan_history.domain.name))\n    leak_target_file.write('%_%@%.{}\\n'.format(scan_history.domain.name))\n\n    leak_target_file.close()\n\ndef get_and_save_leaked_credentials(scan_history, results_dir):\n    logger.info('OSINT: Getting leaked credentials...')\n\n    leak_target_file = '{}/creds_target.txt'.format(results_dir)\n    leak_output_file = '{}/pwndb.json'.format(results_dir)\n\n    pwndb_command = 'python3 /usr/src/github/pwndb/pwndb.py --proxy tor:9150 --output json --list {}'.format(\n        leak_target_file\n    )\n\n    pwndb_output = subprocess.getoutput(pwndb_command)\n\n    print(pwndb_output)\n\n    try:\n        creds = json.loads(pwndb_output)\n\n        for cred in creds:\n            if cred['username'] != 'donate':\n                email_id = \"{}@{}\".format(cred['username'], cred['domain'])\n\n                email_obj, _ = Email.objects.get_or_create(\n                    address=email_id,\n                )\n                email_obj.password = cred['password']\n                email_obj.save()\n                scan_history.emails.add(email_obj)\n    except Exception as e:\n        logger.error(e)\n\n\ndef get_and_save_meta_info(meta_dict):\n    logger.info('Getting METADATA for {}'.format(meta_dict.osint_target))\n    proxy = get_random_proxy()\n    if proxy:\n        os.environ['https_proxy'] = proxy\n        os.environ['HTTPS_PROXY'] = proxy\n    result = metadata_extractor.extract_metadata_from_google_search(meta_dict.osint_target, meta_dict.documents_limit)\n    if result:\n        results = result.get_metadata()\n        for meta in results:\n            meta_finder_document = MetaFinderDocument()\n            subdomain = Subdomain.objects.get(scan_history=meta_dict.scan_id, name=meta_dict.osint_target)\n            meta_finder_document.subdomain = subdomain\n            meta_finder_document.target_domain = meta_dict.domain\n            meta_finder_document.scan_history = meta_dict.scan_id\n\n            item = DottedDict(results[meta])\n            meta_finder_document.url = item.url\n            meta_finder_document.doc_name = meta\n            meta_finder_document.http_status = item.status_code\n\n            metadata = results[meta]['metadata']\n            for data in metadata:\n                if 'Producer' in metadata and metadata['Producer']:\n                    meta_finder_document.producer = metadata['Producer'].rstrip('\\x00')\n                if 'Creator' in metadata and metadata['Creator']:\n                    meta_finder_document.creator = metadata['Creator'].rstrip('\\x00')\n                if 'CreationDate' in metadata and metadata['CreationDate']:\n                    meta_finder_document.creation_date = metadata['CreationDate'].rstrip('\\x00')\n                if 'ModDate' in metadata and metadata['ModDate']:\n                    meta_finder_document.modified_date = metadata['ModDate'].rstrip('\\x00')\n                if 'Author' in metadata and metadata['Author']:\n                    meta_finder_document.author = metadata['Author'].rstrip('\\x00')\n                if 'Title' in metadata and metadata['Title']:\n                    meta_finder_document.title = metadata['Title'].rstrip('\\x00')\n                if 'OSInfo' in metadata and metadata['OSInfo']:\n                    meta_finder_document.os = metadata['OSInfo'].rstrip('\\x00')\n\n            meta_finder_document.save()\n\n@app.task(bind=True)\ndef test_task(self):\n    print('*' * 40)\n    print('test task run')\n    print('*' * 40)\n", "import os\nimport requests\nimport itertools\n\nfrom datetime import datetime\n\nfrom django.shortcuts import render, get_object_or_404\nfrom django.contrib import messages\nfrom django.http import JsonResponse, HttpResponseRedirect, HttpResponse\nfrom django.urls import reverse\nfrom django_celery_beat.models import PeriodicTask, IntervalSchedule, ClockedSchedule\nfrom django.utils import timezone\nfrom django.conf import settings\nfrom django.core import serializers\n\nfrom startScan.models import *\nfrom targetApp.models import *\nfrom scanEngine.models import EngineType, Configuration\nfrom reNgine.tasks import initiate_scan, create_scan_activity\nfrom reNgine.celery import app\n\nfrom reNgine.common_func import *\n\n\ndef scan_history(request):\n    host = ScanHistory.objects.all().order_by('-start_scan_date')\n    context = {'scan_history_active': 'active', \"scan_history\": host}\n    return render(request, 'startScan/history.html', context)\n\n\ndef detail_scan(request, id=None):\n    context = {}\n    if id:\n        context['scan_history_id'] = id\n        context['subdomain_count'] = Subdomain.objects.filter(\n            scan_history__id=id).values('name').distinct().count()\n        context['alive_count'] = Subdomain.objects.filter(\n            scan_history__id=id).values('name').distinct().filter(\n            http_status__exact=200).count()\n        context['important_count'] = Subdomain.objects.filter(\n            scan_history__id=id).values('name').distinct().filter(\n            is_important=True).count()\n        context['scan_activity'] = ScanActivity.objects.filter(\n            scan_of__id=id).order_by('-time')\n        context['endpoint_count'] = EndPoint.objects.filter(\n            scan_history__id=id).values('http_url').distinct().count()\n        context['endpoint_alive_count'] = EndPoint.objects.filter(\n            scan_history__id=id, http_status__exact=200).values('http_url').distinct().count()\n        history = get_object_or_404(ScanHistory, id=id)\n        context['history'] = history\n        info_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=0).count()\n        low_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=1).count()\n        medium_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=2).count()\n        high_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=3).count()\n        critical_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=4).count()\n        context['vulnerability_list'] = Vulnerability.objects.filter(\n            scan_history__id=id).order_by('-severity').all()[:20]\n        context['total_vulnerability_count'] = info_count + low_count + \\\n            medium_count + high_count + critical_count\n        context['info_count'] = info_count\n        context['low_count'] = low_count\n        context['medium_count'] = medium_count\n        context['high_count'] = high_count\n        context['critical_count'] = critical_count\n        context['scan_history_active'] = 'active'\n\n        emails = Email.objects.filter(\n            emails__in=ScanHistory.objects.filter(\n                id=id))\n\n        context['exposed_count'] = emails.exclude(password__isnull=True).count()\n\n        context['email_count'] = emails.count()\n\n        context['employees_count'] = Employee.objects.filter(\n            employees__in=ScanHistory.objects.filter(id=id)).count()\n\n        domain_id = ScanHistory.objects.filter(id=id)\n\n        context['most_recent_scans'] = ScanHistory.objects.filter(domain__id=domain_id[0].domain.id).order_by('-start_scan_date')[:5]\n\n        if domain_id:\n            domain_id = domain_id[0].domain.id\n            scan_history = ScanHistory.objects.filter(domain=domain_id).filter(subdomain_discovery=True).filter(id__lte=id).filter(scan_status=2)\n            if scan_history.count() > 1:\n                last_scan = scan_history.order_by('-start_scan_date')[1]\n                context['last_scan'] = last_scan\n\n    # badge count for gfs\n    if history.used_gf_patterns:\n        count_gf = {}\n        for gf in history.used_gf_patterns.split(','):\n            count_gf[gf] = EndPoint.objects.filter(scan_history__id=id, matched_gf_patterns__icontains=gf).count()\n            context['matched_gf_count'] = count_gf\n    return render(request, 'startScan/detail_scan.html', context)\n\ndef all_subdomains(request):\n    context = {}\n    context['scan_history_id'] = id\n    context['subdomain_count'] = Subdomain.objects.values('name').distinct().count()\n    context['alive_count'] = Subdomain.objects.values('name').distinct().filter(\n        http_status__exact=200).count()\n    context['important_count'] = Subdomain.objects.values('name').distinct().filter(\n        is_important=True).count()\n\n    context['scan_history_active'] = 'active'\n\n    return render(request, 'startScan/subdomains.html', context)\n\ndef detail_vuln_scan(request, id=None):\n    if id:\n        history = get_object_or_404(ScanHistory, id=id)\n        context = {'scan_history_id': id, 'history': history}\n    else:\n        context = {'vuln_scan_active': 'true'}\n    return render(request, 'startScan/vulnerabilities.html', context)\n\n\ndef all_endpoints(request):\n    context = {}\n    context['scan_history_active'] = 'active'\n    return render(request, 'startScan/endpoints.html', context)\n\n\ndef start_scan_ui(request, domain_id):\n    domain = get_object_or_404(Domain, id=domain_id)\n    if request.method == \"POST\":\n        # get imported subdomains\n        imported_subdomains = [subdomain.rstrip() for subdomain in request.POST['importSubdomainTextArea'].split('\\n')]\n        imported_subdomains = [subdomain for subdomain in imported_subdomains if subdomain]\n\n        out_of_scope_subdomains = [subdomain.rstrip() for subdomain in request.POST['outOfScopeSubdomainTextarea'].split('\\n')]\n        out_of_scope_subdomains = [subdomain for subdomain in out_of_scope_subdomains if subdomain]\n        # get engine type\n        engine_type = request.POST['scan_mode']\n        scan_history_id = create_scan_object(domain_id, engine_type)\n        # start the celery task\n        celery_task = initiate_scan.apply_async(\n            args=(\n                domain_id,\n                scan_history_id,\n                0,\n                engine_type,\n                imported_subdomains,\n                out_of_scope_subdomains\n                ))\n        ScanHistory.objects.filter(\n            id=scan_history_id).update(\n            celery_id=celery_task.id)\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Started for ' +\n            domain.name)\n        return HttpResponseRedirect(reverse('scan_history'))\n    engine = EngineType.objects.order_by('id')\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'domain': domain,\n        'engines': engine,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'startScan/start_scan_ui.html', context)\n\n\ndef start_multiple_scan(request):\n    # domain = get_object_or_404(Domain, id=host_id)\n    domain_text = \"\"\n    if request.method == \"POST\":\n        if request.POST.get('scan_mode', 0):\n            # if scan mode is available, then start the scan\n            # get engine type\n            engine_type = request.POST['scan_mode']\n            list_of_domains = request.POST['list_of_domain_id']\n            for domain_id in list_of_domains.split(\",\"):\n                # start the celery task\n                scan_history_id = create_scan_object(domain_id, engine_type)\n                celery_task = initiate_scan.apply_async(\n                    args=(domain_id, scan_history_id, 0, engine_type))\n                ScanHistory.objects.filter(\n                    id=scan_history_id).update(\n                    celery_id=celery_task.id)\n            messages.add_message(\n                request,\n                messages.INFO,\n                'Scan Started for multiple targets')\n            return HttpResponseRedirect(reverse('scan_history'))\n        else:\n            # this else condition will have post request from the scan page\n            # containing all the targets id\n            list_of_domain_name = []\n            list_of_domain_id = []\n            for key, value in request.POST.items():\n                print(value)\n                if key != \"list_target_table_length\" and key != \"csrfmiddlewaretoken\":\n                    domain = get_object_or_404(Domain, id=value)\n                    list_of_domain_name.append(domain.name)\n                    list_of_domain_id.append(value)\n            domain_text = \", \".join(list_of_domain_name)\n            domain_ids = \",\".join(list_of_domain_id)\n    engine = EngineType.objects\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'engines': engine,\n        'domain_list': domain_text,\n        'domain_ids': domain_ids,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'startScan/start_multiple_scan_ui.html', context)\n\ndef export_subdomains(request, scan_id):\n    subdomain_list = Subdomain.objects.filter(scan_history__id=scan_id)\n    domain_results = ScanHistory.objects.get(id=scan_id)\n    response_body = \"\"\n    for name in subdomain_list:\n        response_body = response_body + name.name + \"\\n\"\n    response = HttpResponse(response_body, content_type='text/plain')\n    response['Content-Disposition'] = 'attachment; filename=\"subdomains_' + \\\n        domain_results.domain.name + '_' + \\\n        str(domain_results.start_scan_date.date()) + '.txt\"'\n    return response\n\n\ndef export_endpoints(request, scan_id):\n    endpoint_list = EndPoint.objects.filter(scan_history__id=scan_id)\n    domain_results = ScanHistory.objects.get(id=scan_id)\n    response_body = \"\"\n    for endpoint in endpoint_list:\n        response_body = response_body + endpoint.http_url + \"\\n\"\n    response = HttpResponse(response_body, content_type='text/plain')\n    response['Content-Disposition'] = 'attachment; filename=\"endpoints_' + \\\n        domain_results.domain.name + '_' + \\\n        str(domain_results.start_scan_date.date()) + '.txt\"'\n    return response\n\n\ndef export_urls(request, scan_id):\n    urls_list = Subdomain.objects.filter(scan_history__id=scan_id)\n    domain_results = ScanHistory.objects.get(id=scan_id)\n    response_body = \"\"\n    for url in urls_list:\n        if url.http_url:\n            response_body = response_body + url.http_url + \"\\n\"\n    response = HttpResponse(response_body, content_type='text/plain')\n    response['Content-Disposition'] = 'attachment; filename=\"urls_' + \\\n        domain_results.domain.name + '_' + \\\n        str(domain_results.start_scan_date.date()) + '.txt\"'\n    return response\n\n\ndef delete_scan(request, id):\n    obj = get_object_or_404(ScanHistory, id=id)\n    if request.method == \"POST\":\n        delete_dir = obj.domain.name + '_' + \\\n            str(datetime.datetime.strftime(obj.start_scan_date, '%Y_%m_%d_%H_%M_%S'))\n        delete_path = settings.TOOL_LOCATION + 'scan_results/' + delete_dir\n        os.system('rm -rf ' + delete_path)\n        obj.delete()\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan history successfully deleted!')\n    else:\n        messageData = {'status': 'false'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Oops! something went wrong!')\n    return JsonResponse(messageData)\n\n\ndef stop_scan(request, id):\n    if request.method == \"POST\":\n        scan_history = get_object_or_404(ScanHistory, celery_id=id)\n        # stop the celery task\n        app.control.revoke(id, terminate=True, signal='SIGKILL')\n        scan_history.scan_status = 3\n        scan_history.save()\n        try:\n            last_activity = ScanActivity.objects.filter(\n                scan_of=scan_history).order_by('-pk')[0]\n            last_activity.status = 0\n            last_activity.time = timezone.now()\n            last_activity.save()\n        except Exception as e:\n            print(e)\n        create_scan_activity(scan_history, \"Scan aborted\", 0)\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan successfully stopped!')\n    else:\n        messageData = {'status': 'false'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Oops! something went wrong!')\n    return JsonResponse(messageData)\n\n\ndef schedule_scan(request, host_id):\n    domain = Domain.objects.get(id=host_id)\n    if request.method == \"POST\":\n        # get imported subdomains\n        imported_subdomains = [subdomain.rstrip() for subdomain in request.POST['importSubdomainTextArea'].split('\\n')]\n        imported_subdomains = [subdomain for subdomain in imported_subdomains if subdomain]\n        # get engine type\n        engine_type = int(request.POST['scan_mode'])\n        engine_object = get_object_or_404(EngineType, id=engine_type)\n        task_name = engine_object.engine_name + ' for ' + \\\n            domain.name + \\\n            ':' + \\\n            str(datetime.datetime.strftime(timezone.now(), '%Y_%m_%d_%H_%M_%S'))\n        if request.POST['scheduled_mode'] == 'periodic':\n            # periodic task\n            frequency_value = int(request.POST['frequency'])\n            frequency_type = request.POST['frequency_type']\n            if frequency_type == 'minutes':\n                period = IntervalSchedule.MINUTES\n            elif frequency_type == 'hours':\n                period = IntervalSchedule.HOURS\n            elif frequency_type == 'days':\n                period = IntervalSchedule.DAYS\n            elif frequency_type == 'weeks':\n                period = IntervalSchedule.DAYS\n                frequency_value *= 7\n            elif frequency_type == 'months':\n                period = IntervalSchedule.DAYS\n                frequency_value *= 30\n\n            schedule, created = IntervalSchedule.objects.get_or_create(\n                every=frequency_value,\n                period=period,)\n            _kwargs = json.dumps({'domain_id': host_id, 'scan_history_id': 0, 'scan_type': 1, 'engine_type': engine_type, 'imported_subdomains': imported_subdomains})\n            PeriodicTask.objects.create(interval=schedule,\n                                        name=task_name,\n                                        task='reNgine.tasks.initiate_scan',\n                                        kwargs=_kwargs)\n        elif request.POST['scheduled_mode'] == 'clocked':\n            # clocked task\n            schedule_time = request.POST['scheduled_time']\n            clock, created = ClockedSchedule.objects.get_or_create(\n                clocked_time=schedule_time,)\n            _kwargs = json.dumps({'domain_id': host_id, 'scan_history_id': 0, 'scan_type': 1, 'engine_type': engine_type, 'imported_subdomains': imported_subdomains})\n            PeriodicTask.objects.create(clocked=clock,\n                                        one_off=True,\n                                        name=task_name,\n                                        task='reNgine.tasks.initiate_scan',\n                                        kwargs=_kwargs)\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Scheduled for ' +\n            domain.name)\n        return HttpResponseRedirect(reverse('scheduled_scan_view'))\n    engine = EngineType.objects\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'domain': domain,\n        'engines': engine,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'startScan/schedule_scan_ui.html', context)\n\n\ndef scheduled_scan_view(request):\n    scheduled_tasks = PeriodicTask.objects.all().exclude(name='celery.backend_cleanup')\n    context = {\n        'scheduled_scan_active': 'active',\n        'scheduled_tasks': scheduled_tasks,\n    }\n    return render(request, 'startScan/schedule_scan_list.html', context)\n\n\ndef delete_scheduled_task(request, id):\n    task_object = get_object_or_404(PeriodicTask, id=id)\n    if request.method == \"POST\":\n        task_object.delete()\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scheduled Scan successfully deleted!')\n    else:\n        messageData = {'status': 'false'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Oops! something went wrong!')\n    return JsonResponse(messageData)\n\n\ndef change_scheduled_task_status(request, id):\n    if request.method == 'POST':\n        task = PeriodicTask.objects.get(id=id)\n        task.enabled = not task.enabled\n        task.save()\n    return HttpResponse('')\n\n\ndef change_vuln_status(request, id):\n    if request.method == 'POST':\n        vuln = Vulnerability.objects.get(id=id)\n        vuln.open_status = not vuln.open_status\n        vuln.save()\n    return HttpResponse('')\n\n\ndef change_subdomain_status(request, id):\n    if request.method == 'POST':\n        name = Subdomain.objects.get(id=id)\n        name.checked = not name.checked\n        name.save()\n    return HttpResponse('')\n\n\ndef change_subdomain_important_status(request, id):\n    if request.method == 'POST':\n        name = Subdomain.objects.get(id=id)\n        name.is_important = not name.is_important\n        name.save()\n    return HttpResponse('')\n\n\ndef create_scan_object(host_id, engine_type):\n    '''\n    create task with pending status so that celery task will execute when\n    threads are free\n    '''\n    # get current time\n    current_scan_time = timezone.now()\n    # fetch engine and domain object\n    engine_object = EngineType.objects.get(pk=engine_type)\n    domain = Domain.objects.get(pk=host_id)\n    task = ScanHistory()\n    task.scan_status = -1\n    task.domain = domain\n    task.scan_type = engine_object\n    task.start_scan_date = current_scan_time\n    task.save()\n    # save last scan date for domain model\n    domain.start_scan_date = current_scan_time\n    domain.save()\n    return task.id\n\n\ndef delete_all_scan_results(request):\n    if request.method == 'POST':\n        ScanHistory.objects.all().delete()\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'All Scan History successfully deleted!')\n    return JsonResponse(messageData)\n\ndef delete_all_screenshots(request):\n    if request.method == 'POST':\n        os.system('rm -rf /usr/src/scan_results/*')\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Screenshots successfully deleted!')\n    return JsonResponse(messageData)\n\n\ndef visualise(request, id):\n    scan_history = ScanHistory.objects.get(id=id)\n    context = {\n        'scan_id': id,\n        'scan_history': scan_history,\n    }\n    return render(request, 'startScan/visualise.html', context)\n\ndef start_organization_scan(request, id):\n    organization = get_object_or_404(Organization, id=id)\n    if request.method == \"POST\":\n        # get engine type\n        engine_type = request.POST['scan_mode']\n        for domain in organization.get_domains():\n            scan_history_id = create_scan_object(domain.id, engine_type)\n            # start the celery task\n            celery_task = initiate_scan.apply_async(\n                args=(domain.id,\n                    scan_history_id,\n                    0,\n                    engine_type,\n                    None\n                ))\n            ScanHistory.objects.filter(\n                id=scan_history_id).update(\n                celery_id=celery_task.id)\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Started for {} domains in organization {}'.format(\n                len(organization.get_domains()),\n                organization.name\n            )\n        )\n        return HttpResponseRedirect(reverse('scan_history'))\n    engine = EngineType.objects.order_by('id')\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    domain_list = organization.get_domains()\n    context = {\n        'organization_data_active': 'true',\n        'list_organization_li': 'active',\n        'organization': organization,\n        'engines': engine,\n        'domain_list': domain_list,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'organization/start_scan.html', context)\n\ndef schedule_organization_scan(request, id):\n    organization =Organization.objects.get(id=id)\n    if request.method == \"POST\":\n        # get engine type\n        engine_type = int(request.POST['scan_mode'])\n        engine_object = get_object_or_404(EngineType, id=engine_type)\n        for domain in organization.get_domains():\n            task_name = engine_object.engine_name + ' for ' + \\\n                domain.name + \\\n                ':' + \\\n                str(datetime.datetime.strftime(\n                    timezone.now(),\n                    '%Y_%m_%d_%H_%M_%S'\n                ))\n            if request.POST['scheduled_mode'] == 'periodic':\n                # periodic task\n                frequency_value = int(request.POST['frequency'])\n                frequency_type = request.POST['frequency_type']\n                if frequency_type == 'minutes':\n                    period = IntervalSchedule.MINUTES\n                elif frequency_type == 'hours':\n                    period = IntervalSchedule.HOURS\n                elif frequency_type == 'days':\n                    period = IntervalSchedule.DAYS\n                elif frequency_type == 'weeks':\n                    period = IntervalSchedule.DAYS\n                    frequency_value *= 7\n                elif frequency_type == 'months':\n                    period = IntervalSchedule.DAYS\n                    frequency_value *= 30\n\n                schedule, created = IntervalSchedule.objects.get_or_create(\n                    every=frequency_value,\n                    period=period,)\n                _kwargs = json.dumps({'domain_id': domain.id,\n                        'scan_history_id': 0,\n                        'scan_type': 1,\n                        'engine_type': engine_type,\n                        'imported_subdomains': None\n                })\n                PeriodicTask.objects.create(interval=schedule,\n                    name=task_name,\n                    task='reNgine.tasks.initiate_scan',\n                    kwargs=_kwargs\n                )\n            elif request.POST['scheduled_mode'] == 'clocked':\n                # clocked task\n                schedule_time = request.POST['scheduled_time']\n                clock, created = ClockedSchedule.objects.get_or_create(\n                    clocked_time=schedule_time,)\n                _kwargs = json.dumps({'domain_id': domain.id,\n                    'scan_history_id': 0,\n                    'scan_type': 1,\n                    'engine_type': engine_type,\n                    'imported_subdomains': None}\n                )\n                PeriodicTask.objects.create(clocked=clock,\n                    one_off=True,\n                    name=task_name,\n                    task='reNgine.tasks.initiate_scan',\n                    kwargs=_kwargs\n                )\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Started for {} domains in organization {}'.format(\n                len(organization.get_domains()),\n                organization.name\n            )\n        )\n        return HttpResponseRedirect(reverse('scheduled_scan_view'))\n    engine = EngineType.objects\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'organization': organization,\n        'domain_list': organization.get_domains(),\n        'engines': engine,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'organization/schedule_scan_ui.html', context)\n\n\ndef delete_scans(request):\n    context = {}\n    if request.method == \"POST\":\n        list_of_scan_id = []\n\n        for key, value in request.POST.items():\n            if key != \"scan_history_table_length\" and key != \"csrfmiddlewaretoken\":\n                ScanHistory.objects.filter(id=value).delete()\n        messages.add_message(\n            request,\n            messages.INFO,\n            'All Scans deleted!')\n    return HttpResponseRedirect(reverse('scan_history'))\n"], "fixing_code": ["import os\nimport traceback\nimport yaml\nimport json\nimport csv\nimport validators\nimport random\nimport requests\nimport logging\nimport metafinder.extractor as metadata_extractor\nimport whatportis\nimport subprocess\n\n\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium import webdriver\nfrom emailfinder.extractor import *\nfrom dotted_dict import DottedDict\nfrom celery import shared_task\nfrom discord_webhook import DiscordWebhook\nfrom reNgine.celery import app\nfrom startScan.models import *\nfrom targetApp.models import Domain\nfrom scanEngine.models import EngineType\nfrom django.conf import settings\nfrom django.shortcuts import get_object_or_404\n\nfrom celery import shared_task\nfrom datetime import datetime\nfrom degoogle import degoogle\n\nfrom django.conf import settings\nfrom django.utils import timezone, dateformat\nfrom django.shortcuts import get_object_or_404\nfrom django.core.exceptions import ObjectDoesNotExist\n\nfrom reNgine.celery import app\nfrom reNgine.definitions import *\n\nfrom startScan.models import *\nfrom targetApp.models import Domain\nfrom scanEngine.models import EngineType, Configuration, Wordlist\n\nfrom .common_func import *\n\n'''\ntask for background scan\n'''\n\n\n@app.task\ndef initiate_scan(\n        domain_id,\n        scan_history_id,\n        scan_type,\n        engine_type,\n        imported_subdomains=None,\n        out_of_scope_subdomains=None\n        ):\n    '''\n    scan_type = 0 -> immediate scan, need not create scan object\n    scan_type = 1 -> scheduled scan\n    '''\n    engine_object = EngineType.objects.get(pk=engine_type)\n    domain = Domain.objects.get(pk=domain_id)\n    if scan_type == 1:\n        task = ScanHistory()\n        task.scan_status = -1\n    elif scan_type == 0:\n        task = ScanHistory.objects.get(pk=scan_history_id)\n\n    # save the last scan date for domain model\n    domain.last_scan_date = timezone.now()\n    domain.save()\n\n    # once the celery task starts, change the task status to Started\n    task.scan_type = engine_object\n    task.celery_id = initiate_scan.request.id\n    task.domain = domain\n    task.scan_status = 1\n    task.start_scan_date = timezone.now()\n    task.subdomain_discovery = True if engine_object.subdomain_discovery else False\n    task.dir_file_search = True if engine_object.dir_file_search else False\n    task.port_scan = True if engine_object.port_scan else False\n    task.fetch_url = True if engine_object.fetch_url else False\n    task.osint = True if engine_object.osint else False\n    task.screenshot = True if engine_object.screenshot else False\n    task.vulnerability_scan = True if engine_object.vulnerability_scan else False\n    task.save()\n\n    activity_id = create_scan_activity(task, \"Scanning Started\", 2)\n    results_dir = '/usr/src/scan_results/'\n    os.chdir(results_dir)\n\n    notification = Notification.objects.all()\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has initiated recon for target {} with engine type {}'.format(domain.name, engine_object.engine_name))\n\n    try:\n        current_scan_dir = domain.name + '_' + str(random.randint(100000000000, 999999999999))\n        os.mkdir(current_scan_dir)\n        task.results_dir = current_scan_dir\n        task.save()\n    except Exception as exception:\n        logger.error(exception)\n        scan_failed(task)\n\n    yaml_configuration = None\n    excluded_subdomains = ''\n\n    try:\n        yaml_configuration = yaml.load(\n            task.scan_type.yaml_configuration,\n            Loader=yaml.FullLoader)\n    except Exception as exception:\n        logger.error(exception)\n        # TODO: Put failed reason on db\n\n    '''\n    Add GF patterns name to db for dynamic URLs menu\n    '''\n    if engine_object.fetch_url and GF_PATTERNS in yaml_configuration[FETCH_URL]:\n        task.used_gf_patterns = ','.join(\n            pattern for pattern in yaml_configuration[FETCH_URL][GF_PATTERNS])\n        task.save()\n\n    results_dir = results_dir + current_scan_dir\n\n    # put all imported subdomains into txt file and also in Subdomain model\n    if imported_subdomains:\n        extract_imported_subdomain(\n            imported_subdomains, task, domain, results_dir)\n\n    if yaml_configuration:\n        '''\n        a target in itself is a subdomain, some tool give subdomains as\n        www.yogeshojha.com but url and everything else resolves to yogeshojha.com\n        In that case, we would already need to store target itself as subdomain\n        '''\n        initial_subdomain_file = '/target_domain.txt' if task.subdomain_discovery else '/sorted_subdomain_collection.txt'\n\n        subdomain_file = open(results_dir + initial_subdomain_file, \"w\")\n        subdomain_file.write(domain.name + \"\\n\")\n        subdomain_file.close()\n\n        if(task.subdomain_discovery):\n            activity_id = create_scan_activity(task, \"Subdomain Scanning\", 1)\n            subdomain_scan(\n                task,\n                domain,\n                yaml_configuration,\n                results_dir,\n                activity_id,\n                out_of_scope_subdomains\n                )\n        else:\n            skip_subdomain_scan(task, domain, results_dir)\n\n        update_last_activity(activity_id, 2)\n        activity_id = create_scan_activity(task, \"HTTP Crawler\", 1)\n        http_crawler(\n            task,\n            domain,\n            results_dir,\n            activity_id)\n        update_last_activity(activity_id, 2)\n\n        try:\n            if task.screenshot:\n                activity_id = create_scan_activity(\n                    task, \"Visual Recon - Screenshot\", 1)\n                grab_screenshot(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    current_scan_dir,\n                    activity_id)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if(task.port_scan):\n                activity_id = create_scan_activity(task, \"Port Scanning\", 1)\n                port_scanning(task, domain, yaml_configuration, results_dir)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if task.osint:\n                activity_id = create_scan_activity(task, \"OSINT Running\", 1)\n                perform_osint(task, domain, yaml_configuration, results_dir)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n\n        try:\n            if task.dir_file_search:\n                activity_id = create_scan_activity(task, \"Directory Search\", 1)\n                directory_brute(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    results_dir,\n                    activity_id\n                )\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if task.fetch_url:\n                activity_id = create_scan_activity(task, \"Fetching endpoints\", 1)\n                fetch_endpoints(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    results_dir,\n                    activity_id)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n        try:\n            if task.vulnerability_scan:\n                activity_id = create_scan_activity(task, \"Vulnerability Scan\", 1)\n                vulnerability_scan(\n                    task,\n                    domain,\n                    yaml_configuration,\n                    results_dir,\n                    activity_id)\n                update_last_activity(activity_id, 2)\n        except Exception as e:\n            logger.error(e)\n            update_last_activity(activity_id, 0)\n\n    activity_id = create_scan_activity(task, \"Scan Completed\", 2)\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('*Scan Completed*\\nreNgine has finished performing recon on target {}.'.format(domain.name))\n\n    '''\n    Once the scan is completed, save the status to successful\n    '''\n    if ScanActivity.objects.filter(scan_of=task).filter(status=0).all():\n        task.scan_status = 0\n    else:\n        task.scan_status = 2\n    task.stop_scan_date = timezone.now()\n    task.save()\n    # cleanup results\n    delete_scan_data(results_dir)\n    return {\"status\": True}\n\n\ndef skip_subdomain_scan(task, domain, results_dir):\n    # store default target as subdomain\n    '''\n    If the imported subdomain already has target domain saved, we can skip this\n    '''\n    if not Subdomain.objects.filter(\n            scan_history=task,\n            name=domain.name).exists():\n\n        subdomain_dict = DottedDict({\n            'name': domain.name,\n            'scan_history': task,\n            'target_domain': domain\n        })\n        save_subdomain(subdomain_dict)\n\n    # Save target into target_domain.txt\n    with open('{}/target_domain.txt'.format(results_dir), 'w+') as file:\n        file.write(domain.name + '\\n')\n\n    file.close()\n\n    '''\n    We can have two conditions, either subdomain scan happens, or subdomain scan\n    does not happen, in either cases, because we are using import subdomain, we\n    need to collect and sort all the subdomains\n\n    Write target domain into subdomain_collection\n    '''\n\n    os.system(\n        'cat {0}/target_domain.txt > {0}/subdomain_collection.txt'.format(results_dir))\n\n    os.system(\n        'cat {0}/from_imported.txt > {0}/subdomain_collection.txt'.format(results_dir))\n\n    os.system('rm -f {}/from_imported.txt'.format(results_dir))\n\n    '''\n    Sort all Subdomains\n    '''\n    os.system(\n        'sort -u {0}/subdomain_collection.txt -o {0}/sorted_subdomain_collection.txt'.format(results_dir))\n\n    os.system('rm -f {}/subdomain_collection.txt'.format(results_dir))\n\n\ndef extract_imported_subdomain(imported_subdomains, task, domain, results_dir):\n    valid_imported_subdomains = [subdomain for subdomain in imported_subdomains if validators.domain(\n        subdomain) and domain.name == get_domain_from_subdomain(subdomain)]\n\n    # remove any duplicate\n    valid_imported_subdomains = list(set(valid_imported_subdomains))\n\n    with open('{}/from_imported.txt'.format(results_dir), 'w+') as file:\n        for subdomain_name in valid_imported_subdomains:\n            # save _subdomain to Subdomain model db\n            if not Subdomain.objects.filter(\n                    scan_history=task, name=subdomain_name).exists():\n\n                subdomain_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'name': subdomain_name,\n                    'is_imported_subdomain': True\n                })\n                save_subdomain(subdomain_dict)\n                # save subdomain to file\n                file.write('{}\\n'.format(subdomain_name))\n\n    file.close()\n\n\ndef subdomain_scan(task, domain, yaml_configuration, results_dir, activity_id, out_of_scope_subdomains=None):\n    '''\n    This function is responsible for performing subdomain enumeration\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Subdomain Gathering for target {} has been started'.format(domain.name))\n\n    subdomain_scan_results_file = results_dir + '/sorted_subdomain_collection.txt'\n\n    # check for all the tools and add them into string\n    # if tool selected is all then make string, no need for loop\n    if ALL in yaml_configuration[SUBDOMAIN_DISCOVERY][USES_TOOLS]:\n        tools = 'amass-active amass-passive assetfinder sublist3r subfinder oneforall'\n    else:\n        tools = ' '.join(\n            str(tool) for tool in yaml_configuration[SUBDOMAIN_DISCOVERY][USES_TOOLS])\n\n    logging.info(tools)\n\n    # check for THREADS, by default 10\n    threads = 10\n    if THREADS in yaml_configuration[SUBDOMAIN_DISCOVERY]:\n        _threads = yaml_configuration[SUBDOMAIN_DISCOVERY][THREADS]\n        if _threads > 0:\n            threads = _threads\n\n    if 'amass' in tools:\n        if 'amass-passive' in tools:\n            amass_command = 'amass enum -passive -d {} -o {}/from_amass.txt'.format(\n                    domain.name, results_dir)\n\n            if USE_AMASS_CONFIG in yaml_configuration[SUBDOMAIN_DISCOVERY] and yaml_configuration[SUBDOMAIN_DISCOVERY][USE_AMASS_CONFIG]:\n                amass_command += ' -config /root/.config/amass.ini'\n            # Run Amass Passive\n            logging.info(amass_command)\n            os.system(amass_command)\n\n        if 'amass-active' in tools:\n            amass_command = 'amass enum -active -d {} -o {}/from_amass_active.txt'.format(\n                    domain.name, results_dir)\n\n            if USE_AMASS_CONFIG in yaml_configuration[SUBDOMAIN_DISCOVERY] and yaml_configuration[SUBDOMAIN_DISCOVERY][USE_AMASS_CONFIG]:\n                amass_command += ' -config /root/.config/amass.ini'\n\n            if AMASS_WORDLIST in yaml_configuration[SUBDOMAIN_DISCOVERY]:\n                wordlist = yaml_configuration[SUBDOMAIN_DISCOVERY][AMASS_WORDLIST]\n                if wordlist == 'default':\n                    wordlist_path = settings.TOOL_LOCATION + AMASS_DEFAULT_WORDLIST_PATH\n                else:\n                    wordlist_path = settings.TOOL_LOCATION + 'wordlist/' + wordlist + '.txt'\n                    if not os.path.exists(wordlist_path):\n                        wordlist_path = settings.TOOL_LOCATION + AMASS_WORDLIST\n                amass_command = amass_command + \\\n                    ' -brute -w {}'.format(wordlist_path)\n            if amass_config_path:\n                amass_command = amass_command + \\\n                    ' -config {}'.format(settings.TOOL_LOCATION +\n                                         'scan_results/' + amass_config_path)\n\n            # Run Amass Active\n            logging.info(amass_command)\n            os.system(amass_command)\n\n    if 'assetfinder' in tools:\n        assetfinder_command = 'assetfinder --subs-only {} > {}/from_assetfinder.txt'.format(\n            domain.name, results_dir)\n\n        # Run Assetfinder\n        logging.info(assetfinder_command)\n        os.system(assetfinder_command)\n\n    if 'sublist3r' in tools:\n        sublist3r_command = 'python3 /usr/src/github/Sublist3r/sublist3r.py -d {} -t {} -o {}/from_sublister.txt'.format(\n            domain.name, threads, results_dir)\n\n        # Run sublist3r\n        logging.info(sublist3r_command)\n        os.system(sublist3r_command)\n\n    if 'subfinder' in tools:\n        subfinder_command = 'subfinder -d {} -t {} -o {}/from_subfinder.txt'.format(\n            domain.name, threads, results_dir)\n\n        if USE_SUBFINDER_CONFIG in yaml_configuration[SUBDOMAIN_DISCOVERY] and yaml_configuration[SUBDOMAIN_DISCOVERY][USE_SUBFINDER_CONFIG]:\n            subfinder_command += ' -config /root/.config/subfinder/config.yaml'\n\n        # Run Subfinder\n        logging.info(subfinder_command)\n        os.system(subfinder_command)\n\n    if 'oneforall' in tools:\n        oneforall_command = 'python3 /usr/src/github/OneForAll/oneforall.py --target {} run'.format(\n            domain.name, results_dir)\n\n        # Run OneForAll\n        logging.info(oneforall_command)\n        os.system(oneforall_command)\n\n        extract_subdomain = \"cut -d',' -f6 /usr/src/github/OneForAll/results/{}.csv >> {}/from_oneforall.txt\".format(\n            domain.name, results_dir)\n\n        os.system(extract_subdomain)\n\n        # remove the results from oneforall directory\n        os.system(\n            'rm -rf /usr/src/github/OneForAll/results/{}.*'.format(domain.name))\n\n    '''\n    All tools have gathered the list of subdomains with filename\n    initials as from_*\n    We will gather all the results in one single file, sort them and\n    remove the older results from_*\n    '''\n    os.system(\n        'cat {0}/*.txt > {0}/subdomain_collection.txt'.format(results_dir))\n\n    '''\n    Write target domain into subdomain_collection\n    '''\n    os.system(\n        'cat {0}/target_domain.txt >> {0}/subdomain_collection.txt'.format(results_dir))\n\n    '''\n    Remove all the from_* files\n    '''\n    os.system('rm -f {}/from*'.format(results_dir))\n\n    '''\n    Sort all Subdomains\n    '''\n    os.system(\n        'sort -u {0}/subdomain_collection.txt -o {0}/sorted_subdomain_collection.txt'.format(results_dir))\n\n    os.system('rm -f {}/subdomain_collection.txt'.format(results_dir))\n\n    '''\n    The final results will be stored in sorted_subdomain_collection.\n    '''\n    # parse the subdomain list file and store in db\n    with open(subdomain_scan_results_file) as subdomain_list:\n        for _subdomain in subdomain_list:\n            __subdomain = _subdomain.rstrip('\\n')\n            if not Subdomain.objects.filter(scan_history=task, name=__subdomain).exists(\n            ) and validators.domain(__subdomain) and __subdomain not in out_of_scope_subdomains:\n                subdomain_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'name': __subdomain,\n                })\n                save_subdomain(subdomain_dict)\n\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        subdomains_count = Subdomain.objects.filter(scan_history=task).count()\n        send_notification('Subdomain Gathering for target {} has been completed and has discovered *{}* subdomains.'.format(domain.name, subdomains_count))\n    if notification and notification[0].send_scan_output_file:\n        send_files_to_discord(results_dir + '/sorted_subdomain_collection.txt')\n\n    # check for any subdomain changes and send notif if any\n    if notification and notification[0].send_subdomain_changes_notif:\n        newly_added_subdomain = get_new_added_subdomain(task.id, domain.id)\n        if newly_added_subdomain:\n            message = \"**{} New Subdomains Discovered on domain {}**\".format(newly_added_subdomain.count(), domain.name)\n            for subdomain in newly_added_subdomain:\n                message += \"\\n\u2022 {}\".format(subdomain.name)\n            send_notification(message)\n\n        removed_subdomain = get_removed_subdomain(task.id, domain.id)\n        if removed_subdomain:\n            message = \"**{} Subdomains are no longer available on domain {}**\".format(removed_subdomain.count(), domain.name)\n            for subdomain in removed_subdomain:\n                message += \"\\n\u2022 {}\".format(subdomain.name)\n            send_notification(message)\n\n    # check for interesting subdomains and send notif if any\n    if notification and notification[0].send_interesting_notif:\n        interesting_subdomain = get_interesting_subdomains(task.id, domain.id)\n        print(interesting_subdomain)\n        if interesting_subdomain:\n            message = \"**{} Interesting Subdomains Found on domain {}**\".format(interesting_subdomain.count(), domain.name)\n            for subdomain in interesting_subdomain:\n                message += \"\\n\u2022 {}\".format(subdomain.name)\n            send_notification(message)\n\n\ndef get_new_added_subdomain(scan_id, domain_id):\n    scan_history = ScanHistory.objects.filter(\n        domain=domain_id).filter(\n            subdomain_discovery=True).filter(\n                id__lte=scan_id)\n    if scan_history.count() > 1:\n        last_scan = scan_history.order_by('-start_scan_date')[1]\n        scanned_host_q1 = Subdomain.objects.filter(\n            scan_history__id=scan_id).values('name')\n        scanned_host_q2 = Subdomain.objects.filter(\n            scan_history__id=last_scan.id).values('name')\n        added_subdomain = scanned_host_q1.difference(scanned_host_q2)\n\n        return Subdomain.objects.filter(\n            scan_history=scan_id).filter(\n                name__in=added_subdomain)\n\ndef get_removed_subdomain(scan_id, domain_id):\n    scan_history = ScanHistory.objects.filter(\n        domain=domain_id).filter(\n            subdomain_discovery=True).filter(\n                id__lte=scan_id)\n    if scan_history.count() > 1:\n        last_scan = scan_history.order_by('-start_scan_date')[1]\n        scanned_host_q1 = Subdomain.objects.filter(\n            scan_history__id=scan_id).values('name')\n        scanned_host_q2 = Subdomain.objects.filter(\n            scan_history__id=last_scan.id).values('name')\n        removed_subdomains = scanned_host_q2.difference(scanned_host_q1)\n\n        print()\n\n        return Subdomain.objects.filter(\n            scan_history=last_scan).filter(\n                name__in=removed_subdomains)\n\n\ndef http_crawler(task, domain, results_dir, activity_id):\n    '''\n    This function is runs right after subdomain gathering, and gathers important\n    like page title, http status, etc\n    HTTP Crawler runs by default\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('HTTP Crawler for target {} has been initiated.'.format(domain.name))\n\n    alive_file_location = results_dir + '/alive.txt'\n    httpx_results_file = results_dir + '/httpx.json'\n\n    subdomain_scan_results_file = results_dir + '/sorted_subdomain_collection.txt'\n    httpx_command = 'httpx -status-code -content-length -title -tech-detect -cdn -ip -follow-host-redirects -random-agent'\n\n    proxy = get_random_proxy()\n    if proxy:\n        httpx_command += ' --http-proxy {}'.format(proxy)\n\n    httpx_command += ' -json -o {}'.format(\n        httpx_results_file\n    )\n    httpx_command = 'cat {} | {}'.format(subdomain_scan_results_file, httpx_command)\n    print(httpx_command)\n    os.system(httpx_command)\n\n    # alive subdomains from httpx\n    alive_file = open(alive_file_location, 'w')\n\n    # writing httpx results\n    if os.path.isfile(httpx_results_file):\n        httpx_json_result = open(httpx_results_file, 'r')\n        lines = httpx_json_result.readlines()\n        for line in lines:\n            json_st = json.loads(line.strip())\n            try:\n                subdomain = Subdomain.objects.get(\n                    scan_history=task, name=json_st['url'].split(\"//\")[-1])\n                '''\n                Saving Default http urls to EndPoint\n                '''\n                endpoint = EndPoint()\n                endpoint.scan_history = task\n                endpoint.target_domain = domain\n                endpoint.subdomain = subdomain\n                if 'url' in json_st:\n                    endpoint.http_url = json_st['url']\n                    subdomain.http_url = json_st['url']\n                if 'status-code' in json_st:\n                    endpoint.http_status = json_st['status-code']\n                    subdomain.http_status = json_st['status-code']\n                if 'title' in json_st:\n                    endpoint.page_title = json_st['title']\n                    subdomain.page_title = json_st['title']\n                if 'content-length' in json_st:\n                    endpoint.content_length = json_st['content-length']\n                    subdomain.content_length = json_st['content-length']\n                if 'content-type' in json_st:\n                    endpoint.content_type = json_st['content-type']\n                    subdomain.content_type = json_st['content-type']\n                if 'webserver' in json_st:\n                    endpoint.webserver = json_st['webserver']\n                    subdomain.webserver = json_st['webserver']\n                if 'response-time' in json_st:\n                    response_time = float(\n                        ''.join(\n                            ch for ch in json_st['response-time'] if not ch.isalpha()))\n                    if json_st['response-time'][-2:] == 'ms':\n                        response_time = response_time / 1000\n                    endpoint.response_time = response_time\n                    subdomain.response_time = response_time\n                if 'cnames' in json_st:\n                    cname_list = ','.join(json_st['cnames'])\n                    subdomain.cname = cname_list\n                discovered_date = timezone.now()\n                endpoint.discovered_date = discovered_date\n                subdomain.discovered_date = discovered_date\n                endpoint.is_default = True\n                endpoint.save()\n                subdomain.save()\n                if 'technologies' in json_st:\n                    for _tech in json_st['technologies']:\n                        if Technology.objects.filter(name=_tech).exists():\n                            tech = Technology.objects.get(name=_tech)\n                        else:\n                            tech = Technology(name=_tech)\n                            tech.save()\n                        subdomain.technologies.add(tech)\n                        endpoint.technologies.add(tech)\n                if 'a' in json_st:\n                    for _ip in json_st['a']:\n                        if IpAddress.objects.filter(address=_ip).exists():\n                            ip = IpAddress.objects.get(address=_ip)\n                        else:\n                            ip = IpAddress(address=_ip)\n                            if 'cdn' in json_st:\n                                ip.is_cdn = json_st['cdn']\n                            ip.save()\n                        subdomain.ip_addresses.add(ip)\n                # see if to ignore 404 or 5xx\n                alive_file.write(json_st['url'] + '\\n')\n                subdomain.save()\n                endpoint.save()\n            except Exception as exception:\n                logging.error(exception)\n    alive_file.close()\n\n    if notification and notification[0].send_scan_status_notif:\n        alive_count = Subdomain.objects.filter(\n            scan_history__id=task.id).values('name').distinct().filter(\n            http_status__exact=200).count()\n        send_notification('HTTP Crawler for target {} has been completed.\\n\\n {} subdomains were alive (http status 200).'.format(domain.name, alive_count))\n\n\ndef grab_screenshot(task, domain, yaml_configuration, results_dir, activity_id):\n    '''\n    This function is responsible for taking screenshots\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine is currently gathering screenshots for {}'.format(domain.name))\n\n    output_screenshots_path = results_dir + '/screenshots'\n    result_csv_path = results_dir + '/screenshots/Requests.csv'\n    alive_subdomains_path = results_dir + '/alive.txt'\n\n    eyewitness_command = 'python3 /usr/src/github/EyeWitness/Python/EyeWitness.py'\n\n    eyewitness_command += ' -f {} -d {} --no-prompt'.format(\n        alive_subdomains_path,\n        output_screenshots_path\n    )\n\n    if EYEWITNESS in yaml_configuration \\\n        and TIMEOUT in yaml_configuration[EYEWITNESS] \\\n        and yaml_configuration[EYEWITNESS][TIMEOUT] > 0:\n        eyewitness_command += ' --timeout {}'.format(\n            yaml_configuration[EYEWITNESS][TIMEOUT]\n        )\n\n    if EYEWITNESS in yaml_configuration \\\n        and THREADS in yaml_configuration[EYEWITNESS] \\\n        and yaml_configuration[EYEWITNESS][THREADS] > 0:\n            eyewitness_command += ' --threads {}'.format(\n                yaml_configuration[EYEWITNESS][THREADS]\n            )\n\n    logger.info(eyewitness_command)\n\n    os.system(eyewitness_command)\n\n    if os.path.isfile(result_csv_path):\n        logger.info('Gathering Eyewitness results')\n        with open(result_csv_path, 'r') as file:\n            reader = csv.reader(file)\n            for row in reader:\n                if row[3] == 'Successful' \\\n                    and Subdomain.objects.filter(\n                        scan_history__id=task.id).filter(name=row[2]).exists():\n                    subdomain = Subdomain.objects.get(\n                        scan_history__id=task.id,\n                        name=row[2]\n                    )\n                    subdomain.screenshot_path = row[4].replace(\n                        '/usr/src/scan_results/',\n                        ''\n                    )\n                    subdomain.save()\n\n    # remove all db, html extra files in screenshot results\n    os.system('rm -rf {0}/*.csv {0}/*.db {0}/*.js {0}/*.html {0}/*.css'.format(\n        output_screenshots_path,\n    ))\n    os.system('rm -rf {0}/source'.format(\n        output_screenshots_path,\n    ))\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has finished gathering screenshots for {}'.format(domain.name))\n\n\ndef port_scanning(task, domain, yaml_configuration, results_dir):\n    '''\n    This function is responsible for running the port scan\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Port Scan initiated for {}'.format(domain.name))\n\n    subdomain_scan_results_file = results_dir + '/sorted_subdomain_collection.txt'\n    port_results_file = results_dir + '/ports.json'\n\n    # check the yaml_configuration and choose the ports to be scanned\n\n    scan_ports = '-'  # default port scan everything\n    if PORTS in yaml_configuration[PORT_SCAN]:\n        # TODO:  legacy code, remove top-100 in future versions\n        all_ports = yaml_configuration[PORT_SCAN][PORTS]\n        if 'full' in all_ports:\n            naabu_command = 'cat {} | naabu -json -o {} -p {}'.format(\n                subdomain_scan_results_file, port_results_file, '-')\n        elif 'top-100' in all_ports:\n            naabu_command = 'cat {} | naabu -json -o {} -top-ports 100'.format(\n                subdomain_scan_results_file, port_results_file)\n        elif 'top-1000' in all_ports:\n            naabu_command = 'cat {} | naabu -json -o {} -top-ports 1000'.format(\n                subdomain_scan_results_file, port_results_file)\n        else:\n            scan_ports = ','.join(\n                str(port) for port in all_ports)\n            naabu_command = 'cat {} | naabu -json -o {} -p {}'.format(\n                subdomain_scan_results_file, port_results_file, scan_ports)\n\n    # check for exclude ports\n    if EXCLUDE_PORTS in yaml_configuration[PORT_SCAN] and yaml_configuration[PORT_SCAN][EXCLUDE_PORTS]:\n        exclude_ports = ','.join(\n            str(port) for port in yaml_configuration['port_scan']['exclude_ports'])\n        naabu_command = naabu_command + \\\n            ' -exclude-ports {}'.format(exclude_ports)\n\n    if NAABU_RATE in yaml_configuration[PORT_SCAN] and yaml_configuration[PORT_SCAN][NAABU_RATE] > 0:\n        naabu_command = naabu_command + \\\n            ' -rate {}'.format(\n                yaml_configuration[PORT_SCAN][NAABU_RATE])\n    else:\n        naabu_command = naabu_command + ' -t 10'\n\n    if USE_NAABU_CONFIG in yaml_configuration[PORT_SCAN] and yaml_configuration[PORT_SCAN][USE_NAABU_CONFIG]:\n        naabu_command += ' -config /root/.config/naabu/naabu.conf'\n\n    # run naabu\n    os.system(naabu_command)\n\n    # writing port results\n    try:\n        port_json_result = open(port_results_file, 'r')\n        lines = port_json_result.readlines()\n        for line in lines:\n            json_st = json.loads(line.strip())\n            port_number = json_st['port']\n            ip_address = json_st['ip']\n\n            # see if port already exists\n            if Port.objects.filter(number__exact=port_number).exists():\n                port = Port.objects.get(number=port_number)\n            else:\n                port = Port()\n                port.number = port_number\n            if port_number in UNCOMMON_WEB_PORTS:\n                port.is_uncommon = True\n            port_detail = whatportis.get_ports(str(port_number))\n            if len(port_detail):\n                port.service_name = port_detail[0].name\n                port.description = port_detail[0].description\n            port.save()\n            if IpAddress.objects.filter(address=json_st['ip']).exists():\n                ip = IpAddress.objects.get(address=json_st['ip'])\n                ip.ports.add(port)\n                ip.save()\n    except BaseException as exception:\n        logging.error(exception)\n        update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        port_count = Port.objects.filter(\n            ports__in=IpAddress.objects.filter(\n                ip_addresses__in=Subdomain.objects.filter(\n                    scan_history__id=task.id))).distinct().count()\n        send_notification('reNgine has finished Port Scanning on {} and has identified {} ports.'.format(domain.name, port_count))\n\n    if notification and notification[0].send_scan_output_file:\n        send_files_to_discord(results_dir + '/ports.json')\n\n\ndef check_waf():\n    '''\n    This function will check for the WAF being used in subdomains using wafw00f\n    '''\n    pass\n\n\ndef directory_brute(task, domain, yaml_configuration, results_dir, activity_id):\n    '''\n    This function is responsible for performing directory scan\n    '''\n    # scan directories for all the alive subdomain with http status >\n    # 200\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Directory Bruteforce has been initiated for {}.'.format(domain.name))\n\n    alive_subdomains = Subdomain.objects.filter(\n        scan_history__id=task.id).exclude(\n        http_url='')\n    dirs_results = results_dir + '/dirs.json'\n\n    # check the yaml settings\n    if EXTENSIONS in yaml_configuration[DIR_FILE_SEARCH]:\n        extensions = ','.join(\n            str(ext) for ext in yaml_configuration[DIR_FILE_SEARCH][EXTENSIONS])\n    else:\n        extensions = 'php,git,yaml,conf,db,mysql,bak,txt'\n\n    # Threads\n    if THREADS in yaml_configuration[DIR_FILE_SEARCH] \\\n        and yaml_configuration[DIR_FILE_SEARCH][THREADS] > 0:\n        threads = yaml_configuration[DIR_FILE_SEARCH][THREADS]\n    else:\n        threads = 10\n\n    for subdomain in alive_subdomains:\n        # delete any existing dirs.json\n        if os.path.isfile(dirs_results):\n            os.system('rm -rf {}'.format(dirs_results))\n        dirsearch_command = 'python3 /usr/src/github/dirsearch/dirsearch.py'\n\n        dirsearch_command += ' -u {}'.format(subdomain.http_url)\n\n        if (WORDLIST not in yaml_configuration[DIR_FILE_SEARCH] or\n            not yaml_configuration[DIR_FILE_SEARCH][WORDLIST] or\n                'default' in yaml_configuration[DIR_FILE_SEARCH][WORDLIST]):\n            wordlist_location = '/usr/src/github/dirsearch/db/dicc.txt'\n        else:\n            wordlist_location = settings.TOOL_LOCATION + 'wordlist/' + \\\n                yaml_configuration[DIR_FILE_SEARCH][WORDLIST] + '.txt'\n\n        dirsearch_command += ' -w {}'.format(wordlist_location)\n\n        dirsearch_command += ' --format json -o {}'.format(dirs_results)\n\n        dirsearch_command += ' -e {}'.format(extensions)\n\n        dirsearch_command += ' -t {}'.format(threads)\n\n        dirsearch_command += ' --random-agent'\n\n        if EXCLUDE_EXTENSIONS in yaml_configuration[DIR_FILE_SEARCH]:\n            exclude_extensions = ','.join(\n                str(ext) for ext in yaml_configuration[DIR_FILE_SEARCH][EXCLUDE_EXTENSIONS])\n            dirsearch_command += ' -X {}'.format(exclude_extensions)\n\n        # check if recursive strategy is set to on\n        if RECURSIVE in yaml_configuration[DIR_FILE_SEARCH] and yaml_configuration[DIR_FILE_SEARCH][RECURSIVE]:\n            dirsearch_command += ' -r'\n\n        if RECURSIVE_LEVEL in yaml_configuration[DIR_FILE_SEARCH]:\n            dirsearch_command += ' --recursion-depth {}'.format(yaml_configuration[DIR_FILE_SEARCH][RECURSIVE_LEVEL])\n\n        # proxy\n        proxy = get_random_proxy()\n        if proxy:\n            dirsearch_command += ' --proxy {}'.format(proxy)\n\n        print(dirsearch_command)\n        os.system(dirsearch_command)\n\n        try:\n            if os.path.isfile(dirs_results):\n                with open(dirs_results, \"r\") as json_file:\n                    json_string = json_file.read()\n                    scanned_host = Subdomain.objects.get(\n                        scan_history__id=task.id, http_url=subdomain.http_url)\n                    scanned_host.directory_json = json_string\n                    scanned_host.save()\n        except Exception as exception:\n            logging.error(exception)\n            update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Directory Bruteforce has been completed for {}.'.format(domain.name))\n\n\ndef fetch_endpoints(\n        task,\n        domain,\n        yaml_configuration,\n        results_dir,\n        activity_id):\n    '''\n    This function is responsible for fetching all the urls associated with target\n    and run HTTP probe\n    It first runs gau to gather all urls from wayback, then we will use hakrawler to identify more urls\n    '''\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine is currently gathering endpoints for {}.'.format(domain.name))\n\n    # check yaml settings\n    if ALL in yaml_configuration[FETCH_URL][USES_TOOLS]:\n        tools = 'gauplus hakrawler waybackurls gospider'\n    else:\n        tools = ' '.join(\n            str(tool) for tool in yaml_configuration[FETCH_URL][USES_TOOLS])\n\n    if INTENSITY in yaml_configuration[FETCH_URL]:\n        scan_type = yaml_configuration[FETCH_URL][INTENSITY]\n    else:\n        scan_type = 'normal'\n\n    domain_regex = \"\\'https?://([a-z0-9]+[.])*{}.*\\'\".format(domain.name)\n\n    if 'deep' in scan_type:\n        # performs deep url gathering for all the subdomains present -\n        # RECOMMENDED\n        logger.info('Deep URLS Fetch')\n        os.system(settings.TOOL_LOCATION + 'get_urls.sh %s %s %s %s %s' %\n            (\"None\", results_dir, scan_type, domain_regex, tools))\n    else:\n        # perform url gathering only for main domain - USE only for quick scan\n        logger.info('Non Deep URLS Fetch')\n        os.system(\n            settings.TOOL_LOCATION +\n            'get_urls.sh %s %s %s %s %s' % (\n                domain.name,\n                results_dir,\n                scan_type,\n                domain_regex,\n                tools\n            ))\n\n    if IGNORE_FILE_EXTENSION in yaml_configuration[FETCH_URL]:\n        ignore_extension = '|'.join(\n            yaml_configuration[FETCH_URL][IGNORE_FILE_EXTENSION])\n        logger.info('Ignore extensions' + ignore_extension)\n        os.system(\n            'cat {0}/all_urls.txt | grep -Eiv \"\\\\.({1}).*\" > {0}/temp_urls.txt'.format(\n                results_dir, ignore_extension))\n        os.system(\n            'rm {0}/all_urls.txt && mv {0}/temp_urls.txt {0}/all_urls.txt'.format(results_dir))\n\n    '''\n    Store all the endpoints and then run the httpx\n    '''\n    try:\n        endpoint_final_url = results_dir + '/all_urls.txt'\n        if os.path.isfile(endpoint_final_url):\n            with open(endpoint_final_url) as endpoint_list:\n                for url in endpoint_list:\n                    http_url = url.rstrip('\\n')\n                    if not EndPoint.objects.filter(scan_history=task, http_url=http_url).exists():\n                        _subdomain = get_subdomain_from_url(http_url)\n                        if Subdomain.objects.filter(\n                                scan_history=task).filter(\n                                name=_subdomain).exists():\n                            subdomain = Subdomain.objects.get(\n                                scan_history=task, name=_subdomain)\n                        else:\n                            '''\n                            gau or gosppider can gather interesting endpoints which\n                            when parsed can give subdomains that were not existent from\n                            subdomain scan. so storing them\n                            '''\n                            logger.error(\n                                'Subdomain {} not found, adding...'.format(_subdomain))\n                            subdomain_dict = DottedDict({\n                                'scan_history': task,\n                                'target_domain': domain,\n                                'name': _subdomain,\n                            })\n                            subdomain = save_subdomain(subdomain_dict)\n                        endpoint_dict = DottedDict({\n                            'scan_history': task,\n                            'target_domain': domain,\n                            'subdomain': subdomain,\n                            'http_url': http_url,\n                        })\n                        save_endpoint(endpoint_dict)\n    except Exception as e:\n        logger.error(e)\n\n    if notification and notification[0].send_scan_output_file:\n        send_files_to_discord(results_dir + '/all_urls.txt')\n\n    '''\n    TODO:\n    Go spider & waybackurls accumulates a lot of urls, which is good but nuclei\n    takes forever to scan even a simple website, so we will do http probing\n    and filter HTTP status 404, this way we can reduce the number of Non Existent\n    URLS\n    '''\n    logger.info('HTTP Probing on collected endpoints')\n\n    httpx_command = 'httpx -l {0}/all_urls.txt -status-code -content-length -ip -cdn -title -tech-detect -json -follow-redirects -random-agent -o {0}/final_httpx_urls.json'.format(results_dir)\n\n    proxy = get_random_proxy()\n    if proxy:\n        httpx_command += ' --http-proxy {}'.format(proxy)\n\n    os.system(httpx_command)\n\n    url_results_file = results_dir + '/final_httpx_urls.json'\n    try:\n        urls_json_result = open(url_results_file, 'r')\n        lines = urls_json_result.readlines()\n        for line in lines:\n            json_st = json.loads(line.strip())\n            http_url = json_st['url']\n            _subdomain = get_subdomain_from_url(http_url)\n\n            if Subdomain.objects.filter(\n                    scan_history=task).filter(\n                    name=_subdomain).exists():\n                subdomain_obj = Subdomain.objects.get(\n                    scan_history=task, name=_subdomain)\n            else:\n                subdomain_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'name': _subdomain,\n                })\n                subdomain_obj = save_subdomain(subdomain_dict)\n\n            if EndPoint.objects.filter(\n                    scan_history=task).filter(\n                    http_url=http_url).exists():\n\n                endpoint = EndPoint.objects.get(\n                    scan_history=task, http_url=http_url)\n            else:\n                endpoint = EndPoint()\n                endpoint_dict = DottedDict({\n                    'scan_history': task,\n                    'target_domain': domain,\n                    'http_url': http_url,\n                    'subdomain': subdomain_obj\n                })\n                endpoint = save_endpoint(endpoint_dict)\n\n            if 'title' in json_st:\n                endpoint.page_title = json_st['title']\n            if 'webserver' in json_st:\n                endpoint.webserver = json_st['webserver']\n            if 'content-length' in json_st:\n                endpoint.content_length = json_st['content-length']\n            if 'content-type' in json_st:\n                endpoint.content_type = json_st['content-type']\n            if 'status-code' in json_st:\n                endpoint.http_status = json_st['status-code']\n            if 'response-time' in json_st:\n                response_time = float(''.join(ch for ch in json_st['response-time'] if not ch.isalpha()))\n                if json_st['response-time'][-2:] == 'ms':\n                    response_time = response_time / 1000\n                endpoint.response_time = response_time\n            endpoint.save()\n            if 'technologies' in json_st:\n                for _tech in json_st['technologies']:\n                    if Technology.objects.filter(name=_tech).exists():\n                        tech = Technology.objects.get(name=_tech)\n                    else:\n                        tech = Technology(name=_tech)\n                        tech.save()\n                    endpoint.technologies.add(tech)\n                    # get subdomain object\n                    subdomain = Subdomain.objects.get(scan_history=task, name=_subdomain)\n                    subdomain.technologies.add(tech)\n                    subdomain.save()\n    except Exception as exception:\n        logging.error(exception)\n        update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        endpoint_count = EndPoint.objects.filter(\n            scan_history__id=task.id).values('http_url').distinct().count()\n        endpoint_alive_count = EndPoint.objects.filter(\n                scan_history__id=task.id, http_status__exact=200).values('http_url').distinct().count()\n        send_notification('reNgine has finished gathering endpoints for {} and has discovered *{}* unique endpoints.\\n\\n{} of those endpoints reported HTTP status 200.'.format(\n            domain.name,\n            endpoint_count,\n            endpoint_alive_count\n        ))\n\n\n    # once endpoint is saved, run gf patterns TODO: run threads\n    if GF_PATTERNS in yaml_configuration[FETCH_URL]:\n        for pattern in yaml_configuration[FETCH_URL][GF_PATTERNS]:\n            logger.info('Running GF for {}'.format(pattern))\n            gf_output_file_path = '{0}/gf_patterns_{1}.txt'.format(\n                results_dir, pattern)\n            gf_command = 'cat {0}/all_urls.txt | gf {1} >> {2}'.format(\n                results_dir, pattern, gf_output_file_path)\n            os.system(gf_command)\n            if os.path.exists(gf_output_file_path):\n                with open(gf_output_file_path) as gf_output:\n                    for line in gf_output:\n                        url = line.rstrip('\\n')\n                        try:\n                            endpoint = EndPoint.objects.get(\n                                scan_history=task, http_url=url)\n                            earlier_pattern = endpoint.matched_gf_patterns\n                            new_pattern = earlier_pattern + ',' + pattern if earlier_pattern else pattern\n                            endpoint.matched_gf_patterns = new_pattern\n                        except Exception as e:\n                            # add the url in db\n                            logger.error(e)\n                            logger.info('Adding URL' + url)\n                            endpoint = EndPoint()\n                            endpoint.http_url = url\n                            endpoint.target_domain = domain\n                            endpoint.scan_history = task\n                            try:\n                                _subdomain = Subdomain.objects.get(\n                                    scan_history=task, name=get_subdomain_from_url(url))\n                                endpoint.subdomain = _subdomain\n                            except Exception as e:\n                                continue\n                            endpoint.matched_gf_patterns = pattern\n                        finally:\n                            endpoint.save()\n\n\ndef vulnerability_scan(\n        task,\n        domain,\n        yaml_configuration,\n        results_dir,\n        activity_id):\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('Vulnerability scan has been initiated for {}.'.format(domain.name))\n    '''\n    This function will run nuclei as a vulnerability scanner\n    ----\n    unfurl the urls to keep only domain and path, this will be sent to vuln scan\n    ignore certain file extensions\n    Thanks: https://github.com/six2dez/reconftw\n    '''\n    urls_path = '/alive.txt'\n    if task.scan_type.fetch_url:\n        os.system('cat {0}/all_urls.txt | grep -Eiv \"\\\\.(eot|jpg|jpeg|gif|css|tif|tiff|png|ttf|otf|woff|woff2|ico|pdf|svg|txt|js|doc|docx)$\" | unfurl -u format %s://%d%p >> {0}/unfurl_urls.txt'.format(results_dir))\n        os.system(\n            'sort -u {0}/unfurl_urls.txt -o {0}/unfurl_urls.txt'.format(results_dir))\n        urls_path = '/unfurl_urls.txt'\n\n    vulnerability_result_path = results_dir + '/vulnerability.json'\n\n    vulnerability_scan_input_file = results_dir + urls_path\n\n    nuclei_command = 'nuclei -json -l {} -o {}'.format(\n        vulnerability_scan_input_file, vulnerability_result_path)\n\n    # check nuclei config\n    if USE_NUCLEI_CONFIG in yaml_configuration[VULNERABILITY_SCAN] and yaml_configuration[VULNERABILITY_SCAN][USE_NUCLEI_CONFIG]:\n        nuclei_command += ' -config /root/.config/nuclei/config.yaml'\n\n    '''\n    Nuclei Templates\n    Either custom template has to be supplied or default template, if neither has\n    been supplied then use all templates including custom templates\n    '''\n\n    if CUSTOM_NUCLEI_TEMPLATE in yaml_configuration[\n            VULNERABILITY_SCAN] or NUCLEI_TEMPLATE in yaml_configuration[VULNERABILITY_SCAN]:\n        # check yaml settings for templates\n        if NUCLEI_TEMPLATE in yaml_configuration[VULNERABILITY_SCAN]:\n            if ALL in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_TEMPLATE]:\n                template = NUCLEI_TEMPLATES_PATH\n            else:\n                _template = ','.join([NUCLEI_TEMPLATES_PATH + str(element)\n                                      for element in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_TEMPLATE]])\n                template = _template.replace(',', ' -t ')\n\n            # Update nuclei command with templates\n            nuclei_command = nuclei_command + ' -t ' + template\n\n        if CUSTOM_NUCLEI_TEMPLATE in yaml_configuration[VULNERABILITY_SCAN]:\n            # add .yaml to the custom template extensions\n            _template = ','.join(\n                [str(element) + '.yaml' for element in yaml_configuration[VULNERABILITY_SCAN][CUSTOM_NUCLEI_TEMPLATE]])\n            template = _template.replace(',', ' -t ')\n            # Update nuclei command with templates\n            nuclei_command = nuclei_command + ' -t ' + template\n    else:\n        nuclei_command = nuclei_command + ' -t /root/nuclei-templates'\n\n    # check yaml settings for  concurrency\n    if NUCLEI_CONCURRENCY in yaml_configuration[VULNERABILITY_SCAN] and yaml_configuration[\n            VULNERABILITY_SCAN][NUCLEI_CONCURRENCY] > 0:\n        concurrency = yaml_configuration[VULNERABILITY_SCAN][NUCLEI_CONCURRENCY]\n        # Update nuclei command with concurrent\n        nuclei_command = nuclei_command + ' -c ' + str(concurrency)\n\n    # for severity\n    if NUCLEI_SEVERITY in yaml_configuration[VULNERABILITY_SCAN] and ALL not in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_SEVERITY]:\n        _severity = ','.join(\n            [str(element) for element in yaml_configuration[VULNERABILITY_SCAN][NUCLEI_SEVERITY]])\n        severity = _severity.replace(\" \", \"\")\n    else:\n        severity = \"critical, high, medium, low, info\"\n\n    # update nuclei templates before running scan\n    os.system('nuclei -update-templates')\n\n    for _severity in severity.split(\",\"):\n        # delete any existing vulnerability.json file\n        if os.path.isfile(vulnerability_result_path):\n            os.system('rm {}'.format(vulnerability_result_path))\n        # run nuclei\n        final_nuclei_command = nuclei_command + ' -severity ' + _severity\n        proxy = get_random_proxy()\n        if proxy:\n            final_nuclei_command += ' --proxy-url {}'.format(proxy)\n\n        logger.info(final_nuclei_command)\n\n        os.system(final_nuclei_command)\n        try:\n            if os.path.isfile(vulnerability_result_path):\n                urls_json_result = open(vulnerability_result_path, 'r')\n                lines = urls_json_result.readlines()\n                for line in lines:\n                    json_st = json.loads(line.strip())\n                    host = json_st['host']\n                    _subdomain = get_subdomain_from_url(host)\n                    try:\n                        subdomain = Subdomain.objects.get(\n                            name=_subdomain, scan_history=task)\n                        vulnerability = Vulnerability()\n                        vulnerability.subdomain = subdomain\n                        vulnerability.scan_history = task\n                        vulnerability.target_domain = domain\n                        try:\n                            endpoint = EndPoint.objects.get(\n                                scan_history=task, target_domain=domain, http_url=host)\n                            vulnerability.endpoint = endpoint\n                        except Exception as exception:\n                            logger.error(exception)\n                        if 'name' in json_st['info']:\n                            vulnerability.name = json_st['info']['name']\n                        if 'severity' in json_st['info']:\n                            if json_st['info']['severity'] == 'info':\n                                severity = 0\n                            elif json_st['info']['severity'] == 'low':\n                                severity = 1\n                            elif json_st['info']['severity'] == 'medium':\n                                severity = 2\n                            elif json_st['info']['severity'] == 'high':\n                                severity = 3\n                            elif json_st['info']['severity'] == 'critical':\n                                severity = 4\n                            else:\n                                severity = 0\n                        else:\n                            severity = 0\n                        vulnerability.severity = severity\n                        if 'tags' in json_st['info']:\n                            vulnerability.tags = json_st['info']['tags']\n                        if 'description' in json_st['info']:\n                            vulnerability.description = json_st['info']['description']\n                        if 'reference' in json_st['info']:\n                            vulnerability.reference = json_st['info']['reference']\n                        if 'matched' in json_st:\n                            vulnerability.http_url = json_st['matched']\n                        if 'templateID' in json_st:\n                            vulnerability.template_used = json_st['templateID']\n                        if 'description' in json_st:\n                            vulnerability.description = json_st['description']\n                        if 'matcher_name' in json_st:\n                            vulnerability.matcher_name = json_st['matcher_name']\n                        if 'extracted_results' in json_st:\n                            vulnerability.extracted_results = json_st['extracted_results']\n                        vulnerability.discovered_date = timezone.now()\n                        vulnerability.open_status = True\n                        vulnerability.save()\n                        # send notification for all vulnerabilities except info\n                        if  severity != \"info\" and notification and notification[0].send_vuln_notif:\n                            message = \"*Alert: Vulnerability Identified*\"\n                            message += \"\\n\\n\"\n                            message += \"A *{}* severity vulnerability has been identified.\".format(json_st['info']['severity'])\n                            message += \"\\nVulnerability Name: {}\".format(json_st['info']['name'])\n                            message += \"\\nVulnerable URL: {}\".format(json_st['host'])\n                            send_notification(message)\n\n                        # send report to hackerone\n                        if Hackerone.objects.all().exists() and severity != 'info' and severity \\\n                            != 'low' and vulnerability.target_domain.h1_team_handle:\n                            hackerone = Hackerone.objects.all()[0]\n\n                            if hackerone.send_critical and severity == 'critical':\n                                send_hackerone_report(vulnerability.id)\n                            elif hackerone.send_high and severity == 'high':\n                                send_hackerone_report(vulnerability.id)\n                            elif hackerone.send_medium and severity == 'medium':\n                                send_hackerone_report(vulnerability.id)\n\n                    except ObjectDoesNotExist:\n                        logger.error('Object not found')\n                        continue\n\n        except Exception as exception:\n            logging.error(exception)\n            update_last_activity(activity_id, 0)\n\n    if notification and notification[0].send_scan_status_notif:\n        info_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=0).count()\n        low_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=1).count()\n        medium_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=2).count()\n        high_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=3).count()\n        critical_count = Vulnerability.objects.filter(\n            scan_history__id=task.id, severity=4).count()\n        vulnerability_count = info + low_count + medium_count + high_count + critical_count\n\n        message = 'Vulnerability scan has been completed for {} and discovered {} vulnerabilities.'.format(\n            domain.name,\n            vulnerability_count\n        )\n        message += '\\n\\n*Vulnerability Stats:*'\n        message += '\\nCritical: {}'.format(critical_count)\n        message += '\\nHigh: {}'.format(high_count)\n        message += '\\nMedium: {}'.format(medium_count)\n        message += '\\nLow: {}'.format(low_count)\n        message += '\\nInfo: {}'.format(info_count)\n\n        send_notification(message)\n\n\ndef scan_failed(task):\n    task.scan_status = 0\n    task.stop_scan_date = timezone.now()\n    task.save()\n\n\ndef create_scan_activity(task, message, status):\n    scan_activity = ScanActivity()\n    scan_activity.scan_of = task\n    scan_activity.title = message\n    scan_activity.time = timezone.now()\n    scan_activity.status = status\n    scan_activity.save()\n    return scan_activity.id\n\n\ndef update_last_activity(id, activity_status):\n    ScanActivity.objects.filter(\n        id=id).update(\n        status=activity_status,\n        time=timezone.now())\n\n\ndef delete_scan_data(results_dir):\n    # remove all txt,html,json files\n    os.system('find {} -name \"*.txt\" -type f -delete'.format(results_dir))\n    os.system('find {} -name \"*.html\" -type f -delete'.format(results_dir))\n    os.system('find {} -name \"*.json\" -type f -delete'.format(results_dir))\n\n\ndef save_subdomain(subdomain_dict):\n    subdomain = Subdomain()\n    subdomain.discovered_date = timezone.now()\n    subdomain.target_domain = subdomain_dict.get('target_domain')\n    subdomain.scan_history = subdomain_dict.get('scan_history')\n    subdomain.name = subdomain_dict.get('name')\n    subdomain.http_url = subdomain_dict.get('http_url')\n    subdomain.screenshot_path = subdomain_dict.get('screenshot_path')\n    subdomain.http_header_path = subdomain_dict.get('http_header_path')\n    subdomain.cname = subdomain_dict.get('cname')\n    subdomain.is_cdn = subdomain_dict.get('is_cdn')\n    subdomain.content_type = subdomain_dict.get('content_type')\n    subdomain.webserver = subdomain_dict.get('webserver')\n    subdomain.page_title = subdomain_dict.get('page_title')\n\n    subdomain.is_imported_subdomain = subdomain_dict.get(\n        'is_imported_subdomain') if 'is_imported_subdomain' in subdomain_dict else False\n\n    if 'http_status' in subdomain_dict:\n        subdomain.http_status = subdomain_dict.get('http_status')\n\n    if 'response_time' in subdomain_dict:\n        subdomain.response_time = subdomain_dict.get('response_time')\n\n    if 'content_length' in subdomain_dict:\n        subdomain.content_length = subdomain_dict.get('content_length')\n\n    subdomain.save()\n    return subdomain\n\n\ndef save_endpoint(endpoint_dict):\n    endpoint = EndPoint()\n    endpoint.discovered_date = timezone.now()\n    endpoint.scan_history = endpoint_dict.get('scan_history')\n    endpoint.target_domain = endpoint_dict.get('target_domain') if 'target_domain' in endpoint_dict else None\n    endpoint.subdomain = endpoint_dict.get('subdomain') if 'target_domain' in endpoint_dict else None\n    endpoint.http_url = endpoint_dict.get('http_url')\n    endpoint.page_title = endpoint_dict.get('page_title') if 'page_title' in endpoint_dict else None\n    endpoint.content_type = endpoint_dict.get('content_type') if 'content_type' in endpoint_dict else None\n    endpoint.webserver = endpoint_dict.get('webserver') if 'webserver' in endpoint_dict else None\n    endpoint.response_time = endpoint_dict.get('response_time') if 'response_time' in endpoint_dict else 0\n    endpoint.http_status = endpoint_dict.get('http_status') if 'http_status' in endpoint_dict else 0\n    endpoint.content_length = endpoint_dict.get('content_length') if 'content_length' in endpoint_dict else 0\n    endpoint.is_default = endpoint_dict.get('is_default') if 'is_default' in endpoint_dict else False\n    endpoint.save()\n\n    return endpoint\n\ndef perform_osint(task, domain, yaml_configuration, results_dir):\n    notification = Notification.objects.all()\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has initiated OSINT on target {}'.format(domain.name))\n\n    if 'discover' in yaml_configuration[OSINT]:\n        osint_discovery(task, domain, yaml_configuration, results_dir)\n\n    if 'dork' in yaml_configuration[OSINT]:\n        dorking(task, yaml_configuration)\n\n    if notification and notification[0].send_scan_status_notif:\n        send_notification('reNgine has completed performing OSINT on target {}'.format(domain.name))\n\ndef osint_discovery(task, domain, yaml_configuration, results_dir):\n    if ALL in yaml_configuration[OSINT][OSINT_DISCOVER]:\n        osint_lookup = 'emails metainfo employees'\n    else:\n        osint_lookup = ' '.join(\n            str(lookup) for lookup in yaml_configuration[OSINT][OSINT_DISCOVER])\n\n    if 'metainfo' in osint_lookup:\n        if INTENSITY in yaml_configuration[OSINT]:\n            osint_intensity = yaml_configuration[OSINT][INTENSITY]\n        else:\n            osint_intensity = 'normal'\n\n        if OSINT_DOCUMENTS_LIMIT in yaml_configuration[OSINT]:\n            documents_limit = yaml_configuration[OSINT][OSINT_DOCUMENTS_LIMIT]\n        else:\n            documents_limit = 50\n\n        if osint_intensity == 'normal':\n            meta_dict = DottedDict({\n                'osint_target': domain.name,\n                'domain': domain,\n                'scan_id': task,\n                'documents_limit': documents_limit\n            })\n            get_and_save_meta_info(meta_dict)\n        elif osint_intensity == 'deep':\n            # get all subdomains in scan_id\n            subdomains = Subdomain.objects.filter(scan_history=task)\n            for subdomain in subdomains:\n                meta_dict = DottedDict({\n                    'osint_target': subdomain.name,\n                    'domain': domain,\n                    'scan_id': task,\n                    'documents_limit': documents_limit\n                })\n                get_and_save_meta_info(meta_dict)\n\n    if 'emails' in osint_lookup:\n        get_and_save_emails(task, results_dir)\n        get_and_save_leaked_credentials(task, results_dir)\n\n    if 'employees' in osint_lookup:\n        get_and_save_employees(task, results_dir)\n\ndef dorking(scan_history, yaml_configuration):\n    # Some dork sources: https://github.com/six2dez/degoogle_hunter/blob/master/degoogle_hunter.sh\n    # look in stackoverflow\n    if ALL in yaml_configuration[OSINT][OSINT_DORK]:\n        dork_lookup = 'stackoverflow, 3rdparty, social_media, project_management, code_sharing, config_files, jenkins, cloud_buckets, php_error, exposed_documents, struts_rce, db_files, traefik, git_exposed'\n    else:\n        dork_lookup = ' '.join(\n            str(lookup) for lookup in yaml_configuration[OSINT][OSINT_DORK])\n\n    if 'stackoverflow' in dork_lookup:\n        dork = 'site:stackoverflow.com'\n        dork_type = 'stackoverflow'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if '3rdparty' in dork_lookup:\n        # look in 3rd party sitee\n        dork_type = '3rdparty'\n        lookup_websites = [\n            'gitter.im',\n            'papaly.com',\n            'productforums.google.com',\n            'coggle.it',\n            'replt.it',\n            'ycombinator.com',\n            'libraries.io',\n            'npm.runkit.com',\n            'npmjs.com',\n            'scribd.com',\n            'gitter.im'\n        ]\n        dork = ''\n        for website in lookup_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'social_media' in dork_lookup:\n        dork_type = 'Social Media'\n        social_websites = [\n            'tiktok.com',\n            'facebook.com',\n            'twitter.com',\n            'youtube.com',\n            'pinterest.com',\n            'tumblr.com',\n            'reddit.com'\n        ]\n        dork = ''\n        for website in social_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'project_management' in dork_lookup:\n        dork_type = 'Project Management'\n        project_websites = [\n            'trello.com',\n            '*.atlassian.net'\n        ]\n        dork = ''\n        for website in project_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'code_sharing' in dork_lookup:\n        dork_type = 'Code Sharing Sites'\n        code_websites = [\n            'github.com',\n            'gitlab.com',\n            'bitbucket.org'\n        ]\n        dork = ''\n        for website in code_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'config_files' in dork_lookup:\n        dork_type = 'Config Files'\n        config_file_ext = [\n            'env',\n            'xml',\n            'conf',\n            'cnf',\n            'inf',\n            'rdp',\n            'ora',\n            'txt',\n            'cfg',\n            'ini'\n        ]\n\n        dork = ''\n        for extension in config_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'jenkins' in dork_lookup:\n        dork_type = 'Jenkins'\n        dork = 'intitle:\\\"Dashboard [Jenkins]\\\"'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'wordpress_files' in dork_lookup:\n        dork_type = 'Wordpress Files'\n        inurl_lookup = [\n            'wp-content',\n            'wp-includes'\n        ]\n\n        dork = ''\n        for lookup in inurl_lookup:\n            dork = dork + ' | ' + 'inurl:' + lookup\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'cloud_buckets' in dork_lookup:\n        dork_type = 'Cloud Buckets'\n        cloud_websites = [\n            '.s3.amazonaws.com',\n            'storage.googleapis.com',\n            'amazonaws.com'\n        ]\n\n        dork = ''\n        for website in cloud_websites:\n            dork = dork + ' | ' + 'site:' + website\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=False\n        )\n\n    if 'php_error' in dork_lookup:\n        dork_type = 'PHP Error'\n        error_words = [\n            '\\\"PHP Parse error\\\"',\n            '\\\"PHP Warning\\\"',\n            '\\\"PHP Error\\\"'\n        ]\n\n        dork = ''\n        for word in error_words:\n            dork = dork + ' | ' + word\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'exposed_documents' in dork_lookup:\n        dork_type = 'Exposed Documents'\n        docs_file_ext = [\n            'doc',\n            'docx',\n            'odt',\n            'pdf',\n            'rtf',\n            'sxw',\n            'psw',\n            'ppt',\n            'pptx',\n            'pps',\n            'csv'\n        ]\n\n        dork = ''\n        for extension in docs_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'struts_rce' in dork_lookup:\n        dork_type = 'Apache Struts RCE'\n        struts_file_ext = [\n            'action',\n            'struts',\n            'do'\n        ]\n\n        dork = ''\n        for extension in struts_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'db_files' in dork_lookup:\n        dork_type = 'Database Files'\n        db_file_ext = [\n            'sql',\n            'db',\n            'dbf',\n            'mdb'\n        ]\n\n        dork = ''\n        for extension in db_file_ext:\n            dork = dork + ' | ' + 'ext:' + extension\n        get_and_save_dork_results(\n            dork[3:],\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'traefik' in dork_lookup:\n        dork = 'intitle:traefik inurl:8080/dashboard'\n        dork_type = 'Traefik'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n    if 'git_exposed' in dork_lookup:\n        dork = 'inurl:\\\"/.git\\\"'\n        dork_type = '.git Exposed'\n        get_and_save_dork_results(\n            dork,\n            dork_type,\n            scan_history,\n            in_target=True\n        )\n\n\ndef get_and_save_dork_results(dork, type, scan_history, in_target=False):\n    degoogle_obj = degoogle.dg()\n    proxy = get_random_proxy()\n    if proxy:\n        os.environ['https_proxy'] = proxy\n        os.environ['HTTPS_PROXY'] = proxy\n\n    if in_target:\n        query = dork + \" site:\" + scan_history.domain.name\n    else:\n        query = dork + \" \\\"{}\\\"\".format(scan_history.domain.name)\n    logger.info(query)\n    degoogle_obj.query = query\n    results = degoogle_obj.run()\n    logger.info(results)\n    for result in results:\n        dork, _ = Dork.objects.get_or_create(\n            type=type,\n            description=result['desc'],\n            url=result['url']\n        )\n        scan_history.dorks.add(dork)\n\ndef get_and_save_employees(scan_history, results_dir):\n    theHarvester_location = '/usr/src/github/theHarvester'\n\n    # update proxies.yaml\n    if Proxy.objects.all().exists():\n        proxy = Proxy.objects.all()[0]\n        if proxy.use_proxy:\n            proxy_list = proxy.proxies.splitlines()\n            yaml_data = {'http' : proxy_list}\n\n            with open(theHarvester_location + '/proxies.yaml', 'w') as file:\n                documents = yaml.dump(yaml_data, file)\n\n\n    os.system('cd {} && python3 theHarvester.py -d {} -b all -f {}/theHarvester.html'.format(\n        theHarvester_location,\n        scan_history.domain.name,\n        results_dir\n    ))\n\n    file_location = results_dir + '/theHarvester.html'\n    print(file_location)\n    # delete proxy environ var\n    del os.environ['https_proxy']\n    del os.environ['HTTPS_PROXY']\n\n    if os.path.isfile(file_location):\n        logger.info('Parsing theHarvester results')\n        options = FirefoxOptions()\n        options.add_argument(\"--headless\")\n        driver = webdriver.Firefox(options=options)\n        driver.get('file://'+file_location)\n        tabledata = driver.execute_script('return tabledata')\n        # save email addresses and linkedin employees\n        for data in tabledata:\n            if data['record'] == 'email':\n                _email = data['result']\n                email, _ = Email.objects.get_or_create(address=_email)\n                scan_history.emails.add(email)\n            elif data['record'] == 'people':\n                _employee = data['result']\n                split_val = _employee.split('-')\n                name = split_val[0]\n                if len(split_val) == 2:\n                    designation = split_val[1]\n                else:\n                    designation = \"\"\n                employee, _ = Employee.objects.get_or_create(name=name, designation=designation)\n                scan_history.employees.add(employee)\n        driver.quit()\n\n\n        print(tabledata)\n\n\ndef get_and_save_emails(scan_history, results_dir):\n    leak_target_path = '{}/creds_target.txt'.format(results_dir)\n\n    # get email address\n    proxy = get_random_proxy()\n    if proxy:\n        os.environ['https_proxy'] = proxy\n        os.environ['HTTPS_PROXY'] = proxy\n\n    emails = []\n\n    try:\n        logger.info('OSINT: Getting emails from Google')\n        email_from_google = get_emails_from_google(scan_history.domain.name)\n        logger.info('OSINT: Getting emails from Bing')\n        email_from_bing = get_emails_from_bing(scan_history.domain.name)\n        logger.info('OSINT: Getting emails from Baidu')\n        email_from_baidu = get_emails_from_baidu(scan_history.domain.name)\n        emails = list(set(email_from_google + email_from_bing + email_from_baidu))\n        logger.info(emails)\n    except Exception as e:\n        logger.error(e)\n\n    leak_target_file = open(leak_target_path, 'w')\n\n    for _email in emails:\n        email, _ = Email.objects.get_or_create(address=_email)\n        scan_history.emails.add(email)\n        leak_target_file.write('{}\\n'.format(_email))\n\n    # fill leak_target_file with possible email address\n    leak_target_file.write('%@{}\\n'.format(scan_history.domain.name))\n    leak_target_file.write('%@%.{}\\n'.format(scan_history.domain.name))\n\n    leak_target_file.write('%.%@{}\\n'.format(scan_history.domain.name))\n    leak_target_file.write('%.%@%.{}\\n'.format(scan_history.domain.name))\n\n    leak_target_file.write('%_%@{}\\n'.format(scan_history.domain.name))\n    leak_target_file.write('%_%@%.{}\\n'.format(scan_history.domain.name))\n\n    leak_target_file.close()\n\ndef get_and_save_leaked_credentials(scan_history, results_dir):\n    logger.info('OSINT: Getting leaked credentials...')\n\n    leak_target_file = '{}/creds_target.txt'.format(results_dir)\n    leak_output_file = '{}/pwndb.json'.format(results_dir)\n\n    pwndb_command = 'python3 /usr/src/github/pwndb/pwndb.py --proxy tor:9150 --output json --list {}'.format(\n        leak_target_file\n    )\n\n    pwndb_output = subprocess.getoutput(pwndb_command)\n\n    print(pwndb_output)\n\n    try:\n        creds = json.loads(pwndb_output)\n\n        for cred in creds:\n            if cred['username'] != 'donate':\n                email_id = \"{}@{}\".format(cred['username'], cred['domain'])\n\n                email_obj, _ = Email.objects.get_or_create(\n                    address=email_id,\n                )\n                email_obj.password = cred['password']\n                email_obj.save()\n                scan_history.emails.add(email_obj)\n    except Exception as e:\n        logger.error(e)\n\n\ndef get_and_save_meta_info(meta_dict):\n    logger.info('Getting METADATA for {}'.format(meta_dict.osint_target))\n    proxy = get_random_proxy()\n    if proxy:\n        os.environ['https_proxy'] = proxy\n        os.environ['HTTPS_PROXY'] = proxy\n    result = metadata_extractor.extract_metadata_from_google_search(meta_dict.osint_target, meta_dict.documents_limit)\n    if result:\n        results = result.get_metadata()\n        for meta in results:\n            meta_finder_document = MetaFinderDocument()\n            subdomain = Subdomain.objects.get(scan_history=meta_dict.scan_id, name=meta_dict.osint_target)\n            meta_finder_document.subdomain = subdomain\n            meta_finder_document.target_domain = meta_dict.domain\n            meta_finder_document.scan_history = meta_dict.scan_id\n\n            item = DottedDict(results[meta])\n            meta_finder_document.url = item.url\n            meta_finder_document.doc_name = meta\n            meta_finder_document.http_status = item.status_code\n\n            metadata = results[meta]['metadata']\n            for data in metadata:\n                if 'Producer' in metadata and metadata['Producer']:\n                    meta_finder_document.producer = metadata['Producer'].rstrip('\\x00')\n                if 'Creator' in metadata and metadata['Creator']:\n                    meta_finder_document.creator = metadata['Creator'].rstrip('\\x00')\n                if 'CreationDate' in metadata and metadata['CreationDate']:\n                    meta_finder_document.creation_date = metadata['CreationDate'].rstrip('\\x00')\n                if 'ModDate' in metadata and metadata['ModDate']:\n                    meta_finder_document.modified_date = metadata['ModDate'].rstrip('\\x00')\n                if 'Author' in metadata and metadata['Author']:\n                    meta_finder_document.author = metadata['Author'].rstrip('\\x00')\n                if 'Title' in metadata and metadata['Title']:\n                    meta_finder_document.title = metadata['Title'].rstrip('\\x00')\n                if 'OSInfo' in metadata and metadata['OSInfo']:\n                    meta_finder_document.os = metadata['OSInfo'].rstrip('\\x00')\n\n            meta_finder_document.save()\n\n@app.task(bind=True)\ndef test_task(self):\n    print('*' * 40)\n    print('test task run')\n    print('*' * 40)\n", "import os\nimport requests\nimport itertools\n\nfrom datetime import datetime\n\nfrom django.shortcuts import render, get_object_or_404\nfrom django.contrib import messages\nfrom django.http import JsonResponse, HttpResponseRedirect, HttpResponse\nfrom django.urls import reverse\nfrom django_celery_beat.models import PeriodicTask, IntervalSchedule, ClockedSchedule\nfrom django.utils import timezone\nfrom django.conf import settings\nfrom django.core import serializers\n\nfrom startScan.models import *\nfrom targetApp.models import *\nfrom scanEngine.models import EngineType, Configuration\nfrom reNgine.tasks import initiate_scan, create_scan_activity\nfrom reNgine.celery import app\n\nfrom reNgine.common_func import *\n\n\ndef scan_history(request):\n    host = ScanHistory.objects.all().order_by('-start_scan_date')\n    context = {'scan_history_active': 'active', \"scan_history\": host}\n    return render(request, 'startScan/history.html', context)\n\n\ndef detail_scan(request, id=None):\n    context = {}\n    if id:\n        context['scan_history_id'] = id\n        context['subdomain_count'] = Subdomain.objects.filter(\n            scan_history__id=id).values('name').distinct().count()\n        context['alive_count'] = Subdomain.objects.filter(\n            scan_history__id=id).values('name').distinct().filter(\n            http_status__exact=200).count()\n        context['important_count'] = Subdomain.objects.filter(\n            scan_history__id=id).values('name').distinct().filter(\n            is_important=True).count()\n        context['scan_activity'] = ScanActivity.objects.filter(\n            scan_of__id=id).order_by('-time')\n        context['endpoint_count'] = EndPoint.objects.filter(\n            scan_history__id=id).values('http_url').distinct().count()\n        context['endpoint_alive_count'] = EndPoint.objects.filter(\n            scan_history__id=id, http_status__exact=200).values('http_url').distinct().count()\n        history = get_object_or_404(ScanHistory, id=id)\n        context['history'] = history\n        info_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=0).count()\n        low_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=1).count()\n        medium_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=2).count()\n        high_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=3).count()\n        critical_count = Vulnerability.objects.filter(\n            scan_history__id=id, severity=4).count()\n        context['vulnerability_list'] = Vulnerability.objects.filter(\n            scan_history__id=id).order_by('-severity').all()[:20]\n        context['total_vulnerability_count'] = info_count + low_count + \\\n            medium_count + high_count + critical_count\n        context['info_count'] = info_count\n        context['low_count'] = low_count\n        context['medium_count'] = medium_count\n        context['high_count'] = high_count\n        context['critical_count'] = critical_count\n        context['scan_history_active'] = 'active'\n\n        emails = Email.objects.filter(\n            emails__in=ScanHistory.objects.filter(\n                id=id))\n\n        context['exposed_count'] = emails.exclude(password__isnull=True).count()\n\n        context['email_count'] = emails.count()\n\n        context['employees_count'] = Employee.objects.filter(\n            employees__in=ScanHistory.objects.filter(id=id)).count()\n\n        domain_id = ScanHistory.objects.filter(id=id)\n\n        context['most_recent_scans'] = ScanHistory.objects.filter(domain__id=domain_id[0].domain.id).order_by('-start_scan_date')[:5]\n\n        if domain_id:\n            domain_id = domain_id[0].domain.id\n            scan_history = ScanHistory.objects.filter(domain=domain_id).filter(subdomain_discovery=True).filter(id__lte=id).filter(scan_status=2)\n            if scan_history.count() > 1:\n                last_scan = scan_history.order_by('-start_scan_date')[1]\n                context['last_scan'] = last_scan\n\n    # badge count for gfs\n    if history.used_gf_patterns:\n        count_gf = {}\n        for gf in history.used_gf_patterns.split(','):\n            count_gf[gf] = EndPoint.objects.filter(scan_history__id=id, matched_gf_patterns__icontains=gf).count()\n            context['matched_gf_count'] = count_gf\n    return render(request, 'startScan/detail_scan.html', context)\n\ndef all_subdomains(request):\n    context = {}\n    context['scan_history_id'] = id\n    context['subdomain_count'] = Subdomain.objects.values('name').distinct().count()\n    context['alive_count'] = Subdomain.objects.values('name').distinct().filter(\n        http_status__exact=200).count()\n    context['important_count'] = Subdomain.objects.values('name').distinct().filter(\n        is_important=True).count()\n\n    context['scan_history_active'] = 'active'\n\n    return render(request, 'startScan/subdomains.html', context)\n\ndef detail_vuln_scan(request, id=None):\n    if id:\n        history = get_object_or_404(ScanHistory, id=id)\n        context = {'scan_history_id': id, 'history': history}\n    else:\n        context = {'vuln_scan_active': 'true'}\n    return render(request, 'startScan/vulnerabilities.html', context)\n\n\ndef all_endpoints(request):\n    context = {}\n    context['scan_history_active'] = 'active'\n    return render(request, 'startScan/endpoints.html', context)\n\n\ndef start_scan_ui(request, domain_id):\n    domain = get_object_or_404(Domain, id=domain_id)\n    if request.method == \"POST\":\n        # get imported subdomains\n        imported_subdomains = [subdomain.rstrip() for subdomain in request.POST['importSubdomainTextArea'].split('\\n')]\n        imported_subdomains = [subdomain for subdomain in imported_subdomains if subdomain]\n\n        out_of_scope_subdomains = [subdomain.rstrip() for subdomain in request.POST['outOfScopeSubdomainTextarea'].split('\\n')]\n        out_of_scope_subdomains = [subdomain for subdomain in out_of_scope_subdomains if subdomain]\n        # get engine type\n        engine_type = request.POST['scan_mode']\n        scan_history_id = create_scan_object(domain_id, engine_type)\n        # start the celery task\n        celery_task = initiate_scan.apply_async(\n            args=(\n                domain_id,\n                scan_history_id,\n                0,\n                engine_type,\n                imported_subdomains,\n                out_of_scope_subdomains\n                ))\n        ScanHistory.objects.filter(\n            id=scan_history_id).update(\n            celery_id=celery_task.id)\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Started for ' +\n            domain.name)\n        return HttpResponseRedirect(reverse('scan_history'))\n    engine = EngineType.objects.order_by('id')\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'domain': domain,\n        'engines': engine,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'startScan/start_scan_ui.html', context)\n\n\ndef start_multiple_scan(request):\n    # domain = get_object_or_404(Domain, id=host_id)\n    domain_text = \"\"\n    if request.method == \"POST\":\n        if request.POST.get('scan_mode', 0):\n            # if scan mode is available, then start the scan\n            # get engine type\n            engine_type = request.POST['scan_mode']\n            list_of_domains = request.POST['list_of_domain_id']\n            for domain_id in list_of_domains.split(\",\"):\n                # start the celery task\n                scan_history_id = create_scan_object(domain_id, engine_type)\n                celery_task = initiate_scan.apply_async(\n                    args=(domain_id, scan_history_id, 0, engine_type))\n                ScanHistory.objects.filter(\n                    id=scan_history_id).update(\n                    celery_id=celery_task.id)\n            messages.add_message(\n                request,\n                messages.INFO,\n                'Scan Started for multiple targets')\n            return HttpResponseRedirect(reverse('scan_history'))\n        else:\n            # this else condition will have post request from the scan page\n            # containing all the targets id\n            list_of_domain_name = []\n            list_of_domain_id = []\n            for key, value in request.POST.items():\n                print(value)\n                if key != \"list_target_table_length\" and key != \"csrfmiddlewaretoken\":\n                    domain = get_object_or_404(Domain, id=value)\n                    list_of_domain_name.append(domain.name)\n                    list_of_domain_id.append(value)\n            domain_text = \", \".join(list_of_domain_name)\n            domain_ids = \",\".join(list_of_domain_id)\n    engine = EngineType.objects\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'engines': engine,\n        'domain_list': domain_text,\n        'domain_ids': domain_ids,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'startScan/start_multiple_scan_ui.html', context)\n\ndef export_subdomains(request, scan_id):\n    subdomain_list = Subdomain.objects.filter(scan_history__id=scan_id)\n    domain_results = ScanHistory.objects.get(id=scan_id)\n    response_body = \"\"\n    for name in subdomain_list:\n        response_body = response_body + name.name + \"\\n\"\n    response = HttpResponse(response_body, content_type='text/plain')\n    response['Content-Disposition'] = 'attachment; filename=\"subdomains_' + \\\n        domain_results.domain.name + '_' + \\\n        str(domain_results.start_scan_date.date()) + '.txt\"'\n    return response\n\n\ndef export_endpoints(request, scan_id):\n    endpoint_list = EndPoint.objects.filter(scan_history__id=scan_id)\n    domain_results = ScanHistory.objects.get(id=scan_id)\n    response_body = \"\"\n    for endpoint in endpoint_list:\n        response_body = response_body + endpoint.http_url + \"\\n\"\n    response = HttpResponse(response_body, content_type='text/plain')\n    response['Content-Disposition'] = 'attachment; filename=\"endpoints_' + \\\n        domain_results.domain.name + '_' + \\\n        str(domain_results.start_scan_date.date()) + '.txt\"'\n    return response\n\n\ndef export_urls(request, scan_id):\n    urls_list = Subdomain.objects.filter(scan_history__id=scan_id)\n    domain_results = ScanHistory.objects.get(id=scan_id)\n    response_body = \"\"\n    for url in urls_list:\n        if url.http_url:\n            response_body = response_body + url.http_url + \"\\n\"\n    response = HttpResponse(response_body, content_type='text/plain')\n    response['Content-Disposition'] = 'attachment; filename=\"urls_' + \\\n        domain_results.domain.name + '_' + \\\n        str(domain_results.start_scan_date.date()) + '.txt\"'\n    return response\n\n\ndef delete_scan(request, id):\n    obj = get_object_or_404(ScanHistory, id=id)\n    if request.method == \"POST\":\n        delete_dir = obj.results_dir\n        os.system('rm -rf /usr/src/scan_results/' + delete_dir)\n        obj.delete()\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan history successfully deleted!')\n    else:\n        messageData = {'status': 'false'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Oops! something went wrong!')\n    return JsonResponse(messageData)\n\n\ndef stop_scan(request, id):\n    if request.method == \"POST\":\n        scan_history = get_object_or_404(ScanHistory, celery_id=id)\n        # stop the celery task\n        app.control.revoke(id, terminate=True, signal='SIGKILL')\n        scan_history.scan_status = 3\n        scan_history.save()\n        try:\n            last_activity = ScanActivity.objects.filter(\n                scan_of=scan_history).order_by('-pk')[0]\n            last_activity.status = 0\n            last_activity.time = timezone.now()\n            last_activity.save()\n        except Exception as e:\n            print(e)\n        create_scan_activity(scan_history, \"Scan aborted\", 0)\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan successfully stopped!')\n    else:\n        messageData = {'status': 'false'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Oops! something went wrong!')\n    return JsonResponse(messageData)\n\n\ndef schedule_scan(request, host_id):\n    domain = Domain.objects.get(id=host_id)\n    if request.method == \"POST\":\n        # get imported subdomains\n        imported_subdomains = [subdomain.rstrip() for subdomain in request.POST['importSubdomainTextArea'].split('\\n')]\n        imported_subdomains = [subdomain for subdomain in imported_subdomains if subdomain]\n        # get engine type\n        engine_type = int(request.POST['scan_mode'])\n        engine_object = get_object_or_404(EngineType, id=engine_type)\n        task_name = engine_object.engine_name + ' for ' + \\\n            domain.name + \\\n            ':' + \\\n            str(datetime.datetime.strftime(timezone.now(), '%Y_%m_%d_%H_%M_%S'))\n        if request.POST['scheduled_mode'] == 'periodic':\n            # periodic task\n            frequency_value = int(request.POST['frequency'])\n            frequency_type = request.POST['frequency_type']\n            if frequency_type == 'minutes':\n                period = IntervalSchedule.MINUTES\n            elif frequency_type == 'hours':\n                period = IntervalSchedule.HOURS\n            elif frequency_type == 'days':\n                period = IntervalSchedule.DAYS\n            elif frequency_type == 'weeks':\n                period = IntervalSchedule.DAYS\n                frequency_value *= 7\n            elif frequency_type == 'months':\n                period = IntervalSchedule.DAYS\n                frequency_value *= 30\n\n            schedule, created = IntervalSchedule.objects.get_or_create(\n                every=frequency_value,\n                period=period,)\n            _kwargs = json.dumps({'domain_id': host_id, 'scan_history_id': 0, 'scan_type': 1, 'engine_type': engine_type, 'imported_subdomains': imported_subdomains})\n            PeriodicTask.objects.create(interval=schedule,\n                                        name=task_name,\n                                        task='reNgine.tasks.initiate_scan',\n                                        kwargs=_kwargs)\n        elif request.POST['scheduled_mode'] == 'clocked':\n            # clocked task\n            schedule_time = request.POST['scheduled_time']\n            clock, created = ClockedSchedule.objects.get_or_create(\n                clocked_time=schedule_time,)\n            _kwargs = json.dumps({'domain_id': host_id, 'scan_history_id': 0, 'scan_type': 1, 'engine_type': engine_type, 'imported_subdomains': imported_subdomains})\n            PeriodicTask.objects.create(clocked=clock,\n                                        one_off=True,\n                                        name=task_name,\n                                        task='reNgine.tasks.initiate_scan',\n                                        kwargs=_kwargs)\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Scheduled for ' +\n            domain.name)\n        return HttpResponseRedirect(reverse('scheduled_scan_view'))\n    engine = EngineType.objects\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'domain': domain,\n        'engines': engine,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'startScan/schedule_scan_ui.html', context)\n\n\ndef scheduled_scan_view(request):\n    scheduled_tasks = PeriodicTask.objects.all().exclude(name='celery.backend_cleanup')\n    context = {\n        'scheduled_scan_active': 'active',\n        'scheduled_tasks': scheduled_tasks,\n    }\n    return render(request, 'startScan/schedule_scan_list.html', context)\n\n\ndef delete_scheduled_task(request, id):\n    task_object = get_object_or_404(PeriodicTask, id=id)\n    if request.method == \"POST\":\n        task_object.delete()\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scheduled Scan successfully deleted!')\n    else:\n        messageData = {'status': 'false'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Oops! something went wrong!')\n    return JsonResponse(messageData)\n\n\ndef change_scheduled_task_status(request, id):\n    if request.method == 'POST':\n        task = PeriodicTask.objects.get(id=id)\n        task.enabled = not task.enabled\n        task.save()\n    return HttpResponse('')\n\n\ndef change_vuln_status(request, id):\n    if request.method == 'POST':\n        vuln = Vulnerability.objects.get(id=id)\n        vuln.open_status = not vuln.open_status\n        vuln.save()\n    return HttpResponse('')\n\n\ndef change_subdomain_status(request, id):\n    if request.method == 'POST':\n        name = Subdomain.objects.get(id=id)\n        name.checked = not name.checked\n        name.save()\n    return HttpResponse('')\n\n\ndef change_subdomain_important_status(request, id):\n    if request.method == 'POST':\n        name = Subdomain.objects.get(id=id)\n        name.is_important = not name.is_important\n        name.save()\n    return HttpResponse('')\n\n\ndef create_scan_object(host_id, engine_type):\n    '''\n    create task with pending status so that celery task will execute when\n    threads are free\n    '''\n    # get current time\n    current_scan_time = timezone.now()\n    # fetch engine and domain object\n    engine_object = EngineType.objects.get(pk=engine_type)\n    domain = Domain.objects.get(pk=host_id)\n    task = ScanHistory()\n    task.scan_status = -1\n    task.domain = domain\n    task.scan_type = engine_object\n    task.start_scan_date = current_scan_time\n    task.save()\n    # save last scan date for domain model\n    domain.start_scan_date = current_scan_time\n    domain.save()\n    return task.id\n\n\ndef delete_all_scan_results(request):\n    if request.method == 'POST':\n        ScanHistory.objects.all().delete()\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'All Scan History successfully deleted!')\n    return JsonResponse(messageData)\n\ndef delete_all_screenshots(request):\n    if request.method == 'POST':\n        os.system('rm -rf /usr/src/scan_results/*')\n        messageData = {'status': 'true'}\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Screenshots successfully deleted!')\n    return JsonResponse(messageData)\n\n\ndef visualise(request, id):\n    scan_history = ScanHistory.objects.get(id=id)\n    context = {\n        'scan_id': id,\n        'scan_history': scan_history,\n    }\n    return render(request, 'startScan/visualise.html', context)\n\ndef start_organization_scan(request, id):\n    organization = get_object_or_404(Organization, id=id)\n    if request.method == \"POST\":\n        # get engine type\n        engine_type = request.POST['scan_mode']\n        for domain in organization.get_domains():\n            scan_history_id = create_scan_object(domain.id, engine_type)\n            # start the celery task\n            celery_task = initiate_scan.apply_async(\n                args=(domain.id,\n                    scan_history_id,\n                    0,\n                    engine_type,\n                    None\n                ))\n            ScanHistory.objects.filter(\n                id=scan_history_id).update(\n                celery_id=celery_task.id)\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Started for {} domains in organization {}'.format(\n                len(organization.get_domains()),\n                organization.name\n            )\n        )\n        return HttpResponseRedirect(reverse('scan_history'))\n    engine = EngineType.objects.order_by('id')\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    domain_list = organization.get_domains()\n    context = {\n        'organization_data_active': 'true',\n        'list_organization_li': 'active',\n        'organization': organization,\n        'engines': engine,\n        'domain_list': domain_list,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'organization/start_scan.html', context)\n\ndef schedule_organization_scan(request, id):\n    organization =Organization.objects.get(id=id)\n    if request.method == \"POST\":\n        # get engine type\n        engine_type = int(request.POST['scan_mode'])\n        engine_object = get_object_or_404(EngineType, id=engine_type)\n        for domain in organization.get_domains():\n            task_name = engine_object.engine_name + ' for ' + \\\n                domain.name + \\\n                ':' + \\\n                str(datetime.datetime.strftime(\n                    timezone.now(),\n                    '%Y_%m_%d_%H_%M_%S'\n                ))\n            if request.POST['scheduled_mode'] == 'periodic':\n                # periodic task\n                frequency_value = int(request.POST['frequency'])\n                frequency_type = request.POST['frequency_type']\n                if frequency_type == 'minutes':\n                    period = IntervalSchedule.MINUTES\n                elif frequency_type == 'hours':\n                    period = IntervalSchedule.HOURS\n                elif frequency_type == 'days':\n                    period = IntervalSchedule.DAYS\n                elif frequency_type == 'weeks':\n                    period = IntervalSchedule.DAYS\n                    frequency_value *= 7\n                elif frequency_type == 'months':\n                    period = IntervalSchedule.DAYS\n                    frequency_value *= 30\n\n                schedule, created = IntervalSchedule.objects.get_or_create(\n                    every=frequency_value,\n                    period=period,)\n                _kwargs = json.dumps({'domain_id': domain.id,\n                        'scan_history_id': 0,\n                        'scan_type': 1,\n                        'engine_type': engine_type,\n                        'imported_subdomains': None\n                })\n                PeriodicTask.objects.create(interval=schedule,\n                    name=task_name,\n                    task='reNgine.tasks.initiate_scan',\n                    kwargs=_kwargs\n                )\n            elif request.POST['scheduled_mode'] == 'clocked':\n                # clocked task\n                schedule_time = request.POST['scheduled_time']\n                clock, created = ClockedSchedule.objects.get_or_create(\n                    clocked_time=schedule_time,)\n                _kwargs = json.dumps({'domain_id': domain.id,\n                    'scan_history_id': 0,\n                    'scan_type': 1,\n                    'engine_type': engine_type,\n                    'imported_subdomains': None}\n                )\n                PeriodicTask.objects.create(clocked=clock,\n                    one_off=True,\n                    name=task_name,\n                    task='reNgine.tasks.initiate_scan',\n                    kwargs=_kwargs\n                )\n        messages.add_message(\n            request,\n            messages.INFO,\n            'Scan Started for {} domains in organization {}'.format(\n                len(organization.get_domains()),\n                organization.name\n            )\n        )\n        return HttpResponseRedirect(reverse('scheduled_scan_view'))\n    engine = EngineType.objects\n    custom_engine_count = EngineType.objects.filter(\n        default_engine=False).count()\n    context = {\n        'scan_history_active': 'active',\n        'organization': organization,\n        'domain_list': organization.get_domains(),\n        'engines': engine,\n        'custom_engine_count': custom_engine_count}\n    return render(request, 'organization/schedule_scan_ui.html', context)\n\n\ndef delete_scans(request):\n    context = {}\n    if request.method == \"POST\":\n        list_of_scan_id = []\n\n        for key, value in request.POST.items():\n            if key != \"scan_history_table_length\" and key != \"csrfmiddlewaretoken\":\n                obj = get_object_or_404(ScanHistory, id=value)\n                delete_dir = obj.results_dir\n                os.system('rm -rf /usr/src/scan_results/' + delete_dir)\n                obj.delete()\n        messages.add_message(\n            request,\n            messages.INFO,\n            'All Scans deleted!')\n    return HttpResponseRedirect(reverse('scan_history'))\n"], "filenames": ["web/reNgine/tasks.py", "web/startScan/views.py"], "buggy_code_start_loc": [6, 261], "buggy_code_end_loc": [1338, 617], "fixing_code_start_loc": [7, 261], "fixing_code_end_loc": [1338, 618], "type": "CWE-330", "message": "reNgine through 0.5 relies on a predictable directory name.", "other": {"cve": {"id": "CVE-2021-38606", "sourceIdentifier": "cve@mitre.org", "published": "2021-08-12T16:15:10.647", "lastModified": "2022-07-12T17:42:04.277", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "reNgine through 0.5 relies on a predictable directory name."}, {"lang": "es", "value": "reNgine versiones hasta 0.5, se basa en un nombre de directorio predecible"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-330"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:rengine_project:rengine:*:*:*:*:*:*:*:*", "versionEndIncluding": "0.5", "matchCriteriaId": "91A91932-773D-47B5-970B-76E2F1418150"}]}]}], "references": [{"url": "https://github.com/yogeshojha/rengine/commit/158367a231335026b8dba633a76b44de290ad37c", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/yogeshojha/rengine/commit/158367a231335026b8dba633a76b44de290ad37c"}}