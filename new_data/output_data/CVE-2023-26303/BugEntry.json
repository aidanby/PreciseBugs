{"buggy_code": ["\"\"\"\nclass Renderer\n\nGenerates HTML from parsed token stream. Each instance has independent\ncopy of rules. Those can be rewritten with ease. Also, you can add new\nrules if you create plugin and adds new token types.\n\"\"\"\nfrom __future__ import annotations\n\nfrom collections.abc import MutableMapping, Sequence\nimport inspect\nfrom typing import Any, ClassVar\n\nfrom .common.utils import escapeHtml, unescapeAll\nfrom .token import Token\nfrom .utils import OptionsDict\n\ntry:\n    from typing import Protocol\nexcept ImportError:  # Python <3.8 doesn't have `Protocol` in the stdlib\n    from typing_extensions import Protocol  # type: ignore\n\n\nclass RendererProtocol(Protocol):\n    __output__: ClassVar[str]\n\n    def render(\n        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping\n    ) -> Any:\n        ...\n\n\nclass RendererHTML(RendererProtocol):\n    \"\"\"Contains render rules for tokens. Can be updated and extended.\n\n    Example:\n\n    Each rule is called as independent static function with fixed signature:\n\n    ::\n\n        class Renderer:\n            def token_type_name(self, tokens, idx, options, env) {\n                # ...\n                return renderedHTML\n\n    ::\n\n        class CustomRenderer(RendererHTML):\n            def strong_open(self, tokens, idx, options, env):\n                return '<b>'\n            def strong_close(self, tokens, idx, options, env):\n                return '</b>'\n\n        md = MarkdownIt(renderer_cls=CustomRenderer)\n\n        result = md.render(...)\n\n    See https://github.com/markdown-it/markdown-it/blob/master/lib/renderer.js\n    for more details and examples.\n    \"\"\"\n\n    __output__ = \"html\"\n\n    def __init__(self, parser=None):\n        self.rules = {\n            k: v\n            for k, v in inspect.getmembers(self, predicate=inspect.ismethod)\n            if not (k.startswith(\"render\") or k.startswith(\"_\"))\n        }\n\n    def render(\n        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping\n    ) -> str:\n        \"\"\"Takes token stream and generates HTML.\n\n        :param tokens: list on block tokens to render\n        :param options: params of parser instance\n        :param env: additional data from parsed input\n\n        \"\"\"\n        result = \"\"\n\n        for i, token in enumerate(tokens):\n            if token.type == \"inline\":\n                assert token.children is not None\n                result += self.renderInline(token.children, options, env)\n            elif token.type in self.rules:\n                result += self.rules[token.type](tokens, i, options, env)\n            else:\n                result += self.renderToken(tokens, i, options, env)\n\n        return result\n\n    def renderInline(\n        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping\n    ) -> str:\n        \"\"\"The same as ``render``, but for single token of `inline` type.\n\n        :param tokens: list on block tokens to render\n        :param options: params of parser instance\n        :param env: additional data from parsed input (references, for example)\n        \"\"\"\n        result = \"\"\n\n        for i, token in enumerate(tokens):\n            if token.type in self.rules:\n                result += self.rules[token.type](tokens, i, options, env)\n            else:\n                result += self.renderToken(tokens, i, options, env)\n\n        return result\n\n    def renderToken(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        \"\"\"Default token renderer.\n\n        Can be overridden by custom function\n\n        :param idx: token index to render\n        :param options: params of parser instance\n        \"\"\"\n        result = \"\"\n        needLf = False\n        token = tokens[idx]\n\n        # Tight list paragraphs\n        if token.hidden:\n            return \"\"\n\n        # Insert a newline between hidden paragraph and subsequent opening\n        # block-level tag.\n        #\n        # For example, here we should insert a newline before blockquote:\n        #  - a\n        #    >\n        #\n        if token.block and token.nesting != -1 and idx and tokens[idx - 1].hidden:\n            result += \"\\n\"\n\n        # Add token name, e.g. `<img`\n        result += (\"</\" if token.nesting == -1 else \"<\") + token.tag\n\n        # Encode attributes, e.g. `<img src=\"foo\"`\n        result += self.renderAttrs(token)\n\n        # Add a slash for self-closing tags, e.g. `<img src=\"foo\" /`\n        if token.nesting == 0 and options[\"xhtmlOut\"]:\n            result += \" /\"\n\n        # Check if we need to add a newline after this tag\n        if token.block:\n            needLf = True\n\n            if token.nesting == 1:\n                if idx + 1 < len(tokens):\n                    nextToken = tokens[idx + 1]\n\n                    if nextToken.type == \"inline\" or nextToken.hidden:\n                        # Block-level tag containing an inline tag.\n                        #\n                        needLf = False\n\n                    elif nextToken.nesting == -1 and nextToken.tag == token.tag:\n                        # Opening tag + closing tag of the same type. E.g. `<li></li>`.\n                        #\n                        needLf = False\n\n        result += \">\\n\" if needLf else \">\"\n\n        return result\n\n    @staticmethod\n    def renderAttrs(token: Token) -> str:\n        \"\"\"Render token attributes to string.\"\"\"\n        result = \"\"\n\n        for key, value in token.attrItems():\n            result += \" \" + escapeHtml(key) + '=\"' + escapeHtml(str(value)) + '\"'\n\n        return result\n\n    def renderInlineAsText(\n        self,\n        tokens: Sequence[Token] | None,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        \"\"\"Special kludge for image `alt` attributes to conform CommonMark spec.\n\n        Don't try to use it! Spec requires to show `alt` content with stripped markup,\n        instead of simple escaping.\n\n        :param tokens: list on block tokens to render\n        :param options: params of parser instance\n        :param env: additional data from parsed input\n        \"\"\"\n        result = \"\"\n\n        for token in tokens or []:\n            if token.type == \"text\":\n                result += token.content\n            elif token.type == \"image\":\n                assert token.children is not None\n                result += self.renderInlineAsText(token.children, options, env)\n            elif token.type == \"softbreak\":\n                result += \"\\n\"\n\n        return result\n\n    ###################################################\n\n    def code_inline(self, tokens: Sequence[Token], idx: int, options, env) -> str:\n        token = tokens[idx]\n        return (\n            \"<code\"\n            + self.renderAttrs(token)\n            + \">\"\n            + escapeHtml(tokens[idx].content)\n            + \"</code>\"\n        )\n\n    def code_block(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n\n        return (\n            \"<pre\"\n            + self.renderAttrs(token)\n            + \"><code>\"\n            + escapeHtml(tokens[idx].content)\n            + \"</code></pre>\\n\"\n        )\n\n    def fence(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n        info = unescapeAll(token.info).strip() if token.info else \"\"\n        langName = \"\"\n        langAttrs = \"\"\n\n        if info:\n            arr = info.split(maxsplit=1)\n            langName = arr[0]\n            if len(arr) == 2:\n                langAttrs = arr[1]\n\n        if options.highlight:\n            highlighted = options.highlight(\n                token.content, langName, langAttrs\n            ) or escapeHtml(token.content)\n        else:\n            highlighted = escapeHtml(token.content)\n\n        if highlighted.startswith(\"<pre\"):\n            return highlighted + \"\\n\"\n\n        # If language exists, inject class gently, without modifying original token.\n        # May be, one day we will add .deepClone() for token and simplify this part, but\n        # now we prefer to keep things local.\n        if info:\n            # Fake token just to render attributes\n            tmpToken = Token(type=\"\", tag=\"\", nesting=0, attrs=token.attrs.copy())\n            tmpToken.attrJoin(\"class\", options.langPrefix + langName)\n\n            return (\n                \"<pre><code\"\n                + self.renderAttrs(tmpToken)\n                + \">\"\n                + highlighted\n                + \"</code></pre>\\n\"\n            )\n\n        return (\n            \"<pre><code\"\n            + self.renderAttrs(token)\n            + \">\"\n            + highlighted\n            + \"</code></pre>\\n\"\n        )\n\n    def image(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n\n        # \"alt\" attr MUST be set, even if empty. Because it's mandatory and\n        # should be placed on proper position for tests.\n\n        assert (\n            token.attrs and \"alt\" in token.attrs\n        ), '\"image\" token\\'s attrs must contain `alt`'\n\n        # Replace content with actual value\n\n        token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))\n\n        return self.renderToken(tokens, idx, options, env)\n\n    def hardbreak(\n        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args\n    ) -> str:\n        return \"<br />\\n\" if options.xhtmlOut else \"<br>\\n\"\n\n    def softbreak(\n        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args\n    ) -> str:\n        return (\n            (\"<br />\\n\" if options.xhtmlOut else \"<br>\\n\") if options.breaks else \"\\n\"\n        )\n\n    def text(self, tokens: Sequence[Token], idx: int, *args) -> str:\n        return escapeHtml(tokens[idx].content)\n\n    def html_block(self, tokens: Sequence[Token], idx: int, *args) -> str:\n        return tokens[idx].content\n\n    def html_inline(self, tokens: Sequence[Token], idx: int, *args) -> str:\n        return tokens[idx].content\n", "\"\"\"Simple typographic replacements\n\n* ``(c)``, ``(C)`` \u2192 \u00a9\n* ``(tm)``, ``(TM)`` \u2192 \u2122\n* ``(r)``, ``(R)`` \u2192 \u00ae\n* ``(p)``, ``(P)`` \u2192 \u00a7\n* ``+-`` \u2192 \u00b1\n* ``...`` \u2192 \u2026\n* ``?....`` \u2192 ?..\n* ``!....`` \u2192 !..\n* ``????????`` \u2192 ???\n* ``!!!!!`` \u2192 !!!\n* ``,,,`` \u2192 ,\n* ``--`` \u2192 &ndash\n* ``---`` \u2192 &mdash\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport re\n\nfrom ..token import Token\nfrom .state_core import StateCore\n\nLOGGER = logging.getLogger(__name__)\n\n# TODO:\n# - fractionals 1/2, 1/4, 3/4 -> \u00bd, \u00bc, \u00be\n# - miltiplication 2 x 4 -> 2 \u00d7 4\n\nRARE_RE = re.compile(r\"\\+-|\\.\\.|\\?\\?\\?\\?|!!!!|,,|--\")\n\n# Workaround for phantomjs - need regex without /g flag,\n# or root check will fail every second time\n# SCOPED_ABBR_TEST_RE = r\"\\((c|tm|r|p)\\)\"\n\nSCOPED_ABBR_RE = re.compile(r\"\\((c|tm|r|p)\\)\", flags=re.IGNORECASE)\n\nPLUS_MINUS_RE = re.compile(r\"\\+-\")\n\nELLIPSIS_RE = re.compile(r\"\\.{2,}\")\n\nELLIPSIS_QUESTION_EXCLAMATION_RE = re.compile(r\"([?!])\u2026\")\n\nQUESTION_EXCLAMATION_RE = re.compile(r\"([?!]){4,}\")\n\nCOMMA_RE = re.compile(r\",{2,}\")\n\nEM_DASH_RE = re.compile(r\"(^|[^-])---(?=[^-]|$)\", flags=re.MULTILINE)\n\nEN_DASH_RE = re.compile(r\"(^|\\s)--(?=\\s|$)\", flags=re.MULTILINE)\n\nEN_DASH_INDENT_RE = re.compile(r\"(^|[^-\\s])--(?=[^-\\s]|$)\", flags=re.MULTILINE)\n\n\nSCOPED_ABBR = {\"c\": \"\u00a9\", \"r\": \"\u00ae\", \"p\": \"\u00a7\", \"tm\": \"\u2122\"}\n\n\ndef replaceFn(match: re.Match[str]):\n    return SCOPED_ABBR[match.group(1).lower()]\n\n\ndef replace_scoped(inlineTokens: list[Token]) -> None:\n    inside_autolink = 0\n\n    for token in inlineTokens:\n        if token.type == \"text\" and not inside_autolink:\n            token.content = SCOPED_ABBR_RE.sub(replaceFn, token.content)\n\n        if token.type == \"link_open\" and token.info == \"auto\":\n            inside_autolink -= 1\n\n        if token.type == \"link_close\" and token.info == \"auto\":\n            inside_autolink += 1\n\n\ndef replace_rare(inlineTokens: list[Token]) -> None:\n    inside_autolink = 0\n\n    for token in inlineTokens:\n        if token.type == \"text\" and not inside_autolink:\n            if RARE_RE.search(token.content):\n                # +- -> \u00b1\n                token.content = PLUS_MINUS_RE.sub(\"\u00b1\", token.content)\n\n                # .., ..., ....... -> \u2026\n                token.content = ELLIPSIS_RE.sub(\"\u2026\", token.content)\n\n                # but ?..... & !..... -> ?.. & !..\n                token.content = ELLIPSIS_QUESTION_EXCLAMATION_RE.sub(\n                    \"\\\\1..\", token.content\n                )\n                token.content = QUESTION_EXCLAMATION_RE.sub(\"\\\\1\\\\1\\\\1\", token.content)\n\n                # ,,  ,,,  ,,,, -> ,\n                token.content = COMMA_RE.sub(\",\", token.content)\n\n                # em-dash\n                token.content = EM_DASH_RE.sub(\"\\\\1\\u2014\", token.content)\n\n                # en-dash\n                token.content = EN_DASH_RE.sub(\"\\\\1\\u2013\", token.content)\n                token.content = EN_DASH_INDENT_RE.sub(\"\\\\1\\u2013\", token.content)\n\n        if token.type == \"link_open\" and token.info == \"auto\":\n            inside_autolink -= 1\n\n        if token.type == \"link_close\" and token.info == \"auto\":\n            inside_autolink += 1\n\n\ndef replace(state: StateCore) -> None:\n    if not state.md.options.typographer:\n        return\n\n    for token in state.tokens:\n        if token.type != \"inline\":\n            continue\n        assert token.children is not None\n\n        if SCOPED_ABBR_RE.search(token.content):\n            replace_scoped(token.children)\n\n        if RARE_RE.search(token.content):\n            replace_rare(token.children)\n", "\"\"\"Convert straight quotation marks to typographic ones\n\"\"\"\nfrom __future__ import annotations\n\nimport re\nfrom typing import Any\n\nfrom ..common.utils import charCodeAt, isMdAsciiPunct, isPunctChar, isWhiteSpace\nfrom ..token import Token\nfrom .state_core import StateCore\n\nQUOTE_TEST_RE = re.compile(r\"['\\\"]\")\nQUOTE_RE = re.compile(r\"['\\\"]\")\nAPOSTROPHE = \"\\u2019\"  # \u2019\n\n\ndef replaceAt(string: str, index: int, ch: str) -> str:\n    # When the index is negative, the behavior is different from the js version.\n    # But basically, the index will not be negative.\n    assert index >= 0\n    return string[:index] + ch + string[index + 1 :]\n\n\ndef process_inlines(tokens: list[Token], state: StateCore) -> None:\n    stack: list[dict[str, Any]] = []\n\n    for i in range(len(tokens)):\n        token = tokens[i]\n\n        thisLevel = token.level\n\n        j = 0\n        for j in range(len(stack))[::-1]:\n            if stack[j][\"level\"] <= thisLevel:\n                break\n        else:\n            # When the loop is terminated without a \"break\".\n            # Subtract 1 to get the same index as the js version.\n            j -= 1\n\n        stack = stack[: j + 1]\n\n        if token.type != \"text\":\n            continue\n\n        text = token.content\n        pos = 0\n        maximum = len(text)\n\n        while pos < maximum:\n            goto_outer = False\n            lastIndex = pos\n            t = QUOTE_RE.search(text[lastIndex:])\n            if not t:\n                break\n\n            canOpen = canClose = True\n            pos = t.start(0) + lastIndex + 1\n            isSingle = t.group(0) == \"'\"\n\n            # Find previous character,\n            # default to space if it's the beginning of the line\n            lastChar = 0x20\n\n            if t.start(0) + lastIndex - 1 >= 0:\n                lastChar = charCodeAt(text, t.start(0) + lastIndex - 1)\n            else:\n                for j in range(i)[::-1]:\n                    # lastChar defaults to 0x20\n                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":\n                        break\n                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'\n                    if not tokens[j].content:\n                        continue\n\n                    lastChar = charCodeAt(tokens[j].content, len(tokens[j].content) - 1)\n                    break\n\n            # Find next character,\n            # default to space if it's the end of the line\n            nextChar = 0x20\n\n            if pos < maximum:\n                nextChar = charCodeAt(text, pos)\n            else:\n                for j in range(i + 1, len(tokens)):\n                    # nextChar defaults to 0x20\n                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":\n                        break\n                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'\n                    if not tokens[j].content:\n                        continue\n\n                    nextChar = charCodeAt(tokens[j].content, 0)\n                    break\n\n            isLastPunctChar = isMdAsciiPunct(lastChar) or isPunctChar(chr(lastChar))\n            isNextPunctChar = isMdAsciiPunct(nextChar) or isPunctChar(chr(nextChar))\n\n            isLastWhiteSpace = isWhiteSpace(lastChar)\n            isNextWhiteSpace = isWhiteSpace(nextChar)\n\n            if isNextWhiteSpace:\n                canOpen = False\n            elif isNextPunctChar:\n                if not (isLastWhiteSpace or isLastPunctChar):\n                    canOpen = False\n\n            if isLastWhiteSpace:\n                canClose = False\n            elif isLastPunctChar:\n                if not (isNextWhiteSpace or isNextPunctChar):\n                    canClose = False\n\n            if nextChar == 0x22 and t.group(0) == '\"':  # 0x22: \"\n                if lastChar >= 0x30 and lastChar <= 0x39:  # 0x30: 0, 0x39: 9\n                    # special case: 1\"\" - count first quote as an inch\n                    canClose = canOpen = False\n\n            if canOpen and canClose:\n                # Replace quotes in the middle of punctuation sequence, but not\n                # in the middle of the words, i.e.:\n                #\n                # 1. foo \" bar \" baz - not replaced\n                # 2. foo-\"-bar-\"-baz - replaced\n                # 3. foo\"bar\"baz     - not replaced\n                canOpen = isLastPunctChar\n                canClose = isNextPunctChar\n\n            if not canOpen and not canClose:\n                # middle of word\n                if isSingle:\n                    token.content = replaceAt(\n                        token.content, t.start(0) + lastIndex, APOSTROPHE\n                    )\n                continue\n\n            if canClose:\n                # this could be a closing quote, rewind the stack to get a match\n                for j in range(len(stack))[::-1]:\n                    item = stack[j]\n                    if stack[j][\"level\"] < thisLevel:\n                        break\n                    if item[\"single\"] == isSingle and stack[j][\"level\"] == thisLevel:\n                        item = stack[j]\n\n                        if isSingle:\n                            openQuote = state.md.options.quotes[2]\n                            closeQuote = state.md.options.quotes[3]\n                        else:\n                            openQuote = state.md.options.quotes[0]\n                            closeQuote = state.md.options.quotes[1]\n\n                        # replace token.content *before* tokens[item.token].content,\n                        # because, if they are pointing at the same token, replaceAt\n                        # could mess up indices when quote length != 1\n                        token.content = replaceAt(\n                            token.content, t.start(0) + lastIndex, closeQuote\n                        )\n                        tokens[item[\"token\"]].content = replaceAt(\n                            tokens[item[\"token\"]].content, item[\"pos\"], openQuote\n                        )\n\n                        pos += len(closeQuote) - 1\n                        if item[\"token\"] == i:\n                            pos += len(openQuote) - 1\n\n                        text = token.content\n                        maximum = len(text)\n\n                        stack = stack[:j]\n                        goto_outer = True\n                        break\n                if goto_outer:\n                    goto_outer = False\n                    continue\n\n            if canOpen:\n                stack.append(\n                    {\n                        \"token\": i,\n                        \"pos\": t.start(0) + lastIndex,\n                        \"single\": isSingle,\n                        \"level\": thisLevel,\n                    }\n                )\n            elif canClose and isSingle:\n                token.content = replaceAt(\n                    token.content, t.start(0) + lastIndex, APOSTROPHE\n                )\n\n\ndef smartquotes(state: StateCore) -> None:\n    if not state.md.options.typographer:\n        return\n\n    for token in state.tokens:\n        if token.type != \"inline\" or not QUOTE_RE.search(token.content):\n            continue\n        assert token.children is not None\n        process_inlines(token.children, state)\n", "#31 empty lines after certain lists raises exception:\n.\n> a\n\n- b\n\n\n.\n<blockquote>\n<p>a</p>\n</blockquote>\n<ul>\n<li>b</li>\n</ul>\n.\n\n#50 blank lines after block quotes\n.\n> A Block Quote\n\n> Another Block Quote\n\n\n.\n<blockquote>\n<p>A Block Quote</p>\n</blockquote>\n<blockquote>\n<p>Another Block Quote</p>\n</blockquote>\n.\n\n#80 UnicodeError with codepoints larger than 0xFFFF\n.\n&#x1F4AC;\n.\n<p>\ud83d\udcac</p>\n.\n", "from pathlib import Path\n\nimport pytest\n\nfrom markdown_it import MarkdownIt\nfrom markdown_it.utils import read_fixture_file\n\nFIXTURE_PATH = Path(__file__).parent.joinpath(\"fixtures\")\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"linkify.md\")),\n)\ndef test_linkify(line, title, input, expected):\n    md = MarkdownIt().enable(\"linkify\")\n    md.options[\"linkify\"] = True\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n    # if not install linkify-it-py\n    md.linkify = None\n    with pytest.raises(ModuleNotFoundError):\n        md.render(input)\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"smartquotes.md\")),\n)\ndef test_smartquotes(line, title, input, expected):\n    md = MarkdownIt().enable(\"replacements\").enable(\"smartquotes\")\n    md.options[\"typographer\"] = True\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"typographer.md\")),\n)\ndef test_typographer(line, title, input, expected):\n    md = MarkdownIt().enable(\"replacements\")\n    md.options[\"typographer\"] = True\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\", read_fixture_file(FIXTURE_PATH.joinpath(\"tables.md\"))\n)\ndef test_table(line, title, input, expected):\n    md = MarkdownIt().enable(\"table\")\n    text = md.render(input)\n    try:\n        assert text.rstrip() == expected.rstrip()\n    except AssertionError:\n        print(text)\n        raise\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"commonmark_extras.md\")),\n)\ndef test_commonmark_extras(line, title, input, expected):\n    md = MarkdownIt(\"commonmark\")\n    md.options[\"langPrefix\"] = \"\"\n    text = md.render(input)\n    if text.rstrip() != expected.rstrip():\n        print(text)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"normalize.md\")),\n)\ndef test_normalize_url(line, title, input, expected):\n    md = MarkdownIt(\"commonmark\")\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\", read_fixture_file(FIXTURE_PATH.joinpath(\"fatal.md\"))\n)\ndef test_fatal(line, title, input, expected):\n    md = MarkdownIt(\"commonmark\").enable(\"replacements\")\n    md.options[\"typographer\"] = True\n    text = md.render(input)\n    if text.rstrip() != expected.rstrip():\n        print(text)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"strikethrough.md\")),\n)\ndef test_strikethrough(line, title, input, expected):\n    md = MarkdownIt().enable(\"strikethrough\")\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"issue-fixes.md\")),\n)\ndef test_issue_fixes(line, title, input, expected):\n    md = MarkdownIt()\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n"], "fixing_code": ["\"\"\"\nclass Renderer\n\nGenerates HTML from parsed token stream. Each instance has independent\ncopy of rules. Those can be rewritten with ease. Also, you can add new\nrules if you create plugin and adds new token types.\n\"\"\"\nfrom __future__ import annotations\n\nfrom collections.abc import MutableMapping, Sequence\nimport inspect\nfrom typing import Any, ClassVar\n\nfrom .common.utils import escapeHtml, unescapeAll\nfrom .token import Token\nfrom .utils import OptionsDict\n\ntry:\n    from typing import Protocol\nexcept ImportError:  # Python <3.8 doesn't have `Protocol` in the stdlib\n    from typing_extensions import Protocol  # type: ignore\n\n\nclass RendererProtocol(Protocol):\n    __output__: ClassVar[str]\n\n    def render(\n        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping\n    ) -> Any:\n        ...\n\n\nclass RendererHTML(RendererProtocol):\n    \"\"\"Contains render rules for tokens. Can be updated and extended.\n\n    Example:\n\n    Each rule is called as independent static function with fixed signature:\n\n    ::\n\n        class Renderer:\n            def token_type_name(self, tokens, idx, options, env) {\n                # ...\n                return renderedHTML\n\n    ::\n\n        class CustomRenderer(RendererHTML):\n            def strong_open(self, tokens, idx, options, env):\n                return '<b>'\n            def strong_close(self, tokens, idx, options, env):\n                return '</b>'\n\n        md = MarkdownIt(renderer_cls=CustomRenderer)\n\n        result = md.render(...)\n\n    See https://github.com/markdown-it/markdown-it/blob/master/lib/renderer.js\n    for more details and examples.\n    \"\"\"\n\n    __output__ = \"html\"\n\n    def __init__(self, parser=None):\n        self.rules = {\n            k: v\n            for k, v in inspect.getmembers(self, predicate=inspect.ismethod)\n            if not (k.startswith(\"render\") or k.startswith(\"_\"))\n        }\n\n    def render(\n        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping\n    ) -> str:\n        \"\"\"Takes token stream and generates HTML.\n\n        :param tokens: list on block tokens to render\n        :param options: params of parser instance\n        :param env: additional data from parsed input\n\n        \"\"\"\n        result = \"\"\n\n        for i, token in enumerate(tokens):\n            if token.type == \"inline\":\n                if token.children:\n                    result += self.renderInline(token.children, options, env)\n            elif token.type in self.rules:\n                result += self.rules[token.type](tokens, i, options, env)\n            else:\n                result += self.renderToken(tokens, i, options, env)\n\n        return result\n\n    def renderInline(\n        self, tokens: Sequence[Token], options: OptionsDict, env: MutableMapping\n    ) -> str:\n        \"\"\"The same as ``render``, but for single token of `inline` type.\n\n        :param tokens: list on block tokens to render\n        :param options: params of parser instance\n        :param env: additional data from parsed input (references, for example)\n        \"\"\"\n        result = \"\"\n\n        for i, token in enumerate(tokens):\n            if token.type in self.rules:\n                result += self.rules[token.type](tokens, i, options, env)\n            else:\n                result += self.renderToken(tokens, i, options, env)\n\n        return result\n\n    def renderToken(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        \"\"\"Default token renderer.\n\n        Can be overridden by custom function\n\n        :param idx: token index to render\n        :param options: params of parser instance\n        \"\"\"\n        result = \"\"\n        needLf = False\n        token = tokens[idx]\n\n        # Tight list paragraphs\n        if token.hidden:\n            return \"\"\n\n        # Insert a newline between hidden paragraph and subsequent opening\n        # block-level tag.\n        #\n        # For example, here we should insert a newline before blockquote:\n        #  - a\n        #    >\n        #\n        if token.block and token.nesting != -1 and idx and tokens[idx - 1].hidden:\n            result += \"\\n\"\n\n        # Add token name, e.g. `<img`\n        result += (\"</\" if token.nesting == -1 else \"<\") + token.tag\n\n        # Encode attributes, e.g. `<img src=\"foo\"`\n        result += self.renderAttrs(token)\n\n        # Add a slash for self-closing tags, e.g. `<img src=\"foo\" /`\n        if token.nesting == 0 and options[\"xhtmlOut\"]:\n            result += \" /\"\n\n        # Check if we need to add a newline after this tag\n        if token.block:\n            needLf = True\n\n            if token.nesting == 1:\n                if idx + 1 < len(tokens):\n                    nextToken = tokens[idx + 1]\n\n                    if nextToken.type == \"inline\" or nextToken.hidden:\n                        # Block-level tag containing an inline tag.\n                        #\n                        needLf = False\n\n                    elif nextToken.nesting == -1 and nextToken.tag == token.tag:\n                        # Opening tag + closing tag of the same type. E.g. `<li></li>`.\n                        #\n                        needLf = False\n\n        result += \">\\n\" if needLf else \">\"\n\n        return result\n\n    @staticmethod\n    def renderAttrs(token: Token) -> str:\n        \"\"\"Render token attributes to string.\"\"\"\n        result = \"\"\n\n        for key, value in token.attrItems():\n            result += \" \" + escapeHtml(key) + '=\"' + escapeHtml(str(value)) + '\"'\n\n        return result\n\n    def renderInlineAsText(\n        self,\n        tokens: Sequence[Token] | None,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        \"\"\"Special kludge for image `alt` attributes to conform CommonMark spec.\n\n        Don't try to use it! Spec requires to show `alt` content with stripped markup,\n        instead of simple escaping.\n\n        :param tokens: list on block tokens to render\n        :param options: params of parser instance\n        :param env: additional data from parsed input\n        \"\"\"\n        result = \"\"\n\n        for token in tokens or []:\n            if token.type == \"text\":\n                result += token.content\n            elif token.type == \"image\":\n                if token.children:\n                    result += self.renderInlineAsText(token.children, options, env)\n            elif token.type == \"softbreak\":\n                result += \"\\n\"\n\n        return result\n\n    ###################################################\n\n    def code_inline(self, tokens: Sequence[Token], idx: int, options, env) -> str:\n        token = tokens[idx]\n        return (\n            \"<code\"\n            + self.renderAttrs(token)\n            + \">\"\n            + escapeHtml(tokens[idx].content)\n            + \"</code>\"\n        )\n\n    def code_block(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n\n        return (\n            \"<pre\"\n            + self.renderAttrs(token)\n            + \"><code>\"\n            + escapeHtml(tokens[idx].content)\n            + \"</code></pre>\\n\"\n        )\n\n    def fence(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n        info = unescapeAll(token.info).strip() if token.info else \"\"\n        langName = \"\"\n        langAttrs = \"\"\n\n        if info:\n            arr = info.split(maxsplit=1)\n            langName = arr[0]\n            if len(arr) == 2:\n                langAttrs = arr[1]\n\n        if options.highlight:\n            highlighted = options.highlight(\n                token.content, langName, langAttrs\n            ) or escapeHtml(token.content)\n        else:\n            highlighted = escapeHtml(token.content)\n\n        if highlighted.startswith(\"<pre\"):\n            return highlighted + \"\\n\"\n\n        # If language exists, inject class gently, without modifying original token.\n        # May be, one day we will add .deepClone() for token and simplify this part, but\n        # now we prefer to keep things local.\n        if info:\n            # Fake token just to render attributes\n            tmpToken = Token(type=\"\", tag=\"\", nesting=0, attrs=token.attrs.copy())\n            tmpToken.attrJoin(\"class\", options.langPrefix + langName)\n\n            return (\n                \"<pre><code\"\n                + self.renderAttrs(tmpToken)\n                + \">\"\n                + highlighted\n                + \"</code></pre>\\n\"\n            )\n\n        return (\n            \"<pre><code\"\n            + self.renderAttrs(token)\n            + \">\"\n            + highlighted\n            + \"</code></pre>\\n\"\n        )\n\n    def image(\n        self,\n        tokens: Sequence[Token],\n        idx: int,\n        options: OptionsDict,\n        env: MutableMapping,\n    ) -> str:\n        token = tokens[idx]\n\n        # \"alt\" attr MUST be set, even if empty. Because it's mandatory and\n        # should be placed on proper position for tests.\n        if token.children:\n            token.attrSet(\"alt\", self.renderInlineAsText(token.children, options, env))\n        else:\n            token.attrSet(\"alt\", \"\")\n\n        return self.renderToken(tokens, idx, options, env)\n\n    def hardbreak(\n        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args\n    ) -> str:\n        return \"<br />\\n\" if options.xhtmlOut else \"<br>\\n\"\n\n    def softbreak(\n        self, tokens: Sequence[Token], idx: int, options: OptionsDict, *args\n    ) -> str:\n        return (\n            (\"<br />\\n\" if options.xhtmlOut else \"<br>\\n\") if options.breaks else \"\\n\"\n        )\n\n    def text(self, tokens: Sequence[Token], idx: int, *args) -> str:\n        return escapeHtml(tokens[idx].content)\n\n    def html_block(self, tokens: Sequence[Token], idx: int, *args) -> str:\n        return tokens[idx].content\n\n    def html_inline(self, tokens: Sequence[Token], idx: int, *args) -> str:\n        return tokens[idx].content\n", "\"\"\"Simple typographic replacements\n\n* ``(c)``, ``(C)`` \u2192 \u00a9\n* ``(tm)``, ``(TM)`` \u2192 \u2122\n* ``(r)``, ``(R)`` \u2192 \u00ae\n* ``(p)``, ``(P)`` \u2192 \u00a7\n* ``+-`` \u2192 \u00b1\n* ``...`` \u2192 \u2026\n* ``?....`` \u2192 ?..\n* ``!....`` \u2192 !..\n* ``????????`` \u2192 ???\n* ``!!!!!`` \u2192 !!!\n* ``,,,`` \u2192 ,\n* ``--`` \u2192 &ndash\n* ``---`` \u2192 &mdash\n\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport re\n\nfrom ..token import Token\nfrom .state_core import StateCore\n\nLOGGER = logging.getLogger(__name__)\n\n# TODO:\n# - fractionals 1/2, 1/4, 3/4 -> \u00bd, \u00bc, \u00be\n# - miltiplication 2 x 4 -> 2 \u00d7 4\n\nRARE_RE = re.compile(r\"\\+-|\\.\\.|\\?\\?\\?\\?|!!!!|,,|--\")\n\n# Workaround for phantomjs - need regex without /g flag,\n# or root check will fail every second time\n# SCOPED_ABBR_TEST_RE = r\"\\((c|tm|r|p)\\)\"\n\nSCOPED_ABBR_RE = re.compile(r\"\\((c|tm|r|p)\\)\", flags=re.IGNORECASE)\n\nPLUS_MINUS_RE = re.compile(r\"\\+-\")\n\nELLIPSIS_RE = re.compile(r\"\\.{2,}\")\n\nELLIPSIS_QUESTION_EXCLAMATION_RE = re.compile(r\"([?!])\u2026\")\n\nQUESTION_EXCLAMATION_RE = re.compile(r\"([?!]){4,}\")\n\nCOMMA_RE = re.compile(r\",{2,}\")\n\nEM_DASH_RE = re.compile(r\"(^|[^-])---(?=[^-]|$)\", flags=re.MULTILINE)\n\nEN_DASH_RE = re.compile(r\"(^|\\s)--(?=\\s|$)\", flags=re.MULTILINE)\n\nEN_DASH_INDENT_RE = re.compile(r\"(^|[^-\\s])--(?=[^-\\s]|$)\", flags=re.MULTILINE)\n\n\nSCOPED_ABBR = {\"c\": \"\u00a9\", \"r\": \"\u00ae\", \"p\": \"\u00a7\", \"tm\": \"\u2122\"}\n\n\ndef replaceFn(match: re.Match[str]):\n    return SCOPED_ABBR[match.group(1).lower()]\n\n\ndef replace_scoped(inlineTokens: list[Token]) -> None:\n    inside_autolink = 0\n\n    for token in inlineTokens:\n        if token.type == \"text\" and not inside_autolink:\n            token.content = SCOPED_ABBR_RE.sub(replaceFn, token.content)\n\n        if token.type == \"link_open\" and token.info == \"auto\":\n            inside_autolink -= 1\n\n        if token.type == \"link_close\" and token.info == \"auto\":\n            inside_autolink += 1\n\n\ndef replace_rare(inlineTokens: list[Token]) -> None:\n    inside_autolink = 0\n\n    for token in inlineTokens:\n        if token.type == \"text\" and not inside_autolink:\n            if RARE_RE.search(token.content):\n                # +- -> \u00b1\n                token.content = PLUS_MINUS_RE.sub(\"\u00b1\", token.content)\n\n                # .., ..., ....... -> \u2026\n                token.content = ELLIPSIS_RE.sub(\"\u2026\", token.content)\n\n                # but ?..... & !..... -> ?.. & !..\n                token.content = ELLIPSIS_QUESTION_EXCLAMATION_RE.sub(\n                    \"\\\\1..\", token.content\n                )\n                token.content = QUESTION_EXCLAMATION_RE.sub(\"\\\\1\\\\1\\\\1\", token.content)\n\n                # ,,  ,,,  ,,,, -> ,\n                token.content = COMMA_RE.sub(\",\", token.content)\n\n                # em-dash\n                token.content = EM_DASH_RE.sub(\"\\\\1\\u2014\", token.content)\n\n                # en-dash\n                token.content = EN_DASH_RE.sub(\"\\\\1\\u2013\", token.content)\n                token.content = EN_DASH_INDENT_RE.sub(\"\\\\1\\u2013\", token.content)\n\n        if token.type == \"link_open\" and token.info == \"auto\":\n            inside_autolink -= 1\n\n        if token.type == \"link_close\" and token.info == \"auto\":\n            inside_autolink += 1\n\n\ndef replace(state: StateCore) -> None:\n    if not state.md.options.typographer:\n        return\n\n    for token in state.tokens:\n        if token.type != \"inline\":\n            continue\n        if token.children is None:\n            continue\n\n        if SCOPED_ABBR_RE.search(token.content):\n            replace_scoped(token.children)\n\n        if RARE_RE.search(token.content):\n            replace_rare(token.children)\n", "\"\"\"Convert straight quotation marks to typographic ones\n\"\"\"\nfrom __future__ import annotations\n\nimport re\nfrom typing import Any\n\nfrom ..common.utils import charCodeAt, isMdAsciiPunct, isPunctChar, isWhiteSpace\nfrom ..token import Token\nfrom .state_core import StateCore\n\nQUOTE_TEST_RE = re.compile(r\"['\\\"]\")\nQUOTE_RE = re.compile(r\"['\\\"]\")\nAPOSTROPHE = \"\\u2019\"  # \u2019\n\n\ndef replaceAt(string: str, index: int, ch: str) -> str:\n    # When the index is negative, the behavior is different from the js version.\n    # But basically, the index will not be negative.\n    assert index >= 0\n    return string[:index] + ch + string[index + 1 :]\n\n\ndef process_inlines(tokens: list[Token], state: StateCore) -> None:\n    stack: list[dict[str, Any]] = []\n\n    for i in range(len(tokens)):\n        token = tokens[i]\n\n        thisLevel = token.level\n\n        j = 0\n        for j in range(len(stack))[::-1]:\n            if stack[j][\"level\"] <= thisLevel:\n                break\n        else:\n            # When the loop is terminated without a \"break\".\n            # Subtract 1 to get the same index as the js version.\n            j -= 1\n\n        stack = stack[: j + 1]\n\n        if token.type != \"text\":\n            continue\n\n        text = token.content\n        pos = 0\n        maximum = len(text)\n\n        while pos < maximum:\n            goto_outer = False\n            lastIndex = pos\n            t = QUOTE_RE.search(text[lastIndex:])\n            if not t:\n                break\n\n            canOpen = canClose = True\n            pos = t.start(0) + lastIndex + 1\n            isSingle = t.group(0) == \"'\"\n\n            # Find previous character,\n            # default to space if it's the beginning of the line\n            lastChar = 0x20\n\n            if t.start(0) + lastIndex - 1 >= 0:\n                lastChar = charCodeAt(text, t.start(0) + lastIndex - 1)\n            else:\n                for j in range(i)[::-1]:\n                    # lastChar defaults to 0x20\n                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":\n                        break\n                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'\n                    if not tokens[j].content:\n                        continue\n\n                    lastChar = charCodeAt(tokens[j].content, len(tokens[j].content) - 1)\n                    break\n\n            # Find next character,\n            # default to space if it's the end of the line\n            nextChar = 0x20\n\n            if pos < maximum:\n                nextChar = charCodeAt(text, pos)\n            else:\n                for j in range(i + 1, len(tokens)):\n                    # nextChar defaults to 0x20\n                    if tokens[j].type == \"softbreak\" or tokens[j].type == \"hardbreak\":\n                        break\n                    # should skip all tokens except 'text', 'html_inline' or 'code_inline'\n                    if not tokens[j].content:\n                        continue\n\n                    nextChar = charCodeAt(tokens[j].content, 0)\n                    break\n\n            isLastPunctChar = isMdAsciiPunct(lastChar) or isPunctChar(chr(lastChar))\n            isNextPunctChar = isMdAsciiPunct(nextChar) or isPunctChar(chr(nextChar))\n\n            isLastWhiteSpace = isWhiteSpace(lastChar)\n            isNextWhiteSpace = isWhiteSpace(nextChar)\n\n            if isNextWhiteSpace:\n                canOpen = False\n            elif isNextPunctChar:\n                if not (isLastWhiteSpace or isLastPunctChar):\n                    canOpen = False\n\n            if isLastWhiteSpace:\n                canClose = False\n            elif isLastPunctChar:\n                if not (isNextWhiteSpace or isNextPunctChar):\n                    canClose = False\n\n            if nextChar == 0x22 and t.group(0) == '\"':  # 0x22: \"\n                if lastChar >= 0x30 and lastChar <= 0x39:  # 0x30: 0, 0x39: 9\n                    # special case: 1\"\" - count first quote as an inch\n                    canClose = canOpen = False\n\n            if canOpen and canClose:\n                # Replace quotes in the middle of punctuation sequence, but not\n                # in the middle of the words, i.e.:\n                #\n                # 1. foo \" bar \" baz - not replaced\n                # 2. foo-\"-bar-\"-baz - replaced\n                # 3. foo\"bar\"baz     - not replaced\n                canOpen = isLastPunctChar\n                canClose = isNextPunctChar\n\n            if not canOpen and not canClose:\n                # middle of word\n                if isSingle:\n                    token.content = replaceAt(\n                        token.content, t.start(0) + lastIndex, APOSTROPHE\n                    )\n                continue\n\n            if canClose:\n                # this could be a closing quote, rewind the stack to get a match\n                for j in range(len(stack))[::-1]:\n                    item = stack[j]\n                    if stack[j][\"level\"] < thisLevel:\n                        break\n                    if item[\"single\"] == isSingle and stack[j][\"level\"] == thisLevel:\n                        item = stack[j]\n\n                        if isSingle:\n                            openQuote = state.md.options.quotes[2]\n                            closeQuote = state.md.options.quotes[3]\n                        else:\n                            openQuote = state.md.options.quotes[0]\n                            closeQuote = state.md.options.quotes[1]\n\n                        # replace token.content *before* tokens[item.token].content,\n                        # because, if they are pointing at the same token, replaceAt\n                        # could mess up indices when quote length != 1\n                        token.content = replaceAt(\n                            token.content, t.start(0) + lastIndex, closeQuote\n                        )\n                        tokens[item[\"token\"]].content = replaceAt(\n                            tokens[item[\"token\"]].content, item[\"pos\"], openQuote\n                        )\n\n                        pos += len(closeQuote) - 1\n                        if item[\"token\"] == i:\n                            pos += len(openQuote) - 1\n\n                        text = token.content\n                        maximum = len(text)\n\n                        stack = stack[:j]\n                        goto_outer = True\n                        break\n                if goto_outer:\n                    goto_outer = False\n                    continue\n\n            if canOpen:\n                stack.append(\n                    {\n                        \"token\": i,\n                        \"pos\": t.start(0) + lastIndex,\n                        \"single\": isSingle,\n                        \"level\": thisLevel,\n                    }\n                )\n            elif canClose and isSingle:\n                token.content = replaceAt(\n                    token.content, t.start(0) + lastIndex, APOSTROPHE\n                )\n\n\ndef smartquotes(state: StateCore) -> None:\n    if not state.md.options.typographer:\n        return\n\n    for token in state.tokens:\n        if token.type != \"inline\" or not QUOTE_RE.search(token.content):\n            continue\n        if token.children is not None:\n            process_inlines(token.children, state)\n", "#31 empty lines after certain lists raises exception:\n.\n> a\n\n- b\n\n\n.\n<blockquote>\n<p>a</p>\n</blockquote>\n<ul>\n<li>b</li>\n</ul>\n.\n\n#50 blank lines after block quotes\n.\n> A Block Quote\n\n> Another Block Quote\n\n\n.\n<blockquote>\n<p>A Block Quote</p>\n</blockquote>\n<blockquote>\n<p>Another Block Quote</p>\n</blockquote>\n.\n\n#80 UnicodeError with codepoints larger than 0xFFFF\n.\n&#x1F4AC;\n.\n<p>\ud83d\udcac</p>\n.\n\nFix CVE-2023-26303\n.\n![![]()\n]([)\n.\n<p><img src=\"%5B\" alt=\"\n\" /></p>\n.\n", "from pathlib import Path\n\nimport pytest\n\nfrom markdown_it import MarkdownIt\nfrom markdown_it.utils import read_fixture_file\n\nFIXTURE_PATH = Path(__file__).parent.joinpath(\"fixtures\")\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"linkify.md\")),\n)\ndef test_linkify(line, title, input, expected):\n    md = MarkdownIt().enable(\"linkify\")\n    md.options[\"linkify\"] = True\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n    # if not install linkify-it-py\n    md.linkify = None\n    with pytest.raises(ModuleNotFoundError):\n        md.render(input)\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"smartquotes.md\")),\n)\ndef test_smartquotes(line, title, input, expected):\n    md = MarkdownIt().enable(\"replacements\").enable(\"smartquotes\")\n    md.options[\"typographer\"] = True\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"typographer.md\")),\n)\ndef test_typographer(line, title, input, expected):\n    md = MarkdownIt().enable(\"replacements\")\n    md.options[\"typographer\"] = True\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\", read_fixture_file(FIXTURE_PATH.joinpath(\"tables.md\"))\n)\ndef test_table(line, title, input, expected):\n    md = MarkdownIt().enable(\"table\")\n    text = md.render(input)\n    try:\n        assert text.rstrip() == expected.rstrip()\n    except AssertionError:\n        print(text)\n        raise\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"commonmark_extras.md\")),\n)\ndef test_commonmark_extras(line, title, input, expected):\n    md = MarkdownIt(\"commonmark\")\n    md.options[\"langPrefix\"] = \"\"\n    text = md.render(input)\n    if text.rstrip() != expected.rstrip():\n        print(text)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"normalize.md\")),\n)\ndef test_normalize_url(line, title, input, expected):\n    md = MarkdownIt(\"commonmark\")\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\", read_fixture_file(FIXTURE_PATH.joinpath(\"fatal.md\"))\n)\ndef test_fatal(line, title, input, expected):\n    md = MarkdownIt(\"commonmark\").enable(\"replacements\")\n    md.options[\"typographer\"] = True\n    text = md.render(input)\n    if text.rstrip() != expected.rstrip():\n        print(text)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"strikethrough.md\")),\n)\ndef test_strikethrough(line, title, input, expected):\n    md = MarkdownIt().enable(\"strikethrough\")\n    text = md.render(input)\n    assert text.rstrip() == expected.rstrip()\n\n\n@pytest.mark.parametrize(\n    \"line,title,input,expected\",\n    read_fixture_file(FIXTURE_PATH.joinpath(\"issue-fixes.md\")),\n)\ndef test_issue_fixes(line, title, input, expected):\n    md = MarkdownIt()\n    text = md.render(input)\n    print(text)\n    assert text.rstrip() == expected.rstrip()\n"], "filenames": ["markdown_it/renderer.py", "markdown_it/rules_core/replacements.py", "markdown_it/rules_core/smartquotes.py", "tests/test_port/fixtures/issue-fixes.md", "tests/test_port/test_fixtures.py"], "buggy_code_start_loc": [86, 119, 200, 38, 113], "buggy_code_end_loc": [316, 120, 202, 38, 113], "fixing_code_start_loc": [86, 119, 200, 39, 114], "fixing_code_end_loc": [312, 121, 202, 48, 115], "type": "NVD-CWE-Other", "message": "Denial of service could be caused to markdown-it-py, before v2.2.0, if an attacker was allowed to force null assertions with specially crafted input.", "other": {"cve": {"id": "CVE-2023-26303", "sourceIdentifier": "security@ubuntu.com", "published": "2023-02-23T00:15:11.390", "lastModified": "2023-03-03T02:20:23.303", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Denial of service could be caused to markdown-it-py, before v2.2.0, if an attacker was allowed to force null assertions with specially crafted input."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security@ubuntu.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 3.3, "baseSeverity": "LOW"}, "exploitabilityScore": 1.8, "impactScore": 1.4}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}, {"source": "security@ubuntu.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-173"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:executablebooks:markdown-it-py:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.2.0", "matchCriteriaId": "5E45476E-D78B-486D-9B62-30E7573AAA6A"}]}]}], "references": [{"url": "https://github.com/executablebooks/markdown-it-py/commit/ae03c6107dfa18e648f6fdd1280f5b89092d5d49", "source": "security@ubuntu.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/executablebooks/markdown-it-py/commit/ae03c6107dfa18e648f6fdd1280f5b89092d5d49"}}