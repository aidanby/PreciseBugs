{"buggy_code": ["/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <net/inet_hashtables.h>\n\n#include \"rds.h\"\n#include \"loop.h\"\n\n#define RDS_CONNECTION_HASH_BITS 12\n#define RDS_CONNECTION_HASH_ENTRIES (1 << RDS_CONNECTION_HASH_BITS)\n#define RDS_CONNECTION_HASH_MASK (RDS_CONNECTION_HASH_ENTRIES - 1)\n\n/* converting this to RCU is a chore for another day.. */\nstatic DEFINE_SPINLOCK(rds_conn_lock);\nstatic unsigned long rds_conn_count;\nstatic struct hlist_head rds_conn_hash[RDS_CONNECTION_HASH_ENTRIES];\nstatic struct kmem_cache *rds_conn_slab;\n\nstatic struct hlist_head *rds_conn_bucket(__be32 laddr, __be32 faddr)\n{\n\tstatic u32 rds_hash_secret __read_mostly;\n\n\tunsigned long hash;\n\n\tnet_get_random_once(&rds_hash_secret, sizeof(rds_hash_secret));\n\n\t/* Pass NULL, don't need struct net for hash */\n\thash = __inet_ehashfn(be32_to_cpu(laddr), 0,\n\t\t\t      be32_to_cpu(faddr), 0,\n\t\t\t      rds_hash_secret);\n\treturn &rds_conn_hash[hash & RDS_CONNECTION_HASH_MASK];\n}\n\n#define rds_conn_info_set(var, test, suffix) do {\t\t\\\n\tif (test)\t\t\t\t\t\t\\\n\t\tvar |= RDS_INFO_CONNECTION_FLAG_##suffix;\t\\\n} while (0)\n\n/* rcu read lock must be held or the connection spinlock */\nstatic struct rds_connection *rds_conn_lookup(struct net *net,\n\t\t\t\t\t      struct hlist_head *head,\n\t\t\t\t\t      __be32 laddr, __be32 faddr,\n\t\t\t\t\t      struct rds_transport *trans)\n{\n\tstruct rds_connection *conn, *ret = NULL;\n\n\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\tif (conn->c_faddr == faddr && conn->c_laddr == laddr &&\n\t\t    conn->c_trans == trans && net == rds_conn_net(conn)) {\n\t\t\tret = conn;\n\t\t\tbreak;\n\t\t}\n\t}\n\trdsdebug(\"returning conn %p for %pI4 -> %pI4\\n\", ret,\n\t\t &laddr, &faddr);\n\treturn ret;\n}\n\n/*\n * This is called by transports as they're bringing down a connection.\n * It clears partial message state so that the transport can start sending\n * and receiving over this connection again in the future.  It is up to\n * the transport to have serialized this call with its send and recv.\n */\nstatic void rds_conn_reset(struct rds_connection *conn)\n{\n\trdsdebug(\"connection %pI4 to %pI4 reset\\n\",\n\t  &conn->c_laddr, &conn->c_faddr);\n\n\trds_stats_inc(s_conn_reset);\n\trds_send_reset(conn);\n\tconn->c_flags = 0;\n\n\t/* Do not clear next_rx_seq here, else we cannot distinguish\n\t * retransmitted packets from new packets, and will hand all\n\t * of them to the application. That is not consistent with the\n\t * reliability guarantees of RDS. */\n}\n\n/*\n * There is only every one 'conn' for a given pair of addresses in the\n * system at a time.  They contain messages to be retransmitted and so\n * span the lifetime of the actual underlying transport connections.\n *\n * For now they are not garbage collected once they're created.  They\n * are torn down as the module is removed, if ever.\n */\nstatic struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp,\n\t\t\t\t       int is_outgoing)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret;\n\tstruct rds_transport *otrans = trans;\n\n\tif (!is_outgoing && otrans->t_type == RDS_TRANS_TCP)\n\t\tgoto new_conn;\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans);\n\tif (conn && conn->c_loopback && conn->c_trans != &rds_loop_transport &&\n\t    laddr == faddr && !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\nnew_conn:\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = laddr;\n\tconn->c_faddr = faddr;\n\tspin_lock_init(&conn->c_lock);\n\tconn->c_next_tx_seq = 1;\n\trds_conn_net_set(conn, net);\n\n\tinit_waitqueue_head(&conn->c_waitq);\n\tINIT_LIST_HEAD(&conn->c_send_queue);\n\tINIT_LIST_HEAD(&conn->c_retrans);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (is_outgoing && trans->t_prefer_loopback) {\n\t\t\t/* \"outgoing\" connection - and the transport\n\t\t\t * says it wants the connection handled by the\n\t\t\t * loopback transport. This is what TCP does.\n\t\t\t */\n\t\t\ttrans = &rds_loop_transport;\n\t\t}\n\t}\n\n\tconn->c_trans = trans;\n\n\tret = trans->conn_alloc(conn, gfp);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tatomic_set(&conn->c_state, RDS_CONN_DOWN);\n\tconn->c_send_gen = 0;\n\tconn->c_reconnect_jiffies = 0;\n\tINIT_DELAYED_WORK(&conn->c_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&conn->c_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&conn->c_conn_w, rds_connect_worker);\n\tINIT_WORK(&conn->c_down_w, rds_shutdown_worker);\n\tmutex_init(&conn->c_cm_lock);\n\tconn->c_flags = 0;\n\n\trdsdebug(\"allocated conn %p for %pI4 -> %pI4 over %s %s\\n\",\n\t  conn, &laddr, &faddr,\n\t  trans->t_name ? trans->t_name : \"[unknown]\",\n\t  is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tif (!is_outgoing && otrans->t_type == RDS_TRANS_TCP)\n\t\t\tfound = NULL;\n\t\telse\n\t\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans);\n\t\tif (found) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\tif ((is_outgoing && otrans->t_type == RDS_TRANS_TCP) ||\n\t\t\t    (otrans->t_type != RDS_TRANS_TCP)) {\n\t\t\t\t/* Only the active side should be added to\n\t\t\t\t * reconnect list for TCP.\n\t\t\t\t */\n\t\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\t}\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\nout:\n\treturn conn;\n}\n\nstruct rds_connection *rds_conn_create(struct net *net,\n\t\t\t\t       __be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, 0);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create);\n\nstruct rds_connection *rds_conn_create_outgoing(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, 1);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create_outgoing);\n\nvoid rds_conn_shutdown(struct rds_connection *conn)\n{\n\t/* shut it down unless it's down already */\n\tif (!rds_conn_transition(conn, RDS_CONN_DOWN, RDS_CONN_DOWN)) {\n\t\t/*\n\t\t * Quiesce the connection mgmt handlers before we start tearing\n\t\t * things down. We don't hold the mutex for the entire\n\t\t * duration of the shutdown operation, else we may be\n\t\t * deadlocking with the CM handler. Instead, the CM event\n\t\t * handler is supposed to check for state DISCONNECTING\n\t\t */\n\t\tmutex_lock(&conn->c_cm_lock);\n\t\tif (!rds_conn_transition(conn, RDS_CONN_UP, RDS_CONN_DISCONNECTING)\n\t\t && !rds_conn_transition(conn, RDS_CONN_ERROR, RDS_CONN_DISCONNECTING)) {\n\t\t\trds_conn_error(conn, \"shutdown called in state %d\\n\",\n\t\t\t\t\tatomic_read(&conn->c_state));\n\t\t\tmutex_unlock(&conn->c_cm_lock);\n\t\t\treturn;\n\t\t}\n\t\tmutex_unlock(&conn->c_cm_lock);\n\n\t\twait_event(conn->c_waitq,\n\t\t\t   !test_bit(RDS_IN_XMIT, &conn->c_flags));\n\t\twait_event(conn->c_waitq,\n\t\t\t   !test_bit(RDS_RECV_REFILL, &conn->c_flags));\n\n\t\tconn->c_trans->conn_shutdown(conn);\n\t\trds_conn_reset(conn);\n\n\t\tif (!rds_conn_transition(conn, RDS_CONN_DISCONNECTING, RDS_CONN_DOWN)) {\n\t\t\t/* This can happen - eg when we're in the middle of tearing\n\t\t\t * down the connection, and someone unloads the rds module.\n\t\t\t * Quite reproduceable with loopback connections.\n\t\t\t * Mostly harmless.\n\t\t\t */\n\t\t\trds_conn_error(conn,\n\t\t\t\t\"%s: failed to transition to state DOWN, \"\n\t\t\t\t\"current state is %d\\n\",\n\t\t\t\t__func__,\n\t\t\t\tatomic_read(&conn->c_state));\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Then reconnect if it's still live.\n\t * The passive side of an IB loopback connection is never added\n\t * to the conn hash, so we never trigger a reconnect on this\n\t * conn - the reconnect is always triggered by the active peer. */\n\tcancel_delayed_work_sync(&conn->c_conn_w);\n\trcu_read_lock();\n\tif (!hlist_unhashed(&conn->c_hash_node)) {\n\t\trcu_read_unlock();\n\t\trds_queue_reconnect(conn);\n\t} else {\n\t\trcu_read_unlock();\n\t}\n}\n\n/*\n * Stop and free a connection.\n *\n * This can only be used in very limited circumstances.  It assumes that once\n * the conn has been shutdown that no one else is referencing the connection.\n * We can only ensure this in the rmmod path in the current code.\n */\nvoid rds_conn_destroy(struct rds_connection *conn)\n{\n\tstruct rds_message *rm, *rtmp;\n\tunsigned long flags;\n\n\trdsdebug(\"freeing conn %p for %pI4 -> \"\n\t\t \"%pI4\\n\", conn, &conn->c_laddr,\n\t\t &conn->c_faddr);\n\n\t/* Ensure conn will not be scheduled for reconnect */\n\tspin_lock_irq(&rds_conn_lock);\n\thlist_del_init_rcu(&conn->c_hash_node);\n\tspin_unlock_irq(&rds_conn_lock);\n\tsynchronize_rcu();\n\n\t/* shut the connection down */\n\trds_conn_drop(conn);\n\tflush_work(&conn->c_down_w);\n\n\t/* make sure lingering queued work won't try to ref the conn */\n\tcancel_delayed_work_sync(&conn->c_send_w);\n\tcancel_delayed_work_sync(&conn->c_recv_w);\n\n\t/* tear down queued messages */\n\tlist_for_each_entry_safe(rm, rtmp,\n\t\t\t\t &conn->c_send_queue,\n\t\t\t\t m_conn_item) {\n\t\tlist_del_init(&rm->m_conn_item);\n\t\tBUG_ON(!list_empty(&rm->m_sock_item));\n\t\trds_message_put(rm);\n\t}\n\tif (conn->c_xmit_rm)\n\t\trds_message_put(conn->c_xmit_rm);\n\n\tconn->c_trans->conn_free(conn->c_transport_data);\n\n\t/*\n\t * The congestion maps aren't freed up here.  They're\n\t * freed by rds_cong_exit() after all the connections\n\t * have been freed.\n\t */\n\trds_cong_remove_conn(conn);\n\n\tBUG_ON(!list_empty(&conn->c_retrans));\n\tkmem_cache_free(rds_conn_slab, conn);\n\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\trds_conn_count--;\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n}\nEXPORT_SYMBOL_GPL(rds_conn_destroy);\n\nstatic void rds_conn_message_info(struct socket *sock, unsigned int len,\n\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t  struct rds_info_lengths *lens,\n\t\t\t\t  int want_send)\n{\n\tstruct hlist_head *head;\n\tstruct list_head *list;\n\tstruct rds_connection *conn;\n\tstruct rds_message *rm;\n\tunsigned int total = 0;\n\tunsigned long flags;\n\tsize_t i;\n\n\tlen /= sizeof(struct rds_info_message);\n\n\trcu_read_lock();\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\t\tif (want_send)\n\t\t\t\tlist = &conn->c_send_queue;\n\t\t\telse\n\t\t\t\tlist = &conn->c_retrans;\n\n\t\t\tspin_lock_irqsave(&conn->c_lock, flags);\n\n\t\t\t/* XXX too lazy to maintain counts.. */\n\t\t\tlist_for_each_entry(rm, list, m_conn_item) {\n\t\t\t\ttotal++;\n\t\t\t\tif (total <= len)\n\t\t\t\t\trds_inc_info_copy(&rm->m_inc, iter,\n\t\t\t\t\t\t\t  conn->c_laddr,\n\t\t\t\t\t\t\t  conn->c_faddr, 0);\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(&conn->c_lock, flags);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tlens->nr = total;\n\tlens->each = sizeof(struct rds_info_message);\n}\n\nstatic void rds_conn_message_info_send(struct socket *sock, unsigned int len,\n\t\t\t\t       struct rds_info_iterator *iter,\n\t\t\t\t       struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 1);\n}\n\nstatic void rds_conn_message_info_retrans(struct socket *sock,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t\t  struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 0);\n}\n\nvoid rds_for_each_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens,\n\t\t\t  int (*visitor)(struct rds_connection *, void *),\n\t\t\t  size_t item_len)\n{\n\tuint64_t buffer[(item_len + 7) / 8];\n\tstruct hlist_head *head;\n\tstruct rds_connection *conn;\n\tsize_t i;\n\n\trcu_read_lock();\n\n\tlens->nr = 0;\n\tlens->each = item_len;\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\n\t\t\t/* XXX no c_lock usage.. */\n\t\t\tif (!visitor(conn, buffer))\n\t\t\t\tcontinue;\n\n\t\t\t/* We copy as much as we can fit in the buffer,\n\t\t\t * but we count all items so that the caller\n\t\t\t * can resize the buffer. */\n\t\t\tif (len >= item_len) {\n\t\t\t\trds_info_copy(iter, buffer, item_len);\n\t\t\t\tlen -= item_len;\n\t\t\t}\n\t\t\tlens->nr++;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_for_each_conn_info);\n\nstatic int rds_conn_info_visitor(struct rds_connection *conn,\n\t\t\t\t  void *buffer)\n{\n\tstruct rds_info_connection *cinfo = buffer;\n\n\tcinfo->next_tx_seq = conn->c_next_tx_seq;\n\tcinfo->next_rx_seq = conn->c_next_rx_seq;\n\tcinfo->laddr = conn->c_laddr;\n\tcinfo->faddr = conn->c_faddr;\n\tstrncpy(cinfo->transport, conn->c_trans->t_name,\n\t\tsizeof(cinfo->transport));\n\tcinfo->flags = 0;\n\n\trds_conn_info_set(cinfo->flags, test_bit(RDS_IN_XMIT, &conn->c_flags),\n\t\t\t  SENDING);\n\t/* XXX Future: return the state rather than these funky bits */\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&conn->c_state) == RDS_CONN_CONNECTING,\n\t\t\t  CONNECTING);\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&conn->c_state) == RDS_CONN_UP,\n\t\t\t  CONNECTED);\n\treturn 1;\n}\n\nstatic void rds_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens)\n{\n\trds_for_each_conn_info(sock, len, iter, lens,\n\t\t\t\trds_conn_info_visitor,\n\t\t\t\tsizeof(struct rds_info_connection));\n}\n\nint rds_conn_init(void)\n{\n\trds_conn_slab = kmem_cache_create(\"rds_connection\",\n\t\t\t\t\t  sizeof(struct rds_connection),\n\t\t\t\t\t  0, 0, NULL);\n\tif (!rds_conn_slab)\n\t\treturn -ENOMEM;\n\n\trds_info_register_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_register_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t       rds_conn_message_info_send);\n\trds_info_register_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t       rds_conn_message_info_retrans);\n\n\treturn 0;\n}\n\nvoid rds_conn_exit(void)\n{\n\trds_loop_exit();\n\n\tWARN_ON(!hlist_empty(rds_conn_hash));\n\n\tkmem_cache_destroy(rds_conn_slab);\n\n\trds_info_deregister_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_deregister_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t\t rds_conn_message_info_send);\n\trds_info_deregister_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t\t rds_conn_message_info_retrans);\n}\n\n/*\n * Force a disconnect\n */\nvoid rds_conn_drop(struct rds_connection *conn)\n{\n\tatomic_set(&conn->c_state, RDS_CONN_ERROR);\n\tqueue_work(rds_wq, &conn->c_down_w);\n}\nEXPORT_SYMBOL_GPL(rds_conn_drop);\n\n/*\n * If the connection is down, trigger a connect. We may have scheduled a\n * delayed reconnect however - in this case we should not interfere.\n */\nvoid rds_conn_connect_if_down(struct rds_connection *conn)\n{\n\tif (rds_conn_state(conn) == RDS_CONN_DOWN &&\n\t    !test_and_set_bit(RDS_RECONNECT_PENDING, &conn->c_flags))\n\t\tqueue_delayed_work(rds_wq, &conn->c_conn_w, 0);\n}\nEXPORT_SYMBOL_GPL(rds_conn_connect_if_down);\n\n/*\n * An error occurred on the connection\n */\nvoid\n__rds_conn_error(struct rds_connection *conn, const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\tvprintk(fmt, ap);\n\tva_end(ap);\n\n\trds_conn_drop(conn);\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2006 Oracle.  All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <net/inet_hashtables.h>\n\n#include \"rds.h\"\n#include \"loop.h\"\n\n#define RDS_CONNECTION_HASH_BITS 12\n#define RDS_CONNECTION_HASH_ENTRIES (1 << RDS_CONNECTION_HASH_BITS)\n#define RDS_CONNECTION_HASH_MASK (RDS_CONNECTION_HASH_ENTRIES - 1)\n\n/* converting this to RCU is a chore for another day.. */\nstatic DEFINE_SPINLOCK(rds_conn_lock);\nstatic unsigned long rds_conn_count;\nstatic struct hlist_head rds_conn_hash[RDS_CONNECTION_HASH_ENTRIES];\nstatic struct kmem_cache *rds_conn_slab;\n\nstatic struct hlist_head *rds_conn_bucket(__be32 laddr, __be32 faddr)\n{\n\tstatic u32 rds_hash_secret __read_mostly;\n\n\tunsigned long hash;\n\n\tnet_get_random_once(&rds_hash_secret, sizeof(rds_hash_secret));\n\n\t/* Pass NULL, don't need struct net for hash */\n\thash = __inet_ehashfn(be32_to_cpu(laddr), 0,\n\t\t\t      be32_to_cpu(faddr), 0,\n\t\t\t      rds_hash_secret);\n\treturn &rds_conn_hash[hash & RDS_CONNECTION_HASH_MASK];\n}\n\n#define rds_conn_info_set(var, test, suffix) do {\t\t\\\n\tif (test)\t\t\t\t\t\t\\\n\t\tvar |= RDS_INFO_CONNECTION_FLAG_##suffix;\t\\\n} while (0)\n\n/* rcu read lock must be held or the connection spinlock */\nstatic struct rds_connection *rds_conn_lookup(struct net *net,\n\t\t\t\t\t      struct hlist_head *head,\n\t\t\t\t\t      __be32 laddr, __be32 faddr,\n\t\t\t\t\t      struct rds_transport *trans)\n{\n\tstruct rds_connection *conn, *ret = NULL;\n\n\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\tif (conn->c_faddr == faddr && conn->c_laddr == laddr &&\n\t\t    conn->c_trans == trans && net == rds_conn_net(conn)) {\n\t\t\tret = conn;\n\t\t\tbreak;\n\t\t}\n\t}\n\trdsdebug(\"returning conn %p for %pI4 -> %pI4\\n\", ret,\n\t\t &laddr, &faddr);\n\treturn ret;\n}\n\n/*\n * This is called by transports as they're bringing down a connection.\n * It clears partial message state so that the transport can start sending\n * and receiving over this connection again in the future.  It is up to\n * the transport to have serialized this call with its send and recv.\n */\nstatic void rds_conn_reset(struct rds_connection *conn)\n{\n\trdsdebug(\"connection %pI4 to %pI4 reset\\n\",\n\t  &conn->c_laddr, &conn->c_faddr);\n\n\trds_stats_inc(s_conn_reset);\n\trds_send_reset(conn);\n\tconn->c_flags = 0;\n\n\t/* Do not clear next_rx_seq here, else we cannot distinguish\n\t * retransmitted packets from new packets, and will hand all\n\t * of them to the application. That is not consistent with the\n\t * reliability guarantees of RDS. */\n}\n\n/*\n * There is only every one 'conn' for a given pair of addresses in the\n * system at a time.  They contain messages to be retransmitted and so\n * span the lifetime of the actual underlying transport connections.\n *\n * For now they are not garbage collected once they're created.  They\n * are torn down as the module is removed, if ever.\n */\nstatic struct rds_connection *__rds_conn_create(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp,\n\t\t\t\t       int is_outgoing)\n{\n\tstruct rds_connection *conn, *parent = NULL;\n\tstruct hlist_head *head = rds_conn_bucket(laddr, faddr);\n\tstruct rds_transport *loop_trans;\n\tunsigned long flags;\n\tint ret;\n\tstruct rds_transport *otrans = trans;\n\n\tif (!is_outgoing && otrans->t_type == RDS_TRANS_TCP)\n\t\tgoto new_conn;\n\trcu_read_lock();\n\tconn = rds_conn_lookup(net, head, laddr, faddr, trans);\n\tif (conn && conn->c_loopback && conn->c_trans != &rds_loop_transport &&\n\t    laddr == faddr && !is_outgoing) {\n\t\t/* This is a looped back IB connection, and we're\n\t\t * called by the code handling the incoming connect.\n\t\t * We need a second connection object into which we\n\t\t * can stick the other QP. */\n\t\tparent = conn;\n\t\tconn = parent->c_passive;\n\t}\n\trcu_read_unlock();\n\tif (conn)\n\t\tgoto out;\n\nnew_conn:\n\tconn = kmem_cache_zalloc(rds_conn_slab, gfp);\n\tif (!conn) {\n\t\tconn = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tINIT_HLIST_NODE(&conn->c_hash_node);\n\tconn->c_laddr = laddr;\n\tconn->c_faddr = faddr;\n\tspin_lock_init(&conn->c_lock);\n\tconn->c_next_tx_seq = 1;\n\trds_conn_net_set(conn, net);\n\n\tinit_waitqueue_head(&conn->c_waitq);\n\tINIT_LIST_HEAD(&conn->c_send_queue);\n\tINIT_LIST_HEAD(&conn->c_retrans);\n\n\tret = rds_cong_get_maps(conn);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This is where a connection becomes loopback.  If *any* RDS sockets\n\t * can bind to the destination address then we'd rather the messages\n\t * flow through loopback rather than either transport.\n\t */\n\tloop_trans = rds_trans_get_preferred(net, faddr);\n\tif (loop_trans) {\n\t\trds_trans_put(loop_trans);\n\t\tconn->c_loopback = 1;\n\t\tif (is_outgoing && trans->t_prefer_loopback) {\n\t\t\t/* \"outgoing\" connection - and the transport\n\t\t\t * says it wants the connection handled by the\n\t\t\t * loopback transport. This is what TCP does.\n\t\t\t */\n\t\t\ttrans = &rds_loop_transport;\n\t\t}\n\t}\n\n\tif (trans == NULL) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(-ENODEV);\n\t\tgoto out;\n\t}\n\n\tconn->c_trans = trans;\n\n\tret = trans->conn_alloc(conn, gfp);\n\tif (ret) {\n\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\tconn = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\tatomic_set(&conn->c_state, RDS_CONN_DOWN);\n\tconn->c_send_gen = 0;\n\tconn->c_reconnect_jiffies = 0;\n\tINIT_DELAYED_WORK(&conn->c_send_w, rds_send_worker);\n\tINIT_DELAYED_WORK(&conn->c_recv_w, rds_recv_worker);\n\tINIT_DELAYED_WORK(&conn->c_conn_w, rds_connect_worker);\n\tINIT_WORK(&conn->c_down_w, rds_shutdown_worker);\n\tmutex_init(&conn->c_cm_lock);\n\tconn->c_flags = 0;\n\n\trdsdebug(\"allocated conn %p for %pI4 -> %pI4 over %s %s\\n\",\n\t  conn, &laddr, &faddr,\n\t  trans->t_name ? trans->t_name : \"[unknown]\",\n\t  is_outgoing ? \"(outgoing)\" : \"\");\n\n\t/*\n\t * Since we ran without holding the conn lock, someone could\n\t * have created the same conn (either normal or passive) in the\n\t * interim. We check while holding the lock. If we won, we complete\n\t * init and return our conn. If we lost, we rollback and return the\n\t * other one.\n\t */\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\tif (parent) {\n\t\t/* Creating passive conn */\n\t\tif (parent->c_passive) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = parent->c_passive;\n\t\t} else {\n\t\t\tparent->c_passive = conn;\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t} else {\n\t\t/* Creating normal conn */\n\t\tstruct rds_connection *found;\n\n\t\tif (!is_outgoing && otrans->t_type == RDS_TRANS_TCP)\n\t\t\tfound = NULL;\n\t\telse\n\t\t\tfound = rds_conn_lookup(net, head, laddr, faddr, trans);\n\t\tif (found) {\n\t\t\ttrans->conn_free(conn->c_transport_data);\n\t\t\tkmem_cache_free(rds_conn_slab, conn);\n\t\t\tconn = found;\n\t\t} else {\n\t\t\tif ((is_outgoing && otrans->t_type == RDS_TRANS_TCP) ||\n\t\t\t    (otrans->t_type != RDS_TRANS_TCP)) {\n\t\t\t\t/* Only the active side should be added to\n\t\t\t\t * reconnect list for TCP.\n\t\t\t\t */\n\t\t\t\thlist_add_head_rcu(&conn->c_hash_node, head);\n\t\t\t}\n\t\t\trds_cong_add_conn(conn);\n\t\t\trds_conn_count++;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n\nout:\n\treturn conn;\n}\n\nstruct rds_connection *rds_conn_create(struct net *net,\n\t\t\t\t       __be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, 0);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create);\n\nstruct rds_connection *rds_conn_create_outgoing(struct net *net,\n\t\t\t\t\t\t__be32 laddr, __be32 faddr,\n\t\t\t\t       struct rds_transport *trans, gfp_t gfp)\n{\n\treturn __rds_conn_create(net, laddr, faddr, trans, gfp, 1);\n}\nEXPORT_SYMBOL_GPL(rds_conn_create_outgoing);\n\nvoid rds_conn_shutdown(struct rds_connection *conn)\n{\n\t/* shut it down unless it's down already */\n\tif (!rds_conn_transition(conn, RDS_CONN_DOWN, RDS_CONN_DOWN)) {\n\t\t/*\n\t\t * Quiesce the connection mgmt handlers before we start tearing\n\t\t * things down. We don't hold the mutex for the entire\n\t\t * duration of the shutdown operation, else we may be\n\t\t * deadlocking with the CM handler. Instead, the CM event\n\t\t * handler is supposed to check for state DISCONNECTING\n\t\t */\n\t\tmutex_lock(&conn->c_cm_lock);\n\t\tif (!rds_conn_transition(conn, RDS_CONN_UP, RDS_CONN_DISCONNECTING)\n\t\t && !rds_conn_transition(conn, RDS_CONN_ERROR, RDS_CONN_DISCONNECTING)) {\n\t\t\trds_conn_error(conn, \"shutdown called in state %d\\n\",\n\t\t\t\t\tatomic_read(&conn->c_state));\n\t\t\tmutex_unlock(&conn->c_cm_lock);\n\t\t\treturn;\n\t\t}\n\t\tmutex_unlock(&conn->c_cm_lock);\n\n\t\twait_event(conn->c_waitq,\n\t\t\t   !test_bit(RDS_IN_XMIT, &conn->c_flags));\n\t\twait_event(conn->c_waitq,\n\t\t\t   !test_bit(RDS_RECV_REFILL, &conn->c_flags));\n\n\t\tconn->c_trans->conn_shutdown(conn);\n\t\trds_conn_reset(conn);\n\n\t\tif (!rds_conn_transition(conn, RDS_CONN_DISCONNECTING, RDS_CONN_DOWN)) {\n\t\t\t/* This can happen - eg when we're in the middle of tearing\n\t\t\t * down the connection, and someone unloads the rds module.\n\t\t\t * Quite reproduceable with loopback connections.\n\t\t\t * Mostly harmless.\n\t\t\t */\n\t\t\trds_conn_error(conn,\n\t\t\t\t\"%s: failed to transition to state DOWN, \"\n\t\t\t\t\"current state is %d\\n\",\n\t\t\t\t__func__,\n\t\t\t\tatomic_read(&conn->c_state));\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Then reconnect if it's still live.\n\t * The passive side of an IB loopback connection is never added\n\t * to the conn hash, so we never trigger a reconnect on this\n\t * conn - the reconnect is always triggered by the active peer. */\n\tcancel_delayed_work_sync(&conn->c_conn_w);\n\trcu_read_lock();\n\tif (!hlist_unhashed(&conn->c_hash_node)) {\n\t\trcu_read_unlock();\n\t\trds_queue_reconnect(conn);\n\t} else {\n\t\trcu_read_unlock();\n\t}\n}\n\n/*\n * Stop and free a connection.\n *\n * This can only be used in very limited circumstances.  It assumes that once\n * the conn has been shutdown that no one else is referencing the connection.\n * We can only ensure this in the rmmod path in the current code.\n */\nvoid rds_conn_destroy(struct rds_connection *conn)\n{\n\tstruct rds_message *rm, *rtmp;\n\tunsigned long flags;\n\n\trdsdebug(\"freeing conn %p for %pI4 -> \"\n\t\t \"%pI4\\n\", conn, &conn->c_laddr,\n\t\t &conn->c_faddr);\n\n\t/* Ensure conn will not be scheduled for reconnect */\n\tspin_lock_irq(&rds_conn_lock);\n\thlist_del_init_rcu(&conn->c_hash_node);\n\tspin_unlock_irq(&rds_conn_lock);\n\tsynchronize_rcu();\n\n\t/* shut the connection down */\n\trds_conn_drop(conn);\n\tflush_work(&conn->c_down_w);\n\n\t/* make sure lingering queued work won't try to ref the conn */\n\tcancel_delayed_work_sync(&conn->c_send_w);\n\tcancel_delayed_work_sync(&conn->c_recv_w);\n\n\t/* tear down queued messages */\n\tlist_for_each_entry_safe(rm, rtmp,\n\t\t\t\t &conn->c_send_queue,\n\t\t\t\t m_conn_item) {\n\t\tlist_del_init(&rm->m_conn_item);\n\t\tBUG_ON(!list_empty(&rm->m_sock_item));\n\t\trds_message_put(rm);\n\t}\n\tif (conn->c_xmit_rm)\n\t\trds_message_put(conn->c_xmit_rm);\n\n\tconn->c_trans->conn_free(conn->c_transport_data);\n\n\t/*\n\t * The congestion maps aren't freed up here.  They're\n\t * freed by rds_cong_exit() after all the connections\n\t * have been freed.\n\t */\n\trds_cong_remove_conn(conn);\n\n\tBUG_ON(!list_empty(&conn->c_retrans));\n\tkmem_cache_free(rds_conn_slab, conn);\n\n\tspin_lock_irqsave(&rds_conn_lock, flags);\n\trds_conn_count--;\n\tspin_unlock_irqrestore(&rds_conn_lock, flags);\n}\nEXPORT_SYMBOL_GPL(rds_conn_destroy);\n\nstatic void rds_conn_message_info(struct socket *sock, unsigned int len,\n\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t  struct rds_info_lengths *lens,\n\t\t\t\t  int want_send)\n{\n\tstruct hlist_head *head;\n\tstruct list_head *list;\n\tstruct rds_connection *conn;\n\tstruct rds_message *rm;\n\tunsigned int total = 0;\n\tunsigned long flags;\n\tsize_t i;\n\n\tlen /= sizeof(struct rds_info_message);\n\n\trcu_read_lock();\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\t\t\tif (want_send)\n\t\t\t\tlist = &conn->c_send_queue;\n\t\t\telse\n\t\t\t\tlist = &conn->c_retrans;\n\n\t\t\tspin_lock_irqsave(&conn->c_lock, flags);\n\n\t\t\t/* XXX too lazy to maintain counts.. */\n\t\t\tlist_for_each_entry(rm, list, m_conn_item) {\n\t\t\t\ttotal++;\n\t\t\t\tif (total <= len)\n\t\t\t\t\trds_inc_info_copy(&rm->m_inc, iter,\n\t\t\t\t\t\t\t  conn->c_laddr,\n\t\t\t\t\t\t\t  conn->c_faddr, 0);\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(&conn->c_lock, flags);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tlens->nr = total;\n\tlens->each = sizeof(struct rds_info_message);\n}\n\nstatic void rds_conn_message_info_send(struct socket *sock, unsigned int len,\n\t\t\t\t       struct rds_info_iterator *iter,\n\t\t\t\t       struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 1);\n}\n\nstatic void rds_conn_message_info_retrans(struct socket *sock,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  struct rds_info_iterator *iter,\n\t\t\t\t\t  struct rds_info_lengths *lens)\n{\n\trds_conn_message_info(sock, len, iter, lens, 0);\n}\n\nvoid rds_for_each_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens,\n\t\t\t  int (*visitor)(struct rds_connection *, void *),\n\t\t\t  size_t item_len)\n{\n\tuint64_t buffer[(item_len + 7) / 8];\n\tstruct hlist_head *head;\n\tstruct rds_connection *conn;\n\tsize_t i;\n\n\trcu_read_lock();\n\n\tlens->nr = 0;\n\tlens->each = item_len;\n\n\tfor (i = 0, head = rds_conn_hash; i < ARRAY_SIZE(rds_conn_hash);\n\t     i++, head++) {\n\t\thlist_for_each_entry_rcu(conn, head, c_hash_node) {\n\n\t\t\t/* XXX no c_lock usage.. */\n\t\t\tif (!visitor(conn, buffer))\n\t\t\t\tcontinue;\n\n\t\t\t/* We copy as much as we can fit in the buffer,\n\t\t\t * but we count all items so that the caller\n\t\t\t * can resize the buffer. */\n\t\t\tif (len >= item_len) {\n\t\t\t\trds_info_copy(iter, buffer, item_len);\n\t\t\t\tlen -= item_len;\n\t\t\t}\n\t\t\tlens->nr++;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(rds_for_each_conn_info);\n\nstatic int rds_conn_info_visitor(struct rds_connection *conn,\n\t\t\t\t  void *buffer)\n{\n\tstruct rds_info_connection *cinfo = buffer;\n\n\tcinfo->next_tx_seq = conn->c_next_tx_seq;\n\tcinfo->next_rx_seq = conn->c_next_rx_seq;\n\tcinfo->laddr = conn->c_laddr;\n\tcinfo->faddr = conn->c_faddr;\n\tstrncpy(cinfo->transport, conn->c_trans->t_name,\n\t\tsizeof(cinfo->transport));\n\tcinfo->flags = 0;\n\n\trds_conn_info_set(cinfo->flags, test_bit(RDS_IN_XMIT, &conn->c_flags),\n\t\t\t  SENDING);\n\t/* XXX Future: return the state rather than these funky bits */\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&conn->c_state) == RDS_CONN_CONNECTING,\n\t\t\t  CONNECTING);\n\trds_conn_info_set(cinfo->flags,\n\t\t\t  atomic_read(&conn->c_state) == RDS_CONN_UP,\n\t\t\t  CONNECTED);\n\treturn 1;\n}\n\nstatic void rds_conn_info(struct socket *sock, unsigned int len,\n\t\t\t  struct rds_info_iterator *iter,\n\t\t\t  struct rds_info_lengths *lens)\n{\n\trds_for_each_conn_info(sock, len, iter, lens,\n\t\t\t\trds_conn_info_visitor,\n\t\t\t\tsizeof(struct rds_info_connection));\n}\n\nint rds_conn_init(void)\n{\n\trds_conn_slab = kmem_cache_create(\"rds_connection\",\n\t\t\t\t\t  sizeof(struct rds_connection),\n\t\t\t\t\t  0, 0, NULL);\n\tif (!rds_conn_slab)\n\t\treturn -ENOMEM;\n\n\trds_info_register_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_register_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t       rds_conn_message_info_send);\n\trds_info_register_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t       rds_conn_message_info_retrans);\n\n\treturn 0;\n}\n\nvoid rds_conn_exit(void)\n{\n\trds_loop_exit();\n\n\tWARN_ON(!hlist_empty(rds_conn_hash));\n\n\tkmem_cache_destroy(rds_conn_slab);\n\n\trds_info_deregister_func(RDS_INFO_CONNECTIONS, rds_conn_info);\n\trds_info_deregister_func(RDS_INFO_SEND_MESSAGES,\n\t\t\t\t rds_conn_message_info_send);\n\trds_info_deregister_func(RDS_INFO_RETRANS_MESSAGES,\n\t\t\t\t rds_conn_message_info_retrans);\n}\n\n/*\n * Force a disconnect\n */\nvoid rds_conn_drop(struct rds_connection *conn)\n{\n\tatomic_set(&conn->c_state, RDS_CONN_ERROR);\n\tqueue_work(rds_wq, &conn->c_down_w);\n}\nEXPORT_SYMBOL_GPL(rds_conn_drop);\n\n/*\n * If the connection is down, trigger a connect. We may have scheduled a\n * delayed reconnect however - in this case we should not interfere.\n */\nvoid rds_conn_connect_if_down(struct rds_connection *conn)\n{\n\tif (rds_conn_state(conn) == RDS_CONN_DOWN &&\n\t    !test_and_set_bit(RDS_RECONNECT_PENDING, &conn->c_flags))\n\t\tqueue_delayed_work(rds_wq, &conn->c_conn_w, 0);\n}\nEXPORT_SYMBOL_GPL(rds_conn_connect_if_down);\n\n/*\n * An error occurred on the connection\n */\nvoid\n__rds_conn_error(struct rds_connection *conn, const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\tvprintk(fmt, ap);\n\tva_end(ap);\n\n\trds_conn_drop(conn);\n}\n"], "filenames": ["net/rds/connection.c"], "buggy_code_start_loc": [190], "buggy_code_end_loc": [190], "fixing_code_start_loc": [191], "fixing_code_end_loc": [197], "type": "NVD-CWE-Other", "message": "The __rds_conn_create function in net/rds/connection.c in the Linux kernel through 4.2.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound.", "other": {"cve": {"id": "CVE-2015-6937", "sourceIdentifier": "cve@mitre.org", "published": "2015-10-19T10:59:07.380", "lastModified": "2018-10-17T01:29:28.147", "vulnStatus": "Modified", "evaluatorComment": "<a href=\"http://cwe.mitre.org/data/definitions/476.html\" rel=\"nofollow\">CWE-476: NULL Pointer Dereference</a>", "descriptions": [{"lang": "en", "value": "The __rds_conn_create function in net/rds/connection.c in the Linux kernel through 4.2.3 allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by using a socket that was not properly bound."}, {"lang": "es", "value": "La funci\u00f3n __rds_conn_create en net/rds/connection.c en el kernel de Linux hasta la versi\u00f3n 4.2.3 permite a usuarios locales provocar una denegaci\u00f3n de servicio (referencia a puntero NULL y ca\u00edda del sistema) o posiblemente tener otro impacto no especificado mediante el uso de un socket que no estaba vinculado adecuadamente."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.2.3", "matchCriteriaId": "2F1B9081-FA4D-426C-B2DE-78AEA0BB6DB1"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B6B7CAD7-9D4E-4FDB-88E3-1E583210A01F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=74e98eb085889b0d2d4908f59f6e00026063014f", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2015-October/168447.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2015-October/168539.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://lists.fedoraproject.org/pipermail/package-announce/2015-September/167358.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-10/msg00009.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-11/msg00035.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-12/msg00026.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-12/msg00031.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00007.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00009.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00013.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00017.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00018.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00019.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00020.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00021.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00022.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-02/msg00034.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2016-08/msg00038.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-updates/2015-12/msg00039.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.debian.org/security/2015/dsa-3364", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2015/09/14/3", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.oracle.com/technetwork/security-advisory/cpuoct2018-4428296.html", "source": "cve@mitre.org"}, {"url": "http://www.oracle.com/technetwork/topics/security/linuxbulletinoct2015-2719645.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/76767", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1034453", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2773-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2774-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2777-1", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1263139", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/74e98eb085889b0d2d4908f59f6e00026063014f", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/74e98eb085889b0d2d4908f59f6e00026063014f"}}