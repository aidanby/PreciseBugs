{"buggy_code": ["import logging\nimport zipfile\nimport base64\nimport shutil\nimport time\nfrom tarfile import TarFile\n\nfrom bz2 import BZ2File\nfrom gzip import GzipFile\nfrom lzma import LZMAFile\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import List, Optional, Union, Tuple, Sequence, Dict, overload, Literal\n\nfrom dfvfs.analyzer import analyzer  # type: ignore\nfrom dfvfs.lib import definitions, raw_helper, errors  # type: ignore\nfrom dfvfs.path import factory  # type: ignore\nfrom dfvfs.resolver import resolver  # type: ignore\nfrom dfvfs.volume import tsk_volume_system  # type: ignore\nfrom dfvfs.vfs.file_entry import FileEntry  # type: ignore\nfrom dfvfs.path.path_spec import PathSpec   # type: ignore\n\n\nfrom extract_msg import Message, MessageSigned, MSGFile\nfrom hachoir.stream import StringInputStream  # type: ignore\nfrom hachoir.parser.archive import CabFile  # type: ignore\nimport py7zr  # type: ignore\nimport pycdlib\nfrom pycdlib.facade import PyCdlibJoliet, PyCdlibUDF, PyCdlibRockRidge, PyCdlibISO9660\nimport pyzipper  # type: ignore\nimport rarfile  # type: ignore\n\nfrom ..default import safe_create_dir, PandoraException\nfrom ..helpers import Status\nfrom ..pandora import Pandora\nfrom ..report import Report\nfrom ..task import Task\nfrom ..file import File\n\nfrom .base import BaseWorker\n\n# Notes:\n# 1. Never blindly extract a file:\n#    * check unpacked size with a method of the unpacker lib.\n#      If it's not (bz2, gz, lzma), read the file up to MAX_EXTRACTED_FILE_SIZE\n#      and throw an exception/warning if we reach that.\n#    * check how many files are in an archive, preferably with a method of the unpacker lib.\n#      If it is not possible, extract files until you reach MAX_EXTRACT_FILES\n# => for those two reasons, we cannot use shutil.unpack_archive, which doesn't check anything\n\n# 2. The file can have a password?\n#    * figure out how to detect that => library method? exception?\n#    * Is possible, keep it in one loop:\n#        1. loop over all the files in the archive\n#        2. inside that loop, try each possible password against 1st file\n#        3. if something works, use a method to set the password in the lib\n#       That's the clean approach, works on zip, and rar files\n#\n#    * Else:\n#        1. Loop over each passwords, try to open the archive file until something works\n#        2. Reopen the file with the working password\n#       => 7z files\n\n\nclass Extractor(BaseWorker):\n\n    max_files_in_archive: int\n    max_extracted_filesize_in_mb: int\n    max_is_error: bool\n    zip_passwords: List[str]\n\n    def __init__(self, module: str, worker_id: int, cache: str, timeout: str,\n                 loglevel: int=logging.INFO, **options):\n        super().__init__(module, worker_id, cache, timeout, loglevel, **options)\n        self.max_extracted_filesize = self.max_extracted_filesize_in_mb * 1000000\n\n        # We might be getting integers from the config file\n        self.zip_passwords = [str(pwd) for pwd in self.zip_passwords]\n\n    @property\n    def passwords(self):\n        return self._passwords\n\n    @passwords.setter\n    def passwords(self, passwords: List[str]):\n        self._passwords = passwords\n\n    def _extract_iso(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        iso = pycdlib.PyCdlib()\n        extracted_files: List[Path] = []\n        try:\n            if not archive_file.data:\n                return extracted_files\n            iso.open_fp(archive_file.data)\n            facade: Union[PyCdlibJoliet, PyCdlibUDF, PyCdlibRockRidge, PyCdlibISO9660]\n            if iso.has_udf():\n                facade = iso.get_udf_facade()\n            elif iso.has_joliet():\n                facade = iso.get_joliet_facade()\n            elif iso.has_rock_ridge():\n                facade = iso.get_rock_ridge_facade()\n            else:\n                facade = iso.get_iso9660_facade()\n            for dirname, _, filelist in facade.walk('/'):\n                if len(extracted_files) > self.max_files_in_archive:\n                    break\n                if not filelist:\n                    continue\n                for filename in filelist:\n                    filename = filename.lstrip('/')\n                    extracted = BytesIO()\n                    facade.get_file_from_iso_fp(extracted, f'{dirname}/{filename}')\n                    if extracted.getbuffer().nbytes >= self.max_extracted_filesize:\n                        self.logger.warning(f'File {archive_file.path.name} too big ({extracted.getbuffer().nbytes}).')\n                        report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                        report.add_details('Warning', f'File {archive_file.path.name} too big ({extracted.getbuffer().nbytes}).')\n                        continue\n                    if len(extracted_files) > self.max_files_in_archive:\n                        break\n                    tmp_dest_dir = dest_dir / f'.{dirname}'\n                    safe_create_dir(tmp_dest_dir)\n                    if ';' in filename:\n                        filepath = tmp_dest_dir / filename.split(';')[0]\n                    else:\n                        filepath = tmp_dest_dir / filename\n                    with filepath.open('wb') as f:\n                        f.write(extracted.getvalue())\n                    extracted_files.append(filepath)\n            if len(extracted_files) > self.max_files_in_archive:\n                self.logger.warning(f'Too many files in the archive (more than {self.max_files_in_archive}).')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'Too many files in the archive (more than {self.max_files_in_archive}).')\n        finally:\n            try:\n                iso.close()\n            except Exception:  # nosec B110\n                pass\n        return extracted_files\n\n    def _extract_zip(self, archive_file: File, report: Report, dest_dir: Path, zip_reader=zipfile.ZipFile) -> List[Path]:\n        found_password = False\n        extracted_files: List[Path] = []\n        with zip_reader(str(archive_file.path)) as archive:\n            for file_number, info in enumerate(archive.infolist()):\n                if file_number >= self.max_files_in_archive:\n                    warning_msg = f'Too many files ({len(archive.infolist())}) in the archive, stopping at {self.max_files_in_archive}.'\n                    self.logger.warning(warning_msg)\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', warning_msg)\n                    break\n                is_encrypted = info.flag_bits & 0x1  # from https://github.com/python/cpython/blob/3.10/Lib/zipfile.py\n                if is_encrypted and not found_password:\n                    for pwd in self.passwords:\n                        try:\n                            archive.read(info, pwd=pwd.encode())\n                            archive.setpassword(pwd.encode())\n                            found_password = True\n                            break\n                        except RuntimeError:\n                            continue\n                    else:\n                        report.status = Status.WARN\n                        report.add_details('Warning', 'File encrypted and unable to find password')\n                        report.add_extra('no_password', True)\n                        break\n                if info.is_dir():\n                    continue\n                if info.file_size > self.max_extracted_filesize:\n                    warning_msg = f'Skipping file {info.filename}, too big ({info.file_size}).'\n                    self.logger.warning(warning_msg)\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', warning_msg)\n                    continue\n                file_path = archive.extract(info, dest_dir)\n                extracted_files.append(Path(file_path))\n            else:\n                # was able to extract everything, except files that are too big.\n                if report.status == Status.RUNNING:\n                    report.status = Status.CLEAN\n        return extracted_files\n\n    def _extract_rar(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        found_password = False\n        extracted_files: List[Path] = []\n        with rarfile.RarFile(archive_file.path) as archive:\n            if not archive.infolist():\n                # Looks like there are no files in the archive, this is suspicious\n                # Also, might be a REV file, which is potentially not supported\n                self.logger.warning(f'Looks like the archive {archive_file.path} is empty.')\n                # NOTE: There is a catchall for that.\n\n            for file_number, info in enumerate(archive.infolist()):\n                if file_number >= self.max_files_in_archive:\n                    self.logger.warning(f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive')\n                    break\n                if info.needs_password() and not found_password:\n                    for pwd in self.passwords:\n                        try:\n                            with archive.open(info, pwd=pwd.encode()) as f:\n                                f.read()\n                            archive.setpassword(pwd.encode())\n                            found_password = True\n                            break\n                        except rarfile.BadRarFile:\n                            continue\n                        except rarfile.PasswordRequired:\n                            continue\n                    else:\n                        report.status = Status.WARN\n                        report.add_details('Warning', 'File encrypted and unable to find password')\n                        report.add_extra('no_password', True)\n                        break\n                if info.is_dir():\n                    continue\n                if info.file_size > self.max_extracted_filesize:\n                    self.logger.warning(f'Skipping file {info.filename}, too big ({info.file_size}).')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Skipping file {info.filename}, too big ({info.file_size}).')\n                    continue\n                file_path = archive.extract(info, dest_dir)\n                extracted_files.append(Path(file_path))\n            else:\n                # was able to extract everything, except files that are too big.\n                if report.status == Status.RUNNING:\n                    report.status = Status.CLEAN\n        return extracted_files\n\n    def _try_password_7z(self, path) -> Optional[str]:\n        for pwd in self.passwords:\n            try:\n                with py7zr.SevenZipFile(file=path, mode='r', password=pwd) as archive:\n                    files_in_archive = archive.getnames()\n                    if files_in_archive:\n                        archive.read(files_in_archive[0])\n                        return pwd\n            except py7zr.exceptions.PasswordRequired:\n                continue\n            except Exception:  # nosec B112\n                # TODO: notify that to the user?\n                continue\n        return None\n\n    def _extract_7z(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # 7z can be encrypted at 2 places, headers, or files. if headers, we have to try.\n        needs_password = False\n        try:\n            with py7zr.SevenZipFile(file=archive_file.path, mode='r') as archive:\n                files_in_archive = archive.getnames()\n                if files_in_archive:\n                    archive.read(files_in_archive[0])\n        except py7zr.exceptions.PasswordRequired:\n            needs_password = True\n\n        if needs_password:\n            password = self._try_password_7z(archive_file.path)\n            if password is None:\n                report.status = Status.WARN\n                report.add_details('Warning', 'Encrypted archive, unable to find password')\n                report.add_extra('no_password', True)\n                return []\n        else:\n            password = None\n\n        with py7zr.SevenZipFile(file=archive_file.path, mode='r', password=password) as archive:\n            if archive.archiveinfo().uncompressed >= self.max_extracted_filesize:\n                self.logger.warning(f'File {archive_file.path.name} too big ({archive.archiveinfo().uncompressed}).')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'File {archive_file.path.name} too big ({archive.archiveinfo().uncompressed}).')\n                return []\n\n            if len(archive.getnames()) > self.max_files_in_archive:\n                self.logger.warning(f'Too many files ({len(archive.getnames())}/{self.max_files_in_archive}) in the archive.')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'Too many files ({len(archive.getnames())}/{self.max_files_in_archive}) in the archive')\n                return []\n\n            archive.extractall(path=str(dest_dir))\n\n        return [path for path in dest_dir.glob('**/*') if path.is_file()]\n\n    def _extract_bz2(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # bz2 is a TAR archive, we basically need to unzip it and then extract the files from the TAR\n        # No password can be used to protect a bz2, so we don't need to check for passwords this time\n        # Sometimes the bz2 won't contain a TAR, but the way to unzip bz2 stays the same either way\n        bz2file = BZ2File(archive_file.path)  # open the file\n        data = bz2file.read(self.max_extracted_filesize + 1)  # get the decompressed data\n        if len(data) > self.max_extracted_filesize:\n            self.logger.warning(f'File {archive_file.path.name} too big ({len(data)}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {archive_file.path.name} too big ({len(data)}).')\n            return []\n\n        if archive_file.path.suffix == \".bz2\":\n            new_file_path = dest_dir / archive_file.path.stem\n        else:\n            new_file_path = dest_dir / archive_file.path.name\n        with new_file_path.open('wb') as f:\n            f.write(data)  # write an uncompressed file\n        return [new_file_path]\n\n    def _extract_tar(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # tar is not a compressed archive but a directory mainly used to regroup other directories\n        extracted_files: List[Path] = []\n        with TarFile(archive_file.path) as tar:\n            for file_number, tarinfo in enumerate(tar.getmembers()):\n                if file_number >= self.max_files_in_archive:\n                    self.logger.warning(f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive')\n                    break\n                if not tarinfo.isfile():\n                    continue\n                if tarinfo.size >= self.max_extracted_filesize:\n                    self.logger.warning(f'File {archive_file.path.name} too big ({tarinfo.size}).')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'File {archive_file.path.name} too big ({tarinfo.size}).')\n                    continue\n                tar.extract(tarinfo, dest_dir)\n                file_path = dest_dir / tarinfo.name\n                extracted_files.append(Path(file_path))\n        return extracted_files\n\n    def _extract_cab(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        extracted_files: List[Path] = []\n        # Code from https://github.com/vstinner/hachoir/issues/65#issuecomment-866965090\n        if not archive_file.data:\n            return extracted_files\n        cab = CabFile(StringInputStream(archive_file.data.getvalue()))\n        cab[\"folder_data[0]\"].getSubIStream()\n        folder_data = BytesIO(cab[\"folder_data[0]\"].uncompressed_data)\n        for file_number, file in enumerate(cab.array(\"file\")):\n            if file_number >= self.max_files_in_archive:\n                self.logger.warning(f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive')\n                break\n            if file[\"filesize\"].value >= self.max_extracted_filesize:\n                self.logger.warning(f'File {archive_file.path.name} too big ({file[\"filesize\"].value}).')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'File {archive_file.path.name} too big ({file[\"filesize\"].value}).')\n                continue\n            file_path = dest_dir / file[\"filename\"].value\n            with file_path.open('wb') as f:\n                f.write(folder_data.read(file[\"filesize\"].value))\n            extracted_files.append(Path(file_path))\n        return extracted_files\n\n    def _extract_gz(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # gz is just like bz2, a compressed archive with a TAR directory inside\n        gz_file = GzipFile(archive_file.path)\n        data = gz_file.read(self.max_extracted_filesize + 1)\n        if len(data) > self.max_extracted_filesize:\n            self.logger.warning(f'File {archive_file.path.name} too big ({len(data)}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {archive_file.path.name} too big ({len(data)}).')\n            return []\n        if archive_file.path.suffix == \".gz\":\n            new_file_path = dest_dir / archive_file.path.stem\n        else:\n            new_file_path = dest_dir / archive_file.path.name\n        with new_file_path.open('wb') as f:\n            f.write(data)  # write an uncompressed file\n        return [new_file_path]\n\n    def _extract_lzma(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # lzma is just like bz2 and gz, a compressed archive with a TAR directory inside\n        lzma_file = LZMAFile(archive_file.path)\n        data = lzma_file.read(self.max_extracted_filesize + 1)\n        if len(data) > self.max_extracted_filesize:\n            self.logger.warning(f'File {archive_file.path.name} too big ({len(data)}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {archive_file.path.name} too big ({len(data)}).')\n            return []\n        if archive_file.path.suffix == \".lzma\":\n            new_file_path = dest_dir / archive_file.path.stem\n        else:\n            new_file_path = dest_dir / archive_file.path.name\n        with new_file_path.open('wb') as f:\n            f.write(data)  # write an uncompressed file\n        return [new_file_path]\n\n    @overload\n    def check_dfvfs(self, submitted_file: File, check_only: Literal[True]) -> bool:\n        ...\n\n    @overload\n    def check_dfvfs(self, submitted_file: File, check_only: Literal[False]) -> List[Tuple[PathSpec, tsk_volume_system.TSKVolumeSystem]]:\n        ...\n\n    def check_dfvfs(self, submitted_file: File, check_only: bool) -> Union[bool, List[Tuple[PathSpec, tsk_volume_system.TSKVolumeSystem]]]:\n\n        to_process = []\n\n        path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_OS, location=submitted_file.path)\n\n        if type_indicators := analyzer.Analyzer.GetStorageMediaImageTypeIndicators(path_spec):\n            # NOTE: type_indicators can be a list, we pick the 1st one, but might want to loop\n            for ti in type_indicators:\n                path = factory.Factory.NewPathSpec(ti, parent=path_spec)\n                to_process.append(path)\n        else:\n            # The RAW storage media image type cannot be detected based on\n            # a signature so we try to detect it based on common file naming\n            # schemas.\n            file_system = resolver.Resolver.OpenFileSystem(path_spec)\n            raw_path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_RAW, parent=path_spec)\n            try:\n                glob_results = raw_helper.RawGlobPathSpec(file_system, raw_path_spec)\n            except errors.PathSpecError:\n                return False\n            if not glob_results:\n                return False\n            # NOTE: what are we supposed to do if we have more?\n            to_process = glob_results\n\n        to_return = []\n        for path_to_process in to_process:\n            volume_path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_TSK_PARTITION, location='/', parent=path_to_process)\n            try:\n                volume_system = tsk_volume_system.TSKVolumeSystem()\n                volume_system.Open(volume_path_spec)\n                if check_only:\n                    return True\n                to_return.append((path_to_process, volume_system))\n            except (OSError, errors.BackEndError):\n                self.logger.info('Not supported by dfvfs')\n            except Exception:\n                self.logger.info('Unknown exception from dfvfs')\n        if check_only:\n            # at this stage, if we are in check_only and didn't return, it's not supported\n            return False\n        return to_return\n\n    def extract_with_dfvfs(self, archive_file: File, report: Report) -> List[Tuple[str, BytesIO]]:\n        extracted: List[Tuple[str, BytesIO]] = []\n\n        def process_dir(file_entry: FileEntry):\n            for sub_file_entry in file_entry.sub_file_entries:\n                if len(extracted) >= self.max_files_in_archive:\n                    self.logger.warning(f'Too many files ({len(extracted)}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Too many files ({len(extracted)}/{self.max_files_in_archive}) in the archive')\n                    break\n                if sub_file_entry.IsFile():\n                    file_object = sub_file_entry.GetFileObject()\n                    file_content = BytesIO(file_object.read())\n                    if file_content.getbuffer().nbytes >= self.max_extracted_filesize:\n                        self.logger.warning(f'File {sub_file_entry.name} from {archive_file.path.name} too big ({file_content.getbuffer().nbytes}).')\n                        report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                        report.add_details('Warning', f'File {archive_file.path.name} too big ({file_content.getbuffer().nbytes}).')\n                        continue\n                    extracted.append((sub_file_entry.name, BytesIO(file_object.read())))\n                elif sub_file_entry.IsDirectory():\n                    process_dir(sub_file_entry)\n\n        for path_spec, volume_system in self.check_dfvfs(archive_file, False):  # pylint: disable=not-an-iterable\n            for volume in volume_system.volumes:\n                if volume_identifier := getattr(volume, 'identifier'):\n                    volume = volume_system.GetVolumeByIdentifier(volume_identifier)\n                    if not volume:\n                        self.logger.warning(f'Unable to find volume {volume_identifier}')\n                        continue\n\n                    # We check the current partition\n                    _path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_TSK_PARTITION,\n                                                             location=f'/{volume.identifier}', parent=path_spec)\n\n                    # We directly mount the /\n                    mft_path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_TSK,\n                                                                location='/', parent=_path_spec)\n\n                    file_entry = resolver.Resolver.OpenFileEntry(mft_path_spec)\n                    process_dir(file_entry)\n                else:\n                    self.logger.warning('Missing volume identifier, cannot do anything.')\n        return extracted\n\n    def extract_eml(self, eml_data: Dict) -> List[Tuple[str, BytesIO]]:\n        extracted: List[Tuple[str, BytesIO]] = []\n        for attachment in eml_data['attachment']:\n            extracted.append((attachment['filename'], BytesIO(base64.b64decode(attachment['raw']))))\n        return extracted\n\n    def extract_msg(self, msg_data: Union[Message, MessageSigned]) -> List[Tuple[str, BytesIO]]:\n        extracted: List[Tuple[str, BytesIO]] = []\n        for attachment in msg_data.attachments:\n            if isinstance(attachment.data, bytes):\n                blob = BytesIO(attachment.data)\n            elif isinstance(attachment.data, MSGFile):\n                blob = BytesIO()\n                attachment.data.export(blob)\n            extracted.append((attachment.getFilename(), blob))\n        return extracted\n\n    def analyse(self, task: Task, report: Report, manual_trigger: bool=False):\n        # The files supported by dfvfs generally don't have proper mime types, so we just try it on everything.\n        dfvfs_info = self.check_dfvfs(task.file, True)\n        if not (task.file.is_archive or task.file.is_eml or task.file.is_msg or dfvfs_info):\n            report.status = Status.NOTAPPLICABLE\n            return\n\n        if not task.user:\n            raise PandoraException('The task user is missing. Should not happen, but investigate if it does.')\n\n        pandora = Pandora()\n\n        tasks: List[Task] = []\n        extracted_dir = task.file.directory / 'extracted'\n        safe_create_dir(extracted_dir)\n        extracted: Sequence[Union[Path, Tuple[str, BytesIO]]] = []\n\n        # Try to extract files from archive\n        # TODO: Support other archive formats\n        if task.file.is_archive:\n            if task.password:\n                self.passwords = [task.password]\n            else:\n                self.passwords = self.zip_passwords\n            try:\n                if task.file.mime_type == \"application/x-7z-compressed\":\n                    extracted = self._extract_7z(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/vnd.ms-cab-compressed\":\n                    extracted = self._extract_cab(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-rar\":\n                    extracted = self._extract_rar(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-bzip2\":\n                    extracted = self._extract_bz2(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/gzip\":\n                    extracted = self._extract_gz(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-tar\":\n                    extracted = self._extract_tar(task.file, report, extracted_dir)\n                elif task.file.mime_type in [\"application/x-lzma\", \"application/x-xz\", \"application/x-lzip\"]:\n                    extracted = self._extract_lzma(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-iso9660-image\":\n                    extracted = self._extract_iso(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/zip\":\n                    extracted = self._extract_zip(task.file, report, extracted_dir)\n                    if not extracted:\n                        report.clear_extras()\n                        report.clear_details()\n                        report.reset_status()\n                        extracted = self._extract_zip(task.file, report, extracted_dir, pyzipper.AESZipFile)\n                else:\n                    raise PandoraException(f'Unsupported mimetype: {task.file.mime_type}')\n            except BaseException as e:\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to extract {task.file.path.name}: {e}.')\n                report.add_extra('no_password', True)\n                extracted = []\n                self.logger.exception(e)\n\n        # Try to extract attachments from EML file\n        if task.file.is_eml:\n            if not task.file.eml_data or 'attachment' not in task.file.eml_data or not task.file.eml_data['attachment']:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_eml(task.file.eml_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif task.file.is_msg:\n            if not task.file.msg_data or not task.file.msg_data.attachments:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_msg(task.file.msg_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif dfvfs_info:\n            # this is a dfvfs supported file\n            try:\n                extracted = self.extract_with_dfvfs(task.file, report)\n            except Exception as e:\n                self.logger.exception('dfVFS dislikes it.')\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to process with dfVFS {task.file.path.name}: {e}.')\n\n        if not extracted and report.status != Status.NOTAPPLICABLE:\n            report.status = Status.WARN\n            report.add_details('Warning', 'Nothing to extract.')\n\n        for ef in extracted:\n            if isinstance(ef, Path):\n                filename = ef.name\n                with ef.open('rb') as f:\n                    sample = BytesIO(f.read())\n            else:\n                filename, sample = ef\n            new_task = Task.new_task(user=task.user, sample=sample,\n                                     filename=filename,\n                                     disabled_workers=task.disabled_workers,\n                                     parent=task)\n            pandora.add_extracted_reference(task, new_task)\n            pandora.enqueue_task(new_task)\n            tasks.append(new_task)\n\n        shutil.rmtree(extracted_dir)\n        # wait for all the tasks to finish\n        while not all(t.workers_done for t in tasks):\n            time.sleep(1)\n\n        if tasks:\n            report.status = max(t.status for t in tasks)\n            if report.status > Status.CLEAN:\n                report.add_details('Warning', 'There are suspicious files in this archive, click on the \"Extracted\" tab for more.')\n        elif not report.status == Status.NOTAPPLICABLE:\n            # Nothing was extracted\n            report.status = Status.WARN\n            report.add_details('Warning', 'Looks like the archive is empty (?). This is suspicious.')\n", "meta:\n  name: Extractor\n  description: Extracts content from the uploaded object (email, archive)\n  replicas: 5\n\nsettings:\n  max_files_in_archive: 15\n  max_extracted_filesize_in_mb: 100\n  max_is_error: no\n  zip_passwords:\n    - ''\n    - virus\n    - CERT_SOC\n    - cert\n    - pandora\n    - infected\n    - 123\n"], "fixing_code": ["import logging\nimport zipfile\nimport base64\nimport shutil\nimport time\nfrom tarfile import TarFile\n\nfrom bz2 import BZ2File\nfrom gzip import GzipFile\nfrom lzma import LZMAFile\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import List, Optional, Union, Tuple, Sequence, Dict, overload, Literal\n\nfrom dfvfs.analyzer import analyzer  # type: ignore\nfrom dfvfs.lib import definitions, raw_helper, errors  # type: ignore\nfrom dfvfs.path import factory  # type: ignore\nfrom dfvfs.resolver import resolver  # type: ignore\nfrom dfvfs.volume import tsk_volume_system  # type: ignore\nfrom dfvfs.vfs.file_entry import FileEntry  # type: ignore\nfrom dfvfs.path.path_spec import PathSpec   # type: ignore\n\n\nfrom extract_msg import Message, MessageSigned, MSGFile\nfrom hachoir.stream import StringInputStream  # type: ignore\nfrom hachoir.parser.archive import CabFile  # type: ignore\nimport py7zr  # type: ignore\nimport pycdlib\nfrom pycdlib.facade import PyCdlibJoliet, PyCdlibUDF, PyCdlibRockRidge, PyCdlibISO9660\nimport pyzipper  # type: ignore\nimport rarfile  # type: ignore\n\nfrom ..default import safe_create_dir, PandoraException\nfrom ..helpers import Status\nfrom ..pandora import Pandora\nfrom ..report import Report\nfrom ..task import Task\nfrom ..file import File\n\nfrom .base import BaseWorker\n\n# Notes:\n# 1. Never blindly extract a file:\n#    * check unpacked size with a method of the unpacker lib.\n#      If it's not (bz2, gz, lzma), read the file up to MAX_EXTRACTED_FILE_SIZE\n#      and throw an exception/warning if we reach that.\n#    * check how many files are in an archive, preferably with a method of the unpacker lib.\n#      If it is not possible, extract files until you reach MAX_EXTRACT_FILES\n# => for those two reasons, we cannot use shutil.unpack_archive, which doesn't check anything\n\n# 2. The file can have a password?\n#    * figure out how to detect that => library method? exception?\n#    * Is possible, keep it in one loop:\n#        1. loop over all the files in the archive\n#        2. inside that loop, try each possible password against 1st file\n#        3. if something works, use a method to set the password in the lib\n#       That's the clean approach, works on zip, and rar files\n#\n#    * Else:\n#        1. Loop over each passwords, try to open the archive file until something works\n#        2. Reopen the file with the working password\n#       => 7z files\n\n\nclass Extractor(BaseWorker):\n\n    max_files_in_archive: int\n    max_extracted_filesize_in_mb: int\n    max_is_error: bool\n    zip_passwords: List[str]\n\n    def __init__(self, module: str, worker_id: int, cache: str, timeout: str,\n                 loglevel: int=logging.INFO, **options):\n        super().__init__(module, worker_id, cache, timeout, loglevel, **options)\n        self.max_extracted_filesize = self.max_extracted_filesize_in_mb * 1000000\n\n        # We might be getting integers from the config file\n        self.zip_passwords = [str(pwd) for pwd in self.zip_passwords]\n\n    @property\n    def passwords(self):\n        return self._passwords\n\n    @passwords.setter\n    def passwords(self, passwords: List[str]):\n        self._passwords = passwords\n\n    def _extract_iso(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        iso = pycdlib.PyCdlib()\n        extracted_files: List[Path] = []\n        try:\n            if not archive_file.data:\n                return extracted_files\n            iso.open_fp(archive_file.data)\n            facade: Union[PyCdlibJoliet, PyCdlibUDF, PyCdlibRockRidge, PyCdlibISO9660]\n            if iso.has_udf():\n                facade = iso.get_udf_facade()\n            elif iso.has_joliet():\n                facade = iso.get_joliet_facade()\n            elif iso.has_rock_ridge():\n                facade = iso.get_rock_ridge_facade()\n            else:\n                facade = iso.get_iso9660_facade()\n            for dirname, _, filelist in facade.walk('/'):\n                if len(extracted_files) > self.max_files_in_archive:\n                    break\n                if not filelist:\n                    continue\n                for filename in filelist:\n                    filename = filename.lstrip('/')\n                    extracted = BytesIO()\n                    facade.get_file_from_iso_fp(extracted, f'{dirname}/{filename}')\n                    if extracted.getbuffer().nbytes >= self.max_extracted_filesize:\n                        self.logger.warning(f'File {archive_file.path.name} too big ({extracted.getbuffer().nbytes}).')\n                        report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                        report.add_details('Warning', f'File {archive_file.path.name} too big ({extracted.getbuffer().nbytes}).')\n                        continue\n                    if len(extracted_files) > self.max_files_in_archive:\n                        break\n                    tmp_dest_dir = dest_dir / f'.{dirname}'\n                    safe_create_dir(tmp_dest_dir)\n                    if ';' in filename:\n                        filepath = tmp_dest_dir / filename.split(';')[0]\n                    else:\n                        filepath = tmp_dest_dir / filename\n                    with filepath.open('wb') as f:\n                        f.write(extracted.getvalue())\n                    extracted_files.append(filepath)\n            if len(extracted_files) > self.max_files_in_archive:\n                self.logger.warning(f'Too many files in the archive (more than {self.max_files_in_archive}).')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'Too many files in the archive (more than {self.max_files_in_archive}).')\n        finally:\n            try:\n                iso.close()\n            except Exception:  # nosec B110\n                pass\n        return extracted_files\n\n    def _extract_zip(self, archive_file: File, report: Report, dest_dir: Path, zip_reader=zipfile.ZipFile) -> List[Path]:\n        found_password = False\n        extracted_files: List[Path] = []\n        with zip_reader(str(archive_file.path)) as archive:\n            for file_number, info in enumerate(archive.infolist()):\n                if file_number >= self.max_files_in_archive:\n                    warning_msg = f'Too many files ({len(archive.infolist())}) in the archive, stopping at {self.max_files_in_archive}.'\n                    self.logger.warning(warning_msg)\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', warning_msg)\n                    break\n                is_encrypted = info.flag_bits & 0x1  # from https://github.com/python/cpython/blob/3.10/Lib/zipfile.py\n                if is_encrypted and not found_password:\n                    for pwd in self.passwords:\n                        try:\n                            archive.read(info, pwd=pwd.encode())\n                            archive.setpassword(pwd.encode())\n                            found_password = True\n                            break\n                        except RuntimeError:\n                            continue\n                    else:\n                        report.status = Status.WARN\n                        report.add_details('Warning', 'File encrypted and unable to find password')\n                        report.add_extra('no_password', True)\n                        break\n                if info.is_dir():\n                    continue\n                if info.file_size > self.max_extracted_filesize:\n                    warning_msg = f'Skipping file {info.filename}, too big ({info.file_size}).'\n                    self.logger.warning(warning_msg)\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', warning_msg)\n                    continue\n                file_path = archive.extract(info, dest_dir)\n                extracted_files.append(Path(file_path))\n            else:\n                # was able to extract everything, except files that are too big.\n                if report.status == Status.RUNNING:\n                    report.status = Status.CLEAN\n        return extracted_files\n\n    def _extract_rar(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        found_password = False\n        extracted_files: List[Path] = []\n        with rarfile.RarFile(archive_file.path) as archive:\n            if not archive.infolist():\n                # Looks like there are no files in the archive, this is suspicious\n                # Also, might be a REV file, which is potentially not supported\n                self.logger.warning(f'Looks like the archive {archive_file.path} is empty.')\n                # NOTE: There is a catchall for that.\n\n            for file_number, info in enumerate(archive.infolist()):\n                if file_number >= self.max_files_in_archive:\n                    self.logger.warning(f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive')\n                    break\n                if info.needs_password() and not found_password:\n                    for pwd in self.passwords:\n                        try:\n                            with archive.open(info, pwd=pwd.encode()) as f:\n                                f.read()\n                            archive.setpassword(pwd.encode())\n                            found_password = True\n                            break\n                        except rarfile.BadRarFile:\n                            continue\n                        except rarfile.PasswordRequired:\n                            continue\n                    else:\n                        report.status = Status.WARN\n                        report.add_details('Warning', 'File encrypted and unable to find password')\n                        report.add_extra('no_password', True)\n                        break\n                if info.is_dir():\n                    continue\n                if info.file_size > self.max_extracted_filesize:\n                    self.logger.warning(f'Skipping file {info.filename}, too big ({info.file_size}).')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Skipping file {info.filename}, too big ({info.file_size}).')\n                    continue\n                file_path = archive.extract(info, dest_dir)\n                extracted_files.append(Path(file_path))\n            else:\n                # was able to extract everything, except files that are too big.\n                if report.status == Status.RUNNING:\n                    report.status = Status.CLEAN\n        return extracted_files\n\n    def _try_password_7z(self, path) -> Optional[str]:\n        for pwd in self.passwords:\n            try:\n                with py7zr.SevenZipFile(file=path, mode='r', password=pwd) as archive:\n                    files_in_archive = archive.getnames()\n                    if files_in_archive:\n                        archive.read(files_in_archive[0])\n                        return pwd\n            except py7zr.exceptions.PasswordRequired:\n                continue\n            except Exception:  # nosec B112\n                # TODO: notify that to the user?\n                continue\n        return None\n\n    def _extract_7z(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # 7z can be encrypted at 2 places, headers, or files. if headers, we have to try.\n        needs_password = False\n        try:\n            with py7zr.SevenZipFile(file=archive_file.path, mode='r') as archive:\n                files_in_archive = archive.getnames()\n                if files_in_archive:\n                    archive.read(files_in_archive[0])\n        except py7zr.exceptions.PasswordRequired:\n            needs_password = True\n\n        if needs_password:\n            password = self._try_password_7z(archive_file.path)\n            if password is None:\n                report.status = Status.WARN\n                report.add_details('Warning', 'Encrypted archive, unable to find password')\n                report.add_extra('no_password', True)\n                return []\n        else:\n            password = None\n\n        with py7zr.SevenZipFile(file=archive_file.path, mode='r', password=password) as archive:\n            if archive.archiveinfo().uncompressed >= self.max_extracted_filesize:\n                self.logger.warning(f'File {archive_file.path.name} too big ({archive.archiveinfo().uncompressed}).')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'File {archive_file.path.name} too big ({archive.archiveinfo().uncompressed}).')\n                return []\n\n            if len(archive.getnames()) > self.max_files_in_archive:\n                self.logger.warning(f'Too many files ({len(archive.getnames())}/{self.max_files_in_archive}) in the archive.')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'Too many files ({len(archive.getnames())}/{self.max_files_in_archive}) in the archive')\n                return []\n\n            archive.extractall(path=str(dest_dir))\n\n        return [path for path in dest_dir.glob('**/*') if path.is_file()]\n\n    def _extract_bz2(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # bz2 is a TAR archive, we basically need to unzip it and then extract the files from the TAR\n        # No password can be used to protect a bz2, so we don't need to check for passwords this time\n        # Sometimes the bz2 won't contain a TAR, but the way to unzip bz2 stays the same either way\n        bz2file = BZ2File(archive_file.path)  # open the file\n        data = bz2file.read(self.max_extracted_filesize + 1)  # get the decompressed data\n        if len(data) > self.max_extracted_filesize:\n            self.logger.warning(f'File {archive_file.path.name} too big ({len(data)}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {archive_file.path.name} too big ({len(data)}).')\n            return []\n\n        if archive_file.path.suffix == \".bz2\":\n            new_file_path = dest_dir / archive_file.path.stem\n        else:\n            new_file_path = dest_dir / archive_file.path.name\n        with new_file_path.open('wb') as f:\n            f.write(data)  # write an uncompressed file\n        return [new_file_path]\n\n    def _extract_tar(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # tar is not a compressed archive but a directory mainly used to regroup other directories\n        extracted_files: List[Path] = []\n        with TarFile(archive_file.path) as tar:\n            for file_number, tarinfo in enumerate(tar.getmembers()):\n                if file_number >= self.max_files_in_archive:\n                    self.logger.warning(f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive')\n                    break\n                if not tarinfo.isfile():\n                    continue\n                if tarinfo.size >= self.max_extracted_filesize:\n                    self.logger.warning(f'File {archive_file.path.name} too big ({tarinfo.size}).')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'File {archive_file.path.name} too big ({tarinfo.size}).')\n                    continue\n                tar.extract(tarinfo, dest_dir)\n                file_path = dest_dir / tarinfo.name\n                extracted_files.append(Path(file_path))\n        return extracted_files\n\n    def _extract_cab(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        extracted_files: List[Path] = []\n        # Code from https://github.com/vstinner/hachoir/issues/65#issuecomment-866965090\n        if not archive_file.data:\n            return extracted_files\n        cab = CabFile(StringInputStream(archive_file.data.getvalue()))\n        cab[\"folder_data[0]\"].getSubIStream()\n        folder_data = BytesIO(cab[\"folder_data[0]\"].uncompressed_data)\n        for file_number, file in enumerate(cab.array(\"file\")):\n            if file_number >= self.max_files_in_archive:\n                self.logger.warning(f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'Too many files ({file_number}/{self.max_files_in_archive}) in the archive')\n                break\n            if file[\"filesize\"].value >= self.max_extracted_filesize:\n                self.logger.warning(f'File {archive_file.path.name} too big ({file[\"filesize\"].value}).')\n                report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                report.add_details('Warning', f'File {archive_file.path.name} too big ({file[\"filesize\"].value}).')\n                continue\n            file_path = dest_dir / file[\"filename\"].value\n            with file_path.open('wb') as f:\n                f.write(folder_data.read(file[\"filesize\"].value))\n            extracted_files.append(Path(file_path))\n        return extracted_files\n\n    def _extract_gz(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # gz is just like bz2, a compressed archive with a TAR directory inside\n        gz_file = GzipFile(archive_file.path)\n        data = gz_file.read(self.max_extracted_filesize + 1)\n        if len(data) > self.max_extracted_filesize:\n            self.logger.warning(f'File {archive_file.path.name} too big ({len(data)}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {archive_file.path.name} too big ({len(data)}).')\n            return []\n        if archive_file.path.suffix == \".gz\":\n            new_file_path = dest_dir / archive_file.path.stem\n        else:\n            new_file_path = dest_dir / archive_file.path.name\n        with new_file_path.open('wb') as f:\n            f.write(data)  # write an uncompressed file\n        return [new_file_path]\n\n    def _extract_lzma(self, archive_file: File, report: Report, dest_dir: Path) -> List[Path]:\n        # lzma is just like bz2 and gz, a compressed archive with a TAR directory inside\n        lzma_file = LZMAFile(archive_file.path)\n        data = lzma_file.read(self.max_extracted_filesize + 1)\n        if len(data) > self.max_extracted_filesize:\n            self.logger.warning(f'File {archive_file.path.name} too big ({len(data)}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {archive_file.path.name} too big ({len(data)}).')\n            return []\n        if archive_file.path.suffix == \".lzma\":\n            new_file_path = dest_dir / archive_file.path.stem\n        else:\n            new_file_path = dest_dir / archive_file.path.name\n        with new_file_path.open('wb') as f:\n            f.write(data)  # write an uncompressed file\n        return [new_file_path]\n\n    @overload\n    def check_dfvfs(self, submitted_file: File, check_only: Literal[True]) -> bool:\n        ...\n\n    @overload\n    def check_dfvfs(self, submitted_file: File, check_only: Literal[False]) -> List[Tuple[PathSpec, tsk_volume_system.TSKVolumeSystem]]:\n        ...\n\n    def check_dfvfs(self, submitted_file: File, check_only: bool) -> Union[bool, List[Tuple[PathSpec, tsk_volume_system.TSKVolumeSystem]]]:\n\n        to_process = []\n\n        path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_OS, location=submitted_file.path)\n\n        if type_indicators := analyzer.Analyzer.GetStorageMediaImageTypeIndicators(path_spec):\n            # NOTE: type_indicators can be a list, we pick the 1st one, but might want to loop\n            for ti in type_indicators:\n                path = factory.Factory.NewPathSpec(ti, parent=path_spec)\n                to_process.append(path)\n        else:\n            # The RAW storage media image type cannot be detected based on\n            # a signature so we try to detect it based on common file naming\n            # schemas.\n            file_system = resolver.Resolver.OpenFileSystem(path_spec)\n            raw_path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_RAW, parent=path_spec)\n            try:\n                glob_results = raw_helper.RawGlobPathSpec(file_system, raw_path_spec)\n            except errors.PathSpecError:\n                return False\n            if not glob_results:\n                return False\n            # NOTE: what are we supposed to do if we have more?\n            to_process = glob_results\n\n        to_return = []\n        for path_to_process in to_process:\n            volume_path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_TSK_PARTITION, location='/', parent=path_to_process)\n            try:\n                volume_system = tsk_volume_system.TSKVolumeSystem()\n                volume_system.Open(volume_path_spec)\n                if check_only:\n                    return True\n                to_return.append((path_to_process, volume_system))\n            except (OSError, errors.BackEndError):\n                self.logger.info('Not supported by dfvfs')\n            except Exception:\n                self.logger.info('Unknown exception from dfvfs')\n        if check_only:\n            # at this stage, if we are in check_only and didn't return, it's not supported\n            return False\n        return to_return\n\n    def extract_with_dfvfs(self, archive_file: File, report: Report) -> List[Tuple[str, BytesIO]]:\n        extracted: List[Tuple[str, BytesIO]] = []\n\n        def process_dir(file_entry: FileEntry):\n            for sub_file_entry in file_entry.sub_file_entries:\n                if len(extracted) >= self.max_files_in_archive:\n                    self.logger.warning(f'Too many files ({len(extracted)}/{self.max_files_in_archive}) in the archive, stop extracting.')\n                    report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                    report.add_details('Warning', f'Too many files ({len(extracted)}/{self.max_files_in_archive}) in the archive')\n                    break\n                if sub_file_entry.IsFile():\n                    file_object = sub_file_entry.GetFileObject()\n                    file_content = BytesIO(file_object.read())\n                    if file_content.getbuffer().nbytes >= self.max_extracted_filesize:\n                        self.logger.warning(f'File {sub_file_entry.name} from {archive_file.path.name} too big ({file_content.getbuffer().nbytes}).')\n                        report.status = Status.ERROR if self.max_is_error else Status.ALERT\n                        report.add_details('Warning', f'File {archive_file.path.name} too big ({file_content.getbuffer().nbytes}).')\n                        continue\n                    extracted.append((sub_file_entry.name, BytesIO(file_object.read())))\n                elif sub_file_entry.IsDirectory():\n                    process_dir(sub_file_entry)\n\n        for path_spec, volume_system in self.check_dfvfs(archive_file, False):  # pylint: disable=not-an-iterable\n            for volume in volume_system.volumes:\n                if volume_identifier := getattr(volume, 'identifier'):\n                    volume = volume_system.GetVolumeByIdentifier(volume_identifier)\n                    if not volume:\n                        self.logger.warning(f'Unable to find volume {volume_identifier}')\n                        continue\n\n                    # We check the current partition\n                    _path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_TSK_PARTITION,\n                                                             location=f'/{volume.identifier}', parent=path_spec)\n\n                    # We directly mount the /\n                    mft_path_spec = factory.Factory.NewPathSpec(definitions.TYPE_INDICATOR_TSK,\n                                                                location='/', parent=_path_spec)\n\n                    file_entry = resolver.Resolver.OpenFileEntry(mft_path_spec)\n                    process_dir(file_entry)\n                else:\n                    self.logger.warning('Missing volume identifier, cannot do anything.')\n        return extracted\n\n    def extract_eml(self, eml_data: Dict) -> List[Tuple[str, BytesIO]]:\n        extracted: List[Tuple[str, BytesIO]] = []\n        for attachment in eml_data['attachment']:\n            extracted.append((attachment['filename'], BytesIO(base64.b64decode(attachment['raw']))))\n        return extracted\n\n    def extract_msg(self, msg_data: Union[Message, MessageSigned]) -> List[Tuple[str, BytesIO]]:\n        extracted: List[Tuple[str, BytesIO]] = []\n        for attachment in msg_data.attachments:\n            if isinstance(attachment.data, bytes):\n                blob = BytesIO(attachment.data)\n            elif isinstance(attachment.data, MSGFile):\n                blob = BytesIO()\n                attachment.data.export(blob)\n            extracted.append((attachment.getFilename(), blob))\n        return extracted\n\n    def analyse(self, task: Task, report: Report, manual_trigger: bool=False):\n        # The files supported by dfvfs generally don't have proper mime types, so we just try it on everything.\n        dfvfs_info = self.check_dfvfs(task.file, True)\n        if not (task.file.is_archive or task.file.is_eml or task.file.is_msg or dfvfs_info):\n            report.status = Status.NOTAPPLICABLE\n            return\n\n        # Check if we reach the max recursivity (archive in archive in archive...)\n        _curtask = task\n        _cur_recurse = self.max_recurse\n        _cur_max_files_in_recurse = self.max_files_in_recursive_archive\n        while _cur_recurse > 0 and _cur_max_files_in_recurse > 0:\n            if not _curtask.parent:\n                break\n            _curtask = _curtask.parent\n            _cur_recurse -= 1\n            _cur_max_files_in_recurse -= len(_curtask.extracted)\n\n        if _cur_recurse <= 0:\n            self.logger.warning(f'File {task.file.path.name} is too deep in the recursion chain (>{self.max_recurse}).')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {task.file.path.name} is too deep in the recursion chain (>{self.max_recurse}). If you want to scan it anyway, click on Actions > Rescan file.')\n            return\n\n        if _cur_max_files_in_recurse <= 0:\n            self.logger.warning(f'File {task.file.path.name} cannot be extracted, too many files (>{self.max_files_in_recursive_archive}) in the recursive archive.')\n            report.status = Status.ERROR if self.max_is_error else Status.ALERT\n            report.add_details('Warning', f'File {task.file.path.name} cannot be extracted, too many files (>{self.max_files_in_recursive_archive}) in the recursive archive. If you want to scan it anyway, click on Actions > Rescan file.')\n            return\n\n        if not task.user:\n            raise PandoraException('The task user is missing. Should not happen, but investigate if it does.')\n\n        pandora = Pandora()\n\n        tasks: List[Task] = []\n        extracted_dir = task.file.directory / 'extracted'\n        safe_create_dir(extracted_dir)\n        extracted: Sequence[Union[Path, Tuple[str, BytesIO]]] = []\n\n        # Try to extract files from archive\n        # TODO: Support other archive formats\n        if task.file.is_archive:\n            if task.password:\n                self.passwords = [task.password]\n            else:\n                self.passwords = self.zip_passwords\n            try:\n                if task.file.mime_type == \"application/x-7z-compressed\":\n                    extracted = self._extract_7z(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/vnd.ms-cab-compressed\":\n                    extracted = self._extract_cab(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-rar\":\n                    extracted = self._extract_rar(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-bzip2\":\n                    extracted = self._extract_bz2(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/gzip\":\n                    extracted = self._extract_gz(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-tar\":\n                    extracted = self._extract_tar(task.file, report, extracted_dir)\n                elif task.file.mime_type in [\"application/x-lzma\", \"application/x-xz\", \"application/x-lzip\"]:\n                    extracted = self._extract_lzma(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/x-iso9660-image\":\n                    extracted = self._extract_iso(task.file, report, extracted_dir)\n                elif task.file.mime_type == \"application/zip\":\n                    extracted = self._extract_zip(task.file, report, extracted_dir)\n                    if not extracted:\n                        report.clear_extras()\n                        report.clear_details()\n                        report.reset_status()\n                        extracted = self._extract_zip(task.file, report, extracted_dir, pyzipper.AESZipFile)\n                else:\n                    raise PandoraException(f'Unsupported mimetype: {task.file.mime_type}')\n            except BaseException as e:\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to extract {task.file.path.name}: {e}.')\n                report.add_extra('no_password', True)\n                extracted = []\n                self.logger.exception(e)\n\n        # Try to extract attachments from EML file\n        if task.file.is_eml:\n            if not task.file.eml_data or 'attachment' not in task.file.eml_data or not task.file.eml_data['attachment']:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_eml(task.file.eml_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif task.file.is_msg:\n            if not task.file.msg_data or not task.file.msg_data.attachments:\n                report.status = Status.NOTAPPLICABLE\n            else:\n                try:\n                    extracted = self.extract_msg(task.file.msg_data)\n                except Exception as e:\n                    self.logger.exception(e)\n\n        elif dfvfs_info:\n            # this is a dfvfs supported file\n            try:\n                extracted = self.extract_with_dfvfs(task.file, report)\n            except Exception as e:\n                self.logger.exception('dfVFS dislikes it.')\n                report.status = Status.WARN\n                report.add_details('Warning', f'Unable to process with dfVFS {task.file.path.name}: {e}.')\n\n        if not extracted and report.status != Status.NOTAPPLICABLE:\n            report.status = Status.WARN\n            report.add_details('Warning', 'Nothing to extract.')\n\n        for ef in extracted:\n            if isinstance(ef, Path):\n                filename = ef.name\n                with ef.open('rb') as f:\n                    sample = BytesIO(f.read())\n            else:\n                filename, sample = ef\n            new_task = Task.new_task(user=task.user, sample=sample,\n                                     filename=filename,\n                                     disabled_workers=task.disabled_workers,\n                                     parent=task)\n            pandora.add_extracted_reference(task, new_task)\n            pandora.enqueue_task(new_task)\n            tasks.append(new_task)\n\n        shutil.rmtree(extracted_dir)\n\n        if not tasks and not report.status == Status.NOTAPPLICABLE:\n            # Nothing was extracted\n            report.status = Status.WARN\n            report.add_details('Warning', 'Looks like the archive is empty (?). This is suspicious.')\n        elif report.status not in [Status.ERROR, Status.WARN, Status.ALERT, Status.OVERWRITE]:\n            # wait for all the workers to finish, or have one of them raising an ALERT\n            while not all(t.workers_done for t in tasks):\n                for t in tasks:\n                    # If any of the task is marked as ALERT or OVERWRITE, we can quit.\n                    if t.workers_done and t.status >= Status.ALERT:\n                        report.add_details('Warning', 'There are suspicious files in this archive, click on the \"Extracted\" tab for more.')\n                        break\n                time.sleep(1)\n            report.status = max(t.status for t in tasks if t.workers_done)\n", "meta:\n  name: Extractor\n  description: Extracts content from the uploaded object (email, archive)\n  replicas: 5\n\nsettings:\n  max_files_in_archive: 15\n  max_files_in_recursive_archive: 25\n  max_recurse: 2\n  max_extracted_filesize_in_mb: 100\n  max_is_error: no\n  zip_passwords:\n    - ''\n    - virus\n    - CERT_SOC\n    - cert\n    - pandora\n    - infected\n    - 123\n"], "filenames": ["pandora/workers/extractor.py", "pandora/workers/extractor.yml.sample"], "buggy_code_start_loc": [503, 7], "buggy_code_end_loc": [613, 7], "fixing_code_start_loc": [504, 8], "fixing_code_end_loc": [640, 10], "type": "CWE-20", "message": "workers/extractor.py in Pandora (aka pandora-analysis/pandora) 1.3.0 allows a denial of service when an attacker submits a deeply nested ZIP archive (aka ZIP bomb).", "other": {"cve": {"id": "CVE-2023-22898", "sourceIdentifier": "cve@mitre.org", "published": "2023-01-10T02:15:09.900", "lastModified": "2023-01-13T15:58:58.583", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "workers/extractor.py in Pandora (aka pandora-analysis/pandora) 1.3.0 allows a denial of service when an attacker submits a deeply nested ZIP archive (aka ZIP bomb)."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:circl:pandora:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.3.1", "matchCriteriaId": "5D921D8A-0EFD-4FBB-8B02-22E682A059C5"}]}]}], "references": [{"url": "https://github.com/pandora-analysis/pandora/commit/1dc06327fdc07c56eae653e497dd137ec70d8265", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/pandora-analysis/pandora/commit/1dc06327fdc07c56eae653e497dd137ec70d8265"}}