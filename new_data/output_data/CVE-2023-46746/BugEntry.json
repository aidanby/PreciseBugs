{"buggy_code": ["#!/usr/bin/env bash\n\n# Script for running the functional tests in CI, outputting an Istambul coverage\n# report. When running the intetgration tests locally, it's probably better to\n# simply run `pnpm functional_tests` directly which will allow e.g. to watch for\n# changes. This script is intended to handle the complexities of spinning up the\n# plugin server with the appropriate environment vars setup, and ensuring we\n# bring down the server such that c8 produces the coverage report.\n# Context is this was originally written in the GitHub Actions workflow file,\n# but it's easier to debug in a script.\n\nset -e -o pipefail\n\nexport WORKER_CONCURRENCY=1\nexport CONVERSION_BUFFER_ENABLED=true\nexport BUFFER_CONVERSION_SECONDS=2 # Make sure we don't have to wait for the default 60 seconds\nexport KAFKA_MAX_MESSAGE_BATCH_SIZE=0\nexport APP_METRICS_GATHERED_FOR_ALL=true\n\n# Not important at all, but I like to see nice red/green for tests\nexport FORCE_COLOR=true\n\nLOG_FILE=$(mktemp)\n\necho '::group::Starting plugin server'\n\nNODE_OPTIONS='--max_old_space_size=4096' ./node_modules/.bin/c8 --reporter html node dist/index.js >\"$LOG_FILE\" 2>&1 &\nSERVER_PID=$!\nSECONDS=0\n\nuntil curl http://localhost:6738/_ready; do\n    if ((SECONDS > 60)); then\n        echo 'Timed out waiting for plugin-server to be ready'\n        echo '::endgroup::'\n        echo '::group::Plugin Server logs'\n        cat \"$LOG_FILE\"\n        echo '::endgroup::'\n        exit 1\n    fi\n\n    echo ''\n    echo 'Waiting for plugin-server to be ready...'\n    sleep 1\ndone\n\necho ''\n\necho '::endgroup::'\n\nset +e\npnpm functional_tests --maxConcurrency=10 --verbose\nexit_code=$?\nset -e\n\nkill $SERVER_PID\nSECONDS=0\n\nwhile kill -0 $SERVER_PID; do\n    if ((SECONDS > 60)); then\n        echo 'Timed out waiting for plugin-server to exit'\n        break\n    fi\n\n    echo \"Waiting for plugin-server to exit, pid $SERVER_PID...\"\n    sleep 1\ndone\n\necho '::group::Plugin Server logs'\ncat \"$LOG_FILE\"\necho '::endgroup::'\n\nexit $exit_code\n", "import { LogLevel, PluginsServerConfig, stringToPluginServerMode, ValueMatcher } from '../types'\nimport { isDevEnv, isTestEnv, stringToBoolean } from '../utils/env-utils'\nimport { KAFKAJS_LOG_LEVEL_MAPPING } from './constants'\nimport {\n    KAFKA_EVENTS_JSON,\n    KAFKA_EVENTS_PLUGIN_INGESTION,\n    KAFKA_EVENTS_PLUGIN_INGESTION_OVERFLOW,\n} from './kafka-topics'\n\nexport const DEFAULT_HTTP_SERVER_PORT = 6738\n\nexport const defaultConfig = overrideWithEnv(getDefaultConfig())\n\nexport function getDefaultConfig(): PluginsServerConfig {\n    return {\n        DATABASE_URL: isTestEnv()\n            ? 'postgres://posthog:posthog@localhost:5432/test_posthog'\n            : isDevEnv()\n            ? 'postgres://posthog:posthog@localhost:5432/posthog'\n            : '',\n        DATABASE_READONLY_URL: '',\n        PLUGIN_STORAGE_DATABASE_URL: '',\n        POSTGRES_CONNECTION_POOL_SIZE: 10,\n        POSTHOG_DB_NAME: null,\n        POSTHOG_DB_USER: 'postgres',\n        POSTHOG_DB_PASSWORD: '',\n        POSTHOG_POSTGRES_HOST: 'localhost',\n        POSTHOG_POSTGRES_PORT: 5432,\n        CLICKHOUSE_HOST: 'localhost',\n        CLICKHOUSE_OFFLINE_CLUSTER_HOST: null,\n        CLICKHOUSE_DATABASE: isTestEnv() ? 'posthog_test' : 'default',\n        CLICKHOUSE_USER: 'default',\n        CLICKHOUSE_PASSWORD: null,\n        CLICKHOUSE_CA: null,\n        CLICKHOUSE_SECURE: false,\n        CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS: true,\n        EVENT_OVERFLOW_BUCKET_CAPACITY: 1000,\n        EVENT_OVERFLOW_BUCKET_REPLENISH_RATE: 1.0,\n        KAFKA_HOSTS: 'kafka:9092', // KEEP IN SYNC WITH posthog/settings/data_stores.py\n        KAFKA_CLIENT_CERT_B64: undefined,\n        KAFKA_CLIENT_CERT_KEY_B64: undefined,\n        KAFKA_TRUSTED_CERT_B64: undefined,\n        KAFKA_SECURITY_PROTOCOL: undefined,\n        KAFKA_SASL_MECHANISM: undefined,\n        KAFKA_SASL_USER: undefined,\n        KAFKA_SASL_PASSWORD: undefined,\n        KAFKA_CLIENT_RACK: undefined,\n        KAFKA_CONSUMPTION_MAX_BYTES: 10_485_760, // Default value for kafkajs\n        KAFKA_CONSUMPTION_MAX_BYTES_PER_PARTITION: 1_048_576, // Default value for kafkajs, must be bigger than message size\n        KAFKA_CONSUMPTION_MAX_WAIT_MS: 50, // Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages.\n        KAFKA_CONSUMPTION_ERROR_BACKOFF_MS: 100, // Timeout when a partition read fails (possibly because empty).\n        KAFKA_CONSUMPTION_BATCHING_TIMEOUT_MS: 500, // Timeout on reads from the prefetch buffer before running consumer loops\n        KAFKA_CONSUMPTION_TOPIC: KAFKA_EVENTS_PLUGIN_INGESTION,\n        KAFKA_CONSUMPTION_OVERFLOW_TOPIC: KAFKA_EVENTS_PLUGIN_INGESTION_OVERFLOW,\n        KAFKA_CONSUMPTION_REBALANCE_TIMEOUT_MS: null,\n        KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS: 30_000,\n        KAFKA_TOPIC_CREATION_TIMEOUT_MS: isDevEnv() ? 30_000 : 5_000, // rdkafka default is 5s, increased in devenv to resist to slow kafka\n        KAFKA_FLUSH_FREQUENCY_MS: isTestEnv() ? 5 : 500,\n        APP_METRICS_FLUSH_FREQUENCY_MS: isTestEnv() ? 5 : 20_000,\n        APP_METRICS_FLUSH_MAX_QUEUE_SIZE: isTestEnv() ? 5 : 1000,\n        KAFKA_PRODUCER_LINGER_MS: 20, // rdkafka default is 5ms\n        KAFKA_PRODUCER_BATCH_SIZE: 8 * 1024 * 1024, // rdkafka default is 1MiB\n        KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES: 100_000, // rdkafka default is 100_000\n        REDIS_URL: 'redis://127.0.0.1',\n        POSTHOG_REDIS_PASSWORD: '',\n        POSTHOG_REDIS_HOST: '',\n        POSTHOG_REDIS_PORT: 6379,\n        BASE_DIR: '.',\n        PLUGINS_RELOAD_PUBSUB_CHANNEL: 'reload-plugins',\n        WORKER_CONCURRENCY: 1,\n        TASK_TIMEOUT: 30,\n        TASKS_PER_WORKER: 10,\n        INGESTION_CONCURRENCY: 10,\n        INGESTION_BATCH_SIZE: 500,\n        LOG_LEVEL: isTestEnv() ? LogLevel.Warn : LogLevel.Info,\n        SENTRY_DSN: null,\n        SENTRY_PLUGIN_SERVER_TRACING_SAMPLE_RATE: 0,\n        SENTRY_PLUGIN_SERVER_PROFILING_SAMPLE_RATE: 0,\n        HTTP_SERVER_PORT: DEFAULT_HTTP_SERVER_PORT,\n        STATSD_HOST: null,\n        STATSD_PORT: 8125,\n        STATSD_PREFIX: 'plugin-server.',\n        SCHEDULE_LOCK_TTL: 60,\n        REDIS_POOL_MIN_SIZE: 1,\n        REDIS_POOL_MAX_SIZE: 3,\n        DISABLE_MMDB: isTestEnv(),\n        DISTINCT_ID_LRU_SIZE: 10000,\n        EVENT_PROPERTY_LRU_SIZE: 10000,\n        JOB_QUEUES: 'graphile',\n        JOB_QUEUE_GRAPHILE_URL: '',\n        JOB_QUEUE_GRAPHILE_SCHEMA: 'graphile_worker',\n        JOB_QUEUE_GRAPHILE_PREPARED_STATEMENTS: false,\n        JOB_QUEUE_GRAPHILE_CONCURRENCY: 1,\n        JOB_QUEUE_S3_AWS_ACCESS_KEY: '',\n        JOB_QUEUE_S3_AWS_SECRET_ACCESS_KEY: '',\n        JOB_QUEUE_S3_AWS_REGION: 'us-west-1',\n        JOB_QUEUE_S3_BUCKET_NAME: '',\n        JOB_QUEUE_S3_PREFIX: '',\n        CRASH_IF_NO_PERSISTENT_JOB_QUEUE: false,\n        HEALTHCHECK_MAX_STALE_SECONDS: 2 * 60 * 60, // 2 hours\n        PISCINA_USE_ATOMICS: true,\n        PISCINA_ATOMICS_TIMEOUT: 5000,\n        SITE_URL: null,\n        MAX_PENDING_PROMISES_PER_WORKER: 100,\n        KAFKA_PARTITIONS_CONSUMED_CONCURRENTLY: 1,\n        RECORDING_PARTITIONS_CONSUMED_CONCURRENTLY: 5,\n        CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS_TEAMS: '',\n        CLICKHOUSE_JSON_EVENTS_KAFKA_TOPIC: KAFKA_EVENTS_JSON,\n        CONVERSION_BUFFER_ENABLED: false,\n        CONVERSION_BUFFER_ENABLED_TEAMS: '',\n        CONVERSION_BUFFER_TOPIC_ENABLED_TEAMS: '',\n        BUFFER_CONVERSION_SECONDS: isDevEnv() ? 2 : 60, // KEEP IN SYNC WITH posthog/settings/ingestion.py\n        FETCH_HOSTNAME_GUARD_TEAMS: '',\n        PERSON_INFO_CACHE_TTL: 5 * 60, // 5 min\n        KAFKA_HEALTHCHECK_SECONDS: 20,\n        OBJECT_STORAGE_ENABLED: true,\n        OBJECT_STORAGE_ENDPOINT: 'http://localhost:19000',\n        OBJECT_STORAGE_REGION: 'us-east-1',\n        OBJECT_STORAGE_ACCESS_KEY_ID: 'object_storage_root_user',\n        OBJECT_STORAGE_SECRET_ACCESS_KEY: 'object_storage_root_password',\n        OBJECT_STORAGE_BUCKET: 'posthog',\n        PLUGIN_SERVER_MODE: null,\n        PLUGIN_LOAD_SEQUENTIALLY: false,\n        KAFKAJS_LOG_LEVEL: 'WARN',\n        HISTORICAL_EXPORTS_ENABLED: true,\n        HISTORICAL_EXPORTS_MAX_RETRY_COUNT: 15,\n        HISTORICAL_EXPORTS_INITIAL_FETCH_TIME_WINDOW: 10 * 60 * 1000,\n        HISTORICAL_EXPORTS_FETCH_WINDOW_MULTIPLIER: 1.5,\n        APP_METRICS_GATHERED_FOR_ALL: isDevEnv() ? true : false,\n        MAX_TEAM_ID_TO_BUFFER_ANONYMOUS_EVENTS_FOR: 0,\n        USE_KAFKA_FOR_SCHEDULED_TASKS: true,\n        CLOUD_DEPLOYMENT: null,\n        EXTERNAL_REQUEST_TIMEOUT_MS: 10 * 1000, // 10 seconds\n        DROP_EVENTS_BY_TOKEN_DISTINCT_ID: '',\n        POE_EMBRACE_JOIN_FOR_TEAMS: '',\n        RELOAD_PLUGIN_JITTER_MAX_MS: 60000,\n\n        STARTUP_PROFILE_DURATION_SECONDS: 300, // 5 minutes\n        STARTUP_PROFILE_CPU: false,\n        STARTUP_PROFILE_HEAP: false,\n        STARTUP_PROFILE_HEAP_INTERVAL: 512 * 1024, // default v8 value\n        STARTUP_PROFILE_HEAP_DEPTH: 16, // default v8 value\n\n        SESSION_RECORDING_KAFKA_HOSTS: undefined,\n        SESSION_RECORDING_KAFKA_SECURITY_PROTOCOL: undefined,\n        SESSION_RECORDING_KAFKA_BATCH_SIZE: 500,\n        SESSION_RECORDING_KAFKA_QUEUE_SIZE: 1500,\n\n        SESSION_RECORDING_LOCAL_DIRECTORY: '.tmp/sessions',\n        // NOTE: 10 minutes\n        SESSION_RECORDING_MAX_BUFFER_AGE_SECONDS: 60 * 10,\n        SESSION_RECORDING_BUFFER_AGE_JITTER: 0.3,\n        SESSION_RECORDING_BUFFER_AGE_IN_MEMORY_MULTIPLIER: 1.2,\n        SESSION_RECORDING_MAX_BUFFER_SIZE_KB: 1024 * 50, // 50MB\n        SESSION_RECORDING_REMOTE_FOLDER: 'session_recordings',\n        SESSION_RECORDING_REDIS_PREFIX: '@posthog/replay/',\n        SESSION_RECORDING_PARTITION_REVOKE_OPTIMIZATION: false,\n        SESSION_RECORDING_PARALLEL_CONSUMPTION: false,\n        POSTHOG_SESSION_RECORDING_REDIS_HOST: undefined,\n        POSTHOG_SESSION_RECORDING_REDIS_PORT: undefined,\n        SESSION_RECORDING_CONSOLE_LOGS_INGESTION_ENABLED: true,\n    }\n}\n\nexport const sessionRecordingConsumerConfig = (config: PluginsServerConfig): PluginsServerConfig => {\n    // When running the blob consumer we override a bunch of settings to use the session recording ones if available\n    return {\n        ...config,\n        KAFKA_HOSTS: config.SESSION_RECORDING_KAFKA_HOSTS || config.KAFKA_HOSTS,\n        KAFKA_SECURITY_PROTOCOL: config.SESSION_RECORDING_KAFKA_SECURITY_PROTOCOL || config.KAFKA_SECURITY_PROTOCOL,\n        POSTHOG_REDIS_HOST: config.POSTHOG_SESSION_RECORDING_REDIS_HOST || config.POSTHOG_REDIS_HOST,\n        POSTHOG_REDIS_PORT: config.POSTHOG_SESSION_RECORDING_REDIS_PORT || config.POSTHOG_REDIS_PORT,\n    }\n}\n\nexport function overrideWithEnv(\n    config: PluginsServerConfig,\n    env: Record<string, string | undefined> = process.env\n): PluginsServerConfig {\n    const defaultConfig = getDefaultConfig() as any // to make typechecker happy to use defaultConfig[key]\n\n    const tmpConfig: any = { ...config }\n    for (const key of Object.keys(config)) {\n        if (typeof env[key] !== 'undefined') {\n            if (key == 'PLUGIN_SERVER_MODE') {\n                const mode = env[key]\n                if (mode == null || mode in stringToPluginServerMode) {\n                    tmpConfig[key] = env[key]\n                } else {\n                    throw Error(`Invalid PLUGIN_SERVER_MODE ${env[key]}`)\n                }\n            } else if (typeof defaultConfig[key] === 'number') {\n                tmpConfig[key] = env[key]?.indexOf('.') ? parseFloat(env[key]!) : parseInt(env[key]!)\n            } else if (typeof defaultConfig[key] === 'boolean') {\n                tmpConfig[key] = stringToBoolean(env[key])\n            } else {\n                tmpConfig[key] = env[key]\n            }\n        }\n    }\n    const newConfig: PluginsServerConfig = { ...tmpConfig }\n\n    if (!newConfig.DATABASE_URL && !newConfig.POSTHOG_DB_NAME) {\n        throw Error(\n            'You must specify either DATABASE_URL or the database options POSTHOG_DB_NAME, POSTHOG_DB_USER, POSTHOG_DB_PASSWORD, POSTHOG_POSTGRES_HOST, POSTHOG_POSTGRES_PORT!'\n        )\n    }\n\n    if (!newConfig.DATABASE_URL) {\n        const encodedUser = encodeURIComponent(newConfig.POSTHOG_DB_USER)\n        const encodedPassword = encodeURIComponent(newConfig.POSTHOG_DB_PASSWORD)\n        newConfig.DATABASE_URL = `postgres://${encodedUser}:${encodedPassword}@${newConfig.POSTHOG_POSTGRES_HOST}:${newConfig.POSTHOG_POSTGRES_PORT}/${newConfig.POSTHOG_DB_NAME}`\n    }\n\n    if (!newConfig.JOB_QUEUE_GRAPHILE_URL) {\n        newConfig.JOB_QUEUE_GRAPHILE_URL = newConfig.DATABASE_URL\n    }\n\n    if (!Object.keys(KAFKAJS_LOG_LEVEL_MAPPING).includes(newConfig.KAFKAJS_LOG_LEVEL)) {\n        throw Error(\n            `Invalid KAFKAJS_LOG_LEVEL ${newConfig.KAFKAJS_LOG_LEVEL}. Valid: ${Object.keys(\n                KAFKAJS_LOG_LEVEL_MAPPING\n            ).join(', ')}`\n        )\n    }\n    return newConfig\n}\n\nexport function buildIntegerMatcher(config: string | undefined, allowStar: boolean): ValueMatcher<number> {\n    // Builds a ValueMatcher on a comma-separated list of values.\n    // Optionally, supports a '*' value to match everything\n    if (!config || config.trim().length == 0) {\n        return () => false\n    } else if (allowStar && config === '*') {\n        return () => true\n    } else {\n        const values = new Set(\n            config\n                .split(',')\n                .map((n) => parseInt(n))\n                .filter((num) => !isNaN(num))\n        )\n        return (v: number) => {\n            return values.has(v)\n        }\n    }\n}\n", "import { StatsD } from 'hot-shots'\nimport { Consumer, Kafka } from 'kafkajs'\nimport * as schedule from 'node-schedule'\nimport { AppMetrics } from 'worker/ingestion/app-metrics'\n\nimport { KAFKA_EVENTS_JSON, prefix as KAFKA_PREFIX } from '../../config/kafka-topics'\nimport { Hub, PluginsServerConfig } from '../../types'\nimport { PostgresRouter } from '../../utils/db/postgres'\nimport { PubSub } from '../../utils/pubsub'\nimport { status } from '../../utils/status'\nimport { ActionManager } from '../../worker/ingestion/action-manager'\nimport { ActionMatcher } from '../../worker/ingestion/action-matcher'\nimport { HookCommander } from '../../worker/ingestion/hooks'\nimport { OrganizationManager } from '../../worker/ingestion/organization-manager'\nimport { TeamManager } from '../../worker/ingestion/team-manager'\nimport { eachBatchAppsOnEventHandlers } from './batch-processing/each-batch-onevent'\nimport { eachBatchWebhooksHandlers } from './batch-processing/each-batch-webhooks'\nimport { KafkaJSIngestionConsumer, setupEventHandlers } from './kafka-queue'\n\nexport const startAsyncOnEventHandlerConsumer = async ({\n    hub, // TODO: remove needing to pass in the whole hub and be more selective on dependency injection.\n}: {\n    hub: Hub\n}) => {\n    /*\n        Consumes analytics events from the Kafka topic `clickhouse_events_json`\n        and processes any onEvent plugin handlers configured for the team. This\n        also includes `exportEvents` handlers defined in plugins as these are\n        also handled via modifying `onEvent` to call `exportEvents`.\n\n        At the moment this is just a wrapper around `IngestionConsumer`. We may\n        want to further remove that abstraction in the future.\n    */\n    status.info('\ud83d\udd01', `Starting onEvent handler consumer`)\n\n    const queue = buildOnEventIngestionConsumer({ hub })\n\n    await queue.start()\n\n    schedule.scheduleJob('0 * * * * *', async () => {\n        await queue.emitConsumerGroupMetrics()\n    })\n\n    const isHealthy = makeHealthCheck(queue.consumer, queue.sessionTimeout)\n\n    return { queue, isHealthy: () => isHealthy() }\n}\n\nexport const startAsyncWebhooksHandlerConsumer = async ({\n    kafka, // TODO: remove needing to pass in the whole hub and be more selective on dependency injection.\n    postgres,\n    teamManager,\n    organizationManager,\n    statsd,\n    serverConfig,\n    appMetrics,\n}: {\n    kafka: Kafka\n    postgres: PostgresRouter\n    teamManager: TeamManager\n    organizationManager: OrganizationManager\n    statsd: StatsD | undefined\n    serverConfig: PluginsServerConfig\n    appMetrics: AppMetrics\n}) => {\n    /*\n        Consumes analytics events from the Kafka topic `clickhouse_events_json`\n        and processes any onEvent plugin handlers configured for the team. This\n        also includes `exportEvents` handlers defined in plugins as these are\n        also handled via modifying `onEvent` to call `exportEvents`.\n\n        At the moment this is just a wrapper around `IngestionConsumer`. We may\n        want to further remove that abstraction in the future.\n    */\n    status.info('\ud83d\udd01', `Starting webhooks handler consumer`)\n\n    const consumer = kafka.consumer({\n        // NOTE: This should never clash with the group ID specified for the kafka engine posthog/ee/clickhouse/sql/clickhouse.py\n        groupId: `${KAFKA_PREFIX}clickhouse-plugin-server-async-webhooks`,\n        sessionTimeout: serverConfig.KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS,\n        readUncommitted: false,\n    })\n    setupEventHandlers(consumer)\n\n    const actionManager = new ActionManager(postgres)\n    await actionManager.prepare()\n    const actionMatcher = new ActionMatcher(postgres, actionManager, statsd)\n    const hookCannon = new HookCommander(\n        postgres,\n        teamManager,\n        organizationManager,\n        new Set(serverConfig.FETCH_HOSTNAME_GUARD_TEAMS.split(',').filter(String).map(Number)),\n        appMetrics,\n        statsd,\n        serverConfig.EXTERNAL_REQUEST_TIMEOUT_MS\n    )\n    const concurrency = serverConfig.TASKS_PER_WORKER || 20\n\n    const pubSub = new PubSub(serverConfig, {\n        'reload-action': async (message) => {\n            const { actionId, teamId } = JSON.parse(message)\n            await actionManager.reloadAction(teamId, actionId)\n        },\n        'drop-action': (message) => {\n            const { actionId, teamId } = JSON.parse(message)\n            actionManager.dropAction(teamId, actionId)\n        },\n    })\n\n    await pubSub.start()\n\n    // every 5 minutes all ActionManager caches are reloaded for eventual consistency\n    schedule.scheduleJob('*/5 * * * *', async () => {\n        await actionManager.reloadAllActions()\n    })\n\n    await consumer.subscribe({ topic: KAFKA_EVENTS_JSON, fromBeginning: false })\n    await consumer.run({\n        eachBatch: (payload) => eachBatchWebhooksHandlers(payload, actionMatcher, hookCannon, statsd, concurrency),\n    })\n\n    const isHealthy = makeHealthCheck(consumer, serverConfig.KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS)\n\n    return {\n        stop: async () => {\n            try {\n                await consumer.stop()\n            } catch (e) {\n                status.error('\ud83d\udea8', 'Error stopping consumer', e)\n            }\n            try {\n                await consumer.disconnect()\n            } catch (e) {\n                status.error('\ud83d\udea8', 'Error disconnecting consumer', e)\n            }\n        },\n        isHealthy,\n    }\n}\n\nexport const buildOnEventIngestionConsumer = ({ hub }: { hub: Hub }) => {\n    return new KafkaJSIngestionConsumer(\n        hub,\n        KAFKA_EVENTS_JSON,\n        `${KAFKA_PREFIX}clickhouse-plugin-server-async-onevent`,\n        eachBatchAppsOnEventHandlers\n    )\n}\n\nexport function makeHealthCheck(consumer: Consumer, sessionTimeout: number) {\n    const { HEARTBEAT } = consumer.events\n    let lastHeartbeat: number = Date.now()\n    consumer.on(HEARTBEAT, ({ timestamp }) => (lastHeartbeat = timestamp))\n\n    const isHealthy = async () => {\n        // Consumer has heartbeat within the session timeout, so it is healthy.\n        const milliSecondsToLastHeartbeat = Date.now() - lastHeartbeat\n        if (milliSecondsToLastHeartbeat < sessionTimeout) {\n            status.info('\ud83d\udc4d', 'Consumer heartbeat is healthy', { milliSecondsToLastHeartbeat, sessionTimeout })\n            return true\n        }\n\n        // Consumer has not heartbeat, but maybe it's because the group is\n        // currently rebalancing.\n        try {\n            const { state } = await consumer.describeGroup()\n\n            status.info('\u2139\ufe0f', 'Consumer group state', { state })\n\n            return ['CompletingRebalance', 'PreparingRebalance'].includes(state)\n        } catch (error) {\n            status.error('\ud83d\udea8', 'Error checking consumer group state', { error })\n            return false\n        }\n    }\n    return isHealthy\n}\n", "import { ReaderModel } from '@maxmind/geoip2-node'\nimport ClickHouse from '@posthog/clickhouse'\nimport {\n    Element,\n    Meta,\n    PluginAttachment,\n    PluginConfigSchema,\n    PluginEvent,\n    PluginSettings,\n    ProcessedPluginEvent,\n    Properties,\n} from '@posthog/plugin-scaffold'\nimport { Pool as GenericPool } from 'generic-pool'\nimport { StatsD } from 'hot-shots'\nimport { Redis } from 'ioredis'\nimport { Kafka } from 'kafkajs'\nimport { DateTime } from 'luxon'\nimport { Job } from 'node-schedule'\nimport { VM } from 'vm2'\n\nimport { ObjectStorage } from './main/services/object_storage'\nimport { DB } from './utils/db/db'\nimport { KafkaProducerWrapper } from './utils/db/kafka-producer-wrapper'\nimport { PostgresRouter } from './utils/db/postgres'\nimport { UUID } from './utils/utils'\nimport { AppMetrics } from './worker/ingestion/app-metrics'\nimport { OrganizationManager } from './worker/ingestion/organization-manager'\nimport { EventsProcessor } from './worker/ingestion/process-event'\nimport { TeamManager } from './worker/ingestion/team-manager'\nimport { PluginsApiKeyManager } from './worker/vm/extensions/helpers/api-key-manager'\nimport { RootAccessManager } from './worker/vm/extensions/helpers/root-acess-manager'\nimport { LazyPluginVM } from './worker/vm/lazy'\nimport { PromiseManager } from './worker/vm/promise-manager'\n\nexport { Element } from '@posthog/plugin-scaffold' // Re-export Element from scaffolding, for backwards compat.\n\ntype Brand<K, T> = K & { __brand: T }\n\nexport enum LogLevel {\n    None = 'none',\n    Debug = 'debug',\n    Info = 'info',\n    Log = 'log',\n    Warn = 'warn',\n    Error = 'error',\n}\n\nexport const logLevelToNumber: Record<LogLevel, number> = {\n    [LogLevel.None]: 0,\n    [LogLevel.Debug]: 10,\n    [LogLevel.Info]: 20,\n    [LogLevel.Log]: 30,\n    [LogLevel.Warn]: 40,\n    [LogLevel.Error]: 50,\n}\n\nexport enum KafkaSecurityProtocol {\n    Plaintext = 'PLAINTEXT',\n    SaslPlaintext = 'SASL_PLAINTEXT',\n    Ssl = 'SSL',\n    SaslSsl = 'SASL_SSL',\n}\n\nexport enum KafkaSaslMechanism {\n    Plain = 'plain',\n    ScramSha256 = 'scram-sha-256',\n    ScramSha512 = 'scram-sha-512',\n}\n\nexport enum PluginServerMode {\n    ingestion = 'ingestion',\n    ingestion_overflow = 'ingestion-overflow',\n    ingestion_historical = 'ingestion-historical',\n    async_onevent = 'async-onevent',\n    async_webhooks = 'async-webhooks',\n    jobs = 'jobs',\n    scheduler = 'scheduler',\n    analytics_ingestion = 'analytics-ingestion',\n    recordings_blob_ingestion = 'recordings-blob-ingestion',\n}\n\nexport const stringToPluginServerMode = Object.fromEntries(\n    Object.entries(PluginServerMode).map(([key, value]) => [\n        value,\n        PluginServerMode[key as keyof typeof PluginServerMode],\n    ])\n) as Record<string, PluginServerMode>\n\nexport interface PluginsServerConfig {\n    WORKER_CONCURRENCY: number // number of concurrent worker threads\n    TASKS_PER_WORKER: number // number of parallel tasks per worker thread\n    INGESTION_CONCURRENCY: number // number of parallel event ingestion queues per batch\n    INGESTION_BATCH_SIZE: number // kafka consumer batch size\n    TASK_TIMEOUT: number // how many seconds until tasks are timed out\n    DATABASE_URL: string // Postgres database URL\n    DATABASE_READONLY_URL: string // Optional read-only replica to the main Postgres database\n    PLUGIN_STORAGE_DATABASE_URL: string // Optional read-write Postgres database for plugin storage\n    POSTGRES_CONNECTION_POOL_SIZE: number\n    POSTHOG_DB_NAME: string | null\n    POSTHOG_DB_USER: string\n    POSTHOG_DB_PASSWORD: string\n    POSTHOG_POSTGRES_HOST: string\n    POSTHOG_POSTGRES_PORT: number\n    CLICKHOUSE_HOST: string\n    CLICKHOUSE_OFFLINE_CLUSTER_HOST: string | null\n    CLICKHOUSE_DATABASE: string\n    CLICKHOUSE_USER: string\n    CLICKHOUSE_PASSWORD: string | null\n    CLICKHOUSE_CA: string | null // ClickHouse CA certs\n    CLICKHOUSE_SECURE: boolean // whether to secure ClickHouse connection\n    CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS: boolean // whether to disallow external schemas like protobuf for clickhouse kafka engine\n    CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS_TEAMS: string // (advanced) a comma separated list of teams to disable clickhouse external schemas for\n    CLICKHOUSE_JSON_EVENTS_KAFKA_TOPIC: string // (advanced) topic to send events to for clickhouse ingestion\n    REDIS_URL: string\n    POSTHOG_REDIS_PASSWORD: string\n    POSTHOG_REDIS_HOST: string\n    POSTHOG_REDIS_PORT: number\n    REDIS_POOL_MIN_SIZE: number // minimum number of Redis connections to use per thread\n    REDIS_POOL_MAX_SIZE: number // maximum number of Redis connections to use per thread\n    KAFKA_HOSTS: string // comma-delimited Kafka hosts\n    KAFKA_CLIENT_CERT_B64: string | undefined\n    KAFKA_CLIENT_CERT_KEY_B64: string | undefined\n    KAFKA_TRUSTED_CERT_B64: string | undefined\n    KAFKA_SECURITY_PROTOCOL: KafkaSecurityProtocol | undefined\n    KAFKA_SASL_MECHANISM: KafkaSaslMechanism | undefined\n    KAFKA_SASL_USER: string | undefined\n    KAFKA_SASL_PASSWORD: string | undefined\n    KAFKA_CLIENT_RACK: string | undefined\n    KAFKA_CONSUMPTION_MAX_BYTES: number\n    KAFKA_CONSUMPTION_MAX_BYTES_PER_PARTITION: number\n    KAFKA_CONSUMPTION_MAX_WAIT_MS: number // fetch.wait.max.ms rdkafka parameter\n    KAFKA_CONSUMPTION_ERROR_BACKOFF_MS: number // fetch.error.backoff.ms rdkafka parameter\n    KAFKA_CONSUMPTION_BATCHING_TIMEOUT_MS: number\n    KAFKA_CONSUMPTION_TOPIC: string | null\n    KAFKA_CONSUMPTION_OVERFLOW_TOPIC: string | null\n    KAFKA_CONSUMPTION_REBALANCE_TIMEOUT_MS: number | null\n    KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS: number\n    KAFKA_TOPIC_CREATION_TIMEOUT_MS: number\n    KAFKA_PRODUCER_LINGER_MS: number // linger.ms rdkafka parameter\n    KAFKA_PRODUCER_BATCH_SIZE: number // batch.size rdkafka parameter\n    KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES: number // queue.buffering.max.messages rdkafka parameter\n    KAFKA_FLUSH_FREQUENCY_MS: number\n    APP_METRICS_FLUSH_FREQUENCY_MS: number\n    APP_METRICS_FLUSH_MAX_QUEUE_SIZE: number\n    BASE_DIR: string // base path for resolving local plugins\n    PLUGINS_RELOAD_PUBSUB_CHANNEL: string // Redis channel for reload events'\n    LOG_LEVEL: LogLevel\n    SENTRY_DSN: string | null\n    SENTRY_PLUGIN_SERVER_TRACING_SAMPLE_RATE: number // Rate of tracing in plugin server (between 0 and 1)\n    SENTRY_PLUGIN_SERVER_PROFILING_SAMPLE_RATE: number // Rate of profiling in plugin server (between 0 and 1)\n    HTTP_SERVER_PORT: number\n    STATSD_HOST: string | null\n    STATSD_PORT: number\n    STATSD_PREFIX: string\n    SCHEDULE_LOCK_TTL: number // how many seconds to hold the lock for the schedule\n    DISABLE_MMDB: boolean // whether to disable fetching MaxMind database for IP location\n    DISTINCT_ID_LRU_SIZE: number\n    EVENT_PROPERTY_LRU_SIZE: number // size of the event property tracker's LRU cache (keyed by [team.id, event])\n    JOB_QUEUES: string // retry queue engine and fallback queues\n    JOB_QUEUE_GRAPHILE_URL: string // use a different postgres connection in the graphile worker\n    JOB_QUEUE_GRAPHILE_SCHEMA: string // the postgres schema that the graphile worker\n    JOB_QUEUE_GRAPHILE_PREPARED_STATEMENTS: boolean // enable this to increase job queue throughput if not using pgbouncer\n    JOB_QUEUE_GRAPHILE_CONCURRENCY: number // concurrent jobs per pod\n    JOB_QUEUE_S3_AWS_ACCESS_KEY: string\n    JOB_QUEUE_S3_AWS_SECRET_ACCESS_KEY: string\n    JOB_QUEUE_S3_AWS_REGION: string\n    JOB_QUEUE_S3_BUCKET_NAME: string\n    JOB_QUEUE_S3_PREFIX: string // S3 filename prefix for the S3 job queue\n    CRASH_IF_NO_PERSISTENT_JOB_QUEUE: boolean // refuse to start unless there is a properly configured persistent job queue (e.g. graphile)\n    HEALTHCHECK_MAX_STALE_SECONDS: number // maximum number of seconds the plugin server can go without ingesting events before the healthcheck fails\n    PISCINA_USE_ATOMICS: boolean // corresponds to the piscina useAtomics config option (https://github.com/piscinajs/piscina#constructor-new-piscinaoptions)\n    PISCINA_ATOMICS_TIMEOUT: number // (advanced) corresponds to the length of time a piscina worker should block for when looking for tasks\n    SITE_URL: string | null\n    MAX_PENDING_PROMISES_PER_WORKER: number // (advanced) maximum number of promises that a worker can have running at once in the background. currently only targets the exportEvents buffer.\n    KAFKA_PARTITIONS_CONSUMED_CONCURRENTLY: number // (advanced) how many kafka partitions the plugin server should consume from concurrently\n    RECORDING_PARTITIONS_CONSUMED_CONCURRENTLY: number\n    CONVERSION_BUFFER_ENABLED: boolean\n    CONVERSION_BUFFER_ENABLED_TEAMS: string\n    CONVERSION_BUFFER_TOPIC_ENABLED_TEAMS: string\n    BUFFER_CONVERSION_SECONDS: number\n    FETCH_HOSTNAME_GUARD_TEAMS: string\n    PERSON_INFO_CACHE_TTL: number\n    KAFKA_HEALTHCHECK_SECONDS: number\n    OBJECT_STORAGE_ENABLED: boolean // Disables or enables the use of object storage. It will become mandatory to use object storage\n    OBJECT_STORAGE_REGION: string // s3 region\n    OBJECT_STORAGE_ENDPOINT: string // s3 endpoint\n    OBJECT_STORAGE_ACCESS_KEY_ID: string\n    OBJECT_STORAGE_SECRET_ACCESS_KEY: string\n    OBJECT_STORAGE_BUCKET: string // the object storage bucket name\n    PLUGIN_SERVER_MODE: PluginServerMode | null\n    PLUGIN_LOAD_SEQUENTIALLY: boolean // could help with reducing memory usage spikes on startup\n    KAFKAJS_LOG_LEVEL: 'NOTHING' | 'DEBUG' | 'INFO' | 'WARN' | 'ERROR'\n    HISTORICAL_EXPORTS_ENABLED: boolean // enables historical exports for export apps\n    HISTORICAL_EXPORTS_MAX_RETRY_COUNT: number\n    HISTORICAL_EXPORTS_INITIAL_FETCH_TIME_WINDOW: number\n    HISTORICAL_EXPORTS_FETCH_WINDOW_MULTIPLIER: number\n    APP_METRICS_GATHERED_FOR_ALL: boolean // whether to gather app metrics for all teams\n    MAX_TEAM_ID_TO_BUFFER_ANONYMOUS_EVENTS_FOR: number\n    USE_KAFKA_FOR_SCHEDULED_TASKS: boolean // distribute scheduled tasks across the scheduler workers\n    EVENT_OVERFLOW_BUCKET_CAPACITY: number\n    EVENT_OVERFLOW_BUCKET_REPLENISH_RATE: number\n    /** Label of the PostHog Cloud environment. Null if not running PostHog Cloud. @example 'US' */\n    CLOUD_DEPLOYMENT: string | null\n    EXTERNAL_REQUEST_TIMEOUT_MS: number\n    DROP_EVENTS_BY_TOKEN_DISTINCT_ID: string\n    POE_EMBRACE_JOIN_FOR_TEAMS: string\n    RELOAD_PLUGIN_JITTER_MAX_MS: number\n\n    // dump profiles to disk, covering the first N seconds of runtime\n    STARTUP_PROFILE_DURATION_SECONDS: number\n    STARTUP_PROFILE_CPU: boolean\n    STARTUP_PROFILE_HEAP: boolean\n    STARTUP_PROFILE_HEAP_INTERVAL: number\n    STARTUP_PROFILE_HEAP_DEPTH: number\n\n    // local directory might be a volume mount or a directory on disk (e.g. in local dev)\n    SESSION_RECORDING_LOCAL_DIRECTORY: string\n    SESSION_RECORDING_MAX_BUFFER_AGE_SECONDS: number\n    SESSION_RECORDING_MAX_BUFFER_SIZE_KB: number\n    SESSION_RECORDING_BUFFER_AGE_IN_MEMORY_MULTIPLIER: number\n    SESSION_RECORDING_BUFFER_AGE_JITTER: number\n    SESSION_RECORDING_REMOTE_FOLDER: string\n    SESSION_RECORDING_REDIS_PREFIX: string\n    SESSION_RECORDING_PARTITION_REVOKE_OPTIMIZATION: boolean\n    SESSION_RECORDING_PARALLEL_CONSUMPTION: boolean\n    SESSION_RECORDING_CONSOLE_LOGS_INGESTION_ENABLED: boolean\n\n    // Dedicated infra values\n    SESSION_RECORDING_KAFKA_HOSTS: string | undefined\n    SESSION_RECORDING_KAFKA_SECURITY_PROTOCOL: KafkaSecurityProtocol | undefined\n    SESSION_RECORDING_KAFKA_BATCH_SIZE: number\n    SESSION_RECORDING_KAFKA_QUEUE_SIZE: number\n\n    POSTHOG_SESSION_RECORDING_REDIS_HOST: string | undefined\n    POSTHOG_SESSION_RECORDING_REDIS_PORT: number | undefined\n}\n\nexport interface Hub extends PluginsServerConfig {\n    instanceId: UUID\n    // what tasks this server will tackle - e.g. ingestion, scheduled plugins or others.\n    capabilities: PluginServerCapabilities\n    // active connections to Postgres, Redis, ClickHouse, Kafka, StatsD\n    db: DB\n    postgres: PostgresRouter\n    redisPool: GenericPool<Redis>\n    clickhouse: ClickHouse\n    kafka: Kafka\n    kafkaProducer: KafkaProducerWrapper\n    objectStorage: ObjectStorage\n    // metrics\n    statsd?: StatsD\n    pluginMetricsJob: Job | undefined\n    // currently enabled plugin status\n    plugins: Map<PluginId, Plugin>\n    pluginConfigs: Map<PluginConfigId, PluginConfig>\n    pluginConfigsPerTeam: Map<TeamId, PluginConfig[]>\n    pluginSchedule: Record<string, PluginConfigId[]> | null\n    // unique hash for each plugin config; used to verify IDs caught on stack traces for unhandled promise rejections\n    pluginConfigSecrets: Map<PluginConfigId, string>\n    pluginConfigSecretLookup: Map<string, PluginConfigId>\n    // tools\n    teamManager: TeamManager\n    organizationManager: OrganizationManager\n    pluginsApiKeyManager: PluginsApiKeyManager\n    rootAccessManager: RootAccessManager\n    promiseManager: PromiseManager\n    eventsProcessor: EventsProcessor\n    appMetrics: AppMetrics\n    // geoip database, setup in workers\n    mmdb?: ReaderModel\n    // diagnostics\n    lastActivity: number\n    lastActivityType: string\n    statelessVms: StatelessVmMap\n    conversionBufferEnabledTeams: Set<number>\n    /** null means that the hostname guard is enabled for everyone */\n    fetchHostnameGuardTeams: Set<number> | null\n    // functions\n    enqueuePluginJob: (job: EnqueuedPluginJob) => Promise<void>\n    // ValueMatchers used for various opt-in/out features\n    pluginConfigsToSkipElementsParsing: ValueMatcher<number>\n    poeEmbraceJoinForTeams: ValueMatcher<number>\n    // lookups\n    eventsToDropByToken: Map<string, string[]>\n}\n\nexport interface PluginServerCapabilities {\n    // Warning: when adding more entries, make sure to update worker/vm/capabilities.ts\n    // and the shouldSetupPluginInServer() test accordingly.\n    ingestion?: boolean\n    ingestionOverflow?: boolean\n    ingestionHistorical?: boolean\n    pluginScheduledTasks?: boolean\n    processPluginJobs?: boolean\n    processAsyncOnEventHandlers?: boolean\n    processAsyncWebhooksHandlers?: boolean\n    sessionRecordingBlobIngestion?: boolean\n    transpileFrontendApps?: boolean // TODO: move this away from pod startup, into a graphile job\n    preflightSchedules?: boolean // Used for instance health checks on hobby deploy, not useful on cloud\n    http?: boolean\n    mmdb?: boolean\n}\n\nexport type EnqueuedJob = EnqueuedPluginJob | GraphileWorkerCronScheduleJob\nexport interface EnqueuedPluginJob {\n    type: string\n    payload: Record<string, any>\n    timestamp: number\n    pluginConfigId: number\n    pluginConfigTeam: number\n    jobKey?: string\n}\n\nexport interface GraphileWorkerCronScheduleJob {\n    timestamp?: number\n    jobKey?: string\n}\n\nexport enum JobName {\n    PLUGIN_JOB = 'pluginJob',\n    BUFFER_JOB = 'bufferJob',\n}\n\nexport type PluginId = Plugin['id']\nexport type PluginConfigId = PluginConfig['id']\nexport type TeamId = Team['id']\n\nexport enum MetricMathOperations {\n    Increment = 'increment',\n    Max = 'max',\n    Min = 'min',\n}\n\nexport type StoredMetricMathOperations = 'max' | 'min' | 'sum'\nexport type StoredPluginMetrics = Record<string, StoredMetricMathOperations> | null\nexport type PluginMetricsVmResponse = Record<string, string> | null\n\nexport interface JobPayloadFieldOptions {\n    type: 'string' | 'boolean' | 'json' | 'number' | 'date' | 'daterange'\n    title?: string\n    required?: boolean\n    default?: any\n    staff_only?: boolean\n}\n\nexport interface JobSpec {\n    payload?: Record<string, JobPayloadFieldOptions>\n}\n\nexport interface Plugin {\n    id: number\n    organization_id: string\n    name: string\n    plugin_type: 'local' | 'respository' | 'custom' | 'source'\n    description?: string\n    is_global: boolean\n    is_preinstalled?: boolean\n    url?: string\n    config_schema?: Record<string, PluginConfigSchema> | PluginConfigSchema[]\n    tag?: string\n    /** Cached source for plugin.json from a joined PluginSourceFile query */\n    source__plugin_json?: string\n    /** Cached source for index.ts from a joined PluginSourceFile query */\n    source__index_ts?: string\n    /** Cached source for frontend.tsx from a joined PluginSourceFile query */\n    source__frontend_tsx?: string\n    /** Cached source for site.ts from a joined PluginSourceFile query */\n    source__site_ts?: string\n    error?: PluginError\n    from_json?: boolean\n    from_web?: boolean\n    created_at?: string\n    updated_at?: string\n    capabilities?: PluginCapabilities\n    metrics?: StoredPluginMetrics\n    is_stateless?: boolean\n    public_jobs?: Record<string, JobSpec>\n    log_level?: PluginLogLevel\n}\n\nexport interface PluginCapabilities {\n    jobs?: string[]\n    scheduled_tasks?: string[]\n    methods?: string[]\n}\n\nexport interface PluginConfig {\n    id: number\n    team_id: TeamId\n    plugin?: Plugin\n    plugin_id: PluginId\n    enabled: boolean\n    order: number\n    config: Record<string, unknown>\n    has_error: boolean\n    attachments?: Record<string, PluginAttachment>\n    vm?: LazyPluginVM | null\n    created_at: string\n    updated_at?: string\n}\n\nexport interface PluginJsonConfig {\n    name?: string\n    description?: string\n    url?: string\n    main?: string\n    lib?: string\n    config?: Record<string, PluginConfigSchema> | PluginConfigSchema[]\n}\n\nexport interface PluginError {\n    message: string\n    time: string\n    name?: string\n    stack?: string\n    event?: PluginEvent | ProcessedPluginEvent | null\n}\n\nexport interface PluginAttachmentDB {\n    id: number\n    team_id: TeamId | null\n    plugin_config_id: PluginConfigId | null\n    key: string\n    content_type: string\n    file_size: number | null\n    file_name: string\n    contents: Buffer | null\n}\n\nexport enum PluginLogEntrySource {\n    System = 'SYSTEM',\n    Plugin = 'PLUGIN',\n    Console = 'CONSOLE',\n}\n\nexport enum PluginLogEntryType {\n    Debug = 'DEBUG',\n    Log = 'LOG',\n    Info = 'INFO',\n    Warn = 'WARN',\n    Error = 'ERROR',\n}\n\nexport enum PluginLogLevel {\n    Full = 0, // all logs\n    Debug = 1, // all except log\n    Warn = 2, // all except log and info\n    Critical = 3, // only error type and system source\n}\n\nexport interface PluginLogEntry {\n    id: string\n    team_id: number\n    plugin_id: number\n    plugin_config_id: number\n    timestamp: string\n    source: PluginLogEntrySource\n    type: PluginLogEntryType\n    message: string\n    instance_id: string\n}\n\nexport enum PluginSourceFileStatus {\n    Transpiled = 'TRANSPILED',\n    Locked = 'LOCKED',\n    Error = 'ERROR',\n}\n\nexport enum PluginTaskType {\n    Job = 'job',\n    Schedule = 'schedule',\n}\n\nexport interface PluginTask {\n    name: string\n    type: PluginTaskType\n    exec: (payload?: Record<string, any>) => Promise<any>\n\n    __ignoreForAppMetrics?: boolean\n}\n\nexport type VMMethods = {\n    setupPlugin?: () => Promise<void>\n    teardownPlugin?: () => Promise<void>\n    getSettings?: () => PluginSettings\n    onEvent?: (event: ProcessedPluginEvent) => Promise<void>\n    exportEvents?: (events: PluginEvent[]) => Promise<void>\n    processEvent?: (event: PluginEvent) => Promise<PluginEvent>\n}\n\nexport enum AlertLevel {\n    P0 = 0,\n    P1 = 1,\n    P2 = 2,\n    P3 = 3,\n    P4 = 4,\n}\n\nexport enum Service {\n    PluginServer = 'plugin_server',\n    DjangoServer = 'django_server',\n    Redis = 'redis',\n    Postgres = 'postgres',\n    ClickHouse = 'clickhouse',\n    Kafka = 'kafka',\n}\nexport interface Alert {\n    id: string\n    level: AlertLevel\n    key: string\n    description?: string\n    trigger_location: Service\n}\nexport interface PluginConfigVMResponse {\n    vm: VM\n    methods: VMMethods\n    tasks: Record<PluginTaskType, Record<string, PluginTask>>\n    vmResponseVariable: string\n}\n\nexport interface PluginConfigVMInternalResponse<M extends Meta = Meta> {\n    methods: VMMethods\n    tasks: Record<PluginTaskType, Record<string, PluginTask>>\n    meta: M\n}\n\nexport interface EventUsage {\n    event: string\n    usage_count: number | null\n    volume: number | null\n}\n\nexport interface PropertyUsage {\n    key: string\n    usage_count: number | null\n    volume: number | null\n}\n\n/** Raw Organization row from database. */\nexport interface RawOrganization {\n    id: string\n    name: string\n    created_at: string\n    updated_at: string\n    available_features: string[]\n}\n\n/** Usable Team model. */\nexport interface Team {\n    id: number\n    uuid: string\n    organization_id: string\n    name: string\n    anonymize_ips: boolean\n    api_token: string\n    slack_incoming_webhook: string | null\n    session_recording_opt_in: boolean\n    ingested_event: boolean\n    person_display_name_properties: string[] | null\n}\n\n/** Properties shared by RawEventMessage and EventMessage. */\nexport interface BaseEventMessage {\n    distinct_id: string\n    ip: string\n    site_url: string\n    team_id: number\n    uuid: string\n}\n\n/** Raw event message as received via Kafka. */\nexport interface RawEventMessage extends BaseEventMessage {\n    /** JSON-encoded object. */\n    data: string\n    /** ISO-formatted datetime. */\n    now: string\n    /** ISO-formatted datetime. May be empty! */\n    sent_at: string\n    /** JSON-encoded number. */\n    kafka_offset: string\n    /** Messages may have a token instead of a team_id, to be used e.g. to\n     * resolve to a team_id */\n    token?: string\n}\n\n/** Usable event message. */\nexport interface EventMessage extends BaseEventMessage {\n    data: PluginEvent\n    now: DateTime\n    sent_at: DateTime | null\n}\n\n/** Properties shared by RawClickHouseEvent and ClickHouseEvent. */\ninterface BaseEvent {\n    uuid: string\n    event: string\n    team_id: number\n    distinct_id: string\n    /** Person UUID. */\n    person_id?: string\n}\n\nexport type ISOTimestamp = Brand<string, 'ISOTimestamp'>\nexport type ClickHouseTimestamp = Brand<string, 'ClickHouseTimestamp'>\nexport type ClickHouseTimestampSecondPrecision = Brand<string, 'ClickHouseTimestamp'>\n\n/** Raw event row from ClickHouse. */\nexport interface RawClickHouseEvent extends BaseEvent {\n    timestamp: ClickHouseTimestamp\n    created_at: ClickHouseTimestamp\n    properties?: string\n    elements_chain: string\n    person_created_at?: ClickHouseTimestamp\n    person_properties?: string\n    group0_properties?: string\n    group1_properties?: string\n    group2_properties?: string\n    group3_properties?: string\n    group4_properties?: string\n    group0_created_at?: ClickHouseTimestamp\n    group1_created_at?: ClickHouseTimestamp\n    group2_created_at?: ClickHouseTimestamp\n    group3_created_at?: ClickHouseTimestamp\n    group4_created_at?: ClickHouseTimestamp\n}\n\n/** Parsed event row from ClickHouse. */\nexport interface ClickHouseEvent extends BaseEvent {\n    timestamp: DateTime\n    created_at: DateTime\n    properties: Record<string, any>\n    elements_chain: Element[] | null\n    person_created_at: DateTime | null\n    person_properties: Record<string, any>\n    group0_properties: Record<string, any>\n    group1_properties: Record<string, any>\n    group2_properties: Record<string, any>\n    group3_properties: Record<string, any>\n    group4_properties: Record<string, any>\n    group0_created_at?: DateTime | null\n    group1_created_at?: DateTime | null\n    group2_created_at?: DateTime | null\n    group3_created_at?: DateTime | null\n    group4_created_at?: DateTime | null\n}\n\n/** Event in a database-agnostic shape, AKA an ingestion event.\n * This is what should be passed around most of the time in the plugin server.\n */\ninterface BaseIngestionEvent {\n    eventUuid: string\n    event: string\n    teamId: TeamId\n    distinctId: string\n    properties: Properties\n    timestamp: ISOTimestamp\n    elementsList: Element[]\n}\n\n/** Ingestion event before saving, BaseIngestionEvent without elementsList */\nexport interface PreIngestionEvent {\n    eventUuid: string\n    event: string\n    teamId: TeamId\n    distinctId: string\n    properties: Properties\n    timestamp: ISOTimestamp\n}\n\n/** Ingestion event after saving, currently just an alias of BaseIngestionEvent */\nexport interface PostIngestionEvent extends BaseIngestionEvent {\n    person_id?: string // This is not optional, but BaseEvent needs to be fixed first\n    person_created_at: ISOTimestamp | null\n    person_properties: Properties\n}\n\nexport interface DeadLetterQueueEvent {\n    id: string\n    event_uuid: string\n    event: string\n    properties: string\n    distinct_id: string\n    team_id: number\n    elements_chain: string\n    created_at: string\n    ip: string\n    site_url: string\n    now: string\n    raw_payload: string\n    error_timestamp: string\n    error_location: string\n    error: string\n    tags: string[]\n    _timestamp: string\n    _offset: number\n}\n\nexport type PropertiesLastUpdatedAt = Record<string, string>\nexport type PropertiesLastOperation = Record<string, PropertyUpdateOperation>\n\n/** Properties shared by RawPerson and Person. */\nexport interface BasePerson {\n    id: number\n    team_id: number\n    properties: Properties\n    is_user_id: number\n    is_identified: boolean\n    uuid: string\n    properties_last_updated_at: PropertiesLastUpdatedAt\n    properties_last_operation: PropertiesLastOperation | null\n}\n\n/** Raw Person row from database. */\nexport interface RawPerson extends BasePerson {\n    created_at: string\n    version: string | null\n}\n\n/** Usable Person model. */\nexport interface Person extends BasePerson {\n    created_at: DateTime\n    version: number\n}\n\n/** Clickhouse Person model. */\nexport interface ClickHousePerson {\n    id: string\n    created_at: string\n    team_id: number\n    properties: string\n    is_identified: number\n    is_deleted: number\n    timestamp: string\n}\n\nexport type GroupTypeIndex = 0 | 1 | 2 | 3 | 4\n\ninterface BaseGroup {\n    id: number\n    team_id: number\n    group_type_index: GroupTypeIndex\n    group_key: string\n    group_properties: Properties\n    properties_last_updated_at: PropertiesLastUpdatedAt\n    properties_last_operation: PropertiesLastOperation\n}\n\n/** Raw Group row from database. */\nexport interface RawGroup extends BaseGroup {\n    created_at: string\n    version: string\n}\n\n/** Usable Group model. */\nexport interface Group extends BaseGroup {\n    created_at: DateTime\n    version: number\n}\n\nexport type GroupKey = string\n/** Clickhouse Group model */\nexport interface ClickhouseGroup {\n    group_type_index: GroupTypeIndex\n    group_key: GroupKey\n    created_at: string\n    team_id: number\n    group_properties: string\n}\n\n/** Usable PersonDistinctId model. */\nexport interface PersonDistinctId {\n    id: number\n    team_id: number\n    person_id: number\n    distinct_id: string\n    version: string | null\n}\n\n/** ClickHouse PersonDistinctId model. (person_distinct_id2 table) */\nexport interface ClickHousePersonDistinctId2 {\n    team_id: number\n    person_id: string\n    distinct_id: string\n    is_deleted: 0 | 1\n    version: number\n}\n\n/** Usable Cohort model. */\nexport interface Cohort {\n    id: number\n    name: string\n    description: string\n    deleted: boolean\n    groups: any[]\n    team_id: Team['id']\n    created_at: string\n    created_by_id: number\n    is_calculating: boolean\n    last_calculation: string\n    errors_calculating: number\n    is_static: boolean\n    version: number | null\n    pending_version: number\n}\n\n/** Usable CohortPeople model. */\nexport interface CohortPeople {\n    id: number\n    cohort_id: number\n    person_id: number\n}\n\n/** Usable Hook model. */\nexport interface Hook {\n    id: string\n    team_id: number\n    user_id: number\n    resource_id: number | null\n    event: string\n    target: string\n    created: string\n    updated: string\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport enum PropertyOperator {\n    Exact = 'exact',\n    IsNot = 'is_not',\n    IContains = 'icontains',\n    NotIContains = 'not_icontains',\n    Regex = 'regex',\n    NotRegex = 'not_regex',\n    GreaterThan = 'gt',\n    LessThan = 'lt',\n    IsSet = 'is_set',\n    IsNotSet = 'is_not_set',\n    IsDateBefore = 'is_date_before',\n    IsDateAfter = 'is_date_after',\n}\n\n/** Sync with posthog/frontend/src/types.ts */\ninterface PropertyFilterBase {\n    key: string\n    value?: string | number | Array<string | number> | null\n    label?: string\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface PropertyFilterWithOperator extends PropertyFilterBase {\n    operator?: PropertyOperator\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface EventPropertyFilter extends PropertyFilterWithOperator {\n    type: 'event'\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface PersonPropertyFilter extends PropertyFilterWithOperator {\n    type: 'person'\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface ElementPropertyFilter extends PropertyFilterWithOperator {\n    type: 'element'\n    key: 'tag_name' | 'text' | 'href' | 'selector'\n    value: string | string[]\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface CohortPropertyFilter extends PropertyFilterBase {\n    type: 'cohort'\n    key: 'id'\n    value: number | string\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport type PropertyFilter = EventPropertyFilter | PersonPropertyFilter | ElementPropertyFilter | CohortPropertyFilter\n\n/** Sync with posthog/frontend/src/types.ts */\nexport enum StringMatching {\n    Contains = 'contains',\n    Regex = 'regex',\n    Exact = 'exact',\n}\n\nexport interface ActionStep {\n    id: number\n    action_id: number\n    tag_name: string | null\n    text: string | null\n    /** @default StringMatching.Exact */\n    text_matching: StringMatching | null\n    href: string | null\n    /** @default StringMatching.Exact */\n    href_matching: StringMatching | null\n    selector: string | null\n    url: string | null\n    /** @default StringMatching.Contains */\n    url_matching: StringMatching | null\n    name: string | null\n    event: string | null\n    properties: PropertyFilter[] | null\n}\n\n/** Raw Action row from database. */\nexport interface RawAction {\n    id: number\n    team_id: TeamId\n    name: string | null\n    description: string\n    created_at: string\n    created_by_id: number | null\n    deleted: boolean\n    post_to_slack: boolean\n    slack_message_format: string\n    is_calculating: boolean\n    updated_at: string\n    last_calculated_at: string\n    bytecode?: any[]\n    bytecode_error?: string\n}\n\n/** Usable Action model. */\nexport interface Action extends RawAction {\n    steps: ActionStep[]\n    hooks: Hook[]\n}\n\n/** Raw session recording event row from ClickHouse. */\nexport interface RawSessionRecordingEvent {\n    uuid: string\n    timestamp: string\n    team_id: number\n    distinct_id: string\n    session_id: string\n    window_id: string\n    snapshot_data: string\n    created_at: string\n}\n\n/** Raw session replay event row from ClickHouse. */\nexport interface RawSessionReplayEvent {\n    min_first_timestamp: string\n    team_id: number\n    distinct_id: string\n    session_id: string\n    /* TODO what columns do we need */\n}\n\nexport interface RawPerformanceEvent {\n    uuid: string\n    team_id: number\n    distinct_id: string\n    session_id: string\n    window_id: string\n    pageview_id: string\n    current_url: string\n\n    // BASE_EVENT_COLUMNS\n    time_origin: number\n    timestamp: string\n    entry_type: string\n    name: string\n\n    // RESOURCE_EVENT_COLUMNS\n    start_time: number\n    redirect_start: number\n    redirect_end: number\n    worker_start: number\n    fetch_start: number\n    domain_lookup_start: number\n    domain_lookup_end: number\n    connect_start: number\n    secure_connection_start: number\n    connect_end: number\n    request_start: number\n    response_start: number\n    response_end: number\n    decoded_body_size: number\n    encoded_body_size: number\n    duration: number\n\n    initiator_type: string\n    next_hop_protocol: string\n    render_blocking_status: string\n    response_status: number\n    transfer_size: number\n\n    // LARGEST_CONTENTFUL_PAINT_EVENT_COLUMNS\n    largest_contentful_paint_element: string\n    largest_contentful_paint_render_time: number\n    largest_contentful_paint_load_time: number\n    largest_contentful_paint_size: number\n    largest_contentful_paint_id: string\n    largest_contentful_paint_url: string\n\n    // NAVIGATION_EVENT_COLUMNS\n    dom_complete: number\n    dom_content_loaded_event: number\n    dom_interactive: number\n    load_event_end: number\n    load_event_start: number\n    redirect_count: number\n    navigation_type: string\n    unload_event_end: number\n    unload_event_start: number\n}\n\nexport const PerformanceEventReverseMapping: { [key: number]: keyof RawPerformanceEvent } = {\n    // BASE_PERFORMANCE_EVENT_COLUMNS\n    0: 'entry_type',\n    1: 'time_origin',\n    2: 'name',\n\n    // RESOURCE_EVENT_COLUMNS\n    3: 'start_time',\n    4: 'redirect_start',\n    5: 'redirect_end',\n    6: 'worker_start',\n    7: 'fetch_start',\n    8: 'domain_lookup_start',\n    9: 'domain_lookup_end',\n    10: 'connect_start',\n    11: 'secure_connection_start',\n    12: 'connect_end',\n    13: 'request_start',\n    14: 'response_start',\n    15: 'response_end',\n    16: 'decoded_body_size',\n    17: 'encoded_body_size',\n    18: 'initiator_type',\n    19: 'next_hop_protocol',\n    20: 'render_blocking_status',\n    21: 'response_status',\n    22: 'transfer_size',\n\n    // LARGEST_CONTENTFUL_PAINT_EVENT_COLUMNS\n    23: 'largest_contentful_paint_element',\n    24: 'largest_contentful_paint_render_time',\n    25: 'largest_contentful_paint_load_time',\n    26: 'largest_contentful_paint_size',\n    27: 'largest_contentful_paint_id',\n    28: 'largest_contentful_paint_url',\n\n    // NAVIGATION_EVENT_COLUMNS\n    29: 'dom_complete',\n    30: 'dom_content_loaded_event',\n    31: 'dom_interactive',\n    32: 'load_event_end',\n    33: 'load_event_start',\n    34: 'redirect_count',\n    35: 'navigation_type',\n    36: 'unload_event_end',\n    37: 'unload_event_start',\n\n    // Added after v1\n    39: 'duration',\n    40: 'timestamp',\n}\n\nexport enum TimestampFormat {\n    ClickHouseSecondPrecision = 'clickhouse-second-precision',\n    ClickHouse = 'clickhouse',\n    ISO = 'iso',\n}\n\nexport enum Database {\n    ClickHouse = 'clickhouse',\n    Postgres = 'postgres',\n}\n\nexport interface PluginScheduleControl {\n    stopSchedule: () => Promise<void>\n    reloadSchedule: () => Promise<void>\n}\n\nexport interface JobsConsumerControl {\n    stop: () => Promise<void>\n    resume: () => Promise<void>\n}\n\nexport type IngestEventResponse =\n    | { success: true; actionMatches: Action[]; preIngestionEvent: PreIngestionEvent | null }\n    | { success: false; error: string }\n\nexport interface EventDefinitionType {\n    id: string\n    name: string\n    volume_30_day: number | null\n    query_usage_30_day: number | null\n    team_id: number\n    last_seen_at: string // DateTime\n    created_at: string // DateTime\n}\n\nexport enum UnixTimestampPropertyTypeFormat {\n    UNIX_TIMESTAMP = 'unix_timestamp',\n    UNIX_TIMESTAMP_MILLISECONDS = 'unix_timestamp_milliseconds',\n}\n\nexport enum DateTimePropertyTypeFormat {\n    ISO8601_DATE = 'YYYY-MM-DDThh:mm:ssZ',\n    FULL_DATE = 'YYYY-MM-DD hh:mm:ss',\n    FULL_DATE_INCREASING = 'DD-MM-YYYY hh:mm:ss',\n    DATE = 'YYYY-MM-DD',\n    RFC_822 = 'rfc_822',\n    WITH_SLASHES = 'YYYY/MM/DD hh:mm:ss',\n    WITH_SLASHES_INCREASING = 'DD/MM/YYYY hh:mm:ss',\n}\n\nexport enum PropertyType {\n    DateTime = 'DateTime',\n    String = 'String',\n    Numeric = 'Numeric',\n    Boolean = 'Boolean',\n}\n\nexport enum PropertyDefinitionTypeEnum {\n    Event = 1,\n    Person = 2,\n    Group = 3,\n}\n\nexport interface PropertyDefinitionType {\n    id: string\n    name: string\n    is_numerical: boolean\n    volume_30_day: number | null\n    query_usage_30_day: number | null\n    team_id: number\n    property_type?: PropertyType\n    type: PropertyDefinitionTypeEnum\n    group_type_index: number | null\n}\n\nexport interface EventPropertyType {\n    id: string\n    event: string\n    property: string\n    team_id: number\n}\n\nexport type PluginFunction = 'onEvent' | 'processEvent' | 'pluginTask'\n\nexport type GroupTypeToColumnIndex = Record<string, GroupTypeIndex>\n\nexport enum PropertyUpdateOperation {\n    Set = 'set',\n    SetOnce = 'set_once',\n}\n\nexport type StatelessVmMap = Record<PluginId, LazyPluginVM>\n\nexport enum OrganizationPluginsAccessLevel {\n    NONE = 0,\n    CONFIG = 3,\n    INSTALL = 6,\n    ROOT = 9,\n}\n\nexport enum OrganizationMembershipLevel {\n    Member = 1,\n    Admin = 8,\n    Owner = 15,\n}\n\nexport interface PipelineEvent extends Omit<PluginEvent, 'team_id'> {\n    team_id?: number | null\n    token?: string\n}\n\nexport type RedisPool = GenericPool<Redis>\n\nexport type RRWebEvent = Record<string, any> & {\n    timestamp: number\n    type: number\n    data: any\n}\n\nexport interface ValueMatcher<T> {\n    (value: T): boolean\n}\n", "import ClickHouse from '@posthog/clickhouse'\nimport * as Sentry from '@sentry/node'\nimport * as fs from 'fs'\nimport { StatsD } from 'hot-shots'\nimport { Kafka, SASLOptions } from 'kafkajs'\nimport { DateTime } from 'luxon'\nimport { hostname } from 'os'\nimport * as path from 'path'\nimport { types as pgTypes } from 'pg'\nimport { ConnectionOptions } from 'tls'\n\nimport { getPluginServerCapabilities } from '../../capabilities'\nimport { buildIntegerMatcher, defaultConfig } from '../../config/config'\nimport { KAFKAJS_LOG_LEVEL_MAPPING } from '../../config/constants'\nimport { KAFKA_JOBS } from '../../config/kafka-topics'\nimport { createRdConnectionConfigFromEnvVars, createRdProducerConfigFromEnvVars } from '../../kafka/config'\nimport { createKafkaProducer } from '../../kafka/producer'\nimport { getObjectStorage } from '../../main/services/object_storage'\nimport {\n    EnqueuedPluginJob,\n    Hub,\n    KafkaSaslMechanism,\n    KafkaSecurityProtocol,\n    PluginServerCapabilities,\n    PluginsServerConfig,\n} from '../../types'\nimport { AppMetrics } from '../../worker/ingestion/app-metrics'\nimport { OrganizationManager } from '../../worker/ingestion/organization-manager'\nimport { EventsProcessor } from '../../worker/ingestion/process-event'\nimport { TeamManager } from '../../worker/ingestion/team-manager'\nimport { isTestEnv } from '../env-utils'\nimport { status } from '../status'\nimport { createRedisPool, UUIDT } from '../utils'\nimport { PluginsApiKeyManager } from './../../worker/vm/extensions/helpers/api-key-manager'\nimport { RootAccessManager } from './../../worker/vm/extensions/helpers/root-acess-manager'\nimport { PromiseManager } from './../../worker/vm/promise-manager'\nimport { DB } from './db'\nimport { KafkaProducerWrapper } from './kafka-producer-wrapper'\nimport { PostgresRouter } from './postgres'\n\n// `node-postgres` would return dates as plain JS Date objects, which would use the local timezone.\n// This converts all date fields to a proper luxon UTC DateTime and then casts them to a string\n// Unfortunately this must be done on a global object before initializing the `Pool`\npgTypes.setTypeParser(1083 /* types.TypeId.TIME */, (timeStr) =>\n    timeStr ? DateTime.fromSQL(timeStr, { zone: 'utc' }).toISO() : null\n)\npgTypes.setTypeParser(1114 /* types.TypeId.TIMESTAMP */, (timeStr) =>\n    timeStr ? DateTime.fromSQL(timeStr, { zone: 'utc' }).toISO() : null\n)\npgTypes.setTypeParser(1184 /* types.TypeId.TIMESTAMPTZ */, (timeStr) =>\n    timeStr ? DateTime.fromSQL(timeStr, { zone: 'utc' }).toISO() : null\n)\n\nexport async function createKafkaProducerWrapper(serverConfig: PluginsServerConfig): Promise<KafkaProducerWrapper> {\n    const kafkaConnectionConfig = createRdConnectionConfigFromEnvVars(serverConfig)\n    const producerConfig = createRdProducerConfigFromEnvVars(serverConfig)\n    const producer = await createKafkaProducer(kafkaConnectionConfig, producerConfig)\n    return new KafkaProducerWrapper(producer)\n}\n\nexport function createEventsToDropByToken(eventsToDropByTokenStr?: string): Map<string, string[]> {\n    const eventsToDropByToken: Map<string, string[]> = new Map()\n    if (eventsToDropByTokenStr) {\n        eventsToDropByTokenStr.split(',').forEach((pair) => {\n            const [token, distinctID] = pair.split(':')\n            eventsToDropByToken.set(token, [...(eventsToDropByToken.get(token) || []), distinctID])\n        })\n    }\n    return eventsToDropByToken\n}\n\nexport async function createHub(\n    config: Partial<PluginsServerConfig> = {},\n    threadId: number | null = null,\n    capabilities: PluginServerCapabilities | null = null\n): Promise<[Hub, () => Promise<void>]> {\n    status.info('\u2139\ufe0f', `Connecting to all services:`)\n\n    const serverConfig: PluginsServerConfig = {\n        ...defaultConfig,\n        ...config,\n    }\n    if (capabilities === null) {\n        capabilities = getPluginServerCapabilities(serverConfig)\n    }\n    status.updatePrompt(serverConfig.PLUGIN_SERVER_MODE)\n    const instanceId = new UUIDT()\n\n    const conversionBufferEnabledTeams = new Set(\n        serverConfig.CONVERSION_BUFFER_ENABLED_TEAMS.split(',').filter(String).map(Number)\n    )\n    const fetchHostnameGuardTeams =\n        serverConfig.FETCH_HOSTNAME_GUARD_TEAMS === '*'\n            ? null\n            : new Set(serverConfig.FETCH_HOSTNAME_GUARD_TEAMS.split(',').filter(String).map(Number))\n\n    const statsd: StatsD | undefined = createStatsdClient(serverConfig, threadId)\n\n    status.info('\ud83e\udd14', `Connecting to ClickHouse...`)\n    const clickhouse = new ClickHouse({\n        // We prefer to run queries on the offline cluster.\n        host: serverConfig.CLICKHOUSE_OFFLINE_CLUSTER_HOST ?? serverConfig.CLICKHOUSE_HOST,\n        port: serverConfig.CLICKHOUSE_SECURE ? 8443 : 8123,\n        protocol: serverConfig.CLICKHOUSE_SECURE ? 'https:' : 'http:',\n        user: serverConfig.CLICKHOUSE_USER,\n        password: serverConfig.CLICKHOUSE_PASSWORD || undefined,\n        dataObjects: true,\n        queryOptions: {\n            database: serverConfig.CLICKHOUSE_DATABASE,\n            output_format_json_quote_64bit_integers: false,\n        },\n        ca: serverConfig.CLICKHOUSE_CA\n            ? fs.readFileSync(path.join(serverConfig.BASE_DIR, serverConfig.CLICKHOUSE_CA)).toString()\n            : undefined,\n        rejectUnauthorized: serverConfig.CLICKHOUSE_CA ? false : undefined,\n    })\n    status.info('\ud83d\udc4d', `ClickHouse ready`)\n\n    status.info('\ud83e\udd14', `Connecting to Kafka...`)\n\n    const kafka = createKafkaClient(serverConfig)\n    const kafkaProducer = await createKafkaProducerWrapper(serverConfig)\n    status.info('\ud83d\udc4d', `Kafka ready`)\n\n    const postgres = new PostgresRouter(serverConfig, statsd)\n    // TODO: assert tables are reachable (async calls that cannot be in a constructor)\n    status.info('\ud83d\udc4d', `Postgres Router ready`)\n\n    status.info('\ud83e\udd14', `Connecting to Redis...`)\n    const redisPool = createRedisPool(serverConfig)\n    status.info('\ud83d\udc4d', `Redis ready`)\n\n    status.info('\ud83e\udd14', `Connecting to object storage...`)\n\n    const objectStorage = getObjectStorage(serverConfig)\n    if (objectStorage) {\n        status.info('\ud83d\udc4d', 'Object storage ready')\n    } else {\n        status.warn('\ud83e\udea3', `Object storage could not be created`)\n    }\n\n    const promiseManager = new PromiseManager(serverConfig, statsd)\n\n    const db = new DB(postgres, redisPool, kafkaProducer, clickhouse, statsd, serverConfig.PERSON_INFO_CACHE_TTL)\n    const teamManager = new TeamManager(postgres, serverConfig, statsd)\n    const organizationManager = new OrganizationManager(postgres, teamManager)\n    const pluginsApiKeyManager = new PluginsApiKeyManager(db)\n    const rootAccessManager = new RootAccessManager(db)\n\n    const enqueuePluginJob = async (job: EnqueuedPluginJob) => {\n        // NOTE: we use the producer directly here rather than using the wrapper\n        // such that we can a response immediately on error, and thus bubble up\n        // any errors in producing. It's important that we ensure that we have\n        // an acknowledgement as for instance there are some jobs that are\n        // chained, and if we do not manage to produce then the chain will be\n        // broken.\n        await kafkaProducer.queueMessage({\n            topic: KAFKA_JOBS,\n            messages: [\n                {\n                    value: Buffer.from(JSON.stringify(job)),\n                    key: Buffer.from(job.pluginConfigTeam.toString()),\n                },\n            ],\n        })\n    }\n\n    const hub: Partial<Hub> = {\n        ...serverConfig,\n        instanceId,\n        capabilities,\n        db,\n        postgres,\n        redisPool,\n        clickhouse,\n        kafka,\n        kafkaProducer,\n        statsd,\n        enqueuePluginJob,\n        objectStorage: objectStorage,\n\n        plugins: new Map(),\n        pluginConfigs: new Map(),\n        pluginConfigsPerTeam: new Map(),\n        pluginConfigSecrets: new Map(),\n        pluginConfigSecretLookup: new Map(),\n\n        pluginSchedule: null,\n\n        teamManager,\n        organizationManager,\n        pluginsApiKeyManager,\n        rootAccessManager,\n        promiseManager,\n        conversionBufferEnabledTeams,\n        fetchHostnameGuardTeams,\n        pluginConfigsToSkipElementsParsing: buildIntegerMatcher(process.env.SKIP_ELEMENTS_PARSING_PLUGINS, true),\n        poeEmbraceJoinForTeams: buildIntegerMatcher(process.env.POE_EMBRACE_JOIN_FOR_TEAMS, true),\n        eventsToDropByToken: createEventsToDropByToken(process.env.DROP_EVENTS_BY_TOKEN_DISTINCT_ID),\n    }\n\n    // :TODO: This is only used on worker threads, not main\n    hub.eventsProcessor = new EventsProcessor(hub as Hub)\n\n    hub.appMetrics = new AppMetrics(\n        kafkaProducer,\n        serverConfig.APP_METRICS_FLUSH_FREQUENCY_MS,\n        serverConfig.APP_METRICS_FLUSH_MAX_QUEUE_SIZE\n    )\n\n    const closeHub = async () => {\n        if (!isTestEnv()) {\n            await hub.appMetrics?.flush()\n        }\n        await Promise.allSettled([kafkaProducer.disconnect(), redisPool.drain(), hub.postgres?.end()])\n        await redisPool.clear()\n\n        // Break circular references to allow the hub to be GCed when running unit tests\n        // TODO: change these structs to not directly reference the hub\n        hub.eventsProcessor = undefined\n        hub.appMetrics = undefined\n    }\n\n    return [hub as Hub, closeHub]\n}\n\nexport type KafkaConfig = {\n    KAFKA_HOSTS: string\n    KAFKAJS_LOG_LEVEL: keyof typeof KAFKAJS_LOG_LEVEL_MAPPING\n    KAFKA_SECURITY_PROTOCOL: 'PLAINTEXT' | 'SSL' | 'SASL_PLAINTEXT' | 'SASL_SSL' | undefined\n    KAFKA_CLIENT_CERT_B64?: string\n    KAFKA_CLIENT_CERT_KEY_B64?: string\n    KAFKA_TRUSTED_CERT_B64?: string\n    KAFKA_SASL_MECHANISM?: KafkaSaslMechanism\n    KAFKA_SASL_USER?: string\n    KAFKA_SASL_PASSWORD?: string\n    KAFKA_CLIENT_RACK?: string\n}\n\nexport function createStatsdClient(serverConfig: PluginsServerConfig, threadId: number | null) {\n    let statsd: StatsD | undefined\n\n    if (serverConfig.STATSD_HOST) {\n        status.info('\ud83e\udd14', `Connecting to StatsD...`)\n        statsd = new StatsD({\n            port: serverConfig.STATSD_PORT,\n            host: serverConfig.STATSD_HOST,\n            prefix: serverConfig.STATSD_PREFIX,\n            telegraf: true,\n            globalTags: serverConfig.PLUGIN_SERVER_MODE\n                ? { pluginServerMode: serverConfig.PLUGIN_SERVER_MODE }\n                : undefined,\n            errorHandler: (error) => {\n                status.warn('\u26a0\ufe0f', 'StatsD error', error)\n                Sentry.captureException(error, {\n                    extra: { threadId },\n                })\n            },\n        })\n        // don't repeat the same info in each thread\n        if (threadId === null) {\n            status.info(\n                '\ud83e\udeb5',\n                `Sending metrics to StatsD at ${serverConfig.STATSD_HOST}:${serverConfig.STATSD_PORT}, prefix: \"${serverConfig.STATSD_PREFIX}\"`\n            )\n        }\n        status.info('\ud83d\udc4d', `StatsD ready`)\n    }\n    return statsd\n}\n\nexport function createKafkaClient({\n    KAFKA_HOSTS,\n    KAFKAJS_LOG_LEVEL,\n    KAFKA_SECURITY_PROTOCOL,\n    KAFKA_CLIENT_CERT_B64,\n    KAFKA_CLIENT_CERT_KEY_B64,\n    KAFKA_TRUSTED_CERT_B64,\n    KAFKA_SASL_MECHANISM,\n    KAFKA_SASL_USER,\n    KAFKA_SASL_PASSWORD,\n}: KafkaConfig) {\n    let kafkaSsl: ConnectionOptions | boolean | undefined\n    if (KAFKA_CLIENT_CERT_B64 && KAFKA_CLIENT_CERT_KEY_B64 && KAFKA_TRUSTED_CERT_B64) {\n        kafkaSsl = {\n            cert: Buffer.from(KAFKA_CLIENT_CERT_B64, 'base64'),\n            key: Buffer.from(KAFKA_CLIENT_CERT_KEY_B64, 'base64'),\n            ca: Buffer.from(KAFKA_TRUSTED_CERT_B64, 'base64'),\n\n            /* Intentionally disabling hostname checking. The Kafka cluster runs in the cloud and Apache\n            Kafka on Heroku doesn't currently provide stable hostnames. We're pinned to a specific certificate\n            #for this connection even though the certificate doesn't include host information. We rely\n            on the ca trust_cert for this purpose. */\n            rejectUnauthorized: false,\n        }\n    } else if (\n        KAFKA_SECURITY_PROTOCOL === KafkaSecurityProtocol.Ssl ||\n        KAFKA_SECURITY_PROTOCOL === KafkaSecurityProtocol.SaslSsl\n    ) {\n        kafkaSsl = true\n    }\n\n    let kafkaSasl: SASLOptions | undefined\n    if (KAFKA_SASL_MECHANISM && KAFKA_SASL_USER && KAFKA_SASL_PASSWORD) {\n        kafkaSasl = {\n            mechanism: KAFKA_SASL_MECHANISM,\n            username: KAFKA_SASL_USER,\n            password: KAFKA_SASL_PASSWORD,\n        }\n    }\n\n    const kafka = new Kafka({\n        /* clientId does not need to be unique, and is used in Kafka logs and quota accounting.\n           os.hostname() returns the pod name in k8s and the container ID in compose stacks.\n           This allows us to quickly find what pod is consuming a given partition */\n        clientId: hostname(),\n        brokers: KAFKA_HOSTS.split(','),\n        logLevel: KAFKAJS_LOG_LEVEL_MAPPING[KAFKAJS_LOG_LEVEL],\n        ssl: kafkaSsl,\n        sasl: kafkaSasl,\n        connectionTimeout: 7000,\n        authenticationTimeout: 7000, // default: 1000\n    })\n    return kafka\n}\n", "// This module wraps node-fetch with a sentry tracing-aware extension\n\nimport { LookupAddress } from 'dns'\nimport dns from 'dns/promises'\nimport * as ipaddr from 'ipaddr.js'\nimport fetch, { type RequestInfo, type RequestInit, type Response, FetchError, Request } from 'node-fetch'\nimport { URL } from 'url'\n\nimport { runInSpan } from '../sentry'\n\nexport async function trackedFetch(url: RequestInfo, init?: RequestInit): Promise<Response> {\n    const request = new Request(url, init)\n    return await runInSpan(\n        {\n            op: 'fetch',\n            description: `${request.method} ${request.url}`,\n        },\n        async () => await fetch(url, init)\n    )\n}\n\ntrackedFetch.isRedirect = fetch.isRedirect\ntrackedFetch.FetchError = FetchError\n\nexport async function safeTrackedFetch(url: RequestInfo, init?: RequestInit): Promise<Response> {\n    const request = new Request(url, init)\n    return await runInSpan(\n        {\n            op: 'fetch',\n            description: `${request.method} ${request.url}`,\n        },\n        async () => {\n            await raiseIfUserProvidedUrlUnsafe(request.url)\n            return await fetch(url, init)\n        }\n    )\n}\n\nsafeTrackedFetch.isRedirect = fetch.isRedirect\nsafeTrackedFetch.FetchError = FetchError\n\n/**\n * Raise if the provided URL seems unsafe, otherwise do nothing.\n *\n * Equivalent of Django raise_if_user_provided_url_unsafe.\n */\nexport async function raiseIfUserProvidedUrlUnsafe(url: string): Promise<void> {\n    // Raise if the provided URL seems unsafe, otherwise do nothing.\n    let parsedUrl: URL\n    try {\n        parsedUrl = new URL(url)\n    } catch (err) {\n        throw new FetchError('Invalid URL', 'posthog-host-guard')\n    }\n    if (!parsedUrl.hostname) {\n        throw new FetchError('No hostname', 'posthog-host-guard')\n    }\n    if (parsedUrl.protocol !== 'http:' && parsedUrl.protocol !== 'https:') {\n        throw new FetchError('Scheme must be either HTTP or HTTPS', 'posthog-host-guard')\n    }\n    let addrinfo: LookupAddress[]\n    try {\n        addrinfo = await dns.lookup(parsedUrl.hostname, { all: true })\n    } catch (err) {\n        throw new FetchError('Invalid hostname', 'posthog-host-guard')\n    }\n    for (const { address } of addrinfo) {\n        // Prevent addressing internal services\n        if (ipaddr.parse(address).range() !== 'unicast') {\n            throw new FetchError('Internal hostname', 'posthog-host-guard')\n        }\n    }\n}\n", "import { captureException } from '@sentry/node'\nimport { StatsD } from 'hot-shots'\nimport { Histogram } from 'prom-client'\nimport { format } from 'util'\n\nimport { Action, Hook, PostIngestionEvent, Team } from '../../types'\nimport { PostgresRouter, PostgresUse } from '../../utils/db/postgres'\nimport { isCloud } from '../../utils/env-utils'\nimport { safeTrackedFetch, trackedFetch } from '../../utils/fetch'\nimport { status } from '../../utils/status'\nimport { getPropertyValueByPath, stringify } from '../../utils/utils'\nimport { AppMetrics } from './app-metrics'\nimport { OrganizationManager } from './organization-manager'\nimport { TeamManager } from './team-manager'\n\nexport const webhookProcessStepDuration = new Histogram({\n    name: 'webhook_process_event_duration',\n    help: 'Processing step latency to during webhooks processing, per tag',\n    labelNames: ['tag'],\n})\n\nexport async function instrumentWebhookStep<T>(tag: string, run: () => Promise<T>): Promise<T> {\n    const end = webhookProcessStepDuration\n        .labels({\n            tag: tag,\n        })\n        .startTimer()\n    const res = await run()\n    end()\n    return res\n}\n\nexport enum WebhookType {\n    Slack = 'slack',\n    Discord = 'discord',\n    Teams = 'teams',\n}\n\nexport function determineWebhookType(url: string): WebhookType {\n    url = url.toLowerCase()\n    if (url.includes('slack.com')) {\n        return WebhookType.Slack\n    }\n    if (url.includes('discord.com')) {\n        return WebhookType.Discord\n    }\n    return WebhookType.Teams\n}\n\n// https://api.slack.com/reference/surfaces/formatting#escaping\nfunction escapeSlack(text: string): string {\n    return text.replaceAll('&', '&amp;').replaceAll('<', '&lt;').replaceAll('>', '&gt;')\n}\n\nfunction escapeMarkdown(text: string): string {\n    const markdownChars: string[] = ['\\\\', '`', '*', '_', '{', '}', '[', ']', '(', ')', '!']\n    const lineStartChars: string[] = ['#', '-', '+']\n\n    let escapedText = ''\n    let isNewLine = true\n\n    for (const char of text) {\n        if (isNewLine && lineStartChars.includes(char)) {\n            escapedText += '\\\\' + char\n        } else if (!isNewLine && markdownChars.includes(char)) {\n            escapedText += '\\\\' + char\n        } else {\n            escapedText += char\n        }\n\n        isNewLine = char === '\\n' || char === '\\r'\n    }\n\n    return escapedText\n}\n\nexport function webhookEscape(text: string, webhookType: WebhookType): string {\n    if (webhookType === WebhookType.Slack) {\n        return escapeSlack(stringify(text))\n    }\n    return escapeMarkdown(stringify(text))\n}\n\nexport function toWebhookLink(text: string | null, url: string, webhookType: WebhookType): [string, string] {\n    const name = stringify(text)\n    if (webhookType === WebhookType.Slack) {\n        return [escapeSlack(name), `<${escapeSlack(url)}|${escapeSlack(name)}>`]\n    } else {\n        return [escapeMarkdown(name), `[${escapeMarkdown(name)}](${escapeMarkdown(url)})`]\n    }\n}\n\n// Sync with .../api/person.py and .../lib/constants.tsx\nexport const PERSON_DEFAULT_DISPLAY_NAME_PROPERTIES = [\n    'email',\n    'Email',\n    'name',\n    'Name',\n    'username',\n    'Username',\n    'UserName',\n]\n\nexport function getPersonLink(event: PostIngestionEvent, siteUrl: string): string {\n    return `${siteUrl}/person/${encodeURIComponent(event.distinctId)}`\n}\nexport function getPersonDetails(\n    event: PostIngestionEvent,\n    siteUrl: string,\n    webhookType: WebhookType,\n    team: Team\n): [string, string] {\n    // Sync the logic below with the frontend `asDisplay`\n    const personDisplayNameProperties = team.person_display_name_properties ?? PERSON_DEFAULT_DISPLAY_NAME_PROPERTIES\n    const customPropertyKey = personDisplayNameProperties.find((x) => event.person_properties?.[x])\n    const propertyIdentifier = customPropertyKey ? event.person_properties[customPropertyKey] : undefined\n\n    const customIdentifier: string =\n        typeof propertyIdentifier !== 'string' ? JSON.stringify(propertyIdentifier) : propertyIdentifier\n\n    const display: string | undefined = (customIdentifier || event.distinctId)?.trim()\n\n    return toWebhookLink(display, getPersonLink(event, siteUrl), webhookType)\n}\n\nexport function getActionLink(action: Action, siteUrl: string): string {\n    return `${siteUrl}/action/${action.id}`\n}\nexport function getActionDetails(action: Action, siteUrl: string, webhookType: WebhookType): [string, string] {\n    return toWebhookLink(action.name, getActionLink(action, siteUrl), webhookType)\n}\n\nexport function getEventLink(event: PostIngestionEvent, siteUrl: string): string {\n    return `${siteUrl}/events/${encodeURIComponent(event.eventUuid)}/${encodeURIComponent(event.timestamp)}`\n}\nexport function getEventDetails(\n    event: PostIngestionEvent,\n    siteUrl: string,\n    webhookType: WebhookType\n): [string, string] {\n    return toWebhookLink(event.event, getEventLink(event, siteUrl), webhookType)\n}\n\nconst TOKENS_REGEX_BRACKETS_EXCLUDED = /(?<=(?<!\\\\)\\[)(.*?)(?=(?<!\\\\)\\])/g\nconst TOKENS_REGEX_BRACKETS_INCLUDED = /(?<!\\\\)\\[(.*?)(?<!\\\\)\\]/g\n\nexport function getTokens(messageFormat: string): [string[], string] {\n    // This finds property value tokens, basically any string contained in square brackets\n    // Examples: \"[foo]\" is matched in \"bar [foo]\", \"[action.name]\" is matched in \"action [action.name]\"\n    // The backslash is used as an escape character - \"\\[foo\\]\" is not matched, allowing square brackets in messages\n    const matchedTokens = messageFormat.match(TOKENS_REGEX_BRACKETS_EXCLUDED) || []\n    // Replace the tokens with placeholders, and unescape leftover brackets\n    const tokenizedMessage = messageFormat.replace(TOKENS_REGEX_BRACKETS_INCLUDED, '%s').replace(/\\\\(\\[|\\])/g, '$1')\n    return [matchedTokens, tokenizedMessage]\n}\n\nexport function getValueOfToken(\n    action: Action,\n    event: PostIngestionEvent,\n    team: Team,\n    siteUrl: string,\n    webhookType: WebhookType,\n    tokenParts: string[]\n): [string, string] {\n    let text = ''\n    let markdown = ''\n\n    if (tokenParts[0] === 'user') {\n        // [user.name] and [user.foo] are DEPRECATED as they had odd mechanics\n        // [person] OR [event.properties.bar] should be used instead\n        if (tokenParts[1] === 'name') {\n            ;[text, markdown] = getPersonDetails(event, siteUrl, webhookType, team)\n        } else {\n            const propertyName = `$${tokenParts[1]}`\n            const property = event.properties?.[propertyName]\n            markdown = text = webhookEscape(property, webhookType)\n        }\n    } else if (tokenParts[0] === 'person') {\n        if (tokenParts.length === 1) {\n            ;[text, markdown] = getPersonDetails(event, siteUrl, webhookType, team)\n        } else if (tokenParts[1] === 'link') {\n            markdown = text = webhookEscape(getPersonLink(event, siteUrl), webhookType)\n        } else if (tokenParts[1] === 'properties' && tokenParts.length > 2) {\n            const property = event.person_properties\n                ? getPropertyValueByPath(event.person_properties, tokenParts.slice(2))\n                : undefined\n            markdown = text = webhookEscape(property, webhookType)\n        }\n    } else if (tokenParts[0] === 'action') {\n        if (tokenParts[1] === 'name') {\n            ;[text, markdown] = getActionDetails(action, siteUrl, webhookType)\n        } else if (tokenParts[1] === 'link') {\n            markdown = text = webhookEscape(getActionLink(action, siteUrl), webhookType)\n        }\n    } else if (tokenParts[0] === 'event') {\n        if (tokenParts.length === 1) {\n            ;[text, markdown] = getEventDetails(event, siteUrl, webhookType)\n        } else if (tokenParts[1] === 'link') {\n            markdown = text = webhookEscape(getEventLink(event, siteUrl), webhookType)\n        } else if (tokenParts[1] === 'uuid') {\n            markdown = text = webhookEscape(event.eventUuid, webhookType)\n        } else if (tokenParts[1] === 'name') {\n            // deprecated\n            markdown = text = webhookEscape(event.event, webhookType)\n        } else if (tokenParts[1] === 'event') {\n            markdown = text = webhookEscape(event.event, webhookType)\n        } else if (tokenParts[1] === 'distinct_id') {\n            markdown = text = webhookEscape(event.distinctId, webhookType)\n        } else if (tokenParts[1] === 'properties' && tokenParts.length > 2) {\n            const property = event.properties\n                ? getPropertyValueByPath(event.properties, tokenParts.slice(2))\n                : undefined\n            markdown = text = webhookEscape(property, webhookType)\n        }\n    } else {\n        throw new Error()\n    }\n    return [text, markdown]\n}\n\nexport function getFormattedMessage(\n    action: Action,\n    event: PostIngestionEvent,\n    team: Team,\n    siteUrl: string,\n    webhookType: WebhookType\n): [string, string] {\n    const messageFormat = action.slack_message_format || '[action.name] was triggered by [person]'\n    let messageText: string\n    let messageMarkdown: string\n\n    try {\n        const [tokens, tokenizedMessage] = getTokens(messageFormat)\n        const values: string[] = []\n        const markdownValues: string[] = []\n\n        for (const token of tokens) {\n            const tokenParts = token.split('.') || []\n\n            const [value, markdownValue] = getValueOfToken(action, event, team, siteUrl, webhookType, tokenParts)\n            values.push(value)\n            markdownValues.push(markdownValue)\n        }\n        messageText = format(tokenizedMessage, ...values)\n        messageMarkdown = format(tokenizedMessage, ...markdownValues)\n    } catch (error) {\n        const [actionName, actionMarkdown] = getActionDetails(action, siteUrl, webhookType)\n        messageText = `\u26a0 Error: There are one or more formatting errors in the message template for action \"${actionName}\".`\n        messageMarkdown = `*\u26a0 Error: There are one or more formatting errors in the message template for action \"${actionMarkdown}\".*`\n    }\n\n    return [messageText, messageMarkdown]\n}\n\nexport class HookCommander {\n    postgres: PostgresRouter\n    teamManager: TeamManager\n    organizationManager: OrganizationManager\n    appMetrics: AppMetrics\n    statsd: StatsD | undefined\n    siteUrl: string\n    /** null means that the hostname guard is enabled for everyone */\n    fetchHostnameGuardTeams: Set<number> | null\n\n    /** Hook request timeout in ms. */\n    EXTERNAL_REQUEST_TIMEOUT: number\n\n    constructor(\n        postgres: PostgresRouter,\n        teamManager: TeamManager,\n        organizationManager: OrganizationManager,\n        fetchHostnameGuardTeams: Set<number> | null = new Set(),\n        appMetrics: AppMetrics,\n        statsd: StatsD | undefined,\n        timeout: number\n    ) {\n        this.postgres = postgres\n        this.teamManager = teamManager\n        this.organizationManager = organizationManager\n        this.fetchHostnameGuardTeams = fetchHostnameGuardTeams\n        if (process.env.SITE_URL) {\n            this.siteUrl = process.env.SITE_URL\n        } else {\n            status.warn('\u26a0\ufe0f', 'SITE_URL env is not set for webhooks')\n            this.siteUrl = ''\n        }\n        this.statsd = statsd\n        this.appMetrics = appMetrics\n        this.EXTERNAL_REQUEST_TIMEOUT = timeout\n    }\n\n    public async findAndFireHooks(event: PostIngestionEvent, actionMatches: Action[]): Promise<void> {\n        status.debug('\ud83d\udd0d', `Looking for hooks to fire for event \"${event.event}\"`)\n        if (!actionMatches.length) {\n            status.debug('\ud83d\udd0d', `No hooks to fire for event \"${event.event}\"`)\n            return\n        }\n        status.debug('\ud83d\udd0d', `Found ${actionMatches.length} matching actions`)\n\n        const team = await this.teamManager.fetchTeam(event.teamId)\n\n        if (!team) {\n            return\n        }\n\n        const webhookUrl = team.slack_incoming_webhook\n\n        if (webhookUrl) {\n            await instrumentWebhookStep('postWebhook', async () => {\n                const webhookRequests = actionMatches\n                    .filter((action) => action.post_to_slack)\n                    .map((action) => this.postWebhook(webhookUrl, action, event, team))\n                await Promise.all(webhookRequests).catch((error) =>\n                    captureException(error, { tags: { team_id: event.teamId } })\n                )\n            })\n        }\n\n        if (await this.organizationManager.hasAvailableFeature(team.id, 'zapier')) {\n            await instrumentWebhookStep('postRestHook', async () => {\n                const restHooks = actionMatches.map(({ hooks }) => hooks).flat()\n\n                if (restHooks.length > 0) {\n                    const restHookRequests = restHooks.map((hook) => this.postRestHook(hook, event))\n                    await Promise.all(restHookRequests).catch((error) =>\n                        captureException(error, { tags: { team_id: event.teamId } })\n                    )\n\n                    this.statsd?.increment('zapier_hooks_fired', {\n                        team_id: String(team.id),\n                    })\n                }\n            })\n        }\n    }\n\n    private formatMessage(\n        webhookUrl: string,\n        action: Action,\n        event: PostIngestionEvent,\n        team: Team\n    ): Record<string, any> {\n        const webhookType = determineWebhookType(webhookUrl)\n        const [messageText, messageMarkdown] = getFormattedMessage(action, event, team, this.siteUrl, webhookType)\n        if (webhookType === WebhookType.Slack) {\n            return {\n                text: messageText,\n                blocks: [{ type: 'section', text: { type: 'mrkdwn', text: messageMarkdown } }],\n            }\n        } else {\n            return {\n                text: messageMarkdown,\n            }\n        }\n    }\n\n    private async postWebhook(\n        webhookUrl: string,\n        action: Action,\n        event: PostIngestionEvent,\n        team: Team\n    ): Promise<void> {\n        const end = webhookProcessStepDuration.labels('messageFormatting').startTimer()\n        const message = this.formatMessage(webhookUrl, action, event, team)\n        end()\n\n        const slowWarningTimeout = this.EXTERNAL_REQUEST_TIMEOUT * 0.7\n        const timeout = setTimeout(() => {\n            status.warn(\n                '\u231b',\n                `Posting Webhook slow. Timeout warning after ${\n                    slowWarningTimeout / 1000\n                } sec! url=${webhookUrl} team_id=${team.id} event_id=${event.eventUuid}`\n            )\n        }, slowWarningTimeout)\n        const relevantFetch =\n            isCloud() && (!this.fetchHostnameGuardTeams || this.fetchHostnameGuardTeams.has(team.id))\n                ? safeTrackedFetch\n                : trackedFetch\n        try {\n            await instrumentWebhookStep('fetch', async () => {\n                const request = await relevantFetch(webhookUrl, {\n                    method: 'POST',\n                    body: JSON.stringify(message, undefined, 4),\n                    headers: { 'Content-Type': 'application/json' },\n                    timeout: this.EXTERNAL_REQUEST_TIMEOUT,\n                })\n                if (!request.ok) {\n                    status.warn('\u26a0\ufe0f', `HTTP status ${request.status} for team ${team.id}`)\n                    await this.appMetrics.queueError(\n                        {\n                            teamId: event.teamId,\n                            pluginConfigId: -2, // -2 is hardcoded to mean webhooks\n                            category: 'webhook',\n                            failures: 1,\n                        },\n                        {\n                            error: `Request failed with HTTP status ${request.status}`,\n                            event,\n                        }\n                    )\n                } else {\n                    await this.appMetrics.queueMetric({\n                        teamId: event.teamId,\n                        pluginConfigId: -2, // -2 is hardcoded to mean webhooks\n                        category: 'webhook',\n                        successes: 1,\n                    })\n                }\n            })\n            this.statsd?.increment('webhook_firings', {\n                team_id: event.teamId.toString(),\n            })\n        } catch (error) {\n            await this.appMetrics.queueError(\n                {\n                    teamId: event.teamId,\n                    pluginConfigId: -2, // -2 is hardcoded to mean webhooks\n                    category: 'webhook',\n                    failures: 1,\n                },\n                {\n                    error,\n                    event,\n                }\n            )\n            throw error\n        } finally {\n            clearTimeout(timeout)\n        }\n    }\n\n    public async postRestHook(hook: Hook, event: PostIngestionEvent): Promise<void> {\n        let sendablePerson: Record<string, any> = {}\n        const { person_id, person_created_at, person_properties, ...data } = event\n        if (person_id) {\n            sendablePerson = {\n                uuid: person_id,\n                properties: person_properties,\n                created_at: person_created_at,\n            }\n        }\n\n        const payload = {\n            hook: { id: hook.id, event: hook.event, target: hook.target },\n            data: { ...data, person: sendablePerson },\n        }\n\n        const slowWarningTimeout = this.EXTERNAL_REQUEST_TIMEOUT * 0.7\n        const timeout = setTimeout(() => {\n            status.warn(\n                '\u231b',\n                `Posting RestHook slow. Timeout warning after ${slowWarningTimeout / 1000} sec! url=${\n                    hook.target\n                } team_id=${event.teamId} event_id=${event.eventUuid}`\n            )\n        }, slowWarningTimeout)\n        const relevantFetch =\n            isCloud() && (!this.fetchHostnameGuardTeams || this.fetchHostnameGuardTeams.has(hook.team_id))\n                ? safeTrackedFetch\n                : trackedFetch\n        try {\n            const request = await relevantFetch(hook.target, {\n                method: 'POST',\n                body: JSON.stringify(payload, undefined, 4),\n                headers: { 'Content-Type': 'application/json' },\n                timeout: this.EXTERNAL_REQUEST_TIMEOUT,\n            })\n            if (request.status === 410) {\n                // Delete hook on our side if it's gone on Zapier's\n                await this.deleteRestHook(hook.id)\n            }\n            if (!request.ok) {\n                status.warn('\u26a0\ufe0f', `Rest hook failed status ${request.status} for team ${event.teamId}`)\n                await this.appMetrics.queueError(\n                    {\n                        teamId: event.teamId,\n                        pluginConfigId: -1, // -1 is hardcoded to mean resthooks\n                        category: 'webhook',\n                        failures: 1,\n                    },\n                    {\n                        error: `Request failed with HTTP status ${request.status}`,\n                        event,\n                    }\n                )\n            } else {\n                await this.appMetrics.queueMetric({\n                    teamId: event.teamId,\n                    pluginConfigId: -1, // -1 is hardcoded to mean resthooks\n                    category: 'webhook',\n                    successes: 1,\n                })\n            }\n            this.statsd?.increment('rest_hook_firings')\n        } catch (error) {\n            await this.appMetrics.queueError(\n                {\n                    teamId: event.teamId,\n                    pluginConfigId: -1, // -1 is hardcoded to mean resthooks\n                    category: 'webhook',\n                    failures: 1,\n                },\n                {\n                    error,\n                    event,\n                }\n            )\n            throw error\n        } finally {\n            clearTimeout(timeout)\n        }\n    }\n\n    private async deleteRestHook(hookId: Hook['id']): Promise<void> {\n        await this.postgres.query(\n            PostgresUse.COMMON_WRITE,\n            `DELETE FROM ee_hook WHERE id = $1`,\n            [hookId],\n            'deleteRestHook'\n        )\n    }\n}\n", "import * as bigquery from '@google-cloud/bigquery'\nimport * as pubsub from '@google-cloud/pubsub'\nimport * as gcs from '@google-cloud/storage'\nimport * as contrib from '@posthog/plugin-contrib'\nimport * as scaffold from '@posthog/plugin-scaffold'\nimport * as AWS from 'aws-sdk'\nimport crypto from 'crypto'\nimport * as ethers from 'ethers'\nimport * as faker from 'faker'\nimport * as genericPool from 'generic-pool'\nimport * as jsonwebtoken from 'jsonwebtoken'\nimport * as pg from 'pg'\nimport snowflake from 'snowflake-sdk'\nimport { PassThrough } from 'stream'\nimport { Hub } from 'types'\nimport * as url from 'url'\nimport * as zlib from 'zlib'\n\nimport { isCloud, isTestEnv } from '../../utils/env-utils'\nimport { safeTrackedFetch, trackedFetch } from '../../utils/fetch'\nimport { writeToFile } from './extensions/test-utils'\n\nexport function determineImports(hub: Hub, teamId: number) {\n    return {\n        ...(isTestEnv()\n            ? {\n                  'test-utils/write-to-file': writeToFile,\n              }\n            : {}),\n        '@google-cloud/bigquery': bigquery,\n        '@google-cloud/pubsub': pubsub,\n        '@google-cloud/storage': gcs,\n        '@posthog/plugin-contrib': contrib,\n        '@posthog/plugin-scaffold': scaffold,\n        'aws-sdk': AWS,\n        ethers: ethers,\n        'generic-pool': genericPool,\n        'node-fetch':\n            isCloud() && (!hub.fetchHostnameGuardTeams || hub.fetchHostnameGuardTeams.has(teamId))\n                ? safeTrackedFetch\n                : trackedFetch,\n        'snowflake-sdk': snowflake,\n        crypto: crypto,\n        jsonwebtoken: jsonwebtoken,\n        faker: faker,\n        pg: pg,\n        stream: { PassThrough },\n        url: url,\n        zlib: zlib,\n    }\n}\n", "import { RetryError } from '@posthog/plugin-scaffold'\nimport { randomBytes } from 'crypto'\nimport { VM } from 'vm2'\n\nimport { Hub, PluginConfig, PluginConfigVMResponse } from '../../types'\nimport { createCache } from './extensions/cache'\nimport { createConsole } from './extensions/console'\nimport { createGeoIp } from './extensions/geoip'\nimport { createGoogle } from './extensions/google'\nimport { createJobs } from './extensions/jobs'\nimport { createPosthog } from './extensions/posthog'\nimport { createStorage } from './extensions/storage'\nimport { createUtils } from './extensions/utilities'\nimport { determineImports } from './imports'\nimport { transformCode } from './transforms'\nimport { upgradeExportEvents } from './upgrades/export-events'\nimport { addHistoricalEventsExportCapability } from './upgrades/historical-export/export-historical-events'\nimport { addHistoricalEventsExportCapabilityV2 } from './upgrades/historical-export/export-historical-events-v2'\n\nexport class TimeoutError extends RetryError {\n    name = 'TimeoutError'\n    caller?: string = undefined\n    pluginConfig?: PluginConfig = undefined\n\n    constructor(message: string, caller?: string, pluginConfig?: PluginConfig) {\n        super(message)\n        this.caller = caller\n        this.pluginConfig = pluginConfig\n    }\n}\n\nexport function createPluginConfigVM(\n    hub: Hub,\n    pluginConfig: PluginConfig, // NB! might have team_id = 0\n    indexJs: string\n): PluginConfigVMResponse {\n    const imports = determineImports(hub, pluginConfig.team_id)\n\n    const timer = new Date()\n\n    const statsdTiming = (metric: string) => {\n        hub.statsd?.timing(metric, timer, {\n            pluginConfigId: String(pluginConfig.id),\n            pluginName: String(pluginConfig.plugin?.name),\n            teamId: String(pluginConfig.team_id),\n        })\n    }\n\n    const transformedCode = transformCode(indexJs, hub, imports)\n\n    // Create virtual machine\n    const vm = new VM({\n        timeout: hub.TASK_TIMEOUT * 1000 + 1,\n        sandbox: {},\n    })\n\n    // Add PostHog utilities to virtual machine\n    vm.freeze(createConsole(hub, pluginConfig), 'console')\n    vm.freeze(createPosthog(hub, pluginConfig), 'posthog')\n\n    // Add non-PostHog utilities to virtual machine\n    vm.freeze(imports['node-fetch'], 'fetch')\n    vm.freeze(createGoogle(), 'google')\n\n    vm.freeze(imports, '__pluginHostImports')\n\n    if (process.env.NODE_ENV === 'test') {\n        vm.freeze(setTimeout, '__jestSetTimeout')\n    }\n\n    vm.freeze(RetryError, 'RetryError')\n\n    // Bring some useful globals into scope\n    vm.freeze(URL, 'URL')\n\n    // Creating this outside the vm (so not in a babel plugin for example)\n    // because `setTimeout` is not available inside the vm... and we don't want to\n    // make it available for now, as it makes it easier to create malicious code\n    const asyncGuard = async (promise: Promise<any>, name?: string) => {\n        const timeout = hub.TASK_TIMEOUT\n        return await Promise.race([\n            promise,\n            new Promise((resolve, reject) =>\n                setTimeout(() => {\n                    const message = `Script execution timed out after promise waited for ${timeout} second${\n                        timeout === 1 ? '' : 's'\n                    } (${pluginConfig.plugin?.name}, name: ${name}, pluginConfigId: ${pluginConfig.id})`\n                    reject(new TimeoutError(message, `${name}`, pluginConfig))\n                }, timeout * 1000)\n            ),\n        ])\n    }\n\n    vm.freeze(asyncGuard, '__asyncGuard')\n\n    vm.freeze(\n        {\n            cache: createCache(hub, pluginConfig.plugin_id, pluginConfig.team_id),\n            config: pluginConfig.config,\n            attachments: pluginConfig.attachments,\n            storage: createStorage(hub, pluginConfig),\n            geoip: createGeoIp(hub),\n            jobs: createJobs(hub, pluginConfig),\n            utils: createUtils(hub, pluginConfig.id),\n        },\n        '__pluginHostMeta'\n    )\n\n    vm.run(`\n        // two ways packages could export themselves (plus \"global\")\n        const module = { exports: {} };\n        let exports = {};\n\n        // the plugin JS code\n        ${transformedCode};\n    `)\n\n    // Add a secret hash to the end of some function names, so that we can (sometimes) identify\n    // the crashed plugin if it throws an uncaught exception in a promise.\n    if (!hub.pluginConfigSecrets.has(pluginConfig.id)) {\n        const secret = randomBytes(16).toString('hex')\n        hub.pluginConfigSecrets.set(pluginConfig.id, secret)\n        hub.pluginConfigSecretLookup.set(secret, pluginConfig.id)\n    }\n\n    // Keep the format of this in sync with `pluginConfigIdFromStack` in utils.ts\n    // Only place this after functions whose names match /^__[a-zA-Z0-9]+$/\n    const pluginConfigIdentifier = `__PluginConfig_${pluginConfig.id}_${hub.pluginConfigSecrets.get(pluginConfig.id)}`\n    const responseVar = `__pluginDetails${randomBytes(64).toString('hex')}`\n\n    // Explicitly passing __asyncGuard to the returned function from `vm.run` in order\n    // to make it harder to override the global `__asyncGuard = noop` inside plugins.\n    // This way even if promises inside plugins are unbounded, the `processEvent` function\n    // itself will still terminate after TASK_TIMEOUT seconds, not clogging the entire ingestion.\n    vm.run(`\n        if (typeof global.${responseVar} !== 'undefined') {\n            throw new Error(\"Plugin created variable ${responseVar} that is reserved for the VM.\")\n        }\n        let ${responseVar} = undefined;\n        ((__asyncGuard) => {\n            // where to find exports\n            let exportDestinations = [\n                exports,\n                exports.default,\n                module.exports\n            ].filter(d => typeof d === 'object'); // filters out exports.default if not there\n\n            // add \"global\" only if nothing exported at all\n            if (!exportDestinations.find(d => Object.keys(d).length > 0)) {\n                // we can't set it to just [global], as abstractions may add exports later\n                exportDestinations.push(global)\n            }\n\n            // export helpers\n            function __getExported (key) { return exportDestinations.find(a => a[key])?.[key] };\n            function __asyncFunctionGuard (func, name) {\n                return func ? function __innerAsyncGuard${pluginConfigIdentifier}(...args) { return __asyncGuard(func(...args), name) } : func\n            };\n\n            // inject the meta object + shareable 'global' to the end of each exported function\n            const __pluginMeta = {\n                ...__pluginHostMeta,\n                global: {}\n            };\n            function __bindMeta (keyOrFunc) {\n                const func = typeof keyOrFunc === 'function' ? keyOrFunc : __getExported(keyOrFunc);\n                if (func) return function __inBindMeta${pluginConfigIdentifier} (...args) { return func(...args, __pluginMeta) };\n            }\n            function __callWithMeta (keyOrFunc, ...args) {\n                const func = __bindMeta(keyOrFunc);\n                if (func) return func(...args);\n            }\n\n            // we have processEventBatch, but not processEvent\n            if (!__getExported('processEvent') && __getExported('processEventBatch')) {\n                exports.processEvent = async function __processEvent${pluginConfigIdentifier} (event, meta) {\n                    return (await (__getExported('processEventBatch'))([event], meta))?.[0]\n                }\n            }\n\n            // export various functions\n            const __methods = {\n                setupPlugin: __asyncFunctionGuard(__bindMeta('setupPlugin'), 'setupPlugin'),\n                teardownPlugin: __asyncFunctionGuard(__bindMeta('teardownPlugin'), 'teardownPlugin'),\n                exportEvents: __asyncFunctionGuard(__bindMeta('exportEvents'), 'exportEvents'),\n                onEvent: __asyncFunctionGuard(__bindMeta('onEvent'), 'onEvent'),\n                processEvent: __asyncFunctionGuard(__bindMeta('processEvent'), 'processEvent'),\n                getSettings: __bindMeta('getSettings'),\n            };\n\n            const __tasks = {\n                schedule: {},\n                job: {},\n            };\n\n            for (const exportDestination of exportDestinations.reverse()) {\n                // gather the runEveryX commands and export in __tasks\n                for (const [name, value] of Object.entries(exportDestination)) {\n                    if (name.startsWith(\"runEvery\") && typeof value === 'function') {\n                        __tasks.schedule[name] = {\n                            name: name,\n                            type: 'schedule',\n                            exec: __bindMeta(value)\n                        }\n                    }\n                }\n\n                // gather all jobs\n                if (typeof exportDestination['jobs'] === 'object') {\n                    for (const [key, value] of Object.entries(exportDestination['jobs'])) {\n                        __tasks.job[key] = {\n                            name: key,\n                            type: 'job',\n                            exec: __bindMeta(value)\n                        }\n                    }\n                }\n\n\n            }\n\n            ${responseVar} = { methods: __methods, tasks: __tasks, meta: __pluginMeta, }\n        })\n    `)(asyncGuard)\n\n    const vmResponse = vm.run(responseVar)\n    const { methods, tasks } = vmResponse\n    const exportEventsExists = !!methods.exportEvents\n\n    if (exportEventsExists) {\n        upgradeExportEvents(hub, pluginConfig, vmResponse)\n        statsdTiming('vm_setup_sync_section')\n\n        if (hub.HISTORICAL_EXPORTS_ENABLED) {\n            addHistoricalEventsExportCapability(hub, pluginConfig, vmResponse)\n            addHistoricalEventsExportCapabilityV2(hub, pluginConfig, vmResponse)\n        }\n    } else {\n        statsdTiming('vm_setup_sync_section')\n    }\n\n    statsdTiming('vm_setup_full')\n\n    return {\n        vm,\n        methods,\n        tasks,\n        vmResponseVariable: responseVar,\n    }\n}\n", "import { PluginEvent } from '@posthog/plugin-scaffold'\nimport { DateTime } from 'luxon'\nimport fetch from 'node-fetch'\n\nimport { Hook, Hub } from '../../../../src/types'\nimport { createHub } from '../../../../src/utils/db/hub'\nimport { PostgresUse } from '../../../../src/utils/db/postgres'\nimport { convertToIngestionEvent } from '../../../../src/utils/event'\nimport { UUIDT } from '../../../../src/utils/utils'\nimport { ActionManager } from '../../../../src/worker/ingestion/action-manager'\nimport { ActionMatcher } from '../../../../src/worker/ingestion/action-matcher'\nimport {\n    processOnEventStep,\n    processWebhooksStep,\n} from '../../../../src/worker/ingestion/event-pipeline/runAsyncHandlersStep'\nimport { EventPipelineRunner } from '../../../../src/worker/ingestion/event-pipeline/runner'\nimport { HookCommander } from '../../../../src/worker/ingestion/hooks'\nimport { setupPlugins } from '../../../../src/worker/plugins/setup'\nimport { delayUntilEventIngested, resetTestDatabaseClickhouse } from '../../../helpers/clickhouse'\nimport { commonUserId } from '../../../helpers/plugins'\nimport { insertRow, resetTestDatabase } from '../../../helpers/sql'\n\njest.mock('../../../../src/utils/status')\n\ndescribe('Event Pipeline integration test', () => {\n    let hub: Hub\n    let actionManager: ActionManager\n    let actionMatcher: ActionMatcher\n    let hookCannon: HookCommander\n    let closeServer: () => Promise<void>\n\n    const ingestEvent = async (event: PluginEvent) => {\n        const runner = new EventPipelineRunner(hub, event)\n        const result = await runner.runEventPipeline(event)\n        const postIngestionEvent = convertToIngestionEvent(result.args[0])\n        return Promise.all([\n            processOnEventStep(runner.hub, postIngestionEvent),\n            processWebhooksStep(postIngestionEvent, actionMatcher, hookCannon),\n        ])\n    }\n\n    beforeEach(async () => {\n        await resetTestDatabase()\n        await resetTestDatabaseClickhouse()\n        process.env.SITE_URL = 'https://example.com'\n        ;[hub, closeServer] = await createHub()\n\n        actionManager = new ActionManager(hub.db.postgres)\n        await actionManager.prepare()\n        actionMatcher = new ActionMatcher(hub.db.postgres, actionManager)\n        hookCannon = new HookCommander(\n            hub.db.postgres,\n            hub.teamManager,\n            hub.organizationManager,\n            new Set(hub.FETCH_HOSTNAME_GUARD_TEAMS.split(',').filter(String).map(Number)),\n            hub.appMetrics,\n            undefined,\n            hub.EXTERNAL_REQUEST_TIMEOUT_MS\n        )\n\n        jest.spyOn(hub.db, 'fetchPerson')\n        jest.spyOn(hub.db, 'createPerson')\n    })\n\n    afterEach(async () => {\n        await closeServer()\n    })\n\n    it('handles plugins setting properties', async () => {\n        await resetTestDatabase(`\n            function processEvent (event) {\n                event.properties = {\n                    ...event.properties,\n                    $browser: 'Chrome',\n                    processed: 'hell yes'\n                }\n                event.$set = {\n                    ...event.$set,\n                    personProp: 'value'\n                }\n                return event\n            }\n        `)\n        await setupPlugins(hub)\n\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            $set: { personProp: 1, anotherValue: 2 },\n            timestamp: new Date().toISOString(),\n            now: new Date().toISOString(),\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'https://example.com',\n            uuid: new UUIDT().toString(),\n        }\n\n        await ingestEvent(event)\n\n        const events = await delayUntilEventIngested(() => hub.db.fetchEvents())\n        const persons = await delayUntilEventIngested(() => hub.db.fetchPersons())\n\n        expect(events.length).toEqual(1)\n        expect(events[0]).toEqual(\n            expect.objectContaining({\n                uuid: event.uuid,\n                event: 'xyz',\n                team_id: 2,\n                timestamp: DateTime.fromISO(event.timestamp!, { zone: 'utc' }),\n                // :KLUDGE: Ignore properties like $plugins_succeeded, etc\n                properties: expect.objectContaining({\n                    foo: 'bar',\n                    $browser: 'Chrome',\n                    processed: 'hell yes',\n                    $set: {\n                        personProp: 'value',\n                        anotherValue: 2,\n                        $browser: 'Chrome',\n                    },\n                    $set_once: {\n                        $initial_browser: 'Chrome',\n                    },\n                }),\n            })\n        )\n\n        expect(persons.length).toEqual(1)\n        expect(persons[0].version).toEqual(0)\n        expect(persons[0].properties).toEqual({\n            $creator_event_uuid: event.uuid,\n            $initial_browser: 'Chrome',\n            $browser: 'Chrome',\n            personProp: 'value',\n            anotherValue: 2,\n        })\n    })\n\n    it('fires a webhook', async () => {\n        await hub.db.postgres.query(\n            PostgresUse.COMMON_WRITE,\n            `UPDATE posthog_team SET slack_incoming_webhook = 'https://webhook.example.com/'`,\n            [],\n            'testTag'\n        )\n\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            timestamp: new Date().toISOString(),\n            now: new Date().toISOString(),\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'not-used-anymore',\n            uuid: new UUIDT().toString(),\n        }\n        await actionManager.reloadAllActions()\n\n        await ingestEvent(event)\n\n        const expectedPayload = {\n            text: '[Test Action](https://example.com/action/69) was triggered by [abc](https://example.com/person/abc)',\n        }\n\n        expect(fetch).toHaveBeenCalledWith('https://webhook.example.com/', {\n            body: JSON.stringify(expectedPayload, undefined, 4),\n            headers: { 'Content-Type': 'application/json' },\n            method: 'POST',\n            timeout: 10000,\n        })\n    })\n\n    it('fires a REST hook', async () => {\n        const timestamp = new Date().toISOString()\n\n        await hub.db.postgres.query(\n            PostgresUse.COMMON_WRITE,\n            `UPDATE posthog_organization\n             SET available_features = '{\"zapier\"}'`,\n            [],\n            'testTag'\n        )\n        await insertRow(hub.db.postgres, 'ee_hook', {\n            id: 'abc',\n            team_id: 2,\n            user_id: commonUserId,\n            resource_id: 69,\n            event: 'action_performed',\n            target: 'https://example.com/',\n            created: timestamp,\n            updated: timestamp,\n        } as Hook)\n\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            timestamp: timestamp,\n            now: timestamp,\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'https://example.com',\n            uuid: new UUIDT().toString(),\n        }\n        await actionManager.reloadAllActions()\n\n        await ingestEvent(event)\n\n        const expectedPayload = {\n            hook: {\n                id: 'abc',\n                event: 'action_performed',\n                target: 'https://example.com/',\n            },\n            data: {\n                event: 'xyz',\n                properties: {\n                    foo: 'bar',\n                },\n                eventUuid: expect.any(String),\n                timestamp,\n                teamId: 2,\n                distinctId: 'abc',\n                elementsList: [],\n                person: {\n                    created_at: expect.any(String),\n                    properties: {\n                        $creator_event_uuid: event.uuid,\n                    },\n                    uuid: expect.any(String),\n                },\n            },\n        }\n\n        // Using a more verbose way instead of toHaveBeenCalledWith because we need to parse request body\n        // and use expect.any for a few payload properties, which wouldn't be possible in a simpler way\n        expect(jest.mocked(fetch).mock.calls[0][0]).toBe('https://example.com/')\n        const secondArg = jest.mocked(fetch).mock.calls[0][1]\n        expect(JSON.parse(secondArg!.body as unknown as string)).toStrictEqual(expectedPayload)\n        expect(JSON.parse(secondArg!.body as unknown as string)).toStrictEqual(expectedPayload)\n        expect(secondArg!.headers).toStrictEqual({ 'Content-Type': 'application/json' })\n        expect(secondArg!.method).toBe('POST')\n    })\n\n    it('single postgres action per run to create or load person', async () => {\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            timestamp: new Date().toISOString(),\n            now: new Date().toISOString(),\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'https://example.com',\n            uuid: new UUIDT().toString(),\n        }\n\n        await new EventPipelineRunner(hub, event).runEventPipeline(event)\n\n        expect(hub.db.fetchPerson).toHaveBeenCalledTimes(1) // we query before creating\n        expect(hub.db.createPerson).toHaveBeenCalledTimes(1)\n\n        // second time single fetch\n        await new EventPipelineRunner(hub, event).runEventPipeline(event)\n        expect(hub.db.fetchPerson).toHaveBeenCalledTimes(2)\n    })\n})\n", "import { DateTime } from 'luxon'\nimport fetch, { FetchError } from 'node-fetch'\n\nimport { Action, PostIngestionEvent, Team } from '../../../src/types'\nimport { isCloud } from '../../../src/utils/env-utils'\nimport { UUIDT } from '../../../src/utils/utils'\nimport { AppMetrics } from '../../../src/worker/ingestion/app-metrics'\nimport {\n    determineWebhookType,\n    getActionDetails,\n    getFormattedMessage,\n    getPersonDetails,\n    getTokens,\n    getValueOfToken,\n    HookCommander,\n    WebhookType,\n} from '../../../src/worker/ingestion/hooks'\nimport { Hook } from './../../../src/types'\n\njest.mock('../../../src/utils/env-utils')\n\ndescribe('hooks', () => {\n    describe('determineWebhookType', () => {\n        test('Slack', () => {\n            const webhookType = determineWebhookType('https://hooks.slack.com/services/')\n\n            expect(webhookType).toBe(WebhookType.Slack)\n        })\n\n        test('Discord', () => {\n            const webhookType = determineWebhookType('https://discord.com/api/webhooks/')\n\n            expect(webhookType).toBe(WebhookType.Discord)\n        })\n\n        test('Teams', () => {\n            const webhookType = determineWebhookType('https://outlook.office.com/webhook/')\n\n            expect(webhookType).toBe(WebhookType.Teams)\n        })\n    })\n\n    describe('getPersonDetails', () => {\n        const event = {\n            distinctId: 'WALL-E',\n            person_properties: { email: 'test@posthog.com' },\n        } as unknown as PostIngestionEvent\n        const team = { person_display_name_properties: null } as Team\n\n        test('Slack', () => {\n            const [userDetails, userDetailsMarkdown] = getPersonDetails(\n                event,\n                'http://localhost:8000',\n                WebhookType.Slack,\n                team\n            )\n\n            expect(userDetails).toBe('test@posthog.com')\n            expect(userDetailsMarkdown).toBe('<http://localhost:8000/person/WALL-E|test@posthog.com>')\n        })\n\n        test('Teams', () => {\n            const [userDetails, userDetailsMarkdown] = getPersonDetails(\n                event,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                team\n            )\n\n            expect(userDetails).toBe('test@posthog.com')\n            expect(userDetailsMarkdown).toBe('[test@posthog.com](http://localhost:8000/person/WALL-E)')\n        })\n    })\n\n    describe('getActionDetails', () => {\n        const action = { id: 1, name: 'action1' } as Action\n\n        test('Slack', () => {\n            const [actionDetails, actionDetailsMarkdown] = getActionDetails(\n                action,\n                'http://localhost:8000',\n                WebhookType.Slack\n            )\n\n            expect(actionDetails).toBe('action1')\n            expect(actionDetailsMarkdown).toBe('<http://localhost:8000/action/1|action1>')\n        })\n\n        test('Teams', () => {\n            const [actionDetails, actionDetailsMarkdown] = getActionDetails(\n                action,\n                'http://localhost:8000',\n                WebhookType.Teams\n            )\n\n            expect(actionDetails).toBe('action1')\n            expect(actionDetailsMarkdown).toBe('[action1](http://localhost:8000/action/1)')\n        })\n    })\n\n    describe('getTokens', () => {\n        test('works', () => {\n            const format = '[action.name] got done by [user.name]'\n\n            const [matchedTokens, tokenisedMessage] = getTokens(format)\n\n            expect(matchedTokens).toStrictEqual(['action.name', 'user.name'])\n            expect(tokenisedMessage).toBe('%s got done by %s')\n        })\n\n        test('allows escaping brackets', () => {\n            const format = '[action.name\\\\] got done by \\\\[user.name\\\\]' // just one of the brackets has to be escaped\n\n            const [matchedTokens, tokenisedMessage] = getTokens(format)\n\n            expect(matchedTokens).toStrictEqual([])\n            expect(tokenisedMessage).toBe('[action.name] got done by [user.name]')\n        })\n    })\n\n    describe('getValueOfToken', () => {\n        const action = { id: 1, name: 'action1' } as Action\n        const event = {\n            eventUuid: '123',\n            event: '$pageview',\n            distinctId: 'WALL-E',\n            properties: { $browser: 'Chrome' },\n            person_properties: { enjoys_broccoli_on_pizza: false },\n            timestamp: '2021-10-31T00:44:00.000Z',\n        } as unknown as PostIngestionEvent\n        const team = { person_display_name_properties: null } as Team\n\n        test('event', () => {\n            const tokenUserName = ['event']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('$pageview')\n            expect(markdown).toBe('[$pageview](http://localhost:8000/events/123/2021-10-31T00%3A44%3A00.000Z)')\n        })\n\n        test('event UUID', () => {\n            const tokenUserName = ['event', 'uuid']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('123')\n            expect(markdown).toBe('123')\n        })\n\n        test('event name', () => {\n            const tokenUserName = ['event', 'name']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('$pageview')\n            expect(markdown).toBe('$pageview')\n        })\n\n        test('event event', () => {\n            const tokenUserName = ['event', 'event']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('$pageview')\n            expect(markdown).toBe('$pageview')\n        })\n\n        test('event distinct_id', () => {\n            const tokenUserName = ['event', 'distinct_id']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('WALL-E')\n            expect(markdown).toBe('WALL-E')\n        })\n\n        test('person with just distinct ID', () => {\n            const tokenUserName = ['person']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('WALL-E')\n            expect(markdown).toBe('[WALL-E](http://localhost:8000/person/WALL-E)')\n        })\n\n        test('person with email', () => {\n            const tokenUserName = ['person']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                { ...event, person_properties: { ...event.person_properties, email: 'wall-e@buynlarge.com' } },\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('wall-e@buynlarge.com')\n            expect(markdown).toBe('[wall-e@buynlarge.com](http://localhost:8000/person/WALL-E)')\n        })\n\n        test('person with custom name property, team-level setting ', () => {\n            const tokenUserName = ['person']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                {\n                    ...event,\n                    person_properties: {\n                        ...event.person_properties,\n                        imi\u0119: 'Grzegorz',\n                        nazwisko: 'Brz\u0119czyszczykiewicz',\n                    },\n                    distinctId: 'fd',\n                },\n                { ...team, person_display_name_properties: ['nazwisko'] },\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('Brz\u0119czyszczykiewicz')\n            expect(markdown).toBe('[Brz\u0119czyszczykiewicz](http://localhost:8000/person/fd)')\n        })\n\n        test('person prop', () => {\n            const tokenUserPropString = ['person', 'properties', 'enjoys_broccoli_on_pizza']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('false')\n            expect(markdown).toBe('false')\n        })\n\n        test('person prop nested', () => {\n            const tokenUserPropString = ['person', 'properties', 'pizza_ingredient_scores', 'broccoli']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                {\n                    ...event,\n                    person_properties: {\n                        ...event.person_properties,\n                        pizza_ingredient_scores: { broccoli: 5, pineapple: 9, aubergine: 0 },\n                    },\n                },\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('5')\n            expect(markdown).toBe('5')\n        })\n\n        test('person prop non-primitive', () => {\n            const tokenUserPropString = ['person', 'properties', 'pizza_ingredient_ranking']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                {\n                    ...event,\n                    person_properties: {\n                        ...event.person_properties,\n                        pizza_ingredient_ranking: ['pineapple', 'broccoli', 'aubergine'],\n                    },\n                },\n                team,\n                'http://localhost:8000',\n                WebhookType.Slack,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('[\"pineapple\",\"broccoli\",\"aubergine\"]')\n            expect(markdown).toBe('[\"pineapple\",\"broccoli\",\"aubergine\"]')\n        })\n\n        test('user name (alias for person name)', () => {\n            const tokenUserName = ['user', 'name']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('WALL-E')\n            expect(markdown).toBe('[WALL-E](http://localhost:8000/person/WALL-E)')\n        })\n\n        test('user prop (actually event prop)', () => {\n            const tokenUserPropString = ['user', 'browser']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('Chrome')\n            expect(markdown).toBe('Chrome')\n        })\n\n        test('user prop but missing', () => {\n            const tokenUserPropMissing = ['user', 'missing_property']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropMissing\n            )\n\n            expect(text).toBe('undefined')\n            expect(markdown).toBe('undefined')\n        })\n\n        test('escapes slack', () => {\n            const [text, markdown] = getValueOfToken(\n                action,\n                { ...event, eventUuid: '**>)', event: 'text><new link' },\n                team,\n                'http://localhost:8000',\n                WebhookType.Slack,\n                ['event']\n            )\n\n            expect(text).toBe('text&gt;&lt;new link')\n            expect(markdown).toBe(\n                '<http://localhost:8000/events/**%3E)/2021-10-31T00%3A44%3A00.000Z|text&gt;&lt;new link>'\n            )\n        })\n\n        test('escapes teams', () => {\n            const [text, markdown] = getValueOfToken(\n                action,\n                { ...event, eventUuid: '**)', event: 'text](yes!), [new link' },\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                ['event']\n            )\n\n            expect(text).toBe('text\\\\]\\\\(yes\\\\!\\\\), \\\\[new link')\n            expect(markdown).toBe(\n                '[text\\\\]\\\\(yes\\\\!\\\\), \\\\[new link](http://localhost:8000/events/\\\\*\\\\*\\\\)/2021-10-31T00%3A44%3A00.000Z)'\n            )\n        })\n    })\n\n    describe('getFormattedMessage', () => {\n        const event = {\n            distinctId: '2',\n            properties: { $browser: 'Chrome', page_title: 'Pricing', 'with space': 'yes sir' },\n        } as unknown as PostIngestionEvent\n        const team = { person_display_name_properties: null } as Team\n\n        test('custom format', () => {\n            const action = {\n                id: 1,\n                name: 'action1',\n                slack_message_format:\n                    '[user.name] from [user.browser] on [event.properties.page_title] page with [event.properties.fruit], [event.properties.with space]',\n            } as Action\n\n            const [text, markdown] = getFormattedMessage(\n                action,\n                event,\n                team,\n                'https://localhost:8000',\n                WebhookType.Slack\n            )\n            expect(text).toBe('2 from Chrome on Pricing page with undefined, yes sir')\n            expect(markdown).toBe(\n                '<https://localhost:8000/person/2|2> from Chrome on Pricing page with undefined, yes sir'\n            )\n        })\n\n        test('default format', () => {\n            const action = { id: 1, name: 'action1', slack_message_format: '' } as Action\n\n            const [text, markdown] = getFormattedMessage(\n                action,\n                event,\n                team,\n                'https://localhost:8000',\n                WebhookType.Slack\n            )\n            expect(text).toBe('action1 was triggered by 2')\n            expect(markdown).toBe(\n                '<https://localhost:8000/action/1|action1> was triggered by <https://localhost:8000/person/2|2>'\n            )\n        })\n\n        test('not quite correct format', () => {\n            const action = {\n                id: 1,\n                name: 'action1',\n                slack_message_format: '[user.name] did thing from browser [user.brauzer]',\n            } as Action\n\n            const [text, markdown] = getFormattedMessage(\n                action,\n                event,\n                team,\n                'https://localhost:8000',\n                WebhookType.Slack\n            )\n            expect(text).toBe('2 did thing from browser undefined')\n            expect(markdown).toBe('<https://localhost:8000/person/2|2> did thing from browser undefined')\n        })\n    })\n\n    describe('postRestHook', () => {\n        let hookCommander: HookCommander\n        let hook: Hook\n\n        beforeEach(() => {\n            jest.mocked(isCloud).mockReturnValue(false) // Disable private IP guard\n            hook = {\n                id: 'id',\n                team_id: 1,\n                user_id: 1,\n                resource_id: 1,\n                event: 'foo',\n                target: 'https://example.com/',\n                created: new Date().toISOString(),\n                updated: new Date().toISOString(),\n            }\n            hookCommander = new HookCommander(\n                {} as any,\n                {} as any,\n                {} as any,\n                new Set([hook.team_id]), // Hostname guard enabled\n                { queueError: () => Promise.resolve(), queueMetric: () => Promise.resolve() } as unknown as AppMetrics,\n                undefined,\n                20000\n            )\n        })\n\n        test('person = undefined', async () => {\n            await hookCommander.postRestHook(hook, { event: 'foo' } as any)\n\n            expect(fetch).toHaveBeenCalledWith('https://example.com/', {\n                body: JSON.stringify(\n                    {\n                        hook: {\n                            id: 'id',\n                            event: 'foo',\n                            target: 'https://example.com/',\n                        },\n                        data: {\n                            event: 'foo',\n                            person: {}, // person becomes empty object if undefined\n                        },\n                    },\n                    undefined,\n                    4\n                ),\n                headers: { 'Content-Type': 'application/json' },\n                method: 'POST',\n                timeout: 20000,\n            })\n        })\n\n        test('person data from the event', async () => {\n            jest.mocked(isCloud).mockReturnValue(true) // Enable private IP guard, which example.com should pass\n\n            const now = new Date().toISOString()\n            const uuid = new UUIDT().toString()\n            await hookCommander.postRestHook(hook, {\n                event: 'foo',\n                teamId: hook.team_id,\n                person_id: uuid,\n                person_properties: { foo: 'bar' },\n                person_created_at: DateTime.fromISO(now).toUTC(),\n            } as any)\n            expect(fetch).toHaveBeenCalledWith('https://example.com/', {\n                body: JSON.stringify(\n                    {\n                        hook: {\n                            id: 'id',\n                            event: 'foo',\n                            target: 'https://example.com/',\n                        },\n                        data: {\n                            event: 'foo',\n                            teamId: hook.team_id,\n                            person: {\n                                uuid: uuid,\n                                properties: { foo: 'bar' },\n                                created_at: now,\n                            },\n                        },\n                    },\n                    undefined,\n                    4\n                ),\n                headers: { 'Content-Type': 'application/json' },\n                method: 'POST',\n                timeout: 20000,\n            })\n        })\n\n        test('private IP hook allowed on self-hosted', async () => {\n            await hookCommander.postRestHook({ ...hook, target: 'http://127.0.0.1' }, { event: 'foo' } as any)\n\n            expect(fetch).toHaveBeenCalledWith('http://127.0.0.1', expect.anything())\n        })\n\n        test('private IP hook forbidden on Cloud', async () => {\n            jest.mocked(isCloud).mockReturnValue(true)\n\n            await expect(\n                hookCommander.postRestHook({ ...hook, target: 'http://127.0.0.1' }, { event: 'foo' } as any)\n            ).rejects.toThrow(new FetchError('Internal hostname', 'posthog-host-guard'))\n        })\n    })\n})\n", "import json\nimport os\nimport secrets\nimport urllib.parse\nfrom base64 import b32encode\nfrom binascii import unhexlify\nfrom typing import Any, Optional, cast\nimport requests\nfrom django.conf import settings\nfrom django.contrib.auth import login, update_session_auth_hash\nfrom django.contrib.auth.password_validation import validate_password\nfrom django.core.exceptions import ValidationError\nfrom django.http import HttpResponse, JsonResponse\nfrom django.shortcuts import redirect\nfrom django.utils import timezone\nfrom django.views.decorators.http import require_http_methods\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom django_otp import login as otp_login\nfrom django_otp.util import random_hex\nfrom loginas.utils import is_impersonated_session\nfrom rest_framework import exceptions, mixins, permissions, serializers, viewsets\nfrom rest_framework.decorators import action\nfrom rest_framework.exceptions import NotFound\nfrom rest_framework.permissions import AllowAny\nfrom rest_framework.response import Response\nfrom rest_framework.throttling import UserRateThrottle\nfrom two_factor.forms import TOTPDeviceForm\nfrom two_factor.utils import default_device\n\nfrom posthog.api.decide import hostname_in_allowed_url_list\nfrom posthog.api.email_verification import EmailVerifier\nfrom posthog.api.organization import OrganizationSerializer\nfrom posthog.api.shared import OrganizationBasicSerializer, TeamBasicSerializer\nfrom posthog.api.utils import raise_if_user_provided_url_unsafe\nfrom posthog.auth import authenticate_secondarily\nfrom posthog.cloud_utils import is_cloud\nfrom posthog.email import is_email_available\nfrom posthog.event_usage import (\n    report_user_logged_in,\n    report_user_updated,\n    report_user_verified_email,\n)\nfrom posthog.models import Team, User, UserScenePersonalisation, Dashboard\nfrom posthog.models.organization import Organization\nfrom posthog.models.user import NOTIFICATION_DEFAULTS, Notifications\nfrom posthog.tasks import user_identify\nfrom posthog.tasks.email import send_email_change_emails\nfrom posthog.user_permissions import UserPermissions\nfrom posthog.utils import get_js_url\n\n\nclass UserAuthenticationThrottle(UserRateThrottle):\n    rate = \"5/minute\"\n\n    def allow_request(self, request, view):\n        # only throttle non-GET requests\n        if request.method == \"GET\":\n            return True\n        return super().allow_request(request, view)\n\n\nclass UserEmailVerificationThrottle(UserRateThrottle):\n    rate = \"6/day\"\n\n\nclass ScenePersonalisationBasicSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserScenePersonalisation\n        fields = [\"scene\", \"dashboard\"]\n\n\nclass UserSerializer(serializers.ModelSerializer):\n    has_password = serializers.SerializerMethodField()\n    is_impersonated = serializers.SerializerMethodField()\n    is_2fa_enabled = serializers.SerializerMethodField()\n    has_social_auth = serializers.SerializerMethodField()\n    team = TeamBasicSerializer(read_only=True)\n    organization = OrganizationSerializer(read_only=True)\n    organizations = OrganizationBasicSerializer(many=True, read_only=True)\n    set_current_organization = serializers.CharField(write_only=True, required=False)\n    set_current_team = serializers.CharField(write_only=True, required=False)\n    current_password = serializers.CharField(write_only=True, required=False)\n    notification_settings = serializers.DictField(required=False)\n    scene_personalisation = ScenePersonalisationBasicSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\n            \"date_joined\",\n            \"uuid\",\n            \"distinct_id\",\n            \"first_name\",\n            \"email\",\n            \"pending_email\",\n            \"email_opt_in\",\n            \"is_email_verified\",\n            \"pending_email\",\n            \"notification_settings\",\n            \"anonymize_data\",\n            \"toolbar_mode\",\n            \"has_password\",\n            \"is_staff\",\n            \"is_impersonated\",\n            \"team\",\n            \"organization\",\n            \"organizations\",\n            \"set_current_organization\",\n            \"set_current_team\",\n            \"password\",\n            \"current_password\",  # used when changing current password\n            \"events_column_config\",\n            \"is_2fa_enabled\",\n            \"has_social_auth\",\n            \"has_seen_product_intro_for\",\n            \"scene_personalisation\",\n        ]\n        extra_kwargs = {\n            \"date_joined\": {\"read_only\": True},\n            \"password\": {\"write_only\": True},\n        }\n\n    def get_has_password(self, instance: User) -> bool:\n        return instance.has_usable_password()\n\n    def get_is_impersonated(self, _) -> Optional[bool]:\n        if \"request\" not in self.context:\n            return None\n        return is_impersonated_session(self.context[\"request\"])\n\n    def get_has_social_auth(self, instance: User) -> bool:\n        return instance.social_auth.exists()  # type: ignore\n\n    def get_is_2fa_enabled(self, instance: User) -> bool:\n        return default_device(instance) is not None\n\n    def validate_set_current_organization(self, value: str) -> Organization:\n        try:\n            organization = Organization.objects.get(id=value)\n            if organization.memberships.filter(user=self.context[\"request\"].user).exists():\n                return organization\n        except Organization.DoesNotExist:\n            pass\n\n        raise serializers.ValidationError(f\"Object with id={value} does not exist.\", code=\"does_not_exist\")\n\n    def validate_set_current_team(self, value: str) -> Team:\n        try:\n            team = Team.objects.get(pk=value)\n            if self.context[\"request\"].user.teams.filter(pk=team.pk).exists():\n                return team\n        except Team.DoesNotExist:\n            pass\n\n        raise serializers.ValidationError(f\"Object with id={value} does not exist.\", code=\"does_not_exist\")\n\n    def validate_notification_settings(self, notification_settings: Notifications) -> Notifications:\n        for key, value in notification_settings.items():\n            if key not in Notifications.__annotations__:\n                raise serializers.ValidationError(f\"Key {key} is not valid as a key for notification settings\")\n\n            if not isinstance(value, Notifications.__annotations__[key]):\n                raise serializers.ValidationError(\n                    f\"{value} is not a valid type for notification settings, should be {Notifications.__annotations__[key]}\"\n                )\n        return {**NOTIFICATION_DEFAULTS, **notification_settings}  # type: ignore\n\n    def validate_password_change(\n        self, instance: User, current_password: Optional[str], password: Optional[str]\n    ) -> Optional[str]:\n        if password:\n            if instance.password and instance.has_usable_password():\n                # If user has a password set, we check it's provided to allow updating it. We need to check that is both\n                # usable (properly hashed) and that a password actually exists.\n                if not current_password:\n                    raise serializers.ValidationError(\n                        {\"current_password\": [\"This field is required when updating your password.\"]},\n                        code=\"required\",\n                    )\n\n                if not instance.check_password(current_password):\n                    raise serializers.ValidationError(\n                        {\"current_password\": [\"Your current password is incorrect.\"]},\n                        code=\"incorrect_password\",\n                    )\n            try:\n                validate_password(password, instance)\n            except ValidationError as e:\n                raise serializers.ValidationError({\"password\": e.messages})\n\n        return password\n\n    def validate_is_staff(self, value: bool) -> bool:\n        if not self.context[\"request\"].user.is_staff:\n            raise exceptions.PermissionDenied(\"You are not a staff user, contact your instance admin.\")\n        return value\n\n    def update(self, instance: \"User\", validated_data: Any) -> Any:\n        # Update current_organization and current_team\n        current_organization = validated_data.pop(\"set_current_organization\", None)\n        current_team = validated_data.pop(\"set_current_team\", None)\n        if current_organization:\n            if current_team and not current_organization.teams.filter(pk=current_team.pk).exists():\n                raise serializers.ValidationError(\n                    {\"set_current_team\": [\"Team must belong to the same organization in set_current_organization.\"]}\n                )\n\n            validated_data[\"current_organization\"] = current_organization\n            validated_data[\"current_team\"] = current_team if current_team else current_organization.teams.first()\n        elif current_team:\n            validated_data[\"current_team\"] = current_team\n            validated_data[\"current_organization\"] = current_team.organization\n\n        if (\n            \"email\" in validated_data\n            and validated_data[\"email\"].lower() != instance.email.lower()\n            and is_email_available()\n        ):\n            instance.pending_email = validated_data.pop(\"email\", None)\n            instance.save()\n            EmailVerifier.create_token_and_send_email_verification(instance)\n\n        # Update password\n        current_password = validated_data.pop(\"current_password\", None)\n        password = self.validate_password_change(\n            cast(User, instance), current_password, validated_data.pop(\"password\", None)\n        )\n\n        if validated_data.get(\"notification_settings\"):\n            validated_data[\"partial_notification_settings\"] = validated_data.pop(\"notification_settings\")\n\n        updated_attrs = list(validated_data.keys())\n        instance = cast(User, super().update(instance, validated_data))\n\n        if password:\n            instance.set_password(password)\n            instance.save()\n            update_session_auth_hash(self.context[\"request\"], instance)\n            updated_attrs.append(\"password\")\n\n        report_user_updated(instance, updated_attrs)\n\n        return instance\n\n    def to_representation(self, instance: Any) -> Any:\n        user_identify.identify_task.delay(user_id=instance.id)\n        return super().to_representation(instance)\n\n\nclass ScenePersonalisationSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserScenePersonalisation\n        fields = [\"scene\", \"dashboard\"]\n        read_only_fields = [\"user\", \"team\"]\n\n    def validate_dashboard(self, value: Dashboard) -> Dashboard:\n        instance = cast(User, self.instance)\n\n        if value.team != instance.current_team:\n            raise serializers.ValidationError(\"Dashboard must belong to the user's current team.\")\n\n        return value\n\n    def validate(self, data):\n        if \"dashboard\" not in data:\n            raise serializers.ValidationError(\"Dashboard must be provided.\")\n\n        if \"scene\" not in data:\n            raise serializers.ValidationError(\"Scene must be provided.\")\n\n        return data\n\n    def save(self, **kwargs):\n        instance = cast(User, self.instance)\n        if not instance:\n            # there must always be a user instance\n            raise NotFound()\n\n        validated_data = {**self.validated_data, **kwargs}\n\n        return UserScenePersonalisation.objects.update_or_create(\n            user=instance,\n            team=instance.current_team,\n            scene=validated_data[\"scene\"],\n            defaults={\"dashboard\": validated_data[\"dashboard\"]},\n        )\n\n\nclass UserViewSet(\n    mixins.RetrieveModelMixin,\n    mixins.UpdateModelMixin,\n    mixins.ListModelMixin,\n    viewsets.GenericViewSet,\n):\n    throttle_classes = [UserAuthenticationThrottle]\n    serializer_class = UserSerializer\n    permission_classes = [permissions.IsAuthenticated]\n    filter_backends = [DjangoFilterBackend]\n    filterset_fields = [\"is_staff\"]\n    queryset = User.objects.filter(is_active=True)\n    lookup_field = \"uuid\"\n\n    def get_object(self) -> User:\n        lookup_value = self.kwargs[self.lookup_field]\n        request_user = cast(User, self.request.user)  # Must be authenticated to access this endpoint\n        if lookup_value == \"@me\":\n            return request_user\n\n        if not request_user.is_staff:\n            raise exceptions.PermissionDenied(\n                \"As a non-staff user you're only allowed to access the `@me` user instance.\"\n            )\n\n        return super().get_object()\n\n    def get_queryset(self):\n        queryset = super().get_queryset()\n        if not self.request.user.is_staff:\n            queryset = queryset.filter(id=self.request.user.id)\n        return queryset\n\n    def get_serializer_context(self):\n        return {\n            **super().get_serializer_context(),\n            \"user_permissions\": UserPermissions(cast(User, self.request.user)),\n        }\n\n    @action(methods=[\"GET\"], detail=True)\n    def start_2fa_setup(self, request, **kwargs):\n        key = random_hex(20)\n        self.request.session[\"django_two_factor-hex\"] = key\n        rawkey = unhexlify(key.encode(\"ascii\"))\n        b32key = b32encode(rawkey).decode(\"utf-8\")\n        self.request.session[\"django_two_factor-qr_secret_key\"] = b32key\n        return Response({\"success\": True})\n\n    @action(methods=[\"POST\"], detail=True)\n    def validate_2fa(self, request, **kwargs):\n        form = TOTPDeviceForm(\n            request.session[\"django_two_factor-hex\"],\n            request.user,\n            data={\"token\": request.data[\"token\"]},\n        )\n        if not form.is_valid():\n            raise serializers.ValidationError(\"Token is not valid\", code=\"token_invalid\")\n        form.save()\n        otp_login(request, default_device(request.user))\n        return Response({\"success\": True})\n\n    @action(methods=[\"POST\"], detail=True, permission_classes=[AllowAny])\n    def verify_email(self, request, **kwargs):\n        token = request.data[\"token\"] if \"token\" in request.data else None\n        user_uuid = request.data[\"uuid\"]\n        if not token:\n            raise serializers.ValidationError({\"token\": [\"This field is required.\"]}, code=\"required\")\n\n        # Special handling for E2E tests\n        if settings.E2E_TESTING and user_uuid == \"e2e_test_user\" and token == \"e2e_test_token\":\n            return {\"success\": True, \"token\": token}\n\n        try:\n            user: Optional[User] = User.objects.filter(is_active=True).get(uuid=user_uuid)\n        except User.DoesNotExist:\n            user = None\n\n        if not user or not EmailVerifier.check_token(user, token):\n            raise serializers.ValidationError(\n                {\"token\": [\"This verification token is invalid or has expired.\"]},\n                code=\"invalid_token\",\n            )\n\n        if user.pending_email:\n            old_email = user.email\n            user.email = user.pending_email\n            user.pending_email = None\n            user.save()\n            send_email_change_emails.delay(timezone.now().isoformat(), user.first_name, old_email, user.email)\n\n        user.is_email_verified = True\n        user.save()\n        report_user_verified_email(user)\n\n        login(self.request, user, backend=\"django.contrib.auth.backends.ModelBackend\")\n        report_user_logged_in(user)\n        return Response({\"success\": True, \"token\": token})\n\n    @action(\n        methods=[\"POST\"],\n        detail=True,\n        permission_classes=[AllowAny],\n        throttle_classes=[UserEmailVerificationThrottle],\n    )\n    def request_email_verification(self, request, **kwargs):\n        uuid = request.data[\"uuid\"]\n        if not is_email_available():\n            raise serializers.ValidationError(\n                \"Cannot verify email address because email is not configured for your instance. Please contact your administrator.\",\n                code=\"email_not_available\",\n            )\n        try:\n            user = User.objects.filter(is_active=True).get(uuid=uuid)\n        except User.DoesNotExist:\n            user = None\n        if user:\n            EmailVerifier.create_token_and_send_email_verification(user)\n\n        return Response({\"success\": True})\n\n    @action(methods=[\"POST\"], detail=True)\n    def scene_personalisation(self, request, **kwargs):\n        instance = self.get_object()\n        request_serializer = ScenePersonalisationSerializer(instance=instance, data=request.data, partial=True)\n        request_serializer.is_valid(raise_exception=True)\n\n        request_serializer.save()\n        instance.refresh_from_db()\n\n        return Response(self.get_serializer(instance=instance).data)\n\n\n@authenticate_secondarily\ndef redirect_to_site(request):\n    team = request.user.team\n    app_url = request.GET.get(\"appUrl\") or (team.app_urls and team.app_urls[0])\n\n    if not app_url:\n        return HttpResponse(status=404)\n\n    if not team or not hostname_in_allowed_url_list(team.app_urls, urllib.parse.urlparse(app_url).hostname):\n        return HttpResponse(f\"Can only redirect to a permitted domain.\", status=403)\n    request.user.temporary_token = secrets.token_urlsafe(32)\n    request.user.save()\n    params = {\n        \"action\": \"ph_authorize\",\n        \"token\": team.api_token,\n        \"temporaryToken\": request.user.temporary_token,\n        \"actionId\": request.GET.get(\"actionId\"),\n        \"userIntent\": request.GET.get(\"userIntent\"),\n        \"toolbarVersion\": \"toolbar\",\n        \"apiURL\": request.build_absolute_uri(\"/\")[:-1],\n        \"dataAttributes\": team.data_attributes,\n    }\n\n    if get_js_url(request):\n        params[\"jsURL\"] = get_js_url(request)\n\n    if not settings.TEST and not os.environ.get(\"OPT_OUT_CAPTURE\"):\n        params[\"instrument\"] = True\n        params[\"userEmail\"] = request.user.email\n        params[\"distinctId\"] = request.user.distinct_id\n\n    # pass the empty string as the safe param so that `//` is encoded correctly.\n    # see https://github.com/PostHog/posthog/issues/9671\n    state = urllib.parse.quote(json.dumps(params), safe=\"\")\n\n    return redirect(\"{}#__posthog={}\".format(app_url, state))\n\n\n@require_http_methods([\"POST\"])\n@authenticate_secondarily\ndef test_slack_webhook(request):\n    \"\"\"Test webhook.\"\"\"\n    try:\n        body = json.loads(request.body)\n    except (TypeError, json.decoder.JSONDecodeError):\n        return JsonResponse({\"error\": \"Cannot parse request body\"}, status=400)\n\n    webhook = body.get(\"webhook\")\n\n    if not webhook:\n        return JsonResponse({\"error\": \"no webhook URL\"})\n    message = {\"text\": \"_Greetings_ from PostHog!\"}\n    try:\n        if is_cloud():  # Protect against SSRF\n            raise_if_user_provided_url_unsafe(webhook)\n        response = requests.post(webhook, verify=False, json=message)\n\n        if response.ok:\n            return JsonResponse({\"success\": True})\n        else:\n            return JsonResponse({\"error\": response.text})\n    except:\n        return JsonResponse({\"error\": \"invalid webhook URL\"})\n"], "fixing_code": ["#!/usr/bin/env bash\n\n# Script for running the functional tests in CI, outputting an Istambul coverage\n# report. When running the intetgration tests locally, it's probably better to\n# simply run `pnpm functional_tests` directly which will allow e.g. to watch for\n# changes. This script is intended to handle the complexities of spinning up the\n# plugin server with the appropriate environment vars setup, and ensuring we\n# bring down the server such that c8 produces the coverage report.\n# Context is this was originally written in the GitHub Actions workflow file,\n# but it's easier to debug in a script.\n\nset -e -o pipefail\n\nexport WORKER_CONCURRENCY=1\nexport CONVERSION_BUFFER_ENABLED=true\nexport BUFFER_CONVERSION_SECONDS=2 # Make sure we don't have to wait for the default 60 seconds\nexport KAFKA_MAX_MESSAGE_BATCH_SIZE=0\nexport APP_METRICS_GATHERED_FOR_ALL=true\nexport NODE_ENV=production-functional-tests\n\n# Not important at all, but I like to see nice red/green for tests\nexport FORCE_COLOR=true\n\nLOG_FILE=$(mktemp)\n\necho '::group::Starting plugin server'\n\nNODE_OPTIONS='--max_old_space_size=4096' ./node_modules/.bin/c8 --reporter html node dist/index.js >\"$LOG_FILE\" 2>&1 &\nSERVER_PID=$!\nSECONDS=0\n\nuntil curl http://localhost:6738/_ready; do\n    if ((SECONDS > 60)); then\n        echo 'Timed out waiting for plugin-server to be ready'\n        echo '::endgroup::'\n        echo '::group::Plugin Server logs'\n        cat \"$LOG_FILE\"\n        echo '::endgroup::'\n        exit 1\n    fi\n\n    echo ''\n    echo 'Waiting for plugin-server to be ready...'\n    sleep 1\ndone\n\necho ''\n\necho '::endgroup::'\n\nset +e\npnpm functional_tests --maxConcurrency=10 --verbose\nexit_code=$?\nset -e\n\nkill $SERVER_PID\nSECONDS=0\n\nwhile kill -0 $SERVER_PID; do\n    if ((SECONDS > 60)); then\n        echo 'Timed out waiting for plugin-server to exit'\n        break\n    fi\n\n    echo \"Waiting for plugin-server to exit, pid $SERVER_PID...\"\n    sleep 1\ndone\n\necho '::group::Plugin Server logs'\ncat \"$LOG_FILE\"\necho '::endgroup::'\n\nexit $exit_code\n", "import { LogLevel, PluginsServerConfig, stringToPluginServerMode, ValueMatcher } from '../types'\nimport { isDevEnv, isTestEnv, stringToBoolean } from '../utils/env-utils'\nimport { KAFKAJS_LOG_LEVEL_MAPPING } from './constants'\nimport {\n    KAFKA_EVENTS_JSON,\n    KAFKA_EVENTS_PLUGIN_INGESTION,\n    KAFKA_EVENTS_PLUGIN_INGESTION_OVERFLOW,\n} from './kafka-topics'\n\nexport const DEFAULT_HTTP_SERVER_PORT = 6738\n\nexport const defaultConfig = overrideWithEnv(getDefaultConfig())\n\nexport function getDefaultConfig(): PluginsServerConfig {\n    return {\n        DATABASE_URL: isTestEnv()\n            ? 'postgres://posthog:posthog@localhost:5432/test_posthog'\n            : isDevEnv()\n            ? 'postgres://posthog:posthog@localhost:5432/posthog'\n            : '',\n        DATABASE_READONLY_URL: '',\n        PLUGIN_STORAGE_DATABASE_URL: '',\n        POSTGRES_CONNECTION_POOL_SIZE: 10,\n        POSTHOG_DB_NAME: null,\n        POSTHOG_DB_USER: 'postgres',\n        POSTHOG_DB_PASSWORD: '',\n        POSTHOG_POSTGRES_HOST: 'localhost',\n        POSTHOG_POSTGRES_PORT: 5432,\n        CLICKHOUSE_HOST: 'localhost',\n        CLICKHOUSE_OFFLINE_CLUSTER_HOST: null,\n        CLICKHOUSE_DATABASE: isTestEnv() ? 'posthog_test' : 'default',\n        CLICKHOUSE_USER: 'default',\n        CLICKHOUSE_PASSWORD: null,\n        CLICKHOUSE_CA: null,\n        CLICKHOUSE_SECURE: false,\n        CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS: true,\n        EVENT_OVERFLOW_BUCKET_CAPACITY: 1000,\n        EVENT_OVERFLOW_BUCKET_REPLENISH_RATE: 1.0,\n        KAFKA_HOSTS: 'kafka:9092', // KEEP IN SYNC WITH posthog/settings/data_stores.py\n        KAFKA_CLIENT_CERT_B64: undefined,\n        KAFKA_CLIENT_CERT_KEY_B64: undefined,\n        KAFKA_TRUSTED_CERT_B64: undefined,\n        KAFKA_SECURITY_PROTOCOL: undefined,\n        KAFKA_SASL_MECHANISM: undefined,\n        KAFKA_SASL_USER: undefined,\n        KAFKA_SASL_PASSWORD: undefined,\n        KAFKA_CLIENT_RACK: undefined,\n        KAFKA_CONSUMPTION_MAX_BYTES: 10_485_760, // Default value for kafkajs\n        KAFKA_CONSUMPTION_MAX_BYTES_PER_PARTITION: 1_048_576, // Default value for kafkajs, must be bigger than message size\n        KAFKA_CONSUMPTION_MAX_WAIT_MS: 50, // Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages.\n        KAFKA_CONSUMPTION_ERROR_BACKOFF_MS: 100, // Timeout when a partition read fails (possibly because empty).\n        KAFKA_CONSUMPTION_BATCHING_TIMEOUT_MS: 500, // Timeout on reads from the prefetch buffer before running consumer loops\n        KAFKA_CONSUMPTION_TOPIC: KAFKA_EVENTS_PLUGIN_INGESTION,\n        KAFKA_CONSUMPTION_OVERFLOW_TOPIC: KAFKA_EVENTS_PLUGIN_INGESTION_OVERFLOW,\n        KAFKA_CONSUMPTION_REBALANCE_TIMEOUT_MS: null,\n        KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS: 30_000,\n        KAFKA_TOPIC_CREATION_TIMEOUT_MS: isDevEnv() ? 30_000 : 5_000, // rdkafka default is 5s, increased in devenv to resist to slow kafka\n        KAFKA_FLUSH_FREQUENCY_MS: isTestEnv() ? 5 : 500,\n        APP_METRICS_FLUSH_FREQUENCY_MS: isTestEnv() ? 5 : 20_000,\n        APP_METRICS_FLUSH_MAX_QUEUE_SIZE: isTestEnv() ? 5 : 1000,\n        KAFKA_PRODUCER_LINGER_MS: 20, // rdkafka default is 5ms\n        KAFKA_PRODUCER_BATCH_SIZE: 8 * 1024 * 1024, // rdkafka default is 1MiB\n        KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES: 100_000, // rdkafka default is 100_000\n        REDIS_URL: 'redis://127.0.0.1',\n        POSTHOG_REDIS_PASSWORD: '',\n        POSTHOG_REDIS_HOST: '',\n        POSTHOG_REDIS_PORT: 6379,\n        BASE_DIR: '.',\n        PLUGINS_RELOAD_PUBSUB_CHANNEL: 'reload-plugins',\n        WORKER_CONCURRENCY: 1,\n        TASK_TIMEOUT: 30,\n        TASKS_PER_WORKER: 10,\n        INGESTION_CONCURRENCY: 10,\n        INGESTION_BATCH_SIZE: 500,\n        LOG_LEVEL: isTestEnv() ? LogLevel.Warn : LogLevel.Info,\n        SENTRY_DSN: null,\n        SENTRY_PLUGIN_SERVER_TRACING_SAMPLE_RATE: 0,\n        SENTRY_PLUGIN_SERVER_PROFILING_SAMPLE_RATE: 0,\n        HTTP_SERVER_PORT: DEFAULT_HTTP_SERVER_PORT,\n        STATSD_HOST: null,\n        STATSD_PORT: 8125,\n        STATSD_PREFIX: 'plugin-server.',\n        SCHEDULE_LOCK_TTL: 60,\n        REDIS_POOL_MIN_SIZE: 1,\n        REDIS_POOL_MAX_SIZE: 3,\n        DISABLE_MMDB: isTestEnv(),\n        DISTINCT_ID_LRU_SIZE: 10000,\n        EVENT_PROPERTY_LRU_SIZE: 10000,\n        JOB_QUEUES: 'graphile',\n        JOB_QUEUE_GRAPHILE_URL: '',\n        JOB_QUEUE_GRAPHILE_SCHEMA: 'graphile_worker',\n        JOB_QUEUE_GRAPHILE_PREPARED_STATEMENTS: false,\n        JOB_QUEUE_GRAPHILE_CONCURRENCY: 1,\n        JOB_QUEUE_S3_AWS_ACCESS_KEY: '',\n        JOB_QUEUE_S3_AWS_SECRET_ACCESS_KEY: '',\n        JOB_QUEUE_S3_AWS_REGION: 'us-west-1',\n        JOB_QUEUE_S3_BUCKET_NAME: '',\n        JOB_QUEUE_S3_PREFIX: '',\n        CRASH_IF_NO_PERSISTENT_JOB_QUEUE: false,\n        HEALTHCHECK_MAX_STALE_SECONDS: 2 * 60 * 60, // 2 hours\n        PISCINA_USE_ATOMICS: true,\n        PISCINA_ATOMICS_TIMEOUT: 5000,\n        SITE_URL: null,\n        MAX_PENDING_PROMISES_PER_WORKER: 100,\n        KAFKA_PARTITIONS_CONSUMED_CONCURRENTLY: 1,\n        RECORDING_PARTITIONS_CONSUMED_CONCURRENTLY: 5,\n        CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS_TEAMS: '',\n        CLICKHOUSE_JSON_EVENTS_KAFKA_TOPIC: KAFKA_EVENTS_JSON,\n        CONVERSION_BUFFER_ENABLED: false,\n        CONVERSION_BUFFER_ENABLED_TEAMS: '',\n        CONVERSION_BUFFER_TOPIC_ENABLED_TEAMS: '',\n        BUFFER_CONVERSION_SECONDS: isDevEnv() ? 2 : 60, // KEEP IN SYNC WITH posthog/settings/ingestion.py\n        PERSON_INFO_CACHE_TTL: 5 * 60, // 5 min\n        KAFKA_HEALTHCHECK_SECONDS: 20,\n        OBJECT_STORAGE_ENABLED: true,\n        OBJECT_STORAGE_ENDPOINT: 'http://localhost:19000',\n        OBJECT_STORAGE_REGION: 'us-east-1',\n        OBJECT_STORAGE_ACCESS_KEY_ID: 'object_storage_root_user',\n        OBJECT_STORAGE_SECRET_ACCESS_KEY: 'object_storage_root_password',\n        OBJECT_STORAGE_BUCKET: 'posthog',\n        PLUGIN_SERVER_MODE: null,\n        PLUGIN_LOAD_SEQUENTIALLY: false,\n        KAFKAJS_LOG_LEVEL: 'WARN',\n        HISTORICAL_EXPORTS_ENABLED: true,\n        HISTORICAL_EXPORTS_MAX_RETRY_COUNT: 15,\n        HISTORICAL_EXPORTS_INITIAL_FETCH_TIME_WINDOW: 10 * 60 * 1000,\n        HISTORICAL_EXPORTS_FETCH_WINDOW_MULTIPLIER: 1.5,\n        APP_METRICS_GATHERED_FOR_ALL: isDevEnv() ? true : false,\n        MAX_TEAM_ID_TO_BUFFER_ANONYMOUS_EVENTS_FOR: 0,\n        USE_KAFKA_FOR_SCHEDULED_TASKS: true,\n        CLOUD_DEPLOYMENT: null,\n        EXTERNAL_REQUEST_TIMEOUT_MS: 10 * 1000, // 10 seconds\n        DROP_EVENTS_BY_TOKEN_DISTINCT_ID: '',\n        POE_EMBRACE_JOIN_FOR_TEAMS: '',\n        RELOAD_PLUGIN_JITTER_MAX_MS: 60000,\n\n        STARTUP_PROFILE_DURATION_SECONDS: 300, // 5 minutes\n        STARTUP_PROFILE_CPU: false,\n        STARTUP_PROFILE_HEAP: false,\n        STARTUP_PROFILE_HEAP_INTERVAL: 512 * 1024, // default v8 value\n        STARTUP_PROFILE_HEAP_DEPTH: 16, // default v8 value\n\n        SESSION_RECORDING_KAFKA_HOSTS: undefined,\n        SESSION_RECORDING_KAFKA_SECURITY_PROTOCOL: undefined,\n        SESSION_RECORDING_KAFKA_BATCH_SIZE: 500,\n        SESSION_RECORDING_KAFKA_QUEUE_SIZE: 1500,\n\n        SESSION_RECORDING_LOCAL_DIRECTORY: '.tmp/sessions',\n        // NOTE: 10 minutes\n        SESSION_RECORDING_MAX_BUFFER_AGE_SECONDS: 60 * 10,\n        SESSION_RECORDING_BUFFER_AGE_JITTER: 0.3,\n        SESSION_RECORDING_BUFFER_AGE_IN_MEMORY_MULTIPLIER: 1.2,\n        SESSION_RECORDING_MAX_BUFFER_SIZE_KB: 1024 * 50, // 50MB\n        SESSION_RECORDING_REMOTE_FOLDER: 'session_recordings',\n        SESSION_RECORDING_REDIS_PREFIX: '@posthog/replay/',\n        SESSION_RECORDING_PARTITION_REVOKE_OPTIMIZATION: false,\n        SESSION_RECORDING_PARALLEL_CONSUMPTION: false,\n        POSTHOG_SESSION_RECORDING_REDIS_HOST: undefined,\n        POSTHOG_SESSION_RECORDING_REDIS_PORT: undefined,\n        SESSION_RECORDING_CONSOLE_LOGS_INGESTION_ENABLED: true,\n    }\n}\n\nexport const sessionRecordingConsumerConfig = (config: PluginsServerConfig): PluginsServerConfig => {\n    // When running the blob consumer we override a bunch of settings to use the session recording ones if available\n    return {\n        ...config,\n        KAFKA_HOSTS: config.SESSION_RECORDING_KAFKA_HOSTS || config.KAFKA_HOSTS,\n        KAFKA_SECURITY_PROTOCOL: config.SESSION_RECORDING_KAFKA_SECURITY_PROTOCOL || config.KAFKA_SECURITY_PROTOCOL,\n        POSTHOG_REDIS_HOST: config.POSTHOG_SESSION_RECORDING_REDIS_HOST || config.POSTHOG_REDIS_HOST,\n        POSTHOG_REDIS_PORT: config.POSTHOG_SESSION_RECORDING_REDIS_PORT || config.POSTHOG_REDIS_PORT,\n    }\n}\n\nexport function overrideWithEnv(\n    config: PluginsServerConfig,\n    env: Record<string, string | undefined> = process.env\n): PluginsServerConfig {\n    const defaultConfig = getDefaultConfig() as any // to make typechecker happy to use defaultConfig[key]\n\n    const tmpConfig: any = { ...config }\n    for (const key of Object.keys(config)) {\n        if (typeof env[key] !== 'undefined') {\n            if (key == 'PLUGIN_SERVER_MODE') {\n                const mode = env[key]\n                if (mode == null || mode in stringToPluginServerMode) {\n                    tmpConfig[key] = env[key]\n                } else {\n                    throw Error(`Invalid PLUGIN_SERVER_MODE ${env[key]}`)\n                }\n            } else if (typeof defaultConfig[key] === 'number') {\n                tmpConfig[key] = env[key]?.indexOf('.') ? parseFloat(env[key]!) : parseInt(env[key]!)\n            } else if (typeof defaultConfig[key] === 'boolean') {\n                tmpConfig[key] = stringToBoolean(env[key])\n            } else {\n                tmpConfig[key] = env[key]\n            }\n        }\n    }\n    const newConfig: PluginsServerConfig = { ...tmpConfig }\n\n    if (!newConfig.DATABASE_URL && !newConfig.POSTHOG_DB_NAME) {\n        throw Error(\n            'You must specify either DATABASE_URL or the database options POSTHOG_DB_NAME, POSTHOG_DB_USER, POSTHOG_DB_PASSWORD, POSTHOG_POSTGRES_HOST, POSTHOG_POSTGRES_PORT!'\n        )\n    }\n\n    if (!newConfig.DATABASE_URL) {\n        const encodedUser = encodeURIComponent(newConfig.POSTHOG_DB_USER)\n        const encodedPassword = encodeURIComponent(newConfig.POSTHOG_DB_PASSWORD)\n        newConfig.DATABASE_URL = `postgres://${encodedUser}:${encodedPassword}@${newConfig.POSTHOG_POSTGRES_HOST}:${newConfig.POSTHOG_POSTGRES_PORT}/${newConfig.POSTHOG_DB_NAME}`\n    }\n\n    if (!newConfig.JOB_QUEUE_GRAPHILE_URL) {\n        newConfig.JOB_QUEUE_GRAPHILE_URL = newConfig.DATABASE_URL\n    }\n\n    if (!Object.keys(KAFKAJS_LOG_LEVEL_MAPPING).includes(newConfig.KAFKAJS_LOG_LEVEL)) {\n        throw Error(\n            `Invalid KAFKAJS_LOG_LEVEL ${newConfig.KAFKAJS_LOG_LEVEL}. Valid: ${Object.keys(\n                KAFKAJS_LOG_LEVEL_MAPPING\n            ).join(', ')}`\n        )\n    }\n    return newConfig\n}\n\nexport function buildIntegerMatcher(config: string | undefined, allowStar: boolean): ValueMatcher<number> {\n    // Builds a ValueMatcher on a comma-separated list of values.\n    // Optionally, supports a '*' value to match everything\n    if (!config || config.trim().length == 0) {\n        return () => false\n    } else if (allowStar && config === '*') {\n        return () => true\n    } else {\n        const values = new Set(\n            config\n                .split(',')\n                .map((n) => parseInt(n))\n                .filter((num) => !isNaN(num))\n        )\n        return (v: number) => {\n            return values.has(v)\n        }\n    }\n}\n", "import { StatsD } from 'hot-shots'\nimport { Consumer, Kafka } from 'kafkajs'\nimport * as schedule from 'node-schedule'\nimport { AppMetrics } from 'worker/ingestion/app-metrics'\n\nimport { KAFKA_EVENTS_JSON, prefix as KAFKA_PREFIX } from '../../config/kafka-topics'\nimport { Hub, PluginsServerConfig } from '../../types'\nimport { PostgresRouter } from '../../utils/db/postgres'\nimport { PubSub } from '../../utils/pubsub'\nimport { status } from '../../utils/status'\nimport { ActionManager } from '../../worker/ingestion/action-manager'\nimport { ActionMatcher } from '../../worker/ingestion/action-matcher'\nimport { HookCommander } from '../../worker/ingestion/hooks'\nimport { OrganizationManager } from '../../worker/ingestion/organization-manager'\nimport { TeamManager } from '../../worker/ingestion/team-manager'\nimport { eachBatchAppsOnEventHandlers } from './batch-processing/each-batch-onevent'\nimport { eachBatchWebhooksHandlers } from './batch-processing/each-batch-webhooks'\nimport { KafkaJSIngestionConsumer, setupEventHandlers } from './kafka-queue'\n\nexport const startAsyncOnEventHandlerConsumer = async ({\n    hub, // TODO: remove needing to pass in the whole hub and be more selective on dependency injection.\n}: {\n    hub: Hub\n}) => {\n    /*\n        Consumes analytics events from the Kafka topic `clickhouse_events_json`\n        and processes any onEvent plugin handlers configured for the team. This\n        also includes `exportEvents` handlers defined in plugins as these are\n        also handled via modifying `onEvent` to call `exportEvents`.\n\n        At the moment this is just a wrapper around `IngestionConsumer`. We may\n        want to further remove that abstraction in the future.\n    */\n    status.info('\ud83d\udd01', `Starting onEvent handler consumer`)\n\n    const queue = buildOnEventIngestionConsumer({ hub })\n\n    await queue.start()\n\n    schedule.scheduleJob('0 * * * * *', async () => {\n        await queue.emitConsumerGroupMetrics()\n    })\n\n    const isHealthy = makeHealthCheck(queue.consumer, queue.sessionTimeout)\n\n    return { queue, isHealthy: () => isHealthy() }\n}\n\nexport const startAsyncWebhooksHandlerConsumer = async ({\n    kafka, // TODO: remove needing to pass in the whole hub and be more selective on dependency injection.\n    postgres,\n    teamManager,\n    organizationManager,\n    statsd,\n    serverConfig,\n    appMetrics,\n}: {\n    kafka: Kafka\n    postgres: PostgresRouter\n    teamManager: TeamManager\n    organizationManager: OrganizationManager\n    statsd: StatsD | undefined\n    serverConfig: PluginsServerConfig\n    appMetrics: AppMetrics\n}) => {\n    /*\n        Consumes analytics events from the Kafka topic `clickhouse_events_json`\n        and processes any onEvent plugin handlers configured for the team. This\n        also includes `exportEvents` handlers defined in plugins as these are\n        also handled via modifying `onEvent` to call `exportEvents`.\n\n        At the moment this is just a wrapper around `IngestionConsumer`. We may\n        want to further remove that abstraction in the future.\n    */\n    status.info('\ud83d\udd01', `Starting webhooks handler consumer`)\n\n    const consumer = kafka.consumer({\n        // NOTE: This should never clash with the group ID specified for the kafka engine posthog/ee/clickhouse/sql/clickhouse.py\n        groupId: `${KAFKA_PREFIX}clickhouse-plugin-server-async-webhooks`,\n        sessionTimeout: serverConfig.KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS,\n        readUncommitted: false,\n    })\n    setupEventHandlers(consumer)\n\n    const actionManager = new ActionManager(postgres)\n    await actionManager.prepare()\n    const actionMatcher = new ActionMatcher(postgres, actionManager, statsd)\n    const hookCannon = new HookCommander(\n        postgres,\n        teamManager,\n        organizationManager,\n        appMetrics,\n        statsd,\n        serverConfig.EXTERNAL_REQUEST_TIMEOUT_MS\n    )\n    const concurrency = serverConfig.TASKS_PER_WORKER || 20\n\n    const pubSub = new PubSub(serverConfig, {\n        'reload-action': async (message) => {\n            const { actionId, teamId } = JSON.parse(message)\n            await actionManager.reloadAction(teamId, actionId)\n        },\n        'drop-action': (message) => {\n            const { actionId, teamId } = JSON.parse(message)\n            actionManager.dropAction(teamId, actionId)\n        },\n    })\n\n    await pubSub.start()\n\n    // every 5 minutes all ActionManager caches are reloaded for eventual consistency\n    schedule.scheduleJob('*/5 * * * *', async () => {\n        await actionManager.reloadAllActions()\n    })\n\n    await consumer.subscribe({ topic: KAFKA_EVENTS_JSON, fromBeginning: false })\n    await consumer.run({\n        eachBatch: (payload) => eachBatchWebhooksHandlers(payload, actionMatcher, hookCannon, statsd, concurrency),\n    })\n\n    const isHealthy = makeHealthCheck(consumer, serverConfig.KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS)\n\n    return {\n        stop: async () => {\n            try {\n                await consumer.stop()\n            } catch (e) {\n                status.error('\ud83d\udea8', 'Error stopping consumer', e)\n            }\n            try {\n                await consumer.disconnect()\n            } catch (e) {\n                status.error('\ud83d\udea8', 'Error disconnecting consumer', e)\n            }\n        },\n        isHealthy,\n    }\n}\n\nexport const buildOnEventIngestionConsumer = ({ hub }: { hub: Hub }) => {\n    return new KafkaJSIngestionConsumer(\n        hub,\n        KAFKA_EVENTS_JSON,\n        `${KAFKA_PREFIX}clickhouse-plugin-server-async-onevent`,\n        eachBatchAppsOnEventHandlers\n    )\n}\n\nexport function makeHealthCheck(consumer: Consumer, sessionTimeout: number) {\n    const { HEARTBEAT } = consumer.events\n    let lastHeartbeat: number = Date.now()\n    consumer.on(HEARTBEAT, ({ timestamp }) => (lastHeartbeat = timestamp))\n\n    const isHealthy = async () => {\n        // Consumer has heartbeat within the session timeout, so it is healthy.\n        const milliSecondsToLastHeartbeat = Date.now() - lastHeartbeat\n        if (milliSecondsToLastHeartbeat < sessionTimeout) {\n            status.info('\ud83d\udc4d', 'Consumer heartbeat is healthy', { milliSecondsToLastHeartbeat, sessionTimeout })\n            return true\n        }\n\n        // Consumer has not heartbeat, but maybe it's because the group is\n        // currently rebalancing.\n        try {\n            const { state } = await consumer.describeGroup()\n\n            status.info('\u2139\ufe0f', 'Consumer group state', { state })\n\n            return ['CompletingRebalance', 'PreparingRebalance'].includes(state)\n        } catch (error) {\n            status.error('\ud83d\udea8', 'Error checking consumer group state', { error })\n            return false\n        }\n    }\n    return isHealthy\n}\n", "import { ReaderModel } from '@maxmind/geoip2-node'\nimport ClickHouse from '@posthog/clickhouse'\nimport {\n    Element,\n    Meta,\n    PluginAttachment,\n    PluginConfigSchema,\n    PluginEvent,\n    PluginSettings,\n    ProcessedPluginEvent,\n    Properties,\n} from '@posthog/plugin-scaffold'\nimport { Pool as GenericPool } from 'generic-pool'\nimport { StatsD } from 'hot-shots'\nimport { Redis } from 'ioredis'\nimport { Kafka } from 'kafkajs'\nimport { DateTime } from 'luxon'\nimport { Job } from 'node-schedule'\nimport { VM } from 'vm2'\n\nimport { ObjectStorage } from './main/services/object_storage'\nimport { DB } from './utils/db/db'\nimport { KafkaProducerWrapper } from './utils/db/kafka-producer-wrapper'\nimport { PostgresRouter } from './utils/db/postgres'\nimport { UUID } from './utils/utils'\nimport { AppMetrics } from './worker/ingestion/app-metrics'\nimport { OrganizationManager } from './worker/ingestion/organization-manager'\nimport { EventsProcessor } from './worker/ingestion/process-event'\nimport { TeamManager } from './worker/ingestion/team-manager'\nimport { PluginsApiKeyManager } from './worker/vm/extensions/helpers/api-key-manager'\nimport { RootAccessManager } from './worker/vm/extensions/helpers/root-acess-manager'\nimport { LazyPluginVM } from './worker/vm/lazy'\nimport { PromiseManager } from './worker/vm/promise-manager'\n\nexport { Element } from '@posthog/plugin-scaffold' // Re-export Element from scaffolding, for backwards compat.\n\ntype Brand<K, T> = K & { __brand: T }\n\nexport enum LogLevel {\n    None = 'none',\n    Debug = 'debug',\n    Info = 'info',\n    Log = 'log',\n    Warn = 'warn',\n    Error = 'error',\n}\n\nexport const logLevelToNumber: Record<LogLevel, number> = {\n    [LogLevel.None]: 0,\n    [LogLevel.Debug]: 10,\n    [LogLevel.Info]: 20,\n    [LogLevel.Log]: 30,\n    [LogLevel.Warn]: 40,\n    [LogLevel.Error]: 50,\n}\n\nexport enum KafkaSecurityProtocol {\n    Plaintext = 'PLAINTEXT',\n    SaslPlaintext = 'SASL_PLAINTEXT',\n    Ssl = 'SSL',\n    SaslSsl = 'SASL_SSL',\n}\n\nexport enum KafkaSaslMechanism {\n    Plain = 'plain',\n    ScramSha256 = 'scram-sha-256',\n    ScramSha512 = 'scram-sha-512',\n}\n\nexport enum PluginServerMode {\n    ingestion = 'ingestion',\n    ingestion_overflow = 'ingestion-overflow',\n    ingestion_historical = 'ingestion-historical',\n    async_onevent = 'async-onevent',\n    async_webhooks = 'async-webhooks',\n    jobs = 'jobs',\n    scheduler = 'scheduler',\n    analytics_ingestion = 'analytics-ingestion',\n    recordings_blob_ingestion = 'recordings-blob-ingestion',\n}\n\nexport const stringToPluginServerMode = Object.fromEntries(\n    Object.entries(PluginServerMode).map(([key, value]) => [\n        value,\n        PluginServerMode[key as keyof typeof PluginServerMode],\n    ])\n) as Record<string, PluginServerMode>\n\nexport interface PluginsServerConfig {\n    WORKER_CONCURRENCY: number // number of concurrent worker threads\n    TASKS_PER_WORKER: number // number of parallel tasks per worker thread\n    INGESTION_CONCURRENCY: number // number of parallel event ingestion queues per batch\n    INGESTION_BATCH_SIZE: number // kafka consumer batch size\n    TASK_TIMEOUT: number // how many seconds until tasks are timed out\n    DATABASE_URL: string // Postgres database URL\n    DATABASE_READONLY_URL: string // Optional read-only replica to the main Postgres database\n    PLUGIN_STORAGE_DATABASE_URL: string // Optional read-write Postgres database for plugin storage\n    POSTGRES_CONNECTION_POOL_SIZE: number\n    POSTHOG_DB_NAME: string | null\n    POSTHOG_DB_USER: string\n    POSTHOG_DB_PASSWORD: string\n    POSTHOG_POSTGRES_HOST: string\n    POSTHOG_POSTGRES_PORT: number\n    CLICKHOUSE_HOST: string\n    CLICKHOUSE_OFFLINE_CLUSTER_HOST: string | null\n    CLICKHOUSE_DATABASE: string\n    CLICKHOUSE_USER: string\n    CLICKHOUSE_PASSWORD: string | null\n    CLICKHOUSE_CA: string | null // ClickHouse CA certs\n    CLICKHOUSE_SECURE: boolean // whether to secure ClickHouse connection\n    CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS: boolean // whether to disallow external schemas like protobuf for clickhouse kafka engine\n    CLICKHOUSE_DISABLE_EXTERNAL_SCHEMAS_TEAMS: string // (advanced) a comma separated list of teams to disable clickhouse external schemas for\n    CLICKHOUSE_JSON_EVENTS_KAFKA_TOPIC: string // (advanced) topic to send events to for clickhouse ingestion\n    REDIS_URL: string\n    POSTHOG_REDIS_PASSWORD: string\n    POSTHOG_REDIS_HOST: string\n    POSTHOG_REDIS_PORT: number\n    REDIS_POOL_MIN_SIZE: number // minimum number of Redis connections to use per thread\n    REDIS_POOL_MAX_SIZE: number // maximum number of Redis connections to use per thread\n    KAFKA_HOSTS: string // comma-delimited Kafka hosts\n    KAFKA_CLIENT_CERT_B64: string | undefined\n    KAFKA_CLIENT_CERT_KEY_B64: string | undefined\n    KAFKA_TRUSTED_CERT_B64: string | undefined\n    KAFKA_SECURITY_PROTOCOL: KafkaSecurityProtocol | undefined\n    KAFKA_SASL_MECHANISM: KafkaSaslMechanism | undefined\n    KAFKA_SASL_USER: string | undefined\n    KAFKA_SASL_PASSWORD: string | undefined\n    KAFKA_CLIENT_RACK: string | undefined\n    KAFKA_CONSUMPTION_MAX_BYTES: number\n    KAFKA_CONSUMPTION_MAX_BYTES_PER_PARTITION: number\n    KAFKA_CONSUMPTION_MAX_WAIT_MS: number // fetch.wait.max.ms rdkafka parameter\n    KAFKA_CONSUMPTION_ERROR_BACKOFF_MS: number // fetch.error.backoff.ms rdkafka parameter\n    KAFKA_CONSUMPTION_BATCHING_TIMEOUT_MS: number\n    KAFKA_CONSUMPTION_TOPIC: string | null\n    KAFKA_CONSUMPTION_OVERFLOW_TOPIC: string | null\n    KAFKA_CONSUMPTION_REBALANCE_TIMEOUT_MS: number | null\n    KAFKA_CONSUMPTION_SESSION_TIMEOUT_MS: number\n    KAFKA_TOPIC_CREATION_TIMEOUT_MS: number\n    KAFKA_PRODUCER_LINGER_MS: number // linger.ms rdkafka parameter\n    KAFKA_PRODUCER_BATCH_SIZE: number // batch.size rdkafka parameter\n    KAFKA_PRODUCER_QUEUE_BUFFERING_MAX_MESSAGES: number // queue.buffering.max.messages rdkafka parameter\n    KAFKA_FLUSH_FREQUENCY_MS: number\n    APP_METRICS_FLUSH_FREQUENCY_MS: number\n    APP_METRICS_FLUSH_MAX_QUEUE_SIZE: number\n    BASE_DIR: string // base path for resolving local plugins\n    PLUGINS_RELOAD_PUBSUB_CHANNEL: string // Redis channel for reload events'\n    LOG_LEVEL: LogLevel\n    SENTRY_DSN: string | null\n    SENTRY_PLUGIN_SERVER_TRACING_SAMPLE_RATE: number // Rate of tracing in plugin server (between 0 and 1)\n    SENTRY_PLUGIN_SERVER_PROFILING_SAMPLE_RATE: number // Rate of profiling in plugin server (between 0 and 1)\n    HTTP_SERVER_PORT: number\n    STATSD_HOST: string | null\n    STATSD_PORT: number\n    STATSD_PREFIX: string\n    SCHEDULE_LOCK_TTL: number // how many seconds to hold the lock for the schedule\n    DISABLE_MMDB: boolean // whether to disable fetching MaxMind database for IP location\n    DISTINCT_ID_LRU_SIZE: number\n    EVENT_PROPERTY_LRU_SIZE: number // size of the event property tracker's LRU cache (keyed by [team.id, event])\n    JOB_QUEUES: string // retry queue engine and fallback queues\n    JOB_QUEUE_GRAPHILE_URL: string // use a different postgres connection in the graphile worker\n    JOB_QUEUE_GRAPHILE_SCHEMA: string // the postgres schema that the graphile worker\n    JOB_QUEUE_GRAPHILE_PREPARED_STATEMENTS: boolean // enable this to increase job queue throughput if not using pgbouncer\n    JOB_QUEUE_GRAPHILE_CONCURRENCY: number // concurrent jobs per pod\n    JOB_QUEUE_S3_AWS_ACCESS_KEY: string\n    JOB_QUEUE_S3_AWS_SECRET_ACCESS_KEY: string\n    JOB_QUEUE_S3_AWS_REGION: string\n    JOB_QUEUE_S3_BUCKET_NAME: string\n    JOB_QUEUE_S3_PREFIX: string // S3 filename prefix for the S3 job queue\n    CRASH_IF_NO_PERSISTENT_JOB_QUEUE: boolean // refuse to start unless there is a properly configured persistent job queue (e.g. graphile)\n    HEALTHCHECK_MAX_STALE_SECONDS: number // maximum number of seconds the plugin server can go without ingesting events before the healthcheck fails\n    PISCINA_USE_ATOMICS: boolean // corresponds to the piscina useAtomics config option (https://github.com/piscinajs/piscina#constructor-new-piscinaoptions)\n    PISCINA_ATOMICS_TIMEOUT: number // (advanced) corresponds to the length of time a piscina worker should block for when looking for tasks\n    SITE_URL: string | null\n    MAX_PENDING_PROMISES_PER_WORKER: number // (advanced) maximum number of promises that a worker can have running at once in the background. currently only targets the exportEvents buffer.\n    KAFKA_PARTITIONS_CONSUMED_CONCURRENTLY: number // (advanced) how many kafka partitions the plugin server should consume from concurrently\n    RECORDING_PARTITIONS_CONSUMED_CONCURRENTLY: number\n    CONVERSION_BUFFER_ENABLED: boolean\n    CONVERSION_BUFFER_ENABLED_TEAMS: string\n    CONVERSION_BUFFER_TOPIC_ENABLED_TEAMS: string\n    BUFFER_CONVERSION_SECONDS: number\n    PERSON_INFO_CACHE_TTL: number\n    KAFKA_HEALTHCHECK_SECONDS: number\n    OBJECT_STORAGE_ENABLED: boolean // Disables or enables the use of object storage. It will become mandatory to use object storage\n    OBJECT_STORAGE_REGION: string // s3 region\n    OBJECT_STORAGE_ENDPOINT: string // s3 endpoint\n    OBJECT_STORAGE_ACCESS_KEY_ID: string\n    OBJECT_STORAGE_SECRET_ACCESS_KEY: string\n    OBJECT_STORAGE_BUCKET: string // the object storage bucket name\n    PLUGIN_SERVER_MODE: PluginServerMode | null\n    PLUGIN_LOAD_SEQUENTIALLY: boolean // could help with reducing memory usage spikes on startup\n    KAFKAJS_LOG_LEVEL: 'NOTHING' | 'DEBUG' | 'INFO' | 'WARN' | 'ERROR'\n    HISTORICAL_EXPORTS_ENABLED: boolean // enables historical exports for export apps\n    HISTORICAL_EXPORTS_MAX_RETRY_COUNT: number\n    HISTORICAL_EXPORTS_INITIAL_FETCH_TIME_WINDOW: number\n    HISTORICAL_EXPORTS_FETCH_WINDOW_MULTIPLIER: number\n    APP_METRICS_GATHERED_FOR_ALL: boolean // whether to gather app metrics for all teams\n    MAX_TEAM_ID_TO_BUFFER_ANONYMOUS_EVENTS_FOR: number\n    USE_KAFKA_FOR_SCHEDULED_TASKS: boolean // distribute scheduled tasks across the scheduler workers\n    EVENT_OVERFLOW_BUCKET_CAPACITY: number\n    EVENT_OVERFLOW_BUCKET_REPLENISH_RATE: number\n    /** Label of the PostHog Cloud environment. Null if not running PostHog Cloud. @example 'US' */\n    CLOUD_DEPLOYMENT: string | null\n    EXTERNAL_REQUEST_TIMEOUT_MS: number\n    DROP_EVENTS_BY_TOKEN_DISTINCT_ID: string\n    POE_EMBRACE_JOIN_FOR_TEAMS: string\n    RELOAD_PLUGIN_JITTER_MAX_MS: number\n\n    // dump profiles to disk, covering the first N seconds of runtime\n    STARTUP_PROFILE_DURATION_SECONDS: number\n    STARTUP_PROFILE_CPU: boolean\n    STARTUP_PROFILE_HEAP: boolean\n    STARTUP_PROFILE_HEAP_INTERVAL: number\n    STARTUP_PROFILE_HEAP_DEPTH: number\n\n    // local directory might be a volume mount or a directory on disk (e.g. in local dev)\n    SESSION_RECORDING_LOCAL_DIRECTORY: string\n    SESSION_RECORDING_MAX_BUFFER_AGE_SECONDS: number\n    SESSION_RECORDING_MAX_BUFFER_SIZE_KB: number\n    SESSION_RECORDING_BUFFER_AGE_IN_MEMORY_MULTIPLIER: number\n    SESSION_RECORDING_BUFFER_AGE_JITTER: number\n    SESSION_RECORDING_REMOTE_FOLDER: string\n    SESSION_RECORDING_REDIS_PREFIX: string\n    SESSION_RECORDING_PARTITION_REVOKE_OPTIMIZATION: boolean\n    SESSION_RECORDING_PARALLEL_CONSUMPTION: boolean\n    SESSION_RECORDING_CONSOLE_LOGS_INGESTION_ENABLED: boolean\n\n    // Dedicated infra values\n    SESSION_RECORDING_KAFKA_HOSTS: string | undefined\n    SESSION_RECORDING_KAFKA_SECURITY_PROTOCOL: KafkaSecurityProtocol | undefined\n    SESSION_RECORDING_KAFKA_BATCH_SIZE: number\n    SESSION_RECORDING_KAFKA_QUEUE_SIZE: number\n\n    POSTHOG_SESSION_RECORDING_REDIS_HOST: string | undefined\n    POSTHOG_SESSION_RECORDING_REDIS_PORT: number | undefined\n}\n\nexport interface Hub extends PluginsServerConfig {\n    instanceId: UUID\n    // what tasks this server will tackle - e.g. ingestion, scheduled plugins or others.\n    capabilities: PluginServerCapabilities\n    // active connections to Postgres, Redis, ClickHouse, Kafka, StatsD\n    db: DB\n    postgres: PostgresRouter\n    redisPool: GenericPool<Redis>\n    clickhouse: ClickHouse\n    kafka: Kafka\n    kafkaProducer: KafkaProducerWrapper\n    objectStorage: ObjectStorage\n    // metrics\n    statsd?: StatsD\n    pluginMetricsJob: Job | undefined\n    // currently enabled plugin status\n    plugins: Map<PluginId, Plugin>\n    pluginConfigs: Map<PluginConfigId, PluginConfig>\n    pluginConfigsPerTeam: Map<TeamId, PluginConfig[]>\n    pluginSchedule: Record<string, PluginConfigId[]> | null\n    // unique hash for each plugin config; used to verify IDs caught on stack traces for unhandled promise rejections\n    pluginConfigSecrets: Map<PluginConfigId, string>\n    pluginConfigSecretLookup: Map<string, PluginConfigId>\n    // tools\n    teamManager: TeamManager\n    organizationManager: OrganizationManager\n    pluginsApiKeyManager: PluginsApiKeyManager\n    rootAccessManager: RootAccessManager\n    promiseManager: PromiseManager\n    eventsProcessor: EventsProcessor\n    appMetrics: AppMetrics\n    // geoip database, setup in workers\n    mmdb?: ReaderModel\n    // diagnostics\n    lastActivity: number\n    lastActivityType: string\n    statelessVms: StatelessVmMap\n    conversionBufferEnabledTeams: Set<number>\n    // functions\n    enqueuePluginJob: (job: EnqueuedPluginJob) => Promise<void>\n    // ValueMatchers used for various opt-in/out features\n    pluginConfigsToSkipElementsParsing: ValueMatcher<number>\n    poeEmbraceJoinForTeams: ValueMatcher<number>\n    // lookups\n    eventsToDropByToken: Map<string, string[]>\n}\n\nexport interface PluginServerCapabilities {\n    // Warning: when adding more entries, make sure to update worker/vm/capabilities.ts\n    // and the shouldSetupPluginInServer() test accordingly.\n    ingestion?: boolean\n    ingestionOverflow?: boolean\n    ingestionHistorical?: boolean\n    pluginScheduledTasks?: boolean\n    processPluginJobs?: boolean\n    processAsyncOnEventHandlers?: boolean\n    processAsyncWebhooksHandlers?: boolean\n    sessionRecordingBlobIngestion?: boolean\n    transpileFrontendApps?: boolean // TODO: move this away from pod startup, into a graphile job\n    preflightSchedules?: boolean // Used for instance health checks on hobby deploy, not useful on cloud\n    http?: boolean\n    mmdb?: boolean\n}\n\nexport type EnqueuedJob = EnqueuedPluginJob | GraphileWorkerCronScheduleJob\nexport interface EnqueuedPluginJob {\n    type: string\n    payload: Record<string, any>\n    timestamp: number\n    pluginConfigId: number\n    pluginConfigTeam: number\n    jobKey?: string\n}\n\nexport interface GraphileWorkerCronScheduleJob {\n    timestamp?: number\n    jobKey?: string\n}\n\nexport enum JobName {\n    PLUGIN_JOB = 'pluginJob',\n    BUFFER_JOB = 'bufferJob',\n}\n\nexport type PluginId = Plugin['id']\nexport type PluginConfigId = PluginConfig['id']\nexport type TeamId = Team['id']\n\nexport enum MetricMathOperations {\n    Increment = 'increment',\n    Max = 'max',\n    Min = 'min',\n}\n\nexport type StoredMetricMathOperations = 'max' | 'min' | 'sum'\nexport type StoredPluginMetrics = Record<string, StoredMetricMathOperations> | null\nexport type PluginMetricsVmResponse = Record<string, string> | null\n\nexport interface JobPayloadFieldOptions {\n    type: 'string' | 'boolean' | 'json' | 'number' | 'date' | 'daterange'\n    title?: string\n    required?: boolean\n    default?: any\n    staff_only?: boolean\n}\n\nexport interface JobSpec {\n    payload?: Record<string, JobPayloadFieldOptions>\n}\n\nexport interface Plugin {\n    id: number\n    organization_id: string\n    name: string\n    plugin_type: 'local' | 'respository' | 'custom' | 'source'\n    description?: string\n    is_global: boolean\n    is_preinstalled?: boolean\n    url?: string\n    config_schema?: Record<string, PluginConfigSchema> | PluginConfigSchema[]\n    tag?: string\n    /** Cached source for plugin.json from a joined PluginSourceFile query */\n    source__plugin_json?: string\n    /** Cached source for index.ts from a joined PluginSourceFile query */\n    source__index_ts?: string\n    /** Cached source for frontend.tsx from a joined PluginSourceFile query */\n    source__frontend_tsx?: string\n    /** Cached source for site.ts from a joined PluginSourceFile query */\n    source__site_ts?: string\n    error?: PluginError\n    from_json?: boolean\n    from_web?: boolean\n    created_at?: string\n    updated_at?: string\n    capabilities?: PluginCapabilities\n    metrics?: StoredPluginMetrics\n    is_stateless?: boolean\n    public_jobs?: Record<string, JobSpec>\n    log_level?: PluginLogLevel\n}\n\nexport interface PluginCapabilities {\n    jobs?: string[]\n    scheduled_tasks?: string[]\n    methods?: string[]\n}\n\nexport interface PluginConfig {\n    id: number\n    team_id: TeamId\n    plugin?: Plugin\n    plugin_id: PluginId\n    enabled: boolean\n    order: number\n    config: Record<string, unknown>\n    has_error: boolean\n    attachments?: Record<string, PluginAttachment>\n    vm?: LazyPluginVM | null\n    created_at: string\n    updated_at?: string\n}\n\nexport interface PluginJsonConfig {\n    name?: string\n    description?: string\n    url?: string\n    main?: string\n    lib?: string\n    config?: Record<string, PluginConfigSchema> | PluginConfigSchema[]\n}\n\nexport interface PluginError {\n    message: string\n    time: string\n    name?: string\n    stack?: string\n    event?: PluginEvent | ProcessedPluginEvent | null\n}\n\nexport interface PluginAttachmentDB {\n    id: number\n    team_id: TeamId | null\n    plugin_config_id: PluginConfigId | null\n    key: string\n    content_type: string\n    file_size: number | null\n    file_name: string\n    contents: Buffer | null\n}\n\nexport enum PluginLogEntrySource {\n    System = 'SYSTEM',\n    Plugin = 'PLUGIN',\n    Console = 'CONSOLE',\n}\n\nexport enum PluginLogEntryType {\n    Debug = 'DEBUG',\n    Log = 'LOG',\n    Info = 'INFO',\n    Warn = 'WARN',\n    Error = 'ERROR',\n}\n\nexport enum PluginLogLevel {\n    Full = 0, // all logs\n    Debug = 1, // all except log\n    Warn = 2, // all except log and info\n    Critical = 3, // only error type and system source\n}\n\nexport interface PluginLogEntry {\n    id: string\n    team_id: number\n    plugin_id: number\n    plugin_config_id: number\n    timestamp: string\n    source: PluginLogEntrySource\n    type: PluginLogEntryType\n    message: string\n    instance_id: string\n}\n\nexport enum PluginSourceFileStatus {\n    Transpiled = 'TRANSPILED',\n    Locked = 'LOCKED',\n    Error = 'ERROR',\n}\n\nexport enum PluginTaskType {\n    Job = 'job',\n    Schedule = 'schedule',\n}\n\nexport interface PluginTask {\n    name: string\n    type: PluginTaskType\n    exec: (payload?: Record<string, any>) => Promise<any>\n\n    __ignoreForAppMetrics?: boolean\n}\n\nexport type VMMethods = {\n    setupPlugin?: () => Promise<void>\n    teardownPlugin?: () => Promise<void>\n    getSettings?: () => PluginSettings\n    onEvent?: (event: ProcessedPluginEvent) => Promise<void>\n    exportEvents?: (events: PluginEvent[]) => Promise<void>\n    processEvent?: (event: PluginEvent) => Promise<PluginEvent>\n}\n\nexport enum AlertLevel {\n    P0 = 0,\n    P1 = 1,\n    P2 = 2,\n    P3 = 3,\n    P4 = 4,\n}\n\nexport enum Service {\n    PluginServer = 'plugin_server',\n    DjangoServer = 'django_server',\n    Redis = 'redis',\n    Postgres = 'postgres',\n    ClickHouse = 'clickhouse',\n    Kafka = 'kafka',\n}\nexport interface Alert {\n    id: string\n    level: AlertLevel\n    key: string\n    description?: string\n    trigger_location: Service\n}\nexport interface PluginConfigVMResponse {\n    vm: VM\n    methods: VMMethods\n    tasks: Record<PluginTaskType, Record<string, PluginTask>>\n    vmResponseVariable: string\n}\n\nexport interface PluginConfigVMInternalResponse<M extends Meta = Meta> {\n    methods: VMMethods\n    tasks: Record<PluginTaskType, Record<string, PluginTask>>\n    meta: M\n}\n\nexport interface EventUsage {\n    event: string\n    usage_count: number | null\n    volume: number | null\n}\n\nexport interface PropertyUsage {\n    key: string\n    usage_count: number | null\n    volume: number | null\n}\n\n/** Raw Organization row from database. */\nexport interface RawOrganization {\n    id: string\n    name: string\n    created_at: string\n    updated_at: string\n    available_features: string[]\n}\n\n/** Usable Team model. */\nexport interface Team {\n    id: number\n    uuid: string\n    organization_id: string\n    name: string\n    anonymize_ips: boolean\n    api_token: string\n    slack_incoming_webhook: string | null\n    session_recording_opt_in: boolean\n    ingested_event: boolean\n    person_display_name_properties: string[] | null\n}\n\n/** Properties shared by RawEventMessage and EventMessage. */\nexport interface BaseEventMessage {\n    distinct_id: string\n    ip: string\n    site_url: string\n    team_id: number\n    uuid: string\n}\n\n/** Raw event message as received via Kafka. */\nexport interface RawEventMessage extends BaseEventMessage {\n    /** JSON-encoded object. */\n    data: string\n    /** ISO-formatted datetime. */\n    now: string\n    /** ISO-formatted datetime. May be empty! */\n    sent_at: string\n    /** JSON-encoded number. */\n    kafka_offset: string\n    /** Messages may have a token instead of a team_id, to be used e.g. to\n     * resolve to a team_id */\n    token?: string\n}\n\n/** Usable event message. */\nexport interface EventMessage extends BaseEventMessage {\n    data: PluginEvent\n    now: DateTime\n    sent_at: DateTime | null\n}\n\n/** Properties shared by RawClickHouseEvent and ClickHouseEvent. */\ninterface BaseEvent {\n    uuid: string\n    event: string\n    team_id: number\n    distinct_id: string\n    /** Person UUID. */\n    person_id?: string\n}\n\nexport type ISOTimestamp = Brand<string, 'ISOTimestamp'>\nexport type ClickHouseTimestamp = Brand<string, 'ClickHouseTimestamp'>\nexport type ClickHouseTimestampSecondPrecision = Brand<string, 'ClickHouseTimestamp'>\n\n/** Raw event row from ClickHouse. */\nexport interface RawClickHouseEvent extends BaseEvent {\n    timestamp: ClickHouseTimestamp\n    created_at: ClickHouseTimestamp\n    properties?: string\n    elements_chain: string\n    person_created_at?: ClickHouseTimestamp\n    person_properties?: string\n    group0_properties?: string\n    group1_properties?: string\n    group2_properties?: string\n    group3_properties?: string\n    group4_properties?: string\n    group0_created_at?: ClickHouseTimestamp\n    group1_created_at?: ClickHouseTimestamp\n    group2_created_at?: ClickHouseTimestamp\n    group3_created_at?: ClickHouseTimestamp\n    group4_created_at?: ClickHouseTimestamp\n}\n\n/** Parsed event row from ClickHouse. */\nexport interface ClickHouseEvent extends BaseEvent {\n    timestamp: DateTime\n    created_at: DateTime\n    properties: Record<string, any>\n    elements_chain: Element[] | null\n    person_created_at: DateTime | null\n    person_properties: Record<string, any>\n    group0_properties: Record<string, any>\n    group1_properties: Record<string, any>\n    group2_properties: Record<string, any>\n    group3_properties: Record<string, any>\n    group4_properties: Record<string, any>\n    group0_created_at?: DateTime | null\n    group1_created_at?: DateTime | null\n    group2_created_at?: DateTime | null\n    group3_created_at?: DateTime | null\n    group4_created_at?: DateTime | null\n}\n\n/** Event in a database-agnostic shape, AKA an ingestion event.\n * This is what should be passed around most of the time in the plugin server.\n */\ninterface BaseIngestionEvent {\n    eventUuid: string\n    event: string\n    teamId: TeamId\n    distinctId: string\n    properties: Properties\n    timestamp: ISOTimestamp\n    elementsList: Element[]\n}\n\n/** Ingestion event before saving, BaseIngestionEvent without elementsList */\nexport interface PreIngestionEvent {\n    eventUuid: string\n    event: string\n    teamId: TeamId\n    distinctId: string\n    properties: Properties\n    timestamp: ISOTimestamp\n}\n\n/** Ingestion event after saving, currently just an alias of BaseIngestionEvent */\nexport interface PostIngestionEvent extends BaseIngestionEvent {\n    person_id?: string // This is not optional, but BaseEvent needs to be fixed first\n    person_created_at: ISOTimestamp | null\n    person_properties: Properties\n}\n\nexport interface DeadLetterQueueEvent {\n    id: string\n    event_uuid: string\n    event: string\n    properties: string\n    distinct_id: string\n    team_id: number\n    elements_chain: string\n    created_at: string\n    ip: string\n    site_url: string\n    now: string\n    raw_payload: string\n    error_timestamp: string\n    error_location: string\n    error: string\n    tags: string[]\n    _timestamp: string\n    _offset: number\n}\n\nexport type PropertiesLastUpdatedAt = Record<string, string>\nexport type PropertiesLastOperation = Record<string, PropertyUpdateOperation>\n\n/** Properties shared by RawPerson and Person. */\nexport interface BasePerson {\n    id: number\n    team_id: number\n    properties: Properties\n    is_user_id: number\n    is_identified: boolean\n    uuid: string\n    properties_last_updated_at: PropertiesLastUpdatedAt\n    properties_last_operation: PropertiesLastOperation | null\n}\n\n/** Raw Person row from database. */\nexport interface RawPerson extends BasePerson {\n    created_at: string\n    version: string | null\n}\n\n/** Usable Person model. */\nexport interface Person extends BasePerson {\n    created_at: DateTime\n    version: number\n}\n\n/** Clickhouse Person model. */\nexport interface ClickHousePerson {\n    id: string\n    created_at: string\n    team_id: number\n    properties: string\n    is_identified: number\n    is_deleted: number\n    timestamp: string\n}\n\nexport type GroupTypeIndex = 0 | 1 | 2 | 3 | 4\n\ninterface BaseGroup {\n    id: number\n    team_id: number\n    group_type_index: GroupTypeIndex\n    group_key: string\n    group_properties: Properties\n    properties_last_updated_at: PropertiesLastUpdatedAt\n    properties_last_operation: PropertiesLastOperation\n}\n\n/** Raw Group row from database. */\nexport interface RawGroup extends BaseGroup {\n    created_at: string\n    version: string\n}\n\n/** Usable Group model. */\nexport interface Group extends BaseGroup {\n    created_at: DateTime\n    version: number\n}\n\nexport type GroupKey = string\n/** Clickhouse Group model */\nexport interface ClickhouseGroup {\n    group_type_index: GroupTypeIndex\n    group_key: GroupKey\n    created_at: string\n    team_id: number\n    group_properties: string\n}\n\n/** Usable PersonDistinctId model. */\nexport interface PersonDistinctId {\n    id: number\n    team_id: number\n    person_id: number\n    distinct_id: string\n    version: string | null\n}\n\n/** ClickHouse PersonDistinctId model. (person_distinct_id2 table) */\nexport interface ClickHousePersonDistinctId2 {\n    team_id: number\n    person_id: string\n    distinct_id: string\n    is_deleted: 0 | 1\n    version: number\n}\n\n/** Usable Cohort model. */\nexport interface Cohort {\n    id: number\n    name: string\n    description: string\n    deleted: boolean\n    groups: any[]\n    team_id: Team['id']\n    created_at: string\n    created_by_id: number\n    is_calculating: boolean\n    last_calculation: string\n    errors_calculating: number\n    is_static: boolean\n    version: number | null\n    pending_version: number\n}\n\n/** Usable CohortPeople model. */\nexport interface CohortPeople {\n    id: number\n    cohort_id: number\n    person_id: number\n}\n\n/** Usable Hook model. */\nexport interface Hook {\n    id: string\n    team_id: number\n    user_id: number\n    resource_id: number | null\n    event: string\n    target: string\n    created: string\n    updated: string\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport enum PropertyOperator {\n    Exact = 'exact',\n    IsNot = 'is_not',\n    IContains = 'icontains',\n    NotIContains = 'not_icontains',\n    Regex = 'regex',\n    NotRegex = 'not_regex',\n    GreaterThan = 'gt',\n    LessThan = 'lt',\n    IsSet = 'is_set',\n    IsNotSet = 'is_not_set',\n    IsDateBefore = 'is_date_before',\n    IsDateAfter = 'is_date_after',\n}\n\n/** Sync with posthog/frontend/src/types.ts */\ninterface PropertyFilterBase {\n    key: string\n    value?: string | number | Array<string | number> | null\n    label?: string\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface PropertyFilterWithOperator extends PropertyFilterBase {\n    operator?: PropertyOperator\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface EventPropertyFilter extends PropertyFilterWithOperator {\n    type: 'event'\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface PersonPropertyFilter extends PropertyFilterWithOperator {\n    type: 'person'\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface ElementPropertyFilter extends PropertyFilterWithOperator {\n    type: 'element'\n    key: 'tag_name' | 'text' | 'href' | 'selector'\n    value: string | string[]\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport interface CohortPropertyFilter extends PropertyFilterBase {\n    type: 'cohort'\n    key: 'id'\n    value: number | string\n}\n\n/** Sync with posthog/frontend/src/types.ts */\nexport type PropertyFilter = EventPropertyFilter | PersonPropertyFilter | ElementPropertyFilter | CohortPropertyFilter\n\n/** Sync with posthog/frontend/src/types.ts */\nexport enum StringMatching {\n    Contains = 'contains',\n    Regex = 'regex',\n    Exact = 'exact',\n}\n\nexport interface ActionStep {\n    id: number\n    action_id: number\n    tag_name: string | null\n    text: string | null\n    /** @default StringMatching.Exact */\n    text_matching: StringMatching | null\n    href: string | null\n    /** @default StringMatching.Exact */\n    href_matching: StringMatching | null\n    selector: string | null\n    url: string | null\n    /** @default StringMatching.Contains */\n    url_matching: StringMatching | null\n    name: string | null\n    event: string | null\n    properties: PropertyFilter[] | null\n}\n\n/** Raw Action row from database. */\nexport interface RawAction {\n    id: number\n    team_id: TeamId\n    name: string | null\n    description: string\n    created_at: string\n    created_by_id: number | null\n    deleted: boolean\n    post_to_slack: boolean\n    slack_message_format: string\n    is_calculating: boolean\n    updated_at: string\n    last_calculated_at: string\n    bytecode?: any[]\n    bytecode_error?: string\n}\n\n/** Usable Action model. */\nexport interface Action extends RawAction {\n    steps: ActionStep[]\n    hooks: Hook[]\n}\n\n/** Raw session recording event row from ClickHouse. */\nexport interface RawSessionRecordingEvent {\n    uuid: string\n    timestamp: string\n    team_id: number\n    distinct_id: string\n    session_id: string\n    window_id: string\n    snapshot_data: string\n    created_at: string\n}\n\n/** Raw session replay event row from ClickHouse. */\nexport interface RawSessionReplayEvent {\n    min_first_timestamp: string\n    team_id: number\n    distinct_id: string\n    session_id: string\n    /* TODO what columns do we need */\n}\n\nexport interface RawPerformanceEvent {\n    uuid: string\n    team_id: number\n    distinct_id: string\n    session_id: string\n    window_id: string\n    pageview_id: string\n    current_url: string\n\n    // BASE_EVENT_COLUMNS\n    time_origin: number\n    timestamp: string\n    entry_type: string\n    name: string\n\n    // RESOURCE_EVENT_COLUMNS\n    start_time: number\n    redirect_start: number\n    redirect_end: number\n    worker_start: number\n    fetch_start: number\n    domain_lookup_start: number\n    domain_lookup_end: number\n    connect_start: number\n    secure_connection_start: number\n    connect_end: number\n    request_start: number\n    response_start: number\n    response_end: number\n    decoded_body_size: number\n    encoded_body_size: number\n    duration: number\n\n    initiator_type: string\n    next_hop_protocol: string\n    render_blocking_status: string\n    response_status: number\n    transfer_size: number\n\n    // LARGEST_CONTENTFUL_PAINT_EVENT_COLUMNS\n    largest_contentful_paint_element: string\n    largest_contentful_paint_render_time: number\n    largest_contentful_paint_load_time: number\n    largest_contentful_paint_size: number\n    largest_contentful_paint_id: string\n    largest_contentful_paint_url: string\n\n    // NAVIGATION_EVENT_COLUMNS\n    dom_complete: number\n    dom_content_loaded_event: number\n    dom_interactive: number\n    load_event_end: number\n    load_event_start: number\n    redirect_count: number\n    navigation_type: string\n    unload_event_end: number\n    unload_event_start: number\n}\n\nexport const PerformanceEventReverseMapping: { [key: number]: keyof RawPerformanceEvent } = {\n    // BASE_PERFORMANCE_EVENT_COLUMNS\n    0: 'entry_type',\n    1: 'time_origin',\n    2: 'name',\n\n    // RESOURCE_EVENT_COLUMNS\n    3: 'start_time',\n    4: 'redirect_start',\n    5: 'redirect_end',\n    6: 'worker_start',\n    7: 'fetch_start',\n    8: 'domain_lookup_start',\n    9: 'domain_lookup_end',\n    10: 'connect_start',\n    11: 'secure_connection_start',\n    12: 'connect_end',\n    13: 'request_start',\n    14: 'response_start',\n    15: 'response_end',\n    16: 'decoded_body_size',\n    17: 'encoded_body_size',\n    18: 'initiator_type',\n    19: 'next_hop_protocol',\n    20: 'render_blocking_status',\n    21: 'response_status',\n    22: 'transfer_size',\n\n    // LARGEST_CONTENTFUL_PAINT_EVENT_COLUMNS\n    23: 'largest_contentful_paint_element',\n    24: 'largest_contentful_paint_render_time',\n    25: 'largest_contentful_paint_load_time',\n    26: 'largest_contentful_paint_size',\n    27: 'largest_contentful_paint_id',\n    28: 'largest_contentful_paint_url',\n\n    // NAVIGATION_EVENT_COLUMNS\n    29: 'dom_complete',\n    30: 'dom_content_loaded_event',\n    31: 'dom_interactive',\n    32: 'load_event_end',\n    33: 'load_event_start',\n    34: 'redirect_count',\n    35: 'navigation_type',\n    36: 'unload_event_end',\n    37: 'unload_event_start',\n\n    // Added after v1\n    39: 'duration',\n    40: 'timestamp',\n}\n\nexport enum TimestampFormat {\n    ClickHouseSecondPrecision = 'clickhouse-second-precision',\n    ClickHouse = 'clickhouse',\n    ISO = 'iso',\n}\n\nexport enum Database {\n    ClickHouse = 'clickhouse',\n    Postgres = 'postgres',\n}\n\nexport interface PluginScheduleControl {\n    stopSchedule: () => Promise<void>\n    reloadSchedule: () => Promise<void>\n}\n\nexport interface JobsConsumerControl {\n    stop: () => Promise<void>\n    resume: () => Promise<void>\n}\n\nexport type IngestEventResponse =\n    | { success: true; actionMatches: Action[]; preIngestionEvent: PreIngestionEvent | null }\n    | { success: false; error: string }\n\nexport interface EventDefinitionType {\n    id: string\n    name: string\n    volume_30_day: number | null\n    query_usage_30_day: number | null\n    team_id: number\n    last_seen_at: string // DateTime\n    created_at: string // DateTime\n}\n\nexport enum UnixTimestampPropertyTypeFormat {\n    UNIX_TIMESTAMP = 'unix_timestamp',\n    UNIX_TIMESTAMP_MILLISECONDS = 'unix_timestamp_milliseconds',\n}\n\nexport enum DateTimePropertyTypeFormat {\n    ISO8601_DATE = 'YYYY-MM-DDThh:mm:ssZ',\n    FULL_DATE = 'YYYY-MM-DD hh:mm:ss',\n    FULL_DATE_INCREASING = 'DD-MM-YYYY hh:mm:ss',\n    DATE = 'YYYY-MM-DD',\n    RFC_822 = 'rfc_822',\n    WITH_SLASHES = 'YYYY/MM/DD hh:mm:ss',\n    WITH_SLASHES_INCREASING = 'DD/MM/YYYY hh:mm:ss',\n}\n\nexport enum PropertyType {\n    DateTime = 'DateTime',\n    String = 'String',\n    Numeric = 'Numeric',\n    Boolean = 'Boolean',\n}\n\nexport enum PropertyDefinitionTypeEnum {\n    Event = 1,\n    Person = 2,\n    Group = 3,\n}\n\nexport interface PropertyDefinitionType {\n    id: string\n    name: string\n    is_numerical: boolean\n    volume_30_day: number | null\n    query_usage_30_day: number | null\n    team_id: number\n    property_type?: PropertyType\n    type: PropertyDefinitionTypeEnum\n    group_type_index: number | null\n}\n\nexport interface EventPropertyType {\n    id: string\n    event: string\n    property: string\n    team_id: number\n}\n\nexport type PluginFunction = 'onEvent' | 'processEvent' | 'pluginTask'\n\nexport type GroupTypeToColumnIndex = Record<string, GroupTypeIndex>\n\nexport enum PropertyUpdateOperation {\n    Set = 'set',\n    SetOnce = 'set_once',\n}\n\nexport type StatelessVmMap = Record<PluginId, LazyPluginVM>\n\nexport enum OrganizationPluginsAccessLevel {\n    NONE = 0,\n    CONFIG = 3,\n    INSTALL = 6,\n    ROOT = 9,\n}\n\nexport enum OrganizationMembershipLevel {\n    Member = 1,\n    Admin = 8,\n    Owner = 15,\n}\n\nexport interface PipelineEvent extends Omit<PluginEvent, 'team_id'> {\n    team_id?: number | null\n    token?: string\n}\n\nexport type RedisPool = GenericPool<Redis>\n\nexport type RRWebEvent = Record<string, any> & {\n    timestamp: number\n    type: number\n    data: any\n}\n\nexport interface ValueMatcher<T> {\n    (value: T): boolean\n}\n", "import ClickHouse from '@posthog/clickhouse'\nimport * as Sentry from '@sentry/node'\nimport * as fs from 'fs'\nimport { StatsD } from 'hot-shots'\nimport { Kafka, SASLOptions } from 'kafkajs'\nimport { DateTime } from 'luxon'\nimport { hostname } from 'os'\nimport * as path from 'path'\nimport { types as pgTypes } from 'pg'\nimport { ConnectionOptions } from 'tls'\n\nimport { getPluginServerCapabilities } from '../../capabilities'\nimport { buildIntegerMatcher, defaultConfig } from '../../config/config'\nimport { KAFKAJS_LOG_LEVEL_MAPPING } from '../../config/constants'\nimport { KAFKA_JOBS } from '../../config/kafka-topics'\nimport { createRdConnectionConfigFromEnvVars, createRdProducerConfigFromEnvVars } from '../../kafka/config'\nimport { createKafkaProducer } from '../../kafka/producer'\nimport { getObjectStorage } from '../../main/services/object_storage'\nimport {\n    EnqueuedPluginJob,\n    Hub,\n    KafkaSaslMechanism,\n    KafkaSecurityProtocol,\n    PluginServerCapabilities,\n    PluginsServerConfig,\n} from '../../types'\nimport { AppMetrics } from '../../worker/ingestion/app-metrics'\nimport { OrganizationManager } from '../../worker/ingestion/organization-manager'\nimport { EventsProcessor } from '../../worker/ingestion/process-event'\nimport { TeamManager } from '../../worker/ingestion/team-manager'\nimport { isTestEnv } from '../env-utils'\nimport { status } from '../status'\nimport { createRedisPool, UUIDT } from '../utils'\nimport { PluginsApiKeyManager } from './../../worker/vm/extensions/helpers/api-key-manager'\nimport { RootAccessManager } from './../../worker/vm/extensions/helpers/root-acess-manager'\nimport { PromiseManager } from './../../worker/vm/promise-manager'\nimport { DB } from './db'\nimport { KafkaProducerWrapper } from './kafka-producer-wrapper'\nimport { PostgresRouter } from './postgres'\n\n// `node-postgres` would return dates as plain JS Date objects, which would use the local timezone.\n// This converts all date fields to a proper luxon UTC DateTime and then casts them to a string\n// Unfortunately this must be done on a global object before initializing the `Pool`\npgTypes.setTypeParser(1083 /* types.TypeId.TIME */, (timeStr) =>\n    timeStr ? DateTime.fromSQL(timeStr, { zone: 'utc' }).toISO() : null\n)\npgTypes.setTypeParser(1114 /* types.TypeId.TIMESTAMP */, (timeStr) =>\n    timeStr ? DateTime.fromSQL(timeStr, { zone: 'utc' }).toISO() : null\n)\npgTypes.setTypeParser(1184 /* types.TypeId.TIMESTAMPTZ */, (timeStr) =>\n    timeStr ? DateTime.fromSQL(timeStr, { zone: 'utc' }).toISO() : null\n)\n\nexport async function createKafkaProducerWrapper(serverConfig: PluginsServerConfig): Promise<KafkaProducerWrapper> {\n    const kafkaConnectionConfig = createRdConnectionConfigFromEnvVars(serverConfig)\n    const producerConfig = createRdProducerConfigFromEnvVars(serverConfig)\n    const producer = await createKafkaProducer(kafkaConnectionConfig, producerConfig)\n    return new KafkaProducerWrapper(producer)\n}\n\nexport function createEventsToDropByToken(eventsToDropByTokenStr?: string): Map<string, string[]> {\n    const eventsToDropByToken: Map<string, string[]> = new Map()\n    if (eventsToDropByTokenStr) {\n        eventsToDropByTokenStr.split(',').forEach((pair) => {\n            const [token, distinctID] = pair.split(':')\n            eventsToDropByToken.set(token, [...(eventsToDropByToken.get(token) || []), distinctID])\n        })\n    }\n    return eventsToDropByToken\n}\n\nexport async function createHub(\n    config: Partial<PluginsServerConfig> = {},\n    threadId: number | null = null,\n    capabilities: PluginServerCapabilities | null = null\n): Promise<[Hub, () => Promise<void>]> {\n    status.info('\u2139\ufe0f', `Connecting to all services:`)\n\n    const serverConfig: PluginsServerConfig = {\n        ...defaultConfig,\n        ...config,\n    }\n    if (capabilities === null) {\n        capabilities = getPluginServerCapabilities(serverConfig)\n    }\n    status.updatePrompt(serverConfig.PLUGIN_SERVER_MODE)\n    const instanceId = new UUIDT()\n\n    const conversionBufferEnabledTeams = new Set(\n        serverConfig.CONVERSION_BUFFER_ENABLED_TEAMS.split(',').filter(String).map(Number)\n    )\n\n    const statsd: StatsD | undefined = createStatsdClient(serverConfig, threadId)\n\n    status.info('\ud83e\udd14', `Connecting to ClickHouse...`)\n    const clickhouse = new ClickHouse({\n        // We prefer to run queries on the offline cluster.\n        host: serverConfig.CLICKHOUSE_OFFLINE_CLUSTER_HOST ?? serverConfig.CLICKHOUSE_HOST,\n        port: serverConfig.CLICKHOUSE_SECURE ? 8443 : 8123,\n        protocol: serverConfig.CLICKHOUSE_SECURE ? 'https:' : 'http:',\n        user: serverConfig.CLICKHOUSE_USER,\n        password: serverConfig.CLICKHOUSE_PASSWORD || undefined,\n        dataObjects: true,\n        queryOptions: {\n            database: serverConfig.CLICKHOUSE_DATABASE,\n            output_format_json_quote_64bit_integers: false,\n        },\n        ca: serverConfig.CLICKHOUSE_CA\n            ? fs.readFileSync(path.join(serverConfig.BASE_DIR, serverConfig.CLICKHOUSE_CA)).toString()\n            : undefined,\n        rejectUnauthorized: serverConfig.CLICKHOUSE_CA ? false : undefined,\n    })\n    status.info('\ud83d\udc4d', `ClickHouse ready`)\n\n    status.info('\ud83e\udd14', `Connecting to Kafka...`)\n\n    const kafka = createKafkaClient(serverConfig)\n    const kafkaProducer = await createKafkaProducerWrapper(serverConfig)\n    status.info('\ud83d\udc4d', `Kafka ready`)\n\n    const postgres = new PostgresRouter(serverConfig, statsd)\n    // TODO: assert tables are reachable (async calls that cannot be in a constructor)\n    status.info('\ud83d\udc4d', `Postgres Router ready`)\n\n    status.info('\ud83e\udd14', `Connecting to Redis...`)\n    const redisPool = createRedisPool(serverConfig)\n    status.info('\ud83d\udc4d', `Redis ready`)\n\n    status.info('\ud83e\udd14', `Connecting to object storage...`)\n\n    const objectStorage = getObjectStorage(serverConfig)\n    if (objectStorage) {\n        status.info('\ud83d\udc4d', 'Object storage ready')\n    } else {\n        status.warn('\ud83e\udea3', `Object storage could not be created`)\n    }\n\n    const promiseManager = new PromiseManager(serverConfig, statsd)\n\n    const db = new DB(postgres, redisPool, kafkaProducer, clickhouse, statsd, serverConfig.PERSON_INFO_CACHE_TTL)\n    const teamManager = new TeamManager(postgres, serverConfig, statsd)\n    const organizationManager = new OrganizationManager(postgres, teamManager)\n    const pluginsApiKeyManager = new PluginsApiKeyManager(db)\n    const rootAccessManager = new RootAccessManager(db)\n\n    const enqueuePluginJob = async (job: EnqueuedPluginJob) => {\n        // NOTE: we use the producer directly here rather than using the wrapper\n        // such that we can a response immediately on error, and thus bubble up\n        // any errors in producing. It's important that we ensure that we have\n        // an acknowledgement as for instance there are some jobs that are\n        // chained, and if we do not manage to produce then the chain will be\n        // broken.\n        await kafkaProducer.queueMessage({\n            topic: KAFKA_JOBS,\n            messages: [\n                {\n                    value: Buffer.from(JSON.stringify(job)),\n                    key: Buffer.from(job.pluginConfigTeam.toString()),\n                },\n            ],\n        })\n    }\n\n    const hub: Partial<Hub> = {\n        ...serverConfig,\n        instanceId,\n        capabilities,\n        db,\n        postgres,\n        redisPool,\n        clickhouse,\n        kafka,\n        kafkaProducer,\n        statsd,\n        enqueuePluginJob,\n        objectStorage: objectStorage,\n\n        plugins: new Map(),\n        pluginConfigs: new Map(),\n        pluginConfigsPerTeam: new Map(),\n        pluginConfigSecrets: new Map(),\n        pluginConfigSecretLookup: new Map(),\n\n        pluginSchedule: null,\n\n        teamManager,\n        organizationManager,\n        pluginsApiKeyManager,\n        rootAccessManager,\n        promiseManager,\n        conversionBufferEnabledTeams,\n        pluginConfigsToSkipElementsParsing: buildIntegerMatcher(process.env.SKIP_ELEMENTS_PARSING_PLUGINS, true),\n        poeEmbraceJoinForTeams: buildIntegerMatcher(process.env.POE_EMBRACE_JOIN_FOR_TEAMS, true),\n        eventsToDropByToken: createEventsToDropByToken(process.env.DROP_EVENTS_BY_TOKEN_DISTINCT_ID),\n    }\n\n    // :TODO: This is only used on worker threads, not main\n    hub.eventsProcessor = new EventsProcessor(hub as Hub)\n\n    hub.appMetrics = new AppMetrics(\n        kafkaProducer,\n        serverConfig.APP_METRICS_FLUSH_FREQUENCY_MS,\n        serverConfig.APP_METRICS_FLUSH_MAX_QUEUE_SIZE\n    )\n\n    const closeHub = async () => {\n        if (!isTestEnv()) {\n            await hub.appMetrics?.flush()\n        }\n        await Promise.allSettled([kafkaProducer.disconnect(), redisPool.drain(), hub.postgres?.end()])\n        await redisPool.clear()\n\n        // Break circular references to allow the hub to be GCed when running unit tests\n        // TODO: change these structs to not directly reference the hub\n        hub.eventsProcessor = undefined\n        hub.appMetrics = undefined\n    }\n\n    return [hub as Hub, closeHub]\n}\n\nexport type KafkaConfig = {\n    KAFKA_HOSTS: string\n    KAFKAJS_LOG_LEVEL: keyof typeof KAFKAJS_LOG_LEVEL_MAPPING\n    KAFKA_SECURITY_PROTOCOL: 'PLAINTEXT' | 'SSL' | 'SASL_PLAINTEXT' | 'SASL_SSL' | undefined\n    KAFKA_CLIENT_CERT_B64?: string\n    KAFKA_CLIENT_CERT_KEY_B64?: string\n    KAFKA_TRUSTED_CERT_B64?: string\n    KAFKA_SASL_MECHANISM?: KafkaSaslMechanism\n    KAFKA_SASL_USER?: string\n    KAFKA_SASL_PASSWORD?: string\n    KAFKA_CLIENT_RACK?: string\n}\n\nexport function createStatsdClient(serverConfig: PluginsServerConfig, threadId: number | null) {\n    let statsd: StatsD | undefined\n\n    if (serverConfig.STATSD_HOST) {\n        status.info('\ud83e\udd14', `Connecting to StatsD...`)\n        statsd = new StatsD({\n            port: serverConfig.STATSD_PORT,\n            host: serverConfig.STATSD_HOST,\n            prefix: serverConfig.STATSD_PREFIX,\n            telegraf: true,\n            globalTags: serverConfig.PLUGIN_SERVER_MODE\n                ? { pluginServerMode: serverConfig.PLUGIN_SERVER_MODE }\n                : undefined,\n            errorHandler: (error) => {\n                status.warn('\u26a0\ufe0f', 'StatsD error', error)\n                Sentry.captureException(error, {\n                    extra: { threadId },\n                })\n            },\n        })\n        // don't repeat the same info in each thread\n        if (threadId === null) {\n            status.info(\n                '\ud83e\udeb5',\n                `Sending metrics to StatsD at ${serverConfig.STATSD_HOST}:${serverConfig.STATSD_PORT}, prefix: \"${serverConfig.STATSD_PREFIX}\"`\n            )\n        }\n        status.info('\ud83d\udc4d', `StatsD ready`)\n    }\n    return statsd\n}\n\nexport function createKafkaClient({\n    KAFKA_HOSTS,\n    KAFKAJS_LOG_LEVEL,\n    KAFKA_SECURITY_PROTOCOL,\n    KAFKA_CLIENT_CERT_B64,\n    KAFKA_CLIENT_CERT_KEY_B64,\n    KAFKA_TRUSTED_CERT_B64,\n    KAFKA_SASL_MECHANISM,\n    KAFKA_SASL_USER,\n    KAFKA_SASL_PASSWORD,\n}: KafkaConfig) {\n    let kafkaSsl: ConnectionOptions | boolean | undefined\n    if (KAFKA_CLIENT_CERT_B64 && KAFKA_CLIENT_CERT_KEY_B64 && KAFKA_TRUSTED_CERT_B64) {\n        kafkaSsl = {\n            cert: Buffer.from(KAFKA_CLIENT_CERT_B64, 'base64'),\n            key: Buffer.from(KAFKA_CLIENT_CERT_KEY_B64, 'base64'),\n            ca: Buffer.from(KAFKA_TRUSTED_CERT_B64, 'base64'),\n\n            /* Intentionally disabling hostname checking. The Kafka cluster runs in the cloud and Apache\n            Kafka on Heroku doesn't currently provide stable hostnames. We're pinned to a specific certificate\n            #for this connection even though the certificate doesn't include host information. We rely\n            on the ca trust_cert for this purpose. */\n            rejectUnauthorized: false,\n        }\n    } else if (\n        KAFKA_SECURITY_PROTOCOL === KafkaSecurityProtocol.Ssl ||\n        KAFKA_SECURITY_PROTOCOL === KafkaSecurityProtocol.SaslSsl\n    ) {\n        kafkaSsl = true\n    }\n\n    let kafkaSasl: SASLOptions | undefined\n    if (KAFKA_SASL_MECHANISM && KAFKA_SASL_USER && KAFKA_SASL_PASSWORD) {\n        kafkaSasl = {\n            mechanism: KAFKA_SASL_MECHANISM,\n            username: KAFKA_SASL_USER,\n            password: KAFKA_SASL_PASSWORD,\n        }\n    }\n\n    const kafka = new Kafka({\n        /* clientId does not need to be unique, and is used in Kafka logs and quota accounting.\n           os.hostname() returns the pod name in k8s and the container ID in compose stacks.\n           This allows us to quickly find what pod is consuming a given partition */\n        clientId: hostname(),\n        brokers: KAFKA_HOSTS.split(','),\n        logLevel: KAFKAJS_LOG_LEVEL_MAPPING[KAFKAJS_LOG_LEVEL],\n        ssl: kafkaSsl,\n        sasl: kafkaSasl,\n        connectionTimeout: 7000,\n        authenticationTimeout: 7000, // default: 1000\n    })\n    return kafka\n}\n", "// This module wraps node-fetch with a sentry tracing-aware extension\n\nimport { LookupAddress } from 'dns'\nimport dns from 'dns/promises'\nimport * as ipaddr from 'ipaddr.js'\nimport fetch, { type RequestInfo, type RequestInit, type Response, FetchError, Request } from 'node-fetch'\nimport { URL } from 'url'\n\nimport { runInSpan } from '../sentry'\nimport { isProdEnv } from './env-utils'\n\nexport async function trackedFetch(url: RequestInfo, init?: RequestInit): Promise<Response> {\n    const request = new Request(url, init)\n    return await runInSpan(\n        {\n            op: 'fetch',\n            description: `${request.method} ${request.url}`,\n        },\n        async () => {\n            if (isProdEnv() && !process.env.NODE_ENV?.includes('functional-tests')) {\n                await raiseIfUserProvidedUrlUnsafe(request.url)\n            }\n            return await fetch(url, init)\n        }\n    )\n}\n\ntrackedFetch.isRedirect = fetch.isRedirect\ntrackedFetch.FetchError = FetchError\n\n/**\n * Raise if the provided URL seems unsafe, otherwise do nothing.\n *\n * Equivalent of Django raise_if_user_provided_url_unsafe.\n */\nexport async function raiseIfUserProvidedUrlUnsafe(url: string): Promise<void> {\n    // Raise if the provided URL seems unsafe, otherwise do nothing.\n    let parsedUrl: URL\n    try {\n        parsedUrl = new URL(url)\n    } catch (err) {\n        throw new FetchError('Invalid URL', 'posthog-host-guard')\n    }\n    if (!parsedUrl.hostname) {\n        throw new FetchError('No hostname', 'posthog-host-guard')\n    }\n    if (parsedUrl.protocol !== 'http:' && parsedUrl.protocol !== 'https:') {\n        throw new FetchError('Scheme must be either HTTP or HTTPS', 'posthog-host-guard')\n    }\n    let addrinfo: LookupAddress[]\n    try {\n        addrinfo = await dns.lookup(parsedUrl.hostname, { all: true })\n    } catch (err) {\n        throw new FetchError('Invalid hostname', 'posthog-host-guard')\n    }\n    for (const { address } of addrinfo) {\n        // Prevent addressing internal services\n        if (ipaddr.parse(address).range() !== 'unicast') {\n            throw new FetchError('Internal hostname', 'posthog-host-guard')\n        }\n    }\n}\n", "import { captureException } from '@sentry/node'\nimport { StatsD } from 'hot-shots'\nimport { Histogram } from 'prom-client'\nimport { format } from 'util'\n\nimport { Action, Hook, PostIngestionEvent, Team } from '../../types'\nimport { PostgresRouter, PostgresUse } from '../../utils/db/postgres'\nimport { trackedFetch } from '../../utils/fetch'\nimport { status } from '../../utils/status'\nimport { getPropertyValueByPath, stringify } from '../../utils/utils'\nimport { AppMetrics } from './app-metrics'\nimport { OrganizationManager } from './organization-manager'\nimport { TeamManager } from './team-manager'\n\nexport const webhookProcessStepDuration = new Histogram({\n    name: 'webhook_process_event_duration',\n    help: 'Processing step latency to during webhooks processing, per tag',\n    labelNames: ['tag'],\n})\n\nexport async function instrumentWebhookStep<T>(tag: string, run: () => Promise<T>): Promise<T> {\n    const end = webhookProcessStepDuration\n        .labels({\n            tag: tag,\n        })\n        .startTimer()\n    const res = await run()\n    end()\n    return res\n}\n\nexport enum WebhookType {\n    Slack = 'slack',\n    Discord = 'discord',\n    Teams = 'teams',\n}\n\nexport function determineWebhookType(url: string): WebhookType {\n    url = url.toLowerCase()\n    if (url.includes('slack.com')) {\n        return WebhookType.Slack\n    }\n    if (url.includes('discord.com')) {\n        return WebhookType.Discord\n    }\n    return WebhookType.Teams\n}\n\n// https://api.slack.com/reference/surfaces/formatting#escaping\nfunction escapeSlack(text: string): string {\n    return text.replaceAll('&', '&amp;').replaceAll('<', '&lt;').replaceAll('>', '&gt;')\n}\n\nfunction escapeMarkdown(text: string): string {\n    const markdownChars: string[] = ['\\\\', '`', '*', '_', '{', '}', '[', ']', '(', ')', '!']\n    const lineStartChars: string[] = ['#', '-', '+']\n\n    let escapedText = ''\n    let isNewLine = true\n\n    for (const char of text) {\n        if (isNewLine && lineStartChars.includes(char)) {\n            escapedText += '\\\\' + char\n        } else if (!isNewLine && markdownChars.includes(char)) {\n            escapedText += '\\\\' + char\n        } else {\n            escapedText += char\n        }\n\n        isNewLine = char === '\\n' || char === '\\r'\n    }\n\n    return escapedText\n}\n\nexport function webhookEscape(text: string, webhookType: WebhookType): string {\n    if (webhookType === WebhookType.Slack) {\n        return escapeSlack(stringify(text))\n    }\n    return escapeMarkdown(stringify(text))\n}\n\nexport function toWebhookLink(text: string | null, url: string, webhookType: WebhookType): [string, string] {\n    const name = stringify(text)\n    if (webhookType === WebhookType.Slack) {\n        return [escapeSlack(name), `<${escapeSlack(url)}|${escapeSlack(name)}>`]\n    } else {\n        return [escapeMarkdown(name), `[${escapeMarkdown(name)}](${escapeMarkdown(url)})`]\n    }\n}\n\n// Sync with .../api/person.py and .../lib/constants.tsx\nexport const PERSON_DEFAULT_DISPLAY_NAME_PROPERTIES = [\n    'email',\n    'Email',\n    'name',\n    'Name',\n    'username',\n    'Username',\n    'UserName',\n]\n\nexport function getPersonLink(event: PostIngestionEvent, siteUrl: string): string {\n    return `${siteUrl}/person/${encodeURIComponent(event.distinctId)}`\n}\nexport function getPersonDetails(\n    event: PostIngestionEvent,\n    siteUrl: string,\n    webhookType: WebhookType,\n    team: Team\n): [string, string] {\n    // Sync the logic below with the frontend `asDisplay`\n    const personDisplayNameProperties = team.person_display_name_properties ?? PERSON_DEFAULT_DISPLAY_NAME_PROPERTIES\n    const customPropertyKey = personDisplayNameProperties.find((x) => event.person_properties?.[x])\n    const propertyIdentifier = customPropertyKey ? event.person_properties[customPropertyKey] : undefined\n\n    const customIdentifier: string =\n        typeof propertyIdentifier !== 'string' ? JSON.stringify(propertyIdentifier) : propertyIdentifier\n\n    const display: string | undefined = (customIdentifier || event.distinctId)?.trim()\n\n    return toWebhookLink(display, getPersonLink(event, siteUrl), webhookType)\n}\n\nexport function getActionLink(action: Action, siteUrl: string): string {\n    return `${siteUrl}/action/${action.id}`\n}\nexport function getActionDetails(action: Action, siteUrl: string, webhookType: WebhookType): [string, string] {\n    return toWebhookLink(action.name, getActionLink(action, siteUrl), webhookType)\n}\n\nexport function getEventLink(event: PostIngestionEvent, siteUrl: string): string {\n    return `${siteUrl}/events/${encodeURIComponent(event.eventUuid)}/${encodeURIComponent(event.timestamp)}`\n}\nexport function getEventDetails(\n    event: PostIngestionEvent,\n    siteUrl: string,\n    webhookType: WebhookType\n): [string, string] {\n    return toWebhookLink(event.event, getEventLink(event, siteUrl), webhookType)\n}\n\nconst TOKENS_REGEX_BRACKETS_EXCLUDED = /(?<=(?<!\\\\)\\[)(.*?)(?=(?<!\\\\)\\])/g\nconst TOKENS_REGEX_BRACKETS_INCLUDED = /(?<!\\\\)\\[(.*?)(?<!\\\\)\\]/g\n\nexport function getTokens(messageFormat: string): [string[], string] {\n    // This finds property value tokens, basically any string contained in square brackets\n    // Examples: \"[foo]\" is matched in \"bar [foo]\", \"[action.name]\" is matched in \"action [action.name]\"\n    // The backslash is used as an escape character - \"\\[foo\\]\" is not matched, allowing square brackets in messages\n    const matchedTokens = messageFormat.match(TOKENS_REGEX_BRACKETS_EXCLUDED) || []\n    // Replace the tokens with placeholders, and unescape leftover brackets\n    const tokenizedMessage = messageFormat.replace(TOKENS_REGEX_BRACKETS_INCLUDED, '%s').replace(/\\\\(\\[|\\])/g, '$1')\n    return [matchedTokens, tokenizedMessage]\n}\n\nexport function getValueOfToken(\n    action: Action,\n    event: PostIngestionEvent,\n    team: Team,\n    siteUrl: string,\n    webhookType: WebhookType,\n    tokenParts: string[]\n): [string, string] {\n    let text = ''\n    let markdown = ''\n\n    if (tokenParts[0] === 'user') {\n        // [user.name] and [user.foo] are DEPRECATED as they had odd mechanics\n        // [person] OR [event.properties.bar] should be used instead\n        if (tokenParts[1] === 'name') {\n            ;[text, markdown] = getPersonDetails(event, siteUrl, webhookType, team)\n        } else {\n            const propertyName = `$${tokenParts[1]}`\n            const property = event.properties?.[propertyName]\n            markdown = text = webhookEscape(property, webhookType)\n        }\n    } else if (tokenParts[0] === 'person') {\n        if (tokenParts.length === 1) {\n            ;[text, markdown] = getPersonDetails(event, siteUrl, webhookType, team)\n        } else if (tokenParts[1] === 'link') {\n            markdown = text = webhookEscape(getPersonLink(event, siteUrl), webhookType)\n        } else if (tokenParts[1] === 'properties' && tokenParts.length > 2) {\n            const property = event.person_properties\n                ? getPropertyValueByPath(event.person_properties, tokenParts.slice(2))\n                : undefined\n            markdown = text = webhookEscape(property, webhookType)\n        }\n    } else if (tokenParts[0] === 'action') {\n        if (tokenParts[1] === 'name') {\n            ;[text, markdown] = getActionDetails(action, siteUrl, webhookType)\n        } else if (tokenParts[1] === 'link') {\n            markdown = text = webhookEscape(getActionLink(action, siteUrl), webhookType)\n        }\n    } else if (tokenParts[0] === 'event') {\n        if (tokenParts.length === 1) {\n            ;[text, markdown] = getEventDetails(event, siteUrl, webhookType)\n        } else if (tokenParts[1] === 'link') {\n            markdown = text = webhookEscape(getEventLink(event, siteUrl), webhookType)\n        } else if (tokenParts[1] === 'uuid') {\n            markdown = text = webhookEscape(event.eventUuid, webhookType)\n        } else if (tokenParts[1] === 'name') {\n            // deprecated\n            markdown = text = webhookEscape(event.event, webhookType)\n        } else if (tokenParts[1] === 'event') {\n            markdown = text = webhookEscape(event.event, webhookType)\n        } else if (tokenParts[1] === 'distinct_id') {\n            markdown = text = webhookEscape(event.distinctId, webhookType)\n        } else if (tokenParts[1] === 'properties' && tokenParts.length > 2) {\n            const property = event.properties\n                ? getPropertyValueByPath(event.properties, tokenParts.slice(2))\n                : undefined\n            markdown = text = webhookEscape(property, webhookType)\n        }\n    } else {\n        throw new Error()\n    }\n    return [text, markdown]\n}\n\nexport function getFormattedMessage(\n    action: Action,\n    event: PostIngestionEvent,\n    team: Team,\n    siteUrl: string,\n    webhookType: WebhookType\n): [string, string] {\n    const messageFormat = action.slack_message_format || '[action.name] was triggered by [person]'\n    let messageText: string\n    let messageMarkdown: string\n\n    try {\n        const [tokens, tokenizedMessage] = getTokens(messageFormat)\n        const values: string[] = []\n        const markdownValues: string[] = []\n\n        for (const token of tokens) {\n            const tokenParts = token.split('.') || []\n\n            const [value, markdownValue] = getValueOfToken(action, event, team, siteUrl, webhookType, tokenParts)\n            values.push(value)\n            markdownValues.push(markdownValue)\n        }\n        messageText = format(tokenizedMessage, ...values)\n        messageMarkdown = format(tokenizedMessage, ...markdownValues)\n    } catch (error) {\n        const [actionName, actionMarkdown] = getActionDetails(action, siteUrl, webhookType)\n        messageText = `\u26a0 Error: There are one or more formatting errors in the message template for action \"${actionName}\".`\n        messageMarkdown = `*\u26a0 Error: There are one or more formatting errors in the message template for action \"${actionMarkdown}\".*`\n    }\n\n    return [messageText, messageMarkdown]\n}\n\nexport class HookCommander {\n    postgres: PostgresRouter\n    teamManager: TeamManager\n    organizationManager: OrganizationManager\n    appMetrics: AppMetrics\n    statsd: StatsD | undefined\n    siteUrl: string\n    /** Hook request timeout in ms. */\n    EXTERNAL_REQUEST_TIMEOUT: number\n\n    constructor(\n        postgres: PostgresRouter,\n        teamManager: TeamManager,\n        organizationManager: OrganizationManager,\n        appMetrics: AppMetrics,\n        statsd: StatsD | undefined,\n        timeout: number\n    ) {\n        this.postgres = postgres\n        this.teamManager = teamManager\n        this.organizationManager = organizationManager\n        if (process.env.SITE_URL) {\n            this.siteUrl = process.env.SITE_URL\n        } else {\n            status.warn('\u26a0\ufe0f', 'SITE_URL env is not set for webhooks')\n            this.siteUrl = ''\n        }\n        this.statsd = statsd\n        this.appMetrics = appMetrics\n        this.EXTERNAL_REQUEST_TIMEOUT = timeout\n    }\n\n    public async findAndFireHooks(event: PostIngestionEvent, actionMatches: Action[]): Promise<void> {\n        status.debug('\ud83d\udd0d', `Looking for hooks to fire for event \"${event.event}\"`)\n        if (!actionMatches.length) {\n            status.debug('\ud83d\udd0d', `No hooks to fire for event \"${event.event}\"`)\n            return\n        }\n        status.debug('\ud83d\udd0d', `Found ${actionMatches.length} matching actions`)\n\n        const team = await this.teamManager.fetchTeam(event.teamId)\n\n        if (!team) {\n            return\n        }\n\n        const webhookUrl = team.slack_incoming_webhook\n\n        if (webhookUrl) {\n            await instrumentWebhookStep('postWebhook', async () => {\n                const webhookRequests = actionMatches\n                    .filter((action) => action.post_to_slack)\n                    .map((action) => this.postWebhook(webhookUrl, action, event, team))\n                await Promise.all(webhookRequests).catch((error) =>\n                    captureException(error, { tags: { team_id: event.teamId } })\n                )\n            })\n        }\n\n        if (await this.organizationManager.hasAvailableFeature(team.id, 'zapier')) {\n            await instrumentWebhookStep('postRestHook', async () => {\n                const restHooks = actionMatches.map(({ hooks }) => hooks).flat()\n\n                if (restHooks.length > 0) {\n                    const restHookRequests = restHooks.map((hook) => this.postRestHook(hook, event))\n                    await Promise.all(restHookRequests).catch((error) =>\n                        captureException(error, { tags: { team_id: event.teamId } })\n                    )\n\n                    this.statsd?.increment('zapier_hooks_fired', {\n                        team_id: String(team.id),\n                    })\n                }\n            })\n        }\n    }\n\n    private formatMessage(\n        webhookUrl: string,\n        action: Action,\n        event: PostIngestionEvent,\n        team: Team\n    ): Record<string, any> {\n        const webhookType = determineWebhookType(webhookUrl)\n        const [messageText, messageMarkdown] = getFormattedMessage(action, event, team, this.siteUrl, webhookType)\n        if (webhookType === WebhookType.Slack) {\n            return {\n                text: messageText,\n                blocks: [{ type: 'section', text: { type: 'mrkdwn', text: messageMarkdown } }],\n            }\n        } else {\n            return {\n                text: messageMarkdown,\n            }\n        }\n    }\n\n    private async postWebhook(\n        webhookUrl: string,\n        action: Action,\n        event: PostIngestionEvent,\n        team: Team\n    ): Promise<void> {\n        const end = webhookProcessStepDuration.labels('messageFormatting').startTimer()\n        const message = this.formatMessage(webhookUrl, action, event, team)\n        end()\n\n        const slowWarningTimeout = this.EXTERNAL_REQUEST_TIMEOUT * 0.7\n        const timeout = setTimeout(() => {\n            status.warn(\n                '\u231b',\n                `Posting Webhook slow. Timeout warning after ${\n                    slowWarningTimeout / 1000\n                } sec! url=${webhookUrl} team_id=${team.id} event_id=${event.eventUuid}`\n            )\n        }, slowWarningTimeout)\n        try {\n            await instrumentWebhookStep('fetch', async () => {\n                const request = await trackedFetch(webhookUrl, {\n                    method: 'POST',\n                    body: JSON.stringify(message, undefined, 4),\n                    headers: { 'Content-Type': 'application/json' },\n                    timeout: this.EXTERNAL_REQUEST_TIMEOUT,\n                })\n                if (!request.ok) {\n                    status.warn('\u26a0\ufe0f', `HTTP status ${request.status} for team ${team.id}`)\n                    await this.appMetrics.queueError(\n                        {\n                            teamId: event.teamId,\n                            pluginConfigId: -2, // -2 is hardcoded to mean webhooks\n                            category: 'webhook',\n                            failures: 1,\n                        },\n                        {\n                            error: `Request failed with HTTP status ${request.status}`,\n                            event,\n                        }\n                    )\n                } else {\n                    await this.appMetrics.queueMetric({\n                        teamId: event.teamId,\n                        pluginConfigId: -2, // -2 is hardcoded to mean webhooks\n                        category: 'webhook',\n                        successes: 1,\n                    })\n                }\n            })\n            this.statsd?.increment('webhook_firings', {\n                team_id: event.teamId.toString(),\n            })\n        } catch (error) {\n            await this.appMetrics.queueError(\n                {\n                    teamId: event.teamId,\n                    pluginConfigId: -2, // -2 is hardcoded to mean webhooks\n                    category: 'webhook',\n                    failures: 1,\n                },\n                {\n                    error,\n                    event,\n                }\n            )\n            throw error\n        } finally {\n            clearTimeout(timeout)\n        }\n    }\n\n    public async postRestHook(hook: Hook, event: PostIngestionEvent): Promise<void> {\n        let sendablePerson: Record<string, any> = {}\n        const { person_id, person_created_at, person_properties, ...data } = event\n        if (person_id) {\n            sendablePerson = {\n                uuid: person_id,\n                properties: person_properties,\n                created_at: person_created_at,\n            }\n        }\n\n        const payload = {\n            hook: { id: hook.id, event: hook.event, target: hook.target },\n            data: { ...data, person: sendablePerson },\n        }\n\n        const slowWarningTimeout = this.EXTERNAL_REQUEST_TIMEOUT * 0.7\n        const timeout = setTimeout(() => {\n            status.warn(\n                '\u231b',\n                `Posting RestHook slow. Timeout warning after ${slowWarningTimeout / 1000} sec! url=${\n                    hook.target\n                } team_id=${event.teamId} event_id=${event.eventUuid}`\n            )\n        }, slowWarningTimeout)\n        try {\n            const request = await trackedFetch(hook.target, {\n                method: 'POST',\n                body: JSON.stringify(payload, undefined, 4),\n                headers: { 'Content-Type': 'application/json' },\n                timeout: this.EXTERNAL_REQUEST_TIMEOUT,\n            })\n            if (request.status === 410) {\n                // Delete hook on our side if it's gone on Zapier's\n                await this.deleteRestHook(hook.id)\n            }\n            if (!request.ok) {\n                status.warn('\u26a0\ufe0f', `Rest hook failed status ${request.status} for team ${event.teamId}`)\n                await this.appMetrics.queueError(\n                    {\n                        teamId: event.teamId,\n                        pluginConfigId: -1, // -1 is hardcoded to mean resthooks\n                        category: 'webhook',\n                        failures: 1,\n                    },\n                    {\n                        error: `Request failed with HTTP status ${request.status}`,\n                        event,\n                    }\n                )\n            } else {\n                await this.appMetrics.queueMetric({\n                    teamId: event.teamId,\n                    pluginConfigId: -1, // -1 is hardcoded to mean resthooks\n                    category: 'webhook',\n                    successes: 1,\n                })\n            }\n            this.statsd?.increment('rest_hook_firings')\n        } catch (error) {\n            await this.appMetrics.queueError(\n                {\n                    teamId: event.teamId,\n                    pluginConfigId: -1, // -1 is hardcoded to mean resthooks\n                    category: 'webhook',\n                    failures: 1,\n                },\n                {\n                    error,\n                    event,\n                }\n            )\n            throw error\n        } finally {\n            clearTimeout(timeout)\n        }\n    }\n\n    private async deleteRestHook(hookId: Hook['id']): Promise<void> {\n        await this.postgres.query(\n            PostgresUse.COMMON_WRITE,\n            `DELETE FROM ee_hook WHERE id = $1`,\n            [hookId],\n            'deleteRestHook'\n        )\n    }\n}\n", "import * as bigquery from '@google-cloud/bigquery'\nimport * as pubsub from '@google-cloud/pubsub'\nimport * as gcs from '@google-cloud/storage'\nimport * as contrib from '@posthog/plugin-contrib'\nimport * as scaffold from '@posthog/plugin-scaffold'\nimport * as AWS from 'aws-sdk'\nimport crypto from 'crypto'\nimport * as ethers from 'ethers'\nimport * as faker from 'faker'\nimport * as genericPool from 'generic-pool'\nimport * as jsonwebtoken from 'jsonwebtoken'\nimport * as pg from 'pg'\nimport snowflake from 'snowflake-sdk'\nimport { PassThrough } from 'stream'\nimport * as url from 'url'\nimport * as zlib from 'zlib'\n\nimport { isTestEnv } from '../../utils/env-utils'\nimport { trackedFetch } from '../../utils/fetch'\nimport { writeToFile } from './extensions/test-utils'\n\nexport const AVAILABLE_IMPORTS = {\n    ...(isTestEnv()\n        ? {\n              'test-utils/write-to-file': writeToFile,\n          }\n        : {}),\n    '@google-cloud/bigquery': bigquery,\n    '@google-cloud/pubsub': pubsub,\n    '@google-cloud/storage': gcs,\n    '@posthog/plugin-contrib': contrib,\n    '@posthog/plugin-scaffold': scaffold,\n    'aws-sdk': AWS,\n    ethers: ethers,\n    'generic-pool': genericPool,\n    'node-fetch': trackedFetch,\n    'snowflake-sdk': snowflake,\n    crypto: crypto,\n    jsonwebtoken: jsonwebtoken,\n    faker: faker,\n    pg: pg,\n    stream: { PassThrough },\n    url: url,\n    zlib: zlib,\n}\n", "import { RetryError } from '@posthog/plugin-scaffold'\nimport { randomBytes } from 'crypto'\nimport { VM } from 'vm2'\n\nimport { Hub, PluginConfig, PluginConfigVMResponse } from '../../types'\nimport { createCache } from './extensions/cache'\nimport { createConsole } from './extensions/console'\nimport { createGeoIp } from './extensions/geoip'\nimport { createGoogle } from './extensions/google'\nimport { createJobs } from './extensions/jobs'\nimport { createPosthog } from './extensions/posthog'\nimport { createStorage } from './extensions/storage'\nimport { createUtils } from './extensions/utilities'\nimport { AVAILABLE_IMPORTS } from './imports'\nimport { transformCode } from './transforms'\nimport { upgradeExportEvents } from './upgrades/export-events'\nimport { addHistoricalEventsExportCapability } from './upgrades/historical-export/export-historical-events'\nimport { addHistoricalEventsExportCapabilityV2 } from './upgrades/historical-export/export-historical-events-v2'\n\nexport class TimeoutError extends RetryError {\n    name = 'TimeoutError'\n    caller?: string = undefined\n    pluginConfig?: PluginConfig = undefined\n\n    constructor(message: string, caller?: string, pluginConfig?: PluginConfig) {\n        super(message)\n        this.caller = caller\n        this.pluginConfig = pluginConfig\n    }\n}\n\nexport function createPluginConfigVM(\n    hub: Hub,\n    pluginConfig: PluginConfig, // NB! might have team_id = 0\n    indexJs: string\n): PluginConfigVMResponse {\n    const timer = new Date()\n\n    const statsdTiming = (metric: string) => {\n        hub.statsd?.timing(metric, timer, {\n            pluginConfigId: String(pluginConfig.id),\n            pluginName: String(pluginConfig.plugin?.name),\n            teamId: String(pluginConfig.team_id),\n        })\n    }\n\n    const transformedCode = transformCode(indexJs, hub, AVAILABLE_IMPORTS)\n\n    // Create virtual machine\n    const vm = new VM({\n        timeout: hub.TASK_TIMEOUT * 1000 + 1,\n        sandbox: {},\n    })\n\n    // Add PostHog utilities to virtual machine\n    vm.freeze(createConsole(hub, pluginConfig), 'console')\n    vm.freeze(createPosthog(hub, pluginConfig), 'posthog')\n\n    // Add non-PostHog utilities to virtual machine\n    vm.freeze(AVAILABLE_IMPORTS['node-fetch'], 'fetch')\n    vm.freeze(createGoogle(), 'google')\n\n    vm.freeze(AVAILABLE_IMPORTS, '__pluginHostImports')\n\n    if (process.env.NODE_ENV === 'test') {\n        vm.freeze(setTimeout, '__jestSetTimeout')\n    }\n\n    vm.freeze(RetryError, 'RetryError')\n\n    // Bring some useful globals into scope\n    vm.freeze(URL, 'URL')\n\n    // Creating this outside the vm (so not in a babel plugin for example)\n    // because `setTimeout` is not available inside the vm... and we don't want to\n    // make it available for now, as it makes it easier to create malicious code\n    const asyncGuard = async (promise: Promise<any>, name?: string) => {\n        const timeout = hub.TASK_TIMEOUT\n        return await Promise.race([\n            promise,\n            new Promise((resolve, reject) =>\n                setTimeout(() => {\n                    const message = `Script execution timed out after promise waited for ${timeout} second${\n                        timeout === 1 ? '' : 's'\n                    } (${pluginConfig.plugin?.name}, name: ${name}, pluginConfigId: ${pluginConfig.id})`\n                    reject(new TimeoutError(message, `${name}`, pluginConfig))\n                }, timeout * 1000)\n            ),\n        ])\n    }\n\n    vm.freeze(asyncGuard, '__asyncGuard')\n\n    vm.freeze(\n        {\n            cache: createCache(hub, pluginConfig.plugin_id, pluginConfig.team_id),\n            config: pluginConfig.config,\n            attachments: pluginConfig.attachments,\n            storage: createStorage(hub, pluginConfig),\n            geoip: createGeoIp(hub),\n            jobs: createJobs(hub, pluginConfig),\n            utils: createUtils(hub, pluginConfig.id),\n        },\n        '__pluginHostMeta'\n    )\n\n    vm.run(`\n        // two ways packages could export themselves (plus \"global\")\n        const module = { exports: {} };\n        let exports = {};\n\n        // the plugin JS code\n        ${transformedCode};\n    `)\n\n    // Add a secret hash to the end of some function names, so that we can (sometimes) identify\n    // the crashed plugin if it throws an uncaught exception in a promise.\n    if (!hub.pluginConfigSecrets.has(pluginConfig.id)) {\n        const secret = randomBytes(16).toString('hex')\n        hub.pluginConfigSecrets.set(pluginConfig.id, secret)\n        hub.pluginConfigSecretLookup.set(secret, pluginConfig.id)\n    }\n\n    // Keep the format of this in sync with `pluginConfigIdFromStack` in utils.ts\n    // Only place this after functions whose names match /^__[a-zA-Z0-9]+$/\n    const pluginConfigIdentifier = `__PluginConfig_${pluginConfig.id}_${hub.pluginConfigSecrets.get(pluginConfig.id)}`\n    const responseVar = `__pluginDetails${randomBytes(64).toString('hex')}`\n\n    // Explicitly passing __asyncGuard to the returned function from `vm.run` in order\n    // to make it harder to override the global `__asyncGuard = noop` inside plugins.\n    // This way even if promises inside plugins are unbounded, the `processEvent` function\n    // itself will still terminate after TASK_TIMEOUT seconds, not clogging the entire ingestion.\n    vm.run(`\n        if (typeof global.${responseVar} !== 'undefined') {\n            throw new Error(\"Plugin created variable ${responseVar} that is reserved for the VM.\")\n        }\n        let ${responseVar} = undefined;\n        ((__asyncGuard) => {\n            // where to find exports\n            let exportDestinations = [\n                exports,\n                exports.default,\n                module.exports\n            ].filter(d => typeof d === 'object'); // filters out exports.default if not there\n\n            // add \"global\" only if nothing exported at all\n            if (!exportDestinations.find(d => Object.keys(d).length > 0)) {\n                // we can't set it to just [global], as abstractions may add exports later\n                exportDestinations.push(global)\n            }\n\n            // export helpers\n            function __getExported (key) { return exportDestinations.find(a => a[key])?.[key] };\n            function __asyncFunctionGuard (func, name) {\n                return func ? function __innerAsyncGuard${pluginConfigIdentifier}(...args) { return __asyncGuard(func(...args), name) } : func\n            };\n\n            // inject the meta object + shareable 'global' to the end of each exported function\n            const __pluginMeta = {\n                ...__pluginHostMeta,\n                global: {}\n            };\n            function __bindMeta (keyOrFunc) {\n                const func = typeof keyOrFunc === 'function' ? keyOrFunc : __getExported(keyOrFunc);\n                if (func) return function __inBindMeta${pluginConfigIdentifier} (...args) { return func(...args, __pluginMeta) };\n            }\n            function __callWithMeta (keyOrFunc, ...args) {\n                const func = __bindMeta(keyOrFunc);\n                if (func) return func(...args);\n            }\n\n            // we have processEventBatch, but not processEvent\n            if (!__getExported('processEvent') && __getExported('processEventBatch')) {\n                exports.processEvent = async function __processEvent${pluginConfigIdentifier} (event, meta) {\n                    return (await (__getExported('processEventBatch'))([event], meta))?.[0]\n                }\n            }\n\n            // export various functions\n            const __methods = {\n                setupPlugin: __asyncFunctionGuard(__bindMeta('setupPlugin'), 'setupPlugin'),\n                teardownPlugin: __asyncFunctionGuard(__bindMeta('teardownPlugin'), 'teardownPlugin'),\n                exportEvents: __asyncFunctionGuard(__bindMeta('exportEvents'), 'exportEvents'),\n                onEvent: __asyncFunctionGuard(__bindMeta('onEvent'), 'onEvent'),\n                processEvent: __asyncFunctionGuard(__bindMeta('processEvent'), 'processEvent'),\n                getSettings: __bindMeta('getSettings'),\n            };\n\n            const __tasks = {\n                schedule: {},\n                job: {},\n            };\n\n            for (const exportDestination of exportDestinations.reverse()) {\n                // gather the runEveryX commands and export in __tasks\n                for (const [name, value] of Object.entries(exportDestination)) {\n                    if (name.startsWith(\"runEvery\") && typeof value === 'function') {\n                        __tasks.schedule[name] = {\n                            name: name,\n                            type: 'schedule',\n                            exec: __bindMeta(value)\n                        }\n                    }\n                }\n\n                // gather all jobs\n                if (typeof exportDestination['jobs'] === 'object') {\n                    for (const [key, value] of Object.entries(exportDestination['jobs'])) {\n                        __tasks.job[key] = {\n                            name: key,\n                            type: 'job',\n                            exec: __bindMeta(value)\n                        }\n                    }\n                }\n\n\n            }\n\n            ${responseVar} = { methods: __methods, tasks: __tasks, meta: __pluginMeta, }\n        })\n    `)(asyncGuard)\n\n    const vmResponse = vm.run(responseVar)\n    const { methods, tasks } = vmResponse\n    const exportEventsExists = !!methods.exportEvents\n\n    if (exportEventsExists) {\n        upgradeExportEvents(hub, pluginConfig, vmResponse)\n        statsdTiming('vm_setup_sync_section')\n\n        if (hub.HISTORICAL_EXPORTS_ENABLED) {\n            addHistoricalEventsExportCapability(hub, pluginConfig, vmResponse)\n            addHistoricalEventsExportCapabilityV2(hub, pluginConfig, vmResponse)\n        }\n    } else {\n        statsdTiming('vm_setup_sync_section')\n    }\n\n    statsdTiming('vm_setup_full')\n\n    return {\n        vm,\n        methods,\n        tasks,\n        vmResponseVariable: responseVar,\n    }\n}\n", "import { PluginEvent } from '@posthog/plugin-scaffold'\nimport { DateTime } from 'luxon'\nimport fetch from 'node-fetch'\n\nimport { Hook, Hub } from '../../../../src/types'\nimport { createHub } from '../../../../src/utils/db/hub'\nimport { PostgresUse } from '../../../../src/utils/db/postgres'\nimport { convertToIngestionEvent } from '../../../../src/utils/event'\nimport { UUIDT } from '../../../../src/utils/utils'\nimport { ActionManager } from '../../../../src/worker/ingestion/action-manager'\nimport { ActionMatcher } from '../../../../src/worker/ingestion/action-matcher'\nimport {\n    processOnEventStep,\n    processWebhooksStep,\n} from '../../../../src/worker/ingestion/event-pipeline/runAsyncHandlersStep'\nimport { EventPipelineRunner } from '../../../../src/worker/ingestion/event-pipeline/runner'\nimport { HookCommander } from '../../../../src/worker/ingestion/hooks'\nimport { setupPlugins } from '../../../../src/worker/plugins/setup'\nimport { delayUntilEventIngested, resetTestDatabaseClickhouse } from '../../../helpers/clickhouse'\nimport { commonUserId } from '../../../helpers/plugins'\nimport { insertRow, resetTestDatabase } from '../../../helpers/sql'\n\njest.mock('../../../../src/utils/status')\n\ndescribe('Event Pipeline integration test', () => {\n    let hub: Hub\n    let actionManager: ActionManager\n    let actionMatcher: ActionMatcher\n    let hookCannon: HookCommander\n    let closeServer: () => Promise<void>\n\n    const ingestEvent = async (event: PluginEvent) => {\n        const runner = new EventPipelineRunner(hub, event)\n        const result = await runner.runEventPipeline(event)\n        const postIngestionEvent = convertToIngestionEvent(result.args[0])\n        return Promise.all([\n            processOnEventStep(runner.hub, postIngestionEvent),\n            processWebhooksStep(postIngestionEvent, actionMatcher, hookCannon),\n        ])\n    }\n\n    beforeEach(async () => {\n        await resetTestDatabase()\n        await resetTestDatabaseClickhouse()\n        process.env.SITE_URL = 'https://example.com'\n        ;[hub, closeServer] = await createHub()\n\n        actionManager = new ActionManager(hub.db.postgres)\n        await actionManager.prepare()\n        actionMatcher = new ActionMatcher(hub.db.postgres, actionManager)\n        hookCannon = new HookCommander(\n            hub.db.postgres,\n            hub.teamManager,\n            hub.organizationManager,\n            hub.appMetrics,\n            undefined,\n            hub.EXTERNAL_REQUEST_TIMEOUT_MS\n        )\n\n        jest.spyOn(hub.db, 'fetchPerson')\n        jest.spyOn(hub.db, 'createPerson')\n    })\n\n    afterEach(async () => {\n        await closeServer()\n    })\n\n    it('handles plugins setting properties', async () => {\n        await resetTestDatabase(`\n            function processEvent (event) {\n                event.properties = {\n                    ...event.properties,\n                    $browser: 'Chrome',\n                    processed: 'hell yes'\n                }\n                event.$set = {\n                    ...event.$set,\n                    personProp: 'value'\n                }\n                return event\n            }\n        `)\n        await setupPlugins(hub)\n\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            $set: { personProp: 1, anotherValue: 2 },\n            timestamp: new Date().toISOString(),\n            now: new Date().toISOString(),\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'https://example.com',\n            uuid: new UUIDT().toString(),\n        }\n\n        await ingestEvent(event)\n\n        const events = await delayUntilEventIngested(() => hub.db.fetchEvents())\n        const persons = await delayUntilEventIngested(() => hub.db.fetchPersons())\n\n        expect(events.length).toEqual(1)\n        expect(events[0]).toEqual(\n            expect.objectContaining({\n                uuid: event.uuid,\n                event: 'xyz',\n                team_id: 2,\n                timestamp: DateTime.fromISO(event.timestamp!, { zone: 'utc' }),\n                // :KLUDGE: Ignore properties like $plugins_succeeded, etc\n                properties: expect.objectContaining({\n                    foo: 'bar',\n                    $browser: 'Chrome',\n                    processed: 'hell yes',\n                    $set: {\n                        personProp: 'value',\n                        anotherValue: 2,\n                        $browser: 'Chrome',\n                    },\n                    $set_once: {\n                        $initial_browser: 'Chrome',\n                    },\n                }),\n            })\n        )\n\n        expect(persons.length).toEqual(1)\n        expect(persons[0].version).toEqual(0)\n        expect(persons[0].properties).toEqual({\n            $creator_event_uuid: event.uuid,\n            $initial_browser: 'Chrome',\n            $browser: 'Chrome',\n            personProp: 'value',\n            anotherValue: 2,\n        })\n    })\n\n    it('fires a webhook', async () => {\n        await hub.db.postgres.query(\n            PostgresUse.COMMON_WRITE,\n            `UPDATE posthog_team SET slack_incoming_webhook = 'https://webhook.example.com/'`,\n            [],\n            'testTag'\n        )\n\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            timestamp: new Date().toISOString(),\n            now: new Date().toISOString(),\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'not-used-anymore',\n            uuid: new UUIDT().toString(),\n        }\n        await actionManager.reloadAllActions()\n\n        await ingestEvent(event)\n\n        const expectedPayload = {\n            text: '[Test Action](https://example.com/action/69) was triggered by [abc](https://example.com/person/abc)',\n        }\n\n        expect(fetch).toHaveBeenCalledWith('https://webhook.example.com/', {\n            body: JSON.stringify(expectedPayload, undefined, 4),\n            headers: { 'Content-Type': 'application/json' },\n            method: 'POST',\n            timeout: 10000,\n        })\n    })\n\n    it('fires a REST hook', async () => {\n        const timestamp = new Date().toISOString()\n\n        await hub.db.postgres.query(\n            PostgresUse.COMMON_WRITE,\n            `UPDATE posthog_organization\n             SET available_features = '{\"zapier\"}'`,\n            [],\n            'testTag'\n        )\n        await insertRow(hub.db.postgres, 'ee_hook', {\n            id: 'abc',\n            team_id: 2,\n            user_id: commonUserId,\n            resource_id: 69,\n            event: 'action_performed',\n            target: 'https://example.com/',\n            created: timestamp,\n            updated: timestamp,\n        } as Hook)\n\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            timestamp: timestamp,\n            now: timestamp,\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'https://example.com',\n            uuid: new UUIDT().toString(),\n        }\n        await actionManager.reloadAllActions()\n\n        await ingestEvent(event)\n\n        const expectedPayload = {\n            hook: {\n                id: 'abc',\n                event: 'action_performed',\n                target: 'https://example.com/',\n            },\n            data: {\n                event: 'xyz',\n                properties: {\n                    foo: 'bar',\n                },\n                eventUuid: expect.any(String),\n                timestamp,\n                teamId: 2,\n                distinctId: 'abc',\n                elementsList: [],\n                person: {\n                    created_at: expect.any(String),\n                    properties: {\n                        $creator_event_uuid: event.uuid,\n                    },\n                    uuid: expect.any(String),\n                },\n            },\n        }\n\n        // Using a more verbose way instead of toHaveBeenCalledWith because we need to parse request body\n        // and use expect.any for a few payload properties, which wouldn't be possible in a simpler way\n        expect(jest.mocked(fetch).mock.calls[0][0]).toBe('https://example.com/')\n        const secondArg = jest.mocked(fetch).mock.calls[0][1]\n        expect(JSON.parse(secondArg!.body as unknown as string)).toStrictEqual(expectedPayload)\n        expect(JSON.parse(secondArg!.body as unknown as string)).toStrictEqual(expectedPayload)\n        expect(secondArg!.headers).toStrictEqual({ 'Content-Type': 'application/json' })\n        expect(secondArg!.method).toBe('POST')\n    })\n\n    it('single postgres action per run to create or load person', async () => {\n        const event: PluginEvent = {\n            event: 'xyz',\n            properties: { foo: 'bar' },\n            timestamp: new Date().toISOString(),\n            now: new Date().toISOString(),\n            team_id: 2,\n            distinct_id: 'abc',\n            ip: null,\n            site_url: 'https://example.com',\n            uuid: new UUIDT().toString(),\n        }\n\n        await new EventPipelineRunner(hub, event).runEventPipeline(event)\n\n        expect(hub.db.fetchPerson).toHaveBeenCalledTimes(1) // we query before creating\n        expect(hub.db.createPerson).toHaveBeenCalledTimes(1)\n\n        // second time single fetch\n        await new EventPipelineRunner(hub, event).runEventPipeline(event)\n        expect(hub.db.fetchPerson).toHaveBeenCalledTimes(2)\n    })\n})\n", "import { DateTime } from 'luxon'\nimport fetch, { FetchError } from 'node-fetch'\n\nimport { Action, PostIngestionEvent, Team } from '../../../src/types'\nimport { UUIDT } from '../../../src/utils/utils'\nimport { AppMetrics } from '../../../src/worker/ingestion/app-metrics'\nimport {\n    determineWebhookType,\n    getActionDetails,\n    getFormattedMessage,\n    getPersonDetails,\n    getTokens,\n    getValueOfToken,\n    HookCommander,\n    WebhookType,\n} from '../../../src/worker/ingestion/hooks'\nimport { Hook } from './../../../src/types'\n\ndescribe('hooks', () => {\n    beforeEach(() => {\n        process.env.NODE_ENV = 'test'\n    })\n\n    describe('determineWebhookType', () => {\n        test('Slack', () => {\n            const webhookType = determineWebhookType('https://hooks.slack.com/services/')\n\n            expect(webhookType).toBe(WebhookType.Slack)\n        })\n\n        test('Discord', () => {\n            const webhookType = determineWebhookType('https://discord.com/api/webhooks/')\n\n            expect(webhookType).toBe(WebhookType.Discord)\n        })\n\n        test('Teams', () => {\n            const webhookType = determineWebhookType('https://outlook.office.com/webhook/')\n\n            expect(webhookType).toBe(WebhookType.Teams)\n        })\n    })\n\n    describe('getPersonDetails', () => {\n        const event = {\n            distinctId: 'WALL-E',\n            person_properties: { email: 'test@posthog.com' },\n        } as unknown as PostIngestionEvent\n        const team = { person_display_name_properties: null } as Team\n\n        test('Slack', () => {\n            const [userDetails, userDetailsMarkdown] = getPersonDetails(\n                event,\n                'http://localhost:8000',\n                WebhookType.Slack,\n                team\n            )\n\n            expect(userDetails).toBe('test@posthog.com')\n            expect(userDetailsMarkdown).toBe('<http://localhost:8000/person/WALL-E|test@posthog.com>')\n        })\n\n        test('Teams', () => {\n            const [userDetails, userDetailsMarkdown] = getPersonDetails(\n                event,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                team\n            )\n\n            expect(userDetails).toBe('test@posthog.com')\n            expect(userDetailsMarkdown).toBe('[test@posthog.com](http://localhost:8000/person/WALL-E)')\n        })\n    })\n\n    describe('getActionDetails', () => {\n        const action = { id: 1, name: 'action1' } as Action\n\n        test('Slack', () => {\n            const [actionDetails, actionDetailsMarkdown] = getActionDetails(\n                action,\n                'http://localhost:8000',\n                WebhookType.Slack\n            )\n\n            expect(actionDetails).toBe('action1')\n            expect(actionDetailsMarkdown).toBe('<http://localhost:8000/action/1|action1>')\n        })\n\n        test('Teams', () => {\n            const [actionDetails, actionDetailsMarkdown] = getActionDetails(\n                action,\n                'http://localhost:8000',\n                WebhookType.Teams\n            )\n\n            expect(actionDetails).toBe('action1')\n            expect(actionDetailsMarkdown).toBe('[action1](http://localhost:8000/action/1)')\n        })\n    })\n\n    describe('getTokens', () => {\n        test('works', () => {\n            const format = '[action.name] got done by [user.name]'\n\n            const [matchedTokens, tokenisedMessage] = getTokens(format)\n\n            expect(matchedTokens).toStrictEqual(['action.name', 'user.name'])\n            expect(tokenisedMessage).toBe('%s got done by %s')\n        })\n\n        test('allows escaping brackets', () => {\n            const format = '[action.name\\\\] got done by \\\\[user.name\\\\]' // just one of the brackets has to be escaped\n\n            const [matchedTokens, tokenisedMessage] = getTokens(format)\n\n            expect(matchedTokens).toStrictEqual([])\n            expect(tokenisedMessage).toBe('[action.name] got done by [user.name]')\n        })\n    })\n\n    describe('getValueOfToken', () => {\n        const action = { id: 1, name: 'action1' } as Action\n        const event = {\n            eventUuid: '123',\n            event: '$pageview',\n            distinctId: 'WALL-E',\n            properties: { $browser: 'Chrome' },\n            person_properties: { enjoys_broccoli_on_pizza: false },\n            timestamp: '2021-10-31T00:44:00.000Z',\n        } as unknown as PostIngestionEvent\n        const team = { person_display_name_properties: null } as Team\n\n        test('event', () => {\n            const tokenUserName = ['event']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('$pageview')\n            expect(markdown).toBe('[$pageview](http://localhost:8000/events/123/2021-10-31T00%3A44%3A00.000Z)')\n        })\n\n        test('event UUID', () => {\n            const tokenUserName = ['event', 'uuid']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('123')\n            expect(markdown).toBe('123')\n        })\n\n        test('event name', () => {\n            const tokenUserName = ['event', 'name']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('$pageview')\n            expect(markdown).toBe('$pageview')\n        })\n\n        test('event event', () => {\n            const tokenUserName = ['event', 'event']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('$pageview')\n            expect(markdown).toBe('$pageview')\n        })\n\n        test('event distinct_id', () => {\n            const tokenUserName = ['event', 'distinct_id']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('WALL-E')\n            expect(markdown).toBe('WALL-E')\n        })\n\n        test('person with just distinct ID', () => {\n            const tokenUserName = ['person']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('WALL-E')\n            expect(markdown).toBe('[WALL-E](http://localhost:8000/person/WALL-E)')\n        })\n\n        test('person with email', () => {\n            const tokenUserName = ['person']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                { ...event, person_properties: { ...event.person_properties, email: 'wall-e@buynlarge.com' } },\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('wall-e@buynlarge.com')\n            expect(markdown).toBe('[wall-e@buynlarge.com](http://localhost:8000/person/WALL-E)')\n        })\n\n        test('person with custom name property, team-level setting ', () => {\n            const tokenUserName = ['person']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                {\n                    ...event,\n                    person_properties: {\n                        ...event.person_properties,\n                        imi\u0119: 'Grzegorz',\n                        nazwisko: 'Brz\u0119czyszczykiewicz',\n                    },\n                    distinctId: 'fd',\n                },\n                { ...team, person_display_name_properties: ['nazwisko'] },\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('Brz\u0119czyszczykiewicz')\n            expect(markdown).toBe('[Brz\u0119czyszczykiewicz](http://localhost:8000/person/fd)')\n        })\n\n        test('person prop', () => {\n            const tokenUserPropString = ['person', 'properties', 'enjoys_broccoli_on_pizza']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('false')\n            expect(markdown).toBe('false')\n        })\n\n        test('person prop nested', () => {\n            const tokenUserPropString = ['person', 'properties', 'pizza_ingredient_scores', 'broccoli']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                {\n                    ...event,\n                    person_properties: {\n                        ...event.person_properties,\n                        pizza_ingredient_scores: { broccoli: 5, pineapple: 9, aubergine: 0 },\n                    },\n                },\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('5')\n            expect(markdown).toBe('5')\n        })\n\n        test('person prop non-primitive', () => {\n            const tokenUserPropString = ['person', 'properties', 'pizza_ingredient_ranking']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                {\n                    ...event,\n                    person_properties: {\n                        ...event.person_properties,\n                        pizza_ingredient_ranking: ['pineapple', 'broccoli', 'aubergine'],\n                    },\n                },\n                team,\n                'http://localhost:8000',\n                WebhookType.Slack,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('[\"pineapple\",\"broccoli\",\"aubergine\"]')\n            expect(markdown).toBe('[\"pineapple\",\"broccoli\",\"aubergine\"]')\n        })\n\n        test('user name (alias for person name)', () => {\n            const tokenUserName = ['user', 'name']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserName\n            )\n\n            expect(text).toBe('WALL-E')\n            expect(markdown).toBe('[WALL-E](http://localhost:8000/person/WALL-E)')\n        })\n\n        test('user prop (actually event prop)', () => {\n            const tokenUserPropString = ['user', 'browser']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropString\n            )\n\n            expect(text).toBe('Chrome')\n            expect(markdown).toBe('Chrome')\n        })\n\n        test('user prop but missing', () => {\n            const tokenUserPropMissing = ['user', 'missing_property']\n\n            const [text, markdown] = getValueOfToken(\n                action,\n                event,\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                tokenUserPropMissing\n            )\n\n            expect(text).toBe('undefined')\n            expect(markdown).toBe('undefined')\n        })\n\n        test('escapes slack', () => {\n            const [text, markdown] = getValueOfToken(\n                action,\n                { ...event, eventUuid: '**>)', event: 'text><new link' },\n                team,\n                'http://localhost:8000',\n                WebhookType.Slack,\n                ['event']\n            )\n\n            expect(text).toBe('text&gt;&lt;new link')\n            expect(markdown).toBe(\n                '<http://localhost:8000/events/**%3E)/2021-10-31T00%3A44%3A00.000Z|text&gt;&lt;new link>'\n            )\n        })\n\n        test('escapes teams', () => {\n            const [text, markdown] = getValueOfToken(\n                action,\n                { ...event, eventUuid: '**)', event: 'text](yes!), [new link' },\n                team,\n                'http://localhost:8000',\n                WebhookType.Teams,\n                ['event']\n            )\n\n            expect(text).toBe('text\\\\]\\\\(yes\\\\!\\\\), \\\\[new link')\n            expect(markdown).toBe(\n                '[text\\\\]\\\\(yes\\\\!\\\\), \\\\[new link](http://localhost:8000/events/\\\\*\\\\*\\\\)/2021-10-31T00%3A44%3A00.000Z)'\n            )\n        })\n    })\n\n    describe('getFormattedMessage', () => {\n        const event = {\n            distinctId: '2',\n            properties: { $browser: 'Chrome', page_title: 'Pricing', 'with space': 'yes sir' },\n        } as unknown as PostIngestionEvent\n        const team = { person_display_name_properties: null } as Team\n\n        test('custom format', () => {\n            const action = {\n                id: 1,\n                name: 'action1',\n                slack_message_format:\n                    '[user.name] from [user.browser] on [event.properties.page_title] page with [event.properties.fruit], [event.properties.with space]',\n            } as Action\n\n            const [text, markdown] = getFormattedMessage(\n                action,\n                event,\n                team,\n                'https://localhost:8000',\n                WebhookType.Slack\n            )\n            expect(text).toBe('2 from Chrome on Pricing page with undefined, yes sir')\n            expect(markdown).toBe(\n                '<https://localhost:8000/person/2|2> from Chrome on Pricing page with undefined, yes sir'\n            )\n        })\n\n        test('default format', () => {\n            const action = { id: 1, name: 'action1', slack_message_format: '' } as Action\n\n            const [text, markdown] = getFormattedMessage(\n                action,\n                event,\n                team,\n                'https://localhost:8000',\n                WebhookType.Slack\n            )\n            expect(text).toBe('action1 was triggered by 2')\n            expect(markdown).toBe(\n                '<https://localhost:8000/action/1|action1> was triggered by <https://localhost:8000/person/2|2>'\n            )\n        })\n\n        test('not quite correct format', () => {\n            const action = {\n                id: 1,\n                name: 'action1',\n                slack_message_format: '[user.name] did thing from browser [user.brauzer]',\n            } as Action\n\n            const [text, markdown] = getFormattedMessage(\n                action,\n                event,\n                team,\n                'https://localhost:8000',\n                WebhookType.Slack\n            )\n            expect(text).toBe('2 did thing from browser undefined')\n            expect(markdown).toBe('<https://localhost:8000/person/2|2> did thing from browser undefined')\n        })\n    })\n\n    describe('postRestHook', () => {\n        let hookCommander: HookCommander\n        let hook: Hook\n\n        beforeEach(() => {\n            hook = {\n                id: 'id',\n                team_id: 1,\n                user_id: 1,\n                resource_id: 1,\n                event: 'foo',\n                target: 'https://example.com/',\n                created: new Date().toISOString(),\n                updated: new Date().toISOString(),\n            }\n            hookCommander = new HookCommander(\n                {} as any,\n                {} as any,\n                {} as any,\n                { queueError: () => Promise.resolve(), queueMetric: () => Promise.resolve() } as unknown as AppMetrics,\n                undefined,\n                20000\n            )\n        })\n\n        test('person = undefined', async () => {\n            await hookCommander.postRestHook(hook, { event: 'foo' } as any)\n\n            expect(fetch).toHaveBeenCalledWith('https://example.com/', {\n                body: JSON.stringify(\n                    {\n                        hook: {\n                            id: 'id',\n                            event: 'foo',\n                            target: 'https://example.com/',\n                        },\n                        data: {\n                            event: 'foo',\n                            person: {}, // person becomes empty object if undefined\n                        },\n                    },\n                    undefined,\n                    4\n                ),\n                headers: { 'Content-Type': 'application/json' },\n                method: 'POST',\n                timeout: 20000,\n            })\n        })\n\n        test('person data from the event', async () => {\n            const now = new Date().toISOString()\n            const uuid = new UUIDT().toString()\n            await hookCommander.postRestHook(hook, {\n                event: 'foo',\n                teamId: hook.team_id,\n                person_id: uuid,\n                person_properties: { foo: 'bar' },\n                person_created_at: DateTime.fromISO(now).toUTC(),\n            } as any)\n            expect(fetch).toHaveBeenCalledWith('https://example.com/', {\n                body: JSON.stringify(\n                    {\n                        hook: {\n                            id: 'id',\n                            event: 'foo',\n                            target: 'https://example.com/',\n                        },\n                        data: {\n                            event: 'foo',\n                            teamId: hook.team_id,\n                            person: {\n                                uuid: uuid,\n                                properties: { foo: 'bar' },\n                                created_at: now,\n                            },\n                        },\n                    },\n                    undefined,\n                    4\n                ),\n                headers: { 'Content-Type': 'application/json' },\n                method: 'POST',\n                timeout: 20000,\n            })\n        })\n\n        test('private IP hook forbidden in prod', async () => {\n            process.env.NODE_ENV = 'production'\n\n            await expect(\n                hookCommander.postRestHook({ ...hook, target: 'http://127.0.0.1' }, { event: 'foo' } as any)\n            ).rejects.toThrow(new FetchError('Internal hostname', 'posthog-host-guard'))\n        })\n    })\n})\n", "import json\nimport os\nimport secrets\nimport urllib.parse\nfrom base64 import b32encode\nfrom binascii import unhexlify\nfrom typing import Any, Optional, cast\nimport requests\nfrom django.conf import settings\nfrom django.contrib.auth import login, update_session_auth_hash\nfrom django.contrib.auth.password_validation import validate_password\nfrom django.core.exceptions import ValidationError\nfrom django.http import HttpResponse, JsonResponse\nfrom django.shortcuts import redirect\nfrom django.utils import timezone\nfrom django.views.decorators.http import require_http_methods\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom django_otp import login as otp_login\nfrom django_otp.util import random_hex\nfrom loginas.utils import is_impersonated_session\nfrom rest_framework import exceptions, mixins, permissions, serializers, viewsets\nfrom rest_framework.decorators import action\nfrom rest_framework.exceptions import NotFound\nfrom rest_framework.permissions import AllowAny\nfrom rest_framework.response import Response\nfrom rest_framework.throttling import UserRateThrottle\nfrom two_factor.forms import TOTPDeviceForm\nfrom two_factor.utils import default_device\n\nfrom posthog.api.decide import hostname_in_allowed_url_list\nfrom posthog.api.email_verification import EmailVerifier\nfrom posthog.api.organization import OrganizationSerializer\nfrom posthog.api.shared import OrganizationBasicSerializer, TeamBasicSerializer\nfrom posthog.api.utils import raise_if_user_provided_url_unsafe\nfrom posthog.auth import authenticate_secondarily\nfrom posthog.email import is_email_available\nfrom posthog.event_usage import (\n    report_user_logged_in,\n    report_user_updated,\n    report_user_verified_email,\n)\nfrom posthog.models import Team, User, UserScenePersonalisation, Dashboard\nfrom posthog.models.organization import Organization\nfrom posthog.models.user import NOTIFICATION_DEFAULTS, Notifications\nfrom posthog.tasks import user_identify\nfrom posthog.tasks.email import send_email_change_emails\nfrom posthog.user_permissions import UserPermissions\nfrom posthog.utils import get_js_url\n\n\nclass UserAuthenticationThrottle(UserRateThrottle):\n    rate = \"5/minute\"\n\n    def allow_request(self, request, view):\n        # only throttle non-GET requests\n        if request.method == \"GET\":\n            return True\n        return super().allow_request(request, view)\n\n\nclass UserEmailVerificationThrottle(UserRateThrottle):\n    rate = \"6/day\"\n\n\nclass ScenePersonalisationBasicSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserScenePersonalisation\n        fields = [\"scene\", \"dashboard\"]\n\n\nclass UserSerializer(serializers.ModelSerializer):\n    has_password = serializers.SerializerMethodField()\n    is_impersonated = serializers.SerializerMethodField()\n    is_2fa_enabled = serializers.SerializerMethodField()\n    has_social_auth = serializers.SerializerMethodField()\n    team = TeamBasicSerializer(read_only=True)\n    organization = OrganizationSerializer(read_only=True)\n    organizations = OrganizationBasicSerializer(many=True, read_only=True)\n    set_current_organization = serializers.CharField(write_only=True, required=False)\n    set_current_team = serializers.CharField(write_only=True, required=False)\n    current_password = serializers.CharField(write_only=True, required=False)\n    notification_settings = serializers.DictField(required=False)\n    scene_personalisation = ScenePersonalisationBasicSerializer(many=True, read_only=True)\n\n    class Meta:\n        model = User\n        fields = [\n            \"date_joined\",\n            \"uuid\",\n            \"distinct_id\",\n            \"first_name\",\n            \"email\",\n            \"pending_email\",\n            \"email_opt_in\",\n            \"is_email_verified\",\n            \"pending_email\",\n            \"notification_settings\",\n            \"anonymize_data\",\n            \"toolbar_mode\",\n            \"has_password\",\n            \"is_staff\",\n            \"is_impersonated\",\n            \"team\",\n            \"organization\",\n            \"organizations\",\n            \"set_current_organization\",\n            \"set_current_team\",\n            \"password\",\n            \"current_password\",  # used when changing current password\n            \"events_column_config\",\n            \"is_2fa_enabled\",\n            \"has_social_auth\",\n            \"has_seen_product_intro_for\",\n            \"scene_personalisation\",\n        ]\n        extra_kwargs = {\n            \"date_joined\": {\"read_only\": True},\n            \"password\": {\"write_only\": True},\n        }\n\n    def get_has_password(self, instance: User) -> bool:\n        return instance.has_usable_password()\n\n    def get_is_impersonated(self, _) -> Optional[bool]:\n        if \"request\" not in self.context:\n            return None\n        return is_impersonated_session(self.context[\"request\"])\n\n    def get_has_social_auth(self, instance: User) -> bool:\n        return instance.social_auth.exists()  # type: ignore\n\n    def get_is_2fa_enabled(self, instance: User) -> bool:\n        return default_device(instance) is not None\n\n    def validate_set_current_organization(self, value: str) -> Organization:\n        try:\n            organization = Organization.objects.get(id=value)\n            if organization.memberships.filter(user=self.context[\"request\"].user).exists():\n                return organization\n        except Organization.DoesNotExist:\n            pass\n\n        raise serializers.ValidationError(f\"Object with id={value} does not exist.\", code=\"does_not_exist\")\n\n    def validate_set_current_team(self, value: str) -> Team:\n        try:\n            team = Team.objects.get(pk=value)\n            if self.context[\"request\"].user.teams.filter(pk=team.pk).exists():\n                return team\n        except Team.DoesNotExist:\n            pass\n\n        raise serializers.ValidationError(f\"Object with id={value} does not exist.\", code=\"does_not_exist\")\n\n    def validate_notification_settings(self, notification_settings: Notifications) -> Notifications:\n        for key, value in notification_settings.items():\n            if key not in Notifications.__annotations__:\n                raise serializers.ValidationError(f\"Key {key} is not valid as a key for notification settings\")\n\n            if not isinstance(value, Notifications.__annotations__[key]):\n                raise serializers.ValidationError(\n                    f\"{value} is not a valid type for notification settings, should be {Notifications.__annotations__[key]}\"\n                )\n        return {**NOTIFICATION_DEFAULTS, **notification_settings}  # type: ignore\n\n    def validate_password_change(\n        self, instance: User, current_password: Optional[str], password: Optional[str]\n    ) -> Optional[str]:\n        if password:\n            if instance.password and instance.has_usable_password():\n                # If user has a password set, we check it's provided to allow updating it. We need to check that is both\n                # usable (properly hashed) and that a password actually exists.\n                if not current_password:\n                    raise serializers.ValidationError(\n                        {\"current_password\": [\"This field is required when updating your password.\"]},\n                        code=\"required\",\n                    )\n\n                if not instance.check_password(current_password):\n                    raise serializers.ValidationError(\n                        {\"current_password\": [\"Your current password is incorrect.\"]},\n                        code=\"incorrect_password\",\n                    )\n            try:\n                validate_password(password, instance)\n            except ValidationError as e:\n                raise serializers.ValidationError({\"password\": e.messages})\n\n        return password\n\n    def validate_is_staff(self, value: bool) -> bool:\n        if not self.context[\"request\"].user.is_staff:\n            raise exceptions.PermissionDenied(\"You are not a staff user, contact your instance admin.\")\n        return value\n\n    def update(self, instance: \"User\", validated_data: Any) -> Any:\n        # Update current_organization and current_team\n        current_organization = validated_data.pop(\"set_current_organization\", None)\n        current_team = validated_data.pop(\"set_current_team\", None)\n        if current_organization:\n            if current_team and not current_organization.teams.filter(pk=current_team.pk).exists():\n                raise serializers.ValidationError(\n                    {\"set_current_team\": [\"Team must belong to the same organization in set_current_organization.\"]}\n                )\n\n            validated_data[\"current_organization\"] = current_organization\n            validated_data[\"current_team\"] = current_team if current_team else current_organization.teams.first()\n        elif current_team:\n            validated_data[\"current_team\"] = current_team\n            validated_data[\"current_organization\"] = current_team.organization\n\n        if (\n            \"email\" in validated_data\n            and validated_data[\"email\"].lower() != instance.email.lower()\n            and is_email_available()\n        ):\n            instance.pending_email = validated_data.pop(\"email\", None)\n            instance.save()\n            EmailVerifier.create_token_and_send_email_verification(instance)\n\n        # Update password\n        current_password = validated_data.pop(\"current_password\", None)\n        password = self.validate_password_change(\n            cast(User, instance), current_password, validated_data.pop(\"password\", None)\n        )\n\n        if validated_data.get(\"notification_settings\"):\n            validated_data[\"partial_notification_settings\"] = validated_data.pop(\"notification_settings\")\n\n        updated_attrs = list(validated_data.keys())\n        instance = cast(User, super().update(instance, validated_data))\n\n        if password:\n            instance.set_password(password)\n            instance.save()\n            update_session_auth_hash(self.context[\"request\"], instance)\n            updated_attrs.append(\"password\")\n\n        report_user_updated(instance, updated_attrs)\n\n        return instance\n\n    def to_representation(self, instance: Any) -> Any:\n        user_identify.identify_task.delay(user_id=instance.id)\n        return super().to_representation(instance)\n\n\nclass ScenePersonalisationSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = UserScenePersonalisation\n        fields = [\"scene\", \"dashboard\"]\n        read_only_fields = [\"user\", \"team\"]\n\n    def validate_dashboard(self, value: Dashboard) -> Dashboard:\n        instance = cast(User, self.instance)\n\n        if value.team != instance.current_team:\n            raise serializers.ValidationError(\"Dashboard must belong to the user's current team.\")\n\n        return value\n\n    def validate(self, data):\n        if \"dashboard\" not in data:\n            raise serializers.ValidationError(\"Dashboard must be provided.\")\n\n        if \"scene\" not in data:\n            raise serializers.ValidationError(\"Scene must be provided.\")\n\n        return data\n\n    def save(self, **kwargs):\n        instance = cast(User, self.instance)\n        if not instance:\n            # there must always be a user instance\n            raise NotFound()\n\n        validated_data = {**self.validated_data, **kwargs}\n\n        return UserScenePersonalisation.objects.update_or_create(\n            user=instance,\n            team=instance.current_team,\n            scene=validated_data[\"scene\"],\n            defaults={\"dashboard\": validated_data[\"dashboard\"]},\n        )\n\n\nclass UserViewSet(\n    mixins.RetrieveModelMixin,\n    mixins.UpdateModelMixin,\n    mixins.ListModelMixin,\n    viewsets.GenericViewSet,\n):\n    throttle_classes = [UserAuthenticationThrottle]\n    serializer_class = UserSerializer\n    permission_classes = [permissions.IsAuthenticated]\n    filter_backends = [DjangoFilterBackend]\n    filterset_fields = [\"is_staff\"]\n    queryset = User.objects.filter(is_active=True)\n    lookup_field = \"uuid\"\n\n    def get_object(self) -> User:\n        lookup_value = self.kwargs[self.lookup_field]\n        request_user = cast(User, self.request.user)  # Must be authenticated to access this endpoint\n        if lookup_value == \"@me\":\n            return request_user\n\n        if not request_user.is_staff:\n            raise exceptions.PermissionDenied(\n                \"As a non-staff user you're only allowed to access the `@me` user instance.\"\n            )\n\n        return super().get_object()\n\n    def get_queryset(self):\n        queryset = super().get_queryset()\n        if not self.request.user.is_staff:\n            queryset = queryset.filter(id=self.request.user.id)\n        return queryset\n\n    def get_serializer_context(self):\n        return {\n            **super().get_serializer_context(),\n            \"user_permissions\": UserPermissions(cast(User, self.request.user)),\n        }\n\n    @action(methods=[\"GET\"], detail=True)\n    def start_2fa_setup(self, request, **kwargs):\n        key = random_hex(20)\n        self.request.session[\"django_two_factor-hex\"] = key\n        rawkey = unhexlify(key.encode(\"ascii\"))\n        b32key = b32encode(rawkey).decode(\"utf-8\")\n        self.request.session[\"django_two_factor-qr_secret_key\"] = b32key\n        return Response({\"success\": True})\n\n    @action(methods=[\"POST\"], detail=True)\n    def validate_2fa(self, request, **kwargs):\n        form = TOTPDeviceForm(\n            request.session[\"django_two_factor-hex\"],\n            request.user,\n            data={\"token\": request.data[\"token\"]},\n        )\n        if not form.is_valid():\n            raise serializers.ValidationError(\"Token is not valid\", code=\"token_invalid\")\n        form.save()\n        otp_login(request, default_device(request.user))\n        return Response({\"success\": True})\n\n    @action(methods=[\"POST\"], detail=True, permission_classes=[AllowAny])\n    def verify_email(self, request, **kwargs):\n        token = request.data[\"token\"] if \"token\" in request.data else None\n        user_uuid = request.data[\"uuid\"]\n        if not token:\n            raise serializers.ValidationError({\"token\": [\"This field is required.\"]}, code=\"required\")\n\n        # Special handling for E2E tests\n        if settings.E2E_TESTING and user_uuid == \"e2e_test_user\" and token == \"e2e_test_token\":\n            return {\"success\": True, \"token\": token}\n\n        try:\n            user: Optional[User] = User.objects.filter(is_active=True).get(uuid=user_uuid)\n        except User.DoesNotExist:\n            user = None\n\n        if not user or not EmailVerifier.check_token(user, token):\n            raise serializers.ValidationError(\n                {\"token\": [\"This verification token is invalid or has expired.\"]},\n                code=\"invalid_token\",\n            )\n\n        if user.pending_email:\n            old_email = user.email\n            user.email = user.pending_email\n            user.pending_email = None\n            user.save()\n            send_email_change_emails.delay(timezone.now().isoformat(), user.first_name, old_email, user.email)\n\n        user.is_email_verified = True\n        user.save()\n        report_user_verified_email(user)\n\n        login(self.request, user, backend=\"django.contrib.auth.backends.ModelBackend\")\n        report_user_logged_in(user)\n        return Response({\"success\": True, \"token\": token})\n\n    @action(\n        methods=[\"POST\"],\n        detail=True,\n        permission_classes=[AllowAny],\n        throttle_classes=[UserEmailVerificationThrottle],\n    )\n    def request_email_verification(self, request, **kwargs):\n        uuid = request.data[\"uuid\"]\n        if not is_email_available():\n            raise serializers.ValidationError(\n                \"Cannot verify email address because email is not configured for your instance. Please contact your administrator.\",\n                code=\"email_not_available\",\n            )\n        try:\n            user = User.objects.filter(is_active=True).get(uuid=uuid)\n        except User.DoesNotExist:\n            user = None\n        if user:\n            EmailVerifier.create_token_and_send_email_verification(user)\n\n        return Response({\"success\": True})\n\n    @action(methods=[\"POST\"], detail=True)\n    def scene_personalisation(self, request, **kwargs):\n        instance = self.get_object()\n        request_serializer = ScenePersonalisationSerializer(instance=instance, data=request.data, partial=True)\n        request_serializer.is_valid(raise_exception=True)\n\n        request_serializer.save()\n        instance.refresh_from_db()\n\n        return Response(self.get_serializer(instance=instance).data)\n\n\n@authenticate_secondarily\ndef redirect_to_site(request):\n    team = request.user.team\n    app_url = request.GET.get(\"appUrl\") or (team.app_urls and team.app_urls[0])\n\n    if not app_url:\n        return HttpResponse(status=404)\n\n    if not team or not hostname_in_allowed_url_list(team.app_urls, urllib.parse.urlparse(app_url).hostname):\n        return HttpResponse(f\"Can only redirect to a permitted domain.\", status=403)\n    request.user.temporary_token = secrets.token_urlsafe(32)\n    request.user.save()\n    params = {\n        \"action\": \"ph_authorize\",\n        \"token\": team.api_token,\n        \"temporaryToken\": request.user.temporary_token,\n        \"actionId\": request.GET.get(\"actionId\"),\n        \"userIntent\": request.GET.get(\"userIntent\"),\n        \"toolbarVersion\": \"toolbar\",\n        \"apiURL\": request.build_absolute_uri(\"/\")[:-1],\n        \"dataAttributes\": team.data_attributes,\n    }\n\n    if get_js_url(request):\n        params[\"jsURL\"] = get_js_url(request)\n\n    if not settings.TEST and not os.environ.get(\"OPT_OUT_CAPTURE\"):\n        params[\"instrument\"] = True\n        params[\"userEmail\"] = request.user.email\n        params[\"distinctId\"] = request.user.distinct_id\n\n    # pass the empty string as the safe param so that `//` is encoded correctly.\n    # see https://github.com/PostHog/posthog/issues/9671\n    state = urllib.parse.quote(json.dumps(params), safe=\"\")\n\n    return redirect(\"{}#__posthog={}\".format(app_url, state))\n\n\n@require_http_methods([\"POST\"])\n@authenticate_secondarily\ndef test_slack_webhook(request):\n    \"\"\"Test webhook.\"\"\"\n    try:\n        body = json.loads(request.body)\n    except (TypeError, json.decoder.JSONDecodeError):\n        return JsonResponse({\"error\": \"Cannot parse request body\"}, status=400)\n\n    webhook = body.get(\"webhook\")\n\n    if not webhook:\n        return JsonResponse({\"error\": \"no webhook URL\"})\n    message = {\"text\": \"_Greetings_ from PostHog!\"}\n    try:\n        if not settings.DEBUG:\n            raise_if_user_provided_url_unsafe(webhook)\n        response = requests.post(webhook, verify=False, json=message)\n\n        if response.ok:\n            return JsonResponse({\"success\": True})\n        else:\n            return JsonResponse({\"error\": response.text})\n    except:\n        return JsonResponse({\"error\": \"invalid webhook URL\"})\n"], "filenames": ["plugin-server/bin/ci_functional_tests.sh", "plugin-server/src/config/config.ts", "plugin-server/src/main/ingestion-queues/on-event-handler-consumer.ts", "plugin-server/src/types.ts", "plugin-server/src/utils/db/hub.ts", "plugin-server/src/utils/fetch.ts", "plugin-server/src/worker/ingestion/hooks.ts", "plugin-server/src/worker/vm/imports.ts", "plugin-server/src/worker/vm/vm.ts", "plugin-server/tests/worker/ingestion/event-pipeline/event-pipeline-integration.test.ts", "plugin-server/tests/worker/ingestion/hooks.test.ts", "posthog/api/user.py"], "buggy_code_start_loc": [18, 113, 92, 181, 92, 9, 8, 15, 14, 55, 5, 36], "buggy_code_end_loc": [18, 114, 93, 278, 197, 41, 464, 51, 66, 56, 572, 474], "fixing_code_start_loc": [19, 112, 91, 180, 91, 10, 8, 14, 14, 54, 4, 35], "fixing_code_end_loc": [20, 112, 91, 274, 191, 29, 450, 45, 64, 54, 563, 473], "type": "CWE-918", "message": "PostHog provides open-source product analytics, session recording, feature flagging and A/B testing that you can self-host. A server-side request forgery (SSRF), which can only be exploited by authenticated users, was found in Posthog. Posthog did not verify whether a URL was local when enabling webhooks, allowing authenticated users to forge a POST request. This vulnerability has been addressed in `22bd5942` and will be included in subsequent releases. There are no known workarounds for this vulnerability.", "other": {"cve": {"id": "CVE-2023-46746", "sourceIdentifier": "security-advisories@github.com", "published": "2023-12-01T22:15:10.167", "lastModified": "2023-12-11T19:15:08.763", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "PostHog provides open-source product analytics, session recording, feature flagging and A/B testing that you can self-host. A server-side request forgery (SSRF), which can only be exploited by authenticated users, was found in Posthog. Posthog did not verify whether a URL was local when enabling webhooks, allowing authenticated users to forge a POST request. This vulnerability has been addressed in `22bd5942` and will be included in subsequent releases. There are no known workarounds for this vulnerability."}, {"lang": "es", "value": "PostHog proporciona an\u00e1lisis de productos de c\u00f3digo abierto, grabaci\u00f3n de sesiones, marcado de funciones y pruebas A/B que usted mismo puede alojar. En Posthog se encontr\u00f3 server-side request forgery (SSRF), que s\u00f3lo puede ser explotada por usuarios autenticados. Posthog no verific\u00f3 si una URL era local al habilitar los webhooks, lo que permiti\u00f3 a los usuarios autenticados falsificar una solicitud POST. Esta vulnerabilidad se solucion\u00f3 en `22bd5942` y se incluir\u00e1 en versiones posteriores. No se conocen workarounds para esta vulnerabilidad."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 4.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:R/S:U/C:L/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 4.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.3, "impactScore": 3.4}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-918"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:posthog:posthog:*:*:*:*:*:*:*:*", "versionEndIncluding": "1.43.1", "matchCriteriaId": "DA68EBBF-D850-4812-BEF5-05E2EDE7FA28"}]}]}], "references": [{"url": "https://github.com/PostHog/posthog/commit/22bd5942638d5d9bc4bd603a9bfe8f8a95572292", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/PostHog/posthog/security/advisories/GHSA-wqqw-r8c5-j67c", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}, {"url": "https://securitylab.github.com/advisories/GHSL-2023-185_posthog_posthog/", "source": "security-advisories@github.com"}]}, "github_commit_url": "https://github.com/PostHog/posthog/commit/22bd5942638d5d9bc4bd603a9bfe8f8a95572292"}}