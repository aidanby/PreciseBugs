{"buggy_code": ["pub mod execution;\nmod siginfo_ext;\npub mod signals;\npub mod state;\n\npub use crate::instance::execution::{KillError, KillState, KillSuccess, KillSwitch};\npub use crate::instance::signals::{signal_handler_none, SignalBehavior, SignalHandler};\npub use crate::instance::state::State;\n\nuse crate::alloc::Alloc;\nuse crate::context::Context;\nuse crate::embed_ctx::CtxMap;\nuse crate::error::Error;\n#[cfg(feature = \"concurrent_testpoints\")]\nuse crate::lock_testpoints::LockTestpoints;\nuse crate::module::{self, FunctionHandle, Global, GlobalValue, Module, TrapCode};\nuse crate::region::RegionInternal;\nuse crate::sysdeps::HOST_PAGE_SIZE_EXPECTED;\nuse crate::val::{UntypedRetVal, Val};\nuse crate::vmctx::Vmctx;\nuse crate::WASM_PAGE_SIZE;\nuse libc::{c_void, pthread_self, siginfo_t, uintptr_t};\nuse lucet_module::InstanceRuntimeData;\nuse memoffset::offset_of;\nuse std::any::Any;\nuse std::cell::{BorrowError, BorrowMutError, Ref, RefCell, RefMut, UnsafeCell};\nuse std::convert::TryFrom;\nuse std::marker::PhantomData;\nuse std::mem;\nuse std::ops::{Deref, DerefMut};\nuse std::ptr::{self, NonNull};\nuse std::sync::Arc;\n\npub const LUCET_INSTANCE_MAGIC: u64 = 746_932_922;\n\nthread_local! {\n    /// The host context.\n    ///\n    /// Control returns here implicitly due to the setup in `Context::init()` when guest functions\n    /// return normally. Control can return here explicitly from signal handlers when the guest\n    /// program needs to be terminated.\n    ///\n    /// This is an `UnsafeCell` due to nested borrows. The context must be borrowed mutably when\n    /// swapping to the guest context, which means that borrow exists for the entire time the guest\n    /// function runs even though the mutation to the host context is done only at the beginning of\n    /// the swap. Meanwhile, the signal handler can run at any point during the guest function, and\n    /// so it also must be able to immutably borrow the host context if it needs to swap back. The\n    /// runtime borrowing constraints for a `RefCell` are therefore too strict for this variable.\n    pub(crate) static HOST_CTX: UnsafeCell<Context> = UnsafeCell::new(Context::new());\n\n    /// The currently-running `Instance`, if one exists.\n    pub(crate) static CURRENT_INSTANCE: RefCell<Option<NonNull<Instance>>> = RefCell::new(None);\n}\n\n/// A smart pointer to an [`Instance`](struct.Instance.html) that properly manages cleanup when dropped.\n///\n/// Instances are always stored in memory backed by a `Region`; we never want to create one directly\n/// with the Rust allocator. This type allows us to abide by that rule while also having an owned\n/// type that cleans up the instance when we are done with it.\n///\n/// Since this type implements `Deref` and `DerefMut` to `Instance`, it can usually be treated as\n/// though it were a `&mut Instance`.\npub struct InstanceHandle {\n    inst: NonNull<Instance>,\n    needs_inst_drop: bool,\n}\n\n// raw pointer lint\nunsafe impl Send for InstanceHandle {}\n\n/// Create a new `InstanceHandle`.\n///\n/// This is not meant for public consumption, but rather is used to make implementations of\n/// `Region`.\npub fn new_instance_handle(\n    instance: *mut Instance,\n    module: Arc<dyn Module>,\n    alloc: Alloc,\n    embed_ctx: CtxMap,\n) -> Result<InstanceHandle, Error> {\n    let inst = NonNull::new(instance)\n        .ok_or_else(|| lucet_format_err!(\"instance pointer is null; this is a bug\"))?;\n\n    lucet_ensure!(\n        unsafe { inst.as_ref().magic } != LUCET_INSTANCE_MAGIC,\n        \"created a new instance handle in memory with existing instance magic; this is a bug\"\n    );\n\n    let mut handle = InstanceHandle {\n        inst,\n        needs_inst_drop: false,\n    };\n\n    let inst = Instance::new(alloc, module, embed_ctx);\n\n    unsafe {\n        // this is wildly unsafe! you must be very careful to not let the drop impls run on the\n        // uninitialized fields; see\n        // <https://doc.rust-lang.org/std/mem/fn.forget.html#use-case-1>\n\n        // write the whole struct into place over the uninitialized page\n        ptr::write(&mut *handle, inst);\n    };\n\n    handle.needs_inst_drop = true;\n\n    handle.reset()?;\n\n    Ok(handle)\n}\n\npub fn instance_handle_to_raw(mut inst: InstanceHandle) -> *mut Instance {\n    inst.needs_inst_drop = false;\n    inst.inst.as_ptr()\n}\n\npub unsafe fn instance_handle_from_raw(\n    ptr: *mut Instance,\n    needs_inst_drop: bool,\n) -> InstanceHandle {\n    InstanceHandle {\n        inst: NonNull::new_unchecked(ptr),\n        needs_inst_drop,\n    }\n}\n\n// Safety argument for these deref impls: the instance's `Alloc` field contains an `Arc` to the\n// region that backs this memory, keeping the page containing the `Instance` alive as long as the\n// region exists\n\nimpl Deref for InstanceHandle {\n    type Target = Instance;\n    fn deref(&self) -> &Self::Target {\n        unsafe { self.inst.as_ref() }\n    }\n}\n\nimpl DerefMut for InstanceHandle {\n    fn deref_mut(&mut self) -> &mut Self::Target {\n        unsafe { self.inst.as_mut() }\n    }\n}\n\nimpl Drop for InstanceHandle {\n    fn drop(&mut self) {\n        if self.needs_inst_drop {\n            unsafe {\n                let inst = self.inst.as_mut();\n\n                // Grab a handle to the region to ensure it outlives `inst`.\n                //\n                // This ensures that the region won't be dropped by `inst` being\n                // dropped, which could result in `inst` being unmapped by the\n                // Region *during* drop of the Instance's fields.\n                let region: Arc<dyn RegionInternal> = inst.alloc().region.clone();\n\n                // drop the actual instance\n                std::ptr::drop_in_place(inst);\n\n                // and now we can drop what may be the last Arc<Region>. If it is\n                // it can safely do what it needs with memory; we're not running\n                // destructors on it anymore.\n                mem::drop(region);\n            }\n        }\n    }\n}\n\n/// A Lucet program, together with its dedicated memory and signal handlers.\n///\n/// This is the primary interface for running programs, examining return values, and accessing the\n/// WebAssembly heap.\n///\n/// `Instance`s are never created by runtime users directly, but rather are acquired from\n/// [`Region`](../region/trait.Region.html)s and often accessed through\n/// [`InstanceHandle`](../instance/struct.InstanceHandle.html) smart pointers. This guarantees that instances\n/// and their fields are never moved in memory, otherwise raw pointers in the metadata could be\n/// unsafely invalidated.\n///\n/// An instance occupies one 4096-byte page in memory, with a layout like:\n/// ```text\n/// 0xXXXXX000:\n///   Instance {\n///     .magic\n///     .embed_ctx\n///      ... etc ...\n///   }\n///\n///   // unused space\n///\n///   InstanceInternals {\n///     .globals\n///     .instruction_counter\n///   } // last address *inside* `InstanceInternals` is 0xXXXXXFFF\n/// 0xXXXXY000: // start of next page, VMContext points here\n///   Heap {\n///     ..\n///   }\n/// ```\n///\n/// This layout allows modules to tightly couple to a handful of fields related to the instance,\n/// rather than possibly requiring compiler-side changes (and recompiles) whenever `Instance`\n/// changes.\n///\n/// It also obligates `Instance` to be immediately followed by the heap, but otherwise leaves the\n/// locations of the stack, globals, and any other data, to be implementation-defined by the\n/// `Region` that actually creates `Slot`s onto which `Instance` are mapped.\n/// For information about the layout of all instance-related memory, see the documentation of\n/// [MmapRegion](../region/mmap/struct.MmapRegion.html).\n#[repr(C)]\n#[repr(align(4096))]\npub struct Instance {\n    /// Used to catch bugs in pointer math used to find the address of the instance\n    magic: u64,\n\n    /// The embedding context is a map containing embedder-specific values that are used to\n    /// implement hostcalls\n    pub(crate) embed_ctx: CtxMap,\n\n    /// The program (WebAssembly module) that is the entrypoint for the instance.\n    pub(crate) module: Arc<dyn Module>,\n\n    /// The `Context` in which the guest program runs\n    pub(crate) ctx: Context,\n\n    /// Instance state and error information\n    pub(crate) state: State,\n\n    /// Small mutexed state used for remote kill switch functionality\n    pub(crate) kill_state: Arc<KillState>,\n\n    #[cfg(feature = \"concurrent_testpoints\")]\n    /// Conditionally-present helpers to force permutations of possible races in testing.\n    pub lock_testpoints: Arc<LockTestpoints>,\n\n    /// The memory allocated for this instance\n    alloc: Alloc,\n\n    /// Handler run for signals that do not arise from a known WebAssembly trap, or that involve\n    /// memory outside of the current instance.\n    fatal_handler: fn(&Instance) -> !,\n\n    /// A fatal handler set from C\n    c_fatal_handler: Option<unsafe extern \"C\" fn(*mut Instance)>,\n\n    /// Handler run when `SIGBUS`, `SIGFPE`, `SIGILL`, or `SIGSEGV` are caught by the instance thread.\n    signal_handler: Box<\n        dyn Fn(\n            &Instance,\n            &Option<TrapCode>,\n            libc::c_int,\n            *const siginfo_t,\n            *const c_void,\n        ) -> SignalBehavior,\n    >,\n\n    /// Whether to ensure the Lucet signal handler is installed when running this instance.\n    ensure_signal_handler_installed: bool,\n\n    /// Whether to install an alternate signal stack while the instance is running.\n    ensure_sigstack_installed: bool,\n\n    /// Pointer to the function used as the entrypoint.\n    entrypoint: Option<FunctionHandle>,\n\n    /// The value passed back to the guest when resuming a yielded instance.\n    pub(crate) resumed_val: Option<Box<dyn Any + 'static>>,\n\n    pub(crate) memory_limiter: Option<Box<dyn MemoryLimiter + Send + Sync + 'static>>,\n\n    /// `_padding` must be the last member of the structure.\n    /// This marks where the padding starts to make the structure exactly 4096 bytes long.\n    /// It is also used to compute the size of the structure up to that point, i.e. without padding.\n    _padding: (),\n}\n\n#[async_trait::async_trait]\npub trait MemoryLimiter {\n    async fn memory_growing(&mut self, current: usize, desired: usize) -> bool;\n    fn memory_grow_failed(&mut self, _error: &Error) {}\n}\n\n/// Users of `Instance` must be very careful about when instances are dropped!\n///\n/// Typically you will not have to worry about this, as InstanceHandle will robustly handle\n/// Instance drop semantics. If an instance is dropped, and the Region it's in has already dropped,\n/// it may contain the last reference counted pointer to its Region. If so, when Instance's\n/// destructor runs, Region will be dropped, and may free or otherwise invalidate the memory that\n/// this Instance exists in, *while* the Instance destructor is executing.\nimpl Drop for Instance {\n    fn drop(&mut self) {\n        // Reset magic to indicate this instance\n        // is no longer valid\n        self.magic = 0;\n    }\n}\n\n/// The result of running or resuming an [`Instance`](struct.Instance.html).\n#[derive(Debug)]\npub enum RunResult {\n    /// An instance returned with a value.\n    ///\n    /// The actual type of the contained value depends on the return type of the guest function that\n    /// was called. For guest functions with no return value, it is undefined behavior to do\n    /// anything with this value.\n    Returned(UntypedRetVal),\n    /// An instance yielded, potentially with a value.\n    ///\n    /// This arises when a hostcall invokes one of the\n    /// [`Vmctx::yield_*()`](vmctx/struct.Vmctx.html#method.yield_) family of methods. Depending on which\n    /// variant is used, the `YieldedVal` may contain a value passed from the guest context to the\n    /// host.\n    ///\n    /// An instance that has yielded may only be resumed\n    /// ([with](struct.Instance.html#method.resume_with_val) or\n    /// [without](struct.Instance.html#method.resume) a value to returned to the guest),\n    /// [reset](struct.Instance.html#method.reset), or dropped. Attempting to run an instance from a\n    /// new entrypoint after it has yielded but without first resetting will result in an error.\n    Yielded(YieldedVal),\n}\n\nimpl RunResult {\n    /// Try to get a return value from a run result, returning `Error::InstanceNotReturned` if the\n    /// instance instead yielded.\n    pub fn returned(self) -> Result<UntypedRetVal, Error> {\n        match self {\n            RunResult::Returned(rv) => Ok(rv),\n            RunResult::Yielded(_) => Err(Error::InstanceNotReturned),\n        }\n    }\n\n    /// Try to get a reference to a return value from a run result, returning\n    /// `Error::InstanceNotReturned` if the instance instead yielded.\n    pub fn returned_ref(&self) -> Result<&UntypedRetVal, Error> {\n        match self {\n            RunResult::Returned(rv) => Ok(rv),\n            RunResult::Yielded(_) => Err(Error::InstanceNotReturned),\n        }\n    }\n\n    /// Returns `true` if the instance returned a value.\n    pub fn is_returned(&self) -> bool {\n        self.returned_ref().is_ok()\n    }\n\n    /// Unwraps a run result into a return value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead yielded, with a panic message including the passed message.\n    pub fn expect_returned(self, msg: &str) -> UntypedRetVal {\n        self.returned().expect(msg)\n    }\n\n    /// Unwraps a run result into a returned value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead yielded.\n    pub fn unwrap_returned(self) -> UntypedRetVal {\n        self.returned().unwrap()\n    }\n\n    /// Try to get a yielded value from a run result, returning `Error::InstanceNotYielded` if the\n    /// instance instead returned.\n    pub fn yielded(self) -> Result<YieldedVal, Error> {\n        match self {\n            RunResult::Returned(_) => Err(Error::InstanceNotYielded),\n            RunResult::Yielded(yv) => Ok(yv),\n        }\n    }\n\n    /// Try to get a reference to a yielded value from a run result, returning\n    /// `Error::InstanceNotYielded` if the instance instead returned.\n    pub fn yielded_ref(&self) -> Result<&YieldedVal, Error> {\n        match self {\n            RunResult::Returned(_) => Err(Error::InstanceNotYielded),\n            RunResult::Yielded(yv) => Ok(yv),\n        }\n    }\n\n    /// Returns `true` if the instance yielded.\n    pub fn is_yielded(&self) -> bool {\n        self.yielded_ref().is_ok()\n    }\n\n    /// Returns `true` if the instance can be resumed: either it has yielded, or its bound has\n    /// expired.\n    pub fn can_resume(&self) -> bool {\n        self.is_yielded()\n    }\n\n    /// Returns `true` if the instance has yielded a value of the given type.\n    pub fn has_yielded<A: Any>(&self) -> bool {\n        match self {\n            RunResult::Yielded(yv) => yv.is::<A>(),\n            _ => false,\n        }\n    }\n\n    /// Unwraps a run result into a yielded value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead returned, with a panic message including the passed message.\n    pub fn expect_yielded(self, msg: &str) -> YieldedVal {\n        self.yielded().expect(msg)\n    }\n\n    /// Unwraps a run result into a yielded value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead returned.\n    pub fn unwrap_yielded(self) -> YieldedVal {\n        self.yielded().unwrap()\n    }\n}\n\n/// An \"internal\" run result: either a `RunResult` or a bound expiration. We do not expose bound\n/// expirations to the caller directly; rather, we only handle them in `run_async()`.\npub(crate) enum InternalRunResult {\n    Normal(RunResult),\n    BoundExpired,\n}\n\nimpl InternalRunResult {\n    pub(crate) fn unwrap(self) -> RunResult {\n        match self {\n            InternalRunResult::Normal(result) => result,\n            InternalRunResult::BoundExpired => panic!(\"should not have had a runtime bound\"),\n        }\n    }\n}\n\nimpl std::convert::Into<InternalRunResult> for RunResult {\n    fn into(self) -> InternalRunResult {\n        InternalRunResult::Normal(self)\n    }\n}\n\n/// APIs that are internal, but useful to implementors of extension modules; you probably don't want\n/// this trait!\n///\n/// This is a trait rather than inherent `impl`s in order to keep the `lucet-runtime` API clean and\n/// safe.\npub trait InstanceInternal {\n    fn alloc(&self) -> &Alloc;\n    fn alloc_mut(&mut self) -> &mut Alloc;\n    fn module(&self) -> &dyn Module;\n    fn state(&self) -> &State;\n    fn valid_magic(&self) -> bool;\n}\n\nimpl InstanceInternal for Instance {\n    /// Get a reference to the instance's `Alloc`.\n    fn alloc(&self) -> &Alloc {\n        &self.alloc\n    }\n\n    /// Get a mutable reference to the instance's `Alloc`.\n    fn alloc_mut(&mut self) -> &mut Alloc {\n        &mut self.alloc\n    }\n\n    /// Get a reference to the instance's `Module`.\n    fn module(&self) -> &dyn Module {\n        self.module.deref()\n    }\n\n    /// Get a reference to the instance's `State`.\n    fn state(&self) -> &State {\n        &self.state\n    }\n\n    /// Check whether the instance magic is valid.\n    fn valid_magic(&self) -> bool {\n        self.magic == LUCET_INSTANCE_MAGIC\n    }\n}\n\n// Public API\nimpl Instance {\n    /// Run a function with arguments in the guest context at the given entrypoint.\n    ///\n    /// ```no_run\n    /// # use lucet_runtime_internals::instance::InstanceHandle;\n    /// # let instance: InstanceHandle = unimplemented!();\n    /// // regular execution yields `Ok(UntypedRetVal)`\n    /// let retval = instance.run(\"factorial\", &[5u64.into()]).unwrap().unwrap_returned();\n    /// assert_eq!(u64::from(retval), 120u64);\n    ///\n    /// // runtime faults yield `Err(Error)`\n    /// let result = instance.run(\"faulting_function\", &[]);\n    /// assert!(result.is_err());\n    /// ```\n    ///\n    /// # Safety\n    ///\n    /// This is unsafe in two ways:\n    ///\n    /// - The type of the entrypoint might not be correct. It might take a different number or\n    /// different types of arguments than are provided to `args`. It might not even point to a\n    /// function! We will likely add type information to `lucetc` output so we can dynamically check\n    /// the type in the future.\n    ///\n    /// - The entrypoint is foreign code. While we may be convinced that WebAssembly compiled to\n    /// native code by `lucetc` is safe, we do not have the same guarantee for the hostcalls that a\n    /// guest may invoke. They might be implemented in an unsafe language, so we must treat this\n    /// call as unsafe, just like any other FFI call.\n    ///\n    /// For the moment, we do not mark this as `unsafe` in the Rust type system, but that may change\n    /// in the future.\n    pub fn run(&mut self, entrypoint: &str, args: &[Val]) -> Result<RunResult, Error> {\n        let func = self.module.get_export_func(entrypoint)?;\n        Ok(self.run_func(func, &args, false, None)?.unwrap())\n    }\n\n    /// Run a function with arguments in the guest context from the [WebAssembly function\n    /// table](https://webassembly.github.io/spec/core/syntax/modules.html#tables).\n    ///\n    /// # Safety\n    ///\n    /// The same safety caveats of [`Instance::run()`](struct.Instance.html#method.run) apply.\n    pub fn run_func_idx(\n        &mut self,\n        table_idx: u32,\n        func_idx: u32,\n        args: &[Val],\n    ) -> Result<RunResult, Error> {\n        let func = self.module.get_func_from_idx(table_idx, func_idx)?;\n        Ok(self.run_func(func, &args, false, None)?.unwrap())\n    }\n\n    /// Resume execution of an instance that has yielded without providing a value to the guest.\n    ///\n    /// This should only be used when the guest yielded with\n    /// [`Vmctx::yield_()`](vmctx/struct.Vmctx.html#method.yield_) or\n    /// [`Vmctx::yield_val()`](vmctx/struct.Vmctx.html#method.yield_val).\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`](struct.Instance.html#method.run)\n    /// applies.\n    pub fn resume(&mut self) -> Result<RunResult, Error> {\n        self.resume_with_val(EmptyYieldVal)\n    }\n\n    /// Resume execution of an instance that has yielded, providing a value to the guest.\n    ///\n    /// The type of the provided value must match the type expected by\n    /// [`Vmctx::yield_expecting_val()`](vmctx/struct.Vmctx.html#method.yield_expecting_val) or\n    /// [`Vmctx::yield_val_expecting_val()`](vmctx/struct.Vmctx.html#method.yield_val_expecting_val).\n    ///\n    /// The provided value will be dynamically typechecked against the type the guest expects to\n    /// receive, and if that check fails, this call will fail with `Error::InvalidArgument`.\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`](struct.Instance.html#method.run)\n    /// applies.\n    pub fn resume_with_val<A: Any + 'static>(&mut self, val: A) -> Result<RunResult, Error> {\n        Ok(self.resume_with_val_impl(val, false, None)?.unwrap())\n    }\n\n    pub(crate) fn resume_with_val_impl<A: Any + 'static>(\n        &mut self,\n        val: A,\n        async_context: bool,\n        max_insn_count: Option<u64>,\n    ) -> Result<InternalRunResult, Error> {\n        match &self.state {\n            State::Yielded { expecting, .. } => {\n                // make sure the resumed value is of the right type\n                if !expecting.is::<PhantomData<A>>() {\n                    return Err(Error::InvalidArgument(\n                        \"type mismatch between yielded instance expected value and resumed value\",\n                    ));\n                }\n            }\n            _ => return Err(Error::InvalidArgument(\"can only resume a yielded instance\")),\n        }\n\n        self.resumed_val = Some(Box::new(val) as Box<dyn Any + 'static>);\n\n        self.set_instruction_bound_delta(max_insn_count);\n        self.swap_and_return(async_context)\n    }\n\n    /// Resume execution of an instance that has previously reached an instruction bound.\n    ///\n    /// The execution slice that begins with this call is bounded by the new bound provided.\n    ///\n    /// This should only be used when `run_func()` returned a `RunResult::Bounded`. This is an\n    /// internal function used by `run_async()`.\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`](struct.Instance.html#method.run)\n    /// applies.\n    pub(crate) fn resume_bounded(\n        &mut self,\n        max_insn_count: u64,\n    ) -> Result<InternalRunResult, Error> {\n        if !self.state.is_bound_expired() {\n            return Err(Error::InvalidArgument(\n                \"can only call resume_bounded() on an instance that hit an instruction bound\",\n            ));\n        }\n        self.set_instruction_bound_delta(Some(max_insn_count));\n        self.swap_and_return(true)\n    }\n\n    /// Run the module's [start function][start], if one exists.\n    ///\n    /// If there is no start function in the module, this does nothing.\n    ///\n    /// If the module contains a start function, you must run it before running any other exported\n    /// functions. If an instance is reset, you must run the start function again.\n    ///\n    /// Start functions may assume that Wasm tables and memories are properly initialized, but may\n    /// not assume that imported functions or globals are available.\n    ///\n    /// # Errors\n    ///\n    /// In addition to the errors that can be returned from [`Instance::run()`][run], this can also\n    /// return `Error::StartYielded` if the start function attempts to yield. This should not arise\n    /// as long as the start function does not attempt to use any imported functions.\n    ///\n    /// This also returns `Error::StartAlreadyRun` if the start function has already run since the\n    /// instance was created or last reset.\n    ///\n    /// Wasm start functions are not allowed to call imported functions. If the start function\n    /// attempts to do so, the instance will be terminated with\n    /// `TerminationDetails::StartCalledImportFunc`.\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`][run]\n    /// applies.\n    ///\n    /// [run]: struct.Instance.html#method.run\n    /// [start]: https://webassembly.github.io/spec/core/syntax/modules.html#syntax-start\n    pub fn run_start(&mut self) -> Result<(), Error> {\n        if let Some(start) = self.module.get_start_func()? {\n            if !self.is_not_started() {\n                return Err(Error::StartAlreadyRun);\n            }\n            self.run_func(start, &[], false, None)?;\n        }\n        Ok(())\n    }\n\n    /// Reset the instance's heap and global variables to their initial state.\n    ///\n    /// The WebAssembly `start` section, if present, will need to be re-run with\n    /// [`Instance::run_start()`][run_start] before running any other exported functions.\n    ///\n    /// The embedder contexts present at instance creation or added with\n    /// [`Instance::insert_embed_ctx()`](struct.Instance.html#method.insert_embed_ctx) are not\n    /// modified by this call; it is the embedder's responsibility to clear or reset their state if\n    /// necessary.\n    ///\n    /// This will also reinitialize the kill state, which means that any outstanding\n    /// [`KillSwitch`](struct.KillSwitch.html) objects will be unable to terminate this instance.\n    /// It is the embedder's responsibility to initialize new `KillSwitch`es after resetting an\n    /// instance.\n    ///\n    /// [run_start]: struct.Instance.html#method.run\n    pub fn reset(&mut self) -> Result<(), Error> {\n        self.alloc.reset_heap(self.module.as_ref())?;\n        let globals = unsafe { self.alloc.globals_mut() };\n        let mod_globals = self.module.globals();\n        for (i, v) in mod_globals.iter().enumerate() {\n            globals[i] = match v.global() {\n                Global::Import { .. } => {\n                    return Err(Error::Unsupported(format!(\n                        \"global imports are unsupported; found: {:?}\",\n                        v\n                    )));\n                }\n                Global::Def(def) => def.init_val(),\n            };\n        }\n\n        if self.module.get_start_func()?.is_some() {\n            self.state = State::NotStarted;\n        } else {\n            self.state = State::Ready;\n        }\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        {\n            self.kill_state = Arc::new(KillState::new(Arc::clone(&self.lock_testpoints)));\n        }\n        #[cfg(not(feature = \"concurrent_testpoints\"))]\n        {\n            self.kill_state = Arc::new(KillState::new());\n        }\n\n        Ok(())\n    }\n\n    /// Grow the guest memory by the given number of WebAssembly pages.\n    ///\n    /// On success, returns the number of pages that existed before the call.\n    pub fn grow_memory(&mut self, additional_pages: u32) -> Result<u32, Error> {\n        let additional_bytes = additional_pages\n            .checked_mul(WASM_PAGE_SIZE)\n            .ok_or_else(|| lucet_format_err!(\"additional pages larger than wasm address space\",))?;\n        let orig_len = self\n            .alloc\n            .expand_heap(additional_bytes, self.module.as_ref())?;\n        Ok(orig_len / WASM_PAGE_SIZE)\n    }\n\n    /// Grow memory from a hostcall context.\n    pub fn grow_memory_from_hostcall(\n        &mut self,\n        vmctx: &Vmctx,\n        additional_pages: u32,\n    ) -> Result<u32, Error> {\n        // Use a function so that we can report all Errs via memory_grow_failed.\n        fn aux(\n            instance: &mut Instance,\n            vmctx: &Vmctx,\n            additional_pages: u32,\n        ) -> Result<u32, Error> {\n            // Calculate current and desired bytes\n            let current_bytes = instance.alloc.heap_len();\n            let additional_bytes =\n                additional_pages\n                    .checked_mul(WASM_PAGE_SIZE)\n                    .ok_or_else(|| {\n                        lucet_format_err!(\"additional pages larger than wasm address space\",)\n                    })? as usize;\n            let desired_bytes = additional_bytes\n                .checked_add(current_bytes)\n                .ok_or_else(|| lucet_format_err!(\"desired bytes overflow\",))?;\n            // Let the limiter reject the grow\n            if let Some(ref mut limiter) = instance.memory_limiter {\n                if !vmctx.block_on(async move {\n                    limiter.memory_growing(current_bytes, desired_bytes).await\n                }) {\n                    lucet_bail!(\"memory limiter denied growth\");\n                }\n            }\n            // Try the grow itself\n            instance.grow_memory(additional_pages)\n        }\n\n        match aux(self, vmctx, additional_pages) {\n            Ok(n) => Ok(n),\n            Err(e) => {\n                if let Some(ref mut limiter) = self.memory_limiter {\n                    limiter.memory_grow_failed(&e);\n                    Err(e)\n                } else {\n                    Err(e)\n                }\n            }\n        }\n    }\n\n    /// Return the WebAssembly heap as a slice of bytes.\n    pub fn heap(&self) -> &[u8] {\n        unsafe { self.alloc.heap() }\n    }\n\n    /// Return the WebAssembly heap as a mutable slice of bytes.\n    pub fn heap_mut(&mut self) -> &mut [u8] {\n        unsafe { self.alloc.heap_mut() }\n    }\n\n    /// Return the WebAssembly heap as a slice of `u32`s.\n    pub fn heap_u32(&self) -> &[u32] {\n        unsafe { self.alloc.heap_u32() }\n    }\n\n    /// Return the WebAssembly heap as a mutable slice of `u32`s.\n    pub fn heap_u32_mut(&mut self) -> &mut [u32] {\n        unsafe { self.alloc.heap_u32_mut() }\n    }\n\n    /// Return the WebAssembly globals as a slice of `i64`s.\n    pub fn globals(&self) -> &[GlobalValue] {\n        unsafe { self.alloc.globals() }\n    }\n\n    /// Return the WebAssembly globals as a mutable slice of `i64`s.\n    pub fn globals_mut(&mut self) -> &mut [GlobalValue] {\n        unsafe { self.alloc.globals_mut() }\n    }\n\n    /// Check whether a given range in the host address space overlaps with the memory that backs\n    /// the instance heap.\n    pub fn check_heap<T>(&self, ptr: *const T, len: usize) -> bool {\n        self.alloc.mem_in_heap(ptr, len)\n    }\n\n    /// Check whether a context value of a particular type exists.\n    pub fn contains_embed_ctx<T: Any>(&self) -> bool {\n        self.embed_ctx.contains::<T>()\n    }\n\n    /// Get a reference to a context value of a particular type, if it exists.\n    pub fn get_embed_ctx<T: Any>(&self) -> Option<Result<Ref<'_, T>, BorrowError>> {\n        self.embed_ctx.try_get::<T>()\n    }\n\n    /// Get a mutable reference to a context value of a particular type, if it exists.\n    pub fn get_embed_ctx_mut<T: Any>(&self) -> Option<Result<RefMut<'_, T>, BorrowMutError>> {\n        self.embed_ctx.try_get_mut::<T>()\n    }\n\n    /// Insert a context value.\n    ///\n    /// If a context value of the same type already existed, it is returned.\n    pub fn insert_embed_ctx<T: Any>(&mut self, x: T) -> Option<T> {\n        self.embed_ctx.insert(x)\n    }\n\n    /// Remove a context value of a particular type, returning it if it exists.\n    pub fn remove_embed_ctx<T: Any>(&mut self) -> Option<T> {\n        self.embed_ctx.remove::<T>()\n    }\n\n    /// Set the handler run when `SIGBUS`, `SIGFPE`, `SIGILL`, or `SIGSEGV` are caught by the\n    /// instance thread.\n    ///\n    /// In most cases, these signals are unrecoverable for the instance that raised them, but do not\n    /// affect the rest of the process.\n    ///\n    /// The default signal handler returns\n    /// [`SignalBehavior::Default`](enum.SignalBehavior.html#variant.Default), which yields a\n    /// runtime fault error.\n    ///\n    /// The signal handler must be\n    /// [signal-safe](http://man7.org/linux/man-pages/man7/signal-safety.7.html).\n    pub fn set_signal_handler<H>(&mut self, handler: H)\n    where\n        H: 'static\n            + Fn(\n                &Instance,\n                &Option<TrapCode>,\n                libc::c_int,\n                *const siginfo_t,\n                *const c_void,\n            ) -> SignalBehavior,\n    {\n        self.signal_handler = Box::new(handler) as Box<SignalHandler>;\n    }\n\n    /// Set the handler run for signals that do not arise from a known WebAssembly trap, or that\n    /// involve memory outside of the current instance.\n    ///\n    /// Fatal signals are not only unrecoverable for the instance that raised them, but may\n    /// compromise the correctness of the rest of the process if unhandled.\n    ///\n    /// The default fatal handler calls `panic!()`.\n    pub fn set_fatal_handler(&mut self, handler: fn(&Instance) -> !) {\n        self.fatal_handler = handler;\n    }\n\n    /// Set the fatal handler to a C-compatible function.\n    ///\n    /// This is a separate interface, because C functions can't return the `!` type. Like the\n    /// regular `fatal_handler`, it is not expected to return, but we cannot enforce that through\n    /// types.\n    ///\n    /// When a fatal error occurs, this handler is run first, and then the regular `fatal_handler`\n    /// runs in case it returns.\n    pub fn set_c_fatal_handler(&mut self, handler: unsafe extern \"C\" fn(*mut Instance)) {\n        self.c_fatal_handler = Some(handler);\n    }\n\n    /// Set whether the Lucet signal handler is installed when running or resuming this instance\n    /// (`true` by default).\n    ///\n    /// If this is `true`, the Lucet runtime checks whether its signal handler is installed whenever\n    /// an instance runs, installing it if it is not present, and uninstalling it when there are no\n    /// longer any Lucet instances running. If this is `false`, that check is disabled, which can\n    /// improve performance when running or resuming an instance.\n    ///\n    /// Use `install_lucet_signal_handler()` and `remove_lucet_signal_handler()` to manually install\n    /// or remove the signal handler.\n    ///\n    /// # Safety\n    ///\n    /// If the Lucet signal handler is not installed when an instance runs, WebAssembly traps such\n    /// as division by zero, assertion failures, or out-of-bounds memory access will raise signals\n    /// to the default signal handlers, usually causing the entire process to crash.\n    pub fn ensure_signal_handler_installed(&mut self, ensure: bool) {\n        self.ensure_signal_handler_installed = ensure;\n    }\n\n    /// Set whether an alternate signal stack is installed for the current thread when running or\n    /// resuming this instance (`true` by default).\n    ///\n    /// If this is `true`, the Lucet runtime installs an alternate signal stack whenever an instance\n    /// runs, and uninstalls it afterwards. If this is `false`, the signal stack is not\n    /// automatically manipulated.\n    ///\n    /// The automatically-installed signal stack uses space allocated in the instance's `Region`,\n    /// sized according to the `signal_stack_size` field of the region's `Limits`.\n    ///\n    /// If you wish to instead provide your own signal stack, we recommend using a stack of size\n    /// `DEFAULT_SIGNAL_STACK_SIZE`, which varies depending on platform and optimization level.\n    ///\n    /// Signal stacks are installed on a per-thread basis, so any thread that runs this instance\n    /// must have a signal stack installed.\n    ///\n    /// # Safety\n    ///\n    /// If an alternate signal stack is not installed when an instance runs, there may not be enough\n    /// stack space for the Lucet signal handler to run. If the signal handler runs out of stack\n    /// space, a double fault could occur and crash the entire process, or the program could\n    /// continue with corrupted memory.\n    pub fn ensure_sigstack_installed(&mut self, ensure: bool) {\n        self.ensure_sigstack_installed = ensure;\n    }\n\n    pub fn kill_switch(&self) -> KillSwitch {\n        KillSwitch::new(Arc::downgrade(&self.kill_state))\n    }\n\n    pub fn is_not_started(&self) -> bool {\n        self.state.is_not_started()\n    }\n\n    pub fn is_ready(&self) -> bool {\n        self.state.is_ready()\n    }\n\n    pub fn is_yielded(&self) -> bool {\n        self.state.is_yielded()\n    }\n\n    pub fn is_bound_expired(&self) -> bool {\n        self.state.is_bound_expired()\n    }\n\n    pub fn is_faulted(&self) -> bool {\n        self.state.is_faulted()\n    }\n\n    pub fn is_terminated(&self) -> bool {\n        self.state.is_terminated()\n    }\n\n    // This needs to be public as it's used in the expansion of `lucet_hostcalls`, available for\n    // external use. But you *really* shouldn't have to call this yourself, so we're going to keep\n    // it out of rustdoc.\n    #[doc(hidden)]\n    pub fn uninterruptable<T, F: FnOnce() -> T>(&mut self, f: F) -> T {\n        self.kill_state.begin_hostcall();\n        let res = f();\n        let stop_reason = self.kill_state.end_hostcall();\n\n        if let Some(termination_details) = stop_reason {\n            // TODO: once we have unwinding, panic here instead so we unwind host frames\n            unsafe {\n                self.terminate(termination_details);\n            }\n        }\n\n        res\n    }\n\n    #[inline]\n    pub fn get_instruction_count(&self) -> Option<u64> {\n        if self.module.is_instruction_count_instrumented() {\n            let implicits = self.get_instance_implicits();\n            let sum = implicits.instruction_count_bound + implicits.instruction_count_adj;\n            // This invariant is ensured as we always set up the fields to have a positive sum, and\n            // generated code only increments `adj`.\n            debug_assert!(sum >= 0);\n            return Some(sum as u64);\n        }\n        None\n    }\n\n    /// Set the total instruction count and bound.\n    #[inline]\n    pub fn set_instruction_count_and_bound(&mut self, instruction_count: u64, bound: u64) {\n        let implicits = self.get_instance_implicits_mut();\n        let instruction_count =\n            i64::try_from(instruction_count).expect(\"instruction count too large\");\n        let bound = i64::try_from(bound).expect(\"bound too large\");\n        // These two sum to `instruction_count`, which must be non-negative.\n        implicits.instruction_count_bound = bound;\n        implicits.instruction_count_adj = instruction_count - bound;\n    }\n\n    /// Set the instruction bound to be `delta` above the current count.\n    ///\n    /// See the comments on `instruction_count_adj` in `InstanceRuntimeData` for more details on\n    /// how this bound works; most relevant is that a bound-yield is only triggered if the bound\n    /// value is *crossed*, but not if execution *begins* with the value exceeded. Hence `delta`\n    /// must be greater than zero for this to set up the instance state to trigger a yield.\n    #[inline]\n    pub fn set_instruction_bound_delta(&mut self, delta: Option<u64>) {\n        let implicits = self.get_instance_implicits_mut();\n        let sum = implicits.instruction_count_adj + implicits.instruction_count_bound;\n        let delta = delta.unwrap_or(i64::MAX as u64);\n        let delta = i64::try_from(delta).expect(\"delta too large\");\n        implicits.instruction_count_bound = sum.wrapping_add(delta);\n        implicits.instruction_count_adj = -delta;\n    }\n\n    #[inline]\n    pub fn set_hostcall_stack_reservation(&mut self) {\n        let slot = self\n            .alloc\n            .slot\n            .as_ref()\n            .expect(\"reachable instance has a slot\");\n\n        let reservation = slot.limits.hostcall_reservation;\n\n        // The `.stack` field is a pointer to the lowest address of the stack - the start of its\n        // allocation. Because the stack grows downward, this is the end of the stack space. So the\n        // limit we'll need to check for hostcalls is some reserved space upwards from here, to\n        // meet some guest stack pointer early.\n        self.get_instance_implicits_mut().stack_limit = slot.stack as u64 + reservation as u64;\n    }\n\n    /// Set a memory limiter for the instance.\n    ///\n    /// If set, this instance must be run asynchronously via [`InstanceHandle::run_async`]\n    pub fn set_memory_limiter(&mut self, limiter: Box<dyn MemoryLimiter + Send + Sync + 'static>) {\n        self.memory_limiter = Some(limiter)\n    }\n}\n\n// Private API\nimpl Instance {\n    fn new(alloc: Alloc, module: Arc<dyn Module>, embed_ctx: CtxMap) -> Self {\n        let globals_ptr = alloc.slot().globals as *mut i64;\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        let lock_testpoints = Arc::new(LockTestpoints::new());\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        let kill_state = Arc::new(KillState::new(Arc::clone(&lock_testpoints)));\n        #[cfg(not(feature = \"concurrent_testpoints\"))]\n        let kill_state = Arc::new(KillState::new());\n\n        let mut inst = Instance {\n            magic: LUCET_INSTANCE_MAGIC,\n            embed_ctx,\n            module,\n            ctx: Context::new(),\n            state: State::Ready,\n            kill_state,\n            #[cfg(feature = \"concurrent_testpoints\")]\n            lock_testpoints,\n            alloc,\n            fatal_handler: default_fatal_handler,\n            c_fatal_handler: None,\n            signal_handler: Box::new(signal_handler_none) as Box<SignalHandler>,\n            ensure_signal_handler_installed: true,\n            ensure_sigstack_installed: true,\n            entrypoint: None,\n            resumed_val: None,\n            memory_limiter: None,\n            _padding: (),\n        };\n        inst.set_globals_ptr(globals_ptr);\n        inst.set_instruction_count_and_bound(0, 0);\n        // Ensure the hostcall limit tracked in this instance's guest-shared data is up-to-date.\n        inst.set_hostcall_stack_reservation();\n\n        assert_eq!(mem::size_of::<Instance>(), HOST_PAGE_SIZE_EXPECTED);\n        let unpadded_size = offset_of!(Instance, _padding);\n        assert!(unpadded_size <= HOST_PAGE_SIZE_EXPECTED - mem::size_of::<*mut i64>());\n        inst\n    }\n\n    // The globals pointer must be stored right before the end of the structure, padded to the page size,\n    // so that it is 8 bytes before the heap.\n    // For this reason, the alignment of the structure is set to 4096, and we define accessors that\n    // read/write the globals pointer as bytes [4096-8..4096] of that structure represented as raw bytes.\n    // InstanceRuntimeData is placed such that it ends at the end of the page this `Instance` starts\n    // on. So we can access it by *self + PAGE_SIZE - size_of::<InstanceRuntimeData>\n    #[inline]\n    fn get_instance_implicits(&self) -> &InstanceRuntimeData {\n        unsafe {\n            let implicits_ptr = (self as *const _ as *const u8)\n                .add(HOST_PAGE_SIZE_EXPECTED - mem::size_of::<InstanceRuntimeData>())\n                as *const InstanceRuntimeData;\n            mem::transmute::<*const InstanceRuntimeData, &InstanceRuntimeData>(implicits_ptr)\n        }\n    }\n\n    #[inline]\n    fn get_instance_implicits_mut(&mut self) -> &mut InstanceRuntimeData {\n        unsafe {\n            let implicits_ptr = (self as *mut _ as *mut u8)\n                .add(HOST_PAGE_SIZE_EXPECTED - mem::size_of::<InstanceRuntimeData>())\n                as *mut InstanceRuntimeData;\n            mem::transmute::<*mut InstanceRuntimeData, &mut InstanceRuntimeData>(implicits_ptr)\n        }\n    }\n\n    #[allow(dead_code)]\n    #[inline]\n    fn get_globals_ptr(&self) -> *mut i64 {\n        self.get_instance_implicits().globals_ptr\n    }\n\n    #[inline]\n    fn set_globals_ptr(&mut self, globals_ptr: *mut i64) {\n        self.get_instance_implicits_mut().globals_ptr = globals_ptr\n    }\n\n    /// Run a function in guest context at the given entrypoint.\n    pub(crate) fn run_func(\n        &mut self,\n        func: FunctionHandle,\n        args: &[Val],\n        async_context: bool,\n        inst_count_bound: Option<u64>,\n    ) -> Result<InternalRunResult, Error> {\n        let needs_start = self.state.is_not_started() && !func.is_start_func;\n        if needs_start {\n            return Err(Error::InstanceNeedsStart);\n        }\n\n        let is_ready = self.state.is_ready();\n        let is_starting = self.state.is_not_started() && func.is_start_func;\n        let is_non_fatally_faulted = self.state.is_faulted() && !self.state.is_fatal();\n        if !(is_ready || is_starting || is_non_fatally_faulted) {\n            return Err(Error::InvalidArgument(\n                \"instance must be ready, starting, or non-fatally faulted\",\n            ));\n        }\n        if func.ptr.as_usize() == 0 {\n            return Err(Error::InvalidArgument(\n                \"entrypoint function cannot be null; this is probably a malformed module\",\n            ));\n        }\n\n        let sig = self.module.get_signature(func.id);\n\n        // in typechecking these values, we can only really check that arguments are correct.\n        // in the future we might want to make return value use more type safe as well.\n\n        if sig.params.len() != args.len() {\n            return Err(Error::InvalidArgument(\n                \"entrypoint function signature mismatch (number of arguments is incorrect)\",\n            ));\n        }\n\n        for (param_ty, arg) in sig.params.iter().zip(args.iter()) {\n            if param_ty != &arg.value_type() {\n                return Err(Error::InvalidArgument(\n                    \"entrypoint function signature mismatch\",\n                ));\n            }\n        }\n\n        self.entrypoint = Some(func);\n\n        let mut args_with_vmctx = vec![Val::from(self.alloc.slot().heap)];\n        args_with_vmctx.extend_from_slice(args);\n\n        self.set_instruction_bound_delta(inst_count_bound);\n\n        let self_ptr = self as *mut _;\n        Context::init_with_callback(\n            unsafe { self.alloc.stack_u64_mut() },\n            &mut self.ctx,\n            execution::exit_guest_region,\n            self_ptr,\n            func.ptr.as_usize(),\n            &args_with_vmctx,\n        )?;\n\n        self.install_activator();\n        self.swap_and_return(async_context)\n    }\n\n    /// Prepare the guest so that it will update its execution domain upon entry.\n    ///\n    /// This mutates the context's registers so that an activation function that will be run after\n    /// performing a context switch. This function (`enter_guest_region`) will mark the guest as\n    /// terminable before continuing to whatever guest code we want to run.\n    ///\n    /// `lucet_context_activate` takes three arguments in the following registers:\n    ///   * rdi: the data for the entry callback.\n    ///   * rsi: the address of the entry callback.\n    ///   * rbx: the address of the guest code to execute.\n    ///\n    /// The appropriate value for `rbx` is the top of the guest stack, which we would otherwise\n    /// return to and start executing immediately. For `rdi`, we want to pass our callback data\n    /// (a raw pointer to the instance). This will be passed as the first argument to the entry\n    /// function, which is responsible for updating the kill state's execution domain.\n    ///\n    /// See `lucet_runtime_internals::context::lucet_context_activate`, and\n    /// `execution::enter_guest_region` for more info.\n    // TODO KTM 2020-03-13: This should be a method on `Context`.\n    fn install_activator(&mut self) {\n        unsafe {\n            // Get a raw pointer to the top of the guest stack.\n            let top_of_stack = self.ctx.gpr.rsp as *mut u64;\n            // Move the guest code address to rbx, and then put the address of the activation thunk\n            // at the top of the stack, so that we will start execution at `enter_guest_region`.\n            self.ctx.gpr.rbx = *top_of_stack;\n            *top_of_stack = crate::context::lucet_context_activate as u64;\n            // Pass a pointer to our guest-side entrypoint bootstrap code in `rsi`, and then put\n            // its first argument (a raw pointer to `self`) in `rdi`.\n            self.ctx.gpr.rsi = execution::enter_guest_region as u64;\n            self.ctx.gpr.rdi = self.ctx.callback_data_ptr() as u64;\n        }\n    }\n\n    /// The core routine for context switching into a guest, and extracting a result.\n    ///\n    /// This must only be called for an instance in a ready, non-fatally faulted, or yielded state,\n    /// or in the not-started state on the start function. The public wrappers around this function\n    /// should make sure the state is appropriate.\n    fn swap_and_return(&mut self, async_context: bool) -> Result<InternalRunResult, Error> {\n        let is_start_func = self\n            .entrypoint\n            .expect(\"we always have an entrypoint by now\")\n            .is_start_func;\n        debug_assert!(\n            self.state.is_ready()\n                || self.state.is_not_started() && is_start_func\n                || (self.state.is_faulted() && !self.state.is_fatal())\n                || self.state.is_yielded()\n                || self.state.is_bound_expired()\n        );\n        self.state = State::Running { async_context };\n\n        let res = self.with_current_instance(|i| {\n            i.with_signals_on(|i| {\n                HOST_CTX.with(|host_ctx| {\n                    // Save the current context into `host_ctx`, and jump to the guest context. The\n                    // lucet context is linked to host_ctx, so it will return here after it finishes,\n                    // successfully or otherwise.\n                    unsafe { Context::swap(&mut *host_ctx.get(), &mut i.ctx) };\n                    Ok(())\n                })\n            })\n        });\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        self.lock_testpoints\n            .instance_after_clearing_current_instance\n            .check();\n\n        if let Err(e) = res {\n            // Something went wrong setting up or tearing down the signal handlers and signal\n            // stack. This is an error, but we don't want it to mask an error that may have arisen\n            // due to a guest fault or guest termination. So, we set the state back to `Ready` or\n            // `NotStarted` only if it is still `Running`, which likely indicates we never even made\n            // it into the guest.\n            //\n            // As of 2020-03-20, the only early return points in the code above happen before the\n            // guest would be able to run, so this should always transition from running to\n            // ready or not started if there's an error.\n            if let State::Running { .. } = self.state {\n                if is_start_func {\n                    self.state = State::NotStarted;\n                } else {\n                    self.state = State::Ready;\n                }\n            }\n            return Err(e);\n        }\n\n        // Sandbox has jumped back to the host process, indicating it has either:\n        //\n        // * returned: state should be `Running`; transition to `Ready` and return a RunResult\n        // * yielded: state should be `Yielding`; transition to `Yielded` and return a RunResult\n        // * trapped: state should be `Faulted`; populate details and return an error or call a handler as appropriate\n        // * terminated: state should be `Terminating`; transition to `Terminated` and return the termination details as an Err\n        //\n        // The state should never be `Ready`, `Terminated`, `Yielded`, or `Transitioning` at this point\n\n        // Set transitioning state temporarily so that we can move values out of the current state\n        let st = mem::replace(&mut self.state, State::Transitioning);\n\n        if !st.is_yielding() && !st.is_bound_expired() {\n            // If the instance is *not* yielding, initialize a fresh `KillState` for subsequent\n            // executions, which will invalidate any existing `KillSwitch`'s weak references.\n            #[cfg(feature = \"concurrent_testpoints\")]\n            {\n                self.kill_state = Arc::new(KillState::new(Arc::clone(&self.lock_testpoints)));\n            }\n            #[cfg(not(feature = \"concurrent_testpoints\"))]\n            {\n                self.kill_state = Arc::new(KillState::default());\n            }\n        }\n\n        match st {\n            State::Running { .. } => {\n                let retval = self.ctx.get_untyped_retval();\n                self.state = State::Ready;\n                Ok(RunResult::Returned(retval).into())\n            }\n            State::Terminating { details, .. } => {\n                self.state = State::Terminated;\n                Err(Error::RuntimeTerminated(details).into())\n            }\n            State::Yielding { val, expecting } => {\n                self.state = State::Yielded { expecting };\n                Ok(RunResult::Yielded(val).into())\n            }\n            State::Faulted {\n                mut details,\n                siginfo,\n                context,\n            } => {\n                // Sandbox is no longer runnable. It's unsafe to determine all error details in the signal\n                // handler, so we fill in extra details here.\n                //\n                // FIXME after lucet-module is complete it should be possible to fill this in without\n                // consulting the process symbol table\n                details.rip_addr_details = self\n                    .module\n                    .addr_details(details.rip_addr as *const c_void)?;\n\n                // fill the state back in with the updated details in case fatal handlers need it\n                self.state = State::Faulted {\n                    details: details.clone(),\n                    siginfo,\n                    context,\n                };\n\n                if details.fatal {\n                    // Some errors indicate that the guest is not functioning correctly or that\n                    // the loaded code violated some assumption, so bail out via the fatal\n                    // handler.\n\n                    // Run the C-style fatal handler, if it exists.\n                    if let Some(h) = self.c_fatal_handler {\n                        unsafe { h(self as *mut Instance) }\n                    }\n\n                    // If there is no C-style fatal handler, or if it (erroneously) returns,\n                    // call the Rust handler that we know will not return\n                    (self.fatal_handler)(self)\n                } else {\n                    // leave the full fault details in the instance state, and return the\n                    // higher-level info to the user\n                    Err(Error::RuntimeFault(details).into())\n                }\n            }\n            State::BoundExpired => {\n                self.state = State::BoundExpired;\n                Ok(InternalRunResult::BoundExpired)\n            }\n            State::NotStarted\n            | State::Ready\n            | State::Terminated\n            | State::Yielded { .. }\n            | State::Transitioning => Err(lucet_format_err!(\n                \"\\\"impossible\\\" state found in `swap_and_return()`: {}\",\n                st\n            )),\n        }\n    }\n\n    fn with_current_instance<F, R>(&mut self, f: F) -> Result<R, Error>\n    where\n        F: FnOnce(&mut Instance) -> Result<R, Error>,\n    {\n        CURRENT_INSTANCE.with(|current_instance| {\n            let mut current_instance = current_instance.borrow_mut();\n            lucet_ensure!(\n                current_instance.is_none(),\n                \"no instance must already be running on this thread\"\n            ); // safety: `self` is not null if we are in this function\n            *current_instance = Some(unsafe { NonNull::new_unchecked(self) });\n            Ok(())\n        })?;\n\n        self.kill_state.schedule(unsafe { pthread_self() });\n\n        let res = f(self);\n\n        self.kill_state.deschedule();\n\n        CURRENT_INSTANCE.with(|current_instance| {\n            *current_instance.borrow_mut() = None;\n        });\n\n        res\n    }\n}\n\n/// Information about a runtime fault.\n///\n/// Runtime faults are raised implictly by signal handlers that return `SignalBehavior::Default` in\n/// response to signals arising while a guest is running.\n#[derive(Clone, Debug)]\npub struct FaultDetails {\n    /// If true, the instance's `fatal_handler` will be called.\n    pub fatal: bool,\n    /// Information about the type of fault that occurred.\n    pub trapcode: Option<TrapCode>,\n    /// The instruction pointer where the fault occurred.\n    pub rip_addr: uintptr_t,\n    /// Extra information about the instruction pointer's location, if available.\n    pub rip_addr_details: Option<module::AddrDetails>,\n}\n\nimpl std::fmt::Display for FaultDetails {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        if self.fatal {\n            write!(f, \"fault FATAL \")?;\n        } else {\n            write!(f, \"fault \")?;\n        }\n\n        if let Some(trapcode) = self.trapcode {\n            write!(f, \"{:?} \", trapcode)?;\n        } else {\n            write!(f, \"TrapCode::UNKNOWN \")?;\n        }\n\n        write!(f, \"code at address {:p}\", self.rip_addr as *const c_void)?;\n\n        if let Some(ref addr_details) = self.rip_addr_details {\n            if let Some(ref fname) = addr_details.file_name {\n                let sname = addr_details.sym_name.as_deref().unwrap_or(\"<unknown>\");\n                write!(f, \" (symbol {}:{})\", fname, sname)?;\n            }\n            if addr_details.in_module_code {\n                write!(f, \" (inside module code)\")\n            } else {\n                write!(f, \" (not inside module code)\")\n            }\n        } else {\n            write!(f, \" (unknown whether in module)\")\n        }\n    }\n}\n\n/// Information about a terminated guest.\npub enum TerminationDetails {\n    /// Returned when a signal handler terminates the instance.\n    Signal,\n    /// Returned when `get_embed_ctx` or `get_embed_ctx_mut` are used with a type that is not present.\n    CtxNotFound,\n    /// Returned when the type of the value passed to `Instance::resume_with_val()` does not match\n    /// the type expected by `Vmctx::yield_expecting_val()` or `Vmctx::yield_val_expecting_val`, or\n    /// if `Instance::resume()` was called when a value was expected.\n    ///\n    /// **Note**: If you see this termination value, please report it as a Lucet bug. The types of\n    /// resumed values are dynamically checked by `Instance::resume()` and\n    /// `Instance::resume_with_val()`, so this should never arise.\n    YieldTypeMismatch,\n    /// Returned when dynamic borrowing rules of methods like `Vmctx::heap()` are violated.\n    BorrowError(&'static str),\n    /// Calls to `lucet_hostcall_terminate` provide a payload for use by the embedder.\n    Provided {\n        type_name: &'static str,\n        provided: Box<dyn Any + 'static>,\n    },\n    /// The instance was terminated by its `KillSwitch`.\n    Remote,\n    /// A panic occurred during a hostcall other than the specialized panic used to implement\n    /// Lucet runtime features.\n    ///\n    /// Panics are raised by the Lucet runtime in order to unwind the hostcall before jumping back\n    /// to the host context for any of the reasons described by the variants of this type. The panic\n    /// payload in that case is a already a `TerminationDetails` value.\n    ///\n    /// This variant is created when any type other than `TerminationDetails` is the payload of a\n    /// panic arising during a hostcall, meaning it was not intentionally raised by the Lucet\n    /// runtime.\n    ///\n    /// The panic payload contained in this variant should be rethrown using\n    /// [`resume_unwind`](https://doc.rust-lang.org/std/panic/fn.resume_unwind.html) once returned\n    /// to the host context.\n    ///\n    /// Note that this variant will be removed once cross-FFI unwinding support lands in\n    /// [Rust](https://github.com/rust-lang/rfcs/pull/2945) and\n    /// [Lucet](https://github.com/bytecodealliance/lucet/pull/254).\n    OtherPanic(Box<dyn Any + Send + 'static>),\n    /// The instance was terminated by `Vmctx::block_on` being called from an instance\n    /// that isnt running in an async context\n    BlockOnNeedsAsync,\n}\n\nimpl TerminationDetails {\n    pub fn provide<A: Any + 'static>(details: A) -> Self {\n        TerminationDetails::Provided {\n            type_name: std::any::type_name::<A>(),\n            provided: Box::new(details),\n        }\n    }\n    pub fn provided_details(&self) -> Option<&dyn Any> {\n        match self {\n            TerminationDetails::Provided { provided, .. } => Some(provided.as_ref()),\n            _ => None,\n        }\n    }\n    /// Try to interpret the termination details as a provided exit code.\n    ///\n    /// The most consistent form of `TerminationDetails::Provided` comes from Lucet's\n    /// implementation of `proc_exit`, which exits with a `Provided` holding the given exit code.\n    /// For cases where a Lucet user simply wants \"`proc_exit` or continue panicking\" behavior,\n    /// `as_exitcode` can simplify handling `TerminationDetails`.\n    pub fn as_exitcode(&self) -> Option<u32> {\n        match self {\n            TerminationDetails::Provided { provided, .. } => {\n                // I apologize for this load-bearing `as u32`.\n                // Wasi uses an u32 for the proc_exist status (`lucet_wasi::Exitcode`) in the\n                // witx. However, wasmtime::Trap exit status is an i32, so the\n                // wiggle::Trap::I32Exit variant mirrors Wasmtime. The `as u32` lets this method\n                // return a type equivalent to `lucet_wasi::Exitcode`, but users interested in the\n                // full range of `wiggle::Trap` will have to handle an i32 variant.\n                match provided.downcast_ref::<wiggle::Trap>() {\n                    Some(wiggle::Trap::I32Exit(code)) => Some(*code as u32),\n                    _ => None,\n                }\n            }\n            _ => None,\n        }\n    }\n}\n\n// Because of deref coercions, the code above was tricky to get right-\n// test that a string makes it through\n#[test]\nfn termination_details_any_typing() {\n    let hello = \"hello, world\".to_owned();\n    let details = TerminationDetails::provide(hello.clone());\n    let provided = details.provided_details().expect(\"got Provided\");\n    assert_eq!(\n        provided.downcast_ref::<String>().expect(\"right type\"),\n        &hello\n    );\n}\n\nimpl PartialEq for TerminationDetails {\n    fn eq(&self, rhs: &TerminationDetails) -> bool {\n        use TerminationDetails::*;\n        match (self, rhs) {\n            (Signal, Signal) => true,\n            (BorrowError(msg1), BorrowError(msg2)) => msg1 == msg2,\n            (CtxNotFound, CtxNotFound) => true,\n            (BlockOnNeedsAsync, BlockOnNeedsAsync) => true,\n            // can't compare `Any`\n            _ => false,\n        }\n    }\n}\n\nimpl std::fmt::Debug for TerminationDetails {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(f, \"TerminationDetails::\")?;\n        match self {\n            TerminationDetails::Signal => write!(f, \"Signal\"),\n            TerminationDetails::BorrowError(msg) => write!(f, \"BorrowError({})\", msg),\n            TerminationDetails::CtxNotFound => write!(f, \"CtxNotFound\"),\n            TerminationDetails::YieldTypeMismatch => write!(f, \"YieldTypeMismatch\"),\n            TerminationDetails::Provided { type_name, .. } => write!(f, \"Provided({})\", type_name),\n            TerminationDetails::Remote => write!(f, \"Remote\"),\n            TerminationDetails::OtherPanic(_) => write!(f, \"OtherPanic(Any)\"),\n            TerminationDetails::BlockOnNeedsAsync => write!(f, \"BlockOnNeedsAsync\"),\n        }\n    }\n}\n\nunsafe impl Send for TerminationDetails {}\nunsafe impl Sync for TerminationDetails {}\n\n/// The value yielded by an instance through a [`Vmctx`](vmctx/struct.Vmctx.html) and returned to\n/// the host.\npub struct YieldedVal {\n    val: Box<dyn Any + Send + 'static>,\n}\n\nimpl std::fmt::Debug for YieldedVal {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        if self.is_none() {\n            write!(f, \"YieldedVal {{ val: None }}\")\n        } else {\n            write!(f, \"YieldedVal {{ val: Some }}\")\n        }\n    }\n}\n\nimpl YieldedVal {\n    pub(crate) fn new<A: Any + Send + 'static>(val: A) -> Self {\n        YieldedVal { val: Box::new(val) }\n    }\n\n    /// Returns `true` if the guest yielded the parameterized type.\n    pub fn is<A: Any>(&self) -> bool {\n        self.val.is::<A>()\n    }\n\n    /// Returns `true` if the guest yielded without a value.\n    pub fn is_none(&self) -> bool {\n        self.is::<EmptyYieldVal>()\n    }\n\n    /// Returns `true` if the guest yielded with a value.\n    pub fn is_some(&self) -> bool {\n        !self.is_none()\n    }\n\n    /// Attempt to downcast the yielded value to a concrete type, returning the original\n    /// `YieldedVal` if unsuccessful.\n    pub fn downcast<A: Any + Send + 'static>(self) -> Result<Box<A>, YieldedVal> {\n        match self.val.downcast() {\n            Ok(val) => Ok(val),\n            Err(val) => Err(YieldedVal { val }),\n        }\n    }\n\n    /// Returns a reference to the yielded value if it is present and of type `A`, or `None` if it\n    /// isn't.\n    pub fn downcast_ref<A: Any + Send + 'static>(&self) -> Option<&A> {\n        self.val.downcast_ref()\n    }\n}\n\n/// A marker value to indicate a yield or resume with no value.\n///\n/// This exists to unify the implementations of the various operators, and should only ever be\n/// created by internal code.\n#[derive(Debug)]\npub(crate) struct EmptyYieldVal;\n\nfn default_fatal_handler(inst: &Instance) -> ! {\n    panic!(\"> instance {:p} had fatal error: {}\", inst, inst.state);\n}\n"], "fixing_code": ["pub mod execution;\nmod siginfo_ext;\npub mod signals;\npub mod state;\n\npub use crate::instance::execution::{KillError, KillState, KillSuccess, KillSwitch};\npub use crate::instance::signals::{signal_handler_none, SignalBehavior, SignalHandler};\npub use crate::instance::state::State;\n\nuse crate::alloc::Alloc;\nuse crate::context::Context;\nuse crate::embed_ctx::CtxMap;\nuse crate::error::Error;\n#[cfg(feature = \"concurrent_testpoints\")]\nuse crate::lock_testpoints::LockTestpoints;\nuse crate::module::{self, FunctionHandle, Global, GlobalValue, Module, TrapCode};\nuse crate::sysdeps::HOST_PAGE_SIZE_EXPECTED;\nuse crate::val::{UntypedRetVal, Val};\nuse crate::vmctx::Vmctx;\nuse crate::WASM_PAGE_SIZE;\nuse libc::{c_void, pthread_self, siginfo_t, uintptr_t};\nuse lucet_module::InstanceRuntimeData;\nuse memoffset::offset_of;\nuse std::any::Any;\nuse std::cell::{BorrowError, BorrowMutError, Ref, RefCell, RefMut, UnsafeCell};\nuse std::convert::TryFrom;\nuse std::marker::PhantomData;\nuse std::mem;\nuse std::mem::ManuallyDrop;\nuse std::ops::{Deref, DerefMut};\nuse std::ptr::{self, NonNull};\nuse std::sync::Arc;\n\npub const LUCET_INSTANCE_MAGIC: u64 = 746_932_922;\n\nthread_local! {\n    /// The host context.\n    ///\n    /// Control returns here implicitly due to the setup in `Context::init()` when guest functions\n    /// return normally. Control can return here explicitly from signal handlers when the guest\n    /// program needs to be terminated.\n    ///\n    /// This is an `UnsafeCell` due to nested borrows. The context must be borrowed mutably when\n    /// swapping to the guest context, which means that borrow exists for the entire time the guest\n    /// function runs even though the mutation to the host context is done only at the beginning of\n    /// the swap. Meanwhile, the signal handler can run at any point during the guest function, and\n    /// so it also must be able to immutably borrow the host context if it needs to swap back. The\n    /// runtime borrowing constraints for a `RefCell` are therefore too strict for this variable.\n    pub(crate) static HOST_CTX: UnsafeCell<Context> = UnsafeCell::new(Context::new());\n\n    /// The currently-running `Instance`, if one exists.\n    pub(crate) static CURRENT_INSTANCE: RefCell<Option<NonNull<Instance>>> = RefCell::new(None);\n}\n\n/// A smart pointer to an [`Instance`](struct.Instance.html) that properly manages cleanup when dropped.\n///\n/// Instances are always stored in memory backed by a `Region`; we never want to create one directly\n/// with the Rust allocator. This type allows us to abide by that rule while also having an owned\n/// type that cleans up the instance when we are done with it.\n///\n/// Since this type implements `Deref` and `DerefMut` to `Instance`, it can usually be treated as\n/// though it were a `&mut Instance`.\npub struct InstanceHandle {\n    inst: NonNull<Instance>,\n    needs_inst_drop: bool,\n}\n\n// raw pointer lint\nunsafe impl Send for InstanceHandle {}\n\n/// Create a new `InstanceHandle`.\n///\n/// This is not meant for public consumption, but rather is used to make implementations of\n/// `Region`.\npub fn new_instance_handle(\n    instance: *mut Instance,\n    module: Arc<dyn Module>,\n    alloc: Alloc,\n    embed_ctx: CtxMap,\n) -> Result<InstanceHandle, Error> {\n    let inst = NonNull::new(instance)\n        .ok_or_else(|| lucet_format_err!(\"instance pointer is null; this is a bug\"))?;\n\n    lucet_ensure!(\n        unsafe { inst.as_ref().magic } != LUCET_INSTANCE_MAGIC,\n        \"created a new instance handle in memory with existing instance magic; this is a bug\"\n    );\n\n    let mut handle = InstanceHandle {\n        inst,\n        needs_inst_drop: false,\n    };\n\n    let inst = Instance::new(alloc, module, embed_ctx);\n\n    unsafe {\n        // this is wildly unsafe! you must be very careful to not let the drop impls run on the\n        // uninitialized fields; see\n        // <https://doc.rust-lang.org/std/mem/fn.forget.html#use-case-1>\n\n        // write the whole struct into place over the uninitialized page\n        ptr::write(&mut *handle, inst);\n    };\n\n    handle.needs_inst_drop = true;\n\n    handle.reset()?;\n\n    Ok(handle)\n}\n\npub fn instance_handle_to_raw(mut inst: InstanceHandle) -> *mut Instance {\n    inst.needs_inst_drop = false;\n    inst.inst.as_ptr()\n}\n\npub unsafe fn instance_handle_from_raw(\n    ptr: *mut Instance,\n    needs_inst_drop: bool,\n) -> InstanceHandle {\n    InstanceHandle {\n        inst: NonNull::new_unchecked(ptr),\n        needs_inst_drop,\n    }\n}\n\n// Safety argument for these deref impls: the instance's `Alloc` field contains an `Arc` to the\n// region that backs this memory, keeping the page containing the `Instance` alive as long as the\n// region exists\n\nimpl Deref for InstanceHandle {\n    type Target = Instance;\n    fn deref(&self) -> &Self::Target {\n        unsafe { self.inst.as_ref() }\n    }\n}\n\nimpl DerefMut for InstanceHandle {\n    fn deref_mut(&mut self) -> &mut Self::Target {\n        unsafe { self.inst.as_mut() }\n    }\n}\n\nimpl Drop for InstanceHandle {\n    fn drop(&mut self) {\n        if self.needs_inst_drop {\n            unsafe {\n                let inst = self.inst.as_mut();\n\n                // The `inst.alloc` field manages the memory of the instance\n                // itself. Note, though, that this field is in a `ManuallyDrop`\n                // so it won't get dropped automatically in `drop_in_place`.\n                // This is the point where we take over that precise drop.\n                //\n                // By using `take` here we're basically calling `ptr::read`\n                // which \"duplicates\" the `alloc` since the `alloc` local\n                // variable here is the exact same as `inst.alloc`. All we do\n                // with `inst`, though, is call `drop_in_place`, which\n                // invalidates every other field in `inst`.\n                let alloc: Alloc = ManuallyDrop::take(&mut inst.alloc);\n\n                // drop the actual instance\n                std::ptr::drop_in_place(inst);\n\n                // Now that we're 100% done with the instance, destructors and\n                // all, we can release the memory of the instance back to the\n                // original allocator from whence it came (be it mmap or uffd\n                // based). This will run the \"official\" destructor for `Alloc`\n                // which internally does the release. Note that after this\n                // operation the `inst` pointer is invalid and can no longer be\n                // used.\n                drop(alloc);\n            }\n        }\n    }\n}\n\n/// A Lucet program, together with its dedicated memory and signal handlers.\n///\n/// This is the primary interface for running programs, examining return values, and accessing the\n/// WebAssembly heap.\n///\n/// `Instance`s are never created by runtime users directly, but rather are acquired from\n/// [`Region`](../region/trait.Region.html)s and often accessed through\n/// [`InstanceHandle`](../instance/struct.InstanceHandle.html) smart pointers. This guarantees that instances\n/// and their fields are never moved in memory, otherwise raw pointers in the metadata could be\n/// unsafely invalidated.\n///\n/// An instance occupies one 4096-byte page in memory, with a layout like:\n/// ```text\n/// 0xXXXXX000:\n///   Instance {\n///     .magic\n///     .embed_ctx\n///      ... etc ...\n///   }\n///\n///   // unused space\n///\n///   InstanceInternals {\n///     .globals\n///     .instruction_counter\n///   } // last address *inside* `InstanceInternals` is 0xXXXXXFFF\n/// 0xXXXXY000: // start of next page, VMContext points here\n///   Heap {\n///     ..\n///   }\n/// ```\n///\n/// This layout allows modules to tightly couple to a handful of fields related to the instance,\n/// rather than possibly requiring compiler-side changes (and recompiles) whenever `Instance`\n/// changes.\n///\n/// It also obligates `Instance` to be immediately followed by the heap, but otherwise leaves the\n/// locations of the stack, globals, and any other data, to be implementation-defined by the\n/// `Region` that actually creates `Slot`s onto which `Instance` are mapped.\n/// For information about the layout of all instance-related memory, see the documentation of\n/// [MmapRegion](../region/mmap/struct.MmapRegion.html).\n#[repr(C)]\n#[repr(align(4096))]\npub struct Instance {\n    /// Used to catch bugs in pointer math used to find the address of the instance\n    magic: u64,\n\n    /// The embedding context is a map containing embedder-specific values that are used to\n    /// implement hostcalls\n    pub(crate) embed_ctx: CtxMap,\n\n    /// The program (WebAssembly module) that is the entrypoint for the instance.\n    pub(crate) module: Arc<dyn Module>,\n\n    /// The `Context` in which the guest program runs\n    pub(crate) ctx: Context,\n\n    /// Instance state and error information\n    pub(crate) state: State,\n\n    /// Small mutexed state used for remote kill switch functionality\n    pub(crate) kill_state: Arc<KillState>,\n\n    #[cfg(feature = \"concurrent_testpoints\")]\n    /// Conditionally-present helpers to force permutations of possible races in testing.\n    pub lock_testpoints: Arc<LockTestpoints>,\n\n    /// The memory allocated for this instance.\n    ///\n    /// Note that this is in a `ManuallyDrop` because this manages the memory of\n    /// this `Instance` itself. To have precise control over this memory we\n    /// handle this in `Drop for InstanceHandle`.\n    alloc: ManuallyDrop<Alloc>,\n\n    /// Handler run for signals that do not arise from a known WebAssembly trap, or that involve\n    /// memory outside of the current instance.\n    fatal_handler: fn(&Instance) -> !,\n\n    /// A fatal handler set from C\n    c_fatal_handler: Option<unsafe extern \"C\" fn(*mut Instance)>,\n\n    /// Handler run when `SIGBUS`, `SIGFPE`, `SIGILL`, or `SIGSEGV` are caught by the instance thread.\n    signal_handler: Box<\n        dyn Fn(\n            &Instance,\n            &Option<TrapCode>,\n            libc::c_int,\n            *const siginfo_t,\n            *const c_void,\n        ) -> SignalBehavior,\n    >,\n\n    /// Whether to ensure the Lucet signal handler is installed when running this instance.\n    ensure_signal_handler_installed: bool,\n\n    /// Whether to install an alternate signal stack while the instance is running.\n    ensure_sigstack_installed: bool,\n\n    /// Pointer to the function used as the entrypoint.\n    entrypoint: Option<FunctionHandle>,\n\n    /// The value passed back to the guest when resuming a yielded instance.\n    pub(crate) resumed_val: Option<Box<dyn Any + 'static>>,\n\n    pub(crate) memory_limiter: Option<Box<dyn MemoryLimiter + Send + Sync + 'static>>,\n\n    /// `_padding` must be the last member of the structure.\n    /// This marks where the padding starts to make the structure exactly 4096 bytes long.\n    /// It is also used to compute the size of the structure up to that point, i.e. without padding.\n    _padding: (),\n}\n\n#[async_trait::async_trait]\npub trait MemoryLimiter {\n    async fn memory_growing(&mut self, current: usize, desired: usize) -> bool;\n    fn memory_grow_failed(&mut self, _error: &Error) {}\n}\n\n/// Users of `Instance` must be very careful about when instances are dropped!\n///\n/// Typically you will not have to worry about this, as InstanceHandle will robustly handle\n/// Instance drop semantics. If an instance is dropped, and the Region it's in has already dropped,\n/// it may contain the last reference counted pointer to its Region. If so, when Instance's\n/// destructor runs, Region will be dropped, and may free or otherwise invalidate the memory that\n/// this Instance exists in, *while* the Instance destructor is executing.\nimpl Drop for Instance {\n    fn drop(&mut self) {\n        // Reset magic to indicate this instance\n        // is no longer valid\n        self.magic = 0;\n    }\n}\n\n/// The result of running or resuming an [`Instance`](struct.Instance.html).\n#[derive(Debug)]\npub enum RunResult {\n    /// An instance returned with a value.\n    ///\n    /// The actual type of the contained value depends on the return type of the guest function that\n    /// was called. For guest functions with no return value, it is undefined behavior to do\n    /// anything with this value.\n    Returned(UntypedRetVal),\n    /// An instance yielded, potentially with a value.\n    ///\n    /// This arises when a hostcall invokes one of the\n    /// [`Vmctx::yield_*()`](vmctx/struct.Vmctx.html#method.yield_) family of methods. Depending on which\n    /// variant is used, the `YieldedVal` may contain a value passed from the guest context to the\n    /// host.\n    ///\n    /// An instance that has yielded may only be resumed\n    /// ([with](struct.Instance.html#method.resume_with_val) or\n    /// [without](struct.Instance.html#method.resume) a value to returned to the guest),\n    /// [reset](struct.Instance.html#method.reset), or dropped. Attempting to run an instance from a\n    /// new entrypoint after it has yielded but without first resetting will result in an error.\n    Yielded(YieldedVal),\n}\n\nimpl RunResult {\n    /// Try to get a return value from a run result, returning `Error::InstanceNotReturned` if the\n    /// instance instead yielded.\n    pub fn returned(self) -> Result<UntypedRetVal, Error> {\n        match self {\n            RunResult::Returned(rv) => Ok(rv),\n            RunResult::Yielded(_) => Err(Error::InstanceNotReturned),\n        }\n    }\n\n    /// Try to get a reference to a return value from a run result, returning\n    /// `Error::InstanceNotReturned` if the instance instead yielded.\n    pub fn returned_ref(&self) -> Result<&UntypedRetVal, Error> {\n        match self {\n            RunResult::Returned(rv) => Ok(rv),\n            RunResult::Yielded(_) => Err(Error::InstanceNotReturned),\n        }\n    }\n\n    /// Returns `true` if the instance returned a value.\n    pub fn is_returned(&self) -> bool {\n        self.returned_ref().is_ok()\n    }\n\n    /// Unwraps a run result into a return value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead yielded, with a panic message including the passed message.\n    pub fn expect_returned(self, msg: &str) -> UntypedRetVal {\n        self.returned().expect(msg)\n    }\n\n    /// Unwraps a run result into a returned value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead yielded.\n    pub fn unwrap_returned(self) -> UntypedRetVal {\n        self.returned().unwrap()\n    }\n\n    /// Try to get a yielded value from a run result, returning `Error::InstanceNotYielded` if the\n    /// instance instead returned.\n    pub fn yielded(self) -> Result<YieldedVal, Error> {\n        match self {\n            RunResult::Returned(_) => Err(Error::InstanceNotYielded),\n            RunResult::Yielded(yv) => Ok(yv),\n        }\n    }\n\n    /// Try to get a reference to a yielded value from a run result, returning\n    /// `Error::InstanceNotYielded` if the instance instead returned.\n    pub fn yielded_ref(&self) -> Result<&YieldedVal, Error> {\n        match self {\n            RunResult::Returned(_) => Err(Error::InstanceNotYielded),\n            RunResult::Yielded(yv) => Ok(yv),\n        }\n    }\n\n    /// Returns `true` if the instance yielded.\n    pub fn is_yielded(&self) -> bool {\n        self.yielded_ref().is_ok()\n    }\n\n    /// Returns `true` if the instance can be resumed: either it has yielded, or its bound has\n    /// expired.\n    pub fn can_resume(&self) -> bool {\n        self.is_yielded()\n    }\n\n    /// Returns `true` if the instance has yielded a value of the given type.\n    pub fn has_yielded<A: Any>(&self) -> bool {\n        match self {\n            RunResult::Yielded(yv) => yv.is::<A>(),\n            _ => false,\n        }\n    }\n\n    /// Unwraps a run result into a yielded value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead returned, with a panic message including the passed message.\n    pub fn expect_yielded(self, msg: &str) -> YieldedVal {\n        self.yielded().expect(msg)\n    }\n\n    /// Unwraps a run result into a yielded value.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the instance instead returned.\n    pub fn unwrap_yielded(self) -> YieldedVal {\n        self.yielded().unwrap()\n    }\n}\n\n/// An \"internal\" run result: either a `RunResult` or a bound expiration. We do not expose bound\n/// expirations to the caller directly; rather, we only handle them in `run_async()`.\npub(crate) enum InternalRunResult {\n    Normal(RunResult),\n    BoundExpired,\n}\n\nimpl InternalRunResult {\n    pub(crate) fn unwrap(self) -> RunResult {\n        match self {\n            InternalRunResult::Normal(result) => result,\n            InternalRunResult::BoundExpired => panic!(\"should not have had a runtime bound\"),\n        }\n    }\n}\n\nimpl std::convert::Into<InternalRunResult> for RunResult {\n    fn into(self) -> InternalRunResult {\n        InternalRunResult::Normal(self)\n    }\n}\n\n/// APIs that are internal, but useful to implementors of extension modules; you probably don't want\n/// this trait!\n///\n/// This is a trait rather than inherent `impl`s in order to keep the `lucet-runtime` API clean and\n/// safe.\npub trait InstanceInternal {\n    fn alloc(&self) -> &Alloc;\n    fn alloc_mut(&mut self) -> &mut Alloc;\n    fn module(&self) -> &dyn Module;\n    fn state(&self) -> &State;\n    fn valid_magic(&self) -> bool;\n}\n\nimpl InstanceInternal for Instance {\n    /// Get a reference to the instance's `Alloc`.\n    fn alloc(&self) -> &Alloc {\n        &self.alloc\n    }\n\n    /// Get a mutable reference to the instance's `Alloc`.\n    fn alloc_mut(&mut self) -> &mut Alloc {\n        &mut self.alloc\n    }\n\n    /// Get a reference to the instance's `Module`.\n    fn module(&self) -> &dyn Module {\n        self.module.deref()\n    }\n\n    /// Get a reference to the instance's `State`.\n    fn state(&self) -> &State {\n        &self.state\n    }\n\n    /// Check whether the instance magic is valid.\n    fn valid_magic(&self) -> bool {\n        self.magic == LUCET_INSTANCE_MAGIC\n    }\n}\n\n// Public API\nimpl Instance {\n    /// Run a function with arguments in the guest context at the given entrypoint.\n    ///\n    /// ```no_run\n    /// # use lucet_runtime_internals::instance::InstanceHandle;\n    /// # let instance: InstanceHandle = unimplemented!();\n    /// // regular execution yields `Ok(UntypedRetVal)`\n    /// let retval = instance.run(\"factorial\", &[5u64.into()]).unwrap().unwrap_returned();\n    /// assert_eq!(u64::from(retval), 120u64);\n    ///\n    /// // runtime faults yield `Err(Error)`\n    /// let result = instance.run(\"faulting_function\", &[]);\n    /// assert!(result.is_err());\n    /// ```\n    ///\n    /// # Safety\n    ///\n    /// This is unsafe in two ways:\n    ///\n    /// - The type of the entrypoint might not be correct. It might take a different number or\n    /// different types of arguments than are provided to `args`. It might not even point to a\n    /// function! We will likely add type information to `lucetc` output so we can dynamically check\n    /// the type in the future.\n    ///\n    /// - The entrypoint is foreign code. While we may be convinced that WebAssembly compiled to\n    /// native code by `lucetc` is safe, we do not have the same guarantee for the hostcalls that a\n    /// guest may invoke. They might be implemented in an unsafe language, so we must treat this\n    /// call as unsafe, just like any other FFI call.\n    ///\n    /// For the moment, we do not mark this as `unsafe` in the Rust type system, but that may change\n    /// in the future.\n    pub fn run(&mut self, entrypoint: &str, args: &[Val]) -> Result<RunResult, Error> {\n        let func = self.module.get_export_func(entrypoint)?;\n        Ok(self.run_func(func, &args, false, None)?.unwrap())\n    }\n\n    /// Run a function with arguments in the guest context from the [WebAssembly function\n    /// table](https://webassembly.github.io/spec/core/syntax/modules.html#tables).\n    ///\n    /// # Safety\n    ///\n    /// The same safety caveats of [`Instance::run()`](struct.Instance.html#method.run) apply.\n    pub fn run_func_idx(\n        &mut self,\n        table_idx: u32,\n        func_idx: u32,\n        args: &[Val],\n    ) -> Result<RunResult, Error> {\n        let func = self.module.get_func_from_idx(table_idx, func_idx)?;\n        Ok(self.run_func(func, &args, false, None)?.unwrap())\n    }\n\n    /// Resume execution of an instance that has yielded without providing a value to the guest.\n    ///\n    /// This should only be used when the guest yielded with\n    /// [`Vmctx::yield_()`](vmctx/struct.Vmctx.html#method.yield_) or\n    /// [`Vmctx::yield_val()`](vmctx/struct.Vmctx.html#method.yield_val).\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`](struct.Instance.html#method.run)\n    /// applies.\n    pub fn resume(&mut self) -> Result<RunResult, Error> {\n        self.resume_with_val(EmptyYieldVal)\n    }\n\n    /// Resume execution of an instance that has yielded, providing a value to the guest.\n    ///\n    /// The type of the provided value must match the type expected by\n    /// [`Vmctx::yield_expecting_val()`](vmctx/struct.Vmctx.html#method.yield_expecting_val) or\n    /// [`Vmctx::yield_val_expecting_val()`](vmctx/struct.Vmctx.html#method.yield_val_expecting_val).\n    ///\n    /// The provided value will be dynamically typechecked against the type the guest expects to\n    /// receive, and if that check fails, this call will fail with `Error::InvalidArgument`.\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`](struct.Instance.html#method.run)\n    /// applies.\n    pub fn resume_with_val<A: Any + 'static>(&mut self, val: A) -> Result<RunResult, Error> {\n        Ok(self.resume_with_val_impl(val, false, None)?.unwrap())\n    }\n\n    pub(crate) fn resume_with_val_impl<A: Any + 'static>(\n        &mut self,\n        val: A,\n        async_context: bool,\n        max_insn_count: Option<u64>,\n    ) -> Result<InternalRunResult, Error> {\n        match &self.state {\n            State::Yielded { expecting, .. } => {\n                // make sure the resumed value is of the right type\n                if !expecting.is::<PhantomData<A>>() {\n                    return Err(Error::InvalidArgument(\n                        \"type mismatch between yielded instance expected value and resumed value\",\n                    ));\n                }\n            }\n            _ => return Err(Error::InvalidArgument(\"can only resume a yielded instance\")),\n        }\n\n        self.resumed_val = Some(Box::new(val) as Box<dyn Any + 'static>);\n\n        self.set_instruction_bound_delta(max_insn_count);\n        self.swap_and_return(async_context)\n    }\n\n    /// Resume execution of an instance that has previously reached an instruction bound.\n    ///\n    /// The execution slice that begins with this call is bounded by the new bound provided.\n    ///\n    /// This should only be used when `run_func()` returned a `RunResult::Bounded`. This is an\n    /// internal function used by `run_async()`.\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`](struct.Instance.html#method.run)\n    /// applies.\n    pub(crate) fn resume_bounded(\n        &mut self,\n        max_insn_count: u64,\n    ) -> Result<InternalRunResult, Error> {\n        if !self.state.is_bound_expired() {\n            return Err(Error::InvalidArgument(\n                \"can only call resume_bounded() on an instance that hit an instruction bound\",\n            ));\n        }\n        self.set_instruction_bound_delta(Some(max_insn_count));\n        self.swap_and_return(true)\n    }\n\n    /// Run the module's [start function][start], if one exists.\n    ///\n    /// If there is no start function in the module, this does nothing.\n    ///\n    /// If the module contains a start function, you must run it before running any other exported\n    /// functions. If an instance is reset, you must run the start function again.\n    ///\n    /// Start functions may assume that Wasm tables and memories are properly initialized, but may\n    /// not assume that imported functions or globals are available.\n    ///\n    /// # Errors\n    ///\n    /// In addition to the errors that can be returned from [`Instance::run()`][run], this can also\n    /// return `Error::StartYielded` if the start function attempts to yield. This should not arise\n    /// as long as the start function does not attempt to use any imported functions.\n    ///\n    /// This also returns `Error::StartAlreadyRun` if the start function has already run since the\n    /// instance was created or last reset.\n    ///\n    /// Wasm start functions are not allowed to call imported functions. If the start function\n    /// attempts to do so, the instance will be terminated with\n    /// `TerminationDetails::StartCalledImportFunc`.\n    ///\n    /// # Safety\n    ///\n    /// The foreign code safety caveat of [`Instance::run()`][run]\n    /// applies.\n    ///\n    /// [run]: struct.Instance.html#method.run\n    /// [start]: https://webassembly.github.io/spec/core/syntax/modules.html#syntax-start\n    pub fn run_start(&mut self) -> Result<(), Error> {\n        if let Some(start) = self.module.get_start_func()? {\n            if !self.is_not_started() {\n                return Err(Error::StartAlreadyRun);\n            }\n            self.run_func(start, &[], false, None)?;\n        }\n        Ok(())\n    }\n\n    /// Reset the instance's heap and global variables to their initial state.\n    ///\n    /// The WebAssembly `start` section, if present, will need to be re-run with\n    /// [`Instance::run_start()`][run_start] before running any other exported functions.\n    ///\n    /// The embedder contexts present at instance creation or added with\n    /// [`Instance::insert_embed_ctx()`](struct.Instance.html#method.insert_embed_ctx) are not\n    /// modified by this call; it is the embedder's responsibility to clear or reset their state if\n    /// necessary.\n    ///\n    /// This will also reinitialize the kill state, which means that any outstanding\n    /// [`KillSwitch`](struct.KillSwitch.html) objects will be unable to terminate this instance.\n    /// It is the embedder's responsibility to initialize new `KillSwitch`es after resetting an\n    /// instance.\n    ///\n    /// [run_start]: struct.Instance.html#method.run\n    pub fn reset(&mut self) -> Result<(), Error> {\n        self.alloc.reset_heap(self.module.as_ref())?;\n        let globals = unsafe { self.alloc.globals_mut() };\n        let mod_globals = self.module.globals();\n        for (i, v) in mod_globals.iter().enumerate() {\n            globals[i] = match v.global() {\n                Global::Import { .. } => {\n                    return Err(Error::Unsupported(format!(\n                        \"global imports are unsupported; found: {:?}\",\n                        v\n                    )));\n                }\n                Global::Def(def) => def.init_val(),\n            };\n        }\n\n        if self.module.get_start_func()?.is_some() {\n            self.state = State::NotStarted;\n        } else {\n            self.state = State::Ready;\n        }\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        {\n            self.kill_state = Arc::new(KillState::new(Arc::clone(&self.lock_testpoints)));\n        }\n        #[cfg(not(feature = \"concurrent_testpoints\"))]\n        {\n            self.kill_state = Arc::new(KillState::new());\n        }\n\n        Ok(())\n    }\n\n    /// Grow the guest memory by the given number of WebAssembly pages.\n    ///\n    /// On success, returns the number of pages that existed before the call.\n    pub fn grow_memory(&mut self, additional_pages: u32) -> Result<u32, Error> {\n        let additional_bytes = additional_pages\n            .checked_mul(WASM_PAGE_SIZE)\n            .ok_or_else(|| lucet_format_err!(\"additional pages larger than wasm address space\",))?;\n        let orig_len = self\n            .alloc\n            .expand_heap(additional_bytes, self.module.as_ref())?;\n        Ok(orig_len / WASM_PAGE_SIZE)\n    }\n\n    /// Grow memory from a hostcall context.\n    pub fn grow_memory_from_hostcall(\n        &mut self,\n        vmctx: &Vmctx,\n        additional_pages: u32,\n    ) -> Result<u32, Error> {\n        // Use a function so that we can report all Errs via memory_grow_failed.\n        fn aux(\n            instance: &mut Instance,\n            vmctx: &Vmctx,\n            additional_pages: u32,\n        ) -> Result<u32, Error> {\n            // Calculate current and desired bytes\n            let current_bytes = instance.alloc.heap_len();\n            let additional_bytes =\n                additional_pages\n                    .checked_mul(WASM_PAGE_SIZE)\n                    .ok_or_else(|| {\n                        lucet_format_err!(\"additional pages larger than wasm address space\",)\n                    })? as usize;\n            let desired_bytes = additional_bytes\n                .checked_add(current_bytes)\n                .ok_or_else(|| lucet_format_err!(\"desired bytes overflow\",))?;\n            // Let the limiter reject the grow\n            if let Some(ref mut limiter) = instance.memory_limiter {\n                if !vmctx.block_on(async move {\n                    limiter.memory_growing(current_bytes, desired_bytes).await\n                }) {\n                    lucet_bail!(\"memory limiter denied growth\");\n                }\n            }\n            // Try the grow itself\n            instance.grow_memory(additional_pages)\n        }\n\n        match aux(self, vmctx, additional_pages) {\n            Ok(n) => Ok(n),\n            Err(e) => {\n                if let Some(ref mut limiter) = self.memory_limiter {\n                    limiter.memory_grow_failed(&e);\n                    Err(e)\n                } else {\n                    Err(e)\n                }\n            }\n        }\n    }\n\n    /// Return the WebAssembly heap as a slice of bytes.\n    pub fn heap(&self) -> &[u8] {\n        unsafe { self.alloc.heap() }\n    }\n\n    /// Return the WebAssembly heap as a mutable slice of bytes.\n    pub fn heap_mut(&mut self) -> &mut [u8] {\n        unsafe { self.alloc.heap_mut() }\n    }\n\n    /// Return the WebAssembly heap as a slice of `u32`s.\n    pub fn heap_u32(&self) -> &[u32] {\n        unsafe { self.alloc.heap_u32() }\n    }\n\n    /// Return the WebAssembly heap as a mutable slice of `u32`s.\n    pub fn heap_u32_mut(&mut self) -> &mut [u32] {\n        unsafe { self.alloc.heap_u32_mut() }\n    }\n\n    /// Return the WebAssembly globals as a slice of `i64`s.\n    pub fn globals(&self) -> &[GlobalValue] {\n        unsafe { self.alloc.globals() }\n    }\n\n    /// Return the WebAssembly globals as a mutable slice of `i64`s.\n    pub fn globals_mut(&mut self) -> &mut [GlobalValue] {\n        unsafe { self.alloc.globals_mut() }\n    }\n\n    /// Check whether a given range in the host address space overlaps with the memory that backs\n    /// the instance heap.\n    pub fn check_heap<T>(&self, ptr: *const T, len: usize) -> bool {\n        self.alloc.mem_in_heap(ptr, len)\n    }\n\n    /// Check whether a context value of a particular type exists.\n    pub fn contains_embed_ctx<T: Any>(&self) -> bool {\n        self.embed_ctx.contains::<T>()\n    }\n\n    /// Get a reference to a context value of a particular type, if it exists.\n    pub fn get_embed_ctx<T: Any>(&self) -> Option<Result<Ref<'_, T>, BorrowError>> {\n        self.embed_ctx.try_get::<T>()\n    }\n\n    /// Get a mutable reference to a context value of a particular type, if it exists.\n    pub fn get_embed_ctx_mut<T: Any>(&self) -> Option<Result<RefMut<'_, T>, BorrowMutError>> {\n        self.embed_ctx.try_get_mut::<T>()\n    }\n\n    /// Insert a context value.\n    ///\n    /// If a context value of the same type already existed, it is returned.\n    pub fn insert_embed_ctx<T: Any>(&mut self, x: T) -> Option<T> {\n        self.embed_ctx.insert(x)\n    }\n\n    /// Remove a context value of a particular type, returning it if it exists.\n    pub fn remove_embed_ctx<T: Any>(&mut self) -> Option<T> {\n        self.embed_ctx.remove::<T>()\n    }\n\n    /// Set the handler run when `SIGBUS`, `SIGFPE`, `SIGILL`, or `SIGSEGV` are caught by the\n    /// instance thread.\n    ///\n    /// In most cases, these signals are unrecoverable for the instance that raised them, but do not\n    /// affect the rest of the process.\n    ///\n    /// The default signal handler returns\n    /// [`SignalBehavior::Default`](enum.SignalBehavior.html#variant.Default), which yields a\n    /// runtime fault error.\n    ///\n    /// The signal handler must be\n    /// [signal-safe](http://man7.org/linux/man-pages/man7/signal-safety.7.html).\n    pub fn set_signal_handler<H>(&mut self, handler: H)\n    where\n        H: 'static\n            + Fn(\n                &Instance,\n                &Option<TrapCode>,\n                libc::c_int,\n                *const siginfo_t,\n                *const c_void,\n            ) -> SignalBehavior,\n    {\n        self.signal_handler = Box::new(handler) as Box<SignalHandler>;\n    }\n\n    /// Set the handler run for signals that do not arise from a known WebAssembly trap, or that\n    /// involve memory outside of the current instance.\n    ///\n    /// Fatal signals are not only unrecoverable for the instance that raised them, but may\n    /// compromise the correctness of the rest of the process if unhandled.\n    ///\n    /// The default fatal handler calls `panic!()`.\n    pub fn set_fatal_handler(&mut self, handler: fn(&Instance) -> !) {\n        self.fatal_handler = handler;\n    }\n\n    /// Set the fatal handler to a C-compatible function.\n    ///\n    /// This is a separate interface, because C functions can't return the `!` type. Like the\n    /// regular `fatal_handler`, it is not expected to return, but we cannot enforce that through\n    /// types.\n    ///\n    /// When a fatal error occurs, this handler is run first, and then the regular `fatal_handler`\n    /// runs in case it returns.\n    pub fn set_c_fatal_handler(&mut self, handler: unsafe extern \"C\" fn(*mut Instance)) {\n        self.c_fatal_handler = Some(handler);\n    }\n\n    /// Set whether the Lucet signal handler is installed when running or resuming this instance\n    /// (`true` by default).\n    ///\n    /// If this is `true`, the Lucet runtime checks whether its signal handler is installed whenever\n    /// an instance runs, installing it if it is not present, and uninstalling it when there are no\n    /// longer any Lucet instances running. If this is `false`, that check is disabled, which can\n    /// improve performance when running or resuming an instance.\n    ///\n    /// Use `install_lucet_signal_handler()` and `remove_lucet_signal_handler()` to manually install\n    /// or remove the signal handler.\n    ///\n    /// # Safety\n    ///\n    /// If the Lucet signal handler is not installed when an instance runs, WebAssembly traps such\n    /// as division by zero, assertion failures, or out-of-bounds memory access will raise signals\n    /// to the default signal handlers, usually causing the entire process to crash.\n    pub fn ensure_signal_handler_installed(&mut self, ensure: bool) {\n        self.ensure_signal_handler_installed = ensure;\n    }\n\n    /// Set whether an alternate signal stack is installed for the current thread when running or\n    /// resuming this instance (`true` by default).\n    ///\n    /// If this is `true`, the Lucet runtime installs an alternate signal stack whenever an instance\n    /// runs, and uninstalls it afterwards. If this is `false`, the signal stack is not\n    /// automatically manipulated.\n    ///\n    /// The automatically-installed signal stack uses space allocated in the instance's `Region`,\n    /// sized according to the `signal_stack_size` field of the region's `Limits`.\n    ///\n    /// If you wish to instead provide your own signal stack, we recommend using a stack of size\n    /// `DEFAULT_SIGNAL_STACK_SIZE`, which varies depending on platform and optimization level.\n    ///\n    /// Signal stacks are installed on a per-thread basis, so any thread that runs this instance\n    /// must have a signal stack installed.\n    ///\n    /// # Safety\n    ///\n    /// If an alternate signal stack is not installed when an instance runs, there may not be enough\n    /// stack space for the Lucet signal handler to run. If the signal handler runs out of stack\n    /// space, a double fault could occur and crash the entire process, or the program could\n    /// continue with corrupted memory.\n    pub fn ensure_sigstack_installed(&mut self, ensure: bool) {\n        self.ensure_sigstack_installed = ensure;\n    }\n\n    pub fn kill_switch(&self) -> KillSwitch {\n        KillSwitch::new(Arc::downgrade(&self.kill_state))\n    }\n\n    pub fn is_not_started(&self) -> bool {\n        self.state.is_not_started()\n    }\n\n    pub fn is_ready(&self) -> bool {\n        self.state.is_ready()\n    }\n\n    pub fn is_yielded(&self) -> bool {\n        self.state.is_yielded()\n    }\n\n    pub fn is_bound_expired(&self) -> bool {\n        self.state.is_bound_expired()\n    }\n\n    pub fn is_faulted(&self) -> bool {\n        self.state.is_faulted()\n    }\n\n    pub fn is_terminated(&self) -> bool {\n        self.state.is_terminated()\n    }\n\n    // This needs to be public as it's used in the expansion of `lucet_hostcalls`, available for\n    // external use. But you *really* shouldn't have to call this yourself, so we're going to keep\n    // it out of rustdoc.\n    #[doc(hidden)]\n    pub fn uninterruptable<T, F: FnOnce() -> T>(&mut self, f: F) -> T {\n        self.kill_state.begin_hostcall();\n        let res = f();\n        let stop_reason = self.kill_state.end_hostcall();\n\n        if let Some(termination_details) = stop_reason {\n            // TODO: once we have unwinding, panic here instead so we unwind host frames\n            unsafe {\n                self.terminate(termination_details);\n            }\n        }\n\n        res\n    }\n\n    #[inline]\n    pub fn get_instruction_count(&self) -> Option<u64> {\n        if self.module.is_instruction_count_instrumented() {\n            let implicits = self.get_instance_implicits();\n            let sum = implicits.instruction_count_bound + implicits.instruction_count_adj;\n            // This invariant is ensured as we always set up the fields to have a positive sum, and\n            // generated code only increments `adj`.\n            debug_assert!(sum >= 0);\n            return Some(sum as u64);\n        }\n        None\n    }\n\n    /// Set the total instruction count and bound.\n    #[inline]\n    pub fn set_instruction_count_and_bound(&mut self, instruction_count: u64, bound: u64) {\n        let implicits = self.get_instance_implicits_mut();\n        let instruction_count =\n            i64::try_from(instruction_count).expect(\"instruction count too large\");\n        let bound = i64::try_from(bound).expect(\"bound too large\");\n        // These two sum to `instruction_count`, which must be non-negative.\n        implicits.instruction_count_bound = bound;\n        implicits.instruction_count_adj = instruction_count - bound;\n    }\n\n    /// Set the instruction bound to be `delta` above the current count.\n    ///\n    /// See the comments on `instruction_count_adj` in `InstanceRuntimeData` for more details on\n    /// how this bound works; most relevant is that a bound-yield is only triggered if the bound\n    /// value is *crossed*, but not if execution *begins* with the value exceeded. Hence `delta`\n    /// must be greater than zero for this to set up the instance state to trigger a yield.\n    #[inline]\n    pub fn set_instruction_bound_delta(&mut self, delta: Option<u64>) {\n        let implicits = self.get_instance_implicits_mut();\n        let sum = implicits.instruction_count_adj + implicits.instruction_count_bound;\n        let delta = delta.unwrap_or(i64::MAX as u64);\n        let delta = i64::try_from(delta).expect(\"delta too large\");\n        implicits.instruction_count_bound = sum.wrapping_add(delta);\n        implicits.instruction_count_adj = -delta;\n    }\n\n    #[inline]\n    pub fn set_hostcall_stack_reservation(&mut self) {\n        let slot = self\n            .alloc\n            .slot\n            .as_ref()\n            .expect(\"reachable instance has a slot\");\n\n        let reservation = slot.limits.hostcall_reservation;\n\n        // The `.stack` field is a pointer to the lowest address of the stack - the start of its\n        // allocation. Because the stack grows downward, this is the end of the stack space. So the\n        // limit we'll need to check for hostcalls is some reserved space upwards from here, to\n        // meet some guest stack pointer early.\n        self.get_instance_implicits_mut().stack_limit = slot.stack as u64 + reservation as u64;\n    }\n\n    /// Set a memory limiter for the instance.\n    ///\n    /// If set, this instance must be run asynchronously via [`InstanceHandle::run_async`]\n    pub fn set_memory_limiter(&mut self, limiter: Box<dyn MemoryLimiter + Send + Sync + 'static>) {\n        self.memory_limiter = Some(limiter)\n    }\n}\n\n// Private API\nimpl Instance {\n    fn new(alloc: Alloc, module: Arc<dyn Module>, embed_ctx: CtxMap) -> Self {\n        let globals_ptr = alloc.slot().globals as *mut i64;\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        let lock_testpoints = Arc::new(LockTestpoints::new());\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        let kill_state = Arc::new(KillState::new(Arc::clone(&lock_testpoints)));\n        #[cfg(not(feature = \"concurrent_testpoints\"))]\n        let kill_state = Arc::new(KillState::new());\n\n        let mut inst = Instance {\n            magic: LUCET_INSTANCE_MAGIC,\n            embed_ctx,\n            module,\n            ctx: Context::new(),\n            state: State::Ready,\n            kill_state,\n            #[cfg(feature = \"concurrent_testpoints\")]\n            lock_testpoints,\n            alloc: ManuallyDrop::new(alloc),\n            fatal_handler: default_fatal_handler,\n            c_fatal_handler: None,\n            signal_handler: Box::new(signal_handler_none) as Box<SignalHandler>,\n            ensure_signal_handler_installed: true,\n            ensure_sigstack_installed: true,\n            entrypoint: None,\n            resumed_val: None,\n            memory_limiter: None,\n            _padding: (),\n        };\n        inst.set_globals_ptr(globals_ptr);\n        inst.set_instruction_count_and_bound(0, 0);\n        // Ensure the hostcall limit tracked in this instance's guest-shared data is up-to-date.\n        inst.set_hostcall_stack_reservation();\n\n        assert_eq!(mem::size_of::<Instance>(), HOST_PAGE_SIZE_EXPECTED);\n        let unpadded_size = offset_of!(Instance, _padding);\n        assert!(unpadded_size <= HOST_PAGE_SIZE_EXPECTED - mem::size_of::<*mut i64>());\n        inst\n    }\n\n    // The globals pointer must be stored right before the end of the structure, padded to the page size,\n    // so that it is 8 bytes before the heap.\n    // For this reason, the alignment of the structure is set to 4096, and we define accessors that\n    // read/write the globals pointer as bytes [4096-8..4096] of that structure represented as raw bytes.\n    // InstanceRuntimeData is placed such that it ends at the end of the page this `Instance` starts\n    // on. So we can access it by *self + PAGE_SIZE - size_of::<InstanceRuntimeData>\n    #[inline]\n    fn get_instance_implicits(&self) -> &InstanceRuntimeData {\n        unsafe {\n            let implicits_ptr = (self as *const _ as *const u8)\n                .add(HOST_PAGE_SIZE_EXPECTED - mem::size_of::<InstanceRuntimeData>())\n                as *const InstanceRuntimeData;\n            mem::transmute::<*const InstanceRuntimeData, &InstanceRuntimeData>(implicits_ptr)\n        }\n    }\n\n    #[inline]\n    fn get_instance_implicits_mut(&mut self) -> &mut InstanceRuntimeData {\n        unsafe {\n            let implicits_ptr = (self as *mut _ as *mut u8)\n                .add(HOST_PAGE_SIZE_EXPECTED - mem::size_of::<InstanceRuntimeData>())\n                as *mut InstanceRuntimeData;\n            mem::transmute::<*mut InstanceRuntimeData, &mut InstanceRuntimeData>(implicits_ptr)\n        }\n    }\n\n    #[allow(dead_code)]\n    #[inline]\n    fn get_globals_ptr(&self) -> *mut i64 {\n        self.get_instance_implicits().globals_ptr\n    }\n\n    #[inline]\n    fn set_globals_ptr(&mut self, globals_ptr: *mut i64) {\n        self.get_instance_implicits_mut().globals_ptr = globals_ptr\n    }\n\n    /// Run a function in guest context at the given entrypoint.\n    pub(crate) fn run_func(\n        &mut self,\n        func: FunctionHandle,\n        args: &[Val],\n        async_context: bool,\n        inst_count_bound: Option<u64>,\n    ) -> Result<InternalRunResult, Error> {\n        let needs_start = self.state.is_not_started() && !func.is_start_func;\n        if needs_start {\n            return Err(Error::InstanceNeedsStart);\n        }\n\n        let is_ready = self.state.is_ready();\n        let is_starting = self.state.is_not_started() && func.is_start_func;\n        let is_non_fatally_faulted = self.state.is_faulted() && !self.state.is_fatal();\n        if !(is_ready || is_starting || is_non_fatally_faulted) {\n            return Err(Error::InvalidArgument(\n                \"instance must be ready, starting, or non-fatally faulted\",\n            ));\n        }\n        if func.ptr.as_usize() == 0 {\n            return Err(Error::InvalidArgument(\n                \"entrypoint function cannot be null; this is probably a malformed module\",\n            ));\n        }\n\n        let sig = self.module.get_signature(func.id);\n\n        // in typechecking these values, we can only really check that arguments are correct.\n        // in the future we might want to make return value use more type safe as well.\n\n        if sig.params.len() != args.len() {\n            return Err(Error::InvalidArgument(\n                \"entrypoint function signature mismatch (number of arguments is incorrect)\",\n            ));\n        }\n\n        for (param_ty, arg) in sig.params.iter().zip(args.iter()) {\n            if param_ty != &arg.value_type() {\n                return Err(Error::InvalidArgument(\n                    \"entrypoint function signature mismatch\",\n                ));\n            }\n        }\n\n        self.entrypoint = Some(func);\n\n        let mut args_with_vmctx = vec![Val::from(self.alloc.slot().heap)];\n        args_with_vmctx.extend_from_slice(args);\n\n        self.set_instruction_bound_delta(inst_count_bound);\n\n        let self_ptr = self as *mut _;\n        Context::init_with_callback(\n            unsafe { self.alloc.stack_u64_mut() },\n            &mut self.ctx,\n            execution::exit_guest_region,\n            self_ptr,\n            func.ptr.as_usize(),\n            &args_with_vmctx,\n        )?;\n\n        self.install_activator();\n        self.swap_and_return(async_context)\n    }\n\n    /// Prepare the guest so that it will update its execution domain upon entry.\n    ///\n    /// This mutates the context's registers so that an activation function that will be run after\n    /// performing a context switch. This function (`enter_guest_region`) will mark the guest as\n    /// terminable before continuing to whatever guest code we want to run.\n    ///\n    /// `lucet_context_activate` takes three arguments in the following registers:\n    ///   * rdi: the data for the entry callback.\n    ///   * rsi: the address of the entry callback.\n    ///   * rbx: the address of the guest code to execute.\n    ///\n    /// The appropriate value for `rbx` is the top of the guest stack, which we would otherwise\n    /// return to and start executing immediately. For `rdi`, we want to pass our callback data\n    /// (a raw pointer to the instance). This will be passed as the first argument to the entry\n    /// function, which is responsible for updating the kill state's execution domain.\n    ///\n    /// See `lucet_runtime_internals::context::lucet_context_activate`, and\n    /// `execution::enter_guest_region` for more info.\n    // TODO KTM 2020-03-13: This should be a method on `Context`.\n    fn install_activator(&mut self) {\n        unsafe {\n            // Get a raw pointer to the top of the guest stack.\n            let top_of_stack = self.ctx.gpr.rsp as *mut u64;\n            // Move the guest code address to rbx, and then put the address of the activation thunk\n            // at the top of the stack, so that we will start execution at `enter_guest_region`.\n            self.ctx.gpr.rbx = *top_of_stack;\n            *top_of_stack = crate::context::lucet_context_activate as u64;\n            // Pass a pointer to our guest-side entrypoint bootstrap code in `rsi`, and then put\n            // its first argument (a raw pointer to `self`) in `rdi`.\n            self.ctx.gpr.rsi = execution::enter_guest_region as u64;\n            self.ctx.gpr.rdi = self.ctx.callback_data_ptr() as u64;\n        }\n    }\n\n    /// The core routine for context switching into a guest, and extracting a result.\n    ///\n    /// This must only be called for an instance in a ready, non-fatally faulted, or yielded state,\n    /// or in the not-started state on the start function. The public wrappers around this function\n    /// should make sure the state is appropriate.\n    fn swap_and_return(&mut self, async_context: bool) -> Result<InternalRunResult, Error> {\n        let is_start_func = self\n            .entrypoint\n            .expect(\"we always have an entrypoint by now\")\n            .is_start_func;\n        debug_assert!(\n            self.state.is_ready()\n                || self.state.is_not_started() && is_start_func\n                || (self.state.is_faulted() && !self.state.is_fatal())\n                || self.state.is_yielded()\n                || self.state.is_bound_expired()\n        );\n        self.state = State::Running { async_context };\n\n        let res = self.with_current_instance(|i| {\n            i.with_signals_on(|i| {\n                HOST_CTX.with(|host_ctx| {\n                    // Save the current context into `host_ctx`, and jump to the guest context. The\n                    // lucet context is linked to host_ctx, so it will return here after it finishes,\n                    // successfully or otherwise.\n                    unsafe { Context::swap(&mut *host_ctx.get(), &mut i.ctx) };\n                    Ok(())\n                })\n            })\n        });\n\n        #[cfg(feature = \"concurrent_testpoints\")]\n        self.lock_testpoints\n            .instance_after_clearing_current_instance\n            .check();\n\n        if let Err(e) = res {\n            // Something went wrong setting up or tearing down the signal handlers and signal\n            // stack. This is an error, but we don't want it to mask an error that may have arisen\n            // due to a guest fault or guest termination. So, we set the state back to `Ready` or\n            // `NotStarted` only if it is still `Running`, which likely indicates we never even made\n            // it into the guest.\n            //\n            // As of 2020-03-20, the only early return points in the code above happen before the\n            // guest would be able to run, so this should always transition from running to\n            // ready or not started if there's an error.\n            if let State::Running { .. } = self.state {\n                if is_start_func {\n                    self.state = State::NotStarted;\n                } else {\n                    self.state = State::Ready;\n                }\n            }\n            return Err(e);\n        }\n\n        // Sandbox has jumped back to the host process, indicating it has either:\n        //\n        // * returned: state should be `Running`; transition to `Ready` and return a RunResult\n        // * yielded: state should be `Yielding`; transition to `Yielded` and return a RunResult\n        // * trapped: state should be `Faulted`; populate details and return an error or call a handler as appropriate\n        // * terminated: state should be `Terminating`; transition to `Terminated` and return the termination details as an Err\n        //\n        // The state should never be `Ready`, `Terminated`, `Yielded`, or `Transitioning` at this point\n\n        // Set transitioning state temporarily so that we can move values out of the current state\n        let st = mem::replace(&mut self.state, State::Transitioning);\n\n        if !st.is_yielding() && !st.is_bound_expired() {\n            // If the instance is *not* yielding, initialize a fresh `KillState` for subsequent\n            // executions, which will invalidate any existing `KillSwitch`'s weak references.\n            #[cfg(feature = \"concurrent_testpoints\")]\n            {\n                self.kill_state = Arc::new(KillState::new(Arc::clone(&self.lock_testpoints)));\n            }\n            #[cfg(not(feature = \"concurrent_testpoints\"))]\n            {\n                self.kill_state = Arc::new(KillState::default());\n            }\n        }\n\n        match st {\n            State::Running { .. } => {\n                let retval = self.ctx.get_untyped_retval();\n                self.state = State::Ready;\n                Ok(RunResult::Returned(retval).into())\n            }\n            State::Terminating { details, .. } => {\n                self.state = State::Terminated;\n                Err(Error::RuntimeTerminated(details).into())\n            }\n            State::Yielding { val, expecting } => {\n                self.state = State::Yielded { expecting };\n                Ok(RunResult::Yielded(val).into())\n            }\n            State::Faulted {\n                mut details,\n                siginfo,\n                context,\n            } => {\n                // Sandbox is no longer runnable. It's unsafe to determine all error details in the signal\n                // handler, so we fill in extra details here.\n                //\n                // FIXME after lucet-module is complete it should be possible to fill this in without\n                // consulting the process symbol table\n                details.rip_addr_details = self\n                    .module\n                    .addr_details(details.rip_addr as *const c_void)?;\n\n                // fill the state back in with the updated details in case fatal handlers need it\n                self.state = State::Faulted {\n                    details: details.clone(),\n                    siginfo,\n                    context,\n                };\n\n                if details.fatal {\n                    // Some errors indicate that the guest is not functioning correctly or that\n                    // the loaded code violated some assumption, so bail out via the fatal\n                    // handler.\n\n                    // Run the C-style fatal handler, if it exists.\n                    if let Some(h) = self.c_fatal_handler {\n                        unsafe { h(self as *mut Instance) }\n                    }\n\n                    // If there is no C-style fatal handler, or if it (erroneously) returns,\n                    // call the Rust handler that we know will not return\n                    (self.fatal_handler)(self)\n                } else {\n                    // leave the full fault details in the instance state, and return the\n                    // higher-level info to the user\n                    Err(Error::RuntimeFault(details).into())\n                }\n            }\n            State::BoundExpired => {\n                self.state = State::BoundExpired;\n                Ok(InternalRunResult::BoundExpired)\n            }\n            State::NotStarted\n            | State::Ready\n            | State::Terminated\n            | State::Yielded { .. }\n            | State::Transitioning => Err(lucet_format_err!(\n                \"\\\"impossible\\\" state found in `swap_and_return()`: {}\",\n                st\n            )),\n        }\n    }\n\n    fn with_current_instance<F, R>(&mut self, f: F) -> Result<R, Error>\n    where\n        F: FnOnce(&mut Instance) -> Result<R, Error>,\n    {\n        CURRENT_INSTANCE.with(|current_instance| {\n            let mut current_instance = current_instance.borrow_mut();\n            lucet_ensure!(\n                current_instance.is_none(),\n                \"no instance must already be running on this thread\"\n            ); // safety: `self` is not null if we are in this function\n            *current_instance = Some(unsafe { NonNull::new_unchecked(self) });\n            Ok(())\n        })?;\n\n        self.kill_state.schedule(unsafe { pthread_self() });\n\n        let res = f(self);\n\n        self.kill_state.deschedule();\n\n        CURRENT_INSTANCE.with(|current_instance| {\n            *current_instance.borrow_mut() = None;\n        });\n\n        res\n    }\n}\n\n/// Information about a runtime fault.\n///\n/// Runtime faults are raised implictly by signal handlers that return `SignalBehavior::Default` in\n/// response to signals arising while a guest is running.\n#[derive(Clone, Debug)]\npub struct FaultDetails {\n    /// If true, the instance's `fatal_handler` will be called.\n    pub fatal: bool,\n    /// Information about the type of fault that occurred.\n    pub trapcode: Option<TrapCode>,\n    /// The instruction pointer where the fault occurred.\n    pub rip_addr: uintptr_t,\n    /// Extra information about the instruction pointer's location, if available.\n    pub rip_addr_details: Option<module::AddrDetails>,\n}\n\nimpl std::fmt::Display for FaultDetails {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        if self.fatal {\n            write!(f, \"fault FATAL \")?;\n        } else {\n            write!(f, \"fault \")?;\n        }\n\n        if let Some(trapcode) = self.trapcode {\n            write!(f, \"{:?} \", trapcode)?;\n        } else {\n            write!(f, \"TrapCode::UNKNOWN \")?;\n        }\n\n        write!(f, \"code at address {:p}\", self.rip_addr as *const c_void)?;\n\n        if let Some(ref addr_details) = self.rip_addr_details {\n            if let Some(ref fname) = addr_details.file_name {\n                let sname = addr_details.sym_name.as_deref().unwrap_or(\"<unknown>\");\n                write!(f, \" (symbol {}:{})\", fname, sname)?;\n            }\n            if addr_details.in_module_code {\n                write!(f, \" (inside module code)\")\n            } else {\n                write!(f, \" (not inside module code)\")\n            }\n        } else {\n            write!(f, \" (unknown whether in module)\")\n        }\n    }\n}\n\n/// Information about a terminated guest.\npub enum TerminationDetails {\n    /// Returned when a signal handler terminates the instance.\n    Signal,\n    /// Returned when `get_embed_ctx` or `get_embed_ctx_mut` are used with a type that is not present.\n    CtxNotFound,\n    /// Returned when the type of the value passed to `Instance::resume_with_val()` does not match\n    /// the type expected by `Vmctx::yield_expecting_val()` or `Vmctx::yield_val_expecting_val`, or\n    /// if `Instance::resume()` was called when a value was expected.\n    ///\n    /// **Note**: If you see this termination value, please report it as a Lucet bug. The types of\n    /// resumed values are dynamically checked by `Instance::resume()` and\n    /// `Instance::resume_with_val()`, so this should never arise.\n    YieldTypeMismatch,\n    /// Returned when dynamic borrowing rules of methods like `Vmctx::heap()` are violated.\n    BorrowError(&'static str),\n    /// Calls to `lucet_hostcall_terminate` provide a payload for use by the embedder.\n    Provided {\n        type_name: &'static str,\n        provided: Box<dyn Any + 'static>,\n    },\n    /// The instance was terminated by its `KillSwitch`.\n    Remote,\n    /// A panic occurred during a hostcall other than the specialized panic used to implement\n    /// Lucet runtime features.\n    ///\n    /// Panics are raised by the Lucet runtime in order to unwind the hostcall before jumping back\n    /// to the host context for any of the reasons described by the variants of this type. The panic\n    /// payload in that case is a already a `TerminationDetails` value.\n    ///\n    /// This variant is created when any type other than `TerminationDetails` is the payload of a\n    /// panic arising during a hostcall, meaning it was not intentionally raised by the Lucet\n    /// runtime.\n    ///\n    /// The panic payload contained in this variant should be rethrown using\n    /// [`resume_unwind`](https://doc.rust-lang.org/std/panic/fn.resume_unwind.html) once returned\n    /// to the host context.\n    ///\n    /// Note that this variant will be removed once cross-FFI unwinding support lands in\n    /// [Rust](https://github.com/rust-lang/rfcs/pull/2945) and\n    /// [Lucet](https://github.com/bytecodealliance/lucet/pull/254).\n    OtherPanic(Box<dyn Any + Send + 'static>),\n    /// The instance was terminated by `Vmctx::block_on` being called from an instance\n    /// that isnt running in an async context\n    BlockOnNeedsAsync,\n}\n\nimpl TerminationDetails {\n    pub fn provide<A: Any + 'static>(details: A) -> Self {\n        TerminationDetails::Provided {\n            type_name: std::any::type_name::<A>(),\n            provided: Box::new(details),\n        }\n    }\n    pub fn provided_details(&self) -> Option<&dyn Any> {\n        match self {\n            TerminationDetails::Provided { provided, .. } => Some(provided.as_ref()),\n            _ => None,\n        }\n    }\n    /// Try to interpret the termination details as a provided exit code.\n    ///\n    /// The most consistent form of `TerminationDetails::Provided` comes from Lucet's\n    /// implementation of `proc_exit`, which exits with a `Provided` holding the given exit code.\n    /// For cases where a Lucet user simply wants \"`proc_exit` or continue panicking\" behavior,\n    /// `as_exitcode` can simplify handling `TerminationDetails`.\n    pub fn as_exitcode(&self) -> Option<u32> {\n        match self {\n            TerminationDetails::Provided { provided, .. } => {\n                // I apologize for this load-bearing `as u32`.\n                // Wasi uses an u32 for the proc_exist status (`lucet_wasi::Exitcode`) in the\n                // witx. However, wasmtime::Trap exit status is an i32, so the\n                // wiggle::Trap::I32Exit variant mirrors Wasmtime. The `as u32` lets this method\n                // return a type equivalent to `lucet_wasi::Exitcode`, but users interested in the\n                // full range of `wiggle::Trap` will have to handle an i32 variant.\n                match provided.downcast_ref::<wiggle::Trap>() {\n                    Some(wiggle::Trap::I32Exit(code)) => Some(*code as u32),\n                    _ => None,\n                }\n            }\n            _ => None,\n        }\n    }\n}\n\n// Because of deref coercions, the code above was tricky to get right-\n// test that a string makes it through\n#[test]\nfn termination_details_any_typing() {\n    let hello = \"hello, world\".to_owned();\n    let details = TerminationDetails::provide(hello.clone());\n    let provided = details.provided_details().expect(\"got Provided\");\n    assert_eq!(\n        provided.downcast_ref::<String>().expect(\"right type\"),\n        &hello\n    );\n}\n\nimpl PartialEq for TerminationDetails {\n    fn eq(&self, rhs: &TerminationDetails) -> bool {\n        use TerminationDetails::*;\n        match (self, rhs) {\n            (Signal, Signal) => true,\n            (BorrowError(msg1), BorrowError(msg2)) => msg1 == msg2,\n            (CtxNotFound, CtxNotFound) => true,\n            (BlockOnNeedsAsync, BlockOnNeedsAsync) => true,\n            // can't compare `Any`\n            _ => false,\n        }\n    }\n}\n\nimpl std::fmt::Debug for TerminationDetails {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(f, \"TerminationDetails::\")?;\n        match self {\n            TerminationDetails::Signal => write!(f, \"Signal\"),\n            TerminationDetails::BorrowError(msg) => write!(f, \"BorrowError({})\", msg),\n            TerminationDetails::CtxNotFound => write!(f, \"CtxNotFound\"),\n            TerminationDetails::YieldTypeMismatch => write!(f, \"YieldTypeMismatch\"),\n            TerminationDetails::Provided { type_name, .. } => write!(f, \"Provided({})\", type_name),\n            TerminationDetails::Remote => write!(f, \"Remote\"),\n            TerminationDetails::OtherPanic(_) => write!(f, \"OtherPanic(Any)\"),\n            TerminationDetails::BlockOnNeedsAsync => write!(f, \"BlockOnNeedsAsync\"),\n        }\n    }\n}\n\nunsafe impl Send for TerminationDetails {}\nunsafe impl Sync for TerminationDetails {}\n\n/// The value yielded by an instance through a [`Vmctx`](vmctx/struct.Vmctx.html) and returned to\n/// the host.\npub struct YieldedVal {\n    val: Box<dyn Any + Send + 'static>,\n}\n\nimpl std::fmt::Debug for YieldedVal {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        if self.is_none() {\n            write!(f, \"YieldedVal {{ val: None }}\")\n        } else {\n            write!(f, \"YieldedVal {{ val: Some }}\")\n        }\n    }\n}\n\nimpl YieldedVal {\n    pub(crate) fn new<A: Any + Send + 'static>(val: A) -> Self {\n        YieldedVal { val: Box::new(val) }\n    }\n\n    /// Returns `true` if the guest yielded the parameterized type.\n    pub fn is<A: Any>(&self) -> bool {\n        self.val.is::<A>()\n    }\n\n    /// Returns `true` if the guest yielded without a value.\n    pub fn is_none(&self) -> bool {\n        self.is::<EmptyYieldVal>()\n    }\n\n    /// Returns `true` if the guest yielded with a value.\n    pub fn is_some(&self) -> bool {\n        !self.is_none()\n    }\n\n    /// Attempt to downcast the yielded value to a concrete type, returning the original\n    /// `YieldedVal` if unsuccessful.\n    pub fn downcast<A: Any + Send + 'static>(self) -> Result<Box<A>, YieldedVal> {\n        match self.val.downcast() {\n            Ok(val) => Ok(val),\n            Err(val) => Err(YieldedVal { val }),\n        }\n    }\n\n    /// Returns a reference to the yielded value if it is present and of type `A`, or `None` if it\n    /// isn't.\n    pub fn downcast_ref<A: Any + Send + 'static>(&self) -> Option<&A> {\n        self.val.downcast_ref()\n    }\n}\n\n/// A marker value to indicate a yield or resume with no value.\n///\n/// This exists to unify the implementations of the various operators, and should only ever be\n/// created by internal code.\n#[derive(Debug)]\npub(crate) struct EmptyYieldVal;\n\nfn default_fatal_handler(inst: &Instance) -> ! {\n    panic!(\"> instance {:p} had fatal error: {}\", inst, inst.state);\n}\n"], "filenames": ["lucet-runtime/lucet-runtime-internals/src/instance.rs"], "buggy_code_start_loc": [17], "buggy_code_end_loc": [1059], "fixing_code_start_loc": [16], "fixing_code_end_loc": [1072], "type": "CWE-416", "message": "Lucet is a native WebAssembly compiler and runtime. There is a bug in the main branch of `lucet-runtime` affecting all versions published to crates.io that allows a use-after-free in an Instance object that could result in memory corruption, data race, or other related issues. This bug was introduced early in the development of Lucet and is present in all releases. As a result of this bug, and dependent on the memory backing for the Instance objects, it is possible to trigger a use-after-free when the Instance is dropped. Users should upgrade to the main branch of the Lucet repository. Lucet no longer provides versioned releases on crates.io. There is no way to remediate this vulnerability without upgrading.", "other": {"cve": {"id": "CVE-2021-43790", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-30T00:15:07.357", "lastModified": "2021-12-01T14:48:38.713", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Lucet is a native WebAssembly compiler and runtime. There is a bug in the main branch of `lucet-runtime` affecting all versions published to crates.io that allows a use-after-free in an Instance object that could result in memory corruption, data race, or other related issues. This bug was introduced early in the development of Lucet and is present in all releases. As a result of this bug, and dependent on the memory backing for the Instance objects, it is possible to trigger a use-after-free when the Instance is dropped. Users should upgrade to the main branch of the Lucet repository. Lucet no longer provides versioned releases on crates.io. There is no way to remediate this vulnerability without upgrading."}, {"lang": "es", "value": "Lucet es un compilador y runtime nativo de WebAssembly. Se presenta un error en la rama principal de \"lucet-runtime\" que afecta a todas las versiones publicadas en crates.io y que permite un uso de memoria previamente liberada en un objeto Instance que podr\u00eda resultar en corrupci\u00f3n de memoria, carrera de datos u otros problemas relacionados. Este bug fue introducido al principio del desarrollo de Lucet y est\u00e1 presente en todas las versiones. Como resultado de este error, y dependiendo del respaldo de memoria para los objetos de Instancia, es posible desencadenar un uso de memoria previamente liberada cuando la Instancia es abandonada. Los usuarios deben actualizar a la rama principal del repositorio de Lucet. Lucet ya no proporciona versiones en crates.io. No se presenta manera de remediar esta vulnerabilidad sin actualizar"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 6.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:lucet:*:*:*:*:*:rust:*:*", "versionEndIncluding": "0.6.1", "matchCriteriaId": "0597E90F-E72F-43AF-A751-BC5D338B6BA4"}]}]}], "references": [{"url": "https://crates.io/crates/lucet-runtime", "source": "security-advisories@github.com", "tags": ["Product", "Third Party Advisory"]}, {"url": "https://github.com/bytecodealliance/lucet/commit/7c7757c772fb709c61b1442bcc1e1fbee97bf4a8", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/bytecodealliance/lucet/security/advisories/GHSA-hf79-8hjp-rrvq", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/bytecodealliance/lucet/commit/7c7757c772fb709c61b1442bcc1e1fbee97bf4a8"}}