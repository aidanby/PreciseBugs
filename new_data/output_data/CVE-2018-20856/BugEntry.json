{"buggy_code": ["/*\n * Copyright (C) 1991, 1992 Linus Torvalds\n * Copyright (C) 1994,      Karl Keyte: Added support for disk statistics\n * Elevator latency, (C) 2000  Andrea Arcangeli <andrea@suse.de> SuSE\n * Queue request tables / lock, selectable elevator, Jens Axboe <axboe@suse.de>\n * kernel-doc documentation started by NeilBrown <neilb@cse.unsw.edu.au>\n *\t-  July2000\n * bio rewrite, highmem i/o, etc, Jens Axboe <axboe@suse.de> - may 2001\n */\n\n/*\n * This handles all read/write requests to block devices\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/backing-dev.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-mq.h>\n#include <linux/highmem.h>\n#include <linux/mm.h>\n#include <linux/kernel_stat.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/completion.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/fault-inject.h>\n#include <linux/list_sort.h>\n#include <linux/delay.h>\n#include <linux/ratelimit.h>\n#include <linux/pm_runtime.h>\n#include <linux/blk-cgroup.h>\n#include <linux/debugfs.h>\n#include <linux/bpf.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/block.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-rq-qos.h\"\n\n#ifdef CONFIG_DEBUG_FS\nstruct dentry *blk_debugfs_root;\n#endif\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_remap);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_rq_remap);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_complete);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_split);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_unplug);\n\nDEFINE_IDA(blk_queue_ida);\n\n/*\n * For the allocated request tables\n */\nstruct kmem_cache *request_cachep;\n\n/*\n * For queue allocation\n */\nstruct kmem_cache *blk_requestq_cachep;\n\n/*\n * Controlling structure to kblockd\n */\nstatic struct workqueue_struct *kblockd_workqueue;\n\n/**\n * blk_queue_flag_set - atomically set a queue flag\n * @flag: flag to be set\n * @q: request queue\n */\nvoid blk_queue_flag_set(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tqueue_flag_set(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n}\nEXPORT_SYMBOL(blk_queue_flag_set);\n\n/**\n * blk_queue_flag_clear - atomically clear a queue flag\n * @flag: flag to be cleared\n * @q: request queue\n */\nvoid blk_queue_flag_clear(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tqueue_flag_clear(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n}\nEXPORT_SYMBOL(blk_queue_flag_clear);\n\n/**\n * blk_queue_flag_test_and_set - atomically test and set a queue flag\n * @flag: flag to be set\n * @q: request queue\n *\n * Returns the previous value of @flag - 0 if the flag was not set and 1 if\n * the flag was already set.\n */\nbool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tres = queue_flag_test_and_set(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(blk_queue_flag_test_and_set);\n\n/**\n * blk_queue_flag_test_and_clear - atomically test and clear a queue flag\n * @flag: flag to be cleared\n * @q: request queue\n *\n * Returns the previous value of @flag - 0 if the flag was not set and 1 if\n * the flag was set.\n */\nbool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tres = queue_flag_test_and_clear(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(blk_queue_flag_test_and_clear);\n\nstatic void blk_clear_congested(struct request_list *rl, int sync)\n{\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tclear_wb_congested(rl->blkg->wb_congested, sync);\n#else\n\t/*\n\t * If !CGROUP_WRITEBACK, all blkg's map to bdi->wb and we shouldn't\n\t * flip its congestion state for events on other blkcgs.\n\t */\n\tif (rl == &rl->q->root_rl)\n\t\tclear_wb_congested(rl->q->backing_dev_info->wb.congested, sync);\n#endif\n}\n\nstatic void blk_set_congested(struct request_list *rl, int sync)\n{\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tset_wb_congested(rl->blkg->wb_congested, sync);\n#else\n\t/* see blk_clear_congested() */\n\tif (rl == &rl->q->root_rl)\n\t\tset_wb_congested(rl->q->backing_dev_info->wb.congested, sync);\n#endif\n}\n\nvoid blk_queue_congestion_threshold(struct request_queue *q)\n{\n\tint nr;\n\n\tnr = q->nr_requests - (q->nr_requests / 8) + 1;\n\tif (nr > q->nr_requests)\n\t\tnr = q->nr_requests;\n\tq->nr_congestion_on = nr;\n\n\tnr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;\n\tif (nr < 1)\n\t\tnr = 1;\n\tq->nr_congestion_off = nr;\n}\n\nvoid blk_rq_init(struct request_queue *q, struct request *rq)\n{\n\tmemset(rq, 0, sizeof(*rq));\n\n\tINIT_LIST_HEAD(&rq->queuelist);\n\tINIT_LIST_HEAD(&rq->timeout_list);\n\trq->cpu = -1;\n\trq->q = q;\n\trq->__sector = (sector_t) -1;\n\tINIT_HLIST_NODE(&rq->hash);\n\tRB_CLEAR_NODE(&rq->rb_node);\n\trq->tag = -1;\n\trq->internal_tag = -1;\n\trq->start_time_ns = ktime_get_ns();\n\trq->part = NULL;\n}\nEXPORT_SYMBOL(blk_rq_init);\n\nstatic const struct {\n\tint\t\terrno;\n\tconst char\t*name;\n} blk_errors[] = {\n\t[BLK_STS_OK]\t\t= { 0,\t\t\"\" },\n\t[BLK_STS_NOTSUPP]\t= { -EOPNOTSUPP, \"operation not supported\" },\n\t[BLK_STS_TIMEOUT]\t= { -ETIMEDOUT,\t\"timeout\" },\n\t[BLK_STS_NOSPC]\t\t= { -ENOSPC,\t\"critical space allocation\" },\n\t[BLK_STS_TRANSPORT]\t= { -ENOLINK,\t\"recoverable transport\" },\n\t[BLK_STS_TARGET]\t= { -EREMOTEIO,\t\"critical target\" },\n\t[BLK_STS_NEXUS]\t\t= { -EBADE,\t\"critical nexus\" },\n\t[BLK_STS_MEDIUM]\t= { -ENODATA,\t\"critical medium\" },\n\t[BLK_STS_PROTECTION]\t= { -EILSEQ,\t\"protection\" },\n\t[BLK_STS_RESOURCE]\t= { -ENOMEM,\t\"kernel resource\" },\n\t[BLK_STS_DEV_RESOURCE]\t= { -EBUSY,\t\"device resource\" },\n\t[BLK_STS_AGAIN]\t\t= { -EAGAIN,\t\"nonblocking retry\" },\n\n\t/* device mapper special case, should not leak out: */\n\t[BLK_STS_DM_REQUEUE]\t= { -EREMCHG, \"dm internal retry\" },\n\n\t/* everything else not covered above: */\n\t[BLK_STS_IOERR]\t\t= { -EIO,\t\"I/O\" },\n};\n\nblk_status_t errno_to_blk_status(int errno)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(blk_errors); i++) {\n\t\tif (blk_errors[i].errno == errno)\n\t\t\treturn (__force blk_status_t)i;\n\t}\n\n\treturn BLK_STS_IOERR;\n}\nEXPORT_SYMBOL_GPL(errno_to_blk_status);\n\nint blk_status_to_errno(blk_status_t status)\n{\n\tint idx = (__force int)status;\n\n\tif (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))\n\t\treturn -EIO;\n\treturn blk_errors[idx].errno;\n}\nEXPORT_SYMBOL_GPL(blk_status_to_errno);\n\nstatic void print_req_error(struct request *req, blk_status_t status)\n{\n\tint idx = (__force int)status;\n\n\tif (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))\n\t\treturn;\n\n\tprintk_ratelimited(KERN_ERR \"%s: %s error, dev %s, sector %llu\\n\",\n\t\t\t   __func__, blk_errors[idx].name, req->rq_disk ?\n\t\t\t   req->rq_disk->disk_name : \"?\",\n\t\t\t   (unsigned long long)blk_rq_pos(req));\n}\n\nstatic void req_bio_endio(struct request *rq, struct bio *bio,\n\t\t\t  unsigned int nbytes, blk_status_t error)\n{\n\tif (error)\n\t\tbio->bi_status = error;\n\n\tif (unlikely(rq->rq_flags & RQF_QUIET))\n\t\tbio_set_flag(bio, BIO_QUIET);\n\n\tbio_advance(bio, nbytes);\n\n\t/* don't actually finish bio if it's part of flush sequence */\n\tif (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))\n\t\tbio_endio(bio);\n}\n\nvoid blk_dump_rq_flags(struct request *rq, char *msg)\n{\n\tprintk(KERN_INFO \"%s: dev %s: flags=%llx\\n\", msg,\n\t\trq->rq_disk ? rq->rq_disk->disk_name : \"?\",\n\t\t(unsigned long long) rq->cmd_flags);\n\n\tprintk(KERN_INFO \"  sector %llu, nr/cnr %u/%u\\n\",\n\t       (unsigned long long)blk_rq_pos(rq),\n\t       blk_rq_sectors(rq), blk_rq_cur_sectors(rq));\n\tprintk(KERN_INFO \"  bio %p, biotail %p, len %u\\n\",\n\t       rq->bio, rq->biotail, blk_rq_bytes(rq));\n}\nEXPORT_SYMBOL(blk_dump_rq_flags);\n\nstatic void blk_delay_work(struct work_struct *work)\n{\n\tstruct request_queue *q;\n\n\tq = container_of(work, struct request_queue, delay_work.work);\n\tspin_lock_irq(q->queue_lock);\n\t__blk_run_queue(q);\n\tspin_unlock_irq(q->queue_lock);\n}\n\n/**\n * blk_delay_queue - restart queueing after defined interval\n * @q:\t\tThe &struct request_queue in question\n * @msecs:\tDelay in msecs\n *\n * Description:\n *   Sometimes queueing needs to be postponed for a little while, to allow\n *   resources to come back. This function will make sure that queueing is\n *   restarted around the specified time.\n */\nvoid blk_delay_queue(struct request_queue *q, unsigned long msecs)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (likely(!blk_queue_dead(q)))\n\t\tqueue_delayed_work(kblockd_workqueue, &q->delay_work,\n\t\t\t\t   msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_delay_queue);\n\n/**\n * blk_start_queue_async - asynchronously restart a previously stopped queue\n * @q:    The &struct request_queue in question\n *\n * Description:\n *   blk_start_queue_async() will clear the stop flag on the queue, and\n *   ensure that the request_fn for the queue is run from an async\n *   context.\n **/\nvoid blk_start_queue_async(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tqueue_flag_clear(QUEUE_FLAG_STOPPED, q);\n\tblk_run_queue_async(q);\n}\nEXPORT_SYMBOL(blk_start_queue_async);\n\n/**\n * blk_start_queue - restart a previously stopped queue\n * @q:    The &struct request_queue in question\n *\n * Description:\n *   blk_start_queue() will clear the stop flag on the queue, and call\n *   the request_fn for the queue if it was in a stopped state when\n *   entered. Also see blk_stop_queue().\n **/\nvoid blk_start_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tqueue_flag_clear(QUEUE_FLAG_STOPPED, q);\n\t__blk_run_queue(q);\n}\nEXPORT_SYMBOL(blk_start_queue);\n\n/**\n * blk_stop_queue - stop a queue\n * @q:    The &struct request_queue in question\n *\n * Description:\n *   The Linux block layer assumes that a block driver will consume all\n *   entries on the request queue when the request_fn strategy is called.\n *   Often this will not happen, because of hardware limitations (queue\n *   depth settings). If a device driver gets a 'queue full' response,\n *   or if it simply chooses not to queue more I/O at one point, it can\n *   call this function to prevent the request_fn from being called until\n *   the driver has signalled it's ready to go again. This happens by calling\n *   blk_start_queue() to restart queue operations.\n **/\nvoid blk_stop_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tcancel_delayed_work(&q->delay_work);\n\tqueue_flag_set(QUEUE_FLAG_STOPPED, q);\n}\nEXPORT_SYMBOL(blk_stop_queue);\n\n/**\n * blk_sync_queue - cancel any pending callbacks on a queue\n * @q: the queue\n *\n * Description:\n *     The block layer may perform asynchronous callback activity\n *     on a queue, such as calling the unplug function after a timeout.\n *     A block device may call blk_sync_queue to ensure that any\n *     such activity is cancelled, thus allowing it to release resources\n *     that the callbacks might use. The caller must already have made sure\n *     that its ->make_request_fn will not re-add plugging prior to calling\n *     this function.\n *\n *     This function does not cancel any asynchronous activity arising\n *     out of elevator or throttling code. That would require elevator_exit()\n *     and blkcg_exit_queue() to be called with queue lock initialized.\n *\n */\nvoid blk_sync_queue(struct request_queue *q)\n{\n\tdel_timer_sync(&q->timeout);\n\tcancel_work_sync(&q->timeout_work);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\t\tint i;\n\n\t\tcancel_delayed_work_sync(&q->requeue_work);\n\t\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\t\tcancel_delayed_work_sync(&hctx->run_work);\n\t} else {\n\t\tcancel_delayed_work_sync(&q->delay_work);\n\t}\n}\nEXPORT_SYMBOL(blk_sync_queue);\n\n/**\n * blk_set_preempt_only - set QUEUE_FLAG_PREEMPT_ONLY\n * @q: request queue pointer\n *\n * Returns the previous value of the PREEMPT_ONLY flag - 0 if the flag was not\n * set and 1 if the flag was already set.\n */\nint blk_set_preempt_only(struct request_queue *q)\n{\n\treturn blk_queue_flag_test_and_set(QUEUE_FLAG_PREEMPT_ONLY, q);\n}\nEXPORT_SYMBOL_GPL(blk_set_preempt_only);\n\nvoid blk_clear_preempt_only(struct request_queue *q)\n{\n\tblk_queue_flag_clear(QUEUE_FLAG_PREEMPT_ONLY, q);\n\twake_up_all(&q->mq_freeze_wq);\n}\nEXPORT_SYMBOL_GPL(blk_clear_preempt_only);\n\n/**\n * __blk_run_queue_uncond - run a queue whether or not it has been stopped\n * @q:\tThe queue to run\n *\n * Description:\n *    Invoke request handling on a queue if there are any pending requests.\n *    May be used to restart request handling after a request has completed.\n *    This variant runs the queue whether or not the queue has been\n *    stopped. Must be called with the queue lock held and interrupts\n *    disabled. See also @blk_run_queue.\n */\ninline void __blk_run_queue_uncond(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (unlikely(blk_queue_dead(q)))\n\t\treturn;\n\n\t/*\n\t * Some request_fn implementations, e.g. scsi_request_fn(), unlock\n\t * the queue lock internally. As a result multiple threads may be\n\t * running such a request function concurrently. Keep track of the\n\t * number of active request_fn invocations such that blk_drain_queue()\n\t * can wait until all these request_fn calls have finished.\n\t */\n\tq->request_fn_active++;\n\tq->request_fn(q);\n\tq->request_fn_active--;\n}\nEXPORT_SYMBOL_GPL(__blk_run_queue_uncond);\n\n/**\n * __blk_run_queue - run a single device queue\n * @q:\tThe queue to run\n *\n * Description:\n *    See @blk_run_queue.\n */\nvoid __blk_run_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (unlikely(blk_queue_stopped(q)))\n\t\treturn;\n\n\t__blk_run_queue_uncond(q);\n}\nEXPORT_SYMBOL(__blk_run_queue);\n\n/**\n * blk_run_queue_async - run a single device queue in workqueue context\n * @q:\tThe queue to run\n *\n * Description:\n *    Tells kblockd to perform the equivalent of @blk_run_queue on behalf\n *    of us.\n *\n * Note:\n *    Since it is not allowed to run q->delay_work after blk_cleanup_queue()\n *    has canceled q->delay_work, callers must hold the queue lock to avoid\n *    race conditions between blk_cleanup_queue() and blk_run_queue_async().\n */\nvoid blk_run_queue_async(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (likely(!blk_queue_stopped(q) && !blk_queue_dead(q)))\n\t\tmod_delayed_work(kblockd_workqueue, &q->delay_work, 0);\n}\nEXPORT_SYMBOL(blk_run_queue_async);\n\n/**\n * blk_run_queue - run a single device queue\n * @q: The queue to run\n *\n * Description:\n *    Invoke request handling on this queue, if it has pending work to do.\n *    May be used to restart queueing when a request has completed.\n */\nvoid blk_run_queue(struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\t__blk_run_queue(q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n}\nEXPORT_SYMBOL(blk_run_queue);\n\nvoid blk_put_queue(struct request_queue *q)\n{\n\tkobject_put(&q->kobj);\n}\nEXPORT_SYMBOL(blk_put_queue);\n\n/**\n * __blk_drain_queue - drain requests from request_queue\n * @q: queue to drain\n * @drain_all: whether to drain all requests or only the ones w/ ELVPRIV\n *\n * Drain requests from @q.  If @drain_all is set, all requests are drained.\n * If not, only ELVPRIV requests are drained.  The caller is responsible\n * for ensuring that no new requests which need to be drained are queued.\n */\nstatic void __blk_drain_queue(struct request_queue *q, bool drain_all)\n\t__releases(q->queue_lock)\n\t__acquires(q->queue_lock)\n{\n\tint i;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\twhile (true) {\n\t\tbool drain = false;\n\n\t\t/*\n\t\t * The caller might be trying to drain @q before its\n\t\t * elevator is initialized.\n\t\t */\n\t\tif (q->elevator)\n\t\t\telv_drain_elevator(q);\n\n\t\tblkcg_drain_queue(q);\n\n\t\t/*\n\t\t * This function might be called on a queue which failed\n\t\t * driver init after queue creation or is not yet fully\n\t\t * active yet.  Some drivers (e.g. fd and loop) get unhappy\n\t\t * in such cases.  Kick queue iff dispatch queue has\n\t\t * something on it and @q has request_fn set.\n\t\t */\n\t\tif (!list_empty(&q->queue_head) && q->request_fn)\n\t\t\t__blk_run_queue(q);\n\n\t\tdrain |= q->nr_rqs_elvpriv;\n\t\tdrain |= q->request_fn_active;\n\n\t\t/*\n\t\t * Unfortunately, requests are queued at and tracked from\n\t\t * multiple places and there's no single counter which can\n\t\t * be drained.  Check all the queues and counters.\n\t\t */\n\t\tif (drain_all) {\n\t\t\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\t\t\tdrain |= !list_empty(&q->queue_head);\n\t\t\tfor (i = 0; i < 2; i++) {\n\t\t\t\tdrain |= q->nr_rqs[i];\n\t\t\t\tdrain |= q->in_flight[i];\n\t\t\t\tif (fq)\n\t\t\t\t    drain |= !list_empty(&fq->flush_queue[i]);\n\t\t\t}\n\t\t}\n\n\t\tif (!drain)\n\t\t\tbreak;\n\n\t\tspin_unlock_irq(q->queue_lock);\n\n\t\tmsleep(10);\n\n\t\tspin_lock_irq(q->queue_lock);\n\t}\n\n\t/*\n\t * With queue marked dead, any woken up waiter will fail the\n\t * allocation path, so the wakeup chaining is lost and we're\n\t * left with hung waiters. We need to wake up those waiters.\n\t */\n\tif (q->request_fn) {\n\t\tstruct request_list *rl;\n\n\t\tblk_queue_for_each_rl(rl, q)\n\t\t\tfor (i = 0; i < ARRAY_SIZE(rl->wait); i++)\n\t\t\t\twake_up_all(&rl->wait[i]);\n\t}\n}\n\nvoid blk_drain_queue(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\t__blk_drain_queue(q, true);\n\tspin_unlock_irq(q->queue_lock);\n}\n\n/**\n * blk_queue_bypass_start - enter queue bypass mode\n * @q: queue of interest\n *\n * In bypass mode, only the dispatch FIFO queue of @q is used.  This\n * function makes @q enter bypass mode and drains all requests which were\n * throttled or issued before.  On return, it's guaranteed that no request\n * is being throttled or has ELVPRIV set and blk_queue_bypass() %true\n * inside queue or RCU read lock.\n */\nvoid blk_queue_bypass_start(struct request_queue *q)\n{\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tspin_lock_irq(q->queue_lock);\n\tq->bypass_depth++;\n\tqueue_flag_set(QUEUE_FLAG_BYPASS, q);\n\tspin_unlock_irq(q->queue_lock);\n\n\t/*\n\t * Queues start drained.  Skip actual draining till init is\n\t * complete.  This avoids lenghty delays during queue init which\n\t * can happen many times during boot.\n\t */\n\tif (blk_queue_init_done(q)) {\n\t\tspin_lock_irq(q->queue_lock);\n\t\t__blk_drain_queue(q, false);\n\t\tspin_unlock_irq(q->queue_lock);\n\n\t\t/* ensure blk_queue_bypass() is %true inside RCU read lock */\n\t\tsynchronize_rcu();\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_queue_bypass_start);\n\n/**\n * blk_queue_bypass_end - leave queue bypass mode\n * @q: queue of interest\n *\n * Leave bypass mode and restore the normal queueing behavior.\n *\n * Note: although blk_queue_bypass_start() is only called for blk-sq queues,\n * this function is called for both blk-sq and blk-mq queues.\n */\nvoid blk_queue_bypass_end(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\tif (!--q->bypass_depth)\n\t\tqueue_flag_clear(QUEUE_FLAG_BYPASS, q);\n\tWARN_ON_ONCE(q->bypass_depth < 0);\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL_GPL(blk_queue_bypass_end);\n\nvoid blk_set_queue_dying(struct request_queue *q)\n{\n\tblk_queue_flag_set(QUEUE_FLAG_DYING, q);\n\n\t/*\n\t * When queue DYING flag is set, we need to block new req\n\t * entering queue, so we call blk_freeze_queue_start() to\n\t * prevent I/O from crossing blk_queue_enter().\n\t */\n\tblk_freeze_queue_start(q);\n\n\tif (q->mq_ops)\n\t\tblk_mq_wake_waiters(q);\n\telse {\n\t\tstruct request_list *rl;\n\n\t\tspin_lock_irq(q->queue_lock);\n\t\tblk_queue_for_each_rl(rl, q) {\n\t\t\tif (rl->rq_pool) {\n\t\t\t\twake_up_all(&rl->wait[BLK_RW_SYNC]);\n\t\t\t\twake_up_all(&rl->wait[BLK_RW_ASYNC]);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(q->queue_lock);\n\t}\n\n\t/* Make blk_queue_enter() reexamine the DYING flag. */\n\twake_up_all(&q->mq_freeze_wq);\n}\nEXPORT_SYMBOL_GPL(blk_set_queue_dying);\n\n/**\n * blk_cleanup_queue - shutdown a request queue\n * @q: request queue to shutdown\n *\n * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and\n * put it.  All future requests will be failed immediately with -ENODEV.\n */\nvoid blk_cleanup_queue(struct request_queue *q)\n{\n\tspinlock_t *lock = q->queue_lock;\n\n\t/* mark @q DYING, no new request or merges will be allowed afterwards */\n\tmutex_lock(&q->sysfs_lock);\n\tblk_set_queue_dying(q);\n\tspin_lock_irq(lock);\n\n\t/*\n\t * A dying queue is permanently in bypass mode till released.  Note\n\t * that, unlike blk_queue_bypass_start(), we aren't performing\n\t * synchronize_rcu() after entering bypass mode to avoid the delay\n\t * as some drivers create and destroy a lot of queues while\n\t * probing.  This is still safe because blk_release_queue() will be\n\t * called only after the queue refcnt drops to zero and nothing,\n\t * RCU or not, would be traversing the queue by then.\n\t */\n\tq->bypass_depth++;\n\tqueue_flag_set(QUEUE_FLAG_BYPASS, q);\n\n\tqueue_flag_set(QUEUE_FLAG_NOMERGES, q);\n\tqueue_flag_set(QUEUE_FLAG_NOXMERGES, q);\n\tqueue_flag_set(QUEUE_FLAG_DYING, q);\n\tspin_unlock_irq(lock);\n\tmutex_unlock(&q->sysfs_lock);\n\n\t/*\n\t * Drain all requests queued before DYING marking. Set DEAD flag to\n\t * prevent that q->request_fn() gets invoked after draining finished.\n\t */\n\tblk_freeze_queue(q);\n\tspin_lock_irq(lock);\n\tqueue_flag_set(QUEUE_FLAG_DEAD, q);\n\tspin_unlock_irq(lock);\n\n\t/*\n\t * make sure all in-progress dispatch are completed because\n\t * blk_freeze_queue() can only complete all requests, and\n\t * dispatch may still be in-progress since we dispatch requests\n\t * from more than one contexts.\n\t *\n\t * No need to quiesce queue if it isn't initialized yet since\n\t * blk_freeze_queue() should be enough for cases of passthrough\n\t * request.\n\t */\n\tif (q->mq_ops && blk_queue_init_done(q))\n\t\tblk_mq_quiesce_queue(q);\n\n\t/* for synchronous bio-based driver finish in-flight integrity i/o */\n\tblk_flush_integrity();\n\n\t/* @q won't process any more request, flush async actions */\n\tdel_timer_sync(&q->backing_dev_info->laptop_mode_wb_timer);\n\tblk_sync_queue(q);\n\n\t/*\n\t * I/O scheduler exit is only safe after the sysfs scheduler attribute\n\t * has been removed.\n\t */\n\tWARN_ON_ONCE(q->kobj.state_in_sysfs);\n\n\t/*\n\t * Since the I/O scheduler exit code may access cgroup information,\n\t * perform I/O scheduler exit before disassociating from the block\n\t * cgroup controller.\n\t */\n\tif (q->elevator) {\n\t\tioc_clear_queue(q);\n\t\televator_exit(q, q->elevator);\n\t\tq->elevator = NULL;\n\t}\n\n\t/*\n\t * Remove all references to @q from the block cgroup controller before\n\t * restoring @q->queue_lock to avoid that restoring this pointer causes\n\t * e.g. blkcg_print_blkgs() to crash.\n\t */\n\tblkcg_exit_queue(q);\n\n\t/*\n\t * Since the cgroup code may dereference the @q->backing_dev_info\n\t * pointer, only decrease its reference count after having removed the\n\t * association with the block cgroup controller.\n\t */\n\tbdi_put(q->backing_dev_info);\n\n\tif (q->mq_ops)\n\t\tblk_mq_free_queue(q);\n\tpercpu_ref_exit(&q->q_usage_counter);\n\n\tspin_lock_irq(lock);\n\tif (q->queue_lock != &q->__queue_lock)\n\t\tq->queue_lock = &q->__queue_lock;\n\tspin_unlock_irq(lock);\n\n\t/* @q is and will stay empty, shutdown and put */\n\tblk_put_queue(q);\n}\nEXPORT_SYMBOL(blk_cleanup_queue);\n\n/* Allocate memory local to the request queue */\nstatic void *alloc_request_simple(gfp_t gfp_mask, void *data)\n{\n\tstruct request_queue *q = data;\n\n\treturn kmem_cache_alloc_node(request_cachep, gfp_mask, q->node);\n}\n\nstatic void free_request_simple(void *element, void *data)\n{\n\tkmem_cache_free(request_cachep, element);\n}\n\nstatic void *alloc_request_size(gfp_t gfp_mask, void *data)\n{\n\tstruct request_queue *q = data;\n\tstruct request *rq;\n\n\trq = kmalloc_node(sizeof(struct request) + q->cmd_size, gfp_mask,\n\t\t\tq->node);\n\tif (rq && q->init_rq_fn && q->init_rq_fn(q, rq, gfp_mask) < 0) {\n\t\tkfree(rq);\n\t\trq = NULL;\n\t}\n\treturn rq;\n}\n\nstatic void free_request_size(void *element, void *data)\n{\n\tstruct request_queue *q = data;\n\n\tif (q->exit_rq_fn)\n\t\tq->exit_rq_fn(q, element);\n\tkfree(element);\n}\n\nint blk_init_rl(struct request_list *rl, struct request_queue *q,\n\t\tgfp_t gfp_mask)\n{\n\tif (unlikely(rl->rq_pool) || q->mq_ops)\n\t\treturn 0;\n\n\trl->q = q;\n\trl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;\n\trl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;\n\tinit_waitqueue_head(&rl->wait[BLK_RW_SYNC]);\n\tinit_waitqueue_head(&rl->wait[BLK_RW_ASYNC]);\n\n\tif (q->cmd_size) {\n\t\trl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ,\n\t\t\t\talloc_request_size, free_request_size,\n\t\t\t\tq, gfp_mask, q->node);\n\t} else {\n\t\trl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ,\n\t\t\t\talloc_request_simple, free_request_simple,\n\t\t\t\tq, gfp_mask, q->node);\n\t}\n\tif (!rl->rq_pool)\n\t\treturn -ENOMEM;\n\n\tif (rl != &q->root_rl)\n\t\tWARN_ON_ONCE(!blk_get_queue(q));\n\n\treturn 0;\n}\n\nvoid blk_exit_rl(struct request_queue *q, struct request_list *rl)\n{\n\tif (rl->rq_pool) {\n\t\tmempool_destroy(rl->rq_pool);\n\t\tif (rl != &q->root_rl)\n\t\t\tblk_put_queue(q);\n\t}\n}\n\nstruct request_queue *blk_alloc_queue(gfp_t gfp_mask)\n{\n\treturn blk_alloc_queue_node(gfp_mask, NUMA_NO_NODE, NULL);\n}\nEXPORT_SYMBOL(blk_alloc_queue);\n\n/**\n * blk_queue_enter() - try to increase q->q_usage_counter\n * @q: request queue pointer\n * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT\n */\nint blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)\n{\n\tconst bool preempt = flags & BLK_MQ_REQ_PREEMPT;\n\n\twhile (true) {\n\t\tbool success = false;\n\n\t\trcu_read_lock();\n\t\tif (percpu_ref_tryget_live(&q->q_usage_counter)) {\n\t\t\t/*\n\t\t\t * The code that sets the PREEMPT_ONLY flag is\n\t\t\t * responsible for ensuring that that flag is globally\n\t\t\t * visible before the queue is unfrozen.\n\t\t\t */\n\t\t\tif (preempt || !blk_queue_preempt_only(q)) {\n\t\t\t\tsuccess = true;\n\t\t\t} else {\n\t\t\t\tpercpu_ref_put(&q->q_usage_counter);\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (success)\n\t\t\treturn 0;\n\n\t\tif (flags & BLK_MQ_REQ_NOWAIT)\n\t\t\treturn -EBUSY;\n\n\t\t/*\n\t\t * read pair of barrier in blk_freeze_queue_start(),\n\t\t * we need to order reading __PERCPU_REF_DEAD flag of\n\t\t * .q_usage_counter and reading .mq_freeze_depth or\n\t\t * queue dying flag, otherwise the following wait may\n\t\t * never return if the two reads are reordered.\n\t\t */\n\t\tsmp_rmb();\n\n\t\twait_event(q->mq_freeze_wq,\n\t\t\t   (atomic_read(&q->mq_freeze_depth) == 0 &&\n\t\t\t    (preempt || !blk_queue_preempt_only(q))) ||\n\t\t\t   blk_queue_dying(q));\n\t\tif (blk_queue_dying(q))\n\t\t\treturn -ENODEV;\n\t}\n}\n\nvoid blk_queue_exit(struct request_queue *q)\n{\n\tpercpu_ref_put(&q->q_usage_counter);\n}\n\nstatic void blk_queue_usage_counter_release(struct percpu_ref *ref)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(ref, struct request_queue, q_usage_counter);\n\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nstatic void blk_rq_timed_out_timer(struct timer_list *t)\n{\n\tstruct request_queue *q = from_timer(q, t, timeout);\n\n\tkblockd_schedule_work(&q->timeout_work);\n}\n\n/**\n * blk_alloc_queue_node - allocate a request queue\n * @gfp_mask: memory allocation flags\n * @node_id: NUMA node to allocate memory from\n * @lock: For legacy queues, pointer to a spinlock that will be used to e.g.\n *        serialize calls to the legacy .request_fn() callback. Ignored for\n *\t  blk-mq request queues.\n *\n * Note: pass the queue lock as the third argument to this function instead of\n * setting the queue lock pointer explicitly to avoid triggering a sporadic\n * crash in the blkcg code. This function namely calls blkcg_init_queue() and\n * the queue lock pointer must be set before blkcg_init_queue() is called.\n */\nstruct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,\n\t\t\t\t\t   spinlock_t *lock)\n{\n\tstruct request_queue *q;\n\tint ret;\n\n\tq = kmem_cache_alloc_node(blk_requestq_cachep,\n\t\t\t\tgfp_mask | __GFP_ZERO, node_id);\n\tif (!q)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&q->queue_head);\n\tq->last_merge = NULL;\n\tq->end_sector = 0;\n\tq->boundary_rq = NULL;\n\n\tq->id = ida_simple_get(&blk_queue_ida, 0, 0, gfp_mask);\n\tif (q->id < 0)\n\t\tgoto fail_q;\n\n\tret = bioset_init(&q->bio_split, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\tif (ret)\n\t\tgoto fail_id;\n\n\tq->backing_dev_info = bdi_alloc_node(gfp_mask, node_id);\n\tif (!q->backing_dev_info)\n\t\tgoto fail_split;\n\n\tq->stats = blk_alloc_queue_stats();\n\tif (!q->stats)\n\t\tgoto fail_stats;\n\n\tq->backing_dev_info->ra_pages =\n\t\t\t(VM_MAX_READAHEAD * 1024) / PAGE_SIZE;\n\tq->backing_dev_info->capabilities = BDI_CAP_CGROUP_WRITEBACK;\n\tq->backing_dev_info->name = \"block\";\n\tq->node = node_id;\n\n\ttimer_setup(&q->backing_dev_info->laptop_mode_wb_timer,\n\t\t    laptop_mode_timer_fn, 0);\n\ttimer_setup(&q->timeout, blk_rq_timed_out_timer, 0);\n\tINIT_WORK(&q->timeout_work, NULL);\n\tINIT_LIST_HEAD(&q->queue_head);\n\tINIT_LIST_HEAD(&q->timeout_list);\n\tINIT_LIST_HEAD(&q->icq_list);\n#ifdef CONFIG_BLK_CGROUP\n\tINIT_LIST_HEAD(&q->blkg_list);\n#endif\n\tINIT_DELAYED_WORK(&q->delay_work, blk_delay_work);\n\n\tkobject_init(&q->kobj, &blk_queue_ktype);\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tmutex_init(&q->blk_trace_mutex);\n#endif\n\tmutex_init(&q->sysfs_lock);\n\tspin_lock_init(&q->__queue_lock);\n\n\tif (!q->mq_ops)\n\t\tq->queue_lock = lock ? : &q->__queue_lock;\n\n\t/*\n\t * A queue starts its life with bypass turned on to avoid\n\t * unnecessary bypass on/off overhead and nasty surprises during\n\t * init.  The initial bypass will be finished when the queue is\n\t * registered by blk_register_queue().\n\t */\n\tq->bypass_depth = 1;\n\tqueue_flag_set_unlocked(QUEUE_FLAG_BYPASS, q);\n\n\tinit_waitqueue_head(&q->mq_freeze_wq);\n\n\t/*\n\t * Init percpu_ref in atomic mode so that it's faster to shutdown.\n\t * See blk_register_queue() for details.\n\t */\n\tif (percpu_ref_init(&q->q_usage_counter,\n\t\t\t\tblk_queue_usage_counter_release,\n\t\t\t\tPERCPU_REF_INIT_ATOMIC, GFP_KERNEL))\n\t\tgoto fail_bdi;\n\n\tif (blkcg_init_queue(q))\n\t\tgoto fail_ref;\n\n\treturn q;\n\nfail_ref:\n\tpercpu_ref_exit(&q->q_usage_counter);\nfail_bdi:\n\tblk_free_queue_stats(q->stats);\nfail_stats:\n\tbdi_put(q->backing_dev_info);\nfail_split:\n\tbioset_exit(&q->bio_split);\nfail_id:\n\tida_simple_remove(&blk_queue_ida, q->id);\nfail_q:\n\tkmem_cache_free(blk_requestq_cachep, q);\n\treturn NULL;\n}\nEXPORT_SYMBOL(blk_alloc_queue_node);\n\n/**\n * blk_init_queue  - prepare a request queue for use with a block device\n * @rfn:  The function to be called to process requests that have been\n *        placed on the queue.\n * @lock: Request queue spin lock\n *\n * Description:\n *    If a block device wishes to use the standard request handling procedures,\n *    which sorts requests and coalesces adjacent requests, then it must\n *    call blk_init_queue().  The function @rfn will be called when there\n *    are requests on the queue that need to be processed.  If the device\n *    supports plugging, then @rfn may not be called immediately when requests\n *    are available on the queue, but may be called at some time later instead.\n *    Plugged queues are generally unplugged when a buffer belonging to one\n *    of the requests on the queue is needed, or due to memory pressure.\n *\n *    @rfn is not required, or even expected, to remove all requests off the\n *    queue, but only as many as it can handle at a time.  If it does leave\n *    requests on the queue, it is responsible for arranging that the requests\n *    get dealt with eventually.\n *\n *    The queue spin lock must be held while manipulating the requests on the\n *    request queue; this lock will be taken also from interrupt context, so irq\n *    disabling is needed for it.\n *\n *    Function returns a pointer to the initialized request queue, or %NULL if\n *    it didn't succeed.\n *\n * Note:\n *    blk_init_queue() must be paired with a blk_cleanup_queue() call\n *    when the block device is deactivated (such as at module unload).\n **/\n\nstruct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)\n{\n\treturn blk_init_queue_node(rfn, lock, NUMA_NO_NODE);\n}\nEXPORT_SYMBOL(blk_init_queue);\n\nstruct request_queue *\nblk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)\n{\n\tstruct request_queue *q;\n\n\tq = blk_alloc_queue_node(GFP_KERNEL, node_id, lock);\n\tif (!q)\n\t\treturn NULL;\n\n\tq->request_fn = rfn;\n\tif (blk_init_allocated_queue(q) < 0) {\n\t\tblk_cleanup_queue(q);\n\t\treturn NULL;\n\t}\n\n\treturn q;\n}\nEXPORT_SYMBOL(blk_init_queue_node);\n\nstatic blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio);\n\n\nint blk_init_allocated_queue(struct request_queue *q)\n{\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tq->fq = blk_alloc_flush_queue(q, NUMA_NO_NODE, q->cmd_size);\n\tif (!q->fq)\n\t\treturn -ENOMEM;\n\n\tif (q->init_rq_fn && q->init_rq_fn(q, q->fq->flush_rq, GFP_KERNEL))\n\t\tgoto out_free_flush_queue;\n\n\tif (blk_init_rl(&q->root_rl, q, GFP_KERNEL))\n\t\tgoto out_exit_flush_rq;\n\n\tINIT_WORK(&q->timeout_work, blk_timeout_work);\n\tq->queue_flags\t\t|= QUEUE_FLAG_DEFAULT;\n\n\t/*\n\t * This also sets hw/phys segments, boundary and size\n\t */\n\tblk_queue_make_request(q, blk_queue_bio);\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tif (elevator_init(q))\n\t\tgoto out_exit_flush_rq;\n\treturn 0;\n\nout_exit_flush_rq:\n\tif (q->exit_rq_fn)\n\t\tq->exit_rq_fn(q, q->fq->flush_rq);\nout_free_flush_queue:\n\tblk_free_flush_queue(q->fq);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(blk_init_allocated_queue);\n\nbool blk_get_queue(struct request_queue *q)\n{\n\tif (likely(!blk_queue_dying(q))) {\n\t\t__blk_get_queue(q);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL(blk_get_queue);\n\nstatic inline void blk_free_request(struct request_list *rl, struct request *rq)\n{\n\tif (rq->rq_flags & RQF_ELVPRIV) {\n\t\telv_put_request(rl->q, rq);\n\t\tif (rq->elv.icq)\n\t\t\tput_io_context(rq->elv.icq->ioc);\n\t}\n\n\tmempool_free(rq, rl->rq_pool);\n}\n\n/*\n * ioc_batching returns true if the ioc is a valid batching request and\n * should be given priority access to a request.\n */\nstatic inline int ioc_batching(struct request_queue *q, struct io_context *ioc)\n{\n\tif (!ioc)\n\t\treturn 0;\n\n\t/*\n\t * Make sure the process is able to allocate at least 1 request\n\t * even if the batch times out, otherwise we could theoretically\n\t * lose wakeups.\n\t */\n\treturn ioc->nr_batch_requests == q->nr_batching ||\n\t\t(ioc->nr_batch_requests > 0\n\t\t&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));\n}\n\n/*\n * ioc_set_batching sets ioc to be a new \"batcher\" if it is not one. This\n * will cause the process to be a \"batcher\" on all queues in the system. This\n * is the behaviour we want though - once it gets a wakeup it should be given\n * a nice run.\n */\nstatic void ioc_set_batching(struct request_queue *q, struct io_context *ioc)\n{\n\tif (!ioc || ioc_batching(q, ioc))\n\t\treturn;\n\n\tioc->nr_batch_requests = q->nr_batching;\n\tioc->last_waited = jiffies;\n}\n\nstatic void __freed_request(struct request_list *rl, int sync)\n{\n\tstruct request_queue *q = rl->q;\n\n\tif (rl->count[sync] < queue_congestion_off_threshold(q))\n\t\tblk_clear_congested(rl, sync);\n\n\tif (rl->count[sync] + 1 <= q->nr_requests) {\n\t\tif (waitqueue_active(&rl->wait[sync]))\n\t\t\twake_up(&rl->wait[sync]);\n\n\t\tblk_clear_rl_full(rl, sync);\n\t}\n}\n\n/*\n * A request has just been released.  Account for it, update the full and\n * congestion status, wake up any waiters.   Called under q->queue_lock.\n */\nstatic void freed_request(struct request_list *rl, bool sync,\n\t\treq_flags_t rq_flags)\n{\n\tstruct request_queue *q = rl->q;\n\n\tq->nr_rqs[sync]--;\n\trl->count[sync]--;\n\tif (rq_flags & RQF_ELVPRIV)\n\t\tq->nr_rqs_elvpriv--;\n\n\t__freed_request(rl, sync);\n\n\tif (unlikely(rl->starved[sync ^ 1]))\n\t\t__freed_request(rl, sync ^ 1);\n}\n\nint blk_update_nr_requests(struct request_queue *q, unsigned int nr)\n{\n\tstruct request_list *rl;\n\tint on_thresh, off_thresh;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tspin_lock_irq(q->queue_lock);\n\tq->nr_requests = nr;\n\tblk_queue_congestion_threshold(q);\n\ton_thresh = queue_congestion_on_threshold(q);\n\toff_thresh = queue_congestion_off_threshold(q);\n\n\tblk_queue_for_each_rl(rl, q) {\n\t\tif (rl->count[BLK_RW_SYNC] >= on_thresh)\n\t\t\tblk_set_congested(rl, BLK_RW_SYNC);\n\t\telse if (rl->count[BLK_RW_SYNC] < off_thresh)\n\t\t\tblk_clear_congested(rl, BLK_RW_SYNC);\n\n\t\tif (rl->count[BLK_RW_ASYNC] >= on_thresh)\n\t\t\tblk_set_congested(rl, BLK_RW_ASYNC);\n\t\telse if (rl->count[BLK_RW_ASYNC] < off_thresh)\n\t\t\tblk_clear_congested(rl, BLK_RW_ASYNC);\n\n\t\tif (rl->count[BLK_RW_SYNC] >= q->nr_requests) {\n\t\t\tblk_set_rl_full(rl, BLK_RW_SYNC);\n\t\t} else {\n\t\t\tblk_clear_rl_full(rl, BLK_RW_SYNC);\n\t\t\twake_up(&rl->wait[BLK_RW_SYNC]);\n\t\t}\n\n\t\tif (rl->count[BLK_RW_ASYNC] >= q->nr_requests) {\n\t\t\tblk_set_rl_full(rl, BLK_RW_ASYNC);\n\t\t} else {\n\t\t\tblk_clear_rl_full(rl, BLK_RW_ASYNC);\n\t\t\twake_up(&rl->wait[BLK_RW_ASYNC]);\n\t\t}\n\t}\n\n\tspin_unlock_irq(q->queue_lock);\n\treturn 0;\n}\n\n/**\n * __get_request - get a free request\n * @rl: request list to allocate from\n * @op: operation and flags\n * @bio: bio to allocate request for (can be %NULL)\n * @flags: BLQ_MQ_REQ_* flags\n * @gfp_mask: allocator flags\n *\n * Get a free request from @q.  This function may fail under memory\n * pressure or if @q is dead.\n *\n * Must be called with @q->queue_lock held and,\n * Returns ERR_PTR on failure, with @q->queue_lock held.\n * Returns request pointer on success, with @q->queue_lock *not held*.\n */\nstatic struct request *__get_request(struct request_list *rl, unsigned int op,\n\t\tstruct bio *bio, blk_mq_req_flags_t flags, gfp_t gfp_mask)\n{\n\tstruct request_queue *q = rl->q;\n\tstruct request *rq;\n\tstruct elevator_type *et = q->elevator->type;\n\tstruct io_context *ioc = rq_ioc(bio);\n\tstruct io_cq *icq = NULL;\n\tconst bool is_sync = op_is_sync(op);\n\tint may_queue;\n\treq_flags_t rq_flags = RQF_ALLOCED;\n\n\tlockdep_assert_held(q->queue_lock);\n\n\tif (unlikely(blk_queue_dying(q)))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmay_queue = elv_may_queue(q, op);\n\tif (may_queue == ELV_MQUEUE_NO)\n\t\tgoto rq_starved;\n\n\tif (rl->count[is_sync]+1 >= queue_congestion_on_threshold(q)) {\n\t\tif (rl->count[is_sync]+1 >= q->nr_requests) {\n\t\t\t/*\n\t\t\t * The queue will fill after this allocation, so set\n\t\t\t * it as full, and mark this process as \"batching\".\n\t\t\t * This process will be allowed to complete a batch of\n\t\t\t * requests, others will be blocked.\n\t\t\t */\n\t\t\tif (!blk_rl_full(rl, is_sync)) {\n\t\t\t\tioc_set_batching(q, ioc);\n\t\t\t\tblk_set_rl_full(rl, is_sync);\n\t\t\t} else {\n\t\t\t\tif (may_queue != ELV_MQUEUE_MUST\n\t\t\t\t\t\t&& !ioc_batching(q, ioc)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * The queue is full and the allocating\n\t\t\t\t\t * process is not a \"batcher\", and not\n\t\t\t\t\t * exempted by the IO scheduler\n\t\t\t\t\t */\n\t\t\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tblk_set_congested(rl, is_sync);\n\t}\n\n\t/*\n\t * Only allow batching queuers to allocate up to 50% over the defined\n\t * limit of requests, otherwise we could have thousands of requests\n\t * allocated with any setting of ->nr_requests\n\t */\n\tif (rl->count[is_sync] >= (3 * q->nr_requests / 2))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tq->nr_rqs[is_sync]++;\n\trl->count[is_sync]++;\n\trl->starved[is_sync] = 0;\n\n\t/*\n\t * Decide whether the new request will be managed by elevator.  If\n\t * so, mark @rq_flags and increment elvpriv.  Non-zero elvpriv will\n\t * prevent the current elevator from being destroyed until the new\n\t * request is freed.  This guarantees icq's won't be destroyed and\n\t * makes creating new ones safe.\n\t *\n\t * Flush requests do not use the elevator so skip initialization.\n\t * This allows a request to share the flush and elevator data.\n\t *\n\t * Also, lookup icq while holding queue_lock.  If it doesn't exist,\n\t * it will be created after releasing queue_lock.\n\t */\n\tif (!op_is_flush(op) && !blk_queue_bypass(q)) {\n\t\trq_flags |= RQF_ELVPRIV;\n\t\tq->nr_rqs_elvpriv++;\n\t\tif (et->icq_cache && ioc)\n\t\t\ticq = ioc_lookup_icq(ioc, q);\n\t}\n\n\tif (blk_queue_io_stat(q))\n\t\trq_flags |= RQF_IO_STAT;\n\tspin_unlock_irq(q->queue_lock);\n\n\t/* allocate and init request */\n\trq = mempool_alloc(rl->rq_pool, gfp_mask);\n\tif (!rq)\n\t\tgoto fail_alloc;\n\n\tblk_rq_init(q, rq);\n\tblk_rq_set_rl(rq, rl);\n\trq->cmd_flags = op;\n\trq->rq_flags = rq_flags;\n\tif (flags & BLK_MQ_REQ_PREEMPT)\n\t\trq->rq_flags |= RQF_PREEMPT;\n\n\t/* init elvpriv */\n\tif (rq_flags & RQF_ELVPRIV) {\n\t\tif (unlikely(et->icq_cache && !icq)) {\n\t\t\tif (ioc)\n\t\t\t\ticq = ioc_create_icq(ioc, q, gfp_mask);\n\t\t\tif (!icq)\n\t\t\t\tgoto fail_elvpriv;\n\t\t}\n\n\t\trq->elv.icq = icq;\n\t\tif (unlikely(elv_set_request(q, rq, bio, gfp_mask)))\n\t\t\tgoto fail_elvpriv;\n\n\t\t/* @rq->elv.icq holds io_context until @rq is freed */\n\t\tif (icq)\n\t\t\tget_io_context(icq->ioc);\n\t}\nout:\n\t/*\n\t * ioc may be NULL here, and ioc_batching will be false. That's\n\t * OK, if the queue is under the request limit then requests need\n\t * not count toward the nr_batch_requests limit. There will always\n\t * be some limit enforced by BLK_BATCH_TIME.\n\t */\n\tif (ioc_batching(q, ioc))\n\t\tioc->nr_batch_requests--;\n\n\ttrace_block_getrq(q, bio, op);\n\treturn rq;\n\nfail_elvpriv:\n\t/*\n\t * elvpriv init failed.  ioc, icq and elvpriv aren't mempool backed\n\t * and may fail indefinitely under memory pressure and thus\n\t * shouldn't stall IO.  Treat this request as !elvpriv.  This will\n\t * disturb iosched and blkcg but weird is bettern than dead.\n\t */\n\tprintk_ratelimited(KERN_WARNING \"%s: dev %s: request aux data allocation failed, iosched may be disturbed\\n\",\n\t\t\t   __func__, dev_name(q->backing_dev_info->dev));\n\n\trq->rq_flags &= ~RQF_ELVPRIV;\n\trq->elv.icq = NULL;\n\n\tspin_lock_irq(q->queue_lock);\n\tq->nr_rqs_elvpriv--;\n\tspin_unlock_irq(q->queue_lock);\n\tgoto out;\n\nfail_alloc:\n\t/*\n\t * Allocation failed presumably due to memory. Undo anything we\n\t * might have messed up.\n\t *\n\t * Allocating task should really be put onto the front of the wait\n\t * queue, but this is pretty rare.\n\t */\n\tspin_lock_irq(q->queue_lock);\n\tfreed_request(rl, is_sync, rq_flags);\n\n\t/*\n\t * in the very unlikely event that allocation failed and no\n\t * requests for this direction was pending, mark us starved so that\n\t * freeing of a request in the other direction will notice\n\t * us. another possible fix would be to split the rq mempool into\n\t * READ and WRITE\n\t */\nrq_starved:\n\tif (unlikely(rl->count[is_sync] == 0))\n\t\trl->starved[is_sync] = 1;\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/**\n * get_request - get a free request\n * @q: request_queue to allocate request from\n * @op: operation and flags\n * @bio: bio to allocate request for (can be %NULL)\n * @flags: BLK_MQ_REQ_* flags.\n * @gfp: allocator flags\n *\n * Get a free request from @q.  If %BLK_MQ_REQ_NOWAIT is set in @flags,\n * this function keeps retrying under memory pressure and fails iff @q is dead.\n *\n * Must be called with @q->queue_lock held and,\n * Returns ERR_PTR on failure, with @q->queue_lock held.\n * Returns request pointer on success, with @q->queue_lock *not held*.\n */\nstatic struct request *get_request(struct request_queue *q, unsigned int op,\n\t\tstruct bio *bio, blk_mq_req_flags_t flags, gfp_t gfp)\n{\n\tconst bool is_sync = op_is_sync(op);\n\tDEFINE_WAIT(wait);\n\tstruct request_list *rl;\n\tstruct request *rq;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\trl = blk_get_rl(q, bio);\t/* transferred to @rq on success */\nretry:\n\trq = __get_request(rl, op, bio, flags, gfp);\n\tif (!IS_ERR(rq))\n\t\treturn rq;\n\n\tif (op & REQ_NOWAIT) {\n\t\tblk_put_rl(rl);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\tif ((flags & BLK_MQ_REQ_NOWAIT) || unlikely(blk_queue_dying(q))) {\n\t\tblk_put_rl(rl);\n\t\treturn rq;\n\t}\n\n\t/* wait on @rl and retry */\n\tprepare_to_wait_exclusive(&rl->wait[is_sync], &wait,\n\t\t\t\t  TASK_UNINTERRUPTIBLE);\n\n\ttrace_block_sleeprq(q, bio, op);\n\n\tspin_unlock_irq(q->queue_lock);\n\tio_schedule();\n\n\t/*\n\t * After sleeping, we become a \"batching\" process and will be able\n\t * to allocate at least one request, and up to a big batch of them\n\t * for a small period time.  See ioc_batching, ioc_set_batching\n\t */\n\tioc_set_batching(q, current->io_context);\n\n\tspin_lock_irq(q->queue_lock);\n\tfinish_wait(&rl->wait[is_sync], &wait);\n\n\tgoto retry;\n}\n\n/* flags: BLK_MQ_REQ_PREEMPT and/or BLK_MQ_REQ_NOWAIT. */\nstatic struct request *blk_old_get_request(struct request_queue *q,\n\t\t\t\tunsigned int op, blk_mq_req_flags_t flags)\n{\n\tstruct request *rq;\n\tgfp_t gfp_mask = flags & BLK_MQ_REQ_NOWAIT ? GFP_ATOMIC : GFP_NOIO;\n\tint ret = 0;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\t/* create ioc upfront */\n\tcreate_io_context(gfp_mask, q->node);\n\n\tret = blk_queue_enter(q, flags);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tspin_lock_irq(q->queue_lock);\n\trq = get_request(q, op, NULL, flags, gfp_mask);\n\tif (IS_ERR(rq)) {\n\t\tspin_unlock_irq(q->queue_lock);\n\t\tblk_queue_exit(q);\n\t\treturn rq;\n\t}\n\n\t/* q->queue_lock is unlocked at this point */\n\trq->__data_len = 0;\n\trq->__sector = (sector_t) -1;\n\trq->bio = rq->biotail = NULL;\n\treturn rq;\n}\n\n/**\n * blk_get_request - allocate a request\n * @q: request queue to allocate a request for\n * @op: operation (REQ_OP_*) and REQ_* flags, e.g. REQ_SYNC.\n * @flags: BLK_MQ_REQ_* flags, e.g. BLK_MQ_REQ_NOWAIT.\n */\nstruct request *blk_get_request(struct request_queue *q, unsigned int op,\n\t\t\t\tblk_mq_req_flags_t flags)\n{\n\tstruct request *req;\n\n\tWARN_ON_ONCE(op & REQ_NOWAIT);\n\tWARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));\n\n\tif (q->mq_ops) {\n\t\treq = blk_mq_alloc_request(q, op, flags);\n\t\tif (!IS_ERR(req) && q->mq_ops->initialize_rq_fn)\n\t\t\tq->mq_ops->initialize_rq_fn(req);\n\t} else {\n\t\treq = blk_old_get_request(q, op, flags);\n\t\tif (!IS_ERR(req) && q->initialize_rq_fn)\n\t\t\tq->initialize_rq_fn(req);\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL(blk_get_request);\n\n/**\n * blk_requeue_request - put a request back on queue\n * @q:\t\trequest queue where request should be inserted\n * @rq:\t\trequest to be inserted\n *\n * Description:\n *    Drivers often keep queueing requests until the hardware cannot accept\n *    more, when that condition happens we need to put the request back\n *    on the queue. Must be called with queue lock held.\n */\nvoid blk_requeue_request(struct request_queue *q, struct request *rq)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tblk_delete_timer(rq);\n\tblk_clear_rq_complete(rq);\n\ttrace_block_rq_requeue(q, rq);\n\trq_qos_requeue(q, rq);\n\n\tif (rq->rq_flags & RQF_QUEUED)\n\t\tblk_queue_end_tag(q, rq);\n\n\tBUG_ON(blk_queued_rq(rq));\n\n\telv_requeue_request(q, rq);\n}\nEXPORT_SYMBOL(blk_requeue_request);\n\nstatic void add_acct_request(struct request_queue *q, struct request *rq,\n\t\t\t     int where)\n{\n\tblk_account_io_start(rq, true);\n\t__elv_add_request(q, rq, where);\n}\n\nstatic void part_round_stats_single(struct request_queue *q, int cpu,\n\t\t\t\t    struct hd_struct *part, unsigned long now,\n\t\t\t\t    unsigned int inflight)\n{\n\tif (inflight) {\n\t\t__part_stat_add(cpu, part, time_in_queue,\n\t\t\t\tinflight * (now - part->stamp));\n\t\t__part_stat_add(cpu, part, io_ticks, (now - part->stamp));\n\t}\n\tpart->stamp = now;\n}\n\n/**\n * part_round_stats() - Round off the performance stats on a struct disk_stats.\n * @q: target block queue\n * @cpu: cpu number for stats access\n * @part: target partition\n *\n * The average IO queue length and utilisation statistics are maintained\n * by observing the current state of the queue length and the amount of\n * time it has been in this state for.\n *\n * Normally, that accounting is done on IO completion, but that can result\n * in more than a second's worth of IO being accounted for within any one\n * second, leading to >100% utilisation.  To deal with that, we call this\n * function to do a round-off before returning the results when reading\n * /proc/diskstats.  This accounts immediately for all queue usage up to\n * the current jiffies and restarts the counters again.\n */\nvoid part_round_stats(struct request_queue *q, int cpu, struct hd_struct *part)\n{\n\tstruct hd_struct *part2 = NULL;\n\tunsigned long now = jiffies;\n\tunsigned int inflight[2];\n\tint stats = 0;\n\n\tif (part->stamp != now)\n\t\tstats |= 1;\n\n\tif (part->partno) {\n\t\tpart2 = &part_to_disk(part)->part0;\n\t\tif (part2->stamp != now)\n\t\t\tstats |= 2;\n\t}\n\n\tif (!stats)\n\t\treturn;\n\n\tpart_in_flight(q, part, inflight);\n\n\tif (stats & 2)\n\t\tpart_round_stats_single(q, cpu, part2, now, inflight[1]);\n\tif (stats & 1)\n\t\tpart_round_stats_single(q, cpu, part, now, inflight[0]);\n}\nEXPORT_SYMBOL_GPL(part_round_stats);\n\n#ifdef CONFIG_PM\nstatic void blk_pm_put_request(struct request *rq)\n{\n\tif (rq->q->dev && !(rq->rq_flags & RQF_PM) && !--rq->q->nr_pending)\n\t\tpm_runtime_mark_last_busy(rq->q->dev);\n}\n#else\nstatic inline void blk_pm_put_request(struct request *rq) {}\n#endif\n\nvoid __blk_put_request(struct request_queue *q, struct request *req)\n{\n\treq_flags_t rq_flags = req->rq_flags;\n\n\tif (unlikely(!q))\n\t\treturn;\n\n\tif (q->mq_ops) {\n\t\tblk_mq_free_request(req);\n\t\treturn;\n\t}\n\n\tlockdep_assert_held(q->queue_lock);\n\n\tblk_req_zone_write_unlock(req);\n\tblk_pm_put_request(req);\n\n\telv_completed_request(q, req);\n\n\t/* this is a bio leak */\n\tWARN_ON(req->bio != NULL);\n\n\trq_qos_done(q, req);\n\n\t/*\n\t * Request may not have originated from ll_rw_blk. if not,\n\t * it didn't come out of our reserved rq pools\n\t */\n\tif (rq_flags & RQF_ALLOCED) {\n\t\tstruct request_list *rl = blk_rq_rl(req);\n\t\tbool sync = op_is_sync(req->cmd_flags);\n\n\t\tBUG_ON(!list_empty(&req->queuelist));\n\t\tBUG_ON(ELV_ON_HASH(req));\n\n\t\tblk_free_request(rl, req);\n\t\tfreed_request(rl, sync, rq_flags);\n\t\tblk_put_rl(rl);\n\t\tblk_queue_exit(q);\n\t}\n}\nEXPORT_SYMBOL_GPL(__blk_put_request);\n\nvoid blk_put_request(struct request *req)\n{\n\tstruct request_queue *q = req->q;\n\n\tif (q->mq_ops)\n\t\tblk_mq_free_request(req);\n\telse {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(q->queue_lock, flags);\n\t\t__blk_put_request(q, req);\n\t\tspin_unlock_irqrestore(q->queue_lock, flags);\n\t}\n}\nEXPORT_SYMBOL(blk_put_request);\n\nbool bio_attempt_back_merge(struct request_queue *q, struct request *req,\n\t\t\t    struct bio *bio)\n{\n\tconst int ff = bio->bi_opf & REQ_FAILFAST_MASK;\n\n\tif (!ll_back_merge_fn(q, req, bio))\n\t\treturn false;\n\n\ttrace_block_bio_backmerge(q, req, bio);\n\n\tif ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)\n\t\tblk_rq_set_mixed_merge(req);\n\n\treq->biotail->bi_next = bio;\n\treq->biotail = bio;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->ioprio = ioprio_best(req->ioprio, bio_prio(bio));\n\n\tblk_account_io_start(req, false);\n\treturn true;\n}\n\nbool bio_attempt_front_merge(struct request_queue *q, struct request *req,\n\t\t\t     struct bio *bio)\n{\n\tconst int ff = bio->bi_opf & REQ_FAILFAST_MASK;\n\n\tif (!ll_front_merge_fn(q, req, bio))\n\t\treturn false;\n\n\ttrace_block_bio_frontmerge(q, req, bio);\n\n\tif ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)\n\t\tblk_rq_set_mixed_merge(req);\n\n\tbio->bi_next = req->bio;\n\treq->bio = bio;\n\n\treq->__sector = bio->bi_iter.bi_sector;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->ioprio = ioprio_best(req->ioprio, bio_prio(bio));\n\n\tblk_account_io_start(req, false);\n\treturn true;\n}\n\nbool bio_attempt_discard_merge(struct request_queue *q, struct request *req,\n\t\tstruct bio *bio)\n{\n\tunsigned short segments = blk_rq_nr_discard_segments(req);\n\n\tif (segments >= queue_max_discard_segments(q))\n\t\tgoto no_merge;\n\tif (blk_rq_sectors(req) + bio_sectors(bio) >\n\t    blk_rq_get_max_sectors(req, blk_rq_pos(req)))\n\t\tgoto no_merge;\n\n\treq->biotail->bi_next = bio;\n\treq->biotail = bio;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->ioprio = ioprio_best(req->ioprio, bio_prio(bio));\n\treq->nr_phys_segments = segments + 1;\n\n\tblk_account_io_start(req, false);\n\treturn true;\nno_merge:\n\treq_set_nomerge(q, req);\n\treturn false;\n}\n\n/**\n * blk_attempt_plug_merge - try to merge with %current's plugged list\n * @q: request_queue new bio is being queued at\n * @bio: new bio being queued\n * @request_count: out parameter for number of traversed plugged requests\n * @same_queue_rq: pointer to &struct request that gets filled in when\n * another request associated with @q is found on the plug list\n * (optional, may be %NULL)\n *\n * Determine whether @bio being queued on @q can be merged with a request\n * on %current's plugged list.  Returns %true if merge was successful,\n * otherwise %false.\n *\n * Plugging coalesces IOs from the same issuer for the same purpose without\n * going through @q->queue_lock.  As such it's more of an issuing mechanism\n * than scheduling, and the request, while may have elvpriv data, is not\n * added on the elevator at this point.  In addition, we don't have\n * reliable access to the elevator outside queue lock.  Only check basic\n * merging parameters without querying the elevator.\n *\n * Caller must ensure !blk_queue_nomerges(q) beforehand.\n */\nbool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,\n\t\t\t    unsigned int *request_count,\n\t\t\t    struct request **same_queue_rq)\n{\n\tstruct blk_plug *plug;\n\tstruct request *rq;\n\tstruct list_head *plug_list;\n\n\tplug = current->plug;\n\tif (!plug)\n\t\treturn false;\n\t*request_count = 0;\n\n\tif (q->mq_ops)\n\t\tplug_list = &plug->mq_list;\n\telse\n\t\tplug_list = &plug->list;\n\n\tlist_for_each_entry_reverse(rq, plug_list, queuelist) {\n\t\tbool merged = false;\n\n\t\tif (rq->q == q) {\n\t\t\t(*request_count)++;\n\t\t\t/*\n\t\t\t * Only blk-mq multiple hardware queues case checks the\n\t\t\t * rq in the same queue, there should be only one such\n\t\t\t * rq in a queue\n\t\t\t **/\n\t\t\tif (same_queue_rq)\n\t\t\t\t*same_queue_rq = rq;\n\t\t}\n\n\t\tif (rq->q != q || !blk_rq_merge_ok(rq, bio))\n\t\t\tcontinue;\n\n\t\tswitch (blk_try_merge(rq, bio)) {\n\t\tcase ELEVATOR_BACK_MERGE:\n\t\t\tmerged = bio_attempt_back_merge(q, rq, bio);\n\t\t\tbreak;\n\t\tcase ELEVATOR_FRONT_MERGE:\n\t\t\tmerged = bio_attempt_front_merge(q, rq, bio);\n\t\t\tbreak;\n\t\tcase ELEVATOR_DISCARD_MERGE:\n\t\t\tmerged = bio_attempt_discard_merge(q, rq, bio);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (merged)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nunsigned int blk_plug_queued_count(struct request_queue *q)\n{\n\tstruct blk_plug *plug;\n\tstruct request *rq;\n\tstruct list_head *plug_list;\n\tunsigned int ret = 0;\n\n\tplug = current->plug;\n\tif (!plug)\n\t\tgoto out;\n\n\tif (q->mq_ops)\n\t\tplug_list = &plug->mq_list;\n\telse\n\t\tplug_list = &plug->list;\n\n\tlist_for_each_entry(rq, plug_list, queuelist) {\n\t\tif (rq->q == q)\n\t\t\tret++;\n\t}\nout:\n\treturn ret;\n}\n\nvoid blk_init_request_from_bio(struct request *req, struct bio *bio)\n{\n\tstruct io_context *ioc = rq_ioc(bio);\n\n\tif (bio->bi_opf & REQ_RAHEAD)\n\t\treq->cmd_flags |= REQ_FAILFAST_MASK;\n\n\treq->__sector = bio->bi_iter.bi_sector;\n\tif (ioprio_valid(bio_prio(bio)))\n\t\treq->ioprio = bio_prio(bio);\n\telse if (ioc)\n\t\treq->ioprio = ioc->ioprio;\n\telse\n\t\treq->ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, 0);\n\treq->write_hint = bio->bi_write_hint;\n\tblk_rq_bio_prep(req->q, req, bio);\n}\nEXPORT_SYMBOL_GPL(blk_init_request_from_bio);\n\nstatic blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)\n{\n\tstruct blk_plug *plug;\n\tint where = ELEVATOR_INSERT_SORT;\n\tstruct request *req, *free;\n\tunsigned int request_count = 0;\n\n\t/*\n\t * low level driver can indicate that it wants pages above a\n\t * certain limit bounced to low memory (ie for highmem, or even\n\t * ISA dma in theory)\n\t */\n\tblk_queue_bounce(q, &bio);\n\n\tblk_queue_split(q, &bio);\n\n\tif (!bio_integrity_prep(bio))\n\t\treturn BLK_QC_T_NONE;\n\n\tif (op_is_flush(bio->bi_opf)) {\n\t\tspin_lock_irq(q->queue_lock);\n\t\twhere = ELEVATOR_INSERT_FLUSH;\n\t\tgoto get_rq;\n\t}\n\n\t/*\n\t * Check if we can merge with the plugged list before grabbing\n\t * any locks.\n\t */\n\tif (!blk_queue_nomerges(q)) {\n\t\tif (blk_attempt_plug_merge(q, bio, &request_count, NULL))\n\t\t\treturn BLK_QC_T_NONE;\n\t} else\n\t\trequest_count = blk_plug_queued_count(q);\n\n\tspin_lock_irq(q->queue_lock);\n\n\tswitch (elv_merge(q, &req, bio)) {\n\tcase ELEVATOR_BACK_MERGE:\n\t\tif (!bio_attempt_back_merge(q, req, bio))\n\t\t\tbreak;\n\t\telv_bio_merged(q, req, bio);\n\t\tfree = attempt_back_merge(q, req);\n\t\tif (free)\n\t\t\t__blk_put_request(q, free);\n\t\telse\n\t\t\telv_merged_request(q, req, ELEVATOR_BACK_MERGE);\n\t\tgoto out_unlock;\n\tcase ELEVATOR_FRONT_MERGE:\n\t\tif (!bio_attempt_front_merge(q, req, bio))\n\t\t\tbreak;\n\t\telv_bio_merged(q, req, bio);\n\t\tfree = attempt_front_merge(q, req);\n\t\tif (free)\n\t\t\t__blk_put_request(q, free);\n\t\telse\n\t\t\telv_merged_request(q, req, ELEVATOR_FRONT_MERGE);\n\t\tgoto out_unlock;\n\tdefault:\n\t\tbreak;\n\t}\n\nget_rq:\n\trq_qos_throttle(q, bio, q->queue_lock);\n\n\t/*\n\t * Grab a free request. This is might sleep but can not fail.\n\t * Returns with the queue unlocked.\n\t */\n\tblk_queue_enter_live(q);\n\treq = get_request(q, bio->bi_opf, bio, 0, GFP_NOIO);\n\tif (IS_ERR(req)) {\n\t\tblk_queue_exit(q);\n\t\trq_qos_cleanup(q, bio);\n\t\tif (PTR_ERR(req) == -ENOMEM)\n\t\t\tbio->bi_status = BLK_STS_RESOURCE;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\tgoto out_unlock;\n\t}\n\n\trq_qos_track(q, req, bio);\n\n\t/*\n\t * After dropping the lock and possibly sleeping here, our request\n\t * may now be mergeable after it had proven unmergeable (above).\n\t * We don't worry about that case for efficiency. It won't happen\n\t * often, and the elevators are able to handle it.\n\t */\n\tblk_init_request_from_bio(req, bio);\n\n\tif (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags))\n\t\treq->cpu = raw_smp_processor_id();\n\n\tplug = current->plug;\n\tif (plug) {\n\t\t/*\n\t\t * If this is the first request added after a plug, fire\n\t\t * of a plug trace.\n\t\t *\n\t\t * @request_count may become stale because of schedule\n\t\t * out, so check plug list again.\n\t\t */\n\t\tif (!request_count || list_empty(&plug->list))\n\t\t\ttrace_block_plug(q);\n\t\telse {\n\t\t\tstruct request *last = list_entry_rq(plug->list.prev);\n\t\t\tif (request_count >= BLK_MAX_REQUEST_COUNT ||\n\t\t\t    blk_rq_bytes(last) >= BLK_PLUG_FLUSH_SIZE) {\n\t\t\t\tblk_flush_plug_list(plug, false);\n\t\t\t\ttrace_block_plug(q);\n\t\t\t}\n\t\t}\n\t\tlist_add_tail(&req->queuelist, &plug->list);\n\t\tblk_account_io_start(req, true);\n\t} else {\n\t\tspin_lock_irq(q->queue_lock);\n\t\tadd_acct_request(q, req, where);\n\t\t__blk_run_queue(q);\nout_unlock:\n\t\tspin_unlock_irq(q->queue_lock);\n\t}\n\n\treturn BLK_QC_T_NONE;\n}\n\nstatic void handle_bad_sector(struct bio *bio, sector_t maxsector)\n{\n\tchar b[BDEVNAME_SIZE];\n\n\tprintk(KERN_INFO \"attempt to access beyond end of device\\n\");\n\tprintk(KERN_INFO \"%s: rw=%d, want=%Lu, limit=%Lu\\n\",\n\t\t\tbio_devname(bio, b), bio->bi_opf,\n\t\t\t(unsigned long long)bio_end_sector(bio),\n\t\t\t(long long)maxsector);\n}\n\n#ifdef CONFIG_FAIL_MAKE_REQUEST\n\nstatic DECLARE_FAULT_ATTR(fail_make_request);\n\nstatic int __init setup_fail_make_request(char *str)\n{\n\treturn setup_fault_attr(&fail_make_request, str);\n}\n__setup(\"fail_make_request=\", setup_fail_make_request);\n\nstatic bool should_fail_request(struct hd_struct *part, unsigned int bytes)\n{\n\treturn part->make_it_fail && should_fail(&fail_make_request, bytes);\n}\n\nstatic int __init fail_make_request_debugfs(void)\n{\n\tstruct dentry *dir = fault_create_debugfs_attr(\"fail_make_request\",\n\t\t\t\t\t\tNULL, &fail_make_request);\n\n\treturn PTR_ERR_OR_ZERO(dir);\n}\n\nlate_initcall(fail_make_request_debugfs);\n\n#else /* CONFIG_FAIL_MAKE_REQUEST */\n\nstatic inline bool should_fail_request(struct hd_struct *part,\n\t\t\t\t\tunsigned int bytes)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_FAIL_MAKE_REQUEST */\n\nstatic inline bool bio_check_ro(struct bio *bio, struct hd_struct *part)\n{\n\tif (part->policy && op_is_write(bio_op(bio))) {\n\t\tchar b[BDEVNAME_SIZE];\n\n\t\tprintk(KERN_ERR\n\t\t       \"generic_make_request: Trying to write \"\n\t\t\t\"to read-only block-device %s (partno %d)\\n\",\n\t\t\tbio_devname(bio, b), part->partno);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic noinline int should_fail_bio(struct bio *bio)\n{\n\tif (should_fail_request(&bio->bi_disk->part0, bio->bi_iter.bi_size))\n\t\treturn -EIO;\n\treturn 0;\n}\nALLOW_ERROR_INJECTION(should_fail_bio, ERRNO);\n\n/*\n * Check whether this bio extends beyond the end of the device or partition.\n * This may well happen - the kernel calls bread() without checking the size of\n * the device, e.g., when mounting a file system.\n */\nstatic inline int bio_check_eod(struct bio *bio, sector_t maxsector)\n{\n\tunsigned int nr_sectors = bio_sectors(bio);\n\n\tif (nr_sectors && maxsector &&\n\t    (nr_sectors > maxsector ||\n\t     bio->bi_iter.bi_sector > maxsector - nr_sectors)) {\n\t\thandle_bad_sector(bio, maxsector);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n/*\n * Remap block n of partition p to block n+start(p) of the disk.\n */\nstatic inline int blk_partition_remap(struct bio *bio)\n{\n\tstruct hd_struct *p;\n\tint ret = -EIO;\n\n\trcu_read_lock();\n\tp = __disk_get_part(bio->bi_disk, bio->bi_partno);\n\tif (unlikely(!p))\n\t\tgoto out;\n\tif (unlikely(should_fail_request(p, bio->bi_iter.bi_size)))\n\t\tgoto out;\n\tif (unlikely(bio_check_ro(bio, p)))\n\t\tgoto out;\n\n\t/*\n\t * Zone reset does not include bi_size so bio_sectors() is always 0.\n\t * Include a test for the reset op code and perform the remap if needed.\n\t */\n\tif (bio_sectors(bio) || bio_op(bio) == REQ_OP_ZONE_RESET) {\n\t\tif (bio_check_eod(bio, part_nr_sects_read(p)))\n\t\t\tgoto out;\n\t\tbio->bi_iter.bi_sector += p->start_sect;\n\t\ttrace_block_bio_remap(bio->bi_disk->queue, bio, part_devt(p),\n\t\t\t\t      bio->bi_iter.bi_sector - p->start_sect);\n\t}\n\tbio->bi_partno = 0;\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic noinline_for_stack bool\ngeneric_make_request_checks(struct bio *bio)\n{\n\tstruct request_queue *q;\n\tint nr_sectors = bio_sectors(bio);\n\tblk_status_t status = BLK_STS_IOERR;\n\tchar b[BDEVNAME_SIZE];\n\n\tmight_sleep();\n\n\tq = bio->bi_disk->queue;\n\tif (unlikely(!q)) {\n\t\tprintk(KERN_ERR\n\t\t       \"generic_make_request: Trying to access \"\n\t\t\t\"nonexistent block-device %s (%Lu)\\n\",\n\t\t\tbio_devname(bio, b), (long long)bio->bi_iter.bi_sector);\n\t\tgoto end_io;\n\t}\n\n\t/*\n\t * For a REQ_NOWAIT based request, return -EOPNOTSUPP\n\t * if queue is not a request based queue.\n\t */\n\tif ((bio->bi_opf & REQ_NOWAIT) && !queue_is_rq_based(q))\n\t\tgoto not_supported;\n\n\tif (should_fail_bio(bio))\n\t\tgoto end_io;\n\n\tif (bio->bi_partno) {\n\t\tif (unlikely(blk_partition_remap(bio)))\n\t\t\tgoto end_io;\n\t} else {\n\t\tif (unlikely(bio_check_ro(bio, &bio->bi_disk->part0)))\n\t\t\tgoto end_io;\n\t\tif (unlikely(bio_check_eod(bio, get_capacity(bio->bi_disk))))\n\t\t\tgoto end_io;\n\t}\n\n\t/*\n\t * Filter flush bio's early so that make_request based\n\t * drivers without flush support don't have to worry\n\t * about them.\n\t */\n\tif (op_is_flush(bio->bi_opf) &&\n\t    !test_bit(QUEUE_FLAG_WC, &q->queue_flags)) {\n\t\tbio->bi_opf &= ~(REQ_PREFLUSH | REQ_FUA);\n\t\tif (!nr_sectors) {\n\t\t\tstatus = BLK_STS_OK;\n\t\t\tgoto end_io;\n\t\t}\n\t}\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\t\tif (!blk_queue_discard(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_SECURE_ERASE:\n\t\tif (!blk_queue_secure_erase(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_WRITE_SAME:\n\t\tif (!q->limits.max_write_same_sectors)\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_ZONE_REPORT:\n\tcase REQ_OP_ZONE_RESET:\n\t\tif (!blk_queue_is_zoned(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tif (!q->limits.max_write_zeroes_sectors)\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/*\n\t * Various block parts want %current->io_context and lazy ioc\n\t * allocation ends up trading a lot of pain for a small amount of\n\t * memory.  Just allocate it upfront.  This may fail and block\n\t * layer knows how to live with it.\n\t */\n\tcreate_io_context(GFP_ATOMIC, q->node);\n\n\tif (!blkcg_bio_issue_check(q, bio))\n\t\treturn false;\n\n\tif (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {\n\t\ttrace_block_bio_queue(q, bio);\n\t\t/* Now that enqueuing has been traced, we need to trace\n\t\t * completion as well.\n\t\t */\n\t\tbio_set_flag(bio, BIO_TRACE_COMPLETION);\n\t}\n\treturn true;\n\nnot_supported:\n\tstatus = BLK_STS_NOTSUPP;\nend_io:\n\tbio->bi_status = status;\n\tbio_endio(bio);\n\treturn false;\n}\n\n/**\n * generic_make_request - hand a buffer to its device driver for I/O\n * @bio:  The bio describing the location in memory and on the device.\n *\n * generic_make_request() is used to make I/O requests of block\n * devices. It is passed a &struct bio, which describes the I/O that needs\n * to be done.\n *\n * generic_make_request() does not return any status.  The\n * success/failure status of the request, along with notification of\n * completion, is delivered asynchronously through the bio->bi_end_io\n * function described (one day) else where.\n *\n * The caller of generic_make_request must make sure that bi_io_vec\n * are set to describe the memory buffer, and that bi_dev and bi_sector are\n * set to describe the device address, and the\n * bi_end_io and optionally bi_private are set to describe how\n * completion notification should be signaled.\n *\n * generic_make_request and the drivers it calls may use bi_next if this\n * bio happens to be merged with someone else, and may resubmit the bio to\n * a lower device by calling into generic_make_request recursively, which\n * means the bio should NOT be touched after the call to ->make_request_fn.\n */\nblk_qc_t generic_make_request(struct bio *bio)\n{\n\t/*\n\t * bio_list_on_stack[0] contains bios submitted by the current\n\t * make_request_fn.\n\t * bio_list_on_stack[1] contains bios that were submitted before\n\t * the current make_request_fn, but that haven't been processed\n\t * yet.\n\t */\n\tstruct bio_list bio_list_on_stack[2];\n\tblk_mq_req_flags_t flags = 0;\n\tstruct request_queue *q = bio->bi_disk->queue;\n\tblk_qc_t ret = BLK_QC_T_NONE;\n\n\tif (bio->bi_opf & REQ_NOWAIT)\n\t\tflags = BLK_MQ_REQ_NOWAIT;\n\tif (bio_flagged(bio, BIO_QUEUE_ENTERED))\n\t\tblk_queue_enter_live(q);\n\telse if (blk_queue_enter(q, flags) < 0) {\n\t\tif (!blk_queue_dying(q) && (bio->bi_opf & REQ_NOWAIT))\n\t\t\tbio_wouldblock_error(bio);\n\t\telse\n\t\t\tbio_io_error(bio);\n\t\treturn ret;\n\t}\n\n\tif (!generic_make_request_checks(bio))\n\t\tgoto out;\n\n\t/*\n\t * We only want one ->make_request_fn to be active at a time, else\n\t * stack usage with stacked devices could be a problem.  So use\n\t * current->bio_list to keep a list of requests submited by a\n\t * make_request_fn function.  current->bio_list is also used as a\n\t * flag to say if generic_make_request is currently active in this\n\t * task or not.  If it is NULL, then no make_request is active.  If\n\t * it is non-NULL, then a make_request is active, and new requests\n\t * should be added at the tail\n\t */\n\tif (current->bio_list) {\n\t\tbio_list_add(&current->bio_list[0], bio);\n\t\tgoto out;\n\t}\n\n\t/* following loop may be a bit non-obvious, and so deserves some\n\t * explanation.\n\t * Before entering the loop, bio->bi_next is NULL (as all callers\n\t * ensure that) so we have a list with a single bio.\n\t * We pretend that we have just taken it off a longer list, so\n\t * we assign bio_list to a pointer to the bio_list_on_stack,\n\t * thus initialising the bio_list of new bios to be\n\t * added.  ->make_request() may indeed add some more bios\n\t * through a recursive call to generic_make_request.  If it\n\t * did, we find a non-NULL value in bio_list and re-enter the loop\n\t * from the top.  In this case we really did just take the bio\n\t * of the top of the list (no pretending) and so remove it from\n\t * bio_list, and call into ->make_request() again.\n\t */\n\tBUG_ON(bio->bi_next);\n\tbio_list_init(&bio_list_on_stack[0]);\n\tcurrent->bio_list = bio_list_on_stack;\n\tdo {\n\t\tbool enter_succeeded = true;\n\n\t\tif (unlikely(q != bio->bi_disk->queue)) {\n\t\t\tif (q)\n\t\t\t\tblk_queue_exit(q);\n\t\t\tq = bio->bi_disk->queue;\n\t\t\tflags = 0;\n\t\t\tif (bio->bi_opf & REQ_NOWAIT)\n\t\t\t\tflags = BLK_MQ_REQ_NOWAIT;\n\t\t\tif (blk_queue_enter(q, flags) < 0) {\n\t\t\t\tenter_succeeded = false;\n\t\t\t\tq = NULL;\n\t\t\t}\n\t\t}\n\n\t\tif (enter_succeeded) {\n\t\t\tstruct bio_list lower, same;\n\n\t\t\t/* Create a fresh bio_list for all subordinate requests */\n\t\t\tbio_list_on_stack[1] = bio_list_on_stack[0];\n\t\t\tbio_list_init(&bio_list_on_stack[0]);\n\t\t\tret = q->make_request_fn(q, bio);\n\n\t\t\t/* sort new bios into those for a lower level\n\t\t\t * and those for the same level\n\t\t\t */\n\t\t\tbio_list_init(&lower);\n\t\t\tbio_list_init(&same);\n\t\t\twhile ((bio = bio_list_pop(&bio_list_on_stack[0])) != NULL)\n\t\t\t\tif (q == bio->bi_disk->queue)\n\t\t\t\t\tbio_list_add(&same, bio);\n\t\t\t\telse\n\t\t\t\t\tbio_list_add(&lower, bio);\n\t\t\t/* now assemble so we handle the lowest level first */\n\t\t\tbio_list_merge(&bio_list_on_stack[0], &lower);\n\t\t\tbio_list_merge(&bio_list_on_stack[0], &same);\n\t\t\tbio_list_merge(&bio_list_on_stack[0], &bio_list_on_stack[1]);\n\t\t} else {\n\t\t\tif (unlikely(!blk_queue_dying(q) &&\n\t\t\t\t\t(bio->bi_opf & REQ_NOWAIT)))\n\t\t\t\tbio_wouldblock_error(bio);\n\t\t\telse\n\t\t\t\tbio_io_error(bio);\n\t\t}\n\t\tbio = bio_list_pop(&bio_list_on_stack[0]);\n\t} while (bio);\n\tcurrent->bio_list = NULL; /* deactivate */\n\nout:\n\tif (q)\n\t\tblk_queue_exit(q);\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_make_request);\n\n/**\n * direct_make_request - hand a buffer directly to its device driver for I/O\n * @bio:  The bio describing the location in memory and on the device.\n *\n * This function behaves like generic_make_request(), but does not protect\n * against recursion.  Must only be used if the called driver is known\n * to not call generic_make_request (or direct_make_request) again from\n * its make_request function.  (Calling direct_make_request again from\n * a workqueue is perfectly fine as that doesn't recurse).\n */\nblk_qc_t direct_make_request(struct bio *bio)\n{\n\tstruct request_queue *q = bio->bi_disk->queue;\n\tbool nowait = bio->bi_opf & REQ_NOWAIT;\n\tblk_qc_t ret;\n\n\tif (!generic_make_request_checks(bio))\n\t\treturn BLK_QC_T_NONE;\n\n\tif (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {\n\t\tif (nowait && !blk_queue_dying(q))\n\t\t\tbio->bi_status = BLK_STS_AGAIN;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn BLK_QC_T_NONE;\n\t}\n\n\tret = q->make_request_fn(q, bio);\n\tblk_queue_exit(q);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(direct_make_request);\n\n/**\n * submit_bio - submit a bio to the block device layer for I/O\n * @bio: The &struct bio which describes the I/O\n *\n * submit_bio() is very similar in purpose to generic_make_request(), and\n * uses that function to do most of the work. Both are fairly rough\n * interfaces; @bio must be presetup and ready for I/O.\n *\n */\nblk_qc_t submit_bio(struct bio *bio)\n{\n\t/*\n\t * If it's a regular read/write or a barrier with data attached,\n\t * go through the normal accounting stuff before submission.\n\t */\n\tif (bio_has_data(bio)) {\n\t\tunsigned int count;\n\n\t\tif (unlikely(bio_op(bio) == REQ_OP_WRITE_SAME))\n\t\t\tcount = queue_logical_block_size(bio->bi_disk->queue) >> 9;\n\t\telse\n\t\t\tcount = bio_sectors(bio);\n\n\t\tif (op_is_write(bio_op(bio))) {\n\t\t\tcount_vm_events(PGPGOUT, count);\n\t\t} else {\n\t\t\ttask_io_account_read(bio->bi_iter.bi_size);\n\t\t\tcount_vm_events(PGPGIN, count);\n\t\t}\n\n\t\tif (unlikely(block_dump)) {\n\t\t\tchar b[BDEVNAME_SIZE];\n\t\t\tprintk(KERN_DEBUG \"%s(%d): %s block %Lu on %s (%u sectors)\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t\top_is_write(bio_op(bio)) ? \"WRITE\" : \"READ\",\n\t\t\t\t(unsigned long long)bio->bi_iter.bi_sector,\n\t\t\t\tbio_devname(bio, b), count);\n\t\t}\n\t}\n\n\treturn generic_make_request(bio);\n}\nEXPORT_SYMBOL(submit_bio);\n\nbool blk_poll(struct request_queue *q, blk_qc_t cookie)\n{\n\tif (!q->poll_fn || !blk_qc_t_valid(cookie))\n\t\treturn false;\n\n\tif (current->plug)\n\t\tblk_flush_plug_list(current->plug, false);\n\treturn q->poll_fn(q, cookie);\n}\nEXPORT_SYMBOL_GPL(blk_poll);\n\n/**\n * blk_cloned_rq_check_limits - Helper function to check a cloned request\n *                              for new the queue limits\n * @q:  the queue\n * @rq: the request being checked\n *\n * Description:\n *    @rq may have been made based on weaker limitations of upper-level queues\n *    in request stacking drivers, and it may violate the limitation of @q.\n *    Since the block layer and the underlying device driver trust @rq\n *    after it is inserted to @q, it should be checked against @q before\n *    the insertion using this generic function.\n *\n *    Request stacking drivers like request-based dm may change the queue\n *    limits when retrying requests on other queues. Those requests need\n *    to be checked against the new queue limits again during dispatch.\n */\nstatic int blk_cloned_rq_check_limits(struct request_queue *q,\n\t\t\t\t      struct request *rq)\n{\n\tif (blk_rq_sectors(rq) > blk_queue_get_max_sectors(q, req_op(rq))) {\n\t\tprintk(KERN_ERR \"%s: over max size limit.\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * queue's settings related to segment counting like q->bounce_pfn\n\t * may differ from that of other stacking queues.\n\t * Recalculate it to check the request correctly on this queue's\n\t * limitation.\n\t */\n\tblk_recalc_rq_segments(rq);\n\tif (rq->nr_phys_segments > queue_max_segments(q)) {\n\t\tprintk(KERN_ERR \"%s: over max segments limit.\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * blk_insert_cloned_request - Helper for stacking drivers to submit a request\n * @q:  the queue to submit the request\n * @rq: the request being queued\n */\nblk_status_t blk_insert_cloned_request(struct request_queue *q, struct request *rq)\n{\n\tunsigned long flags;\n\tint where = ELEVATOR_INSERT_BACK;\n\n\tif (blk_cloned_rq_check_limits(q, rq))\n\t\treturn BLK_STS_IOERR;\n\n\tif (rq->rq_disk &&\n\t    should_fail_request(&rq->rq_disk->part0, blk_rq_bytes(rq)))\n\t\treturn BLK_STS_IOERR;\n\n\tif (q->mq_ops) {\n\t\tif (blk_queue_io_stat(q))\n\t\t\tblk_account_io_start(rq, true);\n\t\t/*\n\t\t * Since we have a scheduler attached on the top device,\n\t\t * bypass a potential scheduler on the bottom device for\n\t\t * insert.\n\t\t */\n\t\treturn blk_mq_request_issue_directly(rq);\n\t}\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tif (unlikely(blk_queue_dying(q))) {\n\t\tspin_unlock_irqrestore(q->queue_lock, flags);\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\t/*\n\t * Submitting request must be dequeued before calling this function\n\t * because it will be linked to another request_queue\n\t */\n\tBUG_ON(blk_queued_rq(rq));\n\n\tif (op_is_flush(rq->cmd_flags))\n\t\twhere = ELEVATOR_INSERT_FLUSH;\n\n\tadd_acct_request(q, rq, where);\n\tif (where == ELEVATOR_INSERT_FLUSH)\n\t\t__blk_run_queue(q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn BLK_STS_OK;\n}\nEXPORT_SYMBOL_GPL(blk_insert_cloned_request);\n\n/**\n * blk_rq_err_bytes - determine number of bytes till the next failure boundary\n * @rq: request to examine\n *\n * Description:\n *     A request could be merge of IOs which require different failure\n *     handling.  This function determines the number of bytes which\n *     can be failed from the beginning of the request without\n *     crossing into area which need to be retried further.\n *\n * Return:\n *     The number of bytes to fail.\n */\nunsigned int blk_rq_err_bytes(const struct request *rq)\n{\n\tunsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;\n\tunsigned int bytes = 0;\n\tstruct bio *bio;\n\n\tif (!(rq->rq_flags & RQF_MIXED_MERGE))\n\t\treturn blk_rq_bytes(rq);\n\n\t/*\n\t * Currently the only 'mixing' which can happen is between\n\t * different fastfail types.  We can safely fail portions\n\t * which have all the failfast bits that the first one has -\n\t * the ones which are at least as eager to fail as the first\n\t * one.\n\t */\n\tfor (bio = rq->bio; bio; bio = bio->bi_next) {\n\t\tif ((bio->bi_opf & ff) != ff)\n\t\t\tbreak;\n\t\tbytes += bio->bi_iter.bi_size;\n\t}\n\n\t/* this could lead to infinite loop */\n\tBUG_ON(blk_rq_bytes(rq) && !bytes);\n\treturn bytes;\n}\nEXPORT_SYMBOL_GPL(blk_rq_err_bytes);\n\nvoid blk_account_io_completion(struct request *req, unsigned int bytes)\n{\n\tif (blk_do_io_stat(req)) {\n\t\tconst int sgrp = op_stat_group(req_op(req));\n\t\tstruct hd_struct *part;\n\t\tint cpu;\n\n\t\tcpu = part_stat_lock();\n\t\tpart = req->part;\n\t\tpart_stat_add(cpu, part, sectors[sgrp], bytes >> 9);\n\t\tpart_stat_unlock();\n\t}\n}\n\nvoid blk_account_io_done(struct request *req, u64 now)\n{\n\t/*\n\t * Account IO completion.  flush_rq isn't accounted as a\n\t * normal IO on queueing nor completion.  Accounting the\n\t * containing request is enough.\n\t */\n\tif (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) {\n\t\tunsigned long duration;\n\t\tconst int sgrp = op_stat_group(req_op(req));\n\t\tstruct hd_struct *part;\n\t\tint cpu;\n\n\t\tduration = nsecs_to_jiffies(now - req->start_time_ns);\n\t\tcpu = part_stat_lock();\n\t\tpart = req->part;\n\n\t\tpart_stat_inc(cpu, part, ios[sgrp]);\n\t\tpart_stat_add(cpu, part, ticks[sgrp], duration);\n\t\tpart_round_stats(req->q, cpu, part);\n\t\tpart_dec_in_flight(req->q, part, rq_data_dir(req));\n\n\t\thd_struct_put(part);\n\t\tpart_stat_unlock();\n\t}\n}\n\n#ifdef CONFIG_PM\n/*\n * Don't process normal requests when queue is suspended\n * or in the process of suspending/resuming\n */\nstatic bool blk_pm_allow_request(struct request *rq)\n{\n\tswitch (rq->q->rpm_status) {\n\tcase RPM_RESUMING:\n\tcase RPM_SUSPENDING:\n\t\treturn rq->rq_flags & RQF_PM;\n\tcase RPM_SUSPENDED:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n#else\nstatic bool blk_pm_allow_request(struct request *rq)\n{\n\treturn true;\n}\n#endif\n\nvoid blk_account_io_start(struct request *rq, bool new_io)\n{\n\tstruct hd_struct *part;\n\tint rw = rq_data_dir(rq);\n\tint cpu;\n\n\tif (!blk_do_io_stat(rq))\n\t\treturn;\n\n\tcpu = part_stat_lock();\n\n\tif (!new_io) {\n\t\tpart = rq->part;\n\t\tpart_stat_inc(cpu, part, merges[rw]);\n\t} else {\n\t\tpart = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));\n\t\tif (!hd_struct_try_get(part)) {\n\t\t\t/*\n\t\t\t * The partition is already being removed,\n\t\t\t * the request will be accounted on the disk only\n\t\t\t *\n\t\t\t * We take a reference on disk->part0 although that\n\t\t\t * partition will never be deleted, so we can treat\n\t\t\t * it as any other partition.\n\t\t\t */\n\t\t\tpart = &rq->rq_disk->part0;\n\t\t\thd_struct_get(part);\n\t\t}\n\t\tpart_round_stats(rq->q, cpu, part);\n\t\tpart_inc_in_flight(rq->q, part, rw);\n\t\trq->part = part;\n\t}\n\n\tpart_stat_unlock();\n}\n\nstatic struct request *elv_next_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\twhile (1) {\n\t\tlist_for_each_entry(rq, &q->queue_head, queuelist) {\n\t\t\tif (blk_pm_allow_request(rq))\n\t\t\t\treturn rq;\n\n\t\t\tif (rq->rq_flags & RQF_SOFTBARRIER)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Flush request is running and flush request isn't queueable\n\t\t * in the drive, we can hold the queue till flush request is\n\t\t * finished. Even we don't do this, driver can't dispatch next\n\t\t * requests and will requeue them. And this can improve\n\t\t * throughput too. For example, we have request flush1, write1,\n\t\t * flush 2. flush1 is dispatched, then queue is hold, write1\n\t\t * isn't inserted to queue. After flush1 is finished, flush2\n\t\t * will be dispatched. Since disk cache is already clean,\n\t\t * flush2 will be finished very soon, so looks like flush2 is\n\t\t * folded to flush1.\n\t\t * Since the queue is hold, a flag is set to indicate the queue\n\t\t * should be restarted later. Please see flush_end_io() for\n\t\t * details.\n\t\t */\n\t\tif (fq->flush_pending_idx != fq->flush_running_idx &&\n\t\t\t\t!queue_flush_queueable(q)) {\n\t\t\tfq->flush_queue_delayed = 1;\n\t\t\treturn NULL;\n\t\t}\n\t\tif (unlikely(blk_queue_bypass(q)) ||\n\t\t    !q->elevator->type->ops.sq.elevator_dispatch_fn(q, 0))\n\t\t\treturn NULL;\n\t}\n}\n\n/**\n * blk_peek_request - peek at the top of a request queue\n * @q: request queue to peek at\n *\n * Description:\n *     Return the request at the top of @q.  The returned request\n *     should be started using blk_start_request() before LLD starts\n *     processing it.\n *\n * Return:\n *     Pointer to the request at the top of @q if available.  Null\n *     otherwise.\n */\nstruct request *blk_peek_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\tint ret;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\twhile ((rq = elv_next_request(q)) != NULL) {\n\t\tif (!(rq->rq_flags & RQF_STARTED)) {\n\t\t\t/*\n\t\t\t * This is the first time the device driver\n\t\t\t * sees this request (possibly after\n\t\t\t * requeueing).  Notify IO scheduler.\n\t\t\t */\n\t\t\tif (rq->rq_flags & RQF_SORTED)\n\t\t\t\telv_activate_rq(q, rq);\n\n\t\t\t/*\n\t\t\t * just mark as started even if we don't start\n\t\t\t * it, a request that has been delayed should\n\t\t\t * not be passed by new incoming requests\n\t\t\t */\n\t\t\trq->rq_flags |= RQF_STARTED;\n\t\t\ttrace_block_rq_issue(q, rq);\n\t\t}\n\n\t\tif (!q->boundary_rq || q->boundary_rq == rq) {\n\t\t\tq->end_sector = rq_end_sector(rq);\n\t\t\tq->boundary_rq = NULL;\n\t\t}\n\n\t\tif (rq->rq_flags & RQF_DONTPREP)\n\t\t\tbreak;\n\n\t\tif (q->dma_drain_size && blk_rq_bytes(rq)) {\n\t\t\t/*\n\t\t\t * make sure space for the drain appears we\n\t\t\t * know we can do this because max_hw_segments\n\t\t\t * has been adjusted to be one fewer than the\n\t\t\t * device can handle\n\t\t\t */\n\t\t\trq->nr_phys_segments++;\n\t\t}\n\n\t\tif (!q->prep_rq_fn)\n\t\t\tbreak;\n\n\t\tret = q->prep_rq_fn(q, rq);\n\t\tif (ret == BLKPREP_OK) {\n\t\t\tbreak;\n\t\t} else if (ret == BLKPREP_DEFER) {\n\t\t\t/*\n\t\t\t * the request may have been (partially) prepped.\n\t\t\t * we need to keep this request in the front to\n\t\t\t * avoid resource deadlock.  RQF_STARTED will\n\t\t\t * prevent other fs requests from passing this one.\n\t\t\t */\n\t\t\tif (q->dma_drain_size && blk_rq_bytes(rq) &&\n\t\t\t    !(rq->rq_flags & RQF_DONTPREP)) {\n\t\t\t\t/*\n\t\t\t\t * remove the space for the drain we added\n\t\t\t\t * so that we don't add it again\n\t\t\t\t */\n\t\t\t\t--rq->nr_phys_segments;\n\t\t\t}\n\n\t\t\trq = NULL;\n\t\t\tbreak;\n\t\t} else if (ret == BLKPREP_KILL || ret == BLKPREP_INVALID) {\n\t\t\trq->rq_flags |= RQF_QUIET;\n\t\t\t/*\n\t\t\t * Mark this request as started so we don't trigger\n\t\t\t * any debug logic in the end I/O path.\n\t\t\t */\n\t\t\tblk_start_request(rq);\n\t\t\t__blk_end_request_all(rq, ret == BLKPREP_INVALID ?\n\t\t\t\t\tBLK_STS_TARGET : BLK_STS_IOERR);\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"%s: bad return=%d\\n\", __func__, ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rq;\n}\nEXPORT_SYMBOL(blk_peek_request);\n\nstatic void blk_dequeue_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tBUG_ON(list_empty(&rq->queuelist));\n\tBUG_ON(ELV_ON_HASH(rq));\n\n\tlist_del_init(&rq->queuelist);\n\n\t/*\n\t * the time frame between a request being removed from the lists\n\t * and to it is freed is accounted as io that is in progress at\n\t * the driver side.\n\t */\n\tif (blk_account_rq(rq))\n\t\tq->in_flight[rq_is_sync(rq)]++;\n}\n\n/**\n * blk_start_request - start request processing on the driver\n * @req: request to dequeue\n *\n * Description:\n *     Dequeue @req and start timeout timer on it.  This hands off the\n *     request to the driver.\n */\nvoid blk_start_request(struct request *req)\n{\n\tlockdep_assert_held(req->q->queue_lock);\n\tWARN_ON_ONCE(req->q->mq_ops);\n\n\tblk_dequeue_request(req);\n\n\tif (test_bit(QUEUE_FLAG_STATS, &req->q->queue_flags)) {\n\t\treq->io_start_time_ns = ktime_get_ns();\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\n\t\treq->throtl_size = blk_rq_sectors(req);\n#endif\n\t\treq->rq_flags |= RQF_STATS;\n\t\trq_qos_issue(req->q, req);\n\t}\n\n\tBUG_ON(blk_rq_is_complete(req));\n\tblk_add_timer(req);\n}\nEXPORT_SYMBOL(blk_start_request);\n\n/**\n * blk_fetch_request - fetch a request from a request queue\n * @q: request queue to fetch a request from\n *\n * Description:\n *     Return the request at the top of @q.  The request is started on\n *     return and LLD can start processing it immediately.\n *\n * Return:\n *     Pointer to the request at the top of @q if available.  Null\n *     otherwise.\n */\nstruct request *blk_fetch_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\trq = blk_peek_request(q);\n\tif (rq)\n\t\tblk_start_request(rq);\n\treturn rq;\n}\nEXPORT_SYMBOL(blk_fetch_request);\n\n/*\n * Steal bios from a request and add them to a bio list.\n * The request must not have been partially completed before.\n */\nvoid blk_steal_bios(struct bio_list *list, struct request *rq)\n{\n\tif (rq->bio) {\n\t\tif (list->tail)\n\t\t\tlist->tail->bi_next = rq->bio;\n\t\telse\n\t\t\tlist->head = rq->bio;\n\t\tlist->tail = rq->biotail;\n\n\t\trq->bio = NULL;\n\t\trq->biotail = NULL;\n\t}\n\n\trq->__data_len = 0;\n}\nEXPORT_SYMBOL_GPL(blk_steal_bios);\n\n/**\n * blk_update_request - Special helper function for request stacking drivers\n * @req:      the request being processed\n * @error:    block status code\n * @nr_bytes: number of bytes to complete @req\n *\n * Description:\n *     Ends I/O on a number of bytes attached to @req, but doesn't complete\n *     the request structure even if @req doesn't have leftover.\n *     If @req has leftover, sets it up for the next range of segments.\n *\n *     This special helper function is only for request stacking drivers\n *     (e.g. request-based dm) so that they can handle partial completion.\n *     Actual device drivers should use blk_end_request instead.\n *\n *     Passing the result of blk_rq_bytes() as @nr_bytes guarantees\n *     %false return from this function.\n *\n * Note:\n *\tThe RQF_SPECIAL_PAYLOAD flag is ignored on purpose in both\n *\tblk_rq_bytes() and in blk_update_request().\n *\n * Return:\n *     %false - this request doesn't have any more data\n *     %true  - this request has more data\n **/\nbool blk_update_request(struct request *req, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tint total_bytes;\n\n\ttrace_block_rq_complete(req, blk_status_to_errno(error), nr_bytes);\n\n\tif (!req->bio)\n\t\treturn false;\n\n\tif (unlikely(error && !blk_rq_is_passthrough(req) &&\n\t\t     !(req->rq_flags & RQF_QUIET)))\n\t\tprint_req_error(req, error);\n\n\tblk_account_io_completion(req, nr_bytes);\n\n\ttotal_bytes = 0;\n\twhile (req->bio) {\n\t\tstruct bio *bio = req->bio;\n\t\tunsigned bio_bytes = min(bio->bi_iter.bi_size, nr_bytes);\n\n\t\tif (bio_bytes == bio->bi_iter.bi_size)\n\t\t\treq->bio = bio->bi_next;\n\n\t\t/* Completion has already been traced */\n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\t\treq_bio_endio(req, bio, bio_bytes, error);\n\n\t\ttotal_bytes += bio_bytes;\n\t\tnr_bytes -= bio_bytes;\n\n\t\tif (!nr_bytes)\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * completely done\n\t */\n\tif (!req->bio) {\n\t\t/*\n\t\t * Reset counters so that the request stacking driver\n\t\t * can find how many bytes remain in the request\n\t\t * later.\n\t\t */\n\t\treq->__data_len = 0;\n\t\treturn false;\n\t}\n\n\treq->__data_len -= total_bytes;\n\n\t/* update sector only for requests with clear definition of sector */\n\tif (!blk_rq_is_passthrough(req))\n\t\treq->__sector += total_bytes >> 9;\n\n\t/* mixed attributes always follow the first bio */\n\tif (req->rq_flags & RQF_MIXED_MERGE) {\n\t\treq->cmd_flags &= ~REQ_FAILFAST_MASK;\n\t\treq->cmd_flags |= req->bio->bi_opf & REQ_FAILFAST_MASK;\n\t}\n\n\tif (!(req->rq_flags & RQF_SPECIAL_PAYLOAD)) {\n\t\t/*\n\t\t * If total number of sectors is less than the first segment\n\t\t * size, something has gone terribly wrong.\n\t\t */\n\t\tif (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {\n\t\t\tblk_dump_rq_flags(req, \"request botched\");\n\t\t\treq->__data_len = blk_rq_cur_bytes(req);\n\t\t}\n\n\t\t/* recalculate the number of segments */\n\t\tblk_recalc_rq_segments(req);\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(blk_update_request);\n\nstatic bool blk_update_bidi_request(struct request *rq, blk_status_t error,\n\t\t\t\t    unsigned int nr_bytes,\n\t\t\t\t    unsigned int bidi_bytes)\n{\n\tif (blk_update_request(rq, error, nr_bytes))\n\t\treturn true;\n\n\t/* Bidi request must be completed as a whole */\n\tif (unlikely(blk_bidi_rq(rq)) &&\n\t    blk_update_request(rq->next_rq, error, bidi_bytes))\n\t\treturn true;\n\n\tif (blk_queue_add_random(rq->q))\n\t\tadd_disk_randomness(rq->rq_disk);\n\n\treturn false;\n}\n\n/**\n * blk_unprep_request - unprepare a request\n * @req:\tthe request\n *\n * This function makes a request ready for complete resubmission (or\n * completion).  It happens only after all error handling is complete,\n * so represents the appropriate moment to deallocate any resources\n * that were allocated to the request in the prep_rq_fn.  The queue\n * lock is held when calling this.\n */\nvoid blk_unprep_request(struct request *req)\n{\n\tstruct request_queue *q = req->q;\n\n\treq->rq_flags &= ~RQF_DONTPREP;\n\tif (q->unprep_rq_fn)\n\t\tq->unprep_rq_fn(q, req);\n}\nEXPORT_SYMBOL_GPL(blk_unprep_request);\n\nvoid blk_finish_request(struct request *req, blk_status_t error)\n{\n\tstruct request_queue *q = req->q;\n\tu64 now = ktime_get_ns();\n\n\tlockdep_assert_held(req->q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (req->rq_flags & RQF_STATS)\n\t\tblk_stat_add(req, now);\n\n\tif (req->rq_flags & RQF_QUEUED)\n\t\tblk_queue_end_tag(q, req);\n\n\tBUG_ON(blk_queued_rq(req));\n\n\tif (unlikely(laptop_mode) && !blk_rq_is_passthrough(req))\n\t\tlaptop_io_completion(req->q->backing_dev_info);\n\n\tblk_delete_timer(req);\n\n\tif (req->rq_flags & RQF_DONTPREP)\n\t\tblk_unprep_request(req);\n\n\tblk_account_io_done(req, now);\n\n\tif (req->end_io) {\n\t\trq_qos_done(q, req);\n\t\treq->end_io(req, error);\n\t} else {\n\t\tif (blk_bidi_rq(req))\n\t\t\t__blk_put_request(req->next_rq->q, req->next_rq);\n\n\t\t__blk_put_request(q, req);\n\t}\n}\nEXPORT_SYMBOL(blk_finish_request);\n\n/**\n * blk_end_bidi_request - Complete a bidi request\n * @rq:         the request to complete\n * @error:      block status code\n * @nr_bytes:   number of bytes to complete @rq\n * @bidi_bytes: number of bytes to complete @rq->next_rq\n *\n * Description:\n *     Ends I/O on a number of bytes attached to @rq and @rq->next_rq.\n *     Drivers that supports bidi can safely call this member for any\n *     type of request, bidi or uni.  In the later case @bidi_bytes is\n *     just ignored.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nstatic bool blk_end_bidi_request(struct request *rq, blk_status_t error,\n\t\t\t\t unsigned int nr_bytes, unsigned int bidi_bytes)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))\n\t\treturn true;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tblk_finish_request(rq, error);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn false;\n}\n\n/**\n * __blk_end_bidi_request - Complete a bidi request with queue lock held\n * @rq:         the request to complete\n * @error:      block status code\n * @nr_bytes:   number of bytes to complete @rq\n * @bidi_bytes: number of bytes to complete @rq->next_rq\n *\n * Description:\n *     Identical to blk_end_bidi_request() except that queue lock is\n *     assumed to be locked on entry and remains so on return.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nstatic bool __blk_end_bidi_request(struct request *rq, blk_status_t error,\n\t\t\t\t   unsigned int nr_bytes, unsigned int bidi_bytes)\n{\n\tlockdep_assert_held(rq->q->queue_lock);\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\n\tif (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))\n\t\treturn true;\n\n\tblk_finish_request(rq, error);\n\n\treturn false;\n}\n\n/**\n * blk_end_request - Helper function for drivers to complete the request.\n * @rq:       the request being processed\n * @error:    block status code\n * @nr_bytes: number of bytes to complete\n *\n * Description:\n *     Ends I/O on a number of bytes attached to @rq.\n *     If @rq has leftover, sets it up for the next range of segments.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nbool blk_end_request(struct request *rq, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\treturn blk_end_bidi_request(rq, error, nr_bytes, 0);\n}\nEXPORT_SYMBOL(blk_end_request);\n\n/**\n * blk_end_request_all - Helper function for drives to finish the request.\n * @rq: the request to finish\n * @error: block status code\n *\n * Description:\n *     Completely finish @rq.\n */\nvoid blk_end_request_all(struct request *rq, blk_status_t error)\n{\n\tbool pending;\n\tunsigned int bidi_bytes = 0;\n\n\tif (unlikely(blk_bidi_rq(rq)))\n\t\tbidi_bytes = blk_rq_bytes(rq->next_rq);\n\n\tpending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);\n\tBUG_ON(pending);\n}\nEXPORT_SYMBOL(blk_end_request_all);\n\n/**\n * __blk_end_request - Helper function for drivers to complete the request.\n * @rq:       the request being processed\n * @error:    block status code\n * @nr_bytes: number of bytes to complete\n *\n * Description:\n *     Must be called with queue lock held unlike blk_end_request().\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nbool __blk_end_request(struct request *rq, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tlockdep_assert_held(rq->q->queue_lock);\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\n\treturn __blk_end_bidi_request(rq, error, nr_bytes, 0);\n}\nEXPORT_SYMBOL(__blk_end_request);\n\n/**\n * __blk_end_request_all - Helper function for drives to finish the request.\n * @rq: the request to finish\n * @error:    block status code\n *\n * Description:\n *     Completely finish @rq.  Must be called with queue lock held.\n */\nvoid __blk_end_request_all(struct request *rq, blk_status_t error)\n{\n\tbool pending;\n\tunsigned int bidi_bytes = 0;\n\n\tlockdep_assert_held(rq->q->queue_lock);\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\n\tif (unlikely(blk_bidi_rq(rq)))\n\t\tbidi_bytes = blk_rq_bytes(rq->next_rq);\n\n\tpending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);\n\tBUG_ON(pending);\n}\nEXPORT_SYMBOL(__blk_end_request_all);\n\n/**\n * __blk_end_request_cur - Helper function to finish the current request chunk.\n * @rq: the request to finish the current chunk for\n * @error:    block status code\n *\n * Description:\n *     Complete the current consecutively mapped chunk from @rq.  Must\n *     be called with queue lock held.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n */\nbool __blk_end_request_cur(struct request *rq, blk_status_t error)\n{\n\treturn __blk_end_request(rq, error, blk_rq_cur_bytes(rq));\n}\nEXPORT_SYMBOL(__blk_end_request_cur);\n\nvoid blk_rq_bio_prep(struct request_queue *q, struct request *rq,\n\t\t     struct bio *bio)\n{\n\tif (bio_has_data(bio))\n\t\trq->nr_phys_segments = bio_phys_segments(q, bio);\n\telse if (bio_op(bio) == REQ_OP_DISCARD)\n\t\trq->nr_phys_segments = 1;\n\n\trq->__data_len = bio->bi_iter.bi_size;\n\trq->bio = rq->biotail = bio;\n\n\tif (bio->bi_disk)\n\t\trq->rq_disk = bio->bi_disk;\n}\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\n/**\n * rq_flush_dcache_pages - Helper function to flush all pages in a request\n * @rq: the request to be flushed\n *\n * Description:\n *     Flush all pages in @rq.\n */\nvoid rq_flush_dcache_pages(struct request *rq)\n{\n\tstruct req_iterator iter;\n\tstruct bio_vec bvec;\n\n\trq_for_each_segment(bvec, rq, iter)\n\t\tflush_dcache_page(bvec.bv_page);\n}\nEXPORT_SYMBOL_GPL(rq_flush_dcache_pages);\n#endif\n\n/**\n * blk_lld_busy - Check if underlying low-level drivers of a device are busy\n * @q : the queue of the device being checked\n *\n * Description:\n *    Check if underlying low-level drivers of a device are busy.\n *    If the drivers want to export their busy state, they must set own\n *    exporting function using blk_queue_lld_busy() first.\n *\n *    Basically, this function is used only by request stacking drivers\n *    to stop dispatching requests to underlying devices when underlying\n *    devices are busy.  This behavior helps more I/O merging on the queue\n *    of the request stacking driver and prevents I/O throughput regression\n *    on burst I/O load.\n *\n * Return:\n *    0 - Not busy (The request stacking driver should dispatch request)\n *    1 - Busy (The request stacking driver should stop dispatching request)\n */\nint blk_lld_busy(struct request_queue *q)\n{\n\tif (q->lld_busy_fn)\n\t\treturn q->lld_busy_fn(q);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blk_lld_busy);\n\n/**\n * blk_rq_unprep_clone - Helper function to free all bios in a cloned request\n * @rq: the clone request to be cleaned up\n *\n * Description:\n *     Free all bios in @rq for a cloned request.\n */\nvoid blk_rq_unprep_clone(struct request *rq)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = rq->bio) != NULL) {\n\t\trq->bio = bio->bi_next;\n\n\t\tbio_put(bio);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_rq_unprep_clone);\n\n/*\n * Copy attributes of the original request to the clone request.\n * The actual data parts (e.g. ->cmd, ->sense) are not copied.\n */\nstatic void __blk_rq_prep_clone(struct request *dst, struct request *src)\n{\n\tdst->cpu = src->cpu;\n\tdst->__sector = blk_rq_pos(src);\n\tdst->__data_len = blk_rq_bytes(src);\n\tif (src->rq_flags & RQF_SPECIAL_PAYLOAD) {\n\t\tdst->rq_flags |= RQF_SPECIAL_PAYLOAD;\n\t\tdst->special_vec = src->special_vec;\n\t}\n\tdst->nr_phys_segments = src->nr_phys_segments;\n\tdst->ioprio = src->ioprio;\n\tdst->extra_len = src->extra_len;\n}\n\n/**\n * blk_rq_prep_clone - Helper function to setup clone request\n * @rq: the request to be setup\n * @rq_src: original request to be cloned\n * @bs: bio_set that bios for clone are allocated from\n * @gfp_mask: memory allocation mask for bio\n * @bio_ctr: setup function to be called for each clone bio.\n *           Returns %0 for success, non %0 for failure.\n * @data: private data to be passed to @bio_ctr\n *\n * Description:\n *     Clones bios in @rq_src to @rq, and copies attributes of @rq_src to @rq.\n *     The actual data parts of @rq_src (e.g. ->cmd, ->sense)\n *     are not copied, and copying such parts is the caller's responsibility.\n *     Also, pages which the original bios are pointing to are not copied\n *     and the cloned bios just point same pages.\n *     So cloned bios must be completed before original bios, which means\n *     the caller must complete @rq before @rq_src.\n */\nint blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\t      struct bio_set *bs, gfp_t gfp_mask,\n\t\t      int (*bio_ctr)(struct bio *, struct bio *, void *),\n\t\t      void *data)\n{\n\tstruct bio *bio, *bio_src;\n\n\tif (!bs)\n\t\tbs = &fs_bio_set;\n\n\t__rq_for_each_bio(bio_src, rq_src) {\n\t\tbio = bio_clone_fast(bio_src, gfp_mask, bs);\n\t\tif (!bio)\n\t\t\tgoto free_and_out;\n\n\t\tif (bio_ctr && bio_ctr(bio, bio_src, data))\n\t\t\tgoto free_and_out;\n\n\t\tif (rq->bio) {\n\t\t\trq->biotail->bi_next = bio;\n\t\t\trq->biotail = bio;\n\t\t} else\n\t\t\trq->bio = rq->biotail = bio;\n\t}\n\n\t__blk_rq_prep_clone(rq, rq_src);\n\n\treturn 0;\n\nfree_and_out:\n\tif (bio)\n\t\tbio_put(bio);\n\tblk_rq_unprep_clone(rq);\n\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(blk_rq_prep_clone);\n\nint kblockd_schedule_work(struct work_struct *work)\n{\n\treturn queue_work(kblockd_workqueue, work);\n}\nEXPORT_SYMBOL(kblockd_schedule_work);\n\nint kblockd_schedule_work_on(int cpu, struct work_struct *work)\n{\n\treturn queue_work_on(cpu, kblockd_workqueue, work);\n}\nEXPORT_SYMBOL(kblockd_schedule_work_on);\n\nint kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork,\n\t\t\t\tunsigned long delay)\n{\n\treturn mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);\n}\nEXPORT_SYMBOL(kblockd_mod_delayed_work_on);\n\n/**\n * blk_start_plug - initialize blk_plug and track it inside the task_struct\n * @plug:\tThe &struct blk_plug that needs to be initialized\n *\n * Description:\n *   Tracking blk_plug inside the task_struct will help with auto-flushing the\n *   pending I/O should the task end up blocking between blk_start_plug() and\n *   blk_finish_plug(). This is important from a performance perspective, but\n *   also ensures that we don't deadlock. For instance, if the task is blocking\n *   for a memory allocation, memory reclaim could end up wanting to free a\n *   page belonging to that request that is currently residing in our private\n *   plug. By flushing the pending I/O when the process goes to sleep, we avoid\n *   this kind of deadlock.\n */\nvoid blk_start_plug(struct blk_plug *plug)\n{\n\tstruct task_struct *tsk = current;\n\n\t/*\n\t * If this is a nested plug, don't actually assign it.\n\t */\n\tif (tsk->plug)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&plug->list);\n\tINIT_LIST_HEAD(&plug->mq_list);\n\tINIT_LIST_HEAD(&plug->cb_list);\n\t/*\n\t * Store ordering should not be needed here, since a potential\n\t * preempt will imply a full memory barrier\n\t */\n\ttsk->plug = plug;\n}\nEXPORT_SYMBOL(blk_start_plug);\n\nstatic int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct request *rqa = container_of(a, struct request, queuelist);\n\tstruct request *rqb = container_of(b, struct request, queuelist);\n\n\treturn !(rqa->q < rqb->q ||\n\t\t(rqa->q == rqb->q && blk_rq_pos(rqa) < blk_rq_pos(rqb)));\n}\n\n/*\n * If 'from_schedule' is true, then postpone the dispatch of requests\n * until a safe kblockd context. We due this to avoid accidental big\n * additional stack usage in driver dispatch, in places where the originally\n * plugger did not intend it.\n */\nstatic void queue_unplugged(struct request_queue *q, unsigned int depth,\n\t\t\t    bool from_schedule)\n\t__releases(q->queue_lock)\n{\n\tlockdep_assert_held(q->queue_lock);\n\n\ttrace_block_unplug(q, depth, !from_schedule);\n\n\tif (from_schedule)\n\t\tblk_run_queue_async(q);\n\telse\n\t\t__blk_run_queue(q);\n\tspin_unlock_irq(q->queue_lock);\n}\n\nstatic void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)\n{\n\tLIST_HEAD(callbacks);\n\n\twhile (!list_empty(&plug->cb_list)) {\n\t\tlist_splice_init(&plug->cb_list, &callbacks);\n\n\t\twhile (!list_empty(&callbacks)) {\n\t\t\tstruct blk_plug_cb *cb = list_first_entry(&callbacks,\n\t\t\t\t\t\t\t  struct blk_plug_cb,\n\t\t\t\t\t\t\t  list);\n\t\t\tlist_del(&cb->list);\n\t\t\tcb->callback(cb, from_schedule);\n\t\t}\n\t}\n}\n\nstruct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,\n\t\t\t\t      int size)\n{\n\tstruct blk_plug *plug = current->plug;\n\tstruct blk_plug_cb *cb;\n\n\tif (!plug)\n\t\treturn NULL;\n\n\tlist_for_each_entry(cb, &plug->cb_list, list)\n\t\tif (cb->callback == unplug && cb->data == data)\n\t\t\treturn cb;\n\n\t/* Not currently on the callback list */\n\tBUG_ON(size < sizeof(*cb));\n\tcb = kzalloc(size, GFP_ATOMIC);\n\tif (cb) {\n\t\tcb->data = data;\n\t\tcb->callback = unplug;\n\t\tlist_add(&cb->list, &plug->cb_list);\n\t}\n\treturn cb;\n}\nEXPORT_SYMBOL(blk_check_plugged);\n\nvoid blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)\n{\n\tstruct request_queue *q;\n\tstruct request *rq;\n\tLIST_HEAD(list);\n\tunsigned int depth;\n\n\tflush_plug_callbacks(plug, from_schedule);\n\n\tif (!list_empty(&plug->mq_list))\n\t\tblk_mq_flush_plug_list(plug, from_schedule);\n\n\tif (list_empty(&plug->list))\n\t\treturn;\n\n\tlist_splice_init(&plug->list, &list);\n\n\tlist_sort(NULL, &list, plug_rq_cmp);\n\n\tq = NULL;\n\tdepth = 0;\n\n\twhile (!list_empty(&list)) {\n\t\trq = list_entry_rq(list.next);\n\t\tlist_del_init(&rq->queuelist);\n\t\tBUG_ON(!rq->q);\n\t\tif (rq->q != q) {\n\t\t\t/*\n\t\t\t * This drops the queue lock\n\t\t\t */\n\t\t\tif (q)\n\t\t\t\tqueue_unplugged(q, depth, from_schedule);\n\t\t\tq = rq->q;\n\t\t\tdepth = 0;\n\t\t\tspin_lock_irq(q->queue_lock);\n\t\t}\n\n\t\t/*\n\t\t * Short-circuit if @q is dead\n\t\t */\n\t\tif (unlikely(blk_queue_dying(q))) {\n\t\t\t__blk_end_request_all(rq, BLK_STS_IOERR);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * rq is already accounted, so use raw insert\n\t\t */\n\t\tif (op_is_flush(rq->cmd_flags))\n\t\t\t__elv_add_request(q, rq, ELEVATOR_INSERT_FLUSH);\n\t\telse\n\t\t\t__elv_add_request(q, rq, ELEVATOR_INSERT_SORT_MERGE);\n\n\t\tdepth++;\n\t}\n\n\t/*\n\t * This drops the queue lock\n\t */\n\tif (q)\n\t\tqueue_unplugged(q, depth, from_schedule);\n}\n\nvoid blk_finish_plug(struct blk_plug *plug)\n{\n\tif (plug != current->plug)\n\t\treturn;\n\tblk_flush_plug_list(plug, false);\n\n\tcurrent->plug = NULL;\n}\nEXPORT_SYMBOL(blk_finish_plug);\n\n#ifdef CONFIG_PM\n/**\n * blk_pm_runtime_init - Block layer runtime PM initialization routine\n * @q: the queue of the device\n * @dev: the device the queue belongs to\n *\n * Description:\n *    Initialize runtime-PM-related fields for @q and start auto suspend for\n *    @dev. Drivers that want to take advantage of request-based runtime PM\n *    should call this function after @dev has been initialized, and its\n *    request queue @q has been allocated, and runtime PM for it can not happen\n *    yet(either due to disabled/forbidden or its usage_count > 0). In most\n *    cases, driver should call this function before any I/O has taken place.\n *\n *    This function takes care of setting up using auto suspend for the device,\n *    the autosuspend delay is set to -1 to make runtime suspend impossible\n *    until an updated value is either set by user or by driver. Drivers do\n *    not need to touch other autosuspend settings.\n *\n *    The block layer runtime PM is request based, so only works for drivers\n *    that use request as their IO unit instead of those directly use bio's.\n */\nvoid blk_pm_runtime_init(struct request_queue *q, struct device *dev)\n{\n\t/* not support for RQF_PM and ->rpm_status in blk-mq yet */\n\tif (q->mq_ops)\n\t\treturn;\n\n\tq->dev = dev;\n\tq->rpm_status = RPM_ACTIVE;\n\tpm_runtime_set_autosuspend_delay(q->dev, -1);\n\tpm_runtime_use_autosuspend(q->dev);\n}\nEXPORT_SYMBOL(blk_pm_runtime_init);\n\n/**\n * blk_pre_runtime_suspend - Pre runtime suspend check\n * @q: the queue of the device\n *\n * Description:\n *    This function will check if runtime suspend is allowed for the device\n *    by examining if there are any requests pending in the queue. If there\n *    are requests pending, the device can not be runtime suspended; otherwise,\n *    the queue's status will be updated to SUSPENDING and the driver can\n *    proceed to suspend the device.\n *\n *    For the not allowed case, we mark last busy for the device so that\n *    runtime PM core will try to autosuspend it some time later.\n *\n *    This function should be called near the start of the device's\n *    runtime_suspend callback.\n *\n * Return:\n *    0\t\t- OK to runtime suspend the device\n *    -EBUSY\t- Device should not be runtime suspended\n */\nint blk_pre_runtime_suspend(struct request_queue *q)\n{\n\tint ret = 0;\n\n\tif (!q->dev)\n\t\treturn ret;\n\n\tspin_lock_irq(q->queue_lock);\n\tif (q->nr_pending) {\n\t\tret = -EBUSY;\n\t\tpm_runtime_mark_last_busy(q->dev);\n\t} else {\n\t\tq->rpm_status = RPM_SUSPENDING;\n\t}\n\tspin_unlock_irq(q->queue_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_pre_runtime_suspend);\n\n/**\n * blk_post_runtime_suspend - Post runtime suspend processing\n * @q: the queue of the device\n * @err: return value of the device's runtime_suspend function\n *\n * Description:\n *    Update the queue's runtime status according to the return value of the\n *    device's runtime suspend function and mark last busy for the device so\n *    that PM core will try to auto suspend the device at a later time.\n *\n *    This function should be called near the end of the device's\n *    runtime_suspend callback.\n */\nvoid blk_post_runtime_suspend(struct request_queue *q, int err)\n{\n\tif (!q->dev)\n\t\treturn;\n\n\tspin_lock_irq(q->queue_lock);\n\tif (!err) {\n\t\tq->rpm_status = RPM_SUSPENDED;\n\t} else {\n\t\tq->rpm_status = RPM_ACTIVE;\n\t\tpm_runtime_mark_last_busy(q->dev);\n\t}\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_post_runtime_suspend);\n\n/**\n * blk_pre_runtime_resume - Pre runtime resume processing\n * @q: the queue of the device\n *\n * Description:\n *    Update the queue's runtime status to RESUMING in preparation for the\n *    runtime resume of the device.\n *\n *    This function should be called near the start of the device's\n *    runtime_resume callback.\n */\nvoid blk_pre_runtime_resume(struct request_queue *q)\n{\n\tif (!q->dev)\n\t\treturn;\n\n\tspin_lock_irq(q->queue_lock);\n\tq->rpm_status = RPM_RESUMING;\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_pre_runtime_resume);\n\n/**\n * blk_post_runtime_resume - Post runtime resume processing\n * @q: the queue of the device\n * @err: return value of the device's runtime_resume function\n *\n * Description:\n *    Update the queue's runtime status according to the return value of the\n *    device's runtime_resume function. If it is successfully resumed, process\n *    the requests that are queued into the device's queue when it is resuming\n *    and then mark last busy and initiate autosuspend for it.\n *\n *    This function should be called near the end of the device's\n *    runtime_resume callback.\n */\nvoid blk_post_runtime_resume(struct request_queue *q, int err)\n{\n\tif (!q->dev)\n\t\treturn;\n\n\tspin_lock_irq(q->queue_lock);\n\tif (!err) {\n\t\tq->rpm_status = RPM_ACTIVE;\n\t\t__blk_run_queue(q);\n\t\tpm_runtime_mark_last_busy(q->dev);\n\t\tpm_request_autosuspend(q->dev);\n\t} else {\n\t\tq->rpm_status = RPM_SUSPENDED;\n\t}\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_post_runtime_resume);\n\n/**\n * blk_set_runtime_active - Force runtime status of the queue to be active\n * @q: the queue of the device\n *\n * If the device is left runtime suspended during system suspend the resume\n * hook typically resumes the device and corrects runtime status\n * accordingly. However, that does not affect the queue runtime PM status\n * which is still \"suspended\". This prevents processing requests from the\n * queue.\n *\n * This function can be used in driver's resume hook to correct queue\n * runtime PM status and re-enable peeking requests from the queue. It\n * should be called before first request is added to the queue.\n */\nvoid blk_set_runtime_active(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\tq->rpm_status = RPM_ACTIVE;\n\tpm_runtime_mark_last_busy(q->dev);\n\tpm_request_autosuspend(q->dev);\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_set_runtime_active);\n#endif\n\nint __init blk_dev_init(void)\n{\n\tBUILD_BUG_ON(REQ_OP_LAST >= (1 << REQ_OP_BITS));\n\tBUILD_BUG_ON(REQ_OP_BITS + REQ_FLAG_BITS > 8 *\n\t\t\tFIELD_SIZEOF(struct request, cmd_flags));\n\tBUILD_BUG_ON(REQ_OP_BITS + REQ_FLAG_BITS > 8 *\n\t\t\tFIELD_SIZEOF(struct bio, bi_opf));\n\n\t/* used for unplugging and affects IO latency/throughput - HIGHPRI */\n\tkblockd_workqueue = alloc_workqueue(\"kblockd\",\n\t\t\t\t\t    WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);\n\tif (!kblockd_workqueue)\n\t\tpanic(\"Failed to create kblockd\\n\");\n\n\trequest_cachep = kmem_cache_create(\"blkdev_requests\",\n\t\t\tsizeof(struct request), 0, SLAB_PANIC, NULL);\n\n\tblk_requestq_cachep = kmem_cache_create(\"request_queue\",\n\t\t\tsizeof(struct request_queue), 0, SLAB_PANIC, NULL);\n\n#ifdef CONFIG_DEBUG_FS\n\tblk_debugfs_root = debugfs_create_dir(\"block\", NULL);\n#endif\n\n\treturn 0;\n}\n"], "fixing_code": ["/*\n * Copyright (C) 1991, 1992 Linus Torvalds\n * Copyright (C) 1994,      Karl Keyte: Added support for disk statistics\n * Elevator latency, (C) 2000  Andrea Arcangeli <andrea@suse.de> SuSE\n * Queue request tables / lock, selectable elevator, Jens Axboe <axboe@suse.de>\n * kernel-doc documentation started by NeilBrown <neilb@cse.unsw.edu.au>\n *\t-  July2000\n * bio rewrite, highmem i/o, etc, Jens Axboe <axboe@suse.de> - may 2001\n */\n\n/*\n * This handles all read/write requests to block devices\n */\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/backing-dev.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-mq.h>\n#include <linux/highmem.h>\n#include <linux/mm.h>\n#include <linux/kernel_stat.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/completion.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/fault-inject.h>\n#include <linux/list_sort.h>\n#include <linux/delay.h>\n#include <linux/ratelimit.h>\n#include <linux/pm_runtime.h>\n#include <linux/blk-cgroup.h>\n#include <linux/debugfs.h>\n#include <linux/bpf.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/block.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-rq-qos.h\"\n\n#ifdef CONFIG_DEBUG_FS\nstruct dentry *blk_debugfs_root;\n#endif\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_remap);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_rq_remap);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_complete);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_split);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_unplug);\n\nDEFINE_IDA(blk_queue_ida);\n\n/*\n * For the allocated request tables\n */\nstruct kmem_cache *request_cachep;\n\n/*\n * For queue allocation\n */\nstruct kmem_cache *blk_requestq_cachep;\n\n/*\n * Controlling structure to kblockd\n */\nstatic struct workqueue_struct *kblockd_workqueue;\n\n/**\n * blk_queue_flag_set - atomically set a queue flag\n * @flag: flag to be set\n * @q: request queue\n */\nvoid blk_queue_flag_set(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tqueue_flag_set(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n}\nEXPORT_SYMBOL(blk_queue_flag_set);\n\n/**\n * blk_queue_flag_clear - atomically clear a queue flag\n * @flag: flag to be cleared\n * @q: request queue\n */\nvoid blk_queue_flag_clear(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tqueue_flag_clear(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n}\nEXPORT_SYMBOL(blk_queue_flag_clear);\n\n/**\n * blk_queue_flag_test_and_set - atomically test and set a queue flag\n * @flag: flag to be set\n * @q: request queue\n *\n * Returns the previous value of @flag - 0 if the flag was not set and 1 if\n * the flag was already set.\n */\nbool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tres = queue_flag_test_and_set(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(blk_queue_flag_test_and_set);\n\n/**\n * blk_queue_flag_test_and_clear - atomically test and clear a queue flag\n * @flag: flag to be cleared\n * @q: request queue\n *\n * Returns the previous value of @flag - 0 if the flag was not set and 1 if\n * the flag was set.\n */\nbool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q)\n{\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tres = queue_flag_test_and_clear(flag, q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(blk_queue_flag_test_and_clear);\n\nstatic void blk_clear_congested(struct request_list *rl, int sync)\n{\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tclear_wb_congested(rl->blkg->wb_congested, sync);\n#else\n\t/*\n\t * If !CGROUP_WRITEBACK, all blkg's map to bdi->wb and we shouldn't\n\t * flip its congestion state for events on other blkcgs.\n\t */\n\tif (rl == &rl->q->root_rl)\n\t\tclear_wb_congested(rl->q->backing_dev_info->wb.congested, sync);\n#endif\n}\n\nstatic void blk_set_congested(struct request_list *rl, int sync)\n{\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tset_wb_congested(rl->blkg->wb_congested, sync);\n#else\n\t/* see blk_clear_congested() */\n\tif (rl == &rl->q->root_rl)\n\t\tset_wb_congested(rl->q->backing_dev_info->wb.congested, sync);\n#endif\n}\n\nvoid blk_queue_congestion_threshold(struct request_queue *q)\n{\n\tint nr;\n\n\tnr = q->nr_requests - (q->nr_requests / 8) + 1;\n\tif (nr > q->nr_requests)\n\t\tnr = q->nr_requests;\n\tq->nr_congestion_on = nr;\n\n\tnr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;\n\tif (nr < 1)\n\t\tnr = 1;\n\tq->nr_congestion_off = nr;\n}\n\nvoid blk_rq_init(struct request_queue *q, struct request *rq)\n{\n\tmemset(rq, 0, sizeof(*rq));\n\n\tINIT_LIST_HEAD(&rq->queuelist);\n\tINIT_LIST_HEAD(&rq->timeout_list);\n\trq->cpu = -1;\n\trq->q = q;\n\trq->__sector = (sector_t) -1;\n\tINIT_HLIST_NODE(&rq->hash);\n\tRB_CLEAR_NODE(&rq->rb_node);\n\trq->tag = -1;\n\trq->internal_tag = -1;\n\trq->start_time_ns = ktime_get_ns();\n\trq->part = NULL;\n}\nEXPORT_SYMBOL(blk_rq_init);\n\nstatic const struct {\n\tint\t\terrno;\n\tconst char\t*name;\n} blk_errors[] = {\n\t[BLK_STS_OK]\t\t= { 0,\t\t\"\" },\n\t[BLK_STS_NOTSUPP]\t= { -EOPNOTSUPP, \"operation not supported\" },\n\t[BLK_STS_TIMEOUT]\t= { -ETIMEDOUT,\t\"timeout\" },\n\t[BLK_STS_NOSPC]\t\t= { -ENOSPC,\t\"critical space allocation\" },\n\t[BLK_STS_TRANSPORT]\t= { -ENOLINK,\t\"recoverable transport\" },\n\t[BLK_STS_TARGET]\t= { -EREMOTEIO,\t\"critical target\" },\n\t[BLK_STS_NEXUS]\t\t= { -EBADE,\t\"critical nexus\" },\n\t[BLK_STS_MEDIUM]\t= { -ENODATA,\t\"critical medium\" },\n\t[BLK_STS_PROTECTION]\t= { -EILSEQ,\t\"protection\" },\n\t[BLK_STS_RESOURCE]\t= { -ENOMEM,\t\"kernel resource\" },\n\t[BLK_STS_DEV_RESOURCE]\t= { -EBUSY,\t\"device resource\" },\n\t[BLK_STS_AGAIN]\t\t= { -EAGAIN,\t\"nonblocking retry\" },\n\n\t/* device mapper special case, should not leak out: */\n\t[BLK_STS_DM_REQUEUE]\t= { -EREMCHG, \"dm internal retry\" },\n\n\t/* everything else not covered above: */\n\t[BLK_STS_IOERR]\t\t= { -EIO,\t\"I/O\" },\n};\n\nblk_status_t errno_to_blk_status(int errno)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(blk_errors); i++) {\n\t\tif (blk_errors[i].errno == errno)\n\t\t\treturn (__force blk_status_t)i;\n\t}\n\n\treturn BLK_STS_IOERR;\n}\nEXPORT_SYMBOL_GPL(errno_to_blk_status);\n\nint blk_status_to_errno(blk_status_t status)\n{\n\tint idx = (__force int)status;\n\n\tif (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))\n\t\treturn -EIO;\n\treturn blk_errors[idx].errno;\n}\nEXPORT_SYMBOL_GPL(blk_status_to_errno);\n\nstatic void print_req_error(struct request *req, blk_status_t status)\n{\n\tint idx = (__force int)status;\n\n\tif (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))\n\t\treturn;\n\n\tprintk_ratelimited(KERN_ERR \"%s: %s error, dev %s, sector %llu\\n\",\n\t\t\t   __func__, blk_errors[idx].name, req->rq_disk ?\n\t\t\t   req->rq_disk->disk_name : \"?\",\n\t\t\t   (unsigned long long)blk_rq_pos(req));\n}\n\nstatic void req_bio_endio(struct request *rq, struct bio *bio,\n\t\t\t  unsigned int nbytes, blk_status_t error)\n{\n\tif (error)\n\t\tbio->bi_status = error;\n\n\tif (unlikely(rq->rq_flags & RQF_QUIET))\n\t\tbio_set_flag(bio, BIO_QUIET);\n\n\tbio_advance(bio, nbytes);\n\n\t/* don't actually finish bio if it's part of flush sequence */\n\tif (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))\n\t\tbio_endio(bio);\n}\n\nvoid blk_dump_rq_flags(struct request *rq, char *msg)\n{\n\tprintk(KERN_INFO \"%s: dev %s: flags=%llx\\n\", msg,\n\t\trq->rq_disk ? rq->rq_disk->disk_name : \"?\",\n\t\t(unsigned long long) rq->cmd_flags);\n\n\tprintk(KERN_INFO \"  sector %llu, nr/cnr %u/%u\\n\",\n\t       (unsigned long long)blk_rq_pos(rq),\n\t       blk_rq_sectors(rq), blk_rq_cur_sectors(rq));\n\tprintk(KERN_INFO \"  bio %p, biotail %p, len %u\\n\",\n\t       rq->bio, rq->biotail, blk_rq_bytes(rq));\n}\nEXPORT_SYMBOL(blk_dump_rq_flags);\n\nstatic void blk_delay_work(struct work_struct *work)\n{\n\tstruct request_queue *q;\n\n\tq = container_of(work, struct request_queue, delay_work.work);\n\tspin_lock_irq(q->queue_lock);\n\t__blk_run_queue(q);\n\tspin_unlock_irq(q->queue_lock);\n}\n\n/**\n * blk_delay_queue - restart queueing after defined interval\n * @q:\t\tThe &struct request_queue in question\n * @msecs:\tDelay in msecs\n *\n * Description:\n *   Sometimes queueing needs to be postponed for a little while, to allow\n *   resources to come back. This function will make sure that queueing is\n *   restarted around the specified time.\n */\nvoid blk_delay_queue(struct request_queue *q, unsigned long msecs)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (likely(!blk_queue_dead(q)))\n\t\tqueue_delayed_work(kblockd_workqueue, &q->delay_work,\n\t\t\t\t   msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_delay_queue);\n\n/**\n * blk_start_queue_async - asynchronously restart a previously stopped queue\n * @q:    The &struct request_queue in question\n *\n * Description:\n *   blk_start_queue_async() will clear the stop flag on the queue, and\n *   ensure that the request_fn for the queue is run from an async\n *   context.\n **/\nvoid blk_start_queue_async(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tqueue_flag_clear(QUEUE_FLAG_STOPPED, q);\n\tblk_run_queue_async(q);\n}\nEXPORT_SYMBOL(blk_start_queue_async);\n\n/**\n * blk_start_queue - restart a previously stopped queue\n * @q:    The &struct request_queue in question\n *\n * Description:\n *   blk_start_queue() will clear the stop flag on the queue, and call\n *   the request_fn for the queue if it was in a stopped state when\n *   entered. Also see blk_stop_queue().\n **/\nvoid blk_start_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tqueue_flag_clear(QUEUE_FLAG_STOPPED, q);\n\t__blk_run_queue(q);\n}\nEXPORT_SYMBOL(blk_start_queue);\n\n/**\n * blk_stop_queue - stop a queue\n * @q:    The &struct request_queue in question\n *\n * Description:\n *   The Linux block layer assumes that a block driver will consume all\n *   entries on the request queue when the request_fn strategy is called.\n *   Often this will not happen, because of hardware limitations (queue\n *   depth settings). If a device driver gets a 'queue full' response,\n *   or if it simply chooses not to queue more I/O at one point, it can\n *   call this function to prevent the request_fn from being called until\n *   the driver has signalled it's ready to go again. This happens by calling\n *   blk_start_queue() to restart queue operations.\n **/\nvoid blk_stop_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tcancel_delayed_work(&q->delay_work);\n\tqueue_flag_set(QUEUE_FLAG_STOPPED, q);\n}\nEXPORT_SYMBOL(blk_stop_queue);\n\n/**\n * blk_sync_queue - cancel any pending callbacks on a queue\n * @q: the queue\n *\n * Description:\n *     The block layer may perform asynchronous callback activity\n *     on a queue, such as calling the unplug function after a timeout.\n *     A block device may call blk_sync_queue to ensure that any\n *     such activity is cancelled, thus allowing it to release resources\n *     that the callbacks might use. The caller must already have made sure\n *     that its ->make_request_fn will not re-add plugging prior to calling\n *     this function.\n *\n *     This function does not cancel any asynchronous activity arising\n *     out of elevator or throttling code. That would require elevator_exit()\n *     and blkcg_exit_queue() to be called with queue lock initialized.\n *\n */\nvoid blk_sync_queue(struct request_queue *q)\n{\n\tdel_timer_sync(&q->timeout);\n\tcancel_work_sync(&q->timeout_work);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\t\tint i;\n\n\t\tcancel_delayed_work_sync(&q->requeue_work);\n\t\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\t\tcancel_delayed_work_sync(&hctx->run_work);\n\t} else {\n\t\tcancel_delayed_work_sync(&q->delay_work);\n\t}\n}\nEXPORT_SYMBOL(blk_sync_queue);\n\n/**\n * blk_set_preempt_only - set QUEUE_FLAG_PREEMPT_ONLY\n * @q: request queue pointer\n *\n * Returns the previous value of the PREEMPT_ONLY flag - 0 if the flag was not\n * set and 1 if the flag was already set.\n */\nint blk_set_preempt_only(struct request_queue *q)\n{\n\treturn blk_queue_flag_test_and_set(QUEUE_FLAG_PREEMPT_ONLY, q);\n}\nEXPORT_SYMBOL_GPL(blk_set_preempt_only);\n\nvoid blk_clear_preempt_only(struct request_queue *q)\n{\n\tblk_queue_flag_clear(QUEUE_FLAG_PREEMPT_ONLY, q);\n\twake_up_all(&q->mq_freeze_wq);\n}\nEXPORT_SYMBOL_GPL(blk_clear_preempt_only);\n\n/**\n * __blk_run_queue_uncond - run a queue whether or not it has been stopped\n * @q:\tThe queue to run\n *\n * Description:\n *    Invoke request handling on a queue if there are any pending requests.\n *    May be used to restart request handling after a request has completed.\n *    This variant runs the queue whether or not the queue has been\n *    stopped. Must be called with the queue lock held and interrupts\n *    disabled. See also @blk_run_queue.\n */\ninline void __blk_run_queue_uncond(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (unlikely(blk_queue_dead(q)))\n\t\treturn;\n\n\t/*\n\t * Some request_fn implementations, e.g. scsi_request_fn(), unlock\n\t * the queue lock internally. As a result multiple threads may be\n\t * running such a request function concurrently. Keep track of the\n\t * number of active request_fn invocations such that blk_drain_queue()\n\t * can wait until all these request_fn calls have finished.\n\t */\n\tq->request_fn_active++;\n\tq->request_fn(q);\n\tq->request_fn_active--;\n}\nEXPORT_SYMBOL_GPL(__blk_run_queue_uncond);\n\n/**\n * __blk_run_queue - run a single device queue\n * @q:\tThe queue to run\n *\n * Description:\n *    See @blk_run_queue.\n */\nvoid __blk_run_queue(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (unlikely(blk_queue_stopped(q)))\n\t\treturn;\n\n\t__blk_run_queue_uncond(q);\n}\nEXPORT_SYMBOL(__blk_run_queue);\n\n/**\n * blk_run_queue_async - run a single device queue in workqueue context\n * @q:\tThe queue to run\n *\n * Description:\n *    Tells kblockd to perform the equivalent of @blk_run_queue on behalf\n *    of us.\n *\n * Note:\n *    Since it is not allowed to run q->delay_work after blk_cleanup_queue()\n *    has canceled q->delay_work, callers must hold the queue lock to avoid\n *    race conditions between blk_cleanup_queue() and blk_run_queue_async().\n */\nvoid blk_run_queue_async(struct request_queue *q)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (likely(!blk_queue_stopped(q) && !blk_queue_dead(q)))\n\t\tmod_delayed_work(kblockd_workqueue, &q->delay_work, 0);\n}\nEXPORT_SYMBOL(blk_run_queue_async);\n\n/**\n * blk_run_queue - run a single device queue\n * @q: The queue to run\n *\n * Description:\n *    Invoke request handling on this queue, if it has pending work to do.\n *    May be used to restart queueing when a request has completed.\n */\nvoid blk_run_queue(struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\t__blk_run_queue(q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n}\nEXPORT_SYMBOL(blk_run_queue);\n\nvoid blk_put_queue(struct request_queue *q)\n{\n\tkobject_put(&q->kobj);\n}\nEXPORT_SYMBOL(blk_put_queue);\n\n/**\n * __blk_drain_queue - drain requests from request_queue\n * @q: queue to drain\n * @drain_all: whether to drain all requests or only the ones w/ ELVPRIV\n *\n * Drain requests from @q.  If @drain_all is set, all requests are drained.\n * If not, only ELVPRIV requests are drained.  The caller is responsible\n * for ensuring that no new requests which need to be drained are queued.\n */\nstatic void __blk_drain_queue(struct request_queue *q, bool drain_all)\n\t__releases(q->queue_lock)\n\t__acquires(q->queue_lock)\n{\n\tint i;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\twhile (true) {\n\t\tbool drain = false;\n\n\t\t/*\n\t\t * The caller might be trying to drain @q before its\n\t\t * elevator is initialized.\n\t\t */\n\t\tif (q->elevator)\n\t\t\telv_drain_elevator(q);\n\n\t\tblkcg_drain_queue(q);\n\n\t\t/*\n\t\t * This function might be called on a queue which failed\n\t\t * driver init after queue creation or is not yet fully\n\t\t * active yet.  Some drivers (e.g. fd and loop) get unhappy\n\t\t * in such cases.  Kick queue iff dispatch queue has\n\t\t * something on it and @q has request_fn set.\n\t\t */\n\t\tif (!list_empty(&q->queue_head) && q->request_fn)\n\t\t\t__blk_run_queue(q);\n\n\t\tdrain |= q->nr_rqs_elvpriv;\n\t\tdrain |= q->request_fn_active;\n\n\t\t/*\n\t\t * Unfortunately, requests are queued at and tracked from\n\t\t * multiple places and there's no single counter which can\n\t\t * be drained.  Check all the queues and counters.\n\t\t */\n\t\tif (drain_all) {\n\t\t\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\t\t\tdrain |= !list_empty(&q->queue_head);\n\t\t\tfor (i = 0; i < 2; i++) {\n\t\t\t\tdrain |= q->nr_rqs[i];\n\t\t\t\tdrain |= q->in_flight[i];\n\t\t\t\tif (fq)\n\t\t\t\t    drain |= !list_empty(&fq->flush_queue[i]);\n\t\t\t}\n\t\t}\n\n\t\tif (!drain)\n\t\t\tbreak;\n\n\t\tspin_unlock_irq(q->queue_lock);\n\n\t\tmsleep(10);\n\n\t\tspin_lock_irq(q->queue_lock);\n\t}\n\n\t/*\n\t * With queue marked dead, any woken up waiter will fail the\n\t * allocation path, so the wakeup chaining is lost and we're\n\t * left with hung waiters. We need to wake up those waiters.\n\t */\n\tif (q->request_fn) {\n\t\tstruct request_list *rl;\n\n\t\tblk_queue_for_each_rl(rl, q)\n\t\t\tfor (i = 0; i < ARRAY_SIZE(rl->wait); i++)\n\t\t\t\twake_up_all(&rl->wait[i]);\n\t}\n}\n\nvoid blk_drain_queue(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\t__blk_drain_queue(q, true);\n\tspin_unlock_irq(q->queue_lock);\n}\n\n/**\n * blk_queue_bypass_start - enter queue bypass mode\n * @q: queue of interest\n *\n * In bypass mode, only the dispatch FIFO queue of @q is used.  This\n * function makes @q enter bypass mode and drains all requests which were\n * throttled or issued before.  On return, it's guaranteed that no request\n * is being throttled or has ELVPRIV set and blk_queue_bypass() %true\n * inside queue or RCU read lock.\n */\nvoid blk_queue_bypass_start(struct request_queue *q)\n{\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tspin_lock_irq(q->queue_lock);\n\tq->bypass_depth++;\n\tqueue_flag_set(QUEUE_FLAG_BYPASS, q);\n\tspin_unlock_irq(q->queue_lock);\n\n\t/*\n\t * Queues start drained.  Skip actual draining till init is\n\t * complete.  This avoids lenghty delays during queue init which\n\t * can happen many times during boot.\n\t */\n\tif (blk_queue_init_done(q)) {\n\t\tspin_lock_irq(q->queue_lock);\n\t\t__blk_drain_queue(q, false);\n\t\tspin_unlock_irq(q->queue_lock);\n\n\t\t/* ensure blk_queue_bypass() is %true inside RCU read lock */\n\t\tsynchronize_rcu();\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_queue_bypass_start);\n\n/**\n * blk_queue_bypass_end - leave queue bypass mode\n * @q: queue of interest\n *\n * Leave bypass mode and restore the normal queueing behavior.\n *\n * Note: although blk_queue_bypass_start() is only called for blk-sq queues,\n * this function is called for both blk-sq and blk-mq queues.\n */\nvoid blk_queue_bypass_end(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\tif (!--q->bypass_depth)\n\t\tqueue_flag_clear(QUEUE_FLAG_BYPASS, q);\n\tWARN_ON_ONCE(q->bypass_depth < 0);\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL_GPL(blk_queue_bypass_end);\n\nvoid blk_set_queue_dying(struct request_queue *q)\n{\n\tblk_queue_flag_set(QUEUE_FLAG_DYING, q);\n\n\t/*\n\t * When queue DYING flag is set, we need to block new req\n\t * entering queue, so we call blk_freeze_queue_start() to\n\t * prevent I/O from crossing blk_queue_enter().\n\t */\n\tblk_freeze_queue_start(q);\n\n\tif (q->mq_ops)\n\t\tblk_mq_wake_waiters(q);\n\telse {\n\t\tstruct request_list *rl;\n\n\t\tspin_lock_irq(q->queue_lock);\n\t\tblk_queue_for_each_rl(rl, q) {\n\t\t\tif (rl->rq_pool) {\n\t\t\t\twake_up_all(&rl->wait[BLK_RW_SYNC]);\n\t\t\t\twake_up_all(&rl->wait[BLK_RW_ASYNC]);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(q->queue_lock);\n\t}\n\n\t/* Make blk_queue_enter() reexamine the DYING flag. */\n\twake_up_all(&q->mq_freeze_wq);\n}\nEXPORT_SYMBOL_GPL(blk_set_queue_dying);\n\n/**\n * blk_cleanup_queue - shutdown a request queue\n * @q: request queue to shutdown\n *\n * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and\n * put it.  All future requests will be failed immediately with -ENODEV.\n */\nvoid blk_cleanup_queue(struct request_queue *q)\n{\n\tspinlock_t *lock = q->queue_lock;\n\n\t/* mark @q DYING, no new request or merges will be allowed afterwards */\n\tmutex_lock(&q->sysfs_lock);\n\tblk_set_queue_dying(q);\n\tspin_lock_irq(lock);\n\n\t/*\n\t * A dying queue is permanently in bypass mode till released.  Note\n\t * that, unlike blk_queue_bypass_start(), we aren't performing\n\t * synchronize_rcu() after entering bypass mode to avoid the delay\n\t * as some drivers create and destroy a lot of queues while\n\t * probing.  This is still safe because blk_release_queue() will be\n\t * called only after the queue refcnt drops to zero and nothing,\n\t * RCU or not, would be traversing the queue by then.\n\t */\n\tq->bypass_depth++;\n\tqueue_flag_set(QUEUE_FLAG_BYPASS, q);\n\n\tqueue_flag_set(QUEUE_FLAG_NOMERGES, q);\n\tqueue_flag_set(QUEUE_FLAG_NOXMERGES, q);\n\tqueue_flag_set(QUEUE_FLAG_DYING, q);\n\tspin_unlock_irq(lock);\n\tmutex_unlock(&q->sysfs_lock);\n\n\t/*\n\t * Drain all requests queued before DYING marking. Set DEAD flag to\n\t * prevent that q->request_fn() gets invoked after draining finished.\n\t */\n\tblk_freeze_queue(q);\n\tspin_lock_irq(lock);\n\tqueue_flag_set(QUEUE_FLAG_DEAD, q);\n\tspin_unlock_irq(lock);\n\n\t/*\n\t * make sure all in-progress dispatch are completed because\n\t * blk_freeze_queue() can only complete all requests, and\n\t * dispatch may still be in-progress since we dispatch requests\n\t * from more than one contexts.\n\t *\n\t * No need to quiesce queue if it isn't initialized yet since\n\t * blk_freeze_queue() should be enough for cases of passthrough\n\t * request.\n\t */\n\tif (q->mq_ops && blk_queue_init_done(q))\n\t\tblk_mq_quiesce_queue(q);\n\n\t/* for synchronous bio-based driver finish in-flight integrity i/o */\n\tblk_flush_integrity();\n\n\t/* @q won't process any more request, flush async actions */\n\tdel_timer_sync(&q->backing_dev_info->laptop_mode_wb_timer);\n\tblk_sync_queue(q);\n\n\t/*\n\t * I/O scheduler exit is only safe after the sysfs scheduler attribute\n\t * has been removed.\n\t */\n\tWARN_ON_ONCE(q->kobj.state_in_sysfs);\n\n\t/*\n\t * Since the I/O scheduler exit code may access cgroup information,\n\t * perform I/O scheduler exit before disassociating from the block\n\t * cgroup controller.\n\t */\n\tif (q->elevator) {\n\t\tioc_clear_queue(q);\n\t\televator_exit(q, q->elevator);\n\t\tq->elevator = NULL;\n\t}\n\n\t/*\n\t * Remove all references to @q from the block cgroup controller before\n\t * restoring @q->queue_lock to avoid that restoring this pointer causes\n\t * e.g. blkcg_print_blkgs() to crash.\n\t */\n\tblkcg_exit_queue(q);\n\n\t/*\n\t * Since the cgroup code may dereference the @q->backing_dev_info\n\t * pointer, only decrease its reference count after having removed the\n\t * association with the block cgroup controller.\n\t */\n\tbdi_put(q->backing_dev_info);\n\n\tif (q->mq_ops)\n\t\tblk_mq_free_queue(q);\n\tpercpu_ref_exit(&q->q_usage_counter);\n\n\tspin_lock_irq(lock);\n\tif (q->queue_lock != &q->__queue_lock)\n\t\tq->queue_lock = &q->__queue_lock;\n\tspin_unlock_irq(lock);\n\n\t/* @q is and will stay empty, shutdown and put */\n\tblk_put_queue(q);\n}\nEXPORT_SYMBOL(blk_cleanup_queue);\n\n/* Allocate memory local to the request queue */\nstatic void *alloc_request_simple(gfp_t gfp_mask, void *data)\n{\n\tstruct request_queue *q = data;\n\n\treturn kmem_cache_alloc_node(request_cachep, gfp_mask, q->node);\n}\n\nstatic void free_request_simple(void *element, void *data)\n{\n\tkmem_cache_free(request_cachep, element);\n}\n\nstatic void *alloc_request_size(gfp_t gfp_mask, void *data)\n{\n\tstruct request_queue *q = data;\n\tstruct request *rq;\n\n\trq = kmalloc_node(sizeof(struct request) + q->cmd_size, gfp_mask,\n\t\t\tq->node);\n\tif (rq && q->init_rq_fn && q->init_rq_fn(q, rq, gfp_mask) < 0) {\n\t\tkfree(rq);\n\t\trq = NULL;\n\t}\n\treturn rq;\n}\n\nstatic void free_request_size(void *element, void *data)\n{\n\tstruct request_queue *q = data;\n\n\tif (q->exit_rq_fn)\n\t\tq->exit_rq_fn(q, element);\n\tkfree(element);\n}\n\nint blk_init_rl(struct request_list *rl, struct request_queue *q,\n\t\tgfp_t gfp_mask)\n{\n\tif (unlikely(rl->rq_pool) || q->mq_ops)\n\t\treturn 0;\n\n\trl->q = q;\n\trl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;\n\trl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;\n\tinit_waitqueue_head(&rl->wait[BLK_RW_SYNC]);\n\tinit_waitqueue_head(&rl->wait[BLK_RW_ASYNC]);\n\n\tif (q->cmd_size) {\n\t\trl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ,\n\t\t\t\talloc_request_size, free_request_size,\n\t\t\t\tq, gfp_mask, q->node);\n\t} else {\n\t\trl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ,\n\t\t\t\talloc_request_simple, free_request_simple,\n\t\t\t\tq, gfp_mask, q->node);\n\t}\n\tif (!rl->rq_pool)\n\t\treturn -ENOMEM;\n\n\tif (rl != &q->root_rl)\n\t\tWARN_ON_ONCE(!blk_get_queue(q));\n\n\treturn 0;\n}\n\nvoid blk_exit_rl(struct request_queue *q, struct request_list *rl)\n{\n\tif (rl->rq_pool) {\n\t\tmempool_destroy(rl->rq_pool);\n\t\tif (rl != &q->root_rl)\n\t\t\tblk_put_queue(q);\n\t}\n}\n\nstruct request_queue *blk_alloc_queue(gfp_t gfp_mask)\n{\n\treturn blk_alloc_queue_node(gfp_mask, NUMA_NO_NODE, NULL);\n}\nEXPORT_SYMBOL(blk_alloc_queue);\n\n/**\n * blk_queue_enter() - try to increase q->q_usage_counter\n * @q: request queue pointer\n * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT\n */\nint blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)\n{\n\tconst bool preempt = flags & BLK_MQ_REQ_PREEMPT;\n\n\twhile (true) {\n\t\tbool success = false;\n\n\t\trcu_read_lock();\n\t\tif (percpu_ref_tryget_live(&q->q_usage_counter)) {\n\t\t\t/*\n\t\t\t * The code that sets the PREEMPT_ONLY flag is\n\t\t\t * responsible for ensuring that that flag is globally\n\t\t\t * visible before the queue is unfrozen.\n\t\t\t */\n\t\t\tif (preempt || !blk_queue_preempt_only(q)) {\n\t\t\t\tsuccess = true;\n\t\t\t} else {\n\t\t\t\tpercpu_ref_put(&q->q_usage_counter);\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (success)\n\t\t\treturn 0;\n\n\t\tif (flags & BLK_MQ_REQ_NOWAIT)\n\t\t\treturn -EBUSY;\n\n\t\t/*\n\t\t * read pair of barrier in blk_freeze_queue_start(),\n\t\t * we need to order reading __PERCPU_REF_DEAD flag of\n\t\t * .q_usage_counter and reading .mq_freeze_depth or\n\t\t * queue dying flag, otherwise the following wait may\n\t\t * never return if the two reads are reordered.\n\t\t */\n\t\tsmp_rmb();\n\n\t\twait_event(q->mq_freeze_wq,\n\t\t\t   (atomic_read(&q->mq_freeze_depth) == 0 &&\n\t\t\t    (preempt || !blk_queue_preempt_only(q))) ||\n\t\t\t   blk_queue_dying(q));\n\t\tif (blk_queue_dying(q))\n\t\t\treturn -ENODEV;\n\t}\n}\n\nvoid blk_queue_exit(struct request_queue *q)\n{\n\tpercpu_ref_put(&q->q_usage_counter);\n}\n\nstatic void blk_queue_usage_counter_release(struct percpu_ref *ref)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(ref, struct request_queue, q_usage_counter);\n\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nstatic void blk_rq_timed_out_timer(struct timer_list *t)\n{\n\tstruct request_queue *q = from_timer(q, t, timeout);\n\n\tkblockd_schedule_work(&q->timeout_work);\n}\n\n/**\n * blk_alloc_queue_node - allocate a request queue\n * @gfp_mask: memory allocation flags\n * @node_id: NUMA node to allocate memory from\n * @lock: For legacy queues, pointer to a spinlock that will be used to e.g.\n *        serialize calls to the legacy .request_fn() callback. Ignored for\n *\t  blk-mq request queues.\n *\n * Note: pass the queue lock as the third argument to this function instead of\n * setting the queue lock pointer explicitly to avoid triggering a sporadic\n * crash in the blkcg code. This function namely calls blkcg_init_queue() and\n * the queue lock pointer must be set before blkcg_init_queue() is called.\n */\nstruct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,\n\t\t\t\t\t   spinlock_t *lock)\n{\n\tstruct request_queue *q;\n\tint ret;\n\n\tq = kmem_cache_alloc_node(blk_requestq_cachep,\n\t\t\t\tgfp_mask | __GFP_ZERO, node_id);\n\tif (!q)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&q->queue_head);\n\tq->last_merge = NULL;\n\tq->end_sector = 0;\n\tq->boundary_rq = NULL;\n\n\tq->id = ida_simple_get(&blk_queue_ida, 0, 0, gfp_mask);\n\tif (q->id < 0)\n\t\tgoto fail_q;\n\n\tret = bioset_init(&q->bio_split, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\tif (ret)\n\t\tgoto fail_id;\n\n\tq->backing_dev_info = bdi_alloc_node(gfp_mask, node_id);\n\tif (!q->backing_dev_info)\n\t\tgoto fail_split;\n\n\tq->stats = blk_alloc_queue_stats();\n\tif (!q->stats)\n\t\tgoto fail_stats;\n\n\tq->backing_dev_info->ra_pages =\n\t\t\t(VM_MAX_READAHEAD * 1024) / PAGE_SIZE;\n\tq->backing_dev_info->capabilities = BDI_CAP_CGROUP_WRITEBACK;\n\tq->backing_dev_info->name = \"block\";\n\tq->node = node_id;\n\n\ttimer_setup(&q->backing_dev_info->laptop_mode_wb_timer,\n\t\t    laptop_mode_timer_fn, 0);\n\ttimer_setup(&q->timeout, blk_rq_timed_out_timer, 0);\n\tINIT_WORK(&q->timeout_work, NULL);\n\tINIT_LIST_HEAD(&q->queue_head);\n\tINIT_LIST_HEAD(&q->timeout_list);\n\tINIT_LIST_HEAD(&q->icq_list);\n#ifdef CONFIG_BLK_CGROUP\n\tINIT_LIST_HEAD(&q->blkg_list);\n#endif\n\tINIT_DELAYED_WORK(&q->delay_work, blk_delay_work);\n\n\tkobject_init(&q->kobj, &blk_queue_ktype);\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tmutex_init(&q->blk_trace_mutex);\n#endif\n\tmutex_init(&q->sysfs_lock);\n\tspin_lock_init(&q->__queue_lock);\n\n\tif (!q->mq_ops)\n\t\tq->queue_lock = lock ? : &q->__queue_lock;\n\n\t/*\n\t * A queue starts its life with bypass turned on to avoid\n\t * unnecessary bypass on/off overhead and nasty surprises during\n\t * init.  The initial bypass will be finished when the queue is\n\t * registered by blk_register_queue().\n\t */\n\tq->bypass_depth = 1;\n\tqueue_flag_set_unlocked(QUEUE_FLAG_BYPASS, q);\n\n\tinit_waitqueue_head(&q->mq_freeze_wq);\n\n\t/*\n\t * Init percpu_ref in atomic mode so that it's faster to shutdown.\n\t * See blk_register_queue() for details.\n\t */\n\tif (percpu_ref_init(&q->q_usage_counter,\n\t\t\t\tblk_queue_usage_counter_release,\n\t\t\t\tPERCPU_REF_INIT_ATOMIC, GFP_KERNEL))\n\t\tgoto fail_bdi;\n\n\tif (blkcg_init_queue(q))\n\t\tgoto fail_ref;\n\n\treturn q;\n\nfail_ref:\n\tpercpu_ref_exit(&q->q_usage_counter);\nfail_bdi:\n\tblk_free_queue_stats(q->stats);\nfail_stats:\n\tbdi_put(q->backing_dev_info);\nfail_split:\n\tbioset_exit(&q->bio_split);\nfail_id:\n\tida_simple_remove(&blk_queue_ida, q->id);\nfail_q:\n\tkmem_cache_free(blk_requestq_cachep, q);\n\treturn NULL;\n}\nEXPORT_SYMBOL(blk_alloc_queue_node);\n\n/**\n * blk_init_queue  - prepare a request queue for use with a block device\n * @rfn:  The function to be called to process requests that have been\n *        placed on the queue.\n * @lock: Request queue spin lock\n *\n * Description:\n *    If a block device wishes to use the standard request handling procedures,\n *    which sorts requests and coalesces adjacent requests, then it must\n *    call blk_init_queue().  The function @rfn will be called when there\n *    are requests on the queue that need to be processed.  If the device\n *    supports plugging, then @rfn may not be called immediately when requests\n *    are available on the queue, but may be called at some time later instead.\n *    Plugged queues are generally unplugged when a buffer belonging to one\n *    of the requests on the queue is needed, or due to memory pressure.\n *\n *    @rfn is not required, or even expected, to remove all requests off the\n *    queue, but only as many as it can handle at a time.  If it does leave\n *    requests on the queue, it is responsible for arranging that the requests\n *    get dealt with eventually.\n *\n *    The queue spin lock must be held while manipulating the requests on the\n *    request queue; this lock will be taken also from interrupt context, so irq\n *    disabling is needed for it.\n *\n *    Function returns a pointer to the initialized request queue, or %NULL if\n *    it didn't succeed.\n *\n * Note:\n *    blk_init_queue() must be paired with a blk_cleanup_queue() call\n *    when the block device is deactivated (such as at module unload).\n **/\n\nstruct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)\n{\n\treturn blk_init_queue_node(rfn, lock, NUMA_NO_NODE);\n}\nEXPORT_SYMBOL(blk_init_queue);\n\nstruct request_queue *\nblk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)\n{\n\tstruct request_queue *q;\n\n\tq = blk_alloc_queue_node(GFP_KERNEL, node_id, lock);\n\tif (!q)\n\t\treturn NULL;\n\n\tq->request_fn = rfn;\n\tif (blk_init_allocated_queue(q) < 0) {\n\t\tblk_cleanup_queue(q);\n\t\treturn NULL;\n\t}\n\n\treturn q;\n}\nEXPORT_SYMBOL(blk_init_queue_node);\n\nstatic blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio);\n\n\nint blk_init_allocated_queue(struct request_queue *q)\n{\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tq->fq = blk_alloc_flush_queue(q, NUMA_NO_NODE, q->cmd_size);\n\tif (!q->fq)\n\t\treturn -ENOMEM;\n\n\tif (q->init_rq_fn && q->init_rq_fn(q, q->fq->flush_rq, GFP_KERNEL))\n\t\tgoto out_free_flush_queue;\n\n\tif (blk_init_rl(&q->root_rl, q, GFP_KERNEL))\n\t\tgoto out_exit_flush_rq;\n\n\tINIT_WORK(&q->timeout_work, blk_timeout_work);\n\tq->queue_flags\t\t|= QUEUE_FLAG_DEFAULT;\n\n\t/*\n\t * This also sets hw/phys segments, boundary and size\n\t */\n\tblk_queue_make_request(q, blk_queue_bio);\n\n\tq->sg_reserved_size = INT_MAX;\n\n\tif (elevator_init(q))\n\t\tgoto out_exit_flush_rq;\n\treturn 0;\n\nout_exit_flush_rq:\n\tif (q->exit_rq_fn)\n\t\tq->exit_rq_fn(q, q->fq->flush_rq);\nout_free_flush_queue:\n\tblk_free_flush_queue(q->fq);\n\tq->fq = NULL;\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(blk_init_allocated_queue);\n\nbool blk_get_queue(struct request_queue *q)\n{\n\tif (likely(!blk_queue_dying(q))) {\n\t\t__blk_get_queue(q);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL(blk_get_queue);\n\nstatic inline void blk_free_request(struct request_list *rl, struct request *rq)\n{\n\tif (rq->rq_flags & RQF_ELVPRIV) {\n\t\telv_put_request(rl->q, rq);\n\t\tif (rq->elv.icq)\n\t\t\tput_io_context(rq->elv.icq->ioc);\n\t}\n\n\tmempool_free(rq, rl->rq_pool);\n}\n\n/*\n * ioc_batching returns true if the ioc is a valid batching request and\n * should be given priority access to a request.\n */\nstatic inline int ioc_batching(struct request_queue *q, struct io_context *ioc)\n{\n\tif (!ioc)\n\t\treturn 0;\n\n\t/*\n\t * Make sure the process is able to allocate at least 1 request\n\t * even if the batch times out, otherwise we could theoretically\n\t * lose wakeups.\n\t */\n\treturn ioc->nr_batch_requests == q->nr_batching ||\n\t\t(ioc->nr_batch_requests > 0\n\t\t&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));\n}\n\n/*\n * ioc_set_batching sets ioc to be a new \"batcher\" if it is not one. This\n * will cause the process to be a \"batcher\" on all queues in the system. This\n * is the behaviour we want though - once it gets a wakeup it should be given\n * a nice run.\n */\nstatic void ioc_set_batching(struct request_queue *q, struct io_context *ioc)\n{\n\tif (!ioc || ioc_batching(q, ioc))\n\t\treturn;\n\n\tioc->nr_batch_requests = q->nr_batching;\n\tioc->last_waited = jiffies;\n}\n\nstatic void __freed_request(struct request_list *rl, int sync)\n{\n\tstruct request_queue *q = rl->q;\n\n\tif (rl->count[sync] < queue_congestion_off_threshold(q))\n\t\tblk_clear_congested(rl, sync);\n\n\tif (rl->count[sync] + 1 <= q->nr_requests) {\n\t\tif (waitqueue_active(&rl->wait[sync]))\n\t\t\twake_up(&rl->wait[sync]);\n\n\t\tblk_clear_rl_full(rl, sync);\n\t}\n}\n\n/*\n * A request has just been released.  Account for it, update the full and\n * congestion status, wake up any waiters.   Called under q->queue_lock.\n */\nstatic void freed_request(struct request_list *rl, bool sync,\n\t\treq_flags_t rq_flags)\n{\n\tstruct request_queue *q = rl->q;\n\n\tq->nr_rqs[sync]--;\n\trl->count[sync]--;\n\tif (rq_flags & RQF_ELVPRIV)\n\t\tq->nr_rqs_elvpriv--;\n\n\t__freed_request(rl, sync);\n\n\tif (unlikely(rl->starved[sync ^ 1]))\n\t\t__freed_request(rl, sync ^ 1);\n}\n\nint blk_update_nr_requests(struct request_queue *q, unsigned int nr)\n{\n\tstruct request_list *rl;\n\tint on_thresh, off_thresh;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tspin_lock_irq(q->queue_lock);\n\tq->nr_requests = nr;\n\tblk_queue_congestion_threshold(q);\n\ton_thresh = queue_congestion_on_threshold(q);\n\toff_thresh = queue_congestion_off_threshold(q);\n\n\tblk_queue_for_each_rl(rl, q) {\n\t\tif (rl->count[BLK_RW_SYNC] >= on_thresh)\n\t\t\tblk_set_congested(rl, BLK_RW_SYNC);\n\t\telse if (rl->count[BLK_RW_SYNC] < off_thresh)\n\t\t\tblk_clear_congested(rl, BLK_RW_SYNC);\n\n\t\tif (rl->count[BLK_RW_ASYNC] >= on_thresh)\n\t\t\tblk_set_congested(rl, BLK_RW_ASYNC);\n\t\telse if (rl->count[BLK_RW_ASYNC] < off_thresh)\n\t\t\tblk_clear_congested(rl, BLK_RW_ASYNC);\n\n\t\tif (rl->count[BLK_RW_SYNC] >= q->nr_requests) {\n\t\t\tblk_set_rl_full(rl, BLK_RW_SYNC);\n\t\t} else {\n\t\t\tblk_clear_rl_full(rl, BLK_RW_SYNC);\n\t\t\twake_up(&rl->wait[BLK_RW_SYNC]);\n\t\t}\n\n\t\tif (rl->count[BLK_RW_ASYNC] >= q->nr_requests) {\n\t\t\tblk_set_rl_full(rl, BLK_RW_ASYNC);\n\t\t} else {\n\t\t\tblk_clear_rl_full(rl, BLK_RW_ASYNC);\n\t\t\twake_up(&rl->wait[BLK_RW_ASYNC]);\n\t\t}\n\t}\n\n\tspin_unlock_irq(q->queue_lock);\n\treturn 0;\n}\n\n/**\n * __get_request - get a free request\n * @rl: request list to allocate from\n * @op: operation and flags\n * @bio: bio to allocate request for (can be %NULL)\n * @flags: BLQ_MQ_REQ_* flags\n * @gfp_mask: allocator flags\n *\n * Get a free request from @q.  This function may fail under memory\n * pressure or if @q is dead.\n *\n * Must be called with @q->queue_lock held and,\n * Returns ERR_PTR on failure, with @q->queue_lock held.\n * Returns request pointer on success, with @q->queue_lock *not held*.\n */\nstatic struct request *__get_request(struct request_list *rl, unsigned int op,\n\t\tstruct bio *bio, blk_mq_req_flags_t flags, gfp_t gfp_mask)\n{\n\tstruct request_queue *q = rl->q;\n\tstruct request *rq;\n\tstruct elevator_type *et = q->elevator->type;\n\tstruct io_context *ioc = rq_ioc(bio);\n\tstruct io_cq *icq = NULL;\n\tconst bool is_sync = op_is_sync(op);\n\tint may_queue;\n\treq_flags_t rq_flags = RQF_ALLOCED;\n\n\tlockdep_assert_held(q->queue_lock);\n\n\tif (unlikely(blk_queue_dying(q)))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tmay_queue = elv_may_queue(q, op);\n\tif (may_queue == ELV_MQUEUE_NO)\n\t\tgoto rq_starved;\n\n\tif (rl->count[is_sync]+1 >= queue_congestion_on_threshold(q)) {\n\t\tif (rl->count[is_sync]+1 >= q->nr_requests) {\n\t\t\t/*\n\t\t\t * The queue will fill after this allocation, so set\n\t\t\t * it as full, and mark this process as \"batching\".\n\t\t\t * This process will be allowed to complete a batch of\n\t\t\t * requests, others will be blocked.\n\t\t\t */\n\t\t\tif (!blk_rl_full(rl, is_sync)) {\n\t\t\t\tioc_set_batching(q, ioc);\n\t\t\t\tblk_set_rl_full(rl, is_sync);\n\t\t\t} else {\n\t\t\t\tif (may_queue != ELV_MQUEUE_MUST\n\t\t\t\t\t\t&& !ioc_batching(q, ioc)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * The queue is full and the allocating\n\t\t\t\t\t * process is not a \"batcher\", and not\n\t\t\t\t\t * exempted by the IO scheduler\n\t\t\t\t\t */\n\t\t\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tblk_set_congested(rl, is_sync);\n\t}\n\n\t/*\n\t * Only allow batching queuers to allocate up to 50% over the defined\n\t * limit of requests, otherwise we could have thousands of requests\n\t * allocated with any setting of ->nr_requests\n\t */\n\tif (rl->count[is_sync] >= (3 * q->nr_requests / 2))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tq->nr_rqs[is_sync]++;\n\trl->count[is_sync]++;\n\trl->starved[is_sync] = 0;\n\n\t/*\n\t * Decide whether the new request will be managed by elevator.  If\n\t * so, mark @rq_flags and increment elvpriv.  Non-zero elvpriv will\n\t * prevent the current elevator from being destroyed until the new\n\t * request is freed.  This guarantees icq's won't be destroyed and\n\t * makes creating new ones safe.\n\t *\n\t * Flush requests do not use the elevator so skip initialization.\n\t * This allows a request to share the flush and elevator data.\n\t *\n\t * Also, lookup icq while holding queue_lock.  If it doesn't exist,\n\t * it will be created after releasing queue_lock.\n\t */\n\tif (!op_is_flush(op) && !blk_queue_bypass(q)) {\n\t\trq_flags |= RQF_ELVPRIV;\n\t\tq->nr_rqs_elvpriv++;\n\t\tif (et->icq_cache && ioc)\n\t\t\ticq = ioc_lookup_icq(ioc, q);\n\t}\n\n\tif (blk_queue_io_stat(q))\n\t\trq_flags |= RQF_IO_STAT;\n\tspin_unlock_irq(q->queue_lock);\n\n\t/* allocate and init request */\n\trq = mempool_alloc(rl->rq_pool, gfp_mask);\n\tif (!rq)\n\t\tgoto fail_alloc;\n\n\tblk_rq_init(q, rq);\n\tblk_rq_set_rl(rq, rl);\n\trq->cmd_flags = op;\n\trq->rq_flags = rq_flags;\n\tif (flags & BLK_MQ_REQ_PREEMPT)\n\t\trq->rq_flags |= RQF_PREEMPT;\n\n\t/* init elvpriv */\n\tif (rq_flags & RQF_ELVPRIV) {\n\t\tif (unlikely(et->icq_cache && !icq)) {\n\t\t\tif (ioc)\n\t\t\t\ticq = ioc_create_icq(ioc, q, gfp_mask);\n\t\t\tif (!icq)\n\t\t\t\tgoto fail_elvpriv;\n\t\t}\n\n\t\trq->elv.icq = icq;\n\t\tif (unlikely(elv_set_request(q, rq, bio, gfp_mask)))\n\t\t\tgoto fail_elvpriv;\n\n\t\t/* @rq->elv.icq holds io_context until @rq is freed */\n\t\tif (icq)\n\t\t\tget_io_context(icq->ioc);\n\t}\nout:\n\t/*\n\t * ioc may be NULL here, and ioc_batching will be false. That's\n\t * OK, if the queue is under the request limit then requests need\n\t * not count toward the nr_batch_requests limit. There will always\n\t * be some limit enforced by BLK_BATCH_TIME.\n\t */\n\tif (ioc_batching(q, ioc))\n\t\tioc->nr_batch_requests--;\n\n\ttrace_block_getrq(q, bio, op);\n\treturn rq;\n\nfail_elvpriv:\n\t/*\n\t * elvpriv init failed.  ioc, icq and elvpriv aren't mempool backed\n\t * and may fail indefinitely under memory pressure and thus\n\t * shouldn't stall IO.  Treat this request as !elvpriv.  This will\n\t * disturb iosched and blkcg but weird is bettern than dead.\n\t */\n\tprintk_ratelimited(KERN_WARNING \"%s: dev %s: request aux data allocation failed, iosched may be disturbed\\n\",\n\t\t\t   __func__, dev_name(q->backing_dev_info->dev));\n\n\trq->rq_flags &= ~RQF_ELVPRIV;\n\trq->elv.icq = NULL;\n\n\tspin_lock_irq(q->queue_lock);\n\tq->nr_rqs_elvpriv--;\n\tspin_unlock_irq(q->queue_lock);\n\tgoto out;\n\nfail_alloc:\n\t/*\n\t * Allocation failed presumably due to memory. Undo anything we\n\t * might have messed up.\n\t *\n\t * Allocating task should really be put onto the front of the wait\n\t * queue, but this is pretty rare.\n\t */\n\tspin_lock_irq(q->queue_lock);\n\tfreed_request(rl, is_sync, rq_flags);\n\n\t/*\n\t * in the very unlikely event that allocation failed and no\n\t * requests for this direction was pending, mark us starved so that\n\t * freeing of a request in the other direction will notice\n\t * us. another possible fix would be to split the rq mempool into\n\t * READ and WRITE\n\t */\nrq_starved:\n\tif (unlikely(rl->count[is_sync] == 0))\n\t\trl->starved[is_sync] = 1;\n\treturn ERR_PTR(-ENOMEM);\n}\n\n/**\n * get_request - get a free request\n * @q: request_queue to allocate request from\n * @op: operation and flags\n * @bio: bio to allocate request for (can be %NULL)\n * @flags: BLK_MQ_REQ_* flags.\n * @gfp: allocator flags\n *\n * Get a free request from @q.  If %BLK_MQ_REQ_NOWAIT is set in @flags,\n * this function keeps retrying under memory pressure and fails iff @q is dead.\n *\n * Must be called with @q->queue_lock held and,\n * Returns ERR_PTR on failure, with @q->queue_lock held.\n * Returns request pointer on success, with @q->queue_lock *not held*.\n */\nstatic struct request *get_request(struct request_queue *q, unsigned int op,\n\t\tstruct bio *bio, blk_mq_req_flags_t flags, gfp_t gfp)\n{\n\tconst bool is_sync = op_is_sync(op);\n\tDEFINE_WAIT(wait);\n\tstruct request_list *rl;\n\tstruct request *rq;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\trl = blk_get_rl(q, bio);\t/* transferred to @rq on success */\nretry:\n\trq = __get_request(rl, op, bio, flags, gfp);\n\tif (!IS_ERR(rq))\n\t\treturn rq;\n\n\tif (op & REQ_NOWAIT) {\n\t\tblk_put_rl(rl);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\tif ((flags & BLK_MQ_REQ_NOWAIT) || unlikely(blk_queue_dying(q))) {\n\t\tblk_put_rl(rl);\n\t\treturn rq;\n\t}\n\n\t/* wait on @rl and retry */\n\tprepare_to_wait_exclusive(&rl->wait[is_sync], &wait,\n\t\t\t\t  TASK_UNINTERRUPTIBLE);\n\n\ttrace_block_sleeprq(q, bio, op);\n\n\tspin_unlock_irq(q->queue_lock);\n\tio_schedule();\n\n\t/*\n\t * After sleeping, we become a \"batching\" process and will be able\n\t * to allocate at least one request, and up to a big batch of them\n\t * for a small period time.  See ioc_batching, ioc_set_batching\n\t */\n\tioc_set_batching(q, current->io_context);\n\n\tspin_lock_irq(q->queue_lock);\n\tfinish_wait(&rl->wait[is_sync], &wait);\n\n\tgoto retry;\n}\n\n/* flags: BLK_MQ_REQ_PREEMPT and/or BLK_MQ_REQ_NOWAIT. */\nstatic struct request *blk_old_get_request(struct request_queue *q,\n\t\t\t\tunsigned int op, blk_mq_req_flags_t flags)\n{\n\tstruct request *rq;\n\tgfp_t gfp_mask = flags & BLK_MQ_REQ_NOWAIT ? GFP_ATOMIC : GFP_NOIO;\n\tint ret = 0;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\t/* create ioc upfront */\n\tcreate_io_context(gfp_mask, q->node);\n\n\tret = blk_queue_enter(q, flags);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tspin_lock_irq(q->queue_lock);\n\trq = get_request(q, op, NULL, flags, gfp_mask);\n\tif (IS_ERR(rq)) {\n\t\tspin_unlock_irq(q->queue_lock);\n\t\tblk_queue_exit(q);\n\t\treturn rq;\n\t}\n\n\t/* q->queue_lock is unlocked at this point */\n\trq->__data_len = 0;\n\trq->__sector = (sector_t) -1;\n\trq->bio = rq->biotail = NULL;\n\treturn rq;\n}\n\n/**\n * blk_get_request - allocate a request\n * @q: request queue to allocate a request for\n * @op: operation (REQ_OP_*) and REQ_* flags, e.g. REQ_SYNC.\n * @flags: BLK_MQ_REQ_* flags, e.g. BLK_MQ_REQ_NOWAIT.\n */\nstruct request *blk_get_request(struct request_queue *q, unsigned int op,\n\t\t\t\tblk_mq_req_flags_t flags)\n{\n\tstruct request *req;\n\n\tWARN_ON_ONCE(op & REQ_NOWAIT);\n\tWARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));\n\n\tif (q->mq_ops) {\n\t\treq = blk_mq_alloc_request(q, op, flags);\n\t\tif (!IS_ERR(req) && q->mq_ops->initialize_rq_fn)\n\t\t\tq->mq_ops->initialize_rq_fn(req);\n\t} else {\n\t\treq = blk_old_get_request(q, op, flags);\n\t\tif (!IS_ERR(req) && q->initialize_rq_fn)\n\t\t\tq->initialize_rq_fn(req);\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL(blk_get_request);\n\n/**\n * blk_requeue_request - put a request back on queue\n * @q:\t\trequest queue where request should be inserted\n * @rq:\t\trequest to be inserted\n *\n * Description:\n *    Drivers often keep queueing requests until the hardware cannot accept\n *    more, when that condition happens we need to put the request back\n *    on the queue. Must be called with queue lock held.\n */\nvoid blk_requeue_request(struct request_queue *q, struct request *rq)\n{\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tblk_delete_timer(rq);\n\tblk_clear_rq_complete(rq);\n\ttrace_block_rq_requeue(q, rq);\n\trq_qos_requeue(q, rq);\n\n\tif (rq->rq_flags & RQF_QUEUED)\n\t\tblk_queue_end_tag(q, rq);\n\n\tBUG_ON(blk_queued_rq(rq));\n\n\telv_requeue_request(q, rq);\n}\nEXPORT_SYMBOL(blk_requeue_request);\n\nstatic void add_acct_request(struct request_queue *q, struct request *rq,\n\t\t\t     int where)\n{\n\tblk_account_io_start(rq, true);\n\t__elv_add_request(q, rq, where);\n}\n\nstatic void part_round_stats_single(struct request_queue *q, int cpu,\n\t\t\t\t    struct hd_struct *part, unsigned long now,\n\t\t\t\t    unsigned int inflight)\n{\n\tif (inflight) {\n\t\t__part_stat_add(cpu, part, time_in_queue,\n\t\t\t\tinflight * (now - part->stamp));\n\t\t__part_stat_add(cpu, part, io_ticks, (now - part->stamp));\n\t}\n\tpart->stamp = now;\n}\n\n/**\n * part_round_stats() - Round off the performance stats on a struct disk_stats.\n * @q: target block queue\n * @cpu: cpu number for stats access\n * @part: target partition\n *\n * The average IO queue length and utilisation statistics are maintained\n * by observing the current state of the queue length and the amount of\n * time it has been in this state for.\n *\n * Normally, that accounting is done on IO completion, but that can result\n * in more than a second's worth of IO being accounted for within any one\n * second, leading to >100% utilisation.  To deal with that, we call this\n * function to do a round-off before returning the results when reading\n * /proc/diskstats.  This accounts immediately for all queue usage up to\n * the current jiffies and restarts the counters again.\n */\nvoid part_round_stats(struct request_queue *q, int cpu, struct hd_struct *part)\n{\n\tstruct hd_struct *part2 = NULL;\n\tunsigned long now = jiffies;\n\tunsigned int inflight[2];\n\tint stats = 0;\n\n\tif (part->stamp != now)\n\t\tstats |= 1;\n\n\tif (part->partno) {\n\t\tpart2 = &part_to_disk(part)->part0;\n\t\tif (part2->stamp != now)\n\t\t\tstats |= 2;\n\t}\n\n\tif (!stats)\n\t\treturn;\n\n\tpart_in_flight(q, part, inflight);\n\n\tif (stats & 2)\n\t\tpart_round_stats_single(q, cpu, part2, now, inflight[1]);\n\tif (stats & 1)\n\t\tpart_round_stats_single(q, cpu, part, now, inflight[0]);\n}\nEXPORT_SYMBOL_GPL(part_round_stats);\n\n#ifdef CONFIG_PM\nstatic void blk_pm_put_request(struct request *rq)\n{\n\tif (rq->q->dev && !(rq->rq_flags & RQF_PM) && !--rq->q->nr_pending)\n\t\tpm_runtime_mark_last_busy(rq->q->dev);\n}\n#else\nstatic inline void blk_pm_put_request(struct request *rq) {}\n#endif\n\nvoid __blk_put_request(struct request_queue *q, struct request *req)\n{\n\treq_flags_t rq_flags = req->rq_flags;\n\n\tif (unlikely(!q))\n\t\treturn;\n\n\tif (q->mq_ops) {\n\t\tblk_mq_free_request(req);\n\t\treturn;\n\t}\n\n\tlockdep_assert_held(q->queue_lock);\n\n\tblk_req_zone_write_unlock(req);\n\tblk_pm_put_request(req);\n\n\telv_completed_request(q, req);\n\n\t/* this is a bio leak */\n\tWARN_ON(req->bio != NULL);\n\n\trq_qos_done(q, req);\n\n\t/*\n\t * Request may not have originated from ll_rw_blk. if not,\n\t * it didn't come out of our reserved rq pools\n\t */\n\tif (rq_flags & RQF_ALLOCED) {\n\t\tstruct request_list *rl = blk_rq_rl(req);\n\t\tbool sync = op_is_sync(req->cmd_flags);\n\n\t\tBUG_ON(!list_empty(&req->queuelist));\n\t\tBUG_ON(ELV_ON_HASH(req));\n\n\t\tblk_free_request(rl, req);\n\t\tfreed_request(rl, sync, rq_flags);\n\t\tblk_put_rl(rl);\n\t\tblk_queue_exit(q);\n\t}\n}\nEXPORT_SYMBOL_GPL(__blk_put_request);\n\nvoid blk_put_request(struct request *req)\n{\n\tstruct request_queue *q = req->q;\n\n\tif (q->mq_ops)\n\t\tblk_mq_free_request(req);\n\telse {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(q->queue_lock, flags);\n\t\t__blk_put_request(q, req);\n\t\tspin_unlock_irqrestore(q->queue_lock, flags);\n\t}\n}\nEXPORT_SYMBOL(blk_put_request);\n\nbool bio_attempt_back_merge(struct request_queue *q, struct request *req,\n\t\t\t    struct bio *bio)\n{\n\tconst int ff = bio->bi_opf & REQ_FAILFAST_MASK;\n\n\tif (!ll_back_merge_fn(q, req, bio))\n\t\treturn false;\n\n\ttrace_block_bio_backmerge(q, req, bio);\n\n\tif ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)\n\t\tblk_rq_set_mixed_merge(req);\n\n\treq->biotail->bi_next = bio;\n\treq->biotail = bio;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->ioprio = ioprio_best(req->ioprio, bio_prio(bio));\n\n\tblk_account_io_start(req, false);\n\treturn true;\n}\n\nbool bio_attempt_front_merge(struct request_queue *q, struct request *req,\n\t\t\t     struct bio *bio)\n{\n\tconst int ff = bio->bi_opf & REQ_FAILFAST_MASK;\n\n\tif (!ll_front_merge_fn(q, req, bio))\n\t\treturn false;\n\n\ttrace_block_bio_frontmerge(q, req, bio);\n\n\tif ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)\n\t\tblk_rq_set_mixed_merge(req);\n\n\tbio->bi_next = req->bio;\n\treq->bio = bio;\n\n\treq->__sector = bio->bi_iter.bi_sector;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->ioprio = ioprio_best(req->ioprio, bio_prio(bio));\n\n\tblk_account_io_start(req, false);\n\treturn true;\n}\n\nbool bio_attempt_discard_merge(struct request_queue *q, struct request *req,\n\t\tstruct bio *bio)\n{\n\tunsigned short segments = blk_rq_nr_discard_segments(req);\n\n\tif (segments >= queue_max_discard_segments(q))\n\t\tgoto no_merge;\n\tif (blk_rq_sectors(req) + bio_sectors(bio) >\n\t    blk_rq_get_max_sectors(req, blk_rq_pos(req)))\n\t\tgoto no_merge;\n\n\treq->biotail->bi_next = bio;\n\treq->biotail = bio;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->ioprio = ioprio_best(req->ioprio, bio_prio(bio));\n\treq->nr_phys_segments = segments + 1;\n\n\tblk_account_io_start(req, false);\n\treturn true;\nno_merge:\n\treq_set_nomerge(q, req);\n\treturn false;\n}\n\n/**\n * blk_attempt_plug_merge - try to merge with %current's plugged list\n * @q: request_queue new bio is being queued at\n * @bio: new bio being queued\n * @request_count: out parameter for number of traversed plugged requests\n * @same_queue_rq: pointer to &struct request that gets filled in when\n * another request associated with @q is found on the plug list\n * (optional, may be %NULL)\n *\n * Determine whether @bio being queued on @q can be merged with a request\n * on %current's plugged list.  Returns %true if merge was successful,\n * otherwise %false.\n *\n * Plugging coalesces IOs from the same issuer for the same purpose without\n * going through @q->queue_lock.  As such it's more of an issuing mechanism\n * than scheduling, and the request, while may have elvpriv data, is not\n * added on the elevator at this point.  In addition, we don't have\n * reliable access to the elevator outside queue lock.  Only check basic\n * merging parameters without querying the elevator.\n *\n * Caller must ensure !blk_queue_nomerges(q) beforehand.\n */\nbool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,\n\t\t\t    unsigned int *request_count,\n\t\t\t    struct request **same_queue_rq)\n{\n\tstruct blk_plug *plug;\n\tstruct request *rq;\n\tstruct list_head *plug_list;\n\n\tplug = current->plug;\n\tif (!plug)\n\t\treturn false;\n\t*request_count = 0;\n\n\tif (q->mq_ops)\n\t\tplug_list = &plug->mq_list;\n\telse\n\t\tplug_list = &plug->list;\n\n\tlist_for_each_entry_reverse(rq, plug_list, queuelist) {\n\t\tbool merged = false;\n\n\t\tif (rq->q == q) {\n\t\t\t(*request_count)++;\n\t\t\t/*\n\t\t\t * Only blk-mq multiple hardware queues case checks the\n\t\t\t * rq in the same queue, there should be only one such\n\t\t\t * rq in a queue\n\t\t\t **/\n\t\t\tif (same_queue_rq)\n\t\t\t\t*same_queue_rq = rq;\n\t\t}\n\n\t\tif (rq->q != q || !blk_rq_merge_ok(rq, bio))\n\t\t\tcontinue;\n\n\t\tswitch (blk_try_merge(rq, bio)) {\n\t\tcase ELEVATOR_BACK_MERGE:\n\t\t\tmerged = bio_attempt_back_merge(q, rq, bio);\n\t\t\tbreak;\n\t\tcase ELEVATOR_FRONT_MERGE:\n\t\t\tmerged = bio_attempt_front_merge(q, rq, bio);\n\t\t\tbreak;\n\t\tcase ELEVATOR_DISCARD_MERGE:\n\t\t\tmerged = bio_attempt_discard_merge(q, rq, bio);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (merged)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nunsigned int blk_plug_queued_count(struct request_queue *q)\n{\n\tstruct blk_plug *plug;\n\tstruct request *rq;\n\tstruct list_head *plug_list;\n\tunsigned int ret = 0;\n\n\tplug = current->plug;\n\tif (!plug)\n\t\tgoto out;\n\n\tif (q->mq_ops)\n\t\tplug_list = &plug->mq_list;\n\telse\n\t\tplug_list = &plug->list;\n\n\tlist_for_each_entry(rq, plug_list, queuelist) {\n\t\tif (rq->q == q)\n\t\t\tret++;\n\t}\nout:\n\treturn ret;\n}\n\nvoid blk_init_request_from_bio(struct request *req, struct bio *bio)\n{\n\tstruct io_context *ioc = rq_ioc(bio);\n\n\tif (bio->bi_opf & REQ_RAHEAD)\n\t\treq->cmd_flags |= REQ_FAILFAST_MASK;\n\n\treq->__sector = bio->bi_iter.bi_sector;\n\tif (ioprio_valid(bio_prio(bio)))\n\t\treq->ioprio = bio_prio(bio);\n\telse if (ioc)\n\t\treq->ioprio = ioc->ioprio;\n\telse\n\t\treq->ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, 0);\n\treq->write_hint = bio->bi_write_hint;\n\tblk_rq_bio_prep(req->q, req, bio);\n}\nEXPORT_SYMBOL_GPL(blk_init_request_from_bio);\n\nstatic blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)\n{\n\tstruct blk_plug *plug;\n\tint where = ELEVATOR_INSERT_SORT;\n\tstruct request *req, *free;\n\tunsigned int request_count = 0;\n\n\t/*\n\t * low level driver can indicate that it wants pages above a\n\t * certain limit bounced to low memory (ie for highmem, or even\n\t * ISA dma in theory)\n\t */\n\tblk_queue_bounce(q, &bio);\n\n\tblk_queue_split(q, &bio);\n\n\tif (!bio_integrity_prep(bio))\n\t\treturn BLK_QC_T_NONE;\n\n\tif (op_is_flush(bio->bi_opf)) {\n\t\tspin_lock_irq(q->queue_lock);\n\t\twhere = ELEVATOR_INSERT_FLUSH;\n\t\tgoto get_rq;\n\t}\n\n\t/*\n\t * Check if we can merge with the plugged list before grabbing\n\t * any locks.\n\t */\n\tif (!blk_queue_nomerges(q)) {\n\t\tif (blk_attempt_plug_merge(q, bio, &request_count, NULL))\n\t\t\treturn BLK_QC_T_NONE;\n\t} else\n\t\trequest_count = blk_plug_queued_count(q);\n\n\tspin_lock_irq(q->queue_lock);\n\n\tswitch (elv_merge(q, &req, bio)) {\n\tcase ELEVATOR_BACK_MERGE:\n\t\tif (!bio_attempt_back_merge(q, req, bio))\n\t\t\tbreak;\n\t\telv_bio_merged(q, req, bio);\n\t\tfree = attempt_back_merge(q, req);\n\t\tif (free)\n\t\t\t__blk_put_request(q, free);\n\t\telse\n\t\t\telv_merged_request(q, req, ELEVATOR_BACK_MERGE);\n\t\tgoto out_unlock;\n\tcase ELEVATOR_FRONT_MERGE:\n\t\tif (!bio_attempt_front_merge(q, req, bio))\n\t\t\tbreak;\n\t\telv_bio_merged(q, req, bio);\n\t\tfree = attempt_front_merge(q, req);\n\t\tif (free)\n\t\t\t__blk_put_request(q, free);\n\t\telse\n\t\t\telv_merged_request(q, req, ELEVATOR_FRONT_MERGE);\n\t\tgoto out_unlock;\n\tdefault:\n\t\tbreak;\n\t}\n\nget_rq:\n\trq_qos_throttle(q, bio, q->queue_lock);\n\n\t/*\n\t * Grab a free request. This is might sleep but can not fail.\n\t * Returns with the queue unlocked.\n\t */\n\tblk_queue_enter_live(q);\n\treq = get_request(q, bio->bi_opf, bio, 0, GFP_NOIO);\n\tif (IS_ERR(req)) {\n\t\tblk_queue_exit(q);\n\t\trq_qos_cleanup(q, bio);\n\t\tif (PTR_ERR(req) == -ENOMEM)\n\t\t\tbio->bi_status = BLK_STS_RESOURCE;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\tgoto out_unlock;\n\t}\n\n\trq_qos_track(q, req, bio);\n\n\t/*\n\t * After dropping the lock and possibly sleeping here, our request\n\t * may now be mergeable after it had proven unmergeable (above).\n\t * We don't worry about that case for efficiency. It won't happen\n\t * often, and the elevators are able to handle it.\n\t */\n\tblk_init_request_from_bio(req, bio);\n\n\tif (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags))\n\t\treq->cpu = raw_smp_processor_id();\n\n\tplug = current->plug;\n\tif (plug) {\n\t\t/*\n\t\t * If this is the first request added after a plug, fire\n\t\t * of a plug trace.\n\t\t *\n\t\t * @request_count may become stale because of schedule\n\t\t * out, so check plug list again.\n\t\t */\n\t\tif (!request_count || list_empty(&plug->list))\n\t\t\ttrace_block_plug(q);\n\t\telse {\n\t\t\tstruct request *last = list_entry_rq(plug->list.prev);\n\t\t\tif (request_count >= BLK_MAX_REQUEST_COUNT ||\n\t\t\t    blk_rq_bytes(last) >= BLK_PLUG_FLUSH_SIZE) {\n\t\t\t\tblk_flush_plug_list(plug, false);\n\t\t\t\ttrace_block_plug(q);\n\t\t\t}\n\t\t}\n\t\tlist_add_tail(&req->queuelist, &plug->list);\n\t\tblk_account_io_start(req, true);\n\t} else {\n\t\tspin_lock_irq(q->queue_lock);\n\t\tadd_acct_request(q, req, where);\n\t\t__blk_run_queue(q);\nout_unlock:\n\t\tspin_unlock_irq(q->queue_lock);\n\t}\n\n\treturn BLK_QC_T_NONE;\n}\n\nstatic void handle_bad_sector(struct bio *bio, sector_t maxsector)\n{\n\tchar b[BDEVNAME_SIZE];\n\n\tprintk(KERN_INFO \"attempt to access beyond end of device\\n\");\n\tprintk(KERN_INFO \"%s: rw=%d, want=%Lu, limit=%Lu\\n\",\n\t\t\tbio_devname(bio, b), bio->bi_opf,\n\t\t\t(unsigned long long)bio_end_sector(bio),\n\t\t\t(long long)maxsector);\n}\n\n#ifdef CONFIG_FAIL_MAKE_REQUEST\n\nstatic DECLARE_FAULT_ATTR(fail_make_request);\n\nstatic int __init setup_fail_make_request(char *str)\n{\n\treturn setup_fault_attr(&fail_make_request, str);\n}\n__setup(\"fail_make_request=\", setup_fail_make_request);\n\nstatic bool should_fail_request(struct hd_struct *part, unsigned int bytes)\n{\n\treturn part->make_it_fail && should_fail(&fail_make_request, bytes);\n}\n\nstatic int __init fail_make_request_debugfs(void)\n{\n\tstruct dentry *dir = fault_create_debugfs_attr(\"fail_make_request\",\n\t\t\t\t\t\tNULL, &fail_make_request);\n\n\treturn PTR_ERR_OR_ZERO(dir);\n}\n\nlate_initcall(fail_make_request_debugfs);\n\n#else /* CONFIG_FAIL_MAKE_REQUEST */\n\nstatic inline bool should_fail_request(struct hd_struct *part,\n\t\t\t\t\tunsigned int bytes)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_FAIL_MAKE_REQUEST */\n\nstatic inline bool bio_check_ro(struct bio *bio, struct hd_struct *part)\n{\n\tif (part->policy && op_is_write(bio_op(bio))) {\n\t\tchar b[BDEVNAME_SIZE];\n\n\t\tprintk(KERN_ERR\n\t\t       \"generic_make_request: Trying to write \"\n\t\t\t\"to read-only block-device %s (partno %d)\\n\",\n\t\t\tbio_devname(bio, b), part->partno);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic noinline int should_fail_bio(struct bio *bio)\n{\n\tif (should_fail_request(&bio->bi_disk->part0, bio->bi_iter.bi_size))\n\t\treturn -EIO;\n\treturn 0;\n}\nALLOW_ERROR_INJECTION(should_fail_bio, ERRNO);\n\n/*\n * Check whether this bio extends beyond the end of the device or partition.\n * This may well happen - the kernel calls bread() without checking the size of\n * the device, e.g., when mounting a file system.\n */\nstatic inline int bio_check_eod(struct bio *bio, sector_t maxsector)\n{\n\tunsigned int nr_sectors = bio_sectors(bio);\n\n\tif (nr_sectors && maxsector &&\n\t    (nr_sectors > maxsector ||\n\t     bio->bi_iter.bi_sector > maxsector - nr_sectors)) {\n\t\thandle_bad_sector(bio, maxsector);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n/*\n * Remap block n of partition p to block n+start(p) of the disk.\n */\nstatic inline int blk_partition_remap(struct bio *bio)\n{\n\tstruct hd_struct *p;\n\tint ret = -EIO;\n\n\trcu_read_lock();\n\tp = __disk_get_part(bio->bi_disk, bio->bi_partno);\n\tif (unlikely(!p))\n\t\tgoto out;\n\tif (unlikely(should_fail_request(p, bio->bi_iter.bi_size)))\n\t\tgoto out;\n\tif (unlikely(bio_check_ro(bio, p)))\n\t\tgoto out;\n\n\t/*\n\t * Zone reset does not include bi_size so bio_sectors() is always 0.\n\t * Include a test for the reset op code and perform the remap if needed.\n\t */\n\tif (bio_sectors(bio) || bio_op(bio) == REQ_OP_ZONE_RESET) {\n\t\tif (bio_check_eod(bio, part_nr_sects_read(p)))\n\t\t\tgoto out;\n\t\tbio->bi_iter.bi_sector += p->start_sect;\n\t\ttrace_block_bio_remap(bio->bi_disk->queue, bio, part_devt(p),\n\t\t\t\t      bio->bi_iter.bi_sector - p->start_sect);\n\t}\n\tbio->bi_partno = 0;\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic noinline_for_stack bool\ngeneric_make_request_checks(struct bio *bio)\n{\n\tstruct request_queue *q;\n\tint nr_sectors = bio_sectors(bio);\n\tblk_status_t status = BLK_STS_IOERR;\n\tchar b[BDEVNAME_SIZE];\n\n\tmight_sleep();\n\n\tq = bio->bi_disk->queue;\n\tif (unlikely(!q)) {\n\t\tprintk(KERN_ERR\n\t\t       \"generic_make_request: Trying to access \"\n\t\t\t\"nonexistent block-device %s (%Lu)\\n\",\n\t\t\tbio_devname(bio, b), (long long)bio->bi_iter.bi_sector);\n\t\tgoto end_io;\n\t}\n\n\t/*\n\t * For a REQ_NOWAIT based request, return -EOPNOTSUPP\n\t * if queue is not a request based queue.\n\t */\n\tif ((bio->bi_opf & REQ_NOWAIT) && !queue_is_rq_based(q))\n\t\tgoto not_supported;\n\n\tif (should_fail_bio(bio))\n\t\tgoto end_io;\n\n\tif (bio->bi_partno) {\n\t\tif (unlikely(blk_partition_remap(bio)))\n\t\t\tgoto end_io;\n\t} else {\n\t\tif (unlikely(bio_check_ro(bio, &bio->bi_disk->part0)))\n\t\t\tgoto end_io;\n\t\tif (unlikely(bio_check_eod(bio, get_capacity(bio->bi_disk))))\n\t\t\tgoto end_io;\n\t}\n\n\t/*\n\t * Filter flush bio's early so that make_request based\n\t * drivers without flush support don't have to worry\n\t * about them.\n\t */\n\tif (op_is_flush(bio->bi_opf) &&\n\t    !test_bit(QUEUE_FLAG_WC, &q->queue_flags)) {\n\t\tbio->bi_opf &= ~(REQ_PREFLUSH | REQ_FUA);\n\t\tif (!nr_sectors) {\n\t\t\tstatus = BLK_STS_OK;\n\t\t\tgoto end_io;\n\t\t}\n\t}\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\t\tif (!blk_queue_discard(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_SECURE_ERASE:\n\t\tif (!blk_queue_secure_erase(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_WRITE_SAME:\n\t\tif (!q->limits.max_write_same_sectors)\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_ZONE_REPORT:\n\tcase REQ_OP_ZONE_RESET:\n\t\tif (!blk_queue_is_zoned(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tif (!q->limits.max_write_zeroes_sectors)\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/*\n\t * Various block parts want %current->io_context and lazy ioc\n\t * allocation ends up trading a lot of pain for a small amount of\n\t * memory.  Just allocate it upfront.  This may fail and block\n\t * layer knows how to live with it.\n\t */\n\tcreate_io_context(GFP_ATOMIC, q->node);\n\n\tif (!blkcg_bio_issue_check(q, bio))\n\t\treturn false;\n\n\tif (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {\n\t\ttrace_block_bio_queue(q, bio);\n\t\t/* Now that enqueuing has been traced, we need to trace\n\t\t * completion as well.\n\t\t */\n\t\tbio_set_flag(bio, BIO_TRACE_COMPLETION);\n\t}\n\treturn true;\n\nnot_supported:\n\tstatus = BLK_STS_NOTSUPP;\nend_io:\n\tbio->bi_status = status;\n\tbio_endio(bio);\n\treturn false;\n}\n\n/**\n * generic_make_request - hand a buffer to its device driver for I/O\n * @bio:  The bio describing the location in memory and on the device.\n *\n * generic_make_request() is used to make I/O requests of block\n * devices. It is passed a &struct bio, which describes the I/O that needs\n * to be done.\n *\n * generic_make_request() does not return any status.  The\n * success/failure status of the request, along with notification of\n * completion, is delivered asynchronously through the bio->bi_end_io\n * function described (one day) else where.\n *\n * The caller of generic_make_request must make sure that bi_io_vec\n * are set to describe the memory buffer, and that bi_dev and bi_sector are\n * set to describe the device address, and the\n * bi_end_io and optionally bi_private are set to describe how\n * completion notification should be signaled.\n *\n * generic_make_request and the drivers it calls may use bi_next if this\n * bio happens to be merged with someone else, and may resubmit the bio to\n * a lower device by calling into generic_make_request recursively, which\n * means the bio should NOT be touched after the call to ->make_request_fn.\n */\nblk_qc_t generic_make_request(struct bio *bio)\n{\n\t/*\n\t * bio_list_on_stack[0] contains bios submitted by the current\n\t * make_request_fn.\n\t * bio_list_on_stack[1] contains bios that were submitted before\n\t * the current make_request_fn, but that haven't been processed\n\t * yet.\n\t */\n\tstruct bio_list bio_list_on_stack[2];\n\tblk_mq_req_flags_t flags = 0;\n\tstruct request_queue *q = bio->bi_disk->queue;\n\tblk_qc_t ret = BLK_QC_T_NONE;\n\n\tif (bio->bi_opf & REQ_NOWAIT)\n\t\tflags = BLK_MQ_REQ_NOWAIT;\n\tif (bio_flagged(bio, BIO_QUEUE_ENTERED))\n\t\tblk_queue_enter_live(q);\n\telse if (blk_queue_enter(q, flags) < 0) {\n\t\tif (!blk_queue_dying(q) && (bio->bi_opf & REQ_NOWAIT))\n\t\t\tbio_wouldblock_error(bio);\n\t\telse\n\t\t\tbio_io_error(bio);\n\t\treturn ret;\n\t}\n\n\tif (!generic_make_request_checks(bio))\n\t\tgoto out;\n\n\t/*\n\t * We only want one ->make_request_fn to be active at a time, else\n\t * stack usage with stacked devices could be a problem.  So use\n\t * current->bio_list to keep a list of requests submited by a\n\t * make_request_fn function.  current->bio_list is also used as a\n\t * flag to say if generic_make_request is currently active in this\n\t * task or not.  If it is NULL, then no make_request is active.  If\n\t * it is non-NULL, then a make_request is active, and new requests\n\t * should be added at the tail\n\t */\n\tif (current->bio_list) {\n\t\tbio_list_add(&current->bio_list[0], bio);\n\t\tgoto out;\n\t}\n\n\t/* following loop may be a bit non-obvious, and so deserves some\n\t * explanation.\n\t * Before entering the loop, bio->bi_next is NULL (as all callers\n\t * ensure that) so we have a list with a single bio.\n\t * We pretend that we have just taken it off a longer list, so\n\t * we assign bio_list to a pointer to the bio_list_on_stack,\n\t * thus initialising the bio_list of new bios to be\n\t * added.  ->make_request() may indeed add some more bios\n\t * through a recursive call to generic_make_request.  If it\n\t * did, we find a non-NULL value in bio_list and re-enter the loop\n\t * from the top.  In this case we really did just take the bio\n\t * of the top of the list (no pretending) and so remove it from\n\t * bio_list, and call into ->make_request() again.\n\t */\n\tBUG_ON(bio->bi_next);\n\tbio_list_init(&bio_list_on_stack[0]);\n\tcurrent->bio_list = bio_list_on_stack;\n\tdo {\n\t\tbool enter_succeeded = true;\n\n\t\tif (unlikely(q != bio->bi_disk->queue)) {\n\t\t\tif (q)\n\t\t\t\tblk_queue_exit(q);\n\t\t\tq = bio->bi_disk->queue;\n\t\t\tflags = 0;\n\t\t\tif (bio->bi_opf & REQ_NOWAIT)\n\t\t\t\tflags = BLK_MQ_REQ_NOWAIT;\n\t\t\tif (blk_queue_enter(q, flags) < 0) {\n\t\t\t\tenter_succeeded = false;\n\t\t\t\tq = NULL;\n\t\t\t}\n\t\t}\n\n\t\tif (enter_succeeded) {\n\t\t\tstruct bio_list lower, same;\n\n\t\t\t/* Create a fresh bio_list for all subordinate requests */\n\t\t\tbio_list_on_stack[1] = bio_list_on_stack[0];\n\t\t\tbio_list_init(&bio_list_on_stack[0]);\n\t\t\tret = q->make_request_fn(q, bio);\n\n\t\t\t/* sort new bios into those for a lower level\n\t\t\t * and those for the same level\n\t\t\t */\n\t\t\tbio_list_init(&lower);\n\t\t\tbio_list_init(&same);\n\t\t\twhile ((bio = bio_list_pop(&bio_list_on_stack[0])) != NULL)\n\t\t\t\tif (q == bio->bi_disk->queue)\n\t\t\t\t\tbio_list_add(&same, bio);\n\t\t\t\telse\n\t\t\t\t\tbio_list_add(&lower, bio);\n\t\t\t/* now assemble so we handle the lowest level first */\n\t\t\tbio_list_merge(&bio_list_on_stack[0], &lower);\n\t\t\tbio_list_merge(&bio_list_on_stack[0], &same);\n\t\t\tbio_list_merge(&bio_list_on_stack[0], &bio_list_on_stack[1]);\n\t\t} else {\n\t\t\tif (unlikely(!blk_queue_dying(q) &&\n\t\t\t\t\t(bio->bi_opf & REQ_NOWAIT)))\n\t\t\t\tbio_wouldblock_error(bio);\n\t\t\telse\n\t\t\t\tbio_io_error(bio);\n\t\t}\n\t\tbio = bio_list_pop(&bio_list_on_stack[0]);\n\t} while (bio);\n\tcurrent->bio_list = NULL; /* deactivate */\n\nout:\n\tif (q)\n\t\tblk_queue_exit(q);\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_make_request);\n\n/**\n * direct_make_request - hand a buffer directly to its device driver for I/O\n * @bio:  The bio describing the location in memory and on the device.\n *\n * This function behaves like generic_make_request(), but does not protect\n * against recursion.  Must only be used if the called driver is known\n * to not call generic_make_request (or direct_make_request) again from\n * its make_request function.  (Calling direct_make_request again from\n * a workqueue is perfectly fine as that doesn't recurse).\n */\nblk_qc_t direct_make_request(struct bio *bio)\n{\n\tstruct request_queue *q = bio->bi_disk->queue;\n\tbool nowait = bio->bi_opf & REQ_NOWAIT;\n\tblk_qc_t ret;\n\n\tif (!generic_make_request_checks(bio))\n\t\treturn BLK_QC_T_NONE;\n\n\tif (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {\n\t\tif (nowait && !blk_queue_dying(q))\n\t\t\tbio->bi_status = BLK_STS_AGAIN;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn BLK_QC_T_NONE;\n\t}\n\n\tret = q->make_request_fn(q, bio);\n\tblk_queue_exit(q);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(direct_make_request);\n\n/**\n * submit_bio - submit a bio to the block device layer for I/O\n * @bio: The &struct bio which describes the I/O\n *\n * submit_bio() is very similar in purpose to generic_make_request(), and\n * uses that function to do most of the work. Both are fairly rough\n * interfaces; @bio must be presetup and ready for I/O.\n *\n */\nblk_qc_t submit_bio(struct bio *bio)\n{\n\t/*\n\t * If it's a regular read/write or a barrier with data attached,\n\t * go through the normal accounting stuff before submission.\n\t */\n\tif (bio_has_data(bio)) {\n\t\tunsigned int count;\n\n\t\tif (unlikely(bio_op(bio) == REQ_OP_WRITE_SAME))\n\t\t\tcount = queue_logical_block_size(bio->bi_disk->queue) >> 9;\n\t\telse\n\t\t\tcount = bio_sectors(bio);\n\n\t\tif (op_is_write(bio_op(bio))) {\n\t\t\tcount_vm_events(PGPGOUT, count);\n\t\t} else {\n\t\t\ttask_io_account_read(bio->bi_iter.bi_size);\n\t\t\tcount_vm_events(PGPGIN, count);\n\t\t}\n\n\t\tif (unlikely(block_dump)) {\n\t\t\tchar b[BDEVNAME_SIZE];\n\t\t\tprintk(KERN_DEBUG \"%s(%d): %s block %Lu on %s (%u sectors)\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t\top_is_write(bio_op(bio)) ? \"WRITE\" : \"READ\",\n\t\t\t\t(unsigned long long)bio->bi_iter.bi_sector,\n\t\t\t\tbio_devname(bio, b), count);\n\t\t}\n\t}\n\n\treturn generic_make_request(bio);\n}\nEXPORT_SYMBOL(submit_bio);\n\nbool blk_poll(struct request_queue *q, blk_qc_t cookie)\n{\n\tif (!q->poll_fn || !blk_qc_t_valid(cookie))\n\t\treturn false;\n\n\tif (current->plug)\n\t\tblk_flush_plug_list(current->plug, false);\n\treturn q->poll_fn(q, cookie);\n}\nEXPORT_SYMBOL_GPL(blk_poll);\n\n/**\n * blk_cloned_rq_check_limits - Helper function to check a cloned request\n *                              for new the queue limits\n * @q:  the queue\n * @rq: the request being checked\n *\n * Description:\n *    @rq may have been made based on weaker limitations of upper-level queues\n *    in request stacking drivers, and it may violate the limitation of @q.\n *    Since the block layer and the underlying device driver trust @rq\n *    after it is inserted to @q, it should be checked against @q before\n *    the insertion using this generic function.\n *\n *    Request stacking drivers like request-based dm may change the queue\n *    limits when retrying requests on other queues. Those requests need\n *    to be checked against the new queue limits again during dispatch.\n */\nstatic int blk_cloned_rq_check_limits(struct request_queue *q,\n\t\t\t\t      struct request *rq)\n{\n\tif (blk_rq_sectors(rq) > blk_queue_get_max_sectors(q, req_op(rq))) {\n\t\tprintk(KERN_ERR \"%s: over max size limit.\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\t/*\n\t * queue's settings related to segment counting like q->bounce_pfn\n\t * may differ from that of other stacking queues.\n\t * Recalculate it to check the request correctly on this queue's\n\t * limitation.\n\t */\n\tblk_recalc_rq_segments(rq);\n\tif (rq->nr_phys_segments > queue_max_segments(q)) {\n\t\tprintk(KERN_ERR \"%s: over max segments limit.\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * blk_insert_cloned_request - Helper for stacking drivers to submit a request\n * @q:  the queue to submit the request\n * @rq: the request being queued\n */\nblk_status_t blk_insert_cloned_request(struct request_queue *q, struct request *rq)\n{\n\tunsigned long flags;\n\tint where = ELEVATOR_INSERT_BACK;\n\n\tif (blk_cloned_rq_check_limits(q, rq))\n\t\treturn BLK_STS_IOERR;\n\n\tif (rq->rq_disk &&\n\t    should_fail_request(&rq->rq_disk->part0, blk_rq_bytes(rq)))\n\t\treturn BLK_STS_IOERR;\n\n\tif (q->mq_ops) {\n\t\tif (blk_queue_io_stat(q))\n\t\t\tblk_account_io_start(rq, true);\n\t\t/*\n\t\t * Since we have a scheduler attached on the top device,\n\t\t * bypass a potential scheduler on the bottom device for\n\t\t * insert.\n\t\t */\n\t\treturn blk_mq_request_issue_directly(rq);\n\t}\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tif (unlikely(blk_queue_dying(q))) {\n\t\tspin_unlock_irqrestore(q->queue_lock, flags);\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\t/*\n\t * Submitting request must be dequeued before calling this function\n\t * because it will be linked to another request_queue\n\t */\n\tBUG_ON(blk_queued_rq(rq));\n\n\tif (op_is_flush(rq->cmd_flags))\n\t\twhere = ELEVATOR_INSERT_FLUSH;\n\n\tadd_acct_request(q, rq, where);\n\tif (where == ELEVATOR_INSERT_FLUSH)\n\t\t__blk_run_queue(q);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn BLK_STS_OK;\n}\nEXPORT_SYMBOL_GPL(blk_insert_cloned_request);\n\n/**\n * blk_rq_err_bytes - determine number of bytes till the next failure boundary\n * @rq: request to examine\n *\n * Description:\n *     A request could be merge of IOs which require different failure\n *     handling.  This function determines the number of bytes which\n *     can be failed from the beginning of the request without\n *     crossing into area which need to be retried further.\n *\n * Return:\n *     The number of bytes to fail.\n */\nunsigned int blk_rq_err_bytes(const struct request *rq)\n{\n\tunsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;\n\tunsigned int bytes = 0;\n\tstruct bio *bio;\n\n\tif (!(rq->rq_flags & RQF_MIXED_MERGE))\n\t\treturn blk_rq_bytes(rq);\n\n\t/*\n\t * Currently the only 'mixing' which can happen is between\n\t * different fastfail types.  We can safely fail portions\n\t * which have all the failfast bits that the first one has -\n\t * the ones which are at least as eager to fail as the first\n\t * one.\n\t */\n\tfor (bio = rq->bio; bio; bio = bio->bi_next) {\n\t\tif ((bio->bi_opf & ff) != ff)\n\t\t\tbreak;\n\t\tbytes += bio->bi_iter.bi_size;\n\t}\n\n\t/* this could lead to infinite loop */\n\tBUG_ON(blk_rq_bytes(rq) && !bytes);\n\treturn bytes;\n}\nEXPORT_SYMBOL_GPL(blk_rq_err_bytes);\n\nvoid blk_account_io_completion(struct request *req, unsigned int bytes)\n{\n\tif (blk_do_io_stat(req)) {\n\t\tconst int sgrp = op_stat_group(req_op(req));\n\t\tstruct hd_struct *part;\n\t\tint cpu;\n\n\t\tcpu = part_stat_lock();\n\t\tpart = req->part;\n\t\tpart_stat_add(cpu, part, sectors[sgrp], bytes >> 9);\n\t\tpart_stat_unlock();\n\t}\n}\n\nvoid blk_account_io_done(struct request *req, u64 now)\n{\n\t/*\n\t * Account IO completion.  flush_rq isn't accounted as a\n\t * normal IO on queueing nor completion.  Accounting the\n\t * containing request is enough.\n\t */\n\tif (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) {\n\t\tunsigned long duration;\n\t\tconst int sgrp = op_stat_group(req_op(req));\n\t\tstruct hd_struct *part;\n\t\tint cpu;\n\n\t\tduration = nsecs_to_jiffies(now - req->start_time_ns);\n\t\tcpu = part_stat_lock();\n\t\tpart = req->part;\n\n\t\tpart_stat_inc(cpu, part, ios[sgrp]);\n\t\tpart_stat_add(cpu, part, ticks[sgrp], duration);\n\t\tpart_round_stats(req->q, cpu, part);\n\t\tpart_dec_in_flight(req->q, part, rq_data_dir(req));\n\n\t\thd_struct_put(part);\n\t\tpart_stat_unlock();\n\t}\n}\n\n#ifdef CONFIG_PM\n/*\n * Don't process normal requests when queue is suspended\n * or in the process of suspending/resuming\n */\nstatic bool blk_pm_allow_request(struct request *rq)\n{\n\tswitch (rq->q->rpm_status) {\n\tcase RPM_RESUMING:\n\tcase RPM_SUSPENDING:\n\t\treturn rq->rq_flags & RQF_PM;\n\tcase RPM_SUSPENDED:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n#else\nstatic bool blk_pm_allow_request(struct request *rq)\n{\n\treturn true;\n}\n#endif\n\nvoid blk_account_io_start(struct request *rq, bool new_io)\n{\n\tstruct hd_struct *part;\n\tint rw = rq_data_dir(rq);\n\tint cpu;\n\n\tif (!blk_do_io_stat(rq))\n\t\treturn;\n\n\tcpu = part_stat_lock();\n\n\tif (!new_io) {\n\t\tpart = rq->part;\n\t\tpart_stat_inc(cpu, part, merges[rw]);\n\t} else {\n\t\tpart = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));\n\t\tif (!hd_struct_try_get(part)) {\n\t\t\t/*\n\t\t\t * The partition is already being removed,\n\t\t\t * the request will be accounted on the disk only\n\t\t\t *\n\t\t\t * We take a reference on disk->part0 although that\n\t\t\t * partition will never be deleted, so we can treat\n\t\t\t * it as any other partition.\n\t\t\t */\n\t\t\tpart = &rq->rq_disk->part0;\n\t\t\thd_struct_get(part);\n\t\t}\n\t\tpart_round_stats(rq->q, cpu, part);\n\t\tpart_inc_in_flight(rq->q, part, rw);\n\t\trq->part = part;\n\t}\n\n\tpart_stat_unlock();\n}\n\nstatic struct request *elv_next_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\twhile (1) {\n\t\tlist_for_each_entry(rq, &q->queue_head, queuelist) {\n\t\t\tif (blk_pm_allow_request(rq))\n\t\t\t\treturn rq;\n\n\t\t\tif (rq->rq_flags & RQF_SOFTBARRIER)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Flush request is running and flush request isn't queueable\n\t\t * in the drive, we can hold the queue till flush request is\n\t\t * finished. Even we don't do this, driver can't dispatch next\n\t\t * requests and will requeue them. And this can improve\n\t\t * throughput too. For example, we have request flush1, write1,\n\t\t * flush 2. flush1 is dispatched, then queue is hold, write1\n\t\t * isn't inserted to queue. After flush1 is finished, flush2\n\t\t * will be dispatched. Since disk cache is already clean,\n\t\t * flush2 will be finished very soon, so looks like flush2 is\n\t\t * folded to flush1.\n\t\t * Since the queue is hold, a flag is set to indicate the queue\n\t\t * should be restarted later. Please see flush_end_io() for\n\t\t * details.\n\t\t */\n\t\tif (fq->flush_pending_idx != fq->flush_running_idx &&\n\t\t\t\t!queue_flush_queueable(q)) {\n\t\t\tfq->flush_queue_delayed = 1;\n\t\t\treturn NULL;\n\t\t}\n\t\tif (unlikely(blk_queue_bypass(q)) ||\n\t\t    !q->elevator->type->ops.sq.elevator_dispatch_fn(q, 0))\n\t\t\treturn NULL;\n\t}\n}\n\n/**\n * blk_peek_request - peek at the top of a request queue\n * @q: request queue to peek at\n *\n * Description:\n *     Return the request at the top of @q.  The returned request\n *     should be started using blk_start_request() before LLD starts\n *     processing it.\n *\n * Return:\n *     Pointer to the request at the top of @q if available.  Null\n *     otherwise.\n */\nstruct request *blk_peek_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\tint ret;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\twhile ((rq = elv_next_request(q)) != NULL) {\n\t\tif (!(rq->rq_flags & RQF_STARTED)) {\n\t\t\t/*\n\t\t\t * This is the first time the device driver\n\t\t\t * sees this request (possibly after\n\t\t\t * requeueing).  Notify IO scheduler.\n\t\t\t */\n\t\t\tif (rq->rq_flags & RQF_SORTED)\n\t\t\t\telv_activate_rq(q, rq);\n\n\t\t\t/*\n\t\t\t * just mark as started even if we don't start\n\t\t\t * it, a request that has been delayed should\n\t\t\t * not be passed by new incoming requests\n\t\t\t */\n\t\t\trq->rq_flags |= RQF_STARTED;\n\t\t\ttrace_block_rq_issue(q, rq);\n\t\t}\n\n\t\tif (!q->boundary_rq || q->boundary_rq == rq) {\n\t\t\tq->end_sector = rq_end_sector(rq);\n\t\t\tq->boundary_rq = NULL;\n\t\t}\n\n\t\tif (rq->rq_flags & RQF_DONTPREP)\n\t\t\tbreak;\n\n\t\tif (q->dma_drain_size && blk_rq_bytes(rq)) {\n\t\t\t/*\n\t\t\t * make sure space for the drain appears we\n\t\t\t * know we can do this because max_hw_segments\n\t\t\t * has been adjusted to be one fewer than the\n\t\t\t * device can handle\n\t\t\t */\n\t\t\trq->nr_phys_segments++;\n\t\t}\n\n\t\tif (!q->prep_rq_fn)\n\t\t\tbreak;\n\n\t\tret = q->prep_rq_fn(q, rq);\n\t\tif (ret == BLKPREP_OK) {\n\t\t\tbreak;\n\t\t} else if (ret == BLKPREP_DEFER) {\n\t\t\t/*\n\t\t\t * the request may have been (partially) prepped.\n\t\t\t * we need to keep this request in the front to\n\t\t\t * avoid resource deadlock.  RQF_STARTED will\n\t\t\t * prevent other fs requests from passing this one.\n\t\t\t */\n\t\t\tif (q->dma_drain_size && blk_rq_bytes(rq) &&\n\t\t\t    !(rq->rq_flags & RQF_DONTPREP)) {\n\t\t\t\t/*\n\t\t\t\t * remove the space for the drain we added\n\t\t\t\t * so that we don't add it again\n\t\t\t\t */\n\t\t\t\t--rq->nr_phys_segments;\n\t\t\t}\n\n\t\t\trq = NULL;\n\t\t\tbreak;\n\t\t} else if (ret == BLKPREP_KILL || ret == BLKPREP_INVALID) {\n\t\t\trq->rq_flags |= RQF_QUIET;\n\t\t\t/*\n\t\t\t * Mark this request as started so we don't trigger\n\t\t\t * any debug logic in the end I/O path.\n\t\t\t */\n\t\t\tblk_start_request(rq);\n\t\t\t__blk_end_request_all(rq, ret == BLKPREP_INVALID ?\n\t\t\t\t\tBLK_STS_TARGET : BLK_STS_IOERR);\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"%s: bad return=%d\\n\", __func__, ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rq;\n}\nEXPORT_SYMBOL(blk_peek_request);\n\nstatic void blk_dequeue_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tBUG_ON(list_empty(&rq->queuelist));\n\tBUG_ON(ELV_ON_HASH(rq));\n\n\tlist_del_init(&rq->queuelist);\n\n\t/*\n\t * the time frame between a request being removed from the lists\n\t * and to it is freed is accounted as io that is in progress at\n\t * the driver side.\n\t */\n\tif (blk_account_rq(rq))\n\t\tq->in_flight[rq_is_sync(rq)]++;\n}\n\n/**\n * blk_start_request - start request processing on the driver\n * @req: request to dequeue\n *\n * Description:\n *     Dequeue @req and start timeout timer on it.  This hands off the\n *     request to the driver.\n */\nvoid blk_start_request(struct request *req)\n{\n\tlockdep_assert_held(req->q->queue_lock);\n\tWARN_ON_ONCE(req->q->mq_ops);\n\n\tblk_dequeue_request(req);\n\n\tif (test_bit(QUEUE_FLAG_STATS, &req->q->queue_flags)) {\n\t\treq->io_start_time_ns = ktime_get_ns();\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\n\t\treq->throtl_size = blk_rq_sectors(req);\n#endif\n\t\treq->rq_flags |= RQF_STATS;\n\t\trq_qos_issue(req->q, req);\n\t}\n\n\tBUG_ON(blk_rq_is_complete(req));\n\tblk_add_timer(req);\n}\nEXPORT_SYMBOL(blk_start_request);\n\n/**\n * blk_fetch_request - fetch a request from a request queue\n * @q: request queue to fetch a request from\n *\n * Description:\n *     Return the request at the top of @q.  The request is started on\n *     return and LLD can start processing it immediately.\n *\n * Return:\n *     Pointer to the request at the top of @q if available.  Null\n *     otherwise.\n */\nstruct request *blk_fetch_request(struct request_queue *q)\n{\n\tstruct request *rq;\n\n\tlockdep_assert_held(q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\trq = blk_peek_request(q);\n\tif (rq)\n\t\tblk_start_request(rq);\n\treturn rq;\n}\nEXPORT_SYMBOL(blk_fetch_request);\n\n/*\n * Steal bios from a request and add them to a bio list.\n * The request must not have been partially completed before.\n */\nvoid blk_steal_bios(struct bio_list *list, struct request *rq)\n{\n\tif (rq->bio) {\n\t\tif (list->tail)\n\t\t\tlist->tail->bi_next = rq->bio;\n\t\telse\n\t\t\tlist->head = rq->bio;\n\t\tlist->tail = rq->biotail;\n\n\t\trq->bio = NULL;\n\t\trq->biotail = NULL;\n\t}\n\n\trq->__data_len = 0;\n}\nEXPORT_SYMBOL_GPL(blk_steal_bios);\n\n/**\n * blk_update_request - Special helper function for request stacking drivers\n * @req:      the request being processed\n * @error:    block status code\n * @nr_bytes: number of bytes to complete @req\n *\n * Description:\n *     Ends I/O on a number of bytes attached to @req, but doesn't complete\n *     the request structure even if @req doesn't have leftover.\n *     If @req has leftover, sets it up for the next range of segments.\n *\n *     This special helper function is only for request stacking drivers\n *     (e.g. request-based dm) so that they can handle partial completion.\n *     Actual device drivers should use blk_end_request instead.\n *\n *     Passing the result of blk_rq_bytes() as @nr_bytes guarantees\n *     %false return from this function.\n *\n * Note:\n *\tThe RQF_SPECIAL_PAYLOAD flag is ignored on purpose in both\n *\tblk_rq_bytes() and in blk_update_request().\n *\n * Return:\n *     %false - this request doesn't have any more data\n *     %true  - this request has more data\n **/\nbool blk_update_request(struct request *req, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tint total_bytes;\n\n\ttrace_block_rq_complete(req, blk_status_to_errno(error), nr_bytes);\n\n\tif (!req->bio)\n\t\treturn false;\n\n\tif (unlikely(error && !blk_rq_is_passthrough(req) &&\n\t\t     !(req->rq_flags & RQF_QUIET)))\n\t\tprint_req_error(req, error);\n\n\tblk_account_io_completion(req, nr_bytes);\n\n\ttotal_bytes = 0;\n\twhile (req->bio) {\n\t\tstruct bio *bio = req->bio;\n\t\tunsigned bio_bytes = min(bio->bi_iter.bi_size, nr_bytes);\n\n\t\tif (bio_bytes == bio->bi_iter.bi_size)\n\t\t\treq->bio = bio->bi_next;\n\n\t\t/* Completion has already been traced */\n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\t\treq_bio_endio(req, bio, bio_bytes, error);\n\n\t\ttotal_bytes += bio_bytes;\n\t\tnr_bytes -= bio_bytes;\n\n\t\tif (!nr_bytes)\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * completely done\n\t */\n\tif (!req->bio) {\n\t\t/*\n\t\t * Reset counters so that the request stacking driver\n\t\t * can find how many bytes remain in the request\n\t\t * later.\n\t\t */\n\t\treq->__data_len = 0;\n\t\treturn false;\n\t}\n\n\treq->__data_len -= total_bytes;\n\n\t/* update sector only for requests with clear definition of sector */\n\tif (!blk_rq_is_passthrough(req))\n\t\treq->__sector += total_bytes >> 9;\n\n\t/* mixed attributes always follow the first bio */\n\tif (req->rq_flags & RQF_MIXED_MERGE) {\n\t\treq->cmd_flags &= ~REQ_FAILFAST_MASK;\n\t\treq->cmd_flags |= req->bio->bi_opf & REQ_FAILFAST_MASK;\n\t}\n\n\tif (!(req->rq_flags & RQF_SPECIAL_PAYLOAD)) {\n\t\t/*\n\t\t * If total number of sectors is less than the first segment\n\t\t * size, something has gone terribly wrong.\n\t\t */\n\t\tif (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {\n\t\t\tblk_dump_rq_flags(req, \"request botched\");\n\t\t\treq->__data_len = blk_rq_cur_bytes(req);\n\t\t}\n\n\t\t/* recalculate the number of segments */\n\t\tblk_recalc_rq_segments(req);\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(blk_update_request);\n\nstatic bool blk_update_bidi_request(struct request *rq, blk_status_t error,\n\t\t\t\t    unsigned int nr_bytes,\n\t\t\t\t    unsigned int bidi_bytes)\n{\n\tif (blk_update_request(rq, error, nr_bytes))\n\t\treturn true;\n\n\t/* Bidi request must be completed as a whole */\n\tif (unlikely(blk_bidi_rq(rq)) &&\n\t    blk_update_request(rq->next_rq, error, bidi_bytes))\n\t\treturn true;\n\n\tif (blk_queue_add_random(rq->q))\n\t\tadd_disk_randomness(rq->rq_disk);\n\n\treturn false;\n}\n\n/**\n * blk_unprep_request - unprepare a request\n * @req:\tthe request\n *\n * This function makes a request ready for complete resubmission (or\n * completion).  It happens only after all error handling is complete,\n * so represents the appropriate moment to deallocate any resources\n * that were allocated to the request in the prep_rq_fn.  The queue\n * lock is held when calling this.\n */\nvoid blk_unprep_request(struct request *req)\n{\n\tstruct request_queue *q = req->q;\n\n\treq->rq_flags &= ~RQF_DONTPREP;\n\tif (q->unprep_rq_fn)\n\t\tq->unprep_rq_fn(q, req);\n}\nEXPORT_SYMBOL_GPL(blk_unprep_request);\n\nvoid blk_finish_request(struct request *req, blk_status_t error)\n{\n\tstruct request_queue *q = req->q;\n\tu64 now = ktime_get_ns();\n\n\tlockdep_assert_held(req->q->queue_lock);\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (req->rq_flags & RQF_STATS)\n\t\tblk_stat_add(req, now);\n\n\tif (req->rq_flags & RQF_QUEUED)\n\t\tblk_queue_end_tag(q, req);\n\n\tBUG_ON(blk_queued_rq(req));\n\n\tif (unlikely(laptop_mode) && !blk_rq_is_passthrough(req))\n\t\tlaptop_io_completion(req->q->backing_dev_info);\n\n\tblk_delete_timer(req);\n\n\tif (req->rq_flags & RQF_DONTPREP)\n\t\tblk_unprep_request(req);\n\n\tblk_account_io_done(req, now);\n\n\tif (req->end_io) {\n\t\trq_qos_done(q, req);\n\t\treq->end_io(req, error);\n\t} else {\n\t\tif (blk_bidi_rq(req))\n\t\t\t__blk_put_request(req->next_rq->q, req->next_rq);\n\n\t\t__blk_put_request(q, req);\n\t}\n}\nEXPORT_SYMBOL(blk_finish_request);\n\n/**\n * blk_end_bidi_request - Complete a bidi request\n * @rq:         the request to complete\n * @error:      block status code\n * @nr_bytes:   number of bytes to complete @rq\n * @bidi_bytes: number of bytes to complete @rq->next_rq\n *\n * Description:\n *     Ends I/O on a number of bytes attached to @rq and @rq->next_rq.\n *     Drivers that supports bidi can safely call this member for any\n *     type of request, bidi or uni.  In the later case @bidi_bytes is\n *     just ignored.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nstatic bool blk_end_bidi_request(struct request *rq, blk_status_t error,\n\t\t\t\t unsigned int nr_bytes, unsigned int bidi_bytes)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(q->mq_ops);\n\n\tif (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))\n\t\treturn true;\n\n\tspin_lock_irqsave(q->queue_lock, flags);\n\tblk_finish_request(rq, error);\n\tspin_unlock_irqrestore(q->queue_lock, flags);\n\n\treturn false;\n}\n\n/**\n * __blk_end_bidi_request - Complete a bidi request with queue lock held\n * @rq:         the request to complete\n * @error:      block status code\n * @nr_bytes:   number of bytes to complete @rq\n * @bidi_bytes: number of bytes to complete @rq->next_rq\n *\n * Description:\n *     Identical to blk_end_bidi_request() except that queue lock is\n *     assumed to be locked on entry and remains so on return.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nstatic bool __blk_end_bidi_request(struct request *rq, blk_status_t error,\n\t\t\t\t   unsigned int nr_bytes, unsigned int bidi_bytes)\n{\n\tlockdep_assert_held(rq->q->queue_lock);\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\n\tif (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))\n\t\treturn true;\n\n\tblk_finish_request(rq, error);\n\n\treturn false;\n}\n\n/**\n * blk_end_request - Helper function for drivers to complete the request.\n * @rq:       the request being processed\n * @error:    block status code\n * @nr_bytes: number of bytes to complete\n *\n * Description:\n *     Ends I/O on a number of bytes attached to @rq.\n *     If @rq has leftover, sets it up for the next range of segments.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nbool blk_end_request(struct request *rq, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\treturn blk_end_bidi_request(rq, error, nr_bytes, 0);\n}\nEXPORT_SYMBOL(blk_end_request);\n\n/**\n * blk_end_request_all - Helper function for drives to finish the request.\n * @rq: the request to finish\n * @error: block status code\n *\n * Description:\n *     Completely finish @rq.\n */\nvoid blk_end_request_all(struct request *rq, blk_status_t error)\n{\n\tbool pending;\n\tunsigned int bidi_bytes = 0;\n\n\tif (unlikely(blk_bidi_rq(rq)))\n\t\tbidi_bytes = blk_rq_bytes(rq->next_rq);\n\n\tpending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);\n\tBUG_ON(pending);\n}\nEXPORT_SYMBOL(blk_end_request_all);\n\n/**\n * __blk_end_request - Helper function for drivers to complete the request.\n * @rq:       the request being processed\n * @error:    block status code\n * @nr_bytes: number of bytes to complete\n *\n * Description:\n *     Must be called with queue lock held unlike blk_end_request().\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n **/\nbool __blk_end_request(struct request *rq, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tlockdep_assert_held(rq->q->queue_lock);\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\n\treturn __blk_end_bidi_request(rq, error, nr_bytes, 0);\n}\nEXPORT_SYMBOL(__blk_end_request);\n\n/**\n * __blk_end_request_all - Helper function for drives to finish the request.\n * @rq: the request to finish\n * @error:    block status code\n *\n * Description:\n *     Completely finish @rq.  Must be called with queue lock held.\n */\nvoid __blk_end_request_all(struct request *rq, blk_status_t error)\n{\n\tbool pending;\n\tunsigned int bidi_bytes = 0;\n\n\tlockdep_assert_held(rq->q->queue_lock);\n\tWARN_ON_ONCE(rq->q->mq_ops);\n\n\tif (unlikely(blk_bidi_rq(rq)))\n\t\tbidi_bytes = blk_rq_bytes(rq->next_rq);\n\n\tpending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);\n\tBUG_ON(pending);\n}\nEXPORT_SYMBOL(__blk_end_request_all);\n\n/**\n * __blk_end_request_cur - Helper function to finish the current request chunk.\n * @rq: the request to finish the current chunk for\n * @error:    block status code\n *\n * Description:\n *     Complete the current consecutively mapped chunk from @rq.  Must\n *     be called with queue lock held.\n *\n * Return:\n *     %false - we are done with this request\n *     %true  - still buffers pending for this request\n */\nbool __blk_end_request_cur(struct request *rq, blk_status_t error)\n{\n\treturn __blk_end_request(rq, error, blk_rq_cur_bytes(rq));\n}\nEXPORT_SYMBOL(__blk_end_request_cur);\n\nvoid blk_rq_bio_prep(struct request_queue *q, struct request *rq,\n\t\t     struct bio *bio)\n{\n\tif (bio_has_data(bio))\n\t\trq->nr_phys_segments = bio_phys_segments(q, bio);\n\telse if (bio_op(bio) == REQ_OP_DISCARD)\n\t\trq->nr_phys_segments = 1;\n\n\trq->__data_len = bio->bi_iter.bi_size;\n\trq->bio = rq->biotail = bio;\n\n\tif (bio->bi_disk)\n\t\trq->rq_disk = bio->bi_disk;\n}\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\n/**\n * rq_flush_dcache_pages - Helper function to flush all pages in a request\n * @rq: the request to be flushed\n *\n * Description:\n *     Flush all pages in @rq.\n */\nvoid rq_flush_dcache_pages(struct request *rq)\n{\n\tstruct req_iterator iter;\n\tstruct bio_vec bvec;\n\n\trq_for_each_segment(bvec, rq, iter)\n\t\tflush_dcache_page(bvec.bv_page);\n}\nEXPORT_SYMBOL_GPL(rq_flush_dcache_pages);\n#endif\n\n/**\n * blk_lld_busy - Check if underlying low-level drivers of a device are busy\n * @q : the queue of the device being checked\n *\n * Description:\n *    Check if underlying low-level drivers of a device are busy.\n *    If the drivers want to export their busy state, they must set own\n *    exporting function using blk_queue_lld_busy() first.\n *\n *    Basically, this function is used only by request stacking drivers\n *    to stop dispatching requests to underlying devices when underlying\n *    devices are busy.  This behavior helps more I/O merging on the queue\n *    of the request stacking driver and prevents I/O throughput regression\n *    on burst I/O load.\n *\n * Return:\n *    0 - Not busy (The request stacking driver should dispatch request)\n *    1 - Busy (The request stacking driver should stop dispatching request)\n */\nint blk_lld_busy(struct request_queue *q)\n{\n\tif (q->lld_busy_fn)\n\t\treturn q->lld_busy_fn(q);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blk_lld_busy);\n\n/**\n * blk_rq_unprep_clone - Helper function to free all bios in a cloned request\n * @rq: the clone request to be cleaned up\n *\n * Description:\n *     Free all bios in @rq for a cloned request.\n */\nvoid blk_rq_unprep_clone(struct request *rq)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = rq->bio) != NULL) {\n\t\trq->bio = bio->bi_next;\n\n\t\tbio_put(bio);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_rq_unprep_clone);\n\n/*\n * Copy attributes of the original request to the clone request.\n * The actual data parts (e.g. ->cmd, ->sense) are not copied.\n */\nstatic void __blk_rq_prep_clone(struct request *dst, struct request *src)\n{\n\tdst->cpu = src->cpu;\n\tdst->__sector = blk_rq_pos(src);\n\tdst->__data_len = blk_rq_bytes(src);\n\tif (src->rq_flags & RQF_SPECIAL_PAYLOAD) {\n\t\tdst->rq_flags |= RQF_SPECIAL_PAYLOAD;\n\t\tdst->special_vec = src->special_vec;\n\t}\n\tdst->nr_phys_segments = src->nr_phys_segments;\n\tdst->ioprio = src->ioprio;\n\tdst->extra_len = src->extra_len;\n}\n\n/**\n * blk_rq_prep_clone - Helper function to setup clone request\n * @rq: the request to be setup\n * @rq_src: original request to be cloned\n * @bs: bio_set that bios for clone are allocated from\n * @gfp_mask: memory allocation mask for bio\n * @bio_ctr: setup function to be called for each clone bio.\n *           Returns %0 for success, non %0 for failure.\n * @data: private data to be passed to @bio_ctr\n *\n * Description:\n *     Clones bios in @rq_src to @rq, and copies attributes of @rq_src to @rq.\n *     The actual data parts of @rq_src (e.g. ->cmd, ->sense)\n *     are not copied, and copying such parts is the caller's responsibility.\n *     Also, pages which the original bios are pointing to are not copied\n *     and the cloned bios just point same pages.\n *     So cloned bios must be completed before original bios, which means\n *     the caller must complete @rq before @rq_src.\n */\nint blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\t      struct bio_set *bs, gfp_t gfp_mask,\n\t\t      int (*bio_ctr)(struct bio *, struct bio *, void *),\n\t\t      void *data)\n{\n\tstruct bio *bio, *bio_src;\n\n\tif (!bs)\n\t\tbs = &fs_bio_set;\n\n\t__rq_for_each_bio(bio_src, rq_src) {\n\t\tbio = bio_clone_fast(bio_src, gfp_mask, bs);\n\t\tif (!bio)\n\t\t\tgoto free_and_out;\n\n\t\tif (bio_ctr && bio_ctr(bio, bio_src, data))\n\t\t\tgoto free_and_out;\n\n\t\tif (rq->bio) {\n\t\t\trq->biotail->bi_next = bio;\n\t\t\trq->biotail = bio;\n\t\t} else\n\t\t\trq->bio = rq->biotail = bio;\n\t}\n\n\t__blk_rq_prep_clone(rq, rq_src);\n\n\treturn 0;\n\nfree_and_out:\n\tif (bio)\n\t\tbio_put(bio);\n\tblk_rq_unprep_clone(rq);\n\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(blk_rq_prep_clone);\n\nint kblockd_schedule_work(struct work_struct *work)\n{\n\treturn queue_work(kblockd_workqueue, work);\n}\nEXPORT_SYMBOL(kblockd_schedule_work);\n\nint kblockd_schedule_work_on(int cpu, struct work_struct *work)\n{\n\treturn queue_work_on(cpu, kblockd_workqueue, work);\n}\nEXPORT_SYMBOL(kblockd_schedule_work_on);\n\nint kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork,\n\t\t\t\tunsigned long delay)\n{\n\treturn mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);\n}\nEXPORT_SYMBOL(kblockd_mod_delayed_work_on);\n\n/**\n * blk_start_plug - initialize blk_plug and track it inside the task_struct\n * @plug:\tThe &struct blk_plug that needs to be initialized\n *\n * Description:\n *   Tracking blk_plug inside the task_struct will help with auto-flushing the\n *   pending I/O should the task end up blocking between blk_start_plug() and\n *   blk_finish_plug(). This is important from a performance perspective, but\n *   also ensures that we don't deadlock. For instance, if the task is blocking\n *   for a memory allocation, memory reclaim could end up wanting to free a\n *   page belonging to that request that is currently residing in our private\n *   plug. By flushing the pending I/O when the process goes to sleep, we avoid\n *   this kind of deadlock.\n */\nvoid blk_start_plug(struct blk_plug *plug)\n{\n\tstruct task_struct *tsk = current;\n\n\t/*\n\t * If this is a nested plug, don't actually assign it.\n\t */\n\tif (tsk->plug)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&plug->list);\n\tINIT_LIST_HEAD(&plug->mq_list);\n\tINIT_LIST_HEAD(&plug->cb_list);\n\t/*\n\t * Store ordering should not be needed here, since a potential\n\t * preempt will imply a full memory barrier\n\t */\n\ttsk->plug = plug;\n}\nEXPORT_SYMBOL(blk_start_plug);\n\nstatic int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)\n{\n\tstruct request *rqa = container_of(a, struct request, queuelist);\n\tstruct request *rqb = container_of(b, struct request, queuelist);\n\n\treturn !(rqa->q < rqb->q ||\n\t\t(rqa->q == rqb->q && blk_rq_pos(rqa) < blk_rq_pos(rqb)));\n}\n\n/*\n * If 'from_schedule' is true, then postpone the dispatch of requests\n * until a safe kblockd context. We due this to avoid accidental big\n * additional stack usage in driver dispatch, in places where the originally\n * plugger did not intend it.\n */\nstatic void queue_unplugged(struct request_queue *q, unsigned int depth,\n\t\t\t    bool from_schedule)\n\t__releases(q->queue_lock)\n{\n\tlockdep_assert_held(q->queue_lock);\n\n\ttrace_block_unplug(q, depth, !from_schedule);\n\n\tif (from_schedule)\n\t\tblk_run_queue_async(q);\n\telse\n\t\t__blk_run_queue(q);\n\tspin_unlock_irq(q->queue_lock);\n}\n\nstatic void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)\n{\n\tLIST_HEAD(callbacks);\n\n\twhile (!list_empty(&plug->cb_list)) {\n\t\tlist_splice_init(&plug->cb_list, &callbacks);\n\n\t\twhile (!list_empty(&callbacks)) {\n\t\t\tstruct blk_plug_cb *cb = list_first_entry(&callbacks,\n\t\t\t\t\t\t\t  struct blk_plug_cb,\n\t\t\t\t\t\t\t  list);\n\t\t\tlist_del(&cb->list);\n\t\t\tcb->callback(cb, from_schedule);\n\t\t}\n\t}\n}\n\nstruct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,\n\t\t\t\t      int size)\n{\n\tstruct blk_plug *plug = current->plug;\n\tstruct blk_plug_cb *cb;\n\n\tif (!plug)\n\t\treturn NULL;\n\n\tlist_for_each_entry(cb, &plug->cb_list, list)\n\t\tif (cb->callback == unplug && cb->data == data)\n\t\t\treturn cb;\n\n\t/* Not currently on the callback list */\n\tBUG_ON(size < sizeof(*cb));\n\tcb = kzalloc(size, GFP_ATOMIC);\n\tif (cb) {\n\t\tcb->data = data;\n\t\tcb->callback = unplug;\n\t\tlist_add(&cb->list, &plug->cb_list);\n\t}\n\treturn cb;\n}\nEXPORT_SYMBOL(blk_check_plugged);\n\nvoid blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)\n{\n\tstruct request_queue *q;\n\tstruct request *rq;\n\tLIST_HEAD(list);\n\tunsigned int depth;\n\n\tflush_plug_callbacks(plug, from_schedule);\n\n\tif (!list_empty(&plug->mq_list))\n\t\tblk_mq_flush_plug_list(plug, from_schedule);\n\n\tif (list_empty(&plug->list))\n\t\treturn;\n\n\tlist_splice_init(&plug->list, &list);\n\n\tlist_sort(NULL, &list, plug_rq_cmp);\n\n\tq = NULL;\n\tdepth = 0;\n\n\twhile (!list_empty(&list)) {\n\t\trq = list_entry_rq(list.next);\n\t\tlist_del_init(&rq->queuelist);\n\t\tBUG_ON(!rq->q);\n\t\tif (rq->q != q) {\n\t\t\t/*\n\t\t\t * This drops the queue lock\n\t\t\t */\n\t\t\tif (q)\n\t\t\t\tqueue_unplugged(q, depth, from_schedule);\n\t\t\tq = rq->q;\n\t\t\tdepth = 0;\n\t\t\tspin_lock_irq(q->queue_lock);\n\t\t}\n\n\t\t/*\n\t\t * Short-circuit if @q is dead\n\t\t */\n\t\tif (unlikely(blk_queue_dying(q))) {\n\t\t\t__blk_end_request_all(rq, BLK_STS_IOERR);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * rq is already accounted, so use raw insert\n\t\t */\n\t\tif (op_is_flush(rq->cmd_flags))\n\t\t\t__elv_add_request(q, rq, ELEVATOR_INSERT_FLUSH);\n\t\telse\n\t\t\t__elv_add_request(q, rq, ELEVATOR_INSERT_SORT_MERGE);\n\n\t\tdepth++;\n\t}\n\n\t/*\n\t * This drops the queue lock\n\t */\n\tif (q)\n\t\tqueue_unplugged(q, depth, from_schedule);\n}\n\nvoid blk_finish_plug(struct blk_plug *plug)\n{\n\tif (plug != current->plug)\n\t\treturn;\n\tblk_flush_plug_list(plug, false);\n\n\tcurrent->plug = NULL;\n}\nEXPORT_SYMBOL(blk_finish_plug);\n\n#ifdef CONFIG_PM\n/**\n * blk_pm_runtime_init - Block layer runtime PM initialization routine\n * @q: the queue of the device\n * @dev: the device the queue belongs to\n *\n * Description:\n *    Initialize runtime-PM-related fields for @q and start auto suspend for\n *    @dev. Drivers that want to take advantage of request-based runtime PM\n *    should call this function after @dev has been initialized, and its\n *    request queue @q has been allocated, and runtime PM for it can not happen\n *    yet(either due to disabled/forbidden or its usage_count > 0). In most\n *    cases, driver should call this function before any I/O has taken place.\n *\n *    This function takes care of setting up using auto suspend for the device,\n *    the autosuspend delay is set to -1 to make runtime suspend impossible\n *    until an updated value is either set by user or by driver. Drivers do\n *    not need to touch other autosuspend settings.\n *\n *    The block layer runtime PM is request based, so only works for drivers\n *    that use request as their IO unit instead of those directly use bio's.\n */\nvoid blk_pm_runtime_init(struct request_queue *q, struct device *dev)\n{\n\t/* not support for RQF_PM and ->rpm_status in blk-mq yet */\n\tif (q->mq_ops)\n\t\treturn;\n\n\tq->dev = dev;\n\tq->rpm_status = RPM_ACTIVE;\n\tpm_runtime_set_autosuspend_delay(q->dev, -1);\n\tpm_runtime_use_autosuspend(q->dev);\n}\nEXPORT_SYMBOL(blk_pm_runtime_init);\n\n/**\n * blk_pre_runtime_suspend - Pre runtime suspend check\n * @q: the queue of the device\n *\n * Description:\n *    This function will check if runtime suspend is allowed for the device\n *    by examining if there are any requests pending in the queue. If there\n *    are requests pending, the device can not be runtime suspended; otherwise,\n *    the queue's status will be updated to SUSPENDING and the driver can\n *    proceed to suspend the device.\n *\n *    For the not allowed case, we mark last busy for the device so that\n *    runtime PM core will try to autosuspend it some time later.\n *\n *    This function should be called near the start of the device's\n *    runtime_suspend callback.\n *\n * Return:\n *    0\t\t- OK to runtime suspend the device\n *    -EBUSY\t- Device should not be runtime suspended\n */\nint blk_pre_runtime_suspend(struct request_queue *q)\n{\n\tint ret = 0;\n\n\tif (!q->dev)\n\t\treturn ret;\n\n\tspin_lock_irq(q->queue_lock);\n\tif (q->nr_pending) {\n\t\tret = -EBUSY;\n\t\tpm_runtime_mark_last_busy(q->dev);\n\t} else {\n\t\tq->rpm_status = RPM_SUSPENDING;\n\t}\n\tspin_unlock_irq(q->queue_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_pre_runtime_suspend);\n\n/**\n * blk_post_runtime_suspend - Post runtime suspend processing\n * @q: the queue of the device\n * @err: return value of the device's runtime_suspend function\n *\n * Description:\n *    Update the queue's runtime status according to the return value of the\n *    device's runtime suspend function and mark last busy for the device so\n *    that PM core will try to auto suspend the device at a later time.\n *\n *    This function should be called near the end of the device's\n *    runtime_suspend callback.\n */\nvoid blk_post_runtime_suspend(struct request_queue *q, int err)\n{\n\tif (!q->dev)\n\t\treturn;\n\n\tspin_lock_irq(q->queue_lock);\n\tif (!err) {\n\t\tq->rpm_status = RPM_SUSPENDED;\n\t} else {\n\t\tq->rpm_status = RPM_ACTIVE;\n\t\tpm_runtime_mark_last_busy(q->dev);\n\t}\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_post_runtime_suspend);\n\n/**\n * blk_pre_runtime_resume - Pre runtime resume processing\n * @q: the queue of the device\n *\n * Description:\n *    Update the queue's runtime status to RESUMING in preparation for the\n *    runtime resume of the device.\n *\n *    This function should be called near the start of the device's\n *    runtime_resume callback.\n */\nvoid blk_pre_runtime_resume(struct request_queue *q)\n{\n\tif (!q->dev)\n\t\treturn;\n\n\tspin_lock_irq(q->queue_lock);\n\tq->rpm_status = RPM_RESUMING;\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_pre_runtime_resume);\n\n/**\n * blk_post_runtime_resume - Post runtime resume processing\n * @q: the queue of the device\n * @err: return value of the device's runtime_resume function\n *\n * Description:\n *    Update the queue's runtime status according to the return value of the\n *    device's runtime_resume function. If it is successfully resumed, process\n *    the requests that are queued into the device's queue when it is resuming\n *    and then mark last busy and initiate autosuspend for it.\n *\n *    This function should be called near the end of the device's\n *    runtime_resume callback.\n */\nvoid blk_post_runtime_resume(struct request_queue *q, int err)\n{\n\tif (!q->dev)\n\t\treturn;\n\n\tspin_lock_irq(q->queue_lock);\n\tif (!err) {\n\t\tq->rpm_status = RPM_ACTIVE;\n\t\t__blk_run_queue(q);\n\t\tpm_runtime_mark_last_busy(q->dev);\n\t\tpm_request_autosuspend(q->dev);\n\t} else {\n\t\tq->rpm_status = RPM_SUSPENDED;\n\t}\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_post_runtime_resume);\n\n/**\n * blk_set_runtime_active - Force runtime status of the queue to be active\n * @q: the queue of the device\n *\n * If the device is left runtime suspended during system suspend the resume\n * hook typically resumes the device and corrects runtime status\n * accordingly. However, that does not affect the queue runtime PM status\n * which is still \"suspended\". This prevents processing requests from the\n * queue.\n *\n * This function can be used in driver's resume hook to correct queue\n * runtime PM status and re-enable peeking requests from the queue. It\n * should be called before first request is added to the queue.\n */\nvoid blk_set_runtime_active(struct request_queue *q)\n{\n\tspin_lock_irq(q->queue_lock);\n\tq->rpm_status = RPM_ACTIVE;\n\tpm_runtime_mark_last_busy(q->dev);\n\tpm_request_autosuspend(q->dev);\n\tspin_unlock_irq(q->queue_lock);\n}\nEXPORT_SYMBOL(blk_set_runtime_active);\n#endif\n\nint __init blk_dev_init(void)\n{\n\tBUILD_BUG_ON(REQ_OP_LAST >= (1 << REQ_OP_BITS));\n\tBUILD_BUG_ON(REQ_OP_BITS + REQ_FLAG_BITS > 8 *\n\t\t\tFIELD_SIZEOF(struct request, cmd_flags));\n\tBUILD_BUG_ON(REQ_OP_BITS + REQ_FLAG_BITS > 8 *\n\t\t\tFIELD_SIZEOF(struct bio, bi_opf));\n\n\t/* used for unplugging and affects IO latency/throughput - HIGHPRI */\n\tkblockd_workqueue = alloc_workqueue(\"kblockd\",\n\t\t\t\t\t    WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);\n\tif (!kblockd_workqueue)\n\t\tpanic(\"Failed to create kblockd\\n\");\n\n\trequest_cachep = kmem_cache_create(\"blkdev_requests\",\n\t\t\tsizeof(struct request), 0, SLAB_PANIC, NULL);\n\n\tblk_requestq_cachep = kmem_cache_create(\"request_queue\",\n\t\t\tsizeof(struct request_queue), 0, SLAB_PANIC, NULL);\n\n#ifdef CONFIG_DEBUG_FS\n\tblk_debugfs_root = debugfs_create_dir(\"block\", NULL);\n#endif\n\n\treturn 0;\n}\n"], "filenames": ["block/blk-core.c"], "buggy_code_start_loc": [1186], "buggy_code_end_loc": [1186], "fixing_code_start_loc": [1187], "fixing_code_end_loc": [1188], "type": "CWE-416", "message": "An issue was discovered in the Linux kernel before 4.18.7. In block/blk-core.c, there is an __blk_drain_queue() use-after-free because a certain error case is mishandled.", "other": {"cve": {"id": "CVE-2018-20856", "sourceIdentifier": "cve@mitre.org", "published": "2019-07-26T05:15:10.517", "lastModified": "2019-08-13T19:15:13.920", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel before 4.18.7. In block/blk-core.c, there is an __blk_drain_queue() use-after-free because a certain error case is mishandled."}, {"lang": "es", "value": "El servidor web en ZENworks Configuration Management (ZCM) de Novell  versi\u00f3n 10.3 y versi\u00f3n 11.2 anteriores a 11.2.4, no realiza apropiadamente la autenticaci\u00f3n para el archivo zenworks/jsp/index.jsp, lo que permite a los atacantes remotos realizar ataques de salto de directorio y en consecuencia cargar y ejecutar programas arbitrarios, por medio de una petici\u00f3n al puerto TCP 443."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.18.7", "matchCriteriaId": "92C6CD12-AE92-4AFB-91FA-F2A97F5537D8"}]}]}], "references": [{"url": "http://packetstormsecurity.com/files/154059/Slackware-Security-Advisory-Slackware-14.2-kernel-Updates.html", "source": "cve@mitre.org"}, {"url": "http://packetstormsecurity.com/files/154408/Kernel-Live-Patch-Security-Notice-LSN-0055-1.html", "source": "cve@mitre.org"}, {"url": "http://packetstormsecurity.com/files/154951/Kernel-Live-Patch-Security-Notice-LSN-0058-1.html", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:3055", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:3076", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:3089", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:3217", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0100", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0103", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0543", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0664", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2020:0698", "source": "cve@mitre.org"}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.18.7", "source": "cve@mitre.org", "tags": ["Mailing List", "Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=54648cf1ec2d7f4b6a71767799c45676a138ca24", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/54648cf1ec2d7f4b6a71767799c45676a138ca24", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/08/msg00017.html", "source": "cve@mitre.org"}, {"url": "https://seclists.org/bugtraq/2019/Aug/18", "source": "cve@mitre.org"}, {"url": "https://seclists.org/bugtraq/2019/Aug/26", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20190905-0002/", "source": "cve@mitre.org"}, {"url": "https://support.f5.com/csp/article/K14673240?utm_source=f5support&amp;utm_medium=RSS", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4094-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4116-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2019/dsa-4497", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/54648cf1ec2d7f4b6a71767799c45676a138ca24"}}