{"buggy_code": [".. Licensed to the Apache Software Foundation (ASF) under one\n.. or more contributor license agreements.  See the NOTICE file\n.. distributed with this work for additional information\n.. regarding copyright ownership.  The ASF licenses this file\n.. to you under the Apache License, Version 2.0 (the\n.. \"License\"); you may not use this file except in compliance\n.. with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n.. software distributed under the License is distributed on an\n.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n.. KIND, either express or implied.  See the License for the\n.. specific language governing permissions and limitations\n.. under the License.\n\n.. currentmodule:: pyarrow\n.. _extending_types:\n\nExtending pyarrow\n=================\n\n.. _arrow_array_protocol:\n\nControlling conversion to pyarrow.Array with the ``__arrow_array__`` protocol\n-----------------------------------------------------------------------------\n\nThe :func:`pyarrow.array` function has built-in support for Python sequences,\nnumpy arrays and pandas 1D objects (Series, Index, Categorical, ..) to convert\nthose to Arrow arrays. This can be extended for other array-like objects\nby implementing the ``__arrow_array__`` method (similar to numpy's ``__array__``\nprotocol).\n\nFor example, to support conversion of your duck array class to an Arrow array,\ndefine the ``__arrow_array__`` method to return an Arrow array::\n\n    class MyDuckArray:\n\n        ...\n\n        def __arrow_array__(self, type=None):\n            # convert the underlying array values to a pyarrow Array\n            import pyarrow\n            return pyarrow.array(..., type=type)\n\nThe ``__arrow_array__`` method takes an optional `type` keyword which is passed\nthrough from :func:`pyarrow.array`. The method is allowed to return either\na :class:`~pyarrow.Array` or a :class:`~pyarrow.ChunkedArray`.\n\n.. note::\n\n    For a more general way to control the conversion of Python objects to Arrow\n    data consider the :doc:`/format/CDataInterface/PyCapsuleInterface`. It is\n    not specific to PyArrow and supports converting other objects such as tables\n    and schemas.\n\n\nDefining extension types (\"user-defined types\")\n-----------------------------------------------\n\nArrow has the notion of extension types in the metadata specification as a\npossibility to extend the built-in types. This is done by annotating any of the\nbuilt-in Arrow logical types (the \"storage type\") with a custom type name and\noptional serialized representation (\"ARROW:extension:name\" and\n\"ARROW:extension:metadata\" keys in the Field\u2019s custom_metadata of an IPC\nmessage).\nSee the :ref:`format_metadata_extension_types` section of the metadata\nspecification for more details.\n\nPyarrow allows you to define such extension types from Python.\n\nThere are currently two ways:\n\n* Subclassing :class:`PyExtensionType`: the (de)serialization is based on pickle.\n  This is a good option for an extension type that is only used from Python.\n* Subclassing :class:`ExtensionType`: this allows to give a custom\n  Python-independent name and serialized metadata, that can potentially be\n  recognized by other (non-Python) Arrow implementations such as PySpark.\n\nFor example, we could define a custom UUID type for 128-bit numbers which can\nbe represented as ``FixedSizeBinary`` type with 16 bytes.\nUsing the first approach, we create a ``UuidType`` subclass, and implement the\n``__reduce__`` method to ensure the class can be properly pickled::\n\n    class UuidType(pa.PyExtensionType):\n\n        def __init__(self):\n            pa.PyExtensionType.__init__(self, pa.binary(16))\n\n        def __reduce__(self):\n            return UuidType, ()\n\nThis can now be used to create arrays and tables holding the extension type::\n\n    >>> uuid_type = UuidType()\n    >>> uuid_type.extension_name\n    'arrow.py_extension_type'\n    >>> uuid_type.storage_type\n    FixedSizeBinaryType(fixed_size_binary[16])\n\n    >>> import uuid\n    >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)], pa.binary(16))\n    >>> arr = pa.ExtensionArray.from_storage(uuid_type, storage_array)\n    >>> arr\n    <pyarrow.lib.ExtensionArray object at 0x7f75c2f300a0>\n    [\n      A6861959108644B797664AEEE686B682,\n      718747F48E5F4058A7261E2B6B228BE8,\n      7FE201227D624D96A5CD8639DEF2A68B,\n      C6CA8C7F95744BFD9462A40B3F57A86C\n    ]\n\nThis array can be included in RecordBatches, sent over IPC and received in\nanother Python process. The custom UUID type will be preserved there, as long\nas the definition of the class is available (the type can be unpickled).\n\nFor example, creating a RecordBatch and writing it to a stream using the\nIPC protocol::\n\n    >>> batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n    >>> sink = pa.BufferOutputStream()\n    >>> with pa.RecordBatchStreamWriter(sink, batch.schema) as writer:\n    ...    writer.write_batch(batch)\n    >>> buf = sink.getvalue()\n\nand then reading it back yields the proper type::\n\n    >>> with pa.ipc.open_stream(buf) as reader:\n    ...    result = reader.read_all()\n    >>> result.column('ext').type\n    UuidType(extension<arrow.py_extension_type>)\n\nWe can define the same type using the other option::\n\n    class UuidType(pa.ExtensionType):\n\n        def __init__(self):\n            pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n\n        def __arrow_ext_serialize__(self):\n            # since we don't have a parameterized type, we don't need extra\n            # metadata to be deserialized\n            return b''\n\n        @classmethod\n        def __arrow_ext_deserialize__(self, storage_type, serialized):\n            # return an instance of this subclass given the serialized\n            # metadata.\n            return UuidType()\n\nThis is a slightly longer implementation (you need to implement the special\nmethods ``__arrow_ext_serialize__`` and ``__arrow_ext_deserialize__``), and the\nextension type needs to be registered to be received through IPC (using\n:func:`register_extension_type`), but it has\nnow a unique name::\n\n    >>> uuid_type = UuidType()\n    >>> uuid_type.extension_name\n    'my_package.uuid'\n\n    >>> pa.register_extension_type(uuid_type)\n\nThe receiving application doesn't need to be Python but can still recognize\nthe extension type as a \"uuid\" type, if it has implemented its own extension\ntype to receive it.\nIf the type is not registered in the receiving application, it will fall back\nto the storage type.\n\nParameterized extension type\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe above example used a fixed storage type with no further metadata. But\nmore flexible, parameterized extension types are also possible.\n\nThe example given here implements an extension type for the `pandas \"period\"\ndata type <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-span-representation>`__,\nrepresenting time spans (e.g., a frequency of a day, a month, a quarter, etc).\nIt is stored as an int64 array which is interpreted as the number of time spans\nof the given frequency since 1970.\n\n::\n\n    class PeriodType(pa.ExtensionType):\n\n        def __init__(self, freq):\n            # attributes need to be set first before calling\n            # super init (as that calls serialize)\n            self._freq = freq\n            pa.ExtensionType.__init__(self, pa.int64(), 'my_package.period')\n\n        @property\n        def freq(self):\n            return self._freq\n\n        def __arrow_ext_serialize__(self):\n            return \"freq={}\".format(self.freq).encode()\n\n        @classmethod\n        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n            # return an instance of this subclass given the serialized\n            # metadata.\n            serialized = serialized.decode()\n            assert serialized.startswith(\"freq=\")\n            freq = serialized.split('=')[1]\n            return PeriodType(freq)\n\nHere, we ensure to store all information in the serialized metadata that is\nneeded to reconstruct the instance (in the ``__arrow_ext_deserialize__`` class\nmethod), in this case the frequency string.\n\nNote that, once created, the data type instance is considered immutable. If,\nin the example above, the ``freq`` parameter would change after instantiation,\nthe reconstruction of the type instance after IPC will be incorrect.\nIn the example above, the ``freq`` parameter is therefore stored in a private\nattribute with a public read-only property to access it.\n\nParameterized extension types are also possible using the pickle-based type\nsubclassing :class:`PyExtensionType`. The equivalent example for the period\ndata type from above would look like::\n\n    class PeriodType(pa.PyExtensionType):\n\n        def __init__(self, freq):\n            self._freq = freq\n            pa.PyExtensionType.__init__(self, pa.int64())\n\n        @property\n        def freq(self):\n            return self._freq\n\n        def __reduce__(self):\n            return PeriodType, (self.freq,)\n\nAlso the storage type does not need to be fixed but can be parameterized.\n\nCustom extension array class\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy default, all arrays with an extension type are constructed or deserialized into\na built-in :class:`ExtensionArray` object. Nevertheless, one could want to subclass\n:class:`ExtensionArray` in order to add some custom logic specific to the extension\ntype. Arrow allows to do so by adding a special method ``__arrow_ext_class__`` to the\ndefinition of the extension type.\n\nFor instance, let us consider the example from the `Numpy Quickstart <https://docs.scipy.org/doc/numpy-1.13.0/user/quickstart.html>`_ of points in 3D space.\nWe can store these as a fixed-size list, where we wish to be able to extract\nthe data as a 2-D Numpy array ``(N, 3)`` without any copy::\n\n    class Point3DArray(pa.ExtensionArray):\n        def to_numpy_array(self):\n            return self.storage.flatten().to_numpy().reshape((-1, 3))\n\n\n    class Point3DType(pa.PyExtensionType):\n        def __init__(self):\n            pa.PyExtensionType.__init__(self, pa.list_(pa.float32(), 3))\n\n        def __reduce__(self):\n            return Point3DType, ()\n\n        def __arrow_ext_class__(self):\n            return Point3DArray\n\nArrays built using this extension type now have the expected custom array class::\n\n    >>> storage = pa.array([[1, 2, 3], [4, 5, 6]], pa.list_(pa.float32(), 3))\n    >>> arr = pa.ExtensionArray.from_storage(Point3DType(), storage)\n    >>> arr\n    <__main__.Point3DArray object at 0x7f40dea80670>\n    [\n        [\n            1,\n            2,\n            3\n        ],\n        [\n            4,\n            5,\n            6\n        ]\n    ]\n\nThe additional methods in the extension class are then available to the user::\n\n    >>> arr.to_numpy_array()\n    array([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)\n\n\nThis array can be sent over IPC, received in another Python process, and the custom\nextension array class will be preserved (as long as the definitions of the classes above\nare available).\n\nThe same ``__arrow_ext_class__`` specialization can be used with custom types defined\nby subclassing :class:`ExtensionType`.\n\nCustom scalar conversion\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want scalars of your custom extension type to convert to a custom type when\n:meth:`ExtensionScalar.as_py()` is called, you can override the\n:meth:`ExtensionScalar.as_py()` method by subclassing :class:`ExtensionScalar`.\nFor example, if we wanted the above example 3D point type to return a custom\n3D point class instead of a list, we would implement::\n\n    Point3D = namedtuple(\"Point3D\", [\"x\", \"y\", \"z\"])\n\n    class Point3DScalar(pa.ExtensionScalar):\n        def as_py(self) -> Point3D:\n            return Point3D(*self.value.as_py())\n\n    class Point3DType(pa.PyExtensionType):\n        def __init__(self):\n            pa.PyExtensionType.__init__(self, pa.list_(pa.float32(), 3))\n\n        def __reduce__(self):\n            return Point3DType, ()\n\n        def __arrow_ext_scalar_class__(self):\n            return Point3DScalar\n\nArrays built using this extension type now provide scalars that convert to our ``Point3D`` class::\n\n    >>> storage = pa.array([[1, 2, 3], [4, 5, 6]], pa.list_(pa.float32(), 3))\n    >>> arr = pa.ExtensionArray.from_storage(Point3DType(), storage)\n    >>> arr[0].as_py()\n    Point3D(x=1.0, y=2.0, z=3.0)\n\n    >>> arr.to_pylist()\n    [Point3D(x=1.0, y=2.0, z=3.0), Point3D(x=4.0, y=5.0, z=6.0)]\n\n\nConversion to pandas\n~~~~~~~~~~~~~~~~~~~~\n\nThe conversion to pandas (in :meth:`Table.to_pandas`) of columns with an\nextension type can controlled in case there is a corresponding\n`pandas extension array <https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extension-types>`__\nfor your extension type.\n\nFor this, the :meth:`ExtensionType.to_pandas_dtype` method needs to be\nimplemented, and should return a ``pandas.api.extensions.ExtensionDtype``\nsubclass instance.\n\nUsing the pandas period type from above as example, this would look like::\n\n    class PeriodType(pa.ExtensionType):\n        ...\n\n        def to_pandas_dtype(self):\n            import pandas as pd\n            return pd.PeriodDtype(freq=self.freq)\n\nSecondly, the pandas ``ExtensionDtype`` on its turn needs to have the\n``__from_arrow__`` method implemented: a method that given a pyarrow Array\nor ChunkedArray of the extension type can construct the corresponding\npandas ``ExtensionArray``. This method should have the following signature::\n\n\n    class MyExtensionDtype(pd.api.extensions.ExtensionDtype):\n        ...\n\n        def __from_arrow__(self, array: pyarrow.Array/ChunkedArray) -> pandas.ExtensionArray:\n            ...\n\nThis way, you can control the conversion of a pyarrow ``Array`` of your pyarrow\nextension type to a pandas ``ExtensionArray`` that can be stored in a DataFrame.\n\n\nCanonical extension types\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can find the official list of canonical extension types in the\n:ref:`format_canonical_extensions` section. Here we add examples on how to\nuse them in pyarrow.\n\nFixed size tensor\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nTo create an array of tensors with equal shape (fixed shape tensor array) we\nfirst need to define a fixed shape tensor extension type with value type\nand shape:\n\n.. code-block:: python\n\n   >>> tensor_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))\n\nThen we need the storage array with :func:`pyarrow.list_` type where ``value_type```\nis the fixed shape tensor value type and list size is a product of ``tensor_type``\nshape elements. Then we can create an array of tensors with\n``pa.ExtensionArray.from_storage()`` method:\n\n.. code-block:: python\n\n   >>> arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n   >>> storage = pa.array(arr, pa.list_(pa.int32(), 4))\n   >>> tensor_array = pa.ExtensionArray.from_storage(tensor_type, storage)\n\nWe can also create another array of tensors with different value type:\n\n.. code-block:: python\n\n   >>> tensor_type_2 = pa.fixed_shape_tensor(pa.float32(), (2, 2))\n   >>> storage_2 = pa.array(arr, pa.list_(pa.float32(), 4))\n   >>> tensor_array_2 = pa.ExtensionArray.from_storage(tensor_type_2, storage_2)\n\nExtension arrays can be used as columns in  ``pyarrow.Table`` or\n``pyarrow.RecordBatch``:\n\n.. code-block:: python\n\n   >>> data = [\n   ...     pa.array([1, 2, 3]),\n   ...     pa.array(['foo', 'bar', None]),\n   ...     pa.array([True, None, True]),\n   ...     tensor_array,\n   ...     tensor_array_2\n   ... ]\n   >>> my_schema = pa.schema([('f0', pa.int8()),\n   ...                        ('f1', pa.string()),\n   ...                        ('f2', pa.bool_()),\n   ...                        ('tensors_int', tensor_type),\n   ...                        ('tensors_float', tensor_type_2)])\n   >>> table = pa.Table.from_arrays(data, schema=my_schema)\n   >>> table\n   pyarrow.Table\n   f0: int8\n   f1: string\n   f2: bool\n   tensors_int: extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>\n   tensors_float: extension<arrow.fixed_shape_tensor[value_type=float, shape=[2,2]]>\n   ----\n   f0: [[1,2,3]]\n   f1: [[\"foo\",\"bar\",null]]\n   f2: [[true,null,true]]\n   tensors_int: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n   tensors_float: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n\nWe can also convert a tensor array to a single multi-dimensional numpy ndarray.\nWith the conversion the length of the arrow array becomes the first dimension\nin the numpy ndarray:\n\n.. code-block:: python\n\n   >>> numpy_tensor = tensor_array_2.to_numpy_ndarray()\n   >>> numpy_tensor\n   array([[[  1.,   2.],\n           [  3.,   4.]],\n          [[ 10.,  20.],\n           [ 30.,  40.]],\n          [[100., 200.],\n           [300., 400.]]])\n    >>> numpy_tensor.shape\n   (3, 2, 2)\n\n.. note::\n\n   Both optional parameters, ``permutation`` and ``dim_names``, are meant to provide the user\n   with the information about the logical layout of the data compared to the physical layout.\n\n   The conversion to numpy ndarray is only possible for trivial permutations (``None`` or\n   ``[0, 1, ... N-1]`` where ``N`` is the number of tensor dimensions).\n\nAnd also the other way around, we can convert a numpy ndarray to a fixed shape tensor array:\n\n.. code-block:: python\n\n   >>> pa.FixedShapeTensorArray.from_numpy_ndarray(numpy_tensor)\n   <pyarrow.lib.FixedShapeTensorArray object at ...>\n   [\n     [\n       1,\n       2,\n       3,\n       4\n     ],\n     [\n       10,\n       20,\n       30,\n       40\n     ],\n     [\n       100,\n       200,\n       300,\n       400\n     ]\n   ]\n\nWith the conversion the first dimension of the ndarray becomes the length of the pyarrow extension\narray. We can see in the example that ndarray of shape ``(3, 2, 2)`` becomes an arrow array of\nlength 3 with tensor elements of shape ``(2, 2)``.\n\n.. code-block:: python\n\n   # ndarray of shape (3, 2, 2)\n   >>> numpy_tensor.shape\n   (3, 2, 2)\n\n   # arrow array of length 3 with tensor elements of shape (2, 2)\n   >>> pyarrow_tensor_array = pa.FixedShapeTensorArray.from_numpy_ndarray(numpy_tensor)\n   >>> len(pyarrow_tensor_array)\n   3\n   >>> pyarrow_tensor_array.type.shape\n   [2, 2]\n\nThe extension type can also have ``permutation`` and ``dim_names`` defined. For\nexample\n\n.. code-block:: python\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.float64(), [2, 2, 3], permutation=[0, 2, 1])\n\nor\n\n.. code-block:: python\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.bool_(), [2, 2, 3], dim_names=['C', 'H', 'W'])\n\nfor ``NCHW`` format where:\n\n* N: number of images which is in our case the length of an array and is always on\n  the first dimension\n* C: number of channels of the image\n* H: height of the image\n* W: width of the image\n", "# -*- coding: utf-8 -*-\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport ctypes\nimport gc\n\nimport pyarrow as pa\ntry:\n    from pyarrow.cffi import ffi\nexcept ImportError:\n    ffi = None\n\nimport pytest\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\nexcept ImportError:\n    pd = tm = None\n\n\nneeds_cffi = pytest.mark.skipif(ffi is None,\n                                reason=\"test needs cffi package installed\")\n\nassert_schema_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowSchema\")\n\nassert_array_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowArray\")\n\nassert_stream_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowArrayStream\")\n\n\ndef PyCapsule_IsValid(capsule, name):\n    return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1\n\n\nclass ParamExtType(pa.PyExtensionType):\n\n    def __init__(self, width):\n        self._width = width\n        pa.PyExtensionType.__init__(self, pa.binary(width))\n\n    @property\n    def width(self):\n        return self._width\n\n    def __reduce__(self):\n        return ParamExtType, (self.width,)\n\n\ndef make_schema():\n    return pa.schema([('ints', pa.list_(pa.int32()))],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_extension_schema():\n    return pa.schema([('ext', ParamExtType(3))],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_batch():\n    return pa.record_batch([[[1], [2, 42]]], make_schema())\n\n\ndef make_extension_batch():\n    schema = make_extension_schema()\n    ext_col = schema[0].type.wrap_array(pa.array([b\"foo\", b\"bar\"],\n                                                 type=pa.binary(3)))\n    return pa.record_batch([ext_col], schema)\n\n\ndef make_batches():\n    schema = make_schema()\n    return [\n        pa.record_batch([[[1], [2, 42]]], schema),\n        pa.record_batch([[None, [], [5, 6]]], schema),\n    ]\n\n\ndef make_serialized(schema, batches):\n    with pa.BufferOutputStream() as sink:\n        with pa.ipc.new_stream(sink, schema) as out:\n            for batch in batches:\n                out.write(batch)\n        return sink.getvalue()\n\n\n@needs_cffi\ndef test_export_import_type():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    typ = pa.list_(pa.int32())\n    typ._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del typ\n    assert pa.total_allocated_bytes() > old_allocated\n    typ_new = pa.DataType._import_from_c(ptr_schema)\n    assert typ_new == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.DataType._import_from_c(ptr_schema)\n\n    # Invalid format string\n    pa.int32()._export_to_c(ptr_schema)\n    bad_format = ffi.new(\"char[]\", b\"zzz\")\n    c_schema.format = bad_format\n    with pytest.raises(ValueError,\n                       match=\"Invalid or unsupported format string\"):\n        pa.DataType._import_from_c(ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.DataType._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_field():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    field = pa.field(\"test\", pa.list_(pa.int32()), nullable=True)\n    field._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del field\n    assert pa.total_allocated_bytes() > old_allocated\n\n    field_new = pa.Field._import_from_c(ptr_schema)\n    assert field_new == pa.field(\"test\", pa.list_(pa.int32()), nullable=True)\n    assert pa.total_allocated_bytes() == old_allocated\n\n    # Now released\n    with assert_schema_released:\n        pa.Field._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_array():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(\"struct ArrowArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    # Type is known up front\n    typ = pa.list_(pa.int32())\n    arr = pa.array([[1], [2, 42]], type=typ)\n    py_value = arr.to_pylist()\n    arr._export_to_c(ptr_array)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete recreate C++ object from exported pointer\n    del arr\n    arr_new = pa.Array._import_from_c(ptr_array, typ)\n    assert arr_new.to_pylist() == py_value\n    assert arr_new.type == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_new, typ\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_array_released:\n        pa.Array._import_from_c(ptr_array, pa.list_(pa.int32()))\n\n    # Type is exported and imported at the same time\n    arr = pa.array([[1], [2, 42]], type=pa.list_(pa.int32()))\n    py_value = arr.to_pylist()\n    arr._export_to_c(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del arr\n    arr_new = pa.Array._import_from_c(ptr_array, ptr_schema)\n    assert arr_new.to_pylist() == py_value\n    assert arr_new.type == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.Array._import_from_c(ptr_array, ptr_schema)\n\n\ndef check_export_import_schema(schema_factory):\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    schema_factory()._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    schema_new = pa.Schema._import_from_c(ptr_schema)\n    assert schema_new == schema_factory()\n    assert pa.total_allocated_bytes() == old_allocated\n    del schema_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.Schema._import_from_c(ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        pa.Schema._import_from_c(ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.Schema._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_schema():\n    check_export_import_schema(make_schema)\n\n\n@needs_cffi\ndef test_export_import_schema_with_extension():\n    check_export_import_schema(make_extension_schema)\n\n\n@needs_cffi\ndef test_export_import_schema_float_pointer():\n    # Previous versions of the R Arrow library used to pass pointer\n    # values as a double.\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    match = \"Passing a pointer value as a float is unsafe\"\n    with pytest.warns(UserWarning, match=match):\n        make_schema()._export_to_c(float(ptr_schema))\n    with pytest.warns(UserWarning, match=match):\n        schema_new = pa.Schema._import_from_c(float(ptr_schema))\n    assert schema_new == make_schema()\n\n\ndef check_export_import_batch(batch_factory):\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(\"struct ArrowArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    # Schema is known up front\n    batch = batch_factory()\n    schema = batch.schema\n    py_value = batch.to_pydict()\n    batch._export_to_c(ptr_array)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del batch\n    batch_new = pa.RecordBatch._import_from_c(ptr_array, schema)\n    assert batch_new.to_pydict() == py_value\n    assert batch_new.schema == schema\n    assert pa.total_allocated_bytes() > old_allocated\n    del batch_new, schema\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_array_released:\n        pa.RecordBatch._import_from_c(ptr_array, make_schema())\n\n    # Type is exported and imported at the same time\n    batch = batch_factory()\n    py_value = batch.to_pydict()\n    batch._export_to_c(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del batch\n    batch_new = pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n    assert batch_new.to_pydict() == py_value\n    assert batch_new.schema == batch_factory().schema\n    assert pa.total_allocated_bytes() > old_allocated\n    del batch_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    batch_factory()._export_to_c(ptr_array)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_batch():\n    check_export_import_batch(make_batch)\n\n\n@needs_cffi\ndef test_export_import_batch_with_extension():\n    check_export_import_batch(make_extension_batch)\n\n\ndef _export_import_batch_reader(ptr_stream, reader_factory):\n    # Prepare input\n    batches = make_batches()\n    schema = batches[0].schema\n\n    reader = reader_factory(schema, batches)\n    reader._export_to_c(ptr_stream)\n    # Delete and recreate C++ object from exported pointer\n    del reader, batches\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    assert reader_new.schema == schema\n    got_batches = list(reader_new)\n    del reader_new\n    assert got_batches == make_batches()\n\n    # Test read_pandas()\n    if pd is not None:\n        batches = make_batches()\n        schema = batches[0].schema\n        expected_df = pa.Table.from_batches(batches).to_pandas()\n\n        reader = reader_factory(schema, batches)\n        reader._export_to_c(ptr_stream)\n        del reader, batches\n\n        reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n        got_df = reader_new.read_pandas()\n        del reader_new\n        tm.assert_frame_equal(expected_df, got_df)\n\n\ndef make_ipc_stream_reader(schema, batches):\n    return pa.ipc.open_stream(make_serialized(schema, batches))\n\n\ndef make_py_record_batch_reader(schema, batches):\n    return pa.RecordBatchReader.from_batches(schema, batches)\n\n\n@needs_cffi\n@pytest.mark.parametrize('reader_factory',\n                         [make_ipc_stream_reader,\n                          make_py_record_batch_reader])\ndef test_export_import_batch_reader(reader_factory):\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    _export_import_batch_reader(ptr_stream, reader_factory)\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    # Now released\n    with assert_stream_released:\n        pa.RecordBatchReader._import_from_c(ptr_stream)\n\n\n@needs_cffi\ndef test_imported_batch_reader_error():\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    schema = pa.schema([('foo', pa.int32())])\n    batches = [pa.record_batch([[1, 2, 3]], schema=schema),\n               pa.record_batch([[4, 5, 6]], schema=schema)]\n    buf = make_serialized(schema, batches)\n\n    # Open a corrupt/incomplete stream and export it\n    reader = pa.ipc.open_stream(buf[:-16])\n    reader._export_to_c(ptr_stream)\n    del reader\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    batch = reader_new.read_next_batch()\n    assert batch == batches[0]\n    with pytest.raises(OSError,\n                       match=\"Expected to be able to read 16 bytes \"\n                             \"for message body, got 8\"):\n        reader_new.read_next_batch()\n\n    # Again, but call read_all()\n    reader = pa.ipc.open_stream(buf[:-16])\n    reader._export_to_c(ptr_stream)\n    del reader\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    with pytest.raises(OSError,\n                       match=\"Expected to be able to read 16 bytes \"\n                             \"for message body, got 8\"):\n        reader_new.read_all()\n\n\n@pytest.mark.parametrize('obj', [pa.int32(), pa.field('foo', pa.int32()),\n                                 pa.schema({'foo': pa.int32()})],\n                         ids=['type', 'field', 'schema'])\ndef test_roundtrip_schema_capsule(obj):\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    capsule = obj.__arrow_c_schema__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_schema\") == 1\n    assert pa.total_allocated_bytes() > old_allocated\n    obj_out = type(obj)._import_from_c_capsule(capsule)\n    assert obj_out == obj\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    capsule = obj.__arrow_c_schema__()\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del capsule\n    assert pa.total_allocated_bytes() == old_allocated\n\n\n@pytest.mark.parametrize('arr,schema_accessor,bad_type,good_type', [\n    (pa.array(['a', 'b', 'c']), lambda x: x.type, pa.int32(), pa.string()),\n    (\n        pa.record_batch([pa.array(['a', 'b', 'c'])], names=['x']),\n        lambda x: x.schema,\n        pa.schema({'x': pa.int32()}),\n        pa.schema({'x': pa.string()})\n    ),\n], ids=['array', 'record_batch'])\ndef test_roundtrip_array_capsule(arr, schema_accessor, bad_type, good_type):\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    import_array = type(arr)._import_from_c_capsule\n\n    schema_capsule, capsule = arr.__arrow_c_array__()\n    assert PyCapsule_IsValid(schema_capsule, b\"arrow_schema\") == 1\n    assert PyCapsule_IsValid(capsule, b\"arrow_array\") == 1\n    arr_out = import_array(schema_capsule, capsule)\n    assert arr_out.equals(arr)\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_out\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    capsule = arr.__arrow_c_array__()\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del capsule\n    assert pa.total_allocated_bytes() == old_allocated\n\n    with pytest.raises(ValueError,\n                       match=r\"Could not cast.* string to requested .* int32\"):\n        arr.__arrow_c_array__(bad_type.__arrow_c_schema__())\n\n    schema_capsule, array_capsule = arr.__arrow_c_array__(\n        good_type.__arrow_c_schema__())\n    arr_out = import_array(schema_capsule, array_capsule)\n    assert schema_accessor(arr_out) == good_type\n\n\n# TODO: implement requested_schema for stream\n@pytest.mark.parametrize('constructor', [\n    pa.RecordBatchReader.from_batches,\n    # Use a lambda because we need to re-order the parameters\n    lambda schema, batches: pa.Table.from_batches(batches, schema),\n], ids=['recordbatchreader', 'table'])\ndef test_roundtrip_reader_capsule(constructor):\n    batches = make_batches()\n    schema = batches[0].schema\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    obj = constructor(schema, batches)\n\n    capsule = obj.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == schema\n    imported_batches = list(imported_reader)\n    assert len(imported_batches) == len(batches)\n    for batch, expected in zip(imported_batches, batches):\n        assert batch.equals(expected)\n\n    del obj, imported_reader, batch, expected, imported_batches\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    obj = constructor(schema, batches)\n\n    # TODO: turn this to ValueError once we implement validation.\n    bad_schema = pa.schema({'ints': pa.int32()})\n    with pytest.raises(NotImplementedError):\n        obj.__arrow_c_stream__(bad_schema.__arrow_c_schema__())\n\n    # Can work with matching schema\n    matching_schema = pa.schema({'ints': pa.list_(pa.int32())})\n    capsule = obj.__arrow_c_stream__(matching_schema.__arrow_c_schema__())\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == matching_schema\n    for batch, expected in zip(imported_reader, batches):\n        assert batch.equals(expected)\n\n\ndef test_roundtrip_batch_reader_capsule():\n    batch = make_batch()\n\n    capsule = batch.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == batch.schema\n    assert imported_reader.read_next_batch().equals(batch)\n    with pytest.raises(StopIteration):\n        imported_reader.read_next_batch()\n", "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport shutil\nimport subprocess\nimport weakref\nfrom uuid import uuid4, UUID\nimport sys\n\nimport numpy as np\nimport pyarrow as pa\nfrom pyarrow.vendored.version import Version\n\nimport pytest\n\n\nclass TinyIntType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.int8())\n\n    def __reduce__(self):\n        return TinyIntType, ()\n\n\nclass IntegerType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.int64())\n\n    def __reduce__(self):\n        return IntegerType, ()\n\n\nclass IntegerEmbeddedType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, IntegerType())\n\n    def __reduce__(self):\n        return IntegerEmbeddedType, ()\n\n\nclass UuidScalarType(pa.ExtensionScalar):\n    def as_py(self):\n        return None if self.value is None else UUID(bytes=self.value.as_py())\n\n\nclass UuidType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.binary(16))\n\n    def __reduce__(self):\n        return UuidType, ()\n\n    def __arrow_ext_scalar_class__(self):\n        return UuidScalarType\n\n\nclass UuidType2(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.binary(16))\n\n    def __reduce__(self):\n        return UuidType2, ()\n\n\nclass LabelType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.string())\n\n    def __reduce__(self):\n        return LabelType, ()\n\n\nclass ParamExtType(pa.PyExtensionType):\n\n    def __init__(self, width):\n        self._width = width\n        pa.PyExtensionType.__init__(self, pa.binary(width))\n\n    @property\n    def width(self):\n        return self._width\n\n    def __reduce__(self):\n        return ParamExtType, (self.width,)\n\n\nclass MyStructType(pa.PyExtensionType):\n    storage_type = pa.struct([('left', pa.int64()),\n                              ('right', pa.int64())])\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, self.storage_type)\n\n    def __reduce__(self):\n        return MyStructType, ()\n\n\nclass MyListType(pa.PyExtensionType):\n\n    def __init__(self, storage_type):\n        pa.PyExtensionType.__init__(self, storage_type)\n\n    def __reduce__(self):\n        return MyListType, (self.storage_type,)\n\n\nclass AnnotatedType(pa.PyExtensionType):\n    \"\"\"\n    Generic extension type that can store any storage type.\n    \"\"\"\n\n    def __init__(self, storage_type, annotation):\n        self.annotation = annotation\n        super().__init__(storage_type)\n\n    def __reduce__(self):\n        return AnnotatedType, (self.storage_type, self.annotation)\n\n\ndef ipc_write_batch(batch):\n    stream = pa.BufferOutputStream()\n    writer = pa.RecordBatchStreamWriter(stream, batch.schema)\n    writer.write_batch(batch)\n    writer.close()\n    return stream.getvalue()\n\n\ndef ipc_read_batch(buf):\n    reader = pa.RecordBatchStreamReader(buf)\n    return reader.read_next_batch()\n\n\ndef test_ext_type_basics():\n    ty = UuidType()\n    assert ty.extension_name == \"arrow.py_extension_type\"\n\n\ndef test_ext_type_str():\n    ty = IntegerType()\n    expected = \"extension<arrow.py_extension_type<IntegerType>>\"\n    assert str(ty) == expected\n    assert pa.DataType.__str__(ty) == expected\n\n\ndef test_ext_type_repr():\n    ty = IntegerType()\n    assert repr(ty) == \"IntegerType(DataType(int64))\"\n\n\ndef test_ext_type__lifetime():\n    ty = UuidType()\n    wr = weakref.ref(ty)\n    del ty\n    assert wr() is None\n\n\ndef test_ext_type__storage_type():\n    ty = UuidType()\n    assert ty.storage_type == pa.binary(16)\n    assert ty.__class__ is UuidType\n    ty = ParamExtType(5)\n    assert ty.storage_type == pa.binary(5)\n    assert ty.__class__ is ParamExtType\n\n\ndef test_ext_type_as_py():\n    ty = UuidType()\n    expected = uuid4()\n    scalar = pa.ExtensionScalar.from_storage(ty, expected.bytes)\n    assert scalar.as_py() == expected\n\n    # test array\n    uuids = [uuid4() for _ in range(3)]\n    storage = pa.array([uuid.bytes for uuid in uuids], type=pa.binary(16))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    # Works for __get_item__\n    for i, expected in enumerate(uuids):\n        assert arr[i].as_py() == expected\n\n    # Works for __iter__\n    for result, expected in zip(arr, uuids):\n        assert result.as_py() == expected\n\n    # test chunked array\n    data = [\n        pa.ExtensionArray.from_storage(ty, storage),\n        pa.ExtensionArray.from_storage(ty, storage)\n    ]\n    carr = pa.chunked_array(data)\n    for i, expected in enumerate(uuids + uuids):\n        assert carr[i].as_py() == expected\n\n    for result, expected in zip(carr, uuids + uuids):\n        assert result.as_py() == expected\n\n\ndef test_uuid_type_pickle(pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        ty = UuidType()\n        ser = pickle_module.dumps(ty, protocol=proto)\n        del ty\n        ty = pickle_module.loads(ser)\n        wr = weakref.ref(ty)\n        assert ty.extension_name == \"arrow.py_extension_type\"\n        del ty\n        assert wr() is None\n\n\ndef test_ext_type_equality():\n    a = ParamExtType(5)\n    b = ParamExtType(6)\n    c = ParamExtType(6)\n    assert a != b\n    assert b == c\n    d = UuidType()\n    e = UuidType()\n    assert a != d\n    assert d == e\n\n\ndef test_ext_array_basics():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n    arr.validate()\n    assert arr.type is ty\n    assert arr.storage.equals(storage)\n\n\ndef test_ext_array_lifetime():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    refs = [weakref.ref(ty), weakref.ref(arr), weakref.ref(storage)]\n    del ty, storage, arr\n    for ref in refs:\n        assert ref() is None\n\n\ndef test_ext_array_to_pylist():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\", None], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    assert arr.to_pylist() == [b\"foo\", b\"bar\", None]\n\n\ndef test_ext_array_errors():\n    ty = ParamExtType(4)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    with pytest.raises(TypeError, match=\"Incompatible storage type\"):\n        pa.ExtensionArray.from_storage(ty, storage)\n\n\ndef test_ext_array_equality():\n    storage1 = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    storage2 = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    storage3 = pa.array([], type=pa.binary(16))\n    ty1 = UuidType()\n    ty2 = ParamExtType(16)\n\n    a = pa.ExtensionArray.from_storage(ty1, storage1)\n    b = pa.ExtensionArray.from_storage(ty1, storage2)\n    assert a.equals(b)\n    c = pa.ExtensionArray.from_storage(ty1, storage3)\n    assert not a.equals(c)\n    d = pa.ExtensionArray.from_storage(ty2, storage1)\n    assert not a.equals(d)\n    e = pa.ExtensionArray.from_storage(ty2, storage2)\n    assert d.equals(e)\n    f = pa.ExtensionArray.from_storage(ty2, storage3)\n    assert not d.equals(f)\n\n\ndef test_ext_array_wrap_array():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\", None], type=pa.binary(3))\n    arr = ty.wrap_array(storage)\n    arr.validate(full=True)\n    assert isinstance(arr, pa.ExtensionArray)\n    assert arr.type == ty\n    assert arr.storage == storage\n\n    storage = pa.chunked_array([[b\"abc\", b\"def\"], [b\"ghi\"]],\n                               type=pa.binary(3))\n    arr = ty.wrap_array(storage)\n    arr.validate(full=True)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.type == ty\n    assert arr.chunk(0).storage == storage.chunk(0)\n    assert arr.chunk(1).storage == storage.chunk(1)\n\n    # Wrong storage type\n    storage = pa.array([b\"foo\", b\"bar\", None])\n    with pytest.raises(TypeError, match=\"Incompatible storage type\"):\n        ty.wrap_array(storage)\n\n    # Not an array or chunked array\n    with pytest.raises(TypeError, match=\"Expected array or chunked array\"):\n        ty.wrap_array(None)\n\n\ndef test_ext_scalar_from_array():\n    data = [b\"0123456789abcdef\", b\"0123456789abcdef\",\n            b\"zyxwvutsrqponmlk\", None]\n    storage = pa.array(data, type=pa.binary(16))\n    ty1 = UuidType()\n    ty2 = ParamExtType(16)\n    ty3 = UuidType2()\n\n    a = pa.ExtensionArray.from_storage(ty1, storage)\n    b = pa.ExtensionArray.from_storage(ty2, storage)\n    c = pa.ExtensionArray.from_storage(ty3, storage)\n\n    scalars_a = list(a)\n    assert len(scalars_a) == 4\n\n    assert ty1.__arrow_ext_scalar_class__() == UuidScalarType\n    assert isinstance(a[0], UuidScalarType)\n    assert isinstance(scalars_a[0], UuidScalarType)\n\n    for s, val in zip(scalars_a, data):\n        assert isinstance(s, pa.ExtensionScalar)\n        assert s.is_valid == (val is not None)\n        assert s.type == ty1\n        if val is not None:\n            assert s.value == pa.scalar(val, storage.type)\n            assert s.as_py() == UUID(bytes=val)\n        else:\n            assert s.value is None\n\n    scalars_b = list(b)\n    assert len(scalars_b) == 4\n\n    for sa, sb in zip(scalars_a, scalars_b):\n        assert isinstance(sb, pa.ExtensionScalar)\n        assert sa.is_valid == sb.is_valid\n        if sa.as_py() is None:\n            assert sa.as_py() == sb.as_py()\n        else:\n            assert sa.as_py().bytes == sb.as_py()\n        assert sa != sb\n\n    scalars_c = list(c)\n    assert len(scalars_c) == 4\n\n    for s, val in zip(scalars_c, data):\n        assert isinstance(s, pa.ExtensionScalar)\n        assert s.is_valid == (val is not None)\n        assert s.type == ty3\n        if val is not None:\n            assert s.value == pa.scalar(val, storage.type)\n            assert s.as_py() == val\n        else:\n            assert s.value is None\n\n    assert a.to_pylist() == [UUID(bytes=x) if x else None for x in data]\n\n\ndef test_ext_scalar_from_storage():\n    ty = UuidType()\n\n    s = pa.ExtensionScalar.from_storage(ty, None)\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is False\n    assert s.value is None\n\n    s = pa.ExtensionScalar.from_storage(ty, b\"0123456789abcdef\")\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is True\n    assert s.value == pa.scalar(b\"0123456789abcdef\", ty.storage_type)\n\n    s = pa.ExtensionScalar.from_storage(ty, pa.scalar(None, ty.storage_type))\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is False\n    assert s.value is None\n\n    s = pa.ExtensionScalar.from_storage(\n        ty, pa.scalar(b\"0123456789abcdef\", ty.storage_type))\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is True\n    assert s.value == pa.scalar(b\"0123456789abcdef\", ty.storage_type)\n\n\ndef test_ext_array_pickling(pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        ty = ParamExtType(3)\n        storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n        arr = pa.ExtensionArray.from_storage(ty, storage)\n        ser = pickle_module.dumps(arr, protocol=proto)\n        del ty, storage, arr\n        arr = pickle_module.loads(ser)\n        arr.validate()\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type == ParamExtType(3)\n        assert arr.type.storage_type == pa.binary(3)\n        assert arr.storage.type == pa.binary(3)\n        assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n\n\ndef test_ext_array_conversion_to_numpy():\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    result = arr1.to_numpy()\n    expected = np.array([1, 2, 3], dtype=\"int64\")\n    np.testing.assert_array_equal(result, expected)\n\n    with pytest.raises(ValueError, match=\"zero_copy_only was True\"):\n        arr2.to_numpy()\n    result = arr2.to_numpy(zero_copy_only=False)\n    expected = np.array([b\"123\", b\"456\", b\"789\"])\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_ext_array_conversion_to_pandas():\n    import pandas as pd\n\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    result = arr1.to_pandas()\n    expected = pd.Series([1, 2, 3], dtype=\"int64\")\n    pd.testing.assert_series_equal(result, expected)\n\n    result = arr2.to_pandas()\n    expected = pd.Series([b\"123\", b\"456\", b\"789\"], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n\n@pytest.fixture\ndef struct_w_ext_data():\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    sarr1 = pa.StructArray.from_arrays([arr1], [\"f0\"])\n    sarr2 = pa.StructArray.from_arrays([arr2], [\"f1\"])\n\n    return [sarr1, sarr2]\n\n\ndef test_struct_w_ext_array_to_numpy(struct_w_ext_data):\n    # ARROW-15291\n    # Check that we don't segfault when trying to build\n    # a numpy array from a StructArray with a field being\n    # an ExtensionArray\n\n    result = struct_w_ext_data[0].to_numpy(zero_copy_only=False)\n    expected = np.array([{'f0': 1}, {'f0': 2},\n                         {'f0': 3}], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n    result = struct_w_ext_data[1].to_numpy(zero_copy_only=False)\n    expected = np.array([{'f1': b'123'}, {'f1': b'456'},\n                         {'f1': b'789'}], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_struct_w_ext_array_to_pandas(struct_w_ext_data):\n    # ARROW-15291\n    # Check that we don't segfault when trying to build\n    # a Pandas dataframe from a StructArray with a field\n    # being an ExtensionArray\n    import pandas as pd\n\n    result = struct_w_ext_data[0].to_pandas()\n    expected = pd.Series([{'f0': 1}, {'f0': 2},\n                         {'f0': 3}], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n    result = struct_w_ext_data[1].to_pandas()\n    expected = pd.Series([{'f1': b'123'}, {'f1': b'456'},\n                         {'f1': b'789'}], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n\ndef test_cast_kernel_on_extension_arrays():\n    # test array casting\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(IntegerType(), storage)\n\n    # test that no allocation happens during identity cast\n    allocated_before_cast = pa.total_allocated_bytes()\n    casted = arr.cast(pa.int64())\n    assert pa.total_allocated_bytes() == allocated_before_cast\n\n    cases = [\n        (pa.int64(), pa.Int64Array),\n        (pa.int32(), pa.Int32Array),\n        (pa.int16(), pa.Int16Array),\n        (pa.uint64(), pa.UInt64Array),\n        (pa.uint32(), pa.UInt32Array),\n        (pa.uint16(), pa.UInt16Array)\n    ]\n    for typ, klass in cases:\n        casted = arr.cast(typ)\n        assert casted.type == typ\n        assert isinstance(casted, klass)\n\n    # test chunked array casting\n    arr = pa.chunked_array([arr, arr])\n    casted = arr.cast(pa.int16())\n    assert casted.type == pa.int16()\n    assert isinstance(casted, pa.ChunkedArray)\n\n\n@pytest.mark.parametrize(\"data,ty\", (\n    ([1, 2], pa.int32),\n    ([1, 2], pa.int64),\n    ([\"1\", \"2\"], pa.string),\n    ([b\"1\", b\"2\"], pa.binary),\n    ([1.0, 2.0], pa.float32),\n    ([1.0, 2.0], pa.float64)\n))\ndef test_casting_to_extension_type(data, ty):\n    arr = pa.array(data, ty())\n    out = arr.cast(IntegerType())\n    assert isinstance(out, pa.ExtensionArray)\n    assert out.type == IntegerType()\n    assert out.to_pylist() == [1, 2]\n\n\ndef test_cast_between_extension_types():\n    array = pa.array([1, 2, 3], pa.int8())\n\n    tiny_int_arr = array.cast(TinyIntType())\n    assert tiny_int_arr.type == TinyIntType()\n\n    # Casting between extension types w/ different storage types not okay.\n    msg = (\"Casting from 'extension<arrow.py_extension_type<TinyIntType>>' \"\n           \"to different extension type \"\n           \"'extension<arrow.py_extension_type<IntegerType>>' not permitted. \"\n           \"One can first cast to the storage type, \"\n           \"then to the extension type.\"\n           )\n    with pytest.raises(TypeError, match=msg):\n        tiny_int_arr.cast(IntegerType())\n    tiny_int_arr.cast(pa.int64()).cast(IntegerType())\n\n    # Between the same extension types is okay\n    array = pa.array([b'1' * 16, b'2' * 16], pa.binary(16)).cast(UuidType())\n    out = array.cast(UuidType())\n    assert out.type == UuidType()\n\n    # Will still fail casting between extensions who share storage type,\n    # can only cast between exactly the same extension types.\n    with pytest.raises(TypeError, match='Casting from *'):\n        array.cast(UuidType2())\n\n\ndef test_cast_to_extension_with_extension_storage():\n    # Test casting directly, and IntegerType -> IntegerEmbeddedType\n    array = pa.array([1, 2, 3], pa.int64())\n    array.cast(IntegerEmbeddedType())\n    array.cast(IntegerType()).cast(IntegerEmbeddedType())\n\n\n@pytest.mark.parametrize(\"data,type_factory\", (\n    # list<extension>\n    ([[1, 2, 3]], lambda: pa.list_(IntegerType())),\n    # struct<extension>\n    ([{\"foo\": 1}], lambda: pa.struct([(\"foo\", IntegerType())])),\n    # list<struct<extension>>\n    ([[{\"foo\": 1}]], lambda: pa.list_(pa.struct([(\"foo\", IntegerType())]))),\n    # struct<list<extension>>\n    ([{\"foo\": [1, 2, 3]}], lambda: pa.struct(\n        [(\"foo\", pa.list_(IntegerType()))])),\n))\ndef test_cast_nested_extension_types(data, type_factory):\n    ty = type_factory()\n    a = pa.array(data)\n    b = a.cast(ty)\n    assert b.type == ty  # casted to target extension\n    assert b.cast(a.type)  # and can cast back\n\n\ndef test_casting_dict_array_to_extension_type():\n    storage = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    arr = pa.ExtensionArray.from_storage(UuidType(), storage)\n    dict_arr = pa.DictionaryArray.from_arrays(pa.array([0, 0], pa.int32()),\n                                              arr)\n    out = dict_arr.cast(UuidType())\n    assert isinstance(out, pa.ExtensionArray)\n    assert out.to_pylist() == [UUID('30313233-3435-3637-3839-616263646566'),\n                               UUID('30313233-3435-3637-3839-616263646566')]\n\n\ndef test_concat():\n    arr1 = pa.array([1, 2, 3], IntegerType())\n    arr2 = pa.array([4, 5, 6], IntegerType())\n\n    result = pa.concat_arrays([arr1, arr2])\n    expected = pa.array([1, 2, 3, 4, 5, 6], IntegerType())\n    assert result.equals(expected)\n\n    # nested in a struct\n    struct_arr1 = pa.StructArray.from_arrays([arr1], names=[\"a\"])\n    struct_arr2 = pa.StructArray.from_arrays([arr2], names=[\"a\"])\n    result = pa.concat_arrays([struct_arr1, struct_arr2])\n    expected = pa.StructArray.from_arrays([expected], names=[\"a\"])\n    assert result.equals(expected)\n\n\ndef test_null_storage_type():\n    ext_type = AnnotatedType(pa.null(), {\"key\": \"value\"})\n    storage = pa.array([None] * 10, pa.null())\n    arr = pa.ExtensionArray.from_storage(ext_type, storage)\n    assert arr.null_count == 10\n    arr.validate(full=True)\n\n\ndef example_batch():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n    return pa.RecordBatch.from_arrays([arr], [\"exts\"])\n\n\ndef check_example_batch(batch):\n    arr = batch.column(0)\n    assert isinstance(arr, pa.ExtensionArray)\n    assert arr.type.storage_type == pa.binary(3)\n    assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n    return arr\n\n\ndef test_ipc():\n    batch = example_batch()\n    buf = ipc_write_batch(batch)\n    del batch\n\n    batch = ipc_read_batch(buf)\n    arr = check_example_batch(batch)\n    assert arr.type == ParamExtType(3)\n\n\ndef test_ipc_unknown_type():\n    batch = example_batch()\n    buf = ipc_write_batch(batch)\n    del batch\n\n    orig_type = ParamExtType\n    try:\n        # Simulate the original Python type being unavailable.\n        # Deserialization should not fail but return a placeholder type.\n        del globals()['ParamExtType']\n\n        batch = ipc_read_batch(buf)\n        arr = check_example_batch(batch)\n        assert isinstance(arr.type, pa.UnknownExtensionType)\n\n        # Can be serialized again\n        buf2 = ipc_write_batch(batch)\n        del batch, arr\n\n        batch = ipc_read_batch(buf2)\n        arr = check_example_batch(batch)\n        assert isinstance(arr.type, pa.UnknownExtensionType)\n    finally:\n        globals()['ParamExtType'] = orig_type\n\n    # Deserialize again with the type restored\n    batch = ipc_read_batch(buf2)\n    arr = check_example_batch(batch)\n    assert arr.type == ParamExtType(3)\n\n\nclass PeriodArray(pa.ExtensionArray):\n    pass\n\n\nclass PeriodType(pa.ExtensionType):\n    def __init__(self, freq):\n        # attributes need to be set first before calling\n        # super init (as that calls serialize)\n        self._freq = freq\n        pa.ExtensionType.__init__(self, pa.int64(), 'test.period')\n\n    @property\n    def freq(self):\n        return self._freq\n\n    def __arrow_ext_serialize__(self):\n        return \"freq={}\".format(self.freq).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        serialized = serialized.decode()\n        assert serialized.startswith(\"freq=\")\n        freq = serialized.split('=')[1]\n        return PeriodType(freq)\n\n    def __eq__(self, other):\n        if isinstance(other, pa.BaseExtensionType):\n            return (isinstance(self, type(other)) and\n                    self.freq == other.freq)\n        else:\n            return NotImplemented\n\n\nclass PeriodTypeWithClass(PeriodType):\n    def __init__(self, freq):\n        PeriodType.__init__(self, freq)\n\n    def __arrow_ext_class__(self):\n        return PeriodArray\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        freq = PeriodType.__arrow_ext_deserialize__(\n            storage_type, serialized).freq\n        return PeriodTypeWithClass(freq)\n\n\nclass PeriodTypeWithToPandasDtype(PeriodType):\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        freq = PeriodType.__arrow_ext_deserialize__(\n            storage_type, serialized).freq\n        return PeriodTypeWithToPandasDtype(freq)\n\n    def to_pandas_dtype(self):\n        import pandas as pd\n        return pd.PeriodDtype(freq=self.freq)\n\n\n@pytest.fixture(params=[PeriodType('D'),\n                        PeriodTypeWithClass('D'),\n                        PeriodTypeWithToPandasDtype('D')])\ndef registered_period_type(request):\n    # setup\n    period_type = request.param\n    period_class = period_type.__arrow_ext_class__()\n    pa.register_extension_type(period_type)\n    yield period_type, period_class\n    # teardown\n    try:\n        pa.unregister_extension_type('test.period')\n    except KeyError:\n        pass\n\n\ndef test_generic_ext_type():\n    period_type = PeriodType('D')\n    assert period_type.extension_name == \"test.period\"\n    assert period_type.storage_type == pa.int64()\n    # default ext_class expected.\n    assert period_type.__arrow_ext_class__() == pa.ExtensionArray\n\n\ndef test_generic_ext_type_ipc(registered_period_type):\n    period_type, period_class = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n    # check the built array has exactly the expected clss\n    assert isinstance(arr, period_class)\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n\n    result = batch.column(0)\n    # check the deserialized array class is the expected one\n    assert isinstance(result, period_class)\n    assert result.type.extension_name == \"test.period\"\n    assert arr.storage.to_pylist() == [1, 2, 3, 4]\n\n    # we get back an actual PeriodType\n    assert isinstance(result.type, PeriodType)\n    assert result.type.freq == 'D'\n    assert result.type == period_type\n\n    # using different parametrization as how it was registered\n    period_type_H = period_type.__class__('H')\n    assert period_type_H.extension_name == \"test.period\"\n    assert period_type_H.freq == 'H'\n\n    arr = pa.ExtensionArray.from_storage(period_type_H, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n    result = batch.column(0)\n    assert isinstance(result.type, PeriodType)\n    assert result.type.freq == 'H'\n    assert isinstance(result, period_class)\n\n\ndef test_generic_ext_type_ipc_unknown(registered_period_type):\n    period_type, _ = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    buf = ipc_write_batch(batch)\n    del batch\n\n    # unregister type before loading again => reading unknown extension type\n    # as plain array (but metadata in schema's field are preserved)\n    pa.unregister_extension_type('test.period')\n\n    batch = ipc_read_batch(buf)\n    result = batch.column(0)\n\n    assert isinstance(result, pa.Int64Array)\n    ext_field = batch.schema.field('ext')\n    assert ext_field.metadata == {\n        b'ARROW:extension:metadata': b'freq=D',\n        b'ARROW:extension:name': b'test.period'\n    }\n\n\ndef test_generic_ext_type_equality():\n    period_type = PeriodType('D')\n    assert period_type.extension_name == \"test.period\"\n\n    period_type2 = PeriodType('D')\n    period_type3 = PeriodType('H')\n    assert period_type == period_type2\n    assert not period_type == period_type3\n\n\ndef test_generic_ext_type_pickling(registered_period_type, pickle_module):\n    # GH-36038\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        period_type, _ = registered_period_type\n        ser = pickle_module.dumps(period_type, protocol=proto)\n        period_type_pickled = pickle_module.loads(ser)\n        assert period_type == period_type_pickled\n\n\ndef test_generic_ext_array_pickling(registered_period_type, pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        period_type, _ = registered_period_type\n        storage = pa.array([1, 2, 3, 4], pa.int64())\n        arr = pa.ExtensionArray.from_storage(period_type, storage)\n        ser = pickle_module.dumps(arr, protocol=proto)\n        del storage, arr\n        arr = pickle_module.loads(ser)\n        arr.validate()\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type == period_type\n        assert arr.type.storage_type == pa.int64()\n        assert arr.storage.type == pa.int64()\n        assert arr.storage.to_pylist() == [1, 2, 3, 4]\n\n\ndef test_generic_ext_type_register(registered_period_type):\n    # test that trying to register other type does not segfault\n    with pytest.raises(TypeError):\n        pa.register_extension_type(pa.string())\n\n    # register second time raises KeyError\n    period_type = PeriodType('D')\n    with pytest.raises(KeyError):\n        pa.register_extension_type(period_type)\n\n\n@pytest.mark.parquet\ndef test_parquet_period(tmpdir, registered_period_type):\n    # Parquet support for primitive extension types\n    period_type, period_class = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    table = pa.table([arr], names=[\"ext\"])\n\n    import pyarrow.parquet as pq\n\n    filename = tmpdir / 'period_extension_type.parquet'\n    pq.write_table(table, filename)\n\n    # Stored in parquet as storage type but with extension metadata saved\n    # in the serialized arrow schema\n    meta = pq.read_metadata(filename)\n    assert meta.schema.column(0).physical_type == \"INT64\"\n    assert b\"ARROW:schema\" in meta.metadata\n\n    import base64\n    decoded_schema = base64.b64decode(meta.metadata[b\"ARROW:schema\"])\n    schema = pa.ipc.read_schema(pa.BufferReader(decoded_schema))\n    # Since the type could be reconstructed, the extension type metadata is\n    # absent.\n    assert schema.field(\"ext\").metadata == {}\n\n    # When reading in, properly create extension type if it is registered\n    result = pq.read_table(filename)\n    assert result.schema.field(\"ext\").type == period_type\n    assert result.schema.field(\"ext\").metadata == {}\n    # Get the exact array class defined by the registered type.\n    result_array = result.column(\"ext\").chunk(0)\n    assert type(result_array) is period_class\n\n    # When the type is not registered, read in as storage type\n    pa.unregister_extension_type(period_type.extension_name)\n    result = pq.read_table(filename)\n    assert result.schema.field(\"ext\").type == pa.int64()\n    # The extension metadata is present for roundtripping.\n    assert result.schema.field(\"ext\").metadata == {\n        b'ARROW:extension:metadata': b'freq=D',\n        b'ARROW:extension:name': b'test.period'\n    }\n\n\n@pytest.mark.parquet\ndef test_parquet_extension_with_nested_storage(tmpdir):\n    # Parquet support for extension types with nested storage type\n    import pyarrow.parquet as pq\n\n    struct_array = pa.StructArray.from_arrays(\n        [pa.array([0, 1], type=\"int64\"), pa.array([4, 5], type=\"int64\")],\n        names=[\"left\", \"right\"])\n    list_array = pa.array([[1, 2, 3], [4, 5]], type=pa.list_(pa.int32()))\n\n    mystruct_array = pa.ExtensionArray.from_storage(MyStructType(),\n                                                    struct_array)\n    mylist_array = pa.ExtensionArray.from_storage(\n        MyListType(list_array.type), list_array)\n\n    orig_table = pa.table({'structs': mystruct_array,\n                           'lists': mylist_array})\n    filename = tmpdir / 'nested_extension_storage.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column('structs').type == mystruct_array.type\n    assert table.column('lists').type == mylist_array.type\n    assert table == orig_table\n\n    with pytest.raises(pa.ArrowInvalid, match='without all of its fields'):\n        pq.ParquetFile(filename).read(columns=['structs.left'])\n\n\n@pytest.mark.parquet\ndef test_parquet_nested_extension(tmpdir):\n    # Parquet support for extension types nested in struct or list\n    import pyarrow.parquet as pq\n\n    ext_type = IntegerType()\n    storage = pa.array([4, 5, 6, 7], type=pa.int64())\n    ext_array = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    # Struct of extensions\n    struct_array = pa.StructArray.from_arrays(\n        [storage, ext_array],\n        names=['ints', 'exts'])\n\n    orig_table = pa.table({'structs': struct_array})\n    filename = tmpdir / 'struct_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column(0).type == struct_array.type\n    assert table == orig_table\n\n    # List of extensions\n    list_array = pa.ListArray.from_arrays([0, 1, None, 3], ext_array)\n\n    orig_table = pa.table({'lists': list_array})\n    filename = tmpdir / 'list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column(0).type == list_array.type\n    assert table == orig_table\n\n    # Large list of extensions\n    list_array = pa.LargeListArray.from_arrays([0, 1, None, 3], ext_array)\n\n    orig_table = pa.table({'lists': list_array})\n    filename = tmpdir / 'list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column(0).type == list_array.type\n    assert table == orig_table\n\n\n@pytest.mark.parquet\ndef test_parquet_extension_nested_in_extension(tmpdir):\n    # Parquet support for extension<list<extension>>\n    import pyarrow.parquet as pq\n\n    inner_ext_type = IntegerType()\n    inner_storage = pa.array([4, 5, 6, 7], type=pa.int64())\n    inner_ext_array = pa.ExtensionArray.from_storage(inner_ext_type,\n                                                     inner_storage)\n\n    list_array = pa.ListArray.from_arrays([0, 1, None, 3], inner_ext_array)\n    mylist_array = pa.ExtensionArray.from_storage(\n        MyListType(list_array.type), list_array)\n\n    orig_table = pa.table({'lists': mylist_array})\n    filename = tmpdir / 'ext_of_list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column(0).type == mylist_array.type\n    assert table == orig_table\n\n\ndef test_to_numpy():\n    period_type = PeriodType('D')\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n\n    expected = storage.to_numpy()\n    result = arr.to_numpy()\n    np.testing.assert_array_equal(result, expected)\n\n    result = np.asarray(arr)\n    np.testing.assert_array_equal(result, expected)\n\n    # chunked array\n    a1 = pa.chunked_array([arr, arr])\n    a2 = pa.chunked_array([arr, arr], type=period_type)\n    expected = np.hstack([expected, expected])\n\n    for charr in [a1, a2]:\n        assert charr.type == period_type\n        for result in [np.asarray(charr), charr.to_numpy()]:\n            assert result.dtype == np.int64\n            np.testing.assert_array_equal(result, expected)\n\n    # zero chunks\n    charr = pa.chunked_array([], type=period_type)\n    assert charr.type == period_type\n\n    for result in [np.asarray(charr), charr.to_numpy()]:\n        assert result.dtype == np.int64\n        np.testing.assert_array_equal(result, np.array([], dtype='int64'))\n\n\ndef test_empty_take():\n    # https://issues.apache.org/jira/browse/ARROW-13474\n    ext_type = IntegerType()\n    storage = pa.array([], type=pa.int64())\n    empty_arr = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = empty_arr.filter(pa.array([], pa.bool_()))\n    assert len(result) == 0\n    assert result.equals(empty_arr)\n\n    result = empty_arr.take(pa.array([], pa.int32()))\n    assert len(result) == 0\n    assert result.equals(empty_arr)\n\n\n@pytest.mark.parametrize(\"data,ty\", (\n    ([1, 2, 3], IntegerType),\n    ([\"cat\", \"dog\", \"horse\"], LabelType)\n))\n@pytest.mark.parametrize(\n    \"into\", [\"to_numpy\", pytest.param(\"to_pandas\", marks=pytest.mark.pandas)])\ndef test_extension_array_to_numpy_pandas(data, ty, into):\n    storage = pa.array(data)\n    ext_arr = pa.ExtensionArray.from_storage(ty(), storage)\n    offsets = pa.array([0, 1, 2, 3])\n    list_arr = pa.ListArray.from_arrays(offsets, ext_arr)\n    result = getattr(list_arr, into)(zero_copy_only=False)\n\n    list_arr_storage_type = list_arr.cast(pa.list_(ext_arr.type.storage_type))\n    expected = getattr(list_arr_storage_type, into)(zero_copy_only=False)\n    if into == \"to_pandas\":\n        assert result.equals(expected)\n    else:\n        assert np.array_equal(result, expected)\n\n\ndef test_array_constructor():\n    ext_type = IntegerType()\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = pa.array([1, 2, 3], type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(np.array([1, 2, 3]), type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(np.array([1.0, 2.0, 3.0]), type=IntegerType())\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_array_constructor_from_pandas():\n    import pandas as pd\n\n    ext_type = IntegerType()\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = pa.array(pd.Series([1, 2, 3]), type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(\n        pd.Series([1, 2, 3], dtype=\"category\"), type=IntegerType()\n    )\n    assert result.equals(expected)\n\n\n@pytest.mark.cython\ndef test_cpp_extension_in_python(tmpdir):\n    from .test_cython import (\n        setup_template, compiler_opts, test_ld_path, test_util, here)\n    with tmpdir.as_cwd():\n        # Set up temporary workspace\n        pyx_file = 'extensions.pyx'\n        shutil.copyfile(os.path.join(here, pyx_file),\n                        os.path.join(str(tmpdir), pyx_file))\n        # Create setup.py file\n        setup_code = setup_template.format(pyx_file=pyx_file,\n                                           compiler_opts=compiler_opts,\n                                           test_ld_path=test_ld_path)\n        with open('setup.py', 'w') as f:\n            f.write(setup_code)\n\n        subprocess_env = test_util.get_modified_env_with_pythonpath()\n\n        # Compile extension module\n        subprocess.check_call([sys.executable, 'setup.py',\n                               'build_ext', '--inplace'],\n                              env=subprocess_env)\n\n    sys.path.insert(0, str(tmpdir))\n    mod = __import__('extensions')\n\n    uuid_type = mod._make_uuid_type()\n    assert uuid_type.extension_name == \"uuid\"\n    assert uuid_type.storage_type == pa.binary(16)\n\n    array = mod._make_uuid_array()\n    assert array.type == uuid_type\n    assert array.to_pylist() == [b'abcdefghijklmno0', b'0onmlkjihgfedcba']\n    assert array[0].as_py() == b'abcdefghijklmno0'\n    assert array[1].as_py() == b'0onmlkjihgfedcba'\n\n    buf = ipc_write_batch(pa.RecordBatch.from_arrays([array], [\"uuid\"]))\n\n    batch = ipc_read_batch(buf)\n    reconstructed_array = batch.column(0)\n    assert reconstructed_array.type == uuid_type\n    assert reconstructed_array == array\n\n\ndef test_tensor_type():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 3])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.int8(), 6)\n    assert tensor_type.shape == [2, 3]\n    assert tensor_type.dim_names is None\n    assert tensor_type.permutation is None\n\n    tensor_type = pa.fixed_shape_tensor(pa.float64(), [2, 2, 3],\n                                        permutation=[0, 2, 1])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.float64(), 12)\n    assert tensor_type.shape == [2, 2, 3]\n    assert tensor_type.dim_names is None\n    assert tensor_type.permutation == [0, 2, 1]\n\n    tensor_type = pa.fixed_shape_tensor(pa.bool_(), [2, 2, 3],\n                                        dim_names=['C', 'H', 'W'])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.bool_(), 12)\n    assert tensor_type.shape == [2, 2, 3]\n    assert tensor_type.dim_names == ['C', 'H', 'W']\n    assert tensor_type.permutation is None\n\n\ndef test_tensor_class_methods():\n    tensor_type = pa.fixed_shape_tensor(pa.float32(), [2, 3])\n    storage = pa.array([[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]],\n                       pa.list_(pa.float32(), 6))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    expected = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]], dtype=np.float32)\n    result = arr.to_numpy_ndarray()\n    np.testing.assert_array_equal(result, expected)\n\n    expected = np.array([[[1, 2, 3], [4, 5, 6]]], dtype=np.float32)\n    result = arr[:1].to_numpy_ndarray()\n    np.testing.assert_array_equal(result, expected)\n\n    arr = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]],\n        dtype=np.float32, order=\"C\")\n    tensor_array_from_numpy = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n    assert isinstance(tensor_array_from_numpy.type, pa.FixedShapeTensorType)\n    assert tensor_array_from_numpy.type.value_type == pa.float32()\n    assert tensor_array_from_numpy.type.shape == [2, 3]\n\n    arr = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]],\n        dtype=np.float32, order=\"F\")\n    with pytest.raises(ValueError, match=\"C-style contiguous segment\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], permutation=[0, 2, 1])\n    storage = pa.array([[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]], pa.list_(pa.int8(), 12))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    with pytest.raises(ValueError, match=\"non-permuted tensors\"):\n        arr.to_numpy_ndarray()\n\n\n@pytest.mark.parametrize(\"tensor_type\", (\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3]),\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], permutation=[0, 2, 1]),\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], dim_names=['C', 'H', 'W'])\n))\ndef test_tensor_type_ipc(tensor_type):\n    storage = pa.array([[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]], pa.list_(pa.int8(), 12))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    # check the built array has exactly the expected clss\n    tensor_class = tensor_type.__arrow_ext_class__()\n    assert isinstance(arr, tensor_class)\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n\n    result = batch.column(0)\n    # check the deserialized array class is the expected one\n    assert isinstance(result, tensor_class)\n    assert result.type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert arr.storage.to_pylist() == [[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]]\n\n    # we get back an actual TensorType\n    assert isinstance(result.type, pa.FixedShapeTensorType)\n    assert result.type.value_type == pa.int8()\n    assert result.type.shape == [2, 2, 3]\n\n\ndef test_tensor_type_equality():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n\n    tensor_type2 = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3])\n    tensor_type3 = pa.fixed_shape_tensor(pa.uint8(), [2, 2, 3])\n    assert tensor_type == tensor_type2\n    assert not tensor_type == tensor_type3\n\n\n@pytest.mark.pandas\ndef test_extension_to_pandas_storage_type(registered_period_type):\n    period_type, _ = registered_period_type\n    np_arr = np.array([1, 2, 3, 4], dtype='i8')\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n\n    if isinstance(period_type, PeriodTypeWithToPandasDtype):\n        pandas_dtype = period_type.to_pandas_dtype()\n    else:\n        pandas_dtype = np_arr.dtype\n\n    # Test arrays\n    result = arr.to_pandas()\n    assert result.dtype == pandas_dtype\n\n    # Test chunked arrays\n    chunked_arr = pa.chunked_array([arr])\n    result = chunked_arr.to_numpy()\n    assert result.dtype == np_arr.dtype\n\n    result = chunked_arr.to_pandas()\n    assert result.dtype == pandas_dtype\n\n    # Test Table.to_pandas\n    data = [\n        pa.array([1, 2, 3, 4]),\n        pa.array(['foo', 'bar', None, None]),\n        pa.array([True, None, True, False]),\n        arr\n    ]\n    my_schema = pa.schema([('f0', pa.int8()),\n                           ('f1', pa.string()),\n                           ('f2', pa.bool_()),\n                           ('ext', period_type)])\n    table = pa.Table.from_arrays(data, schema=my_schema)\n    result = table.to_pandas()\n    assert result[\"ext\"].dtype == pandas_dtype\n\n    import pandas as pd\n    # Skip tests for 2.0.x, See: GH-35821\n    if (\n        Version(pd.__version__) >= Version(\"2.1.0\")\n    ):\n        # Check the usage of types_mapper\n        result = table.to_pandas(types_mapper=pd.ArrowDtype)\n        assert isinstance(result[\"ext\"].dtype, pd.ArrowDtype)\n\n\ndef test_tensor_type_is_picklable(pickle_module):\n    # GH-35599\n\n    expected_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))\n    result = pickle_module.loads(pickle_module.dumps(expected_type))\n\n    assert result == expected_type\n\n    arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n    storage = pa.array(arr, pa.list_(pa.int32(), 4))\n    expected_arr = pa.ExtensionArray.from_storage(expected_type, storage)\n    result = pickle_module.loads(pickle_module.dumps(expected_arr))\n\n    assert result == expected_arr\n\n\n@pytest.mark.parametrize((\"tensor_type\", \"text\"), [\n    (\n        pa.fixed_shape_tensor(pa.int8(), [2, 2, 3]),\n        'fixed_shape_tensor[value_type=int8, shape=[2,2,3]]'\n    ),\n    (\n        pa.fixed_shape_tensor(pa.int32(), [2, 2, 3], permutation=[0, 2, 1]),\n        'fixed_shape_tensor[value_type=int32, shape=[2,2,3], permutation=[0,2,1]]'\n    ),\n    (\n        pa.fixed_shape_tensor(pa.int64(), [2, 2, 3], dim_names=['C', 'H', 'W']),\n        'fixed_shape_tensor[value_type=int64, shape=[2,2,3], dim_names=[C,H,W]]'\n    )\n])\ndef test_tensor_type_str(tensor_type, text):\n    tensor_type_str = tensor_type.__str__()\n    assert text in tensor_type_str\n", "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport gc\nimport decimal\nimport json\nimport multiprocessing as mp\nimport sys\nimport warnings\n\nfrom collections import OrderedDict\nfrom datetime import date, datetime, time, timedelta, timezone\n\nimport hypothesis as h\nimport hypothesis.strategies as st\nimport numpy as np\nimport numpy.testing as npt\nimport pytest\n\nfrom pyarrow.pandas_compat import get_logical_type, _pandas_api\nfrom pyarrow.tests.util import invoke_script, random_ascii, rands\nimport pyarrow.tests.strategies as past\nfrom pyarrow.vendored.version import Version\n\nimport pyarrow as pa\ntry:\n    from pyarrow import parquet as pq\nexcept ImportError:\n    pass\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n    from .pandas_examples import dataframe_with_arrays, dataframe_with_lists\nexcept ImportError:\n    pass\n\n\ntry:\n    _np_VisibleDeprecationWarning = np.VisibleDeprecationWarning\nexcept AttributeError:\n    from numpy.exceptions import (\n        VisibleDeprecationWarning as _np_VisibleDeprecationWarning\n    )\n\n\n# Marks all of the tests in this module\npytestmark = pytest.mark.pandas\n\n\ndef _alltypes_example(size=100):\n    return pd.DataFrame({\n        'uint8': np.arange(size, dtype=np.uint8),\n        'uint16': np.arange(size, dtype=np.uint16),\n        'uint32': np.arange(size, dtype=np.uint32),\n        'uint64': np.arange(size, dtype=np.uint64),\n        'int8': np.arange(size, dtype=np.int16),\n        'int16': np.arange(size, dtype=np.int16),\n        'int32': np.arange(size, dtype=np.int32),\n        'int64': np.arange(size, dtype=np.int64),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'datetime[s]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                 dtype='datetime64[s]'),\n        'datetime[ms]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[ms]'),\n        'datetime[us]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[us]'),\n        'datetime[ns]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[ns]'),\n        'timedelta64[s]': np.arange(0, size, dtype='timedelta64[s]'),\n        'timedelta64[ms]': np.arange(0, size, dtype='timedelta64[ms]'),\n        'timedelta64[us]': np.arange(0, size, dtype='timedelta64[us]'),\n        'timedelta64[ns]': np.arange(0, size, dtype='timedelta64[ns]'),\n        'str': [str(x) for x in range(size)],\n        'str_with_nulls': [None] + [str(x) for x in range(size - 2)] + [None],\n        'empty_str': [''] * size\n    })\n\n\ndef _check_pandas_roundtrip(df, expected=None, use_threads=False,\n                            expected_schema=None,\n                            check_dtype=True, schema=None,\n                            preserve_index=False,\n                            as_batch=False):\n    klass = pa.RecordBatch if as_batch else pa.Table\n    table = klass.from_pandas(df, schema=schema,\n                              preserve_index=preserve_index,\n                              nthreads=2 if use_threads else 1)\n    result = table.to_pandas(use_threads=use_threads)\n\n    if expected_schema:\n        # all occurrences of _check_pandas_roundtrip passes expected_schema\n        # without the pandas generated key-value metadata\n        assert table.schema.equals(expected_schema)\n\n    if expected is None:\n        expected = df\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \"elementwise comparison failed\", DeprecationWarning)\n        tm.assert_frame_equal(result, expected, check_dtype=check_dtype,\n                              check_index_type=('equiv' if preserve_index\n                                                else False))\n\n\ndef _check_series_roundtrip(s, type_=None, expected_pa_type=None):\n    arr = pa.array(s, from_pandas=True, type=type_)\n\n    if type_ is not None and expected_pa_type is None:\n        expected_pa_type = type_\n\n    if expected_pa_type is not None:\n        assert arr.type == expected_pa_type\n\n    result = pd.Series(arr.to_pandas(), name=s.name)\n    tm.assert_series_equal(s, result)\n\n\ndef _check_array_roundtrip(values, expected=None, mask=None,\n                           type=None):\n    arr = pa.array(values, from_pandas=True, mask=mask, type=type)\n    result = arr.to_pandas()\n\n    values_nulls = pd.isnull(values)\n    if mask is None:\n        assert arr.null_count == values_nulls.sum()\n    else:\n        assert arr.null_count == (mask | values_nulls).sum()\n\n    if expected is None:\n        if mask is None:\n            expected = pd.Series(values)\n        else:\n            expected = pd.Series(values).copy()\n            expected[mask.copy()] = None\n\n    tm.assert_series_equal(pd.Series(result), expected, check_names=False)\n\n\ndef _check_array_from_pandas_roundtrip(np_array, type=None):\n    arr = pa.array(np_array, from_pandas=True, type=type)\n    result = arr.to_pandas()\n    npt.assert_array_equal(result, np_array)\n\n\nclass TestConvertMetadata:\n    \"\"\"\n    Conversion tests for Pandas metadata & indices.\n    \"\"\"\n\n    def test_non_string_columns(self):\n        df = pd.DataFrame({0: [1, 2, 3]})\n        table = pa.Table.from_pandas(df)\n        assert table.field(0).name == '0'\n\n    def test_non_string_columns_with_index(self):\n        df = pd.DataFrame({0: [1.0, 2.0, 3.0], 1: [4.0, 5.0, 6.0]})\n        df = df.set_index(0)\n\n        # assert that the from_pandas raises the warning\n        with pytest.warns(UserWarning):\n            table = pa.Table.from_pandas(df)\n            assert table.field(0).name == '1'\n\n        expected = df.copy()\n        # non-str index name will be converted to str\n        expected.index.name = str(expected.index.name)\n        with pytest.warns(UserWarning):\n            _check_pandas_roundtrip(df, expected=expected,\n                                    preserve_index=True)\n\n    def test_from_pandas_with_columns(self):\n        df = pd.DataFrame({0: [1, 2, 3], 1: [1, 3, 3], 2: [2, 4, 5]},\n                          columns=[1, 0])\n\n        table = pa.Table.from_pandas(df, columns=[0, 1])\n        expected = pa.Table.from_pandas(df[[0, 1]])\n        assert expected.equals(table)\n\n        record_batch_table = pa.RecordBatch.from_pandas(df, columns=[0, 1])\n        record_batch_expected = pa.RecordBatch.from_pandas(df[[0, 1]])\n        assert record_batch_expected.equals(record_batch_table)\n\n    def test_column_index_names_are_preserved(self):\n        df = pd.DataFrame({'data': [1, 2, 3]})\n        df.columns.names = ['a']\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_column_index_names_with_tz(self):\n        # ARROW-13756\n        # Bug if index is timezone aware DataTimeIndex\n\n        df = pd.DataFrame(\n            np.random.randn(5, 3),\n            columns=pd.date_range(\"2021-01-01\", periods=3, freq=\"50D\", tz=\"CET\")\n        )\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_range_index_shortcut(self):\n        # ARROW-1639\n        index_name = 'foo'\n        df = pd.DataFrame({'a': [1, 2, 3, 4]},\n                          index=pd.RangeIndex(0, 8, step=2, name=index_name))\n\n        df2 = pd.DataFrame({'a': [4, 5, 6, 7]},\n                           index=pd.RangeIndex(0, 4))\n\n        table = pa.Table.from_pandas(df)\n        table_no_index_name = pa.Table.from_pandas(df2)\n\n        # The RangeIndex is tracked in the metadata only\n        assert len(table.schema) == 1\n\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n        assert isinstance(result.index, pd.RangeIndex)\n        assert _pandas_api.get_rangeindex_attribute(result.index, 'step') == 2\n        assert result.index.name == index_name\n\n        result2 = table_no_index_name.to_pandas()\n        tm.assert_frame_equal(result2, df2)\n        assert isinstance(result2.index, pd.RangeIndex)\n        assert _pandas_api.get_rangeindex_attribute(result2.index, 'step') == 1\n        assert result2.index.name is None\n\n    def test_range_index_force_serialization(self):\n        # ARROW-5427: preserve_index=True will force the RangeIndex to\n        # be serialized as a column rather than tracked more\n        # efficiently as metadata\n        df = pd.DataFrame({'a': [1, 2, 3, 4]},\n                          index=pd.RangeIndex(0, 8, step=2, name='foo'))\n\n        table = pa.Table.from_pandas(df, preserve_index=True)\n        assert table.num_columns == 2\n        assert 'foo' in table.column_names\n\n        restored = table.to_pandas()\n        tm.assert_frame_equal(restored, df)\n\n    def test_rangeindex_doesnt_warn(self):\n        # ARROW-5606: pandas 0.25 deprecated private _start/stop/step\n        # attributes -> can be removed if support < pd 0.25 is dropped\n        df = pd.DataFrame(np.random.randn(4, 2), columns=['a', 'b'])\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"error\")\n            _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns(self):\n        columns = pd.MultiIndex.from_arrays([\n            ['one', 'two'], ['X', 'Y']\n        ])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns_with_dtypes(self):\n        columns = pd.MultiIndex.from_arrays(\n            [\n                ['one', 'two'],\n                pd.DatetimeIndex(['2017-08-01', '2017-08-02']),\n            ],\n            names=['level_1', 'level_2'],\n        )\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_with_column_dtype_object(self):\n        # ARROW-3651 & ARROW-9096\n        # Bug when dtype of the columns is object.\n\n        # uinderlying dtype: integer\n        df = pd.DataFrame([1], columns=pd.Index([1], dtype=object))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # underlying dtype: floating\n        df = pd.DataFrame([1], columns=pd.Index([1.1], dtype=object))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # underlying dtype: datetime\n        # ARROW-9096: a simple roundtrip now works\n        df = pd.DataFrame([1], columns=pd.Index(\n            [datetime(2018, 1, 1)], dtype=\"object\"))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns_unicode(self):\n        columns = pd.MultiIndex.from_arrays([['\u3042', '\u3044'], ['X', 'Y']])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_doesnt_warn(self):\n        # ARROW-3953: pandas 0.24 rename of MultiIndex labels to codes\n        columns = pd.MultiIndex.from_arrays([['one', 'two'], ['X', 'Y']])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"error\")\n            _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_integer_index_column(self):\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')])\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_index_metadata_field_name(self):\n        # test None case, and strangely named non-index columns\n        df = pd.DataFrame(\n            [(1, 'a', 3.1), (2, 'b', 2.2), (3, 'c', 1.3)],\n            index=pd.MultiIndex.from_arrays(\n                [['c', 'b', 'a'], [3, 2, 1]],\n                names=[None, 'foo']\n            ),\n            columns=['a', None, '__index_level_0__'],\n        )\n        with pytest.warns(UserWarning):\n            t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        col1, col2, col3, idx0, foo = js['columns']\n\n        assert col1['name'] == 'a'\n        assert col1['name'] == col1['field_name']\n\n        assert col2['name'] is None\n        assert col2['field_name'] == 'None'\n\n        assert col3['name'] == '__index_level_0__'\n        assert col3['name'] == col3['field_name']\n\n        idx0_descr, foo_descr = js['index_columns']\n        assert idx0_descr == '__index_level_0__'\n        assert idx0['field_name'] == idx0_descr\n        assert idx0['name'] is None\n\n        assert foo_descr == 'foo'\n        assert foo['field_name'] == foo_descr\n        assert foo['name'] == foo_descr\n\n    def test_categorical_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.Index(list('def'), dtype='category')\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] is None\n        assert column_indexes['pandas_type'] == 'categorical'\n        assert column_indexes['numpy_type'] == 'int8'\n\n        md = column_indexes['metadata']\n        assert md['num_categories'] == 3\n        assert md['ordered'] is False\n\n    def test_string_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.Index(list('def'), name='stringz')\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] == 'stringz'\n        assert column_indexes['name'] == column_indexes['field_name']\n        assert column_indexes['numpy_type'] == 'object'\n        assert column_indexes['pandas_type'] == 'unicode'\n\n        md = column_indexes['metadata']\n\n        assert len(md) == 1\n        assert md['encoding'] == 'UTF-8'\n\n    def test_datetimetz_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.date_range(\n                start='2017-01-01', periods=3, tz='America/New_York'\n            )\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] is None\n        assert column_indexes['pandas_type'] == 'datetimetz'\n        assert column_indexes['numpy_type'] == 'datetime64[ns]'\n\n        md = column_indexes['metadata']\n        assert md['timezone'] == 'America/New_York'\n\n    def test_datetimetz_row_index(self):\n        df = pd.DataFrame({\n            'a': pd.date_range(\n                start='2017-01-01', periods=3, tz='America/New_York'\n            )\n        })\n        df = df.set_index('a')\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_categorical_row_index(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [1, 2, 3]})\n        df['a'] = df.a.astype('category')\n        df = df.set_index('a')\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_duplicate_column_names_does_not_crash(self):\n        df = pd.DataFrame([(1, 'a'), (2, 'b')], columns=list('aa'))\n        with pytest.raises(ValueError):\n            pa.Table.from_pandas(df)\n\n    def test_dictionary_indices_boundscheck(self):\n        # ARROW-1658. No validation of indices leads to segfaults in pandas\n        indices = [[0, 1], [0, -1]]\n\n        for inds in indices:\n            arr = pa.DictionaryArray.from_arrays(inds, ['a'], safe=False)\n            batch = pa.RecordBatch.from_arrays([arr], ['foo'])\n            table = pa.Table.from_batches([batch, batch, batch])\n\n            with pytest.raises(IndexError):\n                arr.to_pandas()\n\n            with pytest.raises(IndexError):\n                table.to_pandas()\n\n    def test_unicode_with_unicode_column_and_index(self):\n        df = pd.DataFrame({'\u3042': ['\u3044']}, index=['\u3046'])\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_mixed_column_names(self):\n        # mixed type column names are not reconstructed exactly\n        df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n\n        for cols in [['\u3042', b'a'], [1, '2'], [1, 1.5]]:\n            df.columns = pd.Index(cols, dtype=object)\n\n            # assert that the from_pandas raises the warning\n            with pytest.warns(UserWarning):\n                pa.Table.from_pandas(df)\n\n            expected = df.copy()\n            expected.columns = df.columns.values.astype(str)\n            with pytest.warns(UserWarning):\n                _check_pandas_roundtrip(df, expected=expected,\n                                        preserve_index=True)\n\n    def test_binary_column_name(self):\n        if Version(\"2.0.0\") <= Version(pd.__version__) < Version(\"2.2.0\"):\n            # TODO: regression in pandas, hopefully fixed in next version\n            # https://issues.apache.org/jira/browse/ARROW-18394\n            # https://github.com/pandas-dev/pandas/issues/50127\n            pytest.skip(\"Regression in pandas 2.0.0\")\n        column_data = ['\u3044']\n        key = '\u3042'.encode()\n        data = {key: column_data}\n        df = pd.DataFrame(data)\n\n        # we can't use _check_pandas_roundtrip here because our metadata\n        # is always decoded as utf8: even if binary goes in, utf8 comes out\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        df2 = t.to_pandas()\n        assert df.values[0] == df2.values[0]\n        assert df.index.values[0] == df2.index.values[0]\n        assert df.columns[0] == key\n\n    def test_multiindex_duplicate_values(self):\n        num_rows = 3\n        numbers = list(range(num_rows))\n        index = pd.MultiIndex.from_arrays(\n            [['foo', 'foo', 'bar'], numbers],\n            names=['foobar', 'some_numbers'],\n        )\n\n        df = pd.DataFrame({'numbers': numbers}, index=index)\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_metadata_with_mixed_types(self):\n        df = pd.DataFrame({'data': [b'some_bytes', 'some_unicode']})\n        table = pa.Table.from_pandas(df)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'bytes'\n        assert data_column['numpy_type'] == 'object'\n\n    def test_ignore_metadata(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': ['foo', 'bar', 'baz']},\n                          index=['one', 'two', 'three'])\n        table = pa.Table.from_pandas(df)\n\n        result = table.to_pandas(ignore_metadata=True)\n        expected = (table.cast(table.schema.remove_metadata())\n                    .to_pandas())\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_list_metadata(self):\n        df = pd.DataFrame({'data': [[1], [2, 3, 4], [5] * 7]})\n        schema = pa.schema([pa.field('data', type=pa.list_(pa.int64()))])\n        table = pa.Table.from_pandas(df, schema=schema)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'list[int64]'\n        assert data_column['numpy_type'] == 'object'\n\n    def test_struct_metadata(self):\n        df = pd.DataFrame({'dicts': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n        table = pa.Table.from_pandas(df)\n        pandas_metadata = table.schema.pandas_metadata\n        assert pandas_metadata['columns'][0]['pandas_type'] == 'object'\n\n    def test_decimal_metadata(self):\n        expected = pd.DataFrame({\n            'decimals': [\n                decimal.Decimal('394092382910493.12341234678'),\n                -decimal.Decimal('314292388910493.12343437128'),\n            ]\n        })\n        table = pa.Table.from_pandas(expected)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'decimal'\n        assert data_column['numpy_type'] == 'object'\n        assert data_column['metadata'] == {'precision': 26, 'scale': 11}\n\n    def test_table_column_subset_metadata(self):\n        # ARROW-1883\n        # non-default index\n        for index in [\n                pd.Index(['a', 'b', 'c'], name='index'),\n                pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')]:\n            df = pd.DataFrame({'a': [1, 2, 3],\n                               'b': [.1, .2, .3]}, index=index)\n            table = pa.Table.from_pandas(df)\n\n            table_subset = table.remove_column(1)\n            result = table_subset.to_pandas()\n            expected = df[['a']]\n            if isinstance(df.index, pd.DatetimeIndex):\n                df.index.freq = None\n            tm.assert_frame_equal(result, expected)\n\n            table_subset2 = table_subset.remove_column(1)\n            result = table_subset2.to_pandas()\n            tm.assert_frame_equal(result, df[['a']].reset_index(drop=True))\n\n    def test_to_pandas_column_subset_multiindex(self):\n        # ARROW-10122\n        df = pd.DataFrame(\n            {\"first\": list(range(5)),\n             \"second\": list(range(5)),\n             \"value\": np.arange(5)}\n        )\n        table = pa.Table.from_pandas(df.set_index([\"first\", \"second\"]))\n\n        subset = table.select([\"first\", \"value\"])\n        result = subset.to_pandas()\n        expected = df[[\"first\", \"value\"]].set_index(\"first\")\n        tm.assert_frame_equal(result, expected)\n\n    def test_empty_list_metadata(self):\n        # Create table with array of empty lists, forced to have type\n        # list(string) in pyarrow\n        c1 = [[\"test\"], [\"a\", \"b\"], None]\n        c2 = [[], [], []]\n        arrays = OrderedDict([\n            ('c1', pa.array(c1, type=pa.list_(pa.string()))),\n            ('c2', pa.array(c2, type=pa.list_(pa.string()))),\n        ])\n        rb = pa.RecordBatch.from_arrays(\n            list(arrays.values()),\n            list(arrays.keys())\n        )\n        tbl = pa.Table.from_batches([rb])\n\n        # First roundtrip changes schema, because pandas cannot preserve the\n        # type of empty lists\n        df = tbl.to_pandas()\n        tbl2 = pa.Table.from_pandas(df)\n        md2 = tbl2.schema.pandas_metadata\n\n        # Second roundtrip\n        df2 = tbl2.to_pandas()\n        expected = pd.DataFrame(OrderedDict([('c1', c1), ('c2', c2)]))\n\n        tm.assert_frame_equal(df2, expected)\n\n        assert md2['columns'] == [\n            {\n                'name': 'c1',\n                'field_name': 'c1',\n                'metadata': None,\n                'numpy_type': 'object',\n                'pandas_type': 'list[unicode]',\n            },\n            {\n                'name': 'c2',\n                'field_name': 'c2',\n                'metadata': None,\n                'numpy_type': 'object',\n                'pandas_type': 'list[empty]',\n            }\n        ]\n\n    def test_metadata_pandas_version(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [1, 2, 3]})\n        table = pa.Table.from_pandas(df)\n        assert table.schema.pandas_metadata['pandas_version'] is not None\n\n    def test_mismatch_metadata_schema(self):\n        # ARROW-10511\n        # It is possible that the metadata and actual schema is not fully\n        # matching (eg no timezone information for tz-aware column)\n        # -> to_pandas() conversion should not fail on that\n        df = pd.DataFrame({\"datetime\": pd.date_range(\"2020-01-01\", periods=3)})\n\n        # OPTION 1: casting after conversion\n        table = pa.Table.from_pandas(df)\n        # cast the \"datetime\" column to be tz-aware\n        new_col = table[\"datetime\"].cast(pa.timestamp('ns', tz=\"UTC\"))\n        new_table1 = table.set_column(\n            0, pa.field(\"datetime\", new_col.type), new_col\n        )\n\n        # OPTION 2: specify schema during conversion\n        schema = pa.schema([(\"datetime\", pa.timestamp('ns', tz=\"UTC\"))])\n        new_table2 = pa.Table.from_pandas(df, schema=schema)\n\n        expected = df.copy()\n        expected[\"datetime\"] = expected[\"datetime\"].dt.tz_localize(\"UTC\")\n\n        for new_table in [new_table1, new_table2]:\n            # ensure the new table still has the pandas metadata\n            assert new_table.schema.pandas_metadata is not None\n            # convert to pandas\n            result = new_table.to_pandas()\n            tm.assert_frame_equal(result, expected)\n\n\nclass TestConvertPrimitiveTypes:\n    \"\"\"\n    Conversion tests for primitive (e.g. numeric) types.\n    \"\"\"\n\n    def test_float_no_nulls(self):\n        data = {}\n        fields = []\n        dtypes = [('f2', pa.float16()),\n                  ('f4', pa.float32()),\n                  ('f8', pa.float64())]\n        num_values = 100\n\n        for numpy_dtype, arrow_dtype in dtypes:\n            values = np.random.randn(num_values)\n            data[numpy_dtype] = values.astype(numpy_dtype)\n            fields.append(pa.field(numpy_dtype, arrow_dtype))\n\n        df = pd.DataFrame(data)\n        schema = pa.schema(fields)\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_float_nulls(self):\n        num_values = 100\n\n        null_mask = np.random.randint(0, 10, size=num_values) < 3\n        dtypes = [('f2', pa.float16()),\n                  ('f4', pa.float32()),\n                  ('f8', pa.float64())]\n        names = ['f2', 'f4', 'f8']\n        expected_cols = []\n\n        arrays = []\n        fields = []\n        for name, arrow_dtype in dtypes:\n            values = np.random.randn(num_values).astype(name)\n\n            arr = pa.array(values, from_pandas=True, mask=null_mask)\n            arrays.append(arr)\n            fields.append(pa.field(name, arrow_dtype))\n            values[null_mask] = np.nan\n\n            expected_cols.append(values)\n\n        ex_frame = pd.DataFrame(dict(zip(names, expected_cols)),\n                                columns=names)\n\n        table = pa.Table.from_arrays(arrays, names)\n        assert table.schema.equals(pa.schema(fields))\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_float_nulls_to_ints(self):\n        # ARROW-2135\n        df = pd.DataFrame({\"a\": [1.0, 2.0, np.nan]})\n        schema = pa.schema([pa.field(\"a\", pa.int16(), nullable=True)])\n        table = pa.Table.from_pandas(df, schema=schema, safe=False)\n        assert table[0].to_pylist() == [1, 2, None]\n        tm.assert_frame_equal(df, table.to_pandas())\n\n    def test_float_nulls_to_boolean(self):\n        s = pd.Series([0.0, 1.0, 2.0, None, -3.0])\n        expected = pd.Series([False, True, True, None, True])\n        _check_array_roundtrip(s, expected=expected, type=pa.bool_())\n\n    def test_series_from_pandas_false_respected(self):\n        # Check that explicit from_pandas=False is respected\n        s = pd.Series([0.0, np.nan])\n        arr = pa.array(s, from_pandas=False)\n        assert arr.null_count == 0\n        assert np.isnan(arr[1].as_py())\n\n    def test_integer_no_nulls(self):\n        data = OrderedDict()\n        fields = []\n\n        numpy_dtypes = [\n            ('i1', pa.int8()), ('i2', pa.int16()),\n            ('i4', pa.int32()), ('i8', pa.int64()),\n            ('u1', pa.uint8()), ('u2', pa.uint16()),\n            ('u4', pa.uint32()), ('u8', pa.uint64()),\n            ('longlong', pa.int64()), ('ulonglong', pa.uint64())\n        ]\n        num_values = 100\n\n        for dtype, arrow_dtype in numpy_dtypes:\n            info = np.iinfo(dtype)\n            values = np.random.randint(max(info.min, np.iinfo(np.int_).min),\n                                       min(info.max, np.iinfo(np.int_).max),\n                                       size=num_values)\n            data[dtype] = values.astype(dtype)\n            fields.append(pa.field(dtype, arrow_dtype))\n\n        df = pd.DataFrame(data)\n        schema = pa.schema(fields)\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_all_integer_types(self):\n        # Test all Numpy integer aliases\n        data = OrderedDict()\n        numpy_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8',\n                        'byte', 'ubyte', 'short', 'ushort', 'intc', 'uintc',\n                        'int_', 'uint', 'longlong', 'ulonglong']\n        for dtype in numpy_dtypes:\n            data[dtype] = np.arange(12, dtype=dtype)\n        df = pd.DataFrame(data)\n        _check_pandas_roundtrip(df)\n\n        # Do the same with pa.array()\n        # (for some reason, it doesn't use the same code paths at all)\n        for np_arr in data.values():\n            arr = pa.array(np_arr)\n            assert arr.to_pylist() == np_arr.tolist()\n\n    def test_integer_byteorder(self):\n        # Byteswapped arrays are not supported yet\n        int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n        for dt in int_dtypes:\n            for order in '=<>':\n                data = np.array([1, 2, 42], dtype=order + dt)\n                for np_arr in (data, data[::2]):\n                    if data.dtype.isnative:\n                        arr = pa.array(data)\n                        assert arr.to_pylist() == data.tolist()\n                    else:\n                        with pytest.raises(NotImplementedError):\n                            arr = pa.array(data)\n\n    def test_integer_with_nulls(self):\n        # pandas requires upcast to float dtype\n\n        int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n        num_values = 100\n\n        null_mask = np.random.randint(0, 10, size=num_values) < 3\n\n        expected_cols = []\n        arrays = []\n        for name in int_dtypes:\n            values = np.random.randint(0, 100, size=num_values)\n\n            arr = pa.array(values, mask=null_mask)\n            arrays.append(arr)\n\n            expected = values.astype('f8')\n            expected[null_mask] = np.nan\n\n            expected_cols.append(expected)\n\n        ex_frame = pd.DataFrame(dict(zip(int_dtypes, expected_cols)),\n                                columns=int_dtypes)\n\n        table = pa.Table.from_arrays(arrays, int_dtypes)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_array_from_pandas_type_cast(self):\n        arr = np.arange(10, dtype='int64')\n\n        target_type = pa.int8()\n\n        result = pa.array(arr, type=target_type)\n        expected = pa.array(arr.astype('int8'))\n        assert result.equals(expected)\n\n    def test_boolean_no_nulls(self):\n        num_values = 100\n\n        np.random.seed(0)\n\n        df = pd.DataFrame({'bools': np.random.randn(num_values) > 0})\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_boolean_nulls(self):\n        # pandas requires upcast to object dtype\n        num_values = 100\n        np.random.seed(0)\n\n        mask = np.random.randint(0, 10, size=num_values) < 3\n        values = np.random.randint(0, 10, size=num_values) < 5\n\n        arr = pa.array(values, mask=mask)\n\n        expected = values.astype(object)\n        expected[mask] = None\n\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        ex_frame = pd.DataFrame({'bools': expected})\n\n        table = pa.Table.from_arrays([arr], ['bools'])\n        assert table.schema.equals(schema)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_boolean_to_int(self):\n        # test from dtype=bool\n        s = pd.Series([True, True, False, True, True] * 2)\n        expected = pd.Series([1, 1, 0, 1, 1] * 2)\n        _check_array_roundtrip(s, expected=expected, type=pa.int64())\n\n    def test_boolean_objects_to_int(self):\n        # test from dtype=object\n        s = pd.Series([True, True, False, True, True] * 2, dtype=object)\n        expected = pd.Series([1, 1, 0, 1, 1] * 2)\n        expected_msg = 'Expected integer, got bool'\n        with pytest.raises(pa.ArrowTypeError, match=expected_msg):\n            _check_array_roundtrip(s, expected=expected, type=pa.int64())\n\n    def test_boolean_nulls_to_float(self):\n        # test from dtype=object\n        s = pd.Series([True, True, False, None, True] * 2)\n        expected = pd.Series([1.0, 1.0, 0.0, None, 1.0] * 2)\n        _check_array_roundtrip(s, expected=expected, type=pa.float64())\n\n    def test_boolean_multiple_columns(self):\n        # ARROW-6325 (multiple columns resulting in strided conversion)\n        df = pd.DataFrame(np.ones((3, 2), dtype='bool'), columns=['a', 'b'])\n        _check_pandas_roundtrip(df)\n\n    def test_float_object_nulls(self):\n        arr = np.array([None, 1.5, np.float64(3.5)] * 5, dtype=object)\n        df = pd.DataFrame({'floats': arr})\n        expected = pd.DataFrame({'floats': pd.to_numeric(arr)})\n        field = pa.field('floats', pa.float64())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected=expected,\n                                expected_schema=schema)\n\n    def test_float_with_null_as_integer(self):\n        # ARROW-2298\n        s = pd.Series([np.nan, 1., 2., np.nan])\n\n        types = [pa.int8(), pa.int16(), pa.int32(), pa.int64(),\n                 pa.uint8(), pa.uint16(), pa.uint32(), pa.uint64()]\n        for ty in types:\n            result = pa.array(s, type=ty)\n            expected = pa.array([None, 1, 2, None], type=ty)\n            assert result.equals(expected)\n\n            df = pd.DataFrame({'has_nulls': s})\n            schema = pa.schema([pa.field('has_nulls', ty)])\n            result = pa.Table.from_pandas(df, schema=schema,\n                                          preserve_index=False)\n            assert result[0].chunk(0).equals(expected)\n\n    def test_int_object_nulls(self):\n        arr = np.array([None, 1, np.int64(3)] * 5, dtype=object)\n        df = pd.DataFrame({'ints': arr})\n        expected = pd.DataFrame({'ints': pd.to_numeric(arr)})\n        field = pa.field('ints', pa.int64())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected=expected,\n                                expected_schema=schema)\n\n    def test_boolean_object_nulls(self):\n        arr = np.array([False, None, True] * 100, dtype=object)\n        df = pd.DataFrame({'bools': arr})\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_all_nulls_cast_numeric(self):\n        arr = np.array([None], dtype=object)\n\n        def _check_type(t):\n            a2 = pa.array(arr, type=t)\n            assert a2.type == t\n            assert a2[0].as_py() is None\n\n        _check_type(pa.int32())\n        _check_type(pa.float64())\n\n    def test_half_floats_from_numpy(self):\n        arr = np.array([1.5, np.nan], dtype=np.float16)\n        a = pa.array(arr, type=pa.float16())\n        x, y = a.to_pylist()\n        assert isinstance(x, np.float16)\n        assert x == 1.5\n        assert isinstance(y, np.float16)\n        assert np.isnan(y)\n\n        a = pa.array(arr, type=pa.float16(), from_pandas=True)\n        x, y = a.to_pylist()\n        assert isinstance(x, np.float16)\n        assert x == 1.5\n        assert y is None\n\n\n@pytest.mark.parametrize('dtype',\n                         ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8'])\ndef test_array_integer_object_nulls_option(dtype):\n    num_values = 100\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 100, size=num_values, dtype=dtype)\n\n    array = pa.array(values, mask=null_mask)\n\n    if null_mask.any():\n        expected = values.astype('O')\n        expected[null_mask] = None\n    else:\n        expected = values\n\n    result = array.to_pandas(integer_object_nulls=True)\n\n    np.testing.assert_equal(result, expected)\n\n\n@pytest.mark.parametrize('dtype',\n                         ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8'])\ndef test_table_integer_object_nulls_option(dtype):\n    num_values = 100\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 100, size=num_values, dtype=dtype)\n\n    array = pa.array(values, mask=null_mask)\n\n    if null_mask.any():\n        expected = values.astype('O')\n        expected[null_mask] = None\n    else:\n        expected = values\n\n    expected = pd.DataFrame({dtype: expected})\n\n    table = pa.Table.from_arrays([array], [dtype])\n    result = table.to_pandas(integer_object_nulls=True)\n\n    tm.assert_frame_equal(result, expected)\n\n\nclass TestConvertDateTimeLikeTypes:\n    \"\"\"\n    Conversion tests for datetime- and timestamp-like types (date64, etc.).\n    \"\"\"\n\n    def test_timestamps_notimezone_no_nulls(self):\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                '2006-01-13T12:34:56.432539784',\n                '2010-08-13T05:46:57.437699912'],\n                dtype='datetime64[ns]')\n        })\n        field = pa.field('datetime64', pa.timestamp('ns'))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    def test_timestamps_notimezone_nulls(self):\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                None,\n                '2010-08-13T05:46:57.437699912'],\n                dtype='datetime64[ns]')\n        })\n        field = pa.field('datetime64', pa.timestamp('ns'))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    @pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\n    def test_timestamps_with_timezone(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\") and unit != 'ns':\n            pytest.skip(\"pandas < 2.0 only supports nanosecond datetime64\")\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123',\n                '2006-01-13T12:34:56.432',\n                '2010-08-13T05:46:57.437'],\n                dtype=f'datetime64[{unit}]')\n        })\n        df['datetime64'] = df['datetime64'].dt.tz_localize('US/Eastern')\n        _check_pandas_roundtrip(df)\n\n        _check_series_roundtrip(df['datetime64'])\n\n        # drop-in a null\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                None,\n                '2006-01-13T12:34:56.432539784',\n                '2010-08-13T05:46:57.437699912'],\n                dtype=f'datetime64[{unit}]')\n        })\n        df['datetime64'] = df['datetime64'].dt.tz_localize('US/Eastern')\n\n        _check_pandas_roundtrip(df)\n\n    def test_python_datetime(self):\n        # ARROW-2106\n        date_array = [datetime.today() + timedelta(days=x) for x in range(10)]\n        df = pd.DataFrame({\n            'datetime': pd.Series(date_array, dtype=object)\n        })\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.TimestampArray)\n\n        result = table.to_pandas()\n        # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n        # so we need to cast the pandas dtype. Pandas v1 will always silently\n        # coerce to [ns] due to lack of non-[ns] support.\n        expected_df = pd.DataFrame({\n            'datetime': pd.Series(date_array, dtype='datetime64[us]')\n        })\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_python_datetime_with_pytz_tzinfo(self):\n        pytz = pytest.importorskip(\"pytz\")\n\n        for tz in [pytz.utc, pytz.timezone('US/Eastern'), pytz.FixedOffset(1)]:\n            values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz)]\n            df = pd.DataFrame({'datetime': values})\n            _check_pandas_roundtrip(df)\n\n    @h.given(st.none() | past.timezones)\n    @h.settings(deadline=None)\n    def test_python_datetime_with_pytz_timezone(self, tz):\n        if str(tz) in [\"build/etc/localtime\", \"Factory\"]:\n            pytest.skip(\"Localtime timezone not supported\")\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz)]\n        df = pd.DataFrame({'datetime': values})\n        _check_pandas_roundtrip(df, check_dtype=False)\n\n    def test_python_datetime_with_timezone_tzinfo(self):\n        pytz = pytest.importorskip(\"pytz\")\n        from datetime import timezone\n\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=timezone.utc)]\n        # also test with index to ensure both paths roundtrip (ARROW-9962)\n        df = pd.DataFrame({'datetime': values}, index=values)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # datetime.timezone is going to be pytz.FixedOffset\n        hours = 1\n        tz_timezone = timezone(timedelta(hours=hours))\n        tz_pytz = pytz.FixedOffset(hours * 60)\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz_timezone)]\n        values_exp = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz_pytz)]\n        df = pd.DataFrame({'datetime': values}, index=values)\n        df_exp = pd.DataFrame({'datetime': values_exp}, index=values_exp)\n        _check_pandas_roundtrip(df, expected=df_exp, preserve_index=True)\n\n    def test_python_datetime_subclass(self):\n\n        class MyDatetime(datetime):\n            # see https://github.com/pandas-dev/pandas/issues/21142\n            nanosecond = 0.0\n\n        date_array = [MyDatetime(2000, 1, 1, 1, 1, 1)]\n        df = pd.DataFrame({\"datetime\": pd.Series(date_array, dtype=object)})\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.TimestampArray)\n\n        result = table.to_pandas()\n\n        # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n        # so we need to cast the pandas dtype. Pandas v1 will always silently\n        # coerce to [ns] due to lack of non-[ns] support.\n        expected_df = pd.DataFrame(\n            {\"datetime\": pd.Series(date_array, dtype='datetime64[us]')})\n\n        # https://github.com/pandas-dev/pandas/issues/21142\n        expected_df[\"datetime\"] = pd.to_datetime(expected_df[\"datetime\"])\n\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_python_date_subclass(self):\n\n        class MyDate(date):\n            pass\n\n        date_array = [MyDate(2000, 1, 1)]\n        df = pd.DataFrame({\"date\": pd.Series(date_array, dtype=object)})\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.Date32Array)\n\n        result = table.to_pandas()\n        expected_df = pd.DataFrame(\n            {\"date\": np.array([date(2000, 1, 1)], dtype=object)}\n        )\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_datetime64_to_date32(self):\n        # ARROW-1718\n        arr = pa.array([date(2017, 10, 23), None])\n        c = pa.chunked_array([arr])\n        s = c.to_pandas()\n\n        arr2 = pa.Array.from_pandas(s, type=pa.date32())\n\n        assert arr2.equals(arr.cast('date32'))\n\n    @pytest.mark.parametrize('mask', [\n        None,\n        np.array([True, False, False, True, False, False]),\n    ])\n    def test_pandas_datetime_to_date64(self, mask):\n        s = pd.to_datetime([\n            '2018-05-10T00:00:00',\n            '2018-05-11T00:00:00',\n            '2018-05-12T00:00:00',\n            '2018-05-10T10:24:01',\n            '2018-05-11T10:24:01',\n            '2018-05-12T10:24:01',\n        ])\n        arr = pa.Array.from_pandas(s, type=pa.date64(), mask=mask)\n\n        data = np.array([\n            date(2018, 5, 10),\n            date(2018, 5, 11),\n            date(2018, 5, 12),\n            date(2018, 5, 10),\n            date(2018, 5, 11),\n            date(2018, 5, 12),\n        ])\n        expected = pa.array(data, mask=mask, type=pa.date64())\n\n        assert arr.equals(expected)\n\n    @pytest.mark.parametrize(\"coerce_to_ns,expected_dtype\",\n                             [(False, 'datetime64[ms]'),\n                              (True, 'datetime64[ns]')])\n    def test_array_types_date_as_object(self, coerce_to_ns, expected_dtype):\n        data = [date(2000, 1, 1),\n                None,\n                date(1970, 1, 1),\n                date(2040, 2, 26)]\n        expected_days = np.array(['2000-01-01', None, '1970-01-01',\n                                  '2040-02-26'], dtype='datetime64[D]')\n\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n            expected_dtype = 'datetime64[ns]'\n\n        expected = np.array(['2000-01-01', None, '1970-01-01',\n                             '2040-02-26'], dtype=expected_dtype)\n\n        objects = [pa.array(data),\n                   pa.chunked_array([data])]\n\n        for obj in objects:\n            result = obj.to_pandas(coerce_temporal_nanoseconds=coerce_to_ns)\n            expected_obj = expected_days.astype(object)\n            assert result.dtype == expected_obj.dtype\n            npt.assert_array_equal(result, expected_obj)\n\n            result = obj.to_pandas(date_as_object=False,\n                                   coerce_temporal_nanoseconds=coerce_to_ns)\n            assert result.dtype == expected.dtype\n            npt.assert_array_equal(result, expected)\n\n    @pytest.mark.parametrize(\"coerce_to_ns,expected_type\",\n                             [(False, 'datetime64[ms]'),\n                              (True, 'datetime64[ns]')])\n    def test_table_convert_date_as_object(self, coerce_to_ns, expected_type):\n        df = pd.DataFrame({\n            'date': [date(2000, 1, 1),\n                     None,\n                     date(1970, 1, 1),\n                     date(2040, 2, 26)]})\n\n        table = pa.Table.from_pandas(df, preserve_index=False)\n\n        df_datetime = table.to_pandas(date_as_object=False,\n                                      coerce_temporal_nanoseconds=coerce_to_ns)\n        df_object = table.to_pandas()\n\n        tm.assert_frame_equal(df.astype(expected_type), df_datetime,\n                              check_dtype=True)\n        tm.assert_frame_equal(df, df_object, check_dtype=True)\n\n    @pytest.mark.parametrize(\"arrow_type\",\n                             [pa.date32(), pa.date64(), pa.timestamp('s'),\n                              pa.timestamp('ms'), pa.timestamp('us'),\n                              pa.timestamp('ns'), pa.timestamp('s', 'UTC'),\n                              pa.timestamp('ms', 'UTC'), pa.timestamp('us', 'UTC'),\n                              pa.timestamp('ns', 'UTC')])\n    def test_array_coerce_temporal_nanoseconds(self, arrow_type):\n        data = [date(2000, 1, 1), datetime(2001, 1, 1)]\n        expected = pd.Series(data)\n        arr = pa.array(data).cast(arrow_type)\n        result = arr.to_pandas(\n            coerce_temporal_nanoseconds=True, date_as_object=False)\n        expected_tz = None\n        if hasattr(arrow_type, 'tz') and arrow_type.tz is not None:\n            expected_tz = 'UTC'\n        expected_type = pa.timestamp('ns', expected_tz).to_pandas_dtype()\n        tm.assert_series_equal(result, expected.astype(expected_type))\n\n    @pytest.mark.parametrize(\"arrow_type\",\n                             [pa.date32(), pa.date64(), pa.timestamp('s'),\n                              pa.timestamp('ms'), pa.timestamp('us'),\n                              pa.timestamp('ns'), pa.timestamp('s', 'UTC'),\n                              pa.timestamp('ms', 'UTC'), pa.timestamp('us', 'UTC'),\n                              pa.timestamp('ns', 'UTC')])\n    def test_table_coerce_temporal_nanoseconds(self, arrow_type):\n        data = [date(2000, 1, 1), datetime(2001, 1, 1)]\n        schema = pa.schema([pa.field('date', arrow_type)])\n        expected_df = pd.DataFrame({'date': data})\n        table = pa.table([pa.array(data)], schema=schema)\n        result_df = table.to_pandas(\n            coerce_temporal_nanoseconds=True, date_as_object=False)\n        expected_tz = None\n        if hasattr(arrow_type, 'tz') and arrow_type.tz is not None:\n            expected_tz = 'UTC'\n        expected_type = pa.timestamp('ns', expected_tz).to_pandas_dtype()\n        tm.assert_frame_equal(result_df, expected_df.astype(expected_type))\n\n    def test_date_infer(self):\n        df = pd.DataFrame({\n            'date': [date(2000, 1, 1),\n                     None,\n                     date(1970, 1, 1),\n                     date(2040, 2, 26)]})\n        table = pa.Table.from_pandas(df, preserve_index=False)\n        field = pa.field('date', pa.date32())\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = pa.schema([field], metadata=table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n\n    def test_date_mask(self):\n        arr = np.array([date(2017, 4, 3), date(2017, 4, 4)],\n                       dtype='datetime64[D]')\n        mask = [True, False]\n        result = pa.array(arr, mask=np.array(mask))\n        expected = np.array([None, date(2017, 4, 4)], dtype='datetime64[D]')\n        expected = pa.array(expected, from_pandas=True)\n        assert expected.equals(result)\n\n    def test_date_objects_typed(self):\n        arr = np.array([\n            date(2017, 4, 3),\n            None,\n            date(2017, 4, 4),\n            date(2017, 4, 5)], dtype=object)\n\n        arr_i4 = np.array([17259, -1, 17260, 17261], dtype='int32')\n        arr_i8 = arr_i4.astype('int64') * 86400000\n        mask = np.array([False, True, False, False])\n\n        t32 = pa.date32()\n        t64 = pa.date64()\n\n        a32 = pa.array(arr, type=t32)\n        a64 = pa.array(arr, type=t64)\n\n        a32_expected = pa.array(arr_i4, mask=mask, type=t32)\n        a64_expected = pa.array(arr_i8, mask=mask, type=t64)\n\n        assert a32.equals(a32_expected)\n        assert a64.equals(a64_expected)\n\n        # Test converting back to pandas\n        colnames = ['date32', 'date64']\n        table = pa.Table.from_arrays([a32, a64], colnames)\n\n        ex_values = (np.array(['2017-04-03', '2017-04-04', '2017-04-04',\n                               '2017-04-05'],\n                              dtype='datetime64[D]'))\n        ex_values[1] = pd.NaT.value\n\n        # date32 and date64 convert to [ms] in pandas v2, but\n        # in pandas v1 they are siliently coerced to [ns]\n        ex_datetime64ms = ex_values.astype('datetime64[ms]')\n        expected_pandas = pd.DataFrame({'date32': ex_datetime64ms,\n                                        'date64': ex_datetime64ms},\n                                       columns=colnames)\n        table_pandas = table.to_pandas(date_as_object=False)\n        tm.assert_frame_equal(table_pandas, expected_pandas)\n\n        table_pandas_objects = table.to_pandas()\n        ex_objects = ex_values.astype('object')\n        expected_pandas_objects = pd.DataFrame({'date32': ex_objects,\n                                                'date64': ex_objects},\n                                               columns=colnames)\n        tm.assert_frame_equal(table_pandas_objects,\n                              expected_pandas_objects)\n\n    def test_pandas_null_values(self):\n        # ARROW-842\n        pd_NA = getattr(pd, 'NA', None)\n        values = np.array([datetime(2000, 1, 1), pd.NaT, pd_NA], dtype=object)\n        values_with_none = np.array([datetime(2000, 1, 1), None, None],\n                                    dtype=object)\n        result = pa.array(values, from_pandas=True)\n        expected = pa.array(values_with_none, from_pandas=True)\n        assert result.equals(expected)\n        assert result.null_count == 2\n\n        # ARROW-9407\n        assert pa.array([pd.NaT], from_pandas=True).type == pa.null()\n        assert pa.array([pd_NA], from_pandas=True).type == pa.null()\n\n    def test_dates_from_integers(self):\n        t1 = pa.date32()\n        t2 = pa.date64()\n\n        arr = np.array([17259, 17260, 17261], dtype='int32')\n        arr2 = arr.astype('int64') * 86400000\n\n        a1 = pa.array(arr, type=t1)\n        a2 = pa.array(arr2, type=t2)\n\n        expected = date(2017, 4, 3)\n        assert a1[0].as_py() == expected\n        assert a2[0].as_py() == expected\n\n    def test_pytime_from_pandas(self):\n        pytimes = [time(1, 2, 3, 1356),\n                   time(4, 5, 6, 1356)]\n\n        # microseconds\n        t1 = pa.time64('us')\n\n        aobjs = np.array(pytimes + [None], dtype=object)\n        parr = pa.array(aobjs)\n        assert parr.type == t1\n        assert parr[0].as_py() == pytimes[0]\n        assert parr[1].as_py() == pytimes[1]\n        assert parr[2].as_py() is None\n\n        # DataFrame\n        df = pd.DataFrame({'times': aobjs})\n        batch = pa.RecordBatch.from_pandas(df)\n        assert batch[0].equals(parr)\n\n        # Test ndarray of int64 values\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n\n        a1 = pa.array(arr, type=pa.time64('us'))\n        assert a1[0].as_py() == pytimes[0]\n\n        a2 = pa.array(arr * 1000, type=pa.time64('ns'))\n        assert a2[0].as_py() == pytimes[0]\n\n        a3 = pa.array((arr / 1000).astype('i4'),\n                      type=pa.time32('ms'))\n        assert a3[0].as_py() == pytimes[0].replace(microsecond=1000)\n\n        a4 = pa.array((arr / 1000000).astype('i4'),\n                      type=pa.time32('s'))\n        assert a4[0].as_py() == pytimes[0].replace(microsecond=0)\n\n    def test_arrow_time_to_pandas(self):\n        pytimes = [time(1, 2, 3, 1356),\n                   time(4, 5, 6, 1356),\n                   time(0, 0, 0)]\n\n        expected = np.array(pytimes[:2] + [None])\n        expected_ms = np.array([x.replace(microsecond=1000)\n                                for x in pytimes[:2]] +\n                               [None])\n        expected_s = np.array([x.replace(microsecond=0)\n                               for x in pytimes[:2]] +\n                              [None])\n\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n\n        null_mask = np.array([False, False, True], dtype=bool)\n\n        a1 = pa.array(arr, mask=null_mask, type=pa.time64('us'))\n        a2 = pa.array(arr * 1000, mask=null_mask,\n                      type=pa.time64('ns'))\n\n        a3 = pa.array((arr / 1000).astype('i4'), mask=null_mask,\n                      type=pa.time32('ms'))\n        a4 = pa.array((arr / 1000000).astype('i4'), mask=null_mask,\n                      type=pa.time32('s'))\n\n        names = ['time64[us]', 'time64[ns]', 'time32[ms]', 'time32[s]']\n        batch = pa.RecordBatch.from_arrays([a1, a2, a3, a4], names)\n\n        for arr, expected_values in [(a1, expected),\n                                     (a2, expected),\n                                     (a3, expected_ms),\n                                     (a4, expected_s)]:\n            result_pandas = arr.to_pandas()\n            assert (result_pandas.values == expected_values).all()\n\n        df = batch.to_pandas()\n        expected_df = pd.DataFrame({'time64[us]': expected,\n                                    'time64[ns]': expected,\n                                    'time32[ms]': expected_ms,\n                                    'time32[s]': expected_s},\n                                   columns=names)\n\n        tm.assert_frame_equal(df, expected_df)\n\n    def test_numpy_datetime64_columns(self):\n        datetime64_ns = np.array([\n            '2007-07-13T01:23:34.123456789',\n            None,\n            '2006-01-13T12:34:56.432539784',\n            '2010-08-13T05:46:57.437699912'],\n            dtype='datetime64[ns]')\n        _check_array_from_pandas_roundtrip(datetime64_ns)\n\n        datetime64_us = np.array([\n            '2007-07-13T01:23:34.123456',\n            None,\n            '2006-01-13T12:34:56.432539',\n            '2010-08-13T05:46:57.437699'],\n            dtype='datetime64[us]')\n        _check_array_from_pandas_roundtrip(datetime64_us)\n\n        datetime64_ms = np.array([\n            '2007-07-13T01:23:34.123',\n            None,\n            '2006-01-13T12:34:56.432',\n            '2010-08-13T05:46:57.437'],\n            dtype='datetime64[ms]')\n        _check_array_from_pandas_roundtrip(datetime64_ms)\n\n        datetime64_s = np.array([\n            '2007-07-13T01:23:34',\n            None,\n            '2006-01-13T12:34:56',\n            '2010-08-13T05:46:57'],\n            dtype='datetime64[s]')\n        _check_array_from_pandas_roundtrip(datetime64_s)\n\n    def test_timestamp_to_pandas_coerces_to_ns(self):\n        # non-ns timestamp gets cast to ns on conversion to pandas\n        if Version(pd.__version__) >= Version(\"2.0.0\"):\n            pytest.skip(\"pandas >= 2.0 supports non-nanosecond datetime64\")\n\n        arr = pa.array([1, 2, 3], pa.timestamp('ms'))\n        expected = pd.Series(pd.to_datetime([1, 2, 3], unit='ms'))\n        s = arr.to_pandas()\n        tm.assert_series_equal(s, expected)\n        arr = pa.chunked_array([arr])\n        s = arr.to_pandas()\n        tm.assert_series_equal(s, expected)\n\n    def test_timestamp_to_pandas_out_of_bounds(self):\n        # ARROW-7758 check for out of bounds timestamps for non-ns timestamps\n        # that end up getting coerced into ns timestamps.\n\n        for unit in ['s', 'ms', 'us']:\n            for tz in [None, 'America/New_York']:\n                arr = pa.array([datetime(1, 1, 1)], pa.timestamp(unit, tz=tz))\n                table = pa.table({'a': arr})\n\n                msg = \"would result in out of bounds timestamp\"\n                with pytest.raises(ValueError, match=msg):\n                    arr.to_pandas(coerce_temporal_nanoseconds=True)\n\n                with pytest.raises(ValueError, match=msg):\n                    table.to_pandas(coerce_temporal_nanoseconds=True)\n\n                with pytest.raises(ValueError, match=msg):\n                    # chunked array\n                    table.column('a').to_pandas(coerce_temporal_nanoseconds=True)\n\n                # just ensure those don't give an error, but do not\n                # check actual garbage output\n                arr.to_pandas(safe=False, coerce_temporal_nanoseconds=True)\n                table.to_pandas(safe=False, coerce_temporal_nanoseconds=True)\n                table.column('a').to_pandas(\n                    safe=False, coerce_temporal_nanoseconds=True)\n\n    def test_timestamp_to_pandas_empty_chunked(self):\n        # ARROW-7907 table with chunked array with 0 chunks\n        table = pa.table({'a': pa.chunked_array([], type=pa.timestamp('us'))})\n        result = table.to_pandas()\n        expected = pd.DataFrame({'a': pd.Series([], dtype=\"datetime64[us]\")})\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize('dtype', [pa.date32(), pa.date64()])\n    def test_numpy_datetime64_day_unit(self, dtype):\n        datetime64_d = np.array([\n            '2007-07-13',\n            None,\n            '2006-01-15',\n            '2010-08-19'],\n            dtype='datetime64[D]')\n        _check_array_from_pandas_roundtrip(datetime64_d, type=dtype)\n\n    def test_array_from_pandas_date_with_mask(self):\n        m = np.array([True, False, True])\n        data = pd.Series([\n            date(1990, 1, 1),\n            date(1991, 1, 1),\n            date(1992, 1, 1)\n        ])\n\n        result = pa.Array.from_pandas(data, mask=m)\n\n        expected = pd.Series([None, date(1991, 1, 1), None])\n        assert pa.Array.from_pandas(expected).equals(result)\n\n    @pytest.mark.skipif(\n        Version('1.16.0') <= Version(np.__version__) < Version('1.16.1'),\n        reason='Until numpy/numpy#12745 is resolved')\n    def test_fixed_offset_timezone(self):\n        df = pd.DataFrame({\n            'a': [\n                pd.Timestamp('2012-11-11 00:00:00+01:00'),\n                pd.NaT\n            ]\n        })\n        # 'check_dtype=False' because pandas >= 2 uses datetime.timezone\n        # instead of pytz.FixedOffset, and thus the dtype is not exactly\n        # identical (pyarrow still defaults to pytz)\n        # TODO remove if https://github.com/apache/arrow/issues/15047 is fixed\n        _check_pandas_roundtrip(df, check_dtype=False)\n\n    @pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n    def test_timedeltas_no_nulls(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            unit = 'ns'\n        df = pd.DataFrame({\n            'timedelta64': np.array([0, 3600000000000, 7200000000000],\n                                    dtype=f'timedelta64[{unit}]')\n        })\n        field = pa.field('timedelta64', pa.duration(unit))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    @pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n    def test_timedeltas_nulls(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            unit = 'ns'\n        df = pd.DataFrame({\n            'timedelta64': np.array([0, None, 7200000000000],\n                                    dtype=f'timedelta64[{unit}]')\n        })\n        field = pa.field('timedelta64', pa.duration(unit))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    def test_month_day_nano_interval(self):\n        from pandas.tseries.offsets import DateOffset\n        df = pd.DataFrame({\n            'date_offset': [None,\n                            DateOffset(days=3600, months=3600, microseconds=3,\n                                       nanoseconds=600)]\n        })\n        schema = pa.schema([('date_offset', pa.month_day_nano_interval())])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema)\n\n\n# ----------------------------------------------------------------------\n# Conversion tests for string and binary types.\n\n\nclass TestConvertStringLikeTypes:\n\n    def test_pandas_unicode(self):\n        repeats = 1000\n        values = ['foo', None, 'bar', 'ma\u00f1ana', np.nan]\n        df = pd.DataFrame({'strings': values * repeats})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        ex_values = ['foo', None, 'bar', 'ma\u00f1ana', None]\n        expected = pd.DataFrame({'strings': ex_values * repeats})\n\n        _check_pandas_roundtrip(df, expected=expected, expected_schema=schema)\n\n    def test_bytes_to_binary(self):\n        values = ['qux', b'foo', None, bytearray(b'barz'), 'qux', np.nan]\n        df = pd.DataFrame({'strings': values})\n\n        table = pa.Table.from_pandas(df)\n        assert table[0].type == pa.binary()\n\n        values2 = [b'qux', b'foo', None, b'barz', b'qux', None]\n        expected = pd.DataFrame({'strings': values2})\n        _check_pandas_roundtrip(df, expected)\n\n    @pytest.mark.large_memory\n    def test_bytes_exceed_2gb(self):\n        v1 = b'x' * 100000000\n        v2 = b'x' * 147483646\n\n        # ARROW-2227, hit exactly 2GB on the nose\n        df = pd.DataFrame({\n            'strings': [v1] * 20 + [v2] + ['x'] * 20\n        })\n        arr = pa.array(df['strings'])\n        assert isinstance(arr, pa.ChunkedArray)\n        assert arr.num_chunks == 2\n        arr = None\n\n        table = pa.Table.from_pandas(df)\n        assert table[0].num_chunks == 2\n\n    @pytest.mark.large_memory\n    @pytest.mark.parametrize('char', ['x', b'x'])\n    def test_auto_chunking_pandas_series_of_strings(self, char):\n        # ARROW-2367\n        v1 = char * 100000000\n        v2 = char * 147483646\n\n        df = pd.DataFrame({\n            'strings': [[v1]] * 20 + [[v2]] + [[b'x']]\n        })\n        arr = pa.array(df['strings'], from_pandas=True)\n        arr.validate(full=True)\n        assert isinstance(arr, pa.ChunkedArray)\n        assert arr.num_chunks == 2\n        assert len(arr.chunk(0)) == 21\n        assert len(arr.chunk(1)) == 1\n\n    def test_fixed_size_bytes(self):\n        values = [b'foo', None, bytearray(b'bar'), None, None, b'hey']\n        df = pd.DataFrame({'strings': values})\n        schema = pa.schema([pa.field('strings', pa.binary(3))])\n        table = pa.Table.from_pandas(df, schema=schema)\n        assert table.schema[0].type == schema[0].type\n        assert table.schema[0].name == schema[0].name\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n\n    def test_fixed_size_bytes_does_not_accept_varying_lengths(self):\n        values = [b'foo', None, b'ba', None, None, b'hey']\n        df = pd.DataFrame({'strings': values})\n        schema = pa.schema([pa.field('strings', pa.binary(3))])\n        with pytest.raises(pa.ArrowInvalid):\n            pa.Table.from_pandas(df, schema=schema)\n\n    def test_variable_size_bytes(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.binary())\n\n    def test_binary_from_bytearray(self):\n        s = pd.Series([bytearray(b'123'), bytearray(b''), bytearray(b'a'),\n                       None])\n        # Explicitly set type\n        _check_series_roundtrip(s, type_=pa.binary())\n        # Infer type from bytearrays\n        _check_series_roundtrip(s, expected_pa_type=pa.binary())\n\n    def test_large_binary(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.large_binary())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.large_binary())]))\n\n    def test_large_string(self):\n        s = pd.Series(['123', '', 'a', None])\n        _check_series_roundtrip(s, type_=pa.large_string())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.large_string())]))\n\n    def test_table_empty_str(self):\n        values = ['', '', '', '', '']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result1 = table.to_pandas(strings_to_categorical=False)\n        expected1 = pd.DataFrame({'strings': values})\n        tm.assert_frame_equal(result1, expected1, check_dtype=True)\n\n        result2 = table.to_pandas(strings_to_categorical=True)\n        expected2 = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result2, expected2, check_dtype=True)\n\n    def test_selective_categoricals(self):\n        values = ['', '', '', '', '']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n        expected_str = pd.DataFrame({'strings': values})\n        expected_cat = pd.DataFrame({'strings': pd.Categorical(values)})\n\n        result1 = table.to_pandas(categories=['strings'])\n        tm.assert_frame_equal(result1, expected_cat, check_dtype=True)\n        result2 = table.to_pandas(categories=[])\n        tm.assert_frame_equal(result2, expected_str, check_dtype=True)\n        result3 = table.to_pandas(categories=('strings',))\n        tm.assert_frame_equal(result3, expected_cat, check_dtype=True)\n        result4 = table.to_pandas(categories=tuple())\n        tm.assert_frame_equal(result4, expected_str, check_dtype=True)\n\n    def test_to_pandas_categorical_zero_length(self):\n        # ARROW-3586\n        array = pa.array([], type=pa.int32())\n        table = pa.Table.from_arrays(arrays=[array], names=['col'])\n        # This would segfault under 0.11.0\n        table.to_pandas(categories=['col'])\n\n    def test_to_pandas_categories_already_dictionary(self):\n        # Showed up in ARROW-6434, ARROW-6435\n        array = pa.array(['foo', 'foo', 'foo', 'bar']).dictionary_encode()\n        table = pa.Table.from_arrays(arrays=[array], names=['col'])\n        result = table.to_pandas(categories=['col'])\n        assert table.to_pandas().equals(result)\n\n    def test_table_str_to_categorical_without_na(self):\n        values = ['a', 'a', 'b', 'b', 'c']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result = table.to_pandas(strings_to_categorical=True)\n        expected = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result, expected, check_dtype=True)\n\n        with pytest.raises(pa.ArrowInvalid):\n            table.to_pandas(strings_to_categorical=True,\n                            zero_copy_only=True)\n\n    def test_table_str_to_categorical_with_na(self):\n        values = [None, 'a', 'b', np.nan]\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result = table.to_pandas(strings_to_categorical=True)\n        expected = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result, expected, check_dtype=True)\n\n        with pytest.raises(pa.ArrowInvalid):\n            table.to_pandas(strings_to_categorical=True,\n                            zero_copy_only=True)\n\n    # Regression test for ARROW-2101\n    def test_array_of_bytes_to_strings(self):\n        converted = pa.array(np.array([b'x'], dtype=object), pa.string())\n        assert converted.type == pa.string()\n\n    # Make sure that if an ndarray of bytes is passed to the array\n    # constructor and the type is string, it will fail if those bytes\n    # cannot be converted to utf-8\n    def test_array_of_bytes_to_strings_bad_data(self):\n        with pytest.raises(\n                pa.lib.ArrowInvalid,\n                match=\"was not a utf8 string\"):\n            pa.array(np.array([b'\\x80\\x81'], dtype=object), pa.string())\n\n    def test_numpy_string_array_to_fixed_size_binary(self):\n        arr = np.array([b'foo', b'bar', b'baz'], dtype='|S3')\n\n        converted = pa.array(arr, type=pa.binary(3))\n        expected = pa.array(list(arr), type=pa.binary(3))\n        assert converted.equals(expected)\n\n        mask = np.array([False, True, False])\n        converted = pa.array(arr, type=pa.binary(3), mask=mask)\n        expected = pa.array([b'foo', None, b'baz'], type=pa.binary(3))\n        assert converted.equals(expected)\n\n        with pytest.raises(pa.lib.ArrowInvalid,\n                           match=r'Got bytestring of length 3 \\(expected 4\\)'):\n            arr = np.array([b'foo', b'bar', b'baz'], dtype='|S3')\n            pa.array(arr, type=pa.binary(4))\n\n        with pytest.raises(\n                pa.lib.ArrowInvalid,\n                match=r'Got bytestring of length 12 \\(expected 3\\)'):\n            arr = np.array([b'foo', b'bar', b'baz'], dtype='|U3')\n            pa.array(arr, type=pa.binary(3))\n\n\nclass TestConvertDecimalTypes:\n    \"\"\"\n    Conversion test for decimal types.\n    \"\"\"\n    decimal32 = [\n        decimal.Decimal('-1234.123'),\n        decimal.Decimal('1234.439')\n    ]\n    decimal64 = [\n        decimal.Decimal('-129934.123331'),\n        decimal.Decimal('129534.123731')\n    ]\n    decimal128 = [\n        decimal.Decimal('394092382910493.12341234678'),\n        decimal.Decimal('-314292388910493.12343437128')\n    ]\n\n    @pytest.mark.parametrize(('values', 'expected_type'), [\n        pytest.param(decimal32, pa.decimal128(7, 3), id='decimal32'),\n        pytest.param(decimal64, pa.decimal128(12, 6), id='decimal64'),\n        pytest.param(decimal128, pa.decimal128(26, 11), id='decimal128')\n    ])\n    def test_decimal_from_pandas(self, values, expected_type):\n        expected = pd.DataFrame({'decimals': values})\n        table = pa.Table.from_pandas(expected, preserve_index=False)\n        field = pa.field('decimals', expected_type)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = pa.schema([field], metadata=table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n    @pytest.mark.parametrize('values', [\n        pytest.param(decimal32, id='decimal32'),\n        pytest.param(decimal64, id='decimal64'),\n        pytest.param(decimal128, id='decimal128')\n    ])\n    def test_decimal_to_pandas(self, values):\n        expected = pd.DataFrame({'decimals': values})\n        converted = pa.Table.from_pandas(expected)\n        df = converted.to_pandas()\n        tm.assert_frame_equal(df, expected)\n\n    def test_decimal_fails_with_truncation(self):\n        data1 = [decimal.Decimal('1.234')]\n        type1 = pa.decimal128(10, 2)\n        with pytest.raises(pa.ArrowInvalid):\n            pa.array(data1, type=type1)\n\n        data2 = [decimal.Decimal('1.2345')]\n        type2 = pa.decimal128(10, 3)\n        with pytest.raises(pa.ArrowInvalid):\n            pa.array(data2, type=type2)\n\n    def test_decimal_with_different_precisions(self):\n        data = [\n            decimal.Decimal('0.01'),\n            decimal.Decimal('0.001'),\n        ]\n        series = pd.Series(data)\n        array = pa.array(series)\n        assert array.to_pylist() == data\n        assert array.type == pa.decimal128(3, 3)\n\n        array = pa.array(data, type=pa.decimal128(12, 5))\n        expected = [decimal.Decimal('0.01000'), decimal.Decimal('0.00100')]\n        assert array.to_pylist() == expected\n\n    def test_decimal_with_None_explicit_type(self):\n        series = pd.Series([decimal.Decimal('3.14'), None])\n        _check_series_roundtrip(series, type_=pa.decimal128(12, 5))\n\n        # Test that having all None values still produces decimal array\n        series = pd.Series([None] * 2)\n        _check_series_roundtrip(series, type_=pa.decimal128(12, 5))\n\n    def test_decimal_with_None_infer_type(self):\n        series = pd.Series([decimal.Decimal('3.14'), None])\n        _check_series_roundtrip(series, expected_pa_type=pa.decimal128(3, 2))\n\n    def test_strided_objects(self, tmpdir):\n        # see ARROW-3053\n        data = {\n            'a': {0: 'a'},\n            'b': {0: decimal.Decimal('0.0')}\n        }\n\n        # This yields strided objects\n        df = pd.DataFrame.from_dict(data)\n        _check_pandas_roundtrip(df)\n\n\nclass TestConvertListTypes:\n    \"\"\"\n    Conversion tests for list<> types.\n    \"\"\"\n\n    def test_column_of_arrays(self):\n        df, schema = dataframe_with_arrays()\n        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = schema.with_metadata(table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        for column in df.columns:\n            field = schema.field(column)\n            _check_array_roundtrip(df[column], type=field.type)\n\n    def test_column_of_arrays_to_py(self):\n        # Test regression in ARROW-1199 not caught in above test\n        dtype = 'i1'\n        arr = np.array([\n            np.arange(10, dtype=dtype),\n            np.arange(5, dtype=dtype),\n            None,\n            np.arange(1, dtype=dtype)\n        ], dtype=object)\n        type_ = pa.list_(pa.int8())\n        parr = pa.array(arr, type=type_)\n\n        assert parr[0].as_py() == list(range(10))\n        assert parr[1].as_py() == list(range(5))\n        assert parr[2].as_py() is None\n        assert parr[3].as_py() == [0]\n\n    def test_column_of_boolean_list(self):\n        # ARROW-4370: Table to pandas conversion fails for list of bool\n        array = pa.array([[True, False], [True]], type=pa.list_(pa.bool_()))\n        table = pa.Table.from_arrays([array], names=['col1'])\n        df = table.to_pandas()\n\n        expected_df = pd.DataFrame({'col1': [[True, False], [True]]})\n        tm.assert_frame_equal(df, expected_df)\n\n        s = table[0].to_pandas()\n        tm.assert_series_equal(pd.Series(s), df['col1'], check_names=False)\n\n    def test_column_of_decimal_list(self):\n        array = pa.array([[decimal.Decimal('1'), decimal.Decimal('2')],\n                          [decimal.Decimal('3.3')]],\n                         type=pa.list_(pa.decimal128(2, 1)))\n        table = pa.Table.from_arrays([array], names=['col1'])\n        df = table.to_pandas()\n\n        expected_df = pd.DataFrame(\n            {'col1': [[decimal.Decimal('1'), decimal.Decimal('2')],\n                      [decimal.Decimal('3.3')]]})\n        tm.assert_frame_equal(df, expected_df)\n\n    def test_nested_types_from_ndarray_null_entries(self):\n        # Root cause of ARROW-6435\n        s = pd.Series(np.array([np.nan, np.nan], dtype=object))\n\n        for ty in [pa.list_(pa.int64()),\n                   pa.large_list(pa.int64()),\n                   pa.struct([pa.field('f0', 'int32')])]:\n            result = pa.array(s, type=ty)\n            expected = pa.array([None, None], type=ty)\n            assert result.equals(expected)\n\n            with pytest.raises(TypeError):\n                pa.array(s.values, type=ty)\n\n    def test_column_of_lists(self):\n        df, schema = dataframe_with_lists()\n        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = schema.with_metadata(table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        for column in df.columns:\n            field = schema.field(column)\n            _check_array_roundtrip(df[column], type=field.type)\n\n    def test_column_of_lists_first_empty(self):\n        # ARROW-2124\n        num_lists = [[], [2, 3, 4], [3, 6, 7, 8], [], [2]]\n        series = pd.Series([np.array(s, dtype=float) for s in num_lists])\n        arr = pa.array(series)\n        result = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(result, series)\n\n    def test_column_of_lists_chunked(self):\n        # ARROW-1357\n        df = pd.DataFrame({\n            'lists': np.array([\n                [1, 2],\n                None,\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]\n            ], dtype=object)\n        })\n\n        schema = pa.schema([\n            pa.field('lists', pa.list_(pa.int64()))\n        ])\n\n        t1 = pa.Table.from_pandas(df[:2], schema=schema)\n        t2 = pa.Table.from_pandas(df[2:], schema=schema)\n\n        table = pa.concat_tables([t1, t2])\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_empty_column_of_lists_chunked(self):\n        df = pd.DataFrame({\n            'lists': np.array([], dtype=object)\n        })\n\n        schema = pa.schema([\n            pa.field('lists', pa.list_(pa.int64()))\n        ])\n\n        table = pa.Table.from_pandas(df, schema=schema)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_column_of_lists_chunked2(self):\n        data1 = [[0, 1], [2, 3], [4, 5], [6, 7], [10, 11],\n                 [12, 13], [14, 15], [16, 17]]\n        data2 = [[8, 9], [18, 19]]\n\n        a1 = pa.array(data1)\n        a2 = pa.array(data2)\n\n        t1 = pa.Table.from_arrays([a1], names=['a'])\n        t2 = pa.Table.from_arrays([a2], names=['a'])\n\n        concatenated = pa.concat_tables([t1, t2])\n\n        result = concatenated.to_pandas()\n        expected = pd.DataFrame({'a': data1 + data2})\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_column_of_lists_strided(self):\n        df, schema = dataframe_with_lists()\n        df = pd.concat([df] * 6, ignore_index=True)\n\n        arr = df['int64'].values[::3]\n        assert arr.strides[0] != 8\n\n        _check_array_roundtrip(arr)\n\n    def test_nested_lists_all_none(self):\n        data = np.array([[None, None], None], dtype=object)\n\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.null())\n\n        data2 = np.array([None, None, [None, None],\n                          np.array([None, None], dtype=object)],\n                         dtype=object)\n        arr = pa.array(data2)\n        expected = pa.array([None, None, [None, None], [None, None]])\n        assert arr.equals(expected)\n\n    def test_nested_lists_all_empty(self):\n        # ARROW-2128\n        data = pd.Series([[], [], []])\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.null())\n\n    def test_nested_list_first_empty(self):\n        # ARROW-2711\n        data = pd.Series([[], [\"a\"]])\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.string())\n\n    def test_nested_smaller_ints(self):\n        # ARROW-1345, ARROW-2008, there were some type inference bugs happening\n        # before\n        data = pd.Series([np.array([1, 2, 3], dtype='i1'), None])\n        result = pa.array(data)\n        result2 = pa.array(data.values)\n        expected = pa.array([[1, 2, 3], None], type=pa.list_(pa.int8()))\n        assert result.equals(expected)\n        assert result2.equals(expected)\n\n        data3 = pd.Series([np.array([1, 2, 3], dtype='f4'), None])\n        result3 = pa.array(data3)\n        expected3 = pa.array([[1, 2, 3], None], type=pa.list_(pa.float32()))\n        assert result3.equals(expected3)\n\n    def test_infer_lists(self):\n        data = OrderedDict([\n            ('nan_ints', [[np.nan, 1], [2, 3]]),\n            ('ints', [[0, 1], [2, 3]]),\n            ('strs', [[None, 'b'], ['c', 'd']]),\n            ('nested_strs', [[[None, 'b'], ['c', 'd']], None])\n        ])\n        df = pd.DataFrame(data)\n\n        expected_schema = pa.schema([\n            pa.field('nan_ints', pa.list_(pa.int64())),\n            pa.field('ints', pa.list_(pa.int64())),\n            pa.field('strs', pa.list_(pa.string())),\n            pa.field('nested_strs', pa.list_(pa.list_(pa.string())))\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n    def test_fixed_size_list(self):\n        # ARROW-7365\n        fixed_ty = pa.list_(pa.int64(), list_size=4)\n        variable_ty = pa.list_(pa.int64())\n\n        data = [[0, 1, 2, 3], None, [4, 5, 6, 7], [8, 9, 10, 11]]\n        fixed_arr = pa.array(data, type=fixed_ty)\n        variable_arr = pa.array(data, type=variable_ty)\n\n        result = fixed_arr.to_pandas()\n        expected = variable_arr.to_pandas()\n\n        for left, right in zip(result, expected):\n            if left is None:\n                assert right is None\n            npt.assert_array_equal(left, right)\n\n    def test_infer_numpy_array(self):\n        data = OrderedDict([\n            ('ints', [\n                np.array([0, 1], dtype=np.int64),\n                np.array([2, 3], dtype=np.int64)\n            ])\n        ])\n        df = pd.DataFrame(data)\n        expected_schema = pa.schema([\n            pa.field('ints', pa.list_(pa.int64()))\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n    def test_to_list_of_structs_pandas(self):\n        ints = pa.array([1, 2, 3], pa.int32())\n        strings = pa.array([['a', 'b'], ['c', 'd'], ['e', 'f']],\n                           pa.list_(pa.string()))\n        structs = pa.StructArray.from_arrays([ints, strings], ['f1', 'f2'])\n        data = pa.ListArray.from_arrays([0, 1, 3], structs)\n\n        expected = pd.Series([\n            [{'f1': 1, 'f2': ['a', 'b']}],\n            [{'f1': 2, 'f2': ['c', 'd']},\n             {'f1': 3, 'f2': ['e', 'f']}]\n        ])\n\n        series = pd.Series(data.to_pandas())\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n\n    def test_to_list_of_maps_pandas(self):\n        if ((Version(np.__version__) >= Version(\"1.25.0.dev0\")) and\n                (Version(pd.__version__) < Version(\"2.0.0\"))):\n            # TODO: regression in pandas with numpy 1.25dev\n            # https://github.com/pandas-dev/pandas/issues/50360\n            pytest.skip(\"Regression in pandas with numpy 1.25\")\n        data = [\n            [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n            [[('baz', []), ('qux', None), ('quux', [None, 'e'])], [('quz', ['f', 'g'])]]\n        ]\n        arr = pa.array(data, pa.list_(pa.map_(pa.utf8(), pa.list_(pa.utf8()))))\n        series = arr.to_pandas()\n        expected = pd.Series(data)\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n\n    def test_to_list_of_maps_pandas_sliced(self):\n        \"\"\"\n        A slightly more rigorous test for chunk/slice combinations\n        \"\"\"\n\n        if ((Version(np.__version__) >= Version(\"1.25.0.dev0\")) and\n                (Version(pd.__version__) < Version(\"2.0.0\"))):\n            # TODO: regression in pandas with numpy 1.25dev\n            # https://github.com/pandas-dev/pandas/issues/50360\n            pytest.skip(\"Regression in pandas with numpy 1.25\")\n\n        keys = pa.array(['ignore', 'foo', 'bar', 'baz',\n                         'qux', 'quux', 'ignore']).slice(1, 5)\n        items = pa.array(\n            [['ignore'], ['ignore'], ['a', 'b'], ['c', 'd'], [], None, [None, 'e']],\n            pa.list_(pa.string()),\n        ).slice(2, 5)\n        map = pa.MapArray.from_arrays([0, 2, 4], keys, items)\n        arr = pa.ListArray.from_arrays([0, 1, 2], map)\n\n        series = arr.to_pandas()\n        expected = pd.Series([\n            [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n            [[('baz', []), ('qux', None)]],\n        ])\n\n        series_sliced = arr.slice(1, 2).to_pandas()\n        expected_sliced = pd.Series([\n            [[('baz', []), ('qux', None)]],\n        ])\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n            tm.assert_series_equal(series_sliced, expected_sliced)\n\n    @pytest.mark.parametrize('t,data,expected', [\n        (\n            pa.int64,\n            [[1, 2], [3], None],\n            [None, [3], None]\n        ),\n        (\n            pa.string,\n            [['aaa', 'bb'], ['c'], None],\n            [None, ['c'], None]\n        ),\n        (\n            pa.null,\n            [[None, None], [None], None],\n            [None, [None], None]\n        )\n    ])\n    def test_array_from_pandas_typed_array_with_mask(self, t, data, expected):\n        m = np.array([True, False, True])\n\n        s = pd.Series(data)\n        result = pa.Array.from_pandas(s, mask=m, type=pa.list_(t()))\n\n        assert pa.Array.from_pandas(expected,\n                                    type=pa.list_(t())).equals(result)\n\n    def test_empty_list_roundtrip(self):\n        empty_list_array = np.empty((3,), dtype=object)\n        empty_list_array.fill([])\n\n        df = pd.DataFrame({'a': np.array(['1', '2', '3']),\n                           'b': empty_list_array})\n        tbl = pa.Table.from_pandas(df)\n\n        result = tbl.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_array_from_nested_arrays(self):\n        df, schema = dataframe_with_arrays()\n        for field in schema:\n            arr = df[field.name].values\n            expected = pa.array(list(arr), type=field.type)\n            result = pa.array(arr)\n            assert result.type == field.type  # == list<scalar>\n            assert result.equals(expected)\n\n    def test_nested_large_list(self):\n        s = (pa.array([[[1, 2, 3], [4]], None],\n                      type=pa.large_list(pa.large_list(pa.int64())))\n             .to_pandas())\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\",\n                                    \"Creating an ndarray from ragged nested\",\n                                    _np_VisibleDeprecationWarning)\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(\n                s, pd.Series([[[1, 2, 3], [4]], None], dtype=object),\n                check_names=False)\n\n    def test_large_binary_list(self):\n        for list_type_factory in (pa.list_, pa.large_list):\n            s = (pa.array([[\"aa\", \"bb\"], None, [\"cc\"], []],\n                          type=list_type_factory(pa.large_binary()))\n                 .to_pandas())\n            tm.assert_series_equal(\n                s, pd.Series([[b\"aa\", b\"bb\"], None, [b\"cc\"], []]),\n                check_names=False)\n            s = (pa.array([[\"aa\", \"bb\"], None, [\"cc\"], []],\n                          type=list_type_factory(pa.large_string()))\n                 .to_pandas())\n            tm.assert_series_equal(\n                s, pd.Series([[\"aa\", \"bb\"], None, [\"cc\"], []]),\n                check_names=False)\n\n    def test_list_of_dictionary(self):\n        child = pa.array([\"foo\", \"bar\", None, \"foo\"]).dictionary_encode()\n        arr = pa.ListArray.from_arrays([0, 1, 3, 3, 4], child)\n\n        # Expected a Series of lists\n        expected = pd.Series(arr.to_pylist())\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n        # Same but with nulls\n        arr = arr.take([0, 1, None, 3])\n        expected[2] = None\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n    @pytest.mark.large_memory\n    def test_auto_chunking_on_list_overflow(self):\n        # ARROW-9976\n        n = 2**21\n        df = pd.DataFrame.from_dict({\n            \"a\": list(np.zeros((n, 2**10), dtype='uint8')),\n            \"b\": range(n)\n        })\n        table = pa.Table.from_pandas(df)\n        table.validate(full=True)\n\n        column_a = table[0]\n        assert column_a.num_chunks == 2\n        assert len(column_a.chunk(0)) == 2**21 - 1\n        assert len(column_a.chunk(1)) == 1\n\n    def test_map_array_roundtrip(self):\n        data = [[(b'a', 1), (b'b', 2)],\n                [(b'c', 3)],\n                [(b'd', 4), (b'e', 5), (b'f', 6)],\n                [(b'g', 7)]]\n\n        df = pd.DataFrame({\"map\": data})\n        schema = pa.schema([(\"map\", pa.map_(pa.binary(), pa.int32()))])\n\n        _check_pandas_roundtrip(df, schema=schema)\n\n    def test_map_array_chunked(self):\n        data1 = [[(b'a', 1), (b'b', 2)],\n                 [(b'c', 3)],\n                 [(b'd', 4), (b'e', 5), (b'f', 6)],\n                 [(b'g', 7)]]\n        data2 = [[(k, v * 2) for k, v in row] for row in data1]\n\n        arr1 = pa.array(data1, type=pa.map_(pa.binary(), pa.int32()))\n        arr2 = pa.array(data2, type=pa.map_(pa.binary(), pa.int32()))\n        arr = pa.chunked_array([arr1, arr2])\n\n        expected = pd.Series(data1 + data2)\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_map_array_with_nulls(self):\n        data = [[(b'a', 1), (b'b', 2)],\n                None,\n                [(b'd', 4), (b'e', 5), (b'f', None)],\n                [(b'g', 7)]]\n\n        # None value in item array causes upcast to float\n        expected = [[(k, float(v) if v is not None else None) for k, v in row]\n                    if row is not None else None for row in data]\n        expected = pd.Series(expected)\n\n        arr = pa.array(data, type=pa.map_(pa.binary(), pa.int32()))\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_map_array_dictionary_encoded(self):\n        offsets = pa.array([0, 3, 5])\n        items = pa.array(['a', 'b', 'c', 'a', 'd']).dictionary_encode()\n        keys = pa.array(list(range(len(items))))\n        arr = pa.MapArray.from_arrays(offsets, keys, items)\n\n        # Dictionary encoded values converted to dense\n        expected = pd.Series(\n            [[(0, 'a'), (1, 'b'), (2, 'c')], [(3, 'a'), (4, 'd')]])\n\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_list_no_duplicate_base(self):\n        # ARROW-18400\n        arr = pa.array([[1, 2], [3, 4, 5], None, [6, None], [7, 8]])\n        chunked_arr = pa.chunked_array([arr.slice(0, 3), arr.slice(3, 1)])\n\n        np_arr = chunked_arr.to_numpy()\n\n        expected = np.array([[1., 2.], [3., 4., 5.], None,\n                            [6., np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n        expected_base = np.array([[1., 2., 3., 4., 5., 6., np.nan]])\n        npt.assert_array_equal(np_arr[0].base, expected_base)\n\n        np_arr_sliced = chunked_arr.slice(1, 3).to_numpy()\n\n        expected = np.array([[3, 4, 5], None, [6, np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr_sliced, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n        expected_base = np.array([[3., 4., 5., 6., np.nan]])\n        npt.assert_array_equal(np_arr_sliced[0].base, expected_base)\n\n    def test_list_values_behind_null(self):\n        arr = pa.ListArray.from_arrays(\n            offsets=pa.array([0, 2, 4, 6]),\n            values=pa.array([1, 2, 99, 99, 3, None]),\n            mask=pa.array([False, True, False])\n        )\n        np_arr = arr.to_numpy(zero_copy_only=False)\n\n        expected = np.array([[1., 2.], None, [3., np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n\nclass TestConvertStructTypes:\n    \"\"\"\n    Conversion tests for struct types.\n    \"\"\"\n\n    def test_pandas_roundtrip(self):\n        df = pd.DataFrame({'dicts': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n\n        expected_schema = pa.schema([\n            ('dicts', pa.struct([('a', pa.int64()), ('b', pa.int64())])),\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n        # specifying schema explicitly in from_pandas\n        _check_pandas_roundtrip(\n            df, schema=expected_schema, expected_schema=expected_schema)\n\n    def test_to_pandas(self):\n        ints = pa.array([None, 2, 3], type=pa.int64())\n        strs = pa.array(['a', None, 'c'], type=pa.string())\n        bools = pa.array([True, False, None], type=pa.bool_())\n        arr = pa.StructArray.from_arrays(\n            [ints, strs, bools],\n            ['ints', 'strs', 'bools'])\n\n        expected = pd.Series([\n            {'ints': None, 'strs': 'a', 'bools': True},\n            {'ints': 2, 'strs': None, 'bools': False},\n            {'ints': 3, 'strs': 'c', 'bools': None},\n        ])\n\n        series = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(series, expected)\n\n    def test_to_pandas_multiple_chunks(self):\n        # ARROW-11855\n        gc.collect()\n        bytes_start = pa.total_allocated_bytes()\n        ints1 = pa.array([1], type=pa.int64())\n        ints2 = pa.array([2], type=pa.int64())\n        arr1 = pa.StructArray.from_arrays([ints1], ['ints'])\n        arr2 = pa.StructArray.from_arrays([ints2], ['ints'])\n        arr = pa.chunked_array([arr1, arr2])\n\n        expected = pd.Series([\n            {'ints': 1},\n            {'ints': 2}\n        ])\n\n        series = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(series, expected)\n\n        del series\n        del arr\n        del arr1\n        del arr2\n        del ints1\n        del ints2\n        bytes_end = pa.total_allocated_bytes()\n        assert bytes_end == bytes_start\n\n    def test_from_numpy(self):\n        dt = np.dtype([('x', np.int32),\n                       (('y_title', 'y'), np.bool_)])\n        ty = pa.struct([pa.field('x', pa.int32()),\n                        pa.field('y', pa.bool_())])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([(42, True), (43, False)], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [{'x': 42, 'y': True},\n                                   {'x': 43, 'y': False}]\n\n        # With mask\n        arr = pa.array(data, mask=np.bool_([False, True]), type=ty)\n        assert arr.to_pylist() == [{'x': 42, 'y': True}, None]\n\n        # Trivial struct type\n        dt = np.dtype([])\n        ty = pa.struct([])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([(), ()], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [{}, {}]\n\n    def test_from_numpy_nested(self):\n        # Note: an object field inside a struct\n        dt = np.dtype([('x', np.dtype([('xx', np.int8),\n                                       ('yy', np.bool_)])),\n                       ('y', np.int16),\n                       ('z', np.object_)])\n        # Note: itemsize is not a multiple of sizeof(object)\n        assert dt.itemsize == 12\n        ty = pa.struct([pa.field('x', pa.struct([pa.field('xx', pa.int8()),\n                                                 pa.field('yy', pa.bool_())])),\n                        pa.field('y', pa.int16()),\n                        pa.field('z', pa.string())])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([\n            ((1, True), 2, 'foo'),\n            ((3, False), 4, 'bar')], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [\n            {'x': {'xx': 1, 'yy': True}, 'y': 2, 'z': 'foo'},\n            {'x': {'xx': 3, 'yy': False}, 'y': 4, 'z': 'bar'}]\n\n    @pytest.mark.slow\n    @pytest.mark.large_memory\n    def test_from_numpy_large(self):\n        # Exercise rechunking + nulls\n        target_size = 3 * 1024**3  # 4GB\n        dt = np.dtype([('x', np.float64), ('y', 'object')])\n        bs = 65536 - dt.itemsize\n        block = b'.' * bs\n        n = target_size // (bs + dt.itemsize)\n        data = np.zeros(n, dtype=dt)\n        data['x'] = np.random.random_sample(n)\n        data['y'] = block\n        # Add implicit nulls\n        data['x'][data['x'] < 0.2] = np.nan\n\n        ty = pa.struct([pa.field('x', pa.float64()),\n                        pa.field('y', pa.binary())])\n        arr = pa.array(data, type=ty, from_pandas=True)\n        arr.validate(full=True)\n        assert arr.num_chunks == 2\n\n        def iter_chunked_array(arr):\n            for chunk in arr.iterchunks():\n                yield from chunk\n\n        def check(arr, data, mask=None):\n            assert len(arr) == len(data)\n            xs = data['x']\n            ys = data['y']\n            for i, obj in enumerate(iter_chunked_array(arr)):\n                try:\n                    d = obj.as_py()\n                    if mask is not None and mask[i]:\n                        assert d is None\n                    else:\n                        x = xs[i]\n                        if np.isnan(x):\n                            assert d['x'] is None\n                        else:\n                            assert d['x'] == x\n                        assert d['y'] == ys[i]\n                except Exception:\n                    print(\"Failed at index\", i)\n                    raise\n\n        check(arr, data)\n        del arr\n\n        # Now with explicit mask\n        mask = np.random.random_sample(n) < 0.2\n        arr = pa.array(data, type=ty, mask=mask, from_pandas=True)\n        arr.validate(full=True)\n        assert arr.num_chunks == 2\n\n        check(arr, data, mask)\n        del arr\n\n    def test_from_numpy_bad_input(self):\n        ty = pa.struct([pa.field('x', pa.int32()),\n                        pa.field('y', pa.bool_())])\n        dt = np.dtype([('x', np.int32),\n                       ('z', np.bool_)])\n\n        data = np.array([], dtype=dt)\n        with pytest.raises(ValueError,\n                           match=\"Missing field 'y'\"):\n            pa.array(data, type=ty)\n        data = np.int32([])\n        with pytest.raises(TypeError,\n                           match=\"Expected struct array\"):\n            pa.array(data, type=ty)\n\n    def test_from_tuples(self):\n        df = pd.DataFrame({'tuples': [(1, 2), (3, 4)]})\n        expected_df = pd.DataFrame(\n            {'tuples': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n\n        # conversion from tuples works when specifying expected struct type\n        struct_type = pa.struct([('a', pa.int64()), ('b', pa.int64())])\n\n        arr = np.asarray(df['tuples'])\n        _check_array_roundtrip(\n            arr, expected=expected_df['tuples'], type=struct_type)\n\n        expected_schema = pa.schema([('tuples', struct_type)])\n        _check_pandas_roundtrip(\n            df, expected=expected_df, schema=expected_schema,\n            expected_schema=expected_schema)\n\n    def test_struct_of_dictionary(self):\n        names = ['ints', 'strs']\n        children = [pa.array([456, 789, 456]).dictionary_encode(),\n                    pa.array([\"foo\", \"foo\", None]).dictionary_encode()]\n        arr = pa.StructArray.from_arrays(children, names=names)\n\n        # Expected a Series of {field name: field value} dicts\n        rows_as_tuples = zip(*(child.to_pylist() for child in children))\n        rows_as_dicts = [dict(zip(names, row)) for row in rows_as_tuples]\n\n        expected = pd.Series(rows_as_dicts)\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n        # Same but with nulls\n        arr = arr.take([0, None, 2])\n        expected[1] = None\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n\nclass TestZeroCopyConversion:\n    \"\"\"\n    Tests that zero-copy conversion works with some types.\n    \"\"\"\n\n    def test_zero_copy_success(self):\n        result = pa.array([0, 1, 2]).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, [0, 1, 2])\n\n    def test_zero_copy_dictionaries(self):\n        arr = pa.DictionaryArray.from_arrays(\n            np.array([0, 0]),\n            np.array([5], dtype=\"int64\"),\n        )\n\n        result = arr.to_pandas(zero_copy_only=True)\n        values = pd.Categorical([5, 5])\n\n        tm.assert_series_equal(pd.Series(result), pd.Series(values),\n                               check_names=False)\n\n    def test_zero_copy_timestamp(self):\n        arr = np.array(['2007-07-13'], dtype='datetime64[ns]')\n        result = pa.array(arr).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, arr)\n\n    def test_zero_copy_duration(self):\n        arr = np.array([1], dtype='timedelta64[ns]')\n        result = pa.array(arr).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, arr)\n\n    def check_zero_copy_failure(self, arr):\n        with pytest.raises(pa.ArrowInvalid):\n            arr.to_pandas(zero_copy_only=True)\n\n    def test_zero_copy_failure_on_object_types(self):\n        self.check_zero_copy_failure(pa.array(['A', 'B', 'C']))\n\n    def test_zero_copy_failure_with_int_when_nulls(self):\n        self.check_zero_copy_failure(pa.array([0, 1, None]))\n\n    def test_zero_copy_failure_with_float_when_nulls(self):\n        self.check_zero_copy_failure(pa.array([0.0, 1.0, None]))\n\n    def test_zero_copy_failure_on_bool_types(self):\n        self.check_zero_copy_failure(pa.array([True, False]))\n\n    def test_zero_copy_failure_on_list_types(self):\n        arr = pa.array([[1, 2], [8, 9]], type=pa.list_(pa.int64()))\n        self.check_zero_copy_failure(arr)\n\n    def test_zero_copy_failure_on_timestamp_with_nulls(self):\n        arr = np.array([1, None], dtype='datetime64[ns]')\n        self.check_zero_copy_failure(pa.array(arr))\n\n    def test_zero_copy_failure_on_duration_with_nulls(self):\n        arr = np.array([1, None], dtype='timedelta64[ns]')\n        self.check_zero_copy_failure(pa.array(arr))\n\n\ndef _non_threaded_conversion():\n    df = _alltypes_example()\n    _check_pandas_roundtrip(df, use_threads=False)\n    _check_pandas_roundtrip(df, use_threads=False, as_batch=True)\n\n\ndef _threaded_conversion():\n    df = _alltypes_example()\n    _check_pandas_roundtrip(df, use_threads=True)\n    _check_pandas_roundtrip(df, use_threads=True, as_batch=True)\n\n\nclass TestConvertMisc:\n    \"\"\"\n    Miscellaneous conversion tests.\n    \"\"\"\n\n    type_pairs = [\n        (np.int8, pa.int8()),\n        (np.int16, pa.int16()),\n        (np.int32, pa.int32()),\n        (np.int64, pa.int64()),\n        (np.uint8, pa.uint8()),\n        (np.uint16, pa.uint16()),\n        (np.uint32, pa.uint32()),\n        (np.uint64, pa.uint64()),\n        (np.float16, pa.float16()),\n        (np.float32, pa.float32()),\n        (np.float64, pa.float64()),\n        # XXX unsupported\n        # (np.dtype([('a', 'i2')]), pa.struct([pa.field('a', pa.int16())])),\n        (np.object_, pa.string()),\n        (np.object_, pa.binary()),\n        (np.object_, pa.binary(10)),\n        (np.object_, pa.list_(pa.int64())),\n    ]\n\n    def test_all_none_objects(self):\n        df = pd.DataFrame({'a': [None, None, None]})\n        _check_pandas_roundtrip(df)\n\n    def test_all_none_category(self):\n        df = pd.DataFrame({'a': [None, None, None]})\n        df['a'] = df['a'].astype('category')\n        _check_pandas_roundtrip(df)\n\n    def test_empty_arrays(self):\n        for dtype, pa_type in self.type_pairs:\n            arr = np.array([], dtype=dtype)\n            _check_array_roundtrip(arr, type=pa_type)\n\n    def test_non_threaded_conversion(self):\n        _non_threaded_conversion()\n\n    def test_threaded_conversion_multiprocess(self):\n        # Parallel conversion should work from child processes too (ARROW-2963)\n        pool = mp.Pool(2)\n        try:\n            pool.apply(_threaded_conversion)\n        finally:\n            pool.close()\n            pool.join()\n\n    def test_category(self):\n        repeats = 5\n        v1 = ['foo', None, 'bar', 'qux', np.nan]\n        v2 = [4, 5, 6, 7, 8]\n        v3 = [b'foo', None, b'bar', b'qux', np.nan]\n\n        arrays = {\n            'cat_strings': pd.Categorical(v1 * repeats),\n            'cat_strings_with_na': pd.Categorical(v1 * repeats,\n                                                  categories=['foo', 'bar']),\n            'cat_ints': pd.Categorical(v2 * repeats),\n            'cat_binary': pd.Categorical(v3 * repeats),\n            'cat_strings_ordered': pd.Categorical(\n                v1 * repeats, categories=['bar', 'qux', 'foo'],\n                ordered=True),\n            'ints': v2 * repeats,\n            'ints2': v2 * repeats,\n            'strings': v1 * repeats,\n            'strings2': v1 * repeats,\n            'strings3': v3 * repeats}\n        df = pd.DataFrame(arrays)\n        _check_pandas_roundtrip(df)\n\n        for k in arrays:\n            _check_array_roundtrip(arrays[k])\n\n    def test_category_implicit_from_pandas(self):\n        # ARROW-3374\n        def _check(v):\n            arr = pa.array(v)\n            result = arr.to_pandas()\n            tm.assert_series_equal(pd.Series(result), pd.Series(v))\n\n        arrays = [\n            pd.Categorical(['a', 'b', 'c'], categories=['a', 'b']),\n            pd.Categorical(['a', 'b', 'c'], categories=['a', 'b'],\n                           ordered=True)\n        ]\n        for arr in arrays:\n            _check(arr)\n\n    def test_empty_category(self):\n        # ARROW-2443\n        df = pd.DataFrame({'cat': pd.Categorical([])})\n        _check_pandas_roundtrip(df)\n\n    def test_category_zero_chunks(self):\n        # ARROW-5952\n        for pa_type, dtype in [(pa.string(), 'object'), (pa.int64(), 'int64')]:\n            a = pa.chunked_array([], pa.dictionary(pa.int8(), pa_type))\n            result = a.to_pandas()\n            expected = pd.Categorical([], categories=np.array([], dtype=dtype))\n            tm.assert_series_equal(pd.Series(result), pd.Series(expected))\n\n            table = pa.table({'a': a})\n            result = table.to_pandas()\n            expected = pd.DataFrame({'a': expected})\n            tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"data,error_type\",\n        [\n            ({\"a\": [\"a\", 1, 2.0]}, pa.ArrowTypeError),\n            ({\"a\": [\"a\", 1, 2.0]}, pa.ArrowTypeError),\n            ({\"a\": [1, True]}, pa.ArrowTypeError),\n            ({\"a\": [True, \"a\"]}, pa.ArrowInvalid),\n            ({\"a\": [1, \"a\"]}, pa.ArrowInvalid),\n            ({\"a\": [1.0, \"a\"]}, pa.ArrowInvalid),\n        ],\n    )\n    def test_mixed_types_fails(self, data, error_type):\n        df = pd.DataFrame(data)\n        msg = \"Conversion failed for column a with type object\"\n        with pytest.raises(error_type, match=msg):\n            pa.Table.from_pandas(df)\n\n    def test_strided_data_import(self):\n        cases = []\n\n        columns = ['a', 'b', 'c']\n        N, K = 100, 3\n        random_numbers = np.random.randn(N, K).copy() * 100\n\n        numeric_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8',\n                          'f4', 'f8']\n\n        for type_name in numeric_dtypes:\n            # Casting np.float64 -> uint32 or uint64 throws a RuntimeWarning\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                cases.append(random_numbers.astype(type_name))\n\n        # strings\n        cases.append(np.array([random_ascii(10) for i in range(N * K)],\n                              dtype=object)\n                     .reshape(N, K).copy())\n\n        # booleans\n        boolean_objects = (np.array([True, False, True] * N, dtype=object)\n                           .reshape(N, K).copy())\n\n        # add some nulls, so dtype comes back as objects\n        boolean_objects[5] = None\n        cases.append(boolean_objects)\n\n        cases.append(np.arange(\"2016-01-01T00:00:00.001\", N * K,\n                               dtype='datetime64[ms]')\n                     .reshape(N, K).copy())\n\n        strided_mask = (random_numbers > 0).astype(bool)[:, 0]\n\n        for case in cases:\n            df = pd.DataFrame(case, columns=columns)\n            col = df['a']\n\n            _check_pandas_roundtrip(df)\n            _check_array_roundtrip(col)\n            _check_array_roundtrip(col, mask=strided_mask)\n\n    def test_all_nones(self):\n        def _check_series(s):\n            converted = pa.array(s)\n            assert isinstance(converted, pa.NullArray)\n            assert len(converted) == 3\n            assert converted.null_count == 3\n            for item in converted:\n                assert item is pa.NA\n\n        _check_series(pd.Series([None] * 3, dtype=object))\n        _check_series(pd.Series([np.nan] * 3, dtype=object))\n        _check_series(pd.Series([None, np.nan, None], dtype=object))\n\n    def test_partial_schema(self):\n        data = OrderedDict([\n            ('a', [0, 1, 2, 3, 4]),\n            ('b', np.array([-10, -5, 0, 5, 10], dtype=np.int32)),\n            ('c', [-10, -5, 0, 5, 10])\n        ])\n        df = pd.DataFrame(data)\n\n        partial_schema = pa.schema([\n            pa.field('c', pa.int64()),\n            pa.field('a', pa.int64())\n        ])\n\n        _check_pandas_roundtrip(df, schema=partial_schema,\n                                expected=df[['c', 'a']],\n                                expected_schema=partial_schema)\n\n    def test_table_batch_empty_dataframe(self):\n        df = pd.DataFrame({})\n        _check_pandas_roundtrip(df, preserve_index=None)\n        _check_pandas_roundtrip(df, preserve_index=None, as_batch=True)\n\n        expected = pd.DataFrame(columns=pd.Index([]))\n        _check_pandas_roundtrip(df, expected, preserve_index=False)\n        _check_pandas_roundtrip(df, expected, preserve_index=False, as_batch=True)\n\n        df2 = pd.DataFrame({}, index=[0, 1, 2])\n        _check_pandas_roundtrip(df2, preserve_index=True)\n        _check_pandas_roundtrip(df2, as_batch=True, preserve_index=True)\n\n    def test_convert_empty_table(self):\n        arr = pa.array([], type=pa.int64())\n        empty_objects = pd.Series(np.array([], dtype=object))\n        tm.assert_series_equal(arr.to_pandas(),\n                               pd.Series(np.array([], dtype=np.int64)))\n        arr = pa.array([], type=pa.string())\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n        arr = pa.array([], type=pa.list_(pa.int64()))\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n        arr = pa.array([], type=pa.struct([pa.field('a', pa.int64())]))\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n\n    def test_non_natural_stride(self):\n        \"\"\"\n        ARROW-2172: converting from a Numpy array with a stride that's\n        not a multiple of itemsize.\n        \"\"\"\n        dtype = np.dtype([('x', np.int32), ('y', np.int16)])\n        data = np.array([(42, -1), (-43, 2)], dtype=dtype)\n        assert data.strides == (6,)\n        arr = pa.array(data['x'], type=pa.int32())\n        assert arr.to_pylist() == [42, -43]\n        arr = pa.array(data['y'], type=pa.int16())\n        assert arr.to_pylist() == [-1, 2]\n\n    def test_array_from_strided_numpy_array(self):\n        # ARROW-5651\n        np_arr = np.arange(0, 10, dtype=np.float32)[1:-1:2]\n        pa_arr = pa.array(np_arr, type=pa.float64())\n        expected = pa.array([1.0, 3.0, 5.0, 7.0], type=pa.float64())\n        pa_arr.equals(expected)\n\n    def test_safe_unsafe_casts(self):\n        # ARROW-2799\n        df = pd.DataFrame({\n            'A': list('abc'),\n            'B': np.linspace(0, 1, 3)\n        })\n\n        schema = pa.schema([\n            pa.field('A', pa.string()),\n            pa.field('B', pa.int32())\n        ])\n\n        with pytest.raises(ValueError):\n            pa.Table.from_pandas(df, schema=schema)\n\n        table = pa.Table.from_pandas(df, schema=schema, safe=False)\n        assert table.column('B').type == pa.int32()\n\n    def test_error_sparse(self):\n        # ARROW-2818\n        try:\n            df = pd.DataFrame({'a': pd.arrays.SparseArray([1, np.nan, 3])})\n        except AttributeError:\n            # pandas.arrays module introduced in pandas 0.24\n            df = pd.DataFrame({'a': pd.SparseArray([1, np.nan, 3])})\n        with pytest.raises(TypeError, match=\"Sparse pandas data\"):\n            pa.Table.from_pandas(df)\n\n\ndef test_safe_cast_from_float_with_nans_to_int():\n    # TODO(kszucs): write tests for creating Date32 and Date64 arrays, see\n    #               ARROW-4258 and https://github.com/apache/arrow/pull/3395\n    values = pd.Series([1, 2, None, 4])\n    arr = pa.Array.from_pandas(values, type=pa.int32(), safe=True)\n    expected = pa.array([1, 2, None, 4], type=pa.int32())\n    assert arr.equals(expected)\n\n\ndef _fully_loaded_dataframe_example():\n    index = pd.MultiIndex.from_arrays([\n        pd.date_range('2000-01-01', periods=5).repeat(2),\n        np.tile(np.array(['foo', 'bar'], dtype=object), 5)\n    ])\n\n    c1 = pd.date_range('2000-01-01', periods=10)\n    data = {\n        0: c1,\n        1: c1.tz_localize('utc'),\n        2: c1.tz_localize('US/Eastern'),\n        3: c1[::2].tz_localize('utc').repeat(2).astype('category'),\n        4: ['foo', 'bar'] * 5,\n        5: pd.Series(['foo', 'bar'] * 5).astype('category').values,\n        6: [True, False] * 5,\n        7: np.random.randn(10),\n        8: np.random.randint(0, 100, size=10),\n        9: pd.period_range('2013', periods=10, freq='M'),\n        10: pd.interval_range(start=1, freq=1, periods=10),\n    }\n    return pd.DataFrame(data, index=index)\n\n\n@pytest.mark.parametrize('columns', ([b'foo'], ['foo']))\ndef test_roundtrip_with_bytes_unicode(columns):\n    if Version(\"2.0.0\") <= Version(pd.__version__) < Version(\"2.2.0\"):\n        # TODO: regression in pandas, hopefully fixed in next version\n        # https://issues.apache.org/jira/browse/ARROW-18394\n        # https://github.com/pandas-dev/pandas/issues/50127\n        pytest.skip(\"Regression in pandas 2.0.0\")\n\n    df = pd.DataFrame(columns=columns)\n    table1 = pa.Table.from_pandas(df)\n    table2 = pa.Table.from_pandas(table1.to_pandas())\n    assert table1.equals(table2)\n    assert table1.schema.equals(table2.schema)\n    assert table1.schema.metadata == table2.schema.metadata\n\n\ndef _pytime_from_micros(val):\n    microseconds = val % 1000000\n    val //= 1000000\n    seconds = val % 60\n    val //= 60\n    minutes = val % 60\n    hours = val // 60\n    return time(hours, minutes, seconds, microseconds)\n\n\ndef _pytime_to_micros(pytime):\n    return (pytime.hour * 3600000000 +\n            pytime.minute * 60000000 +\n            pytime.second * 1000000 +\n            pytime.microsecond)\n\n\ndef test_convert_unsupported_type_error_message():\n    # ARROW-1454\n\n    # custom python objects\n    class A:\n        pass\n\n    df = pd.DataFrame({'a': [A(), A()]})\n\n    msg = 'Conversion failed for column a with type object'\n    with pytest.raises(ValueError, match=msg):\n        pa.Table.from_pandas(df)\n\n\n# ----------------------------------------------------------------------\n# Hypothesis tests\n\n\n@h.given(past.arrays(past.pandas_compatible_types))\ndef test_array_to_pandas_roundtrip(arr):\n    s = arr.to_pandas()\n    restored = pa.array(s, type=arr.type, from_pandas=True)\n    assert restored.equals(arr)\n\n\n# ----------------------------------------------------------------------\n# Test object deduplication in to_pandas\n\n\ndef _generate_dedup_example(nunique, repeats):\n    unique_values = [rands(10) for i in range(nunique)]\n    return unique_values * repeats\n\n\ndef _assert_nunique(obj, expected):\n    assert len({id(x) for x in obj}) == expected\n\n\ndef test_to_pandas_deduplicate_strings_array_types():\n    nunique = 100\n    repeats = 10\n    values = _generate_dedup_example(nunique, repeats)\n\n    for arr in [pa.array(values, type=pa.binary()),\n                pa.array(values, type=pa.utf8()),\n                pa.chunked_array([values, values])]:\n        _assert_nunique(arr.to_pandas(), nunique)\n        _assert_nunique(arr.to_pandas(deduplicate_objects=False), len(arr))\n\n\ndef test_to_pandas_deduplicate_strings_table_types():\n    nunique = 100\n    repeats = 10\n    values = _generate_dedup_example(nunique, repeats)\n\n    arr = pa.array(values)\n    rb = pa.RecordBatch.from_arrays([arr], ['foo'])\n    tbl = pa.Table.from_batches([rb])\n\n    for obj in [rb, tbl]:\n        _assert_nunique(obj.to_pandas()['foo'], nunique)\n        _assert_nunique(obj.to_pandas(deduplicate_objects=False)['foo'],\n                        len(obj))\n\n\ndef test_to_pandas_deduplicate_integers_as_objects():\n    nunique = 100\n    repeats = 10\n\n    # Python automatically interns smaller integers\n    unique_values = list(np.random.randint(10000000, 1000000000, size=nunique))\n    unique_values[nunique // 2] = None\n\n    arr = pa.array(unique_values * repeats)\n\n    _assert_nunique(arr.to_pandas(integer_object_nulls=True), nunique)\n    _assert_nunique(arr.to_pandas(integer_object_nulls=True,\n                                  deduplicate_objects=False),\n                    # Account for None\n                    (nunique - 1) * repeats + 1)\n\n\ndef test_to_pandas_deduplicate_date_time():\n    nunique = 100\n    repeats = 10\n\n    unique_values = list(range(nunique))\n\n    cases = [\n        # raw type, array type, to_pandas options\n        ('int32', 'date32', {'date_as_object': True}),\n        ('int64', 'date64', {'date_as_object': True}),\n        ('int32', 'time32[ms]', {}),\n        ('int64', 'time64[us]', {})\n    ]\n\n    for raw_type, array_type, pandas_options in cases:\n        raw_arr = pa.array(unique_values * repeats, type=raw_type)\n        casted_arr = raw_arr.cast(array_type)\n\n        _assert_nunique(casted_arr.to_pandas(**pandas_options),\n                        nunique)\n        _assert_nunique(casted_arr.to_pandas(deduplicate_objects=False,\n                                             **pandas_options),\n                        len(casted_arr))\n\n\n# ---------------------------------------------------------------------\n\ndef test_table_from_pandas_checks_field_nullability():\n    # ARROW-2136\n    df = pd.DataFrame({'a': [1.2, 2.1, 3.1],\n                       'b': [np.nan, 'string', 'foo']})\n    schema = pa.schema([pa.field('a', pa.float64(), nullable=False),\n                        pa.field('b', pa.utf8(), nullable=False)])\n\n    with pytest.raises(ValueError):\n        pa.Table.from_pandas(df, schema=schema)\n\n\ndef test_table_from_pandas_keeps_column_order_of_dataframe():\n    df1 = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n    df2 = df1[['floats', 'partition', 'arrays']]\n\n    schema1 = pa.schema([\n        ('partition', pa.int64()),\n        ('arrays', pa.list_(pa.int64())),\n        ('floats', pa.float64()),\n    ])\n    schema2 = pa.schema([\n        ('floats', pa.float64()),\n        ('partition', pa.int64()),\n        ('arrays', pa.list_(pa.int64()))\n    ])\n\n    table1 = pa.Table.from_pandas(df1, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, preserve_index=False)\n\n    assert table1.schema.equals(schema1)\n    assert table2.schema.equals(schema2)\n\n\ndef test_table_from_pandas_keeps_column_order_of_schema():\n    # ARROW-3766\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n\n    schema = pa.schema([\n        ('floats', pa.float64()),\n        ('arrays', pa.list_(pa.int32())),\n        ('partition', pa.int32())\n    ])\n\n    df1 = df[df.partition == 0]\n    df2 = df[df.partition == 1][['floats', 'partition', 'arrays']]\n\n    table1 = pa.Table.from_pandas(df1, schema=schema, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, schema=schema, preserve_index=False)\n\n    assert table1.schema.equals(schema)\n    assert table1.schema.equals(table2.schema)\n\n\ndef test_table_from_pandas_columns_argument_only_does_filtering():\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n\n    columns1 = ['arrays', 'floats', 'partition']\n    schema1 = pa.schema([\n        ('arrays', pa.list_(pa.int64())),\n        ('floats', pa.float64()),\n        ('partition', pa.int64())\n    ])\n\n    columns2 = ['floats', 'partition']\n    schema2 = pa.schema([\n        ('floats', pa.float64()),\n        ('partition', pa.int64())\n    ])\n\n    table1 = pa.Table.from_pandas(df, columns=columns1, preserve_index=False)\n    table2 = pa.Table.from_pandas(df, columns=columns2, preserve_index=False)\n\n    assert table1.schema.equals(schema1)\n    assert table2.schema.equals(schema2)\n\n\ndef test_table_from_pandas_columns_and_schema_are_mutually_exclusive():\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n    schema = pa.schema([\n        ('partition', pa.int32()),\n        ('arrays', pa.list_(pa.int32())),\n        ('floats', pa.float64()),\n    ])\n    columns = ['arrays', 'floats']\n\n    with pytest.raises(ValueError):\n        pa.Table.from_pandas(df, schema=schema, columns=columns)\n\n\ndef test_table_from_pandas_keeps_schema_nullability():\n    # ARROW-5169\n    df = pd.DataFrame({'a': [1, 2, 3, 4]})\n\n    schema = pa.schema([\n        pa.field('a', pa.int64(), nullable=False),\n    ])\n\n    table = pa.Table.from_pandas(df)\n    assert table.schema.field('a').nullable is True\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.field('a').nullable is False\n\n\ndef test_table_from_pandas_schema_index_columns():\n    # ARROW-5220\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})\n\n    schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n        ('index', pa.int64()),\n    ])\n\n    # schema includes index with name not in dataframe\n    with pytest.raises(KeyError, match=\"name 'index' present in the\"):\n        pa.Table.from_pandas(df, schema=schema)\n\n    df.index.name = 'index'\n\n    # schema includes correct index name -> roundtrip works\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema includes correct index name but preserve_index=False\n    with pytest.raises(ValueError, match=\"'preserve_index=False' was\"):\n        pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n    # in case of preserve_index=None -> RangeIndex serialized as metadata\n    # clashes with the index in the schema\n    with pytest.raises(ValueError, match=\"name 'index' is present in the \"\n                                         \"schema, but it is a RangeIndex\"):\n        pa.Table.from_pandas(df, schema=schema, preserve_index=None)\n\n    df.index = pd.Index([0, 1, 2], name='index')\n\n    # for non-RangeIndex, both preserve_index=None and True work\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema has different order (index column not at the end)\n    schema = pa.schema([\n        ('index', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema does not include the index -> index is not included as column\n    # even though preserve_index=True/None\n    schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    expected = df.copy()\n    expected = expected.reset_index(drop=True)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema, expected=expected)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema, expected=expected)\n\n    # dataframe with a MultiIndex\n    df.index = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1)],\n                                         names=['level1', 'level2'])\n    schema = pa.schema([\n        ('level1', pa.string()),\n        ('level2', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n\n    # only one of the levels of the MultiIndex is included\n    schema = pa.schema([\n        ('level2', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    expected = df.copy()\n    expected = expected.reset_index('level1', drop=True)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema, expected=expected)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema, expected=expected)\n\n\ndef test_table_from_pandas_schema_index_columns__unnamed_index():\n    # ARROW-6999 - unnamed indices in specified schema\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})\n\n    expected_schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n        ('__index_level_0__', pa.int64()),\n    ])\n\n    schema = pa.Schema.from_pandas(df, preserve_index=True)\n    table = pa.Table.from_pandas(df, preserve_index=True, schema=schema)\n    assert table.schema.remove_metadata().equals(expected_schema)\n\n    # non-RangeIndex (preserved by default)\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]}, index=[0, 1, 2])\n    schema = pa.Schema.from_pandas(df)\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.remove_metadata().equals(expected_schema)\n\n\ndef test_table_from_pandas_schema_with_custom_metadata():\n    # ARROW-7087 - metadata disappear from pandas\n    df = pd.DataFrame()\n    schema = pa.Schema.from_pandas(df).with_metadata({'meta': 'True'})\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.metadata.get(b'meta') == b'True'\n\n\ndef test_table_from_pandas_schema_field_order_metadata():\n    # ARROW-10532\n    # ensure that a different field order in specified schema doesn't\n    # mangle metadata\n    df = pd.DataFrame({\n        \"datetime\": pd.date_range(\"2020-01-01T00:00:00Z\", freq=\"H\", periods=2),\n        \"float\": np.random.randn(2)\n    })\n\n    schema = pa.schema([\n        pa.field(\"float\", pa.float32(), nullable=True),\n        pa.field(\"datetime\", pa.timestamp(\"s\", tz=\"UTC\"), nullable=False)\n    ])\n\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.equals(schema)\n    metadata_float = table.schema.pandas_metadata[\"columns\"][0]\n    assert metadata_float[\"name\"] == \"float\"\n    assert metadata_float[\"metadata\"] is None\n    metadata_datetime = table.schema.pandas_metadata[\"columns\"][1]\n    assert metadata_datetime[\"name\"] == \"datetime\"\n    assert metadata_datetime[\"metadata\"] == {'timezone': 'UTC'}\n\n    result = table.to_pandas()\n    coerce_cols_to_types = {\"float\": \"float32\"}\n    if Version(pd.__version__) >= Version(\"2.0.0\"):\n        # Pandas v2 now support non-nanosecond time units\n        coerce_cols_to_types[\"datetime\"] = \"datetime64[s, UTC]\"\n    expected = df[[\"float\", \"datetime\"]].astype(coerce_cols_to_types)\n\n    tm.assert_frame_equal(result, expected)\n\n\n# ----------------------------------------------------------------------\n# RecordBatch, Table\n\n\ndef test_recordbatch_from_to_pandas():\n    data = pd.DataFrame({\n        'c1': np.array([1, 2, 3, 4, 5], dtype='int64'),\n        'c2': np.array([1, 2, 3, 4, 5], dtype='uint32'),\n        'c3': np.random.randn(5),\n        'c4': ['foo', 'bar', None, 'baz', 'qux'],\n        'c5': [False, True, False, True, False]\n    })\n\n    batch = pa.RecordBatch.from_pandas(data)\n    result = batch.to_pandas()\n    tm.assert_frame_equal(data, result)\n\n\ndef test_recordbatchlist_to_pandas():\n    data1 = pd.DataFrame({\n        'c1': np.array([1, 1, 2], dtype='uint32'),\n        'c2': np.array([1.0, 2.0, 3.0], dtype='float64'),\n        'c3': [True, None, False],\n        'c4': ['foo', 'bar', None]\n    })\n\n    data2 = pd.DataFrame({\n        'c1': np.array([3, 5], dtype='uint32'),\n        'c2': np.array([4.0, 5.0], dtype='float64'),\n        'c3': [True, True],\n        'c4': ['baz', 'qux']\n    })\n\n    batch1 = pa.RecordBatch.from_pandas(data1)\n    batch2 = pa.RecordBatch.from_pandas(data2)\n\n    table = pa.Table.from_batches([batch1, batch2])\n    result = table.to_pandas()\n    data = pd.concat([data1, data2]).reset_index(drop=True)\n    tm.assert_frame_equal(data, result)\n\n\ndef test_recordbatch_table_pass_name_to_pandas():\n    rb = pa.record_batch([pa.array([1, 2, 3, 4])], names=['a0'])\n    t = pa.table([pa.array([1, 2, 3, 4])], names=['a0'])\n    assert rb[0].to_pandas().name == 'a0'\n    assert t[0].to_pandas().name == 'a0'\n\n\n# ----------------------------------------------------------------------\n# Metadata serialization\n\n\n@pytest.mark.parametrize(\n    ('type', 'expected'),\n    [\n        (pa.null(), 'empty'),\n        (pa.bool_(), 'bool'),\n        (pa.int8(), 'int8'),\n        (pa.int16(), 'int16'),\n        (pa.int32(), 'int32'),\n        (pa.int64(), 'int64'),\n        (pa.uint8(), 'uint8'),\n        (pa.uint16(), 'uint16'),\n        (pa.uint32(), 'uint32'),\n        (pa.uint64(), 'uint64'),\n        (pa.float16(), 'float16'),\n        (pa.float32(), 'float32'),\n        (pa.float64(), 'float64'),\n        (pa.date32(), 'date'),\n        (pa.date64(), 'date'),\n        (pa.binary(), 'bytes'),\n        (pa.binary(length=4), 'bytes'),\n        (pa.string(), 'unicode'),\n        (pa.list_(pa.list_(pa.int16())), 'list[list[int16]]'),\n        (pa.decimal128(18, 3), 'decimal'),\n        (pa.timestamp('ms'), 'datetime'),\n        (pa.timestamp('us', 'UTC'), 'datetimetz'),\n        (pa.time32('s'), 'time'),\n        (pa.time64('us'), 'time')\n    ]\n)\ndef test_logical_type(type, expected):\n    assert get_logical_type(type) == expected\n\n\n# ----------------------------------------------------------------------\n# to_pandas uses MemoryPool\n\ndef test_array_uses_memory_pool():\n    # ARROW-6570\n    N = 10000\n    arr = pa.array(np.arange(N, dtype=np.int64),\n                   mask=np.random.randint(0, 2, size=N).astype(np.bool_))\n\n    # In the case the gc is caught loading\n    gc.collect()\n\n    prior_allocation = pa.total_allocated_bytes()\n\n    x = arr.to_pandas()\n    assert pa.total_allocated_bytes() == (prior_allocation + N * 8)\n    x = None  # noqa\n    gc.collect()\n\n    assert pa.total_allocated_bytes() == prior_allocation\n\n    # zero copy does not allocate memory\n    arr = pa.array(np.arange(N, dtype=np.int64))\n\n    prior_allocation = pa.total_allocated_bytes()\n    x = arr.to_pandas()  # noqa\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_singleton_blocks_zero_copy():\n    # Part of ARROW-3789\n    t = pa.table([pa.array(np.arange(1000, dtype=np.int64))], ['f0'])\n\n    # Zero copy if split_blocks=True\n    _check_to_pandas_memory_unchanged(t, split_blocks=True)\n\n    prior_allocation = pa.total_allocated_bytes()\n    result = t.to_pandas()\n    assert result['f0'].values.flags.writeable\n    assert pa.total_allocated_bytes() > prior_allocation\n\n\ndef _check_to_pandas_memory_unchanged(obj, **kwargs):\n    prior_allocation = pa.total_allocated_bytes()\n    x = obj.to_pandas(**kwargs)  # noqa\n\n    # Memory allocation unchanged -- either zero copy or self-destructing\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_to_pandas_split_blocks():\n    # ARROW-3789\n    t = pa.table([\n        pa.array([1, 2, 3, 4, 5], type='i1'),\n        pa.array([1, 2, 3, 4, 5], type='i4'),\n        pa.array([1, 2, 3, 4, 5], type='i8'),\n        pa.array([1, 2, 3, 4, 5], type='f4'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n    ], ['f{}'.format(i) for i in range(8)])\n\n    _check_blocks_created(t, 8)\n    _check_to_pandas_memory_unchanged(t, split_blocks=True)\n\n\ndef _get_mgr(df):\n    if Version(pd.__version__) < Version(\"1.1.0\"):\n        return df._data\n    else:\n        return df._mgr\n\n\ndef _check_blocks_created(t, number):\n    x = t.to_pandas(split_blocks=True)\n    assert len(_get_mgr(x).blocks) == number\n\n\ndef test_to_pandas_self_destruct():\n    K = 50\n\n    def _make_table():\n        return pa.table([\n            # Slice to force a copy\n            pa.array(np.random.randn(10000)[::2])\n            for i in range(K)\n        ], ['f{}'.format(i) for i in range(K)])\n\n    t = _make_table()\n    _check_to_pandas_memory_unchanged(t, split_blocks=True, self_destruct=True)\n\n    # Check non-split-block behavior\n    t = _make_table()\n    _check_to_pandas_memory_unchanged(t, self_destruct=True)\n\n\ndef test_table_uses_memory_pool():\n    N = 10000\n    arr = pa.array(np.arange(N, dtype=np.int64))\n    t = pa.table([arr, arr, arr], ['f0', 'f1', 'f2'])\n\n    prior_allocation = pa.total_allocated_bytes()\n    x = t.to_pandas()\n\n    assert pa.total_allocated_bytes() == (prior_allocation + 3 * N * 8)\n\n    # Check successful garbage collection\n    x = None  # noqa\n    gc.collect()\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_object_leak_in_numpy_array():\n    # ARROW-6876\n    arr = pa.array([{'a': 1}])\n    np_arr = arr.to_pandas()\n    assert np_arr.dtype == np.dtype('object')\n    obj = np_arr[0]\n    refcount = sys.getrefcount(obj)\n    assert sys.getrefcount(obj) == refcount\n    del np_arr\n    assert sys.getrefcount(obj) == refcount - 1\n\n\ndef test_object_leak_in_dataframe():\n    # ARROW-6876\n    arr = pa.array([{'a': 1}])\n    table = pa.table([arr], ['f0'])\n    col = table.to_pandas()['f0']\n    assert col.dtype == np.dtype('object')\n    obj = col[0]\n    refcount = sys.getrefcount(obj)\n    assert sys.getrefcount(obj) == refcount\n    del col\n    assert sys.getrefcount(obj) == refcount - 1\n\n\n# ----------------------------------------------------------------------\n# Some nested array tests array tests\n\n\ndef test_array_from_py_float32():\n    data = [[1.2, 3.4], [9.0, 42.0]]\n\n    t = pa.float32()\n\n    arr1 = pa.array(data[0], type=t)\n    arr2 = pa.array(data, type=pa.list_(t))\n\n    expected1 = np.array(data[0], dtype=np.float32)\n    expected2 = pd.Series([np.array(data[0], dtype=np.float32),\n                           np.array(data[1], dtype=np.float32)])\n\n    assert arr1.type == t\n    assert arr1.equals(pa.array(expected1))\n    assert arr2.equals(pa.array(expected2))\n\n\n# ----------------------------------------------------------------------\n# Timestamp tests\n\n\ndef test_cast_timestamp_unit():\n    # ARROW-1680\n    val = datetime.now()\n    s = pd.Series([val])\n    s_nyc = s.dt.tz_localize('tzlocal()').dt.tz_convert('America/New_York')\n\n    us_with_tz = pa.timestamp('us', tz='America/New_York')\n\n    arr = pa.Array.from_pandas(s_nyc, type=us_with_tz)\n\n    # ARROW-1906\n    assert arr.type == us_with_tz\n\n    arr2 = pa.Array.from_pandas(s, type=pa.timestamp('us'))\n\n    assert arr[0].as_py() == s_nyc[0].to_pydatetime()\n    assert arr2[0].as_py() == s[0].to_pydatetime()\n\n    # Disallow truncation\n    arr = pa.array([123123], type='int64').cast(pa.timestamp('ms'))\n    expected = pa.array([123], type='int64').cast(pa.timestamp('s'))\n\n    # sanity check that the cast worked right\n    assert arr.type == pa.timestamp('ms')\n\n    target = pa.timestamp('s')\n    with pytest.raises(ValueError):\n        arr.cast(target)\n\n    result = arr.cast(target, safe=False)\n    assert result.equals(expected)\n\n    # ARROW-1949\n    series = pd.Series([pd.Timestamp(1), pd.Timestamp(10), pd.Timestamp(1000)])\n    expected = pa.array([0, 0, 1], type=pa.timestamp('us'))\n\n    with pytest.raises(ValueError):\n        pa.array(series, type=pa.timestamp('us'))\n\n    with pytest.raises(ValueError):\n        pa.Array.from_pandas(series, type=pa.timestamp('us'))\n\n    result = pa.Array.from_pandas(series, type=pa.timestamp('us'), safe=False)\n    assert result.equals(expected)\n\n    result = pa.array(series, type=pa.timestamp('us'), safe=False)\n    assert result.equals(expected)\n\n\ndef test_nested_with_timestamp_tz_round_trip():\n    ts = pd.Timestamp.now()\n    ts_dt = ts.to_pydatetime()\n    arr = pa.array([ts_dt], type=pa.timestamp('us', tz='America/New_York'))\n    struct = pa.StructArray.from_arrays([arr, arr], ['start', 'stop'])\n\n    result = struct.to_pandas()\n    restored = pa.array(result)\n    assert restored.equals(struct)\n\n\ndef test_nested_with_timestamp_tz():\n    # ARROW-7723\n    ts = pd.Timestamp.now()\n    ts_dt = ts.to_pydatetime()\n\n    # XXX: Ensure that this data does not get promoted to nanoseconds (and thus\n    # integers) to preserve behavior in 0.15.1\n    for unit in ['s', 'ms', 'us']:\n        if unit in ['s', 'ms']:\n            # This is used for verifying timezone conversion to micros are not\n            # important\n            def truncate(x): return x.replace(microsecond=0)\n        else:\n            def truncate(x): return x\n        arr = pa.array([ts], type=pa.timestamp(unit))\n        arr2 = pa.array([ts], type=pa.timestamp(unit, tz='America/New_York'))\n\n        arr3 = pa.StructArray.from_arrays([arr, arr], ['start', 'stop'])\n        arr4 = pa.StructArray.from_arrays([arr2, arr2], ['start', 'stop'])\n\n        result = arr3.to_pandas()\n        assert isinstance(result[0]['start'], datetime)\n        assert result[0]['start'].tzinfo is None\n        assert isinstance(result[0]['stop'], datetime)\n        assert result[0]['stop'].tzinfo is None\n\n        result = arr4.to_pandas()\n        assert isinstance(result[0]['start'], datetime)\n        assert result[0]['start'].tzinfo is not None\n        utc_dt = result[0]['start'].astimezone(timezone.utc)\n        assert truncate(utc_dt).replace(tzinfo=None) == truncate(ts_dt)\n        assert isinstance(result[0]['stop'], datetime)\n        assert result[0]['stop'].tzinfo is not None\n\n        # same conversion for table\n        result = pa.table({'a': arr3}).to_pandas()\n        assert isinstance(result['a'][0]['start'], datetime)\n        assert result['a'][0]['start'].tzinfo is None\n        assert isinstance(result['a'][0]['stop'], datetime)\n        assert result['a'][0]['stop'].tzinfo is None\n\n        result = pa.table({'a': arr4}).to_pandas()\n        assert isinstance(result['a'][0]['start'], datetime)\n        assert result['a'][0]['start'].tzinfo is not None\n        assert isinstance(result['a'][0]['stop'], datetime)\n        assert result['a'][0]['stop'].tzinfo is not None\n\n\n# ----------------------------------------------------------------------\n# DictionaryArray tests\n\n\ndef test_dictionary_with_pandas():\n    src_indices = np.repeat([0, 1, 2], 2)\n    dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n    mask = np.array([False, False, True, False, False, False])\n\n    for index_type in ['uint8', 'int8', 'uint16', 'int16', 'uint32', 'int32',\n                       'uint64', 'int64']:\n        indices = src_indices.astype(index_type)\n        d1 = pa.DictionaryArray.from_arrays(indices, dictionary)\n        d2 = pa.DictionaryArray.from_arrays(indices, dictionary, mask=mask)\n\n        if index_type[0] == 'u':\n            # TODO: unsigned dictionary indices to pandas\n            with pytest.raises(TypeError):\n                d1.to_pandas()\n            continue\n\n        pandas1 = d1.to_pandas()\n        ex_pandas1 = pd.Categorical.from_codes(indices, categories=dictionary)\n\n        tm.assert_series_equal(pd.Series(pandas1), pd.Series(ex_pandas1))\n\n        pandas2 = d2.to_pandas()\n        assert pandas2.isnull().sum() == 1\n\n        # Unsigned integers converted to signed\n        signed_indices = indices\n        if index_type[0] == 'u':\n            signed_indices = indices.astype(index_type[1:])\n        ex_pandas2 = pd.Categorical.from_codes(np.where(mask, -1,\n                                                        signed_indices),\n                                               categories=dictionary)\n\n        tm.assert_series_equal(pd.Series(pandas2), pd.Series(ex_pandas2))\n\n\ndef random_strings(n, item_size, pct_null=0, dictionary=None):\n    if dictionary is not None:\n        result = dictionary[np.random.randint(0, len(dictionary), size=n)]\n    else:\n        result = np.array([random_ascii(item_size) for i in range(n)],\n                          dtype=object)\n\n    if pct_null > 0:\n        result[np.random.rand(n) < pct_null] = None\n\n    return result\n\n\ndef test_variable_dictionary_to_pandas():\n    np.random.seed(12345)\n\n    d1 = pa.array(random_strings(100, 32), type='string')\n    d2 = pa.array(random_strings(100, 16), type='string')\n    d3 = pa.array(random_strings(10000, 10), type='string')\n\n    a1 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d1), size=1000, dtype='i4'),\n        d1\n    )\n    a2 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d2), size=1000, dtype='i4'),\n        d2\n    )\n\n    # With some nulls\n    a3 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d3), size=1000, dtype='i4'), d3)\n\n    i4 = pa.array(\n        np.random.randint(0, len(d3), size=1000, dtype='i4'),\n        mask=np.random.rand(1000) < 0.1\n    )\n    a4 = pa.DictionaryArray.from_arrays(i4, d3)\n\n    expected_dict = pa.concat_arrays([d1, d2, d3])\n\n    a = pa.chunked_array([a1, a2, a3, a4])\n    a_dense = pa.chunked_array([a1.cast('string'),\n                                a2.cast('string'),\n                                a3.cast('string'),\n                                a4.cast('string')])\n\n    result = a.to_pandas()\n    result_dense = a_dense.to_pandas()\n\n    assert (result.cat.categories == expected_dict.to_pandas()).all()\n\n    expected_dense = result.astype('str')\n    expected_dense[result_dense.isnull()] = None\n    tm.assert_series_equal(result_dense, expected_dense)\n\n\ndef test_dictionary_encoded_nested_to_pandas():\n    # ARROW-6899\n    child = pa.array(['a', 'a', 'a', 'b', 'b']).dictionary_encode()\n\n    arr = pa.ListArray.from_arrays([0, 3, 5], child)\n\n    result = arr.to_pandas()\n    expected = pd.Series([np.array(['a', 'a', 'a'], dtype=object),\n                          np.array(['b', 'b'], dtype=object)])\n\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dictionary_from_pandas():\n    cat = pd.Categorical(['a', 'b', 'a'])\n    expected_type = pa.dictionary(pa.int8(), pa.string())\n\n    result = pa.array(cat)\n    assert result.to_pylist() == ['a', 'b', 'a']\n    assert result.type.equals(expected_type)\n\n    # with missing values in categorical\n    cat = pd.Categorical(['a', 'b', None, 'a'])\n\n    result = pa.array(cat)\n    assert result.to_pylist() == ['a', 'b', None, 'a']\n    assert result.type.equals(expected_type)\n\n    # with additional mask\n    result = pa.array(cat, mask=np.array([False, False, False, True]))\n    assert result.to_pylist() == ['a', 'b', None, None]\n    assert result.type.equals(expected_type)\n\n\ndef test_dictionary_from_pandas_specified_type():\n    # ARROW-7168 - ensure specified type is always respected\n\n    # the same as cat = pd.Categorical(['a', 'b']) but explicit about dtypes\n    cat = pd.Categorical.from_codes(\n        np.array([0, 1], dtype='int8'), np.array(['a', 'b'], dtype=object))\n\n    # different index type -> allow this\n    # (the type of the 'codes' in pandas is not part of the data type)\n    typ = pa.dictionary(index_type=pa.int16(), value_type=pa.string())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == ['a', 'b']\n\n    # mismatching values type -> raise error\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.int64())\n    with pytest.raises(pa.ArrowInvalid):\n        result = pa.array(cat, type=typ)\n\n    # mismatching order -> raise error\n    typ = pa.dictionary(\n        index_type=pa.int8(), value_type=pa.string(), ordered=True)\n    msg = \"The 'ordered' flag of the passed categorical values \"\n    with pytest.raises(ValueError, match=msg):\n        result = pa.array(cat, type=typ)\n    assert result.to_pylist() == ['a', 'b']\n\n    # with mask\n    typ = pa.dictionary(index_type=pa.int16(), value_type=pa.string())\n    result = pa.array(cat, type=typ, mask=np.array([False, True]))\n    assert result.type.equals(typ)\n    assert result.to_pylist() == ['a', None]\n\n    # empty categorical -> be flexible in values type to allow\n    cat = pd.Categorical([])\n\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.string())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == []\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.int64())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == []\n\n    # passing non-dictionary type\n    cat = pd.Categorical(['a', 'b'])\n    result = pa.array(cat, type=pa.string())\n    expected = pa.array(['a', 'b'], type=pa.string())\n    assert result.equals(expected)\n    assert result.to_pylist() == ['a', 'b']\n\n\ndef test_convert_categories_to_array_with_string_pyarrow_dtype():\n    # gh-33727: categories should be converted to pa.Array\n    if Version(pd.__version__) < Version(\"1.3.0\"):\n        pytest.skip(\"PyArrow backed string data type introduced in pandas 1.3.0\")\n\n    df = pd.DataFrame({\"x\": [\"foo\", \"bar\", \"foo\"]}, dtype=\"string[pyarrow]\")\n    df = df.astype(\"category\")\n    indices = pa.array(df['x'].cat.codes)\n    dictionary = pa.array(df[\"x\"].cat.categories.values)\n    assert isinstance(dictionary, pa.Array)\n\n    expected = pa.Array.from_pandas(df['x'])\n    result = pa.DictionaryArray.from_arrays(indices, dictionary)\n    assert result == expected\n\n\n# ----------------------------------------------------------------------\n# Array protocol in pandas conversions tests\n\n\ndef test_array_protocol():\n    df = pd.DataFrame({'a': pd.Series([1, 2, None], dtype='Int64')})\n\n    # __arrow_array__ added to pandas IntegerArray in 0.26.0.dev\n\n    # default conversion\n    result = pa.table(df)\n    expected = pa.array([1, 2, None], pa.int64())\n    assert result[0].chunk(0).equals(expected)\n\n    # with specifying schema\n    schema = pa.schema([('a', pa.float64())])\n    result = pa.table(df, schema=schema)\n    expected2 = pa.array([1, 2, None], pa.float64())\n    assert result[0].chunk(0).equals(expected2)\n\n    # pass Series to pa.array\n    result = pa.array(df['a'])\n    assert result.equals(expected)\n    result = pa.array(df['a'], type=pa.float64())\n    assert result.equals(expected2)\n\n    # pass actual ExtensionArray to pa.array\n    result = pa.array(df['a'].values)\n    assert result.equals(expected)\n    result = pa.array(df['a'].values, type=pa.float64())\n    assert result.equals(expected2)\n\n\nclass DummyExtensionType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.int64())\n\n    def __reduce__(self):\n        return DummyExtensionType, ()\n\n\ndef PandasArray__arrow_array__(self, type=None):\n    # hardcode dummy return regardless of self - we only want to check that\n    # this method is correctly called\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    return pa.ExtensionArray.from_storage(DummyExtensionType(), storage)\n\n\ndef test_array_protocol_pandas_extension_types(monkeypatch):\n    # ARROW-7022 - ensure protocol works for Period / Interval extension dtypes\n\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(DummyExtensionType(), storage)\n\n    monkeypatch.setattr(pd.arrays.PeriodArray, \"__arrow_array__\",\n                        PandasArray__arrow_array__, raising=False)\n    monkeypatch.setattr(pd.arrays.IntervalArray, \"__arrow_array__\",\n                        PandasArray__arrow_array__, raising=False)\n    for arr in [pd.period_range(\"2012-01-01\", periods=3, freq=\"D\").array,\n                pd.interval_range(1, 4).array]:\n        result = pa.array(arr)\n        assert result.equals(expected)\n        result = pa.array(pd.Series(arr))\n        assert result.equals(expected)\n        result = pa.array(pd.Index(arr))\n        assert result.equals(expected)\n        result = pa.table(pd.DataFrame({'a': arr})).column('a').chunk(0)\n        assert result.equals(expected)\n\n\n# ----------------------------------------------------------------------\n# Pandas ExtensionArray support\n\n\ndef _Int64Dtype__from_arrow__(self, array):\n    # for test only deal with single chunk for now\n    # TODO: do we require handling of chunked arrays in the protocol?\n    if isinstance(array, pa.Array):\n        arr = array\n    else:\n        # ChunkedArray - here only deal with a single chunk for the test\n        arr = array.chunk(0)\n    buflist = arr.buffers()\n    data = np.frombuffer(buflist[-1], dtype='int64')[\n        arr.offset:arr.offset + len(arr)]\n    bitmask = buflist[0]\n    if bitmask is not None:\n        mask = pa.BooleanArray.from_buffers(\n            pa.bool_(), len(arr), [None, bitmask])\n        mask = np.asarray(mask)\n    else:\n        mask = np.ones(len(arr), dtype=bool)\n    int_arr = pd.arrays.IntegerArray(data.copy(), ~mask, copy=False)\n    return int_arr\n\n\ndef test_convert_to_extension_array(monkeypatch):\n    import pandas.core.internals as _int\n\n    # table converted from dataframe with extension types (so pandas_metadata\n    # has this information)\n    df = pd.DataFrame(\n        {'a': [1, 2, 3], 'b': pd.array([2, 3, 4], dtype='Int64'),\n         'c': [4, 5, 6]})\n    table = pa.table(df)\n\n    # Int64Dtype is recognized -> convert to extension block by default\n    # for a proper roundtrip\n    result = table.to_pandas()\n    assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    assert _get_mgr(result).blocks[0].values.dtype == np.dtype(\"int64\")\n    assert isinstance(_get_mgr(result).blocks[1], _int.ExtensionBlock)\n    tm.assert_frame_equal(result, df)\n\n    # test with missing values\n    df2 = pd.DataFrame({'a': pd.array([1, 2, None], dtype='Int64')})\n    table2 = pa.table(df2)\n    result = table2.to_pandas()\n    assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    tm.assert_frame_equal(result, df2)\n\n    # monkeypatch pandas Int64Dtype to *not* have the protocol method\n    if Version(pd.__version__) < Version(\"1.3.0.dev\"):\n        monkeypatch.delattr(\n            pd.core.arrays.integer._IntegerDtype, \"__from_arrow__\")\n    else:\n        monkeypatch.delattr(\n            pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")\n    # Int64Dtype has no __from_arrow__ -> use normal conversion\n    result = table.to_pandas()\n    assert len(_get_mgr(result).blocks) == 1\n    assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n\n\nclass MyCustomIntegerType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.int64())\n\n    def __reduce__(self):\n        return MyCustomIntegerType, ()\n\n    def to_pandas_dtype(self):\n        return pd.Int64Dtype()\n\n\ndef test_conversion_extensiontype_to_extensionarray(monkeypatch):\n    # converting extension type to linked pandas ExtensionDtype/Array\n    import pandas.core.internals as _int\n\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(MyCustomIntegerType(), storage)\n    table = pa.table({'a': arr})\n\n    # extension type points to Int64Dtype, which knows how to create a\n    # pandas ExtensionArray\n    result = arr.to_pandas()\n    assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    expected = pd.Series([1, 2, 3, 4], dtype='Int64')\n    tm.assert_series_equal(result, expected)\n\n    result = table.to_pandas()\n    assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    expected = pd.DataFrame({'a': pd.array([1, 2, 3, 4], dtype='Int64')})\n    tm.assert_frame_equal(result, expected)\n\n    # monkeypatch pandas Int64Dtype to *not* have the protocol method\n    # (remove the version added above and the actual version for recent pandas)\n    if Version(pd.__version__) < Version(\"1.3.0.dev\"):\n        monkeypatch.delattr(\n            pd.core.arrays.integer._IntegerDtype, \"__from_arrow__\")\n    else:\n        monkeypatch.delattr(\n            pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")\n\n    result = arr.to_pandas()\n    assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    expected = pd.Series([1, 2, 3, 4])\n    tm.assert_series_equal(result, expected)\n\n    with pytest.raises(ValueError):\n        table.to_pandas()\n\n\ndef test_to_pandas_extension_dtypes_mapping():\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int64())})\n\n    # default use numpy dtype\n    result = table.to_pandas()\n    assert result['a'].dtype == np.dtype('int64')\n\n    # specify to override the default\n    result = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n    assert isinstance(result['a'].dtype, pd.Int64Dtype)\n\n    # types that return None in function get normal conversion\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int32())})\n    result = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n    assert result['a'].dtype == np.dtype('int32')\n\n    # `types_mapper` overrules the pandas metadata\n    table = pa.table(pd.DataFrame({'a': pd.array([1, 2, 3], dtype=\"Int64\")}))\n    result = table.to_pandas()\n    assert isinstance(result['a'].dtype, pd.Int64Dtype)\n    result = table.to_pandas(\n        types_mapper={pa.int64(): pd.PeriodDtype('D')}.get)\n    assert isinstance(result['a'].dtype, pd.PeriodDtype)\n\n\ndef test_array_to_pandas():\n    if Version(pd.__version__) < Version(\"1.1\"):\n        pytest.skip(\"ExtensionDtype to_pandas method missing\")\n\n    for arr in [pd.period_range(\"2012-01-01\", periods=3, freq=\"D\").array,\n                pd.interval_range(1, 4).array]:\n        result = pa.array(arr).to_pandas()\n        expected = pd.Series(arr)\n        tm.assert_series_equal(result, expected)\n\n        result = pa.table({\"col\": arr})[\"col\"].to_pandas()\n        expected = pd.Series(arr, name=\"col\")\n        tm.assert_series_equal(result, expected)\n\n\ndef test_roundtrip_empty_table_with_extension_dtype_index():\n    df = pd.DataFrame(index=pd.interval_range(start=0, end=3))\n    table = pa.table(df)\n    table.to_pandas().index == pd.Index([{'left': 0, 'right': 1},\n                                         {'left': 1, 'right': 2},\n                                         {'left': 2, 'right': 3}],\n                                        dtype='object')\n\n\n@pytest.mark.parametrize(\"index\", [\"a\", [\"a\", \"b\"]])\ndef test_to_pandas_types_mapper_index(index):\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"ArrowDtype missing\")\n    df = pd.DataFrame(\n        {\n            \"a\": [1, 2],\n            \"b\": [3, 4],\n            \"c\": [5, 6],\n        },\n        dtype=pd.ArrowDtype(pa.int64()),\n    ).set_index(index)\n    expected = df.copy()\n    table = pa.table(df)\n    result = table.to_pandas(types_mapper=pd.ArrowDtype)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_array_to_pandas_types_mapper():\n    # https://issues.apache.org/jira/browse/ARROW-9664\n    if Version(pd.__version__) < Version(\"1.2.0\"):\n        pytest.skip(\"Float64Dtype extension dtype missing\")\n\n    data = pa.array([1, 2, 3], pa.int64())\n\n    # Test with mapper function\n    types_mapper = {pa.int64(): pd.Int64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == pd.Int64Dtype()\n\n    # Test mapper function returning None\n    types_mapper = {pa.int64(): None}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n    # Test mapper function not containing the dtype\n    types_mapper = {pa.float64(): pd.Float64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n\n@pytest.mark.pandas\ndef test_chunked_array_to_pandas_types_mapper():\n    # https://issues.apache.org/jira/browse/ARROW-9664\n    if Version(pd.__version__) < Version(\"1.2.0\"):\n        pytest.skip(\"Float64Dtype extension dtype missing\")\n\n    data = pa.chunked_array([pa.array([1, 2, 3], pa.int64())])\n    assert isinstance(data, pa.ChunkedArray)\n\n    # Test with mapper function\n    types_mapper = {pa.int64(): pd.Int64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == pd.Int64Dtype()\n\n    # Test mapper function returning None\n    types_mapper = {pa.int64(): None}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n    # Test mapper function not containing the dtype\n    types_mapper = {pa.float64(): pd.Float64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n\n# ----------------------------------------------------------------------\n# Legacy metadata compatibility tests\n\n\ndef test_metadata_compat_range_index_pre_0_12():\n    # Forward compatibility for metadata created from pandas.RangeIndex\n    # prior to pyarrow 0.13.0\n    a_values = ['foo', 'bar', None, 'baz']\n    b_values = ['a', 'a', 'b', 'b']\n    a_arrow = pa.array(a_values, type='utf8')\n    b_arrow = pa.array(b_values, type='utf8')\n\n    rng_index_arrow = pa.array([0, 2, 4, 6], type='int64')\n\n    gen_name_0 = '__index_level_0__'\n    gen_name_1 = '__index_level_1__'\n\n    # Case 1: named RangeIndex\n    e1 = pd.DataFrame({\n        'a': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    t1 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['a', 'qux'])\n    t1 = t1.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': ['qux'],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': 'qux',\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r1 = t1.to_pandas()\n    tm.assert_frame_equal(r1, e1)\n\n    # Case 2: named RangeIndex, but conflicts with an actual column\n    e2 = pd.DataFrame({\n        'qux': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    t2 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['qux', gen_name_0])\n    t2 = t2.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r2 = t2.to_pandas()\n    tm.assert_frame_equal(r2, e2)\n\n    # Case 3: unnamed RangeIndex\n    e3 = pd.DataFrame({\n        'a': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name=None))\n    t3 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['a', gen_name_0])\n    t3 = t3.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r3 = t3.to_pandas()\n    tm.assert_frame_equal(r3, e3)\n\n    # Case 4: MultiIndex with named RangeIndex\n    e4 = pd.DataFrame({\n        'a': a_values\n    }, index=[pd.RangeIndex(0, 8, step=2, name='qux'), b_values])\n    t4 = pa.Table.from_arrays([a_arrow, rng_index_arrow, b_arrow],\n                              names=['a', 'qux', gen_name_1])\n    t4 = t4.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': ['qux', gen_name_1],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': 'qux',\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_1,\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r4 = t4.to_pandas()\n    tm.assert_frame_equal(r4, e4)\n\n    # Case 4: MultiIndex with unnamed RangeIndex\n    e5 = pd.DataFrame({\n        'a': a_values\n    }, index=[pd.RangeIndex(0, 8, step=2, name=None), b_values])\n    t5 = pa.Table.from_arrays([a_arrow, rng_index_arrow, b_arrow],\n                              names=['a', gen_name_0, gen_name_1])\n    t5 = t5.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0, gen_name_1],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_1,\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r5 = t5.to_pandas()\n    tm.assert_frame_equal(r5, e5)\n\n\ndef test_metadata_compat_missing_field_name():\n    # Combination of missing field name but with index column as metadata.\n    # This combo occurs in the latest versions of fastparquet (0.3.2), but not\n    # in pyarrow itself (since field_name was added in 0.8, index as metadata\n    # only added later)\n\n    a_values = [1, 2, 3, 4]\n    b_values = ['a', 'b', 'c', 'd']\n    a_arrow = pa.array(a_values, type='int64')\n    b_arrow = pa.array(b_values, type='utf8')\n\n    expected = pd.DataFrame({\n        'a': a_values,\n        'b': b_values,\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    table = pa.table({'a': a_arrow, 'b': b_arrow})\n\n    # metadata generated by fastparquet 0.3.2 with missing field_names\n    table = table.replace_schema_metadata({\n        b'pandas': json.dumps({\n            'column_indexes': [\n                {'field_name': None,\n                 'metadata': None,\n                 'name': None,\n                 'numpy_type': 'object',\n                 'pandas_type': 'mixed-integer'}\n            ],\n            'columns': [\n                {'metadata': None,\n                 'name': 'a',\n                 'numpy_type': 'int64',\n                 'pandas_type': 'int64'},\n                {'metadata': None,\n                 'name': 'b',\n                 'numpy_type': 'object',\n                 'pandas_type': 'unicode'}\n            ],\n            'index_columns': [\n                {'kind': 'range',\n                 'name': 'qux',\n                 'start': 0,\n                 'step': 2,\n                 'stop': 8}\n            ],\n            'pandas_version': '0.25.0'}\n\n        )})\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_metadata_index_name_not_json_serializable():\n    name = np.int64(6)  # not json serializable by default\n    table = pa.table(pd.DataFrame(index=pd.RangeIndex(0, 4, name=name)))\n    metadata = table.schema.pandas_metadata\n    assert metadata['index_columns'][0]['name'] == '6'\n\n\ndef test_metadata_index_name_is_json_serializable():\n    name = 6  # json serializable by default\n    table = pa.table(pd.DataFrame(index=pd.RangeIndex(0, 4, name=name)))\n    metadata = table.schema.pandas_metadata\n    assert metadata['index_columns'][0]['name'] == 6\n\n\ndef make_df_with_timestamps():\n    # Some of the milliseconds timestamps deliberately don't fit in the range\n    # that is possible with nanosecond timestamps.\n    df = pd.DataFrame({\n        'dateTimeMs': [\n            np.datetime64('0001-01-01 00:00', 'ms'),\n            np.datetime64('2012-05-02 12:35', 'ms'),\n            np.datetime64('2012-05-03 15:42', 'ms'),\n            np.datetime64('3000-05-03 15:42', 'ms'),\n        ],\n        'dateTimeNs': [\n            np.datetime64('1991-01-01 00:00', 'ns'),\n            np.datetime64('2012-05-02 12:35', 'ns'),\n            np.datetime64('2012-05-03 15:42', 'ns'),\n            np.datetime64('2050-05-03 15:42', 'ns'),\n        ],\n    })\n    # Not part of what we're testing, just ensuring that the inputs are what we\n    # expect.\n    assert (df.dateTimeMs.dtype, df.dateTimeNs.dtype) == (\n        # O == object, M8[ns] == timestamp64[ns]\n        np.dtype(\"O\"), np.dtype(\"M8[ns]\")\n    )\n    return df\n\n\n@pytest.mark.parquet\n@pytest.mark.filterwarnings(\"ignore:Parquet format '2.0':FutureWarning\")\ndef test_timestamp_as_object_parquet(tempdir):\n    # Timestamps can be stored as Parquet and reloaded into Pandas with no loss\n    # of information if the timestamp_as_object option is True.\n    df = make_df_with_timestamps()\n    table = pa.Table.from_pandas(df)\n    filename = tempdir / \"timestamps_from_pandas.parquet\"\n    pq.write_table(table, filename, version=\"2.0\")\n    result = pq.read_table(filename)\n    df2 = result.to_pandas(timestamp_as_object=True)\n    tm.assert_frame_equal(df, df2)\n\n\ndef test_timestamp_as_object_out_of_range():\n    # Out of range timestamps can be converted Arrow and reloaded into Pandas\n    # with no loss of information if the timestamp_as_object option is True.\n    df = make_df_with_timestamps()\n    table = pa.Table.from_pandas(df)\n    df2 = table.to_pandas(timestamp_as_object=True)\n    tm.assert_frame_equal(df, df2)\n\n\n@pytest.mark.parametrize(\"resolution\", [\"s\", \"ms\", \"us\"])\n@pytest.mark.parametrize(\"tz\", [None, \"America/New_York\"])\n# One datetime outside nanosecond range, one inside nanosecond range:\n@pytest.mark.parametrize(\"dt\", [datetime(1553, 1, 1), datetime(2020, 1, 1)])\ndef test_timestamp_as_object_non_nanosecond(resolution, tz, dt):\n    # Timestamps can be converted Arrow and reloaded into Pandas with no loss\n    # of information if the timestamp_as_object option is True.\n    arr = pa.array([dt], type=pa.timestamp(resolution, tz=tz))\n    table = pa.table({'a': arr})\n\n    for result in [\n        arr.to_pandas(timestamp_as_object=True),\n        table.to_pandas(timestamp_as_object=True)['a']\n    ]:\n        assert result.dtype == object\n        assert isinstance(result[0], datetime)\n        if tz:\n            assert result[0].tzinfo is not None\n            expected = result[0].tzinfo.fromutc(dt)\n        else:\n            assert result[0].tzinfo is None\n            expected = dt\n        assert result[0] == expected\n\n\ndef test_timestamp_as_object_fixed_offset():\n    # ARROW-16547 to_pandas with timestamp_as_object=True and FixedOffset\n    pytz = pytest.importorskip(\"pytz\")\n    import datetime\n    timezone = pytz.FixedOffset(120)\n    dt = timezone.localize(datetime.datetime(2022, 5, 12, 16, 57))\n\n    table = pa.table({\"timestamp_col\": pa.array([dt])})\n    result = table.to_pandas(timestamp_as_object=True)\n    assert pa.table(result) == table\n\n\ndef test_threaded_pandas_import():\n    invoke_script(\"pandas_threaded_import.py\")\n\n\ndef test_does_not_mutate_timedelta_dtype():\n    expected = np.dtype('m8')\n\n    assert np.dtype(np.timedelta64) == expected\n\n    df = pd.DataFrame({\"a\": [np.timedelta64()]})\n    t = pa.Table.from_pandas(df)\n    t.to_pandas()\n\n    assert np.dtype(np.timedelta64) == expected\n\n\ndef test_does_not_mutate_timedelta_nested():\n    # ARROW-17893: dataframe with timedelta and a list of dictionary\n    # also with timedelta produces wrong result with to_pandas\n\n    from datetime import timedelta\n    timedelta_1 = [{\"timedelta_1\": timedelta(seconds=12, microseconds=1)}]\n    timedelta_2 = [timedelta(hours=3, minutes=40, seconds=23)]\n    table = pa.table({\"timedelta_1\": timedelta_1, \"timedelta_2\": timedelta_2})\n    df = table.to_pandas()\n\n    assert df[\"timedelta_2\"][0].to_pytimedelta() == timedelta_2[0]\n\n\ndef test_roundtrip_nested_map_table_with_pydicts():\n    schema = pa.schema([\n        pa.field(\n            \"a\",\n            pa.list_(\n                pa.map_(pa.int8(), pa.struct([pa.field(\"b\", pa.binary())]))\n            )\n        )\n    ])\n    table = pa.table([[\n        [[(1, None)]],\n        None,\n        [\n            [(2, {\"b\": b\"abc\"})],\n            [(3, {\"b\": None}), (4, {\"b\": b\"def\"})],\n        ]\n    ]],\n        schema=schema,\n    )\n\n    expected_default_df = pd.DataFrame(\n        {\"a\": [[[(1, None)]], None, [[(2, {\"b\": b\"abc\"})],\n                                     [(3, {\"b\": None}), (4, {\"b\": b\"def\"})]]]}\n    )\n    expected_as_pydicts_df = pd.DataFrame(\n        {\"a\": [\n            [{1: None}],\n            None,\n            [{2: {\"b\": b\"abc\"}}, {3: {\"b\": None}, 4: {\"b\": b\"def\"}}],\n        ]}\n    )\n\n    default_df = table.to_pandas()\n    as_pydicts_df = table.to_pandas(maps_as_pydicts=\"strict\")\n\n    tm.assert_frame_equal(default_df, expected_default_df)\n    tm.assert_frame_equal(as_pydicts_df, expected_as_pydicts_df)\n\n    table_default_roundtrip = pa.Table.from_pandas(default_df, schema=schema)\n    assert table.equals(table_default_roundtrip)\n\n    table_as_pydicts_roundtrip = pa.Table.from_pandas(as_pydicts_df, schema=schema)\n    assert table.equals(table_as_pydicts_roundtrip)\n\n\ndef test_roundtrip_nested_map_array_with_pydicts_sliced():\n    \"\"\"\n    Slightly more robust test with chunking and slicing\n    \"\"\"\n    keys_1 = pa.array(['foo', 'bar'])\n    keys_2 = pa.array(['baz', 'qux', 'quux', 'quz'])\n    keys_3 = pa.array([], pa.string())\n\n    items_1 = pa.array(\n        [['a', 'b'], ['c', 'd']],\n        pa.list_(pa.string()),\n    )\n    items_2 = pa.array(\n        [[], None, [None, 'e'], ['f', 'g']],\n        pa.list_(pa.string()),\n    )\n    items_3 = pa.array(\n        [],\n        pa.list_(pa.string()),\n    )\n\n    map_chunk_1 = pa.MapArray.from_arrays([0, 2], keys_1, items_1)\n    map_chunk_2 = pa.MapArray.from_arrays([0, 3, 4], keys_2, items_2)\n    map_chunk_3 = pa.MapArray.from_arrays([0, 0], keys_3, items_3)\n    chunked_array = pa.chunked_array([\n        pa.ListArray.from_arrays([0, 1], map_chunk_1).slice(0),\n        pa.ListArray.from_arrays([0, 1], map_chunk_2.slice(1)).slice(0),\n        pa.ListArray.from_arrays([0, 0], map_chunk_3).slice(0),\n    ])\n\n    series_default = chunked_array.to_pandas()\n    expected_series_default = pd.Series([\n        [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n        [[('quz', ['f', 'g'])]],\n        [],\n    ])\n\n    series_pydicts = chunked_array.to_pandas(maps_as_pydicts=\"strict\")\n    expected_series_pydicts = pd.Series([\n        [{'foo': ['a', 'b'], 'bar': ['c', 'd']}],\n        [{'quz': ['f', 'g']}],\n        [],\n    ])\n\n    sliced = chunked_array.slice(1, 3)\n    series_default_sliced = sliced.to_pandas()\n    expected_series_default_sliced = pd.Series([\n        [[('quz', ['f', 'g'])]],\n        [],\n    ])\n\n    series_pydicts_sliced = sliced.to_pandas(maps_as_pydicts=\"strict\")\n    expected_series_pydicts_sliced = pd.Series([\n        [{'quz': ['f', 'g']}],\n        [],\n    ])\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                DeprecationWarning)\n        tm.assert_series_equal(series_default, expected_series_default)\n        tm.assert_series_equal(series_pydicts, expected_series_pydicts)\n        tm.assert_series_equal(series_default_sliced, expected_series_default_sliced)\n        tm.assert_series_equal(series_pydicts_sliced, expected_series_pydicts_sliced)\n\n    ty = pa.list_(pa.map_(pa.string(), pa.list_(pa.string())))\n\n    def assert_roundtrip(series: pd.Series, data) -> None:\n        array_roundtrip = pa.chunked_array(pa.Array.from_pandas(series, type=ty))\n        array_roundtrip.validate(full=True)\n        assert data.equals(array_roundtrip)\n\n    assert_roundtrip(series_default, chunked_array)\n    assert_roundtrip(series_pydicts, chunked_array)\n    assert_roundtrip(series_default_sliced, sliced)\n    assert_roundtrip(series_pydicts_sliced, sliced)\n\n\ndef test_roundtrip_map_array_with_pydicts_duplicate_keys():\n    keys = pa.array(['foo', 'bar', 'foo'])\n    items = pa.array(\n        [['a', 'b'], ['c', 'd'], ['1', '2']],\n        pa.list_(pa.string()),\n    )\n    offsets = [0, 3]\n    maps = pa.MapArray.from_arrays(offsets, keys, items)\n    ty = pa.map_(pa.string(), pa.list_(pa.string()))\n\n    # ------------------------\n    # With maps as pydicts\n    with pytest.raises(pa.lib.ArrowException):\n        # raises because of duplicate keys\n        maps.to_pandas(maps_as_pydicts=\"strict\")\n    series_pydicts = maps.to_pandas(maps_as_pydicts=\"lossy\")\n    # some data loss occurs for duplicate keys\n    expected_series_pydicts = pd.Series([\n        {'foo': ['1', '2'], 'bar': ['c', 'd']},\n    ])\n    # roundtrip is not possible because of data loss\n    assert not maps.equals(pa.Array.from_pandas(series_pydicts, type=ty))\n\n    # ------------------------\n    # With default assoc list of tuples\n    series_default = maps.to_pandas()\n    expected_series_default = pd.Series([\n        [('foo', ['a', 'b']), ('bar', ['c', 'd']), ('foo', ['1', '2'])],\n    ])\n    assert maps.equals(pa.Array.from_pandas(series_default, type=ty))\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series_pydicts, expected_series_pydicts)\n    assert len(series_pydicts) == len(expected_series_pydicts)\n    for row1, row2 in zip(series_pydicts, expected_series_pydicts):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1.items(), row2.items()):\n            assert tup1[0] == tup2[0]\n            assert np.array_equal(tup1[1], tup2[1])\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series_default, expected_series_default)\n    assert len(series_default) == len(expected_series_default)\n    for row1, row2 in zip(series_default, expected_series_default):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1, row2):\n            assert tup1[0] == tup2[0]\n            assert np.array_equal(tup1[1], tup2[1])\n\n\ndef test_unhashable_map_keys_with_pydicts():\n    keys = pa.array(\n        [['a', 'b'], ['c', 'd'], [], ['e'], [None, 'f'], ['g', 'h']],\n        pa.list_(pa.string()),\n    )\n    items = pa.array(['foo', 'bar', 'baz', 'qux', 'quux', 'quz'])\n    offsets = [0, 2, 6]\n    maps = pa.MapArray.from_arrays(offsets, keys, items)\n\n    # ------------------------\n    # With maps as pydicts\n    with pytest.raises(TypeError):\n        maps.to_pandas(maps_as_pydicts=\"lossy\")\n\n    # ------------------------\n    # With default assoc list of tuples\n    series = maps.to_pandas()\n    expected_series_default = pd.Series([\n        [(['a', 'b'], 'foo'), (['c', 'd'], 'bar')],\n        [([], 'baz'), (['e'], 'qux'), ([None, 'f'], 'quux'), (['g', 'h'], 'quz')],\n    ])\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series, expected_series_default)\n    assert len(series) == len(expected_series_default)\n    for row1, row2 in zip(series, expected_series_default):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1, row2):\n            assert np.array_equal(tup1[0], tup2[0])\n            assert tup1[1] == tup2[1]\n\n\ndef test_table_column_conversion_for_datetime():\n    # GH-35235\n    # pandas implemented __from_arrow__ for DatetimeTZDtype,\n    # but we choose to do the conversion in Arrow instead.\n    # https://github.com/pandas-dev/pandas/pull/52201\n    series = pd.Series(pd.date_range(\"2012\", periods=2, tz=\"Europe/Brussels\"),\n                       name=\"datetime_column\")\n    table = pa.table({\"datetime_column\": pa.array(series)})\n    table_col = table.column(\"datetime_column\")\n\n    result = table_col.to_pandas()\n    assert result.name == \"datetime_column\"\n    tm.assert_series_equal(result, series)\n\n\ndef test_array_conversion_for_datetime():\n    # GH-35235\n    # pandas implemented __from_arrow__ for DatetimeTZDtype,\n    # but we choose to do the conversion in Arrow instead.\n    # https://github.com/pandas-dev/pandas/pull/52201\n    series = pd.Series(pd.date_range(\"2012\", periods=2, tz=\"Europe/Brussels\"))\n    arr = pa.array(series)\n\n    result = arr.to_pandas()\n    tm.assert_series_equal(result, series)\n\n\n@pytest.mark.large_memory\ndef test_nested_chunking_valid():\n    # GH-32439: Chunking can cause arrays to be in invalid state\n    # when nested types are involved.\n    # Here we simply ensure we validate correctly.\n\n    def roundtrip(df, schema=None):\n        tab = pa.Table.from_pandas(df, schema=schema)\n        tab.validate(full=True)\n        # we expect to trigger chunking internally\n        # an assertion failure here may just mean this threshold has changed\n        num_chunks = tab.column(0).num_chunks\n        assert num_chunks > 1\n        tm.assert_frame_equal(tab.to_pandas(self_destruct=True,\n                                            maps_as_pydicts=\"strict\"), df)\n\n    x = b\"0\" * 720000000\n    roundtrip(pd.DataFrame({\"strings\": [x, x, x]}))\n\n    struct = {\"struct_field\": x}\n    roundtrip(pd.DataFrame({\"structs\": [struct, struct, struct]}))\n\n    lists = [x]\n    roundtrip(pd.DataFrame({\"lists\": [lists, lists, lists]}))\n\n    los = [struct]\n    roundtrip(pd.DataFrame({\"los\": [los, los, los]}))\n\n    sol = {\"struct_field\": lists}\n    roundtrip(pd.DataFrame({\"sol\": [sol, sol, sol]}))\n\n    map_of_los = {\"a\": los}\n    map_type = pa.map_(pa.string(),\n                       pa.list_(pa.struct([(\"struct_field\", pa.binary())])))\n    schema = pa.schema([(\"maps\", map_type)])\n    roundtrip(pd.DataFrame({\"maps\": [map_of_los, map_of_los, map_of_los]}),\n              schema=schema)\n", "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer, PyCapsule_New, PyCapsule_IsValid\n\nimport atexit\nfrom collections.abc import Mapping\nimport pickle\nimport re\nimport sys\nimport warnings\nfrom cython import sizeof\n\n# These are imprecise because the type (in pandas 0.x) depends on the presence\n# of nulls\ncdef dict _pandas_type_map = {\n    _Type_NA: np.object_,  # NaNs\n    _Type_BOOL: np.bool_,\n    _Type_INT8: np.int8,\n    _Type_INT16: np.int16,\n    _Type_INT32: np.int32,\n    _Type_INT64: np.int64,\n    _Type_UINT8: np.uint8,\n    _Type_UINT16: np.uint16,\n    _Type_UINT32: np.uint32,\n    _Type_UINT64: np.uint64,\n    _Type_HALF_FLOAT: np.float16,\n    _Type_FLOAT: np.float32,\n    _Type_DOUBLE: np.float64,\n    # Pandas does not support [D]ay, so default to [ms] for date32\n    _Type_DATE32: np.dtype('datetime64[ms]'),\n    _Type_DATE64: np.dtype('datetime64[ms]'),\n    _Type_TIMESTAMP: {\n        's': np.dtype('datetime64[s]'),\n        'ms': np.dtype('datetime64[ms]'),\n        'us': np.dtype('datetime64[us]'),\n        'ns': np.dtype('datetime64[ns]'),\n    },\n    _Type_DURATION: {\n        's': np.dtype('timedelta64[s]'),\n        'ms': np.dtype('timedelta64[ms]'),\n        'us': np.dtype('timedelta64[us]'),\n        'ns': np.dtype('timedelta64[ns]'),\n    },\n    _Type_BINARY: np.object_,\n    _Type_FIXED_SIZE_BINARY: np.object_,\n    _Type_STRING: np.object_,\n    _Type_LIST: np.object_,\n    _Type_MAP: np.object_,\n    _Type_DECIMAL128: np.object_,\n}\n\ncdef dict _pep3118_type_map = {\n    _Type_INT8: b'b',\n    _Type_INT16: b'h',\n    _Type_INT32: b'i',\n    _Type_INT64: b'q',\n    _Type_UINT8: b'B',\n    _Type_UINT16: b'H',\n    _Type_UINT32: b'I',\n    _Type_UINT64: b'Q',\n    _Type_HALF_FLOAT: b'e',\n    _Type_FLOAT: b'f',\n    _Type_DOUBLE: b'd',\n}\n\n\ncdef bytes _datatype_to_pep3118(CDataType* type):\n    \"\"\"\n    Construct a PEP 3118 format string describing the given datatype.\n    None is returned for unsupported types.\n    \"\"\"\n    try:\n        char = _pep3118_type_map[type.id()]\n    except KeyError:\n        return None\n    else:\n        if char in b'bBhHiIqQ':\n            # Use \"standard\" int widths, not native\n            return b'=' + char\n        else:\n            return char\n\n\ncdef void* _as_c_pointer(v, allow_null=False) except *:\n    \"\"\"\n    Convert a Python object to a raw C pointer.\n\n    Used mainly for the C data interface.\n    Integers are accepted as well as capsule objects with a NULL name.\n    (the latter for compatibility with raw pointers exported by reticulate)\n    \"\"\"\n    cdef void* c_ptr\n    if isinstance(v, int):\n        c_ptr = <void*> <uintptr_t > v\n    elif isinstance(v, float):\n        warnings.warn(\n            \"Passing a pointer value as a float is unsafe and only \"\n            \"supported for compatibility with older versions of the R \"\n            \"Arrow library\", UserWarning, stacklevel=2)\n        c_ptr = <void*> <uintptr_t > v\n    elif PyCapsule_CheckExact(v):\n        c_ptr = PyCapsule_GetPointer(v, NULL)\n    else:\n        raise TypeError(f\"Expected a pointer value, got {type(v)!r}\")\n    if not allow_null and c_ptr == NULL:\n        raise ValueError(f\"Null pointer (value before cast = {v!r})\")\n    return c_ptr\n\n\ndef _is_primitive(Type type):\n    # This is simply a redirect, the official API is in pyarrow.types.\n    return is_primitive(type)\n\n\ndef _get_pandas_type(arrow_type, coerce_to_ns=False):\n    cdef Type type_id = arrow_type.id\n    if type_id not in _pandas_type_map:\n        return None\n    if coerce_to_ns:\n        # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n        if type_id == _Type_DURATION:\n            return np.dtype('timedelta64[ns]')\n        return np.dtype('datetime64[ns]')\n    pandas_type = _pandas_type_map[type_id]\n    if isinstance(pandas_type, dict):\n        unit = getattr(arrow_type, 'unit', None)\n        pandas_type = pandas_type.get(unit, None)\n    return pandas_type\n\n\ndef _get_pandas_tz_type(arrow_type, coerce_to_ns=False):\n    from pyarrow.pandas_compat import make_datetimetz\n    unit = 'ns' if coerce_to_ns else arrow_type.unit\n    return make_datetimetz(unit, arrow_type.tz)\n\n\ndef _to_pandas_dtype(arrow_type, options=None):\n    coerce_to_ns = (options and options.get('coerce_temporal_nanoseconds', False)) or (\n        _pandas_api.is_v1() and arrow_type.id in\n        [_Type_DATE32, _Type_DATE64, _Type_TIMESTAMP, _Type_DURATION])\n\n    if getattr(arrow_type, 'tz', None):\n        dtype = _get_pandas_tz_type(arrow_type, coerce_to_ns)\n    else:\n        dtype = _get_pandas_type(arrow_type, coerce_to_ns)\n\n    if not dtype:\n        raise NotImplementedError(str(arrow_type))\n\n    return dtype\n\n\n# Workaround for Cython parsing bug\n# https://github.com/cython/cython/issues/2143\nctypedef CFixedWidthType* _CFixedWidthTypePtr\n\n\ncdef class DataType(_Weakrefable):\n    \"\"\"\n    Base class of all Arrow data types.\n\n    Each data type is an *instance* of this class.\n\n    Examples\n    --------\n    Instance of int64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int64()\n    DataType(int64)\n    \"\"\"\n\n    def __cinit__(self):\n        pass\n\n    def __init__(self):\n        raise TypeError(\"Do not call {}'s constructor directly, use public \"\n                        \"functions like pyarrow.int64, pyarrow.list_, etc. \"\n                        \"instead.\".format(self.__class__.__name__))\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        assert type != nullptr\n        self.sp_type = type\n        self.type = type.get()\n        self.pep3118_format = _datatype_to_pep3118(self.type)\n\n    cpdef Field field(self, i):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        pyarrow.Field\n        \"\"\"\n        if not isinstance(i, int):\n            raise TypeError(f\"Expected int index, got type '{type(i)}'\")\n        cdef int index = <int> _normalize_index(i, self.type.num_fields())\n        return pyarrow_wrap_field(self.type.field(index))\n\n    @property\n    def id(self):\n        return self.type.id()\n\n    @property\n    def bit_width(self):\n        \"\"\"\n        Bit width for fixed width type.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64()\n        DataType(int64)\n        >>> pa.int64().bit_width\n        64\n        \"\"\"\n        cdef _CFixedWidthTypePtr ty\n        ty = dynamic_cast[_CFixedWidthTypePtr](self.type)\n        if ty == nullptr:\n            raise ValueError(\"Non-fixed width type\")\n        return ty.bit_width()\n\n    @property\n    def num_fields(self):\n        \"\"\"\n        The number of child fields.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64()\n        DataType(int64)\n        >>> pa.int64().num_fields\n        0\n        >>> pa.list_(pa.string())\n        ListType(list<item: string>)\n        >>> pa.list_(pa.string()).num_fields\n        1\n        >>> struct = pa.struct({'x': pa.int32(), 'y': pa.string()})\n        >>> struct.num_fields\n        2\n        \"\"\"\n        return self.type.num_fields()\n\n    @property\n    def num_buffers(self):\n        \"\"\"\n        Number of data buffers required to construct Array type\n        excluding children.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64().num_buffers\n        2\n        >>> pa.string().num_buffers\n        3\n        \"\"\"\n        return self.type.layout().buffers.size()\n\n    def __str__(self):\n        return frombytes(self.type.ToString(), safe=True)\n\n    def __hash__(self):\n        return hash(str(self))\n\n    def __reduce__(self):\n        return type_for_alias, (str(self),)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0})'.format(self)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except (TypeError, ValueError):\n            return NotImplemented\n\n    def equals(self, other, *, check_metadata=False):\n        \"\"\"\n        Return true if type is equivalent to passed value.\n\n        Parameters\n        ----------\n        other : DataType or string convertible to DataType\n        check_metadata : bool\n            Whether nested Field metadata equality should be checked as well.\n\n        Returns\n        -------\n        is_equal : bool\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64().equals(pa.string())\n        False\n        >>> pa.int64().equals(pa.int64())\n        True\n        \"\"\"\n        cdef:\n            DataType other_type\n            c_bool c_check_metadata\n\n        other_type = ensure_type(other)\n        c_check_metadata = check_metadata\n        return self.type.Equals(deref(other_type.type), c_check_metadata)\n\n    def to_pandas_dtype(self):\n        \"\"\"\n        Return the equivalent NumPy / Pandas dtype.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64().to_pandas_dtype()\n        <class 'numpy.int64'>\n        \"\"\"\n        return _to_pandas_dtype(self)\n\n    def _export_to_c(self, out_ptr):\n        \"\"\"\n        Export to a C ArrowSchema struct, given its pointer.\n\n        Be careful: if you don't pass the ArrowSchema struct to a consumer,\n        its memory will leak.  This is a low-level function intended for\n        expert users.\n        \"\"\"\n        check_status(ExportType(deref(self.type),\n                                <ArrowSchema*> _as_c_pointer(out_ptr)))\n\n    @staticmethod\n    def _import_from_c(in_ptr):\n        \"\"\"\n        Import DataType from a C ArrowSchema struct, given its pointer.\n\n        This is a low-level function intended for expert users.\n        \"\"\"\n        result = GetResultValue(ImportType(<ArrowSchema*>\n                                           _as_c_pointer(in_ptr)))\n        return pyarrow_wrap_data_type(result)\n\n    def __arrow_c_schema__(self):\n        \"\"\"\n        Export to a ArrowSchema PyCapsule\n\n        Unlike _export_to_c, this will not leak memory if the capsule is not used.\n        \"\"\"\n        cdef ArrowSchema* c_schema\n        capsule = alloc_c_schema(&c_schema)\n\n        with nogil:\n            check_status(ExportType(deref(self.type), c_schema))\n\n        return capsule\n\n    @staticmethod\n    def _import_from_c_capsule(schema):\n        \"\"\"\n        Import a DataType from a ArrowSchema PyCapsule\n\n        Parameters\n        ----------\n        schema : PyCapsule\n            A valid PyCapsule with name 'arrow_schema' containing an\n            ArrowSchema pointer.\n        \"\"\"\n        cdef:\n            ArrowSchema* c_schema\n            shared_ptr[CDataType] c_type\n\n        if not PyCapsule_IsValid(schema, 'arrow_schema'):\n            raise TypeError(\n                \"Not an ArrowSchema object\"\n            )\n        c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')\n\n        with nogil:\n            c_type = GetResultValue(ImportType(c_schema))\n\n        return pyarrow_wrap_data_type(c_type)\n\n\ncdef class DictionaryMemo(_Weakrefable):\n    \"\"\"\n    Tracking container for dictionary-encoded fields.\n    \"\"\"\n\n    def __cinit__(self):\n        self.sp_memo.reset(new CDictionaryMemo())\n        self.memo = self.sp_memo.get()\n\n\ncdef class DictionaryType(DataType):\n    \"\"\"\n    Concrete class for dictionary data types.\n\n    Examples\n    --------\n    Create an instance of dictionary type:\n\n    >>> import pyarrow as pa\n    >>> pa.dictionary(pa.int64(), pa.utf8())\n    DictionaryType(dictionary<values=string, indices=int64, ordered=0>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.dict_type = <const CDictionaryType*> type.get()\n\n    def __reduce__(self):\n        return dictionary, (self.index_type, self.value_type, self.ordered)\n\n    @property\n    def ordered(self):\n        \"\"\"\n        Whether the dictionary is ordered, i.e. whether the ordering of values\n        in the dictionary is important.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.dictionary(pa.int64(), pa.utf8()).ordered\n        False\n        \"\"\"\n        return self.dict_type.ordered()\n\n    @property\n    def index_type(self):\n        \"\"\"\n        The data type of dictionary indices (a signed integer type).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.dictionary(pa.int16(), pa.utf8()).index_type\n        DataType(int16)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.dict_type.index_type())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The dictionary value type.\n\n        The dictionary values are found in an instance of DictionaryArray.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.dictionary(pa.int16(), pa.utf8()).value_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.dict_type.value_type())\n\n\ncdef class ListType(DataType):\n    \"\"\"\n    Concrete class for list data types.\n\n    Examples\n    --------\n    Create an instance of ListType:\n\n    >>> import pyarrow as pa\n    >>> pa.list_(pa.string())\n    ListType(list<item: string>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.list_type = <const CListType*> type.get()\n\n    def __reduce__(self):\n        return list_, (self.value_field,)\n\n    @property\n    def value_field(self):\n        \"\"\"\n        The field for list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.string()).value_field\n        pyarrow.Field<item: string>\n        \"\"\"\n        return pyarrow_wrap_field(self.list_type.value_field())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The data type of list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.string()).value_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.list_type.value_type())\n\n\ncdef class LargeListType(DataType):\n    \"\"\"\n    Concrete class for large list data types\n    (like ListType, but with 64-bit offsets).\n\n    Examples\n    --------\n    Create an instance of LargeListType:\n\n    >>> import pyarrow as pa\n    >>> pa.large_list(pa.string())\n    LargeListType(large_list<item: string>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.list_type = <const CLargeListType*> type.get()\n\n    def __reduce__(self):\n        return large_list, (self.value_field,)\n\n    @property\n    def value_field(self):\n        return pyarrow_wrap_field(self.list_type.value_field())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The data type of large list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.large_list(pa.string()).value_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.list_type.value_type())\n\n\ncdef class MapType(DataType):\n    \"\"\"\n    Concrete class for map data types.\n\n    Examples\n    --------\n    Create an instance of MapType:\n\n    >>> import pyarrow as pa\n    >>> pa.map_(pa.string(), pa.int32())\n    MapType(map<string, int32>)\n    >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True)\n    MapType(map<string, int32, keys_sorted>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.map_type = <const CMapType*> type.get()\n\n    def __reduce__(self):\n        return map_, (self.key_field, self.item_field)\n\n    @property\n    def key_field(self):\n        \"\"\"\n        The field for keys in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).key_field\n        pyarrow.Field<key: string not null>\n        \"\"\"\n        return pyarrow_wrap_field(self.map_type.key_field())\n\n    @property\n    def key_type(self):\n        \"\"\"\n        The data type of keys in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).key_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.map_type.key_type())\n\n    @property\n    def item_field(self):\n        \"\"\"\n        The field for items in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).item_field\n        pyarrow.Field<value: int32>\n        \"\"\"\n        return pyarrow_wrap_field(self.map_type.item_field())\n\n    @property\n    def item_type(self):\n        \"\"\"\n        The data type of items in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).item_type\n        DataType(int32)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.map_type.item_type())\n\n    @property\n    def keys_sorted(self):\n        \"\"\"\n        Should the entries be sorted according to keys.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True).keys_sorted\n        True\n        \"\"\"\n        return self.map_type.keys_sorted()\n\n\ncdef class FixedSizeListType(DataType):\n    \"\"\"\n    Concrete class for fixed size list data types.\n\n    Examples\n    --------\n    Create an instance of FixedSizeListType:\n\n    >>> import pyarrow as pa\n    >>> pa.list_(pa.int32(), 2)\n    FixedSizeListType(fixed_size_list<item: int32>[2])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.list_type = <const CFixedSizeListType*> type.get()\n\n    def __reduce__(self):\n        return list_, (self.value_type, self.list_size)\n\n    @property\n    def value_field(self):\n        \"\"\"\n        The field for list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.int32(), 2).value_field\n        pyarrow.Field<item: int32>\n        \"\"\"\n        return pyarrow_wrap_field(self.list_type.value_field())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The data type of large list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.int32(), 2).value_type\n        DataType(int32)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.list_type.value_type())\n\n    @property\n    def list_size(self):\n        \"\"\"\n        The size of the fixed size lists.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.int32(), 2).list_size\n        2\n        \"\"\"\n        return self.list_type.list_size()\n\n\ncdef class StructType(DataType):\n    \"\"\"\n    Concrete class for struct data types.\n\n    ``StructType`` supports direct indexing using ``[...]`` (implemented via\n    ``__getitem__``) to access its fields.\n    It will return the struct field with the given index or name.\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n\n    Accessing fields using direct indexing:\n\n    >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n    >>> struct_type[0]\n    pyarrow.Field<x: int32>\n    >>> struct_type['y']\n    pyarrow.Field<y: string>\n\n    Accessing fields using ``field()``:\n\n    >>> struct_type.field(1)\n    pyarrow.Field<y: string>\n    >>> struct_type.field('x')\n    pyarrow.Field<x: int32>\n\n    # Creating a schema from the struct type's fields:\n    >>> pa.schema(list(struct_type))\n    x: int32\n    y: string\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.struct_type = <const CStructType*> type.get()\n\n    cdef Field field_by_name(self, name):\n        \"\"\"\n        Return a child field by its name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        field : Field\n            The child field with the given name.\n\n        Raises\n        ------\n        KeyError\n            If the name isn't found, or if several fields have the given\n            name.\n        \"\"\"\n        cdef vector[shared_ptr[CField]] fields\n\n        fields = self.struct_type.GetAllFieldsByName(tobytes(name))\n        if fields.size() == 0:\n            raise KeyError(name)\n        elif fields.size() > 1:\n            warnings.warn(\"Struct field name corresponds to more \"\n                          \"than one field\", UserWarning)\n            raise KeyError(name)\n        else:\n            return pyarrow_wrap_field(fields[0])\n\n    def get_field_index(self, name):\n        \"\"\"\n        Return index of the unique field with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        index : int\n            The index of the field with the given name; -1 if the\n            name isn't found or there are several fields with the given\n            name.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n\n        Index of the field with a name 'y':\n\n        >>> struct_type.get_field_index('y')\n        1\n\n        Index of the field that does not exist:\n\n        >>> struct_type.get_field_index('z')\n        -1\n        \"\"\"\n        return self.struct_type.GetFieldIndex(tobytes(name))\n\n    cpdef Field field(self, i):\n        \"\"\"\n        Select a field by its column name or numeric index.\n\n        Parameters\n        ----------\n        i : int or str\n\n        Returns\n        -------\n        pyarrow.Field\n\n        Examples\n        --------\n\n        >>> import pyarrow as pa\n        >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n\n        Select the second field:\n\n        >>> struct_type.field(1)\n        pyarrow.Field<y: string>\n\n        Select the field named 'x':\n\n        >>> struct_type.field('x')\n        pyarrow.Field<x: int32>\n        \"\"\"\n        if isinstance(i, (bytes, str)):\n            return self.field_by_name(i)\n        elif isinstance(i, int):\n            return DataType.field(self, i)\n        else:\n            raise TypeError('Expected integer or string index')\n\n    def get_all_field_indices(self, name):\n        \"\"\"\n        Return sorted list of indices for the fields with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        indices : List[int]\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n        >>> struct_type.get_all_field_indices('x')\n        [0]\n        \"\"\"\n        return self.struct_type.GetAllFieldIndices(tobytes(name))\n\n    def __len__(self):\n        \"\"\"\n        Like num_fields().\n        \"\"\"\n        return self.type.num_fields()\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over struct fields, in order.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, i):\n        \"\"\"\n        Return the struct field with the given index or name.\n\n        Alias of ``field``.\n        \"\"\"\n        return self.field(i)\n\n    def __reduce__(self):\n        return struct, (list(self),)\n\n\ncdef class UnionType(DataType):\n    \"\"\"\n    Base class for union data types.\n\n    Examples\n    --------\n    Create an instance of a dense UnionType using ``pa.union``:\n\n    >>> import pyarrow as pa\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_DENSE),\n    (DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a dense UnionType using ``pa.dense_union``:\n\n    >>> pa.dense_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>)\n\n    Create an instance of a sparse UnionType using ``pa.union``:\n\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_SPARSE),\n    (SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a sparse UnionType using ``pa.sparse_union``:\n\n    >>> pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n\n    @property\n    def mode(self):\n        \"\"\"\n        The mode of the union (\"dense\" or \"sparse\").\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n        >>> union.mode\n        'sparse'\n        \"\"\"\n        cdef CUnionType* type = <CUnionType*> self.sp_type.get()\n        cdef int mode = type.mode()\n        if mode == _UnionMode_DENSE:\n            return 'dense'\n        if mode == _UnionMode_SPARSE:\n            return 'sparse'\n        assert 0\n\n    @property\n    def type_codes(self):\n        \"\"\"\n        The type code to indicate each data type in this union.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n        >>> union.type_codes\n        [0, 1]\n        \"\"\"\n        cdef CUnionType* type = <CUnionType*> self.sp_type.get()\n        return type.type_codes()\n\n    def __len__(self):\n        \"\"\"\n        Like num_fields().\n        \"\"\"\n        return self.type.num_fields()\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over union members, in order.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    cpdef Field field(self, i):\n        \"\"\"\n        Return a child field by its numeric index.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n        >>> union[0]\n        pyarrow.Field<a: fixed_size_binary[10]>\n        \"\"\"\n        if isinstance(i, int):\n            return DataType.field(self, i)\n        else:\n            raise TypeError('Expected integer')\n\n    def __getitem__(self, i):\n        \"\"\"\n        Return a child field by its index.\n\n        Alias of ``field``.\n        \"\"\"\n        return self.field(i)\n\n    def __reduce__(self):\n        return union, (list(self), self.mode, self.type_codes)\n\n\ncdef class SparseUnionType(UnionType):\n    \"\"\"\n    Concrete class for sparse union types.\n\n    Examples\n    --------\n    Create an instance of a sparse UnionType using ``pa.union``:\n\n    >>> import pyarrow as pa\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_SPARSE),\n    (SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a sparse UnionType using ``pa.sparse_union``:\n\n    >>> pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>)\n    \"\"\"\n\n\ncdef class DenseUnionType(UnionType):\n    \"\"\"\n    Concrete class for dense union types.\n\n    Examples\n    --------\n    Create an instance of a dense UnionType using ``pa.union``:\n\n    >>> import pyarrow as pa\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_DENSE),\n    (DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a dense UnionType using ``pa.dense_union``:\n\n    >>> pa.dense_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>)\n    \"\"\"\n\n\ncdef class TimestampType(DataType):\n    \"\"\"\n    Concrete class for timestamp data types.\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n\n    Create an instance of timestamp type:\n\n    >>> pa.timestamp('us')\n    TimestampType(timestamp[us])\n\n    Create an instance of timestamp type with timezone:\n\n    >>> pa.timestamp('s', tz='UTC')\n    TimestampType(timestamp[s, tz=UTC])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.ts_type = <const CTimestampType*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The timestamp unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.timestamp('us')\n        >>> t.unit\n        'us'\n        \"\"\"\n        return timeunit_to_string(self.ts_type.unit())\n\n    @property\n    def tz(self):\n        \"\"\"\n        The timestamp time zone, if any, or None.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.timestamp('s', tz='UTC')\n        >>> t.tz\n        'UTC'\n        \"\"\"\n        if self.ts_type.timezone().size() > 0:\n            return frombytes(self.ts_type.timezone())\n        else:\n            return None\n\n    def __reduce__(self):\n        return timestamp, (self.unit, self.tz)\n\n\ncdef class Time32Type(DataType):\n    \"\"\"\n    Concrete class for time32 data types.\n\n    Examples\n    --------\n    Create an instance of time32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.time32('ms')\n    Time32Type(time32[ms])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.time_type = <const CTime32Type*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The time unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.time32('ms')\n        >>> t.unit\n        'ms'\n        \"\"\"\n        return timeunit_to_string(self.time_type.unit())\n\n\ncdef class Time64Type(DataType):\n    \"\"\"\n    Concrete class for time64 data types.\n\n    Examples\n    --------\n    Create an instance of time64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.time64('us')\n    Time64Type(time64[us])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.time_type = <const CTime64Type*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The time unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.time64('us')\n        >>> t.unit\n        'us'\n        \"\"\"\n        return timeunit_to_string(self.time_type.unit())\n\n\ncdef class DurationType(DataType):\n    \"\"\"\n    Concrete class for duration data types.\n\n    Examples\n    --------\n    Create an instance of duration type:\n\n    >>> import pyarrow as pa\n    >>> pa.duration('s')\n    DurationType(duration[s])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.duration_type = <const CDurationType*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The duration unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.duration('s')\n        >>> t.unit\n        's'\n        \"\"\"\n        return timeunit_to_string(self.duration_type.unit())\n\n\ncdef class FixedSizeBinaryType(DataType):\n    \"\"\"\n    Concrete class for fixed-size binary data types.\n\n    Examples\n    --------\n    Create an instance of fixed-size binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.binary(3)\n    FixedSizeBinaryType(fixed_size_binary[3])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.fixed_size_binary_type = (\n            <const CFixedSizeBinaryType*> type.get())\n\n    def __reduce__(self):\n        return binary, (self.byte_width,)\n\n    @property\n    def byte_width(self):\n        \"\"\"\n        The binary size in bytes.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.binary(3)\n        >>> t.byte_width\n        3\n        \"\"\"\n        return self.fixed_size_binary_type.byte_width()\n\n\ncdef class Decimal128Type(FixedSizeBinaryType):\n    \"\"\"\n    Concrete class for decimal128 data types.\n\n    Examples\n    --------\n    Create an instance of decimal128 type:\n\n    >>> import pyarrow as pa\n    >>> pa.decimal128(5, 2)\n    Decimal128Type(decimal128(5, 2))\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        FixedSizeBinaryType.init(self, type)\n        self.decimal128_type = <const CDecimal128Type*> type.get()\n\n    def __reduce__(self):\n        return decimal128, (self.precision, self.scale)\n\n    @property\n    def precision(self):\n        \"\"\"\n        The decimal precision, in number of decimal digits (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal128(5, 2)\n        >>> t.precision\n        5\n        \"\"\"\n        return self.decimal128_type.precision()\n\n    @property\n    def scale(self):\n        \"\"\"\n        The decimal scale (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal128(5, 2)\n        >>> t.scale\n        2\n        \"\"\"\n        return self.decimal128_type.scale()\n\n\ncdef class Decimal256Type(FixedSizeBinaryType):\n    \"\"\"\n    Concrete class for decimal256 data types.\n\n    Examples\n    --------\n    Create an instance of decimal256 type:\n\n    >>> import pyarrow as pa\n    >>> pa.decimal256(76, 38)\n    Decimal256Type(decimal256(76, 38))\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        FixedSizeBinaryType.init(self, type)\n        self.decimal256_type = <const CDecimal256Type*> type.get()\n\n    def __reduce__(self):\n        return decimal256, (self.precision, self.scale)\n\n    @property\n    def precision(self):\n        \"\"\"\n        The decimal precision, in number of decimal digits (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal256(76, 38)\n        >>> t.precision\n        76\n        \"\"\"\n        return self.decimal256_type.precision()\n\n    @property\n    def scale(self):\n        \"\"\"\n        The decimal scale (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal256(76, 38)\n        >>> t.scale\n        38\n        \"\"\"\n        return self.decimal256_type.scale()\n\n\ncdef class RunEndEncodedType(DataType):\n    \"\"\"\n    Concrete class for run-end encoded types.\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.run_end_encoded_type = <const CRunEndEncodedType*> type.get()\n\n    def __reduce__(self):\n        return run_end_encoded, (self.run_end_type, self.value_type)\n\n    @property\n    def run_end_type(self):\n        return pyarrow_wrap_data_type(self.run_end_encoded_type.run_end_type())\n\n    @property\n    def value_type(self):\n        return pyarrow_wrap_data_type(self.run_end_encoded_type.value_type())\n\n\ncdef class BaseExtensionType(DataType):\n    \"\"\"\n    Concrete base class for extension types.\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.ext_type = <const CExtensionType*> type.get()\n\n    def __arrow_ext_class__(self):\n        \"\"\"\n        The associated array extension class\n        \"\"\"\n        return ExtensionArray\n\n    def __arrow_ext_scalar_class__(self):\n        \"\"\"\n        The associated scalar class\n        \"\"\"\n        return ExtensionScalar\n\n    @property\n    def extension_name(self):\n        \"\"\"\n        The extension type name.\n        \"\"\"\n        return frombytes(self.ext_type.extension_name())\n\n    @property\n    def storage_type(self):\n        \"\"\"\n        The underlying storage type.\n        \"\"\"\n        return pyarrow_wrap_data_type(self.ext_type.storage_type())\n\n    def wrap_array(self, storage):\n        \"\"\"\n        Wrap the given storage array as an extension array.\n\n        Parameters\n        ----------\n        storage : Array or ChunkedArray\n\n        Returns\n        -------\n        array : Array or ChunkedArray\n            Extension array wrapping the storage array\n        \"\"\"\n        cdef:\n            shared_ptr[CDataType] c_storage_type\n\n        if isinstance(storage, Array):\n            c_storage_type = (<Array> storage).ap.type()\n        elif isinstance(storage, ChunkedArray):\n            c_storage_type = (<ChunkedArray> storage).chunked_array.type()\n        else:\n            raise TypeError(\n                f\"Expected array or chunked array, got {storage.__class__}\")\n\n        if not c_storage_type.get().Equals(deref(self.ext_type)\n                                           .storage_type(), False):\n            raise TypeError(\n                f\"Incompatible storage type for {self}: \"\n                f\"expected {self.storage_type}, got {storage.type}\")\n\n        if isinstance(storage, Array):\n            return pyarrow_wrap_array(\n                self.ext_type.WrapArray(\n                    self.sp_type, (<Array> storage).sp_array))\n        else:\n            return pyarrow_wrap_chunked_array(\n                self.ext_type.WrapArray(\n                    self.sp_type, (<ChunkedArray> storage).sp_chunked_array))\n\n\ncdef class ExtensionType(BaseExtensionType):\n    \"\"\"\n    Concrete base class for Python-defined extension types.\n\n    Parameters\n    ----------\n    storage_type : DataType\n    extension_name : str\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing ExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.ExtensionType):\n    ...    def __init__(self):\n    ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n    ...    def __arrow_ext_serialize__(self):\n    ...       # since we don't have a parameterized type, we don't need extra\n    ...       # metadata to be deserialized\n    ...       return b''\n    ...    @classmethod\n    ...    def __arrow_ext_deserialize__(self, storage_type, serialized):\n    ...       # return an instance of this subclass given the serialized\n    ...       # metadata.\n    ...       return UuidType()\n    ...\n\n    Register the extension type:\n\n    >>> pa.register_extension_type(UuidType())\n\n    Create an instance of UuidType extension type:\n\n    >>> uuid_type = UuidType()\n\n    Inspect the extension type:\n\n    >>> uuid_type.extension_name\n    'my_package.uuid'\n    >>> uuid_type.storage_type\n    FixedSizeBinaryType(fixed_size_binary[16])\n\n    Wrap an array as an extension array:\n\n    >>> import uuid\n    >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)], pa.binary(16))\n    >>> uuid_type.wrap_array(storage_array)\n    <pyarrow.lib.ExtensionArray object at ...>\n    [\n      ...\n    ]\n\n    Or do the same with creating an ExtensionArray:\n\n    >>> pa.ExtensionArray.from_storage(uuid_type, storage_array)\n    <pyarrow.lib.ExtensionArray object at ...>\n    [\n      ...\n    ]\n\n    Unregister the extension type:\n\n    >>> pa.unregister_extension_type(\"my_package.uuid\")\n    \"\"\"\n\n    def __cinit__(self):\n        if type(self) is ExtensionType:\n            raise TypeError(\"Can only instantiate subclasses of \"\n                            \"ExtensionType\")\n\n    def __init__(self, DataType storage_type, extension_name):\n        \"\"\"\n        Initialize an extension type instance.\n\n        This should be called at the end of the subclass'\n        ``__init__`` method.\n        \"\"\"\n        cdef:\n            shared_ptr[CExtensionType] cpy_ext_type\n            c_string c_extension_name\n\n        c_extension_name = tobytes(extension_name)\n\n        assert storage_type is not None\n        check_status(CPyExtensionType.FromClass(\n            storage_type.sp_type, c_extension_name, type(self),\n            &cpy_ext_type))\n        self.init(<shared_ptr[CDataType]> cpy_ext_type)\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        BaseExtensionType.init(self, type)\n        self.cpy_ext_type = <const CPyExtensionType*> type.get()\n        # Store weakref and serialized version of self on C++ type instance\n        check_status(self.cpy_ext_type.SetInstance(self))\n\n    def __eq__(self, other):\n        # Default implementation to avoid infinite recursion through\n        # DataType.__eq__ -> ExtensionType::ExtensionEquals -> DataType.__eq__\n        if isinstance(other, ExtensionType):\n            return (type(self) == type(other) and\n                    self.extension_name == other.extension_name and\n                    self.storage_type == other.storage_type)\n        else:\n            return NotImplemented\n\n    def __repr__(self):\n        fmt = '{0.__class__.__name__}({1})'\n        return fmt.format(self, repr(self.storage_type))\n\n    def __arrow_ext_serialize__(self):\n        \"\"\"\n        Serialized representation of metadata to reconstruct the type object.\n\n        This method should return a bytes object, and those serialized bytes\n        are stored in the custom metadata of the Field holding an extension\n        type in an IPC message.\n        The bytes are passed to ``__arrow_ext_deserialize`` and should hold\n        sufficient information to reconstruct the data type instance.\n        \"\"\"\n        return NotImplementedError\n\n    @classmethod\n    def __arrow_ext_deserialize__(self, storage_type, serialized):\n        \"\"\"\n        Return an extension type instance from the storage type and serialized\n        metadata.\n\n        This method should return an instance of the ExtensionType subclass\n        that matches the passed storage type and serialized metadata (the\n        return value of ``__arrow_ext_serialize__``).\n        \"\"\"\n        return NotImplementedError\n\n    def __reduce__(self):\n        return self.__arrow_ext_deserialize__, (self.storage_type, self.__arrow_ext_serialize__())\n\n    def __arrow_ext_class__(self):\n        \"\"\"Return an extension array class to be used for building or\n        deserializing arrays with this extension type.\n\n        This method should return a subclass of the ExtensionArray class. By\n        default, if not specialized in the extension implementation, an\n        extension type array will be a built-in ExtensionArray instance.\n        \"\"\"\n        return ExtensionArray\n\n    def __arrow_ext_scalar_class__(self):\n        \"\"\"Return an extension scalar class for building scalars with this\n        extension type.\n\n        This method should return subclass of the ExtensionScalar class. By\n        default, if not specialized in the extension implementation, an\n        extension type scalar will be a built-in ExtensionScalar instance.\n        \"\"\"\n        return ExtensionScalar\n\n\ncdef class FixedShapeTensorType(BaseExtensionType):\n    \"\"\"\n    Concrete class for fixed shape tensor extension type.\n\n    Examples\n    --------\n    Create an instance of fixed shape tensor extension type:\n\n    >>> import pyarrow as pa\n    >>> pa.fixed_shape_tensor(pa.int32(), [2, 2])\n    FixedShapeTensorType(extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>)\n\n    Create an instance of fixed shape tensor extension type with\n    permutation:\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int8(), (2, 2, 3),\n    ...                                     permutation=[0, 2, 1])\n    >>> tensor_type.permutation\n    [0, 2, 1]\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        BaseExtensionType.init(self, type)\n        self.tensor_ext_type = <const CFixedShapeTensorType*> type.get()\n\n    @property\n    def value_type(self):\n        \"\"\"\n        Data type of an individual tensor.\n        \"\"\"\n        return pyarrow_wrap_data_type(self.tensor_ext_type.value_type())\n\n    @property\n    def shape(self):\n        \"\"\"\n        Shape of the tensors.\n        \"\"\"\n        return self.tensor_ext_type.shape()\n\n    @property\n    def dim_names(self):\n        \"\"\"\n        Explicit names of the dimensions.\n        \"\"\"\n        list_of_bytes = self.tensor_ext_type.dim_names()\n        if len(list_of_bytes) != 0:\n            return [frombytes(x) for x in list_of_bytes]\n        else:\n            return None\n\n    @property\n    def permutation(self):\n        \"\"\"\n        Indices of the dimensions ordering.\n        \"\"\"\n        indices = self.tensor_ext_type.permutation()\n        if len(indices) != 0:\n            return indices\n        else:\n            return None\n\n    def __arrow_ext_serialize__(self):\n        \"\"\"\n        Serialized representation of metadata to reconstruct the type object.\n        \"\"\"\n        return self.tensor_ext_type.Serialize()\n\n    @classmethod\n    def __arrow_ext_deserialize__(self, storage_type, serialized):\n        \"\"\"\n        Return an FixedShapeTensor type instance from the storage type and serialized\n        metadata.\n        \"\"\"\n        return self.tensor_ext_type.Deserialize(storage_type, serialized)\n\n    def __arrow_ext_class__(self):\n        return FixedShapeTensorArray\n\n    def __reduce__(self):\n        return fixed_shape_tensor, (self.value_type, self.shape,\n                                    self.dim_names, self.permutation)\n\n\ncdef class PyExtensionType(ExtensionType):\n    \"\"\"\n    Concrete base class for Python-defined extension types based on pickle\n    for (de)serialization.\n\n    Parameters\n    ----------\n    storage_type : DataType\n        The storage type for which the extension is built.\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing PyExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.PyExtensionType):\n    ...     def __init__(self):\n    ...         pa.PyExtensionType.__init__(self, pa.binary(16))\n    ...     def __reduce__(self):\n    ...         return UuidType, ()\n    ...\n\n    Create an instance of UuidType extension type:\n\n    >>> uuid_type = UuidType() # doctest: +SKIP\n    >>> uuid_type # doctest: +SKIP\n    UuidType(FixedSizeBinaryType(fixed_size_binary[16]))\n\n    Inspect the extension type:\n\n    >>> uuid_type.extension_name # doctest: +SKIP\n    'arrow.py_extension_type'\n    >>> uuid_type.storage_type # doctest: +SKIP\n    FixedSizeBinaryType(fixed_size_binary[16])\n\n    Wrap an array as an extension array:\n\n    >>> import uuid\n    >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)],\n    ...                          pa.binary(16)) # doctest: +SKIP\n    >>> uuid_type.wrap_array(storage_array) # doctest: +SKIP\n    <pyarrow.lib.ExtensionArray object at ...>\n    [\n      ...\n    ]\n\n    Or do the same with creating an ExtensionArray:\n\n    >>> pa.ExtensionArray.from_storage(uuid_type,\n    ...                                storage_array) # doctest: +SKIP\n    <pyarrow.lib.ExtensionArray object at ...>\n    [\n      ...\n    ]\n    \"\"\"\n\n    def __cinit__(self):\n        if type(self) is PyExtensionType:\n            raise TypeError(\"Can only instantiate subclasses of \"\n                            \"PyExtensionType\")\n\n    def __init__(self, DataType storage_type):\n        ExtensionType.__init__(self, storage_type, \"arrow.py_extension_type\")\n\n    def __reduce__(self):\n        raise NotImplementedError(\"Please implement {0}.__reduce__\"\n                                  .format(type(self).__name__))\n\n    def __arrow_ext_serialize__(self):\n        return pickle.dumps(self)\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        try:\n            ty = pickle.loads(serialized)\n        except Exception:\n            # For some reason, it's impossible to deserialize the\n            # ExtensionType instance.  Perhaps the serialized data is\n            # corrupt, or more likely the type is being deserialized\n            # in an environment where the original Python class or module\n            # is not available.  Fall back on a generic BaseExtensionType.\n            return UnknownExtensionType(storage_type, serialized)\n\n        if ty.storage_type != storage_type:\n            raise TypeError(\"Expected storage type {0} but got {1}\"\n                            .format(ty.storage_type, storage_type))\n        return ty\n\n\ncdef class UnknownExtensionType(PyExtensionType):\n    \"\"\"\n    A concrete class for Python-defined extension types that refer to\n    an unknown Python implementation.\n\n    Parameters\n    ----------\n    storage_type : DataType\n        The storage type for which the extension is built.\n    serialized : bytes\n        The serialised output.\n    \"\"\"\n\n    cdef:\n        bytes serialized\n\n    def __init__(self, DataType storage_type, serialized):\n        self.serialized = serialized\n        PyExtensionType.__init__(self, storage_type)\n\n    def __arrow_ext_serialize__(self):\n        return self.serialized\n\n\n_python_extension_types_registry = []\n\n\ndef register_extension_type(ext_type):\n    \"\"\"\n    Register a Python extension type.\n\n    Registration is based on the extension name (so different registered types\n    need unique extension names). Registration needs an extension type\n    instance, but then works for any instance of the same subclass regardless\n    of parametrization of the type.\n\n    Parameters\n    ----------\n    ext_type : BaseExtensionType instance\n        The ExtensionType subclass to register.\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing ExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.ExtensionType):\n    ...    def __init__(self):\n    ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n    ...    def __arrow_ext_serialize__(self):\n    ...       # since we don't have a parameterized type, we don't need extra\n    ...       # metadata to be deserialized\n    ...       return b''\n    ...    @classmethod\n    ...    def __arrow_ext_deserialize__(self, storage_type, serialized):\n    ...       # return an instance of this subclass given the serialized\n    ...       # metadata.\n    ...       return UuidType()\n    ...\n\n    Register the extension type:\n\n    >>> pa.register_extension_type(UuidType())\n\n    Unregister the extension type:\n\n    >>> pa.unregister_extension_type(\"my_package.uuid\")\n    \"\"\"\n    cdef:\n        DataType _type = ensure_type(ext_type, allow_none=False)\n\n    if not isinstance(_type, BaseExtensionType):\n        raise TypeError(\"Only extension types can be registered\")\n\n    # register on the C++ side\n    check_status(\n        RegisterPyExtensionType(<shared_ptr[CDataType]> _type.sp_type))\n\n    # register on the python side\n    _python_extension_types_registry.append(_type)\n\n\ndef unregister_extension_type(type_name):\n    \"\"\"\n    Unregister a Python extension type.\n\n    Parameters\n    ----------\n    type_name : str\n        The name of the ExtensionType subclass to unregister.\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing ExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.ExtensionType):\n    ...    def __init__(self):\n    ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n    ...    def __arrow_ext_serialize__(self):\n    ...       # since we don't have a parameterized type, we don't need extra\n    ...       # metadata to be deserialized\n    ...       return b''\n    ...    @classmethod\n    ...    def __arrow_ext_deserialize__(self, storage_type, serialized):\n    ...       # return an instance of this subclass given the serialized\n    ...       # metadata.\n    ...       return UuidType()\n    ...\n\n    Register the extension type:\n\n    >>> pa.register_extension_type(UuidType())\n\n    Unregister the extension type:\n\n    >>> pa.unregister_extension_type(\"my_package.uuid\")\n    \"\"\"\n    cdef:\n        c_string c_type_name = tobytes(type_name)\n    check_status(UnregisterPyExtensionType(c_type_name))\n\n\ncdef class KeyValueMetadata(_Metadata, Mapping):\n    \"\"\"\n    KeyValueMetadata\n\n    Parameters\n    ----------\n    __arg0__ : dict\n        A dict of the key-value metadata\n    **kwargs : optional\n        additional key-value metadata\n    \"\"\"\n\n    def __init__(self, __arg0__=None, **kwargs):\n        cdef:\n            vector[c_string] keys, values\n            shared_ptr[const CKeyValueMetadata] result\n\n        items = []\n        if __arg0__ is not None:\n            other = (__arg0__.items() if isinstance(__arg0__, Mapping)\n                     else __arg0__)\n            items.extend((tobytes(k), v) for k, v in other)\n\n        prior_keys = {k for k, v in items}\n        for k, v in kwargs.items():\n            k = tobytes(k)\n            if k in prior_keys:\n                raise KeyError(\"Duplicate key {}, \"\n                               \"use pass all items as list of tuples if you \"\n                               \"intend to have duplicate keys\")\n            items.append((k, v))\n\n        keys.reserve(len(items))\n        for key, value in items:\n            keys.push_back(tobytes(key))\n            values.push_back(tobytes(value))\n        result.reset(new CKeyValueMetadata(move(keys), move(values)))\n        self.init(result)\n\n    cdef void init(self, const shared_ptr[const CKeyValueMetadata]& wrapped):\n        self.wrapped = wrapped\n        self.metadata = wrapped.get()\n\n    @staticmethod\n    cdef wrap(const shared_ptr[const CKeyValueMetadata]& sp):\n        cdef KeyValueMetadata self = KeyValueMetadata.__new__(KeyValueMetadata)\n        self.init(sp)\n        return self\n\n    cdef inline shared_ptr[const CKeyValueMetadata] unwrap(self) nogil:\n        return self.wrapped\n\n    def equals(self, KeyValueMetadata other):\n        \"\"\"\n        Parameters\n        ----------\n        other : pyarrow.KeyValueMetadata\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return self.metadata.Equals(deref(other.wrapped))\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        return frombytes(self.metadata.ToString(), safe=True)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            pass\n\n        if isinstance(other, Mapping):\n            try:\n                other = KeyValueMetadata(other)\n                return self.equals(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    def __len__(self):\n        return self.metadata.size()\n\n    def __contains__(self, key):\n        return self.metadata.Contains(tobytes(key))\n\n    def __getitem__(self, key):\n        return GetResultValue(self.metadata.Get(tobytes(key)))\n\n    def __iter__(self):\n        return self.keys()\n\n    def __reduce__(self):\n        return KeyValueMetadata, (list(self.items()),)\n\n    def key(self, i):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        byte\n        \"\"\"\n        return self.metadata.key(i)\n\n    def value(self, i):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        byte\n        \"\"\"\n        return self.metadata.value(i)\n\n    def keys(self):\n        for i in range(self.metadata.size()):\n            yield self.metadata.key(i)\n\n    def values(self):\n        for i in range(self.metadata.size()):\n            yield self.metadata.value(i)\n\n    def items(self):\n        for i in range(self.metadata.size()):\n            yield (self.metadata.key(i), self.metadata.value(i))\n\n    def get_all(self, key):\n        \"\"\"\n        Parameters\n        ----------\n        key : str\n\n        Returns\n        -------\n        list[byte]\n        \"\"\"\n        key = tobytes(key)\n        return [v for k, v in self.items() if k == key]\n\n    def to_dict(self):\n        \"\"\"\n        Convert KeyValueMetadata to dict. If a key occurs twice, the value for\n        the first one is returned\n        \"\"\"\n        cdef object key  # to force coercion to Python\n        result = ordered_dict()\n        for i in range(self.metadata.size()):\n            key = self.metadata.key(i)\n            if key not in result:\n                result[key] = self.metadata.value(i)\n        return result\n\n\ncpdef KeyValueMetadata ensure_metadata(object meta, c_bool allow_none=False):\n    if allow_none and meta is None:\n        return None\n    elif isinstance(meta, KeyValueMetadata):\n        return meta\n    else:\n        return KeyValueMetadata(meta)\n\n\ncdef class Field(_Weakrefable):\n    \"\"\"\n    A named field, with a data type, nullability, and optional metadata.\n\n    Notes\n    -----\n    Do not use this class's constructor directly; use pyarrow.field\n\n    Examples\n    --------\n    Create an instance of pyarrow.Field:\n\n    >>> import pyarrow as pa\n    >>> pa.field('key', pa.int32())\n    pyarrow.Field<key: int32>\n    >>> pa.field('key', pa.int32(), nullable=False)\n    pyarrow.Field<key: int32 not null>\n    >>> field = pa.field('key', pa.int32(),\n    ...                  metadata={\"key\": \"Something important\"})\n    >>> field\n    pyarrow.Field<key: int32>\n    >>> field.metadata\n    {b'key': b'Something important'}\n\n    Use the field to create a struct type:\n\n    >>> pa.struct([field])\n    StructType(struct<key: int32>)\n    \"\"\"\n\n    def __cinit__(self):\n        pass\n\n    def __init__(self):\n        raise TypeError(\"Do not call Field's constructor directly, use \"\n                        \"`pyarrow.field` instead.\")\n\n    cdef void init(self, const shared_ptr[CField]& field):\n        self.sp_field = field\n        self.field = field.get()\n        self.type = pyarrow_wrap_data_type(field.get().type())\n\n    def equals(self, Field other, bint check_metadata=False):\n        \"\"\"\n        Test if this field is equal to the other\n\n        Parameters\n        ----------\n        other : pyarrow.Field\n        check_metadata : bool, default False\n            Whether Field metadata equality should be checked as well.\n\n        Returns\n        -------\n        is_equal : bool\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> f1 = pa.field('key', pa.int32())\n        >>> f2 = pa.field('key', pa.int32(), nullable=False)\n        >>> f1.equals(f2)\n        False\n        >>> f1.equals(f1)\n        True\n        \"\"\"\n        return self.field.Equals(deref(other.field), check_metadata)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            return NotImplemented\n\n    def __reduce__(self):\n        return field, (self.name, self.type, self.nullable, self.metadata)\n\n    def __str__(self):\n        return 'pyarrow.Field<{0}>'.format(\n            frombytes(self.field.ToString(), safe=True))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __hash__(self):\n        return hash((self.field.name(), self.type, self.field.nullable()))\n\n    @property\n    def nullable(self):\n        \"\"\"\n        The field nullability.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> f1 = pa.field('key', pa.int32())\n        >>> f2 = pa.field('key', pa.int32(), nullable=False)\n        >>> f1.nullable\n        True\n        >>> f2.nullable\n        False\n        \"\"\"\n        return self.field.nullable()\n\n    @property\n    def name(self):\n        \"\"\"\n        The field name.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field.name\n        'key'\n        \"\"\"\n        return frombytes(self.field.name())\n\n    @property\n    def metadata(self):\n        \"\"\"\n        The field metadata.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32(),\n        ...                  metadata={\"key\": \"Something important\"})\n        >>> field.metadata\n        {b'key': b'Something important'}\n        \"\"\"\n        wrapped = pyarrow_wrap_metadata(self.field.metadata())\n        if wrapped is not None:\n            return wrapped.to_dict()\n        else:\n            return wrapped\n\n    def with_metadata(self, metadata):\n        \"\"\"\n        Add metadata as dict of string keys and values to Field\n\n        Parameters\n        ----------\n        metadata : dict\n            Keys and values must be string-like / coercible to bytes\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n\n        Create new field by adding metadata to existing one:\n\n        >>> field_new = field.with_metadata({\"key\": \"Something important\"})\n        >>> field_new\n        pyarrow.Field<key: int32>\n        >>> field_new.metadata\n        {b'key': b'Something important'}\n        \"\"\"\n        cdef shared_ptr[CField] c_field\n\n        meta = ensure_metadata(metadata, allow_none=False)\n        with nogil:\n            c_field = self.field.WithMetadata(meta.unwrap())\n\n        return pyarrow_wrap_field(c_field)\n\n    def remove_metadata(self):\n        \"\"\"\n        Create new field without metadata, if any\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32(),\n        ...                  metadata={\"key\": \"Something important\"})\n        >>> field.metadata\n        {b'key': b'Something important'}\n\n        Create new field by removing the metadata from the existing one:\n\n        >>> field_new = field.remove_metadata()\n        >>> field_new.metadata\n        \"\"\"\n        cdef shared_ptr[CField] new_field\n        with nogil:\n            new_field = self.field.RemoveMetadata()\n        return pyarrow_wrap_field(new_field)\n\n    def with_type(self, DataType new_type):\n        \"\"\"\n        A copy of this field with the replaced type\n\n        Parameters\n        ----------\n        new_type : pyarrow.DataType\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field\n        pyarrow.Field<key: int32>\n\n        Create new field by replacing type of an existing one:\n\n        >>> field_new = field.with_type(pa.int64())\n        >>> field_new\n        pyarrow.Field<key: int64>\n        \"\"\"\n        cdef:\n            shared_ptr[CField] c_field\n            shared_ptr[CDataType] c_datatype\n\n        c_datatype = pyarrow_unwrap_data_type(new_type)\n        with nogil:\n            c_field = self.field.WithType(c_datatype)\n\n        return pyarrow_wrap_field(c_field)\n\n    def with_name(self, name):\n        \"\"\"\n        A copy of this field with the replaced name\n\n        Parameters\n        ----------\n        name : str\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field\n        pyarrow.Field<key: int32>\n\n        Create new field by replacing the name of an existing one:\n\n        >>> field_new = field.with_name('lock')\n        >>> field_new\n        pyarrow.Field<lock: int32>\n        \"\"\"\n        cdef:\n            shared_ptr[CField] c_field\n\n        c_field = self.field.WithName(tobytes(name))\n\n        return pyarrow_wrap_field(c_field)\n\n    def with_nullable(self, nullable):\n        \"\"\"\n        A copy of this field with the replaced nullability\n\n        Parameters\n        ----------\n        nullable : bool\n\n        Returns\n        -------\n        field: pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field\n        pyarrow.Field<key: int32>\n        >>> field.nullable\n        True\n\n        Create new field by replacing the nullability of an existing one:\n\n        >>> field_new = field.with_nullable(False)\n        >>> field_new\n        pyarrow.Field<key: int32 not null>\n        >>> field_new.nullable\n        False\n        \"\"\"\n        cdef:\n            shared_ptr[CField] field\n            c_bool c_nullable\n\n        c_nullable = bool(nullable)\n        with nogil:\n            c_field = self.field.WithNullable(c_nullable)\n\n        return pyarrow_wrap_field(c_field)\n\n    def flatten(self):\n        \"\"\"\n        Flatten this field.  If a struct field, individual child fields\n        will be returned with their names prefixed by the parent's name.\n\n        Returns\n        -------\n        fields : List[pyarrow.Field]\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> f1 = pa.field('bar', pa.float64(), nullable=False)\n        >>> f2 = pa.field('foo', pa.int32()).with_metadata({\"key\": \"Something important\"})\n        >>> ff = pa.field('ff', pa.struct([f1, f2]), nullable=False)\n\n        Flatten a struct field:\n\n        >>> ff\n        pyarrow.Field<ff: struct<bar: double not null, foo: int32> not null>\n        >>> ff.flatten()\n        [pyarrow.Field<ff.bar: double not null>, pyarrow.Field<ff.foo: int32>]\n        \"\"\"\n        cdef vector[shared_ptr[CField]] flattened\n        with nogil:\n            flattened = self.field.Flatten()\n        return [pyarrow_wrap_field(f) for f in flattened]\n\n    def _export_to_c(self, out_ptr):\n        \"\"\"\n        Export to a C ArrowSchema struct, given its pointer.\n\n        Be careful: if you don't pass the ArrowSchema struct to a consumer,\n        its memory will leak.  This is a low-level function intended for\n        expert users.\n        \"\"\"\n        check_status(ExportField(deref(self.field),\n                                 <ArrowSchema*> _as_c_pointer(out_ptr)))\n\n    @staticmethod\n    def _import_from_c(in_ptr):\n        \"\"\"\n        Import Field from a C ArrowSchema struct, given its pointer.\n\n        This is a low-level function intended for expert users.\n        \"\"\"\n        cdef void* c_ptr = _as_c_pointer(in_ptr)\n        with nogil:\n            result = GetResultValue(ImportField(<ArrowSchema*> c_ptr))\n        return pyarrow_wrap_field(result)\n\n    def __arrow_c_schema__(self):\n        \"\"\"\n        Export to a ArrowSchema PyCapsule\n\n        Unlike _export_to_c, this will not leak memory if the capsule is not used.\n        \"\"\"\n        cdef ArrowSchema* c_schema\n        capsule = alloc_c_schema(&c_schema)\n\n        with nogil:\n            check_status(ExportField(deref(self.field), c_schema))\n\n        return capsule\n\n    @staticmethod\n    def _import_from_c_capsule(schema):\n        \"\"\"\n        Import a Field from a ArrowSchema PyCapsule\n\n        Parameters\n        ----------\n        schema : PyCapsule\n            A valid PyCapsule with name 'arrow_schema' containing an\n            ArrowSchema pointer.\n        \"\"\"\n        cdef:\n            ArrowSchema* c_schema\n            shared_ptr[CField] c_field\n\n        if not PyCapsule_IsValid(schema, 'arrow_schema'):\n            raise ValueError(\n                \"Not an ArrowSchema object\"\n            )\n        c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')\n\n        with nogil:\n            c_field = GetResultValue(ImportField(c_schema))\n\n        return pyarrow_wrap_field(c_field)\n\n\ncdef class Schema(_Weakrefable):\n    \"\"\"\n    A named collection of types a.k.a schema. A schema defines the\n    column names and types in a record batch or table data structure.\n    They also contain metadata about the columns. For example, schemas\n    converted from Pandas contain metadata about their original Pandas\n    types so they can be converted back to the same types.\n\n    Warnings\n    --------\n    Do not call this class's constructor directly. Instead use\n    :func:`pyarrow.schema` factory function which makes a new Arrow\n    Schema object.\n\n    Examples\n    --------\n    Create a new Arrow Schema object:\n\n    >>> import pyarrow as pa\n    >>> pa.schema([\n    ...     ('some_int', pa.int32()),\n    ...     ('some_string', pa.string())\n    ... ])\n    some_int: int32\n    some_string: string\n\n    Create Arrow Schema with metadata:\n\n    >>> pa.schema([\n    ...     pa.field('n_legs', pa.int64()),\n    ...     pa.field('animals', pa.string())],\n    ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n    n_legs: int64\n    animals: string\n    -- schema metadata --\n    n_legs: 'Number of legs per animal'\n    \"\"\"\n\n    def __cinit__(self):\n        pass\n\n    def __init__(self):\n        raise TypeError(\"Do not call Schema's constructor directly, use \"\n                        \"`pyarrow.schema` instead.\")\n\n    def __len__(self):\n        return self.schema.num_fields()\n\n    def __getitem__(self, key):\n        # access by integer index\n        return self._field(key)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    cdef void init(self, const vector[shared_ptr[CField]]& fields):\n        self.schema = new CSchema(fields)\n        self.sp_schema.reset(self.schema)\n\n    cdef void init_schema(self, const shared_ptr[CSchema]& schema):\n        self.schema = schema.get()\n        self.sp_schema = schema\n\n    def __reduce__(self):\n        return schema, (list(self), self.metadata)\n\n    def __hash__(self):\n        return hash((tuple(self), self.metadata))\n\n    def __sizeof__(self):\n        size = 0\n        if self.metadata:\n            for key, value in self.metadata.items():\n                size += sys.getsizeof(key)\n                size += sys.getsizeof(value)\n\n        return size + super(Schema, self).__sizeof__()\n\n    @property\n    def pandas_metadata(self):\n        \"\"\"\n        Return deserialized-from-JSON pandas metadata field (if it exists)\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> import pandas as pd\n        >>> df = pd.DataFrame({'n_legs': [2, 4, 5, 100],\n        ...                    'animals': [\"Flamingo\", \"Horse\", \"Brittle stars\", \"Centipede\"]})\n        >>> schema = pa.Table.from_pandas(df).schema\n\n        Select pandas metadata field from Arrow Schema:\n\n        >>> schema.pandas_metadata\n        {'index_columns': [{'kind': 'range', 'name': None, 'start': 0, 'stop': 4, 'step': 1}], ...\n        \"\"\"\n        metadata = self.metadata\n        key = b'pandas'\n        if metadata is None or key not in metadata:\n            return None\n\n        import json\n        return json.loads(metadata[key].decode('utf8'))\n\n    @property\n    def names(self):\n        \"\"\"\n        The schema's field names.\n\n        Returns\n        -------\n        list of str\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Get the names of the schema's fields:\n\n        >>> schema.names\n        ['n_legs', 'animals']\n        \"\"\"\n        cdef int i\n        result = []\n        for i in range(self.schema.num_fields()):\n            name = frombytes(self.schema.field(i).get().name())\n            result.append(name)\n        return result\n\n    @property\n    def types(self):\n        \"\"\"\n        The schema's field types.\n\n        Returns\n        -------\n        list of DataType\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Get the types of the schema's fields:\n\n        >>> schema.types\n        [DataType(int64), DataType(string)]\n        \"\"\"\n        return [field.type for field in self]\n\n    @property\n    def metadata(self):\n        \"\"\"\n        The schema's metadata.\n\n        Returns\n        -------\n        metadata: dict\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n\n        Get the metadata of the schema's fields:\n\n        >>> schema.metadata\n        {b'n_legs': b'Number of legs per animal'}\n        \"\"\"\n        wrapped = pyarrow_wrap_metadata(self.schema.metadata())\n        if wrapped is not None:\n            return wrapped.to_dict()\n        else:\n            return wrapped\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            return NotImplemented\n\n    def empty_table(self):\n        \"\"\"\n        Provide an empty table according to the schema.\n\n        Returns\n        -------\n        table: pyarrow.Table\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Create an empty table with schema's fields:\n\n        >>> schema.empty_table()\n        pyarrow.Table\n        n_legs: int64\n        animals: string\n        ----\n        n_legs: [[]]\n        animals: [[]]\n        \"\"\"\n        arrays = [_empty_array(field.type) for field in self]\n        return Table.from_arrays(arrays, schema=self)\n\n    def equals(self, Schema other not None, bint check_metadata=False):\n        \"\"\"\n        Test if this schema is equal to the other\n\n        Parameters\n        ----------\n        other :  pyarrow.Schema\n        check_metadata : bool, default False\n            Key/value metadata must be equal too\n\n        Returns\n        -------\n        is_equal : bool\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema1 = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n        >>> schema2 = pa.schema([\n        ...     ('some_int', pa.int32()),\n        ...     ('some_string', pa.string())\n        ... ])\n\n        Test two equal schemas:\n\n        >>> schema1.equals(schema1)\n        True\n\n        Test two unequal schemas:\n\n        >>> schema1.equals(schema2)\n        False\n        \"\"\"\n        return self.sp_schema.get().Equals(deref(other.schema),\n                                           check_metadata)\n\n    @classmethod\n    def from_pandas(cls, df, preserve_index=None):\n        \"\"\"\n        Returns implied schema from dataframe\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n        preserve_index : bool, default True\n            Whether to store the index as an additional column (or columns, for\n            MultiIndex) in the resulting `Table`.\n            The default of None will store the index as a column, except for\n            RangeIndex which is stored as metadata only. Use\n            ``preserve_index=True`` to force it to be stored as a column.\n\n        Returns\n        -------\n        pyarrow.Schema\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> import pyarrow as pa\n        >>> df = pd.DataFrame({\n        ...     'int': [1, 2],\n        ...     'str': ['a', 'b']\n        ... })\n\n        Create an Arrow Schema from the schema of a pandas dataframe:\n\n        >>> pa.Schema.from_pandas(df)\n        int: int64\n        str: string\n        -- schema metadata --\n        pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, ...\n        \"\"\"\n        from pyarrow.pandas_compat import dataframe_to_types\n        names, types, metadata = dataframe_to_types(\n            df,\n            preserve_index=preserve_index\n        )\n        fields = []\n        for name, type_ in zip(names, types):\n            fields.append(field(name, type_))\n        return schema(fields, metadata)\n\n    def field(self, i):\n        \"\"\"\n        Select a field by its column name or numeric index.\n\n        Parameters\n        ----------\n        i : int or string\n\n        Returns\n        -------\n        pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Select the second field:\n\n        >>> schema.field(1)\n        pyarrow.Field<animals: string>\n\n        Select the field of the column named 'n_legs':\n\n        >>> schema.field('n_legs')\n        pyarrow.Field<n_legs: int64>\n        \"\"\"\n        if isinstance(i, (bytes, str)):\n            field_index = self.get_field_index(i)\n            if field_index < 0:\n                raise KeyError(\"Column {} does not exist in schema\".format(i))\n            else:\n                return self._field(field_index)\n        elif isinstance(i, int):\n            return self._field(i)\n        else:\n            raise TypeError(\"Index must either be string or integer\")\n\n    def _field(self, int i):\n        \"\"\"\n        Select a field by its numeric index.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        pyarrow.Field\n        \"\"\"\n        cdef int index = <int> _normalize_index(i, self.schema.num_fields())\n        return pyarrow_wrap_field(self.schema.field(index))\n\n    def field_by_name(self, name):\n        \"\"\"\n        DEPRECATED\n\n        Parameters\n        ----------\n        name : str\n\n        Returns\n        -------\n        field: pyarrow.Field\n        \"\"\"\n        cdef:\n            vector[shared_ptr[CField]] results\n\n        warnings.warn(\n            \"The 'field_by_name' method is deprecated, use 'field' instead\",\n            FutureWarning, stacklevel=2)\n\n        results = self.schema.GetAllFieldsByName(tobytes(name))\n        if results.size() == 0:\n            return None\n        elif results.size() > 1:\n            warnings.warn(\"Schema field name corresponds to more \"\n                          \"than one field\", UserWarning)\n            return None\n        else:\n            return pyarrow_wrap_field(results[0])\n\n    def get_field_index(self, name):\n        \"\"\"\n        Return index of the unique field with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        index : int\n            The index of the field with the given name; -1 if the\n            name isn't found or there are several fields with the given\n            name.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Get the index of the field named 'animals':\n\n        >>> schema.get_field_index(\"animals\")\n        1\n\n        Index in case of several fields with the given name:\n\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string()),\n        ...     pa.field('animals', pa.bool_())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n        >>> schema.get_field_index(\"animals\")\n        -1\n        \"\"\"\n        return self.schema.GetFieldIndex(tobytes(name))\n\n    def get_all_field_indices(self, name):\n        \"\"\"\n        Return sorted list of indices for the fields with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        indices : List[int]\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string()),\n        ...     pa.field('animals', pa.bool_())])\n\n        Get the indexes of the fields named 'animals':\n\n        >>> schema.get_all_field_indices(\"animals\")\n        [1, 2]\n        \"\"\"\n        return self.schema.GetAllFieldIndices(tobytes(name))\n\n    def append(self, Field field):\n        \"\"\"\n        Append a field at the end of the schema.\n\n        In contrast to Python's ``list.append()`` it does return a new\n        object, leaving the original Schema unmodified.\n\n        Parameters\n        ----------\n        field : Field\n\n        Returns\n        -------\n        schema: Schema\n            New object with appended field.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Append a field 'extra' at the end of the schema:\n\n        >>> schema_new = schema.append(pa.field('extra', pa.bool_()))\n        >>> schema_new\n        n_legs: int64\n        animals: string\n        extra: bool\n\n        Original schema is unmodified:\n\n        >>> schema\n        n_legs: int64\n        animals: string\n        \"\"\"\n        return self.insert(self.schema.num_fields(), field)\n\n    def insert(self, int i, Field field):\n        \"\"\"\n        Add a field at position i to the schema.\n\n        Parameters\n        ----------\n        i : int\n        field : Field\n\n        Returns\n        -------\n        schema: Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Insert a new field on the second position:\n\n        >>> schema.insert(1, pa.field('extra', pa.bool_()))\n        n_legs: int64\n        extra: bool\n        animals: string\n        \"\"\"\n        cdef:\n            shared_ptr[CSchema] new_schema\n            shared_ptr[CField] c_field\n\n        c_field = field.sp_field\n\n        with nogil:\n            new_schema = GetResultValue(self.schema.AddField(i, c_field))\n\n        return pyarrow_wrap_schema(new_schema)\n\n    def remove(self, int i):\n        \"\"\"\n        Remove the field at index i from the schema.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        schema: Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Remove the second field of the schema:\n\n        >>> schema.remove(1)\n        n_legs: int64\n        \"\"\"\n        cdef shared_ptr[CSchema] new_schema\n\n        with nogil:\n            new_schema = GetResultValue(self.schema.RemoveField(i))\n\n        return pyarrow_wrap_schema(new_schema)\n\n    def set(self, int i, Field field):\n        \"\"\"\n        Replace a field at position i in the schema.\n\n        Parameters\n        ----------\n        i : int\n        field : Field\n\n        Returns\n        -------\n        schema: Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Replace the second field of the schema with a new field 'extra':\n\n        >>> schema.set(1, pa.field('replaced', pa.bool_()))\n        n_legs: int64\n        replaced: bool\n        \"\"\"\n        cdef:\n            shared_ptr[CSchema] new_schema\n            shared_ptr[CField] c_field\n\n        c_field = field.sp_field\n\n        with nogil:\n            new_schema = GetResultValue(self.schema.SetField(i, c_field))\n\n        return pyarrow_wrap_schema(new_schema)\n\n    def add_metadata(self, metadata):\n        \"\"\"\n        DEPRECATED\n\n        Parameters\n        ----------\n        metadata : dict\n            Keys and values must be string-like / coercible to bytes\n        \"\"\"\n        warnings.warn(\"The 'add_metadata' method is deprecated, use \"\n                      \"'with_metadata' instead\", FutureWarning, stacklevel=2)\n        return self.with_metadata(metadata)\n\n    def with_metadata(self, metadata):\n        \"\"\"\n        Add metadata as dict of string keys and values to Schema\n\n        Parameters\n        ----------\n        metadata : dict\n            Keys and values must be string-like / coercible to bytes\n\n        Returns\n        -------\n        schema : pyarrow.Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Add metadata to existing schema field:\n\n        >>> schema.with_metadata({\"n_legs\": \"Number of legs per animal\"})\n        n_legs: int64\n        animals: string\n        -- schema metadata --\n        n_legs: 'Number of legs per animal'\n        \"\"\"\n        cdef shared_ptr[CSchema] c_schema\n\n        meta = ensure_metadata(metadata, allow_none=False)\n        with nogil:\n            c_schema = self.schema.WithMetadata(meta.unwrap())\n\n        return pyarrow_wrap_schema(c_schema)\n\n    def serialize(self, memory_pool=None):\n        \"\"\"\n        Write Schema to Buffer as encapsulated IPC message\n\n        Parameters\n        ----------\n        memory_pool : MemoryPool, default None\n            Uses default memory pool if not specified\n\n        Returns\n        -------\n        serialized : Buffer\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Write schema to Buffer:\n\n        >>> schema.serialize()\n        <pyarrow.Buffer address=0x... size=... is_cpu=True is_mutable=True>\n        \"\"\"\n        cdef:\n            shared_ptr[CBuffer] buffer\n            CMemoryPool* pool = maybe_unbox_memory_pool(memory_pool)\n\n        with nogil:\n            buffer = GetResultValue(SerializeSchema(deref(self.schema),\n                                                    pool))\n        return pyarrow_wrap_buffer(buffer)\n\n    def remove_metadata(self):\n        \"\"\"\n        Create new schema without metadata, if any\n\n        Returns\n        -------\n        schema : pyarrow.Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n        >>> schema\n        n_legs: int64\n        animals: string\n        -- schema metadata --\n        n_legs: 'Number of legs per animal'\n\n        Create a new schema with removing the metadata from the original:\n\n        >>> schema.remove_metadata()\n        n_legs: int64\n        animals: string\n        \"\"\"\n        cdef shared_ptr[CSchema] new_schema\n        with nogil:\n            new_schema = self.schema.RemoveMetadata()\n        return pyarrow_wrap_schema(new_schema)\n\n    def to_string(self, truncate_metadata=True, show_field_metadata=True,\n                  show_schema_metadata=True):\n        \"\"\"\n        Return human-readable representation of Schema\n\n        Parameters\n        ----------\n        truncate_metadata : boolean, default True\n            Limit metadata key/value display to a single line of ~80 characters\n            or less\n        show_field_metadata : boolean, default True\n            Display Field-level KeyValueMetadata\n        show_schema_metadata : boolean, default True\n            Display Schema-level KeyValueMetadata\n\n        Returns\n        -------\n        str : the formatted output\n        \"\"\"\n        cdef:\n            c_string result\n            PrettyPrintOptions options = PrettyPrintOptions.Defaults()\n\n        options.indent = 0\n        options.truncate_metadata = truncate_metadata\n        options.show_field_metadata = show_field_metadata\n        options.show_schema_metadata = show_schema_metadata\n\n        with nogil:\n            check_status(\n                PrettyPrint(\n                    deref(self.schema),\n                    options,\n                    &result\n                )\n            )\n\n        return frombytes(result, safe=True)\n\n    def _export_to_c(self, out_ptr):\n        \"\"\"\n        Export to a C ArrowSchema struct, given its pointer.\n\n        Be careful: if you don't pass the ArrowSchema struct to a consumer,\n        its memory will leak.  This is a low-level function intended for\n        expert users.\n        \"\"\"\n        check_status(ExportSchema(deref(self.schema),\n                                  <ArrowSchema*> _as_c_pointer(out_ptr)))\n\n    @staticmethod\n    def _import_from_c(in_ptr):\n        \"\"\"\n        Import Schema from a C ArrowSchema struct, given its pointer.\n\n        This is a low-level function intended for expert users.\n        \"\"\"\n        cdef void* c_ptr = _as_c_pointer(in_ptr)\n        with nogil:\n            result = GetResultValue(ImportSchema(<ArrowSchema*> c_ptr))\n        return pyarrow_wrap_schema(result)\n\n    def __str__(self):\n        return self.to_string()\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __arrow_c_schema__(self):\n        \"\"\"\n        Export to a ArrowSchema PyCapsule\n\n        Unlike _export_to_c, this will not leak memory if the capsule is not used.\n        \"\"\"\n        cdef ArrowSchema* c_schema\n        capsule = alloc_c_schema(&c_schema)\n\n        with nogil:\n            check_status(ExportSchema(deref(self.schema), c_schema))\n\n        return capsule\n\n    @staticmethod\n    def _import_from_c_capsule(schema):\n        \"\"\"\n        Import a Schema from a ArrowSchema PyCapsule\n\n        Parameters\n        ----------\n        schema : PyCapsule\n            A valid PyCapsule with name 'arrow_schema' containing an\n            ArrowSchema pointer.\n        \"\"\"\n        cdef:\n            ArrowSchema* c_schema\n\n        if not PyCapsule_IsValid(schema, 'arrow_schema'):\n            raise ValueError(\n                \"Not an ArrowSchema object\"\n            )\n        c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')\n\n        with nogil:\n            result = GetResultValue(ImportSchema(c_schema))\n\n        return pyarrow_wrap_schema(result)\n\n\ndef unify_schemas(schemas, *, promote_options=\"default\"):\n    \"\"\"\n    Unify schemas by merging fields by name.\n\n    The resulting schema will contain the union of fields from all schemas.\n    Fields with the same name will be merged. Note that two fields with\n    different types will fail merging by default.\n\n    - The unified field will inherit the metadata from the schema where\n        that field is first defined.\n    - The first N fields in the schema will be ordered the same as the\n        N fields in the first schema.\n\n    The resulting schema will inherit its metadata from the first input\n    schema.\n\n    Parameters\n    ----------\n    schemas : list of Schema\n        Schemas to merge into a single one.\n    promote_options : str, default default\n        Accepts strings \"default\" and \"permissive\".\n        Default: null and only null can be unified with another type.\n        Permissive: types are promoted to the greater common denominator.\n\n    Returns\n    -------\n    Schema\n\n    Raises\n    ------\n    ArrowInvalid :\n        If any input schema contains fields with duplicate names.\n        If Fields of the same name are not mergeable.\n    \"\"\"\n    cdef:\n        Schema schema\n        CField.CMergeOptions c_options\n        vector[shared_ptr[CSchema]] c_schemas\n    for schema in schemas:\n        if not isinstance(schema, Schema):\n            raise TypeError(\"Expected Schema, got {}\".format(type(schema)))\n        c_schemas.push_back(pyarrow_unwrap_schema(schema))\n\n    if promote_options == \"default\":\n        c_options = CField.CMergeOptions.Defaults()\n    elif promote_options == \"permissive\":\n        c_options = CField.CMergeOptions.Permissive()\n    else:\n        raise ValueError(f\"Invalid merge mode: {promote_options}\")\n\n    return pyarrow_wrap_schema(\n        GetResultValue(UnifySchemas(c_schemas, c_options)))\n\n\ncdef dict _type_cache = {}\n\n\ncdef DataType primitive_type(Type type):\n    if type in _type_cache:\n        return _type_cache[type]\n\n    cdef DataType out = DataType.__new__(DataType)\n    out.init(GetPrimitiveType(type))\n\n    _type_cache[type] = out\n    return out\n\n\n# -----------------------------------------------------------\n# Type factory functions\n\n\ndef field(name, type, bint nullable=True, metadata=None):\n    \"\"\"\n    Create a pyarrow.Field instance.\n\n    Parameters\n    ----------\n    name : str or bytes\n        Name of the field.\n    type : pyarrow.DataType\n        Arrow datatype of the field.\n    nullable : bool, default True\n        Whether the field's values are nullable.\n    metadata : dict, default None\n        Optional field metadata, the keys and values must be coercible to\n        bytes.\n\n    Returns\n    -------\n    field : pyarrow.Field\n\n    Examples\n    --------\n    Create an instance of pyarrow.Field:\n\n    >>> import pyarrow as pa\n    >>> pa.field('key', pa.int32())\n    pyarrow.Field<key: int32>\n    >>> pa.field('key', pa.int32(), nullable=False)\n    pyarrow.Field<key: int32 not null>\n\n    >>> field = pa.field('key', pa.int32(),\n    ...                  metadata={\"key\": \"Something important\"})\n    >>> field\n    pyarrow.Field<key: int32>\n    >>> field.metadata\n    {b'key': b'Something important'}\n\n    Use the field to create a struct type:\n\n    >>> pa.struct([field])\n    StructType(struct<key: int32>)\n    \"\"\"\n    cdef:\n        Field result = Field.__new__(Field)\n        DataType _type = ensure_type(type, allow_none=False)\n        shared_ptr[const CKeyValueMetadata] c_meta\n\n    metadata = ensure_metadata(metadata, allow_none=True)\n    c_meta = pyarrow_unwrap_metadata(metadata)\n\n    if _type.type.id() == _Type_NA and not nullable:\n        raise ValueError(\"A null type field may not be non-nullable\")\n\n    result.sp_field.reset(\n        new CField(tobytes(name), _type.sp_type, nullable, c_meta)\n    )\n    result.field = result.sp_field.get()\n    result.type = _type\n\n    return result\n\n\ncdef set PRIMITIVE_TYPES = set([\n    _Type_NA, _Type_BOOL,\n    _Type_UINT8, _Type_INT8,\n    _Type_UINT16, _Type_INT16,\n    _Type_UINT32, _Type_INT32,\n    _Type_UINT64, _Type_INT64,\n    _Type_TIMESTAMP, _Type_DATE32,\n    _Type_TIME32, _Type_TIME64,\n    _Type_DATE64,\n    _Type_HALF_FLOAT,\n    _Type_FLOAT,\n    _Type_DOUBLE])\n\n\ndef null():\n    \"\"\"\n    Create instance of null type.\n\n    Examples\n    --------\n    Create an instance of a null type:\n\n    >>> import pyarrow as pa\n    >>> pa.null()\n    DataType(null)\n    >>> print(pa.null())\n    null\n\n    Create a ``Field`` type with a null type and a name:\n\n    >>> pa.field('null_field', pa.null())\n    pyarrow.Field<null_field: null>\n    \"\"\"\n    return primitive_type(_Type_NA)\n\n\ndef bool_():\n    \"\"\"\n    Create instance of boolean type.\n\n    Examples\n    --------\n    Create an instance of a boolean type:\n\n    >>> import pyarrow as pa\n    >>> pa.bool_()\n    DataType(bool)\n    >>> print(pa.bool_())\n    bool\n\n    Create a ``Field`` type with a boolean type\n    and a name:\n\n    >>> pa.field('bool_field', pa.bool_())\n    pyarrow.Field<bool_field: bool>\n    \"\"\"\n    return primitive_type(_Type_BOOL)\n\n\ndef uint8():\n    \"\"\"\n    Create instance of unsigned int8 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int8 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint8()\n    DataType(uint8)\n    >>> print(pa.uint8())\n    uint8\n\n    Create an array with unsigned int8 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint8())\n    <pyarrow.lib.UInt8Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT8)\n\n\ndef int8():\n    \"\"\"\n    Create instance of signed int8 type.\n\n    Examples\n    --------\n    Create an instance of int8 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int8()\n    DataType(int8)\n    >>> print(pa.int8())\n    int8\n\n    Create an array with int8 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int8())\n    <pyarrow.lib.Int8Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT8)\n\n\ndef uint16():\n    \"\"\"\n    Create instance of unsigned uint16 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int16 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint16()\n    DataType(uint16)\n    >>> print(pa.uint16())\n    uint16\n\n    Create an array with unsigned int16 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint16())\n    <pyarrow.lib.UInt16Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT16)\n\n\ndef int16():\n    \"\"\"\n    Create instance of signed int16 type.\n\n    Examples\n    --------\n    Create an instance of int16 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int16()\n    DataType(int16)\n    >>> print(pa.int16())\n    int16\n\n    Create an array with int16 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int16())\n    <pyarrow.lib.Int16Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT16)\n\n\ndef uint32():\n    \"\"\"\n    Create instance of unsigned uint32 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint32()\n    DataType(uint32)\n    >>> print(pa.uint32())\n    uint32\n\n    Create an array with unsigned int32 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint32())\n    <pyarrow.lib.UInt32Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT32)\n\n\ndef int32():\n    \"\"\"\n    Create instance of signed int32 type.\n\n    Examples\n    --------\n    Create an instance of int32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int32()\n    DataType(int32)\n    >>> print(pa.int32())\n    int32\n\n    Create an array with int32 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int32())\n    <pyarrow.lib.Int32Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT32)\n\n\ndef uint64():\n    \"\"\"\n    Create instance of unsigned uint64 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint64()\n    DataType(uint64)\n    >>> print(pa.uint64())\n    uint64\n\n    Create an array with unsigned uint64 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint64())\n    <pyarrow.lib.UInt64Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT64)\n\n\ndef int64():\n    \"\"\"\n    Create instance of signed int64 type.\n\n    Examples\n    --------\n    Create an instance of int64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int64()\n    DataType(int64)\n    >>> print(pa.int64())\n    int64\n\n    Create an array with int64 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int64())\n    <pyarrow.lib.Int64Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT64)\n\n\ncdef dict _timestamp_type_cache = {}\ncdef dict _time_type_cache = {}\ncdef dict _duration_type_cache = {}\n\n\ncdef timeunit_to_string(TimeUnit unit):\n    if unit == TimeUnit_SECOND:\n        return 's'\n    elif unit == TimeUnit_MILLI:\n        return 'ms'\n    elif unit == TimeUnit_MICRO:\n        return 'us'\n    elif unit == TimeUnit_NANO:\n        return 'ns'\n\n\ncdef TimeUnit string_to_timeunit(unit) except *:\n    if unit == 's':\n        return TimeUnit_SECOND\n    elif unit == 'ms':\n        return TimeUnit_MILLI\n    elif unit == 'us':\n        return TimeUnit_MICRO\n    elif unit == 'ns':\n        return TimeUnit_NANO\n    else:\n        raise ValueError(f\"Invalid time unit: {unit!r}\")\n\n\ndef tzinfo_to_string(tz):\n    \"\"\"\n    Converts a time zone object into a string indicating the name of a time\n    zone, one of:\n    * As used in the Olson time zone database (the \"tz database\" or\n      \"tzdata\"), such as \"America/New_York\"\n    * An absolute time zone offset of the form +XX:XX or -XX:XX, such as +07:30\n\n    Parameters\n    ----------\n      tz : datetime.tzinfo\n        Time zone object\n\n    Returns\n    -------\n      name : str\n        Time zone name\n    \"\"\"\n    return frombytes(GetResultValue(TzinfoToString(<PyObject*>tz)))\n\n\ndef string_to_tzinfo(name):\n    \"\"\"\n    Convert a time zone name into a time zone object.\n\n    Supported input strings are:\n    * As used in the Olson time zone database (the \"tz database\" or\n      \"tzdata\"), such as \"America/New_York\"\n    * An absolute time zone offset of the form +XX:XX or -XX:XX, such as +07:30\n\n    Parameters\n    ----------\n      name: str\n        Time zone name.\n\n    Returns\n    -------\n      tz : datetime.tzinfo\n        Time zone object\n    \"\"\"\n    cdef PyObject* tz = GetResultValue(StringToTzinfo(name.encode('utf-8')))\n    return PyObject_to_object(tz)\n\n\ndef timestamp(unit, tz=None):\n    \"\"\"\n    Create instance of timestamp type with resolution and optional time zone.\n\n    Parameters\n    ----------\n    unit : str\n        one of 's' [second], 'ms' [millisecond], 'us' [microsecond], or 'ns'\n        [nanosecond]\n    tz : str, default None\n        Time zone name. None indicates time zone naive\n\n    Examples\n    --------\n    Create an instance of timestamp type:\n\n    >>> import pyarrow as pa\n    >>> pa.timestamp('us')\n    TimestampType(timestamp[us])\n    >>> pa.timestamp('s', tz='America/New_York')\n    TimestampType(timestamp[s, tz=America/New_York])\n    >>> pa.timestamp('s', tz='+07:30')\n    TimestampType(timestamp[s, tz=+07:30])\n\n    Use timestamp type when creating a scalar object:\n\n    >>> from datetime import datetime\n    >>> pa.scalar(datetime(2012, 1, 1), type=pa.timestamp('s', tz='UTC'))\n    <pyarrow.TimestampScalar: '2012-01-01T00:00:00+0000'>\n    >>> pa.scalar(datetime(2012, 1, 1), type=pa.timestamp('us'))\n    <pyarrow.TimestampScalar: '2012-01-01T00:00:00.000000'>\n\n    Returns\n    -------\n    timestamp_type : TimestampType\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n        c_string c_timezone\n\n    unit_code = string_to_timeunit(unit)\n\n    cdef TimestampType out = TimestampType.__new__(TimestampType)\n\n    if tz is None:\n        out.init(ctimestamp(unit_code))\n        if unit_code in _timestamp_type_cache:\n            return _timestamp_type_cache[unit_code]\n        _timestamp_type_cache[unit_code] = out\n    else:\n        if not isinstance(tz, (bytes, str)):\n            tz = tzinfo_to_string(tz)\n\n        c_timezone = tobytes(tz)\n        out.init(ctimestamp(unit_code, c_timezone))\n\n    return out\n\n\ndef time32(unit):\n    \"\"\"\n    Create instance of 32-bit time (time of day) type with unit resolution.\n\n    Parameters\n    ----------\n    unit : str\n        one of 's' [second], or 'ms' [millisecond]\n\n    Returns\n    -------\n    type : pyarrow.Time32Type\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> pa.time32('s')\n    Time32Type(time32[s])\n    >>> pa.time32('ms')\n    Time32Type(time32[ms])\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n        c_string c_timezone\n\n    if unit == 's':\n        unit_code = TimeUnit_SECOND\n    elif unit == 'ms':\n        unit_code = TimeUnit_MILLI\n    else:\n        raise ValueError(f\"Invalid time unit for time32: {unit!r}\")\n\n    if unit_code in _time_type_cache:\n        return _time_type_cache[unit_code]\n\n    cdef Time32Type out = Time32Type.__new__(Time32Type)\n\n    out.init(ctime32(unit_code))\n    _time_type_cache[unit_code] = out\n\n    return out\n\n\ndef time64(unit):\n    \"\"\"\n    Create instance of 64-bit time (time of day) type with unit resolution.\n\n    Parameters\n    ----------\n    unit : str\n        One of 'us' [microsecond], or 'ns' [nanosecond].\n\n    Returns\n    -------\n    type : pyarrow.Time64Type\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> pa.time64('us')\n    Time64Type(time64[us])\n    >>> pa.time64('ns')\n    Time64Type(time64[ns])\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n        c_string c_timezone\n\n    if unit == 'us':\n        unit_code = TimeUnit_MICRO\n    elif unit == 'ns':\n        unit_code = TimeUnit_NANO\n    else:\n        raise ValueError(f\"Invalid time unit for time64: {unit!r}\")\n\n    if unit_code in _time_type_cache:\n        return _time_type_cache[unit_code]\n\n    cdef Time64Type out = Time64Type.__new__(Time64Type)\n\n    out.init(ctime64(unit_code))\n    _time_type_cache[unit_code] = out\n\n    return out\n\n\ndef duration(unit):\n    \"\"\"\n    Create instance of a duration type with unit resolution.\n\n    Parameters\n    ----------\n    unit : str\n        One of 's' [second], 'ms' [millisecond], 'us' [microsecond], or\n        'ns' [nanosecond].\n\n    Returns\n    -------\n    type : pyarrow.DurationType\n\n    Examples\n    --------\n    Create an instance of duration type:\n\n    >>> import pyarrow as pa\n    >>> pa.duration('us')\n    DurationType(duration[us])\n    >>> pa.duration('s')\n    DurationType(duration[s])\n\n    Create an array with duration type:\n\n    >>> pa.array([0, 1, 2], type=pa.duration('s'))\n    <pyarrow.lib.DurationArray object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n\n    unit_code = string_to_timeunit(unit)\n\n    if unit_code in _duration_type_cache:\n        return _duration_type_cache[unit_code]\n\n    cdef DurationType out = DurationType.__new__(DurationType)\n\n    out.init(cduration(unit_code))\n    _duration_type_cache[unit_code] = out\n\n    return out\n\n\ndef month_day_nano_interval():\n    \"\"\"\n    Create instance of an interval type representing months, days and\n    nanoseconds between two dates.\n\n    Examples\n    --------\n    Create an instance of an month_day_nano_interval type:\n\n    >>> import pyarrow as pa\n    >>> pa.month_day_nano_interval()\n    DataType(month_day_nano_interval)\n\n    Create a scalar with month_day_nano_interval type:\n\n    >>> pa.scalar((1, 15, -30), type=pa.month_day_nano_interval())\n    <pyarrow.MonthDayNanoIntervalScalar: MonthDayNano(months=1, days=15, nanoseconds=-30)>\n    \"\"\"\n    return primitive_type(_Type_INTERVAL_MONTH_DAY_NANO)\n\n\ndef date32():\n    \"\"\"\n    Create instance of 32-bit date (days since UNIX epoch 1970-01-01).\n\n    Examples\n    --------\n    Create an instance of 32-bit date type:\n\n    >>> import pyarrow as pa\n    >>> pa.date32()\n    DataType(date32[day])\n\n    Create a scalar with 32-bit date type:\n\n    >>> from datetime import date\n    >>> pa.scalar(date(2012, 1, 1), type=pa.date32())\n    <pyarrow.Date32Scalar: datetime.date(2012, 1, 1)>\n    \"\"\"\n    return primitive_type(_Type_DATE32)\n\n\ndef date64():\n    \"\"\"\n    Create instance of 64-bit date (milliseconds since UNIX epoch 1970-01-01).\n\n    Examples\n    --------\n    Create an instance of 64-bit date type:\n\n    >>> import pyarrow as pa\n    >>> pa.date64()\n    DataType(date64[ms])\n\n    Create a scalar with 64-bit date type:\n\n    >>> from datetime import datetime\n    >>> pa.scalar(datetime(2012, 1, 1), type=pa.date64())\n    <pyarrow.Date64Scalar: datetime.date(2012, 1, 1)>\n    \"\"\"\n    return primitive_type(_Type_DATE64)\n\n\ndef float16():\n    \"\"\"\n    Create half-precision floating point type.\n\n    Examples\n    --------\n    Create an instance of float16 type:\n\n    >>> import pyarrow as pa\n    >>> pa.float16()\n    DataType(halffloat)\n    >>> print(pa.float16())\n    halffloat\n\n    Create an array with float16 type:\n\n    >>> arr = np.array([1.5, np.nan], dtype=np.float16)\n    >>> a = pa.array(arr, type=pa.float16())\n    >>> a\n    <pyarrow.lib.HalfFloatArray object at ...>\n    [\n      15872,\n      32256\n    ]\n    >>> a.to_pylist()\n    [1.5, nan]\n    \"\"\"\n    return primitive_type(_Type_HALF_FLOAT)\n\n\ndef float32():\n    \"\"\"\n    Create single-precision floating point type.\n\n    Examples\n    --------\n    Create an instance of float32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.float32()\n    DataType(float)\n    >>> print(pa.float32())\n    float\n\n    Create an array with float32 type:\n\n    >>> pa.array([0.0, 1.0, 2.0], type=pa.float32())\n    <pyarrow.lib.FloatArray object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_FLOAT)\n\n\ndef float64():\n    \"\"\"\n    Create double-precision floating point type.\n\n    Examples\n    --------\n    Create an instance of float64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.float64()\n    DataType(double)\n    >>> print(pa.float64())\n    double\n\n    Create an array with float64 type:\n\n    >>> pa.array([0.0, 1.0, 2.0], type=pa.float64())\n    <pyarrow.lib.DoubleArray object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_DOUBLE)\n\n\ncpdef DataType decimal128(int precision, int scale=0):\n    \"\"\"\n    Create decimal type with precision and scale and 128-bit width.\n\n    Arrow decimals are fixed-point decimal numbers encoded as a scaled\n    integer.  The precision is the number of significant digits that the\n    decimal type can represent; the scale is the number of digits after\n    the decimal point (note the scale can be negative).\n\n    As an example, ``decimal128(7, 3)`` can exactly represent the numbers\n    1234.567 and -1234.567 (encoded internally as the 128-bit integers\n    1234567 and -1234567, respectively), but neither 12345.67 nor 123.4567.\n\n    ``decimal128(5, -3)`` can exactly represent the number 12345000\n    (encoded internally as the 128-bit integer 12345), but neither\n    123450000 nor 1234500.\n\n    If you need a precision higher than 38 significant digits, consider\n    using ``decimal256``.\n\n    Parameters\n    ----------\n    precision : int\n        Must be between 1 and 38\n    scale : int\n\n    Returns\n    -------\n    decimal_type : Decimal128Type\n\n    Examples\n    --------\n    Create an instance of decimal type:\n\n    >>> import pyarrow as pa\n    >>> pa.decimal128(5, 2)\n    Decimal128Type(decimal128(5, 2))\n\n    Create an array with decimal type:\n\n    >>> import decimal\n    >>> a = decimal.Decimal('123.45')\n    >>> pa.array([a], pa.decimal128(5, 2))\n    <pyarrow.lib.Decimal128Array object at ...>\n    [\n      123.45\n    ]\n    \"\"\"\n    cdef shared_ptr[CDataType] decimal_type\n    if precision < 1 or precision > 38:\n        raise ValueError(\"precision should be between 1 and 38\")\n    decimal_type.reset(new CDecimal128Type(precision, scale))\n    return pyarrow_wrap_data_type(decimal_type)\n\n\ncpdef DataType decimal256(int precision, int scale=0):\n    \"\"\"\n    Create decimal type with precision and scale and 256-bit width.\n\n    Arrow decimals are fixed-point decimal numbers encoded as a scaled\n    integer.  The precision is the number of significant digits that the\n    decimal type can represent; the scale is the number of digits after\n    the decimal point (note the scale can be negative).\n\n    For most use cases, the maximum precision offered by ``decimal128``\n    is sufficient, and it will result in a more compact and more efficient\n    encoding.  ``decimal256`` is useful if you need a precision higher\n    than 38 significant digits.\n\n    Parameters\n    ----------\n    precision : int\n        Must be between 1 and 76\n    scale : int\n\n    Returns\n    -------\n    decimal_type : Decimal256Type\n    \"\"\"\n    cdef shared_ptr[CDataType] decimal_type\n    if precision < 1 or precision > 76:\n        raise ValueError(\"precision should be between 1 and 76\")\n    decimal_type.reset(new CDecimal256Type(precision, scale))\n    return pyarrow_wrap_data_type(decimal_type)\n\n\ndef string():\n    \"\"\"\n    Create UTF8 variable-length string type.\n\n    Examples\n    --------\n    Create an instance of a string type:\n\n    >>> import pyarrow as pa\n    >>> pa.string()\n    DataType(string)\n\n    and use the string type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.string())\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      \"baz\"\n    ]\n    \"\"\"\n    return primitive_type(_Type_STRING)\n\n\ndef utf8():\n    \"\"\"\n    Alias for string().\n\n    Examples\n    --------\n    Create an instance of a string type:\n\n    >>> import pyarrow as pa\n    >>> pa.utf8()\n    DataType(string)\n\n    and use the string type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.utf8())\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      \"baz\"\n    ]\n    \"\"\"\n    return string()\n\n\ndef binary(int length=-1):\n    \"\"\"\n    Create variable-length or fixed size binary type.\n\n    Parameters\n    ----------\n    length : int, optional, default -1\n        If length == -1 then return a variable length binary type. If length is\n        greater than or equal to 0 then return a fixed size binary type of\n        width `length`.\n\n    Examples\n    --------\n    Create an instance of a variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.binary()\n    DataType(binary)\n\n    and use the variable-length binary type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.binary())\n    <pyarrow.lib.BinaryArray object at ...>\n    [\n      666F6F,\n      626172,\n      62617A\n    ]\n\n    Create an instance of a fixed-size binary type:\n\n    >>> pa.binary(3)\n    FixedSizeBinaryType(fixed_size_binary[3])\n\n    and use the fixed-length binary type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.binary(3))\n    <pyarrow.lib.FixedSizeBinaryArray object at ...>\n    [\n      666F6F,\n      626172,\n      62617A\n    ]\n    \"\"\"\n    if length == -1:\n        return primitive_type(_Type_BINARY)\n\n    cdef shared_ptr[CDataType] fixed_size_binary_type\n    fixed_size_binary_type.reset(new CFixedSizeBinaryType(length))\n    return pyarrow_wrap_data_type(fixed_size_binary_type)\n\n\ndef large_binary():\n    \"\"\"\n    Create large variable-length binary type.\n\n    This data type may not be supported by all Arrow implementations.  Unless\n    you need to represent data larger than 2GB, you should prefer binary().\n\n    Examples\n    --------\n    Create an instance of large variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.large_binary()\n    DataType(large_binary)\n\n    and use the type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.large_binary())\n    <pyarrow.lib.LargeBinaryArray object at ...>\n    [\n      666F6F,\n      626172,\n      62617A\n    ]\n    \"\"\"\n    return primitive_type(_Type_LARGE_BINARY)\n\n\ndef large_string():\n    \"\"\"\n    Create large UTF8 variable-length string type.\n\n    This data type may not be supported by all Arrow implementations.  Unless\n    you need to represent data larger than 2GB, you should prefer string().\n\n    Examples\n    --------\n    Create an instance of large UTF8 variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.large_string()\n    DataType(large_string)\n\n    and use the type to create an array:\n\n    >>> pa.array(['foo', 'bar'] * 50, type=pa.large_string())\n    <pyarrow.lib.LargeStringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      ...\n      \"foo\",\n      \"bar\"\n    ]\n    \"\"\"\n    return primitive_type(_Type_LARGE_STRING)\n\n\ndef large_utf8():\n    \"\"\"\n    Alias for large_string().\n\n    Examples\n    --------\n    Create an instance of large UTF8 variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.large_utf8()\n    DataType(large_string)\n\n    and use the type to create an array:\n\n    >>> pa.array(['foo', 'bar'] * 50, type=pa.large_utf8())\n    <pyarrow.lib.LargeStringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      ...\n      \"foo\",\n      \"bar\"\n    ]\n    \"\"\"\n    return large_string()\n\n\ndef list_(value_type, int list_size=-1):\n    \"\"\"\n    Create ListType instance from child data type or field.\n\n    Parameters\n    ----------\n    value_type : DataType or Field\n    list_size : int, optional, default -1\n        If length == -1 then return a variable length list type. If length is\n        greater than or equal to 0 then return a fixed size list type.\n\n    Returns\n    -------\n    list_type : DataType\n\n    Examples\n    --------\n    Create an instance of ListType:\n\n    >>> import pyarrow as pa\n    >>> pa.list_(pa.string())\n    ListType(list<item: string>)\n    >>> pa.list_(pa.int32(), 2)\n    FixedSizeListType(fixed_size_list<item: int32>[2])\n\n    Use the ListType to create a scalar:\n\n    >>> pa.scalar(['foo', None], type=pa.list_(pa.string(), 2))\n    <pyarrow.FixedSizeListScalar: ['foo', None]>\n\n    or an array:\n\n    >>> pa.array([[1, 2], [3, 4]], pa.list_(pa.int32(), 2))\n    <pyarrow.lib.FixedSizeListArray object at ...>\n    [\n      [\n        1,\n        2\n      ],\n      [\n        3,\n        4\n      ]\n    ]\n    \"\"\"\n    cdef:\n        Field _field\n        shared_ptr[CDataType] list_type\n\n    if isinstance(value_type, DataType):\n        _field = field('item', value_type)\n    elif isinstance(value_type, Field):\n        _field = value_type\n    else:\n        raise TypeError('List requires DataType or Field')\n\n    if list_size == -1:\n        list_type.reset(new CListType(_field.sp_field))\n    else:\n        if list_size < 0:\n            raise ValueError(\"list_size should be a positive integer\")\n        list_type.reset(new CFixedSizeListType(_field.sp_field, list_size))\n\n    return pyarrow_wrap_data_type(list_type)\n\n\ncpdef LargeListType large_list(value_type):\n    \"\"\"\n    Create LargeListType instance from child data type or field.\n\n    This data type may not be supported by all Arrow implementations.\n    Unless you need to represent data larger than 2**31 elements, you should\n    prefer list_().\n\n    Parameters\n    ----------\n    value_type : DataType or Field\n\n    Returns\n    -------\n    list_type : DataType\n\n    Examples\n    --------\n    Create an instance of LargeListType:\n\n    >>> import pyarrow as pa\n    >>> pa.large_list(pa.int8())\n    LargeListType(large_list<item: int8>)\n\n    Use the LargeListType to create an array:\n\n    >>> pa.array([[-1, 3]] * 5, type=pa.large_list(pa.int8()))\n    <pyarrow.lib.LargeListArray object at ...>\n    [\n      [\n        -1,\n        3\n      ],\n      [\n        -1,\n        3\n      ],\n    ...\n    \"\"\"\n    cdef:\n        DataType data_type\n        Field _field\n        shared_ptr[CDataType] list_type\n        LargeListType out = LargeListType.__new__(LargeListType)\n\n    if isinstance(value_type, DataType):\n        _field = field('item', value_type)\n    elif isinstance(value_type, Field):\n        _field = value_type\n    else:\n        raise TypeError('List requires DataType or Field')\n\n    list_type.reset(new CLargeListType(_field.sp_field))\n    out.init(list_type)\n    return out\n\n\ncpdef MapType map_(key_type, item_type, keys_sorted=False):\n    \"\"\"\n    Create MapType instance from key and item data types or fields.\n\n    Parameters\n    ----------\n    key_type : DataType or Field\n    item_type : DataType or Field\n    keys_sorted : bool\n\n    Returns\n    -------\n    map_type : DataType\n\n    Examples\n    --------\n    Create an instance of MapType:\n\n    >>> import pyarrow as pa\n    >>> pa.map_(pa.string(), pa.int32())\n    MapType(map<string, int32>)\n    >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True)\n    MapType(map<string, int32, keys_sorted>)\n\n    Use MapType to create an array:\n\n    >>> data = [[{'key': 'a', 'value': 1}, {'key': 'b', 'value': 2}], [{'key': 'c', 'value': 3}]]\n    >>> pa.array(data, type=pa.map_(pa.string(), pa.int32(), keys_sorted=True))\n    <pyarrow.lib.MapArray object at ...>\n    [\n      keys:\n      [\n        \"a\",\n        \"b\"\n      ]\n      values:\n      [\n        1,\n        2\n      ],\n      keys:\n      [\n        \"c\"\n      ]\n      values:\n      [\n        3\n      ]\n    ]\n    \"\"\"\n    cdef:\n        Field _key_field\n        Field _item_field\n        shared_ptr[CDataType] map_type\n        MapType out = MapType.__new__(MapType)\n\n    if isinstance(key_type, Field):\n        if key_type.nullable:\n            raise TypeError('Map key field should be non-nullable')\n        _key_field = key_type\n    else:\n        _key_field = field('key', ensure_type(key_type, allow_none=False),\n                           nullable=False)\n\n    if isinstance(item_type, Field):\n        _item_field = item_type\n    else:\n        _item_field = field('value', ensure_type(item_type, allow_none=False))\n\n    map_type.reset(new CMapType(_key_field.sp_field, _item_field.sp_field,\n                                keys_sorted))\n    out.init(map_type)\n    return out\n\n\ncpdef DictionaryType dictionary(index_type, value_type, bint ordered=False):\n    \"\"\"\n    Dictionary (categorical, or simply encoded) type.\n\n    Parameters\n    ----------\n    index_type : DataType\n    value_type : DataType\n    ordered : bool\n\n    Returns\n    -------\n    type : DictionaryType\n\n    Examples\n    --------\n    Create an instance of dictionary type:\n\n    >>> import pyarrow as pa\n    >>> pa.dictionary(pa.int64(), pa.utf8())\n    DictionaryType(dictionary<values=string, indices=int64, ordered=0>)\n\n    Use dictionary type to create an array:\n\n    >>> pa.array([\"a\", \"b\", None, \"d\"], pa.dictionary(pa.int64(), pa.utf8()))\n    <pyarrow.lib.DictionaryArray object at ...>\n    ...\n    -- dictionary:\n      [\n        \"a\",\n        \"b\",\n        \"d\"\n      ]\n    -- indices:\n      [\n        0,\n        1,\n        null,\n        2\n      ]\n    \"\"\"\n    cdef:\n        DataType _index_type = ensure_type(index_type, allow_none=False)\n        DataType _value_type = ensure_type(value_type, allow_none=False)\n        DictionaryType out = DictionaryType.__new__(DictionaryType)\n        shared_ptr[CDataType] dict_type\n\n    if _index_type.id not in {\n        Type_INT8, Type_INT16, Type_INT32, Type_INT64,\n        Type_UINT8, Type_UINT16, Type_UINT32, Type_UINT64,\n    }:\n        raise TypeError(\"The dictionary index type should be integer.\")\n\n    dict_type.reset(new CDictionaryType(_index_type.sp_type,\n                                        _value_type.sp_type, ordered == 1))\n    out.init(dict_type)\n    return out\n\n\ndef struct(fields):\n    \"\"\"\n    Create StructType instance from fields.\n\n    A struct is a nested type parameterized by an ordered sequence of types\n    (which can all be distinct), called its fields.\n\n    Parameters\n    ----------\n    fields : iterable of Fields or tuples, or mapping of strings to DataTypes\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n\n    Examples\n    --------\n    Create an instance of StructType from an iterable of tuples:\n\n    >>> import pyarrow as pa\n    >>> fields = [\n    ...     ('f1', pa.int32()),\n    ...     ('f2', pa.string()),\n    ... ]\n    >>> struct_type = pa.struct(fields)\n    >>> struct_type\n    StructType(struct<f1: int32, f2: string>)\n\n    Retrieve a field from a StructType:\n\n    >>> struct_type[0]\n    pyarrow.Field<f1: int32>\n    >>> struct_type['f1']\n    pyarrow.Field<f1: int32>\n\n    Create an instance of StructType from an iterable of Fields:\n\n    >>> fields = [\n    ...     pa.field('f1', pa.int32()),\n    ...     pa.field('f2', pa.string(), nullable=False),\n    ... ]\n    >>> pa.struct(fields)\n    StructType(struct<f1: int32, f2: string not null>)\n\n    Returns\n    -------\n    type : DataType\n    \"\"\"\n    cdef:\n        Field py_field\n        vector[shared_ptr[CField]] c_fields\n        cdef shared_ptr[CDataType] struct_type\n\n    if isinstance(fields, Mapping):\n        fields = fields.items()\n\n    for item in fields:\n        if isinstance(item, tuple):\n            py_field = field(*item)\n        else:\n            py_field = item\n        c_fields.push_back(py_field.sp_field)\n\n    struct_type.reset(new CStructType(c_fields))\n    return pyarrow_wrap_data_type(struct_type)\n\n\ncdef _extract_union_params(child_fields, type_codes,\n                           vector[shared_ptr[CField]]* c_fields,\n                           vector[int8_t]* c_type_codes):\n    cdef:\n        Field child_field\n\n    for child_field in child_fields:\n        c_fields[0].push_back(child_field.sp_field)\n\n    if type_codes is not None:\n        if len(type_codes) != <Py_ssize_t>(c_fields.size()):\n            raise ValueError(\"type_codes should have the same length \"\n                             \"as fields\")\n        for code in type_codes:\n            c_type_codes[0].push_back(code)\n    else:\n        c_type_codes[0] = range(c_fields.size())\n\n\ndef sparse_union(child_fields, type_codes=None):\n    \"\"\"\n    Create SparseUnionType from child fields.\n\n    A sparse union is a nested type where each logical value is taken from\n    a single child.  A buffer of 8-bit type ids indicates which child\n    a given logical value is to be taken from.\n\n    In a sparse union, each child array should have the same length as the\n    union array, regardless of the actual number of union values that\n    refer to it.\n\n    Parameters\n    ----------\n    child_fields : sequence of Field values\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n    type_codes : list of integers, default None\n\n    Returns\n    -------\n    type : SparseUnionType\n    \"\"\"\n    cdef:\n        vector[shared_ptr[CField]] c_fields\n        vector[int8_t] c_type_codes\n\n    _extract_union_params(child_fields, type_codes,\n                          &c_fields, &c_type_codes)\n\n    return pyarrow_wrap_data_type(\n        CMakeSparseUnionType(move(c_fields), move(c_type_codes)))\n\n\ndef dense_union(child_fields, type_codes=None):\n    \"\"\"\n    Create DenseUnionType from child fields.\n\n    A dense union is a nested type where each logical value is taken from\n    a single child, at a specific offset.  A buffer of 8-bit type ids\n    indicates which child a given logical value is to be taken from,\n    and a buffer of 32-bit offsets indicates at which physical position\n    in the given child array the logical value is to be taken from.\n\n    Unlike a sparse union, a dense union allows encoding only the child array\n    values which are actually referred to by the union array.  This is\n    counterbalanced by the additional footprint of the offsets buffer, and\n    the additional indirection cost when looking up values.\n\n    Parameters\n    ----------\n    child_fields : sequence of Field values\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n    type_codes : list of integers, default None\n\n    Returns\n    -------\n    type : DenseUnionType\n    \"\"\"\n    cdef:\n        vector[shared_ptr[CField]] c_fields\n        vector[int8_t] c_type_codes\n\n    _extract_union_params(child_fields, type_codes,\n                          &c_fields, &c_type_codes)\n\n    return pyarrow_wrap_data_type(\n        CMakeDenseUnionType(move(c_fields), move(c_type_codes)))\n\n\ndef union(child_fields, mode, type_codes=None):\n    \"\"\"\n    Create UnionType from child fields.\n\n    A union is a nested type where each logical value is taken from a\n    single child.  A buffer of 8-bit type ids indicates which child\n    a given logical value is to be taken from.\n\n    Unions come in two flavors: sparse and dense\n    (see also `pyarrow.sparse_union` and `pyarrow.dense_union`).\n\n    Parameters\n    ----------\n    child_fields : sequence of Field values\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n    mode : str\n        Must be 'sparse' or 'dense'\n    type_codes : list of integers, default None\n\n    Returns\n    -------\n    type : UnionType\n    \"\"\"\n    if isinstance(mode, int):\n        if mode not in (_UnionMode_SPARSE, _UnionMode_DENSE):\n            raise ValueError(\"Invalid union mode {0!r}\".format(mode))\n    else:\n        if mode == 'sparse':\n            mode = _UnionMode_SPARSE\n        elif mode == 'dense':\n            mode = _UnionMode_DENSE\n        else:\n            raise ValueError(\"Invalid union mode {0!r}\".format(mode))\n\n    if mode == _UnionMode_SPARSE:\n        return sparse_union(child_fields, type_codes)\n    else:\n        return dense_union(child_fields, type_codes)\n\n\ndef run_end_encoded(run_end_type, value_type):\n    \"\"\"\n    Create RunEndEncodedType from run-end and value types.\n\n    Parameters\n    ----------\n    run_end_type : pyarrow.DataType\n        The integer type of the run_ends array. Must be 'int16', 'int32', or 'int64'.\n    value_type : pyarrow.DataType\n        The type of the values array.\n\n    Returns\n    -------\n    type : RunEndEncodedType\n    \"\"\"\n    cdef:\n        DataType _run_end_type = ensure_type(run_end_type, allow_none=False)\n        DataType _value_type = ensure_type(value_type, allow_none=False)\n        shared_ptr[CDataType] ree_type\n\n    if not _run_end_type.type.id() in [_Type_INT16, _Type_INT32, _Type_INT64]:\n        raise ValueError(\"The run_end_type should be 'int16', 'int32', or 'int64'\")\n    ree_type = CMakeRunEndEncodedType(_run_end_type.sp_type, _value_type.sp_type)\n    return pyarrow_wrap_data_type(ree_type)\n\n\ndef fixed_shape_tensor(DataType value_type, shape, dim_names=None, permutation=None):\n    \"\"\"\n    Create instance of fixed shape tensor extension type with shape and optional\n    names of tensor dimensions and indices of the desired logical\n    ordering of dimensions.\n\n    Parameters\n    ----------\n    value_type : DataType\n        Data type of individual tensor elements.\n    shape : tuple or list of integers\n        The physical shape of the contained tensors.\n    dim_names : tuple or list of strings, default None\n        Explicit names to tensor dimensions.\n    permutation : tuple or list integers, default None\n        Indices of the desired ordering of the original dimensions.\n        The indices contain a permutation of the values ``[0, 1, .., N-1]`` where\n        N is the number of dimensions. The permutation indicates which dimension\n        of the logical layout corresponds to which dimension of the physical tensor.\n        For more information on this parameter see\n        :ref:`fixed_shape_tensor_extension`.\n\n    Examples\n    --------\n    Create an instance of fixed shape tensor extension type:\n\n    >>> import pyarrow as pa\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int32(), [2, 2])\n    >>> tensor_type\n    FixedShapeTensorType(extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>)\n\n    Inspect the data type:\n\n    >>> tensor_type.value_type\n    DataType(int32)\n    >>> tensor_type.shape\n    [2, 2]\n\n    Create a table with fixed shape tensor extension array:\n\n    >>> arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n    >>> storage = pa.array(arr, pa.list_(pa.int32(), 4))\n    >>> tensor = pa.ExtensionArray.from_storage(tensor_type, storage)\n    >>> pa.table([tensor], names=[\"tensor_array\"])\n    pyarrow.Table\n    tensor_array: extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>\n    ----\n    tensor_array: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n\n    Create an instance of fixed shape tensor extension type with names\n    of tensor dimensions:\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int8(), (2, 2, 3),\n    ...                                     dim_names=['C', 'H', 'W'])\n    >>> tensor_type.dim_names\n    ['C', 'H', 'W']\n\n    Create an instance of fixed shape tensor extension type with\n    permutation:\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int8(), (2, 2, 3),\n    ...                                     permutation=[0, 2, 1])\n    >>> tensor_type.permutation\n    [0, 2, 1]\n\n    Returns\n    -------\n    type : FixedShapeTensorType\n    \"\"\"\n\n    cdef:\n        vector[int64_t] c_shape\n        vector[int64_t] c_permutation\n        vector[c_string] c_dim_names\n        shared_ptr[CDataType] c_tensor_ext_type\n\n    assert value_type is not None\n    assert shape is not None\n\n    for i in shape:\n        c_shape.push_back(i)\n\n    if permutation is not None:\n        for i in permutation:\n            c_permutation.push_back(i)\n\n    if dim_names is not None:\n        for x in dim_names:\n            c_dim_names.push_back(tobytes(x))\n\n    cdef FixedShapeTensorType out = FixedShapeTensorType.__new__(FixedShapeTensorType)\n\n    c_tensor_ext_type = GetResultValue(CFixedShapeTensorType.Make(\n        value_type.sp_type, c_shape, c_permutation, c_dim_names))\n\n    out.init(c_tensor_ext_type)\n\n    return out\n\n\ncdef dict _type_aliases = {\n    'null': null,\n    'bool': bool_,\n    'boolean': bool_,\n    'i1': int8,\n    'int8': int8,\n    'i2': int16,\n    'int16': int16,\n    'i4': int32,\n    'int32': int32,\n    'i8': int64,\n    'int64': int64,\n    'u1': uint8,\n    'uint8': uint8,\n    'u2': uint16,\n    'uint16': uint16,\n    'u4': uint32,\n    'uint32': uint32,\n    'u8': uint64,\n    'uint64': uint64,\n    'f2': float16,\n    'halffloat': float16,\n    'float16': float16,\n    'f4': float32,\n    'float': float32,\n    'float32': float32,\n    'f8': float64,\n    'double': float64,\n    'float64': float64,\n    'string': string,\n    'str': string,\n    'utf8': string,\n    'binary': binary,\n    'large_string': large_string,\n    'large_str': large_string,\n    'large_utf8': large_string,\n    'large_binary': large_binary,\n    'date32': date32,\n    'date64': date64,\n    'date32[day]': date32,\n    'date64[ms]': date64,\n    'time32[s]': time32('s'),\n    'time32[ms]': time32('ms'),\n    'time64[us]': time64('us'),\n    'time64[ns]': time64('ns'),\n    'timestamp[s]': timestamp('s'),\n    'timestamp[ms]': timestamp('ms'),\n    'timestamp[us]': timestamp('us'),\n    'timestamp[ns]': timestamp('ns'),\n    'duration[s]': duration('s'),\n    'duration[ms]': duration('ms'),\n    'duration[us]': duration('us'),\n    'duration[ns]': duration('ns'),\n    'month_day_nano_interval': month_day_nano_interval(),\n}\n\n\ndef type_for_alias(name):\n    \"\"\"\n    Return DataType given a string alias if one exists.\n\n    Parameters\n    ----------\n    name : str\n        The alias of the DataType that should be retrieved.\n\n    Returns\n    -------\n    type : DataType\n    \"\"\"\n    name = name.lower()\n    try:\n        alias = _type_aliases[name]\n    except KeyError:\n        raise ValueError('No type alias for {0}'.format(name))\n\n    if isinstance(alias, DataType):\n        return alias\n    return alias()\n\n\ncpdef DataType ensure_type(object ty, bint allow_none=False):\n    if allow_none and ty is None:\n        return None\n    elif isinstance(ty, DataType):\n        return ty\n    elif isinstance(ty, str):\n        return type_for_alias(ty)\n    else:\n        raise TypeError('DataType expected, got {!r}'.format(type(ty)))\n\n\ndef schema(fields, metadata=None):\n    \"\"\"\n    Construct pyarrow.Schema from collection of fields.\n\n    Parameters\n    ----------\n    fields : iterable of Fields or tuples, or mapping of strings to DataTypes\n        Can also pass an object that implements the Arrow PyCapsule Protocol\n        for schemas (has an ``__arrow_c_schema__`` method).\n    metadata : dict, default None\n        Keys and values must be coercible to bytes.\n\n    Examples\n    --------\n    Create a Schema from iterable of tuples:\n\n    >>> import pyarrow as pa\n    >>> pa.schema([\n    ...     ('some_int', pa.int32()),\n    ...     ('some_string', pa.string()),\n    ...     pa.field('some_required_string', pa.string(), nullable=False)\n    ... ])\n    some_int: int32\n    some_string: string\n    some_required_string: string not null\n\n    Create a Schema from iterable of Fields:\n\n    >>> pa.schema([\n    ...     pa.field('some_int', pa.int32()),\n    ...     pa.field('some_string', pa.string())\n    ... ])\n    some_int: int32\n    some_string: string\n\n    Returns\n    -------\n    schema : pyarrow.Schema\n    \"\"\"\n    cdef:\n        shared_ptr[const CKeyValueMetadata] c_meta\n        shared_ptr[CSchema] c_schema\n        Schema result\n        Field py_field\n        vector[shared_ptr[CField]] c_fields\n\n    if isinstance(fields, Mapping):\n        fields = fields.items()\n    elif hasattr(fields, \"__arrow_c_schema__\"):\n        return Schema._import_from_c_capsule(fields.__arrow_c_schema__())\n\n    for item in fields:\n        if isinstance(item, tuple):\n            py_field = field(*item)\n        else:\n            py_field = item\n        if py_field is None:\n            raise TypeError(\"field or tuple expected, got None\")\n        c_fields.push_back(py_field.sp_field)\n\n    metadata = ensure_metadata(metadata, allow_none=True)\n    c_meta = pyarrow_unwrap_metadata(metadata)\n\n    c_schema.reset(new CSchema(c_fields, c_meta))\n    result = Schema.__new__(Schema)\n    result.init_schema(c_schema)\n\n    return result\n\n\ndef from_numpy_dtype(object dtype):\n    \"\"\"\n    Convert NumPy dtype to pyarrow.DataType.\n\n    Parameters\n    ----------\n    dtype : the numpy dtype to convert\n\n\n    Examples\n    --------\n    Create a pyarrow DataType from NumPy dtype:\n\n    >>> import pyarrow as pa\n    >>> import numpy as np\n    >>> pa.from_numpy_dtype(np.dtype('float16'))\n    DataType(halffloat)\n    >>> pa.from_numpy_dtype('U')\n    DataType(string)\n    >>> pa.from_numpy_dtype(bool)\n    DataType(bool)\n    >>> pa.from_numpy_dtype(np.str_)\n    DataType(string)\n    \"\"\"\n    cdef shared_ptr[CDataType] c_type\n    dtype = np.dtype(dtype)\n    with nogil:\n        check_status(NumPyDtypeToArrow(dtype, &c_type))\n\n    return pyarrow_wrap_data_type(c_type)\n\n\ndef is_boolean_value(object obj):\n    \"\"\"\n    Check if the object is a boolean.\n\n    Parameters\n    ----------\n    obj : object\n        The object to check\n    \"\"\"\n    return IsPyBool(obj)\n\n\ndef is_integer_value(object obj):\n    \"\"\"\n    Check if the object is an integer.\n\n    Parameters\n    ----------\n    obj : object\n        The object to check\n    \"\"\"\n    return IsPyInt(obj)\n\n\ndef is_float_value(object obj):\n    \"\"\"\n    Check if the object is a float.\n\n    Parameters\n    ----------\n    obj : object\n        The object to check\n    \"\"\"\n    return IsPyFloat(obj)\n\n\ncdef class _ExtensionRegistryNanny(_Weakrefable):\n    # Keep the registry alive until we have unregistered PyExtensionType\n    cdef:\n        shared_ptr[CExtensionTypeRegistry] registry\n\n    def __cinit__(self):\n        self.registry = CExtensionTypeRegistry.GetGlobalRegistry()\n\n    def release_registry(self):\n        self.registry.reset()\n\n\n_registry_nanny = _ExtensionRegistryNanny()\n\n\ndef _register_py_extension_type():\n    cdef:\n        DataType storage_type\n        shared_ptr[CExtensionType] cpy_ext_type\n        c_string c_extension_name = tobytes(\"arrow.py_extension_type\")\n\n    # Make a dummy C++ ExtensionType\n    storage_type = null()\n    check_status(CPyExtensionType.FromClass(\n        storage_type.sp_type, c_extension_name, PyExtensionType,\n        &cpy_ext_type))\n    check_status(\n        RegisterPyExtensionType(<shared_ptr[CDataType]> cpy_ext_type))\n\n\ndef _unregister_py_extension_types():\n    # This needs to be done explicitly before the Python interpreter is\n    # finalized.  If the C++ type is destroyed later in the process\n    # teardown stage, it will invoke CPython APIs such as Py_DECREF\n    # with a destroyed interpreter.\n    unregister_extension_type(\"arrow.py_extension_type\")\n    for ext_type in _python_extension_types_registry:\n        try:\n            unregister_extension_type(ext_type.extension_name)\n        except KeyError:\n            pass\n    _registry_nanny.release_registry()\n\n\n_register_py_extension_type()\natexit.register(_unregister_py_extension_types)\n\n\n#\n# PyCapsule export utilities\n#\n\ncdef void pycapsule_schema_deleter(object schema_capsule) noexcept:\n    cdef ArrowSchema* schema = <ArrowSchema*>PyCapsule_GetPointer(\n        schema_capsule, 'arrow_schema'\n    )\n    if schema.release != NULL:\n        schema.release(schema)\n\n    free(schema)\n\ncdef object alloc_c_schema(ArrowSchema** c_schema) noexcept:\n    c_schema[0] = <ArrowSchema*> malloc(sizeof(ArrowSchema))\n    # Ensure the capsule destructor doesn't call a random release pointer\n    c_schema[0].release = NULL\n    return PyCapsule_New(c_schema[0], 'arrow_schema', &pycapsule_schema_deleter)\n\n\ncdef void pycapsule_array_deleter(object array_capsule) noexcept:\n    cdef:\n        ArrowArray* array\n    # Do not invoke the deleter on a used/moved capsule\n    array = <ArrowArray*>cpython.PyCapsule_GetPointer(\n        array_capsule, 'arrow_array'\n    )\n    if array.release != NULL:\n        array.release(array)\n\n    free(array)\n\ncdef object alloc_c_array(ArrowArray** c_array) noexcept:\n    c_array[0] = <ArrowArray*> malloc(sizeof(ArrowArray))\n    # Ensure the capsule destructor doesn't call a random release pointer\n    c_array[0].release = NULL\n    return PyCapsule_New(c_array[0], 'arrow_array', &pycapsule_array_deleter)\n\n\ncdef void pycapsule_stream_deleter(object stream_capsule) noexcept:\n    cdef:\n        ArrowArrayStream* stream\n    # Do not invoke the deleter on a used/moved capsule\n    stream = <ArrowArrayStream*>PyCapsule_GetPointer(\n        stream_capsule, 'arrow_array_stream'\n    )\n    if stream.release != NULL:\n        stream.release(stream)\n\n    free(stream)\n\ncdef object alloc_c_stream(ArrowArrayStream** c_stream) noexcept:\n    c_stream[0] = <ArrowArrayStream*> malloc(sizeof(ArrowArrayStream))\n    # Ensure the capsule destructor doesn't call a random release pointer\n    c_stream[0].release = NULL\n    return PyCapsule_New(c_stream[0], 'arrow_array_stream', &pycapsule_stream_deleter)\n"], "fixing_code": [".. Licensed to the Apache Software Foundation (ASF) under one\n.. or more contributor license agreements.  See the NOTICE file\n.. distributed with this work for additional information\n.. regarding copyright ownership.  The ASF licenses this file\n.. to you under the Apache License, Version 2.0 (the\n.. \"License\"); you may not use this file except in compliance\n.. with the License.  You may obtain a copy of the License at\n\n..   http://www.apache.org/licenses/LICENSE-2.0\n\n.. Unless required by applicable law or agreed to in writing,\n.. software distributed under the License is distributed on an\n.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n.. KIND, either express or implied.  See the License for the\n.. specific language governing permissions and limitations\n.. under the License.\n\n.. currentmodule:: pyarrow\n.. _extending_types:\n\nExtending pyarrow\n=================\n\n.. _arrow_array_protocol:\n\nControlling conversion to pyarrow.Array with the ``__arrow_array__`` protocol\n-----------------------------------------------------------------------------\n\nThe :func:`pyarrow.array` function has built-in support for Python sequences,\nnumpy arrays and pandas 1D objects (Series, Index, Categorical, ..) to convert\nthose to Arrow arrays. This can be extended for other array-like objects\nby implementing the ``__arrow_array__`` method (similar to numpy's ``__array__``\nprotocol).\n\nFor example, to support conversion of your duck array class to an Arrow array,\ndefine the ``__arrow_array__`` method to return an Arrow array::\n\n    class MyDuckArray:\n\n        ...\n\n        def __arrow_array__(self, type=None):\n            # convert the underlying array values to a pyarrow Array\n            import pyarrow\n            return pyarrow.array(..., type=type)\n\nThe ``__arrow_array__`` method takes an optional `type` keyword which is passed\nthrough from :func:`pyarrow.array`. The method is allowed to return either\na :class:`~pyarrow.Array` or a :class:`~pyarrow.ChunkedArray`.\n\n.. note::\n\n    For a more general way to control the conversion of Python objects to Arrow\n    data consider the :doc:`/format/CDataInterface/PyCapsuleInterface`. It is\n    not specific to PyArrow and supports converting other objects such as tables\n    and schemas.\n\n\nDefining extension types (\"user-defined types\")\n-----------------------------------------------\n\nArrow has the notion of extension types in the metadata specification as a\npossibility to extend the built-in types. This is done by annotating any of the\nbuilt-in Arrow logical types (the \"storage type\") with a custom type name and\noptional serialized representation (\"ARROW:extension:name\" and\n\"ARROW:extension:metadata\" keys in the Field\u2019s custom_metadata of an IPC\nmessage).\nSee the :ref:`format_metadata_extension_types` section of the metadata\nspecification for more details.\n\nPyarrow allows you to define such extension types from Python by subclassing\n:class:`ExtensionType` and giving the derived class its own extension name\nand serialization mechanism. The extension name and serialized metadata\ncan potentially be recognized by other (non-Python) Arrow implementations\nsuch as PySpark.\n\nFor example, we could define a custom UUID type for 128-bit numbers which can\nbe represented as ``FixedSizeBinary`` type with 16 bytes::\n\n    class UuidType(pa.ExtensionType):\n\n        def __init__(self):\n            super().__init__(pa.binary(16), \"my_package.uuid\")\n\n        def __arrow_ext_serialize__(self):\n            # Since we don't have a parameterized type, we don't need extra\n            # metadata to be deserialized\n            return b''\n\n        @classmethod\n        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n            # Sanity checks, not required but illustrate the method signature.\n            assert storage_type == pa.binary(16)\n            assert serialized == b''\n            # Return an instance of this subclass given the serialized\n            # metadata.\n            return UuidType()\n\nThe special methods ``__arrow_ext_serialize__`` and ``__arrow_ext_deserialize__``\ndefine the serialization of an extension type instance. For non-parametric\ntypes such as the above, the serialization payload can be left empty.\n\nThis can now be used to create arrays and tables holding the extension type::\n\n    >>> uuid_type = UuidType()\n    >>> uuid_type.extension_name\n    'my_package.uuid'\n    >>> uuid_type.storage_type\n    FixedSizeBinaryType(fixed_size_binary[16])\n\n    >>> import uuid\n    >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)], pa.binary(16))\n    >>> arr = pa.ExtensionArray.from_storage(uuid_type, storage_array)\n    >>> arr\n    <pyarrow.lib.ExtensionArray object at 0x7f75c2f300a0>\n    [\n      A6861959108644B797664AEEE686B682,\n      718747F48E5F4058A7261E2B6B228BE8,\n      7FE201227D624D96A5CD8639DEF2A68B,\n      C6CA8C7F95744BFD9462A40B3F57A86C\n    ]\n\nThis array can be included in RecordBatches, sent over IPC and received in\nanother Python process. The receiving process must explicitly register the\nextension type for deserialization, otherwise it will fall back to the\nstorage type::\n\n    >>> pa.register_extension_type(UuidType())\n\nFor example, creating a RecordBatch and writing it to a stream using the\nIPC protocol::\n\n    >>> batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n    >>> sink = pa.BufferOutputStream()\n    >>> with pa.RecordBatchStreamWriter(sink, batch.schema) as writer:\n    ...    writer.write_batch(batch)\n    >>> buf = sink.getvalue()\n\nand then reading it back yields the proper type::\n\n    >>> with pa.ipc.open_stream(buf) as reader:\n    ...    result = reader.read_all()\n    >>> result.column('ext').type\n    UuidType(FixedSizeBinaryType(fixed_size_binary[16]))\n\nThe receiving application doesn't need to be Python but can still recognize\nthe extension type as a \"my_package.uuid\" type, if it has implemented its own\nextension type to receive it. If the type is not registered in the receiving\napplication, it will fall back to the storage type.\n\nParameterized extension type\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe above example used a fixed storage type with no further metadata. But\nmore flexible, parameterized extension types are also possible.\n\nThe example given here implements an extension type for the `pandas \"period\"\ndata type <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-span-representation>`__,\nrepresenting time spans (e.g., a frequency of a day, a month, a quarter, etc).\nIt is stored as an int64 array which is interpreted as the number of time spans\nof the given frequency since 1970.\n\n::\n\n    class PeriodType(pa.ExtensionType):\n\n        def __init__(self, freq):\n            # attributes need to be set first before calling\n            # super init (as that calls serialize)\n            self._freq = freq\n            super().__init__(pa.int64(), 'my_package.period')\n\n        @property\n        def freq(self):\n            return self._freq\n\n        def __arrow_ext_serialize__(self):\n            return \"freq={}\".format(self.freq).encode()\n\n        @classmethod\n        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n            # Return an instance of this subclass given the serialized\n            # metadata.\n            serialized = serialized.decode()\n            assert serialized.startswith(\"freq=\")\n            freq = serialized.split('=')[1]\n            return PeriodType(freq)\n\nHere, we ensure to store all information in the serialized metadata that is\nneeded to reconstruct the instance (in the ``__arrow_ext_deserialize__`` class\nmethod), in this case the frequency string.\n\nNote that, once created, the data type instance is considered immutable.\nIn the example above, the ``freq`` parameter is therefore stored in a private\nattribute with a public read-only property to access it.\n\nCustom extension array class\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy default, all arrays with an extension type are constructed or deserialized into\na built-in :class:`ExtensionArray` object. Nevertheless, one could want to subclass\n:class:`ExtensionArray` in order to add some custom logic specific to the extension\ntype. Arrow allows to do so by adding a special method ``__arrow_ext_class__`` to the\ndefinition of the extension type.\n\nFor instance, let us consider the example from the `Numpy Quickstart <https://docs.scipy.org/doc/numpy-1.13.0/user/quickstart.html>`_ of points in 3D space.\nWe can store these as a fixed-size list, where we wish to be able to extract\nthe data as a 2-D Numpy array ``(N, 3)`` without any copy::\n\n    class Point3DArray(pa.ExtensionArray):\n        def to_numpy_array(self):\n            return self.storage.flatten().to_numpy().reshape((-1, 3))\n\n\n    class Point3DType(pa.ExtensionType):\n        def __init__(self):\n            super().__init__(pa.list_(pa.float32(), 3), \"my_package.Point3DType\")\n\n        def __arrow_ext_serialize__(self):\n            return b''\n\n        @classmethod\n        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n            return Point3DType()\n\n        def __arrow_ext_class__(self):\n            return Point3DArray\n\nArrays built using this extension type now have the expected custom array class::\n\n    >>> storage = pa.array([[1, 2, 3], [4, 5, 6]], pa.list_(pa.float32(), 3))\n    >>> arr = pa.ExtensionArray.from_storage(Point3DType(), storage)\n    >>> arr\n    <__main__.Point3DArray object at 0x7f40dea80670>\n    [\n        [\n            1,\n            2,\n            3\n        ],\n        [\n            4,\n            5,\n            6\n        ]\n    ]\n\nThe additional methods in the extension class are then available to the user::\n\n    >>> arr.to_numpy_array()\n    array([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)\n\n\nThis array can be sent over IPC, received in another Python process, and the custom\nextension array class will be preserved (as long as the receiving process registers\nthe extension type using :func:`register_extension_type` before reading the IPC data).\n\nCustom scalar conversion\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want scalars of your custom extension type to convert to a custom type when\n:meth:`ExtensionScalar.as_py()` is called, you can override the\n:meth:`ExtensionScalar.as_py()` method by subclassing :class:`ExtensionScalar`.\nFor example, if we wanted the above example 3D point type to return a custom\n3D point class instead of a list, we would implement::\n\n    from collections import namedtuple\n\n    Point3D = namedtuple(\"Point3D\", [\"x\", \"y\", \"z\"])\n\n    class Point3DScalar(pa.ExtensionScalar):\n        def as_py(self) -> Point3D:\n            return Point3D(*self.value.as_py())\n\n    class Point3DType(pa.ExtensionType):\n        def __init__(self):\n            super().__init__(pa.list_(pa.float32(), 3), \"my_package.Point3DType\")\n\n        def __arrow_ext_serialize__(self):\n            return b''\n\n        @classmethod\n        def __arrow_ext_deserialize__(cls, storage_type, serialized):\n            return Point3DType()\n\n        def __arrow_ext_scalar_class__(self):\n            return Point3DScalar\n\nArrays built using this extension type now provide scalars that convert to our ``Point3D`` class::\n\n    >>> storage = pa.array([[1, 2, 3], [4, 5, 6]], pa.list_(pa.float32(), 3))\n    >>> arr = pa.ExtensionArray.from_storage(Point3DType(), storage)\n    >>> arr[0].as_py()\n    Point3D(x=1.0, y=2.0, z=3.0)\n\n    >>> arr.to_pylist()\n    [Point3D(x=1.0, y=2.0, z=3.0), Point3D(x=4.0, y=5.0, z=6.0)]\n\n\nConversion to pandas\n~~~~~~~~~~~~~~~~~~~~\n\nThe conversion to pandas (in :meth:`Table.to_pandas`) of columns with an\nextension type can controlled in case there is a corresponding\n`pandas extension array <https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extension-types>`__\nfor your extension type.\n\nFor this, the :meth:`ExtensionType.to_pandas_dtype` method needs to be\nimplemented, and should return a ``pandas.api.extensions.ExtensionDtype``\nsubclass instance.\n\nUsing the pandas period type from above as example, this would look like::\n\n    class PeriodType(pa.ExtensionType):\n        ...\n\n        def to_pandas_dtype(self):\n            import pandas as pd\n            return pd.PeriodDtype(freq=self.freq)\n\nSecondly, the pandas ``ExtensionDtype`` on its turn needs to have the\n``__from_arrow__`` method implemented: a method that given a pyarrow Array\nor ChunkedArray of the extension type can construct the corresponding\npandas ``ExtensionArray``. This method should have the following signature::\n\n\n    class MyExtensionDtype(pd.api.extensions.ExtensionDtype):\n        ...\n\n        def __from_arrow__(self, array: pyarrow.Array/ChunkedArray) -> pandas.ExtensionArray:\n            ...\n\nThis way, you can control the conversion of a pyarrow ``Array`` of your pyarrow\nextension type to a pandas ``ExtensionArray`` that can be stored in a DataFrame.\n\n\nCanonical extension types\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can find the official list of canonical extension types in the\n:ref:`format_canonical_extensions` section. Here we add examples on how to\nuse them in pyarrow.\n\nFixed size tensor\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nTo create an array of tensors with equal shape (fixed shape tensor array) we\nfirst need to define a fixed shape tensor extension type with value type\nand shape:\n\n.. code-block:: python\n\n   >>> tensor_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))\n\nThen we need the storage array with :func:`pyarrow.list_` type where ``value_type```\nis the fixed shape tensor value type and list size is a product of ``tensor_type``\nshape elements. Then we can create an array of tensors with\n``pa.ExtensionArray.from_storage()`` method:\n\n.. code-block:: python\n\n   >>> arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n   >>> storage = pa.array(arr, pa.list_(pa.int32(), 4))\n   >>> tensor_array = pa.ExtensionArray.from_storage(tensor_type, storage)\n\nWe can also create another array of tensors with different value type:\n\n.. code-block:: python\n\n   >>> tensor_type_2 = pa.fixed_shape_tensor(pa.float32(), (2, 2))\n   >>> storage_2 = pa.array(arr, pa.list_(pa.float32(), 4))\n   >>> tensor_array_2 = pa.ExtensionArray.from_storage(tensor_type_2, storage_2)\n\nExtension arrays can be used as columns in  ``pyarrow.Table`` or\n``pyarrow.RecordBatch``:\n\n.. code-block:: python\n\n   >>> data = [\n   ...     pa.array([1, 2, 3]),\n   ...     pa.array(['foo', 'bar', None]),\n   ...     pa.array([True, None, True]),\n   ...     tensor_array,\n   ...     tensor_array_2\n   ... ]\n   >>> my_schema = pa.schema([('f0', pa.int8()),\n   ...                        ('f1', pa.string()),\n   ...                        ('f2', pa.bool_()),\n   ...                        ('tensors_int', tensor_type),\n   ...                        ('tensors_float', tensor_type_2)])\n   >>> table = pa.Table.from_arrays(data, schema=my_schema)\n   >>> table\n   pyarrow.Table\n   f0: int8\n   f1: string\n   f2: bool\n   tensors_int: extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>\n   tensors_float: extension<arrow.fixed_shape_tensor[value_type=float, shape=[2,2]]>\n   ----\n   f0: [[1,2,3]]\n   f1: [[\"foo\",\"bar\",null]]\n   f2: [[true,null,true]]\n   tensors_int: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n   tensors_float: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n\nWe can also convert a tensor array to a single multi-dimensional numpy ndarray.\nWith the conversion the length of the arrow array becomes the first dimension\nin the numpy ndarray:\n\n.. code-block:: python\n\n   >>> numpy_tensor = tensor_array_2.to_numpy_ndarray()\n   >>> numpy_tensor\n   array([[[  1.,   2.],\n           [  3.,   4.]],\n          [[ 10.,  20.],\n           [ 30.,  40.]],\n          [[100., 200.],\n           [300., 400.]]])\n    >>> numpy_tensor.shape\n   (3, 2, 2)\n\n.. note::\n\n   Both optional parameters, ``permutation`` and ``dim_names``, are meant to provide the user\n   with the information about the logical layout of the data compared to the physical layout.\n\n   The conversion to numpy ndarray is only possible for trivial permutations (``None`` or\n   ``[0, 1, ... N-1]`` where ``N`` is the number of tensor dimensions).\n\nAnd also the other way around, we can convert a numpy ndarray to a fixed shape tensor array:\n\n.. code-block:: python\n\n   >>> pa.FixedShapeTensorArray.from_numpy_ndarray(numpy_tensor)\n   <pyarrow.lib.FixedShapeTensorArray object at ...>\n   [\n     [\n       1,\n       2,\n       3,\n       4\n     ],\n     [\n       10,\n       20,\n       30,\n       40\n     ],\n     [\n       100,\n       200,\n       300,\n       400\n     ]\n   ]\n\nWith the conversion the first dimension of the ndarray becomes the length of the pyarrow extension\narray. We can see in the example that ndarray of shape ``(3, 2, 2)`` becomes an arrow array of\nlength 3 with tensor elements of shape ``(2, 2)``.\n\n.. code-block:: python\n\n   # ndarray of shape (3, 2, 2)\n   >>> numpy_tensor.shape\n   (3, 2, 2)\n\n   # arrow array of length 3 with tensor elements of shape (2, 2)\n   >>> pyarrow_tensor_array = pa.FixedShapeTensorArray.from_numpy_ndarray(numpy_tensor)\n   >>> len(pyarrow_tensor_array)\n   3\n   >>> pyarrow_tensor_array.type.shape\n   [2, 2]\n\nThe extension type can also have ``permutation`` and ``dim_names`` defined. For\nexample\n\n.. code-block:: python\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.float64(), [2, 2, 3], permutation=[0, 2, 1])\n\nor\n\n.. code-block:: python\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.bool_(), [2, 2, 3], dim_names=['C', 'H', 'W'])\n\nfor ``NCHW`` format where:\n\n* N: number of images which is in our case the length of an array and is always on\n  the first dimension\n* C: number of channels of the image\n* H: height of the image\n* W: width of the image\n", "# -*- coding: utf-8 -*-\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport ctypes\nimport gc\n\nimport pyarrow as pa\ntry:\n    from pyarrow.cffi import ffi\nexcept ImportError:\n    ffi = None\n\nimport pytest\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\nexcept ImportError:\n    pd = tm = None\n\n\nneeds_cffi = pytest.mark.skipif(ffi is None,\n                                reason=\"test needs cffi package installed\")\n\nassert_schema_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowSchema\")\n\nassert_array_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowArray\")\n\nassert_stream_released = pytest.raises(\n    ValueError, match=\"Cannot import released ArrowArrayStream\")\n\n\ndef PyCapsule_IsValid(capsule, name):\n    return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1\n\n\n@contextlib.contextmanager\ndef registered_extension_type(ext_type):\n    pa.register_extension_type(ext_type)\n    try:\n        yield\n    finally:\n        pa.unregister_extension_type(ext_type.extension_name)\n\n\nclass ParamExtType(pa.ExtensionType):\n\n    def __init__(self, width):\n        self._width = width\n        super().__init__(pa.binary(width),\n                         \"pyarrow.tests.test_cffi.ParamExtType\")\n\n    @property\n    def width(self):\n        return self._width\n\n    def __arrow_ext_serialize__(self):\n        return str(self.width).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        width = int(serialized.decode())\n        return cls(width)\n\n\ndef make_schema():\n    return pa.schema([('ints', pa.list_(pa.int32()))],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_extension_schema():\n    return pa.schema([('ext', ParamExtType(3))],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_extension_storage_schema():\n    # Should be kept in sync with make_extension_schema\n    return pa.schema([('ext', ParamExtType(3).storage_type)],\n                     metadata={b'key1': b'value1'})\n\n\ndef make_batch():\n    return pa.record_batch([[[1], [2, 42]]], make_schema())\n\n\ndef make_extension_batch():\n    schema = make_extension_schema()\n    ext_col = schema[0].type.wrap_array(pa.array([b\"foo\", b\"bar\"],\n                                                 type=pa.binary(3)))\n    return pa.record_batch([ext_col], schema)\n\n\ndef make_batches():\n    schema = make_schema()\n    return [\n        pa.record_batch([[[1], [2, 42]]], schema),\n        pa.record_batch([[None, [], [5, 6]]], schema),\n    ]\n\n\ndef make_serialized(schema, batches):\n    with pa.BufferOutputStream() as sink:\n        with pa.ipc.new_stream(sink, schema) as out:\n            for batch in batches:\n                out.write(batch)\n        return sink.getvalue()\n\n\n@needs_cffi\ndef test_export_import_type():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    typ = pa.list_(pa.int32())\n    typ._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del typ\n    assert pa.total_allocated_bytes() > old_allocated\n    typ_new = pa.DataType._import_from_c(ptr_schema)\n    assert typ_new == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.DataType._import_from_c(ptr_schema)\n\n    # Invalid format string\n    pa.int32()._export_to_c(ptr_schema)\n    bad_format = ffi.new(\"char[]\", b\"zzz\")\n    c_schema.format = bad_format\n    with pytest.raises(ValueError,\n                       match=\"Invalid or unsupported format string\"):\n        pa.DataType._import_from_c(ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.DataType._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_field():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    field = pa.field(\"test\", pa.list_(pa.int32()), nullable=True)\n    field._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del field\n    assert pa.total_allocated_bytes() > old_allocated\n\n    field_new = pa.Field._import_from_c(ptr_schema)\n    assert field_new == pa.field(\"test\", pa.list_(pa.int32()), nullable=True)\n    assert pa.total_allocated_bytes() == old_allocated\n\n    # Now released\n    with assert_schema_released:\n        pa.Field._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_array():\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(\"struct ArrowArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    # Type is known up front\n    typ = pa.list_(pa.int32())\n    arr = pa.array([[1], [2, 42]], type=typ)\n    py_value = arr.to_pylist()\n    arr._export_to_c(ptr_array)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete recreate C++ object from exported pointer\n    del arr\n    arr_new = pa.Array._import_from_c(ptr_array, typ)\n    assert arr_new.to_pylist() == py_value\n    assert arr_new.type == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_new, typ\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_array_released:\n        pa.Array._import_from_c(ptr_array, pa.list_(pa.int32()))\n\n    # Type is exported and imported at the same time\n    arr = pa.array([[1], [2, 42]], type=pa.list_(pa.int32()))\n    py_value = arr.to_pylist()\n    arr._export_to_c(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del arr\n    arr_new = pa.Array._import_from_c(ptr_array, ptr_schema)\n    assert arr_new.to_pylist() == py_value\n    assert arr_new.type == pa.list_(pa.int32())\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.Array._import_from_c(ptr_array, ptr_schema)\n\n\ndef check_export_import_schema(schema_factory, expected_schema_factory=None):\n    if expected_schema_factory is None:\n        expected_schema_factory = schema_factory\n\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    schema_factory()._export_to_c(ptr_schema)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    schema_new = pa.Schema._import_from_c(ptr_schema)\n    assert schema_new == expected_schema_factory()\n    assert pa.total_allocated_bytes() == old_allocated\n    del schema_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.Schema._import_from_c(ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        pa.Schema._import_from_c(ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.Schema._import_from_c(ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_schema():\n    check_export_import_schema(make_schema)\n\n\n@needs_cffi\ndef test_export_import_schema_with_extension():\n    # Extension type is unregistered => the storage type is imported\n    check_export_import_schema(make_extension_schema,\n                               make_extension_storage_schema)\n\n    # Extension type is registered => the extension type is imported\n    with registered_extension_type(ParamExtType(1)):\n        check_export_import_schema(make_extension_schema)\n\n\n@needs_cffi\ndef test_export_import_schema_float_pointer():\n    # Previous versions of the R Arrow library used to pass pointer\n    # values as a double.\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n\n    match = \"Passing a pointer value as a float is unsafe\"\n    with pytest.warns(UserWarning, match=match):\n        make_schema()._export_to_c(float(ptr_schema))\n    with pytest.warns(UserWarning, match=match):\n        schema_new = pa.Schema._import_from_c(float(ptr_schema))\n    assert schema_new == make_schema()\n\n\ndef check_export_import_batch(batch_factory):\n    c_schema = ffi.new(\"struct ArrowSchema*\")\n    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))\n    c_array = ffi.new(\"struct ArrowArray*\")\n    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    # Schema is known up front\n    batch = batch_factory()\n    schema = batch.schema\n    py_value = batch.to_pydict()\n    batch._export_to_c(ptr_array)\n    assert pa.total_allocated_bytes() > old_allocated\n    # Delete and recreate C++ object from exported pointer\n    del batch\n    batch_new = pa.RecordBatch._import_from_c(ptr_array, schema)\n    assert batch_new.to_pydict() == py_value\n    assert batch_new.schema == schema\n    assert pa.total_allocated_bytes() > old_allocated\n    del batch_new, schema\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_array_released:\n        pa.RecordBatch._import_from_c(ptr_array, make_schema())\n\n    # Type is exported and imported at the same time\n    batch = batch_factory()\n    py_value = batch.to_pydict()\n    batch._export_to_c(ptr_array, ptr_schema)\n    # Delete and recreate C++ objects from exported pointers\n    del batch\n    batch_new = pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n    assert batch_new.to_pydict() == py_value\n    assert batch_new.schema == batch_factory().schema\n    assert pa.total_allocated_bytes() > old_allocated\n    del batch_new\n    assert pa.total_allocated_bytes() == old_allocated\n    # Now released\n    with assert_schema_released:\n        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n\n    # Not a struct type\n    pa.int32()._export_to_c(ptr_schema)\n    batch_factory()._export_to_c(ptr_array)\n    with pytest.raises(ValueError,\n                       match=\"ArrowSchema describes non-struct type\"):\n        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n    # Now released\n    with assert_schema_released:\n        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)\n\n\n@needs_cffi\ndef test_export_import_batch():\n    check_export_import_batch(make_batch)\n\n\n@needs_cffi\ndef test_export_import_batch_with_extension():\n    with registered_extension_type(ParamExtType(1)):\n        check_export_import_batch(make_extension_batch)\n\n\ndef _export_import_batch_reader(ptr_stream, reader_factory):\n    # Prepare input\n    batches = make_batches()\n    schema = batches[0].schema\n\n    reader = reader_factory(schema, batches)\n    reader._export_to_c(ptr_stream)\n    # Delete and recreate C++ object from exported pointer\n    del reader, batches\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    assert reader_new.schema == schema\n    got_batches = list(reader_new)\n    del reader_new\n    assert got_batches == make_batches()\n\n    # Test read_pandas()\n    if pd is not None:\n        batches = make_batches()\n        schema = batches[0].schema\n        expected_df = pa.Table.from_batches(batches).to_pandas()\n\n        reader = reader_factory(schema, batches)\n        reader._export_to_c(ptr_stream)\n        del reader, batches\n\n        reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n        got_df = reader_new.read_pandas()\n        del reader_new\n        tm.assert_frame_equal(expected_df, got_df)\n\n\ndef make_ipc_stream_reader(schema, batches):\n    return pa.ipc.open_stream(make_serialized(schema, batches))\n\n\ndef make_py_record_batch_reader(schema, batches):\n    return pa.RecordBatchReader.from_batches(schema, batches)\n\n\n@needs_cffi\n@pytest.mark.parametrize('reader_factory',\n                         [make_ipc_stream_reader,\n                          make_py_record_batch_reader])\ndef test_export_import_batch_reader(reader_factory):\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    _export_import_batch_reader(ptr_stream, reader_factory)\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    # Now released\n    with assert_stream_released:\n        pa.RecordBatchReader._import_from_c(ptr_stream)\n\n\n@needs_cffi\ndef test_imported_batch_reader_error():\n    c_stream = ffi.new(\"struct ArrowArrayStream*\")\n    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))\n\n    schema = pa.schema([('foo', pa.int32())])\n    batches = [pa.record_batch([[1, 2, 3]], schema=schema),\n               pa.record_batch([[4, 5, 6]], schema=schema)]\n    buf = make_serialized(schema, batches)\n\n    # Open a corrupt/incomplete stream and export it\n    reader = pa.ipc.open_stream(buf[:-16])\n    reader._export_to_c(ptr_stream)\n    del reader\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    batch = reader_new.read_next_batch()\n    assert batch == batches[0]\n    with pytest.raises(OSError,\n                       match=\"Expected to be able to read 16 bytes \"\n                             \"for message body, got 8\"):\n        reader_new.read_next_batch()\n\n    # Again, but call read_all()\n    reader = pa.ipc.open_stream(buf[:-16])\n    reader._export_to_c(ptr_stream)\n    del reader\n\n    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)\n    with pytest.raises(OSError,\n                       match=\"Expected to be able to read 16 bytes \"\n                             \"for message body, got 8\"):\n        reader_new.read_all()\n\n\n@pytest.mark.parametrize('obj', [pa.int32(), pa.field('foo', pa.int32()),\n                                 pa.schema({'foo': pa.int32()})],\n                         ids=['type', 'field', 'schema'])\ndef test_roundtrip_schema_capsule(obj):\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    capsule = obj.__arrow_c_schema__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_schema\") == 1\n    assert pa.total_allocated_bytes() > old_allocated\n    obj_out = type(obj)._import_from_c_capsule(capsule)\n    assert obj_out == obj\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    capsule = obj.__arrow_c_schema__()\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del capsule\n    assert pa.total_allocated_bytes() == old_allocated\n\n\n@pytest.mark.parametrize('arr,schema_accessor,bad_type,good_type', [\n    (pa.array(['a', 'b', 'c']), lambda x: x.type, pa.int32(), pa.string()),\n    (\n        pa.record_batch([pa.array(['a', 'b', 'c'])], names=['x']),\n        lambda x: x.schema,\n        pa.schema({'x': pa.int32()}),\n        pa.schema({'x': pa.string()})\n    ),\n], ids=['array', 'record_batch'])\ndef test_roundtrip_array_capsule(arr, schema_accessor, bad_type, good_type):\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    import_array = type(arr)._import_from_c_capsule\n\n    schema_capsule, capsule = arr.__arrow_c_array__()\n    assert PyCapsule_IsValid(schema_capsule, b\"arrow_schema\") == 1\n    assert PyCapsule_IsValid(capsule, b\"arrow_array\") == 1\n    arr_out = import_array(schema_capsule, capsule)\n    assert arr_out.equals(arr)\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del arr_out\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    capsule = arr.__arrow_c_array__()\n\n    assert pa.total_allocated_bytes() > old_allocated\n    del capsule\n    assert pa.total_allocated_bytes() == old_allocated\n\n    with pytest.raises(ValueError,\n                       match=r\"Could not cast.* string to requested .* int32\"):\n        arr.__arrow_c_array__(bad_type.__arrow_c_schema__())\n\n    schema_capsule, array_capsule = arr.__arrow_c_array__(\n        good_type.__arrow_c_schema__())\n    arr_out = import_array(schema_capsule, array_capsule)\n    assert schema_accessor(arr_out) == good_type\n\n\n# TODO: implement requested_schema for stream\n@pytest.mark.parametrize('constructor', [\n    pa.RecordBatchReader.from_batches,\n    # Use a lambda because we need to re-order the parameters\n    lambda schema, batches: pa.Table.from_batches(batches, schema),\n], ids=['recordbatchreader', 'table'])\ndef test_roundtrip_reader_capsule(constructor):\n    batches = make_batches()\n    schema = batches[0].schema\n\n    gc.collect()  # Make sure no Arrow data dangles in a ref cycle\n    old_allocated = pa.total_allocated_bytes()\n\n    obj = constructor(schema, batches)\n\n    capsule = obj.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == schema\n    imported_batches = list(imported_reader)\n    assert len(imported_batches) == len(batches)\n    for batch, expected in zip(imported_batches, batches):\n        assert batch.equals(expected)\n\n    del obj, imported_reader, batch, expected, imported_batches\n\n    assert pa.total_allocated_bytes() == old_allocated\n\n    obj = constructor(schema, batches)\n\n    # TODO: turn this to ValueError once we implement validation.\n    bad_schema = pa.schema({'ints': pa.int32()})\n    with pytest.raises(NotImplementedError):\n        obj.__arrow_c_stream__(bad_schema.__arrow_c_schema__())\n\n    # Can work with matching schema\n    matching_schema = pa.schema({'ints': pa.list_(pa.int32())})\n    capsule = obj.__arrow_c_stream__(matching_schema.__arrow_c_schema__())\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == matching_schema\n    for batch, expected in zip(imported_reader, batches):\n        assert batch.equals(expected)\n\n\ndef test_roundtrip_batch_reader_capsule():\n    batch = make_batch()\n\n    capsule = batch.__arrow_c_stream__()\n    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1\n    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)\n    assert imported_reader.schema == batch.schema\n    assert imported_reader.read_next_batch().equals(batch)\n    with pytest.raises(StopIteration):\n        imported_reader.read_next_batch()\n", "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport contextlib\nimport os\nimport shutil\nimport subprocess\nimport weakref\nfrom uuid import uuid4, UUID\nimport sys\n\nimport numpy as np\nimport pyarrow as pa\nfrom pyarrow.vendored.version import Version\n\nimport pytest\n\n\n@contextlib.contextmanager\ndef registered_extension_type(ext_type):\n    pa.register_extension_type(ext_type)\n    try:\n        yield\n    finally:\n        pa.unregister_extension_type(ext_type.extension_name)\n\n\n@contextlib.contextmanager\ndef enabled_auto_load():\n    pa.PyExtensionType.set_auto_load(True)\n    try:\n        yield\n    finally:\n        pa.PyExtensionType.set_auto_load(False)\n\n\nclass TinyIntType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int8(), 'pyarrow.tests.TinyIntType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == pa.int8()\n        return cls()\n\n\nclass IntegerType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int64(), 'pyarrow.tests.IntegerType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == pa.int64()\n        return cls()\n\n\nclass IntegerEmbeddedType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(IntegerType(), 'pyarrow.tests.IntegerType')\n\n    def __arrow_ext_serialize__(self):\n        # XXX pa.BaseExtensionType should expose C++ serialization method\n        return self.storage_type.__arrow_ext_serialize__()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        deserialized_storage_type = storage_type.__arrow_ext_deserialize__(\n            serialized)\n        assert deserialized_storage_type == storage_type\n        return cls()\n\n\nclass UuidScalarType(pa.ExtensionScalar):\n    def as_py(self):\n        return None if self.value is None else UUID(bytes=self.value.as_py())\n\n\nclass UuidType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.binary(16), 'pyarrow.tests.UuidType')\n\n    def __arrow_ext_scalar_class__(self):\n        return UuidScalarType\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return cls()\n\n\nclass UuidType2(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.binary(16), 'pyarrow.tests.UuidType2')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return cls()\n\n\nclass LabelType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.string(), 'pyarrow.tests.LabelType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        return cls()\n\n\nclass ParamExtType(pa.ExtensionType):\n\n    def __init__(self, width):\n        self._width = width\n        super().__init__(pa.binary(width), 'pyarrow.tests.ParamExtType')\n\n    @property\n    def width(self):\n        return self._width\n\n    def __arrow_ext_serialize__(self):\n        return str(self._width).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        width = int(serialized.decode())\n        assert storage_type == pa.binary(width)\n        return cls(width)\n\n\nclass MyStructType(pa.ExtensionType):\n    storage_type = pa.struct([('left', pa.int64()),\n                              ('right', pa.int64())])\n\n    def __init__(self):\n        super().__init__(self.storage_type, 'pyarrow.tests.MyStructType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == cls.storage_type\n        return cls()\n\n\nclass MyListType(pa.ExtensionType):\n\n    def __init__(self, storage_type):\n        assert isinstance(storage_type, pa.ListType)\n        super().__init__(storage_type, 'pyarrow.tests.MyListType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        return cls(storage_type)\n\n\nclass AnnotatedType(pa.ExtensionType):\n    \"\"\"\n    Generic extension type that can store any storage type.\n    \"\"\"\n\n    def __init__(self, storage_type, annotation):\n        self.annotation = annotation\n        super().__init__(storage_type, 'pyarrow.tests.AnnotatedType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        return cls(storage_type)\n\n\nclass LegacyIntType(pa.PyExtensionType):\n\n    def __init__(self):\n        pa.PyExtensionType.__init__(self, pa.int8())\n\n    def __reduce__(self):\n        return LegacyIntType, ()\n\n\ndef ipc_write_batch(batch):\n    stream = pa.BufferOutputStream()\n    writer = pa.RecordBatchStreamWriter(stream, batch.schema)\n    writer.write_batch(batch)\n    writer.close()\n    return stream.getvalue()\n\n\ndef ipc_read_batch(buf):\n    reader = pa.RecordBatchStreamReader(buf)\n    return reader.read_next_batch()\n\n\ndef test_ext_type_basics():\n    ty = UuidType()\n    assert ty.extension_name == \"pyarrow.tests.UuidType\"\n\n\ndef test_ext_type_str():\n    ty = IntegerType()\n    expected = \"extension<pyarrow.tests.IntegerType<IntegerType>>\"\n    assert str(ty) == expected\n    assert pa.DataType.__str__(ty) == expected\n\n\ndef test_ext_type_repr():\n    ty = IntegerType()\n    assert repr(ty) == \"IntegerType(DataType(int64))\"\n\n\ndef test_ext_type__lifetime():\n    ty = UuidType()\n    wr = weakref.ref(ty)\n    del ty\n    assert wr() is None\n\n\ndef test_ext_type__storage_type():\n    ty = UuidType()\n    assert ty.storage_type == pa.binary(16)\n    assert ty.__class__ is UuidType\n    ty = ParamExtType(5)\n    assert ty.storage_type == pa.binary(5)\n    assert ty.__class__ is ParamExtType\n\n\ndef test_ext_type_as_py():\n    ty = UuidType()\n    expected = uuid4()\n    scalar = pa.ExtensionScalar.from_storage(ty, expected.bytes)\n    assert scalar.as_py() == expected\n\n    # test array\n    uuids = [uuid4() for _ in range(3)]\n    storage = pa.array([uuid.bytes for uuid in uuids], type=pa.binary(16))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    # Works for __get_item__\n    for i, expected in enumerate(uuids):\n        assert arr[i].as_py() == expected\n\n    # Works for __iter__\n    for result, expected in zip(arr, uuids):\n        assert result.as_py() == expected\n\n    # test chunked array\n    data = [\n        pa.ExtensionArray.from_storage(ty, storage),\n        pa.ExtensionArray.from_storage(ty, storage)\n    ]\n    carr = pa.chunked_array(data)\n    for i, expected in enumerate(uuids + uuids):\n        assert carr[i].as_py() == expected\n\n    for result, expected in zip(carr, uuids + uuids):\n        assert result.as_py() == expected\n\n\ndef test_uuid_type_pickle(pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        ty = UuidType()\n        ser = pickle_module.dumps(ty, protocol=proto)\n        del ty\n        ty = pickle_module.loads(ser)\n        wr = weakref.ref(ty)\n        assert ty.extension_name == \"pyarrow.tests.UuidType\"\n        del ty\n        assert wr() is None\n\n\ndef test_ext_type_equality():\n    a = ParamExtType(5)\n    b = ParamExtType(6)\n    c = ParamExtType(6)\n    assert a != b\n    assert b == c\n    d = UuidType()\n    e = UuidType()\n    assert a != d\n    assert d == e\n\n\ndef test_ext_array_basics():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n    arr.validate()\n    assert arr.type is ty\n    assert arr.storage.equals(storage)\n\n\ndef test_ext_array_lifetime():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    refs = [weakref.ref(ty), weakref.ref(arr), weakref.ref(storage)]\n    del ty, storage, arr\n    for ref in refs:\n        assert ref() is None\n\n\ndef test_ext_array_to_pylist():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\", None], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n\n    assert arr.to_pylist() == [b\"foo\", b\"bar\", None]\n\n\ndef test_ext_array_errors():\n    ty = ParamExtType(4)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    with pytest.raises(TypeError, match=\"Incompatible storage type\"):\n        pa.ExtensionArray.from_storage(ty, storage)\n\n\ndef test_ext_array_equality():\n    storage1 = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    storage2 = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    storage3 = pa.array([], type=pa.binary(16))\n    ty1 = UuidType()\n    ty2 = ParamExtType(16)\n\n    a = pa.ExtensionArray.from_storage(ty1, storage1)\n    b = pa.ExtensionArray.from_storage(ty1, storage2)\n    assert a.equals(b)\n    c = pa.ExtensionArray.from_storage(ty1, storage3)\n    assert not a.equals(c)\n    d = pa.ExtensionArray.from_storage(ty2, storage1)\n    assert not a.equals(d)\n    e = pa.ExtensionArray.from_storage(ty2, storage2)\n    assert d.equals(e)\n    f = pa.ExtensionArray.from_storage(ty2, storage3)\n    assert not d.equals(f)\n\n\ndef test_ext_array_wrap_array():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\", None], type=pa.binary(3))\n    arr = ty.wrap_array(storage)\n    arr.validate(full=True)\n    assert isinstance(arr, pa.ExtensionArray)\n    assert arr.type == ty\n    assert arr.storage == storage\n\n    storage = pa.chunked_array([[b\"abc\", b\"def\"], [b\"ghi\"]],\n                               type=pa.binary(3))\n    arr = ty.wrap_array(storage)\n    arr.validate(full=True)\n    assert isinstance(arr, pa.ChunkedArray)\n    assert arr.type == ty\n    assert arr.chunk(0).storage == storage.chunk(0)\n    assert arr.chunk(1).storage == storage.chunk(1)\n\n    # Wrong storage type\n    storage = pa.array([b\"foo\", b\"bar\", None])\n    with pytest.raises(TypeError, match=\"Incompatible storage type\"):\n        ty.wrap_array(storage)\n\n    # Not an array or chunked array\n    with pytest.raises(TypeError, match=\"Expected array or chunked array\"):\n        ty.wrap_array(None)\n\n\ndef test_ext_scalar_from_array():\n    data = [b\"0123456789abcdef\", b\"0123456789abcdef\",\n            b\"zyxwvutsrqponmlk\", None]\n    storage = pa.array(data, type=pa.binary(16))\n    ty1 = UuidType()\n    ty2 = ParamExtType(16)\n    ty3 = UuidType2()\n\n    a = pa.ExtensionArray.from_storage(ty1, storage)\n    b = pa.ExtensionArray.from_storage(ty2, storage)\n    c = pa.ExtensionArray.from_storage(ty3, storage)\n\n    scalars_a = list(a)\n    assert len(scalars_a) == 4\n\n    assert ty1.__arrow_ext_scalar_class__() == UuidScalarType\n    assert isinstance(a[0], UuidScalarType)\n    assert isinstance(scalars_a[0], UuidScalarType)\n\n    for s, val in zip(scalars_a, data):\n        assert isinstance(s, pa.ExtensionScalar)\n        assert s.is_valid == (val is not None)\n        assert s.type == ty1\n        if val is not None:\n            assert s.value == pa.scalar(val, storage.type)\n            assert s.as_py() == UUID(bytes=val)\n        else:\n            assert s.value is None\n\n    scalars_b = list(b)\n    assert len(scalars_b) == 4\n\n    for sa, sb in zip(scalars_a, scalars_b):\n        assert isinstance(sb, pa.ExtensionScalar)\n        assert sa.is_valid == sb.is_valid\n        if sa.as_py() is None:\n            assert sa.as_py() == sb.as_py()\n        else:\n            assert sa.as_py().bytes == sb.as_py()\n        assert sa != sb\n\n    scalars_c = list(c)\n    assert len(scalars_c) == 4\n\n    for s, val in zip(scalars_c, data):\n        assert isinstance(s, pa.ExtensionScalar)\n        assert s.is_valid == (val is not None)\n        assert s.type == ty3\n        if val is not None:\n            assert s.value == pa.scalar(val, storage.type)\n            assert s.as_py() == val\n        else:\n            assert s.value is None\n\n    assert a.to_pylist() == [UUID(bytes=x) if x else None for x in data]\n\n\ndef test_ext_scalar_from_storage():\n    ty = UuidType()\n\n    s = pa.ExtensionScalar.from_storage(ty, None)\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is False\n    assert s.value is None\n\n    s = pa.ExtensionScalar.from_storage(ty, b\"0123456789abcdef\")\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is True\n    assert s.value == pa.scalar(b\"0123456789abcdef\", ty.storage_type)\n\n    s = pa.ExtensionScalar.from_storage(ty, pa.scalar(None, ty.storage_type))\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is False\n    assert s.value is None\n\n    s = pa.ExtensionScalar.from_storage(\n        ty, pa.scalar(b\"0123456789abcdef\", ty.storage_type))\n    assert isinstance(s, pa.ExtensionScalar)\n    assert s.type == ty\n    assert s.is_valid is True\n    assert s.value == pa.scalar(b\"0123456789abcdef\", ty.storage_type)\n\n\ndef test_ext_array_pickling(pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        ty = ParamExtType(3)\n        storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n        arr = pa.ExtensionArray.from_storage(ty, storage)\n        ser = pickle_module.dumps(arr, protocol=proto)\n        del ty, storage, arr\n        arr = pickle_module.loads(ser)\n        arr.validate()\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type == ParamExtType(3)\n        assert arr.type.storage_type == pa.binary(3)\n        assert arr.storage.type == pa.binary(3)\n        assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n\n\ndef test_ext_array_conversion_to_numpy():\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    result = arr1.to_numpy()\n    expected = np.array([1, 2, 3], dtype=\"int64\")\n    np.testing.assert_array_equal(result, expected)\n\n    with pytest.raises(ValueError, match=\"zero_copy_only was True\"):\n        arr2.to_numpy()\n    result = arr2.to_numpy(zero_copy_only=False)\n    expected = np.array([b\"123\", b\"456\", b\"789\"])\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_ext_array_conversion_to_pandas():\n    import pandas as pd\n\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    result = arr1.to_pandas()\n    expected = pd.Series([1, 2, 3], dtype=\"int64\")\n    pd.testing.assert_series_equal(result, expected)\n\n    result = arr2.to_pandas()\n    expected = pd.Series([b\"123\", b\"456\", b\"789\"], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n\n@pytest.fixture\ndef struct_w_ext_data():\n    storage1 = pa.array([1, 2, 3], type=pa.int64())\n    storage2 = pa.array([b\"123\", b\"456\", b\"789\"], type=pa.binary(3))\n    ty1 = IntegerType()\n    ty2 = ParamExtType(3)\n\n    arr1 = pa.ExtensionArray.from_storage(ty1, storage1)\n    arr2 = pa.ExtensionArray.from_storage(ty2, storage2)\n\n    sarr1 = pa.StructArray.from_arrays([arr1], [\"f0\"])\n    sarr2 = pa.StructArray.from_arrays([arr2], [\"f1\"])\n\n    return [sarr1, sarr2]\n\n\ndef test_struct_w_ext_array_to_numpy(struct_w_ext_data):\n    # ARROW-15291\n    # Check that we don't segfault when trying to build\n    # a numpy array from a StructArray with a field being\n    # an ExtensionArray\n\n    result = struct_w_ext_data[0].to_numpy(zero_copy_only=False)\n    expected = np.array([{'f0': 1}, {'f0': 2},\n                         {'f0': 3}], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n    result = struct_w_ext_data[1].to_numpy(zero_copy_only=False)\n    expected = np.array([{'f1': b'123'}, {'f1': b'456'},\n                         {'f1': b'789'}], dtype=object)\n    np.testing.assert_array_equal(result, expected)\n\n\n@pytest.mark.pandas\ndef test_struct_w_ext_array_to_pandas(struct_w_ext_data):\n    # ARROW-15291\n    # Check that we don't segfault when trying to build\n    # a Pandas dataframe from a StructArray with a field\n    # being an ExtensionArray\n    import pandas as pd\n\n    result = struct_w_ext_data[0].to_pandas()\n    expected = pd.Series([{'f0': 1}, {'f0': 2},\n                         {'f0': 3}], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n    result = struct_w_ext_data[1].to_pandas()\n    expected = pd.Series([{'f1': b'123'}, {'f1': b'456'},\n                         {'f1': b'789'}], dtype=object)\n    pd.testing.assert_series_equal(result, expected)\n\n\ndef test_cast_kernel_on_extension_arrays():\n    # test array casting\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(IntegerType(), storage)\n\n    # test that no allocation happens during identity cast\n    allocated_before_cast = pa.total_allocated_bytes()\n    casted = arr.cast(pa.int64())\n    assert pa.total_allocated_bytes() == allocated_before_cast\n\n    cases = [\n        (pa.int64(), pa.Int64Array),\n        (pa.int32(), pa.Int32Array),\n        (pa.int16(), pa.Int16Array),\n        (pa.uint64(), pa.UInt64Array),\n        (pa.uint32(), pa.UInt32Array),\n        (pa.uint16(), pa.UInt16Array)\n    ]\n    for typ, klass in cases:\n        casted = arr.cast(typ)\n        assert casted.type == typ\n        assert isinstance(casted, klass)\n\n    # test chunked array casting\n    arr = pa.chunked_array([arr, arr])\n    casted = arr.cast(pa.int16())\n    assert casted.type == pa.int16()\n    assert isinstance(casted, pa.ChunkedArray)\n\n\n@pytest.mark.parametrize(\"data,ty\", (\n    ([1, 2], pa.int32),\n    ([1, 2], pa.int64),\n    ([\"1\", \"2\"], pa.string),\n    ([b\"1\", b\"2\"], pa.binary),\n    ([1.0, 2.0], pa.float32),\n    ([1.0, 2.0], pa.float64)\n))\ndef test_casting_to_extension_type(data, ty):\n    arr = pa.array(data, ty())\n    out = arr.cast(IntegerType())\n    assert isinstance(out, pa.ExtensionArray)\n    assert out.type == IntegerType()\n    assert out.to_pylist() == [1, 2]\n\n\ndef test_cast_between_extension_types():\n    array = pa.array([1, 2, 3], pa.int8())\n\n    tiny_int_arr = array.cast(TinyIntType())\n    assert tiny_int_arr.type == TinyIntType()\n\n    # Casting between extension types w/ different storage types not okay.\n    msg = (\"Casting from 'extension<.*?<TinyIntType>>' \"\n           \"to different extension type \"\n           \"'extension<.*?<IntegerType>>' not permitted. \"\n           \"One can first cast to the storage type, \"\n           \"then to the extension type.\"\n           )\n    with pytest.raises(TypeError, match=msg):\n        tiny_int_arr.cast(IntegerType())\n    tiny_int_arr.cast(pa.int64()).cast(IntegerType())\n\n    # Between the same extension types is okay\n    array = pa.array([b'1' * 16, b'2' * 16], pa.binary(16)).cast(UuidType())\n    out = array.cast(UuidType())\n    assert out.type == UuidType()\n\n    # Will still fail casting between extensions who share storage type,\n    # can only cast between exactly the same extension types.\n    with pytest.raises(TypeError, match='Casting from *'):\n        array.cast(UuidType2())\n\n\ndef test_cast_to_extension_with_extension_storage():\n    # Test casting directly, and IntegerType -> IntegerEmbeddedType\n    array = pa.array([1, 2, 3], pa.int64())\n    array.cast(IntegerEmbeddedType())\n    array.cast(IntegerType()).cast(IntegerEmbeddedType())\n\n\n@pytest.mark.parametrize(\"data,type_factory\", (\n    # list<extension>\n    ([[1, 2, 3]], lambda: pa.list_(IntegerType())),\n    # struct<extension>\n    ([{\"foo\": 1}], lambda: pa.struct([(\"foo\", IntegerType())])),\n    # list<struct<extension>>\n    ([[{\"foo\": 1}]], lambda: pa.list_(pa.struct([(\"foo\", IntegerType())]))),\n    # struct<list<extension>>\n    ([{\"foo\": [1, 2, 3]}], lambda: pa.struct(\n        [(\"foo\", pa.list_(IntegerType()))])),\n))\ndef test_cast_nested_extension_types(data, type_factory):\n    ty = type_factory()\n    a = pa.array(data)\n    b = a.cast(ty)\n    assert b.type == ty  # casted to target extension\n    assert b.cast(a.type)  # and can cast back\n\n\ndef test_casting_dict_array_to_extension_type():\n    storage = pa.array([b\"0123456789abcdef\"], type=pa.binary(16))\n    arr = pa.ExtensionArray.from_storage(UuidType(), storage)\n    dict_arr = pa.DictionaryArray.from_arrays(pa.array([0, 0], pa.int32()),\n                                              arr)\n    out = dict_arr.cast(UuidType())\n    assert isinstance(out, pa.ExtensionArray)\n    assert out.to_pylist() == [UUID('30313233-3435-3637-3839-616263646566'),\n                               UUID('30313233-3435-3637-3839-616263646566')]\n\n\ndef test_concat():\n    arr1 = pa.array([1, 2, 3], IntegerType())\n    arr2 = pa.array([4, 5, 6], IntegerType())\n\n    result = pa.concat_arrays([arr1, arr2])\n    expected = pa.array([1, 2, 3, 4, 5, 6], IntegerType())\n    assert result.equals(expected)\n\n    # nested in a struct\n    struct_arr1 = pa.StructArray.from_arrays([arr1], names=[\"a\"])\n    struct_arr2 = pa.StructArray.from_arrays([arr2], names=[\"a\"])\n    result = pa.concat_arrays([struct_arr1, struct_arr2])\n    expected = pa.StructArray.from_arrays([expected], names=[\"a\"])\n    assert result.equals(expected)\n\n\ndef test_null_storage_type():\n    ext_type = AnnotatedType(pa.null(), {\"key\": \"value\"})\n    storage = pa.array([None] * 10, pa.null())\n    arr = pa.ExtensionArray.from_storage(ext_type, storage)\n    assert arr.null_count == 10\n    arr.validate(full=True)\n\n\ndef example_batch():\n    ty = ParamExtType(3)\n    storage = pa.array([b\"foo\", b\"bar\"], type=pa.binary(3))\n    arr = pa.ExtensionArray.from_storage(ty, storage)\n    return pa.RecordBatch.from_arrays([arr], [\"exts\"])\n\n\ndef check_example_batch(batch, *, expect_extension):\n    arr = batch.column(0)\n    if expect_extension:\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type.storage_type == pa.binary(3)\n        assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]\n    else:\n        assert arr.type == pa.binary(3)\n        assert arr.to_pylist() == [b\"foo\", b\"bar\"]\n    return arr\n\n\ndef test_ipc_unregistered():\n    batch = example_batch()\n    buf = ipc_write_batch(batch)\n    del batch\n\n    batch = ipc_read_batch(buf)\n    batch.validate(full=True)\n    check_example_batch(batch, expect_extension=False)\n\n\ndef test_ipc_registered():\n    with registered_extension_type(ParamExtType(1)):\n        batch = example_batch()\n        buf = ipc_write_batch(batch)\n        del batch\n\n        batch = ipc_read_batch(buf)\n        batch.validate(full=True)\n        arr = check_example_batch(batch, expect_extension=True)\n        assert arr.type == ParamExtType(3)\n\n\nclass PeriodArray(pa.ExtensionArray):\n    pass\n\n\nclass PeriodType(pa.ExtensionType):\n    def __init__(self, freq):\n        # attributes need to be set first before calling\n        # super init (as that calls serialize)\n        self._freq = freq\n        pa.ExtensionType.__init__(self, pa.int64(), 'test.period')\n\n    @property\n    def freq(self):\n        return self._freq\n\n    def __arrow_ext_serialize__(self):\n        return \"freq={}\".format(self.freq).encode()\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        serialized = serialized.decode()\n        assert serialized.startswith(\"freq=\")\n        freq = serialized.split('=')[1]\n        return PeriodType(freq)\n\n    def __eq__(self, other):\n        if isinstance(other, pa.BaseExtensionType):\n            return (isinstance(self, type(other)) and\n                    self.freq == other.freq)\n        else:\n            return NotImplemented\n\n\nclass PeriodTypeWithClass(PeriodType):\n    def __init__(self, freq):\n        PeriodType.__init__(self, freq)\n\n    def __arrow_ext_class__(self):\n        return PeriodArray\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        freq = PeriodType.__arrow_ext_deserialize__(\n            storage_type, serialized).freq\n        return PeriodTypeWithClass(freq)\n\n\nclass PeriodTypeWithToPandasDtype(PeriodType):\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        freq = PeriodType.__arrow_ext_deserialize__(\n            storage_type, serialized).freq\n        return PeriodTypeWithToPandasDtype(freq)\n\n    def to_pandas_dtype(self):\n        import pandas as pd\n        return pd.PeriodDtype(freq=self.freq)\n\n\n@pytest.fixture(params=[PeriodType('D'),\n                        PeriodTypeWithClass('D'),\n                        PeriodTypeWithToPandasDtype('D')])\ndef registered_period_type(request):\n    # setup\n    period_type = request.param\n    period_class = period_type.__arrow_ext_class__()\n    pa.register_extension_type(period_type)\n    yield period_type, period_class\n    # teardown\n    try:\n        pa.unregister_extension_type('test.period')\n    except KeyError:\n        pass\n\n\ndef test_generic_ext_type():\n    period_type = PeriodType('D')\n    assert period_type.extension_name == \"test.period\"\n    assert period_type.storage_type == pa.int64()\n    # default ext_class expected.\n    assert period_type.__arrow_ext_class__() == pa.ExtensionArray\n\n\ndef test_generic_ext_type_ipc(registered_period_type):\n    period_type, period_class = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n    # check the built array has exactly the expected clss\n    assert isinstance(arr, period_class)\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n\n    result = batch.column(0)\n    # check the deserialized array class is the expected one\n    assert isinstance(result, period_class)\n    assert result.type.extension_name == \"test.period\"\n    assert arr.storage.to_pylist() == [1, 2, 3, 4]\n\n    # we get back an actual PeriodType\n    assert isinstance(result.type, PeriodType)\n    assert result.type.freq == 'D'\n    assert result.type == period_type\n\n    # using different parametrization as how it was registered\n    period_type_H = period_type.__class__('H')\n    assert period_type_H.extension_name == \"test.period\"\n    assert period_type_H.freq == 'H'\n\n    arr = pa.ExtensionArray.from_storage(period_type_H, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n    result = batch.column(0)\n    assert isinstance(result.type, PeriodType)\n    assert result.type.freq == 'H'\n    assert isinstance(result, period_class)\n\n\ndef test_generic_ext_type_ipc_unknown(registered_period_type):\n    period_type, _ = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    buf = ipc_write_batch(batch)\n    del batch\n\n    # unregister type before loading again => reading unknown extension type\n    # as plain array (but metadata in schema's field are preserved)\n    pa.unregister_extension_type('test.period')\n\n    batch = ipc_read_batch(buf)\n    result = batch.column(0)\n\n    assert isinstance(result, pa.Int64Array)\n    ext_field = batch.schema.field('ext')\n    assert ext_field.metadata == {\n        b'ARROW:extension:metadata': b'freq=D',\n        b'ARROW:extension:name': b'test.period'\n    }\n\n\ndef test_generic_ext_type_equality():\n    period_type = PeriodType('D')\n    assert period_type.extension_name == \"test.period\"\n\n    period_type2 = PeriodType('D')\n    period_type3 = PeriodType('H')\n    assert period_type == period_type2\n    assert not period_type == period_type3\n\n\ndef test_generic_ext_type_pickling(registered_period_type, pickle_module):\n    # GH-36038\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        period_type, _ = registered_period_type\n        ser = pickle_module.dumps(period_type, protocol=proto)\n        period_type_pickled = pickle_module.loads(ser)\n        assert period_type == period_type_pickled\n\n\ndef test_generic_ext_array_pickling(registered_period_type, pickle_module):\n    for proto in range(0, pickle_module.HIGHEST_PROTOCOL + 1):\n        period_type, _ = registered_period_type\n        storage = pa.array([1, 2, 3, 4], pa.int64())\n        arr = pa.ExtensionArray.from_storage(period_type, storage)\n        ser = pickle_module.dumps(arr, protocol=proto)\n        del storage, arr\n        arr = pickle_module.loads(ser)\n        arr.validate()\n        assert isinstance(arr, pa.ExtensionArray)\n        assert arr.type == period_type\n        assert arr.type.storage_type == pa.int64()\n        assert arr.storage.type == pa.int64()\n        assert arr.storage.to_pylist() == [1, 2, 3, 4]\n\n\ndef test_generic_ext_type_register(registered_period_type):\n    # test that trying to register other type does not segfault\n    with pytest.raises(TypeError):\n        pa.register_extension_type(pa.string())\n\n    # register second time raises KeyError\n    period_type = PeriodType('D')\n    with pytest.raises(KeyError):\n        pa.register_extension_type(period_type)\n\n\n@pytest.mark.parquet\ndef test_parquet_period(tmpdir, registered_period_type):\n    # Parquet support for primitive extension types\n    period_type, period_class = registered_period_type\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n    table = pa.table([arr], names=[\"ext\"])\n\n    import pyarrow.parquet as pq\n\n    filename = tmpdir / 'period_extension_type.parquet'\n    pq.write_table(table, filename)\n\n    # Stored in parquet as storage type but with extension metadata saved\n    # in the serialized arrow schema\n    meta = pq.read_metadata(filename)\n    assert meta.schema.column(0).physical_type == \"INT64\"\n    assert b\"ARROW:schema\" in meta.metadata\n\n    import base64\n    decoded_schema = base64.b64decode(meta.metadata[b\"ARROW:schema\"])\n    schema = pa.ipc.read_schema(pa.BufferReader(decoded_schema))\n    # Since the type could be reconstructed, the extension type metadata is\n    # absent.\n    assert schema.field(\"ext\").metadata == {}\n\n    # When reading in, properly create extension type if it is registered\n    result = pq.read_table(filename)\n    result.validate(full=True)\n    assert result.schema.field(\"ext\").type == period_type\n    assert result.schema.field(\"ext\").metadata == {}\n    # Get the exact array class defined by the registered type.\n    result_array = result.column(\"ext\").chunk(0)\n    assert type(result_array) is period_class\n\n    # When the type is not registered, read in as storage type\n    pa.unregister_extension_type(period_type.extension_name)\n    result = pq.read_table(filename)\n    result.validate(full=True)\n    assert result.schema.field(\"ext\").type == pa.int64()\n    # The extension metadata is present for roundtripping.\n    assert result.schema.field(\"ext\").metadata == {\n        b'ARROW:extension:metadata': b'freq=D',\n        b'ARROW:extension:name': b'test.period'\n    }\n\n\n@pytest.mark.parquet\ndef test_parquet_extension_with_nested_storage(tmpdir):\n    # Parquet support for extension types with nested storage type\n    import pyarrow.parquet as pq\n\n    struct_array = pa.StructArray.from_arrays(\n        [pa.array([0, 1], type=\"int64\"), pa.array([4, 5], type=\"int64\")],\n        names=[\"left\", \"right\"])\n    list_array = pa.array([[1, 2, 3], [4, 5]], type=pa.list_(pa.int32()))\n\n    mystruct_array = pa.ExtensionArray.from_storage(MyStructType(),\n                                                    struct_array)\n    mylist_array = pa.ExtensionArray.from_storage(\n        MyListType(list_array.type), list_array)\n\n    orig_table = pa.table({'structs': mystruct_array,\n                           'lists': mylist_array})\n    filename = tmpdir / 'nested_extension_storage.parquet'\n    pq.write_table(orig_table, filename)\n\n    # Unregistered\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column('structs').type == struct_array.type\n    assert table.column('structs').combine_chunks() == struct_array\n    assert table.column('lists').type == list_array.type\n    assert table.column('lists').combine_chunks() == list_array\n\n    # Registered\n    with registered_extension_type(mystruct_array.type):\n        with registered_extension_type(mylist_array.type):\n            table = pq.read_table(filename)\n            table.validate(full=True)\n            assert table.column('structs').type == mystruct_array.type\n            assert table.column('lists').type == mylist_array.type\n            assert table == orig_table\n\n            # Cannot select a subfield of an extension type with\n            # a struct storage type.\n            with pytest.raises(pa.ArrowInvalid,\n                               match='without all of its fields'):\n                pq.ParquetFile(filename).read(columns=['structs.left'])\n\n\n@pytest.mark.parquet\ndef test_parquet_nested_extension(tmpdir):\n    # Parquet support for extension types nested in struct or list\n    import pyarrow.parquet as pq\n\n    ext_type = IntegerType()\n    storage = pa.array([4, 5, 6, 7], type=pa.int64())\n    ext_array = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    # Struct of extensions\n    struct_array = pa.StructArray.from_arrays(\n        [storage, ext_array],\n        names=['ints', 'exts'])\n\n    orig_table = pa.table({'structs': struct_array})\n    filename = tmpdir / 'struct_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column(0).type == pa.struct({'ints': pa.int64(),\n                                              'exts': pa.int64()})\n    with registered_extension_type(ext_type):\n        table = pq.read_table(filename)\n        table.validate(full=True)\n        assert table.column(0).type == struct_array.type\n        assert table == orig_table\n\n    # List of extensions\n    list_array = pa.ListArray.from_arrays([0, 1, None, 3], ext_array)\n\n    orig_table = pa.table({'lists': list_array})\n    filename = tmpdir / 'list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column(0).type == pa.list_(pa.int64())\n    with registered_extension_type(ext_type):\n        table = pq.read_table(filename)\n        table.validate(full=True)\n        assert table.column(0).type == list_array.type\n        assert table == orig_table\n\n    # Large list of extensions\n    list_array = pa.LargeListArray.from_arrays([0, 1, None, 3], ext_array)\n\n    orig_table = pa.table({'lists': list_array})\n    filename = tmpdir / 'list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    table.validate(full=True)\n    assert table.column(0).type == pa.large_list(pa.int64())\n    with registered_extension_type(ext_type):\n        table = pq.read_table(filename)\n        table.validate(full=True)\n        assert table.column(0).type == list_array.type\n        assert table == orig_table\n\n\n@pytest.mark.parquet\ndef test_parquet_extension_nested_in_extension(tmpdir):\n    # Parquet support for extension<list<extension>>\n    import pyarrow.parquet as pq\n\n    inner_ext_type = IntegerType()\n    inner_storage = pa.array([4, 5, 6, 7], type=pa.int64())\n    inner_ext_array = pa.ExtensionArray.from_storage(inner_ext_type,\n                                                     inner_storage)\n\n    list_array = pa.ListArray.from_arrays([0, 1, None, 3], inner_ext_array)\n    mylist_array = pa.ExtensionArray.from_storage(\n        MyListType(list_array.type), list_array)\n\n    orig_table = pa.table({'lists': mylist_array})\n    filename = tmpdir / 'ext_of_list_of_ext.parquet'\n    pq.write_table(orig_table, filename)\n\n    table = pq.read_table(filename)\n    assert table.column(0).type == pa.list_(pa.int64())\n    with registered_extension_type(mylist_array.type):\n        with registered_extension_type(inner_ext_array.type):\n            table = pq.read_table(filename)\n            assert table.column(0).type == mylist_array.type\n            assert table == orig_table\n\n\ndef test_to_numpy():\n    period_type = PeriodType('D')\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n\n    expected = storage.to_numpy()\n    result = arr.to_numpy()\n    np.testing.assert_array_equal(result, expected)\n\n    result = np.asarray(arr)\n    np.testing.assert_array_equal(result, expected)\n\n    # chunked array\n    a1 = pa.chunked_array([arr, arr])\n    a2 = pa.chunked_array([arr, arr], type=period_type)\n    expected = np.hstack([expected, expected])\n\n    for charr in [a1, a2]:\n        assert charr.type == period_type\n        for result in [np.asarray(charr), charr.to_numpy()]:\n            assert result.dtype == np.int64\n            np.testing.assert_array_equal(result, expected)\n\n    # zero chunks\n    charr = pa.chunked_array([], type=period_type)\n    assert charr.type == period_type\n\n    for result in [np.asarray(charr), charr.to_numpy()]:\n        assert result.dtype == np.int64\n        np.testing.assert_array_equal(result, np.array([], dtype='int64'))\n\n\ndef test_empty_take():\n    # https://issues.apache.org/jira/browse/ARROW-13474\n    ext_type = IntegerType()\n    storage = pa.array([], type=pa.int64())\n    empty_arr = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = empty_arr.filter(pa.array([], pa.bool_()))\n    assert len(result) == 0\n    assert result.equals(empty_arr)\n\n    result = empty_arr.take(pa.array([], pa.int32()))\n    assert len(result) == 0\n    assert result.equals(empty_arr)\n\n\n@pytest.mark.parametrize(\"data,ty\", (\n    ([1, 2, 3], IntegerType),\n    ([\"cat\", \"dog\", \"horse\"], LabelType)\n))\n@pytest.mark.parametrize(\n    \"into\", [\"to_numpy\", pytest.param(\"to_pandas\", marks=pytest.mark.pandas)])\ndef test_extension_array_to_numpy_pandas(data, ty, into):\n    storage = pa.array(data)\n    ext_arr = pa.ExtensionArray.from_storage(ty(), storage)\n    offsets = pa.array([0, 1, 2, 3])\n    list_arr = pa.ListArray.from_arrays(offsets, ext_arr)\n    result = getattr(list_arr, into)(zero_copy_only=False)\n\n    list_arr_storage_type = list_arr.cast(pa.list_(ext_arr.type.storage_type))\n    expected = getattr(list_arr_storage_type, into)(zero_copy_only=False)\n    if into == \"to_pandas\":\n        assert result.equals(expected)\n    else:\n        assert np.array_equal(result, expected)\n\n\ndef test_array_constructor():\n    ext_type = IntegerType()\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = pa.array([1, 2, 3], type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(np.array([1, 2, 3]), type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(np.array([1.0, 2.0, 3.0]), type=IntegerType())\n    assert result.equals(expected)\n\n\n@pytest.mark.pandas\ndef test_array_constructor_from_pandas():\n    import pandas as pd\n\n    ext_type = IntegerType()\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(ext_type, storage)\n\n    result = pa.array(pd.Series([1, 2, 3]), type=IntegerType())\n    assert result.equals(expected)\n\n    result = pa.array(\n        pd.Series([1, 2, 3], dtype=\"category\"), type=IntegerType()\n    )\n    assert result.equals(expected)\n\n\n@pytest.mark.cython\ndef test_cpp_extension_in_python(tmpdir):\n    from .test_cython import (\n        setup_template, compiler_opts, test_ld_path, test_util, here)\n    with tmpdir.as_cwd():\n        # Set up temporary workspace\n        pyx_file = 'extensions.pyx'\n        shutil.copyfile(os.path.join(here, pyx_file),\n                        os.path.join(str(tmpdir), pyx_file))\n        # Create setup.py file\n        setup_code = setup_template.format(pyx_file=pyx_file,\n                                           compiler_opts=compiler_opts,\n                                           test_ld_path=test_ld_path)\n        with open('setup.py', 'w') as f:\n            f.write(setup_code)\n\n        subprocess_env = test_util.get_modified_env_with_pythonpath()\n\n        # Compile extension module\n        subprocess.check_call([sys.executable, 'setup.py',\n                               'build_ext', '--inplace'],\n                              env=subprocess_env)\n\n    sys.path.insert(0, str(tmpdir))\n    mod = __import__('extensions')\n\n    uuid_type = mod._make_uuid_type()\n    assert uuid_type.extension_name == \"uuid\"\n    assert uuid_type.storage_type == pa.binary(16)\n\n    array = mod._make_uuid_array()\n    assert array.type == uuid_type\n    assert array.to_pylist() == [b'abcdefghijklmno0', b'0onmlkjihgfedcba']\n    assert array[0].as_py() == b'abcdefghijklmno0'\n    assert array[1].as_py() == b'0onmlkjihgfedcba'\n\n    buf = ipc_write_batch(pa.RecordBatch.from_arrays([array], [\"uuid\"]))\n\n    batch = ipc_read_batch(buf)\n    reconstructed_array = batch.column(0)\n    assert reconstructed_array.type == uuid_type\n    assert reconstructed_array == array\n\n\ndef test_tensor_type():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 3])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.int8(), 6)\n    assert tensor_type.shape == [2, 3]\n    assert tensor_type.dim_names is None\n    assert tensor_type.permutation is None\n\n    tensor_type = pa.fixed_shape_tensor(pa.float64(), [2, 2, 3],\n                                        permutation=[0, 2, 1])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.float64(), 12)\n    assert tensor_type.shape == [2, 2, 3]\n    assert tensor_type.dim_names is None\n    assert tensor_type.permutation == [0, 2, 1]\n\n    tensor_type = pa.fixed_shape_tensor(pa.bool_(), [2, 2, 3],\n                                        dim_names=['C', 'H', 'W'])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert tensor_type.storage_type == pa.list_(pa.bool_(), 12)\n    assert tensor_type.shape == [2, 2, 3]\n    assert tensor_type.dim_names == ['C', 'H', 'W']\n    assert tensor_type.permutation is None\n\n\ndef test_tensor_class_methods():\n    tensor_type = pa.fixed_shape_tensor(pa.float32(), [2, 3])\n    storage = pa.array([[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]],\n                       pa.list_(pa.float32(), 6))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    expected = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]], dtype=np.float32)\n    result = arr.to_numpy_ndarray()\n    np.testing.assert_array_equal(result, expected)\n\n    expected = np.array([[[1, 2, 3], [4, 5, 6]]], dtype=np.float32)\n    result = arr[:1].to_numpy_ndarray()\n    np.testing.assert_array_equal(result, expected)\n\n    arr = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]],\n        dtype=np.float32, order=\"C\")\n    tensor_array_from_numpy = pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n    assert isinstance(tensor_array_from_numpy.type, pa.FixedShapeTensorType)\n    assert tensor_array_from_numpy.type.value_type == pa.float32()\n    assert tensor_array_from_numpy.type.shape == [2, 3]\n\n    arr = np.array(\n        [[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]],\n        dtype=np.float32, order=\"F\")\n    with pytest.raises(ValueError, match=\"C-style contiguous segment\"):\n        pa.FixedShapeTensorArray.from_numpy_ndarray(arr)\n\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], permutation=[0, 2, 1])\n    storage = pa.array([[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]], pa.list_(pa.int8(), 12))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    with pytest.raises(ValueError, match=\"non-permuted tensors\"):\n        arr.to_numpy_ndarray()\n\n\n@pytest.mark.parametrize(\"tensor_type\", (\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3]),\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], permutation=[0, 2, 1]),\n    pa.fixed_shape_tensor(pa.int8(), [2, 2, 3], dim_names=['C', 'H', 'W'])\n))\ndef test_tensor_type_ipc(tensor_type):\n    storage = pa.array([[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]], pa.list_(pa.int8(), 12))\n    arr = pa.ExtensionArray.from_storage(tensor_type, storage)\n    batch = pa.RecordBatch.from_arrays([arr], [\"ext\"])\n\n    # check the built array has exactly the expected clss\n    tensor_class = tensor_type.__arrow_ext_class__()\n    assert isinstance(arr, tensor_class)\n\n    buf = ipc_write_batch(batch)\n    del batch\n    batch = ipc_read_batch(buf)\n\n    result = batch.column(0)\n    # check the deserialized array class is the expected one\n    assert isinstance(result, tensor_class)\n    assert result.type.extension_name == \"arrow.fixed_shape_tensor\"\n    assert arr.storage.to_pylist() == [[1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]]\n\n    # we get back an actual TensorType\n    assert isinstance(result.type, pa.FixedShapeTensorType)\n    assert result.type.value_type == pa.int8()\n    assert result.type.shape == [2, 2, 3]\n\n\ndef test_tensor_type_equality():\n    tensor_type = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3])\n    assert tensor_type.extension_name == \"arrow.fixed_shape_tensor\"\n\n    tensor_type2 = pa.fixed_shape_tensor(pa.int8(), [2, 2, 3])\n    tensor_type3 = pa.fixed_shape_tensor(pa.uint8(), [2, 2, 3])\n    assert tensor_type == tensor_type2\n    assert not tensor_type == tensor_type3\n\n\n@pytest.mark.pandas\ndef test_extension_to_pandas_storage_type(registered_period_type):\n    period_type, _ = registered_period_type\n    np_arr = np.array([1, 2, 3, 4], dtype='i8')\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(period_type, storage)\n\n    if isinstance(period_type, PeriodTypeWithToPandasDtype):\n        pandas_dtype = period_type.to_pandas_dtype()\n    else:\n        pandas_dtype = np_arr.dtype\n\n    # Test arrays\n    result = arr.to_pandas()\n    assert result.dtype == pandas_dtype\n\n    # Test chunked arrays\n    chunked_arr = pa.chunked_array([arr])\n    result = chunked_arr.to_numpy()\n    assert result.dtype == np_arr.dtype\n\n    result = chunked_arr.to_pandas()\n    assert result.dtype == pandas_dtype\n\n    # Test Table.to_pandas\n    data = [\n        pa.array([1, 2, 3, 4]),\n        pa.array(['foo', 'bar', None, None]),\n        pa.array([True, None, True, False]),\n        arr\n    ]\n    my_schema = pa.schema([('f0', pa.int8()),\n                           ('f1', pa.string()),\n                           ('f2', pa.bool_()),\n                           ('ext', period_type)])\n    table = pa.Table.from_arrays(data, schema=my_schema)\n    result = table.to_pandas()\n    assert result[\"ext\"].dtype == pandas_dtype\n\n    import pandas as pd\n    # Skip tests for 2.0.x, See: GH-35821\n    if (\n        Version(pd.__version__) >= Version(\"2.1.0\")\n    ):\n        # Check the usage of types_mapper\n        result = table.to_pandas(types_mapper=pd.ArrowDtype)\n        assert isinstance(result[\"ext\"].dtype, pd.ArrowDtype)\n\n\ndef test_tensor_type_is_picklable(pickle_module):\n    # GH-35599\n\n    expected_type = pa.fixed_shape_tensor(pa.int32(), (2, 2))\n    result = pickle_module.loads(pickle_module.dumps(expected_type))\n\n    assert result == expected_type\n\n    arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n    storage = pa.array(arr, pa.list_(pa.int32(), 4))\n    expected_arr = pa.ExtensionArray.from_storage(expected_type, storage)\n    result = pickle_module.loads(pickle_module.dumps(expected_arr))\n\n    assert result == expected_arr\n\n\n@pytest.mark.parametrize((\"tensor_type\", \"text\"), [\n    (\n        pa.fixed_shape_tensor(pa.int8(), [2, 2, 3]),\n        'fixed_shape_tensor[value_type=int8, shape=[2,2,3]]'\n    ),\n    (\n        pa.fixed_shape_tensor(pa.int32(), [2, 2, 3], permutation=[0, 2, 1]),\n        'fixed_shape_tensor[value_type=int32, shape=[2,2,3], permutation=[0,2,1]]'\n    ),\n    (\n        pa.fixed_shape_tensor(pa.int64(), [2, 2, 3], dim_names=['C', 'H', 'W']),\n        'fixed_shape_tensor[value_type=int64, shape=[2,2,3], dim_names=[C,H,W]]'\n    )\n])\ndef test_tensor_type_str(tensor_type, text):\n    tensor_type_str = tensor_type.__str__()\n    assert text in tensor_type_str\n\n\ndef test_legacy_int_type():\n    with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):\n        ext_ty = LegacyIntType()\n    arr = pa.array([1, 2, 3], type=ext_ty.storage_type)\n    ext_arr = pa.ExtensionArray.from_storage(ext_ty, arr)\n    batch = pa.RecordBatch.from_arrays([ext_arr], names=['ext'])\n    buf = ipc_write_batch(batch)\n\n    with pytest.warns(\n            RuntimeWarning,\n            match=\"pickle-based deserialization of pyarrow.PyExtensionType \"\n                  \"subclasses is disabled by default\"):\n        batch = ipc_read_batch(buf)\n        assert isinstance(batch.column(0).type, pa.UnknownExtensionType)\n\n    with enabled_auto_load():\n        with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):\n            batch = ipc_read_batch(buf)\n            assert isinstance(batch.column(0).type, LegacyIntType)\n            assert batch.column(0) == ext_arr\n", "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport gc\nimport decimal\nimport json\nimport multiprocessing as mp\nimport sys\nimport warnings\n\nfrom collections import OrderedDict\nfrom datetime import date, datetime, time, timedelta, timezone\n\nimport hypothesis as h\nimport hypothesis.strategies as st\nimport numpy as np\nimport numpy.testing as npt\nimport pytest\n\nfrom pyarrow.pandas_compat import get_logical_type, _pandas_api\nfrom pyarrow.tests.util import invoke_script, random_ascii, rands\nimport pyarrow.tests.strategies as past\nfrom pyarrow.vendored.version import Version\n\nimport pyarrow as pa\ntry:\n    from pyarrow import parquet as pq\nexcept ImportError:\n    pass\n\ntry:\n    import pandas as pd\n    import pandas.testing as tm\n    from .pandas_examples import dataframe_with_arrays, dataframe_with_lists\nexcept ImportError:\n    pass\n\n\ntry:\n    _np_VisibleDeprecationWarning = np.VisibleDeprecationWarning\nexcept AttributeError:\n    from numpy.exceptions import (\n        VisibleDeprecationWarning as _np_VisibleDeprecationWarning\n    )\n\n\n# Marks all of the tests in this module\npytestmark = pytest.mark.pandas\n\n\ndef _alltypes_example(size=100):\n    return pd.DataFrame({\n        'uint8': np.arange(size, dtype=np.uint8),\n        'uint16': np.arange(size, dtype=np.uint16),\n        'uint32': np.arange(size, dtype=np.uint32),\n        'uint64': np.arange(size, dtype=np.uint64),\n        'int8': np.arange(size, dtype=np.int16),\n        'int16': np.arange(size, dtype=np.int16),\n        'int32': np.arange(size, dtype=np.int32),\n        'int64': np.arange(size, dtype=np.int64),\n        'float32': np.arange(size, dtype=np.float32),\n        'float64': np.arange(size, dtype=np.float64),\n        'bool': np.random.randn(size) > 0,\n        'datetime[s]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                 dtype='datetime64[s]'),\n        'datetime[ms]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[ms]'),\n        'datetime[us]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[us]'),\n        'datetime[ns]': np.arange(\"2016-01-01T00:00:00.001\", size,\n                                  dtype='datetime64[ns]'),\n        'timedelta64[s]': np.arange(0, size, dtype='timedelta64[s]'),\n        'timedelta64[ms]': np.arange(0, size, dtype='timedelta64[ms]'),\n        'timedelta64[us]': np.arange(0, size, dtype='timedelta64[us]'),\n        'timedelta64[ns]': np.arange(0, size, dtype='timedelta64[ns]'),\n        'str': [str(x) for x in range(size)],\n        'str_with_nulls': [None] + [str(x) for x in range(size - 2)] + [None],\n        'empty_str': [''] * size\n    })\n\n\ndef _check_pandas_roundtrip(df, expected=None, use_threads=False,\n                            expected_schema=None,\n                            check_dtype=True, schema=None,\n                            preserve_index=False,\n                            as_batch=False):\n    klass = pa.RecordBatch if as_batch else pa.Table\n    table = klass.from_pandas(df, schema=schema,\n                              preserve_index=preserve_index,\n                              nthreads=2 if use_threads else 1)\n    result = table.to_pandas(use_threads=use_threads)\n\n    if expected_schema:\n        # all occurrences of _check_pandas_roundtrip passes expected_schema\n        # without the pandas generated key-value metadata\n        assert table.schema.equals(expected_schema)\n\n    if expected is None:\n        expected = df\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", \"elementwise comparison failed\", DeprecationWarning)\n        tm.assert_frame_equal(result, expected, check_dtype=check_dtype,\n                              check_index_type=('equiv' if preserve_index\n                                                else False))\n\n\ndef _check_series_roundtrip(s, type_=None, expected_pa_type=None):\n    arr = pa.array(s, from_pandas=True, type=type_)\n\n    if type_ is not None and expected_pa_type is None:\n        expected_pa_type = type_\n\n    if expected_pa_type is not None:\n        assert arr.type == expected_pa_type\n\n    result = pd.Series(arr.to_pandas(), name=s.name)\n    tm.assert_series_equal(s, result)\n\n\ndef _check_array_roundtrip(values, expected=None, mask=None,\n                           type=None):\n    arr = pa.array(values, from_pandas=True, mask=mask, type=type)\n    result = arr.to_pandas()\n\n    values_nulls = pd.isnull(values)\n    if mask is None:\n        assert arr.null_count == values_nulls.sum()\n    else:\n        assert arr.null_count == (mask | values_nulls).sum()\n\n    if expected is None:\n        if mask is None:\n            expected = pd.Series(values)\n        else:\n            expected = pd.Series(values).copy()\n            expected[mask.copy()] = None\n\n    tm.assert_series_equal(pd.Series(result), expected, check_names=False)\n\n\ndef _check_array_from_pandas_roundtrip(np_array, type=None):\n    arr = pa.array(np_array, from_pandas=True, type=type)\n    result = arr.to_pandas()\n    npt.assert_array_equal(result, np_array)\n\n\nclass TestConvertMetadata:\n    \"\"\"\n    Conversion tests for Pandas metadata & indices.\n    \"\"\"\n\n    def test_non_string_columns(self):\n        df = pd.DataFrame({0: [1, 2, 3]})\n        table = pa.Table.from_pandas(df)\n        assert table.field(0).name == '0'\n\n    def test_non_string_columns_with_index(self):\n        df = pd.DataFrame({0: [1.0, 2.0, 3.0], 1: [4.0, 5.0, 6.0]})\n        df = df.set_index(0)\n\n        # assert that the from_pandas raises the warning\n        with pytest.warns(UserWarning):\n            table = pa.Table.from_pandas(df)\n            assert table.field(0).name == '1'\n\n        expected = df.copy()\n        # non-str index name will be converted to str\n        expected.index.name = str(expected.index.name)\n        with pytest.warns(UserWarning):\n            _check_pandas_roundtrip(df, expected=expected,\n                                    preserve_index=True)\n\n    def test_from_pandas_with_columns(self):\n        df = pd.DataFrame({0: [1, 2, 3], 1: [1, 3, 3], 2: [2, 4, 5]},\n                          columns=[1, 0])\n\n        table = pa.Table.from_pandas(df, columns=[0, 1])\n        expected = pa.Table.from_pandas(df[[0, 1]])\n        assert expected.equals(table)\n\n        record_batch_table = pa.RecordBatch.from_pandas(df, columns=[0, 1])\n        record_batch_expected = pa.RecordBatch.from_pandas(df[[0, 1]])\n        assert record_batch_expected.equals(record_batch_table)\n\n    def test_column_index_names_are_preserved(self):\n        df = pd.DataFrame({'data': [1, 2, 3]})\n        df.columns.names = ['a']\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_column_index_names_with_tz(self):\n        # ARROW-13756\n        # Bug if index is timezone aware DataTimeIndex\n\n        df = pd.DataFrame(\n            np.random.randn(5, 3),\n            columns=pd.date_range(\"2021-01-01\", periods=3, freq=\"50D\", tz=\"CET\")\n        )\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_range_index_shortcut(self):\n        # ARROW-1639\n        index_name = 'foo'\n        df = pd.DataFrame({'a': [1, 2, 3, 4]},\n                          index=pd.RangeIndex(0, 8, step=2, name=index_name))\n\n        df2 = pd.DataFrame({'a': [4, 5, 6, 7]},\n                           index=pd.RangeIndex(0, 4))\n\n        table = pa.Table.from_pandas(df)\n        table_no_index_name = pa.Table.from_pandas(df2)\n\n        # The RangeIndex is tracked in the metadata only\n        assert len(table.schema) == 1\n\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n        assert isinstance(result.index, pd.RangeIndex)\n        assert _pandas_api.get_rangeindex_attribute(result.index, 'step') == 2\n        assert result.index.name == index_name\n\n        result2 = table_no_index_name.to_pandas()\n        tm.assert_frame_equal(result2, df2)\n        assert isinstance(result2.index, pd.RangeIndex)\n        assert _pandas_api.get_rangeindex_attribute(result2.index, 'step') == 1\n        assert result2.index.name is None\n\n    def test_range_index_force_serialization(self):\n        # ARROW-5427: preserve_index=True will force the RangeIndex to\n        # be serialized as a column rather than tracked more\n        # efficiently as metadata\n        df = pd.DataFrame({'a': [1, 2, 3, 4]},\n                          index=pd.RangeIndex(0, 8, step=2, name='foo'))\n\n        table = pa.Table.from_pandas(df, preserve_index=True)\n        assert table.num_columns == 2\n        assert 'foo' in table.column_names\n\n        restored = table.to_pandas()\n        tm.assert_frame_equal(restored, df)\n\n    def test_rangeindex_doesnt_warn(self):\n        # ARROW-5606: pandas 0.25 deprecated private _start/stop/step\n        # attributes -> can be removed if support < pd 0.25 is dropped\n        df = pd.DataFrame(np.random.randn(4, 2), columns=['a', 'b'])\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"error\")\n            _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns(self):\n        columns = pd.MultiIndex.from_arrays([\n            ['one', 'two'], ['X', 'Y']\n        ])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns_with_dtypes(self):\n        columns = pd.MultiIndex.from_arrays(\n            [\n                ['one', 'two'],\n                pd.DatetimeIndex(['2017-08-01', '2017-08-02']),\n            ],\n            names=['level_1', 'level_2'],\n        )\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_with_column_dtype_object(self):\n        # ARROW-3651 & ARROW-9096\n        # Bug when dtype of the columns is object.\n\n        # uinderlying dtype: integer\n        df = pd.DataFrame([1], columns=pd.Index([1], dtype=object))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # underlying dtype: floating\n        df = pd.DataFrame([1], columns=pd.Index([1.1], dtype=object))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # underlying dtype: datetime\n        # ARROW-9096: a simple roundtrip now works\n        df = pd.DataFrame([1], columns=pd.Index(\n            [datetime(2018, 1, 1)], dtype=\"object\"))\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_columns_unicode(self):\n        columns = pd.MultiIndex.from_arrays([['\u3042', '\u3044'], ['X', 'Y']])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_multiindex_doesnt_warn(self):\n        # ARROW-3953: pandas 0.24 rename of MultiIndex labels to codes\n        columns = pd.MultiIndex.from_arrays([['one', 'two'], ['X', 'Y']])\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(action=\"error\")\n            _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_integer_index_column(self):\n        df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')])\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_index_metadata_field_name(self):\n        # test None case, and strangely named non-index columns\n        df = pd.DataFrame(\n            [(1, 'a', 3.1), (2, 'b', 2.2), (3, 'c', 1.3)],\n            index=pd.MultiIndex.from_arrays(\n                [['c', 'b', 'a'], [3, 2, 1]],\n                names=[None, 'foo']\n            ),\n            columns=['a', None, '__index_level_0__'],\n        )\n        with pytest.warns(UserWarning):\n            t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        col1, col2, col3, idx0, foo = js['columns']\n\n        assert col1['name'] == 'a'\n        assert col1['name'] == col1['field_name']\n\n        assert col2['name'] is None\n        assert col2['field_name'] == 'None'\n\n        assert col3['name'] == '__index_level_0__'\n        assert col3['name'] == col3['field_name']\n\n        idx0_descr, foo_descr = js['index_columns']\n        assert idx0_descr == '__index_level_0__'\n        assert idx0['field_name'] == idx0_descr\n        assert idx0['name'] is None\n\n        assert foo_descr == 'foo'\n        assert foo['field_name'] == foo_descr\n        assert foo['name'] == foo_descr\n\n    def test_categorical_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.Index(list('def'), dtype='category')\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] is None\n        assert column_indexes['pandas_type'] == 'categorical'\n        assert column_indexes['numpy_type'] == 'int8'\n\n        md = column_indexes['metadata']\n        assert md['num_categories'] == 3\n        assert md['ordered'] is False\n\n    def test_string_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.Index(list('def'), name='stringz')\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] == 'stringz'\n        assert column_indexes['name'] == column_indexes['field_name']\n        assert column_indexes['numpy_type'] == 'object'\n        assert column_indexes['pandas_type'] == 'unicode'\n\n        md = column_indexes['metadata']\n\n        assert len(md) == 1\n        assert md['encoding'] == 'UTF-8'\n\n    def test_datetimetz_column_index(self):\n        df = pd.DataFrame(\n            [(1, 'a', 2.0), (2, 'b', 3.0), (3, 'c', 4.0)],\n            columns=pd.date_range(\n                start='2017-01-01', periods=3, tz='America/New_York'\n            )\n        )\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        js = t.schema.pandas_metadata\n\n        column_indexes, = js['column_indexes']\n        assert column_indexes['name'] is None\n        assert column_indexes['pandas_type'] == 'datetimetz'\n        assert column_indexes['numpy_type'] == 'datetime64[ns]'\n\n        md = column_indexes['metadata']\n        assert md['timezone'] == 'America/New_York'\n\n    def test_datetimetz_row_index(self):\n        df = pd.DataFrame({\n            'a': pd.date_range(\n                start='2017-01-01', periods=3, tz='America/New_York'\n            )\n        })\n        df = df.set_index('a')\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_categorical_row_index(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [1, 2, 3]})\n        df['a'] = df.a.astype('category')\n        df = df.set_index('a')\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_duplicate_column_names_does_not_crash(self):\n        df = pd.DataFrame([(1, 'a'), (2, 'b')], columns=list('aa'))\n        with pytest.raises(ValueError):\n            pa.Table.from_pandas(df)\n\n    def test_dictionary_indices_boundscheck(self):\n        # ARROW-1658. No validation of indices leads to segfaults in pandas\n        indices = [[0, 1], [0, -1]]\n\n        for inds in indices:\n            arr = pa.DictionaryArray.from_arrays(inds, ['a'], safe=False)\n            batch = pa.RecordBatch.from_arrays([arr], ['foo'])\n            table = pa.Table.from_batches([batch, batch, batch])\n\n            with pytest.raises(IndexError):\n                arr.to_pandas()\n\n            with pytest.raises(IndexError):\n                table.to_pandas()\n\n    def test_unicode_with_unicode_column_and_index(self):\n        df = pd.DataFrame({'\u3042': ['\u3044']}, index=['\u3046'])\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_mixed_column_names(self):\n        # mixed type column names are not reconstructed exactly\n        df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n\n        for cols in [['\u3042', b'a'], [1, '2'], [1, 1.5]]:\n            df.columns = pd.Index(cols, dtype=object)\n\n            # assert that the from_pandas raises the warning\n            with pytest.warns(UserWarning):\n                pa.Table.from_pandas(df)\n\n            expected = df.copy()\n            expected.columns = df.columns.values.astype(str)\n            with pytest.warns(UserWarning):\n                _check_pandas_roundtrip(df, expected=expected,\n                                        preserve_index=True)\n\n    def test_binary_column_name(self):\n        if Version(\"2.0.0\") <= Version(pd.__version__) < Version(\"2.2.0\"):\n            # TODO: regression in pandas, hopefully fixed in next version\n            # https://issues.apache.org/jira/browse/ARROW-18394\n            # https://github.com/pandas-dev/pandas/issues/50127\n            pytest.skip(\"Regression in pandas 2.0.0\")\n        column_data = ['\u3044']\n        key = '\u3042'.encode()\n        data = {key: column_data}\n        df = pd.DataFrame(data)\n\n        # we can't use _check_pandas_roundtrip here because our metadata\n        # is always decoded as utf8: even if binary goes in, utf8 comes out\n        t = pa.Table.from_pandas(df, preserve_index=True)\n        df2 = t.to_pandas()\n        assert df.values[0] == df2.values[0]\n        assert df.index.values[0] == df2.index.values[0]\n        assert df.columns[0] == key\n\n    def test_multiindex_duplicate_values(self):\n        num_rows = 3\n        numbers = list(range(num_rows))\n        index = pd.MultiIndex.from_arrays(\n            [['foo', 'foo', 'bar'], numbers],\n            names=['foobar', 'some_numbers'],\n        )\n\n        df = pd.DataFrame({'numbers': numbers}, index=index)\n\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n    def test_metadata_with_mixed_types(self):\n        df = pd.DataFrame({'data': [b'some_bytes', 'some_unicode']})\n        table = pa.Table.from_pandas(df)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'bytes'\n        assert data_column['numpy_type'] == 'object'\n\n    def test_ignore_metadata(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': ['foo', 'bar', 'baz']},\n                          index=['one', 'two', 'three'])\n        table = pa.Table.from_pandas(df)\n\n        result = table.to_pandas(ignore_metadata=True)\n        expected = (table.cast(table.schema.remove_metadata())\n                    .to_pandas())\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_list_metadata(self):\n        df = pd.DataFrame({'data': [[1], [2, 3, 4], [5] * 7]})\n        schema = pa.schema([pa.field('data', type=pa.list_(pa.int64()))])\n        table = pa.Table.from_pandas(df, schema=schema)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'list[int64]'\n        assert data_column['numpy_type'] == 'object'\n\n    def test_struct_metadata(self):\n        df = pd.DataFrame({'dicts': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n        table = pa.Table.from_pandas(df)\n        pandas_metadata = table.schema.pandas_metadata\n        assert pandas_metadata['columns'][0]['pandas_type'] == 'object'\n\n    def test_decimal_metadata(self):\n        expected = pd.DataFrame({\n            'decimals': [\n                decimal.Decimal('394092382910493.12341234678'),\n                -decimal.Decimal('314292388910493.12343437128'),\n            ]\n        })\n        table = pa.Table.from_pandas(expected)\n        js = table.schema.pandas_metadata\n        assert 'mixed' not in js\n        data_column = js['columns'][0]\n        assert data_column['pandas_type'] == 'decimal'\n        assert data_column['numpy_type'] == 'object'\n        assert data_column['metadata'] == {'precision': 26, 'scale': 11}\n\n    def test_table_column_subset_metadata(self):\n        # ARROW-1883\n        # non-default index\n        for index in [\n                pd.Index(['a', 'b', 'c'], name='index'),\n                pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')]:\n            df = pd.DataFrame({'a': [1, 2, 3],\n                               'b': [.1, .2, .3]}, index=index)\n            table = pa.Table.from_pandas(df)\n\n            table_subset = table.remove_column(1)\n            result = table_subset.to_pandas()\n            expected = df[['a']]\n            if isinstance(df.index, pd.DatetimeIndex):\n                df.index.freq = None\n            tm.assert_frame_equal(result, expected)\n\n            table_subset2 = table_subset.remove_column(1)\n            result = table_subset2.to_pandas()\n            tm.assert_frame_equal(result, df[['a']].reset_index(drop=True))\n\n    def test_to_pandas_column_subset_multiindex(self):\n        # ARROW-10122\n        df = pd.DataFrame(\n            {\"first\": list(range(5)),\n             \"second\": list(range(5)),\n             \"value\": np.arange(5)}\n        )\n        table = pa.Table.from_pandas(df.set_index([\"first\", \"second\"]))\n\n        subset = table.select([\"first\", \"value\"])\n        result = subset.to_pandas()\n        expected = df[[\"first\", \"value\"]].set_index(\"first\")\n        tm.assert_frame_equal(result, expected)\n\n    def test_empty_list_metadata(self):\n        # Create table with array of empty lists, forced to have type\n        # list(string) in pyarrow\n        c1 = [[\"test\"], [\"a\", \"b\"], None]\n        c2 = [[], [], []]\n        arrays = OrderedDict([\n            ('c1', pa.array(c1, type=pa.list_(pa.string()))),\n            ('c2', pa.array(c2, type=pa.list_(pa.string()))),\n        ])\n        rb = pa.RecordBatch.from_arrays(\n            list(arrays.values()),\n            list(arrays.keys())\n        )\n        tbl = pa.Table.from_batches([rb])\n\n        # First roundtrip changes schema, because pandas cannot preserve the\n        # type of empty lists\n        df = tbl.to_pandas()\n        tbl2 = pa.Table.from_pandas(df)\n        md2 = tbl2.schema.pandas_metadata\n\n        # Second roundtrip\n        df2 = tbl2.to_pandas()\n        expected = pd.DataFrame(OrderedDict([('c1', c1), ('c2', c2)]))\n\n        tm.assert_frame_equal(df2, expected)\n\n        assert md2['columns'] == [\n            {\n                'name': 'c1',\n                'field_name': 'c1',\n                'metadata': None,\n                'numpy_type': 'object',\n                'pandas_type': 'list[unicode]',\n            },\n            {\n                'name': 'c2',\n                'field_name': 'c2',\n                'metadata': None,\n                'numpy_type': 'object',\n                'pandas_type': 'list[empty]',\n            }\n        ]\n\n    def test_metadata_pandas_version(self):\n        df = pd.DataFrame({'a': [1, 2, 3], 'b': [1, 2, 3]})\n        table = pa.Table.from_pandas(df)\n        assert table.schema.pandas_metadata['pandas_version'] is not None\n\n    def test_mismatch_metadata_schema(self):\n        # ARROW-10511\n        # It is possible that the metadata and actual schema is not fully\n        # matching (eg no timezone information for tz-aware column)\n        # -> to_pandas() conversion should not fail on that\n        df = pd.DataFrame({\"datetime\": pd.date_range(\"2020-01-01\", periods=3)})\n\n        # OPTION 1: casting after conversion\n        table = pa.Table.from_pandas(df)\n        # cast the \"datetime\" column to be tz-aware\n        new_col = table[\"datetime\"].cast(pa.timestamp('ns', tz=\"UTC\"))\n        new_table1 = table.set_column(\n            0, pa.field(\"datetime\", new_col.type), new_col\n        )\n\n        # OPTION 2: specify schema during conversion\n        schema = pa.schema([(\"datetime\", pa.timestamp('ns', tz=\"UTC\"))])\n        new_table2 = pa.Table.from_pandas(df, schema=schema)\n\n        expected = df.copy()\n        expected[\"datetime\"] = expected[\"datetime\"].dt.tz_localize(\"UTC\")\n\n        for new_table in [new_table1, new_table2]:\n            # ensure the new table still has the pandas metadata\n            assert new_table.schema.pandas_metadata is not None\n            # convert to pandas\n            result = new_table.to_pandas()\n            tm.assert_frame_equal(result, expected)\n\n\nclass TestConvertPrimitiveTypes:\n    \"\"\"\n    Conversion tests for primitive (e.g. numeric) types.\n    \"\"\"\n\n    def test_float_no_nulls(self):\n        data = {}\n        fields = []\n        dtypes = [('f2', pa.float16()),\n                  ('f4', pa.float32()),\n                  ('f8', pa.float64())]\n        num_values = 100\n\n        for numpy_dtype, arrow_dtype in dtypes:\n            values = np.random.randn(num_values)\n            data[numpy_dtype] = values.astype(numpy_dtype)\n            fields.append(pa.field(numpy_dtype, arrow_dtype))\n\n        df = pd.DataFrame(data)\n        schema = pa.schema(fields)\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_float_nulls(self):\n        num_values = 100\n\n        null_mask = np.random.randint(0, 10, size=num_values) < 3\n        dtypes = [('f2', pa.float16()),\n                  ('f4', pa.float32()),\n                  ('f8', pa.float64())]\n        names = ['f2', 'f4', 'f8']\n        expected_cols = []\n\n        arrays = []\n        fields = []\n        for name, arrow_dtype in dtypes:\n            values = np.random.randn(num_values).astype(name)\n\n            arr = pa.array(values, from_pandas=True, mask=null_mask)\n            arrays.append(arr)\n            fields.append(pa.field(name, arrow_dtype))\n            values[null_mask] = np.nan\n\n            expected_cols.append(values)\n\n        ex_frame = pd.DataFrame(dict(zip(names, expected_cols)),\n                                columns=names)\n\n        table = pa.Table.from_arrays(arrays, names)\n        assert table.schema.equals(pa.schema(fields))\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_float_nulls_to_ints(self):\n        # ARROW-2135\n        df = pd.DataFrame({\"a\": [1.0, 2.0, np.nan]})\n        schema = pa.schema([pa.field(\"a\", pa.int16(), nullable=True)])\n        table = pa.Table.from_pandas(df, schema=schema, safe=False)\n        assert table[0].to_pylist() == [1, 2, None]\n        tm.assert_frame_equal(df, table.to_pandas())\n\n    def test_float_nulls_to_boolean(self):\n        s = pd.Series([0.0, 1.0, 2.0, None, -3.0])\n        expected = pd.Series([False, True, True, None, True])\n        _check_array_roundtrip(s, expected=expected, type=pa.bool_())\n\n    def test_series_from_pandas_false_respected(self):\n        # Check that explicit from_pandas=False is respected\n        s = pd.Series([0.0, np.nan])\n        arr = pa.array(s, from_pandas=False)\n        assert arr.null_count == 0\n        assert np.isnan(arr[1].as_py())\n\n    def test_integer_no_nulls(self):\n        data = OrderedDict()\n        fields = []\n\n        numpy_dtypes = [\n            ('i1', pa.int8()), ('i2', pa.int16()),\n            ('i4', pa.int32()), ('i8', pa.int64()),\n            ('u1', pa.uint8()), ('u2', pa.uint16()),\n            ('u4', pa.uint32()), ('u8', pa.uint64()),\n            ('longlong', pa.int64()), ('ulonglong', pa.uint64())\n        ]\n        num_values = 100\n\n        for dtype, arrow_dtype in numpy_dtypes:\n            info = np.iinfo(dtype)\n            values = np.random.randint(max(info.min, np.iinfo(np.int_).min),\n                                       min(info.max, np.iinfo(np.int_).max),\n                                       size=num_values)\n            data[dtype] = values.astype(dtype)\n            fields.append(pa.field(dtype, arrow_dtype))\n\n        df = pd.DataFrame(data)\n        schema = pa.schema(fields)\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_all_integer_types(self):\n        # Test all Numpy integer aliases\n        data = OrderedDict()\n        numpy_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8',\n                        'byte', 'ubyte', 'short', 'ushort', 'intc', 'uintc',\n                        'int_', 'uint', 'longlong', 'ulonglong']\n        for dtype in numpy_dtypes:\n            data[dtype] = np.arange(12, dtype=dtype)\n        df = pd.DataFrame(data)\n        _check_pandas_roundtrip(df)\n\n        # Do the same with pa.array()\n        # (for some reason, it doesn't use the same code paths at all)\n        for np_arr in data.values():\n            arr = pa.array(np_arr)\n            assert arr.to_pylist() == np_arr.tolist()\n\n    def test_integer_byteorder(self):\n        # Byteswapped arrays are not supported yet\n        int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n        for dt in int_dtypes:\n            for order in '=<>':\n                data = np.array([1, 2, 42], dtype=order + dt)\n                for np_arr in (data, data[::2]):\n                    if data.dtype.isnative:\n                        arr = pa.array(data)\n                        assert arr.to_pylist() == data.tolist()\n                    else:\n                        with pytest.raises(NotImplementedError):\n                            arr = pa.array(data)\n\n    def test_integer_with_nulls(self):\n        # pandas requires upcast to float dtype\n\n        int_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']\n        num_values = 100\n\n        null_mask = np.random.randint(0, 10, size=num_values) < 3\n\n        expected_cols = []\n        arrays = []\n        for name in int_dtypes:\n            values = np.random.randint(0, 100, size=num_values)\n\n            arr = pa.array(values, mask=null_mask)\n            arrays.append(arr)\n\n            expected = values.astype('f8')\n            expected[null_mask] = np.nan\n\n            expected_cols.append(expected)\n\n        ex_frame = pd.DataFrame(dict(zip(int_dtypes, expected_cols)),\n                                columns=int_dtypes)\n\n        table = pa.Table.from_arrays(arrays, int_dtypes)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_array_from_pandas_type_cast(self):\n        arr = np.arange(10, dtype='int64')\n\n        target_type = pa.int8()\n\n        result = pa.array(arr, type=target_type)\n        expected = pa.array(arr.astype('int8'))\n        assert result.equals(expected)\n\n    def test_boolean_no_nulls(self):\n        num_values = 100\n\n        np.random.seed(0)\n\n        df = pd.DataFrame({'bools': np.random.randn(num_values) > 0})\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_boolean_nulls(self):\n        # pandas requires upcast to object dtype\n        num_values = 100\n        np.random.seed(0)\n\n        mask = np.random.randint(0, 10, size=num_values) < 3\n        values = np.random.randint(0, 10, size=num_values) < 5\n\n        arr = pa.array(values, mask=mask)\n\n        expected = values.astype(object)\n        expected[mask] = None\n\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        ex_frame = pd.DataFrame({'bools': expected})\n\n        table = pa.Table.from_arrays([arr], ['bools'])\n        assert table.schema.equals(schema)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, ex_frame)\n\n    def test_boolean_to_int(self):\n        # test from dtype=bool\n        s = pd.Series([True, True, False, True, True] * 2)\n        expected = pd.Series([1, 1, 0, 1, 1] * 2)\n        _check_array_roundtrip(s, expected=expected, type=pa.int64())\n\n    def test_boolean_objects_to_int(self):\n        # test from dtype=object\n        s = pd.Series([True, True, False, True, True] * 2, dtype=object)\n        expected = pd.Series([1, 1, 0, 1, 1] * 2)\n        expected_msg = 'Expected integer, got bool'\n        with pytest.raises(pa.ArrowTypeError, match=expected_msg):\n            _check_array_roundtrip(s, expected=expected, type=pa.int64())\n\n    def test_boolean_nulls_to_float(self):\n        # test from dtype=object\n        s = pd.Series([True, True, False, None, True] * 2)\n        expected = pd.Series([1.0, 1.0, 0.0, None, 1.0] * 2)\n        _check_array_roundtrip(s, expected=expected, type=pa.float64())\n\n    def test_boolean_multiple_columns(self):\n        # ARROW-6325 (multiple columns resulting in strided conversion)\n        df = pd.DataFrame(np.ones((3, 2), dtype='bool'), columns=['a', 'b'])\n        _check_pandas_roundtrip(df)\n\n    def test_float_object_nulls(self):\n        arr = np.array([None, 1.5, np.float64(3.5)] * 5, dtype=object)\n        df = pd.DataFrame({'floats': arr})\n        expected = pd.DataFrame({'floats': pd.to_numeric(arr)})\n        field = pa.field('floats', pa.float64())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected=expected,\n                                expected_schema=schema)\n\n    def test_float_with_null_as_integer(self):\n        # ARROW-2298\n        s = pd.Series([np.nan, 1., 2., np.nan])\n\n        types = [pa.int8(), pa.int16(), pa.int32(), pa.int64(),\n                 pa.uint8(), pa.uint16(), pa.uint32(), pa.uint64()]\n        for ty in types:\n            result = pa.array(s, type=ty)\n            expected = pa.array([None, 1, 2, None], type=ty)\n            assert result.equals(expected)\n\n            df = pd.DataFrame({'has_nulls': s})\n            schema = pa.schema([pa.field('has_nulls', ty)])\n            result = pa.Table.from_pandas(df, schema=schema,\n                                          preserve_index=False)\n            assert result[0].chunk(0).equals(expected)\n\n    def test_int_object_nulls(self):\n        arr = np.array([None, 1, np.int64(3)] * 5, dtype=object)\n        df = pd.DataFrame({'ints': arr})\n        expected = pd.DataFrame({'ints': pd.to_numeric(arr)})\n        field = pa.field('ints', pa.int64())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected=expected,\n                                expected_schema=schema)\n\n    def test_boolean_object_nulls(self):\n        arr = np.array([False, None, True] * 100, dtype=object)\n        df = pd.DataFrame({'bools': arr})\n        field = pa.field('bools', pa.bool_())\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(df, expected_schema=schema)\n\n    def test_all_nulls_cast_numeric(self):\n        arr = np.array([None], dtype=object)\n\n        def _check_type(t):\n            a2 = pa.array(arr, type=t)\n            assert a2.type == t\n            assert a2[0].as_py() is None\n\n        _check_type(pa.int32())\n        _check_type(pa.float64())\n\n    def test_half_floats_from_numpy(self):\n        arr = np.array([1.5, np.nan], dtype=np.float16)\n        a = pa.array(arr, type=pa.float16())\n        x, y = a.to_pylist()\n        assert isinstance(x, np.float16)\n        assert x == 1.5\n        assert isinstance(y, np.float16)\n        assert np.isnan(y)\n\n        a = pa.array(arr, type=pa.float16(), from_pandas=True)\n        x, y = a.to_pylist()\n        assert isinstance(x, np.float16)\n        assert x == 1.5\n        assert y is None\n\n\n@pytest.mark.parametrize('dtype',\n                         ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8'])\ndef test_array_integer_object_nulls_option(dtype):\n    num_values = 100\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 100, size=num_values, dtype=dtype)\n\n    array = pa.array(values, mask=null_mask)\n\n    if null_mask.any():\n        expected = values.astype('O')\n        expected[null_mask] = None\n    else:\n        expected = values\n\n    result = array.to_pandas(integer_object_nulls=True)\n\n    np.testing.assert_equal(result, expected)\n\n\n@pytest.mark.parametrize('dtype',\n                         ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8'])\ndef test_table_integer_object_nulls_option(dtype):\n    num_values = 100\n\n    null_mask = np.random.randint(0, 10, size=num_values) < 3\n    values = np.random.randint(0, 100, size=num_values, dtype=dtype)\n\n    array = pa.array(values, mask=null_mask)\n\n    if null_mask.any():\n        expected = values.astype('O')\n        expected[null_mask] = None\n    else:\n        expected = values\n\n    expected = pd.DataFrame({dtype: expected})\n\n    table = pa.Table.from_arrays([array], [dtype])\n    result = table.to_pandas(integer_object_nulls=True)\n\n    tm.assert_frame_equal(result, expected)\n\n\nclass TestConvertDateTimeLikeTypes:\n    \"\"\"\n    Conversion tests for datetime- and timestamp-like types (date64, etc.).\n    \"\"\"\n\n    def test_timestamps_notimezone_no_nulls(self):\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                '2006-01-13T12:34:56.432539784',\n                '2010-08-13T05:46:57.437699912'],\n                dtype='datetime64[ns]')\n        })\n        field = pa.field('datetime64', pa.timestamp('ns'))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    def test_timestamps_notimezone_nulls(self):\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                None,\n                '2010-08-13T05:46:57.437699912'],\n                dtype='datetime64[ns]')\n        })\n        field = pa.field('datetime64', pa.timestamp('ns'))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    @pytest.mark.parametrize('unit', ['s', 'ms', 'us', 'ns'])\n    def test_timestamps_with_timezone(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\") and unit != 'ns':\n            pytest.skip(\"pandas < 2.0 only supports nanosecond datetime64\")\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123',\n                '2006-01-13T12:34:56.432',\n                '2010-08-13T05:46:57.437'],\n                dtype=f'datetime64[{unit}]')\n        })\n        df['datetime64'] = df['datetime64'].dt.tz_localize('US/Eastern')\n        _check_pandas_roundtrip(df)\n\n        _check_series_roundtrip(df['datetime64'])\n\n        # drop-in a null\n        df = pd.DataFrame({\n            'datetime64': np.array([\n                '2007-07-13T01:23:34.123456789',\n                None,\n                '2006-01-13T12:34:56.432539784',\n                '2010-08-13T05:46:57.437699912'],\n                dtype=f'datetime64[{unit}]')\n        })\n        df['datetime64'] = df['datetime64'].dt.tz_localize('US/Eastern')\n\n        _check_pandas_roundtrip(df)\n\n    def test_python_datetime(self):\n        # ARROW-2106\n        date_array = [datetime.today() + timedelta(days=x) for x in range(10)]\n        df = pd.DataFrame({\n            'datetime': pd.Series(date_array, dtype=object)\n        })\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.TimestampArray)\n\n        result = table.to_pandas()\n        # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n        # so we need to cast the pandas dtype. Pandas v1 will always silently\n        # coerce to [ns] due to lack of non-[ns] support.\n        expected_df = pd.DataFrame({\n            'datetime': pd.Series(date_array, dtype='datetime64[us]')\n        })\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_python_datetime_with_pytz_tzinfo(self):\n        pytz = pytest.importorskip(\"pytz\")\n\n        for tz in [pytz.utc, pytz.timezone('US/Eastern'), pytz.FixedOffset(1)]:\n            values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz)]\n            df = pd.DataFrame({'datetime': values})\n            _check_pandas_roundtrip(df)\n\n    @h.given(st.none() | past.timezones)\n    @h.settings(deadline=None)\n    def test_python_datetime_with_pytz_timezone(self, tz):\n        if str(tz) in [\"build/etc/localtime\", \"Factory\"]:\n            pytest.skip(\"Localtime timezone not supported\")\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz)]\n        df = pd.DataFrame({'datetime': values})\n        _check_pandas_roundtrip(df, check_dtype=False)\n\n    def test_python_datetime_with_timezone_tzinfo(self):\n        pytz = pytest.importorskip(\"pytz\")\n        from datetime import timezone\n\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=timezone.utc)]\n        # also test with index to ensure both paths roundtrip (ARROW-9962)\n        df = pd.DataFrame({'datetime': values}, index=values)\n        _check_pandas_roundtrip(df, preserve_index=True)\n\n        # datetime.timezone is going to be pytz.FixedOffset\n        hours = 1\n        tz_timezone = timezone(timedelta(hours=hours))\n        tz_pytz = pytz.FixedOffset(hours * 60)\n        values = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz_timezone)]\n        values_exp = [datetime(2018, 1, 1, 12, 23, 45, tzinfo=tz_pytz)]\n        df = pd.DataFrame({'datetime': values}, index=values)\n        df_exp = pd.DataFrame({'datetime': values_exp}, index=values_exp)\n        _check_pandas_roundtrip(df, expected=df_exp, preserve_index=True)\n\n    def test_python_datetime_subclass(self):\n\n        class MyDatetime(datetime):\n            # see https://github.com/pandas-dev/pandas/issues/21142\n            nanosecond = 0.0\n\n        date_array = [MyDatetime(2000, 1, 1, 1, 1, 1)]\n        df = pd.DataFrame({\"datetime\": pd.Series(date_array, dtype=object)})\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.TimestampArray)\n\n        result = table.to_pandas()\n\n        # Pandas v2 defaults to [ns], but Arrow defaults to [us] time units\n        # so we need to cast the pandas dtype. Pandas v1 will always silently\n        # coerce to [ns] due to lack of non-[ns] support.\n        expected_df = pd.DataFrame(\n            {\"datetime\": pd.Series(date_array, dtype='datetime64[us]')})\n\n        # https://github.com/pandas-dev/pandas/issues/21142\n        expected_df[\"datetime\"] = pd.to_datetime(expected_df[\"datetime\"])\n\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_python_date_subclass(self):\n\n        class MyDate(date):\n            pass\n\n        date_array = [MyDate(2000, 1, 1)]\n        df = pd.DataFrame({\"date\": pd.Series(date_array, dtype=object)})\n\n        table = pa.Table.from_pandas(df)\n        assert isinstance(table[0].chunk(0), pa.Date32Array)\n\n        result = table.to_pandas()\n        expected_df = pd.DataFrame(\n            {\"date\": np.array([date(2000, 1, 1)], dtype=object)}\n        )\n        tm.assert_frame_equal(expected_df, result)\n\n    def test_datetime64_to_date32(self):\n        # ARROW-1718\n        arr = pa.array([date(2017, 10, 23), None])\n        c = pa.chunked_array([arr])\n        s = c.to_pandas()\n\n        arr2 = pa.Array.from_pandas(s, type=pa.date32())\n\n        assert arr2.equals(arr.cast('date32'))\n\n    @pytest.mark.parametrize('mask', [\n        None,\n        np.array([True, False, False, True, False, False]),\n    ])\n    def test_pandas_datetime_to_date64(self, mask):\n        s = pd.to_datetime([\n            '2018-05-10T00:00:00',\n            '2018-05-11T00:00:00',\n            '2018-05-12T00:00:00',\n            '2018-05-10T10:24:01',\n            '2018-05-11T10:24:01',\n            '2018-05-12T10:24:01',\n        ])\n        arr = pa.Array.from_pandas(s, type=pa.date64(), mask=mask)\n\n        data = np.array([\n            date(2018, 5, 10),\n            date(2018, 5, 11),\n            date(2018, 5, 12),\n            date(2018, 5, 10),\n            date(2018, 5, 11),\n            date(2018, 5, 12),\n        ])\n        expected = pa.array(data, mask=mask, type=pa.date64())\n\n        assert arr.equals(expected)\n\n    @pytest.mark.parametrize(\"coerce_to_ns,expected_dtype\",\n                             [(False, 'datetime64[ms]'),\n                              (True, 'datetime64[ns]')])\n    def test_array_types_date_as_object(self, coerce_to_ns, expected_dtype):\n        data = [date(2000, 1, 1),\n                None,\n                date(1970, 1, 1),\n                date(2040, 2, 26)]\n        expected_days = np.array(['2000-01-01', None, '1970-01-01',\n                                  '2040-02-26'], dtype='datetime64[D]')\n\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n            expected_dtype = 'datetime64[ns]'\n\n        expected = np.array(['2000-01-01', None, '1970-01-01',\n                             '2040-02-26'], dtype=expected_dtype)\n\n        objects = [pa.array(data),\n                   pa.chunked_array([data])]\n\n        for obj in objects:\n            result = obj.to_pandas(coerce_temporal_nanoseconds=coerce_to_ns)\n            expected_obj = expected_days.astype(object)\n            assert result.dtype == expected_obj.dtype\n            npt.assert_array_equal(result, expected_obj)\n\n            result = obj.to_pandas(date_as_object=False,\n                                   coerce_temporal_nanoseconds=coerce_to_ns)\n            assert result.dtype == expected.dtype\n            npt.assert_array_equal(result, expected)\n\n    @pytest.mark.parametrize(\"coerce_to_ns,expected_type\",\n                             [(False, 'datetime64[ms]'),\n                              (True, 'datetime64[ns]')])\n    def test_table_convert_date_as_object(self, coerce_to_ns, expected_type):\n        df = pd.DataFrame({\n            'date': [date(2000, 1, 1),\n                     None,\n                     date(1970, 1, 1),\n                     date(2040, 2, 26)]})\n\n        table = pa.Table.from_pandas(df, preserve_index=False)\n\n        df_datetime = table.to_pandas(date_as_object=False,\n                                      coerce_temporal_nanoseconds=coerce_to_ns)\n        df_object = table.to_pandas()\n\n        tm.assert_frame_equal(df.astype(expected_type), df_datetime,\n                              check_dtype=True)\n        tm.assert_frame_equal(df, df_object, check_dtype=True)\n\n    @pytest.mark.parametrize(\"arrow_type\",\n                             [pa.date32(), pa.date64(), pa.timestamp('s'),\n                              pa.timestamp('ms'), pa.timestamp('us'),\n                              pa.timestamp('ns'), pa.timestamp('s', 'UTC'),\n                              pa.timestamp('ms', 'UTC'), pa.timestamp('us', 'UTC'),\n                              pa.timestamp('ns', 'UTC')])\n    def test_array_coerce_temporal_nanoseconds(self, arrow_type):\n        data = [date(2000, 1, 1), datetime(2001, 1, 1)]\n        expected = pd.Series(data)\n        arr = pa.array(data).cast(arrow_type)\n        result = arr.to_pandas(\n            coerce_temporal_nanoseconds=True, date_as_object=False)\n        expected_tz = None\n        if hasattr(arrow_type, 'tz') and arrow_type.tz is not None:\n            expected_tz = 'UTC'\n        expected_type = pa.timestamp('ns', expected_tz).to_pandas_dtype()\n        tm.assert_series_equal(result, expected.astype(expected_type))\n\n    @pytest.mark.parametrize(\"arrow_type\",\n                             [pa.date32(), pa.date64(), pa.timestamp('s'),\n                              pa.timestamp('ms'), pa.timestamp('us'),\n                              pa.timestamp('ns'), pa.timestamp('s', 'UTC'),\n                              pa.timestamp('ms', 'UTC'), pa.timestamp('us', 'UTC'),\n                              pa.timestamp('ns', 'UTC')])\n    def test_table_coerce_temporal_nanoseconds(self, arrow_type):\n        data = [date(2000, 1, 1), datetime(2001, 1, 1)]\n        schema = pa.schema([pa.field('date', arrow_type)])\n        expected_df = pd.DataFrame({'date': data})\n        table = pa.table([pa.array(data)], schema=schema)\n        result_df = table.to_pandas(\n            coerce_temporal_nanoseconds=True, date_as_object=False)\n        expected_tz = None\n        if hasattr(arrow_type, 'tz') and arrow_type.tz is not None:\n            expected_tz = 'UTC'\n        expected_type = pa.timestamp('ns', expected_tz).to_pandas_dtype()\n        tm.assert_frame_equal(result_df, expected_df.astype(expected_type))\n\n    def test_date_infer(self):\n        df = pd.DataFrame({\n            'date': [date(2000, 1, 1),\n                     None,\n                     date(1970, 1, 1),\n                     date(2040, 2, 26)]})\n        table = pa.Table.from_pandas(df, preserve_index=False)\n        field = pa.field('date', pa.date32())\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = pa.schema([field], metadata=table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n\n    def test_date_mask(self):\n        arr = np.array([date(2017, 4, 3), date(2017, 4, 4)],\n                       dtype='datetime64[D]')\n        mask = [True, False]\n        result = pa.array(arr, mask=np.array(mask))\n        expected = np.array([None, date(2017, 4, 4)], dtype='datetime64[D]')\n        expected = pa.array(expected, from_pandas=True)\n        assert expected.equals(result)\n\n    def test_date_objects_typed(self):\n        arr = np.array([\n            date(2017, 4, 3),\n            None,\n            date(2017, 4, 4),\n            date(2017, 4, 5)], dtype=object)\n\n        arr_i4 = np.array([17259, -1, 17260, 17261], dtype='int32')\n        arr_i8 = arr_i4.astype('int64') * 86400000\n        mask = np.array([False, True, False, False])\n\n        t32 = pa.date32()\n        t64 = pa.date64()\n\n        a32 = pa.array(arr, type=t32)\n        a64 = pa.array(arr, type=t64)\n\n        a32_expected = pa.array(arr_i4, mask=mask, type=t32)\n        a64_expected = pa.array(arr_i8, mask=mask, type=t64)\n\n        assert a32.equals(a32_expected)\n        assert a64.equals(a64_expected)\n\n        # Test converting back to pandas\n        colnames = ['date32', 'date64']\n        table = pa.Table.from_arrays([a32, a64], colnames)\n\n        ex_values = (np.array(['2017-04-03', '2017-04-04', '2017-04-04',\n                               '2017-04-05'],\n                              dtype='datetime64[D]'))\n        ex_values[1] = pd.NaT.value\n\n        # date32 and date64 convert to [ms] in pandas v2, but\n        # in pandas v1 they are siliently coerced to [ns]\n        ex_datetime64ms = ex_values.astype('datetime64[ms]')\n        expected_pandas = pd.DataFrame({'date32': ex_datetime64ms,\n                                        'date64': ex_datetime64ms},\n                                       columns=colnames)\n        table_pandas = table.to_pandas(date_as_object=False)\n        tm.assert_frame_equal(table_pandas, expected_pandas)\n\n        table_pandas_objects = table.to_pandas()\n        ex_objects = ex_values.astype('object')\n        expected_pandas_objects = pd.DataFrame({'date32': ex_objects,\n                                                'date64': ex_objects},\n                                               columns=colnames)\n        tm.assert_frame_equal(table_pandas_objects,\n                              expected_pandas_objects)\n\n    def test_pandas_null_values(self):\n        # ARROW-842\n        pd_NA = getattr(pd, 'NA', None)\n        values = np.array([datetime(2000, 1, 1), pd.NaT, pd_NA], dtype=object)\n        values_with_none = np.array([datetime(2000, 1, 1), None, None],\n                                    dtype=object)\n        result = pa.array(values, from_pandas=True)\n        expected = pa.array(values_with_none, from_pandas=True)\n        assert result.equals(expected)\n        assert result.null_count == 2\n\n        # ARROW-9407\n        assert pa.array([pd.NaT], from_pandas=True).type == pa.null()\n        assert pa.array([pd_NA], from_pandas=True).type == pa.null()\n\n    def test_dates_from_integers(self):\n        t1 = pa.date32()\n        t2 = pa.date64()\n\n        arr = np.array([17259, 17260, 17261], dtype='int32')\n        arr2 = arr.astype('int64') * 86400000\n\n        a1 = pa.array(arr, type=t1)\n        a2 = pa.array(arr2, type=t2)\n\n        expected = date(2017, 4, 3)\n        assert a1[0].as_py() == expected\n        assert a2[0].as_py() == expected\n\n    def test_pytime_from_pandas(self):\n        pytimes = [time(1, 2, 3, 1356),\n                   time(4, 5, 6, 1356)]\n\n        # microseconds\n        t1 = pa.time64('us')\n\n        aobjs = np.array(pytimes + [None], dtype=object)\n        parr = pa.array(aobjs)\n        assert parr.type == t1\n        assert parr[0].as_py() == pytimes[0]\n        assert parr[1].as_py() == pytimes[1]\n        assert parr[2].as_py() is None\n\n        # DataFrame\n        df = pd.DataFrame({'times': aobjs})\n        batch = pa.RecordBatch.from_pandas(df)\n        assert batch[0].equals(parr)\n\n        # Test ndarray of int64 values\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n\n        a1 = pa.array(arr, type=pa.time64('us'))\n        assert a1[0].as_py() == pytimes[0]\n\n        a2 = pa.array(arr * 1000, type=pa.time64('ns'))\n        assert a2[0].as_py() == pytimes[0]\n\n        a3 = pa.array((arr / 1000).astype('i4'),\n                      type=pa.time32('ms'))\n        assert a3[0].as_py() == pytimes[0].replace(microsecond=1000)\n\n        a4 = pa.array((arr / 1000000).astype('i4'),\n                      type=pa.time32('s'))\n        assert a4[0].as_py() == pytimes[0].replace(microsecond=0)\n\n    def test_arrow_time_to_pandas(self):\n        pytimes = [time(1, 2, 3, 1356),\n                   time(4, 5, 6, 1356),\n                   time(0, 0, 0)]\n\n        expected = np.array(pytimes[:2] + [None])\n        expected_ms = np.array([x.replace(microsecond=1000)\n                                for x in pytimes[:2]] +\n                               [None])\n        expected_s = np.array([x.replace(microsecond=0)\n                               for x in pytimes[:2]] +\n                              [None])\n\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n        arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                       dtype='int64')\n\n        null_mask = np.array([False, False, True], dtype=bool)\n\n        a1 = pa.array(arr, mask=null_mask, type=pa.time64('us'))\n        a2 = pa.array(arr * 1000, mask=null_mask,\n                      type=pa.time64('ns'))\n\n        a3 = pa.array((arr / 1000).astype('i4'), mask=null_mask,\n                      type=pa.time32('ms'))\n        a4 = pa.array((arr / 1000000).astype('i4'), mask=null_mask,\n                      type=pa.time32('s'))\n\n        names = ['time64[us]', 'time64[ns]', 'time32[ms]', 'time32[s]']\n        batch = pa.RecordBatch.from_arrays([a1, a2, a3, a4], names)\n\n        for arr, expected_values in [(a1, expected),\n                                     (a2, expected),\n                                     (a3, expected_ms),\n                                     (a4, expected_s)]:\n            result_pandas = arr.to_pandas()\n            assert (result_pandas.values == expected_values).all()\n\n        df = batch.to_pandas()\n        expected_df = pd.DataFrame({'time64[us]': expected,\n                                    'time64[ns]': expected,\n                                    'time32[ms]': expected_ms,\n                                    'time32[s]': expected_s},\n                                   columns=names)\n\n        tm.assert_frame_equal(df, expected_df)\n\n    def test_numpy_datetime64_columns(self):\n        datetime64_ns = np.array([\n            '2007-07-13T01:23:34.123456789',\n            None,\n            '2006-01-13T12:34:56.432539784',\n            '2010-08-13T05:46:57.437699912'],\n            dtype='datetime64[ns]')\n        _check_array_from_pandas_roundtrip(datetime64_ns)\n\n        datetime64_us = np.array([\n            '2007-07-13T01:23:34.123456',\n            None,\n            '2006-01-13T12:34:56.432539',\n            '2010-08-13T05:46:57.437699'],\n            dtype='datetime64[us]')\n        _check_array_from_pandas_roundtrip(datetime64_us)\n\n        datetime64_ms = np.array([\n            '2007-07-13T01:23:34.123',\n            None,\n            '2006-01-13T12:34:56.432',\n            '2010-08-13T05:46:57.437'],\n            dtype='datetime64[ms]')\n        _check_array_from_pandas_roundtrip(datetime64_ms)\n\n        datetime64_s = np.array([\n            '2007-07-13T01:23:34',\n            None,\n            '2006-01-13T12:34:56',\n            '2010-08-13T05:46:57'],\n            dtype='datetime64[s]')\n        _check_array_from_pandas_roundtrip(datetime64_s)\n\n    def test_timestamp_to_pandas_coerces_to_ns(self):\n        # non-ns timestamp gets cast to ns on conversion to pandas\n        if Version(pd.__version__) >= Version(\"2.0.0\"):\n            pytest.skip(\"pandas >= 2.0 supports non-nanosecond datetime64\")\n\n        arr = pa.array([1, 2, 3], pa.timestamp('ms'))\n        expected = pd.Series(pd.to_datetime([1, 2, 3], unit='ms'))\n        s = arr.to_pandas()\n        tm.assert_series_equal(s, expected)\n        arr = pa.chunked_array([arr])\n        s = arr.to_pandas()\n        tm.assert_series_equal(s, expected)\n\n    def test_timestamp_to_pandas_out_of_bounds(self):\n        # ARROW-7758 check for out of bounds timestamps for non-ns timestamps\n        # that end up getting coerced into ns timestamps.\n\n        for unit in ['s', 'ms', 'us']:\n            for tz in [None, 'America/New_York']:\n                arr = pa.array([datetime(1, 1, 1)], pa.timestamp(unit, tz=tz))\n                table = pa.table({'a': arr})\n\n                msg = \"would result in out of bounds timestamp\"\n                with pytest.raises(ValueError, match=msg):\n                    arr.to_pandas(coerce_temporal_nanoseconds=True)\n\n                with pytest.raises(ValueError, match=msg):\n                    table.to_pandas(coerce_temporal_nanoseconds=True)\n\n                with pytest.raises(ValueError, match=msg):\n                    # chunked array\n                    table.column('a').to_pandas(coerce_temporal_nanoseconds=True)\n\n                # just ensure those don't give an error, but do not\n                # check actual garbage output\n                arr.to_pandas(safe=False, coerce_temporal_nanoseconds=True)\n                table.to_pandas(safe=False, coerce_temporal_nanoseconds=True)\n                table.column('a').to_pandas(\n                    safe=False, coerce_temporal_nanoseconds=True)\n\n    def test_timestamp_to_pandas_empty_chunked(self):\n        # ARROW-7907 table with chunked array with 0 chunks\n        table = pa.table({'a': pa.chunked_array([], type=pa.timestamp('us'))})\n        result = table.to_pandas()\n        expected = pd.DataFrame({'a': pd.Series([], dtype=\"datetime64[us]\")})\n        tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize('dtype', [pa.date32(), pa.date64()])\n    def test_numpy_datetime64_day_unit(self, dtype):\n        datetime64_d = np.array([\n            '2007-07-13',\n            None,\n            '2006-01-15',\n            '2010-08-19'],\n            dtype='datetime64[D]')\n        _check_array_from_pandas_roundtrip(datetime64_d, type=dtype)\n\n    def test_array_from_pandas_date_with_mask(self):\n        m = np.array([True, False, True])\n        data = pd.Series([\n            date(1990, 1, 1),\n            date(1991, 1, 1),\n            date(1992, 1, 1)\n        ])\n\n        result = pa.Array.from_pandas(data, mask=m)\n\n        expected = pd.Series([None, date(1991, 1, 1), None])\n        assert pa.Array.from_pandas(expected).equals(result)\n\n    @pytest.mark.skipif(\n        Version('1.16.0') <= Version(np.__version__) < Version('1.16.1'),\n        reason='Until numpy/numpy#12745 is resolved')\n    def test_fixed_offset_timezone(self):\n        df = pd.DataFrame({\n            'a': [\n                pd.Timestamp('2012-11-11 00:00:00+01:00'),\n                pd.NaT\n            ]\n        })\n        # 'check_dtype=False' because pandas >= 2 uses datetime.timezone\n        # instead of pytz.FixedOffset, and thus the dtype is not exactly\n        # identical (pyarrow still defaults to pytz)\n        # TODO remove if https://github.com/apache/arrow/issues/15047 is fixed\n        _check_pandas_roundtrip(df, check_dtype=False)\n\n    @pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n    def test_timedeltas_no_nulls(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            unit = 'ns'\n        df = pd.DataFrame({\n            'timedelta64': np.array([0, 3600000000000, 7200000000000],\n                                    dtype=f'timedelta64[{unit}]')\n        })\n        field = pa.field('timedelta64', pa.duration(unit))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    @pytest.mark.parametrize(\"unit\", ['s', 'ms', 'us', 'ns'])\n    def test_timedeltas_nulls(self, unit):\n        if Version(pd.__version__) < Version(\"2.0.0\"):\n            unit = 'ns'\n        df = pd.DataFrame({\n            'timedelta64': np.array([0, None, 7200000000000],\n                                    dtype=f'timedelta64[{unit}]')\n        })\n        field = pa.field('timedelta64', pa.duration(unit))\n        schema = pa.schema([field])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema,\n        )\n\n    def test_month_day_nano_interval(self):\n        from pandas.tseries.offsets import DateOffset\n        df = pd.DataFrame({\n            'date_offset': [None,\n                            DateOffset(days=3600, months=3600, microseconds=3,\n                                       nanoseconds=600)]\n        })\n        schema = pa.schema([('date_offset', pa.month_day_nano_interval())])\n        _check_pandas_roundtrip(\n            df,\n            expected_schema=schema)\n\n\n# ----------------------------------------------------------------------\n# Conversion tests for string and binary types.\n\n\nclass TestConvertStringLikeTypes:\n\n    def test_pandas_unicode(self):\n        repeats = 1000\n        values = ['foo', None, 'bar', 'ma\u00f1ana', np.nan]\n        df = pd.DataFrame({'strings': values * repeats})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        ex_values = ['foo', None, 'bar', 'ma\u00f1ana', None]\n        expected = pd.DataFrame({'strings': ex_values * repeats})\n\n        _check_pandas_roundtrip(df, expected=expected, expected_schema=schema)\n\n    def test_bytes_to_binary(self):\n        values = ['qux', b'foo', None, bytearray(b'barz'), 'qux', np.nan]\n        df = pd.DataFrame({'strings': values})\n\n        table = pa.Table.from_pandas(df)\n        assert table[0].type == pa.binary()\n\n        values2 = [b'qux', b'foo', None, b'barz', b'qux', None]\n        expected = pd.DataFrame({'strings': values2})\n        _check_pandas_roundtrip(df, expected)\n\n    @pytest.mark.large_memory\n    def test_bytes_exceed_2gb(self):\n        v1 = b'x' * 100000000\n        v2 = b'x' * 147483646\n\n        # ARROW-2227, hit exactly 2GB on the nose\n        df = pd.DataFrame({\n            'strings': [v1] * 20 + [v2] + ['x'] * 20\n        })\n        arr = pa.array(df['strings'])\n        assert isinstance(arr, pa.ChunkedArray)\n        assert arr.num_chunks == 2\n        arr = None\n\n        table = pa.Table.from_pandas(df)\n        assert table[0].num_chunks == 2\n\n    @pytest.mark.large_memory\n    @pytest.mark.parametrize('char', ['x', b'x'])\n    def test_auto_chunking_pandas_series_of_strings(self, char):\n        # ARROW-2367\n        v1 = char * 100000000\n        v2 = char * 147483646\n\n        df = pd.DataFrame({\n            'strings': [[v1]] * 20 + [[v2]] + [[b'x']]\n        })\n        arr = pa.array(df['strings'], from_pandas=True)\n        arr.validate(full=True)\n        assert isinstance(arr, pa.ChunkedArray)\n        assert arr.num_chunks == 2\n        assert len(arr.chunk(0)) == 21\n        assert len(arr.chunk(1)) == 1\n\n    def test_fixed_size_bytes(self):\n        values = [b'foo', None, bytearray(b'bar'), None, None, b'hey']\n        df = pd.DataFrame({'strings': values})\n        schema = pa.schema([pa.field('strings', pa.binary(3))])\n        table = pa.Table.from_pandas(df, schema=schema)\n        assert table.schema[0].type == schema[0].type\n        assert table.schema[0].name == schema[0].name\n        result = table.to_pandas()\n        tm.assert_frame_equal(result, df)\n\n    def test_fixed_size_bytes_does_not_accept_varying_lengths(self):\n        values = [b'foo', None, b'ba', None, None, b'hey']\n        df = pd.DataFrame({'strings': values})\n        schema = pa.schema([pa.field('strings', pa.binary(3))])\n        with pytest.raises(pa.ArrowInvalid):\n            pa.Table.from_pandas(df, schema=schema)\n\n    def test_variable_size_bytes(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.binary())\n\n    def test_binary_from_bytearray(self):\n        s = pd.Series([bytearray(b'123'), bytearray(b''), bytearray(b'a'),\n                       None])\n        # Explicitly set type\n        _check_series_roundtrip(s, type_=pa.binary())\n        # Infer type from bytearrays\n        _check_series_roundtrip(s, expected_pa_type=pa.binary())\n\n    def test_large_binary(self):\n        s = pd.Series([b'123', b'', b'a', None])\n        _check_series_roundtrip(s, type_=pa.large_binary())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.large_binary())]))\n\n    def test_large_string(self):\n        s = pd.Series(['123', '', 'a', None])\n        _check_series_roundtrip(s, type_=pa.large_string())\n        df = pd.DataFrame({'a': s})\n        _check_pandas_roundtrip(\n            df, schema=pa.schema([('a', pa.large_string())]))\n\n    def test_table_empty_str(self):\n        values = ['', '', '', '', '']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result1 = table.to_pandas(strings_to_categorical=False)\n        expected1 = pd.DataFrame({'strings': values})\n        tm.assert_frame_equal(result1, expected1, check_dtype=True)\n\n        result2 = table.to_pandas(strings_to_categorical=True)\n        expected2 = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result2, expected2, check_dtype=True)\n\n    def test_selective_categoricals(self):\n        values = ['', '', '', '', '']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n        expected_str = pd.DataFrame({'strings': values})\n        expected_cat = pd.DataFrame({'strings': pd.Categorical(values)})\n\n        result1 = table.to_pandas(categories=['strings'])\n        tm.assert_frame_equal(result1, expected_cat, check_dtype=True)\n        result2 = table.to_pandas(categories=[])\n        tm.assert_frame_equal(result2, expected_str, check_dtype=True)\n        result3 = table.to_pandas(categories=('strings',))\n        tm.assert_frame_equal(result3, expected_cat, check_dtype=True)\n        result4 = table.to_pandas(categories=tuple())\n        tm.assert_frame_equal(result4, expected_str, check_dtype=True)\n\n    def test_to_pandas_categorical_zero_length(self):\n        # ARROW-3586\n        array = pa.array([], type=pa.int32())\n        table = pa.Table.from_arrays(arrays=[array], names=['col'])\n        # This would segfault under 0.11.0\n        table.to_pandas(categories=['col'])\n\n    def test_to_pandas_categories_already_dictionary(self):\n        # Showed up in ARROW-6434, ARROW-6435\n        array = pa.array(['foo', 'foo', 'foo', 'bar']).dictionary_encode()\n        table = pa.Table.from_arrays(arrays=[array], names=['col'])\n        result = table.to_pandas(categories=['col'])\n        assert table.to_pandas().equals(result)\n\n    def test_table_str_to_categorical_without_na(self):\n        values = ['a', 'a', 'b', 'b', 'c']\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result = table.to_pandas(strings_to_categorical=True)\n        expected = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result, expected, check_dtype=True)\n\n        with pytest.raises(pa.ArrowInvalid):\n            table.to_pandas(strings_to_categorical=True,\n                            zero_copy_only=True)\n\n    def test_table_str_to_categorical_with_na(self):\n        values = [None, 'a', 'b', np.nan]\n        df = pd.DataFrame({'strings': values})\n        field = pa.field('strings', pa.string())\n        schema = pa.schema([field])\n        table = pa.Table.from_pandas(df, schema=schema)\n\n        result = table.to_pandas(strings_to_categorical=True)\n        expected = pd.DataFrame({'strings': pd.Categorical(values)})\n        tm.assert_frame_equal(result, expected, check_dtype=True)\n\n        with pytest.raises(pa.ArrowInvalid):\n            table.to_pandas(strings_to_categorical=True,\n                            zero_copy_only=True)\n\n    # Regression test for ARROW-2101\n    def test_array_of_bytes_to_strings(self):\n        converted = pa.array(np.array([b'x'], dtype=object), pa.string())\n        assert converted.type == pa.string()\n\n    # Make sure that if an ndarray of bytes is passed to the array\n    # constructor and the type is string, it will fail if those bytes\n    # cannot be converted to utf-8\n    def test_array_of_bytes_to_strings_bad_data(self):\n        with pytest.raises(\n                pa.lib.ArrowInvalid,\n                match=\"was not a utf8 string\"):\n            pa.array(np.array([b'\\x80\\x81'], dtype=object), pa.string())\n\n    def test_numpy_string_array_to_fixed_size_binary(self):\n        arr = np.array([b'foo', b'bar', b'baz'], dtype='|S3')\n\n        converted = pa.array(arr, type=pa.binary(3))\n        expected = pa.array(list(arr), type=pa.binary(3))\n        assert converted.equals(expected)\n\n        mask = np.array([False, True, False])\n        converted = pa.array(arr, type=pa.binary(3), mask=mask)\n        expected = pa.array([b'foo', None, b'baz'], type=pa.binary(3))\n        assert converted.equals(expected)\n\n        with pytest.raises(pa.lib.ArrowInvalid,\n                           match=r'Got bytestring of length 3 \\(expected 4\\)'):\n            arr = np.array([b'foo', b'bar', b'baz'], dtype='|S3')\n            pa.array(arr, type=pa.binary(4))\n\n        with pytest.raises(\n                pa.lib.ArrowInvalid,\n                match=r'Got bytestring of length 12 \\(expected 3\\)'):\n            arr = np.array([b'foo', b'bar', b'baz'], dtype='|U3')\n            pa.array(arr, type=pa.binary(3))\n\n\nclass TestConvertDecimalTypes:\n    \"\"\"\n    Conversion test for decimal types.\n    \"\"\"\n    decimal32 = [\n        decimal.Decimal('-1234.123'),\n        decimal.Decimal('1234.439')\n    ]\n    decimal64 = [\n        decimal.Decimal('-129934.123331'),\n        decimal.Decimal('129534.123731')\n    ]\n    decimal128 = [\n        decimal.Decimal('394092382910493.12341234678'),\n        decimal.Decimal('-314292388910493.12343437128')\n    ]\n\n    @pytest.mark.parametrize(('values', 'expected_type'), [\n        pytest.param(decimal32, pa.decimal128(7, 3), id='decimal32'),\n        pytest.param(decimal64, pa.decimal128(12, 6), id='decimal64'),\n        pytest.param(decimal128, pa.decimal128(26, 11), id='decimal128')\n    ])\n    def test_decimal_from_pandas(self, values, expected_type):\n        expected = pd.DataFrame({'decimals': values})\n        table = pa.Table.from_pandas(expected, preserve_index=False)\n        field = pa.field('decimals', expected_type)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = pa.schema([field], metadata=table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n    @pytest.mark.parametrize('values', [\n        pytest.param(decimal32, id='decimal32'),\n        pytest.param(decimal64, id='decimal64'),\n        pytest.param(decimal128, id='decimal128')\n    ])\n    def test_decimal_to_pandas(self, values):\n        expected = pd.DataFrame({'decimals': values})\n        converted = pa.Table.from_pandas(expected)\n        df = converted.to_pandas()\n        tm.assert_frame_equal(df, expected)\n\n    def test_decimal_fails_with_truncation(self):\n        data1 = [decimal.Decimal('1.234')]\n        type1 = pa.decimal128(10, 2)\n        with pytest.raises(pa.ArrowInvalid):\n            pa.array(data1, type=type1)\n\n        data2 = [decimal.Decimal('1.2345')]\n        type2 = pa.decimal128(10, 3)\n        with pytest.raises(pa.ArrowInvalid):\n            pa.array(data2, type=type2)\n\n    def test_decimal_with_different_precisions(self):\n        data = [\n            decimal.Decimal('0.01'),\n            decimal.Decimal('0.001'),\n        ]\n        series = pd.Series(data)\n        array = pa.array(series)\n        assert array.to_pylist() == data\n        assert array.type == pa.decimal128(3, 3)\n\n        array = pa.array(data, type=pa.decimal128(12, 5))\n        expected = [decimal.Decimal('0.01000'), decimal.Decimal('0.00100')]\n        assert array.to_pylist() == expected\n\n    def test_decimal_with_None_explicit_type(self):\n        series = pd.Series([decimal.Decimal('3.14'), None])\n        _check_series_roundtrip(series, type_=pa.decimal128(12, 5))\n\n        # Test that having all None values still produces decimal array\n        series = pd.Series([None] * 2)\n        _check_series_roundtrip(series, type_=pa.decimal128(12, 5))\n\n    def test_decimal_with_None_infer_type(self):\n        series = pd.Series([decimal.Decimal('3.14'), None])\n        _check_series_roundtrip(series, expected_pa_type=pa.decimal128(3, 2))\n\n    def test_strided_objects(self, tmpdir):\n        # see ARROW-3053\n        data = {\n            'a': {0: 'a'},\n            'b': {0: decimal.Decimal('0.0')}\n        }\n\n        # This yields strided objects\n        df = pd.DataFrame.from_dict(data)\n        _check_pandas_roundtrip(df)\n\n\nclass TestConvertListTypes:\n    \"\"\"\n    Conversion tests for list<> types.\n    \"\"\"\n\n    def test_column_of_arrays(self):\n        df, schema = dataframe_with_arrays()\n        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = schema.with_metadata(table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        for column in df.columns:\n            field = schema.field(column)\n            _check_array_roundtrip(df[column], type=field.type)\n\n    def test_column_of_arrays_to_py(self):\n        # Test regression in ARROW-1199 not caught in above test\n        dtype = 'i1'\n        arr = np.array([\n            np.arange(10, dtype=dtype),\n            np.arange(5, dtype=dtype),\n            None,\n            np.arange(1, dtype=dtype)\n        ], dtype=object)\n        type_ = pa.list_(pa.int8())\n        parr = pa.array(arr, type=type_)\n\n        assert parr[0].as_py() == list(range(10))\n        assert parr[1].as_py() == list(range(5))\n        assert parr[2].as_py() is None\n        assert parr[3].as_py() == [0]\n\n    def test_column_of_boolean_list(self):\n        # ARROW-4370: Table to pandas conversion fails for list of bool\n        array = pa.array([[True, False], [True]], type=pa.list_(pa.bool_()))\n        table = pa.Table.from_arrays([array], names=['col1'])\n        df = table.to_pandas()\n\n        expected_df = pd.DataFrame({'col1': [[True, False], [True]]})\n        tm.assert_frame_equal(df, expected_df)\n\n        s = table[0].to_pandas()\n        tm.assert_series_equal(pd.Series(s), df['col1'], check_names=False)\n\n    def test_column_of_decimal_list(self):\n        array = pa.array([[decimal.Decimal('1'), decimal.Decimal('2')],\n                          [decimal.Decimal('3.3')]],\n                         type=pa.list_(pa.decimal128(2, 1)))\n        table = pa.Table.from_arrays([array], names=['col1'])\n        df = table.to_pandas()\n\n        expected_df = pd.DataFrame(\n            {'col1': [[decimal.Decimal('1'), decimal.Decimal('2')],\n                      [decimal.Decimal('3.3')]]})\n        tm.assert_frame_equal(df, expected_df)\n\n    def test_nested_types_from_ndarray_null_entries(self):\n        # Root cause of ARROW-6435\n        s = pd.Series(np.array([np.nan, np.nan], dtype=object))\n\n        for ty in [pa.list_(pa.int64()),\n                   pa.large_list(pa.int64()),\n                   pa.struct([pa.field('f0', 'int32')])]:\n            result = pa.array(s, type=ty)\n            expected = pa.array([None, None], type=ty)\n            assert result.equals(expected)\n\n            with pytest.raises(TypeError):\n                pa.array(s.values, type=ty)\n\n    def test_column_of_lists(self):\n        df, schema = dataframe_with_lists()\n        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n        # schema's metadata is generated by from_pandas conversion\n        expected_schema = schema.with_metadata(table.schema.metadata)\n        assert table.schema.equals(expected_schema)\n\n        for column in df.columns:\n            field = schema.field(column)\n            _check_array_roundtrip(df[column], type=field.type)\n\n    def test_column_of_lists_first_empty(self):\n        # ARROW-2124\n        num_lists = [[], [2, 3, 4], [3, 6, 7, 8], [], [2]]\n        series = pd.Series([np.array(s, dtype=float) for s in num_lists])\n        arr = pa.array(series)\n        result = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(result, series)\n\n    def test_column_of_lists_chunked(self):\n        # ARROW-1357\n        df = pd.DataFrame({\n            'lists': np.array([\n                [1, 2],\n                None,\n                [2, 3],\n                [4, 5],\n                [6, 7],\n                [8, 9]\n            ], dtype=object)\n        })\n\n        schema = pa.schema([\n            pa.field('lists', pa.list_(pa.int64()))\n        ])\n\n        t1 = pa.Table.from_pandas(df[:2], schema=schema)\n        t2 = pa.Table.from_pandas(df[2:], schema=schema)\n\n        table = pa.concat_tables([t1, t2])\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_empty_column_of_lists_chunked(self):\n        df = pd.DataFrame({\n            'lists': np.array([], dtype=object)\n        })\n\n        schema = pa.schema([\n            pa.field('lists', pa.list_(pa.int64()))\n        ])\n\n        table = pa.Table.from_pandas(df, schema=schema)\n        result = table.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_column_of_lists_chunked2(self):\n        data1 = [[0, 1], [2, 3], [4, 5], [6, 7], [10, 11],\n                 [12, 13], [14, 15], [16, 17]]\n        data2 = [[8, 9], [18, 19]]\n\n        a1 = pa.array(data1)\n        a2 = pa.array(data2)\n\n        t1 = pa.Table.from_arrays([a1], names=['a'])\n        t2 = pa.Table.from_arrays([a2], names=['a'])\n\n        concatenated = pa.concat_tables([t1, t2])\n\n        result = concatenated.to_pandas()\n        expected = pd.DataFrame({'a': data1 + data2})\n\n        tm.assert_frame_equal(result, expected)\n\n    def test_column_of_lists_strided(self):\n        df, schema = dataframe_with_lists()\n        df = pd.concat([df] * 6, ignore_index=True)\n\n        arr = df['int64'].values[::3]\n        assert arr.strides[0] != 8\n\n        _check_array_roundtrip(arr)\n\n    def test_nested_lists_all_none(self):\n        data = np.array([[None, None], None], dtype=object)\n\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.null())\n\n        data2 = np.array([None, None, [None, None],\n                          np.array([None, None], dtype=object)],\n                         dtype=object)\n        arr = pa.array(data2)\n        expected = pa.array([None, None, [None, None], [None, None]])\n        assert arr.equals(expected)\n\n    def test_nested_lists_all_empty(self):\n        # ARROW-2128\n        data = pd.Series([[], [], []])\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.null())\n\n    def test_nested_list_first_empty(self):\n        # ARROW-2711\n        data = pd.Series([[], [\"a\"]])\n        arr = pa.array(data)\n        expected = pa.array(list(data))\n        assert arr.equals(expected)\n        assert arr.type == pa.list_(pa.string())\n\n    def test_nested_smaller_ints(self):\n        # ARROW-1345, ARROW-2008, there were some type inference bugs happening\n        # before\n        data = pd.Series([np.array([1, 2, 3], dtype='i1'), None])\n        result = pa.array(data)\n        result2 = pa.array(data.values)\n        expected = pa.array([[1, 2, 3], None], type=pa.list_(pa.int8()))\n        assert result.equals(expected)\n        assert result2.equals(expected)\n\n        data3 = pd.Series([np.array([1, 2, 3], dtype='f4'), None])\n        result3 = pa.array(data3)\n        expected3 = pa.array([[1, 2, 3], None], type=pa.list_(pa.float32()))\n        assert result3.equals(expected3)\n\n    def test_infer_lists(self):\n        data = OrderedDict([\n            ('nan_ints', [[np.nan, 1], [2, 3]]),\n            ('ints', [[0, 1], [2, 3]]),\n            ('strs', [[None, 'b'], ['c', 'd']]),\n            ('nested_strs', [[[None, 'b'], ['c', 'd']], None])\n        ])\n        df = pd.DataFrame(data)\n\n        expected_schema = pa.schema([\n            pa.field('nan_ints', pa.list_(pa.int64())),\n            pa.field('ints', pa.list_(pa.int64())),\n            pa.field('strs', pa.list_(pa.string())),\n            pa.field('nested_strs', pa.list_(pa.list_(pa.string())))\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n    def test_fixed_size_list(self):\n        # ARROW-7365\n        fixed_ty = pa.list_(pa.int64(), list_size=4)\n        variable_ty = pa.list_(pa.int64())\n\n        data = [[0, 1, 2, 3], None, [4, 5, 6, 7], [8, 9, 10, 11]]\n        fixed_arr = pa.array(data, type=fixed_ty)\n        variable_arr = pa.array(data, type=variable_ty)\n\n        result = fixed_arr.to_pandas()\n        expected = variable_arr.to_pandas()\n\n        for left, right in zip(result, expected):\n            if left is None:\n                assert right is None\n            npt.assert_array_equal(left, right)\n\n    def test_infer_numpy_array(self):\n        data = OrderedDict([\n            ('ints', [\n                np.array([0, 1], dtype=np.int64),\n                np.array([2, 3], dtype=np.int64)\n            ])\n        ])\n        df = pd.DataFrame(data)\n        expected_schema = pa.schema([\n            pa.field('ints', pa.list_(pa.int64()))\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n    def test_to_list_of_structs_pandas(self):\n        ints = pa.array([1, 2, 3], pa.int32())\n        strings = pa.array([['a', 'b'], ['c', 'd'], ['e', 'f']],\n                           pa.list_(pa.string()))\n        structs = pa.StructArray.from_arrays([ints, strings], ['f1', 'f2'])\n        data = pa.ListArray.from_arrays([0, 1, 3], structs)\n\n        expected = pd.Series([\n            [{'f1': 1, 'f2': ['a', 'b']}],\n            [{'f1': 2, 'f2': ['c', 'd']},\n             {'f1': 3, 'f2': ['e', 'f']}]\n        ])\n\n        series = pd.Series(data.to_pandas())\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n\n    def test_to_list_of_maps_pandas(self):\n        if ((Version(np.__version__) >= Version(\"1.25.0.dev0\")) and\n                (Version(pd.__version__) < Version(\"2.0.0\"))):\n            # TODO: regression in pandas with numpy 1.25dev\n            # https://github.com/pandas-dev/pandas/issues/50360\n            pytest.skip(\"Regression in pandas with numpy 1.25\")\n        data = [\n            [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n            [[('baz', []), ('qux', None), ('quux', [None, 'e'])], [('quz', ['f', 'g'])]]\n        ]\n        arr = pa.array(data, pa.list_(pa.map_(pa.utf8(), pa.list_(pa.utf8()))))\n        series = arr.to_pandas()\n        expected = pd.Series(data)\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n\n    def test_to_list_of_maps_pandas_sliced(self):\n        \"\"\"\n        A slightly more rigorous test for chunk/slice combinations\n        \"\"\"\n\n        if ((Version(np.__version__) >= Version(\"1.25.0.dev0\")) and\n                (Version(pd.__version__) < Version(\"2.0.0\"))):\n            # TODO: regression in pandas with numpy 1.25dev\n            # https://github.com/pandas-dev/pandas/issues/50360\n            pytest.skip(\"Regression in pandas with numpy 1.25\")\n\n        keys = pa.array(['ignore', 'foo', 'bar', 'baz',\n                         'qux', 'quux', 'ignore']).slice(1, 5)\n        items = pa.array(\n            [['ignore'], ['ignore'], ['a', 'b'], ['c', 'd'], [], None, [None, 'e']],\n            pa.list_(pa.string()),\n        ).slice(2, 5)\n        map = pa.MapArray.from_arrays([0, 2, 4], keys, items)\n        arr = pa.ListArray.from_arrays([0, 1, 2], map)\n\n        series = arr.to_pandas()\n        expected = pd.Series([\n            [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n            [[('baz', []), ('qux', None)]],\n        ])\n\n        series_sliced = arr.slice(1, 2).to_pandas()\n        expected_sliced = pd.Series([\n            [[('baz', []), ('qux', None)]],\n        ])\n\n        # pandas.testing generates a\n        # DeprecationWarning: elementwise comparison failed\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(series, expected)\n            tm.assert_series_equal(series_sliced, expected_sliced)\n\n    @pytest.mark.parametrize('t,data,expected', [\n        (\n            pa.int64,\n            [[1, 2], [3], None],\n            [None, [3], None]\n        ),\n        (\n            pa.string,\n            [['aaa', 'bb'], ['c'], None],\n            [None, ['c'], None]\n        ),\n        (\n            pa.null,\n            [[None, None], [None], None],\n            [None, [None], None]\n        )\n    ])\n    def test_array_from_pandas_typed_array_with_mask(self, t, data, expected):\n        m = np.array([True, False, True])\n\n        s = pd.Series(data)\n        result = pa.Array.from_pandas(s, mask=m, type=pa.list_(t()))\n\n        assert pa.Array.from_pandas(expected,\n                                    type=pa.list_(t())).equals(result)\n\n    def test_empty_list_roundtrip(self):\n        empty_list_array = np.empty((3,), dtype=object)\n        empty_list_array.fill([])\n\n        df = pd.DataFrame({'a': np.array(['1', '2', '3']),\n                           'b': empty_list_array})\n        tbl = pa.Table.from_pandas(df)\n\n        result = tbl.to_pandas()\n\n        tm.assert_frame_equal(result, df)\n\n    def test_array_from_nested_arrays(self):\n        df, schema = dataframe_with_arrays()\n        for field in schema:\n            arr = df[field.name].values\n            expected = pa.array(list(arr), type=field.type)\n            result = pa.array(arr)\n            assert result.type == field.type  # == list<scalar>\n            assert result.equals(expected)\n\n    def test_nested_large_list(self):\n        s = (pa.array([[[1, 2, 3], [4]], None],\n                      type=pa.large_list(pa.large_list(pa.int64())))\n             .to_pandas())\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\",\n                                    \"Creating an ndarray from ragged nested\",\n                                    _np_VisibleDeprecationWarning)\n            warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                    DeprecationWarning)\n            tm.assert_series_equal(\n                s, pd.Series([[[1, 2, 3], [4]], None], dtype=object),\n                check_names=False)\n\n    def test_large_binary_list(self):\n        for list_type_factory in (pa.list_, pa.large_list):\n            s = (pa.array([[\"aa\", \"bb\"], None, [\"cc\"], []],\n                          type=list_type_factory(pa.large_binary()))\n                 .to_pandas())\n            tm.assert_series_equal(\n                s, pd.Series([[b\"aa\", b\"bb\"], None, [b\"cc\"], []]),\n                check_names=False)\n            s = (pa.array([[\"aa\", \"bb\"], None, [\"cc\"], []],\n                          type=list_type_factory(pa.large_string()))\n                 .to_pandas())\n            tm.assert_series_equal(\n                s, pd.Series([[\"aa\", \"bb\"], None, [\"cc\"], []]),\n                check_names=False)\n\n    def test_list_of_dictionary(self):\n        child = pa.array([\"foo\", \"bar\", None, \"foo\"]).dictionary_encode()\n        arr = pa.ListArray.from_arrays([0, 1, 3, 3, 4], child)\n\n        # Expected a Series of lists\n        expected = pd.Series(arr.to_pylist())\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n        # Same but with nulls\n        arr = arr.take([0, 1, None, 3])\n        expected[2] = None\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n    @pytest.mark.large_memory\n    def test_auto_chunking_on_list_overflow(self):\n        # ARROW-9976\n        n = 2**21\n        df = pd.DataFrame.from_dict({\n            \"a\": list(np.zeros((n, 2**10), dtype='uint8')),\n            \"b\": range(n)\n        })\n        table = pa.Table.from_pandas(df)\n        table.validate(full=True)\n\n        column_a = table[0]\n        assert column_a.num_chunks == 2\n        assert len(column_a.chunk(0)) == 2**21 - 1\n        assert len(column_a.chunk(1)) == 1\n\n    def test_map_array_roundtrip(self):\n        data = [[(b'a', 1), (b'b', 2)],\n                [(b'c', 3)],\n                [(b'd', 4), (b'e', 5), (b'f', 6)],\n                [(b'g', 7)]]\n\n        df = pd.DataFrame({\"map\": data})\n        schema = pa.schema([(\"map\", pa.map_(pa.binary(), pa.int32()))])\n\n        _check_pandas_roundtrip(df, schema=schema)\n\n    def test_map_array_chunked(self):\n        data1 = [[(b'a', 1), (b'b', 2)],\n                 [(b'c', 3)],\n                 [(b'd', 4), (b'e', 5), (b'f', 6)],\n                 [(b'g', 7)]]\n        data2 = [[(k, v * 2) for k, v in row] for row in data1]\n\n        arr1 = pa.array(data1, type=pa.map_(pa.binary(), pa.int32()))\n        arr2 = pa.array(data2, type=pa.map_(pa.binary(), pa.int32()))\n        arr = pa.chunked_array([arr1, arr2])\n\n        expected = pd.Series(data1 + data2)\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_map_array_with_nulls(self):\n        data = [[(b'a', 1), (b'b', 2)],\n                None,\n                [(b'd', 4), (b'e', 5), (b'f', None)],\n                [(b'g', 7)]]\n\n        # None value in item array causes upcast to float\n        expected = [[(k, float(v) if v is not None else None) for k, v in row]\n                    if row is not None else None for row in data]\n        expected = pd.Series(expected)\n\n        arr = pa.array(data, type=pa.map_(pa.binary(), pa.int32()))\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_map_array_dictionary_encoded(self):\n        offsets = pa.array([0, 3, 5])\n        items = pa.array(['a', 'b', 'c', 'a', 'd']).dictionary_encode()\n        keys = pa.array(list(range(len(items))))\n        arr = pa.MapArray.from_arrays(offsets, keys, items)\n\n        # Dictionary encoded values converted to dense\n        expected = pd.Series(\n            [[(0, 'a'), (1, 'b'), (2, 'c')], [(3, 'a'), (4, 'd')]])\n\n        actual = arr.to_pandas()\n        tm.assert_series_equal(actual, expected, check_names=False)\n\n    def test_list_no_duplicate_base(self):\n        # ARROW-18400\n        arr = pa.array([[1, 2], [3, 4, 5], None, [6, None], [7, 8]])\n        chunked_arr = pa.chunked_array([arr.slice(0, 3), arr.slice(3, 1)])\n\n        np_arr = chunked_arr.to_numpy()\n\n        expected = np.array([[1., 2.], [3., 4., 5.], None,\n                            [6., np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n        expected_base = np.array([[1., 2., 3., 4., 5., 6., np.nan]])\n        npt.assert_array_equal(np_arr[0].base, expected_base)\n\n        np_arr_sliced = chunked_arr.slice(1, 3).to_numpy()\n\n        expected = np.array([[3, 4, 5], None, [6, np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr_sliced, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n        expected_base = np.array([[3., 4., 5., 6., np.nan]])\n        npt.assert_array_equal(np_arr_sliced[0].base, expected_base)\n\n    def test_list_values_behind_null(self):\n        arr = pa.ListArray.from_arrays(\n            offsets=pa.array([0, 2, 4, 6]),\n            values=pa.array([1, 2, 99, 99, 3, None]),\n            mask=pa.array([False, True, False])\n        )\n        np_arr = arr.to_numpy(zero_copy_only=False)\n\n        expected = np.array([[1., 2.], None, [3., np.nan]], dtype=\"object\")\n        for left, right in zip(np_arr, expected):\n            if right is None:\n                assert left == right\n            else:\n                npt.assert_array_equal(left, right)\n\n\nclass TestConvertStructTypes:\n    \"\"\"\n    Conversion tests for struct types.\n    \"\"\"\n\n    def test_pandas_roundtrip(self):\n        df = pd.DataFrame({'dicts': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n\n        expected_schema = pa.schema([\n            ('dicts', pa.struct([('a', pa.int64()), ('b', pa.int64())])),\n        ])\n\n        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n\n        # specifying schema explicitly in from_pandas\n        _check_pandas_roundtrip(\n            df, schema=expected_schema, expected_schema=expected_schema)\n\n    def test_to_pandas(self):\n        ints = pa.array([None, 2, 3], type=pa.int64())\n        strs = pa.array(['a', None, 'c'], type=pa.string())\n        bools = pa.array([True, False, None], type=pa.bool_())\n        arr = pa.StructArray.from_arrays(\n            [ints, strs, bools],\n            ['ints', 'strs', 'bools'])\n\n        expected = pd.Series([\n            {'ints': None, 'strs': 'a', 'bools': True},\n            {'ints': 2, 'strs': None, 'bools': False},\n            {'ints': 3, 'strs': 'c', 'bools': None},\n        ])\n\n        series = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(series, expected)\n\n    def test_to_pandas_multiple_chunks(self):\n        # ARROW-11855\n        gc.collect()\n        bytes_start = pa.total_allocated_bytes()\n        ints1 = pa.array([1], type=pa.int64())\n        ints2 = pa.array([2], type=pa.int64())\n        arr1 = pa.StructArray.from_arrays([ints1], ['ints'])\n        arr2 = pa.StructArray.from_arrays([ints2], ['ints'])\n        arr = pa.chunked_array([arr1, arr2])\n\n        expected = pd.Series([\n            {'ints': 1},\n            {'ints': 2}\n        ])\n\n        series = pd.Series(arr.to_pandas())\n        tm.assert_series_equal(series, expected)\n\n        del series\n        del arr\n        del arr1\n        del arr2\n        del ints1\n        del ints2\n        bytes_end = pa.total_allocated_bytes()\n        assert bytes_end == bytes_start\n\n    def test_from_numpy(self):\n        dt = np.dtype([('x', np.int32),\n                       (('y_title', 'y'), np.bool_)])\n        ty = pa.struct([pa.field('x', pa.int32()),\n                        pa.field('y', pa.bool_())])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([(42, True), (43, False)], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [{'x': 42, 'y': True},\n                                   {'x': 43, 'y': False}]\n\n        # With mask\n        arr = pa.array(data, mask=np.bool_([False, True]), type=ty)\n        assert arr.to_pylist() == [{'x': 42, 'y': True}, None]\n\n        # Trivial struct type\n        dt = np.dtype([])\n        ty = pa.struct([])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([(), ()], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [{}, {}]\n\n    def test_from_numpy_nested(self):\n        # Note: an object field inside a struct\n        dt = np.dtype([('x', np.dtype([('xx', np.int8),\n                                       ('yy', np.bool_)])),\n                       ('y', np.int16),\n                       ('z', np.object_)])\n        # Note: itemsize is not a multiple of sizeof(object)\n        assert dt.itemsize == 12\n        ty = pa.struct([pa.field('x', pa.struct([pa.field('xx', pa.int8()),\n                                                 pa.field('yy', pa.bool_())])),\n                        pa.field('y', pa.int16()),\n                        pa.field('z', pa.string())])\n\n        data = np.array([], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == []\n\n        data = np.array([\n            ((1, True), 2, 'foo'),\n            ((3, False), 4, 'bar')], dtype=dt)\n        arr = pa.array(data, type=ty)\n        assert arr.to_pylist() == [\n            {'x': {'xx': 1, 'yy': True}, 'y': 2, 'z': 'foo'},\n            {'x': {'xx': 3, 'yy': False}, 'y': 4, 'z': 'bar'}]\n\n    @pytest.mark.slow\n    @pytest.mark.large_memory\n    def test_from_numpy_large(self):\n        # Exercise rechunking + nulls\n        target_size = 3 * 1024**3  # 4GB\n        dt = np.dtype([('x', np.float64), ('y', 'object')])\n        bs = 65536 - dt.itemsize\n        block = b'.' * bs\n        n = target_size // (bs + dt.itemsize)\n        data = np.zeros(n, dtype=dt)\n        data['x'] = np.random.random_sample(n)\n        data['y'] = block\n        # Add implicit nulls\n        data['x'][data['x'] < 0.2] = np.nan\n\n        ty = pa.struct([pa.field('x', pa.float64()),\n                        pa.field('y', pa.binary())])\n        arr = pa.array(data, type=ty, from_pandas=True)\n        arr.validate(full=True)\n        assert arr.num_chunks == 2\n\n        def iter_chunked_array(arr):\n            for chunk in arr.iterchunks():\n                yield from chunk\n\n        def check(arr, data, mask=None):\n            assert len(arr) == len(data)\n            xs = data['x']\n            ys = data['y']\n            for i, obj in enumerate(iter_chunked_array(arr)):\n                try:\n                    d = obj.as_py()\n                    if mask is not None and mask[i]:\n                        assert d is None\n                    else:\n                        x = xs[i]\n                        if np.isnan(x):\n                            assert d['x'] is None\n                        else:\n                            assert d['x'] == x\n                        assert d['y'] == ys[i]\n                except Exception:\n                    print(\"Failed at index\", i)\n                    raise\n\n        check(arr, data)\n        del arr\n\n        # Now with explicit mask\n        mask = np.random.random_sample(n) < 0.2\n        arr = pa.array(data, type=ty, mask=mask, from_pandas=True)\n        arr.validate(full=True)\n        assert arr.num_chunks == 2\n\n        check(arr, data, mask)\n        del arr\n\n    def test_from_numpy_bad_input(self):\n        ty = pa.struct([pa.field('x', pa.int32()),\n                        pa.field('y', pa.bool_())])\n        dt = np.dtype([('x', np.int32),\n                       ('z', np.bool_)])\n\n        data = np.array([], dtype=dt)\n        with pytest.raises(ValueError,\n                           match=\"Missing field 'y'\"):\n            pa.array(data, type=ty)\n        data = np.int32([])\n        with pytest.raises(TypeError,\n                           match=\"Expected struct array\"):\n            pa.array(data, type=ty)\n\n    def test_from_tuples(self):\n        df = pd.DataFrame({'tuples': [(1, 2), (3, 4)]})\n        expected_df = pd.DataFrame(\n            {'tuples': [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]})\n\n        # conversion from tuples works when specifying expected struct type\n        struct_type = pa.struct([('a', pa.int64()), ('b', pa.int64())])\n\n        arr = np.asarray(df['tuples'])\n        _check_array_roundtrip(\n            arr, expected=expected_df['tuples'], type=struct_type)\n\n        expected_schema = pa.schema([('tuples', struct_type)])\n        _check_pandas_roundtrip(\n            df, expected=expected_df, schema=expected_schema,\n            expected_schema=expected_schema)\n\n    def test_struct_of_dictionary(self):\n        names = ['ints', 'strs']\n        children = [pa.array([456, 789, 456]).dictionary_encode(),\n                    pa.array([\"foo\", \"foo\", None]).dictionary_encode()]\n        arr = pa.StructArray.from_arrays(children, names=names)\n\n        # Expected a Series of {field name: field value} dicts\n        rows_as_tuples = zip(*(child.to_pylist() for child in children))\n        rows_as_dicts = [dict(zip(names, row)) for row in rows_as_tuples]\n\n        expected = pd.Series(rows_as_dicts)\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n        # Same but with nulls\n        arr = arr.take([0, None, 2])\n        expected[1] = None\n        tm.assert_series_equal(arr.to_pandas(), expected)\n\n\nclass TestZeroCopyConversion:\n    \"\"\"\n    Tests that zero-copy conversion works with some types.\n    \"\"\"\n\n    def test_zero_copy_success(self):\n        result = pa.array([0, 1, 2]).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, [0, 1, 2])\n\n    def test_zero_copy_dictionaries(self):\n        arr = pa.DictionaryArray.from_arrays(\n            np.array([0, 0]),\n            np.array([5], dtype=\"int64\"),\n        )\n\n        result = arr.to_pandas(zero_copy_only=True)\n        values = pd.Categorical([5, 5])\n\n        tm.assert_series_equal(pd.Series(result), pd.Series(values),\n                               check_names=False)\n\n    def test_zero_copy_timestamp(self):\n        arr = np.array(['2007-07-13'], dtype='datetime64[ns]')\n        result = pa.array(arr).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, arr)\n\n    def test_zero_copy_duration(self):\n        arr = np.array([1], dtype='timedelta64[ns]')\n        result = pa.array(arr).to_pandas(zero_copy_only=True)\n        npt.assert_array_equal(result, arr)\n\n    def check_zero_copy_failure(self, arr):\n        with pytest.raises(pa.ArrowInvalid):\n            arr.to_pandas(zero_copy_only=True)\n\n    def test_zero_copy_failure_on_object_types(self):\n        self.check_zero_copy_failure(pa.array(['A', 'B', 'C']))\n\n    def test_zero_copy_failure_with_int_when_nulls(self):\n        self.check_zero_copy_failure(pa.array([0, 1, None]))\n\n    def test_zero_copy_failure_with_float_when_nulls(self):\n        self.check_zero_copy_failure(pa.array([0.0, 1.0, None]))\n\n    def test_zero_copy_failure_on_bool_types(self):\n        self.check_zero_copy_failure(pa.array([True, False]))\n\n    def test_zero_copy_failure_on_list_types(self):\n        arr = pa.array([[1, 2], [8, 9]], type=pa.list_(pa.int64()))\n        self.check_zero_copy_failure(arr)\n\n    def test_zero_copy_failure_on_timestamp_with_nulls(self):\n        arr = np.array([1, None], dtype='datetime64[ns]')\n        self.check_zero_copy_failure(pa.array(arr))\n\n    def test_zero_copy_failure_on_duration_with_nulls(self):\n        arr = np.array([1, None], dtype='timedelta64[ns]')\n        self.check_zero_copy_failure(pa.array(arr))\n\n\ndef _non_threaded_conversion():\n    df = _alltypes_example()\n    _check_pandas_roundtrip(df, use_threads=False)\n    _check_pandas_roundtrip(df, use_threads=False, as_batch=True)\n\n\ndef _threaded_conversion():\n    df = _alltypes_example()\n    _check_pandas_roundtrip(df, use_threads=True)\n    _check_pandas_roundtrip(df, use_threads=True, as_batch=True)\n\n\nclass TestConvertMisc:\n    \"\"\"\n    Miscellaneous conversion tests.\n    \"\"\"\n\n    type_pairs = [\n        (np.int8, pa.int8()),\n        (np.int16, pa.int16()),\n        (np.int32, pa.int32()),\n        (np.int64, pa.int64()),\n        (np.uint8, pa.uint8()),\n        (np.uint16, pa.uint16()),\n        (np.uint32, pa.uint32()),\n        (np.uint64, pa.uint64()),\n        (np.float16, pa.float16()),\n        (np.float32, pa.float32()),\n        (np.float64, pa.float64()),\n        # XXX unsupported\n        # (np.dtype([('a', 'i2')]), pa.struct([pa.field('a', pa.int16())])),\n        (np.object_, pa.string()),\n        (np.object_, pa.binary()),\n        (np.object_, pa.binary(10)),\n        (np.object_, pa.list_(pa.int64())),\n    ]\n\n    def test_all_none_objects(self):\n        df = pd.DataFrame({'a': [None, None, None]})\n        _check_pandas_roundtrip(df)\n\n    def test_all_none_category(self):\n        df = pd.DataFrame({'a': [None, None, None]})\n        df['a'] = df['a'].astype('category')\n        _check_pandas_roundtrip(df)\n\n    def test_empty_arrays(self):\n        for dtype, pa_type in self.type_pairs:\n            arr = np.array([], dtype=dtype)\n            _check_array_roundtrip(arr, type=pa_type)\n\n    def test_non_threaded_conversion(self):\n        _non_threaded_conversion()\n\n    def test_threaded_conversion_multiprocess(self):\n        # Parallel conversion should work from child processes too (ARROW-2963)\n        pool = mp.Pool(2)\n        try:\n            pool.apply(_threaded_conversion)\n        finally:\n            pool.close()\n            pool.join()\n\n    def test_category(self):\n        repeats = 5\n        v1 = ['foo', None, 'bar', 'qux', np.nan]\n        v2 = [4, 5, 6, 7, 8]\n        v3 = [b'foo', None, b'bar', b'qux', np.nan]\n\n        arrays = {\n            'cat_strings': pd.Categorical(v1 * repeats),\n            'cat_strings_with_na': pd.Categorical(v1 * repeats,\n                                                  categories=['foo', 'bar']),\n            'cat_ints': pd.Categorical(v2 * repeats),\n            'cat_binary': pd.Categorical(v3 * repeats),\n            'cat_strings_ordered': pd.Categorical(\n                v1 * repeats, categories=['bar', 'qux', 'foo'],\n                ordered=True),\n            'ints': v2 * repeats,\n            'ints2': v2 * repeats,\n            'strings': v1 * repeats,\n            'strings2': v1 * repeats,\n            'strings3': v3 * repeats}\n        df = pd.DataFrame(arrays)\n        _check_pandas_roundtrip(df)\n\n        for k in arrays:\n            _check_array_roundtrip(arrays[k])\n\n    def test_category_implicit_from_pandas(self):\n        # ARROW-3374\n        def _check(v):\n            arr = pa.array(v)\n            result = arr.to_pandas()\n            tm.assert_series_equal(pd.Series(result), pd.Series(v))\n\n        arrays = [\n            pd.Categorical(['a', 'b', 'c'], categories=['a', 'b']),\n            pd.Categorical(['a', 'b', 'c'], categories=['a', 'b'],\n                           ordered=True)\n        ]\n        for arr in arrays:\n            _check(arr)\n\n    def test_empty_category(self):\n        # ARROW-2443\n        df = pd.DataFrame({'cat': pd.Categorical([])})\n        _check_pandas_roundtrip(df)\n\n    def test_category_zero_chunks(self):\n        # ARROW-5952\n        for pa_type, dtype in [(pa.string(), 'object'), (pa.int64(), 'int64')]:\n            a = pa.chunked_array([], pa.dictionary(pa.int8(), pa_type))\n            result = a.to_pandas()\n            expected = pd.Categorical([], categories=np.array([], dtype=dtype))\n            tm.assert_series_equal(pd.Series(result), pd.Series(expected))\n\n            table = pa.table({'a': a})\n            result = table.to_pandas()\n            expected = pd.DataFrame({'a': expected})\n            tm.assert_frame_equal(result, expected)\n\n    @pytest.mark.parametrize(\n        \"data,error_type\",\n        [\n            ({\"a\": [\"a\", 1, 2.0]}, pa.ArrowTypeError),\n            ({\"a\": [\"a\", 1, 2.0]}, pa.ArrowTypeError),\n            ({\"a\": [1, True]}, pa.ArrowTypeError),\n            ({\"a\": [True, \"a\"]}, pa.ArrowInvalid),\n            ({\"a\": [1, \"a\"]}, pa.ArrowInvalid),\n            ({\"a\": [1.0, \"a\"]}, pa.ArrowInvalid),\n        ],\n    )\n    def test_mixed_types_fails(self, data, error_type):\n        df = pd.DataFrame(data)\n        msg = \"Conversion failed for column a with type object\"\n        with pytest.raises(error_type, match=msg):\n            pa.Table.from_pandas(df)\n\n    def test_strided_data_import(self):\n        cases = []\n\n        columns = ['a', 'b', 'c']\n        N, K = 100, 3\n        random_numbers = np.random.randn(N, K).copy() * 100\n\n        numeric_dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8',\n                          'f4', 'f8']\n\n        for type_name in numeric_dtypes:\n            # Casting np.float64 -> uint32 or uint64 throws a RuntimeWarning\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                cases.append(random_numbers.astype(type_name))\n\n        # strings\n        cases.append(np.array([random_ascii(10) for i in range(N * K)],\n                              dtype=object)\n                     .reshape(N, K).copy())\n\n        # booleans\n        boolean_objects = (np.array([True, False, True] * N, dtype=object)\n                           .reshape(N, K).copy())\n\n        # add some nulls, so dtype comes back as objects\n        boolean_objects[5] = None\n        cases.append(boolean_objects)\n\n        cases.append(np.arange(\"2016-01-01T00:00:00.001\", N * K,\n                               dtype='datetime64[ms]')\n                     .reshape(N, K).copy())\n\n        strided_mask = (random_numbers > 0).astype(bool)[:, 0]\n\n        for case in cases:\n            df = pd.DataFrame(case, columns=columns)\n            col = df['a']\n\n            _check_pandas_roundtrip(df)\n            _check_array_roundtrip(col)\n            _check_array_roundtrip(col, mask=strided_mask)\n\n    def test_all_nones(self):\n        def _check_series(s):\n            converted = pa.array(s)\n            assert isinstance(converted, pa.NullArray)\n            assert len(converted) == 3\n            assert converted.null_count == 3\n            for item in converted:\n                assert item is pa.NA\n\n        _check_series(pd.Series([None] * 3, dtype=object))\n        _check_series(pd.Series([np.nan] * 3, dtype=object))\n        _check_series(pd.Series([None, np.nan, None], dtype=object))\n\n    def test_partial_schema(self):\n        data = OrderedDict([\n            ('a', [0, 1, 2, 3, 4]),\n            ('b', np.array([-10, -5, 0, 5, 10], dtype=np.int32)),\n            ('c', [-10, -5, 0, 5, 10])\n        ])\n        df = pd.DataFrame(data)\n\n        partial_schema = pa.schema([\n            pa.field('c', pa.int64()),\n            pa.field('a', pa.int64())\n        ])\n\n        _check_pandas_roundtrip(df, schema=partial_schema,\n                                expected=df[['c', 'a']],\n                                expected_schema=partial_schema)\n\n    def test_table_batch_empty_dataframe(self):\n        df = pd.DataFrame({})\n        _check_pandas_roundtrip(df, preserve_index=None)\n        _check_pandas_roundtrip(df, preserve_index=None, as_batch=True)\n\n        expected = pd.DataFrame(columns=pd.Index([]))\n        _check_pandas_roundtrip(df, expected, preserve_index=False)\n        _check_pandas_roundtrip(df, expected, preserve_index=False, as_batch=True)\n\n        df2 = pd.DataFrame({}, index=[0, 1, 2])\n        _check_pandas_roundtrip(df2, preserve_index=True)\n        _check_pandas_roundtrip(df2, as_batch=True, preserve_index=True)\n\n    def test_convert_empty_table(self):\n        arr = pa.array([], type=pa.int64())\n        empty_objects = pd.Series(np.array([], dtype=object))\n        tm.assert_series_equal(arr.to_pandas(),\n                               pd.Series(np.array([], dtype=np.int64)))\n        arr = pa.array([], type=pa.string())\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n        arr = pa.array([], type=pa.list_(pa.int64()))\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n        arr = pa.array([], type=pa.struct([pa.field('a', pa.int64())]))\n        tm.assert_series_equal(arr.to_pandas(), empty_objects)\n\n    def test_non_natural_stride(self):\n        \"\"\"\n        ARROW-2172: converting from a Numpy array with a stride that's\n        not a multiple of itemsize.\n        \"\"\"\n        dtype = np.dtype([('x', np.int32), ('y', np.int16)])\n        data = np.array([(42, -1), (-43, 2)], dtype=dtype)\n        assert data.strides == (6,)\n        arr = pa.array(data['x'], type=pa.int32())\n        assert arr.to_pylist() == [42, -43]\n        arr = pa.array(data['y'], type=pa.int16())\n        assert arr.to_pylist() == [-1, 2]\n\n    def test_array_from_strided_numpy_array(self):\n        # ARROW-5651\n        np_arr = np.arange(0, 10, dtype=np.float32)[1:-1:2]\n        pa_arr = pa.array(np_arr, type=pa.float64())\n        expected = pa.array([1.0, 3.0, 5.0, 7.0], type=pa.float64())\n        pa_arr.equals(expected)\n\n    def test_safe_unsafe_casts(self):\n        # ARROW-2799\n        df = pd.DataFrame({\n            'A': list('abc'),\n            'B': np.linspace(0, 1, 3)\n        })\n\n        schema = pa.schema([\n            pa.field('A', pa.string()),\n            pa.field('B', pa.int32())\n        ])\n\n        with pytest.raises(ValueError):\n            pa.Table.from_pandas(df, schema=schema)\n\n        table = pa.Table.from_pandas(df, schema=schema, safe=False)\n        assert table.column('B').type == pa.int32()\n\n    def test_error_sparse(self):\n        # ARROW-2818\n        try:\n            df = pd.DataFrame({'a': pd.arrays.SparseArray([1, np.nan, 3])})\n        except AttributeError:\n            # pandas.arrays module introduced in pandas 0.24\n            df = pd.DataFrame({'a': pd.SparseArray([1, np.nan, 3])})\n        with pytest.raises(TypeError, match=\"Sparse pandas data\"):\n            pa.Table.from_pandas(df)\n\n\ndef test_safe_cast_from_float_with_nans_to_int():\n    # TODO(kszucs): write tests for creating Date32 and Date64 arrays, see\n    #               ARROW-4258 and https://github.com/apache/arrow/pull/3395\n    values = pd.Series([1, 2, None, 4])\n    arr = pa.Array.from_pandas(values, type=pa.int32(), safe=True)\n    expected = pa.array([1, 2, None, 4], type=pa.int32())\n    assert arr.equals(expected)\n\n\ndef _fully_loaded_dataframe_example():\n    index = pd.MultiIndex.from_arrays([\n        pd.date_range('2000-01-01', periods=5).repeat(2),\n        np.tile(np.array(['foo', 'bar'], dtype=object), 5)\n    ])\n\n    c1 = pd.date_range('2000-01-01', periods=10)\n    data = {\n        0: c1,\n        1: c1.tz_localize('utc'),\n        2: c1.tz_localize('US/Eastern'),\n        3: c1[::2].tz_localize('utc').repeat(2).astype('category'),\n        4: ['foo', 'bar'] * 5,\n        5: pd.Series(['foo', 'bar'] * 5).astype('category').values,\n        6: [True, False] * 5,\n        7: np.random.randn(10),\n        8: np.random.randint(0, 100, size=10),\n        9: pd.period_range('2013', periods=10, freq='M'),\n        10: pd.interval_range(start=1, freq=1, periods=10),\n    }\n    return pd.DataFrame(data, index=index)\n\n\n@pytest.mark.parametrize('columns', ([b'foo'], ['foo']))\ndef test_roundtrip_with_bytes_unicode(columns):\n    if Version(\"2.0.0\") <= Version(pd.__version__) < Version(\"2.2.0\"):\n        # TODO: regression in pandas, hopefully fixed in next version\n        # https://issues.apache.org/jira/browse/ARROW-18394\n        # https://github.com/pandas-dev/pandas/issues/50127\n        pytest.skip(\"Regression in pandas 2.0.0\")\n\n    df = pd.DataFrame(columns=columns)\n    table1 = pa.Table.from_pandas(df)\n    table2 = pa.Table.from_pandas(table1.to_pandas())\n    assert table1.equals(table2)\n    assert table1.schema.equals(table2.schema)\n    assert table1.schema.metadata == table2.schema.metadata\n\n\ndef _pytime_from_micros(val):\n    microseconds = val % 1000000\n    val //= 1000000\n    seconds = val % 60\n    val //= 60\n    minutes = val % 60\n    hours = val // 60\n    return time(hours, minutes, seconds, microseconds)\n\n\ndef _pytime_to_micros(pytime):\n    return (pytime.hour * 3600000000 +\n            pytime.minute * 60000000 +\n            pytime.second * 1000000 +\n            pytime.microsecond)\n\n\ndef test_convert_unsupported_type_error_message():\n    # ARROW-1454\n\n    # custom python objects\n    class A:\n        pass\n\n    df = pd.DataFrame({'a': [A(), A()]})\n\n    msg = 'Conversion failed for column a with type object'\n    with pytest.raises(ValueError, match=msg):\n        pa.Table.from_pandas(df)\n\n\n# ----------------------------------------------------------------------\n# Hypothesis tests\n\n\n@h.given(past.arrays(past.pandas_compatible_types))\ndef test_array_to_pandas_roundtrip(arr):\n    s = arr.to_pandas()\n    restored = pa.array(s, type=arr.type, from_pandas=True)\n    assert restored.equals(arr)\n\n\n# ----------------------------------------------------------------------\n# Test object deduplication in to_pandas\n\n\ndef _generate_dedup_example(nunique, repeats):\n    unique_values = [rands(10) for i in range(nunique)]\n    return unique_values * repeats\n\n\ndef _assert_nunique(obj, expected):\n    assert len({id(x) for x in obj}) == expected\n\n\ndef test_to_pandas_deduplicate_strings_array_types():\n    nunique = 100\n    repeats = 10\n    values = _generate_dedup_example(nunique, repeats)\n\n    for arr in [pa.array(values, type=pa.binary()),\n                pa.array(values, type=pa.utf8()),\n                pa.chunked_array([values, values])]:\n        _assert_nunique(arr.to_pandas(), nunique)\n        _assert_nunique(arr.to_pandas(deduplicate_objects=False), len(arr))\n\n\ndef test_to_pandas_deduplicate_strings_table_types():\n    nunique = 100\n    repeats = 10\n    values = _generate_dedup_example(nunique, repeats)\n\n    arr = pa.array(values)\n    rb = pa.RecordBatch.from_arrays([arr], ['foo'])\n    tbl = pa.Table.from_batches([rb])\n\n    for obj in [rb, tbl]:\n        _assert_nunique(obj.to_pandas()['foo'], nunique)\n        _assert_nunique(obj.to_pandas(deduplicate_objects=False)['foo'],\n                        len(obj))\n\n\ndef test_to_pandas_deduplicate_integers_as_objects():\n    nunique = 100\n    repeats = 10\n\n    # Python automatically interns smaller integers\n    unique_values = list(np.random.randint(10000000, 1000000000, size=nunique))\n    unique_values[nunique // 2] = None\n\n    arr = pa.array(unique_values * repeats)\n\n    _assert_nunique(arr.to_pandas(integer_object_nulls=True), nunique)\n    _assert_nunique(arr.to_pandas(integer_object_nulls=True,\n                                  deduplicate_objects=False),\n                    # Account for None\n                    (nunique - 1) * repeats + 1)\n\n\ndef test_to_pandas_deduplicate_date_time():\n    nunique = 100\n    repeats = 10\n\n    unique_values = list(range(nunique))\n\n    cases = [\n        # raw type, array type, to_pandas options\n        ('int32', 'date32', {'date_as_object': True}),\n        ('int64', 'date64', {'date_as_object': True}),\n        ('int32', 'time32[ms]', {}),\n        ('int64', 'time64[us]', {})\n    ]\n\n    for raw_type, array_type, pandas_options in cases:\n        raw_arr = pa.array(unique_values * repeats, type=raw_type)\n        casted_arr = raw_arr.cast(array_type)\n\n        _assert_nunique(casted_arr.to_pandas(**pandas_options),\n                        nunique)\n        _assert_nunique(casted_arr.to_pandas(deduplicate_objects=False,\n                                             **pandas_options),\n                        len(casted_arr))\n\n\n# ---------------------------------------------------------------------\n\ndef test_table_from_pandas_checks_field_nullability():\n    # ARROW-2136\n    df = pd.DataFrame({'a': [1.2, 2.1, 3.1],\n                       'b': [np.nan, 'string', 'foo']})\n    schema = pa.schema([pa.field('a', pa.float64(), nullable=False),\n                        pa.field('b', pa.utf8(), nullable=False)])\n\n    with pytest.raises(ValueError):\n        pa.Table.from_pandas(df, schema=schema)\n\n\ndef test_table_from_pandas_keeps_column_order_of_dataframe():\n    df1 = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n    df2 = df1[['floats', 'partition', 'arrays']]\n\n    schema1 = pa.schema([\n        ('partition', pa.int64()),\n        ('arrays', pa.list_(pa.int64())),\n        ('floats', pa.float64()),\n    ])\n    schema2 = pa.schema([\n        ('floats', pa.float64()),\n        ('partition', pa.int64()),\n        ('arrays', pa.list_(pa.int64()))\n    ])\n\n    table1 = pa.Table.from_pandas(df1, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, preserve_index=False)\n\n    assert table1.schema.equals(schema1)\n    assert table2.schema.equals(schema2)\n\n\ndef test_table_from_pandas_keeps_column_order_of_schema():\n    # ARROW-3766\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n\n    schema = pa.schema([\n        ('floats', pa.float64()),\n        ('arrays', pa.list_(pa.int32())),\n        ('partition', pa.int32())\n    ])\n\n    df1 = df[df.partition == 0]\n    df2 = df[df.partition == 1][['floats', 'partition', 'arrays']]\n\n    table1 = pa.Table.from_pandas(df1, schema=schema, preserve_index=False)\n    table2 = pa.Table.from_pandas(df2, schema=schema, preserve_index=False)\n\n    assert table1.schema.equals(schema)\n    assert table1.schema.equals(table2.schema)\n\n\ndef test_table_from_pandas_columns_argument_only_does_filtering():\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n\n    columns1 = ['arrays', 'floats', 'partition']\n    schema1 = pa.schema([\n        ('arrays', pa.list_(pa.int64())),\n        ('floats', pa.float64()),\n        ('partition', pa.int64())\n    ])\n\n    columns2 = ['floats', 'partition']\n    schema2 = pa.schema([\n        ('floats', pa.float64()),\n        ('partition', pa.int64())\n    ])\n\n    table1 = pa.Table.from_pandas(df, columns=columns1, preserve_index=False)\n    table2 = pa.Table.from_pandas(df, columns=columns2, preserve_index=False)\n\n    assert table1.schema.equals(schema1)\n    assert table2.schema.equals(schema2)\n\n\ndef test_table_from_pandas_columns_and_schema_are_mutually_exclusive():\n    df = pd.DataFrame(OrderedDict([\n        ('partition', [0, 0, 1, 1]),\n        ('arrays', [[0, 1, 2], [3, 4], None, None]),\n        ('floats', [None, None, 1.1, 3.3])\n    ]))\n    schema = pa.schema([\n        ('partition', pa.int32()),\n        ('arrays', pa.list_(pa.int32())),\n        ('floats', pa.float64()),\n    ])\n    columns = ['arrays', 'floats']\n\n    with pytest.raises(ValueError):\n        pa.Table.from_pandas(df, schema=schema, columns=columns)\n\n\ndef test_table_from_pandas_keeps_schema_nullability():\n    # ARROW-5169\n    df = pd.DataFrame({'a': [1, 2, 3, 4]})\n\n    schema = pa.schema([\n        pa.field('a', pa.int64(), nullable=False),\n    ])\n\n    table = pa.Table.from_pandas(df)\n    assert table.schema.field('a').nullable is True\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.field('a').nullable is False\n\n\ndef test_table_from_pandas_schema_index_columns():\n    # ARROW-5220\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})\n\n    schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n        ('index', pa.int64()),\n    ])\n\n    # schema includes index with name not in dataframe\n    with pytest.raises(KeyError, match=\"name 'index' present in the\"):\n        pa.Table.from_pandas(df, schema=schema)\n\n    df.index.name = 'index'\n\n    # schema includes correct index name -> roundtrip works\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema includes correct index name but preserve_index=False\n    with pytest.raises(ValueError, match=\"'preserve_index=False' was\"):\n        pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n\n    # in case of preserve_index=None -> RangeIndex serialized as metadata\n    # clashes with the index in the schema\n    with pytest.raises(ValueError, match=\"name 'index' is present in the \"\n                                         \"schema, but it is a RangeIndex\"):\n        pa.Table.from_pandas(df, schema=schema, preserve_index=None)\n\n    df.index = pd.Index([0, 1, 2], name='index')\n\n    # for non-RangeIndex, both preserve_index=None and True work\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema has different order (index column not at the end)\n    schema = pa.schema([\n        ('index', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n\n    # schema does not include the index -> index is not included as column\n    # even though preserve_index=True/None\n    schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    expected = df.copy()\n    expected = expected.reset_index(drop=True)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema, expected=expected)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema, expected=expected)\n\n    # dataframe with a MultiIndex\n    df.index = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1)],\n                                         names=['level1', 'level2'])\n    schema = pa.schema([\n        ('level1', pa.string()),\n        ('level2', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema)\n\n    # only one of the levels of the MultiIndex is included\n    schema = pa.schema([\n        ('level2', pa.int64()),\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n    ])\n    expected = df.copy()\n    expected = expected.reset_index('level1', drop=True)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=True,\n                            expected_schema=schema, expected=expected)\n    _check_pandas_roundtrip(df, schema=schema, preserve_index=None,\n                            expected_schema=schema, expected=expected)\n\n\ndef test_table_from_pandas_schema_index_columns__unnamed_index():\n    # ARROW-6999 - unnamed indices in specified schema\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})\n\n    expected_schema = pa.schema([\n        ('a', pa.int64()),\n        ('b', pa.float64()),\n        ('__index_level_0__', pa.int64()),\n    ])\n\n    schema = pa.Schema.from_pandas(df, preserve_index=True)\n    table = pa.Table.from_pandas(df, preserve_index=True, schema=schema)\n    assert table.schema.remove_metadata().equals(expected_schema)\n\n    # non-RangeIndex (preserved by default)\n    df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]}, index=[0, 1, 2])\n    schema = pa.Schema.from_pandas(df)\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.remove_metadata().equals(expected_schema)\n\n\ndef test_table_from_pandas_schema_with_custom_metadata():\n    # ARROW-7087 - metadata disappear from pandas\n    df = pd.DataFrame()\n    schema = pa.Schema.from_pandas(df).with_metadata({'meta': 'True'})\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.metadata.get(b'meta') == b'True'\n\n\ndef test_table_from_pandas_schema_field_order_metadata():\n    # ARROW-10532\n    # ensure that a different field order in specified schema doesn't\n    # mangle metadata\n    df = pd.DataFrame({\n        \"datetime\": pd.date_range(\"2020-01-01T00:00:00Z\", freq=\"H\", periods=2),\n        \"float\": np.random.randn(2)\n    })\n\n    schema = pa.schema([\n        pa.field(\"float\", pa.float32(), nullable=True),\n        pa.field(\"datetime\", pa.timestamp(\"s\", tz=\"UTC\"), nullable=False)\n    ])\n\n    table = pa.Table.from_pandas(df, schema=schema)\n    assert table.schema.equals(schema)\n    metadata_float = table.schema.pandas_metadata[\"columns\"][0]\n    assert metadata_float[\"name\"] == \"float\"\n    assert metadata_float[\"metadata\"] is None\n    metadata_datetime = table.schema.pandas_metadata[\"columns\"][1]\n    assert metadata_datetime[\"name\"] == \"datetime\"\n    assert metadata_datetime[\"metadata\"] == {'timezone': 'UTC'}\n\n    result = table.to_pandas()\n    coerce_cols_to_types = {\"float\": \"float32\"}\n    if Version(pd.__version__) >= Version(\"2.0.0\"):\n        # Pandas v2 now support non-nanosecond time units\n        coerce_cols_to_types[\"datetime\"] = \"datetime64[s, UTC]\"\n    expected = df[[\"float\", \"datetime\"]].astype(coerce_cols_to_types)\n\n    tm.assert_frame_equal(result, expected)\n\n\n# ----------------------------------------------------------------------\n# RecordBatch, Table\n\n\ndef test_recordbatch_from_to_pandas():\n    data = pd.DataFrame({\n        'c1': np.array([1, 2, 3, 4, 5], dtype='int64'),\n        'c2': np.array([1, 2, 3, 4, 5], dtype='uint32'),\n        'c3': np.random.randn(5),\n        'c4': ['foo', 'bar', None, 'baz', 'qux'],\n        'c5': [False, True, False, True, False]\n    })\n\n    batch = pa.RecordBatch.from_pandas(data)\n    result = batch.to_pandas()\n    tm.assert_frame_equal(data, result)\n\n\ndef test_recordbatchlist_to_pandas():\n    data1 = pd.DataFrame({\n        'c1': np.array([1, 1, 2], dtype='uint32'),\n        'c2': np.array([1.0, 2.0, 3.0], dtype='float64'),\n        'c3': [True, None, False],\n        'c4': ['foo', 'bar', None]\n    })\n\n    data2 = pd.DataFrame({\n        'c1': np.array([3, 5], dtype='uint32'),\n        'c2': np.array([4.0, 5.0], dtype='float64'),\n        'c3': [True, True],\n        'c4': ['baz', 'qux']\n    })\n\n    batch1 = pa.RecordBatch.from_pandas(data1)\n    batch2 = pa.RecordBatch.from_pandas(data2)\n\n    table = pa.Table.from_batches([batch1, batch2])\n    result = table.to_pandas()\n    data = pd.concat([data1, data2]).reset_index(drop=True)\n    tm.assert_frame_equal(data, result)\n\n\ndef test_recordbatch_table_pass_name_to_pandas():\n    rb = pa.record_batch([pa.array([1, 2, 3, 4])], names=['a0'])\n    t = pa.table([pa.array([1, 2, 3, 4])], names=['a0'])\n    assert rb[0].to_pandas().name == 'a0'\n    assert t[0].to_pandas().name == 'a0'\n\n\n# ----------------------------------------------------------------------\n# Metadata serialization\n\n\n@pytest.mark.parametrize(\n    ('type', 'expected'),\n    [\n        (pa.null(), 'empty'),\n        (pa.bool_(), 'bool'),\n        (pa.int8(), 'int8'),\n        (pa.int16(), 'int16'),\n        (pa.int32(), 'int32'),\n        (pa.int64(), 'int64'),\n        (pa.uint8(), 'uint8'),\n        (pa.uint16(), 'uint16'),\n        (pa.uint32(), 'uint32'),\n        (pa.uint64(), 'uint64'),\n        (pa.float16(), 'float16'),\n        (pa.float32(), 'float32'),\n        (pa.float64(), 'float64'),\n        (pa.date32(), 'date'),\n        (pa.date64(), 'date'),\n        (pa.binary(), 'bytes'),\n        (pa.binary(length=4), 'bytes'),\n        (pa.string(), 'unicode'),\n        (pa.list_(pa.list_(pa.int16())), 'list[list[int16]]'),\n        (pa.decimal128(18, 3), 'decimal'),\n        (pa.timestamp('ms'), 'datetime'),\n        (pa.timestamp('us', 'UTC'), 'datetimetz'),\n        (pa.time32('s'), 'time'),\n        (pa.time64('us'), 'time')\n    ]\n)\ndef test_logical_type(type, expected):\n    assert get_logical_type(type) == expected\n\n\n# ----------------------------------------------------------------------\n# to_pandas uses MemoryPool\n\ndef test_array_uses_memory_pool():\n    # ARROW-6570\n    N = 10000\n    arr = pa.array(np.arange(N, dtype=np.int64),\n                   mask=np.random.randint(0, 2, size=N).astype(np.bool_))\n\n    # In the case the gc is caught loading\n    gc.collect()\n\n    prior_allocation = pa.total_allocated_bytes()\n\n    x = arr.to_pandas()\n    assert pa.total_allocated_bytes() == (prior_allocation + N * 8)\n    x = None  # noqa\n    gc.collect()\n\n    assert pa.total_allocated_bytes() == prior_allocation\n\n    # zero copy does not allocate memory\n    arr = pa.array(np.arange(N, dtype=np.int64))\n\n    prior_allocation = pa.total_allocated_bytes()\n    x = arr.to_pandas()  # noqa\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_singleton_blocks_zero_copy():\n    # Part of ARROW-3789\n    t = pa.table([pa.array(np.arange(1000, dtype=np.int64))], ['f0'])\n\n    # Zero copy if split_blocks=True\n    _check_to_pandas_memory_unchanged(t, split_blocks=True)\n\n    prior_allocation = pa.total_allocated_bytes()\n    result = t.to_pandas()\n    assert result['f0'].values.flags.writeable\n    assert pa.total_allocated_bytes() > prior_allocation\n\n\ndef _check_to_pandas_memory_unchanged(obj, **kwargs):\n    prior_allocation = pa.total_allocated_bytes()\n    x = obj.to_pandas(**kwargs)  # noqa\n\n    # Memory allocation unchanged -- either zero copy or self-destructing\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_to_pandas_split_blocks():\n    # ARROW-3789\n    t = pa.table([\n        pa.array([1, 2, 3, 4, 5], type='i1'),\n        pa.array([1, 2, 3, 4, 5], type='i4'),\n        pa.array([1, 2, 3, 4, 5], type='i8'),\n        pa.array([1, 2, 3, 4, 5], type='f4'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n        pa.array([1, 2, 3, 4, 5], type='f8'),\n    ], ['f{}'.format(i) for i in range(8)])\n\n    _check_blocks_created(t, 8)\n    _check_to_pandas_memory_unchanged(t, split_blocks=True)\n\n\ndef _get_mgr(df):\n    if Version(pd.__version__) < Version(\"1.1.0\"):\n        return df._data\n    else:\n        return df._mgr\n\n\ndef _check_blocks_created(t, number):\n    x = t.to_pandas(split_blocks=True)\n    assert len(_get_mgr(x).blocks) == number\n\n\ndef test_to_pandas_self_destruct():\n    K = 50\n\n    def _make_table():\n        return pa.table([\n            # Slice to force a copy\n            pa.array(np.random.randn(10000)[::2])\n            for i in range(K)\n        ], ['f{}'.format(i) for i in range(K)])\n\n    t = _make_table()\n    _check_to_pandas_memory_unchanged(t, split_blocks=True, self_destruct=True)\n\n    # Check non-split-block behavior\n    t = _make_table()\n    _check_to_pandas_memory_unchanged(t, self_destruct=True)\n\n\ndef test_table_uses_memory_pool():\n    N = 10000\n    arr = pa.array(np.arange(N, dtype=np.int64))\n    t = pa.table([arr, arr, arr], ['f0', 'f1', 'f2'])\n\n    prior_allocation = pa.total_allocated_bytes()\n    x = t.to_pandas()\n\n    assert pa.total_allocated_bytes() == (prior_allocation + 3 * N * 8)\n\n    # Check successful garbage collection\n    x = None  # noqa\n    gc.collect()\n    assert pa.total_allocated_bytes() == prior_allocation\n\n\ndef test_object_leak_in_numpy_array():\n    # ARROW-6876\n    arr = pa.array([{'a': 1}])\n    np_arr = arr.to_pandas()\n    assert np_arr.dtype == np.dtype('object')\n    obj = np_arr[0]\n    refcount = sys.getrefcount(obj)\n    assert sys.getrefcount(obj) == refcount\n    del np_arr\n    assert sys.getrefcount(obj) == refcount - 1\n\n\ndef test_object_leak_in_dataframe():\n    # ARROW-6876\n    arr = pa.array([{'a': 1}])\n    table = pa.table([arr], ['f0'])\n    col = table.to_pandas()['f0']\n    assert col.dtype == np.dtype('object')\n    obj = col[0]\n    refcount = sys.getrefcount(obj)\n    assert sys.getrefcount(obj) == refcount\n    del col\n    assert sys.getrefcount(obj) == refcount - 1\n\n\n# ----------------------------------------------------------------------\n# Some nested array tests array tests\n\n\ndef test_array_from_py_float32():\n    data = [[1.2, 3.4], [9.0, 42.0]]\n\n    t = pa.float32()\n\n    arr1 = pa.array(data[0], type=t)\n    arr2 = pa.array(data, type=pa.list_(t))\n\n    expected1 = np.array(data[0], dtype=np.float32)\n    expected2 = pd.Series([np.array(data[0], dtype=np.float32),\n                           np.array(data[1], dtype=np.float32)])\n\n    assert arr1.type == t\n    assert arr1.equals(pa.array(expected1))\n    assert arr2.equals(pa.array(expected2))\n\n\n# ----------------------------------------------------------------------\n# Timestamp tests\n\n\ndef test_cast_timestamp_unit():\n    # ARROW-1680\n    val = datetime.now()\n    s = pd.Series([val])\n    s_nyc = s.dt.tz_localize('tzlocal()').dt.tz_convert('America/New_York')\n\n    us_with_tz = pa.timestamp('us', tz='America/New_York')\n\n    arr = pa.Array.from_pandas(s_nyc, type=us_with_tz)\n\n    # ARROW-1906\n    assert arr.type == us_with_tz\n\n    arr2 = pa.Array.from_pandas(s, type=pa.timestamp('us'))\n\n    assert arr[0].as_py() == s_nyc[0].to_pydatetime()\n    assert arr2[0].as_py() == s[0].to_pydatetime()\n\n    # Disallow truncation\n    arr = pa.array([123123], type='int64').cast(pa.timestamp('ms'))\n    expected = pa.array([123], type='int64').cast(pa.timestamp('s'))\n\n    # sanity check that the cast worked right\n    assert arr.type == pa.timestamp('ms')\n\n    target = pa.timestamp('s')\n    with pytest.raises(ValueError):\n        arr.cast(target)\n\n    result = arr.cast(target, safe=False)\n    assert result.equals(expected)\n\n    # ARROW-1949\n    series = pd.Series([pd.Timestamp(1), pd.Timestamp(10), pd.Timestamp(1000)])\n    expected = pa.array([0, 0, 1], type=pa.timestamp('us'))\n\n    with pytest.raises(ValueError):\n        pa.array(series, type=pa.timestamp('us'))\n\n    with pytest.raises(ValueError):\n        pa.Array.from_pandas(series, type=pa.timestamp('us'))\n\n    result = pa.Array.from_pandas(series, type=pa.timestamp('us'), safe=False)\n    assert result.equals(expected)\n\n    result = pa.array(series, type=pa.timestamp('us'), safe=False)\n    assert result.equals(expected)\n\n\ndef test_nested_with_timestamp_tz_round_trip():\n    ts = pd.Timestamp.now()\n    ts_dt = ts.to_pydatetime()\n    arr = pa.array([ts_dt], type=pa.timestamp('us', tz='America/New_York'))\n    struct = pa.StructArray.from_arrays([arr, arr], ['start', 'stop'])\n\n    result = struct.to_pandas()\n    restored = pa.array(result)\n    assert restored.equals(struct)\n\n\ndef test_nested_with_timestamp_tz():\n    # ARROW-7723\n    ts = pd.Timestamp.now()\n    ts_dt = ts.to_pydatetime()\n\n    # XXX: Ensure that this data does not get promoted to nanoseconds (and thus\n    # integers) to preserve behavior in 0.15.1\n    for unit in ['s', 'ms', 'us']:\n        if unit in ['s', 'ms']:\n            # This is used for verifying timezone conversion to micros are not\n            # important\n            def truncate(x): return x.replace(microsecond=0)\n        else:\n            def truncate(x): return x\n        arr = pa.array([ts], type=pa.timestamp(unit))\n        arr2 = pa.array([ts], type=pa.timestamp(unit, tz='America/New_York'))\n\n        arr3 = pa.StructArray.from_arrays([arr, arr], ['start', 'stop'])\n        arr4 = pa.StructArray.from_arrays([arr2, arr2], ['start', 'stop'])\n\n        result = arr3.to_pandas()\n        assert isinstance(result[0]['start'], datetime)\n        assert result[0]['start'].tzinfo is None\n        assert isinstance(result[0]['stop'], datetime)\n        assert result[0]['stop'].tzinfo is None\n\n        result = arr4.to_pandas()\n        assert isinstance(result[0]['start'], datetime)\n        assert result[0]['start'].tzinfo is not None\n        utc_dt = result[0]['start'].astimezone(timezone.utc)\n        assert truncate(utc_dt).replace(tzinfo=None) == truncate(ts_dt)\n        assert isinstance(result[0]['stop'], datetime)\n        assert result[0]['stop'].tzinfo is not None\n\n        # same conversion for table\n        result = pa.table({'a': arr3}).to_pandas()\n        assert isinstance(result['a'][0]['start'], datetime)\n        assert result['a'][0]['start'].tzinfo is None\n        assert isinstance(result['a'][0]['stop'], datetime)\n        assert result['a'][0]['stop'].tzinfo is None\n\n        result = pa.table({'a': arr4}).to_pandas()\n        assert isinstance(result['a'][0]['start'], datetime)\n        assert result['a'][0]['start'].tzinfo is not None\n        assert isinstance(result['a'][0]['stop'], datetime)\n        assert result['a'][0]['stop'].tzinfo is not None\n\n\n# ----------------------------------------------------------------------\n# DictionaryArray tests\n\n\ndef test_dictionary_with_pandas():\n    src_indices = np.repeat([0, 1, 2], 2)\n    dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n    mask = np.array([False, False, True, False, False, False])\n\n    for index_type in ['uint8', 'int8', 'uint16', 'int16', 'uint32', 'int32',\n                       'uint64', 'int64']:\n        indices = src_indices.astype(index_type)\n        d1 = pa.DictionaryArray.from_arrays(indices, dictionary)\n        d2 = pa.DictionaryArray.from_arrays(indices, dictionary, mask=mask)\n\n        if index_type[0] == 'u':\n            # TODO: unsigned dictionary indices to pandas\n            with pytest.raises(TypeError):\n                d1.to_pandas()\n            continue\n\n        pandas1 = d1.to_pandas()\n        ex_pandas1 = pd.Categorical.from_codes(indices, categories=dictionary)\n\n        tm.assert_series_equal(pd.Series(pandas1), pd.Series(ex_pandas1))\n\n        pandas2 = d2.to_pandas()\n        assert pandas2.isnull().sum() == 1\n\n        # Unsigned integers converted to signed\n        signed_indices = indices\n        if index_type[0] == 'u':\n            signed_indices = indices.astype(index_type[1:])\n        ex_pandas2 = pd.Categorical.from_codes(np.where(mask, -1,\n                                                        signed_indices),\n                                               categories=dictionary)\n\n        tm.assert_series_equal(pd.Series(pandas2), pd.Series(ex_pandas2))\n\n\ndef random_strings(n, item_size, pct_null=0, dictionary=None):\n    if dictionary is not None:\n        result = dictionary[np.random.randint(0, len(dictionary), size=n)]\n    else:\n        result = np.array([random_ascii(item_size) for i in range(n)],\n                          dtype=object)\n\n    if pct_null > 0:\n        result[np.random.rand(n) < pct_null] = None\n\n    return result\n\n\ndef test_variable_dictionary_to_pandas():\n    np.random.seed(12345)\n\n    d1 = pa.array(random_strings(100, 32), type='string')\n    d2 = pa.array(random_strings(100, 16), type='string')\n    d3 = pa.array(random_strings(10000, 10), type='string')\n\n    a1 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d1), size=1000, dtype='i4'),\n        d1\n    )\n    a2 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d2), size=1000, dtype='i4'),\n        d2\n    )\n\n    # With some nulls\n    a3 = pa.DictionaryArray.from_arrays(\n        np.random.randint(0, len(d3), size=1000, dtype='i4'), d3)\n\n    i4 = pa.array(\n        np.random.randint(0, len(d3), size=1000, dtype='i4'),\n        mask=np.random.rand(1000) < 0.1\n    )\n    a4 = pa.DictionaryArray.from_arrays(i4, d3)\n\n    expected_dict = pa.concat_arrays([d1, d2, d3])\n\n    a = pa.chunked_array([a1, a2, a3, a4])\n    a_dense = pa.chunked_array([a1.cast('string'),\n                                a2.cast('string'),\n                                a3.cast('string'),\n                                a4.cast('string')])\n\n    result = a.to_pandas()\n    result_dense = a_dense.to_pandas()\n\n    assert (result.cat.categories == expected_dict.to_pandas()).all()\n\n    expected_dense = result.astype('str')\n    expected_dense[result_dense.isnull()] = None\n    tm.assert_series_equal(result_dense, expected_dense)\n\n\ndef test_dictionary_encoded_nested_to_pandas():\n    # ARROW-6899\n    child = pa.array(['a', 'a', 'a', 'b', 'b']).dictionary_encode()\n\n    arr = pa.ListArray.from_arrays([0, 3, 5], child)\n\n    result = arr.to_pandas()\n    expected = pd.Series([np.array(['a', 'a', 'a'], dtype=object),\n                          np.array(['b', 'b'], dtype=object)])\n\n    tm.assert_series_equal(result, expected)\n\n\ndef test_dictionary_from_pandas():\n    cat = pd.Categorical(['a', 'b', 'a'])\n    expected_type = pa.dictionary(pa.int8(), pa.string())\n\n    result = pa.array(cat)\n    assert result.to_pylist() == ['a', 'b', 'a']\n    assert result.type.equals(expected_type)\n\n    # with missing values in categorical\n    cat = pd.Categorical(['a', 'b', None, 'a'])\n\n    result = pa.array(cat)\n    assert result.to_pylist() == ['a', 'b', None, 'a']\n    assert result.type.equals(expected_type)\n\n    # with additional mask\n    result = pa.array(cat, mask=np.array([False, False, False, True]))\n    assert result.to_pylist() == ['a', 'b', None, None]\n    assert result.type.equals(expected_type)\n\n\ndef test_dictionary_from_pandas_specified_type():\n    # ARROW-7168 - ensure specified type is always respected\n\n    # the same as cat = pd.Categorical(['a', 'b']) but explicit about dtypes\n    cat = pd.Categorical.from_codes(\n        np.array([0, 1], dtype='int8'), np.array(['a', 'b'], dtype=object))\n\n    # different index type -> allow this\n    # (the type of the 'codes' in pandas is not part of the data type)\n    typ = pa.dictionary(index_type=pa.int16(), value_type=pa.string())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == ['a', 'b']\n\n    # mismatching values type -> raise error\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.int64())\n    with pytest.raises(pa.ArrowInvalid):\n        result = pa.array(cat, type=typ)\n\n    # mismatching order -> raise error\n    typ = pa.dictionary(\n        index_type=pa.int8(), value_type=pa.string(), ordered=True)\n    msg = \"The 'ordered' flag of the passed categorical values \"\n    with pytest.raises(ValueError, match=msg):\n        result = pa.array(cat, type=typ)\n    assert result.to_pylist() == ['a', 'b']\n\n    # with mask\n    typ = pa.dictionary(index_type=pa.int16(), value_type=pa.string())\n    result = pa.array(cat, type=typ, mask=np.array([False, True]))\n    assert result.type.equals(typ)\n    assert result.to_pylist() == ['a', None]\n\n    # empty categorical -> be flexible in values type to allow\n    cat = pd.Categorical([])\n\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.string())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == []\n    typ = pa.dictionary(index_type=pa.int8(), value_type=pa.int64())\n    result = pa.array(cat, type=typ)\n    assert result.type.equals(typ)\n    assert result.to_pylist() == []\n\n    # passing non-dictionary type\n    cat = pd.Categorical(['a', 'b'])\n    result = pa.array(cat, type=pa.string())\n    expected = pa.array(['a', 'b'], type=pa.string())\n    assert result.equals(expected)\n    assert result.to_pylist() == ['a', 'b']\n\n\ndef test_convert_categories_to_array_with_string_pyarrow_dtype():\n    # gh-33727: categories should be converted to pa.Array\n    if Version(pd.__version__) < Version(\"1.3.0\"):\n        pytest.skip(\"PyArrow backed string data type introduced in pandas 1.3.0\")\n\n    df = pd.DataFrame({\"x\": [\"foo\", \"bar\", \"foo\"]}, dtype=\"string[pyarrow]\")\n    df = df.astype(\"category\")\n    indices = pa.array(df['x'].cat.codes)\n    dictionary = pa.array(df[\"x\"].cat.categories.values)\n    assert isinstance(dictionary, pa.Array)\n\n    expected = pa.Array.from_pandas(df['x'])\n    result = pa.DictionaryArray.from_arrays(indices, dictionary)\n    assert result == expected\n\n\n# ----------------------------------------------------------------------\n# Array protocol in pandas conversions tests\n\n\ndef test_array_protocol():\n    df = pd.DataFrame({'a': pd.Series([1, 2, None], dtype='Int64')})\n\n    # __arrow_array__ added to pandas IntegerArray in 0.26.0.dev\n\n    # default conversion\n    result = pa.table(df)\n    expected = pa.array([1, 2, None], pa.int64())\n    assert result[0].chunk(0).equals(expected)\n\n    # with specifying schema\n    schema = pa.schema([('a', pa.float64())])\n    result = pa.table(df, schema=schema)\n    expected2 = pa.array([1, 2, None], pa.float64())\n    assert result[0].chunk(0).equals(expected2)\n\n    # pass Series to pa.array\n    result = pa.array(df['a'])\n    assert result.equals(expected)\n    result = pa.array(df['a'], type=pa.float64())\n    assert result.equals(expected2)\n\n    # pass actual ExtensionArray to pa.array\n    result = pa.array(df['a'].values)\n    assert result.equals(expected)\n    result = pa.array(df['a'].values, type=pa.float64())\n    assert result.equals(expected2)\n\n\nclass DummyExtensionType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int64(),\n                         'pyarrow.tests.test_pandas.DummyExtensionType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        assert serialized == b''\n        assert storage_type == pa.int64()\n        return cls()\n\n\ndef PandasArray__arrow_array__(self, type=None):\n    # hardcode dummy return regardless of self - we only want to check that\n    # this method is correctly called\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    return pa.ExtensionArray.from_storage(DummyExtensionType(), storage)\n\n\ndef test_array_protocol_pandas_extension_types(monkeypatch):\n    # ARROW-7022 - ensure protocol works for Period / Interval extension dtypes\n\n    storage = pa.array([1, 2, 3], type=pa.int64())\n    expected = pa.ExtensionArray.from_storage(DummyExtensionType(), storage)\n\n    monkeypatch.setattr(pd.arrays.PeriodArray, \"__arrow_array__\",\n                        PandasArray__arrow_array__, raising=False)\n    monkeypatch.setattr(pd.arrays.IntervalArray, \"__arrow_array__\",\n                        PandasArray__arrow_array__, raising=False)\n    for arr in [pd.period_range(\"2012-01-01\", periods=3, freq=\"D\").array,\n                pd.interval_range(1, 4).array]:\n        result = pa.array(arr)\n        assert result.equals(expected)\n        result = pa.array(pd.Series(arr))\n        assert result.equals(expected)\n        result = pa.array(pd.Index(arr))\n        assert result.equals(expected)\n        result = pa.table(pd.DataFrame({'a': arr})).column('a').chunk(0)\n        assert result.equals(expected)\n\n\n# ----------------------------------------------------------------------\n# Pandas ExtensionArray support\n\n\ndef _Int64Dtype__from_arrow__(self, array):\n    # for test only deal with single chunk for now\n    # TODO: do we require handling of chunked arrays in the protocol?\n    if isinstance(array, pa.Array):\n        arr = array\n    else:\n        # ChunkedArray - here only deal with a single chunk for the test\n        arr = array.chunk(0)\n    buflist = arr.buffers()\n    data = np.frombuffer(buflist[-1], dtype='int64')[\n        arr.offset:arr.offset + len(arr)]\n    bitmask = buflist[0]\n    if bitmask is not None:\n        mask = pa.BooleanArray.from_buffers(\n            pa.bool_(), len(arr), [None, bitmask])\n        mask = np.asarray(mask)\n    else:\n        mask = np.ones(len(arr), dtype=bool)\n    int_arr = pd.arrays.IntegerArray(data.copy(), ~mask, copy=False)\n    return int_arr\n\n\ndef test_convert_to_extension_array(monkeypatch):\n    import pandas.core.internals as _int\n\n    # table converted from dataframe with extension types (so pandas_metadata\n    # has this information)\n    df = pd.DataFrame(\n        {'a': [1, 2, 3], 'b': pd.array([2, 3, 4], dtype='Int64'),\n         'c': [4, 5, 6]})\n    table = pa.table(df)\n\n    # Int64Dtype is recognized -> convert to extension block by default\n    # for a proper roundtrip\n    result = table.to_pandas()\n    assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    assert _get_mgr(result).blocks[0].values.dtype == np.dtype(\"int64\")\n    assert isinstance(_get_mgr(result).blocks[1], _int.ExtensionBlock)\n    tm.assert_frame_equal(result, df)\n\n    # test with missing values\n    df2 = pd.DataFrame({'a': pd.array([1, 2, None], dtype='Int64')})\n    table2 = pa.table(df2)\n    result = table2.to_pandas()\n    assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    tm.assert_frame_equal(result, df2)\n\n    # monkeypatch pandas Int64Dtype to *not* have the protocol method\n    if Version(pd.__version__) < Version(\"1.3.0.dev\"):\n        monkeypatch.delattr(\n            pd.core.arrays.integer._IntegerDtype, \"__from_arrow__\")\n    else:\n        monkeypatch.delattr(\n            pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")\n    # Int64Dtype has no __from_arrow__ -> use normal conversion\n    result = table.to_pandas()\n    assert len(_get_mgr(result).blocks) == 1\n    assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n\n\nclass MyCustomIntegerType(pa.ExtensionType):\n\n    def __init__(self):\n        super().__init__(pa.int64(),\n                         'pyarrow.tests.test_pandas.MyCustomIntegerType')\n\n    def __arrow_ext_serialize__(self):\n        return b''\n\n    def to_pandas_dtype(self):\n        return pd.Int64Dtype()\n\n\ndef test_conversion_extensiontype_to_extensionarray(monkeypatch):\n    # converting extension type to linked pandas ExtensionDtype/Array\n    import pandas.core.internals as _int\n\n    storage = pa.array([1, 2, 3, 4], pa.int64())\n    arr = pa.ExtensionArray.from_storage(MyCustomIntegerType(), storage)\n    table = pa.table({'a': arr})\n\n    # extension type points to Int64Dtype, which knows how to create a\n    # pandas ExtensionArray\n    result = arr.to_pandas()\n    assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    expected = pd.Series([1, 2, 3, 4], dtype='Int64')\n    tm.assert_series_equal(result, expected)\n\n    result = table.to_pandas()\n    assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    expected = pd.DataFrame({'a': pd.array([1, 2, 3, 4], dtype='Int64')})\n    tm.assert_frame_equal(result, expected)\n\n    # monkeypatch pandas Int64Dtype to *not* have the protocol method\n    # (remove the version added above and the actual version for recent pandas)\n    if Version(pd.__version__) < Version(\"1.3.0.dev\"):\n        monkeypatch.delattr(\n            pd.core.arrays.integer._IntegerDtype, \"__from_arrow__\")\n    else:\n        monkeypatch.delattr(\n            pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")\n\n    result = arr.to_pandas()\n    assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)\n    expected = pd.Series([1, 2, 3, 4])\n    tm.assert_series_equal(result, expected)\n\n    with pytest.raises(ValueError):\n        table.to_pandas()\n\n\ndef test_to_pandas_extension_dtypes_mapping():\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int64())})\n\n    # default use numpy dtype\n    result = table.to_pandas()\n    assert result['a'].dtype == np.dtype('int64')\n\n    # specify to override the default\n    result = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n    assert isinstance(result['a'].dtype, pd.Int64Dtype)\n\n    # types that return None in function get normal conversion\n    table = pa.table({'a': pa.array([1, 2, 3], pa.int32())})\n    result = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\n    assert result['a'].dtype == np.dtype('int32')\n\n    # `types_mapper` overrules the pandas metadata\n    table = pa.table(pd.DataFrame({'a': pd.array([1, 2, 3], dtype=\"Int64\")}))\n    result = table.to_pandas()\n    assert isinstance(result['a'].dtype, pd.Int64Dtype)\n    result = table.to_pandas(\n        types_mapper={pa.int64(): pd.PeriodDtype('D')}.get)\n    assert isinstance(result['a'].dtype, pd.PeriodDtype)\n\n\ndef test_array_to_pandas():\n    if Version(pd.__version__) < Version(\"1.1\"):\n        pytest.skip(\"ExtensionDtype to_pandas method missing\")\n\n    for arr in [pd.period_range(\"2012-01-01\", periods=3, freq=\"D\").array,\n                pd.interval_range(1, 4).array]:\n        result = pa.array(arr).to_pandas()\n        expected = pd.Series(arr)\n        tm.assert_series_equal(result, expected)\n\n        result = pa.table({\"col\": arr})[\"col\"].to_pandas()\n        expected = pd.Series(arr, name=\"col\")\n        tm.assert_series_equal(result, expected)\n\n\ndef test_roundtrip_empty_table_with_extension_dtype_index():\n    df = pd.DataFrame(index=pd.interval_range(start=0, end=3))\n    table = pa.table(df)\n    table.to_pandas().index == pd.Index([{'left': 0, 'right': 1},\n                                         {'left': 1, 'right': 2},\n                                         {'left': 2, 'right': 3}],\n                                        dtype='object')\n\n\n@pytest.mark.parametrize(\"index\", [\"a\", [\"a\", \"b\"]])\ndef test_to_pandas_types_mapper_index(index):\n    if Version(pd.__version__) < Version(\"1.5.0\"):\n        pytest.skip(\"ArrowDtype missing\")\n    df = pd.DataFrame(\n        {\n            \"a\": [1, 2],\n            \"b\": [3, 4],\n            \"c\": [5, 6],\n        },\n        dtype=pd.ArrowDtype(pa.int64()),\n    ).set_index(index)\n    expected = df.copy()\n    table = pa.table(df)\n    result = table.to_pandas(types_mapper=pd.ArrowDtype)\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_array_to_pandas_types_mapper():\n    # https://issues.apache.org/jira/browse/ARROW-9664\n    if Version(pd.__version__) < Version(\"1.2.0\"):\n        pytest.skip(\"Float64Dtype extension dtype missing\")\n\n    data = pa.array([1, 2, 3], pa.int64())\n\n    # Test with mapper function\n    types_mapper = {pa.int64(): pd.Int64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == pd.Int64Dtype()\n\n    # Test mapper function returning None\n    types_mapper = {pa.int64(): None}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n    # Test mapper function not containing the dtype\n    types_mapper = {pa.float64(): pd.Float64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n\n@pytest.mark.pandas\ndef test_chunked_array_to_pandas_types_mapper():\n    # https://issues.apache.org/jira/browse/ARROW-9664\n    if Version(pd.__version__) < Version(\"1.2.0\"):\n        pytest.skip(\"Float64Dtype extension dtype missing\")\n\n    data = pa.chunked_array([pa.array([1, 2, 3], pa.int64())])\n    assert isinstance(data, pa.ChunkedArray)\n\n    # Test with mapper function\n    types_mapper = {pa.int64(): pd.Int64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == pd.Int64Dtype()\n\n    # Test mapper function returning None\n    types_mapper = {pa.int64(): None}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n    # Test mapper function not containing the dtype\n    types_mapper = {pa.float64(): pd.Float64Dtype()}.get\n    result = data.to_pandas(types_mapper=types_mapper)\n    assert result.dtype == np.dtype(\"int64\")\n\n\n# ----------------------------------------------------------------------\n# Legacy metadata compatibility tests\n\n\ndef test_metadata_compat_range_index_pre_0_12():\n    # Forward compatibility for metadata created from pandas.RangeIndex\n    # prior to pyarrow 0.13.0\n    a_values = ['foo', 'bar', None, 'baz']\n    b_values = ['a', 'a', 'b', 'b']\n    a_arrow = pa.array(a_values, type='utf8')\n    b_arrow = pa.array(b_values, type='utf8')\n\n    rng_index_arrow = pa.array([0, 2, 4, 6], type='int64')\n\n    gen_name_0 = '__index_level_0__'\n    gen_name_1 = '__index_level_1__'\n\n    # Case 1: named RangeIndex\n    e1 = pd.DataFrame({\n        'a': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    t1 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['a', 'qux'])\n    t1 = t1.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': ['qux'],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': 'qux',\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r1 = t1.to_pandas()\n    tm.assert_frame_equal(r1, e1)\n\n    # Case 2: named RangeIndex, but conflicts with an actual column\n    e2 = pd.DataFrame({\n        'qux': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    t2 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['qux', gen_name_0])\n    t2 = t2.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r2 = t2.to_pandas()\n    tm.assert_frame_equal(r2, e2)\n\n    # Case 3: unnamed RangeIndex\n    e3 = pd.DataFrame({\n        'a': a_values\n    }, index=pd.RangeIndex(0, 8, step=2, name=None))\n    t3 = pa.Table.from_arrays([a_arrow, rng_index_arrow],\n                              names=['a', gen_name_0])\n    t3 = t3.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r3 = t3.to_pandas()\n    tm.assert_frame_equal(r3, e3)\n\n    # Case 4: MultiIndex with named RangeIndex\n    e4 = pd.DataFrame({\n        'a': a_values\n    }, index=[pd.RangeIndex(0, 8, step=2, name='qux'), b_values])\n    t4 = pa.Table.from_arrays([a_arrow, rng_index_arrow, b_arrow],\n                              names=['a', 'qux', gen_name_1])\n    t4 = t4.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': ['qux', gen_name_1],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': 'qux',\n                          'field_name': 'qux',\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_1,\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r4 = t4.to_pandas()\n    tm.assert_frame_equal(r4, e4)\n\n    # Case 4: MultiIndex with unnamed RangeIndex\n    e5 = pd.DataFrame({\n        'a': a_values\n    }, index=[pd.RangeIndex(0, 8, step=2, name=None), b_values])\n    t5 = pa.Table.from_arrays([a_arrow, rng_index_arrow, b_arrow],\n                              names=['a', gen_name_0, gen_name_1])\n    t5 = t5.replace_schema_metadata({\n        b'pandas': json.dumps(\n            {'index_columns': [gen_name_0, gen_name_1],\n             'column_indexes': [{'name': None,\n                                 'field_name': None,\n                                 'pandas_type': 'unicode',\n                                 'numpy_type': 'object',\n                                 'metadata': {'encoding': 'UTF-8'}}],\n             'columns': [{'name': 'a',\n                          'field_name': 'a',\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_0,\n                          'pandas_type': 'int64',\n                          'numpy_type': 'int64',\n                          'metadata': None},\n                         {'name': None,\n                          'field_name': gen_name_1,\n                          'pandas_type': 'unicode',\n                          'numpy_type': 'object',\n                          'metadata': None}],\n             'pandas_version': '0.23.4'}\n        )})\n    r5 = t5.to_pandas()\n    tm.assert_frame_equal(r5, e5)\n\n\ndef test_metadata_compat_missing_field_name():\n    # Combination of missing field name but with index column as metadata.\n    # This combo occurs in the latest versions of fastparquet (0.3.2), but not\n    # in pyarrow itself (since field_name was added in 0.8, index as metadata\n    # only added later)\n\n    a_values = [1, 2, 3, 4]\n    b_values = ['a', 'b', 'c', 'd']\n    a_arrow = pa.array(a_values, type='int64')\n    b_arrow = pa.array(b_values, type='utf8')\n\n    expected = pd.DataFrame({\n        'a': a_values,\n        'b': b_values,\n    }, index=pd.RangeIndex(0, 8, step=2, name='qux'))\n    table = pa.table({'a': a_arrow, 'b': b_arrow})\n\n    # metadata generated by fastparquet 0.3.2 with missing field_names\n    table = table.replace_schema_metadata({\n        b'pandas': json.dumps({\n            'column_indexes': [\n                {'field_name': None,\n                 'metadata': None,\n                 'name': None,\n                 'numpy_type': 'object',\n                 'pandas_type': 'mixed-integer'}\n            ],\n            'columns': [\n                {'metadata': None,\n                 'name': 'a',\n                 'numpy_type': 'int64',\n                 'pandas_type': 'int64'},\n                {'metadata': None,\n                 'name': 'b',\n                 'numpy_type': 'object',\n                 'pandas_type': 'unicode'}\n            ],\n            'index_columns': [\n                {'kind': 'range',\n                 'name': 'qux',\n                 'start': 0,\n                 'step': 2,\n                 'stop': 8}\n            ],\n            'pandas_version': '0.25.0'}\n\n        )})\n    result = table.to_pandas()\n    tm.assert_frame_equal(result, expected)\n\n\ndef test_metadata_index_name_not_json_serializable():\n    name = np.int64(6)  # not json serializable by default\n    table = pa.table(pd.DataFrame(index=pd.RangeIndex(0, 4, name=name)))\n    metadata = table.schema.pandas_metadata\n    assert metadata['index_columns'][0]['name'] == '6'\n\n\ndef test_metadata_index_name_is_json_serializable():\n    name = 6  # json serializable by default\n    table = pa.table(pd.DataFrame(index=pd.RangeIndex(0, 4, name=name)))\n    metadata = table.schema.pandas_metadata\n    assert metadata['index_columns'][0]['name'] == 6\n\n\ndef make_df_with_timestamps():\n    # Some of the milliseconds timestamps deliberately don't fit in the range\n    # that is possible with nanosecond timestamps.\n    df = pd.DataFrame({\n        'dateTimeMs': [\n            np.datetime64('0001-01-01 00:00', 'ms'),\n            np.datetime64('2012-05-02 12:35', 'ms'),\n            np.datetime64('2012-05-03 15:42', 'ms'),\n            np.datetime64('3000-05-03 15:42', 'ms'),\n        ],\n        'dateTimeNs': [\n            np.datetime64('1991-01-01 00:00', 'ns'),\n            np.datetime64('2012-05-02 12:35', 'ns'),\n            np.datetime64('2012-05-03 15:42', 'ns'),\n            np.datetime64('2050-05-03 15:42', 'ns'),\n        ],\n    })\n    # Not part of what we're testing, just ensuring that the inputs are what we\n    # expect.\n    assert (df.dateTimeMs.dtype, df.dateTimeNs.dtype) == (\n        # O == object, M8[ns] == timestamp64[ns]\n        np.dtype(\"O\"), np.dtype(\"M8[ns]\")\n    )\n    return df\n\n\n@pytest.mark.parquet\n@pytest.mark.filterwarnings(\"ignore:Parquet format '2.0':FutureWarning\")\ndef test_timestamp_as_object_parquet(tempdir):\n    # Timestamps can be stored as Parquet and reloaded into Pandas with no loss\n    # of information if the timestamp_as_object option is True.\n    df = make_df_with_timestamps()\n    table = pa.Table.from_pandas(df)\n    filename = tempdir / \"timestamps_from_pandas.parquet\"\n    pq.write_table(table, filename, version=\"2.0\")\n    result = pq.read_table(filename)\n    df2 = result.to_pandas(timestamp_as_object=True)\n    tm.assert_frame_equal(df, df2)\n\n\ndef test_timestamp_as_object_out_of_range():\n    # Out of range timestamps can be converted Arrow and reloaded into Pandas\n    # with no loss of information if the timestamp_as_object option is True.\n    df = make_df_with_timestamps()\n    table = pa.Table.from_pandas(df)\n    df2 = table.to_pandas(timestamp_as_object=True)\n    tm.assert_frame_equal(df, df2)\n\n\n@pytest.mark.parametrize(\"resolution\", [\"s\", \"ms\", \"us\"])\n@pytest.mark.parametrize(\"tz\", [None, \"America/New_York\"])\n# One datetime outside nanosecond range, one inside nanosecond range:\n@pytest.mark.parametrize(\"dt\", [datetime(1553, 1, 1), datetime(2020, 1, 1)])\ndef test_timestamp_as_object_non_nanosecond(resolution, tz, dt):\n    # Timestamps can be converted Arrow and reloaded into Pandas with no loss\n    # of information if the timestamp_as_object option is True.\n    arr = pa.array([dt], type=pa.timestamp(resolution, tz=tz))\n    table = pa.table({'a': arr})\n\n    for result in [\n        arr.to_pandas(timestamp_as_object=True),\n        table.to_pandas(timestamp_as_object=True)['a']\n    ]:\n        assert result.dtype == object\n        assert isinstance(result[0], datetime)\n        if tz:\n            assert result[0].tzinfo is not None\n            expected = result[0].tzinfo.fromutc(dt)\n        else:\n            assert result[0].tzinfo is None\n            expected = dt\n        assert result[0] == expected\n\n\ndef test_timestamp_as_object_fixed_offset():\n    # ARROW-16547 to_pandas with timestamp_as_object=True and FixedOffset\n    pytz = pytest.importorskip(\"pytz\")\n    import datetime\n    timezone = pytz.FixedOffset(120)\n    dt = timezone.localize(datetime.datetime(2022, 5, 12, 16, 57))\n\n    table = pa.table({\"timestamp_col\": pa.array([dt])})\n    result = table.to_pandas(timestamp_as_object=True)\n    assert pa.table(result) == table\n\n\ndef test_threaded_pandas_import():\n    invoke_script(\"pandas_threaded_import.py\")\n\n\ndef test_does_not_mutate_timedelta_dtype():\n    expected = np.dtype('m8')\n\n    assert np.dtype(np.timedelta64) == expected\n\n    df = pd.DataFrame({\"a\": [np.timedelta64()]})\n    t = pa.Table.from_pandas(df)\n    t.to_pandas()\n\n    assert np.dtype(np.timedelta64) == expected\n\n\ndef test_does_not_mutate_timedelta_nested():\n    # ARROW-17893: dataframe with timedelta and a list of dictionary\n    # also with timedelta produces wrong result with to_pandas\n\n    from datetime import timedelta\n    timedelta_1 = [{\"timedelta_1\": timedelta(seconds=12, microseconds=1)}]\n    timedelta_2 = [timedelta(hours=3, minutes=40, seconds=23)]\n    table = pa.table({\"timedelta_1\": timedelta_1, \"timedelta_2\": timedelta_2})\n    df = table.to_pandas()\n\n    assert df[\"timedelta_2\"][0].to_pytimedelta() == timedelta_2[0]\n\n\ndef test_roundtrip_nested_map_table_with_pydicts():\n    schema = pa.schema([\n        pa.field(\n            \"a\",\n            pa.list_(\n                pa.map_(pa.int8(), pa.struct([pa.field(\"b\", pa.binary())]))\n            )\n        )\n    ])\n    table = pa.table([[\n        [[(1, None)]],\n        None,\n        [\n            [(2, {\"b\": b\"abc\"})],\n            [(3, {\"b\": None}), (4, {\"b\": b\"def\"})],\n        ]\n    ]],\n        schema=schema,\n    )\n\n    expected_default_df = pd.DataFrame(\n        {\"a\": [[[(1, None)]], None, [[(2, {\"b\": b\"abc\"})],\n                                     [(3, {\"b\": None}), (4, {\"b\": b\"def\"})]]]}\n    )\n    expected_as_pydicts_df = pd.DataFrame(\n        {\"a\": [\n            [{1: None}],\n            None,\n            [{2: {\"b\": b\"abc\"}}, {3: {\"b\": None}, 4: {\"b\": b\"def\"}}],\n        ]}\n    )\n\n    default_df = table.to_pandas()\n    as_pydicts_df = table.to_pandas(maps_as_pydicts=\"strict\")\n\n    tm.assert_frame_equal(default_df, expected_default_df)\n    tm.assert_frame_equal(as_pydicts_df, expected_as_pydicts_df)\n\n    table_default_roundtrip = pa.Table.from_pandas(default_df, schema=schema)\n    assert table.equals(table_default_roundtrip)\n\n    table_as_pydicts_roundtrip = pa.Table.from_pandas(as_pydicts_df, schema=schema)\n    assert table.equals(table_as_pydicts_roundtrip)\n\n\ndef test_roundtrip_nested_map_array_with_pydicts_sliced():\n    \"\"\"\n    Slightly more robust test with chunking and slicing\n    \"\"\"\n    keys_1 = pa.array(['foo', 'bar'])\n    keys_2 = pa.array(['baz', 'qux', 'quux', 'quz'])\n    keys_3 = pa.array([], pa.string())\n\n    items_1 = pa.array(\n        [['a', 'b'], ['c', 'd']],\n        pa.list_(pa.string()),\n    )\n    items_2 = pa.array(\n        [[], None, [None, 'e'], ['f', 'g']],\n        pa.list_(pa.string()),\n    )\n    items_3 = pa.array(\n        [],\n        pa.list_(pa.string()),\n    )\n\n    map_chunk_1 = pa.MapArray.from_arrays([0, 2], keys_1, items_1)\n    map_chunk_2 = pa.MapArray.from_arrays([0, 3, 4], keys_2, items_2)\n    map_chunk_3 = pa.MapArray.from_arrays([0, 0], keys_3, items_3)\n    chunked_array = pa.chunked_array([\n        pa.ListArray.from_arrays([0, 1], map_chunk_1).slice(0),\n        pa.ListArray.from_arrays([0, 1], map_chunk_2.slice(1)).slice(0),\n        pa.ListArray.from_arrays([0, 0], map_chunk_3).slice(0),\n    ])\n\n    series_default = chunked_array.to_pandas()\n    expected_series_default = pd.Series([\n        [[('foo', ['a', 'b']), ('bar', ['c', 'd'])]],\n        [[('quz', ['f', 'g'])]],\n        [],\n    ])\n\n    series_pydicts = chunked_array.to_pandas(maps_as_pydicts=\"strict\")\n    expected_series_pydicts = pd.Series([\n        [{'foo': ['a', 'b'], 'bar': ['c', 'd']}],\n        [{'quz': ['f', 'g']}],\n        [],\n    ])\n\n    sliced = chunked_array.slice(1, 3)\n    series_default_sliced = sliced.to_pandas()\n    expected_series_default_sliced = pd.Series([\n        [[('quz', ['f', 'g'])]],\n        [],\n    ])\n\n    series_pydicts_sliced = sliced.to_pandas(maps_as_pydicts=\"strict\")\n    expected_series_pydicts_sliced = pd.Series([\n        [{'quz': ['f', 'g']}],\n        [],\n    ])\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"elementwise comparison failed\",\n                                DeprecationWarning)\n        tm.assert_series_equal(series_default, expected_series_default)\n        tm.assert_series_equal(series_pydicts, expected_series_pydicts)\n        tm.assert_series_equal(series_default_sliced, expected_series_default_sliced)\n        tm.assert_series_equal(series_pydicts_sliced, expected_series_pydicts_sliced)\n\n    ty = pa.list_(pa.map_(pa.string(), pa.list_(pa.string())))\n\n    def assert_roundtrip(series: pd.Series, data) -> None:\n        array_roundtrip = pa.chunked_array(pa.Array.from_pandas(series, type=ty))\n        array_roundtrip.validate(full=True)\n        assert data.equals(array_roundtrip)\n\n    assert_roundtrip(series_default, chunked_array)\n    assert_roundtrip(series_pydicts, chunked_array)\n    assert_roundtrip(series_default_sliced, sliced)\n    assert_roundtrip(series_pydicts_sliced, sliced)\n\n\ndef test_roundtrip_map_array_with_pydicts_duplicate_keys():\n    keys = pa.array(['foo', 'bar', 'foo'])\n    items = pa.array(\n        [['a', 'b'], ['c', 'd'], ['1', '2']],\n        pa.list_(pa.string()),\n    )\n    offsets = [0, 3]\n    maps = pa.MapArray.from_arrays(offsets, keys, items)\n    ty = pa.map_(pa.string(), pa.list_(pa.string()))\n\n    # ------------------------\n    # With maps as pydicts\n    with pytest.raises(pa.lib.ArrowException):\n        # raises because of duplicate keys\n        maps.to_pandas(maps_as_pydicts=\"strict\")\n    series_pydicts = maps.to_pandas(maps_as_pydicts=\"lossy\")\n    # some data loss occurs for duplicate keys\n    expected_series_pydicts = pd.Series([\n        {'foo': ['1', '2'], 'bar': ['c', 'd']},\n    ])\n    # roundtrip is not possible because of data loss\n    assert not maps.equals(pa.Array.from_pandas(series_pydicts, type=ty))\n\n    # ------------------------\n    # With default assoc list of tuples\n    series_default = maps.to_pandas()\n    expected_series_default = pd.Series([\n        [('foo', ['a', 'b']), ('bar', ['c', 'd']), ('foo', ['1', '2'])],\n    ])\n    assert maps.equals(pa.Array.from_pandas(series_default, type=ty))\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series_pydicts, expected_series_pydicts)\n    assert len(series_pydicts) == len(expected_series_pydicts)\n    for row1, row2 in zip(series_pydicts, expected_series_pydicts):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1.items(), row2.items()):\n            assert tup1[0] == tup2[0]\n            assert np.array_equal(tup1[1], tup2[1])\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series_default, expected_series_default)\n    assert len(series_default) == len(expected_series_default)\n    for row1, row2 in zip(series_default, expected_series_default):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1, row2):\n            assert tup1[0] == tup2[0]\n            assert np.array_equal(tup1[1], tup2[1])\n\n\ndef test_unhashable_map_keys_with_pydicts():\n    keys = pa.array(\n        [['a', 'b'], ['c', 'd'], [], ['e'], [None, 'f'], ['g', 'h']],\n        pa.list_(pa.string()),\n    )\n    items = pa.array(['foo', 'bar', 'baz', 'qux', 'quux', 'quz'])\n    offsets = [0, 2, 6]\n    maps = pa.MapArray.from_arrays(offsets, keys, items)\n\n    # ------------------------\n    # With maps as pydicts\n    with pytest.raises(TypeError):\n        maps.to_pandas(maps_as_pydicts=\"lossy\")\n\n    # ------------------------\n    # With default assoc list of tuples\n    series = maps.to_pandas()\n    expected_series_default = pd.Series([\n        [(['a', 'b'], 'foo'), (['c', 'd'], 'bar')],\n        [([], 'baz'), (['e'], 'qux'), ([None, 'f'], 'quux'), (['g', 'h'], 'quz')],\n    ])\n\n    # custom comparison for compatibility w/ Pandas 1.0.0\n    # would otherwise run:\n    #   tm.assert_series_equal(series, expected_series_default)\n    assert len(series) == len(expected_series_default)\n    for row1, row2 in zip(series, expected_series_default):\n        assert len(row1) == len(row2)\n        for tup1, tup2 in zip(row1, row2):\n            assert np.array_equal(tup1[0], tup2[0])\n            assert tup1[1] == tup2[1]\n\n\ndef test_table_column_conversion_for_datetime():\n    # GH-35235\n    # pandas implemented __from_arrow__ for DatetimeTZDtype,\n    # but we choose to do the conversion in Arrow instead.\n    # https://github.com/pandas-dev/pandas/pull/52201\n    series = pd.Series(pd.date_range(\"2012\", periods=2, tz=\"Europe/Brussels\"),\n                       name=\"datetime_column\")\n    table = pa.table({\"datetime_column\": pa.array(series)})\n    table_col = table.column(\"datetime_column\")\n\n    result = table_col.to_pandas()\n    assert result.name == \"datetime_column\"\n    tm.assert_series_equal(result, series)\n\n\ndef test_array_conversion_for_datetime():\n    # GH-35235\n    # pandas implemented __from_arrow__ for DatetimeTZDtype,\n    # but we choose to do the conversion in Arrow instead.\n    # https://github.com/pandas-dev/pandas/pull/52201\n    series = pd.Series(pd.date_range(\"2012\", periods=2, tz=\"Europe/Brussels\"))\n    arr = pa.array(series)\n\n    result = arr.to_pandas()\n    tm.assert_series_equal(result, series)\n\n\n@pytest.mark.large_memory\ndef test_nested_chunking_valid():\n    # GH-32439: Chunking can cause arrays to be in invalid state\n    # when nested types are involved.\n    # Here we simply ensure we validate correctly.\n\n    def roundtrip(df, schema=None):\n        tab = pa.Table.from_pandas(df, schema=schema)\n        tab.validate(full=True)\n        # we expect to trigger chunking internally\n        # an assertion failure here may just mean this threshold has changed\n        num_chunks = tab.column(0).num_chunks\n        assert num_chunks > 1\n        tm.assert_frame_equal(tab.to_pandas(self_destruct=True,\n                                            maps_as_pydicts=\"strict\"), df)\n\n    x = b\"0\" * 720000000\n    roundtrip(pd.DataFrame({\"strings\": [x, x, x]}))\n\n    struct = {\"struct_field\": x}\n    roundtrip(pd.DataFrame({\"structs\": [struct, struct, struct]}))\n\n    lists = [x]\n    roundtrip(pd.DataFrame({\"lists\": [lists, lists, lists]}))\n\n    los = [struct]\n    roundtrip(pd.DataFrame({\"los\": [los, los, los]}))\n\n    sol = {\"struct_field\": lists}\n    roundtrip(pd.DataFrame({\"sol\": [sol, sol, sol]}))\n\n    map_of_los = {\"a\": los}\n    map_type = pa.map_(pa.string(),\n                       pa.list_(pa.struct([(\"struct_field\", pa.binary())])))\n    schema = pa.schema([(\"maps\", map_type)])\n    roundtrip(pd.DataFrame({\"maps\": [map_of_los, map_of_los, map_of_los]}),\n              schema=schema)\n", "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer, PyCapsule_New, PyCapsule_IsValid\n\nimport atexit\nfrom collections.abc import Mapping\nimport pickle\nimport re\nimport sys\nimport warnings\nfrom cython import sizeof\n\n# These are imprecise because the type (in pandas 0.x) depends on the presence\n# of nulls\ncdef dict _pandas_type_map = {\n    _Type_NA: np.object_,  # NaNs\n    _Type_BOOL: np.bool_,\n    _Type_INT8: np.int8,\n    _Type_INT16: np.int16,\n    _Type_INT32: np.int32,\n    _Type_INT64: np.int64,\n    _Type_UINT8: np.uint8,\n    _Type_UINT16: np.uint16,\n    _Type_UINT32: np.uint32,\n    _Type_UINT64: np.uint64,\n    _Type_HALF_FLOAT: np.float16,\n    _Type_FLOAT: np.float32,\n    _Type_DOUBLE: np.float64,\n    # Pandas does not support [D]ay, so default to [ms] for date32\n    _Type_DATE32: np.dtype('datetime64[ms]'),\n    _Type_DATE64: np.dtype('datetime64[ms]'),\n    _Type_TIMESTAMP: {\n        's': np.dtype('datetime64[s]'),\n        'ms': np.dtype('datetime64[ms]'),\n        'us': np.dtype('datetime64[us]'),\n        'ns': np.dtype('datetime64[ns]'),\n    },\n    _Type_DURATION: {\n        's': np.dtype('timedelta64[s]'),\n        'ms': np.dtype('timedelta64[ms]'),\n        'us': np.dtype('timedelta64[us]'),\n        'ns': np.dtype('timedelta64[ns]'),\n    },\n    _Type_BINARY: np.object_,\n    _Type_FIXED_SIZE_BINARY: np.object_,\n    _Type_STRING: np.object_,\n    _Type_LIST: np.object_,\n    _Type_MAP: np.object_,\n    _Type_DECIMAL128: np.object_,\n}\n\ncdef dict _pep3118_type_map = {\n    _Type_INT8: b'b',\n    _Type_INT16: b'h',\n    _Type_INT32: b'i',\n    _Type_INT64: b'q',\n    _Type_UINT8: b'B',\n    _Type_UINT16: b'H',\n    _Type_UINT32: b'I',\n    _Type_UINT64: b'Q',\n    _Type_HALF_FLOAT: b'e',\n    _Type_FLOAT: b'f',\n    _Type_DOUBLE: b'd',\n}\n\n\ncdef bytes _datatype_to_pep3118(CDataType* type):\n    \"\"\"\n    Construct a PEP 3118 format string describing the given datatype.\n    None is returned for unsupported types.\n    \"\"\"\n    try:\n        char = _pep3118_type_map[type.id()]\n    except KeyError:\n        return None\n    else:\n        if char in b'bBhHiIqQ':\n            # Use \"standard\" int widths, not native\n            return b'=' + char\n        else:\n            return char\n\n\ncdef void* _as_c_pointer(v, allow_null=False) except *:\n    \"\"\"\n    Convert a Python object to a raw C pointer.\n\n    Used mainly for the C data interface.\n    Integers are accepted as well as capsule objects with a NULL name.\n    (the latter for compatibility with raw pointers exported by reticulate)\n    \"\"\"\n    cdef void* c_ptr\n    if isinstance(v, int):\n        c_ptr = <void*> <uintptr_t > v\n    elif isinstance(v, float):\n        warnings.warn(\n            \"Passing a pointer value as a float is unsafe and only \"\n            \"supported for compatibility with older versions of the R \"\n            \"Arrow library\", UserWarning, stacklevel=2)\n        c_ptr = <void*> <uintptr_t > v\n    elif PyCapsule_CheckExact(v):\n        c_ptr = PyCapsule_GetPointer(v, NULL)\n    else:\n        raise TypeError(f\"Expected a pointer value, got {type(v)!r}\")\n    if not allow_null and c_ptr == NULL:\n        raise ValueError(f\"Null pointer (value before cast = {v!r})\")\n    return c_ptr\n\n\ndef _is_primitive(Type type):\n    # This is simply a redirect, the official API is in pyarrow.types.\n    return is_primitive(type)\n\n\ndef _get_pandas_type(arrow_type, coerce_to_ns=False):\n    cdef Type type_id = arrow_type.id\n    if type_id not in _pandas_type_map:\n        return None\n    if coerce_to_ns:\n        # ARROW-3789: Coerce date/timestamp types to datetime64[ns]\n        if type_id == _Type_DURATION:\n            return np.dtype('timedelta64[ns]')\n        return np.dtype('datetime64[ns]')\n    pandas_type = _pandas_type_map[type_id]\n    if isinstance(pandas_type, dict):\n        unit = getattr(arrow_type, 'unit', None)\n        pandas_type = pandas_type.get(unit, None)\n    return pandas_type\n\n\ndef _get_pandas_tz_type(arrow_type, coerce_to_ns=False):\n    from pyarrow.pandas_compat import make_datetimetz\n    unit = 'ns' if coerce_to_ns else arrow_type.unit\n    return make_datetimetz(unit, arrow_type.tz)\n\n\ndef _to_pandas_dtype(arrow_type, options=None):\n    coerce_to_ns = (options and options.get('coerce_temporal_nanoseconds', False)) or (\n        _pandas_api.is_v1() and arrow_type.id in\n        [_Type_DATE32, _Type_DATE64, _Type_TIMESTAMP, _Type_DURATION])\n\n    if getattr(arrow_type, 'tz', None):\n        dtype = _get_pandas_tz_type(arrow_type, coerce_to_ns)\n    else:\n        dtype = _get_pandas_type(arrow_type, coerce_to_ns)\n\n    if not dtype:\n        raise NotImplementedError(str(arrow_type))\n\n    return dtype\n\n\n# Workaround for Cython parsing bug\n# https://github.com/cython/cython/issues/2143\nctypedef CFixedWidthType* _CFixedWidthTypePtr\n\n\ncdef class DataType(_Weakrefable):\n    \"\"\"\n    Base class of all Arrow data types.\n\n    Each data type is an *instance* of this class.\n\n    Examples\n    --------\n    Instance of int64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int64()\n    DataType(int64)\n    \"\"\"\n\n    def __cinit__(self):\n        pass\n\n    def __init__(self):\n        raise TypeError(\"Do not call {}'s constructor directly, use public \"\n                        \"functions like pyarrow.int64, pyarrow.list_, etc. \"\n                        \"instead.\".format(self.__class__.__name__))\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        assert type != nullptr\n        self.sp_type = type\n        self.type = type.get()\n        self.pep3118_format = _datatype_to_pep3118(self.type)\n\n    cpdef Field field(self, i):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        pyarrow.Field\n        \"\"\"\n        if not isinstance(i, int):\n            raise TypeError(f\"Expected int index, got type '{type(i)}'\")\n        cdef int index = <int> _normalize_index(i, self.type.num_fields())\n        return pyarrow_wrap_field(self.type.field(index))\n\n    @property\n    def id(self):\n        return self.type.id()\n\n    @property\n    def bit_width(self):\n        \"\"\"\n        Bit width for fixed width type.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64()\n        DataType(int64)\n        >>> pa.int64().bit_width\n        64\n        \"\"\"\n        cdef _CFixedWidthTypePtr ty\n        ty = dynamic_cast[_CFixedWidthTypePtr](self.type)\n        if ty == nullptr:\n            raise ValueError(\"Non-fixed width type\")\n        return ty.bit_width()\n\n    @property\n    def num_fields(self):\n        \"\"\"\n        The number of child fields.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64()\n        DataType(int64)\n        >>> pa.int64().num_fields\n        0\n        >>> pa.list_(pa.string())\n        ListType(list<item: string>)\n        >>> pa.list_(pa.string()).num_fields\n        1\n        >>> struct = pa.struct({'x': pa.int32(), 'y': pa.string()})\n        >>> struct.num_fields\n        2\n        \"\"\"\n        return self.type.num_fields()\n\n    @property\n    def num_buffers(self):\n        \"\"\"\n        Number of data buffers required to construct Array type\n        excluding children.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64().num_buffers\n        2\n        >>> pa.string().num_buffers\n        3\n        \"\"\"\n        return self.type.layout().buffers.size()\n\n    def __str__(self):\n        return frombytes(self.type.ToString(), safe=True)\n\n    def __hash__(self):\n        return hash(str(self))\n\n    def __reduce__(self):\n        return type_for_alias, (str(self),)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0})'.format(self)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except (TypeError, ValueError):\n            return NotImplemented\n\n    def equals(self, other, *, check_metadata=False):\n        \"\"\"\n        Return true if type is equivalent to passed value.\n\n        Parameters\n        ----------\n        other : DataType or string convertible to DataType\n        check_metadata : bool\n            Whether nested Field metadata equality should be checked as well.\n\n        Returns\n        -------\n        is_equal : bool\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64().equals(pa.string())\n        False\n        >>> pa.int64().equals(pa.int64())\n        True\n        \"\"\"\n        cdef:\n            DataType other_type\n            c_bool c_check_metadata\n\n        other_type = ensure_type(other)\n        c_check_metadata = check_metadata\n        return self.type.Equals(deref(other_type.type), c_check_metadata)\n\n    def to_pandas_dtype(self):\n        \"\"\"\n        Return the equivalent NumPy / Pandas dtype.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.int64().to_pandas_dtype()\n        <class 'numpy.int64'>\n        \"\"\"\n        return _to_pandas_dtype(self)\n\n    def _export_to_c(self, out_ptr):\n        \"\"\"\n        Export to a C ArrowSchema struct, given its pointer.\n\n        Be careful: if you don't pass the ArrowSchema struct to a consumer,\n        its memory will leak.  This is a low-level function intended for\n        expert users.\n        \"\"\"\n        check_status(ExportType(deref(self.type),\n                                <ArrowSchema*> _as_c_pointer(out_ptr)))\n\n    @staticmethod\n    def _import_from_c(in_ptr):\n        \"\"\"\n        Import DataType from a C ArrowSchema struct, given its pointer.\n\n        This is a low-level function intended for expert users.\n        \"\"\"\n        result = GetResultValue(ImportType(<ArrowSchema*>\n                                           _as_c_pointer(in_ptr)))\n        return pyarrow_wrap_data_type(result)\n\n    def __arrow_c_schema__(self):\n        \"\"\"\n        Export to a ArrowSchema PyCapsule\n\n        Unlike _export_to_c, this will not leak memory if the capsule is not used.\n        \"\"\"\n        cdef ArrowSchema* c_schema\n        capsule = alloc_c_schema(&c_schema)\n\n        with nogil:\n            check_status(ExportType(deref(self.type), c_schema))\n\n        return capsule\n\n    @staticmethod\n    def _import_from_c_capsule(schema):\n        \"\"\"\n        Import a DataType from a ArrowSchema PyCapsule\n\n        Parameters\n        ----------\n        schema : PyCapsule\n            A valid PyCapsule with name 'arrow_schema' containing an\n            ArrowSchema pointer.\n        \"\"\"\n        cdef:\n            ArrowSchema* c_schema\n            shared_ptr[CDataType] c_type\n\n        if not PyCapsule_IsValid(schema, 'arrow_schema'):\n            raise TypeError(\n                \"Not an ArrowSchema object\"\n            )\n        c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')\n\n        with nogil:\n            c_type = GetResultValue(ImportType(c_schema))\n\n        return pyarrow_wrap_data_type(c_type)\n\n\ncdef class DictionaryMemo(_Weakrefable):\n    \"\"\"\n    Tracking container for dictionary-encoded fields.\n    \"\"\"\n\n    def __cinit__(self):\n        self.sp_memo.reset(new CDictionaryMemo())\n        self.memo = self.sp_memo.get()\n\n\ncdef class DictionaryType(DataType):\n    \"\"\"\n    Concrete class for dictionary data types.\n\n    Examples\n    --------\n    Create an instance of dictionary type:\n\n    >>> import pyarrow as pa\n    >>> pa.dictionary(pa.int64(), pa.utf8())\n    DictionaryType(dictionary<values=string, indices=int64, ordered=0>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.dict_type = <const CDictionaryType*> type.get()\n\n    def __reduce__(self):\n        return dictionary, (self.index_type, self.value_type, self.ordered)\n\n    @property\n    def ordered(self):\n        \"\"\"\n        Whether the dictionary is ordered, i.e. whether the ordering of values\n        in the dictionary is important.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.dictionary(pa.int64(), pa.utf8()).ordered\n        False\n        \"\"\"\n        return self.dict_type.ordered()\n\n    @property\n    def index_type(self):\n        \"\"\"\n        The data type of dictionary indices (a signed integer type).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.dictionary(pa.int16(), pa.utf8()).index_type\n        DataType(int16)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.dict_type.index_type())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The dictionary value type.\n\n        The dictionary values are found in an instance of DictionaryArray.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.dictionary(pa.int16(), pa.utf8()).value_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.dict_type.value_type())\n\n\ncdef class ListType(DataType):\n    \"\"\"\n    Concrete class for list data types.\n\n    Examples\n    --------\n    Create an instance of ListType:\n\n    >>> import pyarrow as pa\n    >>> pa.list_(pa.string())\n    ListType(list<item: string>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.list_type = <const CListType*> type.get()\n\n    def __reduce__(self):\n        return list_, (self.value_field,)\n\n    @property\n    def value_field(self):\n        \"\"\"\n        The field for list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.string()).value_field\n        pyarrow.Field<item: string>\n        \"\"\"\n        return pyarrow_wrap_field(self.list_type.value_field())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The data type of list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.string()).value_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.list_type.value_type())\n\n\ncdef class LargeListType(DataType):\n    \"\"\"\n    Concrete class for large list data types\n    (like ListType, but with 64-bit offsets).\n\n    Examples\n    --------\n    Create an instance of LargeListType:\n\n    >>> import pyarrow as pa\n    >>> pa.large_list(pa.string())\n    LargeListType(large_list<item: string>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.list_type = <const CLargeListType*> type.get()\n\n    def __reduce__(self):\n        return large_list, (self.value_field,)\n\n    @property\n    def value_field(self):\n        return pyarrow_wrap_field(self.list_type.value_field())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The data type of large list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.large_list(pa.string()).value_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.list_type.value_type())\n\n\ncdef class MapType(DataType):\n    \"\"\"\n    Concrete class for map data types.\n\n    Examples\n    --------\n    Create an instance of MapType:\n\n    >>> import pyarrow as pa\n    >>> pa.map_(pa.string(), pa.int32())\n    MapType(map<string, int32>)\n    >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True)\n    MapType(map<string, int32, keys_sorted>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.map_type = <const CMapType*> type.get()\n\n    def __reduce__(self):\n        return map_, (self.key_field, self.item_field)\n\n    @property\n    def key_field(self):\n        \"\"\"\n        The field for keys in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).key_field\n        pyarrow.Field<key: string not null>\n        \"\"\"\n        return pyarrow_wrap_field(self.map_type.key_field())\n\n    @property\n    def key_type(self):\n        \"\"\"\n        The data type of keys in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).key_type\n        DataType(string)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.map_type.key_type())\n\n    @property\n    def item_field(self):\n        \"\"\"\n        The field for items in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).item_field\n        pyarrow.Field<value: int32>\n        \"\"\"\n        return pyarrow_wrap_field(self.map_type.item_field())\n\n    @property\n    def item_type(self):\n        \"\"\"\n        The data type of items in the map entries.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32()).item_type\n        DataType(int32)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.map_type.item_type())\n\n    @property\n    def keys_sorted(self):\n        \"\"\"\n        Should the entries be sorted according to keys.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True).keys_sorted\n        True\n        \"\"\"\n        return self.map_type.keys_sorted()\n\n\ncdef class FixedSizeListType(DataType):\n    \"\"\"\n    Concrete class for fixed size list data types.\n\n    Examples\n    --------\n    Create an instance of FixedSizeListType:\n\n    >>> import pyarrow as pa\n    >>> pa.list_(pa.int32(), 2)\n    FixedSizeListType(fixed_size_list<item: int32>[2])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.list_type = <const CFixedSizeListType*> type.get()\n\n    def __reduce__(self):\n        return list_, (self.value_type, self.list_size)\n\n    @property\n    def value_field(self):\n        \"\"\"\n        The field for list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.int32(), 2).value_field\n        pyarrow.Field<item: int32>\n        \"\"\"\n        return pyarrow_wrap_field(self.list_type.value_field())\n\n    @property\n    def value_type(self):\n        \"\"\"\n        The data type of large list values.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.int32(), 2).value_type\n        DataType(int32)\n        \"\"\"\n        return pyarrow_wrap_data_type(self.list_type.value_type())\n\n    @property\n    def list_size(self):\n        \"\"\"\n        The size of the fixed size lists.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> pa.list_(pa.int32(), 2).list_size\n        2\n        \"\"\"\n        return self.list_type.list_size()\n\n\ncdef class StructType(DataType):\n    \"\"\"\n    Concrete class for struct data types.\n\n    ``StructType`` supports direct indexing using ``[...]`` (implemented via\n    ``__getitem__``) to access its fields.\n    It will return the struct field with the given index or name.\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n\n    Accessing fields using direct indexing:\n\n    >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n    >>> struct_type[0]\n    pyarrow.Field<x: int32>\n    >>> struct_type['y']\n    pyarrow.Field<y: string>\n\n    Accessing fields using ``field()``:\n\n    >>> struct_type.field(1)\n    pyarrow.Field<y: string>\n    >>> struct_type.field('x')\n    pyarrow.Field<x: int32>\n\n    # Creating a schema from the struct type's fields:\n    >>> pa.schema(list(struct_type))\n    x: int32\n    y: string\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.struct_type = <const CStructType*> type.get()\n\n    cdef Field field_by_name(self, name):\n        \"\"\"\n        Return a child field by its name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        field : Field\n            The child field with the given name.\n\n        Raises\n        ------\n        KeyError\n            If the name isn't found, or if several fields have the given\n            name.\n        \"\"\"\n        cdef vector[shared_ptr[CField]] fields\n\n        fields = self.struct_type.GetAllFieldsByName(tobytes(name))\n        if fields.size() == 0:\n            raise KeyError(name)\n        elif fields.size() > 1:\n            warnings.warn(\"Struct field name corresponds to more \"\n                          \"than one field\", UserWarning)\n            raise KeyError(name)\n        else:\n            return pyarrow_wrap_field(fields[0])\n\n    def get_field_index(self, name):\n        \"\"\"\n        Return index of the unique field with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        index : int\n            The index of the field with the given name; -1 if the\n            name isn't found or there are several fields with the given\n            name.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n\n        Index of the field with a name 'y':\n\n        >>> struct_type.get_field_index('y')\n        1\n\n        Index of the field that does not exist:\n\n        >>> struct_type.get_field_index('z')\n        -1\n        \"\"\"\n        return self.struct_type.GetFieldIndex(tobytes(name))\n\n    cpdef Field field(self, i):\n        \"\"\"\n        Select a field by its column name or numeric index.\n\n        Parameters\n        ----------\n        i : int or str\n\n        Returns\n        -------\n        pyarrow.Field\n\n        Examples\n        --------\n\n        >>> import pyarrow as pa\n        >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n\n        Select the second field:\n\n        >>> struct_type.field(1)\n        pyarrow.Field<y: string>\n\n        Select the field named 'x':\n\n        >>> struct_type.field('x')\n        pyarrow.Field<x: int32>\n        \"\"\"\n        if isinstance(i, (bytes, str)):\n            return self.field_by_name(i)\n        elif isinstance(i, int):\n            return DataType.field(self, i)\n        else:\n            raise TypeError('Expected integer or string index')\n\n    def get_all_field_indices(self, name):\n        \"\"\"\n        Return sorted list of indices for the fields with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        indices : List[int]\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})\n        >>> struct_type.get_all_field_indices('x')\n        [0]\n        \"\"\"\n        return self.struct_type.GetAllFieldIndices(tobytes(name))\n\n    def __len__(self):\n        \"\"\"\n        Like num_fields().\n        \"\"\"\n        return self.type.num_fields()\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over struct fields, in order.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, i):\n        \"\"\"\n        Return the struct field with the given index or name.\n\n        Alias of ``field``.\n        \"\"\"\n        return self.field(i)\n\n    def __reduce__(self):\n        return struct, (list(self),)\n\n\ncdef class UnionType(DataType):\n    \"\"\"\n    Base class for union data types.\n\n    Examples\n    --------\n    Create an instance of a dense UnionType using ``pa.union``:\n\n    >>> import pyarrow as pa\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_DENSE),\n    (DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a dense UnionType using ``pa.dense_union``:\n\n    >>> pa.dense_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>)\n\n    Create an instance of a sparse UnionType using ``pa.union``:\n\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_SPARSE),\n    (SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a sparse UnionType using ``pa.sparse_union``:\n\n    >>> pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>)\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n\n    @property\n    def mode(self):\n        \"\"\"\n        The mode of the union (\"dense\" or \"sparse\").\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n        >>> union.mode\n        'sparse'\n        \"\"\"\n        cdef CUnionType* type = <CUnionType*> self.sp_type.get()\n        cdef int mode = type.mode()\n        if mode == _UnionMode_DENSE:\n            return 'dense'\n        if mode == _UnionMode_SPARSE:\n            return 'sparse'\n        assert 0\n\n    @property\n    def type_codes(self):\n        \"\"\"\n        The type code to indicate each data type in this union.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n        >>> union.type_codes\n        [0, 1]\n        \"\"\"\n        cdef CUnionType* type = <CUnionType*> self.sp_type.get()\n        return type.type_codes()\n\n    def __len__(self):\n        \"\"\"\n        Like num_fields().\n        \"\"\"\n        return self.type.num_fields()\n\n    def __iter__(self):\n        \"\"\"\n        Iterate over union members, in order.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    cpdef Field field(self, i):\n        \"\"\"\n        Return a child field by its numeric index.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n        >>> union[0]\n        pyarrow.Field<a: fixed_size_binary[10]>\n        \"\"\"\n        if isinstance(i, int):\n            return DataType.field(self, i)\n        else:\n            raise TypeError('Expected integer')\n\n    def __getitem__(self, i):\n        \"\"\"\n        Return a child field by its index.\n\n        Alias of ``field``.\n        \"\"\"\n        return self.field(i)\n\n    def __reduce__(self):\n        return union, (list(self), self.mode, self.type_codes)\n\n\ncdef class SparseUnionType(UnionType):\n    \"\"\"\n    Concrete class for sparse union types.\n\n    Examples\n    --------\n    Create an instance of a sparse UnionType using ``pa.union``:\n\n    >>> import pyarrow as pa\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_SPARSE),\n    (SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a sparse UnionType using ``pa.sparse_union``:\n\n    >>> pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>)\n    \"\"\"\n\n\ncdef class DenseUnionType(UnionType):\n    \"\"\"\n    Concrete class for dense union types.\n\n    Examples\n    --------\n    Create an instance of a dense UnionType using ``pa.union``:\n\n    >>> import pyarrow as pa\n    >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],\n    ...          mode=pa.lib.UnionMode_DENSE),\n    (DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>),)\n\n    Create an instance of a dense UnionType using ``pa.dense_union``:\n\n    >>> pa.dense_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])\n    DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>)\n    \"\"\"\n\n\ncdef class TimestampType(DataType):\n    \"\"\"\n    Concrete class for timestamp data types.\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n\n    Create an instance of timestamp type:\n\n    >>> pa.timestamp('us')\n    TimestampType(timestamp[us])\n\n    Create an instance of timestamp type with timezone:\n\n    >>> pa.timestamp('s', tz='UTC')\n    TimestampType(timestamp[s, tz=UTC])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.ts_type = <const CTimestampType*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The timestamp unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.timestamp('us')\n        >>> t.unit\n        'us'\n        \"\"\"\n        return timeunit_to_string(self.ts_type.unit())\n\n    @property\n    def tz(self):\n        \"\"\"\n        The timestamp time zone, if any, or None.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.timestamp('s', tz='UTC')\n        >>> t.tz\n        'UTC'\n        \"\"\"\n        if self.ts_type.timezone().size() > 0:\n            return frombytes(self.ts_type.timezone())\n        else:\n            return None\n\n    def __reduce__(self):\n        return timestamp, (self.unit, self.tz)\n\n\ncdef class Time32Type(DataType):\n    \"\"\"\n    Concrete class for time32 data types.\n\n    Examples\n    --------\n    Create an instance of time32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.time32('ms')\n    Time32Type(time32[ms])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.time_type = <const CTime32Type*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The time unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.time32('ms')\n        >>> t.unit\n        'ms'\n        \"\"\"\n        return timeunit_to_string(self.time_type.unit())\n\n\ncdef class Time64Type(DataType):\n    \"\"\"\n    Concrete class for time64 data types.\n\n    Examples\n    --------\n    Create an instance of time64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.time64('us')\n    Time64Type(time64[us])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.time_type = <const CTime64Type*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The time unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.time64('us')\n        >>> t.unit\n        'us'\n        \"\"\"\n        return timeunit_to_string(self.time_type.unit())\n\n\ncdef class DurationType(DataType):\n    \"\"\"\n    Concrete class for duration data types.\n\n    Examples\n    --------\n    Create an instance of duration type:\n\n    >>> import pyarrow as pa\n    >>> pa.duration('s')\n    DurationType(duration[s])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.duration_type = <const CDurationType*> type.get()\n\n    @property\n    def unit(self):\n        \"\"\"\n        The duration unit ('s', 'ms', 'us' or 'ns').\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.duration('s')\n        >>> t.unit\n        's'\n        \"\"\"\n        return timeunit_to_string(self.duration_type.unit())\n\n\ncdef class FixedSizeBinaryType(DataType):\n    \"\"\"\n    Concrete class for fixed-size binary data types.\n\n    Examples\n    --------\n    Create an instance of fixed-size binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.binary(3)\n    FixedSizeBinaryType(fixed_size_binary[3])\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.fixed_size_binary_type = (\n            <const CFixedSizeBinaryType*> type.get())\n\n    def __reduce__(self):\n        return binary, (self.byte_width,)\n\n    @property\n    def byte_width(self):\n        \"\"\"\n        The binary size in bytes.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.binary(3)\n        >>> t.byte_width\n        3\n        \"\"\"\n        return self.fixed_size_binary_type.byte_width()\n\n\ncdef class Decimal128Type(FixedSizeBinaryType):\n    \"\"\"\n    Concrete class for decimal128 data types.\n\n    Examples\n    --------\n    Create an instance of decimal128 type:\n\n    >>> import pyarrow as pa\n    >>> pa.decimal128(5, 2)\n    Decimal128Type(decimal128(5, 2))\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        FixedSizeBinaryType.init(self, type)\n        self.decimal128_type = <const CDecimal128Type*> type.get()\n\n    def __reduce__(self):\n        return decimal128, (self.precision, self.scale)\n\n    @property\n    def precision(self):\n        \"\"\"\n        The decimal precision, in number of decimal digits (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal128(5, 2)\n        >>> t.precision\n        5\n        \"\"\"\n        return self.decimal128_type.precision()\n\n    @property\n    def scale(self):\n        \"\"\"\n        The decimal scale (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal128(5, 2)\n        >>> t.scale\n        2\n        \"\"\"\n        return self.decimal128_type.scale()\n\n\ncdef class Decimal256Type(FixedSizeBinaryType):\n    \"\"\"\n    Concrete class for decimal256 data types.\n\n    Examples\n    --------\n    Create an instance of decimal256 type:\n\n    >>> import pyarrow as pa\n    >>> pa.decimal256(76, 38)\n    Decimal256Type(decimal256(76, 38))\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        FixedSizeBinaryType.init(self, type)\n        self.decimal256_type = <const CDecimal256Type*> type.get()\n\n    def __reduce__(self):\n        return decimal256, (self.precision, self.scale)\n\n    @property\n    def precision(self):\n        \"\"\"\n        The decimal precision, in number of decimal digits (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal256(76, 38)\n        >>> t.precision\n        76\n        \"\"\"\n        return self.decimal256_type.precision()\n\n    @property\n    def scale(self):\n        \"\"\"\n        The decimal scale (an integer).\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> t = pa.decimal256(76, 38)\n        >>> t.scale\n        38\n        \"\"\"\n        return self.decimal256_type.scale()\n\n\ncdef class RunEndEncodedType(DataType):\n    \"\"\"\n    Concrete class for run-end encoded types.\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.run_end_encoded_type = <const CRunEndEncodedType*> type.get()\n\n    def __reduce__(self):\n        return run_end_encoded, (self.run_end_type, self.value_type)\n\n    @property\n    def run_end_type(self):\n        return pyarrow_wrap_data_type(self.run_end_encoded_type.run_end_type())\n\n    @property\n    def value_type(self):\n        return pyarrow_wrap_data_type(self.run_end_encoded_type.value_type())\n\n\ncdef class BaseExtensionType(DataType):\n    \"\"\"\n    Concrete base class for extension types.\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        DataType.init(self, type)\n        self.ext_type = <const CExtensionType*> type.get()\n\n    def __arrow_ext_class__(self):\n        \"\"\"\n        The associated array extension class\n        \"\"\"\n        return ExtensionArray\n\n    def __arrow_ext_scalar_class__(self):\n        \"\"\"\n        The associated scalar class\n        \"\"\"\n        return ExtensionScalar\n\n    @property\n    def extension_name(self):\n        \"\"\"\n        The extension type name.\n        \"\"\"\n        return frombytes(self.ext_type.extension_name())\n\n    @property\n    def storage_type(self):\n        \"\"\"\n        The underlying storage type.\n        \"\"\"\n        return pyarrow_wrap_data_type(self.ext_type.storage_type())\n\n    def wrap_array(self, storage):\n        \"\"\"\n        Wrap the given storage array as an extension array.\n\n        Parameters\n        ----------\n        storage : Array or ChunkedArray\n\n        Returns\n        -------\n        array : Array or ChunkedArray\n            Extension array wrapping the storage array\n        \"\"\"\n        cdef:\n            shared_ptr[CDataType] c_storage_type\n\n        if isinstance(storage, Array):\n            c_storage_type = (<Array> storage).ap.type()\n        elif isinstance(storage, ChunkedArray):\n            c_storage_type = (<ChunkedArray> storage).chunked_array.type()\n        else:\n            raise TypeError(\n                f\"Expected array or chunked array, got {storage.__class__}\")\n\n        if not c_storage_type.get().Equals(deref(self.ext_type)\n                                           .storage_type(), False):\n            raise TypeError(\n                f\"Incompatible storage type for {self}: \"\n                f\"expected {self.storage_type}, got {storage.type}\")\n\n        if isinstance(storage, Array):\n            return pyarrow_wrap_array(\n                self.ext_type.WrapArray(\n                    self.sp_type, (<Array> storage).sp_array))\n        else:\n            return pyarrow_wrap_chunked_array(\n                self.ext_type.WrapArray(\n                    self.sp_type, (<ChunkedArray> storage).sp_chunked_array))\n\n\ncdef class ExtensionType(BaseExtensionType):\n    \"\"\"\n    Concrete base class for Python-defined extension types.\n\n    Parameters\n    ----------\n    storage_type : DataType\n        The underlying storage type for the extension type.\n    extension_name : str\n        A unique name distinguishing this extension type. The name will be\n        used when deserializing IPC data.\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing ExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.ExtensionType):\n    ...    def __init__(self):\n    ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n    ...    def __arrow_ext_serialize__(self):\n    ...       # since we don't have a parameterized type, we don't need extra\n    ...       # metadata to be deserialized\n    ...       return b''\n    ...    @classmethod\n    ...    def __arrow_ext_deserialize__(self, storage_type, serialized):\n    ...       # return an instance of this subclass given the serialized\n    ...       # metadata.\n    ...       return UuidType()\n    ...\n\n    Register the extension type:\n\n    >>> pa.register_extension_type(UuidType())\n\n    Create an instance of UuidType extension type:\n\n    >>> uuid_type = UuidType()\n\n    Inspect the extension type:\n\n    >>> uuid_type.extension_name\n    'my_package.uuid'\n    >>> uuid_type.storage_type\n    FixedSizeBinaryType(fixed_size_binary[16])\n\n    Wrap an array as an extension array:\n\n    >>> import uuid\n    >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)], pa.binary(16))\n    >>> uuid_type.wrap_array(storage_array)\n    <pyarrow.lib.ExtensionArray object at ...>\n    [\n      ...\n    ]\n\n    Or do the same with creating an ExtensionArray:\n\n    >>> pa.ExtensionArray.from_storage(uuid_type, storage_array)\n    <pyarrow.lib.ExtensionArray object at ...>\n    [\n      ...\n    ]\n\n    Unregister the extension type:\n\n    >>> pa.unregister_extension_type(\"my_package.uuid\")\n    \"\"\"\n\n    def __cinit__(self):\n        if type(self) is ExtensionType:\n            raise TypeError(\"Can only instantiate subclasses of \"\n                            \"ExtensionType\")\n\n    def __init__(self, DataType storage_type, extension_name):\n        \"\"\"\n        Initialize an extension type instance.\n\n        This should be called at the end of the subclass'\n        ``__init__`` method.\n        \"\"\"\n        cdef:\n            shared_ptr[CExtensionType] cpy_ext_type\n            c_string c_extension_name\n\n        c_extension_name = tobytes(extension_name)\n\n        assert storage_type is not None\n        check_status(CPyExtensionType.FromClass(\n            storage_type.sp_type, c_extension_name, type(self),\n            &cpy_ext_type))\n        self.init(<shared_ptr[CDataType]> cpy_ext_type)\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        BaseExtensionType.init(self, type)\n        self.cpy_ext_type = <const CPyExtensionType*> type.get()\n        # Store weakref and serialized version of self on C++ type instance\n        check_status(self.cpy_ext_type.SetInstance(self))\n\n    def __eq__(self, other):\n        # Default implementation to avoid infinite recursion through\n        # DataType.__eq__ -> ExtensionType::ExtensionEquals -> DataType.__eq__\n        if isinstance(other, ExtensionType):\n            return (type(self) == type(other) and\n                    self.extension_name == other.extension_name and\n                    self.storage_type == other.storage_type)\n        else:\n            return NotImplemented\n\n    def __repr__(self):\n        fmt = '{0.__class__.__name__}({1})'\n        return fmt.format(self, repr(self.storage_type))\n\n    def __arrow_ext_serialize__(self):\n        \"\"\"\n        Serialized representation of metadata to reconstruct the type object.\n\n        This method should return a bytes object, and those serialized bytes\n        are stored in the custom metadata of the Field holding an extension\n        type in an IPC message.\n        The bytes are passed to ``__arrow_ext_deserialize`` and should hold\n        sufficient information to reconstruct the data type instance.\n        \"\"\"\n        return NotImplementedError\n\n    @classmethod\n    def __arrow_ext_deserialize__(self, storage_type, serialized):\n        \"\"\"\n        Return an extension type instance from the storage type and serialized\n        metadata.\n\n        This method should return an instance of the ExtensionType subclass\n        that matches the passed storage type and serialized metadata (the\n        return value of ``__arrow_ext_serialize__``).\n        \"\"\"\n        return NotImplementedError\n\n    def __reduce__(self):\n        return self.__arrow_ext_deserialize__, (self.storage_type, self.__arrow_ext_serialize__())\n\n    def __arrow_ext_class__(self):\n        \"\"\"Return an extension array class to be used for building or\n        deserializing arrays with this extension type.\n\n        This method should return a subclass of the ExtensionArray class. By\n        default, if not specialized in the extension implementation, an\n        extension type array will be a built-in ExtensionArray instance.\n        \"\"\"\n        return ExtensionArray\n\n    def __arrow_ext_scalar_class__(self):\n        \"\"\"Return an extension scalar class for building scalars with this\n        extension type.\n\n        This method should return subclass of the ExtensionScalar class. By\n        default, if not specialized in the extension implementation, an\n        extension type scalar will be a built-in ExtensionScalar instance.\n        \"\"\"\n        return ExtensionScalar\n\n\ncdef class FixedShapeTensorType(BaseExtensionType):\n    \"\"\"\n    Concrete class for fixed shape tensor extension type.\n\n    Examples\n    --------\n    Create an instance of fixed shape tensor extension type:\n\n    >>> import pyarrow as pa\n    >>> pa.fixed_shape_tensor(pa.int32(), [2, 2])\n    FixedShapeTensorType(extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>)\n\n    Create an instance of fixed shape tensor extension type with\n    permutation:\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int8(), (2, 2, 3),\n    ...                                     permutation=[0, 2, 1])\n    >>> tensor_type.permutation\n    [0, 2, 1]\n    \"\"\"\n\n    cdef void init(self, const shared_ptr[CDataType]& type) except *:\n        BaseExtensionType.init(self, type)\n        self.tensor_ext_type = <const CFixedShapeTensorType*> type.get()\n\n    @property\n    def value_type(self):\n        \"\"\"\n        Data type of an individual tensor.\n        \"\"\"\n        return pyarrow_wrap_data_type(self.tensor_ext_type.value_type())\n\n    @property\n    def shape(self):\n        \"\"\"\n        Shape of the tensors.\n        \"\"\"\n        return self.tensor_ext_type.shape()\n\n    @property\n    def dim_names(self):\n        \"\"\"\n        Explicit names of the dimensions.\n        \"\"\"\n        list_of_bytes = self.tensor_ext_type.dim_names()\n        if len(list_of_bytes) != 0:\n            return [frombytes(x) for x in list_of_bytes]\n        else:\n            return None\n\n    @property\n    def permutation(self):\n        \"\"\"\n        Indices of the dimensions ordering.\n        \"\"\"\n        indices = self.tensor_ext_type.permutation()\n        if len(indices) != 0:\n            return indices\n        else:\n            return None\n\n    def __arrow_ext_serialize__(self):\n        \"\"\"\n        Serialized representation of metadata to reconstruct the type object.\n        \"\"\"\n        return self.tensor_ext_type.Serialize()\n\n    @classmethod\n    def __arrow_ext_deserialize__(self, storage_type, serialized):\n        \"\"\"\n        Return an FixedShapeTensor type instance from the storage type and serialized\n        metadata.\n        \"\"\"\n        return self.tensor_ext_type.Deserialize(storage_type, serialized)\n\n    def __arrow_ext_class__(self):\n        return FixedShapeTensorArray\n\n    def __reduce__(self):\n        return fixed_shape_tensor, (self.value_type, self.shape,\n                                    self.dim_names, self.permutation)\n\n\n_py_extension_type_auto_load = False\n\n\ncdef class PyExtensionType(ExtensionType):\n    \"\"\"\n    Concrete base class for Python-defined extension types based on pickle\n    for (de)serialization.\n\n    .. warning::\n       This class is deprecated and its deserialization is disabled by default.\n       :class:`ExtensionType` is recommended instead.\n\n    Parameters\n    ----------\n    storage_type : DataType\n        The storage type for which the extension is built.\n    \"\"\"\n\n    def __cinit__(self):\n        if type(self) is PyExtensionType:\n            raise TypeError(\"Can only instantiate subclasses of \"\n                            \"PyExtensionType\")\n\n    def __init__(self, DataType storage_type):\n        warnings.warn(\n            \"pyarrow.PyExtensionType is deprecated \"\n            \"and will refuse deserialization by default. \"\n            \"Instead, please derive from pyarrow.ExtensionType and implement \"\n            \"your own serialization mechanism.\",\n            FutureWarning)\n        ExtensionType.__init__(self, storage_type, \"arrow.py_extension_type\")\n\n    def __reduce__(self):\n        raise NotImplementedError(\"Please implement {0}.__reduce__\"\n                                  .format(type(self).__name__))\n\n    def __arrow_ext_serialize__(self):\n        return pickle.dumps(self)\n\n    @classmethod\n    def __arrow_ext_deserialize__(cls, storage_type, serialized):\n        if not _py_extension_type_auto_load:\n            warnings.warn(\n                \"pickle-based deserialization of pyarrow.PyExtensionType subclasses \"\n                \"is disabled by default; if you only ingest \"\n                \"trusted data files, you may re-enable this using \"\n                \"`pyarrow.PyExtensionType.set_auto_load(True)`.\\n\"\n                \"In the future, Python-defined extension subclasses should \"\n                \"derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) \"\n                \"and implement their own serialization mechanism.\\n\",\n                RuntimeWarning)\n            return UnknownExtensionType(storage_type, serialized)\n        try:\n            ty = pickle.loads(serialized)\n        except Exception:\n            # For some reason, it's impossible to deserialize the\n            # ExtensionType instance.  Perhaps the serialized data is\n            # corrupt, or more likely the type is being deserialized\n            # in an environment where the original Python class or module\n            # is not available.  Fall back on a generic BaseExtensionType.\n            return UnknownExtensionType(storage_type, serialized)\n\n        if ty.storage_type != storage_type:\n            raise TypeError(\"Expected storage type {0} but got {1}\"\n                            .format(ty.storage_type, storage_type))\n        return ty\n\n    # XXX Cython marks extension types as immutable, so cannot expose this\n    # as a writable class attribute.\n    @classmethod\n    def set_auto_load(cls, value):\n        \"\"\"\n        Enable or disable auto-loading of serialized PyExtensionType instances.\n\n        Parameters\n        ----------\n        value : bool\n            Whether to enable auto-loading.\n        \"\"\"\n        global _py_extension_type_auto_load\n        assert isinstance(value, bool)\n        _py_extension_type_auto_load = value\n\n\ncdef class UnknownExtensionType(PyExtensionType):\n    \"\"\"\n    A concrete class for Python-defined extension types that refer to\n    an unknown Python implementation.\n\n    Parameters\n    ----------\n    storage_type : DataType\n        The storage type for which the extension is built.\n    serialized : bytes\n        The serialised output.\n    \"\"\"\n\n    cdef:\n        bytes serialized\n\n    def __init__(self, DataType storage_type, serialized):\n        self.serialized = serialized\n        PyExtensionType.__init__(self, storage_type)\n\n    def __arrow_ext_serialize__(self):\n        return self.serialized\n\n\n_python_extension_types_registry = []\n\n\ndef register_extension_type(ext_type):\n    \"\"\"\n    Register a Python extension type.\n\n    Registration is based on the extension name (so different registered types\n    need unique extension names). Registration needs an extension type\n    instance, but then works for any instance of the same subclass regardless\n    of parametrization of the type.\n\n    Parameters\n    ----------\n    ext_type : BaseExtensionType instance\n        The ExtensionType subclass to register.\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing ExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.ExtensionType):\n    ...    def __init__(self):\n    ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n    ...    def __arrow_ext_serialize__(self):\n    ...       # since we don't have a parameterized type, we don't need extra\n    ...       # metadata to be deserialized\n    ...       return b''\n    ...    @classmethod\n    ...    def __arrow_ext_deserialize__(self, storage_type, serialized):\n    ...       # return an instance of this subclass given the serialized\n    ...       # metadata.\n    ...       return UuidType()\n    ...\n\n    Register the extension type:\n\n    >>> pa.register_extension_type(UuidType())\n\n    Unregister the extension type:\n\n    >>> pa.unregister_extension_type(\"my_package.uuid\")\n    \"\"\"\n    cdef:\n        DataType _type = ensure_type(ext_type, allow_none=False)\n\n    if not isinstance(_type, BaseExtensionType):\n        raise TypeError(\"Only extension types can be registered\")\n\n    # register on the C++ side\n    check_status(\n        RegisterPyExtensionType(<shared_ptr[CDataType]> _type.sp_type))\n\n    # register on the python side\n    _python_extension_types_registry.append(_type)\n\n\ndef unregister_extension_type(type_name):\n    \"\"\"\n    Unregister a Python extension type.\n\n    Parameters\n    ----------\n    type_name : str\n        The name of the ExtensionType subclass to unregister.\n\n    Examples\n    --------\n    Define a UuidType extension type subclassing ExtensionType:\n\n    >>> import pyarrow as pa\n    >>> class UuidType(pa.ExtensionType):\n    ...    def __init__(self):\n    ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")\n    ...    def __arrow_ext_serialize__(self):\n    ...       # since we don't have a parameterized type, we don't need extra\n    ...       # metadata to be deserialized\n    ...       return b''\n    ...    @classmethod\n    ...    def __arrow_ext_deserialize__(self, storage_type, serialized):\n    ...       # return an instance of this subclass given the serialized\n    ...       # metadata.\n    ...       return UuidType()\n    ...\n\n    Register the extension type:\n\n    >>> pa.register_extension_type(UuidType())\n\n    Unregister the extension type:\n\n    >>> pa.unregister_extension_type(\"my_package.uuid\")\n    \"\"\"\n    cdef:\n        c_string c_type_name = tobytes(type_name)\n    check_status(UnregisterPyExtensionType(c_type_name))\n\n\ncdef class KeyValueMetadata(_Metadata, Mapping):\n    \"\"\"\n    KeyValueMetadata\n\n    Parameters\n    ----------\n    __arg0__ : dict\n        A dict of the key-value metadata\n    **kwargs : optional\n        additional key-value metadata\n    \"\"\"\n\n    def __init__(self, __arg0__=None, **kwargs):\n        cdef:\n            vector[c_string] keys, values\n            shared_ptr[const CKeyValueMetadata] result\n\n        items = []\n        if __arg0__ is not None:\n            other = (__arg0__.items() if isinstance(__arg0__, Mapping)\n                     else __arg0__)\n            items.extend((tobytes(k), v) for k, v in other)\n\n        prior_keys = {k for k, v in items}\n        for k, v in kwargs.items():\n            k = tobytes(k)\n            if k in prior_keys:\n                raise KeyError(\"Duplicate key {}, \"\n                               \"use pass all items as list of tuples if you \"\n                               \"intend to have duplicate keys\")\n            items.append((k, v))\n\n        keys.reserve(len(items))\n        for key, value in items:\n            keys.push_back(tobytes(key))\n            values.push_back(tobytes(value))\n        result.reset(new CKeyValueMetadata(move(keys), move(values)))\n        self.init(result)\n\n    cdef void init(self, const shared_ptr[const CKeyValueMetadata]& wrapped):\n        self.wrapped = wrapped\n        self.metadata = wrapped.get()\n\n    @staticmethod\n    cdef wrap(const shared_ptr[const CKeyValueMetadata]& sp):\n        cdef KeyValueMetadata self = KeyValueMetadata.__new__(KeyValueMetadata)\n        self.init(sp)\n        return self\n\n    cdef inline shared_ptr[const CKeyValueMetadata] unwrap(self) nogil:\n        return self.wrapped\n\n    def equals(self, KeyValueMetadata other):\n        \"\"\"\n        Parameters\n        ----------\n        other : pyarrow.KeyValueMetadata\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return self.metadata.Equals(deref(other.wrapped))\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        return frombytes(self.metadata.ToString(), safe=True)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            pass\n\n        if isinstance(other, Mapping):\n            try:\n                other = KeyValueMetadata(other)\n                return self.equals(other)\n            except TypeError:\n                pass\n\n        return NotImplemented\n\n    def __len__(self):\n        return self.metadata.size()\n\n    def __contains__(self, key):\n        return self.metadata.Contains(tobytes(key))\n\n    def __getitem__(self, key):\n        return GetResultValue(self.metadata.Get(tobytes(key)))\n\n    def __iter__(self):\n        return self.keys()\n\n    def __reduce__(self):\n        return KeyValueMetadata, (list(self.items()),)\n\n    def key(self, i):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        byte\n        \"\"\"\n        return self.metadata.key(i)\n\n    def value(self, i):\n        \"\"\"\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        byte\n        \"\"\"\n        return self.metadata.value(i)\n\n    def keys(self):\n        for i in range(self.metadata.size()):\n            yield self.metadata.key(i)\n\n    def values(self):\n        for i in range(self.metadata.size()):\n            yield self.metadata.value(i)\n\n    def items(self):\n        for i in range(self.metadata.size()):\n            yield (self.metadata.key(i), self.metadata.value(i))\n\n    def get_all(self, key):\n        \"\"\"\n        Parameters\n        ----------\n        key : str\n\n        Returns\n        -------\n        list[byte]\n        \"\"\"\n        key = tobytes(key)\n        return [v for k, v in self.items() if k == key]\n\n    def to_dict(self):\n        \"\"\"\n        Convert KeyValueMetadata to dict. If a key occurs twice, the value for\n        the first one is returned\n        \"\"\"\n        cdef object key  # to force coercion to Python\n        result = ordered_dict()\n        for i in range(self.metadata.size()):\n            key = self.metadata.key(i)\n            if key not in result:\n                result[key] = self.metadata.value(i)\n        return result\n\n\ncpdef KeyValueMetadata ensure_metadata(object meta, c_bool allow_none=False):\n    if allow_none and meta is None:\n        return None\n    elif isinstance(meta, KeyValueMetadata):\n        return meta\n    else:\n        return KeyValueMetadata(meta)\n\n\ncdef class Field(_Weakrefable):\n    \"\"\"\n    A named field, with a data type, nullability, and optional metadata.\n\n    Notes\n    -----\n    Do not use this class's constructor directly; use pyarrow.field\n\n    Examples\n    --------\n    Create an instance of pyarrow.Field:\n\n    >>> import pyarrow as pa\n    >>> pa.field('key', pa.int32())\n    pyarrow.Field<key: int32>\n    >>> pa.field('key', pa.int32(), nullable=False)\n    pyarrow.Field<key: int32 not null>\n    >>> field = pa.field('key', pa.int32(),\n    ...                  metadata={\"key\": \"Something important\"})\n    >>> field\n    pyarrow.Field<key: int32>\n    >>> field.metadata\n    {b'key': b'Something important'}\n\n    Use the field to create a struct type:\n\n    >>> pa.struct([field])\n    StructType(struct<key: int32>)\n    \"\"\"\n\n    def __cinit__(self):\n        pass\n\n    def __init__(self):\n        raise TypeError(\"Do not call Field's constructor directly, use \"\n                        \"`pyarrow.field` instead.\")\n\n    cdef void init(self, const shared_ptr[CField]& field):\n        self.sp_field = field\n        self.field = field.get()\n        self.type = pyarrow_wrap_data_type(field.get().type())\n\n    def equals(self, Field other, bint check_metadata=False):\n        \"\"\"\n        Test if this field is equal to the other\n\n        Parameters\n        ----------\n        other : pyarrow.Field\n        check_metadata : bool, default False\n            Whether Field metadata equality should be checked as well.\n\n        Returns\n        -------\n        is_equal : bool\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> f1 = pa.field('key', pa.int32())\n        >>> f2 = pa.field('key', pa.int32(), nullable=False)\n        >>> f1.equals(f2)\n        False\n        >>> f1.equals(f1)\n        True\n        \"\"\"\n        return self.field.Equals(deref(other.field), check_metadata)\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            return NotImplemented\n\n    def __reduce__(self):\n        return field, (self.name, self.type, self.nullable, self.metadata)\n\n    def __str__(self):\n        return 'pyarrow.Field<{0}>'.format(\n            frombytes(self.field.ToString(), safe=True))\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __hash__(self):\n        return hash((self.field.name(), self.type, self.field.nullable()))\n\n    @property\n    def nullable(self):\n        \"\"\"\n        The field nullability.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> f1 = pa.field('key', pa.int32())\n        >>> f2 = pa.field('key', pa.int32(), nullable=False)\n        >>> f1.nullable\n        True\n        >>> f2.nullable\n        False\n        \"\"\"\n        return self.field.nullable()\n\n    @property\n    def name(self):\n        \"\"\"\n        The field name.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field.name\n        'key'\n        \"\"\"\n        return frombytes(self.field.name())\n\n    @property\n    def metadata(self):\n        \"\"\"\n        The field metadata.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32(),\n        ...                  metadata={\"key\": \"Something important\"})\n        >>> field.metadata\n        {b'key': b'Something important'}\n        \"\"\"\n        wrapped = pyarrow_wrap_metadata(self.field.metadata())\n        if wrapped is not None:\n            return wrapped.to_dict()\n        else:\n            return wrapped\n\n    def with_metadata(self, metadata):\n        \"\"\"\n        Add metadata as dict of string keys and values to Field\n\n        Parameters\n        ----------\n        metadata : dict\n            Keys and values must be string-like / coercible to bytes\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n\n        Create new field by adding metadata to existing one:\n\n        >>> field_new = field.with_metadata({\"key\": \"Something important\"})\n        >>> field_new\n        pyarrow.Field<key: int32>\n        >>> field_new.metadata\n        {b'key': b'Something important'}\n        \"\"\"\n        cdef shared_ptr[CField] c_field\n\n        meta = ensure_metadata(metadata, allow_none=False)\n        with nogil:\n            c_field = self.field.WithMetadata(meta.unwrap())\n\n        return pyarrow_wrap_field(c_field)\n\n    def remove_metadata(self):\n        \"\"\"\n        Create new field without metadata, if any\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32(),\n        ...                  metadata={\"key\": \"Something important\"})\n        >>> field.metadata\n        {b'key': b'Something important'}\n\n        Create new field by removing the metadata from the existing one:\n\n        >>> field_new = field.remove_metadata()\n        >>> field_new.metadata\n        \"\"\"\n        cdef shared_ptr[CField] new_field\n        with nogil:\n            new_field = self.field.RemoveMetadata()\n        return pyarrow_wrap_field(new_field)\n\n    def with_type(self, DataType new_type):\n        \"\"\"\n        A copy of this field with the replaced type\n\n        Parameters\n        ----------\n        new_type : pyarrow.DataType\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field\n        pyarrow.Field<key: int32>\n\n        Create new field by replacing type of an existing one:\n\n        >>> field_new = field.with_type(pa.int64())\n        >>> field_new\n        pyarrow.Field<key: int64>\n        \"\"\"\n        cdef:\n            shared_ptr[CField] c_field\n            shared_ptr[CDataType] c_datatype\n\n        c_datatype = pyarrow_unwrap_data_type(new_type)\n        with nogil:\n            c_field = self.field.WithType(c_datatype)\n\n        return pyarrow_wrap_field(c_field)\n\n    def with_name(self, name):\n        \"\"\"\n        A copy of this field with the replaced name\n\n        Parameters\n        ----------\n        name : str\n\n        Returns\n        -------\n        field : pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field\n        pyarrow.Field<key: int32>\n\n        Create new field by replacing the name of an existing one:\n\n        >>> field_new = field.with_name('lock')\n        >>> field_new\n        pyarrow.Field<lock: int32>\n        \"\"\"\n        cdef:\n            shared_ptr[CField] c_field\n\n        c_field = self.field.WithName(tobytes(name))\n\n        return pyarrow_wrap_field(c_field)\n\n    def with_nullable(self, nullable):\n        \"\"\"\n        A copy of this field with the replaced nullability\n\n        Parameters\n        ----------\n        nullable : bool\n\n        Returns\n        -------\n        field: pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> field = pa.field('key', pa.int32())\n        >>> field\n        pyarrow.Field<key: int32>\n        >>> field.nullable\n        True\n\n        Create new field by replacing the nullability of an existing one:\n\n        >>> field_new = field.with_nullable(False)\n        >>> field_new\n        pyarrow.Field<key: int32 not null>\n        >>> field_new.nullable\n        False\n        \"\"\"\n        cdef:\n            shared_ptr[CField] field\n            c_bool c_nullable\n\n        c_nullable = bool(nullable)\n        with nogil:\n            c_field = self.field.WithNullable(c_nullable)\n\n        return pyarrow_wrap_field(c_field)\n\n    def flatten(self):\n        \"\"\"\n        Flatten this field.  If a struct field, individual child fields\n        will be returned with their names prefixed by the parent's name.\n\n        Returns\n        -------\n        fields : List[pyarrow.Field]\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> f1 = pa.field('bar', pa.float64(), nullable=False)\n        >>> f2 = pa.field('foo', pa.int32()).with_metadata({\"key\": \"Something important\"})\n        >>> ff = pa.field('ff', pa.struct([f1, f2]), nullable=False)\n\n        Flatten a struct field:\n\n        >>> ff\n        pyarrow.Field<ff: struct<bar: double not null, foo: int32> not null>\n        >>> ff.flatten()\n        [pyarrow.Field<ff.bar: double not null>, pyarrow.Field<ff.foo: int32>]\n        \"\"\"\n        cdef vector[shared_ptr[CField]] flattened\n        with nogil:\n            flattened = self.field.Flatten()\n        return [pyarrow_wrap_field(f) for f in flattened]\n\n    def _export_to_c(self, out_ptr):\n        \"\"\"\n        Export to a C ArrowSchema struct, given its pointer.\n\n        Be careful: if you don't pass the ArrowSchema struct to a consumer,\n        its memory will leak.  This is a low-level function intended for\n        expert users.\n        \"\"\"\n        check_status(ExportField(deref(self.field),\n                                 <ArrowSchema*> _as_c_pointer(out_ptr)))\n\n    @staticmethod\n    def _import_from_c(in_ptr):\n        \"\"\"\n        Import Field from a C ArrowSchema struct, given its pointer.\n\n        This is a low-level function intended for expert users.\n        \"\"\"\n        cdef void* c_ptr = _as_c_pointer(in_ptr)\n        with nogil:\n            result = GetResultValue(ImportField(<ArrowSchema*> c_ptr))\n        return pyarrow_wrap_field(result)\n\n    def __arrow_c_schema__(self):\n        \"\"\"\n        Export to a ArrowSchema PyCapsule\n\n        Unlike _export_to_c, this will not leak memory if the capsule is not used.\n        \"\"\"\n        cdef ArrowSchema* c_schema\n        capsule = alloc_c_schema(&c_schema)\n\n        with nogil:\n            check_status(ExportField(deref(self.field), c_schema))\n\n        return capsule\n\n    @staticmethod\n    def _import_from_c_capsule(schema):\n        \"\"\"\n        Import a Field from a ArrowSchema PyCapsule\n\n        Parameters\n        ----------\n        schema : PyCapsule\n            A valid PyCapsule with name 'arrow_schema' containing an\n            ArrowSchema pointer.\n        \"\"\"\n        cdef:\n            ArrowSchema* c_schema\n            shared_ptr[CField] c_field\n\n        if not PyCapsule_IsValid(schema, 'arrow_schema'):\n            raise ValueError(\n                \"Not an ArrowSchema object\"\n            )\n        c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')\n\n        with nogil:\n            c_field = GetResultValue(ImportField(c_schema))\n\n        return pyarrow_wrap_field(c_field)\n\n\ncdef class Schema(_Weakrefable):\n    \"\"\"\n    A named collection of types a.k.a schema. A schema defines the\n    column names and types in a record batch or table data structure.\n    They also contain metadata about the columns. For example, schemas\n    converted from Pandas contain metadata about their original Pandas\n    types so they can be converted back to the same types.\n\n    Warnings\n    --------\n    Do not call this class's constructor directly. Instead use\n    :func:`pyarrow.schema` factory function which makes a new Arrow\n    Schema object.\n\n    Examples\n    --------\n    Create a new Arrow Schema object:\n\n    >>> import pyarrow as pa\n    >>> pa.schema([\n    ...     ('some_int', pa.int32()),\n    ...     ('some_string', pa.string())\n    ... ])\n    some_int: int32\n    some_string: string\n\n    Create Arrow Schema with metadata:\n\n    >>> pa.schema([\n    ...     pa.field('n_legs', pa.int64()),\n    ...     pa.field('animals', pa.string())],\n    ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n    n_legs: int64\n    animals: string\n    -- schema metadata --\n    n_legs: 'Number of legs per animal'\n    \"\"\"\n\n    def __cinit__(self):\n        pass\n\n    def __init__(self):\n        raise TypeError(\"Do not call Schema's constructor directly, use \"\n                        \"`pyarrow.schema` instead.\")\n\n    def __len__(self):\n        return self.schema.num_fields()\n\n    def __getitem__(self, key):\n        # access by integer index\n        return self._field(key)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    cdef void init(self, const vector[shared_ptr[CField]]& fields):\n        self.schema = new CSchema(fields)\n        self.sp_schema.reset(self.schema)\n\n    cdef void init_schema(self, const shared_ptr[CSchema]& schema):\n        self.schema = schema.get()\n        self.sp_schema = schema\n\n    def __reduce__(self):\n        return schema, (list(self), self.metadata)\n\n    def __hash__(self):\n        return hash((tuple(self), self.metadata))\n\n    def __sizeof__(self):\n        size = 0\n        if self.metadata:\n            for key, value in self.metadata.items():\n                size += sys.getsizeof(key)\n                size += sys.getsizeof(value)\n\n        return size + super(Schema, self).__sizeof__()\n\n    @property\n    def pandas_metadata(self):\n        \"\"\"\n        Return deserialized-from-JSON pandas metadata field (if it exists)\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> import pandas as pd\n        >>> df = pd.DataFrame({'n_legs': [2, 4, 5, 100],\n        ...                    'animals': [\"Flamingo\", \"Horse\", \"Brittle stars\", \"Centipede\"]})\n        >>> schema = pa.Table.from_pandas(df).schema\n\n        Select pandas metadata field from Arrow Schema:\n\n        >>> schema.pandas_metadata\n        {'index_columns': [{'kind': 'range', 'name': None, 'start': 0, 'stop': 4, 'step': 1}], ...\n        \"\"\"\n        metadata = self.metadata\n        key = b'pandas'\n        if metadata is None or key not in metadata:\n            return None\n\n        import json\n        return json.loads(metadata[key].decode('utf8'))\n\n    @property\n    def names(self):\n        \"\"\"\n        The schema's field names.\n\n        Returns\n        -------\n        list of str\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Get the names of the schema's fields:\n\n        >>> schema.names\n        ['n_legs', 'animals']\n        \"\"\"\n        cdef int i\n        result = []\n        for i in range(self.schema.num_fields()):\n            name = frombytes(self.schema.field(i).get().name())\n            result.append(name)\n        return result\n\n    @property\n    def types(self):\n        \"\"\"\n        The schema's field types.\n\n        Returns\n        -------\n        list of DataType\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Get the types of the schema's fields:\n\n        >>> schema.types\n        [DataType(int64), DataType(string)]\n        \"\"\"\n        return [field.type for field in self]\n\n    @property\n    def metadata(self):\n        \"\"\"\n        The schema's metadata.\n\n        Returns\n        -------\n        metadata: dict\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n\n        Get the metadata of the schema's fields:\n\n        >>> schema.metadata\n        {b'n_legs': b'Number of legs per animal'}\n        \"\"\"\n        wrapped = pyarrow_wrap_metadata(self.schema.metadata())\n        if wrapped is not None:\n            return wrapped.to_dict()\n        else:\n            return wrapped\n\n    def __eq__(self, other):\n        try:\n            return self.equals(other)\n        except TypeError:\n            return NotImplemented\n\n    def empty_table(self):\n        \"\"\"\n        Provide an empty table according to the schema.\n\n        Returns\n        -------\n        table: pyarrow.Table\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Create an empty table with schema's fields:\n\n        >>> schema.empty_table()\n        pyarrow.Table\n        n_legs: int64\n        animals: string\n        ----\n        n_legs: [[]]\n        animals: [[]]\n        \"\"\"\n        arrays = [_empty_array(field.type) for field in self]\n        return Table.from_arrays(arrays, schema=self)\n\n    def equals(self, Schema other not None, bint check_metadata=False):\n        \"\"\"\n        Test if this schema is equal to the other\n\n        Parameters\n        ----------\n        other :  pyarrow.Schema\n        check_metadata : bool, default False\n            Key/value metadata must be equal too\n\n        Returns\n        -------\n        is_equal : bool\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema1 = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n        >>> schema2 = pa.schema([\n        ...     ('some_int', pa.int32()),\n        ...     ('some_string', pa.string())\n        ... ])\n\n        Test two equal schemas:\n\n        >>> schema1.equals(schema1)\n        True\n\n        Test two unequal schemas:\n\n        >>> schema1.equals(schema2)\n        False\n        \"\"\"\n        return self.sp_schema.get().Equals(deref(other.schema),\n                                           check_metadata)\n\n    @classmethod\n    def from_pandas(cls, df, preserve_index=None):\n        \"\"\"\n        Returns implied schema from dataframe\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n        preserve_index : bool, default True\n            Whether to store the index as an additional column (or columns, for\n            MultiIndex) in the resulting `Table`.\n            The default of None will store the index as a column, except for\n            RangeIndex which is stored as metadata only. Use\n            ``preserve_index=True`` to force it to be stored as a column.\n\n        Returns\n        -------\n        pyarrow.Schema\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> import pyarrow as pa\n        >>> df = pd.DataFrame({\n        ...     'int': [1, 2],\n        ...     'str': ['a', 'b']\n        ... })\n\n        Create an Arrow Schema from the schema of a pandas dataframe:\n\n        >>> pa.Schema.from_pandas(df)\n        int: int64\n        str: string\n        -- schema metadata --\n        pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, ...\n        \"\"\"\n        from pyarrow.pandas_compat import dataframe_to_types\n        names, types, metadata = dataframe_to_types(\n            df,\n            preserve_index=preserve_index\n        )\n        fields = []\n        for name, type_ in zip(names, types):\n            fields.append(field(name, type_))\n        return schema(fields, metadata)\n\n    def field(self, i):\n        \"\"\"\n        Select a field by its column name or numeric index.\n\n        Parameters\n        ----------\n        i : int or string\n\n        Returns\n        -------\n        pyarrow.Field\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Select the second field:\n\n        >>> schema.field(1)\n        pyarrow.Field<animals: string>\n\n        Select the field of the column named 'n_legs':\n\n        >>> schema.field('n_legs')\n        pyarrow.Field<n_legs: int64>\n        \"\"\"\n        if isinstance(i, (bytes, str)):\n            field_index = self.get_field_index(i)\n            if field_index < 0:\n                raise KeyError(\"Column {} does not exist in schema\".format(i))\n            else:\n                return self._field(field_index)\n        elif isinstance(i, int):\n            return self._field(i)\n        else:\n            raise TypeError(\"Index must either be string or integer\")\n\n    def _field(self, int i):\n        \"\"\"\n        Select a field by its numeric index.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        pyarrow.Field\n        \"\"\"\n        cdef int index = <int> _normalize_index(i, self.schema.num_fields())\n        return pyarrow_wrap_field(self.schema.field(index))\n\n    def field_by_name(self, name):\n        \"\"\"\n        DEPRECATED\n\n        Parameters\n        ----------\n        name : str\n\n        Returns\n        -------\n        field: pyarrow.Field\n        \"\"\"\n        cdef:\n            vector[shared_ptr[CField]] results\n\n        warnings.warn(\n            \"The 'field_by_name' method is deprecated, use 'field' instead\",\n            FutureWarning, stacklevel=2)\n\n        results = self.schema.GetAllFieldsByName(tobytes(name))\n        if results.size() == 0:\n            return None\n        elif results.size() > 1:\n            warnings.warn(\"Schema field name corresponds to more \"\n                          \"than one field\", UserWarning)\n            return None\n        else:\n            return pyarrow_wrap_field(results[0])\n\n    def get_field_index(self, name):\n        \"\"\"\n        Return index of the unique field with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        index : int\n            The index of the field with the given name; -1 if the\n            name isn't found or there are several fields with the given\n            name.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Get the index of the field named 'animals':\n\n        >>> schema.get_field_index(\"animals\")\n        1\n\n        Index in case of several fields with the given name:\n\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string()),\n        ...     pa.field('animals', pa.bool_())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n        >>> schema.get_field_index(\"animals\")\n        -1\n        \"\"\"\n        return self.schema.GetFieldIndex(tobytes(name))\n\n    def get_all_field_indices(self, name):\n        \"\"\"\n        Return sorted list of indices for the fields with the given name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the field to look up.\n\n        Returns\n        -------\n        indices : List[int]\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string()),\n        ...     pa.field('animals', pa.bool_())])\n\n        Get the indexes of the fields named 'animals':\n\n        >>> schema.get_all_field_indices(\"animals\")\n        [1, 2]\n        \"\"\"\n        return self.schema.GetAllFieldIndices(tobytes(name))\n\n    def append(self, Field field):\n        \"\"\"\n        Append a field at the end of the schema.\n\n        In contrast to Python's ``list.append()`` it does return a new\n        object, leaving the original Schema unmodified.\n\n        Parameters\n        ----------\n        field : Field\n\n        Returns\n        -------\n        schema: Schema\n            New object with appended field.\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Append a field 'extra' at the end of the schema:\n\n        >>> schema_new = schema.append(pa.field('extra', pa.bool_()))\n        >>> schema_new\n        n_legs: int64\n        animals: string\n        extra: bool\n\n        Original schema is unmodified:\n\n        >>> schema\n        n_legs: int64\n        animals: string\n        \"\"\"\n        return self.insert(self.schema.num_fields(), field)\n\n    def insert(self, int i, Field field):\n        \"\"\"\n        Add a field at position i to the schema.\n\n        Parameters\n        ----------\n        i : int\n        field : Field\n\n        Returns\n        -------\n        schema: Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Insert a new field on the second position:\n\n        >>> schema.insert(1, pa.field('extra', pa.bool_()))\n        n_legs: int64\n        extra: bool\n        animals: string\n        \"\"\"\n        cdef:\n            shared_ptr[CSchema] new_schema\n            shared_ptr[CField] c_field\n\n        c_field = field.sp_field\n\n        with nogil:\n            new_schema = GetResultValue(self.schema.AddField(i, c_field))\n\n        return pyarrow_wrap_schema(new_schema)\n\n    def remove(self, int i):\n        \"\"\"\n        Remove the field at index i from the schema.\n\n        Parameters\n        ----------\n        i : int\n\n        Returns\n        -------\n        schema: Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Remove the second field of the schema:\n\n        >>> schema.remove(1)\n        n_legs: int64\n        \"\"\"\n        cdef shared_ptr[CSchema] new_schema\n\n        with nogil:\n            new_schema = GetResultValue(self.schema.RemoveField(i))\n\n        return pyarrow_wrap_schema(new_schema)\n\n    def set(self, int i, Field field):\n        \"\"\"\n        Replace a field at position i in the schema.\n\n        Parameters\n        ----------\n        i : int\n        field : Field\n\n        Returns\n        -------\n        schema: Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Replace the second field of the schema with a new field 'extra':\n\n        >>> schema.set(1, pa.field('replaced', pa.bool_()))\n        n_legs: int64\n        replaced: bool\n        \"\"\"\n        cdef:\n            shared_ptr[CSchema] new_schema\n            shared_ptr[CField] c_field\n\n        c_field = field.sp_field\n\n        with nogil:\n            new_schema = GetResultValue(self.schema.SetField(i, c_field))\n\n        return pyarrow_wrap_schema(new_schema)\n\n    def add_metadata(self, metadata):\n        \"\"\"\n        DEPRECATED\n\n        Parameters\n        ----------\n        metadata : dict\n            Keys and values must be string-like / coercible to bytes\n        \"\"\"\n        warnings.warn(\"The 'add_metadata' method is deprecated, use \"\n                      \"'with_metadata' instead\", FutureWarning, stacklevel=2)\n        return self.with_metadata(metadata)\n\n    def with_metadata(self, metadata):\n        \"\"\"\n        Add metadata as dict of string keys and values to Schema\n\n        Parameters\n        ----------\n        metadata : dict\n            Keys and values must be string-like / coercible to bytes\n\n        Returns\n        -------\n        schema : pyarrow.Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Add metadata to existing schema field:\n\n        >>> schema.with_metadata({\"n_legs\": \"Number of legs per animal\"})\n        n_legs: int64\n        animals: string\n        -- schema metadata --\n        n_legs: 'Number of legs per animal'\n        \"\"\"\n        cdef shared_ptr[CSchema] c_schema\n\n        meta = ensure_metadata(metadata, allow_none=False)\n        with nogil:\n            c_schema = self.schema.WithMetadata(meta.unwrap())\n\n        return pyarrow_wrap_schema(c_schema)\n\n    def serialize(self, memory_pool=None):\n        \"\"\"\n        Write Schema to Buffer as encapsulated IPC message\n\n        Parameters\n        ----------\n        memory_pool : MemoryPool, default None\n            Uses default memory pool if not specified\n\n        Returns\n        -------\n        serialized : Buffer\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())])\n\n        Write schema to Buffer:\n\n        >>> schema.serialize()\n        <pyarrow.Buffer address=0x... size=... is_cpu=True is_mutable=True>\n        \"\"\"\n        cdef:\n            shared_ptr[CBuffer] buffer\n            CMemoryPool* pool = maybe_unbox_memory_pool(memory_pool)\n\n        with nogil:\n            buffer = GetResultValue(SerializeSchema(deref(self.schema),\n                                                    pool))\n        return pyarrow_wrap_buffer(buffer)\n\n    def remove_metadata(self):\n        \"\"\"\n        Create new schema without metadata, if any\n\n        Returns\n        -------\n        schema : pyarrow.Schema\n\n        Examples\n        --------\n        >>> import pyarrow as pa\n        >>> schema = pa.schema([\n        ...     pa.field('n_legs', pa.int64()),\n        ...     pa.field('animals', pa.string())],\n        ...     metadata={\"n_legs\": \"Number of legs per animal\"})\n        >>> schema\n        n_legs: int64\n        animals: string\n        -- schema metadata --\n        n_legs: 'Number of legs per animal'\n\n        Create a new schema with removing the metadata from the original:\n\n        >>> schema.remove_metadata()\n        n_legs: int64\n        animals: string\n        \"\"\"\n        cdef shared_ptr[CSchema] new_schema\n        with nogil:\n            new_schema = self.schema.RemoveMetadata()\n        return pyarrow_wrap_schema(new_schema)\n\n    def to_string(self, truncate_metadata=True, show_field_metadata=True,\n                  show_schema_metadata=True):\n        \"\"\"\n        Return human-readable representation of Schema\n\n        Parameters\n        ----------\n        truncate_metadata : boolean, default True\n            Limit metadata key/value display to a single line of ~80 characters\n            or less\n        show_field_metadata : boolean, default True\n            Display Field-level KeyValueMetadata\n        show_schema_metadata : boolean, default True\n            Display Schema-level KeyValueMetadata\n\n        Returns\n        -------\n        str : the formatted output\n        \"\"\"\n        cdef:\n            c_string result\n            PrettyPrintOptions options = PrettyPrintOptions.Defaults()\n\n        options.indent = 0\n        options.truncate_metadata = truncate_metadata\n        options.show_field_metadata = show_field_metadata\n        options.show_schema_metadata = show_schema_metadata\n\n        with nogil:\n            check_status(\n                PrettyPrint(\n                    deref(self.schema),\n                    options,\n                    &result\n                )\n            )\n\n        return frombytes(result, safe=True)\n\n    def _export_to_c(self, out_ptr):\n        \"\"\"\n        Export to a C ArrowSchema struct, given its pointer.\n\n        Be careful: if you don't pass the ArrowSchema struct to a consumer,\n        its memory will leak.  This is a low-level function intended for\n        expert users.\n        \"\"\"\n        check_status(ExportSchema(deref(self.schema),\n                                  <ArrowSchema*> _as_c_pointer(out_ptr)))\n\n    @staticmethod\n    def _import_from_c(in_ptr):\n        \"\"\"\n        Import Schema from a C ArrowSchema struct, given its pointer.\n\n        This is a low-level function intended for expert users.\n        \"\"\"\n        cdef void* c_ptr = _as_c_pointer(in_ptr)\n        with nogil:\n            result = GetResultValue(ImportSchema(<ArrowSchema*> c_ptr))\n        return pyarrow_wrap_schema(result)\n\n    def __str__(self):\n        return self.to_string()\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __arrow_c_schema__(self):\n        \"\"\"\n        Export to a ArrowSchema PyCapsule\n\n        Unlike _export_to_c, this will not leak memory if the capsule is not used.\n        \"\"\"\n        cdef ArrowSchema* c_schema\n        capsule = alloc_c_schema(&c_schema)\n\n        with nogil:\n            check_status(ExportSchema(deref(self.schema), c_schema))\n\n        return capsule\n\n    @staticmethod\n    def _import_from_c_capsule(schema):\n        \"\"\"\n        Import a Schema from a ArrowSchema PyCapsule\n\n        Parameters\n        ----------\n        schema : PyCapsule\n            A valid PyCapsule with name 'arrow_schema' containing an\n            ArrowSchema pointer.\n        \"\"\"\n        cdef:\n            ArrowSchema* c_schema\n\n        if not PyCapsule_IsValid(schema, 'arrow_schema'):\n            raise ValueError(\n                \"Not an ArrowSchema object\"\n            )\n        c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')\n\n        with nogil:\n            result = GetResultValue(ImportSchema(c_schema))\n\n        return pyarrow_wrap_schema(result)\n\n\ndef unify_schemas(schemas, *, promote_options=\"default\"):\n    \"\"\"\n    Unify schemas by merging fields by name.\n\n    The resulting schema will contain the union of fields from all schemas.\n    Fields with the same name will be merged. Note that two fields with\n    different types will fail merging by default.\n\n    - The unified field will inherit the metadata from the schema where\n        that field is first defined.\n    - The first N fields in the schema will be ordered the same as the\n        N fields in the first schema.\n\n    The resulting schema will inherit its metadata from the first input\n    schema.\n\n    Parameters\n    ----------\n    schemas : list of Schema\n        Schemas to merge into a single one.\n    promote_options : str, default default\n        Accepts strings \"default\" and \"permissive\".\n        Default: null and only null can be unified with another type.\n        Permissive: types are promoted to the greater common denominator.\n\n    Returns\n    -------\n    Schema\n\n    Raises\n    ------\n    ArrowInvalid :\n        If any input schema contains fields with duplicate names.\n        If Fields of the same name are not mergeable.\n    \"\"\"\n    cdef:\n        Schema schema\n        CField.CMergeOptions c_options\n        vector[shared_ptr[CSchema]] c_schemas\n    for schema in schemas:\n        if not isinstance(schema, Schema):\n            raise TypeError(\"Expected Schema, got {}\".format(type(schema)))\n        c_schemas.push_back(pyarrow_unwrap_schema(schema))\n\n    if promote_options == \"default\":\n        c_options = CField.CMergeOptions.Defaults()\n    elif promote_options == \"permissive\":\n        c_options = CField.CMergeOptions.Permissive()\n    else:\n        raise ValueError(f\"Invalid merge mode: {promote_options}\")\n\n    return pyarrow_wrap_schema(\n        GetResultValue(UnifySchemas(c_schemas, c_options)))\n\n\ncdef dict _type_cache = {}\n\n\ncdef DataType primitive_type(Type type):\n    if type in _type_cache:\n        return _type_cache[type]\n\n    cdef DataType out = DataType.__new__(DataType)\n    out.init(GetPrimitiveType(type))\n\n    _type_cache[type] = out\n    return out\n\n\n# -----------------------------------------------------------\n# Type factory functions\n\n\ndef field(name, type, bint nullable=True, metadata=None):\n    \"\"\"\n    Create a pyarrow.Field instance.\n\n    Parameters\n    ----------\n    name : str or bytes\n        Name of the field.\n    type : pyarrow.DataType\n        Arrow datatype of the field.\n    nullable : bool, default True\n        Whether the field's values are nullable.\n    metadata : dict, default None\n        Optional field metadata, the keys and values must be coercible to\n        bytes.\n\n    Returns\n    -------\n    field : pyarrow.Field\n\n    Examples\n    --------\n    Create an instance of pyarrow.Field:\n\n    >>> import pyarrow as pa\n    >>> pa.field('key', pa.int32())\n    pyarrow.Field<key: int32>\n    >>> pa.field('key', pa.int32(), nullable=False)\n    pyarrow.Field<key: int32 not null>\n\n    >>> field = pa.field('key', pa.int32(),\n    ...                  metadata={\"key\": \"Something important\"})\n    >>> field\n    pyarrow.Field<key: int32>\n    >>> field.metadata\n    {b'key': b'Something important'}\n\n    Use the field to create a struct type:\n\n    >>> pa.struct([field])\n    StructType(struct<key: int32>)\n    \"\"\"\n    cdef:\n        Field result = Field.__new__(Field)\n        DataType _type = ensure_type(type, allow_none=False)\n        shared_ptr[const CKeyValueMetadata] c_meta\n\n    metadata = ensure_metadata(metadata, allow_none=True)\n    c_meta = pyarrow_unwrap_metadata(metadata)\n\n    if _type.type.id() == _Type_NA and not nullable:\n        raise ValueError(\"A null type field may not be non-nullable\")\n\n    result.sp_field.reset(\n        new CField(tobytes(name), _type.sp_type, nullable, c_meta)\n    )\n    result.field = result.sp_field.get()\n    result.type = _type\n\n    return result\n\n\ncdef set PRIMITIVE_TYPES = set([\n    _Type_NA, _Type_BOOL,\n    _Type_UINT8, _Type_INT8,\n    _Type_UINT16, _Type_INT16,\n    _Type_UINT32, _Type_INT32,\n    _Type_UINT64, _Type_INT64,\n    _Type_TIMESTAMP, _Type_DATE32,\n    _Type_TIME32, _Type_TIME64,\n    _Type_DATE64,\n    _Type_HALF_FLOAT,\n    _Type_FLOAT,\n    _Type_DOUBLE])\n\n\ndef null():\n    \"\"\"\n    Create instance of null type.\n\n    Examples\n    --------\n    Create an instance of a null type:\n\n    >>> import pyarrow as pa\n    >>> pa.null()\n    DataType(null)\n    >>> print(pa.null())\n    null\n\n    Create a ``Field`` type with a null type and a name:\n\n    >>> pa.field('null_field', pa.null())\n    pyarrow.Field<null_field: null>\n    \"\"\"\n    return primitive_type(_Type_NA)\n\n\ndef bool_():\n    \"\"\"\n    Create instance of boolean type.\n\n    Examples\n    --------\n    Create an instance of a boolean type:\n\n    >>> import pyarrow as pa\n    >>> pa.bool_()\n    DataType(bool)\n    >>> print(pa.bool_())\n    bool\n\n    Create a ``Field`` type with a boolean type\n    and a name:\n\n    >>> pa.field('bool_field', pa.bool_())\n    pyarrow.Field<bool_field: bool>\n    \"\"\"\n    return primitive_type(_Type_BOOL)\n\n\ndef uint8():\n    \"\"\"\n    Create instance of unsigned int8 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int8 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint8()\n    DataType(uint8)\n    >>> print(pa.uint8())\n    uint8\n\n    Create an array with unsigned int8 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint8())\n    <pyarrow.lib.UInt8Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT8)\n\n\ndef int8():\n    \"\"\"\n    Create instance of signed int8 type.\n\n    Examples\n    --------\n    Create an instance of int8 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int8()\n    DataType(int8)\n    >>> print(pa.int8())\n    int8\n\n    Create an array with int8 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int8())\n    <pyarrow.lib.Int8Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT8)\n\n\ndef uint16():\n    \"\"\"\n    Create instance of unsigned uint16 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int16 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint16()\n    DataType(uint16)\n    >>> print(pa.uint16())\n    uint16\n\n    Create an array with unsigned int16 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint16())\n    <pyarrow.lib.UInt16Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT16)\n\n\ndef int16():\n    \"\"\"\n    Create instance of signed int16 type.\n\n    Examples\n    --------\n    Create an instance of int16 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int16()\n    DataType(int16)\n    >>> print(pa.int16())\n    int16\n\n    Create an array with int16 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int16())\n    <pyarrow.lib.Int16Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT16)\n\n\ndef uint32():\n    \"\"\"\n    Create instance of unsigned uint32 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint32()\n    DataType(uint32)\n    >>> print(pa.uint32())\n    uint32\n\n    Create an array with unsigned int32 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint32())\n    <pyarrow.lib.UInt32Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT32)\n\n\ndef int32():\n    \"\"\"\n    Create instance of signed int32 type.\n\n    Examples\n    --------\n    Create an instance of int32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int32()\n    DataType(int32)\n    >>> print(pa.int32())\n    int32\n\n    Create an array with int32 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int32())\n    <pyarrow.lib.Int32Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT32)\n\n\ndef uint64():\n    \"\"\"\n    Create instance of unsigned uint64 type.\n\n    Examples\n    --------\n    Create an instance of unsigned int64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.uint64()\n    DataType(uint64)\n    >>> print(pa.uint64())\n    uint64\n\n    Create an array with unsigned uint64 type:\n\n    >>> pa.array([0, 1, 2], type=pa.uint64())\n    <pyarrow.lib.UInt64Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_UINT64)\n\n\ndef int64():\n    \"\"\"\n    Create instance of signed int64 type.\n\n    Examples\n    --------\n    Create an instance of int64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.int64()\n    DataType(int64)\n    >>> print(pa.int64())\n    int64\n\n    Create an array with int64 type:\n\n    >>> pa.array([0, 1, 2], type=pa.int64())\n    <pyarrow.lib.Int64Array object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_INT64)\n\n\ncdef dict _timestamp_type_cache = {}\ncdef dict _time_type_cache = {}\ncdef dict _duration_type_cache = {}\n\n\ncdef timeunit_to_string(TimeUnit unit):\n    if unit == TimeUnit_SECOND:\n        return 's'\n    elif unit == TimeUnit_MILLI:\n        return 'ms'\n    elif unit == TimeUnit_MICRO:\n        return 'us'\n    elif unit == TimeUnit_NANO:\n        return 'ns'\n\n\ncdef TimeUnit string_to_timeunit(unit) except *:\n    if unit == 's':\n        return TimeUnit_SECOND\n    elif unit == 'ms':\n        return TimeUnit_MILLI\n    elif unit == 'us':\n        return TimeUnit_MICRO\n    elif unit == 'ns':\n        return TimeUnit_NANO\n    else:\n        raise ValueError(f\"Invalid time unit: {unit!r}\")\n\n\ndef tzinfo_to_string(tz):\n    \"\"\"\n    Converts a time zone object into a string indicating the name of a time\n    zone, one of:\n    * As used in the Olson time zone database (the \"tz database\" or\n      \"tzdata\"), such as \"America/New_York\"\n    * An absolute time zone offset of the form +XX:XX or -XX:XX, such as +07:30\n\n    Parameters\n    ----------\n      tz : datetime.tzinfo\n        Time zone object\n\n    Returns\n    -------\n      name : str\n        Time zone name\n    \"\"\"\n    return frombytes(GetResultValue(TzinfoToString(<PyObject*>tz)))\n\n\ndef string_to_tzinfo(name):\n    \"\"\"\n    Convert a time zone name into a time zone object.\n\n    Supported input strings are:\n    * As used in the Olson time zone database (the \"tz database\" or\n      \"tzdata\"), such as \"America/New_York\"\n    * An absolute time zone offset of the form +XX:XX or -XX:XX, such as +07:30\n\n    Parameters\n    ----------\n      name: str\n        Time zone name.\n\n    Returns\n    -------\n      tz : datetime.tzinfo\n        Time zone object\n    \"\"\"\n    cdef PyObject* tz = GetResultValue(StringToTzinfo(name.encode('utf-8')))\n    return PyObject_to_object(tz)\n\n\ndef timestamp(unit, tz=None):\n    \"\"\"\n    Create instance of timestamp type with resolution and optional time zone.\n\n    Parameters\n    ----------\n    unit : str\n        one of 's' [second], 'ms' [millisecond], 'us' [microsecond], or 'ns'\n        [nanosecond]\n    tz : str, default None\n        Time zone name. None indicates time zone naive\n\n    Examples\n    --------\n    Create an instance of timestamp type:\n\n    >>> import pyarrow as pa\n    >>> pa.timestamp('us')\n    TimestampType(timestamp[us])\n    >>> pa.timestamp('s', tz='America/New_York')\n    TimestampType(timestamp[s, tz=America/New_York])\n    >>> pa.timestamp('s', tz='+07:30')\n    TimestampType(timestamp[s, tz=+07:30])\n\n    Use timestamp type when creating a scalar object:\n\n    >>> from datetime import datetime\n    >>> pa.scalar(datetime(2012, 1, 1), type=pa.timestamp('s', tz='UTC'))\n    <pyarrow.TimestampScalar: '2012-01-01T00:00:00+0000'>\n    >>> pa.scalar(datetime(2012, 1, 1), type=pa.timestamp('us'))\n    <pyarrow.TimestampScalar: '2012-01-01T00:00:00.000000'>\n\n    Returns\n    -------\n    timestamp_type : TimestampType\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n        c_string c_timezone\n\n    unit_code = string_to_timeunit(unit)\n\n    cdef TimestampType out = TimestampType.__new__(TimestampType)\n\n    if tz is None:\n        out.init(ctimestamp(unit_code))\n        if unit_code in _timestamp_type_cache:\n            return _timestamp_type_cache[unit_code]\n        _timestamp_type_cache[unit_code] = out\n    else:\n        if not isinstance(tz, (bytes, str)):\n            tz = tzinfo_to_string(tz)\n\n        c_timezone = tobytes(tz)\n        out.init(ctimestamp(unit_code, c_timezone))\n\n    return out\n\n\ndef time32(unit):\n    \"\"\"\n    Create instance of 32-bit time (time of day) type with unit resolution.\n\n    Parameters\n    ----------\n    unit : str\n        one of 's' [second], or 'ms' [millisecond]\n\n    Returns\n    -------\n    type : pyarrow.Time32Type\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> pa.time32('s')\n    Time32Type(time32[s])\n    >>> pa.time32('ms')\n    Time32Type(time32[ms])\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n        c_string c_timezone\n\n    if unit == 's':\n        unit_code = TimeUnit_SECOND\n    elif unit == 'ms':\n        unit_code = TimeUnit_MILLI\n    else:\n        raise ValueError(f\"Invalid time unit for time32: {unit!r}\")\n\n    if unit_code in _time_type_cache:\n        return _time_type_cache[unit_code]\n\n    cdef Time32Type out = Time32Type.__new__(Time32Type)\n\n    out.init(ctime32(unit_code))\n    _time_type_cache[unit_code] = out\n\n    return out\n\n\ndef time64(unit):\n    \"\"\"\n    Create instance of 64-bit time (time of day) type with unit resolution.\n\n    Parameters\n    ----------\n    unit : str\n        One of 'us' [microsecond], or 'ns' [nanosecond].\n\n    Returns\n    -------\n    type : pyarrow.Time64Type\n\n    Examples\n    --------\n    >>> import pyarrow as pa\n    >>> pa.time64('us')\n    Time64Type(time64[us])\n    >>> pa.time64('ns')\n    Time64Type(time64[ns])\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n        c_string c_timezone\n\n    if unit == 'us':\n        unit_code = TimeUnit_MICRO\n    elif unit == 'ns':\n        unit_code = TimeUnit_NANO\n    else:\n        raise ValueError(f\"Invalid time unit for time64: {unit!r}\")\n\n    if unit_code in _time_type_cache:\n        return _time_type_cache[unit_code]\n\n    cdef Time64Type out = Time64Type.__new__(Time64Type)\n\n    out.init(ctime64(unit_code))\n    _time_type_cache[unit_code] = out\n\n    return out\n\n\ndef duration(unit):\n    \"\"\"\n    Create instance of a duration type with unit resolution.\n\n    Parameters\n    ----------\n    unit : str\n        One of 's' [second], 'ms' [millisecond], 'us' [microsecond], or\n        'ns' [nanosecond].\n\n    Returns\n    -------\n    type : pyarrow.DurationType\n\n    Examples\n    --------\n    Create an instance of duration type:\n\n    >>> import pyarrow as pa\n    >>> pa.duration('us')\n    DurationType(duration[us])\n    >>> pa.duration('s')\n    DurationType(duration[s])\n\n    Create an array with duration type:\n\n    >>> pa.array([0, 1, 2], type=pa.duration('s'))\n    <pyarrow.lib.DurationArray object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    cdef:\n        TimeUnit unit_code\n\n    unit_code = string_to_timeunit(unit)\n\n    if unit_code in _duration_type_cache:\n        return _duration_type_cache[unit_code]\n\n    cdef DurationType out = DurationType.__new__(DurationType)\n\n    out.init(cduration(unit_code))\n    _duration_type_cache[unit_code] = out\n\n    return out\n\n\ndef month_day_nano_interval():\n    \"\"\"\n    Create instance of an interval type representing months, days and\n    nanoseconds between two dates.\n\n    Examples\n    --------\n    Create an instance of an month_day_nano_interval type:\n\n    >>> import pyarrow as pa\n    >>> pa.month_day_nano_interval()\n    DataType(month_day_nano_interval)\n\n    Create a scalar with month_day_nano_interval type:\n\n    >>> pa.scalar((1, 15, -30), type=pa.month_day_nano_interval())\n    <pyarrow.MonthDayNanoIntervalScalar: MonthDayNano(months=1, days=15, nanoseconds=-30)>\n    \"\"\"\n    return primitive_type(_Type_INTERVAL_MONTH_DAY_NANO)\n\n\ndef date32():\n    \"\"\"\n    Create instance of 32-bit date (days since UNIX epoch 1970-01-01).\n\n    Examples\n    --------\n    Create an instance of 32-bit date type:\n\n    >>> import pyarrow as pa\n    >>> pa.date32()\n    DataType(date32[day])\n\n    Create a scalar with 32-bit date type:\n\n    >>> from datetime import date\n    >>> pa.scalar(date(2012, 1, 1), type=pa.date32())\n    <pyarrow.Date32Scalar: datetime.date(2012, 1, 1)>\n    \"\"\"\n    return primitive_type(_Type_DATE32)\n\n\ndef date64():\n    \"\"\"\n    Create instance of 64-bit date (milliseconds since UNIX epoch 1970-01-01).\n\n    Examples\n    --------\n    Create an instance of 64-bit date type:\n\n    >>> import pyarrow as pa\n    >>> pa.date64()\n    DataType(date64[ms])\n\n    Create a scalar with 64-bit date type:\n\n    >>> from datetime import datetime\n    >>> pa.scalar(datetime(2012, 1, 1), type=pa.date64())\n    <pyarrow.Date64Scalar: datetime.date(2012, 1, 1)>\n    \"\"\"\n    return primitive_type(_Type_DATE64)\n\n\ndef float16():\n    \"\"\"\n    Create half-precision floating point type.\n\n    Examples\n    --------\n    Create an instance of float16 type:\n\n    >>> import pyarrow as pa\n    >>> pa.float16()\n    DataType(halffloat)\n    >>> print(pa.float16())\n    halffloat\n\n    Create an array with float16 type:\n\n    >>> arr = np.array([1.5, np.nan], dtype=np.float16)\n    >>> a = pa.array(arr, type=pa.float16())\n    >>> a\n    <pyarrow.lib.HalfFloatArray object at ...>\n    [\n      15872,\n      32256\n    ]\n    >>> a.to_pylist()\n    [1.5, nan]\n    \"\"\"\n    return primitive_type(_Type_HALF_FLOAT)\n\n\ndef float32():\n    \"\"\"\n    Create single-precision floating point type.\n\n    Examples\n    --------\n    Create an instance of float32 type:\n\n    >>> import pyarrow as pa\n    >>> pa.float32()\n    DataType(float)\n    >>> print(pa.float32())\n    float\n\n    Create an array with float32 type:\n\n    >>> pa.array([0.0, 1.0, 2.0], type=pa.float32())\n    <pyarrow.lib.FloatArray object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_FLOAT)\n\n\ndef float64():\n    \"\"\"\n    Create double-precision floating point type.\n\n    Examples\n    --------\n    Create an instance of float64 type:\n\n    >>> import pyarrow as pa\n    >>> pa.float64()\n    DataType(double)\n    >>> print(pa.float64())\n    double\n\n    Create an array with float64 type:\n\n    >>> pa.array([0.0, 1.0, 2.0], type=pa.float64())\n    <pyarrow.lib.DoubleArray object at ...>\n    [\n      0,\n      1,\n      2\n    ]\n    \"\"\"\n    return primitive_type(_Type_DOUBLE)\n\n\ncpdef DataType decimal128(int precision, int scale=0):\n    \"\"\"\n    Create decimal type with precision and scale and 128-bit width.\n\n    Arrow decimals are fixed-point decimal numbers encoded as a scaled\n    integer.  The precision is the number of significant digits that the\n    decimal type can represent; the scale is the number of digits after\n    the decimal point (note the scale can be negative).\n\n    As an example, ``decimal128(7, 3)`` can exactly represent the numbers\n    1234.567 and -1234.567 (encoded internally as the 128-bit integers\n    1234567 and -1234567, respectively), but neither 12345.67 nor 123.4567.\n\n    ``decimal128(5, -3)`` can exactly represent the number 12345000\n    (encoded internally as the 128-bit integer 12345), but neither\n    123450000 nor 1234500.\n\n    If you need a precision higher than 38 significant digits, consider\n    using ``decimal256``.\n\n    Parameters\n    ----------\n    precision : int\n        Must be between 1 and 38\n    scale : int\n\n    Returns\n    -------\n    decimal_type : Decimal128Type\n\n    Examples\n    --------\n    Create an instance of decimal type:\n\n    >>> import pyarrow as pa\n    >>> pa.decimal128(5, 2)\n    Decimal128Type(decimal128(5, 2))\n\n    Create an array with decimal type:\n\n    >>> import decimal\n    >>> a = decimal.Decimal('123.45')\n    >>> pa.array([a], pa.decimal128(5, 2))\n    <pyarrow.lib.Decimal128Array object at ...>\n    [\n      123.45\n    ]\n    \"\"\"\n    cdef shared_ptr[CDataType] decimal_type\n    if precision < 1 or precision > 38:\n        raise ValueError(\"precision should be between 1 and 38\")\n    decimal_type.reset(new CDecimal128Type(precision, scale))\n    return pyarrow_wrap_data_type(decimal_type)\n\n\ncpdef DataType decimal256(int precision, int scale=0):\n    \"\"\"\n    Create decimal type with precision and scale and 256-bit width.\n\n    Arrow decimals are fixed-point decimal numbers encoded as a scaled\n    integer.  The precision is the number of significant digits that the\n    decimal type can represent; the scale is the number of digits after\n    the decimal point (note the scale can be negative).\n\n    For most use cases, the maximum precision offered by ``decimal128``\n    is sufficient, and it will result in a more compact and more efficient\n    encoding.  ``decimal256`` is useful if you need a precision higher\n    than 38 significant digits.\n\n    Parameters\n    ----------\n    precision : int\n        Must be between 1 and 76\n    scale : int\n\n    Returns\n    -------\n    decimal_type : Decimal256Type\n    \"\"\"\n    cdef shared_ptr[CDataType] decimal_type\n    if precision < 1 or precision > 76:\n        raise ValueError(\"precision should be between 1 and 76\")\n    decimal_type.reset(new CDecimal256Type(precision, scale))\n    return pyarrow_wrap_data_type(decimal_type)\n\n\ndef string():\n    \"\"\"\n    Create UTF8 variable-length string type.\n\n    Examples\n    --------\n    Create an instance of a string type:\n\n    >>> import pyarrow as pa\n    >>> pa.string()\n    DataType(string)\n\n    and use the string type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.string())\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      \"baz\"\n    ]\n    \"\"\"\n    return primitive_type(_Type_STRING)\n\n\ndef utf8():\n    \"\"\"\n    Alias for string().\n\n    Examples\n    --------\n    Create an instance of a string type:\n\n    >>> import pyarrow as pa\n    >>> pa.utf8()\n    DataType(string)\n\n    and use the string type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.utf8())\n    <pyarrow.lib.StringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      \"baz\"\n    ]\n    \"\"\"\n    return string()\n\n\ndef binary(int length=-1):\n    \"\"\"\n    Create variable-length or fixed size binary type.\n\n    Parameters\n    ----------\n    length : int, optional, default -1\n        If length == -1 then return a variable length binary type. If length is\n        greater than or equal to 0 then return a fixed size binary type of\n        width `length`.\n\n    Examples\n    --------\n    Create an instance of a variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.binary()\n    DataType(binary)\n\n    and use the variable-length binary type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.binary())\n    <pyarrow.lib.BinaryArray object at ...>\n    [\n      666F6F,\n      626172,\n      62617A\n    ]\n\n    Create an instance of a fixed-size binary type:\n\n    >>> pa.binary(3)\n    FixedSizeBinaryType(fixed_size_binary[3])\n\n    and use the fixed-length binary type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.binary(3))\n    <pyarrow.lib.FixedSizeBinaryArray object at ...>\n    [\n      666F6F,\n      626172,\n      62617A\n    ]\n    \"\"\"\n    if length == -1:\n        return primitive_type(_Type_BINARY)\n\n    cdef shared_ptr[CDataType] fixed_size_binary_type\n    fixed_size_binary_type.reset(new CFixedSizeBinaryType(length))\n    return pyarrow_wrap_data_type(fixed_size_binary_type)\n\n\ndef large_binary():\n    \"\"\"\n    Create large variable-length binary type.\n\n    This data type may not be supported by all Arrow implementations.  Unless\n    you need to represent data larger than 2GB, you should prefer binary().\n\n    Examples\n    --------\n    Create an instance of large variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.large_binary()\n    DataType(large_binary)\n\n    and use the type to create an array:\n\n    >>> pa.array(['foo', 'bar', 'baz'], type=pa.large_binary())\n    <pyarrow.lib.LargeBinaryArray object at ...>\n    [\n      666F6F,\n      626172,\n      62617A\n    ]\n    \"\"\"\n    return primitive_type(_Type_LARGE_BINARY)\n\n\ndef large_string():\n    \"\"\"\n    Create large UTF8 variable-length string type.\n\n    This data type may not be supported by all Arrow implementations.  Unless\n    you need to represent data larger than 2GB, you should prefer string().\n\n    Examples\n    --------\n    Create an instance of large UTF8 variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.large_string()\n    DataType(large_string)\n\n    and use the type to create an array:\n\n    >>> pa.array(['foo', 'bar'] * 50, type=pa.large_string())\n    <pyarrow.lib.LargeStringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      ...\n      \"foo\",\n      \"bar\"\n    ]\n    \"\"\"\n    return primitive_type(_Type_LARGE_STRING)\n\n\ndef large_utf8():\n    \"\"\"\n    Alias for large_string().\n\n    Examples\n    --------\n    Create an instance of large UTF8 variable-length binary type:\n\n    >>> import pyarrow as pa\n    >>> pa.large_utf8()\n    DataType(large_string)\n\n    and use the type to create an array:\n\n    >>> pa.array(['foo', 'bar'] * 50, type=pa.large_utf8())\n    <pyarrow.lib.LargeStringArray object at ...>\n    [\n      \"foo\",\n      \"bar\",\n      ...\n      \"foo\",\n      \"bar\"\n    ]\n    \"\"\"\n    return large_string()\n\n\ndef list_(value_type, int list_size=-1):\n    \"\"\"\n    Create ListType instance from child data type or field.\n\n    Parameters\n    ----------\n    value_type : DataType or Field\n    list_size : int, optional, default -1\n        If length == -1 then return a variable length list type. If length is\n        greater than or equal to 0 then return a fixed size list type.\n\n    Returns\n    -------\n    list_type : DataType\n\n    Examples\n    --------\n    Create an instance of ListType:\n\n    >>> import pyarrow as pa\n    >>> pa.list_(pa.string())\n    ListType(list<item: string>)\n    >>> pa.list_(pa.int32(), 2)\n    FixedSizeListType(fixed_size_list<item: int32>[2])\n\n    Use the ListType to create a scalar:\n\n    >>> pa.scalar(['foo', None], type=pa.list_(pa.string(), 2))\n    <pyarrow.FixedSizeListScalar: ['foo', None]>\n\n    or an array:\n\n    >>> pa.array([[1, 2], [3, 4]], pa.list_(pa.int32(), 2))\n    <pyarrow.lib.FixedSizeListArray object at ...>\n    [\n      [\n        1,\n        2\n      ],\n      [\n        3,\n        4\n      ]\n    ]\n    \"\"\"\n    cdef:\n        Field _field\n        shared_ptr[CDataType] list_type\n\n    if isinstance(value_type, DataType):\n        _field = field('item', value_type)\n    elif isinstance(value_type, Field):\n        _field = value_type\n    else:\n        raise TypeError('List requires DataType or Field')\n\n    if list_size == -1:\n        list_type.reset(new CListType(_field.sp_field))\n    else:\n        if list_size < 0:\n            raise ValueError(\"list_size should be a positive integer\")\n        list_type.reset(new CFixedSizeListType(_field.sp_field, list_size))\n\n    return pyarrow_wrap_data_type(list_type)\n\n\ncpdef LargeListType large_list(value_type):\n    \"\"\"\n    Create LargeListType instance from child data type or field.\n\n    This data type may not be supported by all Arrow implementations.\n    Unless you need to represent data larger than 2**31 elements, you should\n    prefer list_().\n\n    Parameters\n    ----------\n    value_type : DataType or Field\n\n    Returns\n    -------\n    list_type : DataType\n\n    Examples\n    --------\n    Create an instance of LargeListType:\n\n    >>> import pyarrow as pa\n    >>> pa.large_list(pa.int8())\n    LargeListType(large_list<item: int8>)\n\n    Use the LargeListType to create an array:\n\n    >>> pa.array([[-1, 3]] * 5, type=pa.large_list(pa.int8()))\n    <pyarrow.lib.LargeListArray object at ...>\n    [\n      [\n        -1,\n        3\n      ],\n      [\n        -1,\n        3\n      ],\n    ...\n    \"\"\"\n    cdef:\n        DataType data_type\n        Field _field\n        shared_ptr[CDataType] list_type\n        LargeListType out = LargeListType.__new__(LargeListType)\n\n    if isinstance(value_type, DataType):\n        _field = field('item', value_type)\n    elif isinstance(value_type, Field):\n        _field = value_type\n    else:\n        raise TypeError('List requires DataType or Field')\n\n    list_type.reset(new CLargeListType(_field.sp_field))\n    out.init(list_type)\n    return out\n\n\ncpdef MapType map_(key_type, item_type, keys_sorted=False):\n    \"\"\"\n    Create MapType instance from key and item data types or fields.\n\n    Parameters\n    ----------\n    key_type : DataType or Field\n    item_type : DataType or Field\n    keys_sorted : bool\n\n    Returns\n    -------\n    map_type : DataType\n\n    Examples\n    --------\n    Create an instance of MapType:\n\n    >>> import pyarrow as pa\n    >>> pa.map_(pa.string(), pa.int32())\n    MapType(map<string, int32>)\n    >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True)\n    MapType(map<string, int32, keys_sorted>)\n\n    Use MapType to create an array:\n\n    >>> data = [[{'key': 'a', 'value': 1}, {'key': 'b', 'value': 2}], [{'key': 'c', 'value': 3}]]\n    >>> pa.array(data, type=pa.map_(pa.string(), pa.int32(), keys_sorted=True))\n    <pyarrow.lib.MapArray object at ...>\n    [\n      keys:\n      [\n        \"a\",\n        \"b\"\n      ]\n      values:\n      [\n        1,\n        2\n      ],\n      keys:\n      [\n        \"c\"\n      ]\n      values:\n      [\n        3\n      ]\n    ]\n    \"\"\"\n    cdef:\n        Field _key_field\n        Field _item_field\n        shared_ptr[CDataType] map_type\n        MapType out = MapType.__new__(MapType)\n\n    if isinstance(key_type, Field):\n        if key_type.nullable:\n            raise TypeError('Map key field should be non-nullable')\n        _key_field = key_type\n    else:\n        _key_field = field('key', ensure_type(key_type, allow_none=False),\n                           nullable=False)\n\n    if isinstance(item_type, Field):\n        _item_field = item_type\n    else:\n        _item_field = field('value', ensure_type(item_type, allow_none=False))\n\n    map_type.reset(new CMapType(_key_field.sp_field, _item_field.sp_field,\n                                keys_sorted))\n    out.init(map_type)\n    return out\n\n\ncpdef DictionaryType dictionary(index_type, value_type, bint ordered=False):\n    \"\"\"\n    Dictionary (categorical, or simply encoded) type.\n\n    Parameters\n    ----------\n    index_type : DataType\n    value_type : DataType\n    ordered : bool\n\n    Returns\n    -------\n    type : DictionaryType\n\n    Examples\n    --------\n    Create an instance of dictionary type:\n\n    >>> import pyarrow as pa\n    >>> pa.dictionary(pa.int64(), pa.utf8())\n    DictionaryType(dictionary<values=string, indices=int64, ordered=0>)\n\n    Use dictionary type to create an array:\n\n    >>> pa.array([\"a\", \"b\", None, \"d\"], pa.dictionary(pa.int64(), pa.utf8()))\n    <pyarrow.lib.DictionaryArray object at ...>\n    ...\n    -- dictionary:\n      [\n        \"a\",\n        \"b\",\n        \"d\"\n      ]\n    -- indices:\n      [\n        0,\n        1,\n        null,\n        2\n      ]\n    \"\"\"\n    cdef:\n        DataType _index_type = ensure_type(index_type, allow_none=False)\n        DataType _value_type = ensure_type(value_type, allow_none=False)\n        DictionaryType out = DictionaryType.__new__(DictionaryType)\n        shared_ptr[CDataType] dict_type\n\n    if _index_type.id not in {\n        Type_INT8, Type_INT16, Type_INT32, Type_INT64,\n        Type_UINT8, Type_UINT16, Type_UINT32, Type_UINT64,\n    }:\n        raise TypeError(\"The dictionary index type should be integer.\")\n\n    dict_type.reset(new CDictionaryType(_index_type.sp_type,\n                                        _value_type.sp_type, ordered == 1))\n    out.init(dict_type)\n    return out\n\n\ndef struct(fields):\n    \"\"\"\n    Create StructType instance from fields.\n\n    A struct is a nested type parameterized by an ordered sequence of types\n    (which can all be distinct), called its fields.\n\n    Parameters\n    ----------\n    fields : iterable of Fields or tuples, or mapping of strings to DataTypes\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n\n    Examples\n    --------\n    Create an instance of StructType from an iterable of tuples:\n\n    >>> import pyarrow as pa\n    >>> fields = [\n    ...     ('f1', pa.int32()),\n    ...     ('f2', pa.string()),\n    ... ]\n    >>> struct_type = pa.struct(fields)\n    >>> struct_type\n    StructType(struct<f1: int32, f2: string>)\n\n    Retrieve a field from a StructType:\n\n    >>> struct_type[0]\n    pyarrow.Field<f1: int32>\n    >>> struct_type['f1']\n    pyarrow.Field<f1: int32>\n\n    Create an instance of StructType from an iterable of Fields:\n\n    >>> fields = [\n    ...     pa.field('f1', pa.int32()),\n    ...     pa.field('f2', pa.string(), nullable=False),\n    ... ]\n    >>> pa.struct(fields)\n    StructType(struct<f1: int32, f2: string not null>)\n\n    Returns\n    -------\n    type : DataType\n    \"\"\"\n    cdef:\n        Field py_field\n        vector[shared_ptr[CField]] c_fields\n        cdef shared_ptr[CDataType] struct_type\n\n    if isinstance(fields, Mapping):\n        fields = fields.items()\n\n    for item in fields:\n        if isinstance(item, tuple):\n            py_field = field(*item)\n        else:\n            py_field = item\n        c_fields.push_back(py_field.sp_field)\n\n    struct_type.reset(new CStructType(c_fields))\n    return pyarrow_wrap_data_type(struct_type)\n\n\ncdef _extract_union_params(child_fields, type_codes,\n                           vector[shared_ptr[CField]]* c_fields,\n                           vector[int8_t]* c_type_codes):\n    cdef:\n        Field child_field\n\n    for child_field in child_fields:\n        c_fields[0].push_back(child_field.sp_field)\n\n    if type_codes is not None:\n        if len(type_codes) != <Py_ssize_t>(c_fields.size()):\n            raise ValueError(\"type_codes should have the same length \"\n                             \"as fields\")\n        for code in type_codes:\n            c_type_codes[0].push_back(code)\n    else:\n        c_type_codes[0] = range(c_fields.size())\n\n\ndef sparse_union(child_fields, type_codes=None):\n    \"\"\"\n    Create SparseUnionType from child fields.\n\n    A sparse union is a nested type where each logical value is taken from\n    a single child.  A buffer of 8-bit type ids indicates which child\n    a given logical value is to be taken from.\n\n    In a sparse union, each child array should have the same length as the\n    union array, regardless of the actual number of union values that\n    refer to it.\n\n    Parameters\n    ----------\n    child_fields : sequence of Field values\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n    type_codes : list of integers, default None\n\n    Returns\n    -------\n    type : SparseUnionType\n    \"\"\"\n    cdef:\n        vector[shared_ptr[CField]] c_fields\n        vector[int8_t] c_type_codes\n\n    _extract_union_params(child_fields, type_codes,\n                          &c_fields, &c_type_codes)\n\n    return pyarrow_wrap_data_type(\n        CMakeSparseUnionType(move(c_fields), move(c_type_codes)))\n\n\ndef dense_union(child_fields, type_codes=None):\n    \"\"\"\n    Create DenseUnionType from child fields.\n\n    A dense union is a nested type where each logical value is taken from\n    a single child, at a specific offset.  A buffer of 8-bit type ids\n    indicates which child a given logical value is to be taken from,\n    and a buffer of 32-bit offsets indicates at which physical position\n    in the given child array the logical value is to be taken from.\n\n    Unlike a sparse union, a dense union allows encoding only the child array\n    values which are actually referred to by the union array.  This is\n    counterbalanced by the additional footprint of the offsets buffer, and\n    the additional indirection cost when looking up values.\n\n    Parameters\n    ----------\n    child_fields : sequence of Field values\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n    type_codes : list of integers, default None\n\n    Returns\n    -------\n    type : DenseUnionType\n    \"\"\"\n    cdef:\n        vector[shared_ptr[CField]] c_fields\n        vector[int8_t] c_type_codes\n\n    _extract_union_params(child_fields, type_codes,\n                          &c_fields, &c_type_codes)\n\n    return pyarrow_wrap_data_type(\n        CMakeDenseUnionType(move(c_fields), move(c_type_codes)))\n\n\ndef union(child_fields, mode, type_codes=None):\n    \"\"\"\n    Create UnionType from child fields.\n\n    A union is a nested type where each logical value is taken from a\n    single child.  A buffer of 8-bit type ids indicates which child\n    a given logical value is to be taken from.\n\n    Unions come in two flavors: sparse and dense\n    (see also `pyarrow.sparse_union` and `pyarrow.dense_union`).\n\n    Parameters\n    ----------\n    child_fields : sequence of Field values\n        Each field must have a UTF8-encoded name, and these field names are\n        part of the type metadata.\n    mode : str\n        Must be 'sparse' or 'dense'\n    type_codes : list of integers, default None\n\n    Returns\n    -------\n    type : UnionType\n    \"\"\"\n    if isinstance(mode, int):\n        if mode not in (_UnionMode_SPARSE, _UnionMode_DENSE):\n            raise ValueError(\"Invalid union mode {0!r}\".format(mode))\n    else:\n        if mode == 'sparse':\n            mode = _UnionMode_SPARSE\n        elif mode == 'dense':\n            mode = _UnionMode_DENSE\n        else:\n            raise ValueError(\"Invalid union mode {0!r}\".format(mode))\n\n    if mode == _UnionMode_SPARSE:\n        return sparse_union(child_fields, type_codes)\n    else:\n        return dense_union(child_fields, type_codes)\n\n\ndef run_end_encoded(run_end_type, value_type):\n    \"\"\"\n    Create RunEndEncodedType from run-end and value types.\n\n    Parameters\n    ----------\n    run_end_type : pyarrow.DataType\n        The integer type of the run_ends array. Must be 'int16', 'int32', or 'int64'.\n    value_type : pyarrow.DataType\n        The type of the values array.\n\n    Returns\n    -------\n    type : RunEndEncodedType\n    \"\"\"\n    cdef:\n        DataType _run_end_type = ensure_type(run_end_type, allow_none=False)\n        DataType _value_type = ensure_type(value_type, allow_none=False)\n        shared_ptr[CDataType] ree_type\n\n    if not _run_end_type.type.id() in [_Type_INT16, _Type_INT32, _Type_INT64]:\n        raise ValueError(\"The run_end_type should be 'int16', 'int32', or 'int64'\")\n    ree_type = CMakeRunEndEncodedType(_run_end_type.sp_type, _value_type.sp_type)\n    return pyarrow_wrap_data_type(ree_type)\n\n\ndef fixed_shape_tensor(DataType value_type, shape, dim_names=None, permutation=None):\n    \"\"\"\n    Create instance of fixed shape tensor extension type with shape and optional\n    names of tensor dimensions and indices of the desired logical\n    ordering of dimensions.\n\n    Parameters\n    ----------\n    value_type : DataType\n        Data type of individual tensor elements.\n    shape : tuple or list of integers\n        The physical shape of the contained tensors.\n    dim_names : tuple or list of strings, default None\n        Explicit names to tensor dimensions.\n    permutation : tuple or list integers, default None\n        Indices of the desired ordering of the original dimensions.\n        The indices contain a permutation of the values ``[0, 1, .., N-1]`` where\n        N is the number of dimensions. The permutation indicates which dimension\n        of the logical layout corresponds to which dimension of the physical tensor.\n        For more information on this parameter see\n        :ref:`fixed_shape_tensor_extension`.\n\n    Examples\n    --------\n    Create an instance of fixed shape tensor extension type:\n\n    >>> import pyarrow as pa\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int32(), [2, 2])\n    >>> tensor_type\n    FixedShapeTensorType(extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>)\n\n    Inspect the data type:\n\n    >>> tensor_type.value_type\n    DataType(int32)\n    >>> tensor_type.shape\n    [2, 2]\n\n    Create a table with fixed shape tensor extension array:\n\n    >>> arr = [[1, 2, 3, 4], [10, 20, 30, 40], [100, 200, 300, 400]]\n    >>> storage = pa.array(arr, pa.list_(pa.int32(), 4))\n    >>> tensor = pa.ExtensionArray.from_storage(tensor_type, storage)\n    >>> pa.table([tensor], names=[\"tensor_array\"])\n    pyarrow.Table\n    tensor_array: extension<arrow.fixed_shape_tensor[value_type=int32, shape=[2,2]]>\n    ----\n    tensor_array: [[[1,2,3,4],[10,20,30,40],[100,200,300,400]]]\n\n    Create an instance of fixed shape tensor extension type with names\n    of tensor dimensions:\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int8(), (2, 2, 3),\n    ...                                     dim_names=['C', 'H', 'W'])\n    >>> tensor_type.dim_names\n    ['C', 'H', 'W']\n\n    Create an instance of fixed shape tensor extension type with\n    permutation:\n\n    >>> tensor_type = pa.fixed_shape_tensor(pa.int8(), (2, 2, 3),\n    ...                                     permutation=[0, 2, 1])\n    >>> tensor_type.permutation\n    [0, 2, 1]\n\n    Returns\n    -------\n    type : FixedShapeTensorType\n    \"\"\"\n\n    cdef:\n        vector[int64_t] c_shape\n        vector[int64_t] c_permutation\n        vector[c_string] c_dim_names\n        shared_ptr[CDataType] c_tensor_ext_type\n\n    assert value_type is not None\n    assert shape is not None\n\n    for i in shape:\n        c_shape.push_back(i)\n\n    if permutation is not None:\n        for i in permutation:\n            c_permutation.push_back(i)\n\n    if dim_names is not None:\n        for x in dim_names:\n            c_dim_names.push_back(tobytes(x))\n\n    cdef FixedShapeTensorType out = FixedShapeTensorType.__new__(FixedShapeTensorType)\n\n    c_tensor_ext_type = GetResultValue(CFixedShapeTensorType.Make(\n        value_type.sp_type, c_shape, c_permutation, c_dim_names))\n\n    out.init(c_tensor_ext_type)\n\n    return out\n\n\ncdef dict _type_aliases = {\n    'null': null,\n    'bool': bool_,\n    'boolean': bool_,\n    'i1': int8,\n    'int8': int8,\n    'i2': int16,\n    'int16': int16,\n    'i4': int32,\n    'int32': int32,\n    'i8': int64,\n    'int64': int64,\n    'u1': uint8,\n    'uint8': uint8,\n    'u2': uint16,\n    'uint16': uint16,\n    'u4': uint32,\n    'uint32': uint32,\n    'u8': uint64,\n    'uint64': uint64,\n    'f2': float16,\n    'halffloat': float16,\n    'float16': float16,\n    'f4': float32,\n    'float': float32,\n    'float32': float32,\n    'f8': float64,\n    'double': float64,\n    'float64': float64,\n    'string': string,\n    'str': string,\n    'utf8': string,\n    'binary': binary,\n    'large_string': large_string,\n    'large_str': large_string,\n    'large_utf8': large_string,\n    'large_binary': large_binary,\n    'date32': date32,\n    'date64': date64,\n    'date32[day]': date32,\n    'date64[ms]': date64,\n    'time32[s]': time32('s'),\n    'time32[ms]': time32('ms'),\n    'time64[us]': time64('us'),\n    'time64[ns]': time64('ns'),\n    'timestamp[s]': timestamp('s'),\n    'timestamp[ms]': timestamp('ms'),\n    'timestamp[us]': timestamp('us'),\n    'timestamp[ns]': timestamp('ns'),\n    'duration[s]': duration('s'),\n    'duration[ms]': duration('ms'),\n    'duration[us]': duration('us'),\n    'duration[ns]': duration('ns'),\n    'month_day_nano_interval': month_day_nano_interval(),\n}\n\n\ndef type_for_alias(name):\n    \"\"\"\n    Return DataType given a string alias if one exists.\n\n    Parameters\n    ----------\n    name : str\n        The alias of the DataType that should be retrieved.\n\n    Returns\n    -------\n    type : DataType\n    \"\"\"\n    name = name.lower()\n    try:\n        alias = _type_aliases[name]\n    except KeyError:\n        raise ValueError('No type alias for {0}'.format(name))\n\n    if isinstance(alias, DataType):\n        return alias\n    return alias()\n\n\ncpdef DataType ensure_type(object ty, bint allow_none=False):\n    if allow_none and ty is None:\n        return None\n    elif isinstance(ty, DataType):\n        return ty\n    elif isinstance(ty, str):\n        return type_for_alias(ty)\n    else:\n        raise TypeError('DataType expected, got {!r}'.format(type(ty)))\n\n\ndef schema(fields, metadata=None):\n    \"\"\"\n    Construct pyarrow.Schema from collection of fields.\n\n    Parameters\n    ----------\n    fields : iterable of Fields or tuples, or mapping of strings to DataTypes\n        Can also pass an object that implements the Arrow PyCapsule Protocol\n        for schemas (has an ``__arrow_c_schema__`` method).\n    metadata : dict, default None\n        Keys and values must be coercible to bytes.\n\n    Examples\n    --------\n    Create a Schema from iterable of tuples:\n\n    >>> import pyarrow as pa\n    >>> pa.schema([\n    ...     ('some_int', pa.int32()),\n    ...     ('some_string', pa.string()),\n    ...     pa.field('some_required_string', pa.string(), nullable=False)\n    ... ])\n    some_int: int32\n    some_string: string\n    some_required_string: string not null\n\n    Create a Schema from iterable of Fields:\n\n    >>> pa.schema([\n    ...     pa.field('some_int', pa.int32()),\n    ...     pa.field('some_string', pa.string())\n    ... ])\n    some_int: int32\n    some_string: string\n\n    Returns\n    -------\n    schema : pyarrow.Schema\n    \"\"\"\n    cdef:\n        shared_ptr[const CKeyValueMetadata] c_meta\n        shared_ptr[CSchema] c_schema\n        Schema result\n        Field py_field\n        vector[shared_ptr[CField]] c_fields\n\n    if isinstance(fields, Mapping):\n        fields = fields.items()\n    elif hasattr(fields, \"__arrow_c_schema__\"):\n        return Schema._import_from_c_capsule(fields.__arrow_c_schema__())\n\n    for item in fields:\n        if isinstance(item, tuple):\n            py_field = field(*item)\n        else:\n            py_field = item\n        if py_field is None:\n            raise TypeError(\"field or tuple expected, got None\")\n        c_fields.push_back(py_field.sp_field)\n\n    metadata = ensure_metadata(metadata, allow_none=True)\n    c_meta = pyarrow_unwrap_metadata(metadata)\n\n    c_schema.reset(new CSchema(c_fields, c_meta))\n    result = Schema.__new__(Schema)\n    result.init_schema(c_schema)\n\n    return result\n\n\ndef from_numpy_dtype(object dtype):\n    \"\"\"\n    Convert NumPy dtype to pyarrow.DataType.\n\n    Parameters\n    ----------\n    dtype : the numpy dtype to convert\n\n\n    Examples\n    --------\n    Create a pyarrow DataType from NumPy dtype:\n\n    >>> import pyarrow as pa\n    >>> import numpy as np\n    >>> pa.from_numpy_dtype(np.dtype('float16'))\n    DataType(halffloat)\n    >>> pa.from_numpy_dtype('U')\n    DataType(string)\n    >>> pa.from_numpy_dtype(bool)\n    DataType(bool)\n    >>> pa.from_numpy_dtype(np.str_)\n    DataType(string)\n    \"\"\"\n    cdef shared_ptr[CDataType] c_type\n    dtype = np.dtype(dtype)\n    with nogil:\n        check_status(NumPyDtypeToArrow(dtype, &c_type))\n\n    return pyarrow_wrap_data_type(c_type)\n\n\ndef is_boolean_value(object obj):\n    \"\"\"\n    Check if the object is a boolean.\n\n    Parameters\n    ----------\n    obj : object\n        The object to check\n    \"\"\"\n    return IsPyBool(obj)\n\n\ndef is_integer_value(object obj):\n    \"\"\"\n    Check if the object is an integer.\n\n    Parameters\n    ----------\n    obj : object\n        The object to check\n    \"\"\"\n    return IsPyInt(obj)\n\n\ndef is_float_value(object obj):\n    \"\"\"\n    Check if the object is a float.\n\n    Parameters\n    ----------\n    obj : object\n        The object to check\n    \"\"\"\n    return IsPyFloat(obj)\n\n\ncdef class _ExtensionRegistryNanny(_Weakrefable):\n    # Keep the registry alive until we have unregistered PyExtensionType\n    cdef:\n        shared_ptr[CExtensionTypeRegistry] registry\n\n    def __cinit__(self):\n        self.registry = CExtensionTypeRegistry.GetGlobalRegistry()\n\n    def release_registry(self):\n        self.registry.reset()\n\n\n_registry_nanny = _ExtensionRegistryNanny()\n\n\ndef _register_py_extension_type():\n    cdef:\n        DataType storage_type\n        shared_ptr[CExtensionType] cpy_ext_type\n        c_string c_extension_name = tobytes(\"arrow.py_extension_type\")\n\n    # Make a dummy C++ ExtensionType\n    storage_type = null()\n    check_status(CPyExtensionType.FromClass(\n        storage_type.sp_type, c_extension_name, PyExtensionType,\n        &cpy_ext_type))\n    check_status(\n        RegisterPyExtensionType(<shared_ptr[CDataType]> cpy_ext_type))\n\n\ndef _unregister_py_extension_types():\n    # This needs to be done explicitly before the Python interpreter is\n    # finalized.  If the C++ type is destroyed later in the process\n    # teardown stage, it will invoke CPython APIs such as Py_DECREF\n    # with a destroyed interpreter.\n    unregister_extension_type(\"arrow.py_extension_type\")\n    for ext_type in _python_extension_types_registry:\n        try:\n            unregister_extension_type(ext_type.extension_name)\n        except KeyError:\n            pass\n    _registry_nanny.release_registry()\n\n\n_register_py_extension_type()\natexit.register(_unregister_py_extension_types)\n\n\n#\n# PyCapsule export utilities\n#\n\ncdef void pycapsule_schema_deleter(object schema_capsule) noexcept:\n    cdef ArrowSchema* schema = <ArrowSchema*>PyCapsule_GetPointer(\n        schema_capsule, 'arrow_schema'\n    )\n    if schema.release != NULL:\n        schema.release(schema)\n\n    free(schema)\n\ncdef object alloc_c_schema(ArrowSchema** c_schema) noexcept:\n    c_schema[0] = <ArrowSchema*> malloc(sizeof(ArrowSchema))\n    # Ensure the capsule destructor doesn't call a random release pointer\n    c_schema[0].release = NULL\n    return PyCapsule_New(c_schema[0], 'arrow_schema', &pycapsule_schema_deleter)\n\n\ncdef void pycapsule_array_deleter(object array_capsule) noexcept:\n    cdef:\n        ArrowArray* array\n    # Do not invoke the deleter on a used/moved capsule\n    array = <ArrowArray*>cpython.PyCapsule_GetPointer(\n        array_capsule, 'arrow_array'\n    )\n    if array.release != NULL:\n        array.release(array)\n\n    free(array)\n\ncdef object alloc_c_array(ArrowArray** c_array) noexcept:\n    c_array[0] = <ArrowArray*> malloc(sizeof(ArrowArray))\n    # Ensure the capsule destructor doesn't call a random release pointer\n    c_array[0].release = NULL\n    return PyCapsule_New(c_array[0], 'arrow_array', &pycapsule_array_deleter)\n\n\ncdef void pycapsule_stream_deleter(object stream_capsule) noexcept:\n    cdef:\n        ArrowArrayStream* stream\n    # Do not invoke the deleter on a used/moved capsule\n    stream = <ArrowArrayStream*>PyCapsule_GetPointer(\n        stream_capsule, 'arrow_array_stream'\n    )\n    if stream.release != NULL:\n        stream.release(stream)\n\n    free(stream)\n\ncdef object alloc_c_stream(ArrowArrayStream** c_stream) noexcept:\n    c_stream[0] = <ArrowArrayStream*> malloc(sizeof(ArrowArrayStream))\n    # Ensure the capsule destructor doesn't call a random release pointer\n    c_stream[0].release = NULL\n    return PyCapsule_New(c_stream[0], 'arrow_array_stream', &pycapsule_stream_deleter)\n"], "filenames": ["docs/source/python/extending_types.rst", "python/pyarrow/tests/test_cffi.py", "python/pyarrow/tests/test_extension_type.py", "python/pyarrow/tests/test_pandas.py", "python/pyarrow/types.pxi"], "buggy_code_start_loc": [71, 18, 17, 4099, 1439], "buggy_code_end_loc": [319, 323, 1372, 4208, 1760], "fixing_code_start_loc": [71, 19, 18, 4099, 1440], "fixing_code_end_loc": [286, 355, 1500, 4216, 1759], "type": "CWE-502", "message": "Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files).\n\nThis vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings.\n\nIt is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI packages are already available, and we hope that conda-forge packages will be available soon.\n\nIf it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` that disables the vulnerability on older PyArrow versions. See  https://pypi.org/project/pyarrow-hotfix/  for instructions.\n\n", "other": {"cve": {"id": "CVE-2023-47248", "sourceIdentifier": "security@apache.org", "published": "2023-11-09T09:15:08.223", "lastModified": "2023-11-29T03:15:42.547", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files).\n\nThis vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings.\n\nIt is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI packages are already available, and we hope that conda-forge packages will be available soon.\n\nIf it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` that disables the vulnerability on older PyArrow versions. See  https://pypi.org/project/pyarrow-hotfix/  for instructions.\n\n"}, {"lang": "es", "value": "La deserializaci\u00f3n de datos que no son de confianza en lectores IPC y Parquet en las versiones de PyArrow 0.14.0 a 14.0.0 permite la ejecuci\u00f3n de c\u00f3digo arbitrario. Una aplicaci\u00f3n es vulnerable si lee datos de Arrow IPC, Feather o Parquet de fuentes que no son de confianza (por ejemplo, archivos de entrada proporcionados por el usuario). Esta vulnerabilidad solo afecta a PyArrow, no a otras implementaciones o enlaces de Apache Arrow. Se recomienda que los usuarios de PyArrow actualicen a 14.0.1. De manera similar, se recomienda que las librer\u00edas posteriores actualicen sus requisitos de dependencia a PyArrow 14.0.1 o posterior. Los paquetes PyPI ya est\u00e1n disponibles y esperamos que los paquetes conda-forge lo est\u00e9n pronto. Si no es posible actualizar, proporcionamos un paquete separado `pyarrow-hotfix` que desactiva la vulnerabilidad en versiones anteriores de PyArrow. Consulte https://pypi.org/project/pyarrow-hotfix/ para obtener instrucciones."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}]}, "weaknesses": [{"source": "security@apache.org", "type": "Primary", "description": [{"lang": "en", "value": "CWE-502"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:apache:pyarrow:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.14.0", "versionEndIncluding": "14.0.0", "matchCriteriaId": "E539AE6F-78DA-44F9-8185-667567004968"}]}]}], "references": [{"url": "https://github.com/apache/arrow/commit/f14170976372436ec1d03a724d8d3f3925484ecf", "source": "security@apache.org", "tags": ["Patch"]}, {"url": "https://lists.apache.org/thread/yhy7tdfjf9hrl9vfrtzo8p2cyjq87v7n", "source": "security@apache.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FR34AIPXVTMB3XPRU5ULV5HHWPMRE33X/", "source": "security@apache.org"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MAGWEAJDWO2ACYATUQCPXLSYY5C3L3XU/", "source": "security@apache.org"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MWFYXLVBTBHNKYRXI572RFX7IJDDQGBL/", "source": "security@apache.org"}, {"url": "https://pypi.org/project/pyarrow-hotfix/", "source": "security@apache.org", "tags": ["Product"]}]}, "github_commit_url": "https://github.com/apache/arrow/commit/f14170976372436ec1d03a724d8d3f3925484ecf"}}