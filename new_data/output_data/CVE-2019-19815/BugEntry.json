{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * fs/f2fs/data.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/buffer_head.h>\n#include <linux/mpage.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/pagevec.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/prefetch.h>\n#include <linux/uio.h>\n#include <linux/cleancache.h>\n#include <linux/sched/signal.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\n#define NUM_PREALLOC_POST_READ_CTXS\t128\n\nstatic struct kmem_cache *bio_post_read_ctx_cache;\nstatic mempool_t *bio_post_read_ctx_pool;\n\nstatic bool __is_cp_guaranteed(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode;\n\tstruct f2fs_sb_info *sbi;\n\n\tif (!mapping)\n\t\treturn false;\n\n\tinode = mapping->host;\n\tsbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_META_INO(sbi) ||\n\t\t\tinode->i_ino ==  F2FS_NODE_INO(sbi) ||\n\t\t\tS_ISDIR(inode->i_mode) ||\n\t\t\t(S_ISREG(inode->i_mode) &&\n\t\t\t(f2fs_is_atomic_file(inode) || IS_NOQUOTA(inode))) ||\n\t\t\tis_cold_data(page))\n\t\treturn true;\n\treturn false;\n}\n\nstatic enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}\n\n/* postprocessing steps for read bios */\nenum bio_post_read_step {\n\tSTEP_INITIAL = 0,\n\tSTEP_DECRYPT,\n};\n\nstruct bio_post_read_ctx {\n\tstruct bio *bio;\n\tstruct work_struct work;\n\tunsigned int cur_step;\n\tunsigned int enabled_steps;\n};\n\nstatic void __read_end_io(struct bio *bio)\n{\n\tstruct page *page;\n\tstruct bio_vec *bv;\n\tstruct bvec_iter_all iter_all;\n\n\tbio_for_each_segment_all(bv, bio, iter_all) {\n\t\tpage = bv->bv_page;\n\n\t\t/* PG_error was set if any post_read step failed */\n\t\tif (bio->bi_status || PageError(page)) {\n\t\t\tClearPageUptodate(page);\n\t\t\t/* will re-read again later */\n\t\t\tClearPageError(page);\n\t\t} else {\n\t\t\tSetPageUptodate(page);\n\t\t}\n\t\tdec_page_count(F2FS_P_SB(page), __read_io_type(page));\n\t\tunlock_page(page);\n\t}\n\tif (bio->bi_private)\n\t\tmempool_free(bio->bi_private, bio_post_read_ctx_pool);\n\tbio_put(bio);\n}\n\nstatic void bio_post_read_processing(struct bio_post_read_ctx *ctx);\n\nstatic void decrypt_work(struct work_struct *work)\n{\n\tstruct bio_post_read_ctx *ctx =\n\t\tcontainer_of(work, struct bio_post_read_ctx, work);\n\n\tfscrypt_decrypt_bio(ctx->bio);\n\n\tbio_post_read_processing(ctx);\n}\n\nstatic void bio_post_read_processing(struct bio_post_read_ctx *ctx)\n{\n\tswitch (++ctx->cur_step) {\n\tcase STEP_DECRYPT:\n\t\tif (ctx->enabled_steps & (1 << STEP_DECRYPT)) {\n\t\t\tINIT_WORK(&ctx->work, decrypt_work);\n\t\t\tfscrypt_enqueue_decrypt_work(&ctx->work);\n\t\t\treturn;\n\t\t}\n\t\tctx->cur_step++;\n\t\t/* fall-through */\n\tdefault:\n\t\t__read_end_io(ctx->bio);\n\t}\n}\n\nstatic bool f2fs_bio_post_read_required(struct bio *bio)\n{\n\treturn bio->bi_private && !bio->bi_status;\n}\n\nstatic void f2fs_read_end_io(struct bio *bio)\n{\n\tif (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)),\n\t\t\t\t\t\tFAULT_READ_IO)) {\n\t\tf2fs_show_injection_info(FAULT_READ_IO);\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t}\n\n\tif (f2fs_bio_post_read_required(bio)) {\n\t\tstruct bio_post_read_ctx *ctx = bio->bi_private;\n\n\t\tctx->cur_step = STEP_INITIAL;\n\t\tbio_post_read_processing(ctx);\n\t\treturn;\n\t}\n\n\t__read_end_io(bio);\n}\n\nstatic void f2fs_write_end_io(struct bio *bio)\n{\n\tstruct f2fs_sb_info *sbi = bio->bi_private;\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tif (time_to_inject(sbi, FAULT_WRITE_IO)) {\n\t\tf2fs_show_injection_info(FAULT_WRITE_IO);\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t}\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct page *page = bvec->bv_page;\n\t\tenum count_type type = WB_DATA_TYPE(page);\n\n\t\tif (IS_DUMMY_WRITTEN_PAGE(page)) {\n\t\t\tset_page_private(page, (unsigned long)NULL);\n\t\t\tClearPagePrivate(page);\n\t\t\tunlock_page(page);\n\t\t\tmempool_free(page, sbi->write_io_dummy);\n\n\t\t\tif (unlikely(bio->bi_status))\n\t\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfscrypt_pullback_bio_page(&page, true);\n\n\t\tif (unlikely(bio->bi_status)) {\n\t\t\tmapping_set_error(page->mapping, -EIO);\n\t\t\tif (type == F2FS_WB_CP_DATA)\n\t\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t}\n\n\t\tf2fs_bug_on(sbi, page->mapping == NODE_MAPPING(sbi) &&\n\t\t\t\t\tpage->index != nid_of_node(page));\n\n\t\tdec_page_count(sbi, type);\n\t\tif (f2fs_in_warm_node_list(sbi, page))\n\t\t\tf2fs_del_fsync_node_entry(sbi, page);\n\t\tclear_cold_data(page);\n\t\tend_page_writeback(page);\n\t}\n\tif (!get_pages(sbi, F2FS_WB_CP_DATA) &&\n\t\t\t\twq_has_sleeper(&sbi->cp_wait))\n\t\twake_up(&sbi->cp_wait);\n\n\tbio_put(bio);\n}\n\n/*\n * Return true, if pre_bio's bdev is same as its target device.\n */\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tint i;\n\n\tif (f2fs_is_multi_device(sbi)) {\n\t\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\t\tif (FDEV(i).start_blk <= blk_addr &&\n\t\t\t    FDEV(i).end_blk >= blk_addr) {\n\t\t\t\tblk_addr -= FDEV(i).start_blk;\n\t\t\t\tbdev = FDEV(i).bdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (bio) {\n\t\tbio_set_dev(bio, bdev);\n\t\tbio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);\n\t}\n\treturn bdev;\n}\n\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn 0;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)\n\t\t\treturn i;\n\treturn 0;\n}\n\nstatic bool __same_bdev(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\tstruct block_device *b = f2fs_target_device(sbi, blk_addr, NULL);\n\treturn bio->bi_disk == b->bd_disk && bio->bi_partno == b->bd_partno;\n}\n\n/*\n * Low-level block read/write IO operations.\n */\nstatic struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,\n\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\tint npages, bool is_read,\n\t\t\t\tenum page_type type, enum temp_type temp)\n{\n\tstruct bio *bio;\n\n\tbio = f2fs_bio_alloc(sbi, npages, true);\n\n\tf2fs_target_device(sbi, blk_addr, bio);\n\tif (is_read) {\n\t\tbio->bi_end_io = f2fs_read_end_io;\n\t\tbio->bi_private = NULL;\n\t} else {\n\t\tbio->bi_end_io = f2fs_write_end_io;\n\t\tbio->bi_private = sbi;\n\t\tbio->bi_write_hint = f2fs_io_type_to_rw_hint(sbi, type, temp);\n\t}\n\tif (wbc)\n\t\twbc_init_bio(wbc, bio);\n\n\treturn bio;\n}\n\nstatic inline void __submit_bio(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct bio *bio, enum page_type type)\n{\n\tif (!is_read_io(bio_op(bio))) {\n\t\tunsigned int start;\n\n\t\tif (type != DATA && type != NODE)\n\t\t\tgoto submit_io;\n\n\t\tif (test_opt(sbi, LFS) && current->plug)\n\t\t\tblk_finish_plug(current->plug);\n\n\t\tstart = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;\n\t\tstart %= F2FS_IO_SIZE(sbi);\n\n\t\tif (start == 0)\n\t\t\tgoto submit_io;\n\n\t\t/* fill dummy pages */\n\t\tfor (; start < F2FS_IO_SIZE(sbi); start++) {\n\t\t\tstruct page *page =\n\t\t\t\tmempool_alloc(sbi->write_io_dummy,\n\t\t\t\t\t      GFP_NOIO | __GFP_NOFAIL);\n\t\t\tf2fs_bug_on(sbi, !page);\n\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tSetPagePrivate(page);\n\t\t\tset_page_private(page, (unsigned long)DUMMY_WRITTEN_PAGE);\n\t\t\tlock_page(page);\n\t\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n\t\t/*\n\t\t * In the NODE case, we lose next block address chain. So, we\n\t\t * need to do checkpoint in f2fs_sync_file.\n\t\t */\n\t\tif (type == NODE)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_CP);\n\t}\nsubmit_io:\n\tif (is_read_io(bio_op(bio)))\n\t\ttrace_f2fs_submit_read_bio(sbi->sb, type, bio);\n\telse\n\t\ttrace_f2fs_submit_write_bio(sbi->sb, type, bio);\n\tsubmit_bio(bio);\n}\n\nstatic void __submit_merged_bio(struct f2fs_bio_info *io)\n{\n\tstruct f2fs_io_info *fio = &io->fio;\n\n\tif (!io->bio)\n\t\treturn;\n\n\tbio_set_op_attrs(io->bio, fio->op, fio->op_flags);\n\n\tif (is_read_io(fio->op))\n\t\ttrace_f2fs_prepare_read_bio(io->sbi->sb, fio->type, io->bio);\n\telse\n\t\ttrace_f2fs_prepare_write_bio(io->sbi->sb, fio->type, io->bio);\n\n\t__submit_bio(io->sbi, io->bio, fio->type);\n\tio->bio = NULL;\n}\n\nstatic bool __has_merged_page(struct bio *bio, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino)\n{\n\tstruct bio_vec *bvec;\n\tstruct page *target;\n\tstruct bvec_iter_all iter_all;\n\n\tif (!bio)\n\t\treturn false;\n\n\tif (!inode && !page && !ino)\n\t\treturn true;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\n\t\tif (bvec->bv_page->mapping)\n\t\t\ttarget = bvec->bv_page;\n\t\telse\n\t\t\ttarget = fscrypt_control_page(bvec->bv_page);\n\n\t\tif (inode && inode == target->mapping->host)\n\t\t\treturn true;\n\t\tif (page && page == target)\n\t\t\treturn true;\n\t\tif (ino && ino == ino_of_node(target))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,\n\t\t\t\tenum page_type type, enum temp_type temp)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io = sbi->write_io[btype] + temp;\n\n\tdown_write(&io->io_rwsem);\n\n\t/* change META to META_FLUSH in the checkpoint procedure */\n\tif (type >= META_FLUSH) {\n\t\tio->fio.type = META_FLUSH;\n\t\tio->fio.op = REQ_OP_WRITE;\n\t\tio->fio.op_flags = REQ_META | REQ_PRIO | REQ_SYNC;\n\t\tif (!test_opt(sbi, NOBARRIER))\n\t\t\tio->fio.op_flags |= REQ_PREFLUSH | REQ_FUA;\n\t}\n\t__submit_merged_bio(io);\n\tup_write(&io->io_rwsem);\n}\n\nstatic void __submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, bool force)\n{\n\tenum temp_type temp;\n\tbool ret = true;\n\n\tfor (temp = HOT; temp < NR_TEMP_TYPE; temp++) {\n\t\tif (!force)\t{\n\t\t\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\t\t\tstruct f2fs_bio_info *io = sbi->write_io[btype] + temp;\n\n\t\t\tdown_read(&io->io_rwsem);\n\t\t\tret = __has_merged_page(io->bio, inode, page, ino);\n\t\t\tup_read(&io->io_rwsem);\n\t\t}\n\t\tif (ret)\n\t\t\t__f2fs_submit_merged_write(sbi, type, temp);\n\n\t\t/* TODO: use HOT temp only for meta pages now. */\n\t\tif (type >= META)\n\t\t\tbreak;\n\t}\n}\n\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)\n{\n\t__submit_merged_write_cond(sbi, NULL, NULL, 0, type, true);\n}\n\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type)\n{\n\t__submit_merged_write_cond(sbi, inode, page, ino, type, false);\n}\n\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)\n{\n\tf2fs_submit_merged_write(sbi, DATA);\n\tf2fs_submit_merged_write(sbi, NODE);\n\tf2fs_submit_merged_write(sbi, META);\n}\n\n/*\n * Fill the locked page with data located in the block address.\n * A caller needs to unlock the page on failure.\n */\nint f2fs_submit_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,\n\t\t\tfio->is_por ? META_POR : (__is_meta_io(fio) ?\n\t\t\tMETA_GENERIC : DATA_GENERIC_ENHANCE)))\n\t\treturn -EFSCORRUPTED;\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\tf2fs_trace_ios(fio, 0);\n\n\t/* Allocate a new bio */\n\tbio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,\n\t\t\t\t1, is_read_io(fio->op), fio->type, fio->temp);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\n\tif (fio->io_wbc && !is_read_io(fio->op))\n\t\twbc_account_io(fio->io_wbc, page, PAGE_SIZE);\n\n\tbio_set_op_attrs(bio, fio->op, fio->op_flags);\n\n\tinc_page_count(fio->sbi, is_read_io(fio->op) ?\n\t\t\t__read_io_type(page): WB_DATA_TYPE(fio->page));\n\n\t__submit_bio(fio->sbi, bio, fio->type);\n\treturn 0;\n}\n\nint f2fs_merge_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio = *fio->bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,\n\t\t\t__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC))\n\t\treturn -EFSCORRUPTED;\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\tf2fs_trace_ios(fio, 0);\n\n\tif (bio && (*fio->last_block + 1 != fio->new_blkaddr ||\n\t\t\t!__same_bdev(fio->sbi, fio->new_blkaddr, bio))) {\n\t\t__submit_bio(fio->sbi, bio, fio->type);\n\t\tbio = NULL;\n\t}\nalloc_new:\n\tif (!bio) {\n\t\tbio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,\n\t\t\t\tBIO_MAX_PAGES, false, fio->type, fio->temp);\n\t\tbio_set_op_attrs(bio, fio->op, fio->op_flags);\n\t}\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t__submit_bio(fio->sbi, bio, fio->type);\n\t\tbio = NULL;\n\t\tgoto alloc_new;\n\t}\n\n\tif (fio->io_wbc)\n\t\twbc_account_io(fio->io_wbc, page, PAGE_SIZE);\n\n\tinc_page_count(fio->sbi, WB_DATA_TYPE(page));\n\n\t*fio->last_block = fio->new_blkaddr;\n\t*fio->bio = bio;\n\n\treturn 0;\n}\n\nstatic void f2fs_submit_ipu_bio(struct f2fs_sb_info *sbi, struct bio **bio,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\tif (!bio)\n\t\treturn;\n\n\tif (!__has_merged_page(*bio, NULL, page, 0))\n\t\treturn;\n\n\t__submit_bio(sbi, *bio, DATA);\n\t*bio = NULL;\n}\n\nvoid f2fs_submit_page_write(struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tenum page_type btype = PAGE_TYPE_OF_BIO(fio->type);\n\tstruct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;\n\tstruct page *bio_page;\n\n\tf2fs_bug_on(sbi, is_read_io(fio->op));\n\n\tdown_write(&io->io_rwsem);\nnext:\n\tif (fio->in_list) {\n\t\tspin_lock(&io->io_lock);\n\t\tif (list_empty(&io->io_list)) {\n\t\t\tspin_unlock(&io->io_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tfio = list_first_entry(&io->io_list,\n\t\t\t\t\t\tstruct f2fs_io_info, list);\n\t\tlist_del(&fio->list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tverify_fio_blkaddr(fio);\n\n\tbio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;\n\n\t/* set submitted = true as a return value */\n\tfio->submitted = true;\n\n\tinc_page_count(sbi, WB_DATA_TYPE(bio_page));\n\n\tif (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||\n\t    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||\n\t\t\t!__same_bdev(sbi, fio->new_blkaddr, io->bio)))\n\t\t__submit_merged_bio(io);\nalloc_new:\n\tif (io->bio == NULL) {\n\t\tif ((fio->type == DATA || fio->type == NODE) &&\n\t\t\t\tfio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {\n\t\t\tdec_page_count(sbi, WB_DATA_TYPE(bio_page));\n\t\t\tfio->retry = true;\n\t\t\tgoto skip;\n\t\t}\n\t\tio->bio = __bio_alloc(sbi, fio->new_blkaddr, fio->io_wbc,\n\t\t\t\t\t\tBIO_MAX_PAGES, false,\n\t\t\t\t\t\tfio->type, fio->temp);\n\t\tio->fio = *fio;\n\t}\n\n\tif (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t__submit_merged_bio(io);\n\t\tgoto alloc_new;\n\t}\n\n\tif (fio->io_wbc)\n\t\twbc_account_io(fio->io_wbc, bio_page, PAGE_SIZE);\n\n\tio->last_block_in_bio = fio->new_blkaddr;\n\tf2fs_trace_ios(fio, 0);\n\n\ttrace_f2fs_submit_page_write(fio->page, fio);\nskip:\n\tif (fio->in_list)\n\t\tgoto next;\nout:\n\tif (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||\n\t\t\t\tf2fs_is_checkpoint_ready(sbi))\n\t\t__submit_merged_bio(io);\n\tup_write(&io->io_rwsem);\n}\n\nstatic struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\t}\n\n\treturn bio;\n}\n\n/* This can handle encryption stuffs */\nstatic int f2fs_submit_page_read(struct inode *inode, struct page *page,\n\t\t\t\t\t\t\tblock_t blkaddr)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\n\tbio = f2fs_grab_read_bio(inode, blkaddr, 1, 0);\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\t/* wait for GCed page writeback via META_MAPPING */\n\tf2fs_wait_on_block_writeback(inode, blkaddr);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\tClearPageError(page);\n\tinc_page_count(sbi, F2FS_RD_DATA);\n\t__submit_bio(sbi, bio, DATA);\n\treturn 0;\n}\n\nstatic void __set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tstruct f2fs_node *rn = F2FS_NODE(dn->node_page);\n\t__le32 *addr_array;\n\tint base = 0;\n\n\tif (IS_INODE(dn->node_page) && f2fs_has_extra_attr(dn->inode))\n\t\tbase = get_extra_isize(dn->inode);\n\n\t/* Get physical address of data block */\n\taddr_array = blkaddr_in_node(rn);\n\taddr_array[base + dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);\n}\n\n/*\n * Lock ordering for the change of data block address:\n * ->data_page\n *  ->node_page\n *    update block addresses in the node page\n */\nvoid f2fs_set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);\n\t__set_data_blkaddr(dn);\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n}\n\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)\n{\n\tdn->data_blkaddr = blkaddr;\n\tf2fs_set_data_blkaddr(dn);\n\tf2fs_update_extent_cache(dn);\n}\n\n/* dn->ofs_in_node will be returned with up-to-date last block pointer */\nint f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tint err;\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\tif (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))\n\t\treturn err;\n\n\ttrace_f2fs_reserve_new_blocks(dn->inode, dn->nid,\n\t\t\t\t\t\tdn->ofs_in_node, count);\n\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);\n\n\tfor (; count > 0; dn->ofs_in_node++) {\n\t\tblock_t blkaddr = datablock_addr(dn->inode,\n\t\t\t\t\tdn->node_page, dn->ofs_in_node);\n\t\tif (blkaddr == NULL_ADDR) {\n\t\t\tdn->data_blkaddr = NEW_ADDR;\n\t\t\t__set_data_blkaddr(dn);\n\t\t\tcount--;\n\t\t}\n\t}\n\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n\treturn 0;\n}\n\n/* Should keep dn->ofs_in_node unchanged */\nint f2fs_reserve_new_block(struct dnode_of_data *dn)\n{\n\tunsigned int ofs_in_node = dn->ofs_in_node;\n\tint ret;\n\n\tret = f2fs_reserve_new_blocks(dn, 1);\n\tdn->ofs_in_node = ofs_in_node;\n\treturn ret;\n}\n\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tbool need_put = dn->inode_page ? false : true;\n\tint err;\n\n\terr = f2fs_get_dnode_of_data(dn, index, ALLOC_NODE);\n\tif (err)\n\t\treturn err;\n\n\tif (dn->data_blkaddr == NULL_ADDR)\n\t\terr = f2fs_reserve_new_block(dn);\n\tif (err || need_put)\n\t\tf2fs_put_dnode(dn);\n\treturn err;\n}\n\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tstruct extent_info ei  = {0,0,0};\n\tstruct inode *inode = dn->inode;\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn->data_blkaddr = ei.blk + index - ei.fofs;\n\t\treturn 0;\n\t}\n\n\treturn f2fs_reserve_block(dn, index);\n}\n\nstruct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\tint op_flags, bool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct dnode_of_data dn;\n\tstruct page *page;\n\tstruct extent_info ei = {0,0,0};\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, for_write);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), dn.data_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto put_err;\n\t\t}\n\t\tgoto got_it;\n\t}\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto put_err;\n\tf2fs_put_dnode(&dn);\n\n\tif (unlikely(dn.data_blkaddr == NULL_ADDR)) {\n\t\terr = -ENOENT;\n\t\tgoto put_err;\n\t}\n\tif (dn.data_blkaddr != NEW_ADDR &&\n\t\t\t!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),\n\t\t\t\t\t\tdn.data_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tgoto put_err;\n\t}\ngot_it:\n\tif (PageUptodate(page)) {\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\t/*\n\t * A new dentry page is allocated but not able to be written, since its\n\t * new inode page couldn't be allocated due to -ENOSPC.\n\t * In such the case, its blkaddr can be remained as NEW_ADDR.\n\t * see, f2fs_add_link -> f2fs_get_new_data_page ->\n\t * f2fs_init_inode_metadata.\n\t */\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\terr = f2fs_submit_page_read(inode, page, dn.data_blkaddr);\n\tif (err)\n\t\tgoto put_err;\n\treturn page;\n\nput_err:\n\tf2fs_put_page(page, 1);\n\treturn ERR_PTR(err);\n}\n\nstruct page *f2fs_find_data_page(struct inode *inode, pgoff_t index)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tpage = find_get_page(mapping, index);\n\tif (page && PageUptodate(page))\n\t\treturn page;\n\tf2fs_put_page(page, 0);\n\n\tpage = f2fs_get_read_data_page(inode, index, 0, false);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\tif (PageUptodate(page))\n\t\treturn page;\n\n\twait_on_page_locked(page);\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 0);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * If it tries to access a hole, return an error.\n * Because, the callers, functions in dir.c and GC, should be able to know\n * whether this page exists or not.\n */\nstruct page *f2fs_get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\t\tbool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\nrepeat:\n\tpage = f2fs_get_read_data_page(inode, index, 0, for_write);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\t/* wait for read completion */\n\tlock_page(page);\n\tif (unlikely(page->mapping != mapping)) {\n\t\tf2fs_put_page(page, 1);\n\t\tgoto repeat;\n\t}\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * Caller ensures that this data page is never allocated.\n * A new zero-filled data page is allocated in the page cache.\n *\n * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and\n * f2fs_unlock_op().\n * Note that, ipage is set only by make_empty_dir, and if any error occur,\n * ipage should be released by this function.\n */\nstruct page *f2fs_get_new_data_page(struct inode *inode,\n\t\tstruct page *ipage, pgoff_t index, bool new_i_size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tstruct dnode_of_data dn;\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, true);\n\tif (!page) {\n\t\t/*\n\t\t * before exiting, we should make sure ipage will be released\n\t\t * if any error occur.\n\t\t */\n\t\tf2fs_put_page(ipage, 1);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, NULL, 0);\n\terr = f2fs_reserve_block(&dn, index);\n\tif (err) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(err);\n\t}\n\tif (!ipage)\n\t\tf2fs_put_dnode(&dn);\n\n\tif (PageUptodate(page))\n\t\tgoto got_it;\n\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t} else {\n\t\tf2fs_put_page(page, 1);\n\n\t\t/* if ipage exists, blkaddr should be NEW_ADDR */\n\t\tf2fs_bug_on(F2FS_I_SB(inode), ipage);\n\t\tpage = f2fs_get_lock_data_page(inode, index, true);\n\t\tif (IS_ERR(page))\n\t\t\treturn page;\n\t}\ngot_it:\n\tif (new_i_size && i_size_read(inode) <\n\t\t\t\t((loff_t)(index + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));\n\treturn page;\n}\n\nstatic int __allocate_data_block(struct dnode_of_data *dn, int seg_type)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\tblock_t old_blkaddr;\n\tblkcnt_t count = 1;\n\tint err;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\n\terr = f2fs_get_node_info(sbi, dn->nid, &ni);\n\tif (err)\n\t\treturn err;\n\n\tdn->data_blkaddr = datablock_addr(dn->inode,\n\t\t\t\tdn->node_page, dn->ofs_in_node);\n\tif (dn->data_blkaddr != NULL_ADDR)\n\t\tgoto alloc;\n\n\tif (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))\n\t\treturn err;\n\nalloc:\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\told_blkaddr = dn->data_blkaddr;\n\tf2fs_allocate_data_block(sbi, NULL, old_blkaddr, &dn->data_blkaddr,\n\t\t\t\t\t&sum, seg_type, NULL, false);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tinvalidate_mapping_pages(META_MAPPING(sbi),\n\t\t\t\t\told_blkaddr, old_blkaddr);\n\tf2fs_set_data_blkaddr(dn);\n\n\t/*\n\t * i_size will be updated by direct_IO. Otherwise, we'll get stale\n\t * data from unwritten block via dio_read.\n\t */\n\treturn 0;\n}\n\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct f2fs_map_blocks map;\n\tint flag;\n\tint err = 0;\n\tbool direct_io = iocb->ki_flags & IOCB_DIRECT;\n\n\t/* convert inline data for Direct I/O*/\n\tif (direct_io) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (direct_io && allow_outplace_dio(inode, iocb, from))\n\t\treturn 0;\n\n\tif (is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\tmap.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);\n\tmap.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));\n\tif (map.m_len > map.m_lblk)\n\t\tmap.m_len -= map.m_lblk;\n\telse\n\t\tmap.m_len = 0;\n\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = true;\n\n\tif (direct_io) {\n\t\tmap.m_seg_type = f2fs_rw_hint_to_seg_type(iocb->ki_hint);\n\t\tflag = f2fs_force_buffered_io(inode, iocb, from) ?\n\t\t\t\t\tF2FS_GET_BLOCK_PRE_AIO :\n\t\t\t\t\tF2FS_GET_BLOCK_PRE_DIO;\n\t\tgoto map_blocks;\n\t}\n\tif (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA(inode)) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (f2fs_has_inline_data(inode))\n\t\treturn err;\n\n\tflag = F2FS_GET_BLOCK_PRE_AIO;\n\nmap_blocks:\n\terr = f2fs_map_blocks(inode, &map, 1, flag);\n\tif (map.m_len > 0 && err == -ENOSPC) {\n\t\tif (!direct_io)\n\t\t\tset_inode_flag(inode, FI_NO_PREALLOC);\n\t\terr = 0;\n\t}\n\treturn err;\n}\n\nvoid __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock)\n{\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO) {\n\t\tif (lock)\n\t\t\tdown_read(&sbi->node_change);\n\t\telse\n\t\t\tup_read(&sbi->node_change);\n\t} else {\n\t\tif (lock)\n\t\t\tf2fs_lock_op(sbi);\n\t\telse\n\t\t\tf2fs_unlock_op(sbi);\n\t}\n}\n\n/*\n * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with\n * f2fs_map_blocks structure.\n * If original data blocks are allocated, then give them to blockdev.\n * Otherwise,\n *     a. preallocate requested block addresses\n *     b. do not use extent cache for better performance\n *     c. give the block addresses to blockdev\n */\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\t\t\t\tint create, int flag)\n{\n\tunsigned int maxblocks = map->m_len;\n\tstruct dnode_of_data dn;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint mode = map->m_may_create ? ALLOC_NODE : LOOKUP_NODE;\n\tpgoff_t pgofs, end_offset, end;\n\tint err = 0, ofs = 1;\n\tunsigned int ofs_in_node, last_ofs_in_node;\n\tblkcnt_t prealloc;\n\tstruct extent_info ei = {0,0,0};\n\tblock_t blkaddr;\n\tunsigned int start_pgofs;\n\n\tif (!maxblocks)\n\t\treturn 0;\n\n\tmap->m_len = 0;\n\tmap->m_flags = 0;\n\n\t/* it only supports block size == page size */\n\tpgofs =\t(pgoff_t)map->m_lblk;\n\tend = pgofs + maxblocks;\n\n\tif (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {\n\t\tif (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&\n\t\t\t\t\t\t\tmap->m_may_create)\n\t\t\tgoto next_dnode;\n\n\t\tmap->m_pblk = ei.blk + pgofs - ei.fofs;\n\t\tmap->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);\n\t\tmap->m_flags = F2FS_MAP_MAPPED;\n\t\tif (map->m_next_extent)\n\t\t\t*map->m_next_extent = pgofs + map->m_len;\n\n\t\t/* for hardware encryption, but to avoid potential issue in future */\n\t\tif (flag == F2FS_GET_BLOCK_DIO)\n\t\t\tf2fs_wait_on_block_writeback_range(inode,\n\t\t\t\t\t\tmap->m_pblk, map->m_len);\n\t\tgoto out;\n\t}\n\nnext_dnode:\n\tif (map->m_may_create)\n\t\t__do_map_lock(sbi, flag, true);\n\n\t/* When reading holes, we need its node page */\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, pgofs, mode);\n\tif (err) {\n\t\tif (flag == F2FS_GET_BLOCK_BMAP)\n\t\t\tmap->m_pblk = 0;\n\t\tif (err == -ENOENT) {\n\t\t\terr = 0;\n\t\t\tif (map->m_next_pgofs)\n\t\t\t\t*map->m_next_pgofs =\n\t\t\t\t\tf2fs_get_next_page_offset(&dn, pgofs);\n\t\t\tif (map->m_next_extent)\n\t\t\t\t*map->m_next_extent =\n\t\t\t\t\tf2fs_get_next_page_offset(&dn, pgofs);\n\t\t}\n\t\tgoto unlock_out;\n\t}\n\n\tstart_pgofs = pgofs;\n\tprealloc = 0;\n\tlast_ofs_in_node = ofs_in_node = dn.ofs_in_node;\n\tend_offset = ADDRS_PER_PAGE(dn.node_page, inode);\n\nnext_block:\n\tblkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);\n\n\tif (__is_valid_data_blkaddr(blkaddr) &&\n\t\t!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tgoto sync_out;\n\t}\n\n\tif (__is_valid_data_blkaddr(blkaddr)) {\n\t\t/* use out-place-update for driect IO under LFS mode */\n\t\tif (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&\n\t\t\t\t\t\t\tmap->m_may_create) {\n\t\t\terr = __allocate_data_block(&dn, map->m_seg_type);\n\t\t\tif (!err) {\n\t\t\t\tblkaddr = dn.data_blkaddr;\n\t\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (create) {\n\t\t\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_PRE_AIO) {\n\t\t\t\tif (blkaddr == NULL_ADDR) {\n\t\t\t\t\tprealloc++;\n\t\t\t\t\tlast_ofs_in_node = dn.ofs_in_node;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tWARN_ON(flag != F2FS_GET_BLOCK_PRE_DIO &&\n\t\t\t\t\tflag != F2FS_GET_BLOCK_DIO);\n\t\t\t\terr = __allocate_data_block(&dn,\n\t\t\t\t\t\t\tmap->m_seg_type);\n\t\t\t\tif (!err)\n\t\t\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tgoto sync_out;\n\t\t\tmap->m_flags |= F2FS_MAP_NEW;\n\t\t\tblkaddr = dn.data_blkaddr;\n\t\t} else {\n\t\t\tif (flag == F2FS_GET_BLOCK_BMAP) {\n\t\t\t\tmap->m_pblk = 0;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_PRECACHE)\n\t\t\t\tgoto sync_out;\n\t\t\tif (flag == F2FS_GET_BLOCK_FIEMAP &&\n\t\t\t\t\t\tblkaddr == NULL_ADDR) {\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag != F2FS_GET_BLOCK_FIEMAP) {\n\t\t\t\t/* for defragment case */\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tgoto skip;\n\n\tif (map->m_len == 0) {\n\t\t/* preallocated unwritten block should be mapped for fiemap. */\n\t\tif (blkaddr == NEW_ADDR)\n\t\t\tmap->m_flags |= F2FS_MAP_UNWRITTEN;\n\t\tmap->m_flags |= F2FS_MAP_MAPPED;\n\n\t\tmap->m_pblk = blkaddr;\n\t\tmap->m_len = 1;\n\t} else if ((map->m_pblk != NEW_ADDR &&\n\t\t\tblkaddr == (map->m_pblk + ofs)) ||\n\t\t\t(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||\n\t\t\tflag == F2FS_GET_BLOCK_PRE_DIO) {\n\t\tofs++;\n\t\tmap->m_len++;\n\t} else {\n\t\tgoto sync_out;\n\t}\n\nskip:\n\tdn.ofs_in_node++;\n\tpgofs++;\n\n\t/* preallocate blocks in batch for one dnode page */\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO &&\n\t\t\t(pgofs == end || dn.ofs_in_node == end_offset)) {\n\n\t\tdn.ofs_in_node = ofs_in_node;\n\t\terr = f2fs_reserve_new_blocks(&dn, prealloc);\n\t\tif (err)\n\t\t\tgoto sync_out;\n\n\t\tmap->m_len += dn.ofs_in_node - ofs_in_node;\n\t\tif (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto sync_out;\n\t\t}\n\t\tdn.ofs_in_node = end_offset;\n\t}\n\n\tif (pgofs >= end)\n\t\tgoto sync_out;\n\telse if (dn.ofs_in_node < end_offset)\n\t\tgoto next_block;\n\n\tif (flag == F2FS_GET_BLOCK_PRECACHE) {\n\t\tif (map->m_flags & F2FS_MAP_MAPPED) {\n\t\t\tunsigned int ofs = start_pgofs - map->m_lblk;\n\n\t\t\tf2fs_update_extent_cache_range(&dn,\n\t\t\t\tstart_pgofs, map->m_pblk + ofs,\n\t\t\t\tmap->m_len - ofs);\n\t\t}\n\t}\n\n\tf2fs_put_dnode(&dn);\n\n\tif (map->m_may_create) {\n\t\t__do_map_lock(sbi, flag, false);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\n\tgoto next_dnode;\n\nsync_out:\n\n\t/* for hardware encryption, but to avoid potential issue in future */\n\tif (flag == F2FS_GET_BLOCK_DIO && map->m_flags & F2FS_MAP_MAPPED)\n\t\tf2fs_wait_on_block_writeback_range(inode,\n\t\t\t\t\t\tmap->m_pblk, map->m_len);\n\n\tif (flag == F2FS_GET_BLOCK_PRECACHE) {\n\t\tif (map->m_flags & F2FS_MAP_MAPPED) {\n\t\t\tunsigned int ofs = start_pgofs - map->m_lblk;\n\n\t\t\tf2fs_update_extent_cache_range(&dn,\n\t\t\t\tstart_pgofs, map->m_pblk + ofs,\n\t\t\t\tmap->m_len - ofs);\n\t\t}\n\t\tif (map->m_next_extent)\n\t\t\t*map->m_next_extent = pgofs + 1;\n\t}\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (map->m_may_create) {\n\t\t__do_map_lock(sbi, flag, false);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\nout:\n\ttrace_f2fs_map_blocks(inode, map, err);\n\treturn err;\n}\n\nbool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len)\n{\n\tstruct f2fs_map_blocks map;\n\tblock_t last_lblk;\n\tint err;\n\n\tif (pos + len > i_size_read(inode))\n\t\treturn false;\n\n\tmap.m_lblk = F2FS_BYTES_TO_BLK(pos);\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\tlast_lblk = F2FS_BLK_ALIGN(pos + len);\n\n\twhile (map.m_lblk < last_lblk) {\n\t\tmap.m_len = last_lblk - map.m_lblk;\n\t\terr = f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_DEFAULT);\n\t\tif (err || map.m_len == 0)\n\t\t\treturn false;\n\t\tmap.m_lblk += map.m_len;\n\t}\n\treturn true;\n}\n\nstatic int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs, int seg_type, bool may_write)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = seg_type;\n\tmap.m_may_create = may_write;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = (u64)map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}\n\nstatic int get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\t\tflag, next_pgofs,\n\t\t\t\t\t\t\tNO_CHECK_TYPE, create);\n}\n\nstatic int get_data_block_dio_write(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\tF2FS_GET_BLOCK_DIO, NULL,\n\t\t\t\tf2fs_rw_hint_to_seg_type(inode->i_write_hint),\n\t\t\t\ttrue);\n}\n\nstatic int get_data_block_dio(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\tF2FS_GET_BLOCK_DIO, NULL,\n\t\t\t\tf2fs_rw_hint_to_seg_type(inode->i_write_hint),\n\t\t\t\tfalse);\n}\n\nstatic int get_data_block_bmap(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\t/* Block number less than F2FS MAX BLOCKS */\n\tif (unlikely(iblock >= F2FS_I_SB(inode)->max_file_blocks))\n\t\treturn -EFBIG;\n\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\tF2FS_GET_BLOCK_BMAP, NULL,\n\t\t\t\t\t\tNO_CHECK_TYPE, create);\n}\n\nstatic inline sector_t logical_to_blk(struct inode *inode, loff_t offset)\n{\n\treturn (offset >> inode->i_blkbits);\n}\n\nstatic inline loff_t blk_to_logical(struct inode *inode, sector_t blk)\n{\n\treturn (blk << inode->i_blkbits);\n}\n\nstatic int f2fs_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page;\n\tstruct node_info ni;\n\t__u64 phys = 0, len;\n\t__u32 flags;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tint err = 0;\n\n\tif (f2fs_has_inline_xattr(inode)) {\n\t\tint offset;\n\n\t\tpage = f2fs_grab_cache_page(NODE_MAPPING(sbi),\n\t\t\t\t\t\tinode->i_ino, false);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\terr = f2fs_get_node_info(sbi, inode->i_ino, &ni);\n\t\tif (err) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn err;\n\t\t}\n\n\t\tphys = (__u64)blk_to_logical(inode, ni.blk_addr);\n\t\toffset = offsetof(struct f2fs_inode, i_addr) +\n\t\t\t\t\tsizeof(__le32) * (DEF_ADDRS_PER_INODE -\n\t\t\t\t\tget_inline_xattr_addrs(inode));\n\n\t\tphys += offset;\n\t\tlen = inline_xattr_size(inode);\n\n\t\tf2fs_put_page(page, 1);\n\n\t\tflags = FIEMAP_EXTENT_DATA_INLINE | FIEMAP_EXTENT_NOT_ALIGNED;\n\n\t\tif (!xnid)\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\t\terr = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);\n\t\tif (err || err == 1)\n\t\t\treturn err;\n\t}\n\n\tif (xnid) {\n\t\tpage = f2fs_grab_cache_page(NODE_MAPPING(sbi), xnid, false);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\terr = f2fs_get_node_info(sbi, xnid, &ni);\n\t\tif (err) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn err;\n\t\t}\n\n\t\tphys = (__u64)blk_to_logical(inode, ni.blk_addr);\n\t\tlen = inode->i_sb->s_blocksize;\n\n\t\tf2fs_put_page(page, 1);\n\n\t\tflags = FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (phys)\n\t\terr = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);\n\n\treturn (err < 0 ? err : 0);\n}\n\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\tu64 start, u64 len)\n{\n\tstruct buffer_head map_bh;\n\tsector_t start_blk, last_blk;\n\tpgoff_t next_pgofs;\n\tu64 logical = 0, phys = 0, size = 0;\n\tu32 flags = 0;\n\tint ret = 0;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\tret = f2fs_precache_extents(inode);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\tret = f2fs_xattr_fiemap(inode, fieinfo);\n\t\tgoto out;\n\t}\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);\n\t\tif (ret != -EAGAIN)\n\t\t\tgoto out;\n\t}\n\n\tif (logical_to_blk(inode, len) == 0)\n\t\tlen = blk_to_logical(inode, 1);\n\n\tstart_blk = logical_to_blk(inode, start);\n\tlast_blk = logical_to_blk(inode, start + len - 1);\n\nnext:\n\tmemset(&map_bh, 0, sizeof(struct buffer_head));\n\tmap_bh.b_size = len;\n\n\tret = get_data_block(inode, start_blk, &map_bh, 0,\n\t\t\t\t\tF2FS_GET_BLOCK_FIEMAP, &next_pgofs);\n\tif (ret)\n\t\tgoto out;\n\n\t/* HOLE */\n\tif (!buffer_mapped(&map_bh)) {\n\t\tstart_blk = next_pgofs;\n\n\t\tif (blk_to_logical(inode, start_blk) < blk_to_logical(inode,\n\t\t\t\t\tF2FS_I_SB(inode)->max_file_blocks))\n\t\t\tgoto prep_next;\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (size) {\n\t\tif (IS_ENCRYPTED(inode))\n\t\t\tflags |= FIEMAP_EXTENT_DATA_ENCRYPTED;\n\n\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\tphys, size, flags);\n\t}\n\n\tif (start_blk > last_blk || ret)\n\t\tgoto out;\n\n\tlogical = blk_to_logical(inode, start_blk);\n\tphys = blk_to_logical(inode, map_bh.b_blocknr);\n\tsize = map_bh.b_size;\n\tflags = 0;\n\tif (buffer_unwritten(&map_bh))\n\t\tflags = FIEMAP_EXTENT_UNWRITTEN;\n\n\tstart_blk += logical_to_blk(inode, size);\n\nprep_next:\n\tcond_resched();\n\tif (fatal_signal_pending(current))\n\t\tret = -EINTR;\n\telse\n\t\tgoto next;\nout:\n\tif (ret == 1)\n\t\tret = 0;\n\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nstatic int f2fs_read_single_page(struct inode *inode, struct page *page,\n\t\t\t\t\tunsigned nr_pages,\n\t\t\t\t\tstruct f2fs_map_blocks *map,\n\t\t\t\t\tstruct bio **bio_ret,\n\t\t\t\t\tsector_t *last_block_in_bio,\n\t\t\t\t\tbool is_readahead)\n{\n\tstruct bio *bio = *bio_ret;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tint ret = 0;\n\n\tblock_in_file = (sector_t)page->index;\n\tlast_block = block_in_file + nr_pages;\n\tlast_block_in_file = (i_size_read(inode) + blocksize - 1) >>\n\t\t\t\t\t\t\tblkbits;\n\tif (last_block > last_block_in_file)\n\t\tlast_block = last_block_in_file;\n\n\t/* just zeroing out page which is beyond EOF */\n\tif (block_in_file >= last_block)\n\t\tgoto zero_out;\n\t/*\n\t * Map blocks using the previous result first.\n\t */\n\tif ((map->m_flags & F2FS_MAP_MAPPED) &&\n\t\t\tblock_in_file > map->m_lblk &&\n\t\t\tblock_in_file < (map->m_lblk + map->m_len))\n\t\tgoto got_it;\n\n\t/*\n\t * Then do more f2fs_map_blocks() calls until we are\n\t * done with this page.\n\t */\n\tmap->m_lblk = block_in_file;\n\tmap->m_len = last_block - block_in_file;\n\n\tret = f2fs_map_blocks(inode, map, 0, F2FS_GET_BLOCK_DEFAULT);\n\tif (ret)\n\t\tgoto out;\ngot_it:\n\tif ((map->m_flags & F2FS_MAP_MAPPED)) {\n\t\tblock_nr = map->m_pblk + block_in_file - map->m_lblk;\n\t\tSetPageMappedToDisk(page);\n\n\t\tif (!PageUptodate(page) && !cleancache_get_page(page)) {\n\t\t\tSetPageUptodate(page);\n\t\t\tgoto confused;\n\t\t}\n\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t} else {\nzero_out:\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This page will go to BIO.  Do we need to send this\n\t * BIO off first?\n\t */\n\tif (bio && (*last_block_in_bio != block_nr - 1 ||\n\t\t!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {\nsubmit_and_realloc:\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tif (bio == NULL) {\n\t\tbio = f2fs_grab_read_bio(inode, block_nr, nr_pages,\n\t\t\t\tis_readahead ? REQ_RAHEAD : 0);\n\t\tif (IS_ERR(bio)) {\n\t\t\tret = PTR_ERR(bio);\n\t\t\tbio = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If the page is under writeback, we need to wait for\n\t * its completion to see the correct decrypted data.\n\t */\n\tf2fs_wait_on_block_writeback(inode, block_nr);\n\n\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\tgoto submit_and_realloc;\n\n\tinc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);\n\tClearPageError(page);\n\t*last_block_in_bio = block_nr;\n\tgoto out;\nconfused:\n\tif (bio) {\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tunlock_page(page);\nout:\n\t*bio_ret = bio;\n\treturn ret;\n}\n\n/*\n * This function was originally taken from fs/mpage.c, and customized for f2fs.\n * Major change was from block_size == page_size in f2fs by default.\n *\n * Note that the aops->readpages() function is ONLY used for read-ahead. If\n * this function ever deviates from doing just read-ahead, it should either\n * use ->readpage() or do the necessary surgery to decouple ->readpages()\n * from read-ahead.\n */\nstatic int f2fs_mpage_readpages(struct address_space *mapping,\n\t\t\tstruct list_head *pages, struct page *page,\n\t\t\tunsigned nr_pages, bool is_readahead)\n{\n\tstruct bio *bio = NULL;\n\tsector_t last_block_in_bio = 0;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_map_blocks map;\n\tint ret = 0;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\n\tfor (; nr_pages; nr_pages--) {\n\t\tif (pages) {\n\t\t\tpage = list_last_entry(pages, struct page, lru);\n\n\t\t\tprefetchw(&page->flags);\n\t\t\tlist_del(&page->lru);\n\t\t\tif (add_to_page_cache_lru(page, mapping,\n\t\t\t\t\t\t  page->index,\n\t\t\t\t\t\t  readahead_gfp_mask(mapping)))\n\t\t\t\tgoto next_page;\n\t\t}\n\n\t\tret = f2fs_read_single_page(inode, page, nr_pages, &map, &bio,\n\t\t\t\t\t&last_block_in_bio, is_readahead);\n\t\tif (ret) {\n\t\t\tSetPageError(page);\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tunlock_page(page);\n\t\t}\nnext_page:\n\t\tif (pages)\n\t\t\tput_page(page);\n\t}\n\tBUG_ON(pages && !list_empty(pages));\n\tif (bio)\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn pages ? 0 : ret;\n}\n\nstatic int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);\n\treturn ret;\n}\n\nstatic int f2fs_read_data_pages(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct page *page = list_last_entry(pages, struct page, lru);\n\n\ttrace_f2fs_readpages(inode, page, nr_pages);\n\n\t/* If the file has inline data, skip readpages */\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\treturn f2fs_mpage_readpages(mapping, pages, NULL, nr_pages, true);\n}\n\nstatic int encrypt_one_page(struct f2fs_io_info *fio)\n{\n\tstruct inode *inode = fio->page->mapping->host;\n\tstruct page *mpage;\n\tgfp_t gfp_flags = GFP_NOFS;\n\n\tif (!f2fs_encrypted_file(inode))\n\t\treturn 0;\n\n\t/* wait for GCed page writeback via META_MAPPING */\n\tf2fs_wait_on_block_writeback(inode, fio->old_blkaddr);\n\nretry_encrypt:\n\tfio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,\n\t\t\tPAGE_SIZE, 0, fio->page->index, gfp_flags);\n\tif (IS_ERR(fio->encrypted_page)) {\n\t\t/* flush pending IOs and wait for a while in the ENOMEM case */\n\t\tif (PTR_ERR(fio->encrypted_page) == -ENOMEM) {\n\t\t\tf2fs_flush_merged_writes(fio->sbi);\n\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\tgfp_flags |= __GFP_NOFAIL;\n\t\t\tgoto retry_encrypt;\n\t\t}\n\t\treturn PTR_ERR(fio->encrypted_page);\n\t}\n\n\tmpage = find_lock_page(META_MAPPING(fio->sbi), fio->old_blkaddr);\n\tif (mpage) {\n\t\tif (PageUptodate(mpage))\n\t\t\tmemcpy(page_address(mpage),\n\t\t\t\tpage_address(fio->encrypted_page), PAGE_SIZE);\n\t\tf2fs_put_page(mpage, 1);\n\t}\n\treturn 0;\n}\n\nstatic inline bool check_inplace_update_policy(struct inode *inode,\n\t\t\t\tstruct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tunsigned int policy = SM_I(sbi)->ipu_policy;\n\n\tif (policy & (0x1 << F2FS_IPU_FORCE))\n\t\treturn true;\n\tif (policy & (0x1 << F2FS_IPU_SSR) && f2fs_need_SSR(sbi))\n\t\treturn true;\n\tif (policy & (0x1 << F2FS_IPU_UTIL) &&\n\t\t\tutilization(sbi) > SM_I(sbi)->min_ipu_util)\n\t\treturn true;\n\tif (policy & (0x1 << F2FS_IPU_SSR_UTIL) && f2fs_need_SSR(sbi) &&\n\t\t\tutilization(sbi) > SM_I(sbi)->min_ipu_util)\n\t\treturn true;\n\n\t/*\n\t * IPU for rewrite async pages\n\t */\n\tif (policy & (0x1 << F2FS_IPU_ASYNC) &&\n\t\t\tfio && fio->op == REQ_OP_WRITE &&\n\t\t\t!(fio->op_flags & REQ_SYNC) &&\n\t\t\t!IS_ENCRYPTED(inode))\n\t\treturn true;\n\n\t/* this is only set during fdatasync */\n\tif (policy & (0x1 << F2FS_IPU_FSYNC) &&\n\t\t\tis_inode_flag_set(inode, FI_NEED_IPU))\n\t\treturn true;\n\n\tif (unlikely(fio && is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&\n\t\t\t!f2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))\n\t\treturn true;\n\n\treturn false;\n}\n\nbool f2fs_should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)\n{\n\tif (f2fs_is_pinned_file(inode))\n\t\treturn true;\n\n\t/* if this is cold file, we should overwrite to avoid fragmentation */\n\tif (file_is_cold(inode))\n\t\treturn true;\n\n\treturn check_inplace_update_policy(inode, fio);\n}\n\nbool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (test_opt(sbi, LFS))\n\t\treturn true;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn true;\n\tif (IS_NOQUOTA(inode))\n\t\treturn true;\n\tif (f2fs_is_atomic_file(inode))\n\t\treturn true;\n\tif (fio) {\n\t\tif (is_cold_data(fio->page))\n\t\t\treturn true;\n\t\tif (IS_ATOMIC_WRITTEN_PAGE(fio->page))\n\t\t\treturn true;\n\t\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&\n\t\t\tf2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool need_inplace_update(struct f2fs_io_info *fio)\n{\n\tstruct inode *inode = fio->page->mapping->host;\n\n\tif (f2fs_should_update_outplace(inode, fio))\n\t\treturn false;\n\n\treturn f2fs_should_update_inplace(inode, fio);\n}\n\nint f2fs_do_write_data_page(struct f2fs_io_info *fio)\n{\n\tstruct page *page = fio->page;\n\tstruct inode *inode = page->mapping->host;\n\tstruct dnode_of_data dn;\n\tstruct extent_info ei = {0,0,0};\n\tstruct node_info ni;\n\tbool ipu_force = false;\n\tint err = 0;\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\tif (need_inplace_update(fio) &&\n\t\t\tf2fs_lookup_extent_cache(inode, page->index, &ei)) {\n\t\tfio->old_blkaddr = ei.blk + page->index - ei.fofs;\n\n\t\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE))\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tipu_force = true;\n\t\tfio->need_lock = LOCK_DONE;\n\t\tgoto got_it;\n\t}\n\n\t/* Deadlock due to between page->lock and f2fs_lock_op */\n\tif (fio->need_lock == LOCK_REQ && !f2fs_trylock_op(fio->sbi))\n\t\treturn -EAGAIN;\n\n\terr = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto out;\n\n\tfio->old_blkaddr = dn.data_blkaddr;\n\n\t/* This page is already truncated */\n\tif (fio->old_blkaddr == NULL_ADDR) {\n\t\tClearPageUptodate(page);\n\t\tclear_cold_data(page);\n\t\tgoto out_writepage;\n\t}\ngot_it:\n\tif (__is_valid_data_blkaddr(fio->old_blkaddr) &&\n\t\t!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tgoto out_writepage;\n\t}\n\t/*\n\t * If current allocation needs SSR,\n\t * it had better in-place writes for updated data.\n\t */\n\tif (ipu_force ||\n\t\t(__is_valid_data_blkaddr(fio->old_blkaddr) &&\n\t\t\t\t\tneed_inplace_update(fio))) {\n\t\terr = encrypt_one_page(fio);\n\t\tif (err)\n\t\t\tgoto out_writepage;\n\n\t\tset_page_writeback(page);\n\t\tClearPageError(page);\n\t\tf2fs_put_dnode(&dn);\n\t\tif (fio->need_lock == LOCK_REQ)\n\t\t\tf2fs_unlock_op(fio->sbi);\n\t\terr = f2fs_inplace_write_data(fio);\n\t\tif (err) {\n\t\t\tif (f2fs_encrypted_file(inode))\n\t\t\t\tfscrypt_pullback_bio_page(&fio->encrypted_page,\n\t\t\t\t\t\t\t\t\ttrue);\n\t\t\tif (PageWriteback(page))\n\t\t\t\tend_page_writeback(page);\n\t\t} else {\n\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\t}\n\t\ttrace_f2fs_do_write_data_page(fio->page, IPU);\n\t\treturn err;\n\t}\n\n\tif (fio->need_lock == LOCK_RETRY) {\n\t\tif (!f2fs_trylock_op(fio->sbi)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_writepage;\n\t\t}\n\t\tfio->need_lock = LOCK_REQ;\n\t}\n\n\terr = f2fs_get_node_info(fio->sbi, dn.nid, &ni);\n\tif (err)\n\t\tgoto out_writepage;\n\n\tfio->version = ni.version;\n\n\terr = encrypt_one_page(fio);\n\tif (err)\n\t\tgoto out_writepage;\n\n\tset_page_writeback(page);\n\tClearPageError(page);\n\n\t/* LFS mode write path */\n\tf2fs_outplace_write_data(&dn, fio);\n\ttrace_f2fs_do_write_data_page(page, OPU);\n\tset_inode_flag(inode, FI_APPEND_WRITE);\n\tif (page->index == 0)\n\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\nout_writepage:\n\tf2fs_put_dnode(&dn);\nout:\n\tif (fio->need_lock == LOCK_REQ)\n\t\tf2fs_unlock_op(fio->sbi);\n\treturn err;\n}\n\nstatic int __write_data_page(struct page *page, bool *submitted,\n\t\t\t\tstruct bio **bio,\n\t\t\t\tsector_t *last_block,\n\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\tenum iostat_type io_type)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = ((unsigned long long) i_size)\n\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\tloff_t psize = (page->index + 1) << PAGE_SHIFT;\n\tunsigned offset = 0;\n\tbool need_balance_fs = false;\n\tint err = 0;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.ino = inode->i_ino,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = wbc_to_write_flags(wbc),\n\t\t.old_blkaddr = NULL_ADDR,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.submitted = false,\n\t\t.need_lock = LOCK_RETRY,\n\t\t.io_type = io_type,\n\t\t.io_wbc = wbc,\n\t\t.bio = bio,\n\t\t.last_block = last_block,\n\t};\n\n\ttrace_f2fs_writepage(page, DATA);\n\n\t/* we should bypass data pages to proceed the kworkder jobs */\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tmapping_set_error(page->mapping, -EIO);\n\t\t/*\n\t\t * don't drop any dirty dentry pages for keeping lastest\n\t\t * directory structure.\n\t\t */\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\tgoto redirty_out;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto redirty_out;\n\n\tif (page->index < end_index)\n\t\tgoto write;\n\n\t/*\n\t * If the offset is out-of-range of file size,\n\t * this page does not have to be written to disk.\n\t */\n\toffset = i_size & (PAGE_SIZE - 1);\n\tif ((page->index >= end_index + 1) || !offset)\n\t\tgoto out;\n\n\tzero_user_segment(page, offset, PAGE_SIZE);\nwrite:\n\tif (f2fs_is_drop_cache(inode))\n\t\tgoto out;\n\t/* we should not write 0'th page having journal header */\n\tif (f2fs_is_volatile_file(inode) && (!page->index ||\n\t\t\t(!wbc->for_reclaim &&\n\t\t\tf2fs_available_free_memory(sbi, BASE_CHECK))))\n\t\tgoto redirty_out;\n\n\t/* Dentry blocks are controlled by checkpoint */\n\tif (S_ISDIR(inode->i_mode)) {\n\t\tfio.need_lock = LOCK_DONE;\n\t\terr = f2fs_do_write_data_page(&fio);\n\t\tgoto done;\n\t}\n\n\tif (!wbc->for_reclaim)\n\t\tneed_balance_fs = true;\n\telse if (has_not_enough_free_secs(sbi, 0, 0))\n\t\tgoto redirty_out;\n\telse\n\t\tset_inode_flag(inode, FI_HOT_DATA);\n\n\terr = -EAGAIN;\n\tif (f2fs_has_inline_data(inode)) {\n\t\terr = f2fs_write_inline_data(inode, page);\n\t\tif (!err)\n\t\t\tgoto out;\n\t}\n\n\tif (err == -EAGAIN) {\n\t\terr = f2fs_do_write_data_page(&fio);\n\t\tif (err == -EAGAIN) {\n\t\t\tfio.need_lock = LOCK_REQ;\n\t\t\terr = f2fs_do_write_data_page(&fio);\n\t\t}\n\t}\n\n\tif (err) {\n\t\tfile_set_keep_isize(inode);\n\t} else {\n\t\tdown_write(&F2FS_I(inode)->i_sem);\n\t\tif (F2FS_I(inode)->last_disk_size < psize)\n\t\t\tF2FS_I(inode)->last_disk_size = psize;\n\t\tup_write(&F2FS_I(inode)->i_sem);\n\t}\n\ndone:\n\tif (err && err != -ENOENT)\n\t\tgoto redirty_out;\n\nout:\n\tinode_dec_dirty_pages(inode);\n\tif (err) {\n\t\tClearPageUptodate(page);\n\t\tclear_cold_data(page);\n\t}\n\n\tif (wbc->for_reclaim) {\n\t\tf2fs_submit_merged_write_cond(sbi, NULL, page, 0, DATA);\n\t\tclear_inode_flag(inode, FI_HOT_DATA);\n\t\tf2fs_remove_dirty_inode(inode);\n\t\tsubmitted = NULL;\n\t}\n\n\tunlock_page(page);\n\tif (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&\n\t\t\t\t\t!F2FS_I(inode)->cp_task) {\n\t\tf2fs_submit_ipu_bio(sbi, bio, page);\n\t\tf2fs_balance_fs(sbi, need_balance_fs);\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_submit_ipu_bio(sbi, bio, page);\n\t\tf2fs_submit_merged_write(sbi, DATA);\n\t\tsubmitted = NULL;\n\t}\n\n\tif (submitted)\n\t\t*submitted = fio.submitted;\n\n\treturn 0;\n\nredirty_out:\n\tredirty_page_for_writepage(wbc, page);\n\t/*\n\t * pageout() in MM traslates EAGAIN, so calls handle_write_error()\n\t * -> mapping_set_error() -> set_bit(AS_EIO, ...).\n\t * file_write_and_wait_range() will see EIO error, which is critical\n\t * to return value of fsync() followed by atomic_write failure to user.\n\t */\n\tif (!err || wbc->for_reclaim)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\tunlock_page(page);\n\treturn err;\n}\n\nstatic int f2fs_write_data_page(struct page *page,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\treturn __write_data_page(page, NULL, NULL, NULL, wbc, FS_DATA_IO);\n}\n\n/*\n * This function was copied from write_cche_pages from mm/page-writeback.c.\n * The major change is making write step of cold data page separately from\n * warm/hot data page.\n */\nstatic int f2fs_write_cache_pages(struct address_space *mapping,\n\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tint ret = 0;\n\tint done = 0;\n\tstruct pagevec pvec;\n\tstruct f2fs_sb_info *sbi = F2FS_M_SB(mapping);\n\tstruct bio *bio = NULL;\n\tsector_t last_block;\n\tint nr_pages;\n\tpgoff_t uninitialized_var(writeback_index);\n\tpgoff_t index;\n\tpgoff_t end;\t\t/* Inclusive */\n\tpgoff_t done_index;\n\tint cycled;\n\tint range_whole = 0;\n\txa_mark_t tag;\n\tint nwritten = 0;\n\n\tpagevec_init(&pvec);\n\n\tif (get_dirty_pages(mapping->host) <=\n\t\t\t\tSM_I(F2FS_M_SB(mapping))->min_hot_blocks)\n\t\tset_inode_flag(mapping->host, FI_HOT_DATA);\n\telse\n\t\tclear_inode_flag(mapping->host, FI_HOT_DATA);\n\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index; /* prev offset */\n\t\tindex = writeback_index;\n\t\tif (index == 0)\n\t\t\tcycled = 1;\n\t\telse\n\t\t\tcycled = 0;\n\t\tend = -1;\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t\tcycled = 1; /* ignore range_cyclic tests */\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && (index <= end)) {\n\t\tint i;\n\n\t\tnr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,\n\t\t\t\ttag);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\t\t\tbool submitted = false;\n\n\t\t\t/* give a priority to WB_SYNC threads */\n\t\t\tif (atomic_read(&sbi->wb_sync_req[DATA]) &&\n\t\t\t\t\twbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdone_index = page->index;\nretry_write:\n\t\t\tlock_page(page);\n\n\t\t\tif (unlikely(page->mapping != mapping)) {\ncontinue_unlock:\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!PageDirty(page)) {\n\t\t\t\t/* someone wrote it for us */\n\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE) {\n\t\t\t\t\tf2fs_wait_on_page_writeback(page,\n\t\t\t\t\t\t\tDATA, true, true);\n\t\t\t\t\tf2fs_submit_ipu_bio(sbi, &bio, page);\n\t\t\t\t} else {\n\t\t\t\t\tgoto continue_unlock;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!clear_page_dirty_for_io(page))\n\t\t\t\tgoto continue_unlock;\n\n\t\t\tret = __write_data_page(page, &submitted, &bio,\n\t\t\t\t\t&last_block, wbc, io_type);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\t/*\n\t\t\t\t * keep nr_to_write, since vfs uses this to\n\t\t\t\t * get # of written pages.\n\t\t\t\t */\n\t\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tret = 0;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else if (ret == -EAGAIN) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tif (wbc->sync_mode == WB_SYNC_ALL) {\n\t\t\t\t\t\tcond_resched();\n\t\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC,\n\t\t\t\t\t\t\t\t\tHZ/50);\n\t\t\t\t\t\tgoto retry_write;\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tdone_index = page->index + 1;\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t} else if (submitted) {\n\t\t\t\tnwritten++;\n\t\t\t}\n\n\t\t\tif (--wbc->nr_to_write <= 0 &&\n\t\t\t\t\twbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\n\tif (!cycled && !done) {\n\t\tcycled = 1;\n\t\tindex = 0;\n\t\tend = writeback_index - 1;\n\t\tgoto retry;\n\t}\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = done_index;\n\n\tif (nwritten)\n\t\tf2fs_submit_merged_write_cond(F2FS_M_SB(mapping), mapping->host,\n\t\t\t\t\t\t\t\tNULL, 0, DATA);\n\t/* submit cached bio of IPU write */\n\tif (bio)\n\t\t__submit_bio(sbi, bio, DATA);\n\n\treturn ret;\n}\n\nstatic inline bool __should_serialize_io(struct inode *inode,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn false;\n\tif (IS_NOQUOTA(inode))\n\t\treturn false;\n\t/* to avoid deadlock in path of data flush */\n\tif (F2FS_I(inode)->cp_task)\n\t\treturn false;\n\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\treturn true;\n\tif (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct blk_plug plug;\n\tint ret;\n\tbool locked = false;\n\n\t/* deal with chardevs and other special file */\n\tif (!mapping->a_ops->writepage)\n\t\treturn 0;\n\n\t/* skip writing if there is no dirty page in this inode */\n\tif (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)\n\t\treturn 0;\n\n\t/* during POR, we don't need to trigger writepage at all. */\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto skip_write;\n\n\tif ((S_ISDIR(inode->i_mode) || IS_NOQUOTA(inode)) &&\n\t\t\twbc->sync_mode == WB_SYNC_NONE &&\n\t\t\tget_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&\n\t\t\tf2fs_available_free_memory(sbi, DIRTY_DENTS))\n\t\tgoto skip_write;\n\n\t/* skip writing during file defragment */\n\tif (is_inode_flag_set(inode, FI_DO_DEFRAG))\n\t\tgoto skip_write;\n\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\n\t/* to avoid spliting IOs due to mixed WB_SYNC_ALL and WB_SYNC_NONE */\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tatomic_inc(&sbi->wb_sync_req[DATA]);\n\telse if (atomic_read(&sbi->wb_sync_req[DATA]))\n\t\tgoto skip_write;\n\n\tif (__should_serialize_io(inode, wbc)) {\n\t\tmutex_lock(&sbi->writepages);\n\t\tlocked = true;\n\t}\n\n\tblk_start_plug(&plug);\n\tret = f2fs_write_cache_pages(mapping, wbc, io_type);\n\tblk_finish_plug(&plug);\n\n\tif (locked)\n\t\tmutex_unlock(&sbi->writepages);\n\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tatomic_dec(&sbi->wb_sync_req[DATA]);\n\t/*\n\t * if some pages were truncated, we cannot guarantee its mapping->host\n\t * to detect pending bios.\n\t */\n\n\tf2fs_remove_dirty_inode(inode);\n\treturn ret;\n\nskip_write:\n\twbc->pages_skipped += get_dirty_pages(inode);\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\treturn 0;\n}\n\nstatic int f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t    struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\n\treturn __f2fs_write_data_pages(mapping, wbc,\n\t\t\tF2FS_I(inode)->cp_task == current ?\n\t\t\tFS_CP_DATA_IO : FS_DATA_IO);\n}\n\nstatic void f2fs_write_failed(struct address_space *mapping, loff_t to)\n{\n\tstruct inode *inode = mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (to > i_size) {\n\t\tdown_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\tdown_write(&F2FS_I(inode)->i_mmap_sem);\n\n\t\ttruncate_pagecache(inode, i_size);\n\t\tif (!IS_NOQUOTA(inode))\n\t\t\tf2fs_truncate_blocks(inode, i_size, true);\n\n\t\tup_write(&F2FS_I(inode)->i_mmap_sem);\n\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t}\n}\n\nstatic int prepare_write_begin(struct f2fs_sb_info *sbi,\n\t\t\tstruct page *page, loff_t pos, unsigned len,\n\t\t\tblock_t *blk_addr, bool *node_changed)\n{\n\tstruct inode *inode = page->mapping->host;\n\tpgoff_t index = page->index;\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tbool locked = false;\n\tstruct extent_info ei = {0,0,0};\n\tint err = 0;\n\tint flag;\n\n\t/*\n\t * we already allocated all the blocks, so we don't need to get\n\t * the block addresses when there is no need to fill the page.\n\t */\n\tif (!f2fs_has_inline_data(inode) && len == PAGE_SIZE &&\n\t\t\t!is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\t/* f2fs_lock_op avoids race between write CP and convert_inline_page */\n\tif (f2fs_has_inline_data(inode) && pos + len > MAX_INLINE_DATA(inode))\n\t\tflag = F2FS_GET_BLOCK_DEFAULT;\n\telse\n\t\tflag = F2FS_GET_BLOCK_PRE_AIO;\n\n\tif (f2fs_has_inline_data(inode) ||\n\t\t\t(pos & PAGE_MASK) >= i_size_read(inode)) {\n\t\t__do_map_lock(sbi, flag, true);\n\t\tlocked = true;\n\t}\nrestart:\n\t/* check inline_data */\n\tipage = f2fs_get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(ipage)) {\n\t\terr = PTR_ERR(ipage);\n\t\tgoto unlock_out;\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tif (pos + len <= MAX_INLINE_DATA(inode)) {\n\t\t\tf2fs_do_read_inline_data(page, ipage);\n\t\t\tset_inode_flag(inode, FI_DATA_EXIST);\n\t\t\tif (inode->i_nlink)\n\t\t\t\tset_inline_node(ipage);\n\t\t} else {\n\t\t\terr = f2fs_convert_inline_page(&dn, page);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (dn.data_blkaddr == NULL_ADDR)\n\t\t\t\terr = f2fs_get_block(&dn, index);\n\t\t}\n\t} else if (locked) {\n\t\terr = f2fs_get_block(&dn, index);\n\t} else {\n\t\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\t} else {\n\t\t\t/* hole case */\n\t\t\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\t\t\tif (err || dn.data_blkaddr == NULL_ADDR) {\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\t__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO,\n\t\t\t\t\t\t\t\ttrue);\n\t\t\t\tWARN_ON(flag != F2FS_GET_BLOCK_PRE_AIO);\n\t\t\t\tlocked = true;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* convert_inline_page can make node_changed */\n\t*blk_addr = dn.data_blkaddr;\n\t*node_changed = dn.node_changed;\nout:\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (locked)\n\t\t__do_map_lock(sbi, flag, false);\n\treturn err;\n}\n\nstatic int f2fs_write_begin(struct file *file, struct address_space *mapping,\n\t\tloff_t pos, unsigned len, unsigned flags,\n\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page = NULL;\n\tpgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;\n\tbool need_balance = false, drop_atomic = false;\n\tblock_t blkaddr = NULL_ADDR;\n\tint err = 0;\n\n\ttrace_f2fs_write_begin(inode, pos, len, flags);\n\n\terr = f2fs_is_checkpoint_ready(sbi);\n\tif (err)\n\t\tgoto fail;\n\n\tif ((f2fs_is_atomic_file(inode) &&\n\t\t\t!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||\n\t\t\tis_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {\n\t\terr = -ENOMEM;\n\t\tdrop_atomic = true;\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * We should check this at this moment to avoid deadlock on inode page\n\t * and #0 page. The locking rule for inline_data conversion should be:\n\t * lock_page(page #0) -> lock_page(inode_page)\n\t */\n\tif (index != 0) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\nrepeat:\n\t/*\n\t * Do not use grab_cache_page_write_begin() to avoid deadlock due to\n\t * wait_for_stable_page. Will wait that below with our IO control.\n\t */\n\tpage = f2fs_pagecache_get_page(mapping, index,\n\t\t\t\tFGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);\n\tif (!page) {\n\t\terr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t*pagep = page;\n\n\terr = prepare_write_begin(sbi, page, pos, len,\n\t\t\t\t\t&blkaddr, &need_balance);\n\tif (err)\n\t\tgoto fail;\n\n\tif (need_balance && !IS_NOQUOTA(inode) &&\n\t\t\thas_not_enough_free_secs(sbi, 0, 0)) {\n\t\tunlock_page(page);\n\t\tf2fs_balance_fs(sbi, true);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\t/* The page got truncated from under us */\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\n\tf2fs_wait_on_page_writeback(page, DATA, false, true);\n\n\tif (len == PAGE_SIZE || PageUptodate(page))\n\t\treturn 0;\n\n\tif (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode)) {\n\t\tzero_user_segment(page, len, PAGE_SIZE);\n\t\treturn 0;\n\t}\n\n\tif (blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t} else {\n\t\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr,\n\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto fail;\n\t\t}\n\t\terr = f2fs_submit_page_read(inode, page, blkaddr);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tlock_page(page);\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tf2fs_put_page(page, 1);\n\tf2fs_write_failed(mapping, pos + len);\n\tif (drop_atomic)\n\t\tf2fs_drop_inmem_pages_all(sbi, false);\n\treturn err;\n}\n\nstatic int f2fs_write_end(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_f2fs_write_end(inode, pos, len, copied);\n\n\t/*\n\t * This should be come from len == PAGE_SIZE, and we expect copied\n\t * should be PAGE_SIZE. Otherwise, we treat it with zero copied and\n\t * let generic_perform_write() try to copy data again through copied=0.\n\t */\n\tif (!PageUptodate(page)) {\n\t\tif (unlikely(copied != len))\n\t\t\tcopied = 0;\n\t\telse\n\t\t\tSetPageUptodate(page);\n\t}\n\tif (!copied)\n\t\tgoto unlock_out;\n\n\tset_page_dirty(page);\n\n\tif (pos + copied > i_size_read(inode))\n\t\tf2fs_i_size_write(inode, pos + copied);\nunlock_out:\n\tf2fs_put_page(page, 1);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn copied;\n}\n\nstatic int check_direct_IO(struct inode *inode, struct iov_iter *iter,\n\t\t\t   loff_t offset)\n{\n\tunsigned i_blkbits = READ_ONCE(inode->i_blkbits);\n\tunsigned blkbits = i_blkbits;\n\tunsigned blocksize_mask = (1 << blkbits) - 1;\n\tunsigned long align = offset | iov_iter_alignment(iter);\n\tstruct block_device *bdev = inode->i_sb->s_bdev;\n\n\tif (align & blocksize_mask) {\n\t\tif (bdev)\n\t\t\tblkbits = blksize_bits(bdev_logical_block_size(bdev));\n\t\tblocksize_mask = (1 << blkbits) - 1;\n\t\tif (align & blocksize_mask)\n\t\t\treturn -EINVAL;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void f2fs_dio_end_io(struct bio *bio)\n{\n\tstruct f2fs_private_dio *dio = bio->bi_private;\n\n\tdec_page_count(F2FS_I_SB(dio->inode),\n\t\t\tdio->write ? F2FS_DIO_WRITE : F2FS_DIO_READ);\n\n\tbio->bi_private = dio->orig_private;\n\tbio->bi_end_io = dio->orig_end_io;\n\n\tkvfree(dio);\n\n\tbio_endio(bio);\n}\n\nstatic void f2fs_dio_submit_bio(struct bio *bio, struct inode *inode,\n\t\t\t\t\t\t\tloff_t file_offset)\n{\n\tstruct f2fs_private_dio *dio;\n\tbool write = (bio_op(bio) == REQ_OP_WRITE);\n\n\tdio = f2fs_kzalloc(F2FS_I_SB(inode),\n\t\t\tsizeof(struct f2fs_private_dio), GFP_NOFS);\n\tif (!dio)\n\t\tgoto out;\n\n\tdio->inode = inode;\n\tdio->orig_end_io = bio->bi_end_io;\n\tdio->orig_private = bio->bi_private;\n\tdio->write = write;\n\n\tbio->bi_end_io = f2fs_dio_end_io;\n\tbio->bi_private = dio;\n\n\tinc_page_count(F2FS_I_SB(inode),\n\t\t\twrite ? F2FS_DIO_WRITE : F2FS_DIO_READ);\n\n\tsubmit_bio(bio);\n\treturn;\nout:\n\tbio->bi_status = BLK_STS_IOERR;\n\tbio_endio(bio);\n}\n\nstatic ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tsize_t count = iov_iter_count(iter);\n\tloff_t offset = iocb->ki_pos;\n\tint rw = iov_iter_rw(iter);\n\tint err;\n\tenum rw_hint hint = iocb->ki_hint;\n\tint whint_mode = F2FS_OPTION(sbi).whint_mode;\n\tbool do_opu;\n\n\terr = check_direct_IO(inode, iter, offset);\n\tif (err)\n\t\treturn err < 0 ? err : 0;\n\n\tif (f2fs_force_buffered_io(inode, iocb, iter))\n\t\treturn 0;\n\n\tdo_opu = allow_outplace_dio(inode, iocb, iter);\n\n\ttrace_f2fs_direct_IO_enter(inode, offset, count, rw);\n\n\tif (rw == WRITE && whint_mode == WHINT_MODE_OFF)\n\t\tiocb->ki_hint = WRITE_LIFE_NOT_SET;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\tif (!down_read_trylock(&fi->i_gc_rwsem[rw])) {\n\t\t\tiocb->ki_hint = hint;\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t\tif (do_opu && !down_read_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\tup_read(&fi->i_gc_rwsem[rw]);\n\t\t\tiocb->ki_hint = hint;\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tdown_read(&fi->i_gc_rwsem[rw]);\n\t\tif (do_opu)\n\t\t\tdown_read(&fi->i_gc_rwsem[READ]);\n\t}\n\n\terr = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n\t\t\titer, rw == WRITE ? get_data_block_dio_write :\n\t\t\tget_data_block_dio, NULL, f2fs_dio_submit_bio,\n\t\t\tDIO_LOCKING | DIO_SKIP_HOLES);\n\n\tif (do_opu)\n\t\tup_read(&fi->i_gc_rwsem[READ]);\n\n\tup_read(&fi->i_gc_rwsem[rw]);\n\n\tif (rw == WRITE) {\n\t\tif (whint_mode == WHINT_MODE_OFF)\n\t\t\tiocb->ki_hint = hint;\n\t\tif (err > 0) {\n\t\t\tf2fs_update_iostat(F2FS_I_SB(inode), APP_DIRECT_IO,\n\t\t\t\t\t\t\t\t\terr);\n\t\t\tif (!do_opu)\n\t\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\t} else if (err < 0) {\n\t\t\tf2fs_write_failed(mapping, offset + count);\n\t\t}\n\t}\n\nout:\n\ttrace_f2fs_direct_IO_exit(inode, offset, count, rw, err);\n\n\treturn err;\n}\n\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\t\t\t\t\tunsigned int length)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino >= F2FS_ROOT_INO(sbi) &&\n\t\t(offset % PAGE_SIZE || length != PAGE_SIZE))\n\t\treturn;\n\n\tif (PageDirty(page)) {\n\t\tif (inode->i_ino == F2FS_META_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_META);\n\t\t} else if (inode->i_ino == F2FS_NODE_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_NODES);\n\t\t} else {\n\t\t\tinode_dec_dirty_pages(inode);\n\t\t\tf2fs_remove_dirty_inode(inode);\n\t\t}\n\t}\n\n\tclear_cold_data(page);\n\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn f2fs_drop_inmem_page(inode, page);\n\n\tf2fs_clear_page_private(page);\n}\n\nint f2fs_release_page(struct page *page, gfp_t wait)\n{\n\t/* If this is dirty page, keep PagePrivate */\n\tif (PageDirty(page))\n\t\treturn 0;\n\n\t/* This is atomic written page, keep Private */\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn 0;\n\n\tclear_cold_data(page);\n\tf2fs_clear_page_private(page);\n\treturn 1;\n}\n\nstatic int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tf2fs_register_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\t__set_page_dirty_nobuffers(page);\n\t\tf2fs_update_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic sector_t f2fs_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\t/* make sure allocating whole blocks */\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tfilemap_write_and_wait(mapping);\n\n\treturn generic_block_bmap(mapping, block, get_data_block_bmap);\n}\n\n#ifdef CONFIG_MIGRATION\n#include <linux/migrate.h>\n\nint f2fs_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tint rc, extra_count;\n\tstruct f2fs_inode_info *fi = F2FS_I(mapping->host);\n\tbool atomic_written = IS_ATOMIC_WRITTEN_PAGE(page);\n\n\tBUG_ON(PageWriteback(page));\n\n\t/* migrating an atomic written page is safe with the inmem_lock hold */\n\tif (atomic_written) {\n\t\tif (mode != MIGRATE_SYNC)\n\t\t\treturn -EBUSY;\n\t\tif (!mutex_trylock(&fi->inmem_lock))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/* one extra reference was held for atomic_write page */\n\textra_count = atomic_written ? 1 : 0;\n\trc = migrate_page_move_mapping(mapping, newpage,\n\t\t\t\tpage, mode, extra_count);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tif (atomic_written)\n\t\t\tmutex_unlock(&fi->inmem_lock);\n\t\treturn rc;\n\t}\n\n\tif (atomic_written) {\n\t\tstruct inmem_pages *cur;\n\t\tlist_for_each_entry(cur, &fi->inmem_pages, list)\n\t\t\tif (cur->page == page) {\n\t\t\t\tcur->page = newpage;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tmutex_unlock(&fi->inmem_lock);\n\t\tput_page(page);\n\t\tget_page(newpage);\n\t}\n\n\tif (PagePrivate(page)) {\n\t\tf2fs_set_page_private(newpage, page_private(page));\n\t\tf2fs_clear_page_private(page);\n\t}\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tmigrate_page_copy(newpage, page);\n\telse\n\t\tmigrate_page_states(newpage, page);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n#endif\n\nconst struct address_space_operations f2fs_dblock_aops = {\n\t.readpage\t= f2fs_read_data_page,\n\t.readpages\t= f2fs_read_data_pages,\n\t.writepage\t= f2fs_write_data_page,\n\t.writepages\t= f2fs_write_data_pages,\n\t.write_begin\t= f2fs_write_begin,\n\t.write_end\t= f2fs_write_end,\n\t.set_page_dirty\t= f2fs_set_data_page_dirty,\n\t.invalidatepage\t= f2fs_invalidate_page,\n\t.releasepage\t= f2fs_release_page,\n\t.direct_IO\t= f2fs_direct_IO,\n\t.bmap\t\t= f2fs_bmap,\n#ifdef CONFIG_MIGRATION\n\t.migratepage    = f2fs_migrate_page,\n#endif\n};\n\nvoid f2fs_clear_page_cache_dirty_tag(struct page *page)\n{\n\tstruct address_space *mapping = page_mapping(page);\n\tunsigned long flags;\n\n\txa_lock_irqsave(&mapping->i_pages, flags);\n\t__xa_clear_mark(&mapping->i_pages, page_index(page),\n\t\t\t\t\t\tPAGECACHE_TAG_DIRTY);\n\txa_unlock_irqrestore(&mapping->i_pages, flags);\n}\n\nint __init f2fs_init_post_read_processing(void)\n{\n\tbio_post_read_ctx_cache = KMEM_CACHE(bio_post_read_ctx, 0);\n\tif (!bio_post_read_ctx_cache)\n\t\tgoto fail;\n\tbio_post_read_ctx_pool =\n\t\tmempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS,\n\t\t\t\t\t bio_post_read_ctx_cache);\n\tif (!bio_post_read_ctx_pool)\n\t\tgoto fail_free_cache;\n\treturn 0;\n\nfail_free_cache:\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid __exit f2fs_destroy_post_read_processing(void)\n{\n\tmempool_destroy(bio_post_read_ctx_pool);\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * fs/f2fs/f2fs.h\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n */\n#ifndef _LINUX_F2FS_H\n#define _LINUX_F2FS_H\n\n#include <linux/uio.h>\n#include <linux/types.h>\n#include <linux/page-flags.h>\n#include <linux/buffer_head.h>\n#include <linux/slab.h>\n#include <linux/crc32.h>\n#include <linux/magic.h>\n#include <linux/kobject.h>\n#include <linux/sched.h>\n#include <linux/cred.h>\n#include <linux/vmalloc.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/quotaops.h>\n#include <crypto/hash.h>\n\n#include <linux/fscrypt.h>\n\n#ifdef CONFIG_F2FS_CHECK_FS\n#define f2fs_bug_on(sbi, condition)\tBUG_ON(condition)\n#else\n#define f2fs_bug_on(sbi, condition)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (unlikely(condition)) {\t\t\t\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\nenum {\n\tFAULT_KMALLOC,\n\tFAULT_KVMALLOC,\n\tFAULT_PAGE_ALLOC,\n\tFAULT_PAGE_GET,\n\tFAULT_ALLOC_BIO,\n\tFAULT_ALLOC_NID,\n\tFAULT_ORPHAN,\n\tFAULT_BLOCK,\n\tFAULT_DIR_DEPTH,\n\tFAULT_EVICT_INODE,\n\tFAULT_TRUNCATE,\n\tFAULT_READ_IO,\n\tFAULT_CHECKPOINT,\n\tFAULT_DISCARD,\n\tFAULT_WRITE_IO,\n\tFAULT_MAX,\n};\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n#define F2FS_ALL_FAULT_TYPE\t\t((1 << FAULT_MAX) - 1)\n\nstruct f2fs_fault_info {\n\tatomic_t inject_ops;\n\tunsigned int inject_rate;\n\tunsigned int inject_type;\n};\n\nextern const char *f2fs_fault_name[FAULT_MAX];\n#define IS_FAULT_SET(fi, type) ((fi)->inject_type & (1 << (type)))\n#endif\n\n/*\n * For mount options\n */\n#define F2FS_MOUNT_BG_GC\t\t0x00000001\n#define F2FS_MOUNT_DISABLE_ROLL_FORWARD\t0x00000002\n#define F2FS_MOUNT_DISCARD\t\t0x00000004\n#define F2FS_MOUNT_NOHEAP\t\t0x00000008\n#define F2FS_MOUNT_XATTR_USER\t\t0x00000010\n#define F2FS_MOUNT_POSIX_ACL\t\t0x00000020\n#define F2FS_MOUNT_DISABLE_EXT_IDENTIFY\t0x00000040\n#define F2FS_MOUNT_INLINE_XATTR\t\t0x00000080\n#define F2FS_MOUNT_INLINE_DATA\t\t0x00000100\n#define F2FS_MOUNT_INLINE_DENTRY\t0x00000200\n#define F2FS_MOUNT_FLUSH_MERGE\t\t0x00000400\n#define F2FS_MOUNT_NOBARRIER\t\t0x00000800\n#define F2FS_MOUNT_FASTBOOT\t\t0x00001000\n#define F2FS_MOUNT_EXTENT_CACHE\t\t0x00002000\n#define F2FS_MOUNT_FORCE_FG_GC\t\t0x00004000\n#define F2FS_MOUNT_DATA_FLUSH\t\t0x00008000\n#define F2FS_MOUNT_FAULT_INJECTION\t0x00010000\n#define F2FS_MOUNT_ADAPTIVE\t\t0x00020000\n#define F2FS_MOUNT_LFS\t\t\t0x00040000\n#define F2FS_MOUNT_USRQUOTA\t\t0x00080000\n#define F2FS_MOUNT_GRPQUOTA\t\t0x00100000\n#define F2FS_MOUNT_PRJQUOTA\t\t0x00200000\n#define F2FS_MOUNT_QUOTA\t\t0x00400000\n#define F2FS_MOUNT_INLINE_XATTR_SIZE\t0x00800000\n#define F2FS_MOUNT_RESERVE_ROOT\t\t0x01000000\n#define F2FS_MOUNT_DISABLE_CHECKPOINT\t0x02000000\n\n#define F2FS_OPTION(sbi)\t((sbi)->mount_opt)\n#define clear_opt(sbi, option)\t(F2FS_OPTION(sbi).opt &= ~F2FS_MOUNT_##option)\n#define set_opt(sbi, option)\t(F2FS_OPTION(sbi).opt |= F2FS_MOUNT_##option)\n#define test_opt(sbi, option)\t(F2FS_OPTION(sbi).opt & F2FS_MOUNT_##option)\n\n#define ver_after(a, b)\t(typecheck(unsigned long long, a) &&\t\t\\\n\t\ttypecheck(unsigned long long, b) &&\t\t\t\\\n\t\t((long long)((a) - (b)) > 0))\n\ntypedef u32 block_t;\t/*\n\t\t\t * should not change u32, since it is the on-disk block\n\t\t\t * address format, __le32.\n\t\t\t */\ntypedef u32 nid_t;\n\nstruct f2fs_mount_info {\n\tunsigned int opt;\n\tint write_io_size_bits;\t\t/* Write IO size bits */\n\tblock_t root_reserved_blocks;\t/* root reserved blocks */\n\tkuid_t s_resuid;\t\t/* reserved blocks for uid */\n\tkgid_t s_resgid;\t\t/* reserved blocks for gid */\n\tint active_logs;\t\t/* # of active logs */\n\tint inline_xattr_size;\t\t/* inline xattr size */\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info fault_info;\t/* For fault injection */\n#endif\n#ifdef CONFIG_QUOTA\n\t/* Names of quota files with journalled quota */\n\tchar *s_qf_names[MAXQUOTAS];\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n\t/* For which write hints are passed down to block layer */\n\tint whint_mode;\n\tint alloc_mode;\t\t\t/* segment allocation policy */\n\tint fsync_mode;\t\t\t/* fsync policy */\n\tbool test_dummy_encryption;\t/* test dummy encryption */\n\tblock_t unusable_cap;\t\t/* Amount of space allowed to be\n\t\t\t\t\t * unusable when disabling checkpoint\n\t\t\t\t\t */\n};\n\n#define F2FS_FEATURE_ENCRYPT\t\t0x0001\n#define F2FS_FEATURE_BLKZONED\t\t0x0002\n#define F2FS_FEATURE_ATOMIC_WRITE\t0x0004\n#define F2FS_FEATURE_EXTRA_ATTR\t\t0x0008\n#define F2FS_FEATURE_PRJQUOTA\t\t0x0010\n#define F2FS_FEATURE_INODE_CHKSUM\t0x0020\n#define F2FS_FEATURE_FLEXIBLE_INLINE_XATTR\t0x0040\n#define F2FS_FEATURE_QUOTA_INO\t\t0x0080\n#define F2FS_FEATURE_INODE_CRTIME\t0x0100\n#define F2FS_FEATURE_LOST_FOUND\t\t0x0200\n#define F2FS_FEATURE_VERITY\t\t0x0400\t/* reserved */\n#define F2FS_FEATURE_SB_CHKSUM\t\t0x0800\n\n#define __F2FS_HAS_FEATURE(raw_super, mask)\t\t\t\t\\\n\t((raw_super->feature & cpu_to_le32(mask)) != 0)\n#define F2FS_HAS_FEATURE(sbi, mask)\t__F2FS_HAS_FEATURE(sbi->raw_super, mask)\n#define F2FS_SET_FEATURE(sbi, mask)\t\t\t\t\t\\\n\t(sbi->raw_super->feature |= cpu_to_le32(mask))\n#define F2FS_CLEAR_FEATURE(sbi, mask)\t\t\t\t\t\\\n\t(sbi->raw_super->feature &= ~cpu_to_le32(mask))\n\n/*\n * Default values for user and/or group using reserved blocks\n */\n#define\tF2FS_DEF_RESUID\t\t0\n#define\tF2FS_DEF_RESGID\t\t0\n\n/*\n * For checkpoint manager\n */\nenum {\n\tNAT_BITMAP,\n\tSIT_BITMAP\n};\n\n#define\tCP_UMOUNT\t0x00000001\n#define\tCP_FASTBOOT\t0x00000002\n#define\tCP_SYNC\t\t0x00000004\n#define\tCP_RECOVERY\t0x00000008\n#define\tCP_DISCARD\t0x00000010\n#define CP_TRIMMED\t0x00000020\n#define CP_PAUSE\t0x00000040\n\n#define MAX_DISCARD_BLOCKS(sbi)\t\tBLKS_PER_SEC(sbi)\n#define DEF_MAX_DISCARD_REQUEST\t\t8\t/* issue 8 discards per round */\n#define DEF_MIN_DISCARD_ISSUE_TIME\t50\t/* 50 ms, if exists */\n#define DEF_MID_DISCARD_ISSUE_TIME\t500\t/* 500 ms, if device busy */\n#define DEF_MAX_DISCARD_ISSUE_TIME\t60000\t/* 60 s, if no candidates */\n#define DEF_DISCARD_URGENT_UTIL\t\t80\t/* do more discard over 80% */\n#define DEF_CP_INTERVAL\t\t\t60\t/* 60 secs */\n#define DEF_IDLE_INTERVAL\t\t5\t/* 5 secs */\n#define DEF_DISABLE_INTERVAL\t\t5\t/* 5 secs */\n#define DEF_DISABLE_QUICK_INTERVAL\t1\t/* 1 secs */\n#define DEF_UMOUNT_DISCARD_TIMEOUT\t5\t/* 5 secs */\n\nstruct cp_control {\n\tint reason;\n\t__u64 trim_start;\n\t__u64 trim_end;\n\t__u64 trim_minlen;\n};\n\n/*\n * indicate meta/data type\n */\nenum {\n\tMETA_CP,\n\tMETA_NAT,\n\tMETA_SIT,\n\tMETA_SSA,\n\tMETA_MAX,\n\tMETA_POR,\n\tDATA_GENERIC,\t\t/* check range only */\n\tDATA_GENERIC_ENHANCE,\t/* strong check on range and segment bitmap */\n\tDATA_GENERIC_ENHANCE_READ,\t/*\n\t\t\t\t\t * strong check on range and segment\n\t\t\t\t\t * bitmap but no warning due to race\n\t\t\t\t\t * condition of read on truncated area\n\t\t\t\t\t * by extent_cache\n\t\t\t\t\t */\n\tMETA_GENERIC,\n};\n\n/* for the list of ino */\nenum {\n\tORPHAN_INO,\t\t/* for orphan ino list */\n\tAPPEND_INO,\t\t/* for append ino list */\n\tUPDATE_INO,\t\t/* for update ino list */\n\tTRANS_DIR_INO,\t\t/* for trasactions dir ino list */\n\tFLUSH_INO,\t\t/* for multiple device flushing */\n\tMAX_INO_ENTRY,\t\t/* max. list */\n};\n\nstruct ino_entry {\n\tstruct list_head list;\t\t/* list head */\n\tnid_t ino;\t\t\t/* inode number */\n\tunsigned int dirty_device;\t/* dirty device bitmap */\n};\n\n/* for the list of inodes to be GCed */\nstruct inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n};\n\nstruct fsync_node_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct page *page;\t/* warm node page pointer */\n\tunsigned int seq_id;\t/* sequence id */\n};\n\n/* for the bitmap indicate blocks to be discarded */\nstruct discard_entry {\n\tstruct list_head list;\t/* list head */\n\tblock_t start_blkaddr;\t/* start blockaddr of current segment */\n\tunsigned char discard_map[SIT_VBLOCK_MAP_SIZE];\t/* segment discard bitmap */\n};\n\n/* default discard granularity of inner discard thread, unit: block count */\n#define DEFAULT_DISCARD_GRANULARITY\t\t16\n\n/* max discard pend list number */\n#define MAX_PLIST_NUM\t\t512\n#define plist_idx(blk_num)\t((blk_num) >= MAX_PLIST_NUM ?\t\t\\\n\t\t\t\t\t(MAX_PLIST_NUM - 1) : ((blk_num) - 1))\n\nenum {\n\tD_PREP,\t\t\t/* initial */\n\tD_PARTIAL,\t\t/* partially submitted */\n\tD_SUBMIT,\t\t/* all submitted */\n\tD_DONE,\t\t\t/* finished */\n};\n\nstruct discard_info {\n\tblock_t lstart;\t\t\t/* logical start address */\n\tblock_t len;\t\t\t/* length */\n\tblock_t start;\t\t\t/* actual start address in dev */\n};\n\nstruct discard_cmd {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunion {\n\t\tstruct {\n\t\t\tblock_t lstart;\t/* logical start address */\n\t\t\tblock_t len;\t/* length */\n\t\t\tblock_t start;\t/* actual start address in dev */\n\t\t};\n\t\tstruct discard_info di;\t/* discard info */\n\n\t};\n\tstruct list_head list;\t\t/* command list */\n\tstruct completion wait;\t\t/* compleation */\n\tstruct block_device *bdev;\t/* bdev */\n\tunsigned short ref;\t\t/* reference count */\n\tunsigned char state;\t\t/* state */\n\tunsigned char queued;\t\t/* queued discard */\n\tint error;\t\t\t/* bio error */\n\tspinlock_t lock;\t\t/* for state/bio_ref updating */\n\tunsigned short bio_ref;\t\t/* bio reference count */\n};\n\nenum {\n\tDPOLICY_BG,\n\tDPOLICY_FORCE,\n\tDPOLICY_FSTRIM,\n\tDPOLICY_UMOUNT,\n\tMAX_DPOLICY,\n};\n\nstruct discard_policy {\n\tint type;\t\t\t/* type of discard */\n\tunsigned int min_interval;\t/* used for candidates exist */\n\tunsigned int mid_interval;\t/* used for device busy */\n\tunsigned int max_interval;\t/* used for candidates not exist */\n\tunsigned int max_requests;\t/* # of discards issued per round */\n\tunsigned int io_aware_gran;\t/* minimum granularity discard not be aware of I/O */\n\tbool io_aware;\t\t\t/* issue discard in idle time */\n\tbool sync;\t\t\t/* submit discard with REQ_SYNC flag */\n\tbool ordered;\t\t\t/* issue discard by lba order */\n\tunsigned int granularity;\t/* discard granularity */\n\tint timeout;\t\t\t/* discard timeout for put_super */\n};\n\nstruct discard_cmd_control {\n\tstruct task_struct *f2fs_issue_discard;\t/* discard thread */\n\tstruct list_head entry_list;\t\t/* 4KB discard entry list */\n\tstruct list_head pend_list[MAX_PLIST_NUM];/* store pending entries */\n\tstruct list_head wait_list;\t\t/* store on-flushing entries */\n\tstruct list_head fstrim_list;\t\t/* in-flight discard from fstrim */\n\twait_queue_head_t discard_wait_queue;\t/* waiting queue for wake-up */\n\tunsigned int discard_wake;\t\t/* to wake up discard thread */\n\tstruct mutex cmd_lock;\n\tunsigned int nr_discards;\t\t/* # of discards in the list */\n\tunsigned int max_discards;\t\t/* max. discards to be issued */\n\tunsigned int discard_granularity;\t/* discard granularity */\n\tunsigned int undiscard_blks;\t\t/* # of undiscard blocks */\n\tunsigned int next_pos;\t\t\t/* next discard position */\n\tatomic_t issued_discard;\t\t/* # of issued discard */\n\tatomic_t queued_discard;\t\t/* # of queued discard */\n\tatomic_t discard_cmd_cnt;\t\t/* # of cached cmd count */\n\tstruct rb_root_cached root;\t\t/* root of discard rb-tree */\n\tbool rbtree_check;\t\t\t/* config for consistence check */\n};\n\n/* for the list of fsync inodes, used only during recovery */\nstruct fsync_inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n\tblock_t blkaddr;\t/* block address locating the last fsync */\n\tblock_t last_dentry;\t/* block address locating the last dentry */\n};\n\n#define nats_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_nats))\n#define sits_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_sits))\n\n#define nat_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].ne)\n#define nid_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].nid)\n#define sit_in_journal(jnl, i)\t\t((jnl)->sit_j.entries[i].se)\n#define segno_in_journal(jnl, i)\t((jnl)->sit_j.entries[i].segno)\n\n#define MAX_NAT_JENTRIES(jnl)\t(NAT_JOURNAL_ENTRIES - nats_in_cursum(jnl))\n#define MAX_SIT_JENTRIES(jnl)\t(SIT_JOURNAL_ENTRIES - sits_in_cursum(jnl))\n\nstatic inline int update_nats_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = nats_in_cursum(journal);\n\n\tjournal->n_nats = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline int update_sits_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = sits_in_cursum(journal);\n\n\tjournal->n_sits = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline bool __has_cursum_space(struct f2fs_journal *journal,\n\t\t\t\t\t\t\tint size, int type)\n{\n\tif (type == NAT_JOURNAL)\n\t\treturn size <= MAX_NAT_JENTRIES(journal);\n\treturn size <= MAX_SIT_JENTRIES(journal);\n}\n\n/*\n * ioctl commands\n */\n#define F2FS_IOC_GETFLAGS\t\tFS_IOC_GETFLAGS\n#define F2FS_IOC_SETFLAGS\t\tFS_IOC_SETFLAGS\n#define F2FS_IOC_GETVERSION\t\tFS_IOC_GETVERSION\n\n#define F2FS_IOCTL_MAGIC\t\t0xf5\n#define F2FS_IOC_START_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 1)\n#define F2FS_IOC_COMMIT_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 2)\n#define F2FS_IOC_START_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 3)\n#define F2FS_IOC_RELEASE_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 4)\n#define F2FS_IOC_ABORT_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 5)\n#define F2FS_IOC_GARBAGE_COLLECT\t_IOW(F2FS_IOCTL_MAGIC, 6, __u32)\n#define F2FS_IOC_WRITE_CHECKPOINT\t_IO(F2FS_IOCTL_MAGIC, 7)\n#define F2FS_IOC_DEFRAGMENT\t\t_IOWR(F2FS_IOCTL_MAGIC, 8,\t\\\n\t\t\t\t\t\tstruct f2fs_defragment)\n#define F2FS_IOC_MOVE_RANGE\t\t_IOWR(F2FS_IOCTL_MAGIC, 9,\t\\\n\t\t\t\t\t\tstruct f2fs_move_range)\n#define F2FS_IOC_FLUSH_DEVICE\t\t_IOW(F2FS_IOCTL_MAGIC, 10,\t\\\n\t\t\t\t\t\tstruct f2fs_flush_device)\n#define F2FS_IOC_GARBAGE_COLLECT_RANGE\t_IOW(F2FS_IOCTL_MAGIC, 11,\t\\\n\t\t\t\t\t\tstruct f2fs_gc_range)\n#define F2FS_IOC_GET_FEATURES\t\t_IOR(F2FS_IOCTL_MAGIC, 12, __u32)\n#define F2FS_IOC_SET_PIN_FILE\t\t_IOW(F2FS_IOCTL_MAGIC, 13, __u32)\n#define F2FS_IOC_GET_PIN_FILE\t\t_IOR(F2FS_IOCTL_MAGIC, 14, __u32)\n#define F2FS_IOC_PRECACHE_EXTENTS\t_IO(F2FS_IOCTL_MAGIC, 15)\n#define F2FS_IOC_RESIZE_FS\t\t_IOW(F2FS_IOCTL_MAGIC, 16, __u64)\n\n#define F2FS_IOC_SET_ENCRYPTION_POLICY\tFS_IOC_SET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_POLICY\tFS_IOC_GET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_PWSALT\tFS_IOC_GET_ENCRYPTION_PWSALT\n\n/*\n * should be same as XFS_IOC_GOINGDOWN.\n * Flags for going down operation used by FS_IOC_GOINGDOWN\n */\n#define F2FS_IOC_SHUTDOWN\t_IOR('X', 125, __u32)\t/* Shutdown */\n#define F2FS_GOING_DOWN_FULLSYNC\t0x0\t/* going down with full sync */\n#define F2FS_GOING_DOWN_METASYNC\t0x1\t/* going down with metadata */\n#define F2FS_GOING_DOWN_NOSYNC\t\t0x2\t/* going down */\n#define F2FS_GOING_DOWN_METAFLUSH\t0x3\t/* going down with meta flush */\n#define F2FS_GOING_DOWN_NEED_FSCK\t0x4\t/* going down to trigger fsck */\n\n#if defined(__KERNEL__) && defined(CONFIG_COMPAT)\n/*\n * ioctl commands in 32 bit emulation\n */\n#define F2FS_IOC32_GETFLAGS\t\tFS_IOC32_GETFLAGS\n#define F2FS_IOC32_SETFLAGS\t\tFS_IOC32_SETFLAGS\n#define F2FS_IOC32_GETVERSION\t\tFS_IOC32_GETVERSION\n#endif\n\n#define F2FS_IOC_FSGETXATTR\t\tFS_IOC_FSGETXATTR\n#define F2FS_IOC_FSSETXATTR\t\tFS_IOC_FSSETXATTR\n\nstruct f2fs_gc_range {\n\tu32 sync;\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_defragment {\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_move_range {\n\tu32 dst_fd;\t\t/* destination fd */\n\tu64 pos_in;\t\t/* start position in src_fd */\n\tu64 pos_out;\t\t/* start position in dst_fd */\n\tu64 len;\t\t/* size to move */\n};\n\nstruct f2fs_flush_device {\n\tu32 dev_num;\t\t/* device number to flush */\n\tu32 segments;\t\t/* # of segments to flush */\n};\n\n/* for inline stuff */\n#define DEF_INLINE_RESERVED_SIZE\t1\nstatic inline int get_extra_isize(struct inode *inode);\nstatic inline int get_inline_xattr_addrs(struct inode *inode);\n#define MAX_INLINE_DATA(inode)\t(sizeof(__le32) *\t\t\t\\\n\t\t\t\t(CUR_ADDRS_PER_INODE(inode) -\t\t\\\n\t\t\t\tget_inline_xattr_addrs(inode) -\t\\\n\t\t\t\tDEF_INLINE_RESERVED_SIZE))\n\n/* for inline dir */\n#define NR_INLINE_DENTRY(inode)\t(MAX_INLINE_DATA(inode) * BITS_PER_BYTE / \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tBITS_PER_BYTE + 1))\n#define INLINE_DENTRY_BITMAP_SIZE(inode) \\\n\tDIV_ROUND_UP(NR_INLINE_DENTRY(inode), BITS_PER_BYTE)\n#define INLINE_RESERVED_SIZE(inode)\t(MAX_INLINE_DATA(inode) - \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tNR_INLINE_DENTRY(inode) + \\\n\t\t\t\tINLINE_DENTRY_BITMAP_SIZE(inode)))\n\n/*\n * For INODE and NODE manager\n */\n/* for directory operations */\nstruct f2fs_dentry_ptr {\n\tstruct inode *inode;\n\tvoid *bitmap;\n\tstruct f2fs_dir_entry *dentry;\n\t__u8 (*filename)[F2FS_SLOT_LEN];\n\tint max;\n\tint nr_bitmap;\n};\n\nstatic inline void make_dentry_ptr_block(struct inode *inode,\n\t\tstruct f2fs_dentry_ptr *d, struct f2fs_dentry_block *t)\n{\n\td->inode = inode;\n\td->max = NR_DENTRY_IN_BLOCK;\n\td->nr_bitmap = SIZE_OF_DENTRY_BITMAP;\n\td->bitmap = t->dentry_bitmap;\n\td->dentry = t->dentry;\n\td->filename = t->filename;\n}\n\nstatic inline void make_dentry_ptr_inline(struct inode *inode,\n\t\t\t\t\tstruct f2fs_dentry_ptr *d, void *t)\n{\n\tint entry_cnt = NR_INLINE_DENTRY(inode);\n\tint bitmap_size = INLINE_DENTRY_BITMAP_SIZE(inode);\n\tint reserved_size = INLINE_RESERVED_SIZE(inode);\n\n\td->inode = inode;\n\td->max = entry_cnt;\n\td->nr_bitmap = bitmap_size;\n\td->bitmap = t;\n\td->dentry = t + bitmap_size + reserved_size;\n\td->filename = t + bitmap_size + reserved_size +\n\t\t\t\t\tSIZE_OF_DIR_ENTRY * entry_cnt;\n}\n\n/*\n * XATTR_NODE_OFFSET stores xattrs to one node block per file keeping -1\n * as its node offset to distinguish from index node blocks.\n * But some bits are used to mark the node block.\n */\n#define XATTR_NODE_OFFSET\t((((unsigned int)-1) << OFFSET_BIT_SHIFT) \\\n\t\t\t\t>> OFFSET_BIT_SHIFT)\nenum {\n\tALLOC_NODE,\t\t\t/* allocate a new node page if needed */\n\tLOOKUP_NODE,\t\t\t/* look up a node without readahead */\n\tLOOKUP_NODE_RA,\t\t\t/*\n\t\t\t\t\t * look up a node with readahead called\n\t\t\t\t\t * by get_data_block.\n\t\t\t\t\t */\n};\n\n#define DEFAULT_RETRY_IO_COUNT\t8\t/* maximum retry read IO count */\n\n/* maximum retry quota flush count */\n#define DEFAULT_RETRY_QUOTA_FLUSH_COUNT\t\t8\n\n#define F2FS_LINK_MAX\t0xffffffff\t/* maximum link count per file */\n\n#define MAX_DIR_RA_PAGES\t4\t/* maximum ra pages of dir */\n\n/* for in-memory extent cache entry */\n#define F2FS_MIN_EXTENT_LEN\t64\t/* minimum extent length */\n\n/* number of extent info in extent cache we try to shrink */\n#define EXTENT_CACHE_SHRINK_NUMBER\t128\n\nstruct rb_entry {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunsigned int ofs;\t\t/* start offset of the entry */\n\tunsigned int len;\t\t/* length of the entry */\n};\n\nstruct extent_info {\n\tunsigned int fofs;\t\t/* start offset in a file */\n\tunsigned int len;\t\t/* length of the extent */\n\tu32 blk;\t\t\t/* start block address of the extent */\n};\n\nstruct extent_node {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tstruct extent_info ei;\t\t/* extent info */\n\tstruct list_head list;\t\t/* node in global extent list of sbi */\n\tstruct extent_tree *et;\t\t/* extent tree pointer */\n};\n\nstruct extent_tree {\n\tnid_t ino;\t\t\t/* inode number */\n\tstruct rb_root_cached root;\t/* root of extent info rb-tree */\n\tstruct extent_node *cached_en;\t/* recently accessed extent node */\n\tstruct extent_info largest;\t/* largested extent info */\n\tstruct list_head list;\t\t/* to be used by sbi->zombie_list */\n\trwlock_t lock;\t\t\t/* protect extent info rb-tree */\n\tatomic_t node_cnt;\t\t/* # of extent node in rb-tree*/\n\tbool largest_updated;\t\t/* largest extent updated */\n};\n\n/*\n * This structure is taken from ext4_map_blocks.\n *\n * Note that, however, f2fs uses NEW and MAPPED flags for f2fs_map_blocks().\n */\n#define F2FS_MAP_NEW\t\t(1 << BH_New)\n#define F2FS_MAP_MAPPED\t\t(1 << BH_Mapped)\n#define F2FS_MAP_UNWRITTEN\t(1 << BH_Unwritten)\n#define F2FS_MAP_FLAGS\t\t(F2FS_MAP_NEW | F2FS_MAP_MAPPED |\\\n\t\t\t\tF2FS_MAP_UNWRITTEN)\n\nstruct f2fs_map_blocks {\n\tblock_t m_pblk;\n\tblock_t m_lblk;\n\tunsigned int m_len;\n\tunsigned int m_flags;\n\tpgoff_t *m_next_pgofs;\t\t/* point next possible non-hole pgofs */\n\tpgoff_t *m_next_extent;\t\t/* point to next possible extent */\n\tint m_seg_type;\n\tbool m_may_create;\t\t/* indicate it is from write path */\n};\n\n/* for flag in get_data_block */\nenum {\n\tF2FS_GET_BLOCK_DEFAULT,\n\tF2FS_GET_BLOCK_FIEMAP,\n\tF2FS_GET_BLOCK_BMAP,\n\tF2FS_GET_BLOCK_DIO,\n\tF2FS_GET_BLOCK_PRE_DIO,\n\tF2FS_GET_BLOCK_PRE_AIO,\n\tF2FS_GET_BLOCK_PRECACHE,\n};\n\n/*\n * i_advise uses FADVISE_XXX_BIT. We can add additional hints later.\n */\n#define FADVISE_COLD_BIT\t0x01\n#define FADVISE_LOST_PINO_BIT\t0x02\n#define FADVISE_ENCRYPT_BIT\t0x04\n#define FADVISE_ENC_NAME_BIT\t0x08\n#define FADVISE_KEEP_SIZE_BIT\t0x10\n#define FADVISE_HOT_BIT\t\t0x20\n#define FADVISE_VERITY_BIT\t0x40\t/* reserved */\n\n#define FADVISE_MODIFIABLE_BITS\t(FADVISE_COLD_BIT | FADVISE_HOT_BIT)\n\n#define file_is_cold(inode)\tis_file(inode, FADVISE_COLD_BIT)\n#define file_wrong_pino(inode)\tis_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_set_cold(inode)\tset_file(inode, FADVISE_COLD_BIT)\n#define file_lost_pino(inode)\tset_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_clear_cold(inode)\tclear_file(inode, FADVISE_COLD_BIT)\n#define file_got_pino(inode)\tclear_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_is_encrypt(inode)\tis_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_set_encrypt(inode)\tset_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_clear_encrypt(inode) clear_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_enc_name(inode)\tis_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_set_enc_name(inode) set_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_keep_isize(inode)\tis_file(inode, FADVISE_KEEP_SIZE_BIT)\n#define file_set_keep_isize(inode) set_file(inode, FADVISE_KEEP_SIZE_BIT)\n#define file_is_hot(inode)\tis_file(inode, FADVISE_HOT_BIT)\n#define file_set_hot(inode)\tset_file(inode, FADVISE_HOT_BIT)\n#define file_clear_hot(inode)\tclear_file(inode, FADVISE_HOT_BIT)\n\n#define DEF_DIR_LEVEL\t\t0\n\nenum {\n\tGC_FAILURE_PIN,\n\tGC_FAILURE_ATOMIC,\n\tMAX_GC_FAILURE\n};\n\nstruct f2fs_inode_info {\n\tstruct inode vfs_inode;\t\t/* serve a vfs inode */\n\tunsigned long i_flags;\t\t/* keep an inode flags for ioctl */\n\tunsigned char i_advise;\t\t/* use to give file attribute hints */\n\tunsigned char i_dir_level;\t/* use for dentry level for large dir */\n\tunsigned int i_current_depth;\t/* only for directory depth */\n\t/* for gc failure statistic */\n\tunsigned int i_gc_failures[MAX_GC_FAILURE];\n\tunsigned int i_pino;\t\t/* parent inode number */\n\tumode_t i_acl_mode;\t\t/* keep file acl mode temporarily */\n\n\t/* Use below internally in f2fs*/\n\tunsigned long flags;\t\t/* use to pass per-file flags */\n\tstruct rw_semaphore i_sem;\t/* protect fi info */\n\tatomic_t dirty_pages;\t\t/* # of dirty pages */\n\tf2fs_hash_t chash;\t\t/* hash value of given file name */\n\tunsigned int clevel;\t\t/* maximum level of given file name */\n\tstruct task_struct *task;\t/* lookup and create consistency */\n\tstruct task_struct *cp_task;\t/* separate cp/wb IO stats*/\n\tnid_t i_xattr_nid;\t\t/* node id that contains xattrs */\n\tloff_t\tlast_disk_size;\t\t/* lastly written file size */\n\n#ifdef CONFIG_QUOTA\n\tstruct dquot *i_dquot[MAXQUOTAS];\n\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\tstruct list_head dirty_list;\t/* dirty list for dirs and files */\n\tstruct list_head gdirty_list;\t/* linked in global dirty list */\n\tstruct list_head inmem_ilist;\t/* list for inmem inodes */\n\tstruct list_head inmem_pages;\t/* inmemory pages managed by f2fs */\n\tstruct task_struct *inmem_task;\t/* store inmemory task */\n\tstruct mutex inmem_lock;\t/* lock for inmemory pages */\n\tstruct extent_tree *extent_tree;\t/* cached extent_tree entry */\n\n\t/* avoid racing between foreground op and gc */\n\tstruct rw_semaphore i_gc_rwsem[2];\n\tstruct rw_semaphore i_mmap_sem;\n\tstruct rw_semaphore i_xattr_sem; /* avoid racing between reading and changing EAs */\n\n\tint i_extra_isize;\t\t/* size of extra space located in i_addr */\n\tkprojid_t i_projid;\t\t/* id for project quota */\n\tint i_inline_xattr_size;\t/* inline xattr size */\n\tstruct timespec64 i_crtime;\t/* inode creation time */\n\tstruct timespec64 i_disk_time[4];/* inode disk times */\n};\n\nstatic inline void get_extent_info(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\text->fofs = le32_to_cpu(i_ext->fofs);\n\text->blk = le32_to_cpu(i_ext->blk);\n\text->len = le32_to_cpu(i_ext->len);\n}\n\nstatic inline void set_raw_extent(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\ti_ext->fofs = cpu_to_le32(ext->fofs);\n\ti_ext->blk = cpu_to_le32(ext->blk);\n\ti_ext->len = cpu_to_le32(ext->len);\n}\n\nstatic inline void set_extent_info(struct extent_info *ei, unsigned int fofs,\n\t\t\t\t\t\tu32 blk, unsigned int len)\n{\n\tei->fofs = fofs;\n\tei->blk = blk;\n\tei->len = len;\n}\n\nstatic inline bool __is_discard_mergeable(struct discard_info *back,\n\t\t\tstruct discard_info *front, unsigned int max_len)\n{\n\treturn (back->lstart + back->len == front->lstart) &&\n\t\t(back->len + front->len <= max_len);\n}\n\nstatic inline bool __is_discard_back_mergeable(struct discard_info *cur,\n\t\t\tstruct discard_info *back, unsigned int max_len)\n{\n\treturn __is_discard_mergeable(back, cur, max_len);\n}\n\nstatic inline bool __is_discard_front_mergeable(struct discard_info *cur,\n\t\t\tstruct discard_info *front, unsigned int max_len)\n{\n\treturn __is_discard_mergeable(cur, front, max_len);\n}\n\nstatic inline bool __is_extent_mergeable(struct extent_info *back,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn (back->fofs + back->len == front->fofs &&\n\t\t\tback->blk + back->len == front->blk);\n}\n\nstatic inline bool __is_back_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *back)\n{\n\treturn __is_extent_mergeable(back, cur);\n}\n\nstatic inline bool __is_front_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn __is_extent_mergeable(cur, front);\n}\n\nextern void f2fs_mark_inode_dirty_sync(struct inode *inode, bool sync);\nstatic inline void __try_update_largest_extent(struct extent_tree *et,\n\t\t\t\t\t\tstruct extent_node *en)\n{\n\tif (en->ei.len > et->largest.len) {\n\t\tet->largest = en->ei;\n\t\tet->largest_updated = true;\n\t}\n}\n\n/*\n * For free nid management\n */\nenum nid_state {\n\tFREE_NID,\t\t/* newly added to free nid list */\n\tPREALLOC_NID,\t\t/* it is preallocated */\n\tMAX_NID_STATE,\n};\n\nstruct f2fs_nm_info {\n\tblock_t nat_blkaddr;\t\t/* base disk address of NAT */\n\tnid_t max_nid;\t\t\t/* maximum possible node ids */\n\tnid_t available_nids;\t\t/* # of available node ids */\n\tnid_t next_scan_nid;\t\t/* the next nid to be scanned */\n\tunsigned int ram_thresh;\t/* control the memory footprint */\n\tunsigned int ra_nid_pages;\t/* # of nid pages to be readaheaded */\n\tunsigned int dirty_nats_ratio;\t/* control dirty nats ratio threshold */\n\n\t/* NAT cache management */\n\tstruct radix_tree_root nat_root;/* root of the nat entry cache */\n\tstruct radix_tree_root nat_set_root;/* root of the nat set cache */\n\tstruct rw_semaphore nat_tree_lock;\t/* protect nat_tree_lock */\n\tstruct list_head nat_entries;\t/* cached nat entry list (clean) */\n\tspinlock_t nat_list_lock;\t/* protect clean nat entry list */\n\tunsigned int nat_cnt;\t\t/* the # of cached nat entries */\n\tunsigned int dirty_nat_cnt;\t/* total num of nat entries in set */\n\tunsigned int nat_blocks;\t/* # of nat blocks */\n\n\t/* free node ids management */\n\tstruct radix_tree_root free_nid_root;/* root of the free_nid cache */\n\tstruct list_head free_nid_list;\t\t/* list for free nids excluding preallocated nids */\n\tunsigned int nid_cnt[MAX_NID_STATE];\t/* the number of free node id */\n\tspinlock_t nid_list_lock;\t/* protect nid lists ops */\n\tstruct mutex build_lock;\t/* lock for build free nids */\n\tunsigned char **free_nid_bitmap;\n\tunsigned char *nat_block_bitmap;\n\tunsigned short *free_nid_count;\t/* free nid count of NAT block */\n\n\t/* for checkpoint */\n\tchar *nat_bitmap;\t\t/* NAT bitmap pointer */\n\n\tunsigned int nat_bits_blocks;\t/* # of nat bits blocks */\n\tunsigned char *nat_bits;\t/* NAT bits blocks */\n\tunsigned char *full_nat_bits;\t/* full NAT pages */\n\tunsigned char *empty_nat_bits;\t/* empty NAT pages */\n#ifdef CONFIG_F2FS_CHECK_FS\n\tchar *nat_bitmap_mir;\t\t/* NAT bitmap mirror */\n#endif\n\tint bitmap_size;\t\t/* bitmap size */\n};\n\n/*\n * this structure is used as one of function parameters.\n * all the information are dedicated to a given direct node block determined\n * by the data offset in a file.\n */\nstruct dnode_of_data {\n\tstruct inode *inode;\t\t/* vfs inode pointer */\n\tstruct page *inode_page;\t/* its inode page, NULL is possible */\n\tstruct page *node_page;\t\t/* cached direct node page */\n\tnid_t nid;\t\t\t/* node id of the direct node block */\n\tunsigned int ofs_in_node;\t/* data offset in the node page */\n\tbool inode_page_locked;\t\t/* inode page is locked or not */\n\tbool node_changed;\t\t/* is node block changed */\n\tchar cur_level;\t\t\t/* level of hole node page */\n\tchar max_level;\t\t\t/* level of current page located */\n\tblock_t\tdata_blkaddr;\t\t/* block address of the node block */\n};\n\nstatic inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,\n\t\tstruct page *ipage, struct page *npage, nid_t nid)\n{\n\tmemset(dn, 0, sizeof(*dn));\n\tdn->inode = inode;\n\tdn->inode_page = ipage;\n\tdn->node_page = npage;\n\tdn->nid = nid;\n}\n\n/*\n * For SIT manager\n *\n * By default, there are 6 active log areas across the whole main area.\n * When considering hot and cold data separation to reduce cleaning overhead,\n * we split 3 for data logs and 3 for node logs as hot, warm, and cold types,\n * respectively.\n * In the current design, you should not change the numbers intentionally.\n * Instead, as a mount option such as active_logs=x, you can use 2, 4, and 6\n * logs individually according to the underlying devices. (default: 6)\n * Just in case, on-disk layout covers maximum 16 logs that consist of 8 for\n * data and 8 for node logs.\n */\n#define\tNR_CURSEG_DATA_TYPE\t(3)\n#define NR_CURSEG_NODE_TYPE\t(3)\n#define NR_CURSEG_TYPE\t(NR_CURSEG_DATA_TYPE + NR_CURSEG_NODE_TYPE)\n\nenum {\n\tCURSEG_HOT_DATA\t= 0,\t/* directory entry blocks */\n\tCURSEG_WARM_DATA,\t/* data blocks */\n\tCURSEG_COLD_DATA,\t/* multimedia or GCed data blocks */\n\tCURSEG_HOT_NODE,\t/* direct node blocks of directory files */\n\tCURSEG_WARM_NODE,\t/* direct node blocks of normal files */\n\tCURSEG_COLD_NODE,\t/* indirect node blocks */\n\tNO_CHECK_TYPE,\n};\n\nstruct flush_cmd {\n\tstruct completion wait;\n\tstruct llist_node llnode;\n\tnid_t ino;\n\tint ret;\n};\n\nstruct flush_cmd_control {\n\tstruct task_struct *f2fs_issue_flush;\t/* flush thread */\n\twait_queue_head_t flush_wait_queue;\t/* waiting queue for wake-up */\n\tatomic_t issued_flush;\t\t\t/* # of issued flushes */\n\tatomic_t queued_flush;\t\t\t/* # of queued flushes */\n\tstruct llist_head issue_list;\t\t/* list for command issue */\n\tstruct llist_node *dispatch_list;\t/* list for command dispatch */\n};\n\nstruct f2fs_sm_info {\n\tstruct sit_info *sit_info;\t\t/* whole segment information */\n\tstruct free_segmap_info *free_info;\t/* free segment information */\n\tstruct dirty_seglist_info *dirty_info;\t/* dirty segment information */\n\tstruct curseg_info *curseg_array;\t/* active segment information */\n\n\tstruct rw_semaphore curseg_lock;\t/* for preventing curseg change */\n\n\tblock_t seg0_blkaddr;\t\t/* block address of 0'th segment */\n\tblock_t main_blkaddr;\t\t/* start block address of main area */\n\tblock_t ssa_blkaddr;\t\t/* start block address of SSA area */\n\n\tunsigned int segment_count;\t/* total # of segments */\n\tunsigned int main_segments;\t/* # of segments in main area */\n\tunsigned int reserved_segments;\t/* # of reserved segments */\n\tunsigned int ovp_segments;\t/* # of overprovision segments */\n\n\t/* a threshold to reclaim prefree segments */\n\tunsigned int rec_prefree_segments;\n\n\t/* for batched trimming */\n\tunsigned int trim_sections;\t\t/* # of sections to trim */\n\n\tstruct list_head sit_entry_set;\t/* sit entry set list */\n\n\tunsigned int ipu_policy;\t/* in-place-update policy */\n\tunsigned int min_ipu_util;\t/* in-place-update threshold */\n\tunsigned int min_fsync_blocks;\t/* threshold for fsync */\n\tunsigned int min_seq_blocks;\t/* threshold for sequential blocks */\n\tunsigned int min_hot_blocks;\t/* threshold for hot block allocation */\n\tunsigned int min_ssr_sections;\t/* threshold to trigger SSR allocation */\n\n\t/* for flush command control */\n\tstruct flush_cmd_control *fcc_info;\n\n\t/* for discard command control */\n\tstruct discard_cmd_control *dcc_info;\n};\n\n/*\n * For superblock\n */\n/*\n * COUNT_TYPE for monitoring\n *\n * f2fs monitors the number of several block types such as on-writeback,\n * dirty dentry blocks, dirty node blocks, and dirty meta blocks.\n */\n#define WB_DATA_TYPE(p)\t(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)\nenum count_type {\n\tF2FS_DIRTY_DENTS,\n\tF2FS_DIRTY_DATA,\n\tF2FS_DIRTY_QDATA,\n\tF2FS_DIRTY_NODES,\n\tF2FS_DIRTY_META,\n\tF2FS_INMEM_PAGES,\n\tF2FS_DIRTY_IMETA,\n\tF2FS_WB_CP_DATA,\n\tF2FS_WB_DATA,\n\tF2FS_RD_DATA,\n\tF2FS_RD_NODE,\n\tF2FS_RD_META,\n\tF2FS_DIO_WRITE,\n\tF2FS_DIO_READ,\n\tNR_COUNT_TYPE,\n};\n\n/*\n * The below are the page types of bios used in submit_bio().\n * The available types are:\n * DATA\t\t\tUser data pages. It operates as async mode.\n * NODE\t\t\tNode pages. It operates as async mode.\n * META\t\t\tFS metadata pages such as SIT, NAT, CP.\n * NR_PAGE_TYPE\t\tThe number of page types.\n * META_FLUSH\t\tMake sure the previous pages are written\n *\t\t\twith waiting the bio's completion\n * ...\t\t\tOnly can be used with META.\n */\n#define PAGE_TYPE_OF_BIO(type)\t((type) > META ? META : (type))\nenum page_type {\n\tDATA,\n\tNODE,\n\tMETA,\n\tNR_PAGE_TYPE,\n\tMETA_FLUSH,\n\tINMEM,\t\t/* the below types are used by tracepoints only. */\n\tINMEM_DROP,\n\tINMEM_INVALIDATE,\n\tINMEM_REVOKE,\n\tIPU,\n\tOPU,\n};\n\nenum temp_type {\n\tHOT = 0,\t/* must be zero for meta bio */\n\tWARM,\n\tCOLD,\n\tNR_TEMP_TYPE,\n};\n\nenum need_lock_type {\n\tLOCK_REQ = 0,\n\tLOCK_DONE,\n\tLOCK_RETRY,\n};\n\nenum cp_reason_type {\n\tCP_NO_NEEDED,\n\tCP_NON_REGULAR,\n\tCP_HARDLINK,\n\tCP_SB_NEED_CP,\n\tCP_WRONG_PINO,\n\tCP_NO_SPC_ROLL,\n\tCP_NODE_NEED_CP,\n\tCP_FASTBOOT_MODE,\n\tCP_SPEC_LOG_NUM,\n\tCP_RECOVER_DIR,\n};\n\nenum iostat_type {\n\tAPP_DIRECT_IO,\t\t\t/* app direct IOs */\n\tAPP_BUFFERED_IO,\t\t/* app buffered IOs */\n\tAPP_WRITE_IO,\t\t\t/* app write IOs */\n\tAPP_MAPPED_IO,\t\t\t/* app mapped IOs */\n\tFS_DATA_IO,\t\t\t/* data IOs from kworker/fsync/reclaimer */\n\tFS_NODE_IO,\t\t\t/* node IOs from kworker/fsync/reclaimer */\n\tFS_META_IO,\t\t\t/* meta IOs from kworker/reclaimer */\n\tFS_GC_DATA_IO,\t\t\t/* data IOs from forground gc */\n\tFS_GC_NODE_IO,\t\t\t/* node IOs from forground gc */\n\tFS_CP_DATA_IO,\t\t\t/* data IOs from checkpoint */\n\tFS_CP_NODE_IO,\t\t\t/* node IOs from checkpoint */\n\tFS_CP_META_IO,\t\t\t/* meta IOs from checkpoint */\n\tFS_DISCARD,\t\t\t/* discard */\n\tNR_IO_TYPE,\n};\n\nstruct f2fs_io_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs_sb_info pointer */\n\tnid_t ino;\t\t/* inode number */\n\tenum page_type type;\t/* contains DATA/NODE/META/META_FLUSH */\n\tenum temp_type temp;\t/* contains HOT/WARM/COLD */\n\tint op;\t\t\t/* contains REQ_OP_ */\n\tint op_flags;\t\t/* req_flag_bits */\n\tblock_t new_blkaddr;\t/* new block address to be written */\n\tblock_t old_blkaddr;\t/* old block address before Cow */\n\tstruct page *page;\t/* page to be written */\n\tstruct page *encrypted_page;\t/* encrypted page */\n\tstruct list_head list;\t\t/* serialize IOs */\n\tbool submitted;\t\t/* indicate IO submission */\n\tint need_lock;\t\t/* indicate we need to lock cp_rwsem */\n\tbool in_list;\t\t/* indicate fio is in io_list */\n\tbool is_por;\t\t/* indicate IO is from recovery or not */\n\tbool retry;\t\t/* need to reallocate block address */\n\tenum iostat_type io_type;\t/* io type */\n\tstruct writeback_control *io_wbc; /* writeback control */\n\tstruct bio **bio;\t\t/* bio for ipu */\n\tsector_t *last_block;\t\t/* last block number in bio */\n\tunsigned char version;\t\t/* version of the node */\n};\n\n#define is_read_io(rw) ((rw) == READ)\nstruct f2fs_bio_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs superblock */\n\tstruct bio *bio;\t\t/* bios to merge */\n\tsector_t last_block_in_bio;\t/* last block number */\n\tstruct f2fs_io_info fio;\t/* store buffered io info. */\n\tstruct rw_semaphore io_rwsem;\t/* blocking op for bio */\n\tspinlock_t io_lock;\t\t/* serialize DATA/NODE IOs */\n\tstruct list_head io_list;\t/* track fios */\n};\n\n#define FDEV(i)\t\t\t\t(sbi->devs[i])\n#define RDEV(i)\t\t\t\t(raw_super->devs[i])\nstruct f2fs_dev_info {\n\tstruct block_device *bdev;\n\tchar path[MAX_PATH_LEN];\n\tunsigned int total_segments;\n\tblock_t start_blk;\n\tblock_t end_blk;\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int nr_blkz;\t\t/* Total number of zones */\n\tunsigned long *blkz_seq;\t/* Bitmap indicating sequential zones */\n#endif\n};\n\nenum inode_type {\n\tDIR_INODE,\t\t\t/* for dirty dir inode */\n\tFILE_INODE,\t\t\t/* for dirty regular/symlink inode */\n\tDIRTY_META,\t\t\t/* for all dirtied inode metadata */\n\tATOMIC_FILE,\t\t\t/* for all atomic files */\n\tNR_INODE_TYPE,\n};\n\n/* for inner inode cache management */\nstruct inode_management {\n\tstruct radix_tree_root ino_root;\t/* ino entry array */\n\tspinlock_t ino_lock;\t\t\t/* for ino entry lock */\n\tstruct list_head ino_list;\t\t/* inode list head */\n\tunsigned long ino_num;\t\t\t/* number of entries */\n};\n\n/* For s_flag in struct f2fs_sb_info */\nenum {\n\tSBI_IS_DIRTY,\t\t\t\t/* dirty flag for checkpoint */\n\tSBI_IS_CLOSE,\t\t\t\t/* specify unmounting */\n\tSBI_NEED_FSCK,\t\t\t\t/* need fsck.f2fs to fix */\n\tSBI_POR_DOING,\t\t\t\t/* recovery is doing or not */\n\tSBI_NEED_SB_WRITE,\t\t\t/* need to recover superblock */\n\tSBI_NEED_CP,\t\t\t\t/* need to checkpoint */\n\tSBI_IS_SHUTDOWN,\t\t\t/* shutdown by ioctl */\n\tSBI_IS_RECOVERED,\t\t\t/* recovered orphan/data */\n\tSBI_CP_DISABLED,\t\t\t/* CP was disabled last mount */\n\tSBI_CP_DISABLED_QUICK,\t\t\t/* CP was disabled quickly */\n\tSBI_QUOTA_NEED_FLUSH,\t\t\t/* need to flush quota info in CP */\n\tSBI_QUOTA_SKIP_FLUSH,\t\t\t/* skip flushing quota in current CP */\n\tSBI_QUOTA_NEED_REPAIR,\t\t\t/* quota file may be corrupted */\n\tSBI_IS_RESIZEFS,\t\t\t/* resizefs is in process */\n};\n\nenum {\n\tCP_TIME,\n\tREQ_TIME,\n\tDISCARD_TIME,\n\tGC_TIME,\n\tDISABLE_TIME,\n\tUMOUNT_DISCARD_TIMEOUT,\n\tMAX_TIME,\n};\n\nenum {\n\tGC_NORMAL,\n\tGC_IDLE_CB,\n\tGC_IDLE_GREEDY,\n\tGC_URGENT,\n};\n\nenum {\n\tWHINT_MODE_OFF,\t\t/* not pass down write hints */\n\tWHINT_MODE_USER,\t/* try to pass down hints given by users */\n\tWHINT_MODE_FS,\t\t/* pass down hints with F2FS policy */\n};\n\nenum {\n\tALLOC_MODE_DEFAULT,\t/* stay default */\n\tALLOC_MODE_REUSE,\t/* reuse segments as much as possible */\n};\n\nenum fsync_mode {\n\tFSYNC_MODE_POSIX,\t/* fsync follows posix semantics */\n\tFSYNC_MODE_STRICT,\t/* fsync behaves in line with ext4 */\n\tFSYNC_MODE_NOBARRIER,\t/* fsync behaves nobarrier based on posix */\n};\n\n#ifdef CONFIG_FS_ENCRYPTION\n#define DUMMY_ENCRYPTION_ENABLED(sbi) \\\n\t\t\t(unlikely(F2FS_OPTION(sbi).test_dummy_encryption))\n#else\n#define DUMMY_ENCRYPTION_ENABLED(sbi) (0)\n#endif\n\nstruct f2fs_sb_info {\n\tstruct super_block *sb;\t\t\t/* pointer to VFS super block */\n\tstruct proc_dir_entry *s_proc;\t\t/* proc entry */\n\tstruct f2fs_super_block *raw_super;\t/* raw super block pointer */\n\tstruct rw_semaphore sb_lock;\t\t/* lock for raw super block */\n\tint valid_super_block;\t\t\t/* valid super block no */\n\tunsigned long s_flag;\t\t\t\t/* flags for sbi */\n\tstruct mutex writepages;\t\t/* mutex for writepages() */\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int blocks_per_blkz;\t\t/* F2FS blocks per zone */\n\tunsigned int log_blocks_per_blkz;\t/* log2 F2FS blocks per zone */\n#endif\n\n\t/* for node-related operations */\n\tstruct f2fs_nm_info *nm_info;\t\t/* node manager */\n\tstruct inode *node_inode;\t\t/* cache node blocks */\n\n\t/* for segment-related operations */\n\tstruct f2fs_sm_info *sm_info;\t\t/* segment manager */\n\n\t/* for bio operations */\n\tstruct f2fs_bio_info *write_io[NR_PAGE_TYPE];\t/* for write bios */\n\t/* keep migration IO order for LFS mode */\n\tstruct rw_semaphore io_order_lock;\n\tmempool_t *write_io_dummy;\t\t/* Dummy pages */\n\n\t/* for checkpoint */\n\tstruct f2fs_checkpoint *ckpt;\t\t/* raw checkpoint pointer */\n\tint cur_cp_pack;\t\t\t/* remain current cp pack */\n\tspinlock_t cp_lock;\t\t\t/* for flag in ckpt */\n\tstruct inode *meta_inode;\t\t/* cache meta blocks */\n\tstruct mutex cp_mutex;\t\t\t/* checkpoint procedure lock */\n\tstruct rw_semaphore cp_rwsem;\t\t/* blocking FS operations */\n\tstruct rw_semaphore node_write;\t\t/* locking node writes */\n\tstruct rw_semaphore node_change;\t/* locking node change */\n\twait_queue_head_t cp_wait;\n\tunsigned long last_time[MAX_TIME];\t/* to store time in jiffies */\n\tlong interval_time[MAX_TIME];\t\t/* to store thresholds */\n\n\tstruct inode_management im[MAX_INO_ENTRY];      /* manage inode cache */\n\n\tspinlock_t fsync_node_lock;\t\t/* for node entry lock */\n\tstruct list_head fsync_node_list;\t/* node list head */\n\tunsigned int fsync_seg_id;\t\t/* sequence id */\n\tunsigned int fsync_node_num;\t\t/* number of node entries */\n\n\t/* for orphan inode, use 0'th array */\n\tunsigned int max_orphans;\t\t/* max orphan inodes */\n\n\t/* for inode management */\n\tstruct list_head inode_list[NR_INODE_TYPE];\t/* dirty inode list */\n\tspinlock_t inode_lock[NR_INODE_TYPE];\t/* for dirty inode list lock */\n\tstruct mutex flush_lock;\t\t/* for flush exclusion */\n\n\t/* for extent tree cache */\n\tstruct radix_tree_root extent_tree_root;/* cache extent cache entries */\n\tstruct mutex extent_tree_lock;\t/* locking extent radix tree */\n\tstruct list_head extent_list;\t\t/* lru list for shrinker */\n\tspinlock_t extent_lock;\t\t\t/* locking extent lru list */\n\tatomic_t total_ext_tree;\t\t/* extent tree count */\n\tstruct list_head zombie_list;\t\t/* extent zombie tree list */\n\tatomic_t total_zombie_tree;\t\t/* extent zombie tree count */\n\tatomic_t total_ext_node;\t\t/* extent info count */\n\n\t/* basic filesystem units */\n\tunsigned int log_sectors_per_block;\t/* log2 sectors per block */\n\tunsigned int log_blocksize;\t\t/* log2 block size */\n\tunsigned int blocksize;\t\t\t/* block size */\n\tunsigned int root_ino_num;\t\t/* root inode number*/\n\tunsigned int node_ino_num;\t\t/* node inode number*/\n\tunsigned int meta_ino_num;\t\t/* meta inode number*/\n\tunsigned int log_blocks_per_seg;\t/* log2 blocks per segment */\n\tunsigned int blocks_per_seg;\t\t/* blocks per segment */\n\tunsigned int segs_per_sec;\t\t/* segments per section */\n\tunsigned int secs_per_zone;\t\t/* sections per zone */\n\tunsigned int total_sections;\t\t/* total section count */\n\tstruct mutex resize_mutex;\t\t/* for resize exclusion */\n\tunsigned int total_node_count;\t\t/* total node block count */\n\tunsigned int total_valid_node_count;\t/* valid node block count */\n\tloff_t max_file_blocks;\t\t\t/* max block index of file */\n\tint dir_level;\t\t\t\t/* directory level */\n\tint readdir_ra;\t\t\t\t/* readahead inode in readdir */\n\n\tblock_t user_block_count;\t\t/* # of user blocks */\n\tblock_t total_valid_block_count;\t/* # of valid blocks */\n\tblock_t discard_blks;\t\t\t/* discard command candidats */\n\tblock_t last_valid_block_count;\t\t/* for recovery */\n\tblock_t reserved_blocks;\t\t/* configurable reserved blocks */\n\tblock_t current_reserved_blocks;\t/* current reserved blocks */\n\n\t/* Additional tracking for no checkpoint mode */\n\tblock_t unusable_block_count;\t\t/* # of blocks saved by last cp */\n\n\tunsigned int nquota_files;\t\t/* # of quota sysfile */\n\tstruct rw_semaphore quota_sem;\t\t/* blocking cp for flags */\n\n\t/* # of pages, see count_type */\n\tatomic_t nr_pages[NR_COUNT_TYPE];\n\t/* # of allocated blocks */\n\tstruct percpu_counter alloc_valid_block_count;\n\n\t/* writeback control */\n\tatomic_t wb_sync_req[META];\t/* count # of WB_SYNC threads */\n\n\t/* valid inode count */\n\tstruct percpu_counter total_valid_inode_count;\n\n\tstruct f2fs_mount_info mount_opt;\t/* mount options */\n\n\t/* for cleaning operations */\n\tstruct mutex gc_mutex;\t\t\t/* mutex for GC */\n\tstruct f2fs_gc_kthread\t*gc_thread;\t/* GC thread */\n\tunsigned int cur_victim_sec;\t\t/* current victim section num */\n\tunsigned int gc_mode;\t\t\t/* current GC state */\n\tunsigned int next_victim_seg[2];\t/* next segment in victim section */\n\t/* for skip statistic */\n\tunsigned long long skipped_atomic_files[2];\t/* FG_GC and BG_GC */\n\tunsigned long long skipped_gc_rwsem;\t\t/* FG_GC only */\n\n\t/* threshold for gc trials on pinned files */\n\tu64 gc_pin_file_threshold;\n\n\t/* maximum # of trials to find a victim segment for SSR and GC */\n\tunsigned int max_victim_search;\n\t/* migration granularity of garbage collection, unit: segment */\n\tunsigned int migration_granularity;\n\n\t/*\n\t * for stat information.\n\t * one is for the LFS mode, and the other is for the SSR mode.\n\t */\n#ifdef CONFIG_F2FS_STAT_FS\n\tstruct f2fs_stat_info *stat_info;\t/* FS status information */\n\tatomic_t meta_count[META_MAX];\t\t/* # of meta blocks */\n\tunsigned int segment_count[2];\t\t/* # of allocated segments */\n\tunsigned int block_count[2];\t\t/* # of allocated blocks */\n\tatomic_t inplace_count;\t\t/* # of inplace update */\n\tatomic64_t total_hit_ext;\t\t/* # of lookup extent cache */\n\tatomic64_t read_hit_rbtree;\t\t/* # of hit rbtree extent node */\n\tatomic64_t read_hit_largest;\t\t/* # of hit largest extent node */\n\tatomic64_t read_hit_cached;\t\t/* # of hit cached extent node */\n\tatomic_t inline_xattr;\t\t\t/* # of inline_xattr inodes */\n\tatomic_t inline_inode;\t\t\t/* # of inline_data inodes */\n\tatomic_t inline_dir;\t\t\t/* # of inline_dentry inodes */\n\tatomic_t aw_cnt;\t\t\t/* # of atomic writes */\n\tatomic_t vw_cnt;\t\t\t/* # of volatile writes */\n\tatomic_t max_aw_cnt;\t\t\t/* max # of atomic writes */\n\tatomic_t max_vw_cnt;\t\t\t/* max # of volatile writes */\n\tint bg_gc;\t\t\t\t/* background gc calls */\n\tunsigned int io_skip_bggc;\t\t/* skip background gc for in-flight IO */\n\tunsigned int other_skip_bggc;\t\t/* skip background gc for other reasons */\n\tunsigned int ndirty_inode[NR_INODE_TYPE];\t/* # of dirty inodes */\n#endif\n\tspinlock_t stat_lock;\t\t\t/* lock for stat operations */\n\n\t/* For app/fs IO statistics */\n\tspinlock_t iostat_lock;\n\tunsigned long long write_iostat[NR_IO_TYPE];\n\tbool iostat_enable;\n\n\t/* For sysfs suppport */\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\n\t/* For shrinker support */\n\tstruct list_head s_list;\n\tint s_ndevs;\t\t\t\t/* number of devices */\n\tstruct f2fs_dev_info *devs;\t\t/* for device list */\n\tunsigned int dirty_device;\t\t/* for checkpoint data flush */\n\tspinlock_t dev_lock;\t\t\t/* protect dirty_device */\n\tstruct mutex umount_mutex;\n\tunsigned int shrinker_run_no;\n\n\t/* For write statistics */\n\tu64 sectors_written_start;\n\tu64 kbytes_written;\n\n\t/* Reference to checksum algorithm driver via cryptoapi */\n\tstruct crypto_shash *s_chksum_driver;\n\n\t/* Precomputed FS UUID checksum for seeding other checksums */\n\t__u32 s_chksum_seed;\n};\n\nstruct f2fs_private_dio {\n\tstruct inode *inode;\n\tvoid *orig_private;\n\tbio_end_io_t *orig_end_io;\n\tbool write;\n};\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n#define f2fs_show_injection_info(type)\t\t\t\t\t\\\n\tprintk_ratelimited(\"%sF2FS-fs : inject %s in %s of %pS\\n\",\t\\\n\t\tKERN_INFO, f2fs_fault_name[type],\t\t\t\\\n\t\t__func__, __builtin_return_address(0))\nstatic inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_fault_info *ffi = &F2FS_OPTION(sbi).fault_info;\n\n\tif (!ffi->inject_rate)\n\t\treturn false;\n\n\tif (!IS_FAULT_SET(ffi, type))\n\t\treturn false;\n\n\tatomic_inc(&ffi->inject_ops);\n\tif (atomic_read(&ffi->inject_ops) >= ffi->inject_rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\treturn true;\n\t}\n\treturn false;\n}\n#else\n#define f2fs_show_injection_info(type) do { } while (0)\nstatic inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Test if the mounted volume is a multi-device volume.\n *   - For a single regular disk volume, sbi->s_ndevs is 0.\n *   - For a single zoned disk volume, sbi->s_ndevs is 1.\n *   - For a multi-device volume, sbi->s_ndevs is always 2 or more.\n */\nstatic inline bool f2fs_is_multi_device(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->s_ndevs > 1;\n}\n\n/* For write statistics. Suppose sector size is 512 bytes,\n * and the return value is in kbytes. s is of struct f2fs_sb_info.\n */\n#define BD_PART_WRITTEN(s)\t\t\t\t\t\t \\\n(((u64)part_stat_read((s)->sb->s_bdev->bd_part, sectors[STAT_WRITE]) -   \\\n\t\t(s)->sectors_written_start) >> 1)\n\nstatic inline void f2fs_update_time(struct f2fs_sb_info *sbi, int type)\n{\n\tunsigned long now = jiffies;\n\n\tsbi->last_time[type] = now;\n\n\t/* DISCARD_TIME and GC_TIME are based on REQ_TIME */\n\tif (type == REQ_TIME) {\n\t\tsbi->last_time[DISCARD_TIME] = now;\n\t\tsbi->last_time[GC_TIME] = now;\n\t}\n}\n\nstatic inline bool f2fs_time_over(struct f2fs_sb_info *sbi, int type)\n{\n\tunsigned long interval = sbi->interval_time[type] * HZ;\n\n\treturn time_after(jiffies, sbi->last_time[type] + interval);\n}\n\nstatic inline unsigned int f2fs_time_to_wait(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type)\n{\n\tunsigned long interval = sbi->interval_time[type] * HZ;\n\tunsigned int wait_ms = 0;\n\tlong delta;\n\n\tdelta = (sbi->last_time[type] + interval) - jiffies;\n\tif (delta > 0)\n\t\twait_ms = jiffies_to_msecs(delta);\n\n\treturn wait_ms;\n}\n\n/*\n * Inline functions\n */\nstatic inline u32 __f2fs_crc32(struct f2fs_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\tstruct {\n\t\tstruct shash_desc shash;\n\t\tchar ctx[4];\n\t} desc;\n\tint err;\n\n\tBUG_ON(crypto_shash_descsize(sbi->s_chksum_driver) != sizeof(desc.ctx));\n\n\tdesc.shash.tfm = sbi->s_chksum_driver;\n\t*(u32 *)desc.ctx = crc;\n\n\terr = crypto_shash_update(&desc.shash, address, length);\n\tBUG_ON(err);\n\n\treturn *(u32 *)desc.ctx;\n}\n\nstatic inline u32 f2fs_crc32(struct f2fs_sb_info *sbi, const void *address,\n\t\t\t   unsigned int length)\n{\n\treturn __f2fs_crc32(sbi, F2FS_SUPER_MAGIC, address, length);\n}\n\nstatic inline bool f2fs_crc_valid(struct f2fs_sb_info *sbi, __u32 blk_crc,\n\t\t\t\t  void *buf, size_t buf_size)\n{\n\treturn f2fs_crc32(sbi, buf, buf_size) == blk_crc;\n}\n\nstatic inline u32 f2fs_chksum(struct f2fs_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\treturn __f2fs_crc32(sbi, crc, address, length);\n}\n\nstatic inline struct f2fs_inode_info *F2FS_I(struct inode *inode)\n{\n\treturn container_of(inode, struct f2fs_inode_info, vfs_inode);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstatic inline struct f2fs_sb_info *F2FS_I_SB(struct inode *inode)\n{\n\treturn F2FS_SB(inode->i_sb);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_M_SB(struct address_space *mapping)\n{\n\treturn F2FS_I_SB(mapping->host);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page->mapping);\n}\n\nstatic inline struct f2fs_super_block *F2FS_RAW_SUPER(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_super_block *)(sbi->raw_super);\n}\n\nstatic inline struct f2fs_checkpoint *F2FS_CKPT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_checkpoint *)(sbi->ckpt);\n}\n\nstatic inline struct f2fs_node *F2FS_NODE(struct page *page)\n{\n\treturn (struct f2fs_node *)page_address(page);\n}\n\nstatic inline struct f2fs_inode *F2FS_INODE(struct page *page)\n{\n\treturn &((struct f2fs_node *)page_address(page))->i;\n}\n\nstatic inline struct f2fs_nm_info *NM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_nm_info *)(sbi->nm_info);\n}\n\nstatic inline struct f2fs_sm_info *SM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_sm_info *)(sbi->sm_info);\n}\n\nstatic inline struct sit_info *SIT_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct sit_info *)(SM_I(sbi)->sit_info);\n}\n\nstatic inline struct free_segmap_info *FREE_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct free_segmap_info *)(SM_I(sbi)->free_info);\n}\n\nstatic inline struct dirty_seglist_info *DIRTY_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct dirty_seglist_info *)(SM_I(sbi)->dirty_info);\n}\n\nstatic inline struct address_space *META_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->meta_inode->i_mapping;\n}\n\nstatic inline struct address_space *NODE_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->node_inode->i_mapping;\n}\n\nstatic inline bool is_sbi_flag_set(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\treturn test_bit(type, &sbi->s_flag);\n}\n\nstatic inline void set_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tset_bit(type, &sbi->s_flag);\n}\n\nstatic inline void clear_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tclear_bit(type, &sbi->s_flag);\n}\n\nstatic inline unsigned long long cur_cp_version(struct f2fs_checkpoint *cp)\n{\n\treturn le64_to_cpu(cp->checkpoint_ver);\n}\n\nstatic inline unsigned long f2fs_qf_ino(struct super_block *sb, int type)\n{\n\tif (type < F2FS_MAX_QUOTAS)\n\t\treturn le32_to_cpu(F2FS_SB(sb)->raw_super->qf_ino[type]);\n\treturn 0;\n}\n\nstatic inline __u64 cur_cp_crc(struct f2fs_checkpoint *cp)\n{\n\tsize_t crc_offset = le32_to_cpu(cp->checksum_offset);\n\treturn le32_to_cpu(*((__le32 *)((unsigned char *)cp + crc_offset)));\n}\n\nstatic inline bool __is_set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\n\treturn ckpt_flags & f;\n}\n\nstatic inline bool is_set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\treturn __is_set_ckpt_flags(F2FS_CKPT(sbi), f);\n}\n\nstatic inline void __set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags |= f;\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__set_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void __clear_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags &= (~f);\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void clear_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void disable_nat_bits(struct f2fs_sb_info *sbi, bool lock)\n{\n\tunsigned long flags;\n\n\t/*\n\t * In order to re-enable nat_bits we need to call fsck.f2fs by\n\t * set_sbi_flag(sbi, SBI_NEED_FSCK). But it may give huge cost,\n\t * so let's rely on regular fsck or unclean shutdown.\n\t */\n\n\tif (lock)\n\t\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), CP_NAT_BITS_FLAG);\n\tkvfree(NM_I(sbi)->nat_bits);\n\tNM_I(sbi)->nat_bits = NULL;\n\tif (lock)\n\t\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline bool enabled_nat_bits(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc)\n{\n\tbool set = is_set_ckpt_flags(sbi, CP_NAT_BITS_FLAG);\n\n\treturn (cpc) ? (cpc->reason & CP_UMOUNT) && set : set;\n}\n\nstatic inline void f2fs_lock_op(struct f2fs_sb_info *sbi)\n{\n\tdown_read(&sbi->cp_rwsem);\n}\n\nstatic inline int f2fs_trylock_op(struct f2fs_sb_info *sbi)\n{\n\treturn down_read_trylock(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_op(struct f2fs_sb_info *sbi)\n{\n\tup_read(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_lock_all(struct f2fs_sb_info *sbi)\n{\n\tdown_write(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_all(struct f2fs_sb_info *sbi)\n{\n\tup_write(&sbi->cp_rwsem);\n}\n\nstatic inline int __get_cp_reason(struct f2fs_sb_info *sbi)\n{\n\tint reason = CP_SYNC;\n\n\tif (test_opt(sbi, FASTBOOT))\n\t\treason = CP_FASTBOOT;\n\tif (is_sbi_flag_set(sbi, SBI_IS_CLOSE))\n\t\treason = CP_UMOUNT;\n\treturn reason;\n}\n\nstatic inline bool __remain_node_summaries(int reason)\n{\n\treturn (reason & (CP_UMOUNT | CP_FASTBOOT));\n}\n\nstatic inline bool __exist_node_summaries(struct f2fs_sb_info *sbi)\n{\n\treturn (is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG) ||\n\t\t\tis_set_ckpt_flags(sbi, CP_FASTBOOT_FLAG));\n}\n\n/*\n * Check whether the inode has blocks or not\n */\nstatic inline int F2FS_HAS_BLOCKS(struct inode *inode)\n{\n\tblock_t xattr_block = F2FS_I(inode)->i_xattr_nid ? 1 : 0;\n\n\treturn (inode->i_blocks >> F2FS_LOG_SECTORS_PER_BLOCK) > xattr_block;\n}\n\nstatic inline bool f2fs_has_xattr_block(unsigned int ofs)\n{\n\treturn ofs == XATTR_NODE_OFFSET;\n}\n\nstatic inline bool __allow_reserved_blocks(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool cap)\n{\n\tif (!inode)\n\t\treturn true;\n\tif (!test_opt(sbi, RESERVE_ROOT))\n\t\treturn false;\n\tif (IS_NOQUOTA(inode))\n\t\treturn true;\n\tif (uid_eq(F2FS_OPTION(sbi).s_resuid, current_fsuid()))\n\t\treturn true;\n\tif (!gid_eq(F2FS_OPTION(sbi).s_resgid, GLOBAL_ROOT_GID) &&\n\t\t\t\t\tin_group_p(F2FS_OPTION(sbi).s_resgid))\n\t\treturn true;\n\tif (cap && capable(CAP_SYS_RESOURCE))\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *, block_t, bool, bool);\nstatic inline int inc_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t struct inode *inode, blkcnt_t *count)\n{\n\tblkcnt_t diff = 0, release = 0;\n\tblock_t avail_user_block_count;\n\tint ret;\n\n\tret = dquot_reserve_block(inode, *count);\n\tif (ret)\n\t\treturn ret;\n\n\tif (time_to_inject(sbi, FAULT_BLOCK)) {\n\t\tf2fs_show_injection_info(FAULT_BLOCK);\n\t\trelease = *count;\n\t\tgoto enospc;\n\t}\n\n\t/*\n\t * let's increase this in prior to actual block count change in order\n\t * for f2fs_sync_file to avoid data races when deciding checkpoint.\n\t */\n\tpercpu_counter_add(&sbi->alloc_valid_block_count, (*count));\n\n\tspin_lock(&sbi->stat_lock);\n\tsbi->total_valid_block_count += (block_t)(*count);\n\tavail_user_block_count = sbi->user_block_count -\n\t\t\t\t\tsbi->current_reserved_blocks;\n\n\tif (!__allow_reserved_blocks(sbi, inode, true))\n\t\tavail_user_block_count -= F2FS_OPTION(sbi).root_reserved_blocks;\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {\n\t\tif (avail_user_block_count > sbi->unusable_block_count)\n\t\t\tavail_user_block_count -= sbi->unusable_block_count;\n\t\telse\n\t\t\tavail_user_block_count = 0;\n\t}\n\tif (unlikely(sbi->total_valid_block_count > avail_user_block_count)) {\n\t\tdiff = sbi->total_valid_block_count - avail_user_block_count;\n\t\tif (diff > *count)\n\t\t\tdiff = *count;\n\t\t*count -= diff;\n\t\trelease = diff;\n\t\tsbi->total_valid_block_count -= diff;\n\t\tif (!*count) {\n\t\t\tspin_unlock(&sbi->stat_lock);\n\t\t\tgoto enospc;\n\t\t}\n\t}\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (unlikely(release)) {\n\t\tpercpu_counter_sub(&sbi->alloc_valid_block_count, release);\n\t\tdquot_release_reservation_block(inode, release);\n\t}\n\tf2fs_i_blocks_write(inode, *count, true, true);\n\treturn 0;\n\nenospc:\n\tpercpu_counter_sub(&sbi->alloc_valid_block_count, release);\n\tdquot_release_reservation_block(inode, release);\n\treturn -ENOSPC;\n}\n\n__printf(2, 3)\nvoid f2fs_printk(struct f2fs_sb_info *sbi, const char *fmt, ...);\n\n#define f2fs_err(sbi, fmt, ...)\t\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_ERR fmt, ##__VA_ARGS__)\n#define f2fs_warn(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_WARNING fmt, ##__VA_ARGS__)\n#define f2fs_notice(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_NOTICE fmt, ##__VA_ARGS__)\n#define f2fs_info(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_INFO fmt, ##__VA_ARGS__)\n#define f2fs_debug(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_DEBUG fmt, ##__VA_ARGS__)\n\nstatic inline void dec_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tblock_t count)\n{\n\tblkcnt_t sectors = count << F2FS_LOG_SECTORS_PER_BLOCK;\n\n\tspin_lock(&sbi->stat_lock);\n\tf2fs_bug_on(sbi, sbi->total_valid_block_count < (block_t) count);\n\tsbi->total_valid_block_count -= (block_t)count;\n\tif (sbi->reserved_blocks &&\n\t\tsbi->current_reserved_blocks < sbi->reserved_blocks)\n\t\tsbi->current_reserved_blocks = min(sbi->reserved_blocks,\n\t\t\t\t\tsbi->current_reserved_blocks + count);\n\tspin_unlock(&sbi->stat_lock);\n\tif (unlikely(inode->i_blocks < sectors)) {\n\t\tf2fs_warn(sbi, \"Inconsistent i_blocks, ino:%lu, iblocks:%llu, sectors:%llu\",\n\t\t\t  inode->i_ino,\n\t\t\t  (unsigned long long)inode->i_blocks,\n\t\t\t  (unsigned long long)sectors);\n\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\t\treturn;\n\t}\n\tf2fs_i_blocks_write(inode, count, false, true);\n}\n\nstatic inline void inc_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_inc(&sbi->nr_pages[count_type]);\n\n\tif (count_type == F2FS_DIRTY_DENTS ||\n\t\t\tcount_type == F2FS_DIRTY_NODES ||\n\t\t\tcount_type == F2FS_DIRTY_META ||\n\t\t\tcount_type == F2FS_DIRTY_QDATA ||\n\t\t\tcount_type == F2FS_DIRTY_IMETA)\n\t\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n}\n\nstatic inline void inode_inc_dirty_pages(struct inode *inode)\n{\n\tatomic_inc(&F2FS_I(inode)->dirty_pages);\n\tinc_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n\tif (IS_NOQUOTA(inode))\n\t\tinc_page_count(F2FS_I_SB(inode), F2FS_DIRTY_QDATA);\n}\n\nstatic inline void dec_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_dec(&sbi->nr_pages[count_type]);\n}\n\nstatic inline void inode_dec_dirty_pages(struct inode *inode)\n{\n\tif (!S_ISDIR(inode->i_mode) && !S_ISREG(inode->i_mode) &&\n\t\t\t!S_ISLNK(inode->i_mode))\n\t\treturn;\n\n\tatomic_dec(&F2FS_I(inode)->dirty_pages);\n\tdec_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n\tif (IS_NOQUOTA(inode))\n\t\tdec_page_count(F2FS_I_SB(inode), F2FS_DIRTY_QDATA);\n}\n\nstatic inline s64 get_pages(struct f2fs_sb_info *sbi, int count_type)\n{\n\treturn atomic_read(&sbi->nr_pages[count_type]);\n}\n\nstatic inline int get_dirty_pages(struct inode *inode)\n{\n\treturn atomic_read(&F2FS_I(inode)->dirty_pages);\n}\n\nstatic inline int get_blocktype_secs(struct f2fs_sb_info *sbi, int block_type)\n{\n\tunsigned int pages_per_sec = sbi->segs_per_sec * sbi->blocks_per_seg;\n\tunsigned int segs = (get_pages(sbi, block_type) + pages_per_sec - 1) >>\n\t\t\t\t\t\tsbi->log_blocks_per_seg;\n\n\treturn segs / sbi->segs_per_sec;\n}\n\nstatic inline block_t valid_user_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_block_count;\n}\n\nstatic inline block_t discard_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->discard_blks;\n}\n\nstatic inline unsigned long __bitmap_size(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\n\t/* return NAT or SIT bitmap */\n\tif (flag == NAT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->nat_ver_bitmap_bytesize);\n\telse if (flag == SIT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->sit_ver_bitmap_bytesize);\n\n\treturn 0;\n}\n\nstatic inline block_t __cp_payload(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_payload);\n}\n\nstatic inline void *__bitmap_ptr(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tint offset;\n\n\tif (is_set_ckpt_flags(sbi, CP_LARGE_NAT_BITMAP_FLAG)) {\n\t\toffset = (flag == SIT_BITMAP) ?\n\t\t\tle32_to_cpu(ckpt->nat_ver_bitmap_bytesize) : 0;\n\t\t/*\n\t\t * if large_nat_bitmap feature is enabled, leave checksum\n\t\t * protection for all nat/sit bitmaps.\n\t\t */\n\t\treturn &ckpt->sit_nat_version_bitmap + offset + sizeof(__le32);\n\t}\n\n\tif (__cp_payload(sbi) > 0) {\n\t\tif (flag == NAT_BITMAP)\n\t\t\treturn &ckpt->sit_nat_version_bitmap;\n\t\telse\n\t\t\treturn (unsigned char *)ckpt + F2FS_BLKSIZE;\n\t} else {\n\t\toffset = (flag == NAT_BITMAP) ?\n\t\t\tle32_to_cpu(ckpt->sit_ver_bitmap_bytesize) : 0;\n\t\treturn &ckpt->sit_nat_version_bitmap + offset;\n\t}\n}\n\nstatic inline block_t __start_cp_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 2)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline block_t __start_cp_next_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 1)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline void __set_cp_next_pack(struct f2fs_sb_info *sbi)\n{\n\tsbi->cur_cp_pack = (sbi->cur_cp_pack == 1) ? 2 : 1;\n}\n\nstatic inline block_t __start_sum_addr(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_start_sum);\n}\n\nstatic inline int inc_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tblock_t\tvalid_block_count;\n\tunsigned int valid_node_count, user_block_count;\n\tint err;\n\n\tif (is_inode) {\n\t\tif (inode) {\n\t\t\terr = dquot_alloc_inode(inode);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t} else {\n\t\terr = dquot_reserve_block(inode, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (time_to_inject(sbi, FAULT_BLOCK)) {\n\t\tf2fs_show_injection_info(FAULT_BLOCK);\n\t\tgoto enospc;\n\t}\n\n\tspin_lock(&sbi->stat_lock);\n\n\tvalid_block_count = sbi->total_valid_block_count +\n\t\t\t\t\tsbi->current_reserved_blocks + 1;\n\n\tif (!__allow_reserved_blocks(sbi, inode, false))\n\t\tvalid_block_count += F2FS_OPTION(sbi).root_reserved_blocks;\n\tuser_block_count = sbi->user_block_count;\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))\n\t\tuser_block_count -= sbi->unusable_block_count;\n\n\tif (unlikely(valid_block_count > user_block_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tvalid_node_count = sbi->total_valid_node_count + 1;\n\tif (unlikely(valid_node_count > sbi->total_node_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tsbi->total_valid_node_count++;\n\tsbi->total_valid_block_count++;\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (inode) {\n\t\tif (is_inode)\n\t\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t\telse\n\t\t\tf2fs_i_blocks_write(inode, 1, true, true);\n\t}\n\n\tpercpu_counter_inc(&sbi->alloc_valid_block_count);\n\treturn 0;\n\nenospc:\n\tif (is_inode) {\n\t\tif (inode)\n\t\t\tdquot_free_inode(inode);\n\t} else {\n\t\tdquot_release_reservation_block(inode, 1);\n\t}\n\treturn -ENOSPC;\n}\n\nstatic inline void dec_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tspin_lock(&sbi->stat_lock);\n\n\tf2fs_bug_on(sbi, !sbi->total_valid_block_count);\n\tf2fs_bug_on(sbi, !sbi->total_valid_node_count);\n\n\tsbi->total_valid_node_count--;\n\tsbi->total_valid_block_count--;\n\tif (sbi->reserved_blocks &&\n\t\tsbi->current_reserved_blocks < sbi->reserved_blocks)\n\t\tsbi->current_reserved_blocks++;\n\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (is_inode) {\n\t\tdquot_free_inode(inode);\n\t} else {\n\t\tif (unlikely(inode->i_blocks == 0)) {\n\t\t\tf2fs_warn(sbi, \"Inconsistent i_blocks, ino:%lu, iblocks:%llu\",\n\t\t\t\t  inode->i_ino,\n\t\t\t\t  (unsigned long long)inode->i_blocks);\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\t\t\treturn;\n\t\t}\n\t\tf2fs_i_blocks_write(inode, 1, false, true);\n\t}\n}\n\nstatic inline unsigned int valid_node_count(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_node_count;\n}\n\nstatic inline void inc_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_inc(&sbi->total_valid_inode_count);\n}\n\nstatic inline void dec_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_dec(&sbi->total_valid_inode_count);\n}\n\nstatic inline s64 valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\treturn percpu_counter_sum_positive(&sbi->total_valid_inode_count);\n}\n\nstatic inline struct page *f2fs_grab_cache_page(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t index, bool for_write)\n{\n\tstruct page *page;\n\n\tif (IS_ENABLED(CONFIG_F2FS_FAULT_INJECTION)) {\n\t\tif (!for_write)\n\t\t\tpage = find_get_page_flags(mapping, index,\n\t\t\t\t\t\t\tFGP_LOCK | FGP_ACCESSED);\n\t\telse\n\t\t\tpage = find_lock_page(mapping, index);\n\t\tif (page)\n\t\t\treturn page;\n\n\t\tif (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_ALLOC)) {\n\t\t\tf2fs_show_injection_info(FAULT_PAGE_ALLOC);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (!for_write)\n\t\treturn grab_cache_page(mapping, index);\n\treturn grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);\n}\n\nstatic inline struct page *f2fs_pagecache_get_page(\n\t\t\t\tstruct address_space *mapping, pgoff_t index,\n\t\t\t\tint fgp_flags, gfp_t gfp_mask)\n{\n\tif (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_GET)) {\n\t\tf2fs_show_injection_info(FAULT_PAGE_GET);\n\t\treturn NULL;\n\t}\n\n\treturn pagecache_get_page(mapping, index, fgp_flags, gfp_mask);\n}\n\nstatic inline void f2fs_copy_page(struct page *src, struct page *dst)\n{\n\tchar *src_kaddr = kmap(src);\n\tchar *dst_kaddr = kmap(dst);\n\n\tmemcpy(dst_kaddr, src_kaddr, PAGE_SIZE);\n\tkunmap(dst);\n\tkunmap(src);\n}\n\nstatic inline void f2fs_put_page(struct page *page, int unlock)\n{\n\tif (!page)\n\t\treturn;\n\n\tif (unlock) {\n\t\tf2fs_bug_on(F2FS_P_SB(page), !PageLocked(page));\n\t\tunlock_page(page);\n\t}\n\tput_page(page);\n}\n\nstatic inline void f2fs_put_dnode(struct dnode_of_data *dn)\n{\n\tif (dn->node_page)\n\t\tf2fs_put_page(dn->node_page, 1);\n\tif (dn->inode_page && dn->node_page != dn->inode_page)\n\t\tf2fs_put_page(dn->inode_page, 0);\n\tdn->node_page = NULL;\n\tdn->inode_page = NULL;\n}\n\nstatic inline struct kmem_cache *f2fs_kmem_cache_create(const char *name,\n\t\t\t\t\tsize_t size)\n{\n\treturn kmem_cache_create(name, size, 0, SLAB_RECLAIM_ACCOUNT, NULL);\n}\n\nstatic inline void *f2fs_kmem_cache_alloc(struct kmem_cache *cachep,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tvoid *entry;\n\n\tentry = kmem_cache_alloc(cachep, flags);\n\tif (!entry)\n\t\tentry = kmem_cache_alloc(cachep, flags | __GFP_NOFAIL);\n\treturn entry;\n}\n\nstatic inline struct bio *f2fs_bio_alloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint npages, bool no_fail)\n{\n\tstruct bio *bio;\n\n\tif (no_fail) {\n\t\t/* No failure on bio allocation */\n\t\tbio = bio_alloc(GFP_NOIO, npages);\n\t\tif (!bio)\n\t\t\tbio = bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);\n\t\treturn bio;\n\t}\n\tif (time_to_inject(sbi, FAULT_ALLOC_BIO)) {\n\t\tf2fs_show_injection_info(FAULT_ALLOC_BIO);\n\t\treturn NULL;\n\t}\n\n\treturn bio_alloc(GFP_KERNEL, npages);\n}\n\nstatic inline bool is_idle(struct f2fs_sb_info *sbi, int type)\n{\n\tif (sbi->gc_mode == GC_URGENT)\n\t\treturn true;\n\n\tif (get_pages(sbi, F2FS_RD_DATA) || get_pages(sbi, F2FS_RD_NODE) ||\n\t\tget_pages(sbi, F2FS_RD_META) || get_pages(sbi, F2FS_WB_DATA) ||\n\t\tget_pages(sbi, F2FS_WB_CP_DATA) ||\n\t\tget_pages(sbi, F2FS_DIO_READ) ||\n\t\tget_pages(sbi, F2FS_DIO_WRITE))\n\t\treturn false;\n\n\tif (type != DISCARD_TIME && SM_I(sbi) && SM_I(sbi)->dcc_info &&\n\t\t\tatomic_read(&SM_I(sbi)->dcc_info->queued_discard))\n\t\treturn false;\n\n\tif (SM_I(sbi) && SM_I(sbi)->fcc_info &&\n\t\t\tatomic_read(&SM_I(sbi)->fcc_info->queued_flush))\n\t\treturn false;\n\n\treturn f2fs_time_over(sbi, type);\n}\n\nstatic inline void f2fs_radix_tree_insert(struct radix_tree_root *root,\n\t\t\t\tunsigned long index, void *item)\n{\n\twhile (radix_tree_insert(root, index, item))\n\t\tcond_resched();\n}\n\n#define RAW_IS_INODE(p)\t((p)->footer.nid == (p)->footer.ino)\n\nstatic inline bool IS_INODE(struct page *page)\n{\n\tstruct f2fs_node *p = F2FS_NODE(page);\n\n\treturn RAW_IS_INODE(p);\n}\n\nstatic inline int offset_in_addr(struct f2fs_inode *i)\n{\n\treturn (i->i_inline & F2FS_EXTRA_ATTR) ?\n\t\t\t(le16_to_cpu(i->i_extra_isize) / sizeof(__le32)) : 0;\n}\n\nstatic inline __le32 *blkaddr_in_node(struct f2fs_node *node)\n{\n\treturn RAW_IS_INODE(node) ? node->i.i_addr : node->dn.addr;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode);\nstatic inline block_t datablock_addr(struct inode *inode,\n\t\t\tstruct page *node_page, unsigned int offset)\n{\n\tstruct f2fs_node *raw_node;\n\t__le32 *addr_array;\n\tint base = 0;\n\tbool is_inode = IS_INODE(node_page);\n\n\traw_node = F2FS_NODE(node_page);\n\n\t/* from GC path only */\n\tif (is_inode) {\n\t\tif (!inode)\n\t\t\tbase = offset_in_addr(&raw_node->i);\n\t\telse if (f2fs_has_extra_attr(inode))\n\t\t\tbase = get_extra_isize(inode);\n\t}\n\n\taddr_array = blkaddr_in_node(raw_node);\n\treturn le32_to_cpu(addr_array[base + offset]);\n}\n\nstatic inline int f2fs_test_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\treturn mask & *addr;\n}\n\nstatic inline void f2fs_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr |= mask;\n}\n\nstatic inline void f2fs_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr &= ~mask;\n}\n\nstatic inline int f2fs_test_and_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr |= mask;\n\treturn ret;\n}\n\nstatic inline int f2fs_test_and_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr &= ~mask;\n\treturn ret;\n}\n\nstatic inline void f2fs_change_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr ^= mask;\n}\n\n/*\n * On-disk inode flags (f2fs_inode::i_flags)\n */\n#define F2FS_SYNC_FL\t\t\t0x00000008 /* Synchronous updates */\n#define F2FS_IMMUTABLE_FL\t\t0x00000010 /* Immutable file */\n#define F2FS_APPEND_FL\t\t\t0x00000020 /* writes to file may only append */\n#define F2FS_NODUMP_FL\t\t\t0x00000040 /* do not dump file */\n#define F2FS_NOATIME_FL\t\t\t0x00000080 /* do not update atime */\n#define F2FS_INDEX_FL\t\t\t0x00001000 /* hash-indexed directory */\n#define F2FS_DIRSYNC_FL\t\t\t0x00010000 /* dirsync behaviour (directories only) */\n#define F2FS_PROJINHERIT_FL\t\t0x20000000 /* Create with parents projid */\n\n/* Flags that should be inherited by new inodes from their parent. */\n#define F2FS_FL_INHERITED (F2FS_SYNC_FL | F2FS_NODUMP_FL | F2FS_NOATIME_FL | \\\n\t\t\t   F2FS_DIRSYNC_FL | F2FS_PROJINHERIT_FL)\n\n/* Flags that are appropriate for regular files (all but dir-specific ones). */\n#define F2FS_REG_FLMASK\t\t(~(F2FS_DIRSYNC_FL | F2FS_PROJINHERIT_FL))\n\n/* Flags that are appropriate for non-directories/regular files. */\n#define F2FS_OTHER_FLMASK\t(F2FS_NODUMP_FL | F2FS_NOATIME_FL)\n\nstatic inline __u32 f2fs_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & F2FS_REG_FLMASK;\n\telse\n\t\treturn flags & F2FS_OTHER_FLMASK;\n}\n\n/* used for f2fs_inode_info->flags */\nenum {\n\tFI_NEW_INODE,\t\t/* indicate newly allocated inode */\n\tFI_DIRTY_INODE,\t\t/* indicate inode is dirty or not */\n\tFI_AUTO_RECOVER,\t/* indicate inode is recoverable */\n\tFI_DIRTY_DIR,\t\t/* indicate directory has dirty pages */\n\tFI_INC_LINK,\t\t/* need to increment i_nlink */\n\tFI_ACL_MODE,\t\t/* indicate acl mode */\n\tFI_NO_ALLOC,\t\t/* should not allocate any blocks */\n\tFI_FREE_NID,\t\t/* free allocated nide */\n\tFI_NO_EXTENT,\t\t/* not to use the extent cache */\n\tFI_INLINE_XATTR,\t/* used for inline xattr */\n\tFI_INLINE_DATA,\t\t/* used for inline data*/\n\tFI_INLINE_DENTRY,\t/* used for inline dentry */\n\tFI_APPEND_WRITE,\t/* inode has appended data */\n\tFI_UPDATE_WRITE,\t/* inode has in-place-update data */\n\tFI_NEED_IPU,\t\t/* used for ipu per file */\n\tFI_ATOMIC_FILE,\t\t/* indicate atomic file */\n\tFI_ATOMIC_COMMIT,\t/* indicate the state of atomical committing */\n\tFI_VOLATILE_FILE,\t/* indicate volatile file */\n\tFI_FIRST_BLOCK_WRITTEN,\t/* indicate #0 data block was written */\n\tFI_DROP_CACHE,\t\t/* drop dirty page cache */\n\tFI_DATA_EXIST,\t\t/* indicate data exists */\n\tFI_INLINE_DOTS,\t\t/* indicate inline dot dentries */\n\tFI_DO_DEFRAG,\t\t/* indicate defragment is running */\n\tFI_DIRTY_FILE,\t\t/* indicate regular/symlink has dirty pages */\n\tFI_NO_PREALLOC,\t\t/* indicate skipped preallocated blocks */\n\tFI_HOT_DATA,\t\t/* indicate file is hot */\n\tFI_EXTRA_ATTR,\t\t/* indicate file has extra attribute */\n\tFI_PROJ_INHERIT,\t/* indicate file inherits projectid */\n\tFI_PIN_FILE,\t\t/* indicate file should not be gced */\n\tFI_ATOMIC_REVOKE_REQUEST, /* request to drop atomic data */\n};\n\nstatic inline void __mark_inode_dirty_flag(struct inode *inode,\n\t\t\t\t\t\tint flag, bool set)\n{\n\tswitch (flag) {\n\tcase FI_INLINE_XATTR:\n\tcase FI_INLINE_DATA:\n\tcase FI_INLINE_DENTRY:\n\tcase FI_NEW_INODE:\n\t\tif (set)\n\t\t\treturn;\n\t\t/* fall through */\n\tcase FI_DATA_EXIST:\n\tcase FI_INLINE_DOTS:\n\tcase FI_PIN_FILE:\n\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t}\n}\n\nstatic inline void set_inode_flag(struct inode *inode, int flag)\n{\n\tif (!test_bit(flag, &F2FS_I(inode)->flags))\n\t\tset_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, true);\n}\n\nstatic inline int is_inode_flag_set(struct inode *inode, int flag)\n{\n\treturn test_bit(flag, &F2FS_I(inode)->flags);\n}\n\nstatic inline void clear_inode_flag(struct inode *inode, int flag)\n{\n\tif (test_bit(flag, &F2FS_I(inode)->flags))\n\t\tclear_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, false);\n}\n\nstatic inline void set_acl_inode(struct inode *inode, umode_t mode)\n{\n\tF2FS_I(inode)->i_acl_mode = mode;\n\tset_inode_flag(inode, FI_ACL_MODE);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n}\n\nstatic inline void f2fs_i_links_write(struct inode *inode, bool inc)\n{\n\tif (inc)\n\t\tinc_nlink(inode);\n\telse\n\t\tdrop_nlink(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *inode,\n\t\t\t\t\tblock_t diff, bool add, bool claim)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\t/* add = 1, claim = 1 should be dquot_reserve_block in pair */\n\tif (add) {\n\t\tif (claim)\n\t\t\tdquot_claim_block(inode, diff);\n\t\telse\n\t\t\tdquot_alloc_block_nofail(inode, diff);\n\t} else {\n\t\tdquot_free_block(inode, diff);\n\t}\n\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_size_write(struct inode *inode, loff_t i_size)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\tif (i_size_read(inode) == i_size)\n\t\treturn;\n\n\ti_size_write(inode, i_size);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_depth_write(struct inode *inode, unsigned int depth)\n{\n\tF2FS_I(inode)->i_current_depth = depth;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_gc_failures_write(struct inode *inode,\n\t\t\t\t\tunsigned int count)\n{\n\tF2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN] = count;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_xnid_write(struct inode *inode, nid_t xnid)\n{\n\tF2FS_I(inode)->i_xattr_nid = xnid;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_pino_write(struct inode *inode, nid_t pino)\n{\n\tF2FS_I(inode)->i_pino = pino;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void get_inline_info(struct inode *inode, struct f2fs_inode *ri)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tif (ri->i_inline & F2FS_INLINE_XATTR)\n\t\tset_bit(FI_INLINE_XATTR, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DATA)\n\t\tset_bit(FI_INLINE_DATA, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DENTRY)\n\t\tset_bit(FI_INLINE_DENTRY, &fi->flags);\n\tif (ri->i_inline & F2FS_DATA_EXIST)\n\t\tset_bit(FI_DATA_EXIST, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DOTS)\n\t\tset_bit(FI_INLINE_DOTS, &fi->flags);\n\tif (ri->i_inline & F2FS_EXTRA_ATTR)\n\t\tset_bit(FI_EXTRA_ATTR, &fi->flags);\n\tif (ri->i_inline & F2FS_PIN_FILE)\n\t\tset_bit(FI_PIN_FILE, &fi->flags);\n}\n\nstatic inline void set_raw_inline(struct inode *inode, struct f2fs_inode *ri)\n{\n\tri->i_inline = 0;\n\n\tif (is_inode_flag_set(inode, FI_INLINE_XATTR))\n\t\tri->i_inline |= F2FS_INLINE_XATTR;\n\tif (is_inode_flag_set(inode, FI_INLINE_DATA))\n\t\tri->i_inline |= F2FS_INLINE_DATA;\n\tif (is_inode_flag_set(inode, FI_INLINE_DENTRY))\n\t\tri->i_inline |= F2FS_INLINE_DENTRY;\n\tif (is_inode_flag_set(inode, FI_DATA_EXIST))\n\t\tri->i_inline |= F2FS_DATA_EXIST;\n\tif (is_inode_flag_set(inode, FI_INLINE_DOTS))\n\t\tri->i_inline |= F2FS_INLINE_DOTS;\n\tif (is_inode_flag_set(inode, FI_EXTRA_ATTR))\n\t\tri->i_inline |= F2FS_EXTRA_ATTR;\n\tif (is_inode_flag_set(inode, FI_PIN_FILE))\n\t\tri->i_inline |= F2FS_PIN_FILE;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_EXTRA_ATTR);\n}\n\nstatic inline int f2fs_has_inline_xattr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_XATTR);\n}\n\nstatic inline unsigned int addrs_per_inode(struct inode *inode)\n{\n\tunsigned int addrs = CUR_ADDRS_PER_INODE(inode) -\n\t\t\t\tget_inline_xattr_addrs(inode);\n\treturn ALIGN_DOWN(addrs, 1);\n}\n\nstatic inline unsigned int addrs_per_block(struct inode *inode)\n{\n\treturn ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, 1);\n}\n\nstatic inline void *inline_xattr_addr(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\n\treturn (void *)&(ri->i_addr[DEF_ADDRS_PER_INODE -\n\t\t\t\t\tget_inline_xattr_addrs(inode)]);\n}\n\nstatic inline int inline_xattr_size(struct inode *inode)\n{\n\tif (f2fs_has_inline_xattr(inode))\n\t\treturn get_inline_xattr_addrs(inode) * sizeof(__le32);\n\treturn 0;\n}\n\nstatic inline int f2fs_has_inline_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DATA);\n}\n\nstatic inline int f2fs_exist_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DATA_EXIST);\n}\n\nstatic inline int f2fs_has_inline_dots(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DOTS);\n}\n\nstatic inline bool f2fs_is_pinned_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_PIN_FILE);\n}\n\nstatic inline bool f2fs_is_atomic_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_FILE);\n}\n\nstatic inline bool f2fs_is_commit_atomic_write(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_COMMIT);\n}\n\nstatic inline bool f2fs_is_volatile_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_VOLATILE_FILE);\n}\n\nstatic inline bool f2fs_is_first_block_written(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_FIRST_BLOCK_WRITTEN);\n}\n\nstatic inline bool f2fs_is_drop_cache(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DROP_CACHE);\n}\n\nstatic inline void *inline_data_addr(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\tint extra_size = get_extra_isize(inode);\n\n\treturn (void *)&(ri->i_addr[extra_size + DEF_INLINE_RESERVED_SIZE]);\n}\n\nstatic inline int f2fs_has_inline_dentry(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DENTRY);\n}\n\nstatic inline int is_file(struct inode *inode, int type)\n{\n\treturn F2FS_I(inode)->i_advise & type;\n}\n\nstatic inline void set_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise |= type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void clear_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise &= ~type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline bool f2fs_skip_inode_update(struct inode *inode, int dsync)\n{\n\tbool ret;\n\n\tif (dsync) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\t\tret = list_empty(&F2FS_I(inode)->gdirty_list);\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn ret;\n\t}\n\tif (!is_inode_flag_set(inode, FI_AUTO_RECOVER) ||\n\t\t\tfile_keep_isize(inode) ||\n\t\t\ti_size_read(inode) & ~PAGE_MASK)\n\t\treturn false;\n\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time, &inode->i_atime))\n\t\treturn false;\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time + 1, &inode->i_ctime))\n\t\treturn false;\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time + 2, &inode->i_mtime))\n\t\treturn false;\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time + 3,\n\t\t\t\t\t\t&F2FS_I(inode)->i_crtime))\n\t\treturn false;\n\n\tdown_read(&F2FS_I(inode)->i_sem);\n\tret = F2FS_I(inode)->last_disk_size == i_size_read(inode);\n\tup_read(&F2FS_I(inode)->i_sem);\n\n\treturn ret;\n}\n\nstatic inline bool f2fs_readonly(struct super_block *sb)\n{\n\treturn sb_rdonly(sb);\n}\n\nstatic inline bool f2fs_cp_error(struct f2fs_sb_info *sbi)\n{\n\treturn is_set_ckpt_flags(sbi, CP_ERROR_FLAG);\n}\n\nstatic inline bool is_dot_dotdot(const struct qstr *str)\n{\n\tif (str->len == 1 && str->name[0] == '.')\n\t\treturn true;\n\n\tif (str->len == 2 && str->name[0] == '.' && str->name[1] == '.')\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool f2fs_may_extent_tree(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (!test_opt(sbi, EXTENT_CACHE) ||\n\t\t\tis_inode_flag_set(inode, FI_NO_EXTENT))\n\t\treturn false;\n\n\t/*\n\t * for recovered files during mount do not create extents\n\t * if shrinker is not registered.\n\t */\n\tif (list_empty(&sbi->s_list))\n\t\treturn false;\n\n\treturn S_ISREG(inode->i_mode);\n}\n\nstatic inline void *f2fs_kmalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tif (time_to_inject(sbi, FAULT_KMALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_KMALLOC);\n\t\treturn NULL;\n\t}\n\n\tret = kmalloc(size, flags);\n\tif (ret)\n\t\treturn ret;\n\n\treturn kvmalloc(size, flags);\n}\n\nstatic inline void *f2fs_kzalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\treturn f2fs_kmalloc(sbi, size, flags | __GFP_ZERO);\n}\n\nstatic inline void *f2fs_kvmalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\tif (time_to_inject(sbi, FAULT_KVMALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_KVMALLOC);\n\t\treturn NULL;\n\t}\n\n\treturn kvmalloc(size, flags);\n}\n\nstatic inline void *f2fs_kvzalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\treturn f2fs_kvmalloc(sbi, size, flags | __GFP_ZERO);\n}\n\nstatic inline int get_extra_isize(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_extra_isize / sizeof(__le32);\n}\n\nstatic inline int get_inline_xattr_addrs(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_inline_xattr_size;\n}\n\n#define f2fs_get_inode_mode(i) \\\n\t((is_inode_flag_set(i, FI_ACL_MODE)) ? \\\n\t (F2FS_I(i)->i_acl_mode) : ((i)->i_mode))\n\n#define F2FS_TOTAL_EXTRA_ATTR_SIZE\t\t\t\\\n\t(offsetof(struct f2fs_inode, i_extra_end) -\t\\\n\toffsetof(struct f2fs_inode, i_extra_isize))\t\\\n\n#define F2FS_OLD_ATTRIBUTE_SIZE\t(offsetof(struct f2fs_inode, i_addr))\n#define F2FS_FITS_IN_INODE(f2fs_inode, extra_isize, field)\t\t\\\n\t\t((offsetof(typeof(*(f2fs_inode)), field) +\t\\\n\t\tsizeof((f2fs_inode)->field))\t\t\t\\\n\t\t<= (F2FS_OLD_ATTRIBUTE_SIZE + (extra_isize)))\t\\\n\nstatic inline void f2fs_reset_iostat(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tspin_lock(&sbi->iostat_lock);\n\tfor (i = 0; i < NR_IO_TYPE; i++)\n\t\tsbi->write_iostat[i] = 0;\n\tspin_unlock(&sbi->iostat_lock);\n}\n\nstatic inline void f2fs_update_iostat(struct f2fs_sb_info *sbi,\n\t\t\tenum iostat_type type, unsigned long long io_bytes)\n{\n\tif (!sbi->iostat_enable)\n\t\treturn;\n\tspin_lock(&sbi->iostat_lock);\n\tsbi->write_iostat[type] += io_bytes;\n\n\tif (type == APP_WRITE_IO || type == APP_DIRECT_IO)\n\t\tsbi->write_iostat[APP_BUFFERED_IO] =\n\t\t\tsbi->write_iostat[APP_WRITE_IO] -\n\t\t\tsbi->write_iostat[APP_DIRECT_IO];\n\tspin_unlock(&sbi->iostat_lock);\n}\n\n#define __is_large_section(sbi)\t\t((sbi)->segs_per_sec > 1)\n\n#define __is_meta_io(fio) (PAGE_TYPE_OF_BIO((fio)->type) == META)\n\nbool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type);\nstatic inline void verify_blkaddr(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, type)) {\n\t\tf2fs_err(sbi, \"invalid blkaddr: %u, type: %d, run fsck to fix.\",\n\t\t\t blkaddr, type);\n\t\tf2fs_bug_on(sbi, 1);\n\t}\n}\n\nstatic inline bool __is_valid_data_blkaddr(block_t blkaddr)\n{\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline void f2fs_set_page_private(struct page *page,\n\t\t\t\t\t\tunsigned long data)\n{\n\tif (PagePrivate(page))\n\t\treturn;\n\n\tget_page(page);\n\tSetPagePrivate(page);\n\tset_page_private(page, data);\n}\n\nstatic inline void f2fs_clear_page_private(struct page *page)\n{\n\tif (!PagePrivate(page))\n\t\treturn;\n\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\tf2fs_put_page(page, 0);\n}\n\n/*\n * file.c\n */\nint f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);\nvoid f2fs_truncate_data_blocks(struct dnode_of_data *dn);\nint f2fs_truncate_blocks(struct inode *inode, u64 from, bool lock);\nint f2fs_truncate(struct inode *inode);\nint f2fs_getattr(const struct path *path, struct kstat *stat,\n\t\t\tu32 request_mask, unsigned int flags);\nint f2fs_setattr(struct dentry *dentry, struct iattr *attr);\nint f2fs_truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end);\nvoid f2fs_truncate_data_blocks_range(struct dnode_of_data *dn, int count);\nint f2fs_precache_extents(struct inode *inode);\nlong f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);\nlong f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\nint f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid);\nint f2fs_pin_file_control(struct inode *inode, bool inc);\n\n/*\n * inode.c\n */\nvoid f2fs_set_inode_flags(struct inode *inode);\nbool f2fs_inode_chksum_verify(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page);\nstruct inode *f2fs_iget(struct super_block *sb, unsigned long ino);\nstruct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);\nint f2fs_try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);\nvoid f2fs_update_inode(struct inode *inode, struct page *node_page);\nvoid f2fs_update_inode_page(struct inode *inode);\nint f2fs_write_inode(struct inode *inode, struct writeback_control *wbc);\nvoid f2fs_evict_inode(struct inode *inode);\nvoid f2fs_handle_failed_inode(struct inode *inode);\n\n/*\n * namei.c\n */\nint f2fs_update_extension_list(struct f2fs_sb_info *sbi, const char *name,\n\t\t\t\t\t\t\tbool hot, bool set);\nstruct dentry *f2fs_get_parent(struct dentry *child);\n\n/*\n * dir.c\n */\nunsigned char f2fs_get_de_type(struct f2fs_dir_entry *de);\nstruct f2fs_dir_entry *f2fs_find_target_dentry(struct fscrypt_name *fname,\n\t\t\tf2fs_hash_t namehash, int *max_slots,\n\t\t\tstruct f2fs_dentry_ptr *d);\nint f2fs_fill_dentries(struct dir_context *ctx, struct f2fs_dentry_ptr *d,\n\t\t\tunsigned int start_pos, struct fscrypt_str *fstr);\nvoid f2fs_do_make_empty_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct f2fs_dentry_ptr *d);\nstruct page *f2fs_init_inode_metadata(struct inode *inode, struct inode *dir,\n\t\t\tconst struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name, struct page *dpage);\nvoid f2fs_update_parent_metadata(struct inode *dir, struct inode *inode,\n\t\t\tunsigned int current_depth);\nint f2fs_room_for_filename(const void *bitmap, int slots, int max_slots);\nvoid f2fs_drop_nlink(struct inode *dir, struct inode *inode);\nstruct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_find_entry(struct inode *dir,\n\t\t\tconst struct qstr *child, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_parent_dir(struct inode *dir, struct page **p);\nino_t f2fs_inode_by_name(struct inode *dir, const struct qstr *qstr,\n\t\t\tstruct page **page);\nvoid f2fs_set_link(struct inode *dir, struct f2fs_dir_entry *de,\n\t\t\tstruct page *page, struct inode *inode);\nvoid f2fs_update_dentry(nid_t ino, umode_t mode, struct f2fs_dentry_ptr *d,\n\t\t\tconst struct qstr *name, f2fs_hash_t name_hash,\n\t\t\tunsigned int bit_pos);\nint f2fs_add_regular_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint f2fs_add_dentry(struct inode *dir, struct fscrypt_name *fname,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint f2fs_do_add_link(struct inode *dir, const struct qstr *name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_entry(struct f2fs_dir_entry *dentry, struct page *page,\n\t\t\tstruct inode *dir, struct inode *inode);\nint f2fs_do_tmpfile(struct inode *inode, struct inode *dir);\nbool f2fs_empty_dir(struct inode *dir);\n\nstatic inline int f2fs_add_link(struct dentry *dentry, struct inode *inode)\n{\n\treturn f2fs_do_add_link(d_inode(dentry->d_parent), &dentry->d_name,\n\t\t\t\tinode, inode->i_ino, inode->i_mode);\n}\n\n/*\n * super.c\n */\nint f2fs_inode_dirtied(struct inode *inode, bool sync);\nvoid f2fs_inode_synced(struct inode *inode);\nint f2fs_enable_quota_files(struct f2fs_sb_info *sbi, bool rdonly);\nint f2fs_quota_sync(struct super_block *sb, int type);\nvoid f2fs_quota_off_umount(struct super_block *sb);\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover);\nint f2fs_sync_fs(struct super_block *sb, int sync);\nint f2fs_sanity_check_ckpt(struct f2fs_sb_info *sbi);\n\n/*\n * hash.c\n */\nf2fs_hash_t f2fs_dentry_hash(const struct qstr *name_info,\n\t\t\t\tstruct fscrypt_name *fname);\n\n/*\n * node.c\n */\nstruct dnode_of_data;\nstruct node_info;\n\nint f2fs_check_nid_range(struct f2fs_sb_info *sbi, nid_t nid);\nbool f2fs_available_free_memory(struct f2fs_sb_info *sbi, int type);\nbool f2fs_in_warm_node_list(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_init_fsync_node_info(struct f2fs_sb_info *sbi);\nvoid f2fs_del_fsync_node_entry(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_reset_fsync_node_info(struct f2fs_sb_info *sbi);\nint f2fs_need_dentry_mark(struct f2fs_sb_info *sbi, nid_t nid);\nbool f2fs_is_checkpointed_node(struct f2fs_sb_info *sbi, nid_t nid);\nbool f2fs_need_inode_block_update(struct f2fs_sb_info *sbi, nid_t ino);\nint f2fs_get_node_info(struct f2fs_sb_info *sbi, nid_t nid,\n\t\t\t\t\t\tstruct node_info *ni);\npgoff_t f2fs_get_next_page_offset(struct dnode_of_data *dn, pgoff_t pgofs);\nint f2fs_get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode);\nint f2fs_truncate_inode_blocks(struct inode *inode, pgoff_t from);\nint f2fs_truncate_xattr_node(struct inode *inode);\nint f2fs_wait_on_node_pages_writeback(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int seq_id);\nint f2fs_remove_inode_page(struct inode *inode);\nstruct page *f2fs_new_inode_page(struct inode *inode);\nstruct page *f2fs_new_node_page(struct dnode_of_data *dn, unsigned int ofs);\nvoid f2fs_ra_node_page(struct f2fs_sb_info *sbi, nid_t nid);\nstruct page *f2fs_get_node_page(struct f2fs_sb_info *sbi, pgoff_t nid);\nstruct page *f2fs_get_node_page_ra(struct page *parent, int start);\nint f2fs_move_node_page(struct page *node_page, int gc_type);\nint f2fs_fsync_node_pages(struct f2fs_sb_info *sbi, struct inode *inode,\n\t\t\tstruct writeback_control *wbc, bool atomic,\n\t\t\tunsigned int *seq_id);\nint f2fs_sync_node_pages(struct f2fs_sb_info *sbi,\n\t\t\tstruct writeback_control *wbc,\n\t\t\tbool do_balance, enum iostat_type io_type);\nint f2fs_build_free_nids(struct f2fs_sb_info *sbi, bool sync, bool mount);\nbool f2fs_alloc_nid(struct f2fs_sb_info *sbi, nid_t *nid);\nvoid f2fs_alloc_nid_done(struct f2fs_sb_info *sbi, nid_t nid);\nvoid f2fs_alloc_nid_failed(struct f2fs_sb_info *sbi, nid_t nid);\nint f2fs_try_to_free_nids(struct f2fs_sb_info *sbi, int nr_shrink);\nvoid f2fs_recover_inline_xattr(struct inode *inode, struct page *page);\nint f2fs_recover_xattr_data(struct inode *inode, struct page *page);\nint f2fs_recover_inode_page(struct f2fs_sb_info *sbi, struct page *page);\nint f2fs_restore_node_summary(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int segno, struct f2fs_summary_block *sum);\nint f2fs_flush_nat_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint f2fs_build_node_manager(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_node_manager(struct f2fs_sb_info *sbi);\nint __init f2fs_create_node_manager_caches(void);\nvoid f2fs_destroy_node_manager_caches(void);\n\n/*\n * segment.c\n */\nbool f2fs_need_SSR(struct f2fs_sb_info *sbi);\nvoid f2fs_register_inmem_page(struct inode *inode, struct page *page);\nvoid f2fs_drop_inmem_pages_all(struct f2fs_sb_info *sbi, bool gc_failure);\nvoid f2fs_drop_inmem_pages(struct inode *inode);\nvoid f2fs_drop_inmem_page(struct inode *inode, struct page *page);\nint f2fs_commit_inmem_pages(struct inode *inode);\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need);\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi);\nint f2fs_issue_flush(struct f2fs_sb_info *sbi, nid_t ino);\nint f2fs_create_flush_cmd_control(struct f2fs_sb_info *sbi);\nint f2fs_flush_device_cache(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);\nvoid f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);\nbool f2fs_is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid f2fs_drop_discard_cmd(struct f2fs_sb_info *sbi);\nvoid f2fs_stop_discard_thread(struct f2fs_sb_info *sbi);\nbool f2fs_issue_discard_timeout(struct f2fs_sb_info *sbi);\nvoid f2fs_clear_prefree_segments(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc);\nvoid f2fs_dirty_to_prefree(struct f2fs_sb_info *sbi);\nblock_t f2fs_get_unusable_blocks(struct f2fs_sb_info *sbi);\nint f2fs_disable_cp_again(struct f2fs_sb_info *sbi, block_t unusable);\nvoid f2fs_release_discard_addrs(struct f2fs_sb_info *sbi);\nint f2fs_npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);\nvoid allocate_segment_for_resize(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int start, unsigned int end);\nvoid f2fs_allocate_new_segments(struct f2fs_sb_info *sbi);\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range);\nbool f2fs_exist_trim_candidates(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc);\nstruct page *f2fs_get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno);\nvoid f2fs_update_meta_page(struct f2fs_sb_info *sbi, void *src,\n\t\t\t\t\tblock_t blk_addr);\nvoid f2fs_do_write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\t\tenum iostat_type io_type);\nvoid f2fs_do_write_node_page(unsigned int nid, struct f2fs_io_info *fio);\nvoid f2fs_outplace_write_data(struct dnode_of_data *dn,\n\t\t\tstruct f2fs_io_info *fio);\nint f2fs_inplace_write_data(struct f2fs_io_info *fio);\nvoid f2fs_do_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\tbool recover_curseg, bool recover_newaddr);\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\tunsigned char version, bool recover_curseg,\n\t\t\tbool recover_newaddr);\nvoid f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\t\tstruct f2fs_summary *sum, int type,\n\t\t\tstruct f2fs_io_info *fio, bool add_list);\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\tenum page_type type, bool ordered, bool locked);\nvoid f2fs_wait_on_block_writeback(struct inode *inode, block_t blkaddr);\nvoid f2fs_wait_on_block_writeback_range(struct inode *inode, block_t blkaddr,\n\t\t\t\t\t\t\t\tblock_t len);\nvoid f2fs_write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nvoid f2fs_write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nint f2fs_lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\tunsigned int val, int alloc);\nvoid f2fs_flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint f2fs_build_segment_manager(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_segment_manager(struct f2fs_sb_info *sbi);\nint __init f2fs_create_segment_manager_caches(void);\nvoid f2fs_destroy_segment_manager_caches(void);\nint f2fs_rw_hint_to_seg_type(enum rw_hint hint);\nenum rw_hint f2fs_io_type_to_rw_hint(struct f2fs_sb_info *sbi,\n\t\t\tenum page_type type, enum temp_type temp);\n\n/*\n * checkpoint.c\n */\nvoid f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io);\nstruct page *f2fs_grab_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *f2fs_get_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *f2fs_get_meta_page_nofail(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *f2fs_get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);\nbool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type);\nint f2fs_ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,\n\t\t\tint type, bool sync);\nvoid f2fs_ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index);\nlong f2fs_sync_meta_pages(struct f2fs_sb_info *sbi, enum page_type type,\n\t\t\tlong nr_to_write, enum iostat_type io_type);\nvoid f2fs_add_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid f2fs_remove_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid f2fs_release_ino_entry(struct f2fs_sb_info *sbi, bool all);\nbool f2fs_exist_written_data(struct f2fs_sb_info *sbi, nid_t ino, int mode);\nvoid f2fs_set_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,\n\t\t\t\t\tunsigned int devidx, int type);\nbool f2fs_is_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,\n\t\t\t\t\tunsigned int devidx, int type);\nint f2fs_sync_inode_meta(struct f2fs_sb_info *sbi);\nint f2fs_acquire_orphan_inode(struct f2fs_sb_info *sbi);\nvoid f2fs_release_orphan_inode(struct f2fs_sb_info *sbi);\nvoid f2fs_add_orphan_inode(struct inode *inode);\nvoid f2fs_remove_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino);\nint f2fs_recover_orphan_inodes(struct f2fs_sb_info *sbi);\nint f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi);\nvoid f2fs_update_dirty_page(struct inode *inode, struct page *page);\nvoid f2fs_remove_dirty_inode(struct inode *inode);\nint f2fs_sync_dirty_inodes(struct f2fs_sb_info *sbi, enum inode_type type);\nvoid f2fs_wait_on_all_pages_writeback(struct f2fs_sb_info *sbi);\nint f2fs_write_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nvoid f2fs_init_ino_entry_info(struct f2fs_sb_info *sbi);\nint __init f2fs_create_checkpoint_caches(void);\nvoid f2fs_destroy_checkpoint_caches(void);\n\n/*\n * data.c\n */\nint f2fs_init_post_read_processing(void);\nvoid f2fs_destroy_post_read_processing(void);\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type);\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type);\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi);\nint f2fs_submit_page_bio(struct f2fs_io_info *fio);\nint f2fs_merge_page_bio(struct f2fs_io_info *fio);\nvoid f2fs_submit_page_write(struct f2fs_io_info *fio);\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\tblock_t blk_addr, struct bio *bio);\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid f2fs_set_data_blkaddr(struct dnode_of_data *dn);\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);\nint f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count);\nint f2fs_reserve_new_block(struct dnode_of_data *dn);\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index);\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from);\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index);\nstruct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\tint op_flags, bool for_write);\nstruct page *f2fs_find_data_page(struct inode *inode, pgoff_t index);\nstruct page *f2fs_get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\tbool for_write);\nstruct page *f2fs_get_new_data_page(struct inode *inode,\n\t\t\tstruct page *ipage, pgoff_t index, bool new_i_size);\nint f2fs_do_write_data_page(struct f2fs_io_info *fio);\nvoid __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock);\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\tint create, int flag);\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\tu64 start, u64 len);\nbool f2fs_should_update_inplace(struct inode *inode, struct f2fs_io_info *fio);\nbool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio);\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\tunsigned int length);\nint f2fs_release_page(struct page *page, gfp_t wait);\n#ifdef CONFIG_MIGRATION\nint f2fs_migrate_page(struct address_space *mapping, struct page *newpage,\n\t\t\tstruct page *page, enum migrate_mode mode);\n#endif\nbool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len);\nvoid f2fs_clear_page_cache_dirty_tag(struct page *page);\n\n/*\n * gc.c\n */\nint f2fs_start_gc_thread(struct f2fs_sb_info *sbi);\nvoid f2fs_stop_gc_thread(struct f2fs_sb_info *sbi);\nblock_t f2fs_start_bidx_of_node(unsigned int node_ofs, struct inode *inode);\nint f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background,\n\t\t\tunsigned int segno);\nvoid f2fs_build_gc_manager(struct f2fs_sb_info *sbi);\nint f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count);\n\n/*\n * recovery.c\n */\nint f2fs_recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);\nbool f2fs_space_for_roll_forward(struct f2fs_sb_info *sbi);\n\n/*\n * debug.c\n */\n#ifdef CONFIG_F2FS_STAT_FS\nstruct f2fs_stat_info {\n\tstruct list_head stat_list;\n\tstruct f2fs_sb_info *sbi;\n\tint all_area_segs, sit_area_segs, nat_area_segs, ssa_area_segs;\n\tint main_area_segs, main_area_sections, main_area_zones;\n\tunsigned long long hit_largest, hit_cached, hit_rbtree;\n\tunsigned long long hit_total, total_ext;\n\tint ext_tree, zombie_tree, ext_node;\n\tint ndirty_node, ndirty_dent, ndirty_meta, ndirty_imeta;\n\tint ndirty_data, ndirty_qdata;\n\tint inmem_pages;\n\tunsigned int ndirty_dirs, ndirty_files, nquota_files, ndirty_all;\n\tint nats, dirty_nats, sits, dirty_sits;\n\tint free_nids, avail_nids, alloc_nids;\n\tint total_count, utilization;\n\tint bg_gc, nr_wb_cp_data, nr_wb_data;\n\tint nr_rd_data, nr_rd_node, nr_rd_meta;\n\tint nr_dio_read, nr_dio_write;\n\tunsigned int io_skip_bggc, other_skip_bggc;\n\tint nr_flushing, nr_flushed, flush_list_empty;\n\tint nr_discarding, nr_discarded;\n\tint nr_discard_cmd;\n\tunsigned int undiscard_blks;\n\tint inline_xattr, inline_inode, inline_dir, append, update, orphans;\n\tint aw_cnt, max_aw_cnt, vw_cnt, max_vw_cnt;\n\tunsigned int valid_count, valid_node_count, valid_inode_count, discard_blks;\n\tunsigned int bimodal, avg_vblocks;\n\tint util_free, util_valid, util_invalid;\n\tint rsvd_segs, overp_segs;\n\tint dirty_count, node_pages, meta_pages;\n\tint prefree_count, call_count, cp_count, bg_cp_count;\n\tint tot_segs, node_segs, data_segs, free_segs, free_secs;\n\tint bg_node_segs, bg_data_segs;\n\tint tot_blks, data_blks, node_blks;\n\tint bg_data_blks, bg_node_blks;\n\tunsigned long long skipped_atomic_files[2];\n\tint curseg[NR_CURSEG_TYPE];\n\tint cursec[NR_CURSEG_TYPE];\n\tint curzone[NR_CURSEG_TYPE];\n\n\tunsigned int meta_count[META_MAX];\n\tunsigned int segment_count[2];\n\tunsigned int block_count[2];\n\tunsigned int inplace_count;\n\tunsigned long long base_mem, cache_mem, page_mem;\n};\n\nstatic inline struct f2fs_stat_info *F2FS_STAT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_stat_info *)sbi->stat_info;\n}\n\n#define stat_inc_cp_count(si)\t\t((si)->cp_count++)\n#define stat_inc_bg_cp_count(si)\t((si)->bg_cp_count++)\n#define stat_inc_call_count(si)\t\t((si)->call_count++)\n#define stat_inc_bggc_count(sbi)\t((sbi)->bg_gc++)\n#define stat_io_skip_bggc_count(sbi)\t((sbi)->io_skip_bggc++)\n#define stat_other_skip_bggc_count(sbi)\t((sbi)->other_skip_bggc++)\n#define stat_inc_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]++)\n#define stat_dec_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]--)\n#define stat_inc_total_hit(sbi)\t\t(atomic64_inc(&(sbi)->total_hit_ext))\n#define stat_inc_rbtree_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_rbtree))\n#define stat_inc_largest_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_largest))\n#define stat_inc_cached_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_cached))\n#define stat_inc_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_dec_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_inc_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_dec_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_inc_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_dec_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_inc_meta_count(sbi, blkaddr)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (blkaddr < SIT_I(sbi)->sit_base_addr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_CP]);\t\\\n\t\telse if (blkaddr < NM_I(sbi)->nat_blkaddr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_SIT]);\t\\\n\t\telse if (blkaddr < SM_I(sbi)->ssa_blkaddr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_NAT]);\t\\\n\t\telse if (blkaddr < SM_I(sbi)->main_blkaddr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_SSA]);\t\\\n\t} while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\t\t\t\\\n\t\t((sbi)->segment_count[(curseg)->alloc_type]++)\n#define stat_inc_block_count(sbi, curseg)\t\t\t\t\\\n\t\t((sbi)->block_count[(curseg)->alloc_type]++)\n#define stat_inc_inplace_blocks(sbi)\t\t\t\t\t\\\n\t\t(atomic_inc(&(sbi)->inplace_count))\n#define stat_inc_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_dec_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_update_max_atomic_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->aw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_aw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_aw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_dec_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_update_max_volatile_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->vw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_vw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_vw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tsi->tot_segs++;\t\t\t\t\t\t\\\n\t\tif ((type) == SUM_TYPE_DATA) {\t\t\t\t\\\n\t\t\tsi->data_segs++;\t\t\t\t\\\n\t\t\tsi->bg_data_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tsi->node_segs++;\t\t\t\t\\\n\t\t\tsi->bg_node_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define stat_inc_tot_blk_count(si, blks)\t\t\t\t\\\n\t((si)->tot_blks += (blks))\n\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->data_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_data_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->node_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_node_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\nint f2fs_build_stats(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_stats(struct f2fs_sb_info *sbi);\nvoid __init f2fs_create_root_stats(void);\nvoid f2fs_destroy_root_stats(void);\n#else\n#define stat_inc_cp_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bg_cp_count(si)\t\t\tdo { } while (0)\n#define stat_inc_call_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bggc_count(si)\t\t\t\tdo { } while (0)\n#define stat_io_skip_bggc_count(sbi)\t\t\tdo { } while (0)\n#define stat_other_skip_bggc_count(sbi)\t\t\tdo { } while (0)\n#define stat_inc_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_dec_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_inc_total_hit(sb)\t\t\t\tdo { } while (0)\n#define stat_inc_rbtree_node_hit(sb)\t\t\tdo { } while (0)\n#define stat_inc_largest_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_cached_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_inc_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_atomic_write(inode)\t\tdo { } while (0)\n#define stat_inc_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_volatile_write(inode)\t\tdo { } while (0)\n#define stat_inc_meta_count(sbi, blkaddr)\t\tdo { } while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\tdo { } while (0)\n#define stat_inc_block_count(sbi, curseg)\t\tdo { } while (0)\n#define stat_inc_inplace_blocks(sbi)\t\t\tdo { } while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\tdo { } while (0)\n#define stat_inc_tot_blk_count(si, blks)\t\tdo { } while (0)\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n\nstatic inline int f2fs_build_stats(struct f2fs_sb_info *sbi) { return 0; }\nstatic inline void f2fs_destroy_stats(struct f2fs_sb_info *sbi) { }\nstatic inline void __init f2fs_create_root_stats(void) { }\nstatic inline void f2fs_destroy_root_stats(void) { }\n#endif\n\nextern const struct file_operations f2fs_dir_operations;\nextern const struct file_operations f2fs_file_operations;\nextern const struct inode_operations f2fs_file_inode_operations;\nextern const struct address_space_operations f2fs_dblock_aops;\nextern const struct address_space_operations f2fs_node_aops;\nextern const struct address_space_operations f2fs_meta_aops;\nextern const struct inode_operations f2fs_dir_inode_operations;\nextern const struct inode_operations f2fs_symlink_inode_operations;\nextern const struct inode_operations f2fs_encrypted_symlink_inode_operations;\nextern const struct inode_operations f2fs_special_inode_operations;\nextern struct kmem_cache *f2fs_inode_entry_slab;\n\n/*\n * inline.c\n */\nbool f2fs_may_inline_data(struct inode *inode);\nbool f2fs_may_inline_dentry(struct inode *inode);\nvoid f2fs_do_read_inline_data(struct page *page, struct page *ipage);\nvoid f2fs_truncate_inline_inode(struct inode *inode,\n\t\t\t\t\t\tstruct page *ipage, u64 from);\nint f2fs_read_inline_data(struct inode *inode, struct page *page);\nint f2fs_convert_inline_page(struct dnode_of_data *dn, struct page *page);\nint f2fs_convert_inline_inode(struct inode *inode);\nint f2fs_write_inline_data(struct inode *inode, struct page *page);\nbool f2fs_recover_inline_data(struct inode *inode, struct page *npage);\nstruct f2fs_dir_entry *f2fs_find_in_inline_dir(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nint f2fs_make_empty_inline_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct page *ipage);\nint f2fs_add_inline_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_inline_entry(struct f2fs_dir_entry *dentry,\n\t\t\t\tstruct page *page, struct inode *dir,\n\t\t\t\tstruct inode *inode);\nbool f2fs_empty_inline_dir(struct inode *dir);\nint f2fs_read_inline_dir(struct file *file, struct dir_context *ctx,\n\t\t\tstruct fscrypt_str *fstr);\nint f2fs_inline_data_fiemap(struct inode *inode,\n\t\t\tstruct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\n\n/*\n * shrinker.c\n */\nunsigned long f2fs_shrink_count(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nunsigned long f2fs_shrink_scan(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nvoid f2fs_join_shrinker(struct f2fs_sb_info *sbi);\nvoid f2fs_leave_shrinker(struct f2fs_sb_info *sbi);\n\n/*\n * extent_cache.c\n */\nstruct rb_entry *f2fs_lookup_rb_tree(struct rb_root_cached *root,\n\t\t\t\tstruct rb_entry *cached_re, unsigned int ofs);\nstruct rb_node **f2fs_lookup_rb_tree_for_insert(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct rb_root_cached *root,\n\t\t\t\tstruct rb_node **parent,\n\t\t\t\tunsigned int ofs, bool *leftmost);\nstruct rb_entry *f2fs_lookup_rb_tree_ret(struct rb_root_cached *root,\n\t\tstruct rb_entry *cached_re, unsigned int ofs,\n\t\tstruct rb_entry **prev_entry, struct rb_entry **next_entry,\n\t\tstruct rb_node ***insert_p, struct rb_node **insert_parent,\n\t\tbool force, bool *leftmost);\nbool f2fs_check_rb_tree_consistence(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct rb_root_cached *root);\nunsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink);\nbool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext);\nvoid f2fs_drop_extent_tree(struct inode *inode);\nunsigned int f2fs_destroy_extent_node(struct inode *inode);\nvoid f2fs_destroy_extent_tree(struct inode *inode);\nbool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,\n\t\t\tstruct extent_info *ei);\nvoid f2fs_update_extent_cache(struct dnode_of_data *dn);\nvoid f2fs_update_extent_cache_range(struct dnode_of_data *dn,\n\t\t\tpgoff_t fofs, block_t blkaddr, unsigned int len);\nvoid f2fs_init_extent_cache_info(struct f2fs_sb_info *sbi);\nint __init f2fs_create_extent_cache(void);\nvoid f2fs_destroy_extent_cache(void);\n\n/*\n * sysfs.c\n */\nint __init f2fs_init_sysfs(void);\nvoid f2fs_exit_sysfs(void);\nint f2fs_register_sysfs(struct f2fs_sb_info *sbi);\nvoid f2fs_unregister_sysfs(struct f2fs_sb_info *sbi);\n\n/*\n * crypto support\n */\nstatic inline bool f2fs_encrypted_file(struct inode *inode)\n{\n\treturn IS_ENCRYPTED(inode) && S_ISREG(inode->i_mode);\n}\n\nstatic inline void f2fs_set_encrypted_inode(struct inode *inode)\n{\n#ifdef CONFIG_FS_ENCRYPTION\n\tfile_set_encrypt(inode);\n\tf2fs_set_inode_flags(inode);\n#endif\n}\n\n/*\n * Returns true if the reads of the inode's data need to undergo some\n * postprocessing step, like decryption or authenticity verification.\n */\nstatic inline bool f2fs_post_read_required(struct inode *inode)\n{\n\treturn f2fs_encrypted_file(inode);\n}\n\n#define F2FS_FEATURE_FUNCS(name, flagname) \\\nstatic inline int f2fs_sb_has_##name(struct f2fs_sb_info *sbi) \\\n{ \\\n\treturn F2FS_HAS_FEATURE(sbi, F2FS_FEATURE_##flagname); \\\n}\n\nF2FS_FEATURE_FUNCS(encrypt, ENCRYPT);\nF2FS_FEATURE_FUNCS(blkzoned, BLKZONED);\nF2FS_FEATURE_FUNCS(extra_attr, EXTRA_ATTR);\nF2FS_FEATURE_FUNCS(project_quota, PRJQUOTA);\nF2FS_FEATURE_FUNCS(inode_chksum, INODE_CHKSUM);\nF2FS_FEATURE_FUNCS(flexible_inline_xattr, FLEXIBLE_INLINE_XATTR);\nF2FS_FEATURE_FUNCS(quota_ino, QUOTA_INO);\nF2FS_FEATURE_FUNCS(inode_crtime, INODE_CRTIME);\nF2FS_FEATURE_FUNCS(lost_found, LOST_FOUND);\nF2FS_FEATURE_FUNCS(sb_chksum, SB_CHKSUM);\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline bool f2fs_blkz_is_seq(struct f2fs_sb_info *sbi, int devi,\n\t\t\t\t    block_t blkaddr)\n{\n\tunsigned int zno = blkaddr >> sbi->log_blocks_per_blkz;\n\n\treturn test_bit(zno, FDEV(devi).blkz_seq);\n}\n#endif\n\nstatic inline bool f2fs_hw_should_discard(struct f2fs_sb_info *sbi)\n{\n\treturn f2fs_sb_has_blkzoned(sbi);\n}\n\nstatic inline bool f2fs_bdev_support_discard(struct block_device *bdev)\n{\n\treturn blk_queue_discard(bdev_get_queue(bdev)) ||\n\t       bdev_is_zoned(bdev);\n}\n\nstatic inline bool f2fs_hw_support_discard(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn f2fs_bdev_support_discard(sbi->sb->s_bdev);\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (f2fs_bdev_support_discard(FDEV(i).bdev))\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic inline bool f2fs_realtime_discard_enable(struct f2fs_sb_info *sbi)\n{\n\treturn (test_opt(sbi, DISCARD) && f2fs_hw_support_discard(sbi)) ||\n\t\t\t\t\tf2fs_hw_should_discard(sbi);\n}\n\nstatic inline bool f2fs_hw_is_readonly(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn bdev_read_only(sbi->sb->s_bdev);\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (bdev_read_only(FDEV(i).bdev))\n\t\t\treturn true;\n\treturn false;\n}\n\n\nstatic inline void set_opt_mode(struct f2fs_sb_info *sbi, unsigned int mt)\n{\n\tclear_opt(sbi, ADAPTIVE);\n\tclear_opt(sbi, LFS);\n\n\tswitch (mt) {\n\tcase F2FS_MOUNT_ADAPTIVE:\n\t\tset_opt(sbi, ADAPTIVE);\n\t\tbreak;\n\tcase F2FS_MOUNT_LFS:\n\t\tset_opt(sbi, LFS);\n\t\tbreak;\n\t}\n}\n\nstatic inline bool f2fs_may_encrypt(struct inode *inode)\n{\n#ifdef CONFIG_FS_ENCRYPTION\n\tumode_t mode = inode->i_mode;\n\n\treturn (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode));\n#else\n\treturn false;\n#endif\n}\n\nstatic inline int block_unaligned_IO(struct inode *inode,\n\t\t\t\tstruct kiocb *iocb, struct iov_iter *iter)\n{\n\tunsigned int i_blkbits = READ_ONCE(inode->i_blkbits);\n\tunsigned int blocksize_mask = (1 << i_blkbits) - 1;\n\tloff_t offset = iocb->ki_pos;\n\tunsigned long align = offset | iov_iter_alignment(iter);\n\n\treturn align & blocksize_mask;\n}\n\nstatic inline int allow_outplace_dio(struct inode *inode,\n\t\t\t\tstruct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint rw = iov_iter_rw(iter);\n\n\treturn (test_opt(sbi, LFS) && (rw == WRITE) &&\n\t\t\t\t!block_unaligned_IO(inode, iocb, iter));\n}\n\nstatic inline bool f2fs_force_buffered_io(struct inode *inode,\n\t\t\t\tstruct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint rw = iov_iter_rw(iter);\n\n\tif (f2fs_post_read_required(inode))\n\t\treturn true;\n\tif (f2fs_is_multi_device(sbi))\n\t\treturn true;\n\t/*\n\t * for blkzoned device, fallback direct IO to buffered IO, so\n\t * all IOs can be serialized by log-structured write.\n\t */\n\tif (f2fs_sb_has_blkzoned(sbi))\n\t\treturn true;\n\tif (test_opt(sbi, LFS) && (rw == WRITE) &&\n\t\t\t\tblock_unaligned_IO(inode, iocb, iter))\n\t\treturn true;\n\tif (is_sbi_flag_set(F2FS_I_SB(inode), SBI_CP_DISABLED))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\nextern void f2fs_build_fault_attr(struct f2fs_sb_info *sbi, unsigned int rate,\n\t\t\t\t\t\t\tunsigned int type);\n#else\n#define f2fs_build_fault_attr(sbi, rate, type)\t\tdo { } while (0)\n#endif\n\nstatic inline bool is_journalled_quota(struct f2fs_sb_info *sbi)\n{\n#ifdef CONFIG_QUOTA\n\tif (f2fs_sb_has_quota_ino(sbi))\n\t\treturn true;\n\tif (F2FS_OPTION(sbi).s_qf_names[USRQUOTA] ||\n\t\tF2FS_OPTION(sbi).s_qf_names[GRPQUOTA] ||\n\t\tF2FS_OPTION(sbi).s_qf_names[PRJQUOTA])\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n#define EFSBADCRC\tEBADMSG\t\t/* Bad CRC detected */\n#define EFSCORRUPTED\tEUCLEAN\t\t/* Filesystem is corrupted */\n\n#endif /* _LINUX_F2FS_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#undef TRACE_SYSTEM\n#define TRACE_SYSTEM f2fs\n\n#if !defined(_TRACE_F2FS_H) || defined(TRACE_HEADER_MULTI_READ)\n#define _TRACE_F2FS_H\n\n#include <linux/tracepoint.h>\n\n#define show_dev(dev)\t\tMAJOR(dev), MINOR(dev)\n#define show_dev_ino(entry)\tshow_dev(entry->dev), (unsigned long)entry->ino\n\nTRACE_DEFINE_ENUM(NODE);\nTRACE_DEFINE_ENUM(DATA);\nTRACE_DEFINE_ENUM(META);\nTRACE_DEFINE_ENUM(META_FLUSH);\nTRACE_DEFINE_ENUM(INMEM);\nTRACE_DEFINE_ENUM(INMEM_DROP);\nTRACE_DEFINE_ENUM(INMEM_INVALIDATE);\nTRACE_DEFINE_ENUM(INMEM_REVOKE);\nTRACE_DEFINE_ENUM(IPU);\nTRACE_DEFINE_ENUM(OPU);\nTRACE_DEFINE_ENUM(HOT);\nTRACE_DEFINE_ENUM(WARM);\nTRACE_DEFINE_ENUM(COLD);\nTRACE_DEFINE_ENUM(CURSEG_HOT_DATA);\nTRACE_DEFINE_ENUM(CURSEG_WARM_DATA);\nTRACE_DEFINE_ENUM(CURSEG_COLD_DATA);\nTRACE_DEFINE_ENUM(CURSEG_HOT_NODE);\nTRACE_DEFINE_ENUM(CURSEG_WARM_NODE);\nTRACE_DEFINE_ENUM(CURSEG_COLD_NODE);\nTRACE_DEFINE_ENUM(NO_CHECK_TYPE);\nTRACE_DEFINE_ENUM(GC_GREEDY);\nTRACE_DEFINE_ENUM(GC_CB);\nTRACE_DEFINE_ENUM(FG_GC);\nTRACE_DEFINE_ENUM(BG_GC);\nTRACE_DEFINE_ENUM(LFS);\nTRACE_DEFINE_ENUM(SSR);\nTRACE_DEFINE_ENUM(__REQ_RAHEAD);\nTRACE_DEFINE_ENUM(__REQ_SYNC);\nTRACE_DEFINE_ENUM(__REQ_IDLE);\nTRACE_DEFINE_ENUM(__REQ_PREFLUSH);\nTRACE_DEFINE_ENUM(__REQ_FUA);\nTRACE_DEFINE_ENUM(__REQ_PRIO);\nTRACE_DEFINE_ENUM(__REQ_META);\nTRACE_DEFINE_ENUM(CP_UMOUNT);\nTRACE_DEFINE_ENUM(CP_FASTBOOT);\nTRACE_DEFINE_ENUM(CP_SYNC);\nTRACE_DEFINE_ENUM(CP_RECOVERY);\nTRACE_DEFINE_ENUM(CP_DISCARD);\nTRACE_DEFINE_ENUM(CP_TRIMMED);\n\n#define show_block_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ NODE,\t\t\"NODE\" },\t\t\t\t\\\n\t\t{ DATA,\t\t\"DATA\" },\t\t\t\t\\\n\t\t{ META,\t\t\"META\" },\t\t\t\t\\\n\t\t{ META_FLUSH,\t\"META_FLUSH\" },\t\t\t\t\\\n\t\t{ INMEM,\t\"INMEM\" },\t\t\t\t\\\n\t\t{ INMEM_DROP,\t\"INMEM_DROP\" },\t\t\t\t\\\n\t\t{ INMEM_INVALIDATE,\t\"INMEM_INVALIDATE\" },\t\t\\\n\t\t{ INMEM_REVOKE,\t\"INMEM_REVOKE\" },\t\t\t\\\n\t\t{ IPU,\t\t\"IN-PLACE\" },\t\t\t\t\\\n\t\t{ OPU,\t\t\"OUT-OF-PLACE\" })\n\n#define show_block_temp(temp)\t\t\t\t\t\t\\\n\t__print_symbolic(temp,\t\t\t\t\t\t\\\n\t\t{ HOT,\t\t\"HOT\" },\t\t\t\t\\\n\t\t{ WARM,\t\t\"WARM\" },\t\t\t\t\\\n\t\t{ COLD,\t\t\"COLD\" })\n\n#define F2FS_OP_FLAGS (REQ_RAHEAD | REQ_SYNC | REQ_META | REQ_PRIO |\t\\\n\t\t\tREQ_PREFLUSH | REQ_FUA)\n#define F2FS_BIO_FLAG_MASK(t)\t(t & F2FS_OP_FLAGS)\n\n#define show_bio_type(op,op_flags)\tshow_bio_op(op),\t\t\\\n\t\t\t\t\t\tshow_bio_op_flags(op_flags)\n\n#define show_bio_op(op)\t\t\t\t\t\t\t\\\n\t__print_symbolic(op,\t\t\t\t\t\t\\\n\t\t{ REQ_OP_READ,\t\t\t\"READ\" },\t\t\\\n\t\t{ REQ_OP_WRITE,\t\t\t\"WRITE\" },\t\t\\\n\t\t{ REQ_OP_FLUSH,\t\t\t\"FLUSH\" },\t\t\\\n\t\t{ REQ_OP_DISCARD,\t\t\"DISCARD\" },\t\t\\\n\t\t{ REQ_OP_SECURE_ERASE,\t\t\"SECURE_ERASE\" },\t\\\n\t\t{ REQ_OP_ZONE_RESET,\t\t\"ZONE_RESET\" },\t\t\\\n\t\t{ REQ_OP_WRITE_SAME,\t\t\"WRITE_SAME\" },\t\t\\\n\t\t{ REQ_OP_WRITE_ZEROES,\t\t\"WRITE_ZEROES\" })\n\n#define show_bio_op_flags(flags)\t\t\t\t\t\\\n\t__print_flags(F2FS_BIO_FLAG_MASK(flags), \"|\",\t\t\t\\\n\t\t{ REQ_RAHEAD,\t\t\"R\" },\t\t\t\t\\\n\t\t{ REQ_SYNC,\t\t\"S\" },\t\t\t\t\\\n\t\t{ REQ_META,\t\t\"M\" },\t\t\t\t\\\n\t\t{ REQ_PRIO,\t\t\"P\" },\t\t\t\t\\\n\t\t{ REQ_PREFLUSH,\t\t\"PF\" },\t\t\t\t\\\n\t\t{ REQ_FUA,\t\t\"FUA\" })\n\n#define show_data_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ CURSEG_HOT_DATA, \t\"Hot DATA\" },\t\t\t\\\n\t\t{ CURSEG_WARM_DATA, \t\"Warm DATA\" },\t\t\t\\\n\t\t{ CURSEG_COLD_DATA, \t\"Cold DATA\" },\t\t\t\\\n\t\t{ CURSEG_HOT_NODE, \t\"Hot NODE\" },\t\t\t\\\n\t\t{ CURSEG_WARM_NODE, \t\"Warm NODE\" },\t\t\t\\\n\t\t{ CURSEG_COLD_NODE, \t\"Cold NODE\" },\t\t\t\\\n\t\t{ NO_CHECK_TYPE, \t\"No TYPE\" })\n\n#define show_file_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ 0,\t\t\"FILE\" },\t\t\t\t\\\n\t\t{ 1,\t\t\"DIR\" })\n\n#define show_gc_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ FG_GC,\t\"Foreground GC\" },\t\t\t\\\n\t\t{ BG_GC,\t\"Background GC\" })\n\n#define show_alloc_mode(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ LFS,\t\"LFS-mode\" },\t\t\t\t\t\\\n\t\t{ SSR,\t\"SSR-mode\" })\n\n#define show_victim_policy(type)\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ GC_GREEDY,\t\"Greedy\" },\t\t\t\t\\\n\t\t{ GC_CB,\t\"Cost-Benefit\" })\n\n#define show_cpreason(type)\t\t\t\t\t\t\\\n\t__print_flags(type, \"|\",\t\t\t\t\t\\\n\t\t{ CP_UMOUNT,\t\"Umount\" },\t\t\t\t\\\n\t\t{ CP_FASTBOOT,\t\"Fastboot\" },\t\t\t\t\\\n\t\t{ CP_SYNC,\t\"Sync\" },\t\t\t\t\\\n\t\t{ CP_RECOVERY,\t\"Recovery\" },\t\t\t\t\\\n\t\t{ CP_DISCARD,\t\"Discard\" },\t\t\t\t\\\n\t\t{ CP_UMOUNT,\t\"Umount\" },\t\t\t\t\\\n\t\t{ CP_TRIMMED,\t\"Trimmed\" })\n\n#define show_fsync_cpreason(type)\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ CP_NO_NEEDED,\t\t\"no needed\" },\t\t\t\\\n\t\t{ CP_NON_REGULAR,\t\"non regular\" },\t\t\\\n\t\t{ CP_HARDLINK,\t\t\"hardlink\" },\t\t\t\\\n\t\t{ CP_SB_NEED_CP,\t\"sb needs cp\" },\t\t\\\n\t\t{ CP_WRONG_PINO,\t\"wrong pino\" },\t\t\t\\\n\t\t{ CP_NO_SPC_ROLL,\t\"no space roll forward\" },\t\\\n\t\t{ CP_NODE_NEED_CP,\t\"node needs cp\" },\t\t\\\n\t\t{ CP_FASTBOOT_MODE,\t\"fastboot mode\" },\t\t\\\n\t\t{ CP_SPEC_LOG_NUM,\t\"log type is 2\" },\t\t\\\n\t\t{ CP_RECOVER_DIR,\t\"dir needs recovery\" })\n\n#define show_shutdown_mode(type)\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ F2FS_GOING_DOWN_FULLSYNC,\t\"full sync\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_METASYNC,\t\"meta sync\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_NOSYNC,\t\"no sync\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_METAFLUSH,\t\"meta flush\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_NEED_FSCK,\t\"need fsck\" })\n\nstruct f2fs_sb_info;\nstruct f2fs_io_info;\nstruct extent_info;\nstruct victim_sel_policy;\nstruct f2fs_map_blocks;\n\nDECLARE_EVENT_CLASS(f2fs__inode,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(ino_t,\tpino)\n\t\t__field(umode_t, mode)\n\t\t__field(loff_t,\tsize)\n\t\t__field(unsigned int, nlink)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(__u8,\tadvise)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pino\t= F2FS_I(inode)->i_pino;\n\t\t__entry->mode\t= inode->i_mode;\n\t\t__entry->nlink\t= inode->i_nlink;\n\t\t__entry->size\t= inode->i_size;\n\t\t__entry->blocks\t= inode->i_blocks;\n\t\t__entry->advise\t= F2FS_I(inode)->i_advise;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pino = %lu, i_mode = 0x%hx, \"\n\t\t\"i_size = %lld, i_nlink = %u, i_blocks = %llu, i_advise = 0x%x\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->pino,\n\t\t__entry->mode,\n\t\t__entry->size,\n\t\t(unsigned int)__entry->nlink,\n\t\t(unsigned long long)__entry->blocks,\n\t\t(unsigned char)__entry->advise)\n);\n\nDECLARE_EVENT_CLASS(f2fs__inode_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->ret)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_sync_file_enter,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nTRACE_EVENT(f2fs_sync_file_exit,\n\n\tTP_PROTO(struct inode *inode, int cp_reason, int datasync, int ret),\n\n\tTP_ARGS(inode, cp_reason, datasync, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\tcp_reason)\n\t\t__field(int,\tdatasync)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->cp_reason\t= cp_reason;\n\t\t__entry->datasync\t= datasync;\n\t\t__entry->ret\t\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, cp_reason: %s, \"\n\t\t\"datasync = %d, ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\tshow_fsync_cpreason(__entry->cp_reason),\n\t\t__entry->datasync,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_sync_fs,\n\n\tTP_PROTO(struct super_block *sb, int wait),\n\n\tTP_ARGS(sb, wait),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(int,\tdirty)\n\t\t__field(int,\twait)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= sb->s_dev;\n\t\t__entry->dirty\t= is_sbi_flag_set(F2FS_SB(sb), SBI_IS_DIRTY);\n\t\t__entry->wait\t= wait;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), superblock is %s, wait = %d\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->dirty ? \"dirty\" : \"not dirty\",\n\t\t__entry->wait)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_iget,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_iget_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_evict_inode,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_new_inode,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nTRACE_EVENT(f2fs_unlink_enter,\n\n\tTP_PROTO(struct inode *dir, struct dentry *dentry),\n\n\tTP_ARGS(dir, dentry),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tsize)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(const char *,\tname)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->size\t= dir->i_size;\n\t\t__entry->blocks\t= dir->i_blocks;\n\t\t__entry->name\t= dentry->d_name.name;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), dir ino = %lu, i_size = %lld, \"\n\t\t\"i_blocks = %llu, name = %s\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->size,\n\t\t(unsigned long long)__entry->blocks,\n\t\t__entry->name)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_unlink_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_drop_inode,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_truncate,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nTRACE_EVENT(f2fs_truncate_data_blocks_range,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, unsigned int ofs, int free),\n\n\tTP_ARGS(inode, nid,  ofs, free),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(nid_t,\tnid)\n\t\t__field(unsigned int,\tofs)\n\t\t__field(int,\tfree)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->nid\t= nid;\n\t\t__entry->ofs\t= ofs;\n\t\t__entry->free\t= free;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, nid = %u, offset = %u, freed = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned int)__entry->nid,\n\t\t__entry->ofs,\n\t\t__entry->free)\n);\n\nDECLARE_EVENT_CLASS(f2fs__truncate_op,\n\n\tTP_PROTO(struct inode *inode, u64 from),\n\n\tTP_ARGS(inode, from),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tsize)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(u64,\tfrom)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->size\t= inode->i_size;\n\t\t__entry->blocks\t= inode->i_blocks;\n\t\t__entry->from\t= from;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, i_size = %lld, i_blocks = %llu, \"\n\t\t\"start file offset = %llu\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->size,\n\t\t(unsigned long long)__entry->blocks,\n\t\t(unsigned long long)__entry->from)\n);\n\nDEFINE_EVENT(f2fs__truncate_op, f2fs_truncate_blocks_enter,\n\n\tTP_PROTO(struct inode *inode, u64 from),\n\n\tTP_ARGS(inode, from)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_truncate_blocks_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__truncate_op, f2fs_truncate_inode_blocks_enter,\n\n\tTP_PROTO(struct inode *inode, u64 from),\n\n\tTP_ARGS(inode, from)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_truncate_inode_blocks_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDECLARE_EVENT_CLASS(f2fs__truncate_node,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, block_t blk_addr),\n\n\tTP_ARGS(inode, nid, blk_addr),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(nid_t,\tnid)\n\t\t__field(block_t,\tblk_addr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->nid\t\t= nid;\n\t\t__entry->blk_addr\t= blk_addr;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, nid = %u, block_address = 0x%llx\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned int)__entry->nid,\n\t\t(unsigned long long)__entry->blk_addr)\n);\n\nDEFINE_EVENT(f2fs__truncate_node, f2fs_truncate_nodes_enter,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, block_t blk_addr),\n\n\tTP_ARGS(inode, nid, blk_addr)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_truncate_nodes_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__truncate_node, f2fs_truncate_node,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, block_t blk_addr),\n\n\tTP_ARGS(inode, nid, blk_addr)\n);\n\nTRACE_EVENT(f2fs_truncate_partial_nodes,\n\n\tTP_PROTO(struct inode *inode, nid_t *nid, int depth, int err),\n\n\tTP_ARGS(inode, nid, depth, err),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(nid_t,\tnid[3])\n\t\t__field(int,\tdepth)\n\t\t__field(int,\terr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->nid[0]\t= nid[0];\n\t\t__entry->nid[1]\t= nid[1];\n\t\t__entry->nid[2]\t= nid[2];\n\t\t__entry->depth\t= depth;\n\t\t__entry->err\t= err;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, \"\n\t\t\"nid[0] = %u, nid[1] = %u, nid[2] = %u, depth = %d, err = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned int)__entry->nid[0],\n\t\t(unsigned int)__entry->nid[1],\n\t\t(unsigned int)__entry->nid[2],\n\t\t__entry->depth,\n\t\t__entry->err)\n);\n\nTRACE_EVENT(f2fs_file_write_iter,\n\n\tTP_PROTO(struct inode *inode, unsigned long offset,\n\t\tunsigned long length, int ret),\n\n\tTP_ARGS(inode, offset, length, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned long, offset)\n\t\t__field(unsigned long, length)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->offset\t= offset;\n\t\t__entry->length\t= length;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, \"\n\t\t\"offset = %lu, length = %lu, written(err) = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->offset,\n\t\t__entry->length,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_map_blocks,\n\tTP_PROTO(struct inode *inode, struct f2fs_map_blocks *map, int ret),\n\n\tTP_ARGS(inode, map, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(block_t,\tm_lblk)\n\t\t__field(block_t,\tm_pblk)\n\t\t__field(unsigned int,\tm_len)\n\t\t__field(unsigned int,\tm_flags)\n\t\t__field(int,\tm_seg_type)\n\t\t__field(bool,\tm_may_create)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->m_lblk\t\t= map->m_lblk;\n\t\t__entry->m_pblk\t\t= map->m_pblk;\n\t\t__entry->m_len\t\t= map->m_len;\n\t\t__entry->m_flags\t= map->m_flags;\n\t\t__entry->m_seg_type\t= map->m_seg_type;\n\t\t__entry->m_may_create\t= map->m_may_create;\n\t\t__entry->ret\t\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, file offset = %llu, \"\n\t\t\"start blkaddr = 0x%llx, len = 0x%llx, flags = %u,\"\n\t\t\"seg_type = %d, may_create = %d, err = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long long)__entry->m_lblk,\n\t\t(unsigned long long)__entry->m_pblk,\n\t\t(unsigned long long)__entry->m_len,\n\t\t__entry->m_flags,\n\t\t__entry->m_seg_type,\n\t\t__entry->m_may_create,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_background_gc,\n\n\tTP_PROTO(struct super_block *sb, unsigned int wait_ms,\n\t\t\tunsigned int prefree, unsigned int free),\n\n\tTP_ARGS(sb, wait_ms, prefree, free),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int,\twait_ms)\n\t\t__field(unsigned int,\tprefree)\n\t\t__field(unsigned int,\tfree)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->wait_ms\t= wait_ms;\n\t\t__entry->prefree\t= prefree;\n\t\t__entry->free\t\t= free;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), wait_ms = %u, prefree = %u, free = %u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->wait_ms,\n\t\t__entry->prefree,\n\t\t__entry->free)\n);\n\nTRACE_EVENT(f2fs_gc_begin,\n\n\tTP_PROTO(struct super_block *sb, bool sync, bool background,\n\t\t\tlong long dirty_nodes, long long dirty_dents,\n\t\t\tlong long dirty_imeta, unsigned int free_sec,\n\t\t\tunsigned int free_seg, int reserved_seg,\n\t\t\tunsigned int prefree_seg),\n\n\tTP_ARGS(sb, sync, background, dirty_nodes, dirty_dents, dirty_imeta,\n\t\tfree_sec, free_seg, reserved_seg, prefree_seg),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev)\n\t\t__field(bool,\t\tsync)\n\t\t__field(bool,\t\tbackground)\n\t\t__field(long long,\tdirty_nodes)\n\t\t__field(long long,\tdirty_dents)\n\t\t__field(long long,\tdirty_imeta)\n\t\t__field(unsigned int,\tfree_sec)\n\t\t__field(unsigned int,\tfree_seg)\n\t\t__field(int,\t\treserved_seg)\n\t\t__field(unsigned int,\tprefree_seg)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->sync\t\t= sync;\n\t\t__entry->background\t= background;\n\t\t__entry->dirty_nodes\t= dirty_nodes;\n\t\t__entry->dirty_dents\t= dirty_dents;\n\t\t__entry->dirty_imeta\t= dirty_imeta;\n\t\t__entry->free_sec\t= free_sec;\n\t\t__entry->free_seg\t= free_seg;\n\t\t__entry->reserved_seg\t= reserved_seg;\n\t\t__entry->prefree_seg\t= prefree_seg;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), sync = %d, background = %d, nodes = %lld, \"\n\t\t\"dents = %lld, imeta = %lld, free_sec:%u, free_seg:%u, \"\n\t\t\"rsv_seg:%d, prefree_seg:%u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->sync,\n\t\t__entry->background,\n\t\t__entry->dirty_nodes,\n\t\t__entry->dirty_dents,\n\t\t__entry->dirty_imeta,\n\t\t__entry->free_sec,\n\t\t__entry->free_seg,\n\t\t__entry->reserved_seg,\n\t\t__entry->prefree_seg)\n);\n\nTRACE_EVENT(f2fs_gc_end,\n\n\tTP_PROTO(struct super_block *sb, int ret, int seg_freed,\n\t\t\tint sec_freed, long long dirty_nodes,\n\t\t\tlong long dirty_dents, long long dirty_imeta,\n\t\t\tunsigned int free_sec, unsigned int free_seg,\n\t\t\tint reserved_seg, unsigned int prefree_seg),\n\n\tTP_ARGS(sb, ret, seg_freed, sec_freed, dirty_nodes, dirty_dents,\n\t\tdirty_imeta, free_sec, free_seg, reserved_seg, prefree_seg),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev)\n\t\t__field(int,\t\tret)\n\t\t__field(int,\t\tseg_freed)\n\t\t__field(int,\t\tsec_freed)\n\t\t__field(long long,\tdirty_nodes)\n\t\t__field(long long,\tdirty_dents)\n\t\t__field(long long,\tdirty_imeta)\n\t\t__field(unsigned int,\tfree_sec)\n\t\t__field(unsigned int,\tfree_seg)\n\t\t__field(int,\t\treserved_seg)\n\t\t__field(unsigned int,\tprefree_seg)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->ret\t\t= ret;\n\t\t__entry->seg_freed\t= seg_freed;\n\t\t__entry->sec_freed\t= sec_freed;\n\t\t__entry->dirty_nodes\t= dirty_nodes;\n\t\t__entry->dirty_dents\t= dirty_dents;\n\t\t__entry->dirty_imeta\t= dirty_imeta;\n\t\t__entry->free_sec\t= free_sec;\n\t\t__entry->free_seg\t= free_seg;\n\t\t__entry->reserved_seg\t= reserved_seg;\n\t\t__entry->prefree_seg\t= prefree_seg;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ret = %d, seg_freed = %d, sec_freed = %d, \"\n\t\t\"nodes = %lld, dents = %lld, imeta = %lld, free_sec:%u, \"\n\t\t\"free_seg:%u, rsv_seg:%d, prefree_seg:%u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->ret,\n\t\t__entry->seg_freed,\n\t\t__entry->sec_freed,\n\t\t__entry->dirty_nodes,\n\t\t__entry->dirty_dents,\n\t\t__entry->dirty_imeta,\n\t\t__entry->free_sec,\n\t\t__entry->free_seg,\n\t\t__entry->reserved_seg,\n\t\t__entry->prefree_seg)\n);\n\nTRACE_EVENT(f2fs_get_victim,\n\n\tTP_PROTO(struct super_block *sb, int type, int gc_type,\n\t\t\tstruct victim_sel_policy *p, unsigned int pre_victim,\n\t\t\tunsigned int prefree, unsigned int free),\n\n\tTP_ARGS(sb, type, gc_type, p, pre_victim, prefree, free),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(int,\ttype)\n\t\t__field(int,\tgc_type)\n\t\t__field(int,\talloc_mode)\n\t\t__field(int,\tgc_mode)\n\t\t__field(unsigned int,\tvictim)\n\t\t__field(unsigned int,\tcost)\n\t\t__field(unsigned int,\tofs_unit)\n\t\t__field(unsigned int,\tpre_victim)\n\t\t__field(unsigned int,\tprefree)\n\t\t__field(unsigned int,\tfree)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->type\t\t= type;\n\t\t__entry->gc_type\t= gc_type;\n\t\t__entry->alloc_mode\t= p->alloc_mode;\n\t\t__entry->gc_mode\t= p->gc_mode;\n\t\t__entry->victim\t\t= p->min_segno;\n\t\t__entry->cost\t\t= p->min_cost;\n\t\t__entry->ofs_unit\t= p->ofs_unit;\n\t\t__entry->pre_victim\t= pre_victim;\n\t\t__entry->prefree\t= prefree;\n\t\t__entry->free\t\t= free;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), type = %s, policy = (%s, %s, %s), \"\n\t\t\"victim = %u, cost = %u, ofs_unit = %u, \"\n\t\t\"pre_victim_secno = %d, prefree = %u, free = %u\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_data_type(__entry->type),\n\t\tshow_gc_type(__entry->gc_type),\n\t\tshow_alloc_mode(__entry->alloc_mode),\n\t\tshow_victim_policy(__entry->gc_mode),\n\t\t__entry->victim,\n\t\t__entry->cost,\n\t\t__entry->ofs_unit,\n\t\t(int)__entry->pre_victim,\n\t\t__entry->prefree,\n\t\t__entry->free)\n);\n\nTRACE_EVENT(f2fs_lookup_start,\n\n\tTP_PROTO(struct inode *dir, struct dentry *dentry, unsigned int flags),\n\n\tTP_ARGS(dir, dentry, flags),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(const char *,\tname)\n\t\t__field(unsigned int, flags)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->name\t= dentry->d_name.name;\n\t\t__entry->flags\t= flags;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), pino = %lu, name:%s, flags:%u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->name,\n\t\t__entry->flags)\n);\n\nTRACE_EVENT(f2fs_lookup_end,\n\n\tTP_PROTO(struct inode *dir, struct dentry *dentry, nid_t ino,\n\t\tint err),\n\n\tTP_ARGS(dir, dentry, ino, err),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(const char *,\tname)\n\t\t__field(nid_t,\tcino)\n\t\t__field(int,\terr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->name\t= dentry->d_name.name;\n\t\t__entry->cino\t= ino;\n\t\t__entry->err\t= err;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), pino = %lu, name:%s, ino:%u, err:%d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->name,\n\t\t__entry->cino,\n\t\t__entry->err)\n);\n\nTRACE_EVENT(f2fs_readdir,\n\n\tTP_PROTO(struct inode *dir, loff_t start_pos, loff_t end_pos, int err),\n\n\tTP_ARGS(dir, start_pos, end_pos, err),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tstart)\n\t\t__field(loff_t,\tend)\n\t\t__field(int,\terr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->start\t= start_pos;\n\t\t__entry->end\t= end_pos;\n\t\t__entry->err\t= err;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, start_pos:%llu, end_pos:%llu, err:%d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->start,\n\t\t__entry->end,\n\t\t__entry->err)\n);\n\nTRACE_EVENT(f2fs_fallocate,\n\n\tTP_PROTO(struct inode *inode, int mode,\n\t\t\t\tloff_t offset, loff_t len, int ret),\n\n\tTP_ARGS(inode, mode, offset, len, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\tmode)\n\t\t__field(loff_t,\toffset)\n\t\t__field(loff_t,\tlen)\n\t\t__field(loff_t, size)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->mode\t= mode;\n\t\t__entry->offset\t= offset;\n\t\t__entry->len\t= len;\n\t\t__entry->size\t= inode->i_size;\n\t\t__entry->blocks = inode->i_blocks;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, mode = %x, offset = %lld, \"\n\t\t\"len = %lld,  i_size = %lld, i_blocks = %llu, ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->mode,\n\t\t(unsigned long long)__entry->offset,\n\t\t(unsigned long long)__entry->len,\n\t\t(unsigned long long)__entry->size,\n\t\t(unsigned long long)__entry->blocks,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_direct_IO_enter,\n\n\tTP_PROTO(struct inode *inode, loff_t offset, unsigned long len, int rw),\n\n\tTP_ARGS(inode, offset, len, rw),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned long,\tlen)\n\t\t__field(int,\trw)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= offset;\n\t\t__entry->len\t= len;\n\t\t__entry->rw\t= rw;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu pos = %lld len = %lu rw = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pos,\n\t\t__entry->len,\n\t\t__entry->rw)\n);\n\nTRACE_EVENT(f2fs_direct_IO_exit,\n\n\tTP_PROTO(struct inode *inode, loff_t offset, unsigned long len,\n\t\t int rw, int ret),\n\n\tTP_ARGS(inode, offset, len, rw, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned long,\tlen)\n\t\t__field(int,\trw)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= offset;\n\t\t__entry->len\t= len;\n\t\t__entry->rw\t= rw;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu pos = %lld len = %lu \"\n\t\t\"rw = %d ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pos,\n\t\t__entry->len,\n\t\t__entry->rw,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_reserve_new_blocks,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, unsigned int ofs_in_node,\n\t\t\t\t\t\t\tblkcnt_t count),\n\n\tTP_ARGS(inode, nid, ofs_in_node, count),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(nid_t, nid)\n\t\t__field(unsigned int, ofs_in_node)\n\t\t__field(blkcnt_t, count)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->nid\t= nid;\n\t\t__entry->ofs_in_node = ofs_in_node;\n\t\t__entry->count = count;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), nid = %u, ofs_in_node = %u, count = %llu\",\n\t\tshow_dev(__entry->dev),\n\t\t(unsigned int)__entry->nid,\n\t\t__entry->ofs_in_node,\n\t\t(unsigned long long)__entry->count)\n);\n\nDECLARE_EVENT_CLASS(f2fs__submit_page_bio,\n\n\tTP_PROTO(struct page *page, struct f2fs_io_info *fio),\n\n\tTP_ARGS(page, fio),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t, dev)\n\t\t__field(ino_t, ino)\n\t\t__field(pgoff_t, index)\n\t\t__field(block_t, old_blkaddr)\n\t\t__field(block_t, new_blkaddr)\n\t\t__field(int, op)\n\t\t__field(int, op_flags)\n\t\t__field(int, temp)\n\t\t__field(int, type)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= page->mapping->host->i_sb->s_dev;\n\t\t__entry->ino\t\t= page->mapping->host->i_ino;\n\t\t__entry->index\t\t= page->index;\n\t\t__entry->old_blkaddr\t= fio->old_blkaddr;\n\t\t__entry->new_blkaddr\t= fio->new_blkaddr;\n\t\t__entry->op\t\t= fio->op;\n\t\t__entry->op_flags\t= fio->op_flags;\n\t\t__entry->temp\t\t= fio->temp;\n\t\t__entry->type\t\t= fio->type;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, page_index = 0x%lx, \"\n\t\t\"oldaddr = 0x%llx, newaddr = 0x%llx, rw = %s(%s), type = %s_%s\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->index,\n\t\t(unsigned long long)__entry->old_blkaddr,\n\t\t(unsigned long long)__entry->new_blkaddr,\n\t\tshow_bio_type(__entry->op, __entry->op_flags),\n\t\tshow_block_temp(__entry->temp),\n\t\tshow_block_type(__entry->type))\n);\n\nDEFINE_EVENT_CONDITION(f2fs__submit_page_bio, f2fs_submit_page_bio,\n\n\tTP_PROTO(struct page *page, struct f2fs_io_info *fio),\n\n\tTP_ARGS(page, fio),\n\n\tTP_CONDITION(page->mapping)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__submit_page_bio, f2fs_submit_page_write,\n\n\tTP_PROTO(struct page *page, struct f2fs_io_info *fio),\n\n\tTP_ARGS(page, fio),\n\n\tTP_CONDITION(page->mapping)\n);\n\nDECLARE_EVENT_CLASS(f2fs__bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(dev_t,\ttarget)\n\t\t__field(int,\top)\n\t\t__field(int,\top_flags)\n\t\t__field(int,\ttype)\n\t\t__field(sector_t,\tsector)\n\t\t__field(unsigned int,\tsize)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->target\t\t= bio_dev(bio);\n\t\t__entry->op\t\t= bio_op(bio);\n\t\t__entry->op_flags\t= bio->bi_opf;\n\t\t__entry->type\t\t= type;\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->size\t\t= bio->bi_iter.bi_size;\n\t),\n\n\tTP_printk(\"dev = (%d,%d)/(%d,%d), rw = %s(%s), %s, sector = %lld, size = %u\",\n\t\tshow_dev(__entry->target),\n\t\tshow_dev(__entry->dev),\n\t\tshow_bio_type(__entry->op, __entry->op_flags),\n\t\tshow_block_type(__entry->type),\n\t\t(unsigned long long)__entry->sector,\n\t\t__entry->size)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_prepare_write_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_prepare_read_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_submit_read_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_submit_write_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nTRACE_EVENT(f2fs_write_begin,\n\n\tTP_PROTO(struct inode *inode, loff_t pos, unsigned int len,\n\t\t\t\tunsigned int flags),\n\n\tTP_ARGS(inode, pos, len, flags),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned int, len)\n\t\t__field(unsigned int, flags)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= pos;\n\t\t__entry->len\t= len;\n\t\t__entry->flags\t= flags;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pos = %llu, len = %u, flags = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long long)__entry->pos,\n\t\t__entry->len,\n\t\t__entry->flags)\n);\n\nTRACE_EVENT(f2fs_write_end,\n\n\tTP_PROTO(struct inode *inode, loff_t pos, unsigned int len,\n\t\t\t\tunsigned int copied),\n\n\tTP_ARGS(inode, pos, len, copied),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned int, len)\n\t\t__field(unsigned int, copied)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= pos;\n\t\t__entry->len\t= len;\n\t\t__entry->copied\t= copied;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pos = %llu, len = %u, copied = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long long)__entry->pos,\n\t\t__entry->len,\n\t\t__entry->copied)\n);\n\nDECLARE_EVENT_CLASS(f2fs__page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int, type)\n\t\t__field(int, dir)\n\t\t__field(pgoff_t, index)\n\t\t__field(int, dirty)\n\t\t__field(int, uptodate)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= page->mapping->host->i_sb->s_dev;\n\t\t__entry->ino\t= page->mapping->host->i_ino;\n\t\t__entry->type\t= type;\n\t\t__entry->dir\t= S_ISDIR(page->mapping->host->i_mode);\n\t\t__entry->index\t= page->index;\n\t\t__entry->dirty\t= PageDirty(page);\n\t\t__entry->uptodate = PageUptodate(page);\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, %s, %s, index = %lu, \"\n\t\t\"dirty = %d, uptodate = %d\",\n\t\tshow_dev_ino(__entry),\n\t\tshow_block_type(__entry->type),\n\t\tshow_file_type(__entry->dir),\n\t\t(unsigned long)__entry->index,\n\t\t__entry->dirty,\n\t\t__entry->uptodate)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_writepage,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_do_write_data_page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_readpage,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_set_page_dirty,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_vm_page_mkwrite,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_register_inmem_page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_commit_inmem_page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nTRACE_EVENT(f2fs_filemap_fault,\n\n\tTP_PROTO(struct inode *inode, pgoff_t index, unsigned long ret),\n\n\tTP_ARGS(inode, index, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(pgoff_t, index)\n\t\t__field(unsigned long, ret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->index\t= index;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, index = %lu, ret = %lx\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->index,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_writepages,\n\n\tTP_PROTO(struct inode *inode, struct writeback_control *wbc, int type),\n\n\tTP_ARGS(inode, wbc, type),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\ttype)\n\t\t__field(int,\tdir)\n\t\t__field(long,\tnr_to_write)\n\t\t__field(long,\tpages_skipped)\n\t\t__field(loff_t,\trange_start)\n\t\t__field(loff_t,\trange_end)\n\t\t__field(pgoff_t, writeback_index)\n\t\t__field(int,\tsync_mode)\n\t\t__field(char,\tfor_kupdate)\n\t\t__field(char,\tfor_background)\n\t\t__field(char,\ttagged_writepages)\n\t\t__field(char,\tfor_reclaim)\n\t\t__field(char,\trange_cyclic)\n\t\t__field(char,\tfor_sync)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->type\t\t= type;\n\t\t__entry->dir\t\t= S_ISDIR(inode->i_mode);\n\t\t__entry->nr_to_write\t= wbc->nr_to_write;\n\t\t__entry->pages_skipped\t= wbc->pages_skipped;\n\t\t__entry->range_start\t= wbc->range_start;\n\t\t__entry->range_end\t= wbc->range_end;\n\t\t__entry->writeback_index = inode->i_mapping->writeback_index;\n\t\t__entry->sync_mode\t= wbc->sync_mode;\n\t\t__entry->for_kupdate\t= wbc->for_kupdate;\n\t\t__entry->for_background\t= wbc->for_background;\n\t\t__entry->tagged_writepages\t= wbc->tagged_writepages;\n\t\t__entry->for_reclaim\t= wbc->for_reclaim;\n\t\t__entry->range_cyclic\t= wbc->range_cyclic;\n\t\t__entry->for_sync\t= wbc->for_sync;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, %s, %s, nr_to_write %ld, \"\n\t\t\"skipped %ld, start %lld, end %lld, wb_idx %lu, sync_mode %d, \"\n\t\t\"kupdate %u background %u tagged %u reclaim %u cyclic %u sync %u\",\n\t\tshow_dev_ino(__entry),\n\t\tshow_block_type(__entry->type),\n\t\tshow_file_type(__entry->dir),\n\t\t__entry->nr_to_write,\n\t\t__entry->pages_skipped,\n\t\t__entry->range_start,\n\t\t__entry->range_end,\n\t\t(unsigned long)__entry->writeback_index,\n\t\t__entry->sync_mode,\n\t\t__entry->for_kupdate,\n\t\t__entry->for_background,\n\t\t__entry->tagged_writepages,\n\t\t__entry->for_reclaim,\n\t\t__entry->range_cyclic,\n\t\t__entry->for_sync)\n);\n\nTRACE_EVENT(f2fs_readpages,\n\n\tTP_PROTO(struct inode *inode, struct page *page, unsigned int nrpage),\n\n\tTP_ARGS(inode, page, nrpage),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(pgoff_t,\tstart)\n\t\t__field(unsigned int,\tnrpage)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->start\t= page->index;\n\t\t__entry->nrpage\t= nrpage;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, start = %lu nrpage = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->start,\n\t\t__entry->nrpage)\n);\n\nTRACE_EVENT(f2fs_write_checkpoint,\n\n\tTP_PROTO(struct super_block *sb, int reason, char *msg),\n\n\tTP_ARGS(sb, reason, msg),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(int,\treason)\n\t\t__field(char *,\tmsg)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->reason\t\t= reason;\n\t\t__entry->msg\t\t= msg;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), checkpoint for %s, state = %s\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_cpreason(__entry->reason),\n\t\t__entry->msg)\n);\n\nDECLARE_EVENT_CLASS(f2fs_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(block_t, blkstart)\n\t\t__field(block_t, blklen)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dev->bd_dev;\n\t\t__entry->blkstart = blkstart;\n\t\t__entry->blklen = blklen;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), blkstart = 0x%llx, blklen = 0x%llx\",\n\t\tshow_dev(__entry->dev),\n\t\t(unsigned long long)__entry->blkstart,\n\t\t(unsigned long long)__entry->blklen)\n);\n\nDEFINE_EVENT(f2fs_discard, f2fs_queue_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen)\n);\n\nDEFINE_EVENT(f2fs_discard, f2fs_issue_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen)\n);\n\nDEFINE_EVENT(f2fs_discard, f2fs_remove_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen)\n);\n\nTRACE_EVENT(f2fs_issue_reset_zone,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart),\n\n\tTP_ARGS(dev, blkstart),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(block_t, blkstart)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dev->bd_dev;\n\t\t__entry->blkstart = blkstart;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), reset zone at block = 0x%llx\",\n\t\tshow_dev(__entry->dev),\n\t\t(unsigned long long)__entry->blkstart)\n);\n\nTRACE_EVENT(f2fs_issue_flush,\n\n\tTP_PROTO(struct block_device *dev, unsigned int nobarrier,\n\t\t\t\tunsigned int flush_merge, int ret),\n\n\tTP_ARGS(dev, nobarrier, flush_merge, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int, nobarrier)\n\t\t__field(unsigned int, flush_merge)\n\t\t__field(int,  ret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dev->bd_dev;\n\t\t__entry->nobarrier = nobarrier;\n\t\t__entry->flush_merge = flush_merge;\n\t\t__entry->ret = ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), %s %s, ret = %d\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->nobarrier ? \"skip (nobarrier)\" : \"issue\",\n\t\t__entry->flush_merge ? \" with flush_merge\" : \"\",\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_lookup_extent_tree_start,\n\n\tTP_PROTO(struct inode *inode, unsigned int pgofs),\n\n\tTP_ARGS(inode, pgofs),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, pgofs)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->pgofs = pgofs;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pgofs = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pgofs)\n);\n\nTRACE_EVENT_CONDITION(f2fs_lookup_extent_tree_end,\n\n\tTP_PROTO(struct inode *inode, unsigned int pgofs,\n\t\t\t\t\t\tstruct extent_info *ei),\n\n\tTP_ARGS(inode, pgofs, ei),\n\n\tTP_CONDITION(ei),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, pgofs)\n\t\t__field(unsigned int, fofs)\n\t\t__field(u32, blk)\n\t\t__field(unsigned int, len)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->pgofs = pgofs;\n\t\t__entry->fofs = ei->fofs;\n\t\t__entry->blk = ei->blk;\n\t\t__entry->len = ei->len;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pgofs = %u, \"\n\t\t\"ext_info(fofs: %u, blk: %u, len: %u)\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pgofs,\n\t\t__entry->fofs,\n\t\t__entry->blk,\n\t\t__entry->len)\n);\n\nTRACE_EVENT(f2fs_update_extent_tree_range,\n\n\tTP_PROTO(struct inode *inode, unsigned int pgofs, block_t blkaddr,\n\t\t\t\t\t\tunsigned int len),\n\n\tTP_ARGS(inode, pgofs, blkaddr, len),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, pgofs)\n\t\t__field(u32, blk)\n\t\t__field(unsigned int, len)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->pgofs = pgofs;\n\t\t__entry->blk = blkaddr;\n\t\t__entry->len = len;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pgofs = %u, \"\n\t\t\t\t\t\"blkaddr = %u, len = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pgofs,\n\t\t__entry->blk,\n\t\t__entry->len)\n);\n\nTRACE_EVENT(f2fs_shrink_extent_tree,\n\n\tTP_PROTO(struct f2fs_sb_info *sbi, unsigned int node_cnt,\n\t\t\t\t\t\tunsigned int tree_cnt),\n\n\tTP_ARGS(sbi, node_cnt, tree_cnt),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int, node_cnt)\n\t\t__field(unsigned int, tree_cnt)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = sbi->sb->s_dev;\n\t\t__entry->node_cnt = node_cnt;\n\t\t__entry->tree_cnt = tree_cnt;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), shrunk: node_cnt = %u, tree_cnt = %u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->node_cnt,\n\t\t__entry->tree_cnt)\n);\n\nTRACE_EVENT(f2fs_destroy_extent_tree,\n\n\tTP_PROTO(struct inode *inode, unsigned int node_cnt),\n\n\tTP_ARGS(inode, node_cnt),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, node_cnt)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->node_cnt = node_cnt;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, destroyed: node_cnt = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->node_cnt)\n);\n\nDECLARE_EVENT_CLASS(f2fs_sync_dirty_inodes,\n\n\tTP_PROTO(struct super_block *sb, int type, s64 count),\n\n\tTP_ARGS(sb, type, count),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t, dev)\n\t\t__field(int, type)\n\t\t__field(s64, count)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= sb->s_dev;\n\t\t__entry->type\t= type;\n\t\t__entry->count\t= count;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), %s, dirty count = %lld\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_file_type(__entry->type),\n\t\t__entry->count)\n);\n\nDEFINE_EVENT(f2fs_sync_dirty_inodes, f2fs_sync_dirty_inodes_enter,\n\n\tTP_PROTO(struct super_block *sb, int type, s64 count),\n\n\tTP_ARGS(sb, type, count)\n);\n\nDEFINE_EVENT(f2fs_sync_dirty_inodes, f2fs_sync_dirty_inodes_exit,\n\n\tTP_PROTO(struct super_block *sb, int type, s64 count),\n\n\tTP_ARGS(sb, type, count)\n);\n\nTRACE_EVENT(f2fs_shutdown,\n\n\tTP_PROTO(struct f2fs_sb_info *sbi, unsigned int mode, int ret),\n\n\tTP_ARGS(sbi, mode, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int, mode)\n\t\t__field(int, ret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = sbi->sb->s_dev;\n\t\t__entry->mode = mode;\n\t\t__entry->ret = ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), mode: %s, ret:%d\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_shutdown_mode(__entry->mode),\n\t\t__entry->ret)\n);\n\n#endif /* _TRACE_F2FS_H */\n\n /* This part must be outside protection */\n#include <trace/define_trace.h>\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n * fs/f2fs/data.c\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n */\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/buffer_head.h>\n#include <linux/mpage.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/pagevec.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/swap.h>\n#include <linux/prefetch.h>\n#include <linux/uio.h>\n#include <linux/cleancache.h>\n#include <linux/sched/signal.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"trace.h\"\n#include <trace/events/f2fs.h>\n\n#define NUM_PREALLOC_POST_READ_CTXS\t128\n\nstatic struct kmem_cache *bio_post_read_ctx_cache;\nstatic mempool_t *bio_post_read_ctx_pool;\n\nstatic bool __is_cp_guaranteed(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode;\n\tstruct f2fs_sb_info *sbi;\n\n\tif (!mapping)\n\t\treturn false;\n\n\tinode = mapping->host;\n\tsbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_META_INO(sbi) ||\n\t\t\tinode->i_ino ==  F2FS_NODE_INO(sbi) ||\n\t\t\tS_ISDIR(inode->i_mode) ||\n\t\t\t(S_ISREG(inode->i_mode) &&\n\t\t\t(f2fs_is_atomic_file(inode) || IS_NOQUOTA(inode))) ||\n\t\t\tis_cold_data(page))\n\t\treturn true;\n\treturn false;\n}\n\nstatic enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page_file_mapping(page);\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}\n\n/* postprocessing steps for read bios */\nenum bio_post_read_step {\n\tSTEP_INITIAL = 0,\n\tSTEP_DECRYPT,\n};\n\nstruct bio_post_read_ctx {\n\tstruct bio *bio;\n\tstruct work_struct work;\n\tunsigned int cur_step;\n\tunsigned int enabled_steps;\n};\n\nstatic void __read_end_io(struct bio *bio)\n{\n\tstruct page *page;\n\tstruct bio_vec *bv;\n\tstruct bvec_iter_all iter_all;\n\n\tbio_for_each_segment_all(bv, bio, iter_all) {\n\t\tpage = bv->bv_page;\n\n\t\t/* PG_error was set if any post_read step failed */\n\t\tif (bio->bi_status || PageError(page)) {\n\t\t\tClearPageUptodate(page);\n\t\t\t/* will re-read again later */\n\t\t\tClearPageError(page);\n\t\t} else {\n\t\t\tSetPageUptodate(page);\n\t\t}\n\t\tdec_page_count(F2FS_P_SB(page), __read_io_type(page));\n\t\tunlock_page(page);\n\t}\n\tif (bio->bi_private)\n\t\tmempool_free(bio->bi_private, bio_post_read_ctx_pool);\n\tbio_put(bio);\n}\n\nstatic void bio_post_read_processing(struct bio_post_read_ctx *ctx);\n\nstatic void decrypt_work(struct work_struct *work)\n{\n\tstruct bio_post_read_ctx *ctx =\n\t\tcontainer_of(work, struct bio_post_read_ctx, work);\n\n\tfscrypt_decrypt_bio(ctx->bio);\n\n\tbio_post_read_processing(ctx);\n}\n\nstatic void bio_post_read_processing(struct bio_post_read_ctx *ctx)\n{\n\tswitch (++ctx->cur_step) {\n\tcase STEP_DECRYPT:\n\t\tif (ctx->enabled_steps & (1 << STEP_DECRYPT)) {\n\t\t\tINIT_WORK(&ctx->work, decrypt_work);\n\t\t\tfscrypt_enqueue_decrypt_work(&ctx->work);\n\t\t\treturn;\n\t\t}\n\t\tctx->cur_step++;\n\t\t/* fall-through */\n\tdefault:\n\t\t__read_end_io(ctx->bio);\n\t}\n}\n\nstatic bool f2fs_bio_post_read_required(struct bio *bio)\n{\n\treturn bio->bi_private && !bio->bi_status;\n}\n\nstatic void f2fs_read_end_io(struct bio *bio)\n{\n\tif (time_to_inject(F2FS_P_SB(bio_first_page_all(bio)),\n\t\t\t\t\t\tFAULT_READ_IO)) {\n\t\tf2fs_show_injection_info(FAULT_READ_IO);\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t}\n\n\tif (f2fs_bio_post_read_required(bio)) {\n\t\tstruct bio_post_read_ctx *ctx = bio->bi_private;\n\n\t\tctx->cur_step = STEP_INITIAL;\n\t\tbio_post_read_processing(ctx);\n\t\treturn;\n\t}\n\n\t__read_end_io(bio);\n}\n\nstatic void f2fs_write_end_io(struct bio *bio)\n{\n\tstruct f2fs_sb_info *sbi = bio->bi_private;\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tif (time_to_inject(sbi, FAULT_WRITE_IO)) {\n\t\tf2fs_show_injection_info(FAULT_WRITE_IO);\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t}\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct page *page = bvec->bv_page;\n\t\tenum count_type type = WB_DATA_TYPE(page);\n\n\t\tif (IS_DUMMY_WRITTEN_PAGE(page)) {\n\t\t\tset_page_private(page, (unsigned long)NULL);\n\t\t\tClearPagePrivate(page);\n\t\t\tunlock_page(page);\n\t\t\tmempool_free(page, sbi->write_io_dummy);\n\n\t\t\tif (unlikely(bio->bi_status))\n\t\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfscrypt_pullback_bio_page(&page, true);\n\n\t\tif (unlikely(bio->bi_status)) {\n\t\t\tmapping_set_error(page->mapping, -EIO);\n\t\t\tif (type == F2FS_WB_CP_DATA)\n\t\t\t\tf2fs_stop_checkpoint(sbi, true);\n\t\t}\n\n\t\tf2fs_bug_on(sbi, page->mapping == NODE_MAPPING(sbi) &&\n\t\t\t\t\tpage->index != nid_of_node(page));\n\n\t\tdec_page_count(sbi, type);\n\t\tif (f2fs_in_warm_node_list(sbi, page))\n\t\t\tf2fs_del_fsync_node_entry(sbi, page);\n\t\tclear_cold_data(page);\n\t\tend_page_writeback(page);\n\t}\n\tif (!get_pages(sbi, F2FS_WB_CP_DATA) &&\n\t\t\t\twq_has_sleeper(&sbi->cp_wait))\n\t\twake_up(&sbi->cp_wait);\n\n\tbio_put(bio);\n}\n\n/*\n * Return true, if pre_bio's bdev is same as its target device.\n */\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tint i;\n\n\tif (f2fs_is_multi_device(sbi)) {\n\t\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\t\tif (FDEV(i).start_blk <= blk_addr &&\n\t\t\t    FDEV(i).end_blk >= blk_addr) {\n\t\t\t\tblk_addr -= FDEV(i).start_blk;\n\t\t\t\tbdev = FDEV(i).bdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (bio) {\n\t\tbio_set_dev(bio, bdev);\n\t\tbio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(blk_addr);\n\t}\n\treturn bdev;\n}\n\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn 0;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)\n\t\t\treturn i;\n\treturn 0;\n}\n\nstatic bool __same_bdev(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blk_addr, struct bio *bio)\n{\n\tstruct block_device *b = f2fs_target_device(sbi, blk_addr, NULL);\n\treturn bio->bi_disk == b->bd_disk && bio->bi_partno == b->bd_partno;\n}\n\n/*\n * Low-level block read/write IO operations.\n */\nstatic struct bio *__bio_alloc(struct f2fs_sb_info *sbi, block_t blk_addr,\n\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\tint npages, bool is_read,\n\t\t\t\tenum page_type type, enum temp_type temp)\n{\n\tstruct bio *bio;\n\n\tbio = f2fs_bio_alloc(sbi, npages, true);\n\n\tf2fs_target_device(sbi, blk_addr, bio);\n\tif (is_read) {\n\t\tbio->bi_end_io = f2fs_read_end_io;\n\t\tbio->bi_private = NULL;\n\t} else {\n\t\tbio->bi_end_io = f2fs_write_end_io;\n\t\tbio->bi_private = sbi;\n\t\tbio->bi_write_hint = f2fs_io_type_to_rw_hint(sbi, type, temp);\n\t}\n\tif (wbc)\n\t\twbc_init_bio(wbc, bio);\n\n\treturn bio;\n}\n\nstatic inline void __submit_bio(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct bio *bio, enum page_type type)\n{\n\tif (!is_read_io(bio_op(bio))) {\n\t\tunsigned int start;\n\n\t\tif (type != DATA && type != NODE)\n\t\t\tgoto submit_io;\n\n\t\tif (test_opt(sbi, LFS) && current->plug)\n\t\t\tblk_finish_plug(current->plug);\n\n\t\tstart = bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS;\n\t\tstart %= F2FS_IO_SIZE(sbi);\n\n\t\tif (start == 0)\n\t\t\tgoto submit_io;\n\n\t\t/* fill dummy pages */\n\t\tfor (; start < F2FS_IO_SIZE(sbi); start++) {\n\t\t\tstruct page *page =\n\t\t\t\tmempool_alloc(sbi->write_io_dummy,\n\t\t\t\t\t      GFP_NOIO | __GFP_NOFAIL);\n\t\t\tf2fs_bug_on(sbi, !page);\n\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tSetPagePrivate(page);\n\t\t\tset_page_private(page, (unsigned long)DUMMY_WRITTEN_PAGE);\n\t\t\tlock_page(page);\n\t\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)\n\t\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n\t\t/*\n\t\t * In the NODE case, we lose next block address chain. So, we\n\t\t * need to do checkpoint in f2fs_sync_file.\n\t\t */\n\t\tif (type == NODE)\n\t\t\tset_sbi_flag(sbi, SBI_NEED_CP);\n\t}\nsubmit_io:\n\tif (is_read_io(bio_op(bio)))\n\t\ttrace_f2fs_submit_read_bio(sbi->sb, type, bio);\n\telse\n\t\ttrace_f2fs_submit_write_bio(sbi->sb, type, bio);\n\tsubmit_bio(bio);\n}\n\nstatic void __submit_merged_bio(struct f2fs_bio_info *io)\n{\n\tstruct f2fs_io_info *fio = &io->fio;\n\n\tif (!io->bio)\n\t\treturn;\n\n\tbio_set_op_attrs(io->bio, fio->op, fio->op_flags);\n\n\tif (is_read_io(fio->op))\n\t\ttrace_f2fs_prepare_read_bio(io->sbi->sb, fio->type, io->bio);\n\telse\n\t\ttrace_f2fs_prepare_write_bio(io->sbi->sb, fio->type, io->bio);\n\n\t__submit_bio(io->sbi, io->bio, fio->type);\n\tio->bio = NULL;\n}\n\nstatic bool __has_merged_page(struct bio *bio, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino)\n{\n\tstruct bio_vec *bvec;\n\tstruct page *target;\n\tstruct bvec_iter_all iter_all;\n\n\tif (!bio)\n\t\treturn false;\n\n\tif (!inode && !page && !ino)\n\t\treturn true;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\n\t\tif (bvec->bv_page->mapping)\n\t\t\ttarget = bvec->bv_page;\n\t\telse\n\t\t\ttarget = fscrypt_control_page(bvec->bv_page);\n\n\t\tif (inode && inode == target->mapping->host)\n\t\t\treturn true;\n\t\tif (page && page == target)\n\t\t\treturn true;\n\t\tif (ino && ino == ino_of_node(target))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,\n\t\t\t\tenum page_type type, enum temp_type temp)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io = sbi->write_io[btype] + temp;\n\n\tdown_write(&io->io_rwsem);\n\n\t/* change META to META_FLUSH in the checkpoint procedure */\n\tif (type >= META_FLUSH) {\n\t\tio->fio.type = META_FLUSH;\n\t\tio->fio.op = REQ_OP_WRITE;\n\t\tio->fio.op_flags = REQ_META | REQ_PRIO | REQ_SYNC;\n\t\tif (!test_opt(sbi, NOBARRIER))\n\t\t\tio->fio.op_flags |= REQ_PREFLUSH | REQ_FUA;\n\t}\n\t__submit_merged_bio(io);\n\tup_write(&io->io_rwsem);\n}\n\nstatic void __submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, bool force)\n{\n\tenum temp_type temp;\n\tbool ret = true;\n\n\tfor (temp = HOT; temp < NR_TEMP_TYPE; temp++) {\n\t\tif (!force)\t{\n\t\t\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\t\t\tstruct f2fs_bio_info *io = sbi->write_io[btype] + temp;\n\n\t\t\tdown_read(&io->io_rwsem);\n\t\t\tret = __has_merged_page(io->bio, inode, page, ino);\n\t\t\tup_read(&io->io_rwsem);\n\t\t}\n\t\tif (ret)\n\t\t\t__f2fs_submit_merged_write(sbi, type, temp);\n\n\t\t/* TODO: use HOT temp only for meta pages now. */\n\t\tif (type >= META)\n\t\t\tbreak;\n\t}\n}\n\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)\n{\n\t__submit_merged_write_cond(sbi, NULL, NULL, 0, type, true);\n}\n\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type)\n{\n\t__submit_merged_write_cond(sbi, inode, page, ino, type, false);\n}\n\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)\n{\n\tf2fs_submit_merged_write(sbi, DATA);\n\tf2fs_submit_merged_write(sbi, NODE);\n\tf2fs_submit_merged_write(sbi, META);\n}\n\n/*\n * Fill the locked page with data located in the block address.\n * A caller needs to unlock the page on failure.\n */\nint f2fs_submit_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,\n\t\t\tfio->is_por ? META_POR : (__is_meta_io(fio) ?\n\t\t\tMETA_GENERIC : DATA_GENERIC_ENHANCE)))\n\t\treturn -EFSCORRUPTED;\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\tf2fs_trace_ios(fio, 0);\n\n\t/* Allocate a new bio */\n\tbio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,\n\t\t\t\t1, is_read_io(fio->op), fio->type, fio->temp);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\n\tif (fio->io_wbc && !is_read_io(fio->op))\n\t\twbc_account_io(fio->io_wbc, page, PAGE_SIZE);\n\n\tbio_set_op_attrs(bio, fio->op, fio->op_flags);\n\n\tinc_page_count(fio->sbi, is_read_io(fio->op) ?\n\t\t\t__read_io_type(page): WB_DATA_TYPE(fio->page));\n\n\t__submit_bio(fio->sbi, bio, fio->type);\n\treturn 0;\n}\n\nint f2fs_merge_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio = *fio->bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,\n\t\t\t__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC))\n\t\treturn -EFSCORRUPTED;\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\tf2fs_trace_ios(fio, 0);\n\n\tif (bio && (*fio->last_block + 1 != fio->new_blkaddr ||\n\t\t\t!__same_bdev(fio->sbi, fio->new_blkaddr, bio))) {\n\t\t__submit_bio(fio->sbi, bio, fio->type);\n\t\tbio = NULL;\n\t}\nalloc_new:\n\tif (!bio) {\n\t\tbio = __bio_alloc(fio->sbi, fio->new_blkaddr, fio->io_wbc,\n\t\t\t\tBIO_MAX_PAGES, false, fio->type, fio->temp);\n\t\tbio_set_op_attrs(bio, fio->op, fio->op_flags);\n\t}\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t__submit_bio(fio->sbi, bio, fio->type);\n\t\tbio = NULL;\n\t\tgoto alloc_new;\n\t}\n\n\tif (fio->io_wbc)\n\t\twbc_account_io(fio->io_wbc, page, PAGE_SIZE);\n\n\tinc_page_count(fio->sbi, WB_DATA_TYPE(page));\n\n\t*fio->last_block = fio->new_blkaddr;\n\t*fio->bio = bio;\n\n\treturn 0;\n}\n\nstatic void f2fs_submit_ipu_bio(struct f2fs_sb_info *sbi, struct bio **bio,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\tif (!bio)\n\t\treturn;\n\n\tif (!__has_merged_page(*bio, NULL, page, 0))\n\t\treturn;\n\n\t__submit_bio(sbi, *bio, DATA);\n\t*bio = NULL;\n}\n\nvoid f2fs_submit_page_write(struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tenum page_type btype = PAGE_TYPE_OF_BIO(fio->type);\n\tstruct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;\n\tstruct page *bio_page;\n\n\tf2fs_bug_on(sbi, is_read_io(fio->op));\n\n\tdown_write(&io->io_rwsem);\nnext:\n\tif (fio->in_list) {\n\t\tspin_lock(&io->io_lock);\n\t\tif (list_empty(&io->io_list)) {\n\t\t\tspin_unlock(&io->io_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tfio = list_first_entry(&io->io_list,\n\t\t\t\t\t\tstruct f2fs_io_info, list);\n\t\tlist_del(&fio->list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tverify_fio_blkaddr(fio);\n\n\tbio_page = fio->encrypted_page ? fio->encrypted_page : fio->page;\n\n\t/* set submitted = true as a return value */\n\tfio->submitted = true;\n\n\tinc_page_count(sbi, WB_DATA_TYPE(bio_page));\n\n\tif (io->bio && (io->last_block_in_bio != fio->new_blkaddr - 1 ||\n\t    (io->fio.op != fio->op || io->fio.op_flags != fio->op_flags) ||\n\t\t\t!__same_bdev(sbi, fio->new_blkaddr, io->bio)))\n\t\t__submit_merged_bio(io);\nalloc_new:\n\tif (io->bio == NULL) {\n\t\tif ((fio->type == DATA || fio->type == NODE) &&\n\t\t\t\tfio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {\n\t\t\tdec_page_count(sbi, WB_DATA_TYPE(bio_page));\n\t\t\tfio->retry = true;\n\t\t\tgoto skip;\n\t\t}\n\t\tio->bio = __bio_alloc(sbi, fio->new_blkaddr, fio->io_wbc,\n\t\t\t\t\t\tBIO_MAX_PAGES, false,\n\t\t\t\t\t\tfio->type, fio->temp);\n\t\tio->fio = *fio;\n\t}\n\n\tif (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t__submit_merged_bio(io);\n\t\tgoto alloc_new;\n\t}\n\n\tif (fio->io_wbc)\n\t\twbc_account_io(fio->io_wbc, bio_page, PAGE_SIZE);\n\n\tio->last_block_in_bio = fio->new_blkaddr;\n\tf2fs_trace_ios(fio, 0);\n\n\ttrace_f2fs_submit_page_write(fio->page, fio);\nskip:\n\tif (fio->in_list)\n\t\tgoto next;\nout:\n\tif (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||\n\t\t\t\tf2fs_is_checkpoint_ready(sbi))\n\t\t__submit_merged_bio(io);\n\tup_write(&io->io_rwsem);\n}\n\nstatic struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\t}\n\n\treturn bio;\n}\n\n/* This can handle encryption stuffs */\nstatic int f2fs_submit_page_read(struct inode *inode, struct page *page,\n\t\t\t\t\t\t\tblock_t blkaddr)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\n\tbio = f2fs_grab_read_bio(inode, blkaddr, 1, 0);\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\t/* wait for GCed page writeback via META_MAPPING */\n\tf2fs_wait_on_block_writeback(inode, blkaddr);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\tClearPageError(page);\n\tinc_page_count(sbi, F2FS_RD_DATA);\n\t__submit_bio(sbi, bio, DATA);\n\treturn 0;\n}\n\nstatic void __set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tstruct f2fs_node *rn = F2FS_NODE(dn->node_page);\n\t__le32 *addr_array;\n\tint base = 0;\n\n\tif (IS_INODE(dn->node_page) && f2fs_has_extra_attr(dn->inode))\n\t\tbase = get_extra_isize(dn->inode);\n\n\t/* Get physical address of data block */\n\taddr_array = blkaddr_in_node(rn);\n\taddr_array[base + dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);\n}\n\n/*\n * Lock ordering for the change of data block address:\n * ->data_page\n *  ->node_page\n *    update block addresses in the node page\n */\nvoid f2fs_set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);\n\t__set_data_blkaddr(dn);\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n}\n\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)\n{\n\tdn->data_blkaddr = blkaddr;\n\tf2fs_set_data_blkaddr(dn);\n\tf2fs_update_extent_cache(dn);\n}\n\n/* dn->ofs_in_node will be returned with up-to-date last block pointer */\nint f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tint err;\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\tif (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))\n\t\treturn err;\n\n\ttrace_f2fs_reserve_new_blocks(dn->inode, dn->nid,\n\t\t\t\t\t\tdn->ofs_in_node, count);\n\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);\n\n\tfor (; count > 0; dn->ofs_in_node++) {\n\t\tblock_t blkaddr = datablock_addr(dn->inode,\n\t\t\t\t\tdn->node_page, dn->ofs_in_node);\n\t\tif (blkaddr == NULL_ADDR) {\n\t\t\tdn->data_blkaddr = NEW_ADDR;\n\t\t\t__set_data_blkaddr(dn);\n\t\t\tcount--;\n\t\t}\n\t}\n\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n\treturn 0;\n}\n\n/* Should keep dn->ofs_in_node unchanged */\nint f2fs_reserve_new_block(struct dnode_of_data *dn)\n{\n\tunsigned int ofs_in_node = dn->ofs_in_node;\n\tint ret;\n\n\tret = f2fs_reserve_new_blocks(dn, 1);\n\tdn->ofs_in_node = ofs_in_node;\n\treturn ret;\n}\n\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tbool need_put = dn->inode_page ? false : true;\n\tint err;\n\n\terr = f2fs_get_dnode_of_data(dn, index, ALLOC_NODE);\n\tif (err)\n\t\treturn err;\n\n\tif (dn->data_blkaddr == NULL_ADDR)\n\t\terr = f2fs_reserve_new_block(dn);\n\tif (err || need_put)\n\t\tf2fs_put_dnode(dn);\n\treturn err;\n}\n\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tstruct extent_info ei  = {0,0,0};\n\tstruct inode *inode = dn->inode;\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn->data_blkaddr = ei.blk + index - ei.fofs;\n\t\treturn 0;\n\t}\n\n\treturn f2fs_reserve_block(dn, index);\n}\n\nstruct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\tint op_flags, bool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct dnode_of_data dn;\n\tstruct page *page;\n\tstruct extent_info ei = {0,0,0};\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, for_write);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), dn.data_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto put_err;\n\t\t}\n\t\tgoto got_it;\n\t}\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto put_err;\n\tf2fs_put_dnode(&dn);\n\n\tif (unlikely(dn.data_blkaddr == NULL_ADDR)) {\n\t\terr = -ENOENT;\n\t\tgoto put_err;\n\t}\n\tif (dn.data_blkaddr != NEW_ADDR &&\n\t\t\t!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),\n\t\t\t\t\t\tdn.data_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tgoto put_err;\n\t}\ngot_it:\n\tif (PageUptodate(page)) {\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\t/*\n\t * A new dentry page is allocated but not able to be written, since its\n\t * new inode page couldn't be allocated due to -ENOSPC.\n\t * In such the case, its blkaddr can be remained as NEW_ADDR.\n\t * see, f2fs_add_link -> f2fs_get_new_data_page ->\n\t * f2fs_init_inode_metadata.\n\t */\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\terr = f2fs_submit_page_read(inode, page, dn.data_blkaddr);\n\tif (err)\n\t\tgoto put_err;\n\treturn page;\n\nput_err:\n\tf2fs_put_page(page, 1);\n\treturn ERR_PTR(err);\n}\n\nstruct page *f2fs_find_data_page(struct inode *inode, pgoff_t index)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tpage = find_get_page(mapping, index);\n\tif (page && PageUptodate(page))\n\t\treturn page;\n\tf2fs_put_page(page, 0);\n\n\tpage = f2fs_get_read_data_page(inode, index, 0, false);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\tif (PageUptodate(page))\n\t\treturn page;\n\n\twait_on_page_locked(page);\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 0);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * If it tries to access a hole, return an error.\n * Because, the callers, functions in dir.c and GC, should be able to know\n * whether this page exists or not.\n */\nstruct page *f2fs_get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\t\tbool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\nrepeat:\n\tpage = f2fs_get_read_data_page(inode, index, 0, for_write);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\t/* wait for read completion */\n\tlock_page(page);\n\tif (unlikely(page->mapping != mapping)) {\n\t\tf2fs_put_page(page, 1);\n\t\tgoto repeat;\n\t}\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n/*\n * Caller ensures that this data page is never allocated.\n * A new zero-filled data page is allocated in the page cache.\n *\n * Also, caller should grab and release a rwsem by calling f2fs_lock_op() and\n * f2fs_unlock_op().\n * Note that, ipage is set only by make_empty_dir, and if any error occur,\n * ipage should be released by this function.\n */\nstruct page *f2fs_get_new_data_page(struct inode *inode,\n\t\tstruct page *ipage, pgoff_t index, bool new_i_size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tstruct dnode_of_data dn;\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, true);\n\tif (!page) {\n\t\t/*\n\t\t * before exiting, we should make sure ipage will be released\n\t\t * if any error occur.\n\t\t */\n\t\tf2fs_put_page(ipage, 1);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, NULL, 0);\n\terr = f2fs_reserve_block(&dn, index);\n\tif (err) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(err);\n\t}\n\tif (!ipage)\n\t\tf2fs_put_dnode(&dn);\n\n\tif (PageUptodate(page))\n\t\tgoto got_it;\n\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t} else {\n\t\tf2fs_put_page(page, 1);\n\n\t\t/* if ipage exists, blkaddr should be NEW_ADDR */\n\t\tf2fs_bug_on(F2FS_I_SB(inode), ipage);\n\t\tpage = f2fs_get_lock_data_page(inode, index, true);\n\t\tif (IS_ERR(page))\n\t\t\treturn page;\n\t}\ngot_it:\n\tif (new_i_size && i_size_read(inode) <\n\t\t\t\t((loff_t)(index + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));\n\treturn page;\n}\n\nstatic int __allocate_data_block(struct dnode_of_data *dn, int seg_type)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\tblock_t old_blkaddr;\n\tblkcnt_t count = 1;\n\tint err;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\n\terr = f2fs_get_node_info(sbi, dn->nid, &ni);\n\tif (err)\n\t\treturn err;\n\n\tdn->data_blkaddr = datablock_addr(dn->inode,\n\t\t\t\tdn->node_page, dn->ofs_in_node);\n\tif (dn->data_blkaddr != NULL_ADDR)\n\t\tgoto alloc;\n\n\tif (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))\n\t\treturn err;\n\nalloc:\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\told_blkaddr = dn->data_blkaddr;\n\tf2fs_allocate_data_block(sbi, NULL, old_blkaddr, &dn->data_blkaddr,\n\t\t\t\t\t&sum, seg_type, NULL, false);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tinvalidate_mapping_pages(META_MAPPING(sbi),\n\t\t\t\t\told_blkaddr, old_blkaddr);\n\tf2fs_set_data_blkaddr(dn);\n\n\t/*\n\t * i_size will be updated by direct_IO. Otherwise, we'll get stale\n\t * data from unwritten block via dio_read.\n\t */\n\treturn 0;\n}\n\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct f2fs_map_blocks map;\n\tint flag;\n\tint err = 0;\n\tbool direct_io = iocb->ki_flags & IOCB_DIRECT;\n\n\t/* convert inline data for Direct I/O*/\n\tif (direct_io) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (direct_io && allow_outplace_dio(inode, iocb, from))\n\t\treturn 0;\n\n\tif (is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\tmap.m_lblk = F2FS_BLK_ALIGN(iocb->ki_pos);\n\tmap.m_len = F2FS_BYTES_TO_BLK(iocb->ki_pos + iov_iter_count(from));\n\tif (map.m_len > map.m_lblk)\n\t\tmap.m_len -= map.m_lblk;\n\telse\n\t\tmap.m_len = 0;\n\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = true;\n\n\tif (direct_io) {\n\t\tmap.m_seg_type = f2fs_rw_hint_to_seg_type(iocb->ki_hint);\n\t\tflag = f2fs_force_buffered_io(inode, iocb, from) ?\n\t\t\t\t\tF2FS_GET_BLOCK_PRE_AIO :\n\t\t\t\t\tF2FS_GET_BLOCK_PRE_DIO;\n\t\tgoto map_blocks;\n\t}\n\tif (iocb->ki_pos + iov_iter_count(from) > MAX_INLINE_DATA(inode)) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (f2fs_has_inline_data(inode))\n\t\treturn err;\n\n\tflag = F2FS_GET_BLOCK_PRE_AIO;\n\nmap_blocks:\n\terr = f2fs_map_blocks(inode, &map, 1, flag);\n\tif (map.m_len > 0 && err == -ENOSPC) {\n\t\tif (!direct_io)\n\t\t\tset_inode_flag(inode, FI_NO_PREALLOC);\n\t\terr = 0;\n\t}\n\treturn err;\n}\n\nvoid __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock)\n{\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO) {\n\t\tif (lock)\n\t\t\tdown_read(&sbi->node_change);\n\t\telse\n\t\t\tup_read(&sbi->node_change);\n\t} else {\n\t\tif (lock)\n\t\t\tf2fs_lock_op(sbi);\n\t\telse\n\t\t\tf2fs_unlock_op(sbi);\n\t}\n}\n\n/*\n * f2fs_map_blocks() now supported readahead/bmap/rw direct_IO with\n * f2fs_map_blocks structure.\n * If original data blocks are allocated, then give them to blockdev.\n * Otherwise,\n *     a. preallocate requested block addresses\n *     b. do not use extent cache for better performance\n *     c. give the block addresses to blockdev\n */\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\t\t\t\tint create, int flag)\n{\n\tunsigned int maxblocks = map->m_len;\n\tstruct dnode_of_data dn;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint mode = map->m_may_create ? ALLOC_NODE : LOOKUP_NODE;\n\tpgoff_t pgofs, end_offset, end;\n\tint err = 0, ofs = 1;\n\tunsigned int ofs_in_node, last_ofs_in_node;\n\tblkcnt_t prealloc;\n\tstruct extent_info ei = {0,0,0};\n\tblock_t blkaddr;\n\tunsigned int start_pgofs;\n\n\tif (!maxblocks)\n\t\treturn 0;\n\n\tmap->m_len = 0;\n\tmap->m_flags = 0;\n\n\t/* it only supports block size == page size */\n\tpgofs =\t(pgoff_t)map->m_lblk;\n\tend = pgofs + maxblocks;\n\n\tif (!create && f2fs_lookup_extent_cache(inode, pgofs, &ei)) {\n\t\tif (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&\n\t\t\t\t\t\t\tmap->m_may_create)\n\t\t\tgoto next_dnode;\n\n\t\tmap->m_pblk = ei.blk + pgofs - ei.fofs;\n\t\tmap->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgofs);\n\t\tmap->m_flags = F2FS_MAP_MAPPED;\n\t\tif (map->m_next_extent)\n\t\t\t*map->m_next_extent = pgofs + map->m_len;\n\n\t\t/* for hardware encryption, but to avoid potential issue in future */\n\t\tif (flag == F2FS_GET_BLOCK_DIO)\n\t\t\tf2fs_wait_on_block_writeback_range(inode,\n\t\t\t\t\t\tmap->m_pblk, map->m_len);\n\t\tgoto out;\n\t}\n\nnext_dnode:\n\tif (map->m_may_create)\n\t\t__do_map_lock(sbi, flag, true);\n\n\t/* When reading holes, we need its node page */\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, pgofs, mode);\n\tif (err) {\n\t\tif (flag == F2FS_GET_BLOCK_BMAP)\n\t\t\tmap->m_pblk = 0;\n\t\tif (err == -ENOENT) {\n\t\t\terr = 0;\n\t\t\tif (map->m_next_pgofs)\n\t\t\t\t*map->m_next_pgofs =\n\t\t\t\t\tf2fs_get_next_page_offset(&dn, pgofs);\n\t\t\tif (map->m_next_extent)\n\t\t\t\t*map->m_next_extent =\n\t\t\t\t\tf2fs_get_next_page_offset(&dn, pgofs);\n\t\t}\n\t\tgoto unlock_out;\n\t}\n\n\tstart_pgofs = pgofs;\n\tprealloc = 0;\n\tlast_ofs_in_node = ofs_in_node = dn.ofs_in_node;\n\tend_offset = ADDRS_PER_PAGE(dn.node_page, inode);\n\nnext_block:\n\tblkaddr = datablock_addr(dn.inode, dn.node_page, dn.ofs_in_node);\n\n\tif (__is_valid_data_blkaddr(blkaddr) &&\n\t\t!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tgoto sync_out;\n\t}\n\n\tif (__is_valid_data_blkaddr(blkaddr)) {\n\t\t/* use out-place-update for driect IO under LFS mode */\n\t\tif (test_opt(sbi, LFS) && flag == F2FS_GET_BLOCK_DIO &&\n\t\t\t\t\t\t\tmap->m_may_create) {\n\t\t\terr = __allocate_data_block(&dn, map->m_seg_type);\n\t\t\tif (!err) {\n\t\t\t\tblkaddr = dn.data_blkaddr;\n\t\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (create) {\n\t\t\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_PRE_AIO) {\n\t\t\t\tif (blkaddr == NULL_ADDR) {\n\t\t\t\t\tprealloc++;\n\t\t\t\t\tlast_ofs_in_node = dn.ofs_in_node;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tWARN_ON(flag != F2FS_GET_BLOCK_PRE_DIO &&\n\t\t\t\t\tflag != F2FS_GET_BLOCK_DIO);\n\t\t\t\terr = __allocate_data_block(&dn,\n\t\t\t\t\t\t\tmap->m_seg_type);\n\t\t\t\tif (!err)\n\t\t\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tgoto sync_out;\n\t\t\tmap->m_flags |= F2FS_MAP_NEW;\n\t\t\tblkaddr = dn.data_blkaddr;\n\t\t} else {\n\t\t\tif (flag == F2FS_GET_BLOCK_BMAP) {\n\t\t\t\tmap->m_pblk = 0;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag == F2FS_GET_BLOCK_PRECACHE)\n\t\t\t\tgoto sync_out;\n\t\t\tif (flag == F2FS_GET_BLOCK_FIEMAP &&\n\t\t\t\t\t\tblkaddr == NULL_ADDR) {\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tif (flag != F2FS_GET_BLOCK_FIEMAP) {\n\t\t\t\t/* for defragment case */\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tgoto skip;\n\n\tif (map->m_len == 0) {\n\t\t/* preallocated unwritten block should be mapped for fiemap. */\n\t\tif (blkaddr == NEW_ADDR)\n\t\t\tmap->m_flags |= F2FS_MAP_UNWRITTEN;\n\t\tmap->m_flags |= F2FS_MAP_MAPPED;\n\n\t\tmap->m_pblk = blkaddr;\n\t\tmap->m_len = 1;\n\t} else if ((map->m_pblk != NEW_ADDR &&\n\t\t\tblkaddr == (map->m_pblk + ofs)) ||\n\t\t\t(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||\n\t\t\tflag == F2FS_GET_BLOCK_PRE_DIO) {\n\t\tofs++;\n\t\tmap->m_len++;\n\t} else {\n\t\tgoto sync_out;\n\t}\n\nskip:\n\tdn.ofs_in_node++;\n\tpgofs++;\n\n\t/* preallocate blocks in batch for one dnode page */\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO &&\n\t\t\t(pgofs == end || dn.ofs_in_node == end_offset)) {\n\n\t\tdn.ofs_in_node = ofs_in_node;\n\t\terr = f2fs_reserve_new_blocks(&dn, prealloc);\n\t\tif (err)\n\t\t\tgoto sync_out;\n\n\t\tmap->m_len += dn.ofs_in_node - ofs_in_node;\n\t\tif (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto sync_out;\n\t\t}\n\t\tdn.ofs_in_node = end_offset;\n\t}\n\n\tif (pgofs >= end)\n\t\tgoto sync_out;\n\telse if (dn.ofs_in_node < end_offset)\n\t\tgoto next_block;\n\n\tif (flag == F2FS_GET_BLOCK_PRECACHE) {\n\t\tif (map->m_flags & F2FS_MAP_MAPPED) {\n\t\t\tunsigned int ofs = start_pgofs - map->m_lblk;\n\n\t\t\tf2fs_update_extent_cache_range(&dn,\n\t\t\t\tstart_pgofs, map->m_pblk + ofs,\n\t\t\t\tmap->m_len - ofs);\n\t\t}\n\t}\n\n\tf2fs_put_dnode(&dn);\n\n\tif (map->m_may_create) {\n\t\t__do_map_lock(sbi, flag, false);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\n\tgoto next_dnode;\n\nsync_out:\n\n\t/* for hardware encryption, but to avoid potential issue in future */\n\tif (flag == F2FS_GET_BLOCK_DIO && map->m_flags & F2FS_MAP_MAPPED)\n\t\tf2fs_wait_on_block_writeback_range(inode,\n\t\t\t\t\t\tmap->m_pblk, map->m_len);\n\n\tif (flag == F2FS_GET_BLOCK_PRECACHE) {\n\t\tif (map->m_flags & F2FS_MAP_MAPPED) {\n\t\t\tunsigned int ofs = start_pgofs - map->m_lblk;\n\n\t\t\tf2fs_update_extent_cache_range(&dn,\n\t\t\t\tstart_pgofs, map->m_pblk + ofs,\n\t\t\t\tmap->m_len - ofs);\n\t\t}\n\t\tif (map->m_next_extent)\n\t\t\t*map->m_next_extent = pgofs + 1;\n\t}\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (map->m_may_create) {\n\t\t__do_map_lock(sbi, flag, false);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\nout:\n\ttrace_f2fs_map_blocks(inode, map, err);\n\treturn err;\n}\n\nbool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len)\n{\n\tstruct f2fs_map_blocks map;\n\tblock_t last_lblk;\n\tint err;\n\n\tif (pos + len > i_size_read(inode))\n\t\treturn false;\n\n\tmap.m_lblk = F2FS_BYTES_TO_BLK(pos);\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\tlast_lblk = F2FS_BLK_ALIGN(pos + len);\n\n\twhile (map.m_lblk < last_lblk) {\n\t\tmap.m_len = last_lblk - map.m_lblk;\n\t\terr = f2fs_map_blocks(inode, &map, 0, F2FS_GET_BLOCK_DEFAULT);\n\t\tif (err || map.m_len == 0)\n\t\t\treturn false;\n\t\tmap.m_lblk += map.m_len;\n\t}\n\treturn true;\n}\n\nstatic int __get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh, int create, int flag,\n\t\t\tpgoff_t *next_pgofs, int seg_type, bool may_write)\n{\n\tstruct f2fs_map_blocks map;\n\tint err;\n\n\tmap.m_lblk = iblock;\n\tmap.m_len = bh->b_size >> inode->i_blkbits;\n\tmap.m_next_pgofs = next_pgofs;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = seg_type;\n\tmap.m_may_create = may_write;\n\n\terr = f2fs_map_blocks(inode, &map, create, flag);\n\tif (!err) {\n\t\tmap_bh(bh, inode->i_sb, map.m_pblk);\n\t\tbh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;\n\t\tbh->b_size = (u64)map.m_len << inode->i_blkbits;\n\t}\n\treturn err;\n}\n\nstatic int get_data_block(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create, int flag,\n\t\t\tpgoff_t *next_pgofs)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\t\tflag, next_pgofs,\n\t\t\t\t\t\t\tNO_CHECK_TYPE, create);\n}\n\nstatic int get_data_block_dio_write(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\tF2FS_GET_BLOCK_DIO, NULL,\n\t\t\t\tf2fs_rw_hint_to_seg_type(inode->i_write_hint),\n\t\t\t\ttrue);\n}\n\nstatic int get_data_block_dio(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\tF2FS_GET_BLOCK_DIO, NULL,\n\t\t\t\tf2fs_rw_hint_to_seg_type(inode->i_write_hint),\n\t\t\t\tfalse);\n}\n\nstatic int get_data_block_bmap(struct inode *inode, sector_t iblock,\n\t\t\tstruct buffer_head *bh_result, int create)\n{\n\t/* Block number less than F2FS MAX BLOCKS */\n\tif (unlikely(iblock >= F2FS_I_SB(inode)->max_file_blocks))\n\t\treturn -EFBIG;\n\n\treturn __get_data_block(inode, iblock, bh_result, create,\n\t\t\t\t\t\tF2FS_GET_BLOCK_BMAP, NULL,\n\t\t\t\t\t\tNO_CHECK_TYPE, create);\n}\n\nstatic inline sector_t logical_to_blk(struct inode *inode, loff_t offset)\n{\n\treturn (offset >> inode->i_blkbits);\n}\n\nstatic inline loff_t blk_to_logical(struct inode *inode, sector_t blk)\n{\n\treturn (blk << inode->i_blkbits);\n}\n\nstatic int f2fs_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page;\n\tstruct node_info ni;\n\t__u64 phys = 0, len;\n\t__u32 flags;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tint err = 0;\n\n\tif (f2fs_has_inline_xattr(inode)) {\n\t\tint offset;\n\n\t\tpage = f2fs_grab_cache_page(NODE_MAPPING(sbi),\n\t\t\t\t\t\tinode->i_ino, false);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\terr = f2fs_get_node_info(sbi, inode->i_ino, &ni);\n\t\tif (err) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn err;\n\t\t}\n\n\t\tphys = (__u64)blk_to_logical(inode, ni.blk_addr);\n\t\toffset = offsetof(struct f2fs_inode, i_addr) +\n\t\t\t\t\tsizeof(__le32) * (DEF_ADDRS_PER_INODE -\n\t\t\t\t\tget_inline_xattr_addrs(inode));\n\n\t\tphys += offset;\n\t\tlen = inline_xattr_size(inode);\n\n\t\tf2fs_put_page(page, 1);\n\n\t\tflags = FIEMAP_EXTENT_DATA_INLINE | FIEMAP_EXTENT_NOT_ALIGNED;\n\n\t\tif (!xnid)\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\t\terr = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);\n\t\tif (err || err == 1)\n\t\t\treturn err;\n\t}\n\n\tif (xnid) {\n\t\tpage = f2fs_grab_cache_page(NODE_MAPPING(sbi), xnid, false);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\terr = f2fs_get_node_info(sbi, xnid, &ni);\n\t\tif (err) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn err;\n\t\t}\n\n\t\tphys = (__u64)blk_to_logical(inode, ni.blk_addr);\n\t\tlen = inode->i_sb->s_blocksize;\n\n\t\tf2fs_put_page(page, 1);\n\n\t\tflags = FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (phys)\n\t\terr = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);\n\n\treturn (err < 0 ? err : 0);\n}\n\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\tu64 start, u64 len)\n{\n\tstruct buffer_head map_bh;\n\tsector_t start_blk, last_blk;\n\tpgoff_t next_pgofs;\n\tu64 logical = 0, phys = 0, size = 0;\n\tu32 flags = 0;\n\tint ret = 0;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\tret = f2fs_precache_extents(inode);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC | FIEMAP_FLAG_XATTR);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\tret = f2fs_xattr_fiemap(inode, fieinfo);\n\t\tgoto out;\n\t}\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);\n\t\tif (ret != -EAGAIN)\n\t\t\tgoto out;\n\t}\n\n\tif (logical_to_blk(inode, len) == 0)\n\t\tlen = blk_to_logical(inode, 1);\n\n\tstart_blk = logical_to_blk(inode, start);\n\tlast_blk = logical_to_blk(inode, start + len - 1);\n\nnext:\n\tmemset(&map_bh, 0, sizeof(struct buffer_head));\n\tmap_bh.b_size = len;\n\n\tret = get_data_block(inode, start_blk, &map_bh, 0,\n\t\t\t\t\tF2FS_GET_BLOCK_FIEMAP, &next_pgofs);\n\tif (ret)\n\t\tgoto out;\n\n\t/* HOLE */\n\tif (!buffer_mapped(&map_bh)) {\n\t\tstart_blk = next_pgofs;\n\n\t\tif (blk_to_logical(inode, start_blk) < blk_to_logical(inode,\n\t\t\t\t\tF2FS_I_SB(inode)->max_file_blocks))\n\t\t\tgoto prep_next;\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (size) {\n\t\tif (IS_ENCRYPTED(inode))\n\t\t\tflags |= FIEMAP_EXTENT_DATA_ENCRYPTED;\n\n\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\tphys, size, flags);\n\t}\n\n\tif (start_blk > last_blk || ret)\n\t\tgoto out;\n\n\tlogical = blk_to_logical(inode, start_blk);\n\tphys = blk_to_logical(inode, map_bh.b_blocknr);\n\tsize = map_bh.b_size;\n\tflags = 0;\n\tif (buffer_unwritten(&map_bh))\n\t\tflags = FIEMAP_EXTENT_UNWRITTEN;\n\n\tstart_blk += logical_to_blk(inode, size);\n\nprep_next:\n\tcond_resched();\n\tif (fatal_signal_pending(current))\n\t\tret = -EINTR;\n\telse\n\t\tgoto next;\nout:\n\tif (ret == 1)\n\t\tret = 0;\n\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nstatic int f2fs_read_single_page(struct inode *inode, struct page *page,\n\t\t\t\t\tunsigned nr_pages,\n\t\t\t\t\tstruct f2fs_map_blocks *map,\n\t\t\t\t\tstruct bio **bio_ret,\n\t\t\t\t\tsector_t *last_block_in_bio,\n\t\t\t\t\tbool is_readahead)\n{\n\tstruct bio *bio = *bio_ret;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tint ret = 0;\n\n\tblock_in_file = (sector_t)page_index(page);\n\tlast_block = block_in_file + nr_pages;\n\tlast_block_in_file = (i_size_read(inode) + blocksize - 1) >>\n\t\t\t\t\t\t\tblkbits;\n\tif (last_block > last_block_in_file)\n\t\tlast_block = last_block_in_file;\n\n\t/* just zeroing out page which is beyond EOF */\n\tif (block_in_file >= last_block)\n\t\tgoto zero_out;\n\t/*\n\t * Map blocks using the previous result first.\n\t */\n\tif ((map->m_flags & F2FS_MAP_MAPPED) &&\n\t\t\tblock_in_file > map->m_lblk &&\n\t\t\tblock_in_file < (map->m_lblk + map->m_len))\n\t\tgoto got_it;\n\n\t/*\n\t * Then do more f2fs_map_blocks() calls until we are\n\t * done with this page.\n\t */\n\tmap->m_lblk = block_in_file;\n\tmap->m_len = last_block - block_in_file;\n\n\tret = f2fs_map_blocks(inode, map, 0, F2FS_GET_BLOCK_DEFAULT);\n\tif (ret)\n\t\tgoto out;\ngot_it:\n\tif ((map->m_flags & F2FS_MAP_MAPPED)) {\n\t\tblock_nr = map->m_pblk + block_in_file - map->m_lblk;\n\t\tSetPageMappedToDisk(page);\n\n\t\tif (!PageUptodate(page) && (!PageSwapCache(page) &&\n\t\t\t\t\t!cleancache_get_page(page))) {\n\t\t\tSetPageUptodate(page);\n\t\t\tgoto confused;\n\t\t}\n\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t} else {\nzero_out:\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This page will go to BIO.  Do we need to send this\n\t * BIO off first?\n\t */\n\tif (bio && (*last_block_in_bio != block_nr - 1 ||\n\t\t!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {\nsubmit_and_realloc:\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tif (bio == NULL) {\n\t\tbio = f2fs_grab_read_bio(inode, block_nr, nr_pages,\n\t\t\t\tis_readahead ? REQ_RAHEAD : 0);\n\t\tif (IS_ERR(bio)) {\n\t\t\tret = PTR_ERR(bio);\n\t\t\tbio = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If the page is under writeback, we need to wait for\n\t * its completion to see the correct decrypted data.\n\t */\n\tf2fs_wait_on_block_writeback(inode, block_nr);\n\n\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\tgoto submit_and_realloc;\n\n\tinc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);\n\tClearPageError(page);\n\t*last_block_in_bio = block_nr;\n\tgoto out;\nconfused:\n\tif (bio) {\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tunlock_page(page);\nout:\n\t*bio_ret = bio;\n\treturn ret;\n}\n\n/*\n * This function was originally taken from fs/mpage.c, and customized for f2fs.\n * Major change was from block_size == page_size in f2fs by default.\n *\n * Note that the aops->readpages() function is ONLY used for read-ahead. If\n * this function ever deviates from doing just read-ahead, it should either\n * use ->readpage() or do the necessary surgery to decouple ->readpages()\n * from read-ahead.\n */\nstatic int f2fs_mpage_readpages(struct address_space *mapping,\n\t\t\tstruct list_head *pages, struct page *page,\n\t\t\tunsigned nr_pages, bool is_readahead)\n{\n\tstruct bio *bio = NULL;\n\tsector_t last_block_in_bio = 0;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_map_blocks map;\n\tint ret = 0;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\n\tfor (; nr_pages; nr_pages--) {\n\t\tif (pages) {\n\t\t\tpage = list_last_entry(pages, struct page, lru);\n\n\t\t\tprefetchw(&page->flags);\n\t\t\tlist_del(&page->lru);\n\t\t\tif (add_to_page_cache_lru(page, mapping,\n\t\t\t\t\t\t  page_index(page),\n\t\t\t\t\t\t  readahead_gfp_mask(mapping)))\n\t\t\t\tgoto next_page;\n\t\t}\n\n\t\tret = f2fs_read_single_page(inode, page, nr_pages, &map, &bio,\n\t\t\t\t\t&last_block_in_bio, is_readahead);\n\t\tif (ret) {\n\t\t\tSetPageError(page);\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tunlock_page(page);\n\t\t}\nnext_page:\n\t\tif (pages)\n\t\t\tput_page(page);\n\t}\n\tBUG_ON(pages && !list_empty(pages));\n\tif (bio)\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn pages ? 0 : ret;\n}\n\nstatic int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page_file_mapping(page),\n\t\t\t\t\t\tNULL, page, 1, false);\n\treturn ret;\n}\n\nstatic int f2fs_read_data_pages(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tstruct list_head *pages, unsigned nr_pages)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct page *page = list_last_entry(pages, struct page, lru);\n\n\ttrace_f2fs_readpages(inode, page, nr_pages);\n\n\t/* If the file has inline data, skip readpages */\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\treturn f2fs_mpage_readpages(mapping, pages, NULL, nr_pages, true);\n}\n\nstatic int encrypt_one_page(struct f2fs_io_info *fio)\n{\n\tstruct inode *inode = fio->page->mapping->host;\n\tstruct page *mpage;\n\tgfp_t gfp_flags = GFP_NOFS;\n\n\tif (!f2fs_encrypted_file(inode))\n\t\treturn 0;\n\n\t/* wait for GCed page writeback via META_MAPPING */\n\tf2fs_wait_on_block_writeback(inode, fio->old_blkaddr);\n\nretry_encrypt:\n\tfio->encrypted_page = fscrypt_encrypt_page(inode, fio->page,\n\t\t\tPAGE_SIZE, 0, fio->page->index, gfp_flags);\n\tif (IS_ERR(fio->encrypted_page)) {\n\t\t/* flush pending IOs and wait for a while in the ENOMEM case */\n\t\tif (PTR_ERR(fio->encrypted_page) == -ENOMEM) {\n\t\t\tf2fs_flush_merged_writes(fio->sbi);\n\t\t\tcongestion_wait(BLK_RW_ASYNC, HZ/50);\n\t\t\tgfp_flags |= __GFP_NOFAIL;\n\t\t\tgoto retry_encrypt;\n\t\t}\n\t\treturn PTR_ERR(fio->encrypted_page);\n\t}\n\n\tmpage = find_lock_page(META_MAPPING(fio->sbi), fio->old_blkaddr);\n\tif (mpage) {\n\t\tif (PageUptodate(mpage))\n\t\t\tmemcpy(page_address(mpage),\n\t\t\t\tpage_address(fio->encrypted_page), PAGE_SIZE);\n\t\tf2fs_put_page(mpage, 1);\n\t}\n\treturn 0;\n}\n\nstatic inline bool check_inplace_update_policy(struct inode *inode,\n\t\t\t\tstruct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tunsigned int policy = SM_I(sbi)->ipu_policy;\n\n\tif (policy & (0x1 << F2FS_IPU_FORCE))\n\t\treturn true;\n\tif (policy & (0x1 << F2FS_IPU_SSR) && f2fs_need_SSR(sbi))\n\t\treturn true;\n\tif (policy & (0x1 << F2FS_IPU_UTIL) &&\n\t\t\tutilization(sbi) > SM_I(sbi)->min_ipu_util)\n\t\treturn true;\n\tif (policy & (0x1 << F2FS_IPU_SSR_UTIL) && f2fs_need_SSR(sbi) &&\n\t\t\tutilization(sbi) > SM_I(sbi)->min_ipu_util)\n\t\treturn true;\n\n\t/*\n\t * IPU for rewrite async pages\n\t */\n\tif (policy & (0x1 << F2FS_IPU_ASYNC) &&\n\t\t\tfio && fio->op == REQ_OP_WRITE &&\n\t\t\t!(fio->op_flags & REQ_SYNC) &&\n\t\t\t!IS_ENCRYPTED(inode))\n\t\treturn true;\n\n\t/* this is only set during fdatasync */\n\tif (policy & (0x1 << F2FS_IPU_FSYNC) &&\n\t\t\tis_inode_flag_set(inode, FI_NEED_IPU))\n\t\treturn true;\n\n\tif (unlikely(fio && is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&\n\t\t\t!f2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))\n\t\treturn true;\n\n\treturn false;\n}\n\nbool f2fs_should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)\n{\n\tif (f2fs_is_pinned_file(inode))\n\t\treturn true;\n\n\t/* if this is cold file, we should overwrite to avoid fragmentation */\n\tif (file_is_cold(inode))\n\t\treturn true;\n\n\treturn check_inplace_update_policy(inode, fio);\n}\n\nbool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (test_opt(sbi, LFS))\n\t\treturn true;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn true;\n\tif (IS_NOQUOTA(inode))\n\t\treturn true;\n\tif (f2fs_is_atomic_file(inode))\n\t\treturn true;\n\tif (fio) {\n\t\tif (is_cold_data(fio->page))\n\t\t\treturn true;\n\t\tif (IS_ATOMIC_WRITTEN_PAGE(fio->page))\n\t\t\treturn true;\n\t\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&\n\t\t\tf2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool need_inplace_update(struct f2fs_io_info *fio)\n{\n\tstruct inode *inode = fio->page->mapping->host;\n\n\tif (f2fs_should_update_outplace(inode, fio))\n\t\treturn false;\n\n\treturn f2fs_should_update_inplace(inode, fio);\n}\n\nint f2fs_do_write_data_page(struct f2fs_io_info *fio)\n{\n\tstruct page *page = fio->page;\n\tstruct inode *inode = page->mapping->host;\n\tstruct dnode_of_data dn;\n\tstruct extent_info ei = {0,0,0};\n\tstruct node_info ni;\n\tbool ipu_force = false;\n\tint err = 0;\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\tif (need_inplace_update(fio) &&\n\t\t\tf2fs_lookup_extent_cache(inode, page->index, &ei)) {\n\t\tfio->old_blkaddr = ei.blk + page->index - ei.fofs;\n\n\t\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE))\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tipu_force = true;\n\t\tfio->need_lock = LOCK_DONE;\n\t\tgoto got_it;\n\t}\n\n\t/* Deadlock due to between page->lock and f2fs_lock_op */\n\tif (fio->need_lock == LOCK_REQ && !f2fs_trylock_op(fio->sbi))\n\t\treturn -EAGAIN;\n\n\terr = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto out;\n\n\tfio->old_blkaddr = dn.data_blkaddr;\n\n\t/* This page is already truncated */\n\tif (fio->old_blkaddr == NULL_ADDR) {\n\t\tClearPageUptodate(page);\n\t\tclear_cold_data(page);\n\t\tgoto out_writepage;\n\t}\ngot_it:\n\tif (__is_valid_data_blkaddr(fio->old_blkaddr) &&\n\t\t!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tgoto out_writepage;\n\t}\n\t/*\n\t * If current allocation needs SSR,\n\t * it had better in-place writes for updated data.\n\t */\n\tif (ipu_force ||\n\t\t(__is_valid_data_blkaddr(fio->old_blkaddr) &&\n\t\t\t\t\tneed_inplace_update(fio))) {\n\t\terr = encrypt_one_page(fio);\n\t\tif (err)\n\t\t\tgoto out_writepage;\n\n\t\tset_page_writeback(page);\n\t\tClearPageError(page);\n\t\tf2fs_put_dnode(&dn);\n\t\tif (fio->need_lock == LOCK_REQ)\n\t\t\tf2fs_unlock_op(fio->sbi);\n\t\terr = f2fs_inplace_write_data(fio);\n\t\tif (err) {\n\t\t\tif (f2fs_encrypted_file(inode))\n\t\t\t\tfscrypt_pullback_bio_page(&fio->encrypted_page,\n\t\t\t\t\t\t\t\t\ttrue);\n\t\t\tif (PageWriteback(page))\n\t\t\t\tend_page_writeback(page);\n\t\t} else {\n\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\t}\n\t\ttrace_f2fs_do_write_data_page(fio->page, IPU);\n\t\treturn err;\n\t}\n\n\tif (fio->need_lock == LOCK_RETRY) {\n\t\tif (!f2fs_trylock_op(fio->sbi)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_writepage;\n\t\t}\n\t\tfio->need_lock = LOCK_REQ;\n\t}\n\n\terr = f2fs_get_node_info(fio->sbi, dn.nid, &ni);\n\tif (err)\n\t\tgoto out_writepage;\n\n\tfio->version = ni.version;\n\n\terr = encrypt_one_page(fio);\n\tif (err)\n\t\tgoto out_writepage;\n\n\tset_page_writeback(page);\n\tClearPageError(page);\n\n\t/* LFS mode write path */\n\tf2fs_outplace_write_data(&dn, fio);\n\ttrace_f2fs_do_write_data_page(page, OPU);\n\tset_inode_flag(inode, FI_APPEND_WRITE);\n\tif (page->index == 0)\n\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\nout_writepage:\n\tf2fs_put_dnode(&dn);\nout:\n\tif (fio->need_lock == LOCK_REQ)\n\t\tf2fs_unlock_op(fio->sbi);\n\treturn err;\n}\n\nstatic int __write_data_page(struct page *page, bool *submitted,\n\t\t\t\tstruct bio **bio,\n\t\t\t\tsector_t *last_block,\n\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\tenum iostat_type io_type)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = ((unsigned long long) i_size)\n\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\tloff_t psize = (page->index + 1) << PAGE_SHIFT;\n\tunsigned offset = 0;\n\tbool need_balance_fs = false;\n\tint err = 0;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.ino = inode->i_ino,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = wbc_to_write_flags(wbc),\n\t\t.old_blkaddr = NULL_ADDR,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.submitted = false,\n\t\t.need_lock = LOCK_RETRY,\n\t\t.io_type = io_type,\n\t\t.io_wbc = wbc,\n\t\t.bio = bio,\n\t\t.last_block = last_block,\n\t};\n\n\ttrace_f2fs_writepage(page, DATA);\n\n\t/* we should bypass data pages to proceed the kworkder jobs */\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tmapping_set_error(page->mapping, -EIO);\n\t\t/*\n\t\t * don't drop any dirty dentry pages for keeping lastest\n\t\t * directory structure.\n\t\t */\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\tgoto redirty_out;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto redirty_out;\n\n\tif (page->index < end_index)\n\t\tgoto write;\n\n\t/*\n\t * If the offset is out-of-range of file size,\n\t * this page does not have to be written to disk.\n\t */\n\toffset = i_size & (PAGE_SIZE - 1);\n\tif ((page->index >= end_index + 1) || !offset)\n\t\tgoto out;\n\n\tzero_user_segment(page, offset, PAGE_SIZE);\nwrite:\n\tif (f2fs_is_drop_cache(inode))\n\t\tgoto out;\n\t/* we should not write 0'th page having journal header */\n\tif (f2fs_is_volatile_file(inode) && (!page->index ||\n\t\t\t(!wbc->for_reclaim &&\n\t\t\tf2fs_available_free_memory(sbi, BASE_CHECK))))\n\t\tgoto redirty_out;\n\n\t/* Dentry blocks are controlled by checkpoint */\n\tif (S_ISDIR(inode->i_mode)) {\n\t\tfio.need_lock = LOCK_DONE;\n\t\terr = f2fs_do_write_data_page(&fio);\n\t\tgoto done;\n\t}\n\n\tif (!wbc->for_reclaim)\n\t\tneed_balance_fs = true;\n\telse if (has_not_enough_free_secs(sbi, 0, 0))\n\t\tgoto redirty_out;\n\telse\n\t\tset_inode_flag(inode, FI_HOT_DATA);\n\n\terr = -EAGAIN;\n\tif (f2fs_has_inline_data(inode)) {\n\t\terr = f2fs_write_inline_data(inode, page);\n\t\tif (!err)\n\t\t\tgoto out;\n\t}\n\n\tif (err == -EAGAIN) {\n\t\terr = f2fs_do_write_data_page(&fio);\n\t\tif (err == -EAGAIN) {\n\t\t\tfio.need_lock = LOCK_REQ;\n\t\t\terr = f2fs_do_write_data_page(&fio);\n\t\t}\n\t}\n\n\tif (err) {\n\t\tfile_set_keep_isize(inode);\n\t} else {\n\t\tdown_write(&F2FS_I(inode)->i_sem);\n\t\tif (F2FS_I(inode)->last_disk_size < psize)\n\t\t\tF2FS_I(inode)->last_disk_size = psize;\n\t\tup_write(&F2FS_I(inode)->i_sem);\n\t}\n\ndone:\n\tif (err && err != -ENOENT)\n\t\tgoto redirty_out;\n\nout:\n\tinode_dec_dirty_pages(inode);\n\tif (err) {\n\t\tClearPageUptodate(page);\n\t\tclear_cold_data(page);\n\t}\n\n\tif (wbc->for_reclaim) {\n\t\tf2fs_submit_merged_write_cond(sbi, NULL, page, 0, DATA);\n\t\tclear_inode_flag(inode, FI_HOT_DATA);\n\t\tf2fs_remove_dirty_inode(inode);\n\t\tsubmitted = NULL;\n\t}\n\n\tunlock_page(page);\n\tif (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&\n\t\t\t\t\t!F2FS_I(inode)->cp_task) {\n\t\tf2fs_submit_ipu_bio(sbi, bio, page);\n\t\tf2fs_balance_fs(sbi, need_balance_fs);\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_submit_ipu_bio(sbi, bio, page);\n\t\tf2fs_submit_merged_write(sbi, DATA);\n\t\tsubmitted = NULL;\n\t}\n\n\tif (submitted)\n\t\t*submitted = fio.submitted;\n\n\treturn 0;\n\nredirty_out:\n\tredirty_page_for_writepage(wbc, page);\n\t/*\n\t * pageout() in MM traslates EAGAIN, so calls handle_write_error()\n\t * -> mapping_set_error() -> set_bit(AS_EIO, ...).\n\t * file_write_and_wait_range() will see EIO error, which is critical\n\t * to return value of fsync() followed by atomic_write failure to user.\n\t */\n\tif (!err || wbc->for_reclaim)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\tunlock_page(page);\n\treturn err;\n}\n\nstatic int f2fs_write_data_page(struct page *page,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\treturn __write_data_page(page, NULL, NULL, NULL, wbc, FS_DATA_IO);\n}\n\n/*\n * This function was copied from write_cche_pages from mm/page-writeback.c.\n * The major change is making write step of cold data page separately from\n * warm/hot data page.\n */\nstatic int f2fs_write_cache_pages(struct address_space *mapping,\n\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tint ret = 0;\n\tint done = 0;\n\tstruct pagevec pvec;\n\tstruct f2fs_sb_info *sbi = F2FS_M_SB(mapping);\n\tstruct bio *bio = NULL;\n\tsector_t last_block;\n\tint nr_pages;\n\tpgoff_t uninitialized_var(writeback_index);\n\tpgoff_t index;\n\tpgoff_t end;\t\t/* Inclusive */\n\tpgoff_t done_index;\n\tint cycled;\n\tint range_whole = 0;\n\txa_mark_t tag;\n\tint nwritten = 0;\n\n\tpagevec_init(&pvec);\n\n\tif (get_dirty_pages(mapping->host) <=\n\t\t\t\tSM_I(F2FS_M_SB(mapping))->min_hot_blocks)\n\t\tset_inode_flag(mapping->host, FI_HOT_DATA);\n\telse\n\t\tclear_inode_flag(mapping->host, FI_HOT_DATA);\n\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index; /* prev offset */\n\t\tindex = writeback_index;\n\t\tif (index == 0)\n\t\t\tcycled = 1;\n\t\telse\n\t\t\tcycled = 0;\n\t\tend = -1;\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t\tcycled = 1; /* ignore range_cyclic tests */\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && (index <= end)) {\n\t\tint i;\n\n\t\tnr_pages = pagevec_lookup_range_tag(&pvec, mapping, &index, end,\n\t\t\t\ttag);\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pvec.pages[i];\n\t\t\tbool submitted = false;\n\n\t\t\t/* give a priority to WB_SYNC threads */\n\t\t\tif (atomic_read(&sbi->wb_sync_req[DATA]) &&\n\t\t\t\t\twbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdone_index = page->index;\nretry_write:\n\t\t\tlock_page(page);\n\n\t\t\tif (unlikely(page->mapping != mapping)) {\ncontinue_unlock:\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!PageDirty(page)) {\n\t\t\t\t/* someone wrote it for us */\n\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tif (PageWriteback(page)) {\n\t\t\t\tif (wbc->sync_mode != WB_SYNC_NONE) {\n\t\t\t\t\tf2fs_wait_on_page_writeback(page,\n\t\t\t\t\t\t\tDATA, true, true);\n\t\t\t\t\tf2fs_submit_ipu_bio(sbi, &bio, page);\n\t\t\t\t} else {\n\t\t\t\t\tgoto continue_unlock;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!clear_page_dirty_for_io(page))\n\t\t\t\tgoto continue_unlock;\n\n\t\t\tret = __write_data_page(page, &submitted, &bio,\n\t\t\t\t\t&last_block, wbc, io_type);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\t/*\n\t\t\t\t * keep nr_to_write, since vfs uses this to\n\t\t\t\t * get # of written pages.\n\t\t\t\t */\n\t\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tret = 0;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else if (ret == -EAGAIN) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tif (wbc->sync_mode == WB_SYNC_ALL) {\n\t\t\t\t\t\tcond_resched();\n\t\t\t\t\t\tcongestion_wait(BLK_RW_ASYNC,\n\t\t\t\t\t\t\t\t\tHZ/50);\n\t\t\t\t\t\tgoto retry_write;\n\t\t\t\t\t}\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tdone_index = page->index + 1;\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t} else if (submitted) {\n\t\t\t\tnwritten++;\n\t\t\t}\n\n\t\t\tif (--wbc->nr_to_write <= 0 &&\n\t\t\t\t\twbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tpagevec_release(&pvec);\n\t\tcond_resched();\n\t}\n\n\tif (!cycled && !done) {\n\t\tcycled = 1;\n\t\tindex = 0;\n\t\tend = writeback_index - 1;\n\t\tgoto retry;\n\t}\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = done_index;\n\n\tif (nwritten)\n\t\tf2fs_submit_merged_write_cond(F2FS_M_SB(mapping), mapping->host,\n\t\t\t\t\t\t\t\tNULL, 0, DATA);\n\t/* submit cached bio of IPU write */\n\tif (bio)\n\t\t__submit_bio(sbi, bio, DATA);\n\n\treturn ret;\n}\n\nstatic inline bool __should_serialize_io(struct inode *inode,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn false;\n\tif (IS_NOQUOTA(inode))\n\t\treturn false;\n\t/* to avoid deadlock in path of data flush */\n\tif (F2FS_I(inode)->cp_task)\n\t\treturn false;\n\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\treturn true;\n\tif (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct blk_plug plug;\n\tint ret;\n\tbool locked = false;\n\n\t/* deal with chardevs and other special file */\n\tif (!mapping->a_ops->writepage)\n\t\treturn 0;\n\n\t/* skip writing if there is no dirty page in this inode */\n\tif (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)\n\t\treturn 0;\n\n\t/* during POR, we don't need to trigger writepage at all. */\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto skip_write;\n\n\tif ((S_ISDIR(inode->i_mode) || IS_NOQUOTA(inode)) &&\n\t\t\twbc->sync_mode == WB_SYNC_NONE &&\n\t\t\tget_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&\n\t\t\tf2fs_available_free_memory(sbi, DIRTY_DENTS))\n\t\tgoto skip_write;\n\n\t/* skip writing during file defragment */\n\tif (is_inode_flag_set(inode, FI_DO_DEFRAG))\n\t\tgoto skip_write;\n\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\n\t/* to avoid spliting IOs due to mixed WB_SYNC_ALL and WB_SYNC_NONE */\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tatomic_inc(&sbi->wb_sync_req[DATA]);\n\telse if (atomic_read(&sbi->wb_sync_req[DATA]))\n\t\tgoto skip_write;\n\n\tif (__should_serialize_io(inode, wbc)) {\n\t\tmutex_lock(&sbi->writepages);\n\t\tlocked = true;\n\t}\n\n\tblk_start_plug(&plug);\n\tret = f2fs_write_cache_pages(mapping, wbc, io_type);\n\tblk_finish_plug(&plug);\n\n\tif (locked)\n\t\tmutex_unlock(&sbi->writepages);\n\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tatomic_dec(&sbi->wb_sync_req[DATA]);\n\t/*\n\t * if some pages were truncated, we cannot guarantee its mapping->host\n\t * to detect pending bios.\n\t */\n\n\tf2fs_remove_dirty_inode(inode);\n\treturn ret;\n\nskip_write:\n\twbc->pages_skipped += get_dirty_pages(inode);\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\treturn 0;\n}\n\nstatic int f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t    struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\n\treturn __f2fs_write_data_pages(mapping, wbc,\n\t\t\tF2FS_I(inode)->cp_task == current ?\n\t\t\tFS_CP_DATA_IO : FS_DATA_IO);\n}\n\nstatic void f2fs_write_failed(struct address_space *mapping, loff_t to)\n{\n\tstruct inode *inode = mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\n\tif (to > i_size) {\n\t\tdown_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\tdown_write(&F2FS_I(inode)->i_mmap_sem);\n\n\t\ttruncate_pagecache(inode, i_size);\n\t\tif (!IS_NOQUOTA(inode))\n\t\t\tf2fs_truncate_blocks(inode, i_size, true);\n\n\t\tup_write(&F2FS_I(inode)->i_mmap_sem);\n\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t}\n}\n\nstatic int prepare_write_begin(struct f2fs_sb_info *sbi,\n\t\t\tstruct page *page, loff_t pos, unsigned len,\n\t\t\tblock_t *blk_addr, bool *node_changed)\n{\n\tstruct inode *inode = page->mapping->host;\n\tpgoff_t index = page->index;\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tbool locked = false;\n\tstruct extent_info ei = {0,0,0};\n\tint err = 0;\n\tint flag;\n\n\t/*\n\t * we already allocated all the blocks, so we don't need to get\n\t * the block addresses when there is no need to fill the page.\n\t */\n\tif (!f2fs_has_inline_data(inode) && len == PAGE_SIZE &&\n\t\t\t!is_inode_flag_set(inode, FI_NO_PREALLOC))\n\t\treturn 0;\n\n\t/* f2fs_lock_op avoids race between write CP and convert_inline_page */\n\tif (f2fs_has_inline_data(inode) && pos + len > MAX_INLINE_DATA(inode))\n\t\tflag = F2FS_GET_BLOCK_DEFAULT;\n\telse\n\t\tflag = F2FS_GET_BLOCK_PRE_AIO;\n\n\tif (f2fs_has_inline_data(inode) ||\n\t\t\t(pos & PAGE_MASK) >= i_size_read(inode)) {\n\t\t__do_map_lock(sbi, flag, true);\n\t\tlocked = true;\n\t}\nrestart:\n\t/* check inline_data */\n\tipage = f2fs_get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(ipage)) {\n\t\terr = PTR_ERR(ipage);\n\t\tgoto unlock_out;\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tif (pos + len <= MAX_INLINE_DATA(inode)) {\n\t\t\tf2fs_do_read_inline_data(page, ipage);\n\t\t\tset_inode_flag(inode, FI_DATA_EXIST);\n\t\t\tif (inode->i_nlink)\n\t\t\t\tset_inline_node(ipage);\n\t\t} else {\n\t\t\terr = f2fs_convert_inline_page(&dn, page);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tif (dn.data_blkaddr == NULL_ADDR)\n\t\t\t\terr = f2fs_get_block(&dn, index);\n\t\t}\n\t} else if (locked) {\n\t\terr = f2fs_get_block(&dn, index);\n\t} else {\n\t\tif (f2fs_lookup_extent_cache(inode, index, &ei)) {\n\t\t\tdn.data_blkaddr = ei.blk + index - ei.fofs;\n\t\t} else {\n\t\t\t/* hole case */\n\t\t\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\t\t\tif (err || dn.data_blkaddr == NULL_ADDR) {\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\t__do_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO,\n\t\t\t\t\t\t\t\ttrue);\n\t\t\t\tWARN_ON(flag != F2FS_GET_BLOCK_PRE_AIO);\n\t\t\t\tlocked = true;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* convert_inline_page can make node_changed */\n\t*blk_addr = dn.data_blkaddr;\n\t*node_changed = dn.node_changed;\nout:\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (locked)\n\t\t__do_map_lock(sbi, flag, false);\n\treturn err;\n}\n\nstatic int f2fs_write_begin(struct file *file, struct address_space *mapping,\n\t\tloff_t pos, unsigned len, unsigned flags,\n\t\tstruct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page = NULL;\n\tpgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;\n\tbool need_balance = false, drop_atomic = false;\n\tblock_t blkaddr = NULL_ADDR;\n\tint err = 0;\n\n\ttrace_f2fs_write_begin(inode, pos, len, flags);\n\n\terr = f2fs_is_checkpoint_ready(sbi);\n\tif (err)\n\t\tgoto fail;\n\n\tif ((f2fs_is_atomic_file(inode) &&\n\t\t\t!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||\n\t\t\tis_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {\n\t\terr = -ENOMEM;\n\t\tdrop_atomic = true;\n\t\tgoto fail;\n\t}\n\n\t/*\n\t * We should check this at this moment to avoid deadlock on inode page\n\t * and #0 page. The locking rule for inline_data conversion should be:\n\t * lock_page(page #0) -> lock_page(inode_page)\n\t */\n\tif (index != 0) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\nrepeat:\n\t/*\n\t * Do not use grab_cache_page_write_begin() to avoid deadlock due to\n\t * wait_for_stable_page. Will wait that below with our IO control.\n\t */\n\tpage = f2fs_pagecache_get_page(mapping, index,\n\t\t\t\tFGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);\n\tif (!page) {\n\t\terr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t*pagep = page;\n\n\terr = prepare_write_begin(sbi, page, pos, len,\n\t\t\t\t\t&blkaddr, &need_balance);\n\tif (err)\n\t\tgoto fail;\n\n\tif (need_balance && !IS_NOQUOTA(inode) &&\n\t\t\thas_not_enough_free_secs(sbi, 0, 0)) {\n\t\tunlock_page(page);\n\t\tf2fs_balance_fs(sbi, true);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\t/* The page got truncated from under us */\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\n\tf2fs_wait_on_page_writeback(page, DATA, false, true);\n\n\tif (len == PAGE_SIZE || PageUptodate(page))\n\t\treturn 0;\n\n\tif (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode)) {\n\t\tzero_user_segment(page, len, PAGE_SIZE);\n\t\treturn 0;\n\t}\n\n\tif (blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t} else {\n\t\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr,\n\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto fail;\n\t\t}\n\t\terr = f2fs_submit_page_read(inode, page, blkaddr);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tlock_page(page);\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tf2fs_put_page(page, 1);\n\tf2fs_write_failed(mapping, pos + len);\n\tif (drop_atomic)\n\t\tf2fs_drop_inmem_pages_all(sbi, false);\n\treturn err;\n}\n\nstatic int f2fs_write_end(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_f2fs_write_end(inode, pos, len, copied);\n\n\t/*\n\t * This should be come from len == PAGE_SIZE, and we expect copied\n\t * should be PAGE_SIZE. Otherwise, we treat it with zero copied and\n\t * let generic_perform_write() try to copy data again through copied=0.\n\t */\n\tif (!PageUptodate(page)) {\n\t\tif (unlikely(copied != len))\n\t\t\tcopied = 0;\n\t\telse\n\t\t\tSetPageUptodate(page);\n\t}\n\tif (!copied)\n\t\tgoto unlock_out;\n\n\tset_page_dirty(page);\n\n\tif (pos + copied > i_size_read(inode))\n\t\tf2fs_i_size_write(inode, pos + copied);\nunlock_out:\n\tf2fs_put_page(page, 1);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn copied;\n}\n\nstatic int check_direct_IO(struct inode *inode, struct iov_iter *iter,\n\t\t\t   loff_t offset)\n{\n\tunsigned i_blkbits = READ_ONCE(inode->i_blkbits);\n\tunsigned blkbits = i_blkbits;\n\tunsigned blocksize_mask = (1 << blkbits) - 1;\n\tunsigned long align = offset | iov_iter_alignment(iter);\n\tstruct block_device *bdev = inode->i_sb->s_bdev;\n\n\tif (align & blocksize_mask) {\n\t\tif (bdev)\n\t\t\tblkbits = blksize_bits(bdev_logical_block_size(bdev));\n\t\tblocksize_mask = (1 << blkbits) - 1;\n\t\tif (align & blocksize_mask)\n\t\t\treturn -EINVAL;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void f2fs_dio_end_io(struct bio *bio)\n{\n\tstruct f2fs_private_dio *dio = bio->bi_private;\n\n\tdec_page_count(F2FS_I_SB(dio->inode),\n\t\t\tdio->write ? F2FS_DIO_WRITE : F2FS_DIO_READ);\n\n\tbio->bi_private = dio->orig_private;\n\tbio->bi_end_io = dio->orig_end_io;\n\n\tkvfree(dio);\n\n\tbio_endio(bio);\n}\n\nstatic void f2fs_dio_submit_bio(struct bio *bio, struct inode *inode,\n\t\t\t\t\t\t\tloff_t file_offset)\n{\n\tstruct f2fs_private_dio *dio;\n\tbool write = (bio_op(bio) == REQ_OP_WRITE);\n\n\tdio = f2fs_kzalloc(F2FS_I_SB(inode),\n\t\t\tsizeof(struct f2fs_private_dio), GFP_NOFS);\n\tif (!dio)\n\t\tgoto out;\n\n\tdio->inode = inode;\n\tdio->orig_end_io = bio->bi_end_io;\n\tdio->orig_private = bio->bi_private;\n\tdio->write = write;\n\n\tbio->bi_end_io = f2fs_dio_end_io;\n\tbio->bi_private = dio;\n\n\tinc_page_count(F2FS_I_SB(inode),\n\t\t\twrite ? F2FS_DIO_WRITE : F2FS_DIO_READ);\n\n\tsubmit_bio(bio);\n\treturn;\nout:\n\tbio->bi_status = BLK_STS_IOERR;\n\tbio_endio(bio);\n}\n\nstatic ssize_t f2fs_direct_IO(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct address_space *mapping = iocb->ki_filp->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tsize_t count = iov_iter_count(iter);\n\tloff_t offset = iocb->ki_pos;\n\tint rw = iov_iter_rw(iter);\n\tint err;\n\tenum rw_hint hint = iocb->ki_hint;\n\tint whint_mode = F2FS_OPTION(sbi).whint_mode;\n\tbool do_opu;\n\n\terr = check_direct_IO(inode, iter, offset);\n\tif (err)\n\t\treturn err < 0 ? err : 0;\n\n\tif (f2fs_force_buffered_io(inode, iocb, iter))\n\t\treturn 0;\n\n\tdo_opu = allow_outplace_dio(inode, iocb, iter);\n\n\ttrace_f2fs_direct_IO_enter(inode, offset, count, rw);\n\n\tif (rw == WRITE && whint_mode == WHINT_MODE_OFF)\n\t\tiocb->ki_hint = WRITE_LIFE_NOT_SET;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\tif (!down_read_trylock(&fi->i_gc_rwsem[rw])) {\n\t\t\tiocb->ki_hint = hint;\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t\tif (do_opu && !down_read_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\tup_read(&fi->i_gc_rwsem[rw]);\n\t\t\tiocb->ki_hint = hint;\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tdown_read(&fi->i_gc_rwsem[rw]);\n\t\tif (do_opu)\n\t\t\tdown_read(&fi->i_gc_rwsem[READ]);\n\t}\n\n\terr = __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,\n\t\t\titer, rw == WRITE ? get_data_block_dio_write :\n\t\t\tget_data_block_dio, NULL, f2fs_dio_submit_bio,\n\t\t\tDIO_LOCKING | DIO_SKIP_HOLES);\n\n\tif (do_opu)\n\t\tup_read(&fi->i_gc_rwsem[READ]);\n\n\tup_read(&fi->i_gc_rwsem[rw]);\n\n\tif (rw == WRITE) {\n\t\tif (whint_mode == WHINT_MODE_OFF)\n\t\t\tiocb->ki_hint = hint;\n\t\tif (err > 0) {\n\t\t\tf2fs_update_iostat(F2FS_I_SB(inode), APP_DIRECT_IO,\n\t\t\t\t\t\t\t\t\terr);\n\t\t\tif (!do_opu)\n\t\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\t} else if (err < 0) {\n\t\t\tf2fs_write_failed(mapping, offset + count);\n\t\t}\n\t}\n\nout:\n\ttrace_f2fs_direct_IO_exit(inode, offset, count, rw, err);\n\n\treturn err;\n}\n\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\t\t\t\t\tunsigned int length)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino >= F2FS_ROOT_INO(sbi) &&\n\t\t(offset % PAGE_SIZE || length != PAGE_SIZE))\n\t\treturn;\n\n\tif (PageDirty(page)) {\n\t\tif (inode->i_ino == F2FS_META_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_META);\n\t\t} else if (inode->i_ino == F2FS_NODE_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_NODES);\n\t\t} else {\n\t\t\tinode_dec_dirty_pages(inode);\n\t\t\tf2fs_remove_dirty_inode(inode);\n\t\t}\n\t}\n\n\tclear_cold_data(page);\n\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn f2fs_drop_inmem_page(inode, page);\n\n\tf2fs_clear_page_private(page);\n}\n\nint f2fs_release_page(struct page *page, gfp_t wait)\n{\n\t/* If this is dirty page, keep PagePrivate */\n\tif (PageDirty(page))\n\t\treturn 0;\n\n\t/* This is atomic written page, keep Private */\n\tif (IS_ATOMIC_WRITTEN_PAGE(page))\n\t\treturn 0;\n\n\tclear_cold_data(page);\n\tf2fs_clear_page_private(page);\n\treturn 1;\n}\n\nstatic int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\tif (PageSwapCache(page))\n\t\treturn __set_page_dirty_nobuffers(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tf2fs_register_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\t__set_page_dirty_nobuffers(page);\n\t\tf2fs_update_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic sector_t f2fs_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\n\tif (f2fs_has_inline_data(inode))\n\t\treturn 0;\n\n\t/* make sure allocating whole blocks */\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tfilemap_write_and_wait(mapping);\n\n\treturn generic_block_bmap(mapping, block, get_data_block_bmap);\n}\n\n#ifdef CONFIG_MIGRATION\n#include <linux/migrate.h>\n\nint f2fs_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tint rc, extra_count;\n\tstruct f2fs_inode_info *fi = F2FS_I(mapping->host);\n\tbool atomic_written = IS_ATOMIC_WRITTEN_PAGE(page);\n\n\tBUG_ON(PageWriteback(page));\n\n\t/* migrating an atomic written page is safe with the inmem_lock hold */\n\tif (atomic_written) {\n\t\tif (mode != MIGRATE_SYNC)\n\t\t\treturn -EBUSY;\n\t\tif (!mutex_trylock(&fi->inmem_lock))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/* one extra reference was held for atomic_write page */\n\textra_count = atomic_written ? 1 : 0;\n\trc = migrate_page_move_mapping(mapping, newpage,\n\t\t\t\tpage, mode, extra_count);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tif (atomic_written)\n\t\t\tmutex_unlock(&fi->inmem_lock);\n\t\treturn rc;\n\t}\n\n\tif (atomic_written) {\n\t\tstruct inmem_pages *cur;\n\t\tlist_for_each_entry(cur, &fi->inmem_pages, list)\n\t\t\tif (cur->page == page) {\n\t\t\t\tcur->page = newpage;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tmutex_unlock(&fi->inmem_lock);\n\t\tput_page(page);\n\t\tget_page(newpage);\n\t}\n\n\tif (PagePrivate(page)) {\n\t\tf2fs_set_page_private(newpage, page_private(page));\n\t\tf2fs_clear_page_private(page);\n\t}\n\n\tif (mode != MIGRATE_SYNC_NO_COPY)\n\t\tmigrate_page_copy(newpage, page);\n\telse\n\t\tmigrate_page_states(newpage, page);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n#endif\n\n#ifdef CONFIG_SWAP\n/* Copied from generic_swapfile_activate() to check any holes */\nstatic int check_swap_activate(struct file *swap_file, unsigned int max)\n{\n\tstruct address_space *mapping = swap_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tunsigned blocks_per_page;\n\tunsigned long page_no;\n\tunsigned blkbits;\n\tsector_t probe_block;\n\tsector_t last_block;\n\tsector_t lowest_block = -1;\n\tsector_t highest_block = 0;\n\n\tblkbits = inode->i_blkbits;\n\tblocks_per_page = PAGE_SIZE >> blkbits;\n\n\t/*\n\t * Map all the blocks into the extent list.  This code doesn't try\n\t * to be very smart.\n\t */\n\tprobe_block = 0;\n\tpage_no = 0;\n\tlast_block = i_size_read(inode) >> blkbits;\n\twhile ((probe_block + blocks_per_page) <= last_block && page_no < max) {\n\t\tunsigned block_in_page;\n\t\tsector_t first_block;\n\n\t\tcond_resched();\n\n\t\tfirst_block = bmap(inode, probe_block);\n\t\tif (first_block == 0)\n\t\t\tgoto bad_bmap;\n\n\t\t/*\n\t\t * It must be PAGE_SIZE aligned on-disk\n\t\t */\n\t\tif (first_block & (blocks_per_page - 1)) {\n\t\t\tprobe_block++;\n\t\t\tgoto reprobe;\n\t\t}\n\n\t\tfor (block_in_page = 1; block_in_page < blocks_per_page;\n\t\t\t\t\tblock_in_page++) {\n\t\t\tsector_t block;\n\n\t\t\tblock = bmap(inode, probe_block + block_in_page);\n\t\t\tif (block == 0)\n\t\t\t\tgoto bad_bmap;\n\t\t\tif (block != first_block + block_in_page) {\n\t\t\t\t/* Discontiguity */\n\t\t\t\tprobe_block++;\n\t\t\t\tgoto reprobe;\n\t\t\t}\n\t\t}\n\n\t\tfirst_block >>= (PAGE_SHIFT - blkbits);\n\t\tif (page_no) {\t/* exclude the header page */\n\t\t\tif (first_block < lowest_block)\n\t\t\t\tlowest_block = first_block;\n\t\t\tif (first_block > highest_block)\n\t\t\t\thighest_block = first_block;\n\t\t}\n\n\t\tpage_no++;\n\t\tprobe_block += blocks_per_page;\nreprobe:\n\t\tcontinue;\n\t}\n\treturn 0;\n\nbad_bmap:\n\tpr_err(\"swapon: swapfile has holes\\n\");\n\treturn -EINVAL;\n}\n\nstatic int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,\n\t\t\t\tsector_t *span)\n{\n\tstruct inode *inode = file_inode(file);\n\tint ret;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\tif (f2fs_readonly(F2FS_I_SB(inode)->sb))\n\t\treturn -EROFS;\n\n\tret = f2fs_convert_inline_inode(inode);\n\tif (ret)\n\t\treturn ret;\n\n\tret = check_swap_activate(file, sis->max);\n\tif (ret)\n\t\treturn ret;\n\n\tset_inode_flag(inode, FI_PIN_FILE);\n\tf2fs_precache_extents(inode);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn 0;\n}\n\nstatic void f2fs_swap_deactivate(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\n\tclear_inode_flag(inode, FI_PIN_FILE);\n}\n#else\nstatic int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,\n\t\t\t\tsector_t *span)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic void f2fs_swap_deactivate(struct file *file)\n{\n}\n#endif\n\nconst struct address_space_operations f2fs_dblock_aops = {\n\t.readpage\t= f2fs_read_data_page,\n\t.readpages\t= f2fs_read_data_pages,\n\t.writepage\t= f2fs_write_data_page,\n\t.writepages\t= f2fs_write_data_pages,\n\t.write_begin\t= f2fs_write_begin,\n\t.write_end\t= f2fs_write_end,\n\t.set_page_dirty\t= f2fs_set_data_page_dirty,\n\t.invalidatepage\t= f2fs_invalidate_page,\n\t.releasepage\t= f2fs_release_page,\n\t.direct_IO\t= f2fs_direct_IO,\n\t.bmap\t\t= f2fs_bmap,\n\t.swap_activate  = f2fs_swap_activate,\n\t.swap_deactivate = f2fs_swap_deactivate,\n#ifdef CONFIG_MIGRATION\n\t.migratepage    = f2fs_migrate_page,\n#endif\n};\n\nvoid f2fs_clear_page_cache_dirty_tag(struct page *page)\n{\n\tstruct address_space *mapping = page_mapping(page);\n\tunsigned long flags;\n\n\txa_lock_irqsave(&mapping->i_pages, flags);\n\t__xa_clear_mark(&mapping->i_pages, page_index(page),\n\t\t\t\t\t\tPAGECACHE_TAG_DIRTY);\n\txa_unlock_irqrestore(&mapping->i_pages, flags);\n}\n\nint __init f2fs_init_post_read_processing(void)\n{\n\tbio_post_read_ctx_cache = KMEM_CACHE(bio_post_read_ctx, 0);\n\tif (!bio_post_read_ctx_cache)\n\t\tgoto fail;\n\tbio_post_read_ctx_pool =\n\t\tmempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS,\n\t\t\t\t\t bio_post_read_ctx_cache);\n\tif (!bio_post_read_ctx_pool)\n\t\tgoto fail_free_cache;\n\treturn 0;\n\nfail_free_cache:\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid __exit f2fs_destroy_post_read_processing(void)\n{\n\tmempool_destroy(bio_post_read_ctx_pool);\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * fs/f2fs/f2fs.h\n *\n * Copyright (c) 2012 Samsung Electronics Co., Ltd.\n *             http://www.samsung.com/\n */\n#ifndef _LINUX_F2FS_H\n#define _LINUX_F2FS_H\n\n#include <linux/uio.h>\n#include <linux/types.h>\n#include <linux/page-flags.h>\n#include <linux/buffer_head.h>\n#include <linux/slab.h>\n#include <linux/crc32.h>\n#include <linux/magic.h>\n#include <linux/kobject.h>\n#include <linux/sched.h>\n#include <linux/cred.h>\n#include <linux/vmalloc.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/quotaops.h>\n#include <crypto/hash.h>\n\n#include <linux/fscrypt.h>\n\n#ifdef CONFIG_F2FS_CHECK_FS\n#define f2fs_bug_on(sbi, condition)\tBUG_ON(condition)\n#else\n#define f2fs_bug_on(sbi, condition)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (unlikely(condition)) {\t\t\t\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n#endif\n\nenum {\n\tFAULT_KMALLOC,\n\tFAULT_KVMALLOC,\n\tFAULT_PAGE_ALLOC,\n\tFAULT_PAGE_GET,\n\tFAULT_ALLOC_BIO,\n\tFAULT_ALLOC_NID,\n\tFAULT_ORPHAN,\n\tFAULT_BLOCK,\n\tFAULT_DIR_DEPTH,\n\tFAULT_EVICT_INODE,\n\tFAULT_TRUNCATE,\n\tFAULT_READ_IO,\n\tFAULT_CHECKPOINT,\n\tFAULT_DISCARD,\n\tFAULT_WRITE_IO,\n\tFAULT_MAX,\n};\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n#define F2FS_ALL_FAULT_TYPE\t\t((1 << FAULT_MAX) - 1)\n\nstruct f2fs_fault_info {\n\tatomic_t inject_ops;\n\tunsigned int inject_rate;\n\tunsigned int inject_type;\n};\n\nextern const char *f2fs_fault_name[FAULT_MAX];\n#define IS_FAULT_SET(fi, type) ((fi)->inject_type & (1 << (type)))\n#endif\n\n/*\n * For mount options\n */\n#define F2FS_MOUNT_BG_GC\t\t0x00000001\n#define F2FS_MOUNT_DISABLE_ROLL_FORWARD\t0x00000002\n#define F2FS_MOUNT_DISCARD\t\t0x00000004\n#define F2FS_MOUNT_NOHEAP\t\t0x00000008\n#define F2FS_MOUNT_XATTR_USER\t\t0x00000010\n#define F2FS_MOUNT_POSIX_ACL\t\t0x00000020\n#define F2FS_MOUNT_DISABLE_EXT_IDENTIFY\t0x00000040\n#define F2FS_MOUNT_INLINE_XATTR\t\t0x00000080\n#define F2FS_MOUNT_INLINE_DATA\t\t0x00000100\n#define F2FS_MOUNT_INLINE_DENTRY\t0x00000200\n#define F2FS_MOUNT_FLUSH_MERGE\t\t0x00000400\n#define F2FS_MOUNT_NOBARRIER\t\t0x00000800\n#define F2FS_MOUNT_FASTBOOT\t\t0x00001000\n#define F2FS_MOUNT_EXTENT_CACHE\t\t0x00002000\n#define F2FS_MOUNT_FORCE_FG_GC\t\t0x00004000\n#define F2FS_MOUNT_DATA_FLUSH\t\t0x00008000\n#define F2FS_MOUNT_FAULT_INJECTION\t0x00010000\n#define F2FS_MOUNT_ADAPTIVE\t\t0x00020000\n#define F2FS_MOUNT_LFS\t\t\t0x00040000\n#define F2FS_MOUNT_USRQUOTA\t\t0x00080000\n#define F2FS_MOUNT_GRPQUOTA\t\t0x00100000\n#define F2FS_MOUNT_PRJQUOTA\t\t0x00200000\n#define F2FS_MOUNT_QUOTA\t\t0x00400000\n#define F2FS_MOUNT_INLINE_XATTR_SIZE\t0x00800000\n#define F2FS_MOUNT_RESERVE_ROOT\t\t0x01000000\n#define F2FS_MOUNT_DISABLE_CHECKPOINT\t0x02000000\n\n#define F2FS_OPTION(sbi)\t((sbi)->mount_opt)\n#define clear_opt(sbi, option)\t(F2FS_OPTION(sbi).opt &= ~F2FS_MOUNT_##option)\n#define set_opt(sbi, option)\t(F2FS_OPTION(sbi).opt |= F2FS_MOUNT_##option)\n#define test_opt(sbi, option)\t(F2FS_OPTION(sbi).opt & F2FS_MOUNT_##option)\n\n#define ver_after(a, b)\t(typecheck(unsigned long long, a) &&\t\t\\\n\t\ttypecheck(unsigned long long, b) &&\t\t\t\\\n\t\t((long long)((a) - (b)) > 0))\n\ntypedef u32 block_t;\t/*\n\t\t\t * should not change u32, since it is the on-disk block\n\t\t\t * address format, __le32.\n\t\t\t */\ntypedef u32 nid_t;\n\nstruct f2fs_mount_info {\n\tunsigned int opt;\n\tint write_io_size_bits;\t\t/* Write IO size bits */\n\tblock_t root_reserved_blocks;\t/* root reserved blocks */\n\tkuid_t s_resuid;\t\t/* reserved blocks for uid */\n\tkgid_t s_resgid;\t\t/* reserved blocks for gid */\n\tint active_logs;\t\t/* # of active logs */\n\tint inline_xattr_size;\t\t/* inline xattr size */\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n\tstruct f2fs_fault_info fault_info;\t/* For fault injection */\n#endif\n#ifdef CONFIG_QUOTA\n\t/* Names of quota files with journalled quota */\n\tchar *s_qf_names[MAXQUOTAS];\n\tint s_jquota_fmt;\t\t\t/* Format of quota to use */\n#endif\n\t/* For which write hints are passed down to block layer */\n\tint whint_mode;\n\tint alloc_mode;\t\t\t/* segment allocation policy */\n\tint fsync_mode;\t\t\t/* fsync policy */\n\tbool test_dummy_encryption;\t/* test dummy encryption */\n\tblock_t unusable_cap;\t\t/* Amount of space allowed to be\n\t\t\t\t\t * unusable when disabling checkpoint\n\t\t\t\t\t */\n};\n\n#define F2FS_FEATURE_ENCRYPT\t\t0x0001\n#define F2FS_FEATURE_BLKZONED\t\t0x0002\n#define F2FS_FEATURE_ATOMIC_WRITE\t0x0004\n#define F2FS_FEATURE_EXTRA_ATTR\t\t0x0008\n#define F2FS_FEATURE_PRJQUOTA\t\t0x0010\n#define F2FS_FEATURE_INODE_CHKSUM\t0x0020\n#define F2FS_FEATURE_FLEXIBLE_INLINE_XATTR\t0x0040\n#define F2FS_FEATURE_QUOTA_INO\t\t0x0080\n#define F2FS_FEATURE_INODE_CRTIME\t0x0100\n#define F2FS_FEATURE_LOST_FOUND\t\t0x0200\n#define F2FS_FEATURE_VERITY\t\t0x0400\t/* reserved */\n#define F2FS_FEATURE_SB_CHKSUM\t\t0x0800\n\n#define __F2FS_HAS_FEATURE(raw_super, mask)\t\t\t\t\\\n\t((raw_super->feature & cpu_to_le32(mask)) != 0)\n#define F2FS_HAS_FEATURE(sbi, mask)\t__F2FS_HAS_FEATURE(sbi->raw_super, mask)\n#define F2FS_SET_FEATURE(sbi, mask)\t\t\t\t\t\\\n\t(sbi->raw_super->feature |= cpu_to_le32(mask))\n#define F2FS_CLEAR_FEATURE(sbi, mask)\t\t\t\t\t\\\n\t(sbi->raw_super->feature &= ~cpu_to_le32(mask))\n\n/*\n * Default values for user and/or group using reserved blocks\n */\n#define\tF2FS_DEF_RESUID\t\t0\n#define\tF2FS_DEF_RESGID\t\t0\n\n/*\n * For checkpoint manager\n */\nenum {\n\tNAT_BITMAP,\n\tSIT_BITMAP\n};\n\n#define\tCP_UMOUNT\t0x00000001\n#define\tCP_FASTBOOT\t0x00000002\n#define\tCP_SYNC\t\t0x00000004\n#define\tCP_RECOVERY\t0x00000008\n#define\tCP_DISCARD\t0x00000010\n#define CP_TRIMMED\t0x00000020\n#define CP_PAUSE\t0x00000040\n\n#define MAX_DISCARD_BLOCKS(sbi)\t\tBLKS_PER_SEC(sbi)\n#define DEF_MAX_DISCARD_REQUEST\t\t8\t/* issue 8 discards per round */\n#define DEF_MIN_DISCARD_ISSUE_TIME\t50\t/* 50 ms, if exists */\n#define DEF_MID_DISCARD_ISSUE_TIME\t500\t/* 500 ms, if device busy */\n#define DEF_MAX_DISCARD_ISSUE_TIME\t60000\t/* 60 s, if no candidates */\n#define DEF_DISCARD_URGENT_UTIL\t\t80\t/* do more discard over 80% */\n#define DEF_CP_INTERVAL\t\t\t60\t/* 60 secs */\n#define DEF_IDLE_INTERVAL\t\t5\t/* 5 secs */\n#define DEF_DISABLE_INTERVAL\t\t5\t/* 5 secs */\n#define DEF_DISABLE_QUICK_INTERVAL\t1\t/* 1 secs */\n#define DEF_UMOUNT_DISCARD_TIMEOUT\t5\t/* 5 secs */\n\nstruct cp_control {\n\tint reason;\n\t__u64 trim_start;\n\t__u64 trim_end;\n\t__u64 trim_minlen;\n};\n\n/*\n * indicate meta/data type\n */\nenum {\n\tMETA_CP,\n\tMETA_NAT,\n\tMETA_SIT,\n\tMETA_SSA,\n\tMETA_MAX,\n\tMETA_POR,\n\tDATA_GENERIC,\t\t/* check range only */\n\tDATA_GENERIC_ENHANCE,\t/* strong check on range and segment bitmap */\n\tDATA_GENERIC_ENHANCE_READ,\t/*\n\t\t\t\t\t * strong check on range and segment\n\t\t\t\t\t * bitmap but no warning due to race\n\t\t\t\t\t * condition of read on truncated area\n\t\t\t\t\t * by extent_cache\n\t\t\t\t\t */\n\tMETA_GENERIC,\n};\n\n/* for the list of ino */\nenum {\n\tORPHAN_INO,\t\t/* for orphan ino list */\n\tAPPEND_INO,\t\t/* for append ino list */\n\tUPDATE_INO,\t\t/* for update ino list */\n\tTRANS_DIR_INO,\t\t/* for trasactions dir ino list */\n\tFLUSH_INO,\t\t/* for multiple device flushing */\n\tMAX_INO_ENTRY,\t\t/* max. list */\n};\n\nstruct ino_entry {\n\tstruct list_head list;\t\t/* list head */\n\tnid_t ino;\t\t\t/* inode number */\n\tunsigned int dirty_device;\t/* dirty device bitmap */\n};\n\n/* for the list of inodes to be GCed */\nstruct inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n};\n\nstruct fsync_node_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct page *page;\t/* warm node page pointer */\n\tunsigned int seq_id;\t/* sequence id */\n};\n\n/* for the bitmap indicate blocks to be discarded */\nstruct discard_entry {\n\tstruct list_head list;\t/* list head */\n\tblock_t start_blkaddr;\t/* start blockaddr of current segment */\n\tunsigned char discard_map[SIT_VBLOCK_MAP_SIZE];\t/* segment discard bitmap */\n};\n\n/* default discard granularity of inner discard thread, unit: block count */\n#define DEFAULT_DISCARD_GRANULARITY\t\t16\n\n/* max discard pend list number */\n#define MAX_PLIST_NUM\t\t512\n#define plist_idx(blk_num)\t((blk_num) >= MAX_PLIST_NUM ?\t\t\\\n\t\t\t\t\t(MAX_PLIST_NUM - 1) : ((blk_num) - 1))\n\nenum {\n\tD_PREP,\t\t\t/* initial */\n\tD_PARTIAL,\t\t/* partially submitted */\n\tD_SUBMIT,\t\t/* all submitted */\n\tD_DONE,\t\t\t/* finished */\n};\n\nstruct discard_info {\n\tblock_t lstart;\t\t\t/* logical start address */\n\tblock_t len;\t\t\t/* length */\n\tblock_t start;\t\t\t/* actual start address in dev */\n};\n\nstruct discard_cmd {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunion {\n\t\tstruct {\n\t\t\tblock_t lstart;\t/* logical start address */\n\t\t\tblock_t len;\t/* length */\n\t\t\tblock_t start;\t/* actual start address in dev */\n\t\t};\n\t\tstruct discard_info di;\t/* discard info */\n\n\t};\n\tstruct list_head list;\t\t/* command list */\n\tstruct completion wait;\t\t/* compleation */\n\tstruct block_device *bdev;\t/* bdev */\n\tunsigned short ref;\t\t/* reference count */\n\tunsigned char state;\t\t/* state */\n\tunsigned char queued;\t\t/* queued discard */\n\tint error;\t\t\t/* bio error */\n\tspinlock_t lock;\t\t/* for state/bio_ref updating */\n\tunsigned short bio_ref;\t\t/* bio reference count */\n};\n\nenum {\n\tDPOLICY_BG,\n\tDPOLICY_FORCE,\n\tDPOLICY_FSTRIM,\n\tDPOLICY_UMOUNT,\n\tMAX_DPOLICY,\n};\n\nstruct discard_policy {\n\tint type;\t\t\t/* type of discard */\n\tunsigned int min_interval;\t/* used for candidates exist */\n\tunsigned int mid_interval;\t/* used for device busy */\n\tunsigned int max_interval;\t/* used for candidates not exist */\n\tunsigned int max_requests;\t/* # of discards issued per round */\n\tunsigned int io_aware_gran;\t/* minimum granularity discard not be aware of I/O */\n\tbool io_aware;\t\t\t/* issue discard in idle time */\n\tbool sync;\t\t\t/* submit discard with REQ_SYNC flag */\n\tbool ordered;\t\t\t/* issue discard by lba order */\n\tunsigned int granularity;\t/* discard granularity */\n\tint timeout;\t\t\t/* discard timeout for put_super */\n};\n\nstruct discard_cmd_control {\n\tstruct task_struct *f2fs_issue_discard;\t/* discard thread */\n\tstruct list_head entry_list;\t\t/* 4KB discard entry list */\n\tstruct list_head pend_list[MAX_PLIST_NUM];/* store pending entries */\n\tstruct list_head wait_list;\t\t/* store on-flushing entries */\n\tstruct list_head fstrim_list;\t\t/* in-flight discard from fstrim */\n\twait_queue_head_t discard_wait_queue;\t/* waiting queue for wake-up */\n\tunsigned int discard_wake;\t\t/* to wake up discard thread */\n\tstruct mutex cmd_lock;\n\tunsigned int nr_discards;\t\t/* # of discards in the list */\n\tunsigned int max_discards;\t\t/* max. discards to be issued */\n\tunsigned int discard_granularity;\t/* discard granularity */\n\tunsigned int undiscard_blks;\t\t/* # of undiscard blocks */\n\tunsigned int next_pos;\t\t\t/* next discard position */\n\tatomic_t issued_discard;\t\t/* # of issued discard */\n\tatomic_t queued_discard;\t\t/* # of queued discard */\n\tatomic_t discard_cmd_cnt;\t\t/* # of cached cmd count */\n\tstruct rb_root_cached root;\t\t/* root of discard rb-tree */\n\tbool rbtree_check;\t\t\t/* config for consistence check */\n};\n\n/* for the list of fsync inodes, used only during recovery */\nstruct fsync_inode_entry {\n\tstruct list_head list;\t/* list head */\n\tstruct inode *inode;\t/* vfs inode pointer */\n\tblock_t blkaddr;\t/* block address locating the last fsync */\n\tblock_t last_dentry;\t/* block address locating the last dentry */\n};\n\n#define nats_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_nats))\n#define sits_in_cursum(jnl)\t\t(le16_to_cpu((jnl)->n_sits))\n\n#define nat_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].ne)\n#define nid_in_journal(jnl, i)\t\t((jnl)->nat_j.entries[i].nid)\n#define sit_in_journal(jnl, i)\t\t((jnl)->sit_j.entries[i].se)\n#define segno_in_journal(jnl, i)\t((jnl)->sit_j.entries[i].segno)\n\n#define MAX_NAT_JENTRIES(jnl)\t(NAT_JOURNAL_ENTRIES - nats_in_cursum(jnl))\n#define MAX_SIT_JENTRIES(jnl)\t(SIT_JOURNAL_ENTRIES - sits_in_cursum(jnl))\n\nstatic inline int update_nats_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = nats_in_cursum(journal);\n\n\tjournal->n_nats = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline int update_sits_in_cursum(struct f2fs_journal *journal, int i)\n{\n\tint before = sits_in_cursum(journal);\n\n\tjournal->n_sits = cpu_to_le16(before + i);\n\treturn before;\n}\n\nstatic inline bool __has_cursum_space(struct f2fs_journal *journal,\n\t\t\t\t\t\t\tint size, int type)\n{\n\tif (type == NAT_JOURNAL)\n\t\treturn size <= MAX_NAT_JENTRIES(journal);\n\treturn size <= MAX_SIT_JENTRIES(journal);\n}\n\n/*\n * ioctl commands\n */\n#define F2FS_IOC_GETFLAGS\t\tFS_IOC_GETFLAGS\n#define F2FS_IOC_SETFLAGS\t\tFS_IOC_SETFLAGS\n#define F2FS_IOC_GETVERSION\t\tFS_IOC_GETVERSION\n\n#define F2FS_IOCTL_MAGIC\t\t0xf5\n#define F2FS_IOC_START_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 1)\n#define F2FS_IOC_COMMIT_ATOMIC_WRITE\t_IO(F2FS_IOCTL_MAGIC, 2)\n#define F2FS_IOC_START_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 3)\n#define F2FS_IOC_RELEASE_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 4)\n#define F2FS_IOC_ABORT_VOLATILE_WRITE\t_IO(F2FS_IOCTL_MAGIC, 5)\n#define F2FS_IOC_GARBAGE_COLLECT\t_IOW(F2FS_IOCTL_MAGIC, 6, __u32)\n#define F2FS_IOC_WRITE_CHECKPOINT\t_IO(F2FS_IOCTL_MAGIC, 7)\n#define F2FS_IOC_DEFRAGMENT\t\t_IOWR(F2FS_IOCTL_MAGIC, 8,\t\\\n\t\t\t\t\t\tstruct f2fs_defragment)\n#define F2FS_IOC_MOVE_RANGE\t\t_IOWR(F2FS_IOCTL_MAGIC, 9,\t\\\n\t\t\t\t\t\tstruct f2fs_move_range)\n#define F2FS_IOC_FLUSH_DEVICE\t\t_IOW(F2FS_IOCTL_MAGIC, 10,\t\\\n\t\t\t\t\t\tstruct f2fs_flush_device)\n#define F2FS_IOC_GARBAGE_COLLECT_RANGE\t_IOW(F2FS_IOCTL_MAGIC, 11,\t\\\n\t\t\t\t\t\tstruct f2fs_gc_range)\n#define F2FS_IOC_GET_FEATURES\t\t_IOR(F2FS_IOCTL_MAGIC, 12, __u32)\n#define F2FS_IOC_SET_PIN_FILE\t\t_IOW(F2FS_IOCTL_MAGIC, 13, __u32)\n#define F2FS_IOC_GET_PIN_FILE\t\t_IOR(F2FS_IOCTL_MAGIC, 14, __u32)\n#define F2FS_IOC_PRECACHE_EXTENTS\t_IO(F2FS_IOCTL_MAGIC, 15)\n#define F2FS_IOC_RESIZE_FS\t\t_IOW(F2FS_IOCTL_MAGIC, 16, __u64)\n\n#define F2FS_IOC_SET_ENCRYPTION_POLICY\tFS_IOC_SET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_POLICY\tFS_IOC_GET_ENCRYPTION_POLICY\n#define F2FS_IOC_GET_ENCRYPTION_PWSALT\tFS_IOC_GET_ENCRYPTION_PWSALT\n\n/*\n * should be same as XFS_IOC_GOINGDOWN.\n * Flags for going down operation used by FS_IOC_GOINGDOWN\n */\n#define F2FS_IOC_SHUTDOWN\t_IOR('X', 125, __u32)\t/* Shutdown */\n#define F2FS_GOING_DOWN_FULLSYNC\t0x0\t/* going down with full sync */\n#define F2FS_GOING_DOWN_METASYNC\t0x1\t/* going down with metadata */\n#define F2FS_GOING_DOWN_NOSYNC\t\t0x2\t/* going down */\n#define F2FS_GOING_DOWN_METAFLUSH\t0x3\t/* going down with meta flush */\n#define F2FS_GOING_DOWN_NEED_FSCK\t0x4\t/* going down to trigger fsck */\n\n#if defined(__KERNEL__) && defined(CONFIG_COMPAT)\n/*\n * ioctl commands in 32 bit emulation\n */\n#define F2FS_IOC32_GETFLAGS\t\tFS_IOC32_GETFLAGS\n#define F2FS_IOC32_SETFLAGS\t\tFS_IOC32_SETFLAGS\n#define F2FS_IOC32_GETVERSION\t\tFS_IOC32_GETVERSION\n#endif\n\n#define F2FS_IOC_FSGETXATTR\t\tFS_IOC_FSGETXATTR\n#define F2FS_IOC_FSSETXATTR\t\tFS_IOC_FSSETXATTR\n\nstruct f2fs_gc_range {\n\tu32 sync;\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_defragment {\n\tu64 start;\n\tu64 len;\n};\n\nstruct f2fs_move_range {\n\tu32 dst_fd;\t\t/* destination fd */\n\tu64 pos_in;\t\t/* start position in src_fd */\n\tu64 pos_out;\t\t/* start position in dst_fd */\n\tu64 len;\t\t/* size to move */\n};\n\nstruct f2fs_flush_device {\n\tu32 dev_num;\t\t/* device number to flush */\n\tu32 segments;\t\t/* # of segments to flush */\n};\n\n/* for inline stuff */\n#define DEF_INLINE_RESERVED_SIZE\t1\nstatic inline int get_extra_isize(struct inode *inode);\nstatic inline int get_inline_xattr_addrs(struct inode *inode);\n#define MAX_INLINE_DATA(inode)\t(sizeof(__le32) *\t\t\t\\\n\t\t\t\t(CUR_ADDRS_PER_INODE(inode) -\t\t\\\n\t\t\t\tget_inline_xattr_addrs(inode) -\t\\\n\t\t\t\tDEF_INLINE_RESERVED_SIZE))\n\n/* for inline dir */\n#define NR_INLINE_DENTRY(inode)\t(MAX_INLINE_DATA(inode) * BITS_PER_BYTE / \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tBITS_PER_BYTE + 1))\n#define INLINE_DENTRY_BITMAP_SIZE(inode) \\\n\tDIV_ROUND_UP(NR_INLINE_DENTRY(inode), BITS_PER_BYTE)\n#define INLINE_RESERVED_SIZE(inode)\t(MAX_INLINE_DATA(inode) - \\\n\t\t\t\t((SIZE_OF_DIR_ENTRY + F2FS_SLOT_LEN) * \\\n\t\t\t\tNR_INLINE_DENTRY(inode) + \\\n\t\t\t\tINLINE_DENTRY_BITMAP_SIZE(inode)))\n\n/*\n * For INODE and NODE manager\n */\n/* for directory operations */\nstruct f2fs_dentry_ptr {\n\tstruct inode *inode;\n\tvoid *bitmap;\n\tstruct f2fs_dir_entry *dentry;\n\t__u8 (*filename)[F2FS_SLOT_LEN];\n\tint max;\n\tint nr_bitmap;\n};\n\nstatic inline void make_dentry_ptr_block(struct inode *inode,\n\t\tstruct f2fs_dentry_ptr *d, struct f2fs_dentry_block *t)\n{\n\td->inode = inode;\n\td->max = NR_DENTRY_IN_BLOCK;\n\td->nr_bitmap = SIZE_OF_DENTRY_BITMAP;\n\td->bitmap = t->dentry_bitmap;\n\td->dentry = t->dentry;\n\td->filename = t->filename;\n}\n\nstatic inline void make_dentry_ptr_inline(struct inode *inode,\n\t\t\t\t\tstruct f2fs_dentry_ptr *d, void *t)\n{\n\tint entry_cnt = NR_INLINE_DENTRY(inode);\n\tint bitmap_size = INLINE_DENTRY_BITMAP_SIZE(inode);\n\tint reserved_size = INLINE_RESERVED_SIZE(inode);\n\n\td->inode = inode;\n\td->max = entry_cnt;\n\td->nr_bitmap = bitmap_size;\n\td->bitmap = t;\n\td->dentry = t + bitmap_size + reserved_size;\n\td->filename = t + bitmap_size + reserved_size +\n\t\t\t\t\tSIZE_OF_DIR_ENTRY * entry_cnt;\n}\n\n/*\n * XATTR_NODE_OFFSET stores xattrs to one node block per file keeping -1\n * as its node offset to distinguish from index node blocks.\n * But some bits are used to mark the node block.\n */\n#define XATTR_NODE_OFFSET\t((((unsigned int)-1) << OFFSET_BIT_SHIFT) \\\n\t\t\t\t>> OFFSET_BIT_SHIFT)\nenum {\n\tALLOC_NODE,\t\t\t/* allocate a new node page if needed */\n\tLOOKUP_NODE,\t\t\t/* look up a node without readahead */\n\tLOOKUP_NODE_RA,\t\t\t/*\n\t\t\t\t\t * look up a node with readahead called\n\t\t\t\t\t * by get_data_block.\n\t\t\t\t\t */\n};\n\n#define DEFAULT_RETRY_IO_COUNT\t8\t/* maximum retry read IO count */\n\n/* maximum retry quota flush count */\n#define DEFAULT_RETRY_QUOTA_FLUSH_COUNT\t\t8\n\n#define F2FS_LINK_MAX\t0xffffffff\t/* maximum link count per file */\n\n#define MAX_DIR_RA_PAGES\t4\t/* maximum ra pages of dir */\n\n/* for in-memory extent cache entry */\n#define F2FS_MIN_EXTENT_LEN\t64\t/* minimum extent length */\n\n/* number of extent info in extent cache we try to shrink */\n#define EXTENT_CACHE_SHRINK_NUMBER\t128\n\nstruct rb_entry {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tunsigned int ofs;\t\t/* start offset of the entry */\n\tunsigned int len;\t\t/* length of the entry */\n};\n\nstruct extent_info {\n\tunsigned int fofs;\t\t/* start offset in a file */\n\tunsigned int len;\t\t/* length of the extent */\n\tu32 blk;\t\t\t/* start block address of the extent */\n};\n\nstruct extent_node {\n\tstruct rb_node rb_node;\t\t/* rb node located in rb-tree */\n\tstruct extent_info ei;\t\t/* extent info */\n\tstruct list_head list;\t\t/* node in global extent list of sbi */\n\tstruct extent_tree *et;\t\t/* extent tree pointer */\n};\n\nstruct extent_tree {\n\tnid_t ino;\t\t\t/* inode number */\n\tstruct rb_root_cached root;\t/* root of extent info rb-tree */\n\tstruct extent_node *cached_en;\t/* recently accessed extent node */\n\tstruct extent_info largest;\t/* largested extent info */\n\tstruct list_head list;\t\t/* to be used by sbi->zombie_list */\n\trwlock_t lock;\t\t\t/* protect extent info rb-tree */\n\tatomic_t node_cnt;\t\t/* # of extent node in rb-tree*/\n\tbool largest_updated;\t\t/* largest extent updated */\n};\n\n/*\n * This structure is taken from ext4_map_blocks.\n *\n * Note that, however, f2fs uses NEW and MAPPED flags for f2fs_map_blocks().\n */\n#define F2FS_MAP_NEW\t\t(1 << BH_New)\n#define F2FS_MAP_MAPPED\t\t(1 << BH_Mapped)\n#define F2FS_MAP_UNWRITTEN\t(1 << BH_Unwritten)\n#define F2FS_MAP_FLAGS\t\t(F2FS_MAP_NEW | F2FS_MAP_MAPPED |\\\n\t\t\t\tF2FS_MAP_UNWRITTEN)\n\nstruct f2fs_map_blocks {\n\tblock_t m_pblk;\n\tblock_t m_lblk;\n\tunsigned int m_len;\n\tunsigned int m_flags;\n\tpgoff_t *m_next_pgofs;\t\t/* point next possible non-hole pgofs */\n\tpgoff_t *m_next_extent;\t\t/* point to next possible extent */\n\tint m_seg_type;\n\tbool m_may_create;\t\t/* indicate it is from write path */\n};\n\n/* for flag in get_data_block */\nenum {\n\tF2FS_GET_BLOCK_DEFAULT,\n\tF2FS_GET_BLOCK_FIEMAP,\n\tF2FS_GET_BLOCK_BMAP,\n\tF2FS_GET_BLOCK_DIO,\n\tF2FS_GET_BLOCK_PRE_DIO,\n\tF2FS_GET_BLOCK_PRE_AIO,\n\tF2FS_GET_BLOCK_PRECACHE,\n};\n\n/*\n * i_advise uses FADVISE_XXX_BIT. We can add additional hints later.\n */\n#define FADVISE_COLD_BIT\t0x01\n#define FADVISE_LOST_PINO_BIT\t0x02\n#define FADVISE_ENCRYPT_BIT\t0x04\n#define FADVISE_ENC_NAME_BIT\t0x08\n#define FADVISE_KEEP_SIZE_BIT\t0x10\n#define FADVISE_HOT_BIT\t\t0x20\n#define FADVISE_VERITY_BIT\t0x40\t/* reserved */\n\n#define FADVISE_MODIFIABLE_BITS\t(FADVISE_COLD_BIT | FADVISE_HOT_BIT)\n\n#define file_is_cold(inode)\tis_file(inode, FADVISE_COLD_BIT)\n#define file_wrong_pino(inode)\tis_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_set_cold(inode)\tset_file(inode, FADVISE_COLD_BIT)\n#define file_lost_pino(inode)\tset_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_clear_cold(inode)\tclear_file(inode, FADVISE_COLD_BIT)\n#define file_got_pino(inode)\tclear_file(inode, FADVISE_LOST_PINO_BIT)\n#define file_is_encrypt(inode)\tis_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_set_encrypt(inode)\tset_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_clear_encrypt(inode) clear_file(inode, FADVISE_ENCRYPT_BIT)\n#define file_enc_name(inode)\tis_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_set_enc_name(inode) set_file(inode, FADVISE_ENC_NAME_BIT)\n#define file_keep_isize(inode)\tis_file(inode, FADVISE_KEEP_SIZE_BIT)\n#define file_set_keep_isize(inode) set_file(inode, FADVISE_KEEP_SIZE_BIT)\n#define file_is_hot(inode)\tis_file(inode, FADVISE_HOT_BIT)\n#define file_set_hot(inode)\tset_file(inode, FADVISE_HOT_BIT)\n#define file_clear_hot(inode)\tclear_file(inode, FADVISE_HOT_BIT)\n\n#define DEF_DIR_LEVEL\t\t0\n\nenum {\n\tGC_FAILURE_PIN,\n\tGC_FAILURE_ATOMIC,\n\tMAX_GC_FAILURE\n};\n\nstruct f2fs_inode_info {\n\tstruct inode vfs_inode;\t\t/* serve a vfs inode */\n\tunsigned long i_flags;\t\t/* keep an inode flags for ioctl */\n\tunsigned char i_advise;\t\t/* use to give file attribute hints */\n\tunsigned char i_dir_level;\t/* use for dentry level for large dir */\n\tunsigned int i_current_depth;\t/* only for directory depth */\n\t/* for gc failure statistic */\n\tunsigned int i_gc_failures[MAX_GC_FAILURE];\n\tunsigned int i_pino;\t\t/* parent inode number */\n\tumode_t i_acl_mode;\t\t/* keep file acl mode temporarily */\n\n\t/* Use below internally in f2fs*/\n\tunsigned long flags;\t\t/* use to pass per-file flags */\n\tstruct rw_semaphore i_sem;\t/* protect fi info */\n\tatomic_t dirty_pages;\t\t/* # of dirty pages */\n\tf2fs_hash_t chash;\t\t/* hash value of given file name */\n\tunsigned int clevel;\t\t/* maximum level of given file name */\n\tstruct task_struct *task;\t/* lookup and create consistency */\n\tstruct task_struct *cp_task;\t/* separate cp/wb IO stats*/\n\tnid_t i_xattr_nid;\t\t/* node id that contains xattrs */\n\tloff_t\tlast_disk_size;\t\t/* lastly written file size */\n\n#ifdef CONFIG_QUOTA\n\tstruct dquot *i_dquot[MAXQUOTAS];\n\n\t/* quota space reservation, managed internally by quota code */\n\tqsize_t i_reserved_quota;\n#endif\n\tstruct list_head dirty_list;\t/* dirty list for dirs and files */\n\tstruct list_head gdirty_list;\t/* linked in global dirty list */\n\tstruct list_head inmem_ilist;\t/* list for inmem inodes */\n\tstruct list_head inmem_pages;\t/* inmemory pages managed by f2fs */\n\tstruct task_struct *inmem_task;\t/* store inmemory task */\n\tstruct mutex inmem_lock;\t/* lock for inmemory pages */\n\tstruct extent_tree *extent_tree;\t/* cached extent_tree entry */\n\n\t/* avoid racing between foreground op and gc */\n\tstruct rw_semaphore i_gc_rwsem[2];\n\tstruct rw_semaphore i_mmap_sem;\n\tstruct rw_semaphore i_xattr_sem; /* avoid racing between reading and changing EAs */\n\n\tint i_extra_isize;\t\t/* size of extra space located in i_addr */\n\tkprojid_t i_projid;\t\t/* id for project quota */\n\tint i_inline_xattr_size;\t/* inline xattr size */\n\tstruct timespec64 i_crtime;\t/* inode creation time */\n\tstruct timespec64 i_disk_time[4];/* inode disk times */\n};\n\nstatic inline void get_extent_info(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\text->fofs = le32_to_cpu(i_ext->fofs);\n\text->blk = le32_to_cpu(i_ext->blk);\n\text->len = le32_to_cpu(i_ext->len);\n}\n\nstatic inline void set_raw_extent(struct extent_info *ext,\n\t\t\t\t\tstruct f2fs_extent *i_ext)\n{\n\ti_ext->fofs = cpu_to_le32(ext->fofs);\n\ti_ext->blk = cpu_to_le32(ext->blk);\n\ti_ext->len = cpu_to_le32(ext->len);\n}\n\nstatic inline void set_extent_info(struct extent_info *ei, unsigned int fofs,\n\t\t\t\t\t\tu32 blk, unsigned int len)\n{\n\tei->fofs = fofs;\n\tei->blk = blk;\n\tei->len = len;\n}\n\nstatic inline bool __is_discard_mergeable(struct discard_info *back,\n\t\t\tstruct discard_info *front, unsigned int max_len)\n{\n\treturn (back->lstart + back->len == front->lstart) &&\n\t\t(back->len + front->len <= max_len);\n}\n\nstatic inline bool __is_discard_back_mergeable(struct discard_info *cur,\n\t\t\tstruct discard_info *back, unsigned int max_len)\n{\n\treturn __is_discard_mergeable(back, cur, max_len);\n}\n\nstatic inline bool __is_discard_front_mergeable(struct discard_info *cur,\n\t\t\tstruct discard_info *front, unsigned int max_len)\n{\n\treturn __is_discard_mergeable(cur, front, max_len);\n}\n\nstatic inline bool __is_extent_mergeable(struct extent_info *back,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn (back->fofs + back->len == front->fofs &&\n\t\t\tback->blk + back->len == front->blk);\n}\n\nstatic inline bool __is_back_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *back)\n{\n\treturn __is_extent_mergeable(back, cur);\n}\n\nstatic inline bool __is_front_mergeable(struct extent_info *cur,\n\t\t\t\t\t\tstruct extent_info *front)\n{\n\treturn __is_extent_mergeable(cur, front);\n}\n\nextern void f2fs_mark_inode_dirty_sync(struct inode *inode, bool sync);\nstatic inline void __try_update_largest_extent(struct extent_tree *et,\n\t\t\t\t\t\tstruct extent_node *en)\n{\n\tif (en->ei.len > et->largest.len) {\n\t\tet->largest = en->ei;\n\t\tet->largest_updated = true;\n\t}\n}\n\n/*\n * For free nid management\n */\nenum nid_state {\n\tFREE_NID,\t\t/* newly added to free nid list */\n\tPREALLOC_NID,\t\t/* it is preallocated */\n\tMAX_NID_STATE,\n};\n\nstruct f2fs_nm_info {\n\tblock_t nat_blkaddr;\t\t/* base disk address of NAT */\n\tnid_t max_nid;\t\t\t/* maximum possible node ids */\n\tnid_t available_nids;\t\t/* # of available node ids */\n\tnid_t next_scan_nid;\t\t/* the next nid to be scanned */\n\tunsigned int ram_thresh;\t/* control the memory footprint */\n\tunsigned int ra_nid_pages;\t/* # of nid pages to be readaheaded */\n\tunsigned int dirty_nats_ratio;\t/* control dirty nats ratio threshold */\n\n\t/* NAT cache management */\n\tstruct radix_tree_root nat_root;/* root of the nat entry cache */\n\tstruct radix_tree_root nat_set_root;/* root of the nat set cache */\n\tstruct rw_semaphore nat_tree_lock;\t/* protect nat_tree_lock */\n\tstruct list_head nat_entries;\t/* cached nat entry list (clean) */\n\tspinlock_t nat_list_lock;\t/* protect clean nat entry list */\n\tunsigned int nat_cnt;\t\t/* the # of cached nat entries */\n\tunsigned int dirty_nat_cnt;\t/* total num of nat entries in set */\n\tunsigned int nat_blocks;\t/* # of nat blocks */\n\n\t/* free node ids management */\n\tstruct radix_tree_root free_nid_root;/* root of the free_nid cache */\n\tstruct list_head free_nid_list;\t\t/* list for free nids excluding preallocated nids */\n\tunsigned int nid_cnt[MAX_NID_STATE];\t/* the number of free node id */\n\tspinlock_t nid_list_lock;\t/* protect nid lists ops */\n\tstruct mutex build_lock;\t/* lock for build free nids */\n\tunsigned char **free_nid_bitmap;\n\tunsigned char *nat_block_bitmap;\n\tunsigned short *free_nid_count;\t/* free nid count of NAT block */\n\n\t/* for checkpoint */\n\tchar *nat_bitmap;\t\t/* NAT bitmap pointer */\n\n\tunsigned int nat_bits_blocks;\t/* # of nat bits blocks */\n\tunsigned char *nat_bits;\t/* NAT bits blocks */\n\tunsigned char *full_nat_bits;\t/* full NAT pages */\n\tunsigned char *empty_nat_bits;\t/* empty NAT pages */\n#ifdef CONFIG_F2FS_CHECK_FS\n\tchar *nat_bitmap_mir;\t\t/* NAT bitmap mirror */\n#endif\n\tint bitmap_size;\t\t/* bitmap size */\n};\n\n/*\n * this structure is used as one of function parameters.\n * all the information are dedicated to a given direct node block determined\n * by the data offset in a file.\n */\nstruct dnode_of_data {\n\tstruct inode *inode;\t\t/* vfs inode pointer */\n\tstruct page *inode_page;\t/* its inode page, NULL is possible */\n\tstruct page *node_page;\t\t/* cached direct node page */\n\tnid_t nid;\t\t\t/* node id of the direct node block */\n\tunsigned int ofs_in_node;\t/* data offset in the node page */\n\tbool inode_page_locked;\t\t/* inode page is locked or not */\n\tbool node_changed;\t\t/* is node block changed */\n\tchar cur_level;\t\t\t/* level of hole node page */\n\tchar max_level;\t\t\t/* level of current page located */\n\tblock_t\tdata_blkaddr;\t\t/* block address of the node block */\n};\n\nstatic inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,\n\t\tstruct page *ipage, struct page *npage, nid_t nid)\n{\n\tmemset(dn, 0, sizeof(*dn));\n\tdn->inode = inode;\n\tdn->inode_page = ipage;\n\tdn->node_page = npage;\n\tdn->nid = nid;\n}\n\n/*\n * For SIT manager\n *\n * By default, there are 6 active log areas across the whole main area.\n * When considering hot and cold data separation to reduce cleaning overhead,\n * we split 3 for data logs and 3 for node logs as hot, warm, and cold types,\n * respectively.\n * In the current design, you should not change the numbers intentionally.\n * Instead, as a mount option such as active_logs=x, you can use 2, 4, and 6\n * logs individually according to the underlying devices. (default: 6)\n * Just in case, on-disk layout covers maximum 16 logs that consist of 8 for\n * data and 8 for node logs.\n */\n#define\tNR_CURSEG_DATA_TYPE\t(3)\n#define NR_CURSEG_NODE_TYPE\t(3)\n#define NR_CURSEG_TYPE\t(NR_CURSEG_DATA_TYPE + NR_CURSEG_NODE_TYPE)\n\nenum {\n\tCURSEG_HOT_DATA\t= 0,\t/* directory entry blocks */\n\tCURSEG_WARM_DATA,\t/* data blocks */\n\tCURSEG_COLD_DATA,\t/* multimedia or GCed data blocks */\n\tCURSEG_HOT_NODE,\t/* direct node blocks of directory files */\n\tCURSEG_WARM_NODE,\t/* direct node blocks of normal files */\n\tCURSEG_COLD_NODE,\t/* indirect node blocks */\n\tNO_CHECK_TYPE,\n};\n\nstruct flush_cmd {\n\tstruct completion wait;\n\tstruct llist_node llnode;\n\tnid_t ino;\n\tint ret;\n};\n\nstruct flush_cmd_control {\n\tstruct task_struct *f2fs_issue_flush;\t/* flush thread */\n\twait_queue_head_t flush_wait_queue;\t/* waiting queue for wake-up */\n\tatomic_t issued_flush;\t\t\t/* # of issued flushes */\n\tatomic_t queued_flush;\t\t\t/* # of queued flushes */\n\tstruct llist_head issue_list;\t\t/* list for command issue */\n\tstruct llist_node *dispatch_list;\t/* list for command dispatch */\n};\n\nstruct f2fs_sm_info {\n\tstruct sit_info *sit_info;\t\t/* whole segment information */\n\tstruct free_segmap_info *free_info;\t/* free segment information */\n\tstruct dirty_seglist_info *dirty_info;\t/* dirty segment information */\n\tstruct curseg_info *curseg_array;\t/* active segment information */\n\n\tstruct rw_semaphore curseg_lock;\t/* for preventing curseg change */\n\n\tblock_t seg0_blkaddr;\t\t/* block address of 0'th segment */\n\tblock_t main_blkaddr;\t\t/* start block address of main area */\n\tblock_t ssa_blkaddr;\t\t/* start block address of SSA area */\n\n\tunsigned int segment_count;\t/* total # of segments */\n\tunsigned int main_segments;\t/* # of segments in main area */\n\tunsigned int reserved_segments;\t/* # of reserved segments */\n\tunsigned int ovp_segments;\t/* # of overprovision segments */\n\n\t/* a threshold to reclaim prefree segments */\n\tunsigned int rec_prefree_segments;\n\n\t/* for batched trimming */\n\tunsigned int trim_sections;\t\t/* # of sections to trim */\n\n\tstruct list_head sit_entry_set;\t/* sit entry set list */\n\n\tunsigned int ipu_policy;\t/* in-place-update policy */\n\tunsigned int min_ipu_util;\t/* in-place-update threshold */\n\tunsigned int min_fsync_blocks;\t/* threshold for fsync */\n\tunsigned int min_seq_blocks;\t/* threshold for sequential blocks */\n\tunsigned int min_hot_blocks;\t/* threshold for hot block allocation */\n\tunsigned int min_ssr_sections;\t/* threshold to trigger SSR allocation */\n\n\t/* for flush command control */\n\tstruct flush_cmd_control *fcc_info;\n\n\t/* for discard command control */\n\tstruct discard_cmd_control *dcc_info;\n};\n\n/*\n * For superblock\n */\n/*\n * COUNT_TYPE for monitoring\n *\n * f2fs monitors the number of several block types such as on-writeback,\n * dirty dentry blocks, dirty node blocks, and dirty meta blocks.\n */\n#define WB_DATA_TYPE(p)\t(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)\nenum count_type {\n\tF2FS_DIRTY_DENTS,\n\tF2FS_DIRTY_DATA,\n\tF2FS_DIRTY_QDATA,\n\tF2FS_DIRTY_NODES,\n\tF2FS_DIRTY_META,\n\tF2FS_INMEM_PAGES,\n\tF2FS_DIRTY_IMETA,\n\tF2FS_WB_CP_DATA,\n\tF2FS_WB_DATA,\n\tF2FS_RD_DATA,\n\tF2FS_RD_NODE,\n\tF2FS_RD_META,\n\tF2FS_DIO_WRITE,\n\tF2FS_DIO_READ,\n\tNR_COUNT_TYPE,\n};\n\n/*\n * The below are the page types of bios used in submit_bio().\n * The available types are:\n * DATA\t\t\tUser data pages. It operates as async mode.\n * NODE\t\t\tNode pages. It operates as async mode.\n * META\t\t\tFS metadata pages such as SIT, NAT, CP.\n * NR_PAGE_TYPE\t\tThe number of page types.\n * META_FLUSH\t\tMake sure the previous pages are written\n *\t\t\twith waiting the bio's completion\n * ...\t\t\tOnly can be used with META.\n */\n#define PAGE_TYPE_OF_BIO(type)\t((type) > META ? META : (type))\nenum page_type {\n\tDATA,\n\tNODE,\n\tMETA,\n\tNR_PAGE_TYPE,\n\tMETA_FLUSH,\n\tINMEM,\t\t/* the below types are used by tracepoints only. */\n\tINMEM_DROP,\n\tINMEM_INVALIDATE,\n\tINMEM_REVOKE,\n\tIPU,\n\tOPU,\n};\n\nenum temp_type {\n\tHOT = 0,\t/* must be zero for meta bio */\n\tWARM,\n\tCOLD,\n\tNR_TEMP_TYPE,\n};\n\nenum need_lock_type {\n\tLOCK_REQ = 0,\n\tLOCK_DONE,\n\tLOCK_RETRY,\n};\n\nenum cp_reason_type {\n\tCP_NO_NEEDED,\n\tCP_NON_REGULAR,\n\tCP_HARDLINK,\n\tCP_SB_NEED_CP,\n\tCP_WRONG_PINO,\n\tCP_NO_SPC_ROLL,\n\tCP_NODE_NEED_CP,\n\tCP_FASTBOOT_MODE,\n\tCP_SPEC_LOG_NUM,\n\tCP_RECOVER_DIR,\n};\n\nenum iostat_type {\n\tAPP_DIRECT_IO,\t\t\t/* app direct IOs */\n\tAPP_BUFFERED_IO,\t\t/* app buffered IOs */\n\tAPP_WRITE_IO,\t\t\t/* app write IOs */\n\tAPP_MAPPED_IO,\t\t\t/* app mapped IOs */\n\tFS_DATA_IO,\t\t\t/* data IOs from kworker/fsync/reclaimer */\n\tFS_NODE_IO,\t\t\t/* node IOs from kworker/fsync/reclaimer */\n\tFS_META_IO,\t\t\t/* meta IOs from kworker/reclaimer */\n\tFS_GC_DATA_IO,\t\t\t/* data IOs from forground gc */\n\tFS_GC_NODE_IO,\t\t\t/* node IOs from forground gc */\n\tFS_CP_DATA_IO,\t\t\t/* data IOs from checkpoint */\n\tFS_CP_NODE_IO,\t\t\t/* node IOs from checkpoint */\n\tFS_CP_META_IO,\t\t\t/* meta IOs from checkpoint */\n\tFS_DISCARD,\t\t\t/* discard */\n\tNR_IO_TYPE,\n};\n\nstruct f2fs_io_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs_sb_info pointer */\n\tnid_t ino;\t\t/* inode number */\n\tenum page_type type;\t/* contains DATA/NODE/META/META_FLUSH */\n\tenum temp_type temp;\t/* contains HOT/WARM/COLD */\n\tint op;\t\t\t/* contains REQ_OP_ */\n\tint op_flags;\t\t/* req_flag_bits */\n\tblock_t new_blkaddr;\t/* new block address to be written */\n\tblock_t old_blkaddr;\t/* old block address before Cow */\n\tstruct page *page;\t/* page to be written */\n\tstruct page *encrypted_page;\t/* encrypted page */\n\tstruct list_head list;\t\t/* serialize IOs */\n\tbool submitted;\t\t/* indicate IO submission */\n\tint need_lock;\t\t/* indicate we need to lock cp_rwsem */\n\tbool in_list;\t\t/* indicate fio is in io_list */\n\tbool is_por;\t\t/* indicate IO is from recovery or not */\n\tbool retry;\t\t/* need to reallocate block address */\n\tenum iostat_type io_type;\t/* io type */\n\tstruct writeback_control *io_wbc; /* writeback control */\n\tstruct bio **bio;\t\t/* bio for ipu */\n\tsector_t *last_block;\t\t/* last block number in bio */\n\tunsigned char version;\t\t/* version of the node */\n};\n\n#define is_read_io(rw) ((rw) == READ)\nstruct f2fs_bio_info {\n\tstruct f2fs_sb_info *sbi;\t/* f2fs superblock */\n\tstruct bio *bio;\t\t/* bios to merge */\n\tsector_t last_block_in_bio;\t/* last block number */\n\tstruct f2fs_io_info fio;\t/* store buffered io info. */\n\tstruct rw_semaphore io_rwsem;\t/* blocking op for bio */\n\tspinlock_t io_lock;\t\t/* serialize DATA/NODE IOs */\n\tstruct list_head io_list;\t/* track fios */\n};\n\n#define FDEV(i)\t\t\t\t(sbi->devs[i])\n#define RDEV(i)\t\t\t\t(raw_super->devs[i])\nstruct f2fs_dev_info {\n\tstruct block_device *bdev;\n\tchar path[MAX_PATH_LEN];\n\tunsigned int total_segments;\n\tblock_t start_blk;\n\tblock_t end_blk;\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int nr_blkz;\t\t/* Total number of zones */\n\tunsigned long *blkz_seq;\t/* Bitmap indicating sequential zones */\n#endif\n};\n\nenum inode_type {\n\tDIR_INODE,\t\t\t/* for dirty dir inode */\n\tFILE_INODE,\t\t\t/* for dirty regular/symlink inode */\n\tDIRTY_META,\t\t\t/* for all dirtied inode metadata */\n\tATOMIC_FILE,\t\t\t/* for all atomic files */\n\tNR_INODE_TYPE,\n};\n\n/* for inner inode cache management */\nstruct inode_management {\n\tstruct radix_tree_root ino_root;\t/* ino entry array */\n\tspinlock_t ino_lock;\t\t\t/* for ino entry lock */\n\tstruct list_head ino_list;\t\t/* inode list head */\n\tunsigned long ino_num;\t\t\t/* number of entries */\n};\n\n/* For s_flag in struct f2fs_sb_info */\nenum {\n\tSBI_IS_DIRTY,\t\t\t\t/* dirty flag for checkpoint */\n\tSBI_IS_CLOSE,\t\t\t\t/* specify unmounting */\n\tSBI_NEED_FSCK,\t\t\t\t/* need fsck.f2fs to fix */\n\tSBI_POR_DOING,\t\t\t\t/* recovery is doing or not */\n\tSBI_NEED_SB_WRITE,\t\t\t/* need to recover superblock */\n\tSBI_NEED_CP,\t\t\t\t/* need to checkpoint */\n\tSBI_IS_SHUTDOWN,\t\t\t/* shutdown by ioctl */\n\tSBI_IS_RECOVERED,\t\t\t/* recovered orphan/data */\n\tSBI_CP_DISABLED,\t\t\t/* CP was disabled last mount */\n\tSBI_CP_DISABLED_QUICK,\t\t\t/* CP was disabled quickly */\n\tSBI_QUOTA_NEED_FLUSH,\t\t\t/* need to flush quota info in CP */\n\tSBI_QUOTA_SKIP_FLUSH,\t\t\t/* skip flushing quota in current CP */\n\tSBI_QUOTA_NEED_REPAIR,\t\t\t/* quota file may be corrupted */\n\tSBI_IS_RESIZEFS,\t\t\t/* resizefs is in process */\n};\n\nenum {\n\tCP_TIME,\n\tREQ_TIME,\n\tDISCARD_TIME,\n\tGC_TIME,\n\tDISABLE_TIME,\n\tUMOUNT_DISCARD_TIMEOUT,\n\tMAX_TIME,\n};\n\nenum {\n\tGC_NORMAL,\n\tGC_IDLE_CB,\n\tGC_IDLE_GREEDY,\n\tGC_URGENT,\n};\n\nenum {\n\tWHINT_MODE_OFF,\t\t/* not pass down write hints */\n\tWHINT_MODE_USER,\t/* try to pass down hints given by users */\n\tWHINT_MODE_FS,\t\t/* pass down hints with F2FS policy */\n};\n\nenum {\n\tALLOC_MODE_DEFAULT,\t/* stay default */\n\tALLOC_MODE_REUSE,\t/* reuse segments as much as possible */\n};\n\nenum fsync_mode {\n\tFSYNC_MODE_POSIX,\t/* fsync follows posix semantics */\n\tFSYNC_MODE_STRICT,\t/* fsync behaves in line with ext4 */\n\tFSYNC_MODE_NOBARRIER,\t/* fsync behaves nobarrier based on posix */\n};\n\n#ifdef CONFIG_FS_ENCRYPTION\n#define DUMMY_ENCRYPTION_ENABLED(sbi) \\\n\t\t\t(unlikely(F2FS_OPTION(sbi).test_dummy_encryption))\n#else\n#define DUMMY_ENCRYPTION_ENABLED(sbi) (0)\n#endif\n\nstruct f2fs_sb_info {\n\tstruct super_block *sb;\t\t\t/* pointer to VFS super block */\n\tstruct proc_dir_entry *s_proc;\t\t/* proc entry */\n\tstruct f2fs_super_block *raw_super;\t/* raw super block pointer */\n\tstruct rw_semaphore sb_lock;\t\t/* lock for raw super block */\n\tint valid_super_block;\t\t\t/* valid super block no */\n\tunsigned long s_flag;\t\t\t\t/* flags for sbi */\n\tstruct mutex writepages;\t\t/* mutex for writepages() */\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tunsigned int blocks_per_blkz;\t\t/* F2FS blocks per zone */\n\tunsigned int log_blocks_per_blkz;\t/* log2 F2FS blocks per zone */\n#endif\n\n\t/* for node-related operations */\n\tstruct f2fs_nm_info *nm_info;\t\t/* node manager */\n\tstruct inode *node_inode;\t\t/* cache node blocks */\n\n\t/* for segment-related operations */\n\tstruct f2fs_sm_info *sm_info;\t\t/* segment manager */\n\n\t/* for bio operations */\n\tstruct f2fs_bio_info *write_io[NR_PAGE_TYPE];\t/* for write bios */\n\t/* keep migration IO order for LFS mode */\n\tstruct rw_semaphore io_order_lock;\n\tmempool_t *write_io_dummy;\t\t/* Dummy pages */\n\n\t/* for checkpoint */\n\tstruct f2fs_checkpoint *ckpt;\t\t/* raw checkpoint pointer */\n\tint cur_cp_pack;\t\t\t/* remain current cp pack */\n\tspinlock_t cp_lock;\t\t\t/* for flag in ckpt */\n\tstruct inode *meta_inode;\t\t/* cache meta blocks */\n\tstruct mutex cp_mutex;\t\t\t/* checkpoint procedure lock */\n\tstruct rw_semaphore cp_rwsem;\t\t/* blocking FS operations */\n\tstruct rw_semaphore node_write;\t\t/* locking node writes */\n\tstruct rw_semaphore node_change;\t/* locking node change */\n\twait_queue_head_t cp_wait;\n\tunsigned long last_time[MAX_TIME];\t/* to store time in jiffies */\n\tlong interval_time[MAX_TIME];\t\t/* to store thresholds */\n\n\tstruct inode_management im[MAX_INO_ENTRY];      /* manage inode cache */\n\n\tspinlock_t fsync_node_lock;\t\t/* for node entry lock */\n\tstruct list_head fsync_node_list;\t/* node list head */\n\tunsigned int fsync_seg_id;\t\t/* sequence id */\n\tunsigned int fsync_node_num;\t\t/* number of node entries */\n\n\t/* for orphan inode, use 0'th array */\n\tunsigned int max_orphans;\t\t/* max orphan inodes */\n\n\t/* for inode management */\n\tstruct list_head inode_list[NR_INODE_TYPE];\t/* dirty inode list */\n\tspinlock_t inode_lock[NR_INODE_TYPE];\t/* for dirty inode list lock */\n\tstruct mutex flush_lock;\t\t/* for flush exclusion */\n\n\t/* for extent tree cache */\n\tstruct radix_tree_root extent_tree_root;/* cache extent cache entries */\n\tstruct mutex extent_tree_lock;\t/* locking extent radix tree */\n\tstruct list_head extent_list;\t\t/* lru list for shrinker */\n\tspinlock_t extent_lock;\t\t\t/* locking extent lru list */\n\tatomic_t total_ext_tree;\t\t/* extent tree count */\n\tstruct list_head zombie_list;\t\t/* extent zombie tree list */\n\tatomic_t total_zombie_tree;\t\t/* extent zombie tree count */\n\tatomic_t total_ext_node;\t\t/* extent info count */\n\n\t/* basic filesystem units */\n\tunsigned int log_sectors_per_block;\t/* log2 sectors per block */\n\tunsigned int log_blocksize;\t\t/* log2 block size */\n\tunsigned int blocksize;\t\t\t/* block size */\n\tunsigned int root_ino_num;\t\t/* root inode number*/\n\tunsigned int node_ino_num;\t\t/* node inode number*/\n\tunsigned int meta_ino_num;\t\t/* meta inode number*/\n\tunsigned int log_blocks_per_seg;\t/* log2 blocks per segment */\n\tunsigned int blocks_per_seg;\t\t/* blocks per segment */\n\tunsigned int segs_per_sec;\t\t/* segments per section */\n\tunsigned int secs_per_zone;\t\t/* sections per zone */\n\tunsigned int total_sections;\t\t/* total section count */\n\tstruct mutex resize_mutex;\t\t/* for resize exclusion */\n\tunsigned int total_node_count;\t\t/* total node block count */\n\tunsigned int total_valid_node_count;\t/* valid node block count */\n\tloff_t max_file_blocks;\t\t\t/* max block index of file */\n\tint dir_level;\t\t\t\t/* directory level */\n\tint readdir_ra;\t\t\t\t/* readahead inode in readdir */\n\n\tblock_t user_block_count;\t\t/* # of user blocks */\n\tblock_t total_valid_block_count;\t/* # of valid blocks */\n\tblock_t discard_blks;\t\t\t/* discard command candidats */\n\tblock_t last_valid_block_count;\t\t/* for recovery */\n\tblock_t reserved_blocks;\t\t/* configurable reserved blocks */\n\tblock_t current_reserved_blocks;\t/* current reserved blocks */\n\n\t/* Additional tracking for no checkpoint mode */\n\tblock_t unusable_block_count;\t\t/* # of blocks saved by last cp */\n\n\tunsigned int nquota_files;\t\t/* # of quota sysfile */\n\tstruct rw_semaphore quota_sem;\t\t/* blocking cp for flags */\n\n\t/* # of pages, see count_type */\n\tatomic_t nr_pages[NR_COUNT_TYPE];\n\t/* # of allocated blocks */\n\tstruct percpu_counter alloc_valid_block_count;\n\n\t/* writeback control */\n\tatomic_t wb_sync_req[META];\t/* count # of WB_SYNC threads */\n\n\t/* valid inode count */\n\tstruct percpu_counter total_valid_inode_count;\n\n\tstruct f2fs_mount_info mount_opt;\t/* mount options */\n\n\t/* for cleaning operations */\n\tstruct mutex gc_mutex;\t\t\t/* mutex for GC */\n\tstruct f2fs_gc_kthread\t*gc_thread;\t/* GC thread */\n\tunsigned int cur_victim_sec;\t\t/* current victim section num */\n\tunsigned int gc_mode;\t\t\t/* current GC state */\n\tunsigned int next_victim_seg[2];\t/* next segment in victim section */\n\t/* for skip statistic */\n\tunsigned long long skipped_atomic_files[2];\t/* FG_GC and BG_GC */\n\tunsigned long long skipped_gc_rwsem;\t\t/* FG_GC only */\n\n\t/* threshold for gc trials on pinned files */\n\tu64 gc_pin_file_threshold;\n\n\t/* maximum # of trials to find a victim segment for SSR and GC */\n\tunsigned int max_victim_search;\n\t/* migration granularity of garbage collection, unit: segment */\n\tunsigned int migration_granularity;\n\n\t/*\n\t * for stat information.\n\t * one is for the LFS mode, and the other is for the SSR mode.\n\t */\n#ifdef CONFIG_F2FS_STAT_FS\n\tstruct f2fs_stat_info *stat_info;\t/* FS status information */\n\tatomic_t meta_count[META_MAX];\t\t/* # of meta blocks */\n\tunsigned int segment_count[2];\t\t/* # of allocated segments */\n\tunsigned int block_count[2];\t\t/* # of allocated blocks */\n\tatomic_t inplace_count;\t\t/* # of inplace update */\n\tatomic64_t total_hit_ext;\t\t/* # of lookup extent cache */\n\tatomic64_t read_hit_rbtree;\t\t/* # of hit rbtree extent node */\n\tatomic64_t read_hit_largest;\t\t/* # of hit largest extent node */\n\tatomic64_t read_hit_cached;\t\t/* # of hit cached extent node */\n\tatomic_t inline_xattr;\t\t\t/* # of inline_xattr inodes */\n\tatomic_t inline_inode;\t\t\t/* # of inline_data inodes */\n\tatomic_t inline_dir;\t\t\t/* # of inline_dentry inodes */\n\tatomic_t aw_cnt;\t\t\t/* # of atomic writes */\n\tatomic_t vw_cnt;\t\t\t/* # of volatile writes */\n\tatomic_t max_aw_cnt;\t\t\t/* max # of atomic writes */\n\tatomic_t max_vw_cnt;\t\t\t/* max # of volatile writes */\n\tint bg_gc;\t\t\t\t/* background gc calls */\n\tunsigned int io_skip_bggc;\t\t/* skip background gc for in-flight IO */\n\tunsigned int other_skip_bggc;\t\t/* skip background gc for other reasons */\n\tunsigned int ndirty_inode[NR_INODE_TYPE];\t/* # of dirty inodes */\n#endif\n\tspinlock_t stat_lock;\t\t\t/* lock for stat operations */\n\n\t/* For app/fs IO statistics */\n\tspinlock_t iostat_lock;\n\tunsigned long long write_iostat[NR_IO_TYPE];\n\tbool iostat_enable;\n\n\t/* For sysfs suppport */\n\tstruct kobject s_kobj;\n\tstruct completion s_kobj_unregister;\n\n\t/* For shrinker support */\n\tstruct list_head s_list;\n\tint s_ndevs;\t\t\t\t/* number of devices */\n\tstruct f2fs_dev_info *devs;\t\t/* for device list */\n\tunsigned int dirty_device;\t\t/* for checkpoint data flush */\n\tspinlock_t dev_lock;\t\t\t/* protect dirty_device */\n\tstruct mutex umount_mutex;\n\tunsigned int shrinker_run_no;\n\n\t/* For write statistics */\n\tu64 sectors_written_start;\n\tu64 kbytes_written;\n\n\t/* Reference to checksum algorithm driver via cryptoapi */\n\tstruct crypto_shash *s_chksum_driver;\n\n\t/* Precomputed FS UUID checksum for seeding other checksums */\n\t__u32 s_chksum_seed;\n};\n\nstruct f2fs_private_dio {\n\tstruct inode *inode;\n\tvoid *orig_private;\n\tbio_end_io_t *orig_end_io;\n\tbool write;\n};\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\n#define f2fs_show_injection_info(type)\t\t\t\t\t\\\n\tprintk_ratelimited(\"%sF2FS-fs : inject %s in %s of %pS\\n\",\t\\\n\t\tKERN_INFO, f2fs_fault_name[type],\t\t\t\\\n\t\t__func__, __builtin_return_address(0))\nstatic inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_fault_info *ffi = &F2FS_OPTION(sbi).fault_info;\n\n\tif (!ffi->inject_rate)\n\t\treturn false;\n\n\tif (!IS_FAULT_SET(ffi, type))\n\t\treturn false;\n\n\tatomic_inc(&ffi->inject_ops);\n\tif (atomic_read(&ffi->inject_ops) >= ffi->inject_rate) {\n\t\tatomic_set(&ffi->inject_ops, 0);\n\t\treturn true;\n\t}\n\treturn false;\n}\n#else\n#define f2fs_show_injection_info(type) do { } while (0)\nstatic inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Test if the mounted volume is a multi-device volume.\n *   - For a single regular disk volume, sbi->s_ndevs is 0.\n *   - For a single zoned disk volume, sbi->s_ndevs is 1.\n *   - For a multi-device volume, sbi->s_ndevs is always 2 or more.\n */\nstatic inline bool f2fs_is_multi_device(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->s_ndevs > 1;\n}\n\n/* For write statistics. Suppose sector size is 512 bytes,\n * and the return value is in kbytes. s is of struct f2fs_sb_info.\n */\n#define BD_PART_WRITTEN(s)\t\t\t\t\t\t \\\n(((u64)part_stat_read((s)->sb->s_bdev->bd_part, sectors[STAT_WRITE]) -   \\\n\t\t(s)->sectors_written_start) >> 1)\n\nstatic inline void f2fs_update_time(struct f2fs_sb_info *sbi, int type)\n{\n\tunsigned long now = jiffies;\n\n\tsbi->last_time[type] = now;\n\n\t/* DISCARD_TIME and GC_TIME are based on REQ_TIME */\n\tif (type == REQ_TIME) {\n\t\tsbi->last_time[DISCARD_TIME] = now;\n\t\tsbi->last_time[GC_TIME] = now;\n\t}\n}\n\nstatic inline bool f2fs_time_over(struct f2fs_sb_info *sbi, int type)\n{\n\tunsigned long interval = sbi->interval_time[type] * HZ;\n\n\treturn time_after(jiffies, sbi->last_time[type] + interval);\n}\n\nstatic inline unsigned int f2fs_time_to_wait(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type)\n{\n\tunsigned long interval = sbi->interval_time[type] * HZ;\n\tunsigned int wait_ms = 0;\n\tlong delta;\n\n\tdelta = (sbi->last_time[type] + interval) - jiffies;\n\tif (delta > 0)\n\t\twait_ms = jiffies_to_msecs(delta);\n\n\treturn wait_ms;\n}\n\n/*\n * Inline functions\n */\nstatic inline u32 __f2fs_crc32(struct f2fs_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\tstruct {\n\t\tstruct shash_desc shash;\n\t\tchar ctx[4];\n\t} desc;\n\tint err;\n\n\tBUG_ON(crypto_shash_descsize(sbi->s_chksum_driver) != sizeof(desc.ctx));\n\n\tdesc.shash.tfm = sbi->s_chksum_driver;\n\t*(u32 *)desc.ctx = crc;\n\n\terr = crypto_shash_update(&desc.shash, address, length);\n\tBUG_ON(err);\n\n\treturn *(u32 *)desc.ctx;\n}\n\nstatic inline u32 f2fs_crc32(struct f2fs_sb_info *sbi, const void *address,\n\t\t\t   unsigned int length)\n{\n\treturn __f2fs_crc32(sbi, F2FS_SUPER_MAGIC, address, length);\n}\n\nstatic inline bool f2fs_crc_valid(struct f2fs_sb_info *sbi, __u32 blk_crc,\n\t\t\t\t  void *buf, size_t buf_size)\n{\n\treturn f2fs_crc32(sbi, buf, buf_size) == blk_crc;\n}\n\nstatic inline u32 f2fs_chksum(struct f2fs_sb_info *sbi, u32 crc,\n\t\t\t      const void *address, unsigned int length)\n{\n\treturn __f2fs_crc32(sbi, crc, address, length);\n}\n\nstatic inline struct f2fs_inode_info *F2FS_I(struct inode *inode)\n{\n\treturn container_of(inode, struct f2fs_inode_info, vfs_inode);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstatic inline struct f2fs_sb_info *F2FS_I_SB(struct inode *inode)\n{\n\treturn F2FS_SB(inode->i_sb);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_M_SB(struct address_space *mapping)\n{\n\treturn F2FS_I_SB(mapping->host);\n}\n\nstatic inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page_file_mapping(page));\n}\n\nstatic inline struct f2fs_super_block *F2FS_RAW_SUPER(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_super_block *)(sbi->raw_super);\n}\n\nstatic inline struct f2fs_checkpoint *F2FS_CKPT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_checkpoint *)(sbi->ckpt);\n}\n\nstatic inline struct f2fs_node *F2FS_NODE(struct page *page)\n{\n\treturn (struct f2fs_node *)page_address(page);\n}\n\nstatic inline struct f2fs_inode *F2FS_INODE(struct page *page)\n{\n\treturn &((struct f2fs_node *)page_address(page))->i;\n}\n\nstatic inline struct f2fs_nm_info *NM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_nm_info *)(sbi->nm_info);\n}\n\nstatic inline struct f2fs_sm_info *SM_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_sm_info *)(sbi->sm_info);\n}\n\nstatic inline struct sit_info *SIT_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct sit_info *)(SM_I(sbi)->sit_info);\n}\n\nstatic inline struct free_segmap_info *FREE_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct free_segmap_info *)(SM_I(sbi)->free_info);\n}\n\nstatic inline struct dirty_seglist_info *DIRTY_I(struct f2fs_sb_info *sbi)\n{\n\treturn (struct dirty_seglist_info *)(SM_I(sbi)->dirty_info);\n}\n\nstatic inline struct address_space *META_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->meta_inode->i_mapping;\n}\n\nstatic inline struct address_space *NODE_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->node_inode->i_mapping;\n}\n\nstatic inline bool is_sbi_flag_set(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\treturn test_bit(type, &sbi->s_flag);\n}\n\nstatic inline void set_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tset_bit(type, &sbi->s_flag);\n}\n\nstatic inline void clear_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)\n{\n\tclear_bit(type, &sbi->s_flag);\n}\n\nstatic inline unsigned long long cur_cp_version(struct f2fs_checkpoint *cp)\n{\n\treturn le64_to_cpu(cp->checkpoint_ver);\n}\n\nstatic inline unsigned long f2fs_qf_ino(struct super_block *sb, int type)\n{\n\tif (type < F2FS_MAX_QUOTAS)\n\t\treturn le32_to_cpu(F2FS_SB(sb)->raw_super->qf_ino[type]);\n\treturn 0;\n}\n\nstatic inline __u64 cur_cp_crc(struct f2fs_checkpoint *cp)\n{\n\tsize_t crc_offset = le32_to_cpu(cp->checksum_offset);\n\treturn le32_to_cpu(*((__le32 *)((unsigned char *)cp + crc_offset)));\n}\n\nstatic inline bool __is_set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\n\treturn ckpt_flags & f;\n}\n\nstatic inline bool is_set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\treturn __is_set_ckpt_flags(F2FS_CKPT(sbi), f);\n}\n\nstatic inline void __set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags |= f;\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__set_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void __clear_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)\n{\n\tunsigned int ckpt_flags;\n\n\tckpt_flags = le32_to_cpu(cp->ckpt_flags);\n\tckpt_flags &= (~f);\n\tcp->ckpt_flags = cpu_to_le32(ckpt_flags);\n}\n\nstatic inline void clear_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), f);\n\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline void disable_nat_bits(struct f2fs_sb_info *sbi, bool lock)\n{\n\tunsigned long flags;\n\n\t/*\n\t * In order to re-enable nat_bits we need to call fsck.f2fs by\n\t * set_sbi_flag(sbi, SBI_NEED_FSCK). But it may give huge cost,\n\t * so let's rely on regular fsck or unclean shutdown.\n\t */\n\n\tif (lock)\n\t\tspin_lock_irqsave(&sbi->cp_lock, flags);\n\t__clear_ckpt_flags(F2FS_CKPT(sbi), CP_NAT_BITS_FLAG);\n\tkvfree(NM_I(sbi)->nat_bits);\n\tNM_I(sbi)->nat_bits = NULL;\n\tif (lock)\n\t\tspin_unlock_irqrestore(&sbi->cp_lock, flags);\n}\n\nstatic inline bool enabled_nat_bits(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc)\n{\n\tbool set = is_set_ckpt_flags(sbi, CP_NAT_BITS_FLAG);\n\n\treturn (cpc) ? (cpc->reason & CP_UMOUNT) && set : set;\n}\n\nstatic inline void f2fs_lock_op(struct f2fs_sb_info *sbi)\n{\n\tdown_read(&sbi->cp_rwsem);\n}\n\nstatic inline int f2fs_trylock_op(struct f2fs_sb_info *sbi)\n{\n\treturn down_read_trylock(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_op(struct f2fs_sb_info *sbi)\n{\n\tup_read(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_lock_all(struct f2fs_sb_info *sbi)\n{\n\tdown_write(&sbi->cp_rwsem);\n}\n\nstatic inline void f2fs_unlock_all(struct f2fs_sb_info *sbi)\n{\n\tup_write(&sbi->cp_rwsem);\n}\n\nstatic inline int __get_cp_reason(struct f2fs_sb_info *sbi)\n{\n\tint reason = CP_SYNC;\n\n\tif (test_opt(sbi, FASTBOOT))\n\t\treason = CP_FASTBOOT;\n\tif (is_sbi_flag_set(sbi, SBI_IS_CLOSE))\n\t\treason = CP_UMOUNT;\n\treturn reason;\n}\n\nstatic inline bool __remain_node_summaries(int reason)\n{\n\treturn (reason & (CP_UMOUNT | CP_FASTBOOT));\n}\n\nstatic inline bool __exist_node_summaries(struct f2fs_sb_info *sbi)\n{\n\treturn (is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG) ||\n\t\t\tis_set_ckpt_flags(sbi, CP_FASTBOOT_FLAG));\n}\n\n/*\n * Check whether the inode has blocks or not\n */\nstatic inline int F2FS_HAS_BLOCKS(struct inode *inode)\n{\n\tblock_t xattr_block = F2FS_I(inode)->i_xattr_nid ? 1 : 0;\n\n\treturn (inode->i_blocks >> F2FS_LOG_SECTORS_PER_BLOCK) > xattr_block;\n}\n\nstatic inline bool f2fs_has_xattr_block(unsigned int ofs)\n{\n\treturn ofs == XATTR_NODE_OFFSET;\n}\n\nstatic inline bool __allow_reserved_blocks(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool cap)\n{\n\tif (!inode)\n\t\treturn true;\n\tif (!test_opt(sbi, RESERVE_ROOT))\n\t\treturn false;\n\tif (IS_NOQUOTA(inode))\n\t\treturn true;\n\tif (uid_eq(F2FS_OPTION(sbi).s_resuid, current_fsuid()))\n\t\treturn true;\n\tif (!gid_eq(F2FS_OPTION(sbi).s_resgid, GLOBAL_ROOT_GID) &&\n\t\t\t\t\tin_group_p(F2FS_OPTION(sbi).s_resgid))\n\t\treturn true;\n\tif (cap && capable(CAP_SYS_RESOURCE))\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *, block_t, bool, bool);\nstatic inline int inc_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t struct inode *inode, blkcnt_t *count)\n{\n\tblkcnt_t diff = 0, release = 0;\n\tblock_t avail_user_block_count;\n\tint ret;\n\n\tret = dquot_reserve_block(inode, *count);\n\tif (ret)\n\t\treturn ret;\n\n\tif (time_to_inject(sbi, FAULT_BLOCK)) {\n\t\tf2fs_show_injection_info(FAULT_BLOCK);\n\t\trelease = *count;\n\t\tgoto enospc;\n\t}\n\n\t/*\n\t * let's increase this in prior to actual block count change in order\n\t * for f2fs_sync_file to avoid data races when deciding checkpoint.\n\t */\n\tpercpu_counter_add(&sbi->alloc_valid_block_count, (*count));\n\n\tspin_lock(&sbi->stat_lock);\n\tsbi->total_valid_block_count += (block_t)(*count);\n\tavail_user_block_count = sbi->user_block_count -\n\t\t\t\t\tsbi->current_reserved_blocks;\n\n\tif (!__allow_reserved_blocks(sbi, inode, true))\n\t\tavail_user_block_count -= F2FS_OPTION(sbi).root_reserved_blocks;\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {\n\t\tif (avail_user_block_count > sbi->unusable_block_count)\n\t\t\tavail_user_block_count -= sbi->unusable_block_count;\n\t\telse\n\t\t\tavail_user_block_count = 0;\n\t}\n\tif (unlikely(sbi->total_valid_block_count > avail_user_block_count)) {\n\t\tdiff = sbi->total_valid_block_count - avail_user_block_count;\n\t\tif (diff > *count)\n\t\t\tdiff = *count;\n\t\t*count -= diff;\n\t\trelease = diff;\n\t\tsbi->total_valid_block_count -= diff;\n\t\tif (!*count) {\n\t\t\tspin_unlock(&sbi->stat_lock);\n\t\t\tgoto enospc;\n\t\t}\n\t}\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (unlikely(release)) {\n\t\tpercpu_counter_sub(&sbi->alloc_valid_block_count, release);\n\t\tdquot_release_reservation_block(inode, release);\n\t}\n\tf2fs_i_blocks_write(inode, *count, true, true);\n\treturn 0;\n\nenospc:\n\tpercpu_counter_sub(&sbi->alloc_valid_block_count, release);\n\tdquot_release_reservation_block(inode, release);\n\treturn -ENOSPC;\n}\n\n__printf(2, 3)\nvoid f2fs_printk(struct f2fs_sb_info *sbi, const char *fmt, ...);\n\n#define f2fs_err(sbi, fmt, ...)\t\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_ERR fmt, ##__VA_ARGS__)\n#define f2fs_warn(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_WARNING fmt, ##__VA_ARGS__)\n#define f2fs_notice(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_NOTICE fmt, ##__VA_ARGS__)\n#define f2fs_info(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_INFO fmt, ##__VA_ARGS__)\n#define f2fs_debug(sbi, fmt, ...)\t\t\t\t\t\\\n\tf2fs_printk(sbi, KERN_DEBUG fmt, ##__VA_ARGS__)\n\nstatic inline void dec_valid_block_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tblock_t count)\n{\n\tblkcnt_t sectors = count << F2FS_LOG_SECTORS_PER_BLOCK;\n\n\tspin_lock(&sbi->stat_lock);\n\tf2fs_bug_on(sbi, sbi->total_valid_block_count < (block_t) count);\n\tsbi->total_valid_block_count -= (block_t)count;\n\tif (sbi->reserved_blocks &&\n\t\tsbi->current_reserved_blocks < sbi->reserved_blocks)\n\t\tsbi->current_reserved_blocks = min(sbi->reserved_blocks,\n\t\t\t\t\tsbi->current_reserved_blocks + count);\n\tspin_unlock(&sbi->stat_lock);\n\tif (unlikely(inode->i_blocks < sectors)) {\n\t\tf2fs_warn(sbi, \"Inconsistent i_blocks, ino:%lu, iblocks:%llu, sectors:%llu\",\n\t\t\t  inode->i_ino,\n\t\t\t  (unsigned long long)inode->i_blocks,\n\t\t\t  (unsigned long long)sectors);\n\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\t\treturn;\n\t}\n\tf2fs_i_blocks_write(inode, count, false, true);\n}\n\nstatic inline void inc_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_inc(&sbi->nr_pages[count_type]);\n\n\tif (count_type == F2FS_DIRTY_DENTS ||\n\t\t\tcount_type == F2FS_DIRTY_NODES ||\n\t\t\tcount_type == F2FS_DIRTY_META ||\n\t\t\tcount_type == F2FS_DIRTY_QDATA ||\n\t\t\tcount_type == F2FS_DIRTY_IMETA)\n\t\tset_sbi_flag(sbi, SBI_IS_DIRTY);\n}\n\nstatic inline void inode_inc_dirty_pages(struct inode *inode)\n{\n\tatomic_inc(&F2FS_I(inode)->dirty_pages);\n\tinc_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n\tif (IS_NOQUOTA(inode))\n\t\tinc_page_count(F2FS_I_SB(inode), F2FS_DIRTY_QDATA);\n}\n\nstatic inline void dec_page_count(struct f2fs_sb_info *sbi, int count_type)\n{\n\tatomic_dec(&sbi->nr_pages[count_type]);\n}\n\nstatic inline void inode_dec_dirty_pages(struct inode *inode)\n{\n\tif (!S_ISDIR(inode->i_mode) && !S_ISREG(inode->i_mode) &&\n\t\t\t!S_ISLNK(inode->i_mode))\n\t\treturn;\n\n\tatomic_dec(&F2FS_I(inode)->dirty_pages);\n\tdec_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?\n\t\t\t\tF2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);\n\tif (IS_NOQUOTA(inode))\n\t\tdec_page_count(F2FS_I_SB(inode), F2FS_DIRTY_QDATA);\n}\n\nstatic inline s64 get_pages(struct f2fs_sb_info *sbi, int count_type)\n{\n\treturn atomic_read(&sbi->nr_pages[count_type]);\n}\n\nstatic inline int get_dirty_pages(struct inode *inode)\n{\n\treturn atomic_read(&F2FS_I(inode)->dirty_pages);\n}\n\nstatic inline int get_blocktype_secs(struct f2fs_sb_info *sbi, int block_type)\n{\n\tunsigned int pages_per_sec = sbi->segs_per_sec * sbi->blocks_per_seg;\n\tunsigned int segs = (get_pages(sbi, block_type) + pages_per_sec - 1) >>\n\t\t\t\t\t\tsbi->log_blocks_per_seg;\n\n\treturn segs / sbi->segs_per_sec;\n}\n\nstatic inline block_t valid_user_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_block_count;\n}\n\nstatic inline block_t discard_blocks(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->discard_blks;\n}\n\nstatic inline unsigned long __bitmap_size(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\n\t/* return NAT or SIT bitmap */\n\tif (flag == NAT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->nat_ver_bitmap_bytesize);\n\telse if (flag == SIT_BITMAP)\n\t\treturn le32_to_cpu(ckpt->sit_ver_bitmap_bytesize);\n\n\treturn 0;\n}\n\nstatic inline block_t __cp_payload(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_payload);\n}\n\nstatic inline void *__bitmap_ptr(struct f2fs_sb_info *sbi, int flag)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tint offset;\n\n\tif (is_set_ckpt_flags(sbi, CP_LARGE_NAT_BITMAP_FLAG)) {\n\t\toffset = (flag == SIT_BITMAP) ?\n\t\t\tle32_to_cpu(ckpt->nat_ver_bitmap_bytesize) : 0;\n\t\t/*\n\t\t * if large_nat_bitmap feature is enabled, leave checksum\n\t\t * protection for all nat/sit bitmaps.\n\t\t */\n\t\treturn &ckpt->sit_nat_version_bitmap + offset + sizeof(__le32);\n\t}\n\n\tif (__cp_payload(sbi) > 0) {\n\t\tif (flag == NAT_BITMAP)\n\t\t\treturn &ckpt->sit_nat_version_bitmap;\n\t\telse\n\t\t\treturn (unsigned char *)ckpt + F2FS_BLKSIZE;\n\t} else {\n\t\toffset = (flag == NAT_BITMAP) ?\n\t\t\tle32_to_cpu(ckpt->sit_ver_bitmap_bytesize) : 0;\n\t\treturn &ckpt->sit_nat_version_bitmap + offset;\n\t}\n}\n\nstatic inline block_t __start_cp_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 2)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline block_t __start_cp_next_addr(struct f2fs_sb_info *sbi)\n{\n\tblock_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);\n\n\tif (sbi->cur_cp_pack == 1)\n\t\tstart_addr += sbi->blocks_per_seg;\n\treturn start_addr;\n}\n\nstatic inline void __set_cp_next_pack(struct f2fs_sb_info *sbi)\n{\n\tsbi->cur_cp_pack = (sbi->cur_cp_pack == 1) ? 2 : 1;\n}\n\nstatic inline block_t __start_sum_addr(struct f2fs_sb_info *sbi)\n{\n\treturn le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_start_sum);\n}\n\nstatic inline int inc_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tblock_t\tvalid_block_count;\n\tunsigned int valid_node_count, user_block_count;\n\tint err;\n\n\tif (is_inode) {\n\t\tif (inode) {\n\t\t\terr = dquot_alloc_inode(inode);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t} else {\n\t\terr = dquot_reserve_block(inode, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (time_to_inject(sbi, FAULT_BLOCK)) {\n\t\tf2fs_show_injection_info(FAULT_BLOCK);\n\t\tgoto enospc;\n\t}\n\n\tspin_lock(&sbi->stat_lock);\n\n\tvalid_block_count = sbi->total_valid_block_count +\n\t\t\t\t\tsbi->current_reserved_blocks + 1;\n\n\tif (!__allow_reserved_blocks(sbi, inode, false))\n\t\tvalid_block_count += F2FS_OPTION(sbi).root_reserved_blocks;\n\tuser_block_count = sbi->user_block_count;\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))\n\t\tuser_block_count -= sbi->unusable_block_count;\n\n\tif (unlikely(valid_block_count > user_block_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tvalid_node_count = sbi->total_valid_node_count + 1;\n\tif (unlikely(valid_node_count > sbi->total_node_count)) {\n\t\tspin_unlock(&sbi->stat_lock);\n\t\tgoto enospc;\n\t}\n\n\tsbi->total_valid_node_count++;\n\tsbi->total_valid_block_count++;\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (inode) {\n\t\tif (is_inode)\n\t\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t\telse\n\t\t\tf2fs_i_blocks_write(inode, 1, true, true);\n\t}\n\n\tpercpu_counter_inc(&sbi->alloc_valid_block_count);\n\treturn 0;\n\nenospc:\n\tif (is_inode) {\n\t\tif (inode)\n\t\t\tdquot_free_inode(inode);\n\t} else {\n\t\tdquot_release_reservation_block(inode, 1);\n\t}\n\treturn -ENOSPC;\n}\n\nstatic inline void dec_valid_node_count(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct inode *inode, bool is_inode)\n{\n\tspin_lock(&sbi->stat_lock);\n\n\tf2fs_bug_on(sbi, !sbi->total_valid_block_count);\n\tf2fs_bug_on(sbi, !sbi->total_valid_node_count);\n\n\tsbi->total_valid_node_count--;\n\tsbi->total_valid_block_count--;\n\tif (sbi->reserved_blocks &&\n\t\tsbi->current_reserved_blocks < sbi->reserved_blocks)\n\t\tsbi->current_reserved_blocks++;\n\n\tspin_unlock(&sbi->stat_lock);\n\n\tif (is_inode) {\n\t\tdquot_free_inode(inode);\n\t} else {\n\t\tif (unlikely(inode->i_blocks == 0)) {\n\t\t\tf2fs_warn(sbi, \"Inconsistent i_blocks, ino:%lu, iblocks:%llu\",\n\t\t\t\t  inode->i_ino,\n\t\t\t\t  (unsigned long long)inode->i_blocks);\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\t\t\treturn;\n\t\t}\n\t\tf2fs_i_blocks_write(inode, 1, false, true);\n\t}\n}\n\nstatic inline unsigned int valid_node_count(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->total_valid_node_count;\n}\n\nstatic inline void inc_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_inc(&sbi->total_valid_inode_count);\n}\n\nstatic inline void dec_valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\tpercpu_counter_dec(&sbi->total_valid_inode_count);\n}\n\nstatic inline s64 valid_inode_count(struct f2fs_sb_info *sbi)\n{\n\treturn percpu_counter_sum_positive(&sbi->total_valid_inode_count);\n}\n\nstatic inline struct page *f2fs_grab_cache_page(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t index, bool for_write)\n{\n\tstruct page *page;\n\n\tif (IS_ENABLED(CONFIG_F2FS_FAULT_INJECTION)) {\n\t\tif (!for_write)\n\t\t\tpage = find_get_page_flags(mapping, index,\n\t\t\t\t\t\t\tFGP_LOCK | FGP_ACCESSED);\n\t\telse\n\t\t\tpage = find_lock_page(mapping, index);\n\t\tif (page)\n\t\t\treturn page;\n\n\t\tif (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_ALLOC)) {\n\t\t\tf2fs_show_injection_info(FAULT_PAGE_ALLOC);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (!for_write)\n\t\treturn grab_cache_page(mapping, index);\n\treturn grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);\n}\n\nstatic inline struct page *f2fs_pagecache_get_page(\n\t\t\t\tstruct address_space *mapping, pgoff_t index,\n\t\t\t\tint fgp_flags, gfp_t gfp_mask)\n{\n\tif (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_GET)) {\n\t\tf2fs_show_injection_info(FAULT_PAGE_GET);\n\t\treturn NULL;\n\t}\n\n\treturn pagecache_get_page(mapping, index, fgp_flags, gfp_mask);\n}\n\nstatic inline void f2fs_copy_page(struct page *src, struct page *dst)\n{\n\tchar *src_kaddr = kmap(src);\n\tchar *dst_kaddr = kmap(dst);\n\n\tmemcpy(dst_kaddr, src_kaddr, PAGE_SIZE);\n\tkunmap(dst);\n\tkunmap(src);\n}\n\nstatic inline void f2fs_put_page(struct page *page, int unlock)\n{\n\tif (!page)\n\t\treturn;\n\n\tif (unlock) {\n\t\tf2fs_bug_on(F2FS_P_SB(page), !PageLocked(page));\n\t\tunlock_page(page);\n\t}\n\tput_page(page);\n}\n\nstatic inline void f2fs_put_dnode(struct dnode_of_data *dn)\n{\n\tif (dn->node_page)\n\t\tf2fs_put_page(dn->node_page, 1);\n\tif (dn->inode_page && dn->node_page != dn->inode_page)\n\t\tf2fs_put_page(dn->inode_page, 0);\n\tdn->node_page = NULL;\n\tdn->inode_page = NULL;\n}\n\nstatic inline struct kmem_cache *f2fs_kmem_cache_create(const char *name,\n\t\t\t\t\tsize_t size)\n{\n\treturn kmem_cache_create(name, size, 0, SLAB_RECLAIM_ACCOUNT, NULL);\n}\n\nstatic inline void *f2fs_kmem_cache_alloc(struct kmem_cache *cachep,\n\t\t\t\t\t\tgfp_t flags)\n{\n\tvoid *entry;\n\n\tentry = kmem_cache_alloc(cachep, flags);\n\tif (!entry)\n\t\tentry = kmem_cache_alloc(cachep, flags | __GFP_NOFAIL);\n\treturn entry;\n}\n\nstatic inline struct bio *f2fs_bio_alloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint npages, bool no_fail)\n{\n\tstruct bio *bio;\n\n\tif (no_fail) {\n\t\t/* No failure on bio allocation */\n\t\tbio = bio_alloc(GFP_NOIO, npages);\n\t\tif (!bio)\n\t\t\tbio = bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);\n\t\treturn bio;\n\t}\n\tif (time_to_inject(sbi, FAULT_ALLOC_BIO)) {\n\t\tf2fs_show_injection_info(FAULT_ALLOC_BIO);\n\t\treturn NULL;\n\t}\n\n\treturn bio_alloc(GFP_KERNEL, npages);\n}\n\nstatic inline bool is_idle(struct f2fs_sb_info *sbi, int type)\n{\n\tif (sbi->gc_mode == GC_URGENT)\n\t\treturn true;\n\n\tif (get_pages(sbi, F2FS_RD_DATA) || get_pages(sbi, F2FS_RD_NODE) ||\n\t\tget_pages(sbi, F2FS_RD_META) || get_pages(sbi, F2FS_WB_DATA) ||\n\t\tget_pages(sbi, F2FS_WB_CP_DATA) ||\n\t\tget_pages(sbi, F2FS_DIO_READ) ||\n\t\tget_pages(sbi, F2FS_DIO_WRITE))\n\t\treturn false;\n\n\tif (type != DISCARD_TIME && SM_I(sbi) && SM_I(sbi)->dcc_info &&\n\t\t\tatomic_read(&SM_I(sbi)->dcc_info->queued_discard))\n\t\treturn false;\n\n\tif (SM_I(sbi) && SM_I(sbi)->fcc_info &&\n\t\t\tatomic_read(&SM_I(sbi)->fcc_info->queued_flush))\n\t\treturn false;\n\n\treturn f2fs_time_over(sbi, type);\n}\n\nstatic inline void f2fs_radix_tree_insert(struct radix_tree_root *root,\n\t\t\t\tunsigned long index, void *item)\n{\n\twhile (radix_tree_insert(root, index, item))\n\t\tcond_resched();\n}\n\n#define RAW_IS_INODE(p)\t((p)->footer.nid == (p)->footer.ino)\n\nstatic inline bool IS_INODE(struct page *page)\n{\n\tstruct f2fs_node *p = F2FS_NODE(page);\n\n\treturn RAW_IS_INODE(p);\n}\n\nstatic inline int offset_in_addr(struct f2fs_inode *i)\n{\n\treturn (i->i_inline & F2FS_EXTRA_ATTR) ?\n\t\t\t(le16_to_cpu(i->i_extra_isize) / sizeof(__le32)) : 0;\n}\n\nstatic inline __le32 *blkaddr_in_node(struct f2fs_node *node)\n{\n\treturn RAW_IS_INODE(node) ? node->i.i_addr : node->dn.addr;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode);\nstatic inline block_t datablock_addr(struct inode *inode,\n\t\t\tstruct page *node_page, unsigned int offset)\n{\n\tstruct f2fs_node *raw_node;\n\t__le32 *addr_array;\n\tint base = 0;\n\tbool is_inode = IS_INODE(node_page);\n\n\traw_node = F2FS_NODE(node_page);\n\n\t/* from GC path only */\n\tif (is_inode) {\n\t\tif (!inode)\n\t\t\tbase = offset_in_addr(&raw_node->i);\n\t\telse if (f2fs_has_extra_attr(inode))\n\t\t\tbase = get_extra_isize(inode);\n\t}\n\n\taddr_array = blkaddr_in_node(raw_node);\n\treturn le32_to_cpu(addr_array[base + offset]);\n}\n\nstatic inline int f2fs_test_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\treturn mask & *addr;\n}\n\nstatic inline void f2fs_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr |= mask;\n}\n\nstatic inline void f2fs_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr &= ~mask;\n}\n\nstatic inline int f2fs_test_and_set_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr |= mask;\n\treturn ret;\n}\n\nstatic inline int f2fs_test_and_clear_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\tint ret;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\tret = mask & *addr;\n\t*addr &= ~mask;\n\treturn ret;\n}\n\nstatic inline void f2fs_change_bit(unsigned int nr, char *addr)\n{\n\tint mask;\n\n\taddr += (nr >> 3);\n\tmask = 1 << (7 - (nr & 0x07));\n\t*addr ^= mask;\n}\n\n/*\n * On-disk inode flags (f2fs_inode::i_flags)\n */\n#define F2FS_SYNC_FL\t\t\t0x00000008 /* Synchronous updates */\n#define F2FS_IMMUTABLE_FL\t\t0x00000010 /* Immutable file */\n#define F2FS_APPEND_FL\t\t\t0x00000020 /* writes to file may only append */\n#define F2FS_NODUMP_FL\t\t\t0x00000040 /* do not dump file */\n#define F2FS_NOATIME_FL\t\t\t0x00000080 /* do not update atime */\n#define F2FS_INDEX_FL\t\t\t0x00001000 /* hash-indexed directory */\n#define F2FS_DIRSYNC_FL\t\t\t0x00010000 /* dirsync behaviour (directories only) */\n#define F2FS_PROJINHERIT_FL\t\t0x20000000 /* Create with parents projid */\n\n/* Flags that should be inherited by new inodes from their parent. */\n#define F2FS_FL_INHERITED (F2FS_SYNC_FL | F2FS_NODUMP_FL | F2FS_NOATIME_FL | \\\n\t\t\t   F2FS_DIRSYNC_FL | F2FS_PROJINHERIT_FL)\n\n/* Flags that are appropriate for regular files (all but dir-specific ones). */\n#define F2FS_REG_FLMASK\t\t(~(F2FS_DIRSYNC_FL | F2FS_PROJINHERIT_FL))\n\n/* Flags that are appropriate for non-directories/regular files. */\n#define F2FS_OTHER_FLMASK\t(F2FS_NODUMP_FL | F2FS_NOATIME_FL)\n\nstatic inline __u32 f2fs_mask_flags(umode_t mode, __u32 flags)\n{\n\tif (S_ISDIR(mode))\n\t\treturn flags;\n\telse if (S_ISREG(mode))\n\t\treturn flags & F2FS_REG_FLMASK;\n\telse\n\t\treturn flags & F2FS_OTHER_FLMASK;\n}\n\n/* used for f2fs_inode_info->flags */\nenum {\n\tFI_NEW_INODE,\t\t/* indicate newly allocated inode */\n\tFI_DIRTY_INODE,\t\t/* indicate inode is dirty or not */\n\tFI_AUTO_RECOVER,\t/* indicate inode is recoverable */\n\tFI_DIRTY_DIR,\t\t/* indicate directory has dirty pages */\n\tFI_INC_LINK,\t\t/* need to increment i_nlink */\n\tFI_ACL_MODE,\t\t/* indicate acl mode */\n\tFI_NO_ALLOC,\t\t/* should not allocate any blocks */\n\tFI_FREE_NID,\t\t/* free allocated nide */\n\tFI_NO_EXTENT,\t\t/* not to use the extent cache */\n\tFI_INLINE_XATTR,\t/* used for inline xattr */\n\tFI_INLINE_DATA,\t\t/* used for inline data*/\n\tFI_INLINE_DENTRY,\t/* used for inline dentry */\n\tFI_APPEND_WRITE,\t/* inode has appended data */\n\tFI_UPDATE_WRITE,\t/* inode has in-place-update data */\n\tFI_NEED_IPU,\t\t/* used for ipu per file */\n\tFI_ATOMIC_FILE,\t\t/* indicate atomic file */\n\tFI_ATOMIC_COMMIT,\t/* indicate the state of atomical committing */\n\tFI_VOLATILE_FILE,\t/* indicate volatile file */\n\tFI_FIRST_BLOCK_WRITTEN,\t/* indicate #0 data block was written */\n\tFI_DROP_CACHE,\t\t/* drop dirty page cache */\n\tFI_DATA_EXIST,\t\t/* indicate data exists */\n\tFI_INLINE_DOTS,\t\t/* indicate inline dot dentries */\n\tFI_DO_DEFRAG,\t\t/* indicate defragment is running */\n\tFI_DIRTY_FILE,\t\t/* indicate regular/symlink has dirty pages */\n\tFI_NO_PREALLOC,\t\t/* indicate skipped preallocated blocks */\n\tFI_HOT_DATA,\t\t/* indicate file is hot */\n\tFI_EXTRA_ATTR,\t\t/* indicate file has extra attribute */\n\tFI_PROJ_INHERIT,\t/* indicate file inherits projectid */\n\tFI_PIN_FILE,\t\t/* indicate file should not be gced */\n\tFI_ATOMIC_REVOKE_REQUEST, /* request to drop atomic data */\n};\n\nstatic inline void __mark_inode_dirty_flag(struct inode *inode,\n\t\t\t\t\t\tint flag, bool set)\n{\n\tswitch (flag) {\n\tcase FI_INLINE_XATTR:\n\tcase FI_INLINE_DATA:\n\tcase FI_INLINE_DENTRY:\n\tcase FI_NEW_INODE:\n\t\tif (set)\n\t\t\treturn;\n\t\t/* fall through */\n\tcase FI_DATA_EXIST:\n\tcase FI_INLINE_DOTS:\n\tcase FI_PIN_FILE:\n\t\tf2fs_mark_inode_dirty_sync(inode, true);\n\t}\n}\n\nstatic inline void set_inode_flag(struct inode *inode, int flag)\n{\n\tif (!test_bit(flag, &F2FS_I(inode)->flags))\n\t\tset_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, true);\n}\n\nstatic inline int is_inode_flag_set(struct inode *inode, int flag)\n{\n\treturn test_bit(flag, &F2FS_I(inode)->flags);\n}\n\nstatic inline void clear_inode_flag(struct inode *inode, int flag)\n{\n\tif (test_bit(flag, &F2FS_I(inode)->flags))\n\t\tclear_bit(flag, &F2FS_I(inode)->flags);\n\t__mark_inode_dirty_flag(inode, flag, false);\n}\n\nstatic inline void set_acl_inode(struct inode *inode, umode_t mode)\n{\n\tF2FS_I(inode)->i_acl_mode = mode;\n\tset_inode_flag(inode, FI_ACL_MODE);\n\tf2fs_mark_inode_dirty_sync(inode, false);\n}\n\nstatic inline void f2fs_i_links_write(struct inode *inode, bool inc)\n{\n\tif (inc)\n\t\tinc_nlink(inode);\n\telse\n\t\tdrop_nlink(inode);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_blocks_write(struct inode *inode,\n\t\t\t\t\tblock_t diff, bool add, bool claim)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\t/* add = 1, claim = 1 should be dquot_reserve_block in pair */\n\tif (add) {\n\t\tif (claim)\n\t\t\tdquot_claim_block(inode, diff);\n\t\telse\n\t\t\tdquot_alloc_block_nofail(inode, diff);\n\t} else {\n\t\tdquot_free_block(inode, diff);\n\t}\n\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_size_write(struct inode *inode, loff_t i_size)\n{\n\tbool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);\n\tbool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);\n\n\tif (i_size_read(inode) == i_size)\n\t\treturn;\n\n\ti_size_write(inode, i_size);\n\tf2fs_mark_inode_dirty_sync(inode, true);\n\tif (clean || recover)\n\t\tset_inode_flag(inode, FI_AUTO_RECOVER);\n}\n\nstatic inline void f2fs_i_depth_write(struct inode *inode, unsigned int depth)\n{\n\tF2FS_I(inode)->i_current_depth = depth;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_gc_failures_write(struct inode *inode,\n\t\t\t\t\tunsigned int count)\n{\n\tF2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN] = count;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_xnid_write(struct inode *inode, nid_t xnid)\n{\n\tF2FS_I(inode)->i_xattr_nid = xnid;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void f2fs_i_pino_write(struct inode *inode, nid_t pino)\n{\n\tF2FS_I(inode)->i_pino = pino;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void get_inline_info(struct inode *inode, struct f2fs_inode *ri)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tif (ri->i_inline & F2FS_INLINE_XATTR)\n\t\tset_bit(FI_INLINE_XATTR, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DATA)\n\t\tset_bit(FI_INLINE_DATA, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DENTRY)\n\t\tset_bit(FI_INLINE_DENTRY, &fi->flags);\n\tif (ri->i_inline & F2FS_DATA_EXIST)\n\t\tset_bit(FI_DATA_EXIST, &fi->flags);\n\tif (ri->i_inline & F2FS_INLINE_DOTS)\n\t\tset_bit(FI_INLINE_DOTS, &fi->flags);\n\tif (ri->i_inline & F2FS_EXTRA_ATTR)\n\t\tset_bit(FI_EXTRA_ATTR, &fi->flags);\n\tif (ri->i_inline & F2FS_PIN_FILE)\n\t\tset_bit(FI_PIN_FILE, &fi->flags);\n}\n\nstatic inline void set_raw_inline(struct inode *inode, struct f2fs_inode *ri)\n{\n\tri->i_inline = 0;\n\n\tif (is_inode_flag_set(inode, FI_INLINE_XATTR))\n\t\tri->i_inline |= F2FS_INLINE_XATTR;\n\tif (is_inode_flag_set(inode, FI_INLINE_DATA))\n\t\tri->i_inline |= F2FS_INLINE_DATA;\n\tif (is_inode_flag_set(inode, FI_INLINE_DENTRY))\n\t\tri->i_inline |= F2FS_INLINE_DENTRY;\n\tif (is_inode_flag_set(inode, FI_DATA_EXIST))\n\t\tri->i_inline |= F2FS_DATA_EXIST;\n\tif (is_inode_flag_set(inode, FI_INLINE_DOTS))\n\t\tri->i_inline |= F2FS_INLINE_DOTS;\n\tif (is_inode_flag_set(inode, FI_EXTRA_ATTR))\n\t\tri->i_inline |= F2FS_EXTRA_ATTR;\n\tif (is_inode_flag_set(inode, FI_PIN_FILE))\n\t\tri->i_inline |= F2FS_PIN_FILE;\n}\n\nstatic inline int f2fs_has_extra_attr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_EXTRA_ATTR);\n}\n\nstatic inline int f2fs_has_inline_xattr(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_XATTR);\n}\n\nstatic inline unsigned int addrs_per_inode(struct inode *inode)\n{\n\tunsigned int addrs = CUR_ADDRS_PER_INODE(inode) -\n\t\t\t\tget_inline_xattr_addrs(inode);\n\treturn ALIGN_DOWN(addrs, 1);\n}\n\nstatic inline unsigned int addrs_per_block(struct inode *inode)\n{\n\treturn ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, 1);\n}\n\nstatic inline void *inline_xattr_addr(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\n\treturn (void *)&(ri->i_addr[DEF_ADDRS_PER_INODE -\n\t\t\t\t\tget_inline_xattr_addrs(inode)]);\n}\n\nstatic inline int inline_xattr_size(struct inode *inode)\n{\n\tif (f2fs_has_inline_xattr(inode))\n\t\treturn get_inline_xattr_addrs(inode) * sizeof(__le32);\n\treturn 0;\n}\n\nstatic inline int f2fs_has_inline_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DATA);\n}\n\nstatic inline int f2fs_exist_data(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DATA_EXIST);\n}\n\nstatic inline int f2fs_has_inline_dots(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DOTS);\n}\n\nstatic inline bool f2fs_is_pinned_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_PIN_FILE);\n}\n\nstatic inline bool f2fs_is_atomic_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_FILE);\n}\n\nstatic inline bool f2fs_is_commit_atomic_write(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_ATOMIC_COMMIT);\n}\n\nstatic inline bool f2fs_is_volatile_file(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_VOLATILE_FILE);\n}\n\nstatic inline bool f2fs_is_first_block_written(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_FIRST_BLOCK_WRITTEN);\n}\n\nstatic inline bool f2fs_is_drop_cache(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_DROP_CACHE);\n}\n\nstatic inline void *inline_data_addr(struct inode *inode, struct page *page)\n{\n\tstruct f2fs_inode *ri = F2FS_INODE(page);\n\tint extra_size = get_extra_isize(inode);\n\n\treturn (void *)&(ri->i_addr[extra_size + DEF_INLINE_RESERVED_SIZE]);\n}\n\nstatic inline int f2fs_has_inline_dentry(struct inode *inode)\n{\n\treturn is_inode_flag_set(inode, FI_INLINE_DENTRY);\n}\n\nstatic inline int is_file(struct inode *inode, int type)\n{\n\treturn F2FS_I(inode)->i_advise & type;\n}\n\nstatic inline void set_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise |= type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline void clear_file(struct inode *inode, int type)\n{\n\tF2FS_I(inode)->i_advise &= ~type;\n\tf2fs_mark_inode_dirty_sync(inode, true);\n}\n\nstatic inline bool f2fs_skip_inode_update(struct inode *inode, int dsync)\n{\n\tbool ret;\n\n\tif (dsync) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tspin_lock(&sbi->inode_lock[DIRTY_META]);\n\t\tret = list_empty(&F2FS_I(inode)->gdirty_list);\n\t\tspin_unlock(&sbi->inode_lock[DIRTY_META]);\n\t\treturn ret;\n\t}\n\tif (!is_inode_flag_set(inode, FI_AUTO_RECOVER) ||\n\t\t\tfile_keep_isize(inode) ||\n\t\t\ti_size_read(inode) & ~PAGE_MASK)\n\t\treturn false;\n\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time, &inode->i_atime))\n\t\treturn false;\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time + 1, &inode->i_ctime))\n\t\treturn false;\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time + 2, &inode->i_mtime))\n\t\treturn false;\n\tif (!timespec64_equal(F2FS_I(inode)->i_disk_time + 3,\n\t\t\t\t\t\t&F2FS_I(inode)->i_crtime))\n\t\treturn false;\n\n\tdown_read(&F2FS_I(inode)->i_sem);\n\tret = F2FS_I(inode)->last_disk_size == i_size_read(inode);\n\tup_read(&F2FS_I(inode)->i_sem);\n\n\treturn ret;\n}\n\nstatic inline bool f2fs_readonly(struct super_block *sb)\n{\n\treturn sb_rdonly(sb);\n}\n\nstatic inline bool f2fs_cp_error(struct f2fs_sb_info *sbi)\n{\n\treturn is_set_ckpt_flags(sbi, CP_ERROR_FLAG);\n}\n\nstatic inline bool is_dot_dotdot(const struct qstr *str)\n{\n\tif (str->len == 1 && str->name[0] == '.')\n\t\treturn true;\n\n\tif (str->len == 2 && str->name[0] == '.' && str->name[1] == '.')\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool f2fs_may_extent_tree(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (!test_opt(sbi, EXTENT_CACHE) ||\n\t\t\tis_inode_flag_set(inode, FI_NO_EXTENT))\n\t\treturn false;\n\n\t/*\n\t * for recovered files during mount do not create extents\n\t * if shrinker is not registered.\n\t */\n\tif (list_empty(&sbi->s_list))\n\t\treturn false;\n\n\treturn S_ISREG(inode->i_mode);\n}\n\nstatic inline void *f2fs_kmalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\tvoid *ret;\n\n\tif (time_to_inject(sbi, FAULT_KMALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_KMALLOC);\n\t\treturn NULL;\n\t}\n\n\tret = kmalloc(size, flags);\n\tif (ret)\n\t\treturn ret;\n\n\treturn kvmalloc(size, flags);\n}\n\nstatic inline void *f2fs_kzalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\treturn f2fs_kmalloc(sbi, size, flags | __GFP_ZERO);\n}\n\nstatic inline void *f2fs_kvmalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\tif (time_to_inject(sbi, FAULT_KVMALLOC)) {\n\t\tf2fs_show_injection_info(FAULT_KVMALLOC);\n\t\treturn NULL;\n\t}\n\n\treturn kvmalloc(size, flags);\n}\n\nstatic inline void *f2fs_kvzalloc(struct f2fs_sb_info *sbi,\n\t\t\t\t\tsize_t size, gfp_t flags)\n{\n\treturn f2fs_kvmalloc(sbi, size, flags | __GFP_ZERO);\n}\n\nstatic inline int get_extra_isize(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_extra_isize / sizeof(__le32);\n}\n\nstatic inline int get_inline_xattr_addrs(struct inode *inode)\n{\n\treturn F2FS_I(inode)->i_inline_xattr_size;\n}\n\n#define f2fs_get_inode_mode(i) \\\n\t((is_inode_flag_set(i, FI_ACL_MODE)) ? \\\n\t (F2FS_I(i)->i_acl_mode) : ((i)->i_mode))\n\n#define F2FS_TOTAL_EXTRA_ATTR_SIZE\t\t\t\\\n\t(offsetof(struct f2fs_inode, i_extra_end) -\t\\\n\toffsetof(struct f2fs_inode, i_extra_isize))\t\\\n\n#define F2FS_OLD_ATTRIBUTE_SIZE\t(offsetof(struct f2fs_inode, i_addr))\n#define F2FS_FITS_IN_INODE(f2fs_inode, extra_isize, field)\t\t\\\n\t\t((offsetof(typeof(*(f2fs_inode)), field) +\t\\\n\t\tsizeof((f2fs_inode)->field))\t\t\t\\\n\t\t<= (F2FS_OLD_ATTRIBUTE_SIZE + (extra_isize)))\t\\\n\nstatic inline void f2fs_reset_iostat(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tspin_lock(&sbi->iostat_lock);\n\tfor (i = 0; i < NR_IO_TYPE; i++)\n\t\tsbi->write_iostat[i] = 0;\n\tspin_unlock(&sbi->iostat_lock);\n}\n\nstatic inline void f2fs_update_iostat(struct f2fs_sb_info *sbi,\n\t\t\tenum iostat_type type, unsigned long long io_bytes)\n{\n\tif (!sbi->iostat_enable)\n\t\treturn;\n\tspin_lock(&sbi->iostat_lock);\n\tsbi->write_iostat[type] += io_bytes;\n\n\tif (type == APP_WRITE_IO || type == APP_DIRECT_IO)\n\t\tsbi->write_iostat[APP_BUFFERED_IO] =\n\t\t\tsbi->write_iostat[APP_WRITE_IO] -\n\t\t\tsbi->write_iostat[APP_DIRECT_IO];\n\tspin_unlock(&sbi->iostat_lock);\n}\n\n#define __is_large_section(sbi)\t\t((sbi)->segs_per_sec > 1)\n\n#define __is_meta_io(fio) (PAGE_TYPE_OF_BIO((fio)->type) == META)\n\nbool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type);\nstatic inline void verify_blkaddr(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, type)) {\n\t\tf2fs_err(sbi, \"invalid blkaddr: %u, type: %d, run fsck to fix.\",\n\t\t\t blkaddr, type);\n\t\tf2fs_bug_on(sbi, 1);\n\t}\n}\n\nstatic inline bool __is_valid_data_blkaddr(block_t blkaddr)\n{\n\tif (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR)\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline void f2fs_set_page_private(struct page *page,\n\t\t\t\t\t\tunsigned long data)\n{\n\tif (PagePrivate(page))\n\t\treturn;\n\n\tget_page(page);\n\tSetPagePrivate(page);\n\tset_page_private(page, data);\n}\n\nstatic inline void f2fs_clear_page_private(struct page *page)\n{\n\tif (!PagePrivate(page))\n\t\treturn;\n\n\tset_page_private(page, 0);\n\tClearPagePrivate(page);\n\tf2fs_put_page(page, 0);\n}\n\n/*\n * file.c\n */\nint f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);\nvoid f2fs_truncate_data_blocks(struct dnode_of_data *dn);\nint f2fs_truncate_blocks(struct inode *inode, u64 from, bool lock);\nint f2fs_truncate(struct inode *inode);\nint f2fs_getattr(const struct path *path, struct kstat *stat,\n\t\t\tu32 request_mask, unsigned int flags);\nint f2fs_setattr(struct dentry *dentry, struct iattr *attr);\nint f2fs_truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end);\nvoid f2fs_truncate_data_blocks_range(struct dnode_of_data *dn, int count);\nint f2fs_precache_extents(struct inode *inode);\nlong f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);\nlong f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\nint f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid);\nint f2fs_pin_file_control(struct inode *inode, bool inc);\n\n/*\n * inode.c\n */\nvoid f2fs_set_inode_flags(struct inode *inode);\nbool f2fs_inode_chksum_verify(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page);\nstruct inode *f2fs_iget(struct super_block *sb, unsigned long ino);\nstruct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);\nint f2fs_try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);\nvoid f2fs_update_inode(struct inode *inode, struct page *node_page);\nvoid f2fs_update_inode_page(struct inode *inode);\nint f2fs_write_inode(struct inode *inode, struct writeback_control *wbc);\nvoid f2fs_evict_inode(struct inode *inode);\nvoid f2fs_handle_failed_inode(struct inode *inode);\n\n/*\n * namei.c\n */\nint f2fs_update_extension_list(struct f2fs_sb_info *sbi, const char *name,\n\t\t\t\t\t\t\tbool hot, bool set);\nstruct dentry *f2fs_get_parent(struct dentry *child);\n\n/*\n * dir.c\n */\nunsigned char f2fs_get_de_type(struct f2fs_dir_entry *de);\nstruct f2fs_dir_entry *f2fs_find_target_dentry(struct fscrypt_name *fname,\n\t\t\tf2fs_hash_t namehash, int *max_slots,\n\t\t\tstruct f2fs_dentry_ptr *d);\nint f2fs_fill_dentries(struct dir_context *ctx, struct f2fs_dentry_ptr *d,\n\t\t\tunsigned int start_pos, struct fscrypt_str *fstr);\nvoid f2fs_do_make_empty_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct f2fs_dentry_ptr *d);\nstruct page *f2fs_init_inode_metadata(struct inode *inode, struct inode *dir,\n\t\t\tconst struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name, struct page *dpage);\nvoid f2fs_update_parent_metadata(struct inode *dir, struct inode *inode,\n\t\t\tunsigned int current_depth);\nint f2fs_room_for_filename(const void *bitmap, int slots, int max_slots);\nvoid f2fs_drop_nlink(struct inode *dir, struct inode *inode);\nstruct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_find_entry(struct inode *dir,\n\t\t\tconst struct qstr *child, struct page **res_page);\nstruct f2fs_dir_entry *f2fs_parent_dir(struct inode *dir, struct page **p);\nino_t f2fs_inode_by_name(struct inode *dir, const struct qstr *qstr,\n\t\t\tstruct page **page);\nvoid f2fs_set_link(struct inode *dir, struct f2fs_dir_entry *de,\n\t\t\tstruct page *page, struct inode *inode);\nvoid f2fs_update_dentry(nid_t ino, umode_t mode, struct f2fs_dentry_ptr *d,\n\t\t\tconst struct qstr *name, f2fs_hash_t name_hash,\n\t\t\tunsigned int bit_pos);\nint f2fs_add_regular_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint f2fs_add_dentry(struct inode *dir, struct fscrypt_name *fname,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nint f2fs_do_add_link(struct inode *dir, const struct qstr *name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_entry(struct f2fs_dir_entry *dentry, struct page *page,\n\t\t\tstruct inode *dir, struct inode *inode);\nint f2fs_do_tmpfile(struct inode *inode, struct inode *dir);\nbool f2fs_empty_dir(struct inode *dir);\n\nstatic inline int f2fs_add_link(struct dentry *dentry, struct inode *inode)\n{\n\treturn f2fs_do_add_link(d_inode(dentry->d_parent), &dentry->d_name,\n\t\t\t\tinode, inode->i_ino, inode->i_mode);\n}\n\n/*\n * super.c\n */\nint f2fs_inode_dirtied(struct inode *inode, bool sync);\nvoid f2fs_inode_synced(struct inode *inode);\nint f2fs_enable_quota_files(struct f2fs_sb_info *sbi, bool rdonly);\nint f2fs_quota_sync(struct super_block *sb, int type);\nvoid f2fs_quota_off_umount(struct super_block *sb);\nint f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover);\nint f2fs_sync_fs(struct super_block *sb, int sync);\nint f2fs_sanity_check_ckpt(struct f2fs_sb_info *sbi);\n\n/*\n * hash.c\n */\nf2fs_hash_t f2fs_dentry_hash(const struct qstr *name_info,\n\t\t\t\tstruct fscrypt_name *fname);\n\n/*\n * node.c\n */\nstruct dnode_of_data;\nstruct node_info;\n\nint f2fs_check_nid_range(struct f2fs_sb_info *sbi, nid_t nid);\nbool f2fs_available_free_memory(struct f2fs_sb_info *sbi, int type);\nbool f2fs_in_warm_node_list(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_init_fsync_node_info(struct f2fs_sb_info *sbi);\nvoid f2fs_del_fsync_node_entry(struct f2fs_sb_info *sbi, struct page *page);\nvoid f2fs_reset_fsync_node_info(struct f2fs_sb_info *sbi);\nint f2fs_need_dentry_mark(struct f2fs_sb_info *sbi, nid_t nid);\nbool f2fs_is_checkpointed_node(struct f2fs_sb_info *sbi, nid_t nid);\nbool f2fs_need_inode_block_update(struct f2fs_sb_info *sbi, nid_t ino);\nint f2fs_get_node_info(struct f2fs_sb_info *sbi, nid_t nid,\n\t\t\t\t\t\tstruct node_info *ni);\npgoff_t f2fs_get_next_page_offset(struct dnode_of_data *dn, pgoff_t pgofs);\nint f2fs_get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode);\nint f2fs_truncate_inode_blocks(struct inode *inode, pgoff_t from);\nint f2fs_truncate_xattr_node(struct inode *inode);\nint f2fs_wait_on_node_pages_writeback(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int seq_id);\nint f2fs_remove_inode_page(struct inode *inode);\nstruct page *f2fs_new_inode_page(struct inode *inode);\nstruct page *f2fs_new_node_page(struct dnode_of_data *dn, unsigned int ofs);\nvoid f2fs_ra_node_page(struct f2fs_sb_info *sbi, nid_t nid);\nstruct page *f2fs_get_node_page(struct f2fs_sb_info *sbi, pgoff_t nid);\nstruct page *f2fs_get_node_page_ra(struct page *parent, int start);\nint f2fs_move_node_page(struct page *node_page, int gc_type);\nint f2fs_fsync_node_pages(struct f2fs_sb_info *sbi, struct inode *inode,\n\t\t\tstruct writeback_control *wbc, bool atomic,\n\t\t\tunsigned int *seq_id);\nint f2fs_sync_node_pages(struct f2fs_sb_info *sbi,\n\t\t\tstruct writeback_control *wbc,\n\t\t\tbool do_balance, enum iostat_type io_type);\nint f2fs_build_free_nids(struct f2fs_sb_info *sbi, bool sync, bool mount);\nbool f2fs_alloc_nid(struct f2fs_sb_info *sbi, nid_t *nid);\nvoid f2fs_alloc_nid_done(struct f2fs_sb_info *sbi, nid_t nid);\nvoid f2fs_alloc_nid_failed(struct f2fs_sb_info *sbi, nid_t nid);\nint f2fs_try_to_free_nids(struct f2fs_sb_info *sbi, int nr_shrink);\nvoid f2fs_recover_inline_xattr(struct inode *inode, struct page *page);\nint f2fs_recover_xattr_data(struct inode *inode, struct page *page);\nint f2fs_recover_inode_page(struct f2fs_sb_info *sbi, struct page *page);\nint f2fs_restore_node_summary(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int segno, struct f2fs_summary_block *sum);\nint f2fs_flush_nat_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint f2fs_build_node_manager(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_node_manager(struct f2fs_sb_info *sbi);\nint __init f2fs_create_node_manager_caches(void);\nvoid f2fs_destroy_node_manager_caches(void);\n\n/*\n * segment.c\n */\nbool f2fs_need_SSR(struct f2fs_sb_info *sbi);\nvoid f2fs_register_inmem_page(struct inode *inode, struct page *page);\nvoid f2fs_drop_inmem_pages_all(struct f2fs_sb_info *sbi, bool gc_failure);\nvoid f2fs_drop_inmem_pages(struct inode *inode);\nvoid f2fs_drop_inmem_page(struct inode *inode, struct page *page);\nint f2fs_commit_inmem_pages(struct inode *inode);\nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need);\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi);\nint f2fs_issue_flush(struct f2fs_sb_info *sbi, nid_t ino);\nint f2fs_create_flush_cmd_control(struct f2fs_sb_info *sbi);\nint f2fs_flush_device_cache(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);\nvoid f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);\nbool f2fs_is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid f2fs_drop_discard_cmd(struct f2fs_sb_info *sbi);\nvoid f2fs_stop_discard_thread(struct f2fs_sb_info *sbi);\nbool f2fs_issue_discard_timeout(struct f2fs_sb_info *sbi);\nvoid f2fs_clear_prefree_segments(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc);\nvoid f2fs_dirty_to_prefree(struct f2fs_sb_info *sbi);\nblock_t f2fs_get_unusable_blocks(struct f2fs_sb_info *sbi);\nint f2fs_disable_cp_again(struct f2fs_sb_info *sbi, block_t unusable);\nvoid f2fs_release_discard_addrs(struct f2fs_sb_info *sbi);\nint f2fs_npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);\nvoid allocate_segment_for_resize(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int start, unsigned int end);\nvoid f2fs_allocate_new_segments(struct f2fs_sb_info *sbi);\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range);\nbool f2fs_exist_trim_candidates(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct cp_control *cpc);\nstruct page *f2fs_get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno);\nvoid f2fs_update_meta_page(struct f2fs_sb_info *sbi, void *src,\n\t\t\t\t\tblock_t blk_addr);\nvoid f2fs_do_write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\t\tenum iostat_type io_type);\nvoid f2fs_do_write_node_page(unsigned int nid, struct f2fs_io_info *fio);\nvoid f2fs_outplace_write_data(struct dnode_of_data *dn,\n\t\t\tstruct f2fs_io_info *fio);\nint f2fs_inplace_write_data(struct f2fs_io_info *fio);\nvoid f2fs_do_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\tbool recover_curseg, bool recover_newaddr);\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\tunsigned char version, bool recover_curseg,\n\t\t\tbool recover_newaddr);\nvoid f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\t\tstruct f2fs_summary *sum, int type,\n\t\t\tstruct f2fs_io_info *fio, bool add_list);\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\tenum page_type type, bool ordered, bool locked);\nvoid f2fs_wait_on_block_writeback(struct inode *inode, block_t blkaddr);\nvoid f2fs_wait_on_block_writeback_range(struct inode *inode, block_t blkaddr,\n\t\t\t\t\t\t\t\tblock_t len);\nvoid f2fs_write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nvoid f2fs_write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk);\nint f2fs_lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\tunsigned int val, int alloc);\nvoid f2fs_flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nint f2fs_build_segment_manager(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_segment_manager(struct f2fs_sb_info *sbi);\nint __init f2fs_create_segment_manager_caches(void);\nvoid f2fs_destroy_segment_manager_caches(void);\nint f2fs_rw_hint_to_seg_type(enum rw_hint hint);\nenum rw_hint f2fs_io_type_to_rw_hint(struct f2fs_sb_info *sbi,\n\t\t\tenum page_type type, enum temp_type temp);\n\n/*\n * checkpoint.c\n */\nvoid f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io);\nstruct page *f2fs_grab_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *f2fs_get_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *f2fs_get_meta_page_nofail(struct f2fs_sb_info *sbi, pgoff_t index);\nstruct page *f2fs_get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);\nbool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type);\nint f2fs_ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,\n\t\t\tint type, bool sync);\nvoid f2fs_ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index);\nlong f2fs_sync_meta_pages(struct f2fs_sb_info *sbi, enum page_type type,\n\t\t\tlong nr_to_write, enum iostat_type io_type);\nvoid f2fs_add_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid f2fs_remove_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);\nvoid f2fs_release_ino_entry(struct f2fs_sb_info *sbi, bool all);\nbool f2fs_exist_written_data(struct f2fs_sb_info *sbi, nid_t ino, int mode);\nvoid f2fs_set_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,\n\t\t\t\t\tunsigned int devidx, int type);\nbool f2fs_is_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,\n\t\t\t\t\tunsigned int devidx, int type);\nint f2fs_sync_inode_meta(struct f2fs_sb_info *sbi);\nint f2fs_acquire_orphan_inode(struct f2fs_sb_info *sbi);\nvoid f2fs_release_orphan_inode(struct f2fs_sb_info *sbi);\nvoid f2fs_add_orphan_inode(struct inode *inode);\nvoid f2fs_remove_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino);\nint f2fs_recover_orphan_inodes(struct f2fs_sb_info *sbi);\nint f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi);\nvoid f2fs_update_dirty_page(struct inode *inode, struct page *page);\nvoid f2fs_remove_dirty_inode(struct inode *inode);\nint f2fs_sync_dirty_inodes(struct f2fs_sb_info *sbi, enum inode_type type);\nvoid f2fs_wait_on_all_pages_writeback(struct f2fs_sb_info *sbi);\nint f2fs_write_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc);\nvoid f2fs_init_ino_entry_info(struct f2fs_sb_info *sbi);\nint __init f2fs_create_checkpoint_caches(void);\nvoid f2fs_destroy_checkpoint_caches(void);\n\n/*\n * data.c\n */\nint f2fs_init_post_read_processing(void);\nvoid f2fs_destroy_post_read_processing(void);\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type);\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type);\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi);\nint f2fs_submit_page_bio(struct f2fs_io_info *fio);\nint f2fs_merge_page_bio(struct f2fs_io_info *fio);\nvoid f2fs_submit_page_write(struct f2fs_io_info *fio);\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\t\tblock_t blk_addr, struct bio *bio);\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr);\nvoid f2fs_set_data_blkaddr(struct dnode_of_data *dn);\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);\nint f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count);\nint f2fs_reserve_new_block(struct dnode_of_data *dn);\nint f2fs_get_block(struct dnode_of_data *dn, pgoff_t index);\nint f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *from);\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index);\nstruct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\tint op_flags, bool for_write);\nstruct page *f2fs_find_data_page(struct inode *inode, pgoff_t index);\nstruct page *f2fs_get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\tbool for_write);\nstruct page *f2fs_get_new_data_page(struct inode *inode,\n\t\t\tstruct page *ipage, pgoff_t index, bool new_i_size);\nint f2fs_do_write_data_page(struct f2fs_io_info *fio);\nvoid __do_map_lock(struct f2fs_sb_info *sbi, int flag, bool lock);\nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,\n\t\t\tint create, int flag);\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t\tu64 start, u64 len);\nbool f2fs_should_update_inplace(struct inode *inode, struct f2fs_io_info *fio);\nbool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio);\nvoid f2fs_invalidate_page(struct page *page, unsigned int offset,\n\t\t\tunsigned int length);\nint f2fs_release_page(struct page *page, gfp_t wait);\n#ifdef CONFIG_MIGRATION\nint f2fs_migrate_page(struct address_space *mapping, struct page *newpage,\n\t\t\tstruct page *page, enum migrate_mode mode);\n#endif\nbool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len);\nvoid f2fs_clear_page_cache_dirty_tag(struct page *page);\n\n/*\n * gc.c\n */\nint f2fs_start_gc_thread(struct f2fs_sb_info *sbi);\nvoid f2fs_stop_gc_thread(struct f2fs_sb_info *sbi);\nblock_t f2fs_start_bidx_of_node(unsigned int node_ofs, struct inode *inode);\nint f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background,\n\t\t\tunsigned int segno);\nvoid f2fs_build_gc_manager(struct f2fs_sb_info *sbi);\nint f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count);\n\n/*\n * recovery.c\n */\nint f2fs_recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);\nbool f2fs_space_for_roll_forward(struct f2fs_sb_info *sbi);\n\n/*\n * debug.c\n */\n#ifdef CONFIG_F2FS_STAT_FS\nstruct f2fs_stat_info {\n\tstruct list_head stat_list;\n\tstruct f2fs_sb_info *sbi;\n\tint all_area_segs, sit_area_segs, nat_area_segs, ssa_area_segs;\n\tint main_area_segs, main_area_sections, main_area_zones;\n\tunsigned long long hit_largest, hit_cached, hit_rbtree;\n\tunsigned long long hit_total, total_ext;\n\tint ext_tree, zombie_tree, ext_node;\n\tint ndirty_node, ndirty_dent, ndirty_meta, ndirty_imeta;\n\tint ndirty_data, ndirty_qdata;\n\tint inmem_pages;\n\tunsigned int ndirty_dirs, ndirty_files, nquota_files, ndirty_all;\n\tint nats, dirty_nats, sits, dirty_sits;\n\tint free_nids, avail_nids, alloc_nids;\n\tint total_count, utilization;\n\tint bg_gc, nr_wb_cp_data, nr_wb_data;\n\tint nr_rd_data, nr_rd_node, nr_rd_meta;\n\tint nr_dio_read, nr_dio_write;\n\tunsigned int io_skip_bggc, other_skip_bggc;\n\tint nr_flushing, nr_flushed, flush_list_empty;\n\tint nr_discarding, nr_discarded;\n\tint nr_discard_cmd;\n\tunsigned int undiscard_blks;\n\tint inline_xattr, inline_inode, inline_dir, append, update, orphans;\n\tint aw_cnt, max_aw_cnt, vw_cnt, max_vw_cnt;\n\tunsigned int valid_count, valid_node_count, valid_inode_count, discard_blks;\n\tunsigned int bimodal, avg_vblocks;\n\tint util_free, util_valid, util_invalid;\n\tint rsvd_segs, overp_segs;\n\tint dirty_count, node_pages, meta_pages;\n\tint prefree_count, call_count, cp_count, bg_cp_count;\n\tint tot_segs, node_segs, data_segs, free_segs, free_secs;\n\tint bg_node_segs, bg_data_segs;\n\tint tot_blks, data_blks, node_blks;\n\tint bg_data_blks, bg_node_blks;\n\tunsigned long long skipped_atomic_files[2];\n\tint curseg[NR_CURSEG_TYPE];\n\tint cursec[NR_CURSEG_TYPE];\n\tint curzone[NR_CURSEG_TYPE];\n\n\tunsigned int meta_count[META_MAX];\n\tunsigned int segment_count[2];\n\tunsigned int block_count[2];\n\tunsigned int inplace_count;\n\tunsigned long long base_mem, cache_mem, page_mem;\n};\n\nstatic inline struct f2fs_stat_info *F2FS_STAT(struct f2fs_sb_info *sbi)\n{\n\treturn (struct f2fs_stat_info *)sbi->stat_info;\n}\n\n#define stat_inc_cp_count(si)\t\t((si)->cp_count++)\n#define stat_inc_bg_cp_count(si)\t((si)->bg_cp_count++)\n#define stat_inc_call_count(si)\t\t((si)->call_count++)\n#define stat_inc_bggc_count(sbi)\t((sbi)->bg_gc++)\n#define stat_io_skip_bggc_count(sbi)\t((sbi)->io_skip_bggc++)\n#define stat_other_skip_bggc_count(sbi)\t((sbi)->other_skip_bggc++)\n#define stat_inc_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]++)\n#define stat_dec_dirty_inode(sbi, type)\t((sbi)->ndirty_inode[type]--)\n#define stat_inc_total_hit(sbi)\t\t(atomic64_inc(&(sbi)->total_hit_ext))\n#define stat_inc_rbtree_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_rbtree))\n#define stat_inc_largest_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_largest))\n#define stat_inc_cached_node_hit(sbi)\t(atomic64_inc(&(sbi)->read_hit_cached))\n#define stat_inc_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_dec_inline_xattr(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_xattr(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_xattr));\t\\\n\t} while (0)\n#define stat_inc_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_dec_inline_inode(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_data(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_inode));\t\\\n\t} while (0)\n#define stat_inc_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_inc(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_dec_inline_dir(inode)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (f2fs_has_inline_dentry(inode))\t\t\t\\\n\t\t\t(atomic_dec(&F2FS_I_SB(inode)->inline_dir));\t\\\n\t} while (0)\n#define stat_inc_meta_count(sbi, blkaddr)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (blkaddr < SIT_I(sbi)->sit_base_addr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_CP]);\t\\\n\t\telse if (blkaddr < NM_I(sbi)->nat_blkaddr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_SIT]);\t\\\n\t\telse if (blkaddr < SM_I(sbi)->ssa_blkaddr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_NAT]);\t\\\n\t\telse if (blkaddr < SM_I(sbi)->main_blkaddr)\t\t\\\n\t\t\tatomic_inc(&(sbi)->meta_count[META_SSA]);\t\\\n\t} while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\t\t\t\\\n\t\t((sbi)->segment_count[(curseg)->alloc_type]++)\n#define stat_inc_block_count(sbi, curseg)\t\t\t\t\\\n\t\t((sbi)->block_count[(curseg)->alloc_type]++)\n#define stat_inc_inplace_blocks(sbi)\t\t\t\t\t\\\n\t\t(atomic_inc(&(sbi)->inplace_count))\n#define stat_inc_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_dec_atomic_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->aw_cnt))\n#define stat_update_max_atomic_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->aw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_aw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_aw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_inc(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_dec_volatile_write(inode)\t\t\t\t\t\\\n\t\t(atomic_dec(&F2FS_I_SB(inode)->vw_cnt))\n#define stat_update_max_volatile_write(inode)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tint cur = atomic_read(&F2FS_I_SB(inode)->vw_cnt);\t\\\n\t\tint max = atomic_read(&F2FS_I_SB(inode)->max_vw_cnt);\t\\\n\t\tif (cur > max)\t\t\t\t\t\t\\\n\t\t\tatomic_set(&F2FS_I_SB(inode)->max_vw_cnt, cur);\t\\\n\t} while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tsi->tot_segs++;\t\t\t\t\t\t\\\n\t\tif ((type) == SUM_TYPE_DATA) {\t\t\t\t\\\n\t\t\tsi->data_segs++;\t\t\t\t\\\n\t\t\tsi->bg_data_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tsi->node_segs++;\t\t\t\t\\\n\t\t\tsi->bg_node_segs += (gc_type == BG_GC) ? 1 : 0;\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#define stat_inc_tot_blk_count(si, blks)\t\t\t\t\\\n\t((si)->tot_blks += (blks))\n\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->data_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_data_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstruct f2fs_stat_info *si = F2FS_STAT(sbi);\t\t\\\n\t\tstat_inc_tot_blk_count(si, blks);\t\t\t\\\n\t\tsi->node_blks += (blks);\t\t\t\t\\\n\t\tsi->bg_node_blks += ((gc_type) == BG_GC) ? (blks) : 0;\t\\\n\t} while (0)\n\nint f2fs_build_stats(struct f2fs_sb_info *sbi);\nvoid f2fs_destroy_stats(struct f2fs_sb_info *sbi);\nvoid __init f2fs_create_root_stats(void);\nvoid f2fs_destroy_root_stats(void);\n#else\n#define stat_inc_cp_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bg_cp_count(si)\t\t\tdo { } while (0)\n#define stat_inc_call_count(si)\t\t\t\tdo { } while (0)\n#define stat_inc_bggc_count(si)\t\t\t\tdo { } while (0)\n#define stat_io_skip_bggc_count(sbi)\t\t\tdo { } while (0)\n#define stat_other_skip_bggc_count(sbi)\t\t\tdo { } while (0)\n#define stat_inc_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_dec_dirty_inode(sbi, type)\t\t\tdo { } while (0)\n#define stat_inc_total_hit(sb)\t\t\t\tdo { } while (0)\n#define stat_inc_rbtree_node_hit(sb)\t\t\tdo { } while (0)\n#define stat_inc_largest_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_cached_node_hit(sbi)\t\t\tdo { } while (0)\n#define stat_inc_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_xattr(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_inode(inode)\t\t\tdo { } while (0)\n#define stat_inc_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_dec_inline_dir(inode)\t\t\tdo { } while (0)\n#define stat_inc_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_atomic_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_atomic_write(inode)\t\tdo { } while (0)\n#define stat_inc_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_dec_volatile_write(inode)\t\t\tdo { } while (0)\n#define stat_update_max_volatile_write(inode)\t\tdo { } while (0)\n#define stat_inc_meta_count(sbi, blkaddr)\t\tdo { } while (0)\n#define stat_inc_seg_type(sbi, curseg)\t\t\tdo { } while (0)\n#define stat_inc_block_count(sbi, curseg)\t\tdo { } while (0)\n#define stat_inc_inplace_blocks(sbi)\t\t\tdo { } while (0)\n#define stat_inc_seg_count(sbi, type, gc_type)\t\tdo { } while (0)\n#define stat_inc_tot_blk_count(si, blks)\t\tdo { } while (0)\n#define stat_inc_data_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n#define stat_inc_node_blk_count(sbi, blks, gc_type)\tdo { } while (0)\n\nstatic inline int f2fs_build_stats(struct f2fs_sb_info *sbi) { return 0; }\nstatic inline void f2fs_destroy_stats(struct f2fs_sb_info *sbi) { }\nstatic inline void __init f2fs_create_root_stats(void) { }\nstatic inline void f2fs_destroy_root_stats(void) { }\n#endif\n\nextern const struct file_operations f2fs_dir_operations;\nextern const struct file_operations f2fs_file_operations;\nextern const struct inode_operations f2fs_file_inode_operations;\nextern const struct address_space_operations f2fs_dblock_aops;\nextern const struct address_space_operations f2fs_node_aops;\nextern const struct address_space_operations f2fs_meta_aops;\nextern const struct inode_operations f2fs_dir_inode_operations;\nextern const struct inode_operations f2fs_symlink_inode_operations;\nextern const struct inode_operations f2fs_encrypted_symlink_inode_operations;\nextern const struct inode_operations f2fs_special_inode_operations;\nextern struct kmem_cache *f2fs_inode_entry_slab;\n\n/*\n * inline.c\n */\nbool f2fs_may_inline_data(struct inode *inode);\nbool f2fs_may_inline_dentry(struct inode *inode);\nvoid f2fs_do_read_inline_data(struct page *page, struct page *ipage);\nvoid f2fs_truncate_inline_inode(struct inode *inode,\n\t\t\t\t\t\tstruct page *ipage, u64 from);\nint f2fs_read_inline_data(struct inode *inode, struct page *page);\nint f2fs_convert_inline_page(struct dnode_of_data *dn, struct page *page);\nint f2fs_convert_inline_inode(struct inode *inode);\nint f2fs_write_inline_data(struct inode *inode, struct page *page);\nbool f2fs_recover_inline_data(struct inode *inode, struct page *npage);\nstruct f2fs_dir_entry *f2fs_find_in_inline_dir(struct inode *dir,\n\t\t\tstruct fscrypt_name *fname, struct page **res_page);\nint f2fs_make_empty_inline_dir(struct inode *inode, struct inode *parent,\n\t\t\tstruct page *ipage);\nint f2fs_add_inline_entry(struct inode *dir, const struct qstr *new_name,\n\t\t\tconst struct qstr *orig_name,\n\t\t\tstruct inode *inode, nid_t ino, umode_t mode);\nvoid f2fs_delete_inline_entry(struct f2fs_dir_entry *dentry,\n\t\t\t\tstruct page *page, struct inode *dir,\n\t\t\t\tstruct inode *inode);\nbool f2fs_empty_inline_dir(struct inode *dir);\nint f2fs_read_inline_dir(struct file *file, struct dir_context *ctx,\n\t\t\tstruct fscrypt_str *fstr);\nint f2fs_inline_data_fiemap(struct inode *inode,\n\t\t\tstruct fiemap_extent_info *fieinfo,\n\t\t\t__u64 start, __u64 len);\n\n/*\n * shrinker.c\n */\nunsigned long f2fs_shrink_count(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nunsigned long f2fs_shrink_scan(struct shrinker *shrink,\n\t\t\tstruct shrink_control *sc);\nvoid f2fs_join_shrinker(struct f2fs_sb_info *sbi);\nvoid f2fs_leave_shrinker(struct f2fs_sb_info *sbi);\n\n/*\n * extent_cache.c\n */\nstruct rb_entry *f2fs_lookup_rb_tree(struct rb_root_cached *root,\n\t\t\t\tstruct rb_entry *cached_re, unsigned int ofs);\nstruct rb_node **f2fs_lookup_rb_tree_for_insert(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct rb_root_cached *root,\n\t\t\t\tstruct rb_node **parent,\n\t\t\t\tunsigned int ofs, bool *leftmost);\nstruct rb_entry *f2fs_lookup_rb_tree_ret(struct rb_root_cached *root,\n\t\tstruct rb_entry *cached_re, unsigned int ofs,\n\t\tstruct rb_entry **prev_entry, struct rb_entry **next_entry,\n\t\tstruct rb_node ***insert_p, struct rb_node **insert_parent,\n\t\tbool force, bool *leftmost);\nbool f2fs_check_rb_tree_consistence(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct rb_root_cached *root);\nunsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink);\nbool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext);\nvoid f2fs_drop_extent_tree(struct inode *inode);\nunsigned int f2fs_destroy_extent_node(struct inode *inode);\nvoid f2fs_destroy_extent_tree(struct inode *inode);\nbool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,\n\t\t\tstruct extent_info *ei);\nvoid f2fs_update_extent_cache(struct dnode_of_data *dn);\nvoid f2fs_update_extent_cache_range(struct dnode_of_data *dn,\n\t\t\tpgoff_t fofs, block_t blkaddr, unsigned int len);\nvoid f2fs_init_extent_cache_info(struct f2fs_sb_info *sbi);\nint __init f2fs_create_extent_cache(void);\nvoid f2fs_destroy_extent_cache(void);\n\n/*\n * sysfs.c\n */\nint __init f2fs_init_sysfs(void);\nvoid f2fs_exit_sysfs(void);\nint f2fs_register_sysfs(struct f2fs_sb_info *sbi);\nvoid f2fs_unregister_sysfs(struct f2fs_sb_info *sbi);\n\n/*\n * crypto support\n */\nstatic inline bool f2fs_encrypted_file(struct inode *inode)\n{\n\treturn IS_ENCRYPTED(inode) && S_ISREG(inode->i_mode);\n}\n\nstatic inline void f2fs_set_encrypted_inode(struct inode *inode)\n{\n#ifdef CONFIG_FS_ENCRYPTION\n\tfile_set_encrypt(inode);\n\tf2fs_set_inode_flags(inode);\n#endif\n}\n\n/*\n * Returns true if the reads of the inode's data need to undergo some\n * postprocessing step, like decryption or authenticity verification.\n */\nstatic inline bool f2fs_post_read_required(struct inode *inode)\n{\n\treturn f2fs_encrypted_file(inode);\n}\n\n#define F2FS_FEATURE_FUNCS(name, flagname) \\\nstatic inline int f2fs_sb_has_##name(struct f2fs_sb_info *sbi) \\\n{ \\\n\treturn F2FS_HAS_FEATURE(sbi, F2FS_FEATURE_##flagname); \\\n}\n\nF2FS_FEATURE_FUNCS(encrypt, ENCRYPT);\nF2FS_FEATURE_FUNCS(blkzoned, BLKZONED);\nF2FS_FEATURE_FUNCS(extra_attr, EXTRA_ATTR);\nF2FS_FEATURE_FUNCS(project_quota, PRJQUOTA);\nF2FS_FEATURE_FUNCS(inode_chksum, INODE_CHKSUM);\nF2FS_FEATURE_FUNCS(flexible_inline_xattr, FLEXIBLE_INLINE_XATTR);\nF2FS_FEATURE_FUNCS(quota_ino, QUOTA_INO);\nF2FS_FEATURE_FUNCS(inode_crtime, INODE_CRTIME);\nF2FS_FEATURE_FUNCS(lost_found, LOST_FOUND);\nF2FS_FEATURE_FUNCS(sb_chksum, SB_CHKSUM);\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline bool f2fs_blkz_is_seq(struct f2fs_sb_info *sbi, int devi,\n\t\t\t\t    block_t blkaddr)\n{\n\tunsigned int zno = blkaddr >> sbi->log_blocks_per_blkz;\n\n\treturn test_bit(zno, FDEV(devi).blkz_seq);\n}\n#endif\n\nstatic inline bool f2fs_hw_should_discard(struct f2fs_sb_info *sbi)\n{\n\treturn f2fs_sb_has_blkzoned(sbi);\n}\n\nstatic inline bool f2fs_bdev_support_discard(struct block_device *bdev)\n{\n\treturn blk_queue_discard(bdev_get_queue(bdev)) ||\n\t       bdev_is_zoned(bdev);\n}\n\nstatic inline bool f2fs_hw_support_discard(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn f2fs_bdev_support_discard(sbi->sb->s_bdev);\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (f2fs_bdev_support_discard(FDEV(i).bdev))\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic inline bool f2fs_realtime_discard_enable(struct f2fs_sb_info *sbi)\n{\n\treturn (test_opt(sbi, DISCARD) && f2fs_hw_support_discard(sbi)) ||\n\t\t\t\t\tf2fs_hw_should_discard(sbi);\n}\n\nstatic inline bool f2fs_hw_is_readonly(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn bdev_read_only(sbi->sb->s_bdev);\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (bdev_read_only(FDEV(i).bdev))\n\t\t\treturn true;\n\treturn false;\n}\n\n\nstatic inline void set_opt_mode(struct f2fs_sb_info *sbi, unsigned int mt)\n{\n\tclear_opt(sbi, ADAPTIVE);\n\tclear_opt(sbi, LFS);\n\n\tswitch (mt) {\n\tcase F2FS_MOUNT_ADAPTIVE:\n\t\tset_opt(sbi, ADAPTIVE);\n\t\tbreak;\n\tcase F2FS_MOUNT_LFS:\n\t\tset_opt(sbi, LFS);\n\t\tbreak;\n\t}\n}\n\nstatic inline bool f2fs_may_encrypt(struct inode *inode)\n{\n#ifdef CONFIG_FS_ENCRYPTION\n\tumode_t mode = inode->i_mode;\n\n\treturn (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode));\n#else\n\treturn false;\n#endif\n}\n\nstatic inline int block_unaligned_IO(struct inode *inode,\n\t\t\t\tstruct kiocb *iocb, struct iov_iter *iter)\n{\n\tunsigned int i_blkbits = READ_ONCE(inode->i_blkbits);\n\tunsigned int blocksize_mask = (1 << i_blkbits) - 1;\n\tloff_t offset = iocb->ki_pos;\n\tunsigned long align = offset | iov_iter_alignment(iter);\n\n\treturn align & blocksize_mask;\n}\n\nstatic inline int allow_outplace_dio(struct inode *inode,\n\t\t\t\tstruct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint rw = iov_iter_rw(iter);\n\n\treturn (test_opt(sbi, LFS) && (rw == WRITE) &&\n\t\t\t\t!block_unaligned_IO(inode, iocb, iter));\n}\n\nstatic inline bool f2fs_force_buffered_io(struct inode *inode,\n\t\t\t\tstruct kiocb *iocb, struct iov_iter *iter)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint rw = iov_iter_rw(iter);\n\n\tif (f2fs_post_read_required(inode))\n\t\treturn true;\n\tif (f2fs_is_multi_device(sbi))\n\t\treturn true;\n\t/*\n\t * for blkzoned device, fallback direct IO to buffered IO, so\n\t * all IOs can be serialized by log-structured write.\n\t */\n\tif (f2fs_sb_has_blkzoned(sbi))\n\t\treturn true;\n\tif (test_opt(sbi, LFS) && (rw == WRITE) &&\n\t\t\t\tblock_unaligned_IO(inode, iocb, iter))\n\t\treturn true;\n\tif (is_sbi_flag_set(F2FS_I_SB(inode), SBI_CP_DISABLED) &&\n\t\t\t\t\t!(inode->i_flags & S_SWAPFILE))\n\t\treturn true;\n\n\treturn false;\n}\n\n#ifdef CONFIG_F2FS_FAULT_INJECTION\nextern void f2fs_build_fault_attr(struct f2fs_sb_info *sbi, unsigned int rate,\n\t\t\t\t\t\t\tunsigned int type);\n#else\n#define f2fs_build_fault_attr(sbi, rate, type)\t\tdo { } while (0)\n#endif\n\nstatic inline bool is_journalled_quota(struct f2fs_sb_info *sbi)\n{\n#ifdef CONFIG_QUOTA\n\tif (f2fs_sb_has_quota_ino(sbi))\n\t\treturn true;\n\tif (F2FS_OPTION(sbi).s_qf_names[USRQUOTA] ||\n\t\tF2FS_OPTION(sbi).s_qf_names[GRPQUOTA] ||\n\t\tF2FS_OPTION(sbi).s_qf_names[PRJQUOTA])\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n#define EFSBADCRC\tEBADMSG\t\t/* Bad CRC detected */\n#define EFSCORRUPTED\tEUCLEAN\t\t/* Filesystem is corrupted */\n\n#endif /* _LINUX_F2FS_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#undef TRACE_SYSTEM\n#define TRACE_SYSTEM f2fs\n\n#if !defined(_TRACE_F2FS_H) || defined(TRACE_HEADER_MULTI_READ)\n#define _TRACE_F2FS_H\n\n#include <linux/tracepoint.h>\n\n#define show_dev(dev)\t\tMAJOR(dev), MINOR(dev)\n#define show_dev_ino(entry)\tshow_dev(entry->dev), (unsigned long)entry->ino\n\nTRACE_DEFINE_ENUM(NODE);\nTRACE_DEFINE_ENUM(DATA);\nTRACE_DEFINE_ENUM(META);\nTRACE_DEFINE_ENUM(META_FLUSH);\nTRACE_DEFINE_ENUM(INMEM);\nTRACE_DEFINE_ENUM(INMEM_DROP);\nTRACE_DEFINE_ENUM(INMEM_INVALIDATE);\nTRACE_DEFINE_ENUM(INMEM_REVOKE);\nTRACE_DEFINE_ENUM(IPU);\nTRACE_DEFINE_ENUM(OPU);\nTRACE_DEFINE_ENUM(HOT);\nTRACE_DEFINE_ENUM(WARM);\nTRACE_DEFINE_ENUM(COLD);\nTRACE_DEFINE_ENUM(CURSEG_HOT_DATA);\nTRACE_DEFINE_ENUM(CURSEG_WARM_DATA);\nTRACE_DEFINE_ENUM(CURSEG_COLD_DATA);\nTRACE_DEFINE_ENUM(CURSEG_HOT_NODE);\nTRACE_DEFINE_ENUM(CURSEG_WARM_NODE);\nTRACE_DEFINE_ENUM(CURSEG_COLD_NODE);\nTRACE_DEFINE_ENUM(NO_CHECK_TYPE);\nTRACE_DEFINE_ENUM(GC_GREEDY);\nTRACE_DEFINE_ENUM(GC_CB);\nTRACE_DEFINE_ENUM(FG_GC);\nTRACE_DEFINE_ENUM(BG_GC);\nTRACE_DEFINE_ENUM(LFS);\nTRACE_DEFINE_ENUM(SSR);\nTRACE_DEFINE_ENUM(__REQ_RAHEAD);\nTRACE_DEFINE_ENUM(__REQ_SYNC);\nTRACE_DEFINE_ENUM(__REQ_IDLE);\nTRACE_DEFINE_ENUM(__REQ_PREFLUSH);\nTRACE_DEFINE_ENUM(__REQ_FUA);\nTRACE_DEFINE_ENUM(__REQ_PRIO);\nTRACE_DEFINE_ENUM(__REQ_META);\nTRACE_DEFINE_ENUM(CP_UMOUNT);\nTRACE_DEFINE_ENUM(CP_FASTBOOT);\nTRACE_DEFINE_ENUM(CP_SYNC);\nTRACE_DEFINE_ENUM(CP_RECOVERY);\nTRACE_DEFINE_ENUM(CP_DISCARD);\nTRACE_DEFINE_ENUM(CP_TRIMMED);\n\n#define show_block_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ NODE,\t\t\"NODE\" },\t\t\t\t\\\n\t\t{ DATA,\t\t\"DATA\" },\t\t\t\t\\\n\t\t{ META,\t\t\"META\" },\t\t\t\t\\\n\t\t{ META_FLUSH,\t\"META_FLUSH\" },\t\t\t\t\\\n\t\t{ INMEM,\t\"INMEM\" },\t\t\t\t\\\n\t\t{ INMEM_DROP,\t\"INMEM_DROP\" },\t\t\t\t\\\n\t\t{ INMEM_INVALIDATE,\t\"INMEM_INVALIDATE\" },\t\t\\\n\t\t{ INMEM_REVOKE,\t\"INMEM_REVOKE\" },\t\t\t\\\n\t\t{ IPU,\t\t\"IN-PLACE\" },\t\t\t\t\\\n\t\t{ OPU,\t\t\"OUT-OF-PLACE\" })\n\n#define show_block_temp(temp)\t\t\t\t\t\t\\\n\t__print_symbolic(temp,\t\t\t\t\t\t\\\n\t\t{ HOT,\t\t\"HOT\" },\t\t\t\t\\\n\t\t{ WARM,\t\t\"WARM\" },\t\t\t\t\\\n\t\t{ COLD,\t\t\"COLD\" })\n\n#define F2FS_OP_FLAGS (REQ_RAHEAD | REQ_SYNC | REQ_META | REQ_PRIO |\t\\\n\t\t\tREQ_PREFLUSH | REQ_FUA)\n#define F2FS_BIO_FLAG_MASK(t)\t(t & F2FS_OP_FLAGS)\n\n#define show_bio_type(op,op_flags)\tshow_bio_op(op),\t\t\\\n\t\t\t\t\t\tshow_bio_op_flags(op_flags)\n\n#define show_bio_op(op)\t\t\t\t\t\t\t\\\n\t__print_symbolic(op,\t\t\t\t\t\t\\\n\t\t{ REQ_OP_READ,\t\t\t\"READ\" },\t\t\\\n\t\t{ REQ_OP_WRITE,\t\t\t\"WRITE\" },\t\t\\\n\t\t{ REQ_OP_FLUSH,\t\t\t\"FLUSH\" },\t\t\\\n\t\t{ REQ_OP_DISCARD,\t\t\"DISCARD\" },\t\t\\\n\t\t{ REQ_OP_SECURE_ERASE,\t\t\"SECURE_ERASE\" },\t\\\n\t\t{ REQ_OP_ZONE_RESET,\t\t\"ZONE_RESET\" },\t\t\\\n\t\t{ REQ_OP_WRITE_SAME,\t\t\"WRITE_SAME\" },\t\t\\\n\t\t{ REQ_OP_WRITE_ZEROES,\t\t\"WRITE_ZEROES\" })\n\n#define show_bio_op_flags(flags)\t\t\t\t\t\\\n\t__print_flags(F2FS_BIO_FLAG_MASK(flags), \"|\",\t\t\t\\\n\t\t{ REQ_RAHEAD,\t\t\"R\" },\t\t\t\t\\\n\t\t{ REQ_SYNC,\t\t\"S\" },\t\t\t\t\\\n\t\t{ REQ_META,\t\t\"M\" },\t\t\t\t\\\n\t\t{ REQ_PRIO,\t\t\"P\" },\t\t\t\t\\\n\t\t{ REQ_PREFLUSH,\t\t\"PF\" },\t\t\t\t\\\n\t\t{ REQ_FUA,\t\t\"FUA\" })\n\n#define show_data_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ CURSEG_HOT_DATA, \t\"Hot DATA\" },\t\t\t\\\n\t\t{ CURSEG_WARM_DATA, \t\"Warm DATA\" },\t\t\t\\\n\t\t{ CURSEG_COLD_DATA, \t\"Cold DATA\" },\t\t\t\\\n\t\t{ CURSEG_HOT_NODE, \t\"Hot NODE\" },\t\t\t\\\n\t\t{ CURSEG_WARM_NODE, \t\"Warm NODE\" },\t\t\t\\\n\t\t{ CURSEG_COLD_NODE, \t\"Cold NODE\" },\t\t\t\\\n\t\t{ NO_CHECK_TYPE, \t\"No TYPE\" })\n\n#define show_file_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ 0,\t\t\"FILE\" },\t\t\t\t\\\n\t\t{ 1,\t\t\"DIR\" })\n\n#define show_gc_type(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ FG_GC,\t\"Foreground GC\" },\t\t\t\\\n\t\t{ BG_GC,\t\"Background GC\" })\n\n#define show_alloc_mode(type)\t\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ LFS,\t\"LFS-mode\" },\t\t\t\t\t\\\n\t\t{ SSR,\t\"SSR-mode\" })\n\n#define show_victim_policy(type)\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ GC_GREEDY,\t\"Greedy\" },\t\t\t\t\\\n\t\t{ GC_CB,\t\"Cost-Benefit\" })\n\n#define show_cpreason(type)\t\t\t\t\t\t\\\n\t__print_flags(type, \"|\",\t\t\t\t\t\\\n\t\t{ CP_UMOUNT,\t\"Umount\" },\t\t\t\t\\\n\t\t{ CP_FASTBOOT,\t\"Fastboot\" },\t\t\t\t\\\n\t\t{ CP_SYNC,\t\"Sync\" },\t\t\t\t\\\n\t\t{ CP_RECOVERY,\t\"Recovery\" },\t\t\t\t\\\n\t\t{ CP_DISCARD,\t\"Discard\" },\t\t\t\t\\\n\t\t{ CP_UMOUNT,\t\"Umount\" },\t\t\t\t\\\n\t\t{ CP_TRIMMED,\t\"Trimmed\" })\n\n#define show_fsync_cpreason(type)\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ CP_NO_NEEDED,\t\t\"no needed\" },\t\t\t\\\n\t\t{ CP_NON_REGULAR,\t\"non regular\" },\t\t\\\n\t\t{ CP_HARDLINK,\t\t\"hardlink\" },\t\t\t\\\n\t\t{ CP_SB_NEED_CP,\t\"sb needs cp\" },\t\t\\\n\t\t{ CP_WRONG_PINO,\t\"wrong pino\" },\t\t\t\\\n\t\t{ CP_NO_SPC_ROLL,\t\"no space roll forward\" },\t\\\n\t\t{ CP_NODE_NEED_CP,\t\"node needs cp\" },\t\t\\\n\t\t{ CP_FASTBOOT_MODE,\t\"fastboot mode\" },\t\t\\\n\t\t{ CP_SPEC_LOG_NUM,\t\"log type is 2\" },\t\t\\\n\t\t{ CP_RECOVER_DIR,\t\"dir needs recovery\" })\n\n#define show_shutdown_mode(type)\t\t\t\t\t\\\n\t__print_symbolic(type,\t\t\t\t\t\t\\\n\t\t{ F2FS_GOING_DOWN_FULLSYNC,\t\"full sync\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_METASYNC,\t\"meta sync\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_NOSYNC,\t\"no sync\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_METAFLUSH,\t\"meta flush\" },\t\t\\\n\t\t{ F2FS_GOING_DOWN_NEED_FSCK,\t\"need fsck\" })\n\nstruct f2fs_sb_info;\nstruct f2fs_io_info;\nstruct extent_info;\nstruct victim_sel_policy;\nstruct f2fs_map_blocks;\n\nDECLARE_EVENT_CLASS(f2fs__inode,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(ino_t,\tpino)\n\t\t__field(umode_t, mode)\n\t\t__field(loff_t,\tsize)\n\t\t__field(unsigned int, nlink)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(__u8,\tadvise)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pino\t= F2FS_I(inode)->i_pino;\n\t\t__entry->mode\t= inode->i_mode;\n\t\t__entry->nlink\t= inode->i_nlink;\n\t\t__entry->size\t= inode->i_size;\n\t\t__entry->blocks\t= inode->i_blocks;\n\t\t__entry->advise\t= F2FS_I(inode)->i_advise;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pino = %lu, i_mode = 0x%hx, \"\n\t\t\"i_size = %lld, i_nlink = %u, i_blocks = %llu, i_advise = 0x%x\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->pino,\n\t\t__entry->mode,\n\t\t__entry->size,\n\t\t(unsigned int)__entry->nlink,\n\t\t(unsigned long long)__entry->blocks,\n\t\t(unsigned char)__entry->advise)\n);\n\nDECLARE_EVENT_CLASS(f2fs__inode_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->ret)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_sync_file_enter,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nTRACE_EVENT(f2fs_sync_file_exit,\n\n\tTP_PROTO(struct inode *inode, int cp_reason, int datasync, int ret),\n\n\tTP_ARGS(inode, cp_reason, datasync, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\tcp_reason)\n\t\t__field(int,\tdatasync)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->cp_reason\t= cp_reason;\n\t\t__entry->datasync\t= datasync;\n\t\t__entry->ret\t\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, cp_reason: %s, \"\n\t\t\"datasync = %d, ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\tshow_fsync_cpreason(__entry->cp_reason),\n\t\t__entry->datasync,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_sync_fs,\n\n\tTP_PROTO(struct super_block *sb, int wait),\n\n\tTP_ARGS(sb, wait),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(int,\tdirty)\n\t\t__field(int,\twait)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= sb->s_dev;\n\t\t__entry->dirty\t= is_sbi_flag_set(F2FS_SB(sb), SBI_IS_DIRTY);\n\t\t__entry->wait\t= wait;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), superblock is %s, wait = %d\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->dirty ? \"dirty\" : \"not dirty\",\n\t\t__entry->wait)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_iget,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_iget_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_evict_inode,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_new_inode,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nTRACE_EVENT(f2fs_unlink_enter,\n\n\tTP_PROTO(struct inode *dir, struct dentry *dentry),\n\n\tTP_ARGS(dir, dentry),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tsize)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(const char *,\tname)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->size\t= dir->i_size;\n\t\t__entry->blocks\t= dir->i_blocks;\n\t\t__entry->name\t= dentry->d_name.name;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), dir ino = %lu, i_size = %lld, \"\n\t\t\"i_blocks = %llu, name = %s\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->size,\n\t\t(unsigned long long)__entry->blocks,\n\t\t__entry->name)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_unlink_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_drop_inode,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__inode, f2fs_truncate,\n\n\tTP_PROTO(struct inode *inode),\n\n\tTP_ARGS(inode)\n);\n\nTRACE_EVENT(f2fs_truncate_data_blocks_range,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, unsigned int ofs, int free),\n\n\tTP_ARGS(inode, nid,  ofs, free),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(nid_t,\tnid)\n\t\t__field(unsigned int,\tofs)\n\t\t__field(int,\tfree)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->nid\t= nid;\n\t\t__entry->ofs\t= ofs;\n\t\t__entry->free\t= free;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, nid = %u, offset = %u, freed = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned int)__entry->nid,\n\t\t__entry->ofs,\n\t\t__entry->free)\n);\n\nDECLARE_EVENT_CLASS(f2fs__truncate_op,\n\n\tTP_PROTO(struct inode *inode, u64 from),\n\n\tTP_ARGS(inode, from),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tsize)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(u64,\tfrom)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->size\t= inode->i_size;\n\t\t__entry->blocks\t= inode->i_blocks;\n\t\t__entry->from\t= from;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, i_size = %lld, i_blocks = %llu, \"\n\t\t\"start file offset = %llu\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->size,\n\t\t(unsigned long long)__entry->blocks,\n\t\t(unsigned long long)__entry->from)\n);\n\nDEFINE_EVENT(f2fs__truncate_op, f2fs_truncate_blocks_enter,\n\n\tTP_PROTO(struct inode *inode, u64 from),\n\n\tTP_ARGS(inode, from)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_truncate_blocks_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__truncate_op, f2fs_truncate_inode_blocks_enter,\n\n\tTP_PROTO(struct inode *inode, u64 from),\n\n\tTP_ARGS(inode, from)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_truncate_inode_blocks_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDECLARE_EVENT_CLASS(f2fs__truncate_node,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, block_t blk_addr),\n\n\tTP_ARGS(inode, nid, blk_addr),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(nid_t,\tnid)\n\t\t__field(block_t,\tblk_addr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->nid\t\t= nid;\n\t\t__entry->blk_addr\t= blk_addr;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, nid = %u, block_address = 0x%llx\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned int)__entry->nid,\n\t\t(unsigned long long)__entry->blk_addr)\n);\n\nDEFINE_EVENT(f2fs__truncate_node, f2fs_truncate_nodes_enter,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, block_t blk_addr),\n\n\tTP_ARGS(inode, nid, blk_addr)\n);\n\nDEFINE_EVENT(f2fs__inode_exit, f2fs_truncate_nodes_exit,\n\n\tTP_PROTO(struct inode *inode, int ret),\n\n\tTP_ARGS(inode, ret)\n);\n\nDEFINE_EVENT(f2fs__truncate_node, f2fs_truncate_node,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, block_t blk_addr),\n\n\tTP_ARGS(inode, nid, blk_addr)\n);\n\nTRACE_EVENT(f2fs_truncate_partial_nodes,\n\n\tTP_PROTO(struct inode *inode, nid_t *nid, int depth, int err),\n\n\tTP_ARGS(inode, nid, depth, err),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(nid_t,\tnid[3])\n\t\t__field(int,\tdepth)\n\t\t__field(int,\terr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->nid[0]\t= nid[0];\n\t\t__entry->nid[1]\t= nid[1];\n\t\t__entry->nid[2]\t= nid[2];\n\t\t__entry->depth\t= depth;\n\t\t__entry->err\t= err;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, \"\n\t\t\"nid[0] = %u, nid[1] = %u, nid[2] = %u, depth = %d, err = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned int)__entry->nid[0],\n\t\t(unsigned int)__entry->nid[1],\n\t\t(unsigned int)__entry->nid[2],\n\t\t__entry->depth,\n\t\t__entry->err)\n);\n\nTRACE_EVENT(f2fs_file_write_iter,\n\n\tTP_PROTO(struct inode *inode, unsigned long offset,\n\t\tunsigned long length, int ret),\n\n\tTP_ARGS(inode, offset, length, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned long, offset)\n\t\t__field(unsigned long, length)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->offset\t= offset;\n\t\t__entry->length\t= length;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, \"\n\t\t\"offset = %lu, length = %lu, written(err) = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->offset,\n\t\t__entry->length,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_map_blocks,\n\tTP_PROTO(struct inode *inode, struct f2fs_map_blocks *map, int ret),\n\n\tTP_ARGS(inode, map, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(block_t,\tm_lblk)\n\t\t__field(block_t,\tm_pblk)\n\t\t__field(unsigned int,\tm_len)\n\t\t__field(unsigned int,\tm_flags)\n\t\t__field(int,\tm_seg_type)\n\t\t__field(bool,\tm_may_create)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->m_lblk\t\t= map->m_lblk;\n\t\t__entry->m_pblk\t\t= map->m_pblk;\n\t\t__entry->m_len\t\t= map->m_len;\n\t\t__entry->m_flags\t= map->m_flags;\n\t\t__entry->m_seg_type\t= map->m_seg_type;\n\t\t__entry->m_may_create\t= map->m_may_create;\n\t\t__entry->ret\t\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, file offset = %llu, \"\n\t\t\"start blkaddr = 0x%llx, len = 0x%llx, flags = %u,\"\n\t\t\"seg_type = %d, may_create = %d, err = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long long)__entry->m_lblk,\n\t\t(unsigned long long)__entry->m_pblk,\n\t\t(unsigned long long)__entry->m_len,\n\t\t__entry->m_flags,\n\t\t__entry->m_seg_type,\n\t\t__entry->m_may_create,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_background_gc,\n\n\tTP_PROTO(struct super_block *sb, unsigned int wait_ms,\n\t\t\tunsigned int prefree, unsigned int free),\n\n\tTP_ARGS(sb, wait_ms, prefree, free),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int,\twait_ms)\n\t\t__field(unsigned int,\tprefree)\n\t\t__field(unsigned int,\tfree)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->wait_ms\t= wait_ms;\n\t\t__entry->prefree\t= prefree;\n\t\t__entry->free\t\t= free;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), wait_ms = %u, prefree = %u, free = %u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->wait_ms,\n\t\t__entry->prefree,\n\t\t__entry->free)\n);\n\nTRACE_EVENT(f2fs_gc_begin,\n\n\tTP_PROTO(struct super_block *sb, bool sync, bool background,\n\t\t\tlong long dirty_nodes, long long dirty_dents,\n\t\t\tlong long dirty_imeta, unsigned int free_sec,\n\t\t\tunsigned int free_seg, int reserved_seg,\n\t\t\tunsigned int prefree_seg),\n\n\tTP_ARGS(sb, sync, background, dirty_nodes, dirty_dents, dirty_imeta,\n\t\tfree_sec, free_seg, reserved_seg, prefree_seg),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev)\n\t\t__field(bool,\t\tsync)\n\t\t__field(bool,\t\tbackground)\n\t\t__field(long long,\tdirty_nodes)\n\t\t__field(long long,\tdirty_dents)\n\t\t__field(long long,\tdirty_imeta)\n\t\t__field(unsigned int,\tfree_sec)\n\t\t__field(unsigned int,\tfree_seg)\n\t\t__field(int,\t\treserved_seg)\n\t\t__field(unsigned int,\tprefree_seg)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->sync\t\t= sync;\n\t\t__entry->background\t= background;\n\t\t__entry->dirty_nodes\t= dirty_nodes;\n\t\t__entry->dirty_dents\t= dirty_dents;\n\t\t__entry->dirty_imeta\t= dirty_imeta;\n\t\t__entry->free_sec\t= free_sec;\n\t\t__entry->free_seg\t= free_seg;\n\t\t__entry->reserved_seg\t= reserved_seg;\n\t\t__entry->prefree_seg\t= prefree_seg;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), sync = %d, background = %d, nodes = %lld, \"\n\t\t\"dents = %lld, imeta = %lld, free_sec:%u, free_seg:%u, \"\n\t\t\"rsv_seg:%d, prefree_seg:%u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->sync,\n\t\t__entry->background,\n\t\t__entry->dirty_nodes,\n\t\t__entry->dirty_dents,\n\t\t__entry->dirty_imeta,\n\t\t__entry->free_sec,\n\t\t__entry->free_seg,\n\t\t__entry->reserved_seg,\n\t\t__entry->prefree_seg)\n);\n\nTRACE_EVENT(f2fs_gc_end,\n\n\tTP_PROTO(struct super_block *sb, int ret, int seg_freed,\n\t\t\tint sec_freed, long long dirty_nodes,\n\t\t\tlong long dirty_dents, long long dirty_imeta,\n\t\t\tunsigned int free_sec, unsigned int free_seg,\n\t\t\tint reserved_seg, unsigned int prefree_seg),\n\n\tTP_ARGS(sb, ret, seg_freed, sec_freed, dirty_nodes, dirty_dents,\n\t\tdirty_imeta, free_sec, free_seg, reserved_seg, prefree_seg),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev)\n\t\t__field(int,\t\tret)\n\t\t__field(int,\t\tseg_freed)\n\t\t__field(int,\t\tsec_freed)\n\t\t__field(long long,\tdirty_nodes)\n\t\t__field(long long,\tdirty_dents)\n\t\t__field(long long,\tdirty_imeta)\n\t\t__field(unsigned int,\tfree_sec)\n\t\t__field(unsigned int,\tfree_seg)\n\t\t__field(int,\t\treserved_seg)\n\t\t__field(unsigned int,\tprefree_seg)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->ret\t\t= ret;\n\t\t__entry->seg_freed\t= seg_freed;\n\t\t__entry->sec_freed\t= sec_freed;\n\t\t__entry->dirty_nodes\t= dirty_nodes;\n\t\t__entry->dirty_dents\t= dirty_dents;\n\t\t__entry->dirty_imeta\t= dirty_imeta;\n\t\t__entry->free_sec\t= free_sec;\n\t\t__entry->free_seg\t= free_seg;\n\t\t__entry->reserved_seg\t= reserved_seg;\n\t\t__entry->prefree_seg\t= prefree_seg;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ret = %d, seg_freed = %d, sec_freed = %d, \"\n\t\t\"nodes = %lld, dents = %lld, imeta = %lld, free_sec:%u, \"\n\t\t\"free_seg:%u, rsv_seg:%d, prefree_seg:%u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->ret,\n\t\t__entry->seg_freed,\n\t\t__entry->sec_freed,\n\t\t__entry->dirty_nodes,\n\t\t__entry->dirty_dents,\n\t\t__entry->dirty_imeta,\n\t\t__entry->free_sec,\n\t\t__entry->free_seg,\n\t\t__entry->reserved_seg,\n\t\t__entry->prefree_seg)\n);\n\nTRACE_EVENT(f2fs_get_victim,\n\n\tTP_PROTO(struct super_block *sb, int type, int gc_type,\n\t\t\tstruct victim_sel_policy *p, unsigned int pre_victim,\n\t\t\tunsigned int prefree, unsigned int free),\n\n\tTP_ARGS(sb, type, gc_type, p, pre_victim, prefree, free),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(int,\ttype)\n\t\t__field(int,\tgc_type)\n\t\t__field(int,\talloc_mode)\n\t\t__field(int,\tgc_mode)\n\t\t__field(unsigned int,\tvictim)\n\t\t__field(unsigned int,\tcost)\n\t\t__field(unsigned int,\tofs_unit)\n\t\t__field(unsigned int,\tpre_victim)\n\t\t__field(unsigned int,\tprefree)\n\t\t__field(unsigned int,\tfree)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->type\t\t= type;\n\t\t__entry->gc_type\t= gc_type;\n\t\t__entry->alloc_mode\t= p->alloc_mode;\n\t\t__entry->gc_mode\t= p->gc_mode;\n\t\t__entry->victim\t\t= p->min_segno;\n\t\t__entry->cost\t\t= p->min_cost;\n\t\t__entry->ofs_unit\t= p->ofs_unit;\n\t\t__entry->pre_victim\t= pre_victim;\n\t\t__entry->prefree\t= prefree;\n\t\t__entry->free\t\t= free;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), type = %s, policy = (%s, %s, %s), \"\n\t\t\"victim = %u, cost = %u, ofs_unit = %u, \"\n\t\t\"pre_victim_secno = %d, prefree = %u, free = %u\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_data_type(__entry->type),\n\t\tshow_gc_type(__entry->gc_type),\n\t\tshow_alloc_mode(__entry->alloc_mode),\n\t\tshow_victim_policy(__entry->gc_mode),\n\t\t__entry->victim,\n\t\t__entry->cost,\n\t\t__entry->ofs_unit,\n\t\t(int)__entry->pre_victim,\n\t\t__entry->prefree,\n\t\t__entry->free)\n);\n\nTRACE_EVENT(f2fs_lookup_start,\n\n\tTP_PROTO(struct inode *dir, struct dentry *dentry, unsigned int flags),\n\n\tTP_ARGS(dir, dentry, flags),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(const char *,\tname)\n\t\t__field(unsigned int, flags)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->name\t= dentry->d_name.name;\n\t\t__entry->flags\t= flags;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), pino = %lu, name:%s, flags:%u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->name,\n\t\t__entry->flags)\n);\n\nTRACE_EVENT(f2fs_lookup_end,\n\n\tTP_PROTO(struct inode *dir, struct dentry *dentry, nid_t ino,\n\t\tint err),\n\n\tTP_ARGS(dir, dentry, ino, err),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(const char *,\tname)\n\t\t__field(nid_t,\tcino)\n\t\t__field(int,\terr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->name\t= dentry->d_name.name;\n\t\t__entry->cino\t= ino;\n\t\t__entry->err\t= err;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), pino = %lu, name:%s, ino:%u, err:%d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->name,\n\t\t__entry->cino,\n\t\t__entry->err)\n);\n\nTRACE_EVENT(f2fs_readdir,\n\n\tTP_PROTO(struct inode *dir, loff_t start_pos, loff_t end_pos, int err),\n\n\tTP_ARGS(dir, start_pos, end_pos, err),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tstart)\n\t\t__field(loff_t,\tend)\n\t\t__field(int,\terr)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dir->i_sb->s_dev;\n\t\t__entry->ino\t= dir->i_ino;\n\t\t__entry->start\t= start_pos;\n\t\t__entry->end\t= end_pos;\n\t\t__entry->err\t= err;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, start_pos:%llu, end_pos:%llu, err:%d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->start,\n\t\t__entry->end,\n\t\t__entry->err)\n);\n\nTRACE_EVENT(f2fs_fallocate,\n\n\tTP_PROTO(struct inode *inode, int mode,\n\t\t\t\tloff_t offset, loff_t len, int ret),\n\n\tTP_ARGS(inode, mode, offset, len, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\tmode)\n\t\t__field(loff_t,\toffset)\n\t\t__field(loff_t,\tlen)\n\t\t__field(loff_t, size)\n\t\t__field(blkcnt_t, blocks)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->mode\t= mode;\n\t\t__entry->offset\t= offset;\n\t\t__entry->len\t= len;\n\t\t__entry->size\t= inode->i_size;\n\t\t__entry->blocks = inode->i_blocks;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, mode = %x, offset = %lld, \"\n\t\t\"len = %lld,  i_size = %lld, i_blocks = %llu, ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->mode,\n\t\t(unsigned long long)__entry->offset,\n\t\t(unsigned long long)__entry->len,\n\t\t(unsigned long long)__entry->size,\n\t\t(unsigned long long)__entry->blocks,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_direct_IO_enter,\n\n\tTP_PROTO(struct inode *inode, loff_t offset, unsigned long len, int rw),\n\n\tTP_ARGS(inode, offset, len, rw),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned long,\tlen)\n\t\t__field(int,\trw)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= offset;\n\t\t__entry->len\t= len;\n\t\t__entry->rw\t= rw;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu pos = %lld len = %lu rw = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pos,\n\t\t__entry->len,\n\t\t__entry->rw)\n);\n\nTRACE_EVENT(f2fs_direct_IO_exit,\n\n\tTP_PROTO(struct inode *inode, loff_t offset, unsigned long len,\n\t\t int rw, int ret),\n\n\tTP_ARGS(inode, offset, len, rw, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned long,\tlen)\n\t\t__field(int,\trw)\n\t\t__field(int,\tret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= offset;\n\t\t__entry->len\t= len;\n\t\t__entry->rw\t= rw;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu pos = %lld len = %lu \"\n\t\t\"rw = %d ret = %d\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pos,\n\t\t__entry->len,\n\t\t__entry->rw,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_reserve_new_blocks,\n\n\tTP_PROTO(struct inode *inode, nid_t nid, unsigned int ofs_in_node,\n\t\t\t\t\t\t\tblkcnt_t count),\n\n\tTP_ARGS(inode, nid, ofs_in_node, count),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(nid_t, nid)\n\t\t__field(unsigned int, ofs_in_node)\n\t\t__field(blkcnt_t, count)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->nid\t= nid;\n\t\t__entry->ofs_in_node = ofs_in_node;\n\t\t__entry->count = count;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), nid = %u, ofs_in_node = %u, count = %llu\",\n\t\tshow_dev(__entry->dev),\n\t\t(unsigned int)__entry->nid,\n\t\t__entry->ofs_in_node,\n\t\t(unsigned long long)__entry->count)\n);\n\nDECLARE_EVENT_CLASS(f2fs__submit_page_bio,\n\n\tTP_PROTO(struct page *page, struct f2fs_io_info *fio),\n\n\tTP_ARGS(page, fio),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t, dev)\n\t\t__field(ino_t, ino)\n\t\t__field(pgoff_t, index)\n\t\t__field(block_t, old_blkaddr)\n\t\t__field(block_t, new_blkaddr)\n\t\t__field(int, op)\n\t\t__field(int, op_flags)\n\t\t__field(int, temp)\n\t\t__field(int, type)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= page_file_mapping(page)->host->i_sb->s_dev;\n\t\t__entry->ino\t\t= page_file_mapping(page)->host->i_ino;\n\t\t__entry->index\t\t= page->index;\n\t\t__entry->old_blkaddr\t= fio->old_blkaddr;\n\t\t__entry->new_blkaddr\t= fio->new_blkaddr;\n\t\t__entry->op\t\t= fio->op;\n\t\t__entry->op_flags\t= fio->op_flags;\n\t\t__entry->temp\t\t= fio->temp;\n\t\t__entry->type\t\t= fio->type;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, page_index = 0x%lx, \"\n\t\t\"oldaddr = 0x%llx, newaddr = 0x%llx, rw = %s(%s), type = %s_%s\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->index,\n\t\t(unsigned long long)__entry->old_blkaddr,\n\t\t(unsigned long long)__entry->new_blkaddr,\n\t\tshow_bio_type(__entry->op, __entry->op_flags),\n\t\tshow_block_temp(__entry->temp),\n\t\tshow_block_type(__entry->type))\n);\n\nDEFINE_EVENT_CONDITION(f2fs__submit_page_bio, f2fs_submit_page_bio,\n\n\tTP_PROTO(struct page *page, struct f2fs_io_info *fio),\n\n\tTP_ARGS(page, fio),\n\n\tTP_CONDITION(page->mapping)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__submit_page_bio, f2fs_submit_page_write,\n\n\tTP_PROTO(struct page *page, struct f2fs_io_info *fio),\n\n\tTP_ARGS(page, fio),\n\n\tTP_CONDITION(page->mapping)\n);\n\nDECLARE_EVENT_CLASS(f2fs__bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(dev_t,\ttarget)\n\t\t__field(int,\top)\n\t\t__field(int,\top_flags)\n\t\t__field(int,\ttype)\n\t\t__field(sector_t,\tsector)\n\t\t__field(unsigned int,\tsize)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->target\t\t= bio_dev(bio);\n\t\t__entry->op\t\t= bio_op(bio);\n\t\t__entry->op_flags\t= bio->bi_opf;\n\t\t__entry->type\t\t= type;\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->size\t\t= bio->bi_iter.bi_size;\n\t),\n\n\tTP_printk(\"dev = (%d,%d)/(%d,%d), rw = %s(%s), %s, sector = %lld, size = %u\",\n\t\tshow_dev(__entry->target),\n\t\tshow_dev(__entry->dev),\n\t\tshow_bio_type(__entry->op, __entry->op_flags),\n\t\tshow_block_type(__entry->type),\n\t\t(unsigned long long)__entry->sector,\n\t\t__entry->size)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_prepare_write_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_prepare_read_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_submit_read_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nDEFINE_EVENT_CONDITION(f2fs__bio, f2fs_submit_write_bio,\n\n\tTP_PROTO(struct super_block *sb, int type, struct bio *bio),\n\n\tTP_ARGS(sb, type, bio),\n\n\tTP_CONDITION(bio)\n);\n\nTRACE_EVENT(f2fs_write_begin,\n\n\tTP_PROTO(struct inode *inode, loff_t pos, unsigned int len,\n\t\t\t\tunsigned int flags),\n\n\tTP_ARGS(inode, pos, len, flags),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned int, len)\n\t\t__field(unsigned int, flags)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= pos;\n\t\t__entry->len\t= len;\n\t\t__entry->flags\t= flags;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pos = %llu, len = %u, flags = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long long)__entry->pos,\n\t\t__entry->len,\n\t\t__entry->flags)\n);\n\nTRACE_EVENT(f2fs_write_end,\n\n\tTP_PROTO(struct inode *inode, loff_t pos, unsigned int len,\n\t\t\t\tunsigned int copied),\n\n\tTP_ARGS(inode, pos, len, copied),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(loff_t,\tpos)\n\t\t__field(unsigned int, len)\n\t\t__field(unsigned int, copied)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->pos\t= pos;\n\t\t__entry->len\t= len;\n\t\t__entry->copied\t= copied;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pos = %llu, len = %u, copied = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long long)__entry->pos,\n\t\t__entry->len,\n\t\t__entry->copied)\n);\n\nDECLARE_EVENT_CLASS(f2fs__page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int, type)\n\t\t__field(int, dir)\n\t\t__field(pgoff_t, index)\n\t\t__field(int, dirty)\n\t\t__field(int, uptodate)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= page_file_mapping(page)->host->i_sb->s_dev;\n\t\t__entry->ino\t= page_file_mapping(page)->host->i_ino;\n\t\t__entry->type\t= type;\n\t\t__entry->dir\t=\n\t\t\tS_ISDIR(page_file_mapping(page)->host->i_mode);\n\t\t__entry->index\t= page->index;\n\t\t__entry->dirty\t= PageDirty(page);\n\t\t__entry->uptodate = PageUptodate(page);\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, %s, %s, index = %lu, \"\n\t\t\"dirty = %d, uptodate = %d\",\n\t\tshow_dev_ino(__entry),\n\t\tshow_block_type(__entry->type),\n\t\tshow_file_type(__entry->dir),\n\t\t(unsigned long)__entry->index,\n\t\t__entry->dirty,\n\t\t__entry->uptodate)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_writepage,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_do_write_data_page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_readpage,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_set_page_dirty,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_vm_page_mkwrite,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_register_inmem_page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nDEFINE_EVENT(f2fs__page, f2fs_commit_inmem_page,\n\n\tTP_PROTO(struct page *page, int type),\n\n\tTP_ARGS(page, type)\n);\n\nTRACE_EVENT(f2fs_filemap_fault,\n\n\tTP_PROTO(struct inode *inode, pgoff_t index, unsigned long ret),\n\n\tTP_ARGS(inode, index, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(pgoff_t, index)\n\t\t__field(unsigned long, ret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->index\t= index;\n\t\t__entry->ret\t= ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, index = %lu, ret = %lx\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->index,\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_writepages,\n\n\tTP_PROTO(struct inode *inode, struct writeback_control *wbc, int type),\n\n\tTP_ARGS(inode, wbc, type),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(int,\ttype)\n\t\t__field(int,\tdir)\n\t\t__field(long,\tnr_to_write)\n\t\t__field(long,\tpages_skipped)\n\t\t__field(loff_t,\trange_start)\n\t\t__field(loff_t,\trange_end)\n\t\t__field(pgoff_t, writeback_index)\n\t\t__field(int,\tsync_mode)\n\t\t__field(char,\tfor_kupdate)\n\t\t__field(char,\tfor_background)\n\t\t__field(char,\ttagged_writepages)\n\t\t__field(char,\tfor_reclaim)\n\t\t__field(char,\trange_cyclic)\n\t\t__field(char,\tfor_sync)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t\t= inode->i_ino;\n\t\t__entry->type\t\t= type;\n\t\t__entry->dir\t\t= S_ISDIR(inode->i_mode);\n\t\t__entry->nr_to_write\t= wbc->nr_to_write;\n\t\t__entry->pages_skipped\t= wbc->pages_skipped;\n\t\t__entry->range_start\t= wbc->range_start;\n\t\t__entry->range_end\t= wbc->range_end;\n\t\t__entry->writeback_index = inode->i_mapping->writeback_index;\n\t\t__entry->sync_mode\t= wbc->sync_mode;\n\t\t__entry->for_kupdate\t= wbc->for_kupdate;\n\t\t__entry->for_background\t= wbc->for_background;\n\t\t__entry->tagged_writepages\t= wbc->tagged_writepages;\n\t\t__entry->for_reclaim\t= wbc->for_reclaim;\n\t\t__entry->range_cyclic\t= wbc->range_cyclic;\n\t\t__entry->for_sync\t= wbc->for_sync;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, %s, %s, nr_to_write %ld, \"\n\t\t\"skipped %ld, start %lld, end %lld, wb_idx %lu, sync_mode %d, \"\n\t\t\"kupdate %u background %u tagged %u reclaim %u cyclic %u sync %u\",\n\t\tshow_dev_ino(__entry),\n\t\tshow_block_type(__entry->type),\n\t\tshow_file_type(__entry->dir),\n\t\t__entry->nr_to_write,\n\t\t__entry->pages_skipped,\n\t\t__entry->range_start,\n\t\t__entry->range_end,\n\t\t(unsigned long)__entry->writeback_index,\n\t\t__entry->sync_mode,\n\t\t__entry->for_kupdate,\n\t\t__entry->for_background,\n\t\t__entry->tagged_writepages,\n\t\t__entry->for_reclaim,\n\t\t__entry->range_cyclic,\n\t\t__entry->for_sync)\n);\n\nTRACE_EVENT(f2fs_readpages,\n\n\tTP_PROTO(struct inode *inode, struct page *page, unsigned int nrpage),\n\n\tTP_ARGS(inode, page, nrpage),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(pgoff_t,\tstart)\n\t\t__field(unsigned int,\tnrpage)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= inode->i_sb->s_dev;\n\t\t__entry->ino\t= inode->i_ino;\n\t\t__entry->start\t= page->index;\n\t\t__entry->nrpage\t= nrpage;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, start = %lu nrpage = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t(unsigned long)__entry->start,\n\t\t__entry->nrpage)\n);\n\nTRACE_EVENT(f2fs_write_checkpoint,\n\n\tTP_PROTO(struct super_block *sb, int reason, char *msg),\n\n\tTP_ARGS(sb, reason, msg),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(int,\treason)\n\t\t__field(char *,\tmsg)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= sb->s_dev;\n\t\t__entry->reason\t\t= reason;\n\t\t__entry->msg\t\t= msg;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), checkpoint for %s, state = %s\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_cpreason(__entry->reason),\n\t\t__entry->msg)\n);\n\nDECLARE_EVENT_CLASS(f2fs_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(block_t, blkstart)\n\t\t__field(block_t, blklen)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dev->bd_dev;\n\t\t__entry->blkstart = blkstart;\n\t\t__entry->blklen = blklen;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), blkstart = 0x%llx, blklen = 0x%llx\",\n\t\tshow_dev(__entry->dev),\n\t\t(unsigned long long)__entry->blkstart,\n\t\t(unsigned long long)__entry->blklen)\n);\n\nDEFINE_EVENT(f2fs_discard, f2fs_queue_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen)\n);\n\nDEFINE_EVENT(f2fs_discard, f2fs_issue_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen)\n);\n\nDEFINE_EVENT(f2fs_discard, f2fs_remove_discard,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart, block_t blklen),\n\n\tTP_ARGS(dev, blkstart, blklen)\n);\n\nTRACE_EVENT(f2fs_issue_reset_zone,\n\n\tTP_PROTO(struct block_device *dev, block_t blkstart),\n\n\tTP_ARGS(dev, blkstart),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(block_t, blkstart)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dev->bd_dev;\n\t\t__entry->blkstart = blkstart;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), reset zone at block = 0x%llx\",\n\t\tshow_dev(__entry->dev),\n\t\t(unsigned long long)__entry->blkstart)\n);\n\nTRACE_EVENT(f2fs_issue_flush,\n\n\tTP_PROTO(struct block_device *dev, unsigned int nobarrier,\n\t\t\t\tunsigned int flush_merge, int ret),\n\n\tTP_ARGS(dev, nobarrier, flush_merge, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int, nobarrier)\n\t\t__field(unsigned int, flush_merge)\n\t\t__field(int,  ret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= dev->bd_dev;\n\t\t__entry->nobarrier = nobarrier;\n\t\t__entry->flush_merge = flush_merge;\n\t\t__entry->ret = ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), %s %s, ret = %d\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->nobarrier ? \"skip (nobarrier)\" : \"issue\",\n\t\t__entry->flush_merge ? \" with flush_merge\" : \"\",\n\t\t__entry->ret)\n);\n\nTRACE_EVENT(f2fs_lookup_extent_tree_start,\n\n\tTP_PROTO(struct inode *inode, unsigned int pgofs),\n\n\tTP_ARGS(inode, pgofs),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, pgofs)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->pgofs = pgofs;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pgofs = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pgofs)\n);\n\nTRACE_EVENT_CONDITION(f2fs_lookup_extent_tree_end,\n\n\tTP_PROTO(struct inode *inode, unsigned int pgofs,\n\t\t\t\t\t\tstruct extent_info *ei),\n\n\tTP_ARGS(inode, pgofs, ei),\n\n\tTP_CONDITION(ei),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, pgofs)\n\t\t__field(unsigned int, fofs)\n\t\t__field(u32, blk)\n\t\t__field(unsigned int, len)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->pgofs = pgofs;\n\t\t__entry->fofs = ei->fofs;\n\t\t__entry->blk = ei->blk;\n\t\t__entry->len = ei->len;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pgofs = %u, \"\n\t\t\"ext_info(fofs: %u, blk: %u, len: %u)\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pgofs,\n\t\t__entry->fofs,\n\t\t__entry->blk,\n\t\t__entry->len)\n);\n\nTRACE_EVENT(f2fs_update_extent_tree_range,\n\n\tTP_PROTO(struct inode *inode, unsigned int pgofs, block_t blkaddr,\n\t\t\t\t\t\tunsigned int len),\n\n\tTP_ARGS(inode, pgofs, blkaddr, len),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, pgofs)\n\t\t__field(u32, blk)\n\t\t__field(unsigned int, len)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->pgofs = pgofs;\n\t\t__entry->blk = blkaddr;\n\t\t__entry->len = len;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, pgofs = %u, \"\n\t\t\t\t\t\"blkaddr = %u, len = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->pgofs,\n\t\t__entry->blk,\n\t\t__entry->len)\n);\n\nTRACE_EVENT(f2fs_shrink_extent_tree,\n\n\tTP_PROTO(struct f2fs_sb_info *sbi, unsigned int node_cnt,\n\t\t\t\t\t\tunsigned int tree_cnt),\n\n\tTP_ARGS(sbi, node_cnt, tree_cnt),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int, node_cnt)\n\t\t__field(unsigned int, tree_cnt)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = sbi->sb->s_dev;\n\t\t__entry->node_cnt = node_cnt;\n\t\t__entry->tree_cnt = tree_cnt;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), shrunk: node_cnt = %u, tree_cnt = %u\",\n\t\tshow_dev(__entry->dev),\n\t\t__entry->node_cnt,\n\t\t__entry->tree_cnt)\n);\n\nTRACE_EVENT(f2fs_destroy_extent_tree,\n\n\tTP_PROTO(struct inode *inode, unsigned int node_cnt),\n\n\tTP_ARGS(inode, node_cnt),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(ino_t,\tino)\n\t\t__field(unsigned int, node_cnt)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = inode->i_sb->s_dev;\n\t\t__entry->ino = inode->i_ino;\n\t\t__entry->node_cnt = node_cnt;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), ino = %lu, destroyed: node_cnt = %u\",\n\t\tshow_dev_ino(__entry),\n\t\t__entry->node_cnt)\n);\n\nDECLARE_EVENT_CLASS(f2fs_sync_dirty_inodes,\n\n\tTP_PROTO(struct super_block *sb, int type, s64 count),\n\n\tTP_ARGS(sb, type, count),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t, dev)\n\t\t__field(int, type)\n\t\t__field(s64, count)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t= sb->s_dev;\n\t\t__entry->type\t= type;\n\t\t__entry->count\t= count;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), %s, dirty count = %lld\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_file_type(__entry->type),\n\t\t__entry->count)\n);\n\nDEFINE_EVENT(f2fs_sync_dirty_inodes, f2fs_sync_dirty_inodes_enter,\n\n\tTP_PROTO(struct super_block *sb, int type, s64 count),\n\n\tTP_ARGS(sb, type, count)\n);\n\nDEFINE_EVENT(f2fs_sync_dirty_inodes, f2fs_sync_dirty_inodes_exit,\n\n\tTP_PROTO(struct super_block *sb, int type, s64 count),\n\n\tTP_ARGS(sb, type, count)\n);\n\nTRACE_EVENT(f2fs_shutdown,\n\n\tTP_PROTO(struct f2fs_sb_info *sbi, unsigned int mode, int ret),\n\n\tTP_ARGS(sbi, mode, ret),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\tdev)\n\t\t__field(unsigned int, mode)\n\t\t__field(int, ret)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev = sbi->sb->s_dev;\n\t\t__entry->mode = mode;\n\t\t__entry->ret = ret;\n\t),\n\n\tTP_printk(\"dev = (%d,%d), mode: %s, ret:%d\",\n\t\tshow_dev(__entry->dev),\n\t\tshow_shutdown_mode(__entry->mode),\n\t\t__entry->ret)\n);\n\n#endif /* _TRACE_F2FS_H */\n\n /* This part must be outside protection */\n#include <trace/define_trace.h>\n"], "filenames": ["fs/f2fs/data.c", "fs/f2fs/f2fs.h", "include/trace/events/f2fs.h"], "buggy_code_start_loc": [16, 1501, 1031], "buggy_code_end_loc": [2963, 3687, 1223], "fixing_code_start_loc": [17, 1501, 1031], "fixing_code_end_loc": [3090, 3688, 1224], "type": "CWE-476", "message": "In the Linux kernel 5.0.21, mounting a crafted f2fs filesystem image can cause a NULL pointer dereference in f2fs_recover_fsync_data in fs/f2fs/recovery.c. This is related to F2FS_P_SB in fs/f2fs/f2fs.h.", "other": {"cve": {"id": "CVE-2019-19815", "sourceIdentifier": "cve@mitre.org", "published": "2019-12-17T07:15:10.433", "lastModified": "2020-01-03T11:15:16.637", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "In the Linux kernel 5.0.21, mounting a crafted f2fs filesystem image can cause a NULL pointer dereference in f2fs_recover_fsync_data in fs/f2fs/recovery.c. This is related to F2FS_P_SB in fs/f2fs/f2fs.h."}, {"lang": "es", "value": "En el kernel de Linux versi\u00f3n 5.0.21, montar una imagen del sistema de archivos f2fs  especialmente dise\u00f1ada puede causar una desreferencia del puntero NULL en la funci\u00f3n f2fs_recover_fsync_data en el archivo fs/f2fs/recovery.c. Esto est\u00e1 relacionado con la funci\u00f3n F2FS_P_SB en el archivo fs/f2fs/f2fs.h."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.1}, "baseSeverity": "HIGH", "exploitabilityScore": 8.6, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "38A8931B-87F4-4F2A-87CE-AB8DD402BE9F"}]}]}], "references": [{"url": "https://github.com/bobfuzzer/CVE/tree/master/CVE-2019-19815", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/4969c06a0d83c9c3dc50b8efcdc8eeedfce896f6#diff-41a7fa4590d2af87e82101f2b4dadb56", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200103-0001/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4969c06a0d83c9c3dc50b8efcdc8eeedfce896f6#diff-41a7fa4590d2af87e82101f2b4dadb56"}}