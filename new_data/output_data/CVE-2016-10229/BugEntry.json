{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe User Datagram Protocol (UDP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() calls\n *\t\tAlan Cox\t: \tstopped close while in use off icmp\n *\t\t\t\t\tmessages. Not a fix but a botch that\n *\t\t\t\t\tfor udp at least is 'valid'.\n *\t\tAlan Cox\t:\tFixed icmp handling properly\n *\t\tAlan Cox\t: \tCorrect error for oversized datagrams\n *\t\tAlan Cox\t:\tTidied select() semantics.\n *\t\tAlan Cox\t:\tudp_err() fixed properly, also now\n *\t\t\t\t\tselect and read wake correctly on errors\n *\t\tAlan Cox\t:\tudp_send verify_area moved to avoid mem leak\n *\t\tAlan Cox\t:\tUDP can count its memory\n *\t\tAlan Cox\t:\tsend to an unknown connection causes\n *\t\t\t\t\tan ECONNREFUSED off the icmp, but\n *\t\t\t\t\tdoes NOT close.\n *\t\tAlan Cox\t:\tSwitched to new sk_buff handlers. No more backlog!\n *\t\tAlan Cox\t:\tUsing generic datagram code. Even smaller and the PEEK\n *\t\t\t\t\tbug no longer crashes it.\n *\t\tFred Van Kempen\t: \tNet2e support for sk->broadcast.\n *\t\tAlan Cox\t:\tUses skb_free_datagram\n *\t\tAlan Cox\t:\tAdded get/set sockopt support.\n *\t\tAlan Cox\t:\tBroadcasting without option set returns EACCES.\n *\t\tAlan Cox\t:\tNo wakeup calls. Instead we now use the callbacks.\n *\t\tAlan Cox\t:\tUse ip_tos and ip_ttl\n *\t\tAlan Cox\t:\tSNMP Mibs\n *\t\tAlan Cox\t:\tMSG_DONTROUTE, and 0.0.0.0 support.\n *\t\tMatt Dillon\t:\tUDP length checks.\n *\t\tAlan Cox\t:\tSmarter af_inet used properly.\n *\t\tAlan Cox\t:\tUse new kernel side addressing.\n *\t\tAlan Cox\t:\tIncorrect return on truncated datagram receive.\n *\tArnt Gulbrandsen \t:\tNew udp_send and stuff\n *\t\tAlan Cox\t:\tCache last socket\n *\t\tAlan Cox\t:\tRoute cache\n *\t\tJon Peatfield\t:\tMinor efficiency fix to sendto().\n *\t\tMike Shaver\t:\tRFC1122 checks.\n *\t\tAlan Cox\t:\tNonblocking error fix.\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tLast socket cache retained as it\n *\t\t\t\t\tdoes have a high hit rate.\n *\t\tOlaf Kirch\t:\tDon't linearise iovec on sendmsg.\n *\t\tAndi Kleen\t:\tSome cleanups, cache destination entry\n *\t\t\t\t\tfor connect.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tMelvin Smith\t:\tCheck msg_name not msg_namelen in sendto(),\n *\t\t\t\t\treturn ENOTCONN for unconnected sockets (POSIX)\n *\t\tJanos Farkas\t:\tdon't deliver multi/broadcasts to a different\n *\t\t\t\t\tbound-to-device socket\n *\tHirokazu Takahashi\t:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\tHirokazu Takahashi\t:\tsendfile() on UDP works now.\n *\t\tArnaldo C. Melo :\tconvert /proc/net/udp to seq_file\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov:\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tDerek Atkins <derek@ihtfp.com>: Add Encapulation Support\n *\tJames Chapman\t\t:\tAdd L2TP encapsulation type.\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"UDP: \" fmt\n\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/tcp_states.h>\n#include <linux/skbuff.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/inet_hashtables.h>\n#include <net/route.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <trace/events/udp.h>\n#include <linux/static_key.h>\n#include <trace/events/skb.h>\n#include <net/busy_poll.h>\n#include \"udp_impl.h\"\n\nstruct udp_table udp_table __read_mostly;\nEXPORT_SYMBOL(udp_table);\n\nlong sysctl_udp_mem[3] __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_mem);\n\nint sysctl_udp_rmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_rmem_min);\n\nint sysctl_udp_wmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_wmem_min);\n\natomic_long_t udp_memory_allocated;\nEXPORT_SYMBOL(udp_memory_allocated);\n\n#define MAX_UDP_PORTS 65536\n#define PORTS_PER_CHAIN (MAX_UDP_PORTS / UDP_HTABLE_SIZE_MIN)\n\nstatic int udp_lib_lport_inuse(struct net *net, __u16 num,\n\t\t\t       const struct udp_hslot *hslot,\n\t\t\t       unsigned long *bitmap,\n\t\t\t       struct sock *sk,\n\t\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t const struct sock *sk2),\n\t\t\t       unsigned int log)\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\tkuid_t uid = sock_i_uid(sk);\n\n\tsk_nulls_for_each(sk2, node, &hslot->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (bitmap || udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (!sk2->sk_reuseport || !sk->sk_reuseport ||\n\t\t     !uid_eq(uid, sock_i_uid(sk2))) &&\n\t\t    saddr_comp(sk, sk2)) {\n\t\t\tif (!bitmap)\n\t\t\t\treturn 1;\n\t\t\t__set_bit(udp_sk(sk2)->udp_port_hash >> log, bitmap);\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * Note: we still hold spinlock of primary hash chain, so no other writer\n * can insert/delete a socket with local_port == num\n */\nstatic int udp_lib_lport_inuse2(struct net *net, __u16 num,\n\t\t\t\tstruct udp_hslot *hslot2,\n\t\t\t\tstruct sock *sk,\n\t\t\t\tint (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t  const struct sock *sk2))\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\tkuid_t uid = sock_i_uid(sk);\n\tint res = 0;\n\n\tspin_lock(&hslot2->lock);\n\tudp_portaddr_for_each_entry(sk2, node, &hslot2->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (!sk2->sk_reuseport || !sk->sk_reuseport ||\n\t\t     !uid_eq(uid, sock_i_uid(sk2))) &&\n\t\t    saddr_comp(sk, sk2)) {\n\t\t\tres = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&hslot2->lock);\n\treturn res;\n}\n\n/**\n *  udp_lib_get_port  -  UDP/-Lite port lookup for IPv4 and IPv6\n *\n *  @sk:          socket struct in question\n *  @snum:        port number to look up\n *  @saddr_comp:  AF-dependent comparison of bound local IP addresses\n *  @hash2_nulladdr: AF-dependent hash value in secondary hash chains,\n *                   with NULL address\n */\nint udp_lib_get_port(struct sock *sk, unsigned short snum,\n\t\t     int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t       const struct sock *sk2),\n\t\t     unsigned int hash2_nulladdr)\n{\n\tstruct udp_hslot *hslot, *hslot2;\n\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\tint    error = 1;\n\tstruct net *net = sock_net(sk);\n\n\tif (!snum) {\n\t\tint low, high, remaining;\n\t\tunsigned int rand;\n\t\tunsigned short first, last;\n\t\tDECLARE_BITMAP(bitmap, PORTS_PER_CHAIN);\n\n\t\tinet_get_local_port_range(net, &low, &high);\n\t\tremaining = (high - low) + 1;\n\n\t\trand = prandom_u32();\n\t\tfirst = reciprocal_scale(rand, remaining) + low;\n\t\t/*\n\t\t * force rand to be an odd multiple of UDP_HTABLE_SIZE\n\t\t */\n\t\trand = (rand | 1) * (udptable->mask + 1);\n\t\tlast = first + udptable->mask + 1;\n\t\tdo {\n\t\t\thslot = udp_hashslot(udptable, net, first);\n\t\t\tbitmap_zero(bitmap, PORTS_PER_CHAIN);\n\t\t\tspin_lock_bh(&hslot->lock);\n\t\t\tudp_lib_lport_inuse(net, snum, hslot, bitmap, sk,\n\t\t\t\t\t    saddr_comp, udptable->log);\n\n\t\t\tsnum = first;\n\t\t\t/*\n\t\t\t * Iterate on all possible values of snum for this hash.\n\t\t\t * Using steps of an odd multiple of UDP_HTABLE_SIZE\n\t\t\t * give us randomization and full range coverage.\n\t\t\t */\n\t\t\tdo {\n\t\t\t\tif (low <= snum && snum <= high &&\n\t\t\t\t    !test_bit(snum >> udptable->log, bitmap) &&\n\t\t\t\t    !inet_is_local_reserved_port(net, snum))\n\t\t\t\t\tgoto found;\n\t\t\t\tsnum += rand;\n\t\t\t} while (snum != first);\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t} while (++first != last);\n\t\tgoto fail;\n\t} else {\n\t\thslot = udp_hashslot(udptable, net, snum);\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (hslot->count > 10) {\n\t\t\tint exist;\n\t\t\tunsigned int slot2 = udp_sk(sk)->udp_portaddr_hash ^ snum;\n\n\t\t\tslot2          &= udptable->mask;\n\t\t\thash2_nulladdr &= udptable->mask;\n\n\t\t\thslot2 = udp_hashslot2(udptable, slot2);\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto scan_primary_hash;\n\n\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\tif (!exist && (hash2_nulladdr != slot2)) {\n\t\t\t\thslot2 = udp_hashslot2(udptable, hash2_nulladdr);\n\t\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\t}\n\t\t\tif (exist)\n\t\t\t\tgoto fail_unlock;\n\t\t\telse\n\t\t\t\tgoto found;\n\t\t}\nscan_primary_hash:\n\t\tif (udp_lib_lport_inuse(net, snum, hslot, NULL, sk,\n\t\t\t\t\tsaddr_comp, 0))\n\t\t\tgoto fail_unlock;\n\t}\nfound:\n\tinet_sk(sk)->inet_num = snum;\n\tudp_sk(sk)->udp_port_hash = snum;\n\tudp_sk(sk)->udp_portaddr_hash ^= snum;\n\tif (sk_unhashed(sk)) {\n\t\tsk_nulls_add_node_rcu(sk, &hslot->head);\n\t\thslot->count++;\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tspin_lock(&hslot2->lock);\n\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t &hslot2->head);\n\t\thslot2->count++;\n\t\tspin_unlock(&hslot2->lock);\n\t}\n\terror = 0;\nfail_unlock:\n\tspin_unlock_bh(&hslot->lock);\nfail:\n\treturn error;\n}\nEXPORT_SYMBOL(udp_lib_get_port);\n\nstatic int ipv4_rcv_saddr_equal(const struct sock *sk1, const struct sock *sk2)\n{\n\tstruct inet_sock *inet1 = inet_sk(sk1), *inet2 = inet_sk(sk2);\n\n\treturn \t(!ipv6_only_sock(sk2)  &&\n\t\t (!inet1->inet_rcv_saddr || !inet2->inet_rcv_saddr ||\n\t\t   inet1->inet_rcv_saddr == inet2->inet_rcv_saddr));\n}\n\nstatic u32 udp4_portaddr_hash(const struct net *net, __be32 saddr,\n\t\t\t      unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nint udp_v4_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp4_portaddr_hash(sock_net(sk), htonl(INADDR_ANY), snum);\n\tunsigned int hash2_partial =\n\t\tudp4_portaddr_hash(sock_net(sk), inet_sk(sk)->inet_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv4_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\t__be32 saddr, unsigned short hnum, __be16 sport,\n\t\t\t\t__be32 daddr, __be16 dport, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    ipv6_only_sock(sk))\n\t\treturn -1;\n\n\tscore = (sk->sk_family == PF_INET) ? 2 : 1;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_rcv_saddr) {\n\t\tif (inet->inet_rcv_saddr != daddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_daddr) {\n\t\tif (inet->inet_daddr != saddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\treturn score;\n}\n\n/*\n * In this second variant, we check (daddr, dport) matches (inet_rcv_sadd, inet_num)\n */\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t __be32 saddr, __be16 sport,\n\t\t\t\t __be32 daddr, unsigned int hnum, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    ipv6_only_sock(sk))\n\t\treturn -1;\n\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_rcv_saddr != daddr ||\n\t    inet->inet_num != hnum)\n\t\treturn -1;\n\n\tscore = (sk->sk_family == PF_INET) ? 2 : 1;\n\n\tif (inet->inet_daddr) {\n\t\tif (inet->inet_daddr != saddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\nstatic u32 udp_ehashfn(const struct net *net, const __be32 laddr,\n\t\t       const __u16 lport, const __be32 faddr,\n\t\t       const __be16 fport)\n{\n\tstatic u32 udp_ehash_secret __read_mostly;\n\n\tnet_get_random_once(&udp_ehash_secret, sizeof(udp_ehash_secret));\n\n\treturn __inet_ehashfn(laddr, lport, faddr, fport,\n\t\t\t      udp_ehash_secret + net_hash_mix(net));\n}\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp4_lib_lookup2(struct net *net,\n\t\t__be32 saddr, __be16 sport,\n\t\t__be32 daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\nbegin:\n\tresult = NULL;\n\tbadness = 0;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t   saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\n/* UDP is nearly always wildcards out the wazoo, it makes no sense to try\n * harder than this. -DaveM\n */\nstruct sock *__udp4_lib_lookup(struct net *net, __be32 saddr,\n\t\t__be16 sport, __be32 daddr, __be16 dport,\n\t\tint dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp4_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp4_portaddr_hash(net, htonl(INADDR_ANY), hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  htonl(INADDR_ANY), hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = 0;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, saddr, hnum, sport,\n\t\t\t\t      daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t   saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, saddr, hnum, sport,\n\t\t\t\t  daddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp4_lib_lookup);\n\nstatic inline struct sock *__udp4_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t\t __be16 sport, __be16 dport,\n\t\t\t\t\t\t struct udp_table *udptable)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\treturn __udp4_lib_lookup(dev_net(skb_dst(skb)->dev), iph->saddr, sport,\n\t\t\t\t iph->daddr, dport, inet_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp4_lib_lookup(struct net *net, __be32 saddr, __be16 sport,\n\t\t\t     __be32 daddr, __be16 dport, int dif)\n{\n\treturn __udp4_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp4_lib_lookup);\n\nstatic inline bool __udp_is_mcast_sock(struct net *net, struct sock *sk,\n\t\t\t\t       __be16 loc_port, __be32 loc_addr,\n\t\t\t\t       __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t       int dif, unsigned short hnum)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    (inet->inet_daddr && inet->inet_daddr != rmt_addr) ||\n\t    (inet->inet_dport != rmt_port && inet->inet_dport) ||\n\t    (inet->inet_rcv_saddr && inet->inet_rcv_saddr != loc_addr) ||\n\t    ipv6_only_sock(sk) ||\n\t    (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\treturn false;\n\tif (!ip_mc_sf_allow(sk, loc_addr, rmt_addr, dif))\n\t\treturn false;\n\treturn true;\n}\n\n/*\n * This routine is called by the ICMP module when it gets some\n * sort of error condition.  If err < 0 then the socket should\n * be closed and the error returned to the user.  If err > 0\n * it's just the icmp type << 8 | icmp code.\n * Header points to the ip header of the error packet. We move\n * on past this. Then (as it used to claim before adjustment)\n * header points to the first 8 bytes of the udp header.  We need\n * to find the appropriate port.\n */\n\nvoid __udp4_lib_err(struct sk_buff *skb, u32 info, struct udp_table *udptable)\n{\n\tstruct inet_sock *inet;\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+(iph->ihl<<2));\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\tint harderr;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp4_lib_lookup(net, iph->daddr, uh->dest,\n\t\t\tiph->saddr, uh->source, skb->dev->ifindex, udptable);\n\tif (!sk) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\t/* No socket for error */\n\t}\n\n\terr = 0;\n\tharderr = 0;\n\tinet = inet_sk(sk);\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code == ICMP_FRAG_NEEDED) { /* Path MTU discovery */\n\t\t\tipv4_sk_update_pmtu(skb, sk, info);\n\t\t\tif (inet->pmtudisc != IP_PMTUDISC_DONT) {\n\t\t\t\terr = EMSGSIZE;\n\t\t\t\tharderr = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\terr = EHOSTUNREACH;\n\t\tif (code <= NR_ICMP_UNREACH) {\n\t\t\tharderr = icmp_err_convert[code].fatal;\n\t\t\terr = icmp_err_convert[code].errno;\n\t\t}\n\t\tbreak;\n\tcase ICMP_REDIRECT:\n\t\tipv4_sk_redirect(skb, sk);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *      RFC1122: OK.  Passes ICMP errors back to application, as per\n\t *\t4.1.3.3.\n\t */\n\tif (!inet->recverr) {\n\t\tif (!harderr || sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t} else\n\t\tip_icmp_error(sk, skb, err, uh->dest, info, (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nvoid udp_err(struct sk_buff *skb, u32 info)\n{\n\t__udp4_lib_err(skb, info, &udp_table);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nvoid udp_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip_flush_pending_frames(sk);\n\t}\n}\nEXPORT_SYMBOL(udp_flush_pending_frames);\n\n/**\n * \tudp4_hwcsum  -  handle outgoing HW checksumming\n * \t@skb: \tsk_buff containing the filled-in UDP header\n * \t        (checksum field must be zeroed out)\n *\t@src:\tsource IP address\n *\t@dst:\tdestination IP address\n */\nvoid udp4_hwcsum(struct sk_buff *skb, __be32 src, __be32 dst)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint hlen = len;\n\t__wsum csum = 0;\n\n\tif (!skb_has_frag_list(skb)) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_tcpudp_magic(src, dst, len,\n\t\t\t\t\t       IPPROTO_UDP, 0);\n\t} else {\n\t\tstruct sk_buff *frags;\n\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\tskb_walk_frags(skb, frags) {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t\thlen -= frags->len;\n\t\t}\n\n\t\tcsum = skb_checksum(skb, offset, hlen, csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tuh->check = csum_tcpudp_magic(src, dst, len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\nEXPORT_SYMBOL_GPL(udp4_hwcsum);\n\n/* Function to set UDP checksum for an IPv4 UDP packet. This is intended\n * for the simple case like when setting the checksum for a UDP tunnel.\n */\nvoid udp_set_csum(bool nocheck, struct sk_buff *skb,\n\t\t  __be32 saddr, __be32 daddr, int len)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\n\tif (nocheck)\n\t\tuh->check = 0;\n\telse if (skb_is_gso(skb))\n\t\tuh->check = ~udp_v4_check(len, saddr, daddr, 0);\n\telse if (skb_dst(skb) && skb_dst(skb)->dev &&\n\t\t (skb_dst(skb)->dev->features &\n\t\t  (NETIF_F_IP_CSUM | NETIF_F_HW_CSUM))) {\n\n\t\tBUG_ON(skb->ip_summed == CHECKSUM_PARTIAL);\n\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~udp_v4_check(len, saddr, daddr, 0);\n\t} else {\n\t\t__wsum csum;\n\n\t\tBUG_ON(skb->ip_summed == CHECKSUM_PARTIAL);\n\n\t\tuh->check = 0;\n\t\tcsum = skb_checksum(skb, 0, len, 0);\n\t\tuh->check = udp_v4_check(len, saddr, daddr, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n}\nEXPORT_SYMBOL(udp_set_csum);\n\nstatic int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\t__wsum csum = 0;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = fl4->fl4_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check_tx) {   /* UDP csum disabled */\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\n\t\tudp4_hwcsum(skb, fl4->saddr, fl4->daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_tcpudp_magic(fl4->saddr, fl4->daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(sock_net(sk), skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet->recverr) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t   UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\n/*\n * Push out all pending data as one UDP datagram. Socket is locked.\n */\nint udp_push_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 *fl4 = &inet->cork.fl.u.ip4;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tskb = ip_finish_skb(sk, fl4);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_send_skb(skb, fl4);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\nEXPORT_SYMBOL(udp_push_pending_frames);\n\nint udp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct flowi4 fl4_stack;\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\t__be16 dport;\n\tu8  tos;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\tstruct ip_options_data opt_copy;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\tif (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */\n\t\treturn -EOPNOTSUPP;\n\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tfl4 = &inet->cork.fl.u.ip4;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t/* Open fast path for connected socket.\n\t\t   Route will not be used, if at least one option is set.\n\t\t */\n\t\tconnected = 1;\n\t}\n\tipc.addr = inet->inet_saddr;\n\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tsock_tx_timestamp(sk, &ipc.tx_flags);\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc,\n\t\t\t\t   sk->sk_family == AF_INET6);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tfaddr = ipc.opt->opt.faddr;\n\t\tconnected = 0;\n\t}\n\ttos = get_rttos(&ipc, inet);\n\tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n\t    (msg->msg_flags & MSG_DONTROUTE) ||\n\t    (ipc.opt && ipc.opt->opt.is_strictroute)) {\n\t\ttos |= RTO_ONLINK;\n\t\tconnected = 0;\n\t}\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (!rt) {\n\t\tstruct net *net = sock_net(sk);\n\t\t__u8 flow_flags = inet_sk_flowi_flags(sk);\n\n\t\tfl4 = &fl4_stack;\n\n\t\tflowi4_init_output(fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   flow_flags,\n\t\t\t\t   faddr, saddr, dport, inet->inet_sport);\n\n\t\tif (!saddr && ipc.oif)\n\t\t\tl3mdev_get_saddr(net, ipc.oif, fl4);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(fl4));\n\t\trt = ip_route_output_flow(net, fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = fl4->saddr;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = fl4->daddr;\n\n\t/* Lockless fast path for the non-corking case. */\n\tif (!corkreq) {\n\t\tskb = ip_make_skb(sk, fl4, getfrag, msg, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_send_skb(skb, fl4);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t/*\n\t *\tNow cork the socket to pend data.\n\t */\n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tup->pending = AF_INET;\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, fl4, getfrag, msg, ulen,\n\t\t\t     sizeof(struct udphdr), &ipc, &rt,\n\t\t\t     corkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\nEXPORT_SYMBOL(udp_sendmsg);\n\nint udp_sendpage(struct sock *sk, struct page *page, int offset,\n\t\t size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tint ret;\n\n\tif (flags & MSG_SENDPAGE_NOTLAST)\n\t\tflags |= MSG_MORE;\n\n\tif (!up->pending) {\n\t\tstruct msghdr msg = {\t.msg_flags = flags|MSG_MORE };\n\n\t\t/* Call udp_sendmsg to specify destination address which\n\t\t * sendpage interface can't pass.\n\t\t * This will succeed only when the socket is connected.\n\t\t */\n\t\tret = udp_sendmsg(sk, &msg, 0);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tlock_sock(sk);\n\n\tif (unlikely(!up->pending)) {\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 3\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = ip_append_page(sk, &inet->cork.fl.u.ip4,\n\t\t\t     page, offset, size, flags);\n\tif (ret == -EOPNOTSUPP) {\n\t\trelease_sock(sk);\n\t\treturn sock_no_sendpage(sk->sk_socket, page, offset,\n\t\t\t\t\tsize, flags);\n\t}\n\tif (ret < 0) {\n\t\tudp_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\tup->len += size;\n\tif (!(up->corkflag || (flags&MSG_MORE)))\n\t\tret = udp_push_pending_frames(sk);\n\tif (!ret)\n\t\tret = size;\nout:\n\trelease_sock(sk);\n\treturn ret;\n}\n\n/**\n *\tfirst_packet_length\t- return length of first packet in receive queue\n *\t@sk: socket\n *\n *\tDrops all bad checksum frames, until a valid one is found.\n *\tReturns the length of found skb, or 0 if none is found.\n */\nstatic unsigned int first_packet_length(struct sock *sk)\n{\n\tstruct sk_buff_head list_kill, *rcvq = &sk->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tunsigned int res;\n\n\t__skb_queue_head_init(&list_kill);\n\n\tspin_lock_bh(&rcvq->lock);\n\twhile ((skb = skb_peek(rcvq)) != NULL &&\n\t\tudp_lib_checksum_complete(skb)) {\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS,\n\t\t\t\t IS_UDPLITE(sk));\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t IS_UDPLITE(sk));\n\t\tatomic_inc(&sk->sk_drops);\n\t\t__skb_unlink(skb, rcvq);\n\t\t__skb_queue_tail(&list_kill, skb);\n\t}\n\tres = skb ? skb->len : 0;\n\tspin_unlock_bh(&rcvq->lock);\n\n\tif (!skb_queue_empty(&list_kill)) {\n\t\tbool slow = lock_sock_fast(sk);\n\n\t\t__skb_queue_purge(&list_kill);\n\t\tsk_mem_reclaim_partial(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t}\n\treturn res;\n}\n\n/*\n *\tIOCTL requests applicable to the UDP protocol\n */\n\nint udp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tcase SIOCINQ:\n\t{\n\t\tunsigned int amount = first_packet_length(sk);\n\n\t\tif (amount)\n\t\t\t/*\n\t\t\t * We will only return the amount\n\t\t\t * of this packet since that is all\n\t\t\t * that will be read.\n\t\t\t */\n\t\t\tamount -= sizeof(struct udphdr);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_ioctl);\n\n/*\n * \tThis should be easy, if there is something there we\n * \treturn it, otherwise we block.\n */\n\nint udp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int noblock,\n\t\tint flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, msg->msg_name);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len, addr_len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t    msg, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t\t     msg);\n\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udp_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\n\tif (!peeked)\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv_offset(msg, skb, sizeof(struct udphdr));\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\t/* starting over for a new packet, but check if we need to yield */\n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nint udp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t/*\n\t *\t1003.1g - break association.\n\t */\n\n\tsk->sk_state = TCP_CLOSE;\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsock_rps_reset_rxhash(sk);\n\tsk->sk_bound_dev_if = 0;\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tif (!(sk->sk_userlocks & SOCK_BINDPORT_LOCK)) {\n\t\tsk->sk_prot->unhash(sk);\n\t\tinet->inet_sport = 0;\n\t}\n\tsk_dst_reset(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_disconnect);\n\nvoid udp_lib_unhash(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2;\n\n\t\thslot  = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t      udp_sk(sk)->udp_port_hash);\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (sk_nulls_del_node_init_rcu(sk)) {\n\t\t\thslot->count--;\n\t\t\tinet_sk(sk)->inet_num = 0;\n\t\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n}\nEXPORT_SYMBOL(udp_lib_unhash);\n\n/*\n * inet_rcv_saddr was changed, we must rehash secondary hash\n */\nvoid udp_lib_rehash(struct sock *sk, u16 newhash)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2, *nhslot2;\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tnhslot2 = udp_hashslot2(udptable, newhash);\n\t\tudp_sk(sk)->udp_portaddr_hash = newhash;\n\t\tif (hslot2 != nhslot2) {\n\t\t\thslot = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t\t     udp_sk(sk)->udp_port_hash);\n\t\t\t/* we must lock primary chain too */\n\t\t\tspin_lock_bh(&hslot->lock);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\n\t\t\tspin_lock(&nhslot2->lock);\n\t\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t\t &nhslot2->head);\n\t\t\tnhslot2->count++;\n\t\t\tspin_unlock(&nhslot2->lock);\n\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(udp_lib_rehash);\n\nstatic void udp_v4_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp4_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  inet_sk(sk)->inet_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic int __udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (inet_sk(sk)->inet_daddr) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t}\n\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t is_udplite);\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\ttrace_udp_fail_queue_rcv_skb(rc, sk);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n\n}\n\nstatic struct static_key udp_encap_needed __read_mostly;\nvoid udp_encap_enable(void)\n{\n\tif (!static_key_enabled(&udp_encap_needed))\n\t\tstatic_key_slow_inc(&udp_encap_needed);\n}\nEXPORT_SYMBOL(udp_encap_enable);\n\n/* returns:\n *  -1: error\n *   0: success\n *  >0: \"udp encap\" protocol resubmission\n *\n * Note that in the success and error cases, the skb is assumed to\n * have either been requeued or freed.\n */\nint udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (static_key_false(&udp_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (skb->len > sizeof(struct udphdr) && encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLite: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLite: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t is_udplite);\n\t\tgoto drop;\n\t}\n\n\trc = 0;\n\n\tipv4_pktinfo_prepare(sk, skb);\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tunsigned int i;\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(!skb1))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udp_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\n\t\tsock_put(sk);\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\n/* For TCP sockets, sk_rx_dst is protected by socket lock\n * For UDP, we use xchg() to guard against concurrent changes.\n */\nstatic void udp_sk_rx_dst_set(struct sock *sk, struct dst_entry *dst)\n{\n\tstruct dst_entry *old;\n\n\tdst_hold(dst);\n\told = xchg(&sk->sk_rx_dst, dst);\n\tdst_release(old);\n}\n\n/*\n *\tMulticasts and broadcasts go to each listener.\n *\n *\tNote: called only from the BH handler context.\n */\nstatic int __udp4_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\t\t\t    struct udphdr  *uh,\n\t\t\t\t    __be32 saddr, __be32 daddr,\n\t\t\t\t    struct udp_table *udptable,\n\t\t\t\t    int proto)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tint dif = skb->dev->ifindex;\n\tunsigned int count = 0, offset = offsetof(typeof(*sk), sk_nulls_node);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tbool inner_flushed = false;\n\n\tif (use_hash2) {\n\t\thash2_any = udp4_portaddr_hash(net, htonl(INADDR_ANY), hnum) &\n\t\t\t    udp_table.mask;\n\t\thash2 = udp4_portaddr_hash(net, daddr, hnum) & udp_table.mask;\nstart_lookup:\n\t\thslot = &udp_table.hash2[hash2];\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tspin_lock(&hslot->lock);\n\tsk_nulls_for_each_entry_offset(sk, node, &hslot->head, offset) {\n\t\tif (__udp_is_mcast_sock(net, sk,\n\t\t\t\t\tuh->dest, daddr,\n\t\t\t\t\tuh->source, saddr,\n\t\t\t\t\tdif, hnum)) {\n\t\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\t\tinner_flushed = true;\n\t\t\t\tcount = 0;\n\t\t\t}\n\t\t\tstack[count++] = sk;\n\t\t\tsock_hold(sk);\n\t\t}\n\t}\n\n\tspin_unlock(&hslot->lock);\n\n\t/* Also lookup *:port if we are using hash2 and haven't done so yet. */\n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\t/*\n\t * do the slow work with no lock held\n\t */\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\t} else {\n\t\tif (!inner_flushed)\n\t\t\tUDP_INC_STATS_BH(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\t\t proto == IPPROTO_UDPLITE);\n\t\tconsume_skb(skb);\n\t}\n\treturn 0;\n}\n\n/* Initialize UDP checksum. If exited with zero value (success),\n * CHECKSUM_UNNECESSARY means, that no more checks are required.\n * Otherwise, csum completion requires chacksumming packet body,\n * including udp header and folding it to skb->csum.\n */\nstatic inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn skb_checksum_init_zero_check(skb, proto, uh->check,\n\t\t\t\t\t    inet_compute_pseudo);\n}\n\n/*\n *\tAll we need to do is get the socket, and then do a checksum.\n */\n\nint __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tunsigned short ulen;\n\tstruct rtable *rt = skb_rtable(skb);\n\t__be32 saddr, daddr;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/*\n\t *  Validate the packet.\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto drop;\t\t/* No space for header. */\n\n\tuh   = udp_hdr(skb);\n\tulen = ntohs(uh->len);\n\tsaddr = ip_hdr(skb)->saddr;\n\tdaddr = ip_hdr(skb)->daddr;\n\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\t\tif (ulen < sizeof(*uh) || pskb_trim_rcsum(skb, ulen))\n\t\t\tgoto short_packet;\n\t\tuh = udp_hdr(skb);\n\t}\n\n\tif (udp4_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\tsk = skb_steal_sock(skb);\n\tif (sk) {\n\t\tstruct dst_entry *dst = skb_dst(skb);\n\t\tint ret;\n\n\t\tif (unlikely(sk->sk_rx_dst != dst))\n\t\t\tudp_sk_rx_dst_set(sk, dst);\n\n\t\tret = udp_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\t\treturn 0;\n\t}\n\n\tif (rt->rt_flags & (RTCF_BROADCAST|RTCF_MULTICAST))\n\t\treturn __udp4_lib_mcast_deliver(net, skb, uh,\n\t\t\t\t\t\tsaddr, daddr, udptable, proto);\n\n\tsk = __udp4_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk) {\n\t\tint ret;\n\n\t\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t inet_compute_pseudo);\n\n\t\tret = udp_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\t\treturn 0;\n\t}\n\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\t/* No socket. Drop packet silently, if checksum is wrong */\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\n\t/*\n\t * Hmm.  We got an UDP packet to a port to which we\n\t * don't wanna listen.  Ignore it.\n\t */\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tnet_dbg_ratelimited(\"UDP%s: short packet: From %pI4:%u %d/%d to %pI4:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"Lite\" : \"\",\n\t\t\t    &saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    &daddr, ntohs(uh->dest));\n\tgoto drop;\n\ncsum_error:\n\t/*\n\t * RFC1122: OK.  Discards the bad packet silently (as far as\n\t * the network is concerned, anyway) as per 4.1.3.4 (MUST).\n\t */\n\tnet_dbg_ratelimited(\"UDP%s: bad checksum. From %pI4:%u to %pI4:%u ulen %d\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"Lite\" : \"\",\n\t\t\t    &saddr, ntohs(uh->source), &daddr, ntohs(uh->dest),\n\t\t\t    ulen);\n\tUDP_INC_STATS_BH(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndrop:\n\tUDP_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\n/* We can only early demux multicast if there is a single matching socket.\n * If more than one socket found returns NULL\n */\nstatic struct sock *__udp4_lib_mcast_demux_lookup(struct net *net,\n\t\t\t\t\t\t  __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t\t  __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t\t  int dif)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(loc_port);\n\tunsigned int count, slot = udp_hashfn(net, hnum, udp_table.mask);\n\tstruct udp_hslot *hslot = &udp_table.hash[slot];\n\n\t/* Do not bother scanning a too big list */\n\tif (hslot->count > 10)\n\t\treturn NULL;\n\n\trcu_read_lock();\nbegin:\n\tcount = 0;\n\tresult = NULL;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tif (__udp_is_mcast_sock(net, sk,\n\t\t\t\t\tloc_port, loc_addr,\n\t\t\t\t\trmt_port, rmt_addr,\n\t\t\t\t\tdif, hnum)) {\n\t\t\tresult = sk;\n\t\t\t++count;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (count != 1 ||\n\t\t    unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(!__udp_is_mcast_sock(net, result,\n\t\t\t\t\t\t       loc_port, loc_addr,\n\t\t\t\t\t\t       rmt_port, rmt_addr,\n\t\t\t\t\t\t       dif, hnum))) {\n\t\t\tsock_put(result);\n\t\t\tresult = NULL;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\n/* For unicast we should only early demux connected sockets or we can\n * break forwarding setups.  The chains here can be long so only check\n * if the first socket is an exact match and if not move on.\n */\nstatic struct sock *__udp4_lib_demux_lookup(struct net *net,\n\t\t\t\t\t    __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t    __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t    int dif)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(loc_port);\n\tunsigned int hash2 = udp4_portaddr_hash(net, loc_addr, hnum);\n\tunsigned int slot2 = hash2 & udp_table.mask;\n\tstruct udp_hslot *hslot2 = &udp_table.hash2[slot2];\n\tINET_ADDR_COOKIE(acookie, rmt_addr, loc_addr);\n\tconst __portpair ports = INET_COMBINED_PORTS(rmt_port, hnum);\n\n\trcu_read_lock();\n\tresult = NULL;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tif (INET_MATCH(sk, net, acookie,\n\t\t\t       rmt_addr, loc_addr, ports, dif))\n\t\t\tresult = sk;\n\t\t/* Only check first socket in chain */\n\t\tbreak;\n\t}\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(!INET_MATCH(sk, net, acookie,\n\t\t\t\t\t      rmt_addr, loc_addr,\n\t\t\t\t\t      ports, dif))) {\n\t\t\tsock_put(result);\n\t\t\tresult = NULL;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\nvoid udp_v4_early_demux(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph;\n\tconst struct udphdr *uh;\n\tstruct sock *sk;\n\tstruct dst_entry *dst;\n\tint dif = skb->dev->ifindex;\n\tint ours;\n\n\t/* validate the packet */\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct udphdr)))\n\t\treturn;\n\n\tiph = ip_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tif (skb->pkt_type == PACKET_BROADCAST ||\n\t    skb->pkt_type == PACKET_MULTICAST) {\n\t\tstruct in_device *in_dev = __in_dev_get_rcu(skb->dev);\n\n\t\tif (!in_dev)\n\t\t\treturn;\n\n\t\tours = ip_check_mc_rcu(in_dev, iph->daddr, iph->saddr,\n\t\t\t\t       iph->protocol);\n\t\tif (!ours)\n\t\t\treturn;\n\t\tsk = __udp4_lib_mcast_demux_lookup(net, uh->dest, iph->daddr,\n\t\t\t\t\t\t   uh->source, iph->saddr, dif);\n\t} else if (skb->pkt_type == PACKET_HOST) {\n\t\tsk = __udp4_lib_demux_lookup(net, uh->dest, iph->daddr,\n\t\t\t\t\t     uh->source, iph->saddr, dif);\n\t} else {\n\t\treturn;\n\t}\n\n\tif (!sk)\n\t\treturn;\n\n\tskb->sk = sk;\n\tskb->destructor = sock_efree;\n\tdst = READ_ONCE(sk->sk_rx_dst);\n\n\tif (dst)\n\t\tdst = dst_check(dst, 0);\n\tif (dst) {\n\t\t/* DST_NOCACHE can not be used without taking a reference */\n\t\tif (dst->flags & DST_NOCACHE) {\n\t\t\tif (likely(atomic_inc_not_zero(&dst->__refcnt)))\n\t\t\t\tskb_dst_set(skb, dst);\n\t\t} else {\n\t\t\tskb_dst_set_noref(skb, dst);\n\t\t}\n\t}\n}\n\nint udp_rcv(struct sk_buff *skb)\n{\n\treturn __udp4_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\nvoid udp_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tbool slow = lock_sock_fast(sk);\n\tudp_flush_pending_frames(sk);\n\tunlock_sock_fast(sk, slow);\n\tif (static_key_false(&udp_encap_needed) && up->encap_type) {\n\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\tencap_destroy = ACCESS_ONCE(up->encap_destroy);\n\t\tif (encap_destroy)\n\t\t\tencap_destroy(sk);\n\t}\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udp_lib_setsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, unsigned int optlen,\n\t\t       int (*push_pending_frames)(struct sock *))\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, valbool;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tif (val != 0) {\n\t\t\tup->corkflag = 1;\n\t\t} else {\n\t\t\tup->corkflag = 0;\n\t\t\tlock_sock(sk);\n\t\t\tpush_pending_frames(sk);\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tswitch (val) {\n\t\tcase 0:\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tup->encap_rcv = xfrm4_udp_encap_rcv;\n\t\t\t/* FALLTHROUGH */\n\t\tcase UDP_ENCAP_L2TPINUDP:\n\t\t\tup->encap_type = val;\n\t\t\tudp_encap_enable();\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_TX:\n\t\tup->no_check6_tx = valbool;\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_RX:\n\t\tup->no_check6_rx = valbool;\n\t\tbreak;\n\n\t/*\n\t * \tUDP-Lite's partial checksum coverage (RFC 3828).\n\t */\n\t/* The sender sets actual checksum coverage length via this option.\n\t * The case coverage > packet length is handled by send module. */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Illegal coverage: use default (8) */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcslen = val;\n\t\tup->pcflag |= UDPLITE_SEND_CC;\n\t\tbreak;\n\n\t/* The receiver specifies a minimum checksum coverage value. To make\n\t * sense, this should be set to at least 8 (as done below). If zero is\n\t * used, this again means full checksum coverage.                     */\n\tcase UDPLITE_RECV_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Avoid silly minimal values.       */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcrlen = val;\n\t\tup->pcflag |= UDPLITE_RECV_CC;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(udp_lib_setsockopt);\n\nint udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn compat_ip_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udp_lib_getsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tval = up->corkflag;\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tval = up->encap_type;\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_TX:\n\t\tval = up->no_check6_tx;\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_RX:\n\t\tval = up->no_check6_rx;\n\t\tbreak;\n\n\t/* The following two cannot be changed on UDP sockets, the return is\n\t * always 0 (which corresponds to the full checksum coverage of UDP). */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tval = up->pcslen;\n\t\tbreak;\n\n\tcase UDPLITE_RECV_CSCOV:\n\t\tval = up->pcrlen;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_lib_getsockopt);\n\nint udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ip_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n/**\n * \tudp_poll - wait for a UDP event.\n *\t@file - file struct\n *\t@sock - socket\n *\t@wait - poll table\n *\n *\tThis is same as datagram poll, except for the special case of\n *\tblocking sockets. If application is using a blocking fd\n *\tand a packet with checksum error is in the queue;\n *\tthen it could get return from select indicating data available\n *\tbut then block when reading it. Add special case code\n *\tto work around these arguably broken applications.\n */\nunsigned int udp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* Check for false positives due to checksum errors */\n\tif ((mask & POLLRDNORM) && !(file->f_flags & O_NONBLOCK) &&\n\t    !(sk->sk_shutdown & RCV_SHUTDOWN) && !first_packet_length(sk))\n\t\tmask &= ~(POLLIN | POLLRDNORM);\n\n\treturn mask;\n\n}\nEXPORT_SYMBOL(udp_poll);\n\nstruct proto udp_prot = {\n\t.name\t\t   = \"UDP\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip4_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udp_destroy_sock,\n\t.setsockopt\t   = udp_setsockopt,\n\t.getsockopt\t   = udp_getsockopt,\n\t.sendmsg\t   = udp_sendmsg,\n\t.recvmsg\t   = udp_recvmsg,\n\t.sendpage\t   = udp_sendpage,\n\t.backlog_rcv\t   = __udp_queue_rcv_skb,\n\t.release_cb\t   = ip4_datagram_release_cb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v4_rehash,\n\t.get_port\t   = udp_v4_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udp_setsockopt,\n\t.compat_getsockopt = compat_udp_getsockopt,\n#endif\n\t.clear_sk\t   = sk_prot_clear_portaddr_nulls,\n};\nEXPORT_SYMBOL(udp_prot);\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\n\nstatic struct sock *udp_get_first(struct seq_file *seq, int start)\n{\n\tstruct sock *sk;\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tfor (state->bucket = start; state->bucket <= state->udp_table->mask;\n\t     ++state->bucket) {\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct udp_hslot *hslot = &state->udp_table->hash[state->bucket];\n\n\t\tif (hlist_nulls_empty(&hslot->head))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tsk_nulls_for_each(sk, node, &hslot->head) {\n\t\t\tif (!net_eq(sock_net(sk), net))\n\t\t\t\tcontinue;\n\t\t\tif (sk->sk_family == state->family)\n\t\t\t\tgoto found;\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tdo {\n\t\tsk = sk_nulls_next(sk);\n\t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n\n\tif (!sk) {\n\t\tif (state->bucket <= state->udp_table->mask)\n\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}\n\nstatic struct sock *udp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = udp_get_first(seq, 0);\n\n\tif (sk)\n\t\twhile (pos && (sk = udp_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nstatic void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstate->bucket = MAX_UDP_PORTS;\n\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}\n\nstatic void *udp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = udp_get_idx(seq, 0);\n\telse\n\t\tsk = udp_get_next(seq, v);\n\n\t++*pos;\n\treturn sk;\n}\n\nstatic void udp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct udp_iter_state *state = seq->private;\n\n\tif (state->bucket <= state->udp_table->mask)\n\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n}\n\nint udp_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct udp_seq_afinfo *afinfo = PDE_DATA(inode);\n\tstruct udp_iter_state *s;\n\tint err;\n\n\terr = seq_open_net(inode, file, &afinfo->seq_ops,\n\t\t\t   sizeof(struct udp_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ts = ((struct seq_file *)file->private_data)->private;\n\ts->family\t\t= afinfo->family;\n\ts->udp_table\t\t= afinfo->udp_table;\n\treturn err;\n}\nEXPORT_SYMBOL(udp_seq_open);\n\n/* ------------------------------------------------------------------------ */\nint udp_proc_register(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tstruct proc_dir_entry *p;\n\tint rc = 0;\n\n\tafinfo->seq_ops.start\t\t= udp_seq_start;\n\tafinfo->seq_ops.next\t\t= udp_seq_next;\n\tafinfo->seq_ops.stop\t\t= udp_seq_stop;\n\n\tp = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,\n\t\t\t     afinfo->seq_fops, afinfo);\n\tif (!p)\n\t\trc = -ENOMEM;\n\treturn rc;\n}\nEXPORT_SYMBOL(udp_proc_register);\n\nvoid udp_proc_unregister(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tremove_proc_entry(afinfo->name, net->proc_net);\n}\nEXPORT_SYMBOL(udp_proc_unregister);\n\n/* ------------------------------------------------------------------------ */\nstatic void udp4_format_sock(struct sock *sp, struct seq_file *f,\n\t\tint bucket)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src  = inet->inet_rcv_saddr;\n\t__u16 destp\t  = ntohs(inet->inet_dport);\n\t__u16 srcp\t  = ntohs(inet->inet_sport);\n\n\tseq_printf(f, \"%5d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %d\",\n\t\tbucket, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tsk_rmem_alloc_get(sp),\n\t\t0, 0L, 0,\n\t\tfrom_kuid_munged(seq_user_ns(f), sock_i_uid(sp)),\n\t\t0, sock_i_ino(sp),\n\t\tatomic_read(&sp->sk_refcnt), sp,\n\t\tatomic_read(&sp->sk_drops));\n}\n\nint udp4_seq_show(struct seq_file *seq, void *v)\n{\n\tseq_setwidth(seq, 127);\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode ref pointer drops\");\n\telse {\n\t\tstruct udp_iter_state *state = seq->private;\n\n\t\tudp4_format_sock(v, seq, state->bucket);\n\t}\n\tseq_pad(seq, '\\n');\n\treturn 0;\n}\n\nstatic const struct file_operations udp_afinfo_seq_fops = {\n\t.owner    = THIS_MODULE,\n\t.open     = udp_seq_open,\n\t.read     = seq_read,\n\t.llseek   = seq_lseek,\n\t.release  = seq_release_net\n};\n\n/* ------------------------------------------------------------------------ */\nstatic struct udp_seq_afinfo udp4_seq_afinfo = {\n\t.name\t\t= \"udp\",\n\t.family\t\t= AF_INET,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= &udp_afinfo_seq_fops,\n\t.seq_ops\t= {\n\t\t.show\t\t= udp4_seq_show,\n\t},\n};\n\nstatic int __net_init udp4_proc_init_net(struct net *net)\n{\n\treturn udp_proc_register(net, &udp4_seq_afinfo);\n}\n\nstatic void __net_exit udp4_proc_exit_net(struct net *net)\n{\n\tudp_proc_unregister(net, &udp4_seq_afinfo);\n}\n\nstatic struct pernet_operations udp4_net_ops = {\n\t.init = udp4_proc_init_net,\n\t.exit = udp4_proc_exit_net,\n};\n\nint __init udp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&udp4_net_ops);\n}\n\nvoid udp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&udp4_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n\nstatic __initdata unsigned long uhash_entries;\nstatic int __init set_uhash_entries(char *str)\n{\n\tssize_t ret;\n\n\tif (!str)\n\t\treturn 0;\n\n\tret = kstrtoul(str, 0, &uhash_entries);\n\tif (ret)\n\t\treturn 0;\n\n\tif (uhash_entries && uhash_entries < UDP_HTABLE_SIZE_MIN)\n\t\tuhash_entries = UDP_HTABLE_SIZE_MIN;\n\treturn 1;\n}\n__setup(\"uhash_entries=\", set_uhash_entries);\n\nvoid __init udp_table_init(struct udp_table *table, const char *name)\n{\n\tunsigned int i;\n\n\ttable->hash = alloc_large_system_hash(name,\n\t\t\t\t\t      2 * sizeof(struct udp_hslot),\n\t\t\t\t\t      uhash_entries,\n\t\t\t\t\t      21, /* one slot per 2 MB */\n\t\t\t\t\t      0,\n\t\t\t\t\t      &table->log,\n\t\t\t\t\t      &table->mask,\n\t\t\t\t\t      UDP_HTABLE_SIZE_MIN,\n\t\t\t\t\t      64 * 1024);\n\n\ttable->hash2 = table->hash + (table->mask + 1);\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash[i].head, i);\n\t\ttable->hash[i].count = 0;\n\t\tspin_lock_init(&table->hash[i].lock);\n\t}\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash2[i].head, i);\n\t\ttable->hash2[i].count = 0;\n\t\tspin_lock_init(&table->hash2[i].lock);\n\t}\n}\n\nu32 udp_flow_hashrnd(void)\n{\n\tstatic u32 hashrnd __read_mostly;\n\n\tnet_get_random_once(&hashrnd, sizeof(hashrnd));\n\n\treturn hashrnd;\n}\nEXPORT_SYMBOL(udp_flow_hashrnd);\n\nvoid __init udp_init(void)\n{\n\tunsigned long limit;\n\n\tudp_table_init(&udp_table, \"UDP\");\n\tlimit = nr_free_buffer_pages() / 8;\n\tlimit = max(limit, 128UL);\n\tsysctl_udp_mem[0] = limit / 4 * 3;\n\tsysctl_udp_mem[1] = limit;\n\tsysctl_udp_mem[2] = sysctl_udp_mem[0] * 2;\n\n\tsysctl_udp_rmem_min = SK_MEM_QUANTUM;\n\tsysctl_udp_wmem_min = SK_MEM_QUANTUM;\n}\n", "/*\n *\tUDP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/ipv4/udp.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *      Kazunori MIYAZAWA @USAGI:       change process style to use ip6_append_data\n *      YOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/udp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/raw.h>\n#include <net/tcp_states.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/inet6_hashtables.h>\n#include <net/busy_poll.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <trace/events/skb.h>\n#include \"udp_impl.h\"\n\nstatic u32 udp6_ehashfn(const struct net *net,\n\t\t\tconst struct in6_addr *laddr,\n\t\t\tconst u16 lport,\n\t\t\tconst struct in6_addr *faddr,\n\t\t\tconst __be16 fport)\n{\n\tstatic u32 udp6_ehash_secret __read_mostly;\n\tstatic u32 udp_ipv6_hash_secret __read_mostly;\n\n\tu32 lhash, fhash;\n\n\tnet_get_random_once(&udp6_ehash_secret,\n\t\t\t    sizeof(udp6_ehash_secret));\n\tnet_get_random_once(&udp_ipv6_hash_secret,\n\t\t\t    sizeof(udp_ipv6_hash_secret));\n\n\tlhash = (__force u32)laddr->s6_addr32[3];\n\tfhash = __ipv6_addr_jhash(faddr, udp_ipv6_hash_secret);\n\n\treturn __inet6_ehashfn(lhash, lport, fhash, fport,\n\t\t\t       udp_ipv6_hash_secret + net_hash_mix(net));\n}\n\nint ipv6_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2)\n{\n\tconst struct in6_addr *sk2_rcv_saddr6 = inet6_rcv_saddr(sk2);\n\tint sk2_ipv6only = inet_v6_ipv6only(sk2);\n\tint addr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED)\n\t\treturn (!sk2_ipv6only &&\n\t\t\t(!sk->sk_rcv_saddr || !sk2->sk_rcv_saddr ||\n\t\t\t  sk->sk_rcv_saddr == sk2->sk_rcv_saddr));\n\n\tif (addr_type2 == IPV6_ADDR_ANY &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY &&\n\t    !(ipv6_only_sock(sk) && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(&sk->sk_v6_rcv_saddr, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic u32 udp6_portaddr_hash(const struct net *net,\n\t\t\t      const struct in6_addr *addr6,\n\t\t\t      unsigned int port)\n{\n\tunsigned int hash, mix = net_hash_mix(net);\n\n\tif (ipv6_addr_any(addr6))\n\t\thash = jhash_1word(0, mix);\n\telse if (ipv6_addr_v4mapped(addr6))\n\t\thash = jhash_1word((__force u32)addr6->s6_addr32[3], mix);\n\telse\n\t\thash = jhash2((__force u32 *)addr6->s6_addr32, 4, mix);\n\n\treturn hash ^ port;\n}\n\nint udp_v6_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp6_portaddr_hash(sock_net(sk), &in6addr_any, snum);\n\tunsigned int hash2_partial =\n\t\tudp6_portaddr_hash(sock_net(sk), &sk->sk_v6_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv6_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic void udp_v6_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp6_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\tunsigned short hnum,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, __be16 dport,\n\t\t\t\tint dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t const struct in6_addr *daddr,\n\t\t\t\t unsigned short hnum, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp6_lib_lookup2(struct net *net,\n\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\tconst struct in6_addr *daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp6_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t    saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\nstruct sock *__udp6_lib_lookup(struct net *net,\n\t\t\t\t      const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t      const struct in6_addr *daddr, __be16 dport,\n\t\t\t\t      int dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp6_portaddr_hash(net, &in6addr_any, hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  &in6addr_any, hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, hnum, saddr, sport, daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp6_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t    saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, hnum, saddr, sport,\n\t\t\t\t\tdaddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp6_lib_lookup);\n\nstatic struct sock *__udp6_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t  __be16 sport, __be16 dport,\n\t\t\t\t\t  struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\n\tsk = skb_steal_sock(skb);\n\tif (unlikely(sk))\n\t\treturn sk;\n\treturn __udp6_lib_lookup(dev_net(skb_dst(skb)->dev), &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, inet6_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp6_lib_lookup(struct net *net, const struct in6_addr *saddr, __be16 sport,\n\t\t\t     const struct in6_addr *daddr, __be16 dport, int dif)\n{\n\treturn __udp6_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp6_lib_lookup);\n\n/*\n *\tThis should be easy, if there is something there we\n *\treturn it, otherwise we block.\n */\n\nint udpv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t    msg, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, sizeof(struct udphdr), msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    inet6_iif(skb));\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\t/* starting over for a new packet, but check if we need to yield */\n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nvoid __udp6_lib_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info,\n\t\t    struct udp_table *udptable)\n{\n\tstruct ipv6_pinfo *np;\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct in6_addr *saddr = &hdr->saddr;\n\tconst struct in6_addr *daddr = &hdr->daddr;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+offset);\n\tstruct sock *sk;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp6_lib_lookup(net, daddr, uh->dest,\n\t\t\t       saddr, uh->source, inet6_iif(skb), udptable);\n\tif (!sk) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tip6_sk_redirect(skb, sk);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_err_convert(type, code, &err) && !np->recverr)\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_ESTABLISHED && !np->recverr)\n\t\tgoto out;\n\n\tif (np->recverr)\n\t\tipv6_icmp_error(sk, skb, err, uh->dest, ntohl(info), (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nstatic int __udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t}\n\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tUDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic __inline__ void udpv6_err(struct sk_buff *skb,\n\t\t\t\t struct inet6_skb_parm *opt, u8 type,\n\t\t\t\t u8 code, int offset, __be32 info)\n{\n\t__udp6_lib_err(skb, opt, type, code, offset, info, &udp_table);\n}\n\nstatic struct static_key udpv6_encap_needed __read_mostly;\nvoid udpv6_encap_enable(void)\n{\n\tif (!static_key_enabled(&udpv6_encap_needed))\n\t\tstatic_key_slow_inc(&udpv6_encap_needed);\n}\nEXPORT_SYMBOL(udpv6_encap_enable);\n\nint udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (skb->len > sizeof(struct udphdr) && encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_error;\n\t}\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t  UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic bool __udp_v6_is_mcast_sock(struct net *net, struct sock *sk,\n\t\t\t\t   __be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t\t   __be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\t\t   int dif, unsigned short hnum)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net))\n\t\treturn false;\n\n\tif (udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6 ||\n\t    (inet->inet_dport && inet->inet_dport != rmt_port) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr)) ||\n\t    (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_rcv_saddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr)))\n\t\treturn false;\n\tif (!inet6_mc_check(sk, loc_addr, rmt_addr))\n\t\treturn false;\n\treturn true;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\tunsigned int i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(!skb1))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t  IS_UDPLITE(sk));\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t  IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udpv6_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\t\tsock_put(sk);\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\nstatic void udp6_csum_zero_error(struct sk_buff *skb)\n{\n\t/* RFC 2460 section 8.1 says that we SHOULD log\n\t * this error. Well, it is reasonable.\n\t */\n\tnet_dbg_ratelimited(\"IPv6: udp checksum is 0 for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t    &ipv6_hdr(skb)->saddr, ntohs(udp_hdr(skb)->source),\n\t\t\t    &ipv6_hdr(skb)->daddr, ntohs(udp_hdr(skb)->dest));\n}\n\n/*\n * Note: called only from the BH handler context,\n * so we don't need to lock the hashes.\n */\nstatic int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\tstruct udp_table *udptable, int proto)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tconst struct udphdr *uh = udp_hdr(skb);\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tint dif = inet6_iif(skb);\n\tunsigned int count = 0, offset = offsetof(typeof(*sk), sk_nulls_node);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tbool inner_flushed = false;\n\n\tif (use_hash2) {\n\t\thash2_any = udp6_portaddr_hash(net, &in6addr_any, hnum) &\n\t\t\t    udp_table.mask;\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum) & udp_table.mask;\nstart_lookup:\n\t\thslot = &udp_table.hash2[hash2];\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tspin_lock(&hslot->lock);\n\tsk_nulls_for_each_entry_offset(sk, node, &hslot->head, offset) {\n\t\tif (__udp_v6_is_mcast_sock(net, sk,\n\t\t\t\t\t   uh->dest, daddr,\n\t\t\t\t\t   uh->source, saddr,\n\t\t\t\t\t   dif, hnum) &&\n\t\t    /* If zero checksum and no_check is not on for\n\t\t     * the socket then skip it.\n\t\t     */\n\t\t    (uh->check || udp_sk(sk)->no_check6_rx)) {\n\t\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\t\tinner_flushed = true;\n\t\t\t\tcount = 0;\n\t\t\t}\n\t\t\tstack[count++] = sk;\n\t\t\tsock_hold(sk);\n\t\t}\n\t}\n\n\tspin_unlock(&hslot->lock);\n\n\t/* Also lookup *:port if we are using hash2 and haven't done so yet. */\n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\t} else {\n\t\tif (!inner_flushed)\n\t\t\tUDP_INC_STATS_BH(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\t\t proto == IPPROTO_UDPLITE);\n\t\tconsume_skb(skb);\n\t}\n\treturn 0;\n}\n\nint __udp6_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tconst struct in6_addr *saddr, *daddr;\n\tu32 ulen = 0;\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto discard;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = &ipv6_hdr(skb)->daddr;\n\tuh = udp_hdr(skb);\n\n\tulen = ntohs(uh->len);\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\n\t\t/* Check for jumbo payload */\n\t\tif (ulen == 0)\n\t\t\tulen = skb->len;\n\n\t\tif (ulen < sizeof(*uh))\n\t\t\tgoto short_packet;\n\n\t\tif (ulen < skb->len) {\n\t\t\tif (pskb_trim_rcsum(skb, ulen))\n\t\t\t\tgoto short_packet;\n\t\t\tsaddr = &ipv6_hdr(skb)->saddr;\n\t\t\tdaddr = &ipv6_hdr(skb)->daddr;\n\t\t\tuh = udp_hdr(skb);\n\t\t}\n\t}\n\n\tif (udp6_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\t/*\n\t *\tMulticast receive code\n\t */\n\tif (ipv6_addr_is_multicast(daddr))\n\t\treturn __udp6_lib_mcast_deliver(net, skb,\n\t\t\t\tsaddr, daddr, udptable, proto);\n\n\t/* Unicast */\n\n\t/*\n\t * check socket cache ... must talk to Alan about his plans\n\t * for sock caches... i'll skip this for now.\n\t */\n\tsk = __udp6_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk) {\n\t\tint ret;\n\n\t\tif (!uh->check && !udp_sk(sk)->no_check6_rx) {\n\t\t\tsock_put(sk);\n\t\t\tudp6_csum_zero_error(skb);\n\t\t\tgoto csum_error;\n\t\t}\n\n\t\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t ip6_compute_pseudo);\n\n\t\tret = udpv6_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\n\t\treturn 0;\n\t}\n\n\tif (!uh->check) {\n\t\tudp6_csum_zero_error(skb);\n\t\tgoto csum_error;\n\t}\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard;\n\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP6_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);\n\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tnet_dbg_ratelimited(\"UDP%sv6: short packet: From [%pI6c]:%u %d/%d to [%pI6c]:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t\t    saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    daddr, ntohs(uh->dest));\n\tgoto discard;\ncsum_error:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndiscard:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic __inline__ int udpv6_rcv(struct sk_buff *skb)\n{\n\treturn __udp6_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nstatic void udp_v6_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending == AF_INET)\n\t\tudp_flush_pending_frames(sk);\n\telse if (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip6_flush_pending_frames(sk);\n\t}\n}\n\n/**\n *\tudp6_hwcsum_outgoing  -  handle outgoing HW checksumming\n *\t@sk:\tsocket we are sending on\n *\t@skb:\tsk_buff containing the filled-in UDP header\n *\t\t(checksum field must be zeroed out)\n */\nstatic void udp6_hwcsum_outgoing(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const struct in6_addr *saddr,\n\t\t\t\t const struct in6_addr *daddr, int len)\n{\n\tunsigned int offset;\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *frags = skb_shinfo(skb)->frag_list;\n\t__wsum csum = 0;\n\n\tif (!frags) {\n\t\t/* Only one fragment on the socket.  */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\toffset = skb_transport_offset(skb);\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tdo {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t} while ((frags = frags->next));\n\n\t\tuh->check = csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP,\n\t\t\t\t\t    csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\n/*\n *\tSending\n */\n\nstatic int udp_v6_send_skb(struct sk_buff *skb, struct flowi6 *fl6)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\t__wsum csum = 0;\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = fl6->fl6_sport;\n\tuh->dest = fl6->fl6_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)\n\t\tcsum = udplite_csum(skb);\n\telse if (udp_sk(sk)->no_check6_tx) {   /* UDP csum disabled */\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\t\tudp6_hwcsum_outgoing(sk, skb, &fl6->saddr, &fl6->daddr, len);\n\t\tgoto send;\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t\t    len, fl6->flowi6_proto, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip6_send_skb(skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet6_sk(sk)->recverr) {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t    UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t    UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\nstatic int udp_v6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct flowi6 fl6;\n\tint err = 0;\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_push_pending_frames(sk);\n\n\t/* ip6_finish_skb will release the cork, so make a copy of\n\t * fl6 here.\n\t */\n\tfl6 = inet_sk(sk)->cork.fl.u.ip6;\n\n\tskb = ip6_finish_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_v6_send_skb(skb, &fl6);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n\nvoid udpv6_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tlock_sock(sk);\n\tudp_v6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\tencap_destroy = ACCESS_ONCE(up->encap_destroy);\n\t\tif (encap_destroy)\n\t\t\tencap_destroy(sk);\n\t}\n\n\tinet6_destroy_sock(sk);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn compat_ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic const struct inet6_protocol udpv6_protocol = {\n\t.handler\t=\tudpv6_rcv,\n\t.err_handler\t=\tudpv6_err,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\nint udp6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tint bucket = ((struct udp_iter_state *)seq->private)->bucket;\n\t\tstruct inet_sock *inet = inet_sk(v);\n\t\t__u16 srcp = ntohs(inet->inet_sport);\n\t\t__u16 destp = ntohs(inet->inet_dport);\n\t\tip6_dgram_sock_seq_show(seq, v, srcp, destp, bucket);\n\t}\n\treturn 0;\n}\n\nstatic const struct file_operations udp6_afinfo_seq_fops = {\n\t.owner    = THIS_MODULE,\n\t.open     = udp_seq_open,\n\t.read     = seq_read,\n\t.llseek   = seq_lseek,\n\t.release  = seq_release_net\n};\n\nstatic struct udp_seq_afinfo udp6_seq_afinfo = {\n\t.name\t\t= \"udp6\",\n\t.family\t\t= AF_INET6,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= &udp6_afinfo_seq_fops,\n\t.seq_ops\t= {\n\t\t.show\t\t= udp6_seq_show,\n\t},\n};\n\nint __net_init udp6_proc_init(struct net *net)\n{\n\treturn udp_proc_register(net, &udp6_seq_afinfo);\n}\n\nvoid udp6_proc_exit(struct net *net)\n{\n\tudp_proc_unregister(net, &udp6_seq_afinfo);\n}\n#endif /* CONFIG_PROC_FS */\n\nvoid udp_v6_clear_sk(struct sock *sk, int size)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t/* we do not want to clear pinet6 field, because of RCU lookups */\n\tsk_prot_clear_portaddr_nulls(sk, offsetof(struct inet_sock, pinet6));\n\n\tsize -= offsetof(struct inet_sock, pinet6) + sizeof(inet->pinet6);\n\tmemset(&inet->pinet6 + 1, 0, size);\n}\n\n/* ------------------------------------------------------------------------ */\n\nstruct proto udpv6_prot = {\n\t.name\t\t   = \"UDPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip6_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udpv6_destroy_sock,\n\t.setsockopt\t   = udpv6_setsockopt,\n\t.getsockopt\t   = udpv6_getsockopt,\n\t.sendmsg\t   = udpv6_sendmsg,\n\t.recvmsg\t   = udpv6_recvmsg,\n\t.backlog_rcv\t   = __udpv6_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v6_rehash,\n\t.get_port\t   = udp_v6_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udpv6_setsockopt,\n\t.compat_getsockopt = compat_udpv6_getsockopt,\n#endif\n\t.clear_sk\t   = udp_v6_clear_sk,\n};\n\nstatic struct inet_protosw udpv6_protosw = {\n\t.type =      SOCK_DGRAM,\n\t.protocol =  IPPROTO_UDP,\n\t.prot =      &udpv6_prot,\n\t.ops =       &inet6_dgram_ops,\n\t.flags =     INET_PROTOSW_PERMANENT,\n};\n\nint __init udpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_register_protosw(&udpv6_protosw);\n\tif (ret)\n\t\tgoto out_udpv6_protocol;\nout:\n\treturn ret;\n\nout_udpv6_protocol:\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tgoto out;\n}\n\nvoid udpv6_exit(void)\n{\n\tinet6_unregister_protosw(&udpv6_protosw);\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n}\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe User Datagram Protocol (UDP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() calls\n *\t\tAlan Cox\t: \tstopped close while in use off icmp\n *\t\t\t\t\tmessages. Not a fix but a botch that\n *\t\t\t\t\tfor udp at least is 'valid'.\n *\t\tAlan Cox\t:\tFixed icmp handling properly\n *\t\tAlan Cox\t: \tCorrect error for oversized datagrams\n *\t\tAlan Cox\t:\tTidied select() semantics.\n *\t\tAlan Cox\t:\tudp_err() fixed properly, also now\n *\t\t\t\t\tselect and read wake correctly on errors\n *\t\tAlan Cox\t:\tudp_send verify_area moved to avoid mem leak\n *\t\tAlan Cox\t:\tUDP can count its memory\n *\t\tAlan Cox\t:\tsend to an unknown connection causes\n *\t\t\t\t\tan ECONNREFUSED off the icmp, but\n *\t\t\t\t\tdoes NOT close.\n *\t\tAlan Cox\t:\tSwitched to new sk_buff handlers. No more backlog!\n *\t\tAlan Cox\t:\tUsing generic datagram code. Even smaller and the PEEK\n *\t\t\t\t\tbug no longer crashes it.\n *\t\tFred Van Kempen\t: \tNet2e support for sk->broadcast.\n *\t\tAlan Cox\t:\tUses skb_free_datagram\n *\t\tAlan Cox\t:\tAdded get/set sockopt support.\n *\t\tAlan Cox\t:\tBroadcasting without option set returns EACCES.\n *\t\tAlan Cox\t:\tNo wakeup calls. Instead we now use the callbacks.\n *\t\tAlan Cox\t:\tUse ip_tos and ip_ttl\n *\t\tAlan Cox\t:\tSNMP Mibs\n *\t\tAlan Cox\t:\tMSG_DONTROUTE, and 0.0.0.0 support.\n *\t\tMatt Dillon\t:\tUDP length checks.\n *\t\tAlan Cox\t:\tSmarter af_inet used properly.\n *\t\tAlan Cox\t:\tUse new kernel side addressing.\n *\t\tAlan Cox\t:\tIncorrect return on truncated datagram receive.\n *\tArnt Gulbrandsen \t:\tNew udp_send and stuff\n *\t\tAlan Cox\t:\tCache last socket\n *\t\tAlan Cox\t:\tRoute cache\n *\t\tJon Peatfield\t:\tMinor efficiency fix to sendto().\n *\t\tMike Shaver\t:\tRFC1122 checks.\n *\t\tAlan Cox\t:\tNonblocking error fix.\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tLast socket cache retained as it\n *\t\t\t\t\tdoes have a high hit rate.\n *\t\tOlaf Kirch\t:\tDon't linearise iovec on sendmsg.\n *\t\tAndi Kleen\t:\tSome cleanups, cache destination entry\n *\t\t\t\t\tfor connect.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tMelvin Smith\t:\tCheck msg_name not msg_namelen in sendto(),\n *\t\t\t\t\treturn ENOTCONN for unconnected sockets (POSIX)\n *\t\tJanos Farkas\t:\tdon't deliver multi/broadcasts to a different\n *\t\t\t\t\tbound-to-device socket\n *\tHirokazu Takahashi\t:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\tHirokazu Takahashi\t:\tsendfile() on UDP works now.\n *\t\tArnaldo C. Melo :\tconvert /proc/net/udp to seq_file\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov:\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tDerek Atkins <derek@ihtfp.com>: Add Encapulation Support\n *\tJames Chapman\t\t:\tAdd L2TP encapsulation type.\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"UDP: \" fmt\n\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/tcp_states.h>\n#include <linux/skbuff.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/inet_hashtables.h>\n#include <net/route.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <trace/events/udp.h>\n#include <linux/static_key.h>\n#include <trace/events/skb.h>\n#include <net/busy_poll.h>\n#include \"udp_impl.h\"\n\nstruct udp_table udp_table __read_mostly;\nEXPORT_SYMBOL(udp_table);\n\nlong sysctl_udp_mem[3] __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_mem);\n\nint sysctl_udp_rmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_rmem_min);\n\nint sysctl_udp_wmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_wmem_min);\n\natomic_long_t udp_memory_allocated;\nEXPORT_SYMBOL(udp_memory_allocated);\n\n#define MAX_UDP_PORTS 65536\n#define PORTS_PER_CHAIN (MAX_UDP_PORTS / UDP_HTABLE_SIZE_MIN)\n\nstatic int udp_lib_lport_inuse(struct net *net, __u16 num,\n\t\t\t       const struct udp_hslot *hslot,\n\t\t\t       unsigned long *bitmap,\n\t\t\t       struct sock *sk,\n\t\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t const struct sock *sk2),\n\t\t\t       unsigned int log)\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\tkuid_t uid = sock_i_uid(sk);\n\n\tsk_nulls_for_each(sk2, node, &hslot->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (bitmap || udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (!sk2->sk_reuseport || !sk->sk_reuseport ||\n\t\t     !uid_eq(uid, sock_i_uid(sk2))) &&\n\t\t    saddr_comp(sk, sk2)) {\n\t\t\tif (!bitmap)\n\t\t\t\treturn 1;\n\t\t\t__set_bit(udp_sk(sk2)->udp_port_hash >> log, bitmap);\n\t\t}\n\t}\n\treturn 0;\n}\n\n/*\n * Note: we still hold spinlock of primary hash chain, so no other writer\n * can insert/delete a socket with local_port == num\n */\nstatic int udp_lib_lport_inuse2(struct net *net, __u16 num,\n\t\t\t\tstruct udp_hslot *hslot2,\n\t\t\t\tstruct sock *sk,\n\t\t\t\tint (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t  const struct sock *sk2))\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\tkuid_t uid = sock_i_uid(sk);\n\tint res = 0;\n\n\tspin_lock(&hslot2->lock);\n\tudp_portaddr_for_each_entry(sk2, node, &hslot2->head) {\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (!sk2->sk_reuseport || !sk->sk_reuseport ||\n\t\t     !uid_eq(uid, sock_i_uid(sk2))) &&\n\t\t    saddr_comp(sk, sk2)) {\n\t\t\tres = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&hslot2->lock);\n\treturn res;\n}\n\n/**\n *  udp_lib_get_port  -  UDP/-Lite port lookup for IPv4 and IPv6\n *\n *  @sk:          socket struct in question\n *  @snum:        port number to look up\n *  @saddr_comp:  AF-dependent comparison of bound local IP addresses\n *  @hash2_nulladdr: AF-dependent hash value in secondary hash chains,\n *                   with NULL address\n */\nint udp_lib_get_port(struct sock *sk, unsigned short snum,\n\t\t     int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t       const struct sock *sk2),\n\t\t     unsigned int hash2_nulladdr)\n{\n\tstruct udp_hslot *hslot, *hslot2;\n\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\tint    error = 1;\n\tstruct net *net = sock_net(sk);\n\n\tif (!snum) {\n\t\tint low, high, remaining;\n\t\tunsigned int rand;\n\t\tunsigned short first, last;\n\t\tDECLARE_BITMAP(bitmap, PORTS_PER_CHAIN);\n\n\t\tinet_get_local_port_range(net, &low, &high);\n\t\tremaining = (high - low) + 1;\n\n\t\trand = prandom_u32();\n\t\tfirst = reciprocal_scale(rand, remaining) + low;\n\t\t/*\n\t\t * force rand to be an odd multiple of UDP_HTABLE_SIZE\n\t\t */\n\t\trand = (rand | 1) * (udptable->mask + 1);\n\t\tlast = first + udptable->mask + 1;\n\t\tdo {\n\t\t\thslot = udp_hashslot(udptable, net, first);\n\t\t\tbitmap_zero(bitmap, PORTS_PER_CHAIN);\n\t\t\tspin_lock_bh(&hslot->lock);\n\t\t\tudp_lib_lport_inuse(net, snum, hslot, bitmap, sk,\n\t\t\t\t\t    saddr_comp, udptable->log);\n\n\t\t\tsnum = first;\n\t\t\t/*\n\t\t\t * Iterate on all possible values of snum for this hash.\n\t\t\t * Using steps of an odd multiple of UDP_HTABLE_SIZE\n\t\t\t * give us randomization and full range coverage.\n\t\t\t */\n\t\t\tdo {\n\t\t\t\tif (low <= snum && snum <= high &&\n\t\t\t\t    !test_bit(snum >> udptable->log, bitmap) &&\n\t\t\t\t    !inet_is_local_reserved_port(net, snum))\n\t\t\t\t\tgoto found;\n\t\t\t\tsnum += rand;\n\t\t\t} while (snum != first);\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t} while (++first != last);\n\t\tgoto fail;\n\t} else {\n\t\thslot = udp_hashslot(udptable, net, snum);\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (hslot->count > 10) {\n\t\t\tint exist;\n\t\t\tunsigned int slot2 = udp_sk(sk)->udp_portaddr_hash ^ snum;\n\n\t\t\tslot2          &= udptable->mask;\n\t\t\thash2_nulladdr &= udptable->mask;\n\n\t\t\thslot2 = udp_hashslot2(udptable, slot2);\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto scan_primary_hash;\n\n\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\tif (!exist && (hash2_nulladdr != slot2)) {\n\t\t\t\thslot2 = udp_hashslot2(udptable, hash2_nulladdr);\n\t\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\t}\n\t\t\tif (exist)\n\t\t\t\tgoto fail_unlock;\n\t\t\telse\n\t\t\t\tgoto found;\n\t\t}\nscan_primary_hash:\n\t\tif (udp_lib_lport_inuse(net, snum, hslot, NULL, sk,\n\t\t\t\t\tsaddr_comp, 0))\n\t\t\tgoto fail_unlock;\n\t}\nfound:\n\tinet_sk(sk)->inet_num = snum;\n\tudp_sk(sk)->udp_port_hash = snum;\n\tudp_sk(sk)->udp_portaddr_hash ^= snum;\n\tif (sk_unhashed(sk)) {\n\t\tsk_nulls_add_node_rcu(sk, &hslot->head);\n\t\thslot->count++;\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tspin_lock(&hslot2->lock);\n\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t &hslot2->head);\n\t\thslot2->count++;\n\t\tspin_unlock(&hslot2->lock);\n\t}\n\terror = 0;\nfail_unlock:\n\tspin_unlock_bh(&hslot->lock);\nfail:\n\treturn error;\n}\nEXPORT_SYMBOL(udp_lib_get_port);\n\nstatic int ipv4_rcv_saddr_equal(const struct sock *sk1, const struct sock *sk2)\n{\n\tstruct inet_sock *inet1 = inet_sk(sk1), *inet2 = inet_sk(sk2);\n\n\treturn \t(!ipv6_only_sock(sk2)  &&\n\t\t (!inet1->inet_rcv_saddr || !inet2->inet_rcv_saddr ||\n\t\t   inet1->inet_rcv_saddr == inet2->inet_rcv_saddr));\n}\n\nstatic u32 udp4_portaddr_hash(const struct net *net, __be32 saddr,\n\t\t\t      unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nint udp_v4_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp4_portaddr_hash(sock_net(sk), htonl(INADDR_ANY), snum);\n\tunsigned int hash2_partial =\n\t\tudp4_portaddr_hash(sock_net(sk), inet_sk(sk)->inet_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv4_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\t__be32 saddr, unsigned short hnum, __be16 sport,\n\t\t\t\t__be32 daddr, __be16 dport, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    ipv6_only_sock(sk))\n\t\treturn -1;\n\n\tscore = (sk->sk_family == PF_INET) ? 2 : 1;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_rcv_saddr) {\n\t\tif (inet->inet_rcv_saddr != daddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_daddr) {\n\t\tif (inet->inet_daddr != saddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\treturn score;\n}\n\n/*\n * In this second variant, we check (daddr, dport) matches (inet_rcv_sadd, inet_num)\n */\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t __be32 saddr, __be16 sport,\n\t\t\t\t __be32 daddr, unsigned int hnum, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    ipv6_only_sock(sk))\n\t\treturn -1;\n\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_rcv_saddr != daddr ||\n\t    inet->inet_num != hnum)\n\t\treturn -1;\n\n\tscore = (sk->sk_family == PF_INET) ? 2 : 1;\n\n\tif (inet->inet_daddr) {\n\t\tif (inet->inet_daddr != saddr)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore += 4;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\nstatic u32 udp_ehashfn(const struct net *net, const __be32 laddr,\n\t\t       const __u16 lport, const __be32 faddr,\n\t\t       const __be16 fport)\n{\n\tstatic u32 udp_ehash_secret __read_mostly;\n\n\tnet_get_random_once(&udp_ehash_secret, sizeof(udp_ehash_secret));\n\n\treturn __inet_ehashfn(laddr, lport, faddr, fport,\n\t\t\t      udp_ehash_secret + net_hash_mix(net));\n}\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp4_lib_lookup2(struct net *net,\n\t\t__be32 saddr, __be16 sport,\n\t\t__be32 daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\nbegin:\n\tresult = NULL;\n\tbadness = 0;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t   saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\n/* UDP is nearly always wildcards out the wazoo, it makes no sense to try\n * harder than this. -DaveM\n */\nstruct sock *__udp4_lib_lookup(struct net *net, __be32 saddr,\n\t\t__be16 sport, __be32 daddr, __be16 dport,\n\t\tint dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp4_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp4_portaddr_hash(net, htonl(INADDR_ANY), hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  htonl(INADDR_ANY), hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = 0;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, saddr, hnum, sport,\n\t\t\t\t      daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t   saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, saddr, hnum, sport,\n\t\t\t\t  daddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp4_lib_lookup);\n\nstatic inline struct sock *__udp4_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t\t __be16 sport, __be16 dport,\n\t\t\t\t\t\t struct udp_table *udptable)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\treturn __udp4_lib_lookup(dev_net(skb_dst(skb)->dev), iph->saddr, sport,\n\t\t\t\t iph->daddr, dport, inet_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp4_lib_lookup(struct net *net, __be32 saddr, __be16 sport,\n\t\t\t     __be32 daddr, __be16 dport, int dif)\n{\n\treturn __udp4_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp4_lib_lookup);\n\nstatic inline bool __udp_is_mcast_sock(struct net *net, struct sock *sk,\n\t\t\t\t       __be16 loc_port, __be32 loc_addr,\n\t\t\t\t       __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t       int dif, unsigned short hnum)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    (inet->inet_daddr && inet->inet_daddr != rmt_addr) ||\n\t    (inet->inet_dport != rmt_port && inet->inet_dport) ||\n\t    (inet->inet_rcv_saddr && inet->inet_rcv_saddr != loc_addr) ||\n\t    ipv6_only_sock(sk) ||\n\t    (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\treturn false;\n\tif (!ip_mc_sf_allow(sk, loc_addr, rmt_addr, dif))\n\t\treturn false;\n\treturn true;\n}\n\n/*\n * This routine is called by the ICMP module when it gets some\n * sort of error condition.  If err < 0 then the socket should\n * be closed and the error returned to the user.  If err > 0\n * it's just the icmp type << 8 | icmp code.\n * Header points to the ip header of the error packet. We move\n * on past this. Then (as it used to claim before adjustment)\n * header points to the first 8 bytes of the udp header.  We need\n * to find the appropriate port.\n */\n\nvoid __udp4_lib_err(struct sk_buff *skb, u32 info, struct udp_table *udptable)\n{\n\tstruct inet_sock *inet;\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+(iph->ihl<<2));\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\tint harderr;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp4_lib_lookup(net, iph->daddr, uh->dest,\n\t\t\tiph->saddr, uh->source, skb->dev->ifindex, udptable);\n\tif (!sk) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\t/* No socket for error */\n\t}\n\n\terr = 0;\n\tharderr = 0;\n\tinet = inet_sk(sk);\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code == ICMP_FRAG_NEEDED) { /* Path MTU discovery */\n\t\t\tipv4_sk_update_pmtu(skb, sk, info);\n\t\t\tif (inet->pmtudisc != IP_PMTUDISC_DONT) {\n\t\t\t\terr = EMSGSIZE;\n\t\t\t\tharderr = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\terr = EHOSTUNREACH;\n\t\tif (code <= NR_ICMP_UNREACH) {\n\t\t\tharderr = icmp_err_convert[code].fatal;\n\t\t\terr = icmp_err_convert[code].errno;\n\t\t}\n\t\tbreak;\n\tcase ICMP_REDIRECT:\n\t\tipv4_sk_redirect(skb, sk);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *      RFC1122: OK.  Passes ICMP errors back to application, as per\n\t *\t4.1.3.3.\n\t */\n\tif (!inet->recverr) {\n\t\tif (!harderr || sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t} else\n\t\tip_icmp_error(sk, skb, err, uh->dest, info, (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nvoid udp_err(struct sk_buff *skb, u32 info)\n{\n\t__udp4_lib_err(skb, info, &udp_table);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nvoid udp_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip_flush_pending_frames(sk);\n\t}\n}\nEXPORT_SYMBOL(udp_flush_pending_frames);\n\n/**\n * \tudp4_hwcsum  -  handle outgoing HW checksumming\n * \t@skb: \tsk_buff containing the filled-in UDP header\n * \t        (checksum field must be zeroed out)\n *\t@src:\tsource IP address\n *\t@dst:\tdestination IP address\n */\nvoid udp4_hwcsum(struct sk_buff *skb, __be32 src, __be32 dst)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint hlen = len;\n\t__wsum csum = 0;\n\n\tif (!skb_has_frag_list(skb)) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_tcpudp_magic(src, dst, len,\n\t\t\t\t\t       IPPROTO_UDP, 0);\n\t} else {\n\t\tstruct sk_buff *frags;\n\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\tskb_walk_frags(skb, frags) {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t\thlen -= frags->len;\n\t\t}\n\n\t\tcsum = skb_checksum(skb, offset, hlen, csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tuh->check = csum_tcpudp_magic(src, dst, len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\nEXPORT_SYMBOL_GPL(udp4_hwcsum);\n\n/* Function to set UDP checksum for an IPv4 UDP packet. This is intended\n * for the simple case like when setting the checksum for a UDP tunnel.\n */\nvoid udp_set_csum(bool nocheck, struct sk_buff *skb,\n\t\t  __be32 saddr, __be32 daddr, int len)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\n\tif (nocheck)\n\t\tuh->check = 0;\n\telse if (skb_is_gso(skb))\n\t\tuh->check = ~udp_v4_check(len, saddr, daddr, 0);\n\telse if (skb_dst(skb) && skb_dst(skb)->dev &&\n\t\t (skb_dst(skb)->dev->features &\n\t\t  (NETIF_F_IP_CSUM | NETIF_F_HW_CSUM))) {\n\n\t\tBUG_ON(skb->ip_summed == CHECKSUM_PARTIAL);\n\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~udp_v4_check(len, saddr, daddr, 0);\n\t} else {\n\t\t__wsum csum;\n\n\t\tBUG_ON(skb->ip_summed == CHECKSUM_PARTIAL);\n\n\t\tuh->check = 0;\n\t\tcsum = skb_checksum(skb, 0, len, 0);\n\t\tuh->check = udp_v4_check(len, saddr, daddr, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n}\nEXPORT_SYMBOL(udp_set_csum);\n\nstatic int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\t__wsum csum = 0;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = fl4->fl4_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check_tx) {   /* UDP csum disabled */\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\n\t\tudp4_hwcsum(skb, fl4->saddr, fl4->daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_tcpudp_magic(fl4->saddr, fl4->daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(sock_net(sk), skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet->recverr) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t   UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\n/*\n * Push out all pending data as one UDP datagram. Socket is locked.\n */\nint udp_push_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 *fl4 = &inet->cork.fl.u.ip4;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tskb = ip_finish_skb(sk, fl4);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_send_skb(skb, fl4);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\nEXPORT_SYMBOL(udp_push_pending_frames);\n\nint udp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct flowi4 fl4_stack;\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\t__be16 dport;\n\tu8  tos;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\tstruct ip_options_data opt_copy;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\tif (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */\n\t\treturn -EOPNOTSUPP;\n\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.ttl = 0;\n\tipc.tos = -1;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tfl4 = &inet->cork.fl.u.ip4;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t/* Open fast path for connected socket.\n\t\t   Route will not be used, if at least one option is set.\n\t\t */\n\t\tconnected = 1;\n\t}\n\tipc.addr = inet->inet_saddr;\n\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tsock_tx_timestamp(sk, &ipc.tx_flags);\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc,\n\t\t\t\t   sk->sk_family == AF_INET6);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tfaddr = ipc.opt->opt.faddr;\n\t\tconnected = 0;\n\t}\n\ttos = get_rttos(&ipc, inet);\n\tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n\t    (msg->msg_flags & MSG_DONTROUTE) ||\n\t    (ipc.opt && ipc.opt->opt.is_strictroute)) {\n\t\ttos |= RTO_ONLINK;\n\t\tconnected = 0;\n\t}\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t} else if (!ipc.oif)\n\t\tipc.oif = inet->uc_index;\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (!rt) {\n\t\tstruct net *net = sock_net(sk);\n\t\t__u8 flow_flags = inet_sk_flowi_flags(sk);\n\n\t\tfl4 = &fl4_stack;\n\n\t\tflowi4_init_output(fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   flow_flags,\n\t\t\t\t   faddr, saddr, dport, inet->inet_sport);\n\n\t\tif (!saddr && ipc.oif)\n\t\t\tl3mdev_get_saddr(net, ipc.oif, fl4);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(fl4));\n\t\trt = ip_route_output_flow(net, fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = fl4->saddr;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = fl4->daddr;\n\n\t/* Lockless fast path for the non-corking case. */\n\tif (!corkreq) {\n\t\tskb = ip_make_skb(sk, fl4, getfrag, msg, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_send_skb(skb, fl4);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t/*\n\t *\tNow cork the socket to pend data.\n\t */\n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tup->pending = AF_INET;\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, fl4, getfrag, msg, ulen,\n\t\t\t     sizeof(struct udphdr), &ipc, &rt,\n\t\t\t     corkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\nEXPORT_SYMBOL(udp_sendmsg);\n\nint udp_sendpage(struct sock *sk, struct page *page, int offset,\n\t\t size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tint ret;\n\n\tif (flags & MSG_SENDPAGE_NOTLAST)\n\t\tflags |= MSG_MORE;\n\n\tif (!up->pending) {\n\t\tstruct msghdr msg = {\t.msg_flags = flags|MSG_MORE };\n\n\t\t/* Call udp_sendmsg to specify destination address which\n\t\t * sendpage interface can't pass.\n\t\t * This will succeed only when the socket is connected.\n\t\t */\n\t\tret = udp_sendmsg(sk, &msg, 0);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tlock_sock(sk);\n\n\tif (unlikely(!up->pending)) {\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 3\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = ip_append_page(sk, &inet->cork.fl.u.ip4,\n\t\t\t     page, offset, size, flags);\n\tif (ret == -EOPNOTSUPP) {\n\t\trelease_sock(sk);\n\t\treturn sock_no_sendpage(sk->sk_socket, page, offset,\n\t\t\t\t\tsize, flags);\n\t}\n\tif (ret < 0) {\n\t\tudp_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\tup->len += size;\n\tif (!(up->corkflag || (flags&MSG_MORE)))\n\t\tret = udp_push_pending_frames(sk);\n\tif (!ret)\n\t\tret = size;\nout:\n\trelease_sock(sk);\n\treturn ret;\n}\n\n/**\n *\tfirst_packet_length\t- return length of first packet in receive queue\n *\t@sk: socket\n *\n *\tDrops all bad checksum frames, until a valid one is found.\n *\tReturns the length of found skb, or 0 if none is found.\n */\nstatic unsigned int first_packet_length(struct sock *sk)\n{\n\tstruct sk_buff_head list_kill, *rcvq = &sk->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tunsigned int res;\n\n\t__skb_queue_head_init(&list_kill);\n\n\tspin_lock_bh(&rcvq->lock);\n\twhile ((skb = skb_peek(rcvq)) != NULL &&\n\t\tudp_lib_checksum_complete(skb)) {\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS,\n\t\t\t\t IS_UDPLITE(sk));\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t IS_UDPLITE(sk));\n\t\tatomic_inc(&sk->sk_drops);\n\t\t__skb_unlink(skb, rcvq);\n\t\t__skb_queue_tail(&list_kill, skb);\n\t}\n\tres = skb ? skb->len : 0;\n\tspin_unlock_bh(&rcvq->lock);\n\n\tif (!skb_queue_empty(&list_kill)) {\n\t\tbool slow = lock_sock_fast(sk);\n\n\t\t__skb_queue_purge(&list_kill);\n\t\tsk_mem_reclaim_partial(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t}\n\treturn res;\n}\n\n/*\n *\tIOCTL requests applicable to the UDP protocol\n */\n\nint udp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tcase SIOCINQ:\n\t{\n\t\tunsigned int amount = first_packet_length(sk);\n\n\t\tif (amount)\n\t\t\t/*\n\t\t\t * We will only return the amount\n\t\t\t * of this packet since that is all\n\t\t\t * that will be read.\n\t\t\t */\n\t\t\tamount -= sizeof(struct udphdr);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_ioctl);\n\n/*\n * \tThis should be easy, if there is something there we\n * \treturn it, otherwise we block.\n */\n\nint udp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int noblock,\n\t\tint flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, msg->msg_name);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool checksum_valid = false;\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len, addr_len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tchecksum_valid = !udp_lib_checksum_complete(skb);\n\t\tif (!checksum_valid)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (checksum_valid || skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t    msg, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t\t     msg);\n\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udp_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\n\tif (!peeked)\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\t*addr_len = sizeof(*sin);\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv_offset(msg, skb, sizeof(struct udphdr));\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\t/* starting over for a new packet, but check if we need to yield */\n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nint udp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t/*\n\t *\t1003.1g - break association.\n\t */\n\n\tsk->sk_state = TCP_CLOSE;\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsock_rps_reset_rxhash(sk);\n\tsk->sk_bound_dev_if = 0;\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tif (!(sk->sk_userlocks & SOCK_BINDPORT_LOCK)) {\n\t\tsk->sk_prot->unhash(sk);\n\t\tinet->inet_sport = 0;\n\t}\n\tsk_dst_reset(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_disconnect);\n\nvoid udp_lib_unhash(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2;\n\n\t\thslot  = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t      udp_sk(sk)->udp_port_hash);\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (sk_nulls_del_node_init_rcu(sk)) {\n\t\t\thslot->count--;\n\t\t\tinet_sk(sk)->inet_num = 0;\n\t\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n}\nEXPORT_SYMBOL(udp_lib_unhash);\n\n/*\n * inet_rcv_saddr was changed, we must rehash secondary hash\n */\nvoid udp_lib_rehash(struct sock *sk, u16 newhash)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2, *nhslot2;\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tnhslot2 = udp_hashslot2(udptable, newhash);\n\t\tudp_sk(sk)->udp_portaddr_hash = newhash;\n\t\tif (hslot2 != nhslot2) {\n\t\t\thslot = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t\t     udp_sk(sk)->udp_port_hash);\n\t\t\t/* we must lock primary chain too */\n\t\t\tspin_lock_bh(&hslot->lock);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\n\t\t\tspin_lock(&nhslot2->lock);\n\t\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t\t &nhslot2->head);\n\t\t\tnhslot2->count++;\n\t\t\tspin_unlock(&nhslot2->lock);\n\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(udp_lib_rehash);\n\nstatic void udp_v4_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp4_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  inet_sk(sk)->inet_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic int __udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (inet_sk(sk)->inet_daddr) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t}\n\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t is_udplite);\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\ttrace_udp_fail_queue_rcv_skb(rc, sk);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n\n}\n\nstatic struct static_key udp_encap_needed __read_mostly;\nvoid udp_encap_enable(void)\n{\n\tif (!static_key_enabled(&udp_encap_needed))\n\t\tstatic_key_slow_inc(&udp_encap_needed);\n}\nEXPORT_SYMBOL(udp_encap_enable);\n\n/* returns:\n *  -1: error\n *   0: success\n *  >0: \"udp encap\" protocol resubmission\n *\n * Note that in the success and error cases, the skb is assumed to\n * have either been requeued or freed.\n */\nint udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (static_key_false(&udp_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (skb->len > sizeof(struct udphdr) && encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLite: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLite: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t is_udplite);\n\t\tgoto drop;\n\t}\n\n\trc = 0;\n\n\tipv4_pktinfo_prepare(sk, skb);\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tunsigned int i;\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(!skb1))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udp_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\n\t\tsock_put(sk);\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\n/* For TCP sockets, sk_rx_dst is protected by socket lock\n * For UDP, we use xchg() to guard against concurrent changes.\n */\nstatic void udp_sk_rx_dst_set(struct sock *sk, struct dst_entry *dst)\n{\n\tstruct dst_entry *old;\n\n\tdst_hold(dst);\n\told = xchg(&sk->sk_rx_dst, dst);\n\tdst_release(old);\n}\n\n/*\n *\tMulticasts and broadcasts go to each listener.\n *\n *\tNote: called only from the BH handler context.\n */\nstatic int __udp4_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\t\t\t    struct udphdr  *uh,\n\t\t\t\t    __be32 saddr, __be32 daddr,\n\t\t\t\t    struct udp_table *udptable,\n\t\t\t\t    int proto)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tint dif = skb->dev->ifindex;\n\tunsigned int count = 0, offset = offsetof(typeof(*sk), sk_nulls_node);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tbool inner_flushed = false;\n\n\tif (use_hash2) {\n\t\thash2_any = udp4_portaddr_hash(net, htonl(INADDR_ANY), hnum) &\n\t\t\t    udp_table.mask;\n\t\thash2 = udp4_portaddr_hash(net, daddr, hnum) & udp_table.mask;\nstart_lookup:\n\t\thslot = &udp_table.hash2[hash2];\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tspin_lock(&hslot->lock);\n\tsk_nulls_for_each_entry_offset(sk, node, &hslot->head, offset) {\n\t\tif (__udp_is_mcast_sock(net, sk,\n\t\t\t\t\tuh->dest, daddr,\n\t\t\t\t\tuh->source, saddr,\n\t\t\t\t\tdif, hnum)) {\n\t\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\t\tinner_flushed = true;\n\t\t\t\tcount = 0;\n\t\t\t}\n\t\t\tstack[count++] = sk;\n\t\t\tsock_hold(sk);\n\t\t}\n\t}\n\n\tspin_unlock(&hslot->lock);\n\n\t/* Also lookup *:port if we are using hash2 and haven't done so yet. */\n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\t/*\n\t * do the slow work with no lock held\n\t */\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\t} else {\n\t\tif (!inner_flushed)\n\t\t\tUDP_INC_STATS_BH(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\t\t proto == IPPROTO_UDPLITE);\n\t\tconsume_skb(skb);\n\t}\n\treturn 0;\n}\n\n/* Initialize UDP checksum. If exited with zero value (success),\n * CHECKSUM_UNNECESSARY means, that no more checks are required.\n * Otherwise, csum completion requires chacksumming packet body,\n * including udp header and folding it to skb->csum.\n */\nstatic inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn skb_checksum_init_zero_check(skb, proto, uh->check,\n\t\t\t\t\t    inet_compute_pseudo);\n}\n\n/*\n *\tAll we need to do is get the socket, and then do a checksum.\n */\n\nint __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tunsigned short ulen;\n\tstruct rtable *rt = skb_rtable(skb);\n\t__be32 saddr, daddr;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/*\n\t *  Validate the packet.\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto drop;\t\t/* No space for header. */\n\n\tuh   = udp_hdr(skb);\n\tulen = ntohs(uh->len);\n\tsaddr = ip_hdr(skb)->saddr;\n\tdaddr = ip_hdr(skb)->daddr;\n\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\t\tif (ulen < sizeof(*uh) || pskb_trim_rcsum(skb, ulen))\n\t\t\tgoto short_packet;\n\t\tuh = udp_hdr(skb);\n\t}\n\n\tif (udp4_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\tsk = skb_steal_sock(skb);\n\tif (sk) {\n\t\tstruct dst_entry *dst = skb_dst(skb);\n\t\tint ret;\n\n\t\tif (unlikely(sk->sk_rx_dst != dst))\n\t\t\tudp_sk_rx_dst_set(sk, dst);\n\n\t\tret = udp_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\t\treturn 0;\n\t}\n\n\tif (rt->rt_flags & (RTCF_BROADCAST|RTCF_MULTICAST))\n\t\treturn __udp4_lib_mcast_deliver(net, skb, uh,\n\t\t\t\t\t\tsaddr, daddr, udptable, proto);\n\n\tsk = __udp4_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk) {\n\t\tint ret;\n\n\t\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t inet_compute_pseudo);\n\n\t\tret = udp_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\t\treturn 0;\n\t}\n\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\t/* No socket. Drop packet silently, if checksum is wrong */\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\n\t/*\n\t * Hmm.  We got an UDP packet to a port to which we\n\t * don't wanna listen.  Ignore it.\n\t */\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tnet_dbg_ratelimited(\"UDP%s: short packet: From %pI4:%u %d/%d to %pI4:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"Lite\" : \"\",\n\t\t\t    &saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    &daddr, ntohs(uh->dest));\n\tgoto drop;\n\ncsum_error:\n\t/*\n\t * RFC1122: OK.  Discards the bad packet silently (as far as\n\t * the network is concerned, anyway) as per 4.1.3.4 (MUST).\n\t */\n\tnet_dbg_ratelimited(\"UDP%s: bad checksum. From %pI4:%u to %pI4:%u ulen %d\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"Lite\" : \"\",\n\t\t\t    &saddr, ntohs(uh->source), &daddr, ntohs(uh->dest),\n\t\t\t    ulen);\n\tUDP_INC_STATS_BH(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndrop:\n\tUDP_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\n/* We can only early demux multicast if there is a single matching socket.\n * If more than one socket found returns NULL\n */\nstatic struct sock *__udp4_lib_mcast_demux_lookup(struct net *net,\n\t\t\t\t\t\t  __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t\t  __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t\t  int dif)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(loc_port);\n\tunsigned int count, slot = udp_hashfn(net, hnum, udp_table.mask);\n\tstruct udp_hslot *hslot = &udp_table.hash[slot];\n\n\t/* Do not bother scanning a too big list */\n\tif (hslot->count > 10)\n\t\treturn NULL;\n\n\trcu_read_lock();\nbegin:\n\tcount = 0;\n\tresult = NULL;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tif (__udp_is_mcast_sock(net, sk,\n\t\t\t\t\tloc_port, loc_addr,\n\t\t\t\t\trmt_port, rmt_addr,\n\t\t\t\t\tdif, hnum)) {\n\t\t\tresult = sk;\n\t\t\t++count;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (count != 1 ||\n\t\t    unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(!__udp_is_mcast_sock(net, result,\n\t\t\t\t\t\t       loc_port, loc_addr,\n\t\t\t\t\t\t       rmt_port, rmt_addr,\n\t\t\t\t\t\t       dif, hnum))) {\n\t\t\tsock_put(result);\n\t\t\tresult = NULL;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\n/* For unicast we should only early demux connected sockets or we can\n * break forwarding setups.  The chains here can be long so only check\n * if the first socket is an exact match and if not move on.\n */\nstatic struct sock *__udp4_lib_demux_lookup(struct net *net,\n\t\t\t\t\t    __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t    __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t    int dif)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(loc_port);\n\tunsigned int hash2 = udp4_portaddr_hash(net, loc_addr, hnum);\n\tunsigned int slot2 = hash2 & udp_table.mask;\n\tstruct udp_hslot *hslot2 = &udp_table.hash2[slot2];\n\tINET_ADDR_COOKIE(acookie, rmt_addr, loc_addr);\n\tconst __portpair ports = INET_COMBINED_PORTS(rmt_port, hnum);\n\n\trcu_read_lock();\n\tresult = NULL;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tif (INET_MATCH(sk, net, acookie,\n\t\t\t       rmt_addr, loc_addr, ports, dif))\n\t\t\tresult = sk;\n\t\t/* Only check first socket in chain */\n\t\tbreak;\n\t}\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(!INET_MATCH(sk, net, acookie,\n\t\t\t\t\t      rmt_addr, loc_addr,\n\t\t\t\t\t      ports, dif))) {\n\t\t\tsock_put(result);\n\t\t\tresult = NULL;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\nvoid udp_v4_early_demux(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct iphdr *iph;\n\tconst struct udphdr *uh;\n\tstruct sock *sk;\n\tstruct dst_entry *dst;\n\tint dif = skb->dev->ifindex;\n\tint ours;\n\n\t/* validate the packet */\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct udphdr)))\n\t\treturn;\n\n\tiph = ip_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tif (skb->pkt_type == PACKET_BROADCAST ||\n\t    skb->pkt_type == PACKET_MULTICAST) {\n\t\tstruct in_device *in_dev = __in_dev_get_rcu(skb->dev);\n\n\t\tif (!in_dev)\n\t\t\treturn;\n\n\t\tours = ip_check_mc_rcu(in_dev, iph->daddr, iph->saddr,\n\t\t\t\t       iph->protocol);\n\t\tif (!ours)\n\t\t\treturn;\n\t\tsk = __udp4_lib_mcast_demux_lookup(net, uh->dest, iph->daddr,\n\t\t\t\t\t\t   uh->source, iph->saddr, dif);\n\t} else if (skb->pkt_type == PACKET_HOST) {\n\t\tsk = __udp4_lib_demux_lookup(net, uh->dest, iph->daddr,\n\t\t\t\t\t     uh->source, iph->saddr, dif);\n\t} else {\n\t\treturn;\n\t}\n\n\tif (!sk)\n\t\treturn;\n\n\tskb->sk = sk;\n\tskb->destructor = sock_efree;\n\tdst = READ_ONCE(sk->sk_rx_dst);\n\n\tif (dst)\n\t\tdst = dst_check(dst, 0);\n\tif (dst) {\n\t\t/* DST_NOCACHE can not be used without taking a reference */\n\t\tif (dst->flags & DST_NOCACHE) {\n\t\t\tif (likely(atomic_inc_not_zero(&dst->__refcnt)))\n\t\t\t\tskb_dst_set(skb, dst);\n\t\t} else {\n\t\t\tskb_dst_set_noref(skb, dst);\n\t\t}\n\t}\n}\n\nint udp_rcv(struct sk_buff *skb)\n{\n\treturn __udp4_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\nvoid udp_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tbool slow = lock_sock_fast(sk);\n\tudp_flush_pending_frames(sk);\n\tunlock_sock_fast(sk, slow);\n\tif (static_key_false(&udp_encap_needed) && up->encap_type) {\n\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\tencap_destroy = ACCESS_ONCE(up->encap_destroy);\n\t\tif (encap_destroy)\n\t\t\tencap_destroy(sk);\n\t}\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udp_lib_setsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, unsigned int optlen,\n\t\t       int (*push_pending_frames)(struct sock *))\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, valbool;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tif (val != 0) {\n\t\t\tup->corkflag = 1;\n\t\t} else {\n\t\t\tup->corkflag = 0;\n\t\t\tlock_sock(sk);\n\t\t\tpush_pending_frames(sk);\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tswitch (val) {\n\t\tcase 0:\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tup->encap_rcv = xfrm4_udp_encap_rcv;\n\t\t\t/* FALLTHROUGH */\n\t\tcase UDP_ENCAP_L2TPINUDP:\n\t\t\tup->encap_type = val;\n\t\t\tudp_encap_enable();\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_TX:\n\t\tup->no_check6_tx = valbool;\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_RX:\n\t\tup->no_check6_rx = valbool;\n\t\tbreak;\n\n\t/*\n\t * \tUDP-Lite's partial checksum coverage (RFC 3828).\n\t */\n\t/* The sender sets actual checksum coverage length via this option.\n\t * The case coverage > packet length is handled by send module. */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Illegal coverage: use default (8) */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcslen = val;\n\t\tup->pcflag |= UDPLITE_SEND_CC;\n\t\tbreak;\n\n\t/* The receiver specifies a minimum checksum coverage value. To make\n\t * sense, this should be set to at least 8 (as done below). If zero is\n\t * used, this again means full checksum coverage.                     */\n\tcase UDPLITE_RECV_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Avoid silly minimal values.       */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcrlen = val;\n\t\tup->pcflag |= UDPLITE_RECV_CC;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(udp_lib_setsockopt);\n\nint udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn compat_ip_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udp_lib_getsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tval = up->corkflag;\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tval = up->encap_type;\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_TX:\n\t\tval = up->no_check6_tx;\n\t\tbreak;\n\n\tcase UDP_NO_CHECK6_RX:\n\t\tval = up->no_check6_rx;\n\t\tbreak;\n\n\t/* The following two cannot be changed on UDP sockets, the return is\n\t * always 0 (which corresponds to the full checksum coverage of UDP). */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tval = up->pcslen;\n\t\tbreak;\n\n\tcase UDPLITE_RECV_CSCOV:\n\t\tval = up->pcrlen;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_lib_getsockopt);\n\nint udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ip_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n/**\n * \tudp_poll - wait for a UDP event.\n *\t@file - file struct\n *\t@sock - socket\n *\t@wait - poll table\n *\n *\tThis is same as datagram poll, except for the special case of\n *\tblocking sockets. If application is using a blocking fd\n *\tand a packet with checksum error is in the queue;\n *\tthen it could get return from select indicating data available\n *\tbut then block when reading it. Add special case code\n *\tto work around these arguably broken applications.\n */\nunsigned int udp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* Check for false positives due to checksum errors */\n\tif ((mask & POLLRDNORM) && !(file->f_flags & O_NONBLOCK) &&\n\t    !(sk->sk_shutdown & RCV_SHUTDOWN) && !first_packet_length(sk))\n\t\tmask &= ~(POLLIN | POLLRDNORM);\n\n\treturn mask;\n\n}\nEXPORT_SYMBOL(udp_poll);\n\nstruct proto udp_prot = {\n\t.name\t\t   = \"UDP\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip4_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udp_destroy_sock,\n\t.setsockopt\t   = udp_setsockopt,\n\t.getsockopt\t   = udp_getsockopt,\n\t.sendmsg\t   = udp_sendmsg,\n\t.recvmsg\t   = udp_recvmsg,\n\t.sendpage\t   = udp_sendpage,\n\t.backlog_rcv\t   = __udp_queue_rcv_skb,\n\t.release_cb\t   = ip4_datagram_release_cb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v4_rehash,\n\t.get_port\t   = udp_v4_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udp_setsockopt,\n\t.compat_getsockopt = compat_udp_getsockopt,\n#endif\n\t.clear_sk\t   = sk_prot_clear_portaddr_nulls,\n};\nEXPORT_SYMBOL(udp_prot);\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\n\nstatic struct sock *udp_get_first(struct seq_file *seq, int start)\n{\n\tstruct sock *sk;\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tfor (state->bucket = start; state->bucket <= state->udp_table->mask;\n\t     ++state->bucket) {\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct udp_hslot *hslot = &state->udp_table->hash[state->bucket];\n\n\t\tif (hlist_nulls_empty(&hslot->head))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tsk_nulls_for_each(sk, node, &hslot->head) {\n\t\t\tif (!net_eq(sock_net(sk), net))\n\t\t\t\tcontinue;\n\t\t\tif (sk->sk_family == state->family)\n\t\t\t\tgoto found;\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tdo {\n\t\tsk = sk_nulls_next(sk);\n\t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n\n\tif (!sk) {\n\t\tif (state->bucket <= state->udp_table->mask)\n\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}\n\nstatic struct sock *udp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = udp_get_first(seq, 0);\n\n\tif (sk)\n\t\twhile (pos && (sk = udp_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nstatic void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstate->bucket = MAX_UDP_PORTS;\n\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}\n\nstatic void *udp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = udp_get_idx(seq, 0);\n\telse\n\t\tsk = udp_get_next(seq, v);\n\n\t++*pos;\n\treturn sk;\n}\n\nstatic void udp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct udp_iter_state *state = seq->private;\n\n\tif (state->bucket <= state->udp_table->mask)\n\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n}\n\nint udp_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct udp_seq_afinfo *afinfo = PDE_DATA(inode);\n\tstruct udp_iter_state *s;\n\tint err;\n\n\terr = seq_open_net(inode, file, &afinfo->seq_ops,\n\t\t\t   sizeof(struct udp_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ts = ((struct seq_file *)file->private_data)->private;\n\ts->family\t\t= afinfo->family;\n\ts->udp_table\t\t= afinfo->udp_table;\n\treturn err;\n}\nEXPORT_SYMBOL(udp_seq_open);\n\n/* ------------------------------------------------------------------------ */\nint udp_proc_register(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tstruct proc_dir_entry *p;\n\tint rc = 0;\n\n\tafinfo->seq_ops.start\t\t= udp_seq_start;\n\tafinfo->seq_ops.next\t\t= udp_seq_next;\n\tafinfo->seq_ops.stop\t\t= udp_seq_stop;\n\n\tp = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,\n\t\t\t     afinfo->seq_fops, afinfo);\n\tif (!p)\n\t\trc = -ENOMEM;\n\treturn rc;\n}\nEXPORT_SYMBOL(udp_proc_register);\n\nvoid udp_proc_unregister(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tremove_proc_entry(afinfo->name, net->proc_net);\n}\nEXPORT_SYMBOL(udp_proc_unregister);\n\n/* ------------------------------------------------------------------------ */\nstatic void udp4_format_sock(struct sock *sp, struct seq_file *f,\n\t\tint bucket)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src  = inet->inet_rcv_saddr;\n\t__u16 destp\t  = ntohs(inet->inet_dport);\n\t__u16 srcp\t  = ntohs(inet->inet_sport);\n\n\tseq_printf(f, \"%5d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %d\",\n\t\tbucket, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tsk_rmem_alloc_get(sp),\n\t\t0, 0L, 0,\n\t\tfrom_kuid_munged(seq_user_ns(f), sock_i_uid(sp)),\n\t\t0, sock_i_ino(sp),\n\t\tatomic_read(&sp->sk_refcnt), sp,\n\t\tatomic_read(&sp->sk_drops));\n}\n\nint udp4_seq_show(struct seq_file *seq, void *v)\n{\n\tseq_setwidth(seq, 127);\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode ref pointer drops\");\n\telse {\n\t\tstruct udp_iter_state *state = seq->private;\n\n\t\tudp4_format_sock(v, seq, state->bucket);\n\t}\n\tseq_pad(seq, '\\n');\n\treturn 0;\n}\n\nstatic const struct file_operations udp_afinfo_seq_fops = {\n\t.owner    = THIS_MODULE,\n\t.open     = udp_seq_open,\n\t.read     = seq_read,\n\t.llseek   = seq_lseek,\n\t.release  = seq_release_net\n};\n\n/* ------------------------------------------------------------------------ */\nstatic struct udp_seq_afinfo udp4_seq_afinfo = {\n\t.name\t\t= \"udp\",\n\t.family\t\t= AF_INET,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= &udp_afinfo_seq_fops,\n\t.seq_ops\t= {\n\t\t.show\t\t= udp4_seq_show,\n\t},\n};\n\nstatic int __net_init udp4_proc_init_net(struct net *net)\n{\n\treturn udp_proc_register(net, &udp4_seq_afinfo);\n}\n\nstatic void __net_exit udp4_proc_exit_net(struct net *net)\n{\n\tudp_proc_unregister(net, &udp4_seq_afinfo);\n}\n\nstatic struct pernet_operations udp4_net_ops = {\n\t.init = udp4_proc_init_net,\n\t.exit = udp4_proc_exit_net,\n};\n\nint __init udp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&udp4_net_ops);\n}\n\nvoid udp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&udp4_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n\nstatic __initdata unsigned long uhash_entries;\nstatic int __init set_uhash_entries(char *str)\n{\n\tssize_t ret;\n\n\tif (!str)\n\t\treturn 0;\n\n\tret = kstrtoul(str, 0, &uhash_entries);\n\tif (ret)\n\t\treturn 0;\n\n\tif (uhash_entries && uhash_entries < UDP_HTABLE_SIZE_MIN)\n\t\tuhash_entries = UDP_HTABLE_SIZE_MIN;\n\treturn 1;\n}\n__setup(\"uhash_entries=\", set_uhash_entries);\n\nvoid __init udp_table_init(struct udp_table *table, const char *name)\n{\n\tunsigned int i;\n\n\ttable->hash = alloc_large_system_hash(name,\n\t\t\t\t\t      2 * sizeof(struct udp_hslot),\n\t\t\t\t\t      uhash_entries,\n\t\t\t\t\t      21, /* one slot per 2 MB */\n\t\t\t\t\t      0,\n\t\t\t\t\t      &table->log,\n\t\t\t\t\t      &table->mask,\n\t\t\t\t\t      UDP_HTABLE_SIZE_MIN,\n\t\t\t\t\t      64 * 1024);\n\n\ttable->hash2 = table->hash + (table->mask + 1);\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash[i].head, i);\n\t\ttable->hash[i].count = 0;\n\t\tspin_lock_init(&table->hash[i].lock);\n\t}\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash2[i].head, i);\n\t\ttable->hash2[i].count = 0;\n\t\tspin_lock_init(&table->hash2[i].lock);\n\t}\n}\n\nu32 udp_flow_hashrnd(void)\n{\n\tstatic u32 hashrnd __read_mostly;\n\n\tnet_get_random_once(&hashrnd, sizeof(hashrnd));\n\n\treturn hashrnd;\n}\nEXPORT_SYMBOL(udp_flow_hashrnd);\n\nvoid __init udp_init(void)\n{\n\tunsigned long limit;\n\n\tudp_table_init(&udp_table, \"UDP\");\n\tlimit = nr_free_buffer_pages() / 8;\n\tlimit = max(limit, 128UL);\n\tsysctl_udp_mem[0] = limit / 4 * 3;\n\tsysctl_udp_mem[1] = limit;\n\tsysctl_udp_mem[2] = sysctl_udp_mem[0] * 2;\n\n\tsysctl_udp_rmem_min = SK_MEM_QUANTUM;\n\tsysctl_udp_wmem_min = SK_MEM_QUANTUM;\n}\n", "/*\n *\tUDP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/ipv4/udp.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *      Kazunori MIYAZAWA @USAGI:       change process style to use ip6_append_data\n *      YOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/udp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/raw.h>\n#include <net/tcp_states.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/inet6_hashtables.h>\n#include <net/busy_poll.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <trace/events/skb.h>\n#include \"udp_impl.h\"\n\nstatic u32 udp6_ehashfn(const struct net *net,\n\t\t\tconst struct in6_addr *laddr,\n\t\t\tconst u16 lport,\n\t\t\tconst struct in6_addr *faddr,\n\t\t\tconst __be16 fport)\n{\n\tstatic u32 udp6_ehash_secret __read_mostly;\n\tstatic u32 udp_ipv6_hash_secret __read_mostly;\n\n\tu32 lhash, fhash;\n\n\tnet_get_random_once(&udp6_ehash_secret,\n\t\t\t    sizeof(udp6_ehash_secret));\n\tnet_get_random_once(&udp_ipv6_hash_secret,\n\t\t\t    sizeof(udp_ipv6_hash_secret));\n\n\tlhash = (__force u32)laddr->s6_addr32[3];\n\tfhash = __ipv6_addr_jhash(faddr, udp_ipv6_hash_secret);\n\n\treturn __inet6_ehashfn(lhash, lport, fhash, fport,\n\t\t\t       udp_ipv6_hash_secret + net_hash_mix(net));\n}\n\nint ipv6_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2)\n{\n\tconst struct in6_addr *sk2_rcv_saddr6 = inet6_rcv_saddr(sk2);\n\tint sk2_ipv6only = inet_v6_ipv6only(sk2);\n\tint addr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED)\n\t\treturn (!sk2_ipv6only &&\n\t\t\t(!sk->sk_rcv_saddr || !sk2->sk_rcv_saddr ||\n\t\t\t  sk->sk_rcv_saddr == sk2->sk_rcv_saddr));\n\n\tif (addr_type2 == IPV6_ADDR_ANY &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY &&\n\t    !(ipv6_only_sock(sk) && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(&sk->sk_v6_rcv_saddr, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic u32 udp6_portaddr_hash(const struct net *net,\n\t\t\t      const struct in6_addr *addr6,\n\t\t\t      unsigned int port)\n{\n\tunsigned int hash, mix = net_hash_mix(net);\n\n\tif (ipv6_addr_any(addr6))\n\t\thash = jhash_1word(0, mix);\n\telse if (ipv6_addr_v4mapped(addr6))\n\t\thash = jhash_1word((__force u32)addr6->s6_addr32[3], mix);\n\telse\n\t\thash = jhash2((__force u32 *)addr6->s6_addr32, 4, mix);\n\n\treturn hash ^ port;\n}\n\nint udp_v6_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp6_portaddr_hash(sock_net(sk), &in6addr_any, snum);\n\tunsigned int hash2_partial =\n\t\tudp6_portaddr_hash(sock_net(sk), &sk->sk_v6_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv6_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic void udp_v6_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp6_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\tunsigned short hnum,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, __be16 dport,\n\t\t\t\tint dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t const struct in6_addr *daddr,\n\t\t\t\t unsigned short hnum, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp6_lib_lookup2(struct net *net,\n\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\tconst struct in6_addr *daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp6_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t    saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\nstruct sock *__udp6_lib_lookup(struct net *net,\n\t\t\t\t      const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t      const struct in6_addr *daddr, __be16 dport,\n\t\t\t\t      int dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp6_portaddr_hash(net, &in6addr_any, hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  &in6addr_any, hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, hnum, saddr, sport, daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp6_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t    saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, hnum, saddr, sport,\n\t\t\t\t\tdaddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp6_lib_lookup);\n\nstatic struct sock *__udp6_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t  __be16 sport, __be16 dport,\n\t\t\t\t\t  struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\n\tsk = skb_steal_sock(skb);\n\tif (unlikely(sk))\n\t\treturn sk;\n\treturn __udp6_lib_lookup(dev_net(skb_dst(skb)->dev), &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, inet6_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp6_lib_lookup(struct net *net, const struct in6_addr *saddr, __be16 sport,\n\t\t\t     const struct in6_addr *daddr, __be16 dport, int dif)\n{\n\treturn __udp6_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp6_lib_lookup);\n\n/*\n *\tThis should be easy, if there is something there we\n *\treturn it, otherwise we block.\n */\n\nint udpv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool checksum_valid = false;\n\tint is_udp4;\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tchecksum_valid = !udp_lib_checksum_complete(skb);\n\t\tif (!checksum_valid)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (checksum_valid || skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t    msg, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, sizeof(struct udphdr), msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    inet6_iif(skb));\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\t/* starting over for a new packet, but check if we need to yield */\n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nvoid __udp6_lib_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info,\n\t\t    struct udp_table *udptable)\n{\n\tstruct ipv6_pinfo *np;\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct in6_addr *saddr = &hdr->saddr;\n\tconst struct in6_addr *daddr = &hdr->daddr;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+offset);\n\tstruct sock *sk;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp6_lib_lookup(net, daddr, uh->dest,\n\t\t\t       saddr, uh->source, inet6_iif(skb), udptable);\n\tif (!sk) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tip6_sk_redirect(skb, sk);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_err_convert(type, code, &err) && !np->recverr)\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_ESTABLISHED && !np->recverr)\n\t\tgoto out;\n\n\tif (np->recverr)\n\t\tipv6_icmp_error(sk, skb, err, uh->dest, ntohl(info), (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nstatic int __udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t}\n\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tUDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic __inline__ void udpv6_err(struct sk_buff *skb,\n\t\t\t\t struct inet6_skb_parm *opt, u8 type,\n\t\t\t\t u8 code, int offset, __be32 info)\n{\n\t__udp6_lib_err(skb, opt, type, code, offset, info, &udp_table);\n}\n\nstatic struct static_key udpv6_encap_needed __read_mostly;\nvoid udpv6_encap_enable(void)\n{\n\tif (!static_key_enabled(&udpv6_encap_needed))\n\t\tstatic_key_slow_inc(&udpv6_encap_needed);\n}\nEXPORT_SYMBOL(udpv6_encap_enable);\n\nint udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (skb->len > sizeof(struct udphdr) && encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_error;\n\t}\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t  UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic bool __udp_v6_is_mcast_sock(struct net *net, struct sock *sk,\n\t\t\t\t   __be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t\t   __be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\t\t   int dif, unsigned short hnum)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net))\n\t\treturn false;\n\n\tif (udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6 ||\n\t    (inet->inet_dport && inet->inet_dport != rmt_port) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr)) ||\n\t    (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_rcv_saddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr)))\n\t\treturn false;\n\tif (!inet6_mc_check(sk, loc_addr, rmt_addr))\n\t\treturn false;\n\treturn true;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\tunsigned int i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(!skb1))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t  IS_UDPLITE(sk));\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t  IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udpv6_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\t\tsock_put(sk);\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\nstatic void udp6_csum_zero_error(struct sk_buff *skb)\n{\n\t/* RFC 2460 section 8.1 says that we SHOULD log\n\t * this error. Well, it is reasonable.\n\t */\n\tnet_dbg_ratelimited(\"IPv6: udp checksum is 0 for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t    &ipv6_hdr(skb)->saddr, ntohs(udp_hdr(skb)->source),\n\t\t\t    &ipv6_hdr(skb)->daddr, ntohs(udp_hdr(skb)->dest));\n}\n\n/*\n * Note: called only from the BH handler context,\n * so we don't need to lock the hashes.\n */\nstatic int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\tstruct udp_table *udptable, int proto)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tconst struct udphdr *uh = udp_hdr(skb);\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tint dif = inet6_iif(skb);\n\tunsigned int count = 0, offset = offsetof(typeof(*sk), sk_nulls_node);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tbool inner_flushed = false;\n\n\tif (use_hash2) {\n\t\thash2_any = udp6_portaddr_hash(net, &in6addr_any, hnum) &\n\t\t\t    udp_table.mask;\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum) & udp_table.mask;\nstart_lookup:\n\t\thslot = &udp_table.hash2[hash2];\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tspin_lock(&hslot->lock);\n\tsk_nulls_for_each_entry_offset(sk, node, &hslot->head, offset) {\n\t\tif (__udp_v6_is_mcast_sock(net, sk,\n\t\t\t\t\t   uh->dest, daddr,\n\t\t\t\t\t   uh->source, saddr,\n\t\t\t\t\t   dif, hnum) &&\n\t\t    /* If zero checksum and no_check is not on for\n\t\t     * the socket then skip it.\n\t\t     */\n\t\t    (uh->check || udp_sk(sk)->no_check6_rx)) {\n\t\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\t\tinner_flushed = true;\n\t\t\t\tcount = 0;\n\t\t\t}\n\t\t\tstack[count++] = sk;\n\t\t\tsock_hold(sk);\n\t\t}\n\t}\n\n\tspin_unlock(&hslot->lock);\n\n\t/* Also lookup *:port if we are using hash2 and haven't done so yet. */\n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\t} else {\n\t\tif (!inner_flushed)\n\t\t\tUDP_INC_STATS_BH(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\t\t proto == IPPROTO_UDPLITE);\n\t\tconsume_skb(skb);\n\t}\n\treturn 0;\n}\n\nint __udp6_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tconst struct in6_addr *saddr, *daddr;\n\tu32 ulen = 0;\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto discard;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = &ipv6_hdr(skb)->daddr;\n\tuh = udp_hdr(skb);\n\n\tulen = ntohs(uh->len);\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\n\t\t/* Check for jumbo payload */\n\t\tif (ulen == 0)\n\t\t\tulen = skb->len;\n\n\t\tif (ulen < sizeof(*uh))\n\t\t\tgoto short_packet;\n\n\t\tif (ulen < skb->len) {\n\t\t\tif (pskb_trim_rcsum(skb, ulen))\n\t\t\t\tgoto short_packet;\n\t\t\tsaddr = &ipv6_hdr(skb)->saddr;\n\t\t\tdaddr = &ipv6_hdr(skb)->daddr;\n\t\t\tuh = udp_hdr(skb);\n\t\t}\n\t}\n\n\tif (udp6_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\t/*\n\t *\tMulticast receive code\n\t */\n\tif (ipv6_addr_is_multicast(daddr))\n\t\treturn __udp6_lib_mcast_deliver(net, skb,\n\t\t\t\tsaddr, daddr, udptable, proto);\n\n\t/* Unicast */\n\n\t/*\n\t * check socket cache ... must talk to Alan about his plans\n\t * for sock caches... i'll skip this for now.\n\t */\n\tsk = __udp6_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk) {\n\t\tint ret;\n\n\t\tif (!uh->check && !udp_sk(sk)->no_check6_rx) {\n\t\t\tsock_put(sk);\n\t\t\tudp6_csum_zero_error(skb);\n\t\t\tgoto csum_error;\n\t\t}\n\n\t\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t ip6_compute_pseudo);\n\n\t\tret = udpv6_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\n\t\treturn 0;\n\t}\n\n\tif (!uh->check) {\n\t\tudp6_csum_zero_error(skb);\n\t\tgoto csum_error;\n\t}\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard;\n\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP6_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);\n\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tnet_dbg_ratelimited(\"UDP%sv6: short packet: From [%pI6c]:%u %d/%d to [%pI6c]:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t\t    saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    daddr, ntohs(uh->dest));\n\tgoto discard;\ncsum_error:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndiscard:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic __inline__ int udpv6_rcv(struct sk_buff *skb)\n{\n\treturn __udp6_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nstatic void udp_v6_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending == AF_INET)\n\t\tudp_flush_pending_frames(sk);\n\telse if (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip6_flush_pending_frames(sk);\n\t}\n}\n\n/**\n *\tudp6_hwcsum_outgoing  -  handle outgoing HW checksumming\n *\t@sk:\tsocket we are sending on\n *\t@skb:\tsk_buff containing the filled-in UDP header\n *\t\t(checksum field must be zeroed out)\n */\nstatic void udp6_hwcsum_outgoing(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const struct in6_addr *saddr,\n\t\t\t\t const struct in6_addr *daddr, int len)\n{\n\tunsigned int offset;\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *frags = skb_shinfo(skb)->frag_list;\n\t__wsum csum = 0;\n\n\tif (!frags) {\n\t\t/* Only one fragment on the socket.  */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\toffset = skb_transport_offset(skb);\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tdo {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t} while ((frags = frags->next));\n\n\t\tuh->check = csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP,\n\t\t\t\t\t    csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\n/*\n *\tSending\n */\n\nstatic int udp_v6_send_skb(struct sk_buff *skb, struct flowi6 *fl6)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\t__wsum csum = 0;\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = fl6->fl6_sport;\n\tuh->dest = fl6->fl6_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)\n\t\tcsum = udplite_csum(skb);\n\telse if (udp_sk(sk)->no_check6_tx) {   /* UDP csum disabled */\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\t\tudp6_hwcsum_outgoing(sk, skb, &fl6->saddr, &fl6->daddr, len);\n\t\tgoto send;\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t\t    len, fl6->flowi6_proto, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip6_send_skb(skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet6_sk(sk)->recverr) {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t    UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t    UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\nstatic int udp_v6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct flowi6 fl6;\n\tint err = 0;\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_push_pending_frames(sk);\n\n\t/* ip6_finish_skb will release the cork, so make a copy of\n\t * fl6 here.\n\t */\n\tfl6 = inet_sk(sk)->cork.fl.u.ip6;\n\n\tskb = ip6_finish_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_v6_send_skb(skb, &fl6);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n\nvoid udpv6_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tlock_sock(sk);\n\tudp_v6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\tencap_destroy = ACCESS_ONCE(up->encap_destroy);\n\t\tif (encap_destroy)\n\t\t\tencap_destroy(sk);\n\t}\n\n\tinet6_destroy_sock(sk);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn compat_ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic const struct inet6_protocol udpv6_protocol = {\n\t.handler\t=\tudpv6_rcv,\n\t.err_handler\t=\tudpv6_err,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\nint udp6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tint bucket = ((struct udp_iter_state *)seq->private)->bucket;\n\t\tstruct inet_sock *inet = inet_sk(v);\n\t\t__u16 srcp = ntohs(inet->inet_sport);\n\t\t__u16 destp = ntohs(inet->inet_dport);\n\t\tip6_dgram_sock_seq_show(seq, v, srcp, destp, bucket);\n\t}\n\treturn 0;\n}\n\nstatic const struct file_operations udp6_afinfo_seq_fops = {\n\t.owner    = THIS_MODULE,\n\t.open     = udp_seq_open,\n\t.read     = seq_read,\n\t.llseek   = seq_lseek,\n\t.release  = seq_release_net\n};\n\nstatic struct udp_seq_afinfo udp6_seq_afinfo = {\n\t.name\t\t= \"udp6\",\n\t.family\t\t= AF_INET6,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= &udp6_afinfo_seq_fops,\n\t.seq_ops\t= {\n\t\t.show\t\t= udp6_seq_show,\n\t},\n};\n\nint __net_init udp6_proc_init(struct net *net)\n{\n\treturn udp_proc_register(net, &udp6_seq_afinfo);\n}\n\nvoid udp6_proc_exit(struct net *net)\n{\n\tudp_proc_unregister(net, &udp6_seq_afinfo);\n}\n#endif /* CONFIG_PROC_FS */\n\nvoid udp_v6_clear_sk(struct sock *sk, int size)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t/* we do not want to clear pinet6 field, because of RCU lookups */\n\tsk_prot_clear_portaddr_nulls(sk, offsetof(struct inet_sock, pinet6));\n\n\tsize -= offsetof(struct inet_sock, pinet6) + sizeof(inet->pinet6);\n\tmemset(&inet->pinet6 + 1, 0, size);\n}\n\n/* ------------------------------------------------------------------------ */\n\nstruct proto udpv6_prot = {\n\t.name\t\t   = \"UDPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip6_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udpv6_destroy_sock,\n\t.setsockopt\t   = udpv6_setsockopt,\n\t.getsockopt\t   = udpv6_getsockopt,\n\t.sendmsg\t   = udpv6_sendmsg,\n\t.recvmsg\t   = udpv6_recvmsg,\n\t.backlog_rcv\t   = __udpv6_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v6_rehash,\n\t.get_port\t   = udp_v6_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udpv6_setsockopt,\n\t.compat_getsockopt = compat_udpv6_getsockopt,\n#endif\n\t.clear_sk\t   = udp_v6_clear_sk,\n};\n\nstatic struct inet_protosw udpv6_protosw = {\n\t.type =      SOCK_DGRAM,\n\t.protocol =  IPPROTO_UDP,\n\t.prot =      &udpv6_prot,\n\t.ops =       &inet6_dgram_ops,\n\t.flags =     INET_PROTOSW_PERMANENT,\n};\n\nint __init udpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_register_protosw(&udpv6_protosw);\n\tif (ret)\n\t\tgoto out_udpv6_protocol;\nout:\n\treturn ret;\n\nout_udpv6_protocol:\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tgoto out;\n}\n\nvoid udpv6_exit(void)\n{\n\tinet6_unregister_protosw(&udpv6_protosw);\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n}\n"], "filenames": ["net/ipv4/udp.c", "net/ipv6/udp.c"], "buggy_code_start_loc": [1273, 404], "buggy_code_end_loc": [1304, 441], "fixing_code_start_loc": [1274, 405], "fixing_code_end_loc": [1306, 443], "type": "CWE-358", "message": "udp.c in the Linux kernel before 4.5 allows remote attackers to execute arbitrary code via UDP traffic that triggers an unsafe second checksum calculation during execution of a recv system call with the MSG_PEEK flag.", "other": {"cve": {"id": "CVE-2016-10229", "sourceIdentifier": "security@android.com", "published": "2017-04-04T05:59:00.233", "lastModified": "2022-11-03T20:19:56.707", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "udp.c in the Linux kernel before 4.5 allows remote attackers to execute arbitrary code via UDP traffic that triggers an unsafe second checksum calculation during execution of a recv system call with the MSG_PEEK flag."}, {"lang": "es", "value": "Udp.c en el kernel de Linux en versiones anteriores a 4.5 permite a los atacantes remotos ejecutar c\u00f3digo arbitrario a trav\u00e9s del tr\u00e1fico UDP que dispara un segundo c\u00e1lculo de checksum inseguro durante la ejecuci\u00f3n de una llamada al sistema recv con el indicador MSG_PEEK."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 10.0}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-358"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.2", "versionEndExcluding": "3.2.76", "matchCriteriaId": "D4B255C1-43F1-42C9-B4FD-0580136EE48F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.4.113", "matchCriteriaId": "9A93F019-B0C0-4723-869E-C715F15E11C9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.5", "versionEndExcluding": "3.10.103", "matchCriteriaId": "B41219F0-BE17-4FE0-98B0-D250A76244A0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.53", "matchCriteriaId": "F7AF3AA1-D6B8-4473-A781-740F7AC7A81F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.14.77", "matchCriteriaId": "C18F08AC-3F58-4983-8D04-DD97A1111A96"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.15", "versionEndExcluding": "3.16.35", "matchCriteriaId": "7DC4BA70-B111-4D2E-BC78-6601CED68F08"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.45", "matchCriteriaId": "44D241BB-782D-4402-95BD-85D44EDABC1D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.40", "matchCriteriaId": "70E79011-B658-40F2-B406-D3619DEBFACD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.21", "matchCriteriaId": "4A46BB08-BDDE-4A5A-BF6A-FC422CB19757"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:google:android:*:*:*:*:*:*:*:*", "versionEndIncluding": "7.1.1", "matchCriteriaId": "0F11609D-D1B4-4DD6-8CC7-A224344E1E67"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=197c949e7798fbf28cfadc69d9ca0c2abbf93191", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://source.android.com/security/bulletin/2017-04-01.html", "source": "security@android.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/97397", "source": "security@android.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1038201", "source": "security@android.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/197c949e7798fbf28cfadc69d9ca0c2abbf93191", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://security.paloaltonetworks.com/CVE-2016-10229", "source": "security@android.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/197c949e7798fbf28cfadc69d9ca0c2abbf93191"}}