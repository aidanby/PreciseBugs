{"buggy_code": ["/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2001, 2004\n * Copyright (c) 1999-2000 Cisco, Inc.\n * Copyright (c) 1999-2001 Motorola, Inc.\n * Copyright (c) 2001-2003 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * The base lksctp header.\n *\n * This SCTP implementation is free software;\n * you can redistribute it and/or modify it under the terms of\n * the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n * This SCTP implementation is distributed in the hope that it\n * will be useful, but WITHOUT ANY WARRANTY; without even the implied\n *                 ************************\n * warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n * See the GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with GNU CC; see the file COPYING.  If not, see\n * <http://www.gnu.org/licenses/>.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    La Monte H.P. Yarroll <piggy@acm.org>\n *    Xingang Guo           <xingang.guo@intel.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Daisy Chang           <daisyc@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Kevin Gao             <kevin.gao@intel.com> \n */\n\n#ifndef __net_sctp_h__\n#define __net_sctp_h__\n\n/* Header Strategy.\n *    Start getting some control over the header file depencies:\n *       includes\n *       constants\n *       structs\n *       prototypes\n *       macros, externs, and inlines\n *\n *   Move test_frame specific items out of the kernel headers\n *   and into the test frame headers.   This is not perfect in any sense\n *   and will continue to evolve.\n */\n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/in.h>\n#include <linux/tty.h>\n#include <linux/proc_fs.h>\n#include <linux/spinlock.h>\n#include <linux/jiffies.h>\n#include <linux/idr.h>\n\n#if IS_ENABLED(CONFIG_IPV6)\n#include <net/ipv6.h>\n#include <net/ip6_route.h>\n#endif\n\n#include <asm/uaccess.h>\n#include <asm/page.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n#include <net/sctp/structs.h>\n#include <net/sctp/constants.h>\n\n#ifdef CONFIG_IP_SCTP_MODULE\n#define SCTP_PROTOSW_FLAG 0\n#else /* static! */\n#define SCTP_PROTOSW_FLAG INET_PROTOSW_PERMANENT\n#endif\n\n/*\n * Function declarations.\n */\n\n/*\n * sctp/protocol.c\n */\nint sctp_copy_local_addr_list(struct net *, struct sctp_bind_addr *,\n\t\t\t      sctp_scope_t, gfp_t gfp, int flags);\nstruct sctp_pf *sctp_get_pf_specific(sa_family_t family);\nint sctp_register_pf(struct sctp_pf *, sa_family_t);\nvoid sctp_addr_wq_mgmt(struct net *, struct sctp_sockaddr_entry *, int);\n\n/*\n * sctp/socket.c\n */\nint sctp_backlog_rcv(struct sock *sk, struct sk_buff *skb);\nint sctp_inet_listen(struct socket *sock, int backlog);\nvoid sctp_write_space(struct sock *sk);\nvoid sctp_data_ready(struct sock *sk);\nunsigned int sctp_poll(struct file *file, struct socket *sock,\n\t\tpoll_table *wait);\nvoid sctp_sock_rfree(struct sk_buff *skb);\nvoid sctp_copy_sock(struct sock *newsk, struct sock *sk,\n\t\t    struct sctp_association *asoc);\nextern struct percpu_counter sctp_sockets_allocated;\nint sctp_asconf_mgmt(struct sctp_sock *, struct sctp_sockaddr_entry *);\nstruct sk_buff *sctp_skb_recv_datagram(struct sock *, int, int, int *);\n\n/*\n * sctp/primitive.c\n */\nint sctp_primitive_ASSOCIATE(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_SHUTDOWN(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_ABORT(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_SEND(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_REQUESTHEARTBEAT(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_ASCONF(struct net *, struct sctp_association *, void *arg);\n\n/*\n * sctp/input.c\n */\nint sctp_rcv(struct sk_buff *skb);\nvoid sctp_v4_err(struct sk_buff *skb, u32 info);\nvoid sctp_hash_established(struct sctp_association *);\nvoid sctp_unhash_established(struct sctp_association *);\nvoid sctp_hash_endpoint(struct sctp_endpoint *);\nvoid sctp_unhash_endpoint(struct sctp_endpoint *);\nstruct sock *sctp_err_lookup(struct net *net, int family, struct sk_buff *,\n\t\t\t     struct sctphdr *, struct sctp_association **,\n\t\t\t     struct sctp_transport **);\nvoid sctp_err_finish(struct sock *, struct sctp_association *);\nvoid sctp_icmp_frag_needed(struct sock *, struct sctp_association *,\n\t\t\t   struct sctp_transport *t, __u32 pmtu);\nvoid sctp_icmp_redirect(struct sock *, struct sctp_transport *,\n\t\t\tstruct sk_buff *);\nvoid sctp_icmp_proto_unreachable(struct sock *sk,\n\t\t\t\t struct sctp_association *asoc,\n\t\t\t\t struct sctp_transport *t);\nvoid sctp_backlog_migrate(struct sctp_association *assoc,\n\t\t\t  struct sock *oldsk, struct sock *newsk);\n\n/*\n * sctp/proc.c\n */\nint sctp_snmp_proc_init(struct net *net);\nvoid sctp_snmp_proc_exit(struct net *net);\nint sctp_eps_proc_init(struct net *net);\nvoid sctp_eps_proc_exit(struct net *net);\nint sctp_assocs_proc_init(struct net *net);\nvoid sctp_assocs_proc_exit(struct net *net);\nint sctp_remaddr_proc_init(struct net *net);\nvoid sctp_remaddr_proc_exit(struct net *net);\n\n\n/*\n * Module global variables\n */\n\n /*\n  * sctp/protocol.c\n  */\nextern struct kmem_cache *sctp_chunk_cachep __read_mostly;\nextern struct kmem_cache *sctp_bucket_cachep __read_mostly;\n\n/*\n *  Section:  Macros, externs, and inlines\n */\n\n/* SCTP SNMP MIB stats handlers */\n#define SCTP_INC_STATS(net, field)      SNMP_INC_STATS((net)->sctp.sctp_statistics, field)\n#define SCTP_INC_STATS_BH(net, field)   SNMP_INC_STATS_BH((net)->sctp.sctp_statistics, field)\n#define SCTP_INC_STATS_USER(net, field) SNMP_INC_STATS_USER((net)->sctp.sctp_statistics, field)\n#define SCTP_DEC_STATS(net, field)      SNMP_DEC_STATS((net)->sctp.sctp_statistics, field)\n\n/* sctp mib definitions */\nenum {\n\tSCTP_MIB_NUM = 0,\n\tSCTP_MIB_CURRESTAB,\t\t\t/* CurrEstab */\n\tSCTP_MIB_ACTIVEESTABS,\t\t\t/* ActiveEstabs */\n\tSCTP_MIB_PASSIVEESTABS,\t\t\t/* PassiveEstabs */\n\tSCTP_MIB_ABORTEDS,\t\t\t/* Aborteds */\n\tSCTP_MIB_SHUTDOWNS,\t\t\t/* Shutdowns */\n\tSCTP_MIB_OUTOFBLUES,\t\t\t/* OutOfBlues */\n\tSCTP_MIB_CHECKSUMERRORS,\t\t/* ChecksumErrors */\n\tSCTP_MIB_OUTCTRLCHUNKS,\t\t\t/* OutCtrlChunks */\n\tSCTP_MIB_OUTORDERCHUNKS,\t\t/* OutOrderChunks */\n\tSCTP_MIB_OUTUNORDERCHUNKS,\t\t/* OutUnorderChunks */\n\tSCTP_MIB_INCTRLCHUNKS,\t\t\t/* InCtrlChunks */\n\tSCTP_MIB_INORDERCHUNKS,\t\t\t/* InOrderChunks */\n\tSCTP_MIB_INUNORDERCHUNKS,\t\t/* InUnorderChunks */\n\tSCTP_MIB_FRAGUSRMSGS,\t\t\t/* FragUsrMsgs */\n\tSCTP_MIB_REASMUSRMSGS,\t\t\t/* ReasmUsrMsgs */\n\tSCTP_MIB_OUTSCTPPACKS,\t\t\t/* OutSCTPPacks */\n\tSCTP_MIB_INSCTPPACKS,\t\t\t/* InSCTPPacks */\n\tSCTP_MIB_T1_INIT_EXPIREDS,\n\tSCTP_MIB_T1_COOKIE_EXPIREDS,\n\tSCTP_MIB_T2_SHUTDOWN_EXPIREDS,\n\tSCTP_MIB_T3_RTX_EXPIREDS,\n\tSCTP_MIB_T4_RTO_EXPIREDS,\n\tSCTP_MIB_T5_SHUTDOWN_GUARD_EXPIREDS,\n\tSCTP_MIB_DELAY_SACK_EXPIREDS,\n\tSCTP_MIB_AUTOCLOSE_EXPIREDS,\n\tSCTP_MIB_T1_RETRANSMITS,\n\tSCTP_MIB_T3_RETRANSMITS,\n\tSCTP_MIB_PMTUD_RETRANSMITS,\n\tSCTP_MIB_FAST_RETRANSMITS,\n\tSCTP_MIB_IN_PKT_SOFTIRQ,\n\tSCTP_MIB_IN_PKT_BACKLOG,\n\tSCTP_MIB_IN_PKT_DISCARDS,\n\tSCTP_MIB_IN_DATA_CHUNK_DISCARDS,\n\t__SCTP_MIB_MAX\n};\n\n#define SCTP_MIB_MAX    __SCTP_MIB_MAX\nstruct sctp_mib {\n        unsigned long   mibs[SCTP_MIB_MAX];\n};\n\n/* helper function to track stats about max rto and related transport */\nstatic inline void sctp_max_rto(struct sctp_association *asoc,\n\t\t\t\tstruct sctp_transport *trans)\n{\n\tif (asoc->stats.max_obs_rto < (__u64)trans->rto) {\n\t\tasoc->stats.max_obs_rto = trans->rto;\n\t\tmemset(&asoc->stats.obs_rto_ipaddr, 0,\n\t\t\tsizeof(struct sockaddr_storage));\n\t\tmemcpy(&asoc->stats.obs_rto_ipaddr, &trans->ipaddr,\n\t\t\ttrans->af_specific->sockaddr_len);\n\t}\n}\n\n/*\n * Macros for keeping a global reference of object allocations.\n */\n#ifdef CONFIG_SCTP_DBG_OBJCNT\n\nextern atomic_t sctp_dbg_objcnt_sock;\nextern atomic_t sctp_dbg_objcnt_ep;\nextern atomic_t sctp_dbg_objcnt_assoc;\nextern atomic_t sctp_dbg_objcnt_transport;\nextern atomic_t sctp_dbg_objcnt_chunk;\nextern atomic_t sctp_dbg_objcnt_bind_addr;\nextern atomic_t sctp_dbg_objcnt_bind_bucket;\nextern atomic_t sctp_dbg_objcnt_addr;\nextern atomic_t sctp_dbg_objcnt_ssnmap;\nextern atomic_t sctp_dbg_objcnt_datamsg;\nextern atomic_t sctp_dbg_objcnt_keys;\n\n/* Macros to atomically increment/decrement objcnt counters.  */\n#define SCTP_DBG_OBJCNT_INC(name) \\\natomic_inc(&sctp_dbg_objcnt_## name)\n#define SCTP_DBG_OBJCNT_DEC(name) \\\natomic_dec(&sctp_dbg_objcnt_## name)\n#define SCTP_DBG_OBJCNT(name) \\\natomic_t sctp_dbg_objcnt_## name = ATOMIC_INIT(0)\n\n/* Macro to help create new entries in in the global array of\n * objcnt counters.\n */\n#define SCTP_DBG_OBJCNT_ENTRY(name) \\\n{.label= #name, .counter= &sctp_dbg_objcnt_## name}\n\nvoid sctp_dbg_objcnt_init(struct net *);\nvoid sctp_dbg_objcnt_exit(struct net *);\n\n#else\n\n#define SCTP_DBG_OBJCNT_INC(name)\n#define SCTP_DBG_OBJCNT_DEC(name)\n\nstatic inline void sctp_dbg_objcnt_init(struct net *net) { return; }\nstatic inline void sctp_dbg_objcnt_exit(struct net *net) { return; }\n\n#endif /* CONFIG_SCTP_DBG_OBJCOUNT */\n\n#if defined CONFIG_SYSCTL\nvoid sctp_sysctl_register(void);\nvoid sctp_sysctl_unregister(void);\nint sctp_sysctl_net_register(struct net *net);\nvoid sctp_sysctl_net_unregister(struct net *net);\n#else\nstatic inline void sctp_sysctl_register(void) { return; }\nstatic inline void sctp_sysctl_unregister(void) { return; }\nstatic inline int sctp_sysctl_net_register(struct net *net) { return 0; }\nstatic inline void sctp_sysctl_net_unregister(struct net *net) { return; }\n#endif\n\n/* Size of Supported Address Parameter for 'x' address types. */\n#define SCTP_SAT_LEN(x) (sizeof(struct sctp_paramhdr) + (x) * sizeof(__u16))\n\n#if IS_ENABLED(CONFIG_IPV6)\n\nvoid sctp_v6_pf_init(void);\nvoid sctp_v6_pf_exit(void);\nint sctp_v6_protosw_init(void);\nvoid sctp_v6_protosw_exit(void);\nint sctp_v6_add_protocol(void);\nvoid sctp_v6_del_protocol(void);\n\n#else /* #ifdef defined(CONFIG_IPV6) */\n\nstatic inline void sctp_v6_pf_init(void) { return; }\nstatic inline void sctp_v6_pf_exit(void) { return; }\nstatic inline int sctp_v6_protosw_init(void) { return 0; }\nstatic inline void sctp_v6_protosw_exit(void) { return; }\nstatic inline int sctp_v6_add_protocol(void) { return 0; }\nstatic inline void sctp_v6_del_protocol(void) { return; }\n\n#endif /* #if defined(CONFIG_IPV6) */\n\n\n/* Map an association to an assoc_id. */\nstatic inline sctp_assoc_t sctp_assoc2id(const struct sctp_association *asoc)\n{\n\treturn asoc ? asoc->assoc_id : 0;\n}\n\nstatic inline enum sctp_sstat_state\nsctp_assoc_to_state(const struct sctp_association *asoc)\n{\n\t/* SCTP's uapi always had SCTP_EMPTY(=0) as a dummy state, but we\n\t * got rid of it in kernel space. Therefore SCTP_CLOSED et al\n\t * start at =1 in user space, but actually as =0 in kernel space.\n\t * Now that we can not break user space and SCTP_EMPTY is exposed\n\t * there, we need to fix it up with an ugly offset not to break\n\t * applications. :(\n\t */\n\treturn asoc->state + 1;\n}\n\n/* Look up the association by its id.  */\nstruct sctp_association *sctp_id2assoc(struct sock *sk, sctp_assoc_t id);\n\nint sctp_do_peeloff(struct sock *sk, sctp_assoc_t id, struct socket **sockp);\n\n/* A macro to walk a list of skbs.  */\n#define sctp_skb_for_each(pos, head, tmp) \\\n\tskb_queue_walk_safe(head, pos, tmp)\n\n/* A helper to append an entire skb list (list) to another (head). */\nstatic inline void sctp_skb_list_tail(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff_head *head)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&head->lock, flags);\n\tspin_lock(&list->lock);\n\n\tskb_queue_splice_tail_init(list, head);\n\n\tspin_unlock(&list->lock);\n\tspin_unlock_irqrestore(&head->lock, flags);\n}\n\n/**\n *\tsctp_list_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstatic inline struct list_head *sctp_list_dequeue(struct list_head *list)\n{\n\tstruct list_head *result = NULL;\n\n\tif (list->next != list) {\n\t\tresult = list->next;\n\t\tlist->next = result->next;\n\t\tlist->next->prev = list;\n\t\tINIT_LIST_HEAD(result);\n\t}\n\treturn result;\n}\n\n/* SCTP version of skb_set_owner_r.  We need this one because\n * of the way we have to do receive buffer accounting on bundled\n * chunks.\n */\nstatic inline void sctp_skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\n{\n\tstruct sctp_ulpevent *event = sctp_skb2event(skb);\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sctp_sock_rfree;\n\tatomic_add(event->rmem_len, &sk->sk_rmem_alloc);\n\t/*\n\t * This mimics the behavior of skb_set_owner_r\n\t */\n\tsk->sk_forward_alloc -= event->rmem_len;\n}\n\n/* Tests if the list has one and only one entry. */\nstatic inline int sctp_list_single_entry(struct list_head *head)\n{\n\treturn (head->next != head) && (head->next == head->prev);\n}\n\n/* Break down data chunks at this point.  */\nstatic inline int sctp_frag_point(const struct sctp_association *asoc, int pmtu)\n{\n\tstruct sctp_sock *sp = sctp_sk(asoc->base.sk);\n\tint frag = pmtu;\n\n\tfrag -= sp->pf->af->net_header_len;\n\tfrag -= sizeof(struct sctphdr) + sizeof(struct sctp_data_chunk);\n\n\tif (asoc->user_frag)\n\t\tfrag = min_t(int, frag, asoc->user_frag);\n\n\tfrag = min_t(int, frag, SCTP_MAX_CHUNK_LEN);\n\n\treturn frag;\n}\n\nstatic inline void sctp_assoc_pending_pmtu(struct sock *sk, struct sctp_association *asoc)\n{\n\n\tsctp_assoc_sync_pmtu(sk, asoc);\n\tasoc->pmtu_pending = 0;\n}\n\n/* Walk through a list of TLV parameters.  Don't trust the\n * individual parameter lengths and instead depend on\n * the chunk length to indicate when to stop.  Make sure\n * there is room for a param header too.\n */\n#define sctp_walk_params(pos, chunk, member)\\\n_sctp_walk_params((pos), (chunk), ntohs((chunk)->chunk_hdr.length), member)\n\n#define _sctp_walk_params(pos, chunk, end, member)\\\nfor (pos.v = chunk->member;\\\n     pos.v <= (void *)chunk + end - ntohs(pos.p->length) &&\\\n     ntohs(pos.p->length) >= sizeof(sctp_paramhdr_t);\\\n     pos.v += WORD_ROUND(ntohs(pos.p->length)))\n\n#define sctp_walk_errors(err, chunk_hdr)\\\n_sctp_walk_errors((err), (chunk_hdr), ntohs((chunk_hdr)->length))\n\n#define _sctp_walk_errors(err, chunk_hdr, end)\\\nfor (err = (sctp_errhdr_t *)((void *)chunk_hdr + \\\n\t    sizeof(sctp_chunkhdr_t));\\\n     (void *)err <= (void *)chunk_hdr + end - ntohs(err->length) &&\\\n     ntohs(err->length) >= sizeof(sctp_errhdr_t); \\\n     err = (sctp_errhdr_t *)((void *)err + WORD_ROUND(ntohs(err->length))))\n\n#define sctp_walk_fwdtsn(pos, chunk)\\\n_sctp_walk_fwdtsn((pos), (chunk), ntohs((chunk)->chunk_hdr->length) - sizeof(struct sctp_fwdtsn_chunk))\n\n#define _sctp_walk_fwdtsn(pos, chunk, end)\\\nfor (pos = chunk->subh.fwdtsn_hdr->skip;\\\n     (void *)pos <= (void *)chunk->subh.fwdtsn_hdr->skip + end - sizeof(struct sctp_fwdtsn_skip);\\\n     pos++)\n\n/* Round an int up to the next multiple of 4.  */\n#define WORD_ROUND(s) (((s)+3)&~3)\n\n/* External references. */\n\nextern struct proto sctp_prot;\nextern struct proto sctpv6_prot;\nvoid sctp_put_port(struct sock *sk);\n\nextern struct idr sctp_assocs_id;\nextern spinlock_t sctp_assocs_id_lock;\n\n/* Static inline functions. */\n\n/* Convert from an IP version number to an Address Family symbol.  */\nstatic inline int ipver2af(__u8 ipver)\n{\n\tswitch (ipver) {\n\tcase 4:\n\t        return  AF_INET;\n\tcase 6:\n\t\treturn AF_INET6;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n/* Convert from an address parameter type to an address family.  */\nstatic inline int param_type2af(__be16 type)\n{\n\tswitch (type) {\n\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t        return  AF_INET;\n\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\treturn AF_INET6;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n/* Warning: The following hash functions assume a power of two 'size'. */\n/* This is the hash function for the SCTP port hash table. */\nstatic inline int sctp_phashfn(struct net *net, __u16 lport)\n{\n\treturn (net_hash_mix(net) + lport) & (sctp_port_hashsize - 1);\n}\n\n/* This is the hash function for the endpoint hash table. */\nstatic inline int sctp_ep_hashfn(struct net *net, __u16 lport)\n{\n\treturn (net_hash_mix(net) + lport) & (sctp_ep_hashsize - 1);\n}\n\n/* This is the hash function for the association hash table. */\nstatic inline int sctp_assoc_hashfn(struct net *net, __u16 lport, __u16 rport)\n{\n\tint h = (lport << 16) + rport + net_hash_mix(net);\n\th ^= h>>8;\n\treturn h & (sctp_assoc_hashsize - 1);\n}\n\n/* This is the hash function for the association hash table.  This is\n * not used yet, but could be used as a better hash function when\n * we have a vtag.\n */\nstatic inline int sctp_vtag_hashfn(__u16 lport, __u16 rport, __u32 vtag)\n{\n\tint h = (lport << 16) + rport;\n\th ^= vtag;\n\treturn h & (sctp_assoc_hashsize - 1);\n}\n\n#define sctp_for_each_hentry(epb, head) \\\n\thlist_for_each_entry(epb, head, node)\n\n/* Is a socket of this style? */\n#define sctp_style(sk, style) __sctp_style((sk), (SCTP_SOCKET_##style))\nstatic inline int __sctp_style(const struct sock *sk, sctp_socket_type_t style)\n{\n\treturn sctp_sk(sk)->type == style;\n}\n\n/* Is the association in this state? */\n#define sctp_state(asoc, state) __sctp_state((asoc), (SCTP_STATE_##state))\nstatic inline int __sctp_state(const struct sctp_association *asoc,\n\t\t\t       sctp_state_t state)\n{\n\treturn asoc->state == state;\n}\n\n/* Is the socket in this state? */\n#define sctp_sstate(sk, state) __sctp_sstate((sk), (SCTP_SS_##state))\nstatic inline int __sctp_sstate(const struct sock *sk, sctp_sock_state_t state)\n{\n\treturn sk->sk_state == state;\n}\n\n/* Map v4-mapped v6 address back to v4 address */\nstatic inline void sctp_v6_map_v4(union sctp_addr *addr)\n{\n\taddr->v4.sin_family = AF_INET;\n\taddr->v4.sin_port = addr->v6.sin6_port;\n\taddr->v4.sin_addr.s_addr = addr->v6.sin6_addr.s6_addr32[3];\n}\n\n/* Map v4 address to v4-mapped v6 address */\nstatic inline void sctp_v4_map_v6(union sctp_addr *addr)\n{\n\taddr->v6.sin6_family = AF_INET6;\n\taddr->v6.sin6_flowinfo = 0;\n\taddr->v6.sin6_scope_id = 0;\n\taddr->v6.sin6_port = addr->v4.sin_port;\n\taddr->v6.sin6_addr.s6_addr32[3] = addr->v4.sin_addr.s_addr;\n\taddr->v6.sin6_addr.s6_addr32[0] = 0;\n\taddr->v6.sin6_addr.s6_addr32[1] = 0;\n\taddr->v6.sin6_addr.s6_addr32[2] = htonl(0x0000ffff);\n}\n\n/* The cookie is always 0 since this is how it's used in the\n * pmtu code.\n */\nstatic inline struct dst_entry *sctp_transport_dst_check(struct sctp_transport *t)\n{\n\tif (t->dst && !dst_check(t->dst, t->dst_cookie)) {\n\t\tdst_release(t->dst);\n\t\tt->dst = NULL;\n\t}\n\n\treturn t->dst;\n}\n\n#endif /* __net_sctp_h__ */\n", "/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2001, 2004\n * Copyright (c) 1999-2000 Cisco, Inc.\n * Copyright (c) 1999-2001 Motorola, Inc.\n * Copyright (c) 2001 Intel Corp.\n * Copyright (c) 2001 La Monte H.P. Yarroll\n *\n * This file is part of the SCTP kernel implementation\n *\n * This module provides the abstraction for an SCTP association.\n *\n * This SCTP implementation is free software;\n * you can redistribute it and/or modify it under the terms of\n * the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n * This SCTP implementation is distributed in the hope that it\n * will be useful, but WITHOUT ANY WARRANTY; without even the implied\n *                 ************************\n * warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n * See the GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with GNU CC; see the file COPYING.  If not, see\n * <http://www.gnu.org/licenses/>.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    La Monte H.P. Yarroll <piggy@acm.org>\n *    Karl Knutson          <karl@athena.chicago.il.us>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Xingang Guo           <xingang.guo@intel.com>\n *    Hui Huang             <hui.huang@nokia.com>\n *    Sridhar Samudrala\t    <sri@us.ibm.com>\n *    Daisy Chang\t    <daisyc@us.ibm.com>\n *    Ryan Layer\t    <rmlayer@us.ibm.com>\n *    Kevin Gao             <kevin.gao@intel.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n\n#include <linux/slab.h>\n#include <linux/in.h>\n#include <net/ipv6.h>\n#include <net/sctp/sctp.h>\n#include <net/sctp/sm.h>\n\n/* Forward declarations for internal functions. */\nstatic void sctp_select_active_and_retran_path(struct sctp_association *asoc);\nstatic void sctp_assoc_bh_rcv(struct work_struct *work);\nstatic void sctp_assoc_free_asconf_acks(struct sctp_association *asoc);\nstatic void sctp_assoc_free_asconf_queue(struct sctp_association *asoc);\n\n/* 1st Level Abstractions. */\n\n/* Initialize a new association from provided memory. */\nstatic struct sctp_association *sctp_association_init(struct sctp_association *asoc,\n\t\t\t\t\t  const struct sctp_endpoint *ep,\n\t\t\t\t\t  const struct sock *sk,\n\t\t\t\t\t  sctp_scope_t scope,\n\t\t\t\t\t  gfp_t gfp)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\tint i;\n\tsctp_paramhdr_t *p;\n\tint err;\n\n\t/* Retrieve the SCTP per socket area.  */\n\tsp = sctp_sk((struct sock *)sk);\n\n\t/* Discarding const is appropriate here.  */\n\tasoc->ep = (struct sctp_endpoint *)ep;\n\tasoc->base.sk = (struct sock *)sk;\n\n\tsctp_endpoint_hold(asoc->ep);\n\tsock_hold(asoc->base.sk);\n\n\t/* Initialize the common base substructure.  */\n\tasoc->base.type = SCTP_EP_TYPE_ASSOCIATION;\n\n\t/* Initialize the object handling fields.  */\n\tatomic_set(&asoc->base.refcnt, 1);\n\n\t/* Initialize the bind addr area.  */\n\tsctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port);\n\n\tasoc->state = SCTP_STATE_CLOSED;\n\tasoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life);\n\tasoc->user_frag = sp->user_frag;\n\n\t/* Set the association max_retrans and RTO values from the\n\t * socket values.\n\t */\n\tasoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt;\n\tasoc->pf_retrans  = net->sctp.pf_retrans;\n\n\tasoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial);\n\tasoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max);\n\tasoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min);\n\n\t/* Initialize the association's heartbeat interval based on the\n\t * sock configured value.\n\t */\n\tasoc->hbinterval = msecs_to_jiffies(sp->hbinterval);\n\n\t/* Initialize path max retrans value. */\n\tasoc->pathmaxrxt = sp->pathmaxrxt;\n\n\t/* Initialize default path MTU. */\n\tasoc->pathmtu = sp->pathmtu;\n\n\t/* Set association default SACK delay */\n\tasoc->sackdelay = msecs_to_jiffies(sp->sackdelay);\n\tasoc->sackfreq = sp->sackfreq;\n\n\t/* Set the association default flags controlling\n\t * Heartbeat, SACK delay, and Path MTU Discovery.\n\t */\n\tasoc->param_flags = sp->param_flags;\n\n\t/* Initialize the maximum number of new data packets that can be sent\n\t * in a burst.\n\t */\n\tasoc->max_burst = sp->max_burst;\n\n\t/* initialize association timers */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial;\n\n\t/* sctpimpguide Section 2.12.2\n\t * If the 'T5-shutdown-guard' timer is used, it SHOULD be set to the\n\t * recommended value of 5 times 'RTO.Max'.\n\t */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD]\n\t\t= 5 * asoc->rto_max;\n\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;\n\n\t/* Initializes the timers */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i)\n\t\tsetup_timer(&asoc->timers[i], sctp_timer_events[i],\n\t\t\t\t(unsigned long)asoc);\n\n\t/* Pull default initialization values from the sock options.\n\t * Note: This assumes that the values have already been\n\t * validated in the sock.\n\t */\n\tasoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams;\n\tasoc->c.sinit_num_ostreams  = sp->initmsg.sinit_num_ostreams;\n\tasoc->max_init_attempts\t= sp->initmsg.sinit_max_attempts;\n\n\tasoc->max_init_timeo =\n\t\t msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo);\n\n\t/* Set the local window size for receive.\n\t * This is also the rcvbuf space per association.\n\t * RFC 6 - A SCTP receiver MUST be able to receive a minimum of\n\t * 1500 bytes in one SCTP packet.\n\t */\n\tif ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW)\n\t\tasoc->rwnd = SCTP_DEFAULT_MINWINDOW;\n\telse\n\t\tasoc->rwnd = sk->sk_rcvbuf/2;\n\n\tasoc->a_rwnd = asoc->rwnd;\n\n\t/* Use my own max window until I learn something better.  */\n\tasoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW;\n\n\t/* Initialize the receive memory counter */\n\tatomic_set(&asoc->rmem_alloc, 0);\n\n\tinit_waitqueue_head(&asoc->wait);\n\n\tasoc->c.my_vtag = sctp_generate_tag(ep);\n\tasoc->c.my_port = ep->base.bind_addr.port;\n\n\tasoc->c.initial_tsn = sctp_generate_tsn(ep);\n\n\tasoc->next_tsn = asoc->c.initial_tsn;\n\n\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\tasoc->highest_sacked = asoc->ctsn_ack_point;\n\tasoc->last_cwr_tsn = asoc->ctsn_ack_point;\n\n\t/* ADDIP Section 4.1 Asconf Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) a serial number should be assigned to the chunk. The serial\n\t * number SHOULD be a monotonically increasing number. The serial\n\t * numbers SHOULD be initialized at the start of the\n\t * association to the same value as the initial TSN.\n\t */\n\tasoc->addip_serial = asoc->c.initial_tsn;\n\n\tINIT_LIST_HEAD(&asoc->addip_chunk_list);\n\tINIT_LIST_HEAD(&asoc->asconf_ack_list);\n\n\t/* Make an empty list of remote transport addresses.  */\n\tINIT_LIST_HEAD(&asoc->peer.transport_addr_list);\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * After the reception of the first data chunk in an\n\t * association the endpoint must immediately respond with a\n\t * sack to acknowledge the data chunk.  Subsequent\n\t * acknowledgements should be done as described in Section\n\t * 6.2.\n\t *\n\t * [We implement this by telling a new association that it\n\t * already received one packet.]\n\t */\n\tasoc->peer.sack_needed = 1;\n\tasoc->peer.sack_generation = 1;\n\n\t/* Assume that the peer will tell us if he recognizes ASCONF\n\t * as part of INIT exchange.\n\t * The sctp_addip_noauth option is there for backward compatibility\n\t * and will revert old behavior.\n\t */\n\tif (net->sctp.addip_noauth)\n\t\tasoc->peer.asconf_capable = 1;\n\n\t/* Create an input queue.  */\n\tsctp_inq_init(&asoc->base.inqueue);\n\tsctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv);\n\n\t/* Create an output queue.  */\n\tsctp_outq_init(asoc, &asoc->outqueue);\n\n\tif (!sctp_ulpq_init(&asoc->ulpq, asoc))\n\t\tgoto fail_init;\n\n\t/* Assume that peer would support both address types unless we are\n\t * told otherwise.\n\t */\n\tasoc->peer.ipv4_address = 1;\n\tif (asoc->base.sk->sk_family == PF_INET6)\n\t\tasoc->peer.ipv6_address = 1;\n\tINIT_LIST_HEAD(&asoc->asocs);\n\n\tasoc->default_stream = sp->default_stream;\n\tasoc->default_ppid = sp->default_ppid;\n\tasoc->default_flags = sp->default_flags;\n\tasoc->default_context = sp->default_context;\n\tasoc->default_timetolive = sp->default_timetolive;\n\tasoc->default_rcv_context = sp->default_rcv_context;\n\n\t/* AUTH related initializations */\n\tINIT_LIST_HEAD(&asoc->endpoint_shared_keys);\n\terr = sctp_auth_asoc_copy_shkeys(ep, asoc, gfp);\n\tif (err)\n\t\tgoto fail_init;\n\n\tasoc->active_key_id = ep->active_key_id;\n\n\t/* Save the hmacs and chunks list into this association */\n\tif (ep->auth_hmacs_list)\n\t\tmemcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list,\n\t\t\tntohs(ep->auth_hmacs_list->param_hdr.length));\n\tif (ep->auth_chunk_list)\n\t\tmemcpy(asoc->c.auth_chunks, ep->auth_chunk_list,\n\t\t\tntohs(ep->auth_chunk_list->param_hdr.length));\n\n\t/* Get the AUTH random number for this association */\n\tp = (sctp_paramhdr_t *)asoc->c.auth_random;\n\tp->type = SCTP_PARAM_RANDOM;\n\tp->length = htons(sizeof(sctp_paramhdr_t) + SCTP_AUTH_RANDOM_LENGTH);\n\tget_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH);\n\n\treturn asoc;\n\nfail_init:\n\tsock_put(asoc->base.sk);\n\tsctp_endpoint_put(asoc->ep);\n\treturn NULL;\n}\n\n/* Allocate and initialize a new association */\nstruct sctp_association *sctp_association_new(const struct sctp_endpoint *ep,\n\t\t\t\t\t const struct sock *sk,\n\t\t\t\t\t sctp_scope_t scope,\n\t\t\t\t\t gfp_t gfp)\n{\n\tstruct sctp_association *asoc;\n\n\tasoc = kzalloc(sizeof(*asoc), gfp);\n\tif (!asoc)\n\t\tgoto fail;\n\n\tif (!sctp_association_init(asoc, ep, sk, scope, gfp))\n\t\tgoto fail_init;\n\n\tSCTP_DBG_OBJCNT_INC(assoc);\n\n\tpr_debug(\"Created asoc %p\\n\", asoc);\n\n\treturn asoc;\n\nfail_init:\n\tkfree(asoc);\nfail:\n\treturn NULL;\n}\n\n/* Free this association if possible.  There may still be users, so\n * the actual deallocation may be delayed.\n */\nvoid sctp_association_free(struct sctp_association *asoc)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tstruct sctp_transport *transport;\n\tstruct list_head *pos, *temp;\n\tint i;\n\n\t/* Only real associations count against the endpoint, so\n\t * don't bother for if this is a temporary association.\n\t */\n\tif (!list_empty(&asoc->asocs)) {\n\t\tlist_del(&asoc->asocs);\n\n\t\t/* Decrement the backlog value for a TCP-style listening\n\t\t * socket.\n\t\t */\n\t\tif (sctp_style(sk, TCP) && sctp_sstate(sk, LISTENING))\n\t\t\tsk->sk_ack_backlog--;\n\t}\n\n\t/* Mark as dead, so other users can know this structure is\n\t * going away.\n\t */\n\tasoc->base.dead = true;\n\n\t/* Dispose of any data lying around in the outqueue. */\n\tsctp_outq_free(&asoc->outqueue);\n\n\t/* Dispose of any pending messages for the upper layer. */\n\tsctp_ulpq_free(&asoc->ulpq);\n\n\t/* Dispose of any pending chunks on the inqueue. */\n\tsctp_inq_free(&asoc->base.inqueue);\n\n\tsctp_tsnmap_free(&asoc->peer.tsn_map);\n\n\t/* Free ssnmap storage. */\n\tsctp_ssnmap_free(asoc->ssnmap);\n\n\t/* Clean up the bound address list. */\n\tsctp_bind_addr_free(&asoc->base.bind_addr);\n\n\t/* Do we need to go through all of our timers and\n\t * delete them?   To be safe we will try to delete all, but we\n\t * should be able to go through and make a guess based\n\t * on our state.\n\t */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i) {\n\t\tif (del_timer(&asoc->timers[i]))\n\t\t\tsctp_association_put(asoc);\n\t}\n\n\t/* Free peer's cached cookie. */\n\tkfree(asoc->peer.cookie);\n\tkfree(asoc->peer.peer_random);\n\tkfree(asoc->peer.peer_chunks);\n\tkfree(asoc->peer.peer_hmacs);\n\n\t/* Release the transport structures. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tlist_del_rcu(pos);\n\t\tsctp_transport_free(transport);\n\t}\n\n\tasoc->peer.transport_count = 0;\n\n\tsctp_asconf_queue_teardown(asoc);\n\n\t/* Free pending address space being deleted */\n\tif (asoc->asconf_addr_del_pending != NULL)\n\t\tkfree(asoc->asconf_addr_del_pending);\n\n\t/* AUTH - Free the endpoint shared keys */\n\tsctp_auth_destroy_keys(&asoc->endpoint_shared_keys);\n\n\t/* AUTH - Free the association shared key */\n\tsctp_auth_key_put(asoc->asoc_shared_key);\n\n\tsctp_association_put(asoc);\n}\n\n/* Cleanup and free up an association. */\nstatic void sctp_association_destroy(struct sctp_association *asoc)\n{\n\tif (unlikely(!asoc->base.dead)) {\n\t\tWARN(1, \"Attempt to destroy undead association %p!\\n\", asoc);\n\t\treturn;\n\t}\n\n\tsctp_endpoint_put(asoc->ep);\n\tsock_put(asoc->base.sk);\n\n\tif (asoc->assoc_id != 0) {\n\t\tspin_lock_bh(&sctp_assocs_id_lock);\n\t\tidr_remove(&sctp_assocs_id, asoc->assoc_id);\n\t\tspin_unlock_bh(&sctp_assocs_id_lock);\n\t}\n\n\tWARN_ON(atomic_read(&asoc->rmem_alloc));\n\n\tkfree(asoc);\n\tSCTP_DBG_OBJCNT_DEC(assoc);\n}\n\n/* Change the primary destination address for the peer. */\nvoid sctp_assoc_set_primary(struct sctp_association *asoc,\n\t\t\t    struct sctp_transport *transport)\n{\n\tint changeover = 0;\n\n\t/* it's a changeover only if we already have a primary path\n\t * that we are changing\n\t */\n\tif (asoc->peer.primary_path != NULL &&\n\t    asoc->peer.primary_path != transport)\n\t\tchangeover = 1 ;\n\n\tasoc->peer.primary_path = transport;\n\n\t/* Set a default msg_name for events. */\n\tmemcpy(&asoc->peer.primary_addr, &transport->ipaddr,\n\t       sizeof(union sctp_addr));\n\n\t/* If the primary path is changing, assume that the\n\t * user wants to use this new path.\n\t */\n\tif ((transport->state == SCTP_ACTIVE) ||\n\t    (transport->state == SCTP_UNKNOWN))\n\t\tasoc->peer.active_path = transport;\n\n\t/*\n\t * SFR-CACC algorithm:\n\t * Upon the receipt of a request to change the primary\n\t * destination address, on the data structure for the new\n\t * primary destination, the sender MUST do the following:\n\t *\n\t * 1) If CHANGEOVER_ACTIVE is set, then there was a switch\n\t * to this destination address earlier. The sender MUST set\n\t * CYCLING_CHANGEOVER to indicate that this switch is a\n\t * double switch to the same destination address.\n\t *\n\t * Really, only bother is we have data queued or outstanding on\n\t * the association.\n\t */\n\tif (!asoc->outqueue.outstanding_bytes && !asoc->outqueue.out_qlen)\n\t\treturn;\n\n\tif (transport->cacc.changeover_active)\n\t\ttransport->cacc.cycling_changeover = changeover;\n\n\t/* 2) The sender MUST set CHANGEOVER_ACTIVE to indicate that\n\t * a changeover has occurred.\n\t */\n\ttransport->cacc.changeover_active = changeover;\n\n\t/* 3) The sender MUST store the next TSN to be sent in\n\t * next_tsn_at_change.\n\t */\n\ttransport->cacc.next_tsn_at_change = asoc->next_tsn;\n}\n\n/* Remove a transport from an association.  */\nvoid sctp_assoc_rm_peer(struct sctp_association *asoc,\n\t\t\tstruct sctp_transport *peer)\n{\n\tstruct list_head\t*pos;\n\tstruct sctp_transport\t*transport;\n\n\tpr_debug(\"%s: association:%p addr:%pISpc\\n\",\n\t\t __func__, asoc, &peer->ipaddr.sa);\n\n\t/* If we are to remove the current retran_path, update it\n\t * to the next peer before removing this peer from the list.\n\t */\n\tif (asoc->peer.retran_path == peer)\n\t\tsctp_assoc_update_retran_path(asoc);\n\n\t/* Remove this peer from the list. */\n\tlist_del_rcu(&peer->transports);\n\n\t/* Get the first transport of asoc. */\n\tpos = asoc->peer.transport_addr_list.next;\n\ttransport = list_entry(pos, struct sctp_transport, transports);\n\n\t/* Update any entries that match the peer to be deleted. */\n\tif (asoc->peer.primary_path == peer)\n\t\tsctp_assoc_set_primary(asoc, transport);\n\tif (asoc->peer.active_path == peer)\n\t\tasoc->peer.active_path = transport;\n\tif (asoc->peer.retran_path == peer)\n\t\tasoc->peer.retran_path = transport;\n\tif (asoc->peer.last_data_from == peer)\n\t\tasoc->peer.last_data_from = transport;\n\n\t/* If we remove the transport an INIT was last sent to, set it to\n\t * NULL. Combined with the update of the retran path above, this\n\t * will cause the next INIT to be sent to the next available\n\t * transport, maintaining the cycle.\n\t */\n\tif (asoc->init_last_sent_to == peer)\n\t\tasoc->init_last_sent_to = NULL;\n\n\t/* If we remove the transport an SHUTDOWN was last sent to, set it\n\t * to NULL. Combined with the update of the retran path above, this\n\t * will cause the next SHUTDOWN to be sent to the next available\n\t * transport, maintaining the cycle.\n\t */\n\tif (asoc->shutdown_last_sent_to == peer)\n\t\tasoc->shutdown_last_sent_to = NULL;\n\n\t/* If we remove the transport an ASCONF was last sent to, set it to\n\t * NULL.\n\t */\n\tif (asoc->addip_last_asconf &&\n\t    asoc->addip_last_asconf->transport == peer)\n\t\tasoc->addip_last_asconf->transport = NULL;\n\n\t/* If we have something on the transmitted list, we have to\n\t * save it off.  The best place is the active path.\n\t */\n\tif (!list_empty(&peer->transmitted)) {\n\t\tstruct sctp_transport *active = asoc->peer.active_path;\n\t\tstruct sctp_chunk *ch;\n\n\t\t/* Reset the transport of each chunk on this list */\n\t\tlist_for_each_entry(ch, &peer->transmitted,\n\t\t\t\t\ttransmitted_list) {\n\t\t\tch->transport = NULL;\n\t\t\tch->rtt_in_progress = 0;\n\t\t}\n\n\t\tlist_splice_tail_init(&peer->transmitted,\n\t\t\t\t\t&active->transmitted);\n\n\t\t/* Start a T3 timer here in case it wasn't running so\n\t\t * that these migrated packets have a chance to get\n\t\t * retransmitted.\n\t\t */\n\t\tif (!timer_pending(&active->T3_rtx_timer))\n\t\t\tif (!mod_timer(&active->T3_rtx_timer,\n\t\t\t\t\tjiffies + active->rto))\n\t\t\t\tsctp_transport_hold(active);\n\t}\n\n\tasoc->peer.transport_count--;\n\n\tsctp_transport_free(peer);\n}\n\n/* Add a transport address to an association.  */\nstruct sctp_transport *sctp_assoc_add_peer(struct sctp_association *asoc,\n\t\t\t\t\t   const union sctp_addr *addr,\n\t\t\t\t\t   const gfp_t gfp,\n\t\t\t\t\t   const int peer_state)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tstruct sctp_transport *peer;\n\tstruct sctp_sock *sp;\n\tunsigned short port;\n\n\tsp = sctp_sk(asoc->base.sk);\n\n\t/* AF_INET and AF_INET6 share common port field. */\n\tport = ntohs(addr->v4.sin_port);\n\n\tpr_debug(\"%s: association:%p addr:%pISpc state:%d\\n\", __func__,\n\t\t asoc, &addr->sa, peer_state);\n\n\t/* Set the port if it has not been set yet.  */\n\tif (0 == asoc->peer.port)\n\t\tasoc->peer.port = port;\n\n\t/* Check to see if this is a duplicate. */\n\tpeer = sctp_assoc_lookup_paddr(asoc, addr);\n\tif (peer) {\n\t\t/* An UNKNOWN state is only set on transports added by\n\t\t * user in sctp_connectx() call.  Such transports should be\n\t\t * considered CONFIRMED per RFC 4960, Section 5.4.\n\t\t */\n\t\tif (peer->state == SCTP_UNKNOWN) {\n\t\t\tpeer->state = SCTP_ACTIVE;\n\t\t}\n\t\treturn peer;\n\t}\n\n\tpeer = sctp_transport_new(net, addr, gfp);\n\tif (!peer)\n\t\treturn NULL;\n\n\tsctp_transport_set_owner(peer, asoc);\n\n\t/* Initialize the peer's heartbeat interval based on the\n\t * association configured value.\n\t */\n\tpeer->hbinterval = asoc->hbinterval;\n\n\t/* Set the path max_retrans.  */\n\tpeer->pathmaxrxt = asoc->pathmaxrxt;\n\n\t/* And the partial failure retrans threshold */\n\tpeer->pf_retrans = asoc->pf_retrans;\n\n\t/* Initialize the peer's SACK delay timeout based on the\n\t * association configured value.\n\t */\n\tpeer->sackdelay = asoc->sackdelay;\n\tpeer->sackfreq = asoc->sackfreq;\n\n\t/* Enable/disable heartbeat, SACK delay, and path MTU discovery\n\t * based on association setting.\n\t */\n\tpeer->param_flags = asoc->param_flags;\n\n\tsctp_transport_route(peer, NULL, sp);\n\n\t/* Initialize the pmtu of the transport. */\n\tif (peer->param_flags & SPP_PMTUD_DISABLE) {\n\t\tif (asoc->pathmtu)\n\t\t\tpeer->pathmtu = asoc->pathmtu;\n\t\telse\n\t\t\tpeer->pathmtu = SCTP_DEFAULT_MAXSEGMENT;\n\t}\n\n\t/* If this is the first transport addr on this association,\n\t * initialize the association PMTU to the peer's PMTU.\n\t * If not and the current association PMTU is higher than the new\n\t * peer's PMTU, reset the association PMTU to the new peer's PMTU.\n\t */\n\tif (asoc->pathmtu)\n\t\tasoc->pathmtu = min_t(int, peer->pathmtu, asoc->pathmtu);\n\telse\n\t\tasoc->pathmtu = peer->pathmtu;\n\n\tpr_debug(\"%s: association:%p PMTU set to %d\\n\", __func__, asoc,\n\t\t asoc->pathmtu);\n\n\tpeer->pmtu_pending = 0;\n\n\tasoc->frag_point = sctp_frag_point(asoc, asoc->pathmtu);\n\n\t/* The asoc->peer.port might not be meaningful yet, but\n\t * initialize the packet structure anyway.\n\t */\n\tsctp_packet_init(&peer->packet, peer, asoc->base.bind_addr.port,\n\t\t\t asoc->peer.port);\n\n\t/* 7.2.1 Slow-Start\n\t *\n\t * o The initial cwnd before DATA transmission or after a sufficiently\n\t *   long idle period MUST be set to\n\t *      min(4*MTU, max(2*MTU, 4380 bytes))\n\t *\n\t * o The initial value of ssthresh MAY be arbitrarily high\n\t *   (for example, implementations MAY use the size of the\n\t *   receiver advertised window).\n\t */\n\tpeer->cwnd = min(4*asoc->pathmtu, max_t(__u32, 2*asoc->pathmtu, 4380));\n\n\t/* At this point, we may not have the receiver's advertised window,\n\t * so initialize ssthresh to the default value and it will be set\n\t * later when we process the INIT.\n\t */\n\tpeer->ssthresh = SCTP_DEFAULT_MAXWINDOW;\n\n\tpeer->partial_bytes_acked = 0;\n\tpeer->flight_size = 0;\n\tpeer->burst_limited = 0;\n\n\t/* Set the transport's RTO.initial value */\n\tpeer->rto = asoc->rto_initial;\n\tsctp_max_rto(asoc, peer);\n\n\t/* Set the peer's active state. */\n\tpeer->state = peer_state;\n\n\t/* Attach the remote transport to our asoc.  */\n\tlist_add_tail_rcu(&peer->transports, &asoc->peer.transport_addr_list);\n\tasoc->peer.transport_count++;\n\n\t/* If we do not yet have a primary path, set one.  */\n\tif (!asoc->peer.primary_path) {\n\t\tsctp_assoc_set_primary(asoc, peer);\n\t\tasoc->peer.retran_path = peer;\n\t}\n\n\tif (asoc->peer.active_path == asoc->peer.retran_path &&\n\t    peer->state != SCTP_UNCONFIRMED) {\n\t\tasoc->peer.retran_path = peer;\n\t}\n\n\treturn peer;\n}\n\n/* Delete a transport address from an association.  */\nvoid sctp_assoc_del_peer(struct sctp_association *asoc,\n\t\t\t const union sctp_addr *addr)\n{\n\tstruct list_head\t*pos;\n\tstruct list_head\t*temp;\n\tstruct sctp_transport\t*transport;\n\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tif (sctp_cmp_addr_exact(addr, &transport->ipaddr)) {\n\t\t\t/* Do book keeping for removing the peer and free it. */\n\t\t\tsctp_assoc_rm_peer(asoc, transport);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/* Lookup a transport by address. */\nstruct sctp_transport *sctp_assoc_lookup_paddr(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\tconst union sctp_addr *address)\n{\n\tstruct sctp_transport *t;\n\n\t/* Cycle through all transports searching for a peer address. */\n\n\tlist_for_each_entry(t, &asoc->peer.transport_addr_list,\n\t\t\ttransports) {\n\t\tif (sctp_cmp_addr_exact(address, &t->ipaddr))\n\t\t\treturn t;\n\t}\n\n\treturn NULL;\n}\n\n/* Remove all transports except a give one */\nvoid sctp_assoc_del_nonprimary_peers(struct sctp_association *asoc,\n\t\t\t\t     struct sctp_transport *primary)\n{\n\tstruct sctp_transport\t*temp;\n\tstruct sctp_transport\t*t;\n\n\tlist_for_each_entry_safe(t, temp, &asoc->peer.transport_addr_list,\n\t\t\t\t transports) {\n\t\t/* if the current transport is not the primary one, delete it */\n\t\tif (t != primary)\n\t\t\tsctp_assoc_rm_peer(asoc, t);\n\t}\n}\n\n/* Engage in transport control operations.\n * Mark the transport up or down and send a notification to the user.\n * Select and update the new active and retran paths.\n */\nvoid sctp_assoc_control_transport(struct sctp_association *asoc,\n\t\t\t\t  struct sctp_transport *transport,\n\t\t\t\t  sctp_transport_cmd_t command,\n\t\t\t\t  sctp_sn_error_t error)\n{\n\tstruct sctp_ulpevent *event;\n\tstruct sockaddr_storage addr;\n\tint spc_state = 0;\n\tbool ulp_notify = true;\n\n\t/* Record the transition on the transport.  */\n\tswitch (command) {\n\tcase SCTP_TRANSPORT_UP:\n\t\t/* If we are moving from UNCONFIRMED state due\n\t\t * to heartbeat success, report the SCTP_ADDR_CONFIRMED\n\t\t * state to the user, otherwise report SCTP_ADDR_AVAILABLE.\n\t\t */\n\t\tif (SCTP_UNCONFIRMED == transport->state &&\n\t\t    SCTP_HEARTBEAT_SUCCESS == error)\n\t\t\tspc_state = SCTP_ADDR_CONFIRMED;\n\t\telse\n\t\t\tspc_state = SCTP_ADDR_AVAILABLE;\n\t\t/* Don't inform ULP about transition from PF to\n\t\t * active state and set cwnd to 1 MTU, see SCTP\n\t\t * Quick failover draft section 5.1, point 5\n\t\t */\n\t\tif (transport->state == SCTP_PF) {\n\t\t\tulp_notify = false;\n\t\t\ttransport->cwnd = asoc->pathmtu;\n\t\t}\n\t\ttransport->state = SCTP_ACTIVE;\n\t\tbreak;\n\n\tcase SCTP_TRANSPORT_DOWN:\n\t\t/* If the transport was never confirmed, do not transition it\n\t\t * to inactive state.  Also, release the cached route since\n\t\t * there may be a better route next time.\n\t\t */\n\t\tif (transport->state != SCTP_UNCONFIRMED)\n\t\t\ttransport->state = SCTP_INACTIVE;\n\t\telse {\n\t\t\tdst_release(transport->dst);\n\t\t\ttransport->dst = NULL;\n\t\t\tulp_notify = false;\n\t\t}\n\n\t\tspc_state = SCTP_ADDR_UNREACHABLE;\n\t\tbreak;\n\n\tcase SCTP_TRANSPORT_PF:\n\t\ttransport->state = SCTP_PF;\n\t\tulp_notify = false;\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\n\t/* Generate and send a SCTP_PEER_ADDR_CHANGE notification\n\t * to the user.\n\t */\n\tif (ulp_notify) {\n\t\tmemset(&addr, 0, sizeof(struct sockaddr_storage));\n\t\tmemcpy(&addr, &transport->ipaddr,\n\t\t       transport->af_specific->sockaddr_len);\n\n\t\tevent = sctp_ulpevent_make_peer_addr_change(asoc, &addr,\n\t\t\t\t\t0, spc_state, error, GFP_ATOMIC);\n\t\tif (event)\n\t\t\tsctp_ulpq_tail_event(&asoc->ulpq, event);\n\t}\n\n\t/* Select new active and retran paths. */\n\tsctp_select_active_and_retran_path(asoc);\n}\n\n/* Hold a reference to an association. */\nvoid sctp_association_hold(struct sctp_association *asoc)\n{\n\tatomic_inc(&asoc->base.refcnt);\n}\n\n/* Release a reference to an association and cleanup\n * if there are no more references.\n */\nvoid sctp_association_put(struct sctp_association *asoc)\n{\n\tif (atomic_dec_and_test(&asoc->base.refcnt))\n\t\tsctp_association_destroy(asoc);\n}\n\n/* Allocate the next TSN, Transmission Sequence Number, for the given\n * association.\n */\n__u32 sctp_association_get_next_tsn(struct sctp_association *asoc)\n{\n\t/* From Section 1.6 Serial Number Arithmetic:\n\t * Transmission Sequence Numbers wrap around when they reach\n\t * 2**32 - 1.  That is, the next TSN a DATA chunk MUST use\n\t * after transmitting TSN = 2*32 - 1 is TSN = 0.\n\t */\n\t__u32 retval = asoc->next_tsn;\n\tasoc->next_tsn++;\n\tasoc->unack_data++;\n\n\treturn retval;\n}\n\n/* Compare two addresses to see if they match.  Wildcard addresses\n * only match themselves.\n */\nint sctp_cmp_addr_exact(const union sctp_addr *ss1,\n\t\t\tconst union sctp_addr *ss2)\n{\n\tstruct sctp_af *af;\n\n\taf = sctp_get_af_specific(ss1->sa.sa_family);\n\tif (unlikely(!af))\n\t\treturn 0;\n\n\treturn af->cmp_addr(ss1, ss2);\n}\n\n/* Return an ecne chunk to get prepended to a packet.\n * Note:  We are sly and return a shared, prealloced chunk.  FIXME:\n * No we don't, but we could/should.\n */\nstruct sctp_chunk *sctp_get_ecne_prepend(struct sctp_association *asoc)\n{\n\tif (!asoc->need_ecne)\n\t\treturn NULL;\n\n\t/* Send ECNE if needed.\n\t * Not being able to allocate a chunk here is not deadly.\n\t */\n\treturn sctp_make_ecne(asoc, asoc->last_ecne_tsn);\n}\n\n/*\n * Find which transport this TSN was sent on.\n */\nstruct sctp_transport *sctp_assoc_lookup_tsn(struct sctp_association *asoc,\n\t\t\t\t\t     __u32 tsn)\n{\n\tstruct sctp_transport *active;\n\tstruct sctp_transport *match;\n\tstruct sctp_transport *transport;\n\tstruct sctp_chunk *chunk;\n\t__be32 key = htonl(tsn);\n\n\tmatch = NULL;\n\n\t/*\n\t * FIXME: In general, find a more efficient data structure for\n\t * searching.\n\t */\n\n\t/*\n\t * The general strategy is to search each transport's transmitted\n\t * list.   Return which transport this TSN lives on.\n\t *\n\t * Let's be hopeful and check the active_path first.\n\t * Another optimization would be to know if there is only one\n\t * outbound path and not have to look for the TSN at all.\n\t *\n\t */\n\n\tactive = asoc->peer.active_path;\n\n\tlist_for_each_entry(chunk, &active->transmitted,\n\t\t\ttransmitted_list) {\n\n\t\tif (key == chunk->subh.data_hdr->tsn) {\n\t\t\tmatch = active;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* If not found, go search all the other transports. */\n\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\ttransports) {\n\n\t\tif (transport == active)\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chunk, &transport->transmitted,\n\t\t\t\ttransmitted_list) {\n\t\t\tif (key == chunk->subh.data_hdr->tsn) {\n\t\t\t\tmatch = transport;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\treturn match;\n}\n\n/* Is this the association we are looking for? */\nstruct sctp_transport *sctp_assoc_is_match(struct sctp_association *asoc,\n\t\t\t\t\t   struct net *net,\n\t\t\t\t\t   const union sctp_addr *laddr,\n\t\t\t\t\t   const union sctp_addr *paddr)\n{\n\tstruct sctp_transport *transport;\n\n\tif ((htons(asoc->base.bind_addr.port) == laddr->v4.sin_port) &&\n\t    (htons(asoc->peer.port) == paddr->v4.sin_port) &&\n\t    net_eq(sock_net(asoc->base.sk), net)) {\n\t\ttransport = sctp_assoc_lookup_paddr(asoc, paddr);\n\t\tif (!transport)\n\t\t\tgoto out;\n\n\t\tif (sctp_bind_addr_match(&asoc->base.bind_addr, laddr,\n\t\t\t\t\t sctp_sk(asoc->base.sk)))\n\t\t\tgoto out;\n\t}\n\ttransport = NULL;\n\nout:\n\treturn transport;\n}\n\n/* Do delayed input processing.  This is scheduled by sctp_rcv(). */\nstatic void sctp_assoc_bh_rcv(struct work_struct *work)\n{\n\tstruct sctp_association *asoc =\n\t\tcontainer_of(work, struct sctp_association,\n\t\t\t     base.inqueue.immediate);\n\tstruct net *net = sock_net(asoc->base.sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_chunk *chunk;\n\tstruct sctp_inq *inqueue;\n\tint state;\n\tsctp_subtype_t subtype;\n\tint error = 0;\n\n\t/* The association should be held so we should be safe. */\n\tep = asoc->ep;\n\n\tinqueue = &asoc->base.inqueue;\n\tsctp_association_hold(asoc);\n\twhile (NULL != (chunk = sctp_inq_pop(inqueue))) {\n\t\tstate = asoc->state;\n\t\tsubtype = SCTP_ST_CHUNK(chunk->chunk_hdr->type);\n\n\t\t/* SCTP-AUTH, Section 6.3:\n\t\t *    The receiver has a list of chunk types which it expects\n\t\t *    to be received only after an AUTH-chunk.  This list has\n\t\t *    been sent to the peer during the association setup.  It\n\t\t *    MUST silently discard these chunks if they are not placed\n\t\t *    after an AUTH chunk in the packet.\n\t\t */\n\t\tif (sctp_auth_recv_cid(subtype.chunk, asoc) && !chunk->auth)\n\t\t\tcontinue;\n\n\t\t/* Remember where the last DATA chunk came from so we\n\t\t * know where to send the SACK.\n\t\t */\n\t\tif (sctp_chunk_is_data(chunk))\n\t\t\tasoc->peer.last_data_from = chunk->transport;\n\t\telse {\n\t\t\tSCTP_INC_STATS(net, SCTP_MIB_INCTRLCHUNKS);\n\t\t\tasoc->stats.ictrlchunks++;\n\t\t\tif (chunk->chunk_hdr->type == SCTP_CID_SACK)\n\t\t\t\tasoc->stats.isacks++;\n\t\t}\n\n\t\tif (chunk->transport)\n\t\t\tchunk->transport->last_time_heard = ktime_get();\n\n\t\t/* Run through the state machine. */\n\t\terror = sctp_do_sm(net, SCTP_EVENT_T_CHUNK, subtype,\n\t\t\t\t   state, ep, asoc, chunk, GFP_ATOMIC);\n\n\t\t/* Check to see if the association is freed in response to\n\t\t * the incoming chunk.  If so, get out of the while loop.\n\t\t */\n\t\tif (asoc->base.dead)\n\t\t\tbreak;\n\n\t\t/* If there is an error on chunk, discard this packet. */\n\t\tif (error && chunk)\n\t\t\tchunk->pdiscard = 1;\n\t}\n\tsctp_association_put(asoc);\n}\n\n/* This routine moves an association from its old sk to a new sk.  */\nvoid sctp_assoc_migrate(struct sctp_association *assoc, struct sock *newsk)\n{\n\tstruct sctp_sock *newsp = sctp_sk(newsk);\n\tstruct sock *oldsk = assoc->base.sk;\n\n\t/* Delete the association from the old endpoint's list of\n\t * associations.\n\t */\n\tlist_del_init(&assoc->asocs);\n\n\t/* Decrement the backlog value for a TCP-style socket. */\n\tif (sctp_style(oldsk, TCP))\n\t\toldsk->sk_ack_backlog--;\n\n\t/* Release references to the old endpoint and the sock.  */\n\tsctp_endpoint_put(assoc->ep);\n\tsock_put(assoc->base.sk);\n\n\t/* Get a reference to the new endpoint.  */\n\tassoc->ep = newsp->ep;\n\tsctp_endpoint_hold(assoc->ep);\n\n\t/* Get a reference to the new sock.  */\n\tassoc->base.sk = newsk;\n\tsock_hold(assoc->base.sk);\n\n\t/* Add the association to the new endpoint's list of associations.  */\n\tsctp_endpoint_add_asoc(newsp->ep, assoc);\n}\n\n/* Update an association (possibly from unexpected COOKIE-ECHO processing).  */\nvoid sctp_assoc_update(struct sctp_association *asoc,\n\t\t       struct sctp_association *new)\n{\n\tstruct sctp_transport *trans;\n\tstruct list_head *pos, *temp;\n\n\t/* Copy in new parameters of peer. */\n\tasoc->c = new->c;\n\tasoc->peer.rwnd = new->peer.rwnd;\n\tasoc->peer.sack_needed = new->peer.sack_needed;\n\tasoc->peer.auth_capable = new->peer.auth_capable;\n\tasoc->peer.i = new->peer.i;\n\tsctp_tsnmap_init(&asoc->peer.tsn_map, SCTP_TSN_MAP_INITIAL,\n\t\t\t asoc->peer.i.initial_tsn, GFP_ATOMIC);\n\n\t/* Remove any peer addresses not present in the new association. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttrans = list_entry(pos, struct sctp_transport, transports);\n\t\tif (!sctp_assoc_lookup_paddr(new, &trans->ipaddr)) {\n\t\t\tsctp_assoc_rm_peer(asoc, trans);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (asoc->state >= SCTP_STATE_ESTABLISHED)\n\t\t\tsctp_transport_reset(trans);\n\t}\n\n\t/* If the case is A (association restart), use\n\t * initial_tsn as next_tsn. If the case is B, use\n\t * current next_tsn in case data sent to peer\n\t * has been discarded and needs retransmission.\n\t */\n\tif (asoc->state >= SCTP_STATE_ESTABLISHED) {\n\t\tasoc->next_tsn = new->next_tsn;\n\t\tasoc->ctsn_ack_point = new->ctsn_ack_point;\n\t\tasoc->adv_peer_ack_point = new->adv_peer_ack_point;\n\n\t\t/* Reinitialize SSN for both local streams\n\t\t * and peer's streams.\n\t\t */\n\t\tsctp_ssnmap_clear(asoc->ssnmap);\n\n\t\t/* Flush the ULP reassembly and ordered queue.\n\t\t * Any data there will now be stale and will\n\t\t * cause problems.\n\t\t */\n\t\tsctp_ulpq_flush(&asoc->ulpq);\n\n\t\t/* reset the overall association error count so\n\t\t * that the restarted association doesn't get torn\n\t\t * down on the next retransmission timer.\n\t\t */\n\t\tasoc->overall_error_count = 0;\n\n\t} else {\n\t\t/* Add any peer addresses from the new association. */\n\t\tlist_for_each_entry(trans, &new->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tif (!sctp_assoc_lookup_paddr(asoc, &trans->ipaddr))\n\t\t\t\tsctp_assoc_add_peer(asoc, &trans->ipaddr,\n\t\t\t\t\t\t    GFP_ATOMIC, trans->state);\n\t\t}\n\n\t\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\t\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\t\tif (!asoc->ssnmap) {\n\t\t\t/* Move the ssnmap. */\n\t\t\tasoc->ssnmap = new->ssnmap;\n\t\t\tnew->ssnmap = NULL;\n\t\t}\n\n\t\tif (!asoc->assoc_id) {\n\t\t\t/* get a new association id since we don't have one\n\t\t\t * yet.\n\t\t\t */\n\t\t\tsctp_assoc_set_id(asoc, GFP_ATOMIC);\n\t\t}\n\t}\n\n\t/* SCTP-AUTH: Save the peer parameters from the new associations\n\t * and also move the association shared keys over\n\t */\n\tkfree(asoc->peer.peer_random);\n\tasoc->peer.peer_random = new->peer.peer_random;\n\tnew->peer.peer_random = NULL;\n\n\tkfree(asoc->peer.peer_chunks);\n\tasoc->peer.peer_chunks = new->peer.peer_chunks;\n\tnew->peer.peer_chunks = NULL;\n\n\tkfree(asoc->peer.peer_hmacs);\n\tasoc->peer.peer_hmacs = new->peer.peer_hmacs;\n\tnew->peer.peer_hmacs = NULL;\n\n\tsctp_auth_key_put(asoc->asoc_shared_key);\n\tsctp_auth_asoc_init_active_key(asoc, GFP_ATOMIC);\n}\n\n/* Update the retran path for sending a retransmitted packet.\n * See also RFC4960, 6.4. Multi-Homed SCTP Endpoints:\n *\n *   When there is outbound data to send and the primary path\n *   becomes inactive (e.g., due to failures), or where the\n *   SCTP user explicitly requests to send data to an\n *   inactive destination transport address, before reporting\n *   an error to its ULP, the SCTP endpoint should try to send\n *   the data to an alternate active destination transport\n *   address if one exists.\n *\n *   When retransmitting data that timed out, if the endpoint\n *   is multihomed, it should consider each source-destination\n *   address pair in its retransmission selection policy.\n *   When retransmitting timed-out data, the endpoint should\n *   attempt to pick the most divergent source-destination\n *   pair from the original source-destination pair to which\n *   the packet was transmitted.\n *\n *   Note: Rules for picking the most divergent source-destination\n *   pair are an implementation decision and are not specified\n *   within this document.\n *\n * Our basic strategy is to round-robin transports in priorities\n * according to sctp_state_prio_map[] e.g., if no such\n * transport with state SCTP_ACTIVE exists, round-robin through\n * SCTP_UNKNOWN, etc. You get the picture.\n */\nstatic const u8 sctp_trans_state_to_prio_map[] = {\n\t[SCTP_ACTIVE]\t= 3,\t/* best case */\n\t[SCTP_UNKNOWN]\t= 2,\n\t[SCTP_PF]\t= 1,\n\t[SCTP_INACTIVE] = 0,\t/* worst case */\n};\n\nstatic u8 sctp_trans_score(const struct sctp_transport *trans)\n{\n\treturn sctp_trans_state_to_prio_map[trans->state];\n}\n\nstatic struct sctp_transport *sctp_trans_elect_tie(struct sctp_transport *trans1,\n\t\t\t\t\t\t   struct sctp_transport *trans2)\n{\n\tif (trans1->error_count > trans2->error_count) {\n\t\treturn trans2;\n\t} else if (trans1->error_count == trans2->error_count &&\n\t\t   ktime_after(trans2->last_time_heard,\n\t\t\t       trans1->last_time_heard)) {\n\t\treturn trans2;\n\t} else {\n\t\treturn trans1;\n\t}\n}\n\nstatic struct sctp_transport *sctp_trans_elect_best(struct sctp_transport *curr,\n\t\t\t\t\t\t    struct sctp_transport *best)\n{\n\tu8 score_curr, score_best;\n\n\tif (best == NULL || curr == best)\n\t\treturn curr;\n\n\tscore_curr = sctp_trans_score(curr);\n\tscore_best = sctp_trans_score(best);\n\n\t/* First, try a score-based selection if both transport states\n\t * differ. If we're in a tie, lets try to make a more clever\n\t * decision here based on error counts and last time heard.\n\t */\n\tif (score_curr > score_best)\n\t\treturn curr;\n\telse if (score_curr == score_best)\n\t\treturn sctp_trans_elect_tie(curr, best);\n\telse\n\t\treturn best;\n}\n\nvoid sctp_assoc_update_retran_path(struct sctp_association *asoc)\n{\n\tstruct sctp_transport *trans = asoc->peer.retran_path;\n\tstruct sctp_transport *trans_next = NULL;\n\n\t/* We're done as we only have the one and only path. */\n\tif (asoc->peer.transport_count == 1)\n\t\treturn;\n\t/* If active_path and retran_path are the same and active,\n\t * then this is the only active path. Use it.\n\t */\n\tif (asoc->peer.active_path == asoc->peer.retran_path &&\n\t    asoc->peer.active_path->state == SCTP_ACTIVE)\n\t\treturn;\n\n\t/* Iterate from retran_path's successor back to retran_path. */\n\tfor (trans = list_next_entry(trans, transports); 1;\n\t     trans = list_next_entry(trans, transports)) {\n\t\t/* Manually skip the head element. */\n\t\tif (&trans->transports == &asoc->peer.transport_addr_list)\n\t\t\tcontinue;\n\t\tif (trans->state == SCTP_UNCONFIRMED)\n\t\t\tcontinue;\n\t\ttrans_next = sctp_trans_elect_best(trans, trans_next);\n\t\t/* Active is good enough for immediate return. */\n\t\tif (trans_next->state == SCTP_ACTIVE)\n\t\t\tbreak;\n\t\t/* We've reached the end, time to update path. */\n\t\tif (trans == asoc->peer.retran_path)\n\t\t\tbreak;\n\t}\n\n\tasoc->peer.retran_path = trans_next;\n\n\tpr_debug(\"%s: association:%p updated new path to addr:%pISpc\\n\",\n\t\t __func__, asoc, &asoc->peer.retran_path->ipaddr.sa);\n}\n\nstatic void sctp_select_active_and_retran_path(struct sctp_association *asoc)\n{\n\tstruct sctp_transport *trans, *trans_pri = NULL, *trans_sec = NULL;\n\tstruct sctp_transport *trans_pf = NULL;\n\n\t/* Look for the two most recently used active transports. */\n\tlist_for_each_entry(trans, &asoc->peer.transport_addr_list,\n\t\t\t    transports) {\n\t\t/* Skip uninteresting transports. */\n\t\tif (trans->state == SCTP_INACTIVE ||\n\t\t    trans->state == SCTP_UNCONFIRMED)\n\t\t\tcontinue;\n\t\t/* Keep track of the best PF transport from our\n\t\t * list in case we don't find an active one.\n\t\t */\n\t\tif (trans->state == SCTP_PF) {\n\t\t\ttrans_pf = sctp_trans_elect_best(trans, trans_pf);\n\t\t\tcontinue;\n\t\t}\n\t\t/* For active transports, pick the most recent ones. */\n\t\tif (trans_pri == NULL ||\n\t\t    ktime_after(trans->last_time_heard,\n\t\t\t\ttrans_pri->last_time_heard)) {\n\t\t\ttrans_sec = trans_pri;\n\t\t\ttrans_pri = trans;\n\t\t} else if (trans_sec == NULL ||\n\t\t\t   ktime_after(trans->last_time_heard,\n\t\t\t\t       trans_sec->last_time_heard)) {\n\t\t\ttrans_sec = trans;\n\t\t}\n\t}\n\n\t/* RFC 2960 6.4 Multi-Homed SCTP Endpoints\n\t *\n\t * By default, an endpoint should always transmit to the primary\n\t * path, unless the SCTP user explicitly specifies the\n\t * destination transport address (and possibly source transport\n\t * address) to use. [If the primary is active but not most recent,\n\t * bump the most recently used transport.]\n\t */\n\tif ((asoc->peer.primary_path->state == SCTP_ACTIVE ||\n\t     asoc->peer.primary_path->state == SCTP_UNKNOWN) &&\n\t     asoc->peer.primary_path != trans_pri) {\n\t\ttrans_sec = trans_pri;\n\t\ttrans_pri = asoc->peer.primary_path;\n\t}\n\n\t/* We did not find anything useful for a possible retransmission\n\t * path; either primary path that we found is the the same as\n\t * the current one, or we didn't generally find an active one.\n\t */\n\tif (trans_sec == NULL)\n\t\ttrans_sec = trans_pri;\n\n\t/* If we failed to find a usable transport, just camp on the\n\t * active or pick a PF iff it's the better choice.\n\t */\n\tif (trans_pri == NULL) {\n\t\ttrans_pri = sctp_trans_elect_best(asoc->peer.active_path, trans_pf);\n\t\ttrans_sec = trans_pri;\n\t}\n\n\t/* Set the active and retran transports. */\n\tasoc->peer.active_path = trans_pri;\n\tasoc->peer.retran_path = trans_sec;\n}\n\nstruct sctp_transport *\nsctp_assoc_choose_alter_transport(struct sctp_association *asoc,\n\t\t\t\t  struct sctp_transport *last_sent_to)\n{\n\t/* If this is the first time packet is sent, use the active path,\n\t * else use the retran path. If the last packet was sent over the\n\t * retran path, update the retran path and use it.\n\t */\n\tif (last_sent_to == NULL) {\n\t\treturn asoc->peer.active_path;\n\t} else {\n\t\tif (last_sent_to == asoc->peer.retran_path)\n\t\t\tsctp_assoc_update_retran_path(asoc);\n\n\t\treturn asoc->peer.retran_path;\n\t}\n}\n\n/* Update the association's pmtu and frag_point by going through all the\n * transports. This routine is called when a transport's PMTU has changed.\n */\nvoid sctp_assoc_sync_pmtu(struct sock *sk, struct sctp_association *asoc)\n{\n\tstruct sctp_transport *t;\n\t__u32 pmtu = 0;\n\n\tif (!asoc)\n\t\treturn;\n\n\t/* Get the lowest pmtu of all the transports. */\n\tlist_for_each_entry(t, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\tif (t->pmtu_pending && t->dst) {\n\t\t\tsctp_transport_update_pmtu(sk, t, dst_mtu(t->dst));\n\t\t\tt->pmtu_pending = 0;\n\t\t}\n\t\tif (!pmtu || (t->pathmtu < pmtu))\n\t\t\tpmtu = t->pathmtu;\n\t}\n\n\tif (pmtu) {\n\t\tasoc->pathmtu = pmtu;\n\t\tasoc->frag_point = sctp_frag_point(asoc, pmtu);\n\t}\n\n\tpr_debug(\"%s: asoc:%p, pmtu:%d, frag_point:%d\\n\", __func__, asoc,\n\t\t asoc->pathmtu, asoc->frag_point);\n}\n\n/* Should we send a SACK to update our peer? */\nstatic inline bool sctp_peer_needs_update(struct sctp_association *asoc)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tswitch (asoc->state) {\n\tcase SCTP_STATE_ESTABLISHED:\n\tcase SCTP_STATE_SHUTDOWN_PENDING:\n\tcase SCTP_STATE_SHUTDOWN_RECEIVED:\n\tcase SCTP_STATE_SHUTDOWN_SENT:\n\t\tif ((asoc->rwnd > asoc->a_rwnd) &&\n\t\t    ((asoc->rwnd - asoc->a_rwnd) >= max_t(__u32,\n\t\t\t   (asoc->base.sk->sk_rcvbuf >> net->sctp.rwnd_upd_shift),\n\t\t\t   asoc->pathmtu)))\n\t\t\treturn true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n}\n\n/* Increase asoc's rwnd by len and send any window update SACK if needed. */\nvoid sctp_assoc_rwnd_increase(struct sctp_association *asoc, unsigned int len)\n{\n\tstruct sctp_chunk *sack;\n\tstruct timer_list *timer;\n\n\tif (asoc->rwnd_over) {\n\t\tif (asoc->rwnd_over >= len) {\n\t\t\tasoc->rwnd_over -= len;\n\t\t} else {\n\t\t\tasoc->rwnd += (len - asoc->rwnd_over);\n\t\t\tasoc->rwnd_over = 0;\n\t\t}\n\t} else {\n\t\tasoc->rwnd += len;\n\t}\n\n\t/* If we had window pressure, start recovering it\n\t * once our rwnd had reached the accumulated pressure\n\t * threshold.  The idea is to recover slowly, but up\n\t * to the initial advertised window.\n\t */\n\tif (asoc->rwnd_press && asoc->rwnd >= asoc->rwnd_press) {\n\t\tint change = min(asoc->pathmtu, asoc->rwnd_press);\n\t\tasoc->rwnd += change;\n\t\tasoc->rwnd_press -= change;\n\t}\n\n\tpr_debug(\"%s: asoc:%p rwnd increased by %d to (%u, %u) - %u\\n\",\n\t\t __func__, asoc, len, asoc->rwnd, asoc->rwnd_over,\n\t\t asoc->a_rwnd);\n\n\t/* Send a window update SACK if the rwnd has increased by at least the\n\t * minimum of the association's PMTU and half of the receive buffer.\n\t * The algorithm used is similar to the one described in\n\t * Section 4.2.3.3 of RFC 1122.\n\t */\n\tif (sctp_peer_needs_update(asoc)) {\n\t\tasoc->a_rwnd = asoc->rwnd;\n\n\t\tpr_debug(\"%s: sending window update SACK- asoc:%p rwnd:%u \"\n\t\t\t \"a_rwnd:%u\\n\", __func__, asoc, asoc->rwnd,\n\t\t\t asoc->a_rwnd);\n\n\t\tsack = sctp_make_sack(asoc);\n\t\tif (!sack)\n\t\t\treturn;\n\n\t\tasoc->peer.sack_needed = 0;\n\n\t\tsctp_outq_tail(&asoc->outqueue, sack);\n\n\t\t/* Stop the SACK timer.  */\n\t\ttimer = &asoc->timers[SCTP_EVENT_TIMEOUT_SACK];\n\t\tif (del_timer(timer))\n\t\t\tsctp_association_put(asoc);\n\t}\n}\n\n/* Decrease asoc's rwnd by len. */\nvoid sctp_assoc_rwnd_decrease(struct sctp_association *asoc, unsigned int len)\n{\n\tint rx_count;\n\tint over = 0;\n\n\tif (unlikely(!asoc->rwnd || asoc->rwnd_over))\n\t\tpr_debug(\"%s: association:%p has asoc->rwnd:%u, \"\n\t\t\t \"asoc->rwnd_over:%u!\\n\", __func__, asoc,\n\t\t\t asoc->rwnd, asoc->rwnd_over);\n\n\tif (asoc->ep->rcvbuf_policy)\n\t\trx_count = atomic_read(&asoc->rmem_alloc);\n\telse\n\t\trx_count = atomic_read(&asoc->base.sk->sk_rmem_alloc);\n\n\t/* If we've reached or overflowed our receive buffer, announce\n\t * a 0 rwnd if rwnd would still be positive.  Store the\n\t * the potential pressure overflow so that the window can be restored\n\t * back to original value.\n\t */\n\tif (rx_count >= asoc->base.sk->sk_rcvbuf)\n\t\tover = 1;\n\n\tif (asoc->rwnd >= len) {\n\t\tasoc->rwnd -= len;\n\t\tif (over) {\n\t\t\tasoc->rwnd_press += asoc->rwnd;\n\t\t\tasoc->rwnd = 0;\n\t\t}\n\t} else {\n\t\tasoc->rwnd_over = len - asoc->rwnd;\n\t\tasoc->rwnd = 0;\n\t}\n\n\tpr_debug(\"%s: asoc:%p rwnd decreased by %d to (%u, %u, %u)\\n\",\n\t\t __func__, asoc, len, asoc->rwnd, asoc->rwnd_over,\n\t\t asoc->rwnd_press);\n}\n\n/* Build the bind address list for the association based on info from the\n * local endpoint and the remote peer.\n */\nint sctp_assoc_set_bind_addr_from_ep(struct sctp_association *asoc,\n\t\t\t\t     sctp_scope_t scope, gfp_t gfp)\n{\n\tint flags;\n\n\t/* Use scoping rules to determine the subset of addresses from\n\t * the endpoint.\n\t */\n\tflags = (PF_INET6 == asoc->base.sk->sk_family) ? SCTP_ADDR6_ALLOWED : 0;\n\tif (asoc->peer.ipv4_address)\n\t\tflags |= SCTP_ADDR4_PEERSUPP;\n\tif (asoc->peer.ipv6_address)\n\t\tflags |= SCTP_ADDR6_PEERSUPP;\n\n\treturn sctp_bind_addr_copy(sock_net(asoc->base.sk),\n\t\t\t\t   &asoc->base.bind_addr,\n\t\t\t\t   &asoc->ep->base.bind_addr,\n\t\t\t\t   scope, gfp, flags);\n}\n\n/* Build the association's bind address list from the cookie.  */\nint sctp_assoc_set_bind_addr_from_cookie(struct sctp_association *asoc,\n\t\t\t\t\t struct sctp_cookie *cookie,\n\t\t\t\t\t gfp_t gfp)\n{\n\tint var_size2 = ntohs(cookie->peer_init->chunk_hdr.length);\n\tint var_size3 = cookie->raw_addr_list_len;\n\t__u8 *raw = (__u8 *)cookie->peer_init + var_size2;\n\n\treturn sctp_raw_to_bind_addrs(&asoc->base.bind_addr, raw, var_size3,\n\t\t\t\t      asoc->ep->base.bind_addr.port, gfp);\n}\n\n/* Lookup laddr in the bind address list of an association. */\nint sctp_assoc_lookup_laddr(struct sctp_association *asoc,\n\t\t\t    const union sctp_addr *laddr)\n{\n\tint found = 0;\n\n\tif ((asoc->base.bind_addr.port == ntohs(laddr->v4.sin_port)) &&\n\t    sctp_bind_addr_match(&asoc->base.bind_addr, laddr,\n\t\t\t\t sctp_sk(asoc->base.sk)))\n\t\tfound = 1;\n\n\treturn found;\n}\n\n/* Set an association id for a given association */\nint sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)\n{\n\tbool preload = !!(gfp & __GFP_WAIT);\n\tint ret;\n\n\t/* If the id is already assigned, keep it. */\n\tif (asoc->assoc_id)\n\t\treturn 0;\n\n\tif (preload)\n\t\tidr_preload(gfp);\n\tspin_lock_bh(&sctp_assocs_id_lock);\n\t/* 0 is not a valid assoc_id, must be >= 1 */\n\tret = idr_alloc_cyclic(&sctp_assocs_id, asoc, 1, 0, GFP_NOWAIT);\n\tspin_unlock_bh(&sctp_assocs_id_lock);\n\tif (preload)\n\t\tidr_preload_end();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tasoc->assoc_id = (sctp_assoc_t)ret;\n\treturn 0;\n}\n\n/* Free the ASCONF queue */\nstatic void sctp_assoc_free_asconf_queue(struct sctp_association *asoc)\n{\n\tstruct sctp_chunk *asconf;\n\tstruct sctp_chunk *tmp;\n\n\tlist_for_each_entry_safe(asconf, tmp, &asoc->addip_chunk_list, list) {\n\t\tlist_del_init(&asconf->list);\n\t\tsctp_chunk_free(asconf);\n\t}\n}\n\n/* Free asconf_ack cache */\nstatic void sctp_assoc_free_asconf_acks(struct sctp_association *asoc)\n{\n\tstruct sctp_chunk *ack;\n\tstruct sctp_chunk *tmp;\n\n\tlist_for_each_entry_safe(ack, tmp, &asoc->asconf_ack_list,\n\t\t\t\ttransmitted_list) {\n\t\tlist_del_init(&ack->transmitted_list);\n\t\tsctp_chunk_free(ack);\n\t}\n}\n\n/* Clean up the ASCONF_ACK queue */\nvoid sctp_assoc_clean_asconf_ack_cache(const struct sctp_association *asoc)\n{\n\tstruct sctp_chunk *ack;\n\tstruct sctp_chunk *tmp;\n\n\t/* We can remove all the entries from the queue up to\n\t * the \"Peer-Sequence-Number\".\n\t */\n\tlist_for_each_entry_safe(ack, tmp, &asoc->asconf_ack_list,\n\t\t\t\ttransmitted_list) {\n\t\tif (ack->subh.addip_hdr->serial ==\n\t\t\t\thtonl(asoc->peer.addip_serial))\n\t\t\tbreak;\n\n\t\tlist_del_init(&ack->transmitted_list);\n\t\tsctp_chunk_free(ack);\n\t}\n}\n\n/* Find the ASCONF_ACK whose serial number matches ASCONF */\nstruct sctp_chunk *sctp_assoc_lookup_asconf_ack(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\t__be32 serial)\n{\n\tstruct sctp_chunk *ack;\n\n\t/* Walk through the list of cached ASCONF-ACKs and find the\n\t * ack chunk whose serial number matches that of the request.\n\t */\n\tlist_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {\n\t\tif (ack->subh.addip_hdr->serial == serial) {\n\t\t\tsctp_chunk_hold(ack);\n\t\t\treturn ack;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nvoid sctp_asconf_queue_teardown(struct sctp_association *asoc)\n{\n\t/* Free any cached ASCONF_ACK chunk. */\n\tsctp_assoc_free_asconf_acks(asoc);\n\n\t/* Free the ASCONF queue. */\n\tsctp_assoc_free_asconf_queue(asoc);\n\n\t/* Free any cached ASCONF chunk. */\n\tif (asoc->addip_last_asconf)\n\t\tsctp_chunk_free(asoc->addip_last_asconf);\n}\n"], "fixing_code": ["/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2001, 2004\n * Copyright (c) 1999-2000 Cisco, Inc.\n * Copyright (c) 1999-2001 Motorola, Inc.\n * Copyright (c) 2001-2003 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * The base lksctp header.\n *\n * This SCTP implementation is free software;\n * you can redistribute it and/or modify it under the terms of\n * the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n * This SCTP implementation is distributed in the hope that it\n * will be useful, but WITHOUT ANY WARRANTY; without even the implied\n *                 ************************\n * warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n * See the GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with GNU CC; see the file COPYING.  If not, see\n * <http://www.gnu.org/licenses/>.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    La Monte H.P. Yarroll <piggy@acm.org>\n *    Xingang Guo           <xingang.guo@intel.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Daisy Chang           <daisyc@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Kevin Gao             <kevin.gao@intel.com> \n */\n\n#ifndef __net_sctp_h__\n#define __net_sctp_h__\n\n/* Header Strategy.\n *    Start getting some control over the header file depencies:\n *       includes\n *       constants\n *       structs\n *       prototypes\n *       macros, externs, and inlines\n *\n *   Move test_frame specific items out of the kernel headers\n *   and into the test frame headers.   This is not perfect in any sense\n *   and will continue to evolve.\n */\n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/in.h>\n#include <linux/tty.h>\n#include <linux/proc_fs.h>\n#include <linux/spinlock.h>\n#include <linux/jiffies.h>\n#include <linux/idr.h>\n\n#if IS_ENABLED(CONFIG_IPV6)\n#include <net/ipv6.h>\n#include <net/ip6_route.h>\n#endif\n\n#include <asm/uaccess.h>\n#include <asm/page.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n#include <net/sctp/structs.h>\n#include <net/sctp/constants.h>\n\n#ifdef CONFIG_IP_SCTP_MODULE\n#define SCTP_PROTOSW_FLAG 0\n#else /* static! */\n#define SCTP_PROTOSW_FLAG INET_PROTOSW_PERMANENT\n#endif\n\n/*\n * Function declarations.\n */\n\n/*\n * sctp/protocol.c\n */\nint sctp_copy_local_addr_list(struct net *, struct sctp_bind_addr *,\n\t\t\t      sctp_scope_t, gfp_t gfp, int flags);\nstruct sctp_pf *sctp_get_pf_specific(sa_family_t family);\nint sctp_register_pf(struct sctp_pf *, sa_family_t);\nvoid sctp_addr_wq_mgmt(struct net *, struct sctp_sockaddr_entry *, int);\n\n/*\n * sctp/socket.c\n */\nint sctp_backlog_rcv(struct sock *sk, struct sk_buff *skb);\nint sctp_inet_listen(struct socket *sock, int backlog);\nvoid sctp_write_space(struct sock *sk);\nvoid sctp_data_ready(struct sock *sk);\nunsigned int sctp_poll(struct file *file, struct socket *sock,\n\t\tpoll_table *wait);\nvoid sctp_sock_rfree(struct sk_buff *skb);\nvoid sctp_copy_sock(struct sock *newsk, struct sock *sk,\n\t\t    struct sctp_association *asoc);\nextern struct percpu_counter sctp_sockets_allocated;\nint sctp_asconf_mgmt(struct sctp_sock *, struct sctp_sockaddr_entry *);\nstruct sk_buff *sctp_skb_recv_datagram(struct sock *, int, int, int *);\n\n/*\n * sctp/primitive.c\n */\nint sctp_primitive_ASSOCIATE(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_SHUTDOWN(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_ABORT(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_SEND(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_REQUESTHEARTBEAT(struct net *, struct sctp_association *, void *arg);\nint sctp_primitive_ASCONF(struct net *, struct sctp_association *, void *arg);\n\n/*\n * sctp/input.c\n */\nint sctp_rcv(struct sk_buff *skb);\nvoid sctp_v4_err(struct sk_buff *skb, u32 info);\nvoid sctp_hash_established(struct sctp_association *);\nvoid sctp_unhash_established(struct sctp_association *);\nvoid sctp_hash_endpoint(struct sctp_endpoint *);\nvoid sctp_unhash_endpoint(struct sctp_endpoint *);\nstruct sock *sctp_err_lookup(struct net *net, int family, struct sk_buff *,\n\t\t\t     struct sctphdr *, struct sctp_association **,\n\t\t\t     struct sctp_transport **);\nvoid sctp_err_finish(struct sock *, struct sctp_association *);\nvoid sctp_icmp_frag_needed(struct sock *, struct sctp_association *,\n\t\t\t   struct sctp_transport *t, __u32 pmtu);\nvoid sctp_icmp_redirect(struct sock *, struct sctp_transport *,\n\t\t\tstruct sk_buff *);\nvoid sctp_icmp_proto_unreachable(struct sock *sk,\n\t\t\t\t struct sctp_association *asoc,\n\t\t\t\t struct sctp_transport *t);\nvoid sctp_backlog_migrate(struct sctp_association *assoc,\n\t\t\t  struct sock *oldsk, struct sock *newsk);\n\n/*\n * sctp/proc.c\n */\nint sctp_snmp_proc_init(struct net *net);\nvoid sctp_snmp_proc_exit(struct net *net);\nint sctp_eps_proc_init(struct net *net);\nvoid sctp_eps_proc_exit(struct net *net);\nint sctp_assocs_proc_init(struct net *net);\nvoid sctp_assocs_proc_exit(struct net *net);\nint sctp_remaddr_proc_init(struct net *net);\nvoid sctp_remaddr_proc_exit(struct net *net);\n\n\n/*\n * Module global variables\n */\n\n /*\n  * sctp/protocol.c\n  */\nextern struct kmem_cache *sctp_chunk_cachep __read_mostly;\nextern struct kmem_cache *sctp_bucket_cachep __read_mostly;\n\n/*\n *  Section:  Macros, externs, and inlines\n */\n\n/* SCTP SNMP MIB stats handlers */\n#define SCTP_INC_STATS(net, field)      SNMP_INC_STATS((net)->sctp.sctp_statistics, field)\n#define SCTP_INC_STATS_BH(net, field)   SNMP_INC_STATS_BH((net)->sctp.sctp_statistics, field)\n#define SCTP_INC_STATS_USER(net, field) SNMP_INC_STATS_USER((net)->sctp.sctp_statistics, field)\n#define SCTP_DEC_STATS(net, field)      SNMP_DEC_STATS((net)->sctp.sctp_statistics, field)\n\n/* sctp mib definitions */\nenum {\n\tSCTP_MIB_NUM = 0,\n\tSCTP_MIB_CURRESTAB,\t\t\t/* CurrEstab */\n\tSCTP_MIB_ACTIVEESTABS,\t\t\t/* ActiveEstabs */\n\tSCTP_MIB_PASSIVEESTABS,\t\t\t/* PassiveEstabs */\n\tSCTP_MIB_ABORTEDS,\t\t\t/* Aborteds */\n\tSCTP_MIB_SHUTDOWNS,\t\t\t/* Shutdowns */\n\tSCTP_MIB_OUTOFBLUES,\t\t\t/* OutOfBlues */\n\tSCTP_MIB_CHECKSUMERRORS,\t\t/* ChecksumErrors */\n\tSCTP_MIB_OUTCTRLCHUNKS,\t\t\t/* OutCtrlChunks */\n\tSCTP_MIB_OUTORDERCHUNKS,\t\t/* OutOrderChunks */\n\tSCTP_MIB_OUTUNORDERCHUNKS,\t\t/* OutUnorderChunks */\n\tSCTP_MIB_INCTRLCHUNKS,\t\t\t/* InCtrlChunks */\n\tSCTP_MIB_INORDERCHUNKS,\t\t\t/* InOrderChunks */\n\tSCTP_MIB_INUNORDERCHUNKS,\t\t/* InUnorderChunks */\n\tSCTP_MIB_FRAGUSRMSGS,\t\t\t/* FragUsrMsgs */\n\tSCTP_MIB_REASMUSRMSGS,\t\t\t/* ReasmUsrMsgs */\n\tSCTP_MIB_OUTSCTPPACKS,\t\t\t/* OutSCTPPacks */\n\tSCTP_MIB_INSCTPPACKS,\t\t\t/* InSCTPPacks */\n\tSCTP_MIB_T1_INIT_EXPIREDS,\n\tSCTP_MIB_T1_COOKIE_EXPIREDS,\n\tSCTP_MIB_T2_SHUTDOWN_EXPIREDS,\n\tSCTP_MIB_T3_RTX_EXPIREDS,\n\tSCTP_MIB_T4_RTO_EXPIREDS,\n\tSCTP_MIB_T5_SHUTDOWN_GUARD_EXPIREDS,\n\tSCTP_MIB_DELAY_SACK_EXPIREDS,\n\tSCTP_MIB_AUTOCLOSE_EXPIREDS,\n\tSCTP_MIB_T1_RETRANSMITS,\n\tSCTP_MIB_T3_RETRANSMITS,\n\tSCTP_MIB_PMTUD_RETRANSMITS,\n\tSCTP_MIB_FAST_RETRANSMITS,\n\tSCTP_MIB_IN_PKT_SOFTIRQ,\n\tSCTP_MIB_IN_PKT_BACKLOG,\n\tSCTP_MIB_IN_PKT_DISCARDS,\n\tSCTP_MIB_IN_DATA_CHUNK_DISCARDS,\n\t__SCTP_MIB_MAX\n};\n\n#define SCTP_MIB_MAX    __SCTP_MIB_MAX\nstruct sctp_mib {\n        unsigned long   mibs[SCTP_MIB_MAX];\n};\n\n/* helper function to track stats about max rto and related transport */\nstatic inline void sctp_max_rto(struct sctp_association *asoc,\n\t\t\t\tstruct sctp_transport *trans)\n{\n\tif (asoc->stats.max_obs_rto < (__u64)trans->rto) {\n\t\tasoc->stats.max_obs_rto = trans->rto;\n\t\tmemset(&asoc->stats.obs_rto_ipaddr, 0,\n\t\t\tsizeof(struct sockaddr_storage));\n\t\tmemcpy(&asoc->stats.obs_rto_ipaddr, &trans->ipaddr,\n\t\t\ttrans->af_specific->sockaddr_len);\n\t}\n}\n\n/*\n * Macros for keeping a global reference of object allocations.\n */\n#ifdef CONFIG_SCTP_DBG_OBJCNT\n\nextern atomic_t sctp_dbg_objcnt_sock;\nextern atomic_t sctp_dbg_objcnt_ep;\nextern atomic_t sctp_dbg_objcnt_assoc;\nextern atomic_t sctp_dbg_objcnt_transport;\nextern atomic_t sctp_dbg_objcnt_chunk;\nextern atomic_t sctp_dbg_objcnt_bind_addr;\nextern atomic_t sctp_dbg_objcnt_bind_bucket;\nextern atomic_t sctp_dbg_objcnt_addr;\nextern atomic_t sctp_dbg_objcnt_ssnmap;\nextern atomic_t sctp_dbg_objcnt_datamsg;\nextern atomic_t sctp_dbg_objcnt_keys;\n\n/* Macros to atomically increment/decrement objcnt counters.  */\n#define SCTP_DBG_OBJCNT_INC(name) \\\natomic_inc(&sctp_dbg_objcnt_## name)\n#define SCTP_DBG_OBJCNT_DEC(name) \\\natomic_dec(&sctp_dbg_objcnt_## name)\n#define SCTP_DBG_OBJCNT(name) \\\natomic_t sctp_dbg_objcnt_## name = ATOMIC_INIT(0)\n\n/* Macro to help create new entries in in the global array of\n * objcnt counters.\n */\n#define SCTP_DBG_OBJCNT_ENTRY(name) \\\n{.label= #name, .counter= &sctp_dbg_objcnt_## name}\n\nvoid sctp_dbg_objcnt_init(struct net *);\nvoid sctp_dbg_objcnt_exit(struct net *);\n\n#else\n\n#define SCTP_DBG_OBJCNT_INC(name)\n#define SCTP_DBG_OBJCNT_DEC(name)\n\nstatic inline void sctp_dbg_objcnt_init(struct net *net) { return; }\nstatic inline void sctp_dbg_objcnt_exit(struct net *net) { return; }\n\n#endif /* CONFIG_SCTP_DBG_OBJCOUNT */\n\n#if defined CONFIG_SYSCTL\nvoid sctp_sysctl_register(void);\nvoid sctp_sysctl_unregister(void);\nint sctp_sysctl_net_register(struct net *net);\nvoid sctp_sysctl_net_unregister(struct net *net);\n#else\nstatic inline void sctp_sysctl_register(void) { return; }\nstatic inline void sctp_sysctl_unregister(void) { return; }\nstatic inline int sctp_sysctl_net_register(struct net *net) { return 0; }\nstatic inline void sctp_sysctl_net_unregister(struct net *net) { return; }\n#endif\n\n/* Size of Supported Address Parameter for 'x' address types. */\n#define SCTP_SAT_LEN(x) (sizeof(struct sctp_paramhdr) + (x) * sizeof(__u16))\n\n#if IS_ENABLED(CONFIG_IPV6)\n\nvoid sctp_v6_pf_init(void);\nvoid sctp_v6_pf_exit(void);\nint sctp_v6_protosw_init(void);\nvoid sctp_v6_protosw_exit(void);\nint sctp_v6_add_protocol(void);\nvoid sctp_v6_del_protocol(void);\n\n#else /* #ifdef defined(CONFIG_IPV6) */\n\nstatic inline void sctp_v6_pf_init(void) { return; }\nstatic inline void sctp_v6_pf_exit(void) { return; }\nstatic inline int sctp_v6_protosw_init(void) { return 0; }\nstatic inline void sctp_v6_protosw_exit(void) { return; }\nstatic inline int sctp_v6_add_protocol(void) { return 0; }\nstatic inline void sctp_v6_del_protocol(void) { return; }\n\n#endif /* #if defined(CONFIG_IPV6) */\n\n\n/* Map an association to an assoc_id. */\nstatic inline sctp_assoc_t sctp_assoc2id(const struct sctp_association *asoc)\n{\n\treturn asoc ? asoc->assoc_id : 0;\n}\n\nstatic inline enum sctp_sstat_state\nsctp_assoc_to_state(const struct sctp_association *asoc)\n{\n\t/* SCTP's uapi always had SCTP_EMPTY(=0) as a dummy state, but we\n\t * got rid of it in kernel space. Therefore SCTP_CLOSED et al\n\t * start at =1 in user space, but actually as =0 in kernel space.\n\t * Now that we can not break user space and SCTP_EMPTY is exposed\n\t * there, we need to fix it up with an ugly offset not to break\n\t * applications. :(\n\t */\n\treturn asoc->state + 1;\n}\n\n/* Look up the association by its id.  */\nstruct sctp_association *sctp_id2assoc(struct sock *sk, sctp_assoc_t id);\n\nint sctp_do_peeloff(struct sock *sk, sctp_assoc_t id, struct socket **sockp);\n\n/* A macro to walk a list of skbs.  */\n#define sctp_skb_for_each(pos, head, tmp) \\\n\tskb_queue_walk_safe(head, pos, tmp)\n\n/* A helper to append an entire skb list (list) to another (head). */\nstatic inline void sctp_skb_list_tail(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff_head *head)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&head->lock, flags);\n\tspin_lock(&list->lock);\n\n\tskb_queue_splice_tail_init(list, head);\n\n\tspin_unlock(&list->lock);\n\tspin_unlock_irqrestore(&head->lock, flags);\n}\n\n/**\n *\tsctp_list_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. The head item is\n *\treturned or %NULL if the list is empty.\n */\n\nstatic inline struct list_head *sctp_list_dequeue(struct list_head *list)\n{\n\tstruct list_head *result = NULL;\n\n\tif (list->next != list) {\n\t\tresult = list->next;\n\t\tlist->next = result->next;\n\t\tlist->next->prev = list;\n\t\tINIT_LIST_HEAD(result);\n\t}\n\treturn result;\n}\n\n/* SCTP version of skb_set_owner_r.  We need this one because\n * of the way we have to do receive buffer accounting on bundled\n * chunks.\n */\nstatic inline void sctp_skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\n{\n\tstruct sctp_ulpevent *event = sctp_skb2event(skb);\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sctp_sock_rfree;\n\tatomic_add(event->rmem_len, &sk->sk_rmem_alloc);\n\t/*\n\t * This mimics the behavior of skb_set_owner_r\n\t */\n\tsk->sk_forward_alloc -= event->rmem_len;\n}\n\n/* Tests if the list has one and only one entry. */\nstatic inline int sctp_list_single_entry(struct list_head *head)\n{\n\treturn (head->next != head) && (head->next == head->prev);\n}\n\n/* Break down data chunks at this point.  */\nstatic inline int sctp_frag_point(const struct sctp_association *asoc, int pmtu)\n{\n\tstruct sctp_sock *sp = sctp_sk(asoc->base.sk);\n\tint frag = pmtu;\n\n\tfrag -= sp->pf->af->net_header_len;\n\tfrag -= sizeof(struct sctphdr) + sizeof(struct sctp_data_chunk);\n\n\tif (asoc->user_frag)\n\t\tfrag = min_t(int, frag, asoc->user_frag);\n\n\tfrag = min_t(int, frag, SCTP_MAX_CHUNK_LEN);\n\n\treturn frag;\n}\n\nstatic inline void sctp_assoc_pending_pmtu(struct sock *sk, struct sctp_association *asoc)\n{\n\n\tsctp_assoc_sync_pmtu(sk, asoc);\n\tasoc->pmtu_pending = 0;\n}\n\nstatic inline bool sctp_chunk_pending(const struct sctp_chunk *chunk)\n{\n\treturn !list_empty(&chunk->list);\n}\n\n/* Walk through a list of TLV parameters.  Don't trust the\n * individual parameter lengths and instead depend on\n * the chunk length to indicate when to stop.  Make sure\n * there is room for a param header too.\n */\n#define sctp_walk_params(pos, chunk, member)\\\n_sctp_walk_params((pos), (chunk), ntohs((chunk)->chunk_hdr.length), member)\n\n#define _sctp_walk_params(pos, chunk, end, member)\\\nfor (pos.v = chunk->member;\\\n     pos.v <= (void *)chunk + end - ntohs(pos.p->length) &&\\\n     ntohs(pos.p->length) >= sizeof(sctp_paramhdr_t);\\\n     pos.v += WORD_ROUND(ntohs(pos.p->length)))\n\n#define sctp_walk_errors(err, chunk_hdr)\\\n_sctp_walk_errors((err), (chunk_hdr), ntohs((chunk_hdr)->length))\n\n#define _sctp_walk_errors(err, chunk_hdr, end)\\\nfor (err = (sctp_errhdr_t *)((void *)chunk_hdr + \\\n\t    sizeof(sctp_chunkhdr_t));\\\n     (void *)err <= (void *)chunk_hdr + end - ntohs(err->length) &&\\\n     ntohs(err->length) >= sizeof(sctp_errhdr_t); \\\n     err = (sctp_errhdr_t *)((void *)err + WORD_ROUND(ntohs(err->length))))\n\n#define sctp_walk_fwdtsn(pos, chunk)\\\n_sctp_walk_fwdtsn((pos), (chunk), ntohs((chunk)->chunk_hdr->length) - sizeof(struct sctp_fwdtsn_chunk))\n\n#define _sctp_walk_fwdtsn(pos, chunk, end)\\\nfor (pos = chunk->subh.fwdtsn_hdr->skip;\\\n     (void *)pos <= (void *)chunk->subh.fwdtsn_hdr->skip + end - sizeof(struct sctp_fwdtsn_skip);\\\n     pos++)\n\n/* Round an int up to the next multiple of 4.  */\n#define WORD_ROUND(s) (((s)+3)&~3)\n\n/* External references. */\n\nextern struct proto sctp_prot;\nextern struct proto sctpv6_prot;\nvoid sctp_put_port(struct sock *sk);\n\nextern struct idr sctp_assocs_id;\nextern spinlock_t sctp_assocs_id_lock;\n\n/* Static inline functions. */\n\n/* Convert from an IP version number to an Address Family symbol.  */\nstatic inline int ipver2af(__u8 ipver)\n{\n\tswitch (ipver) {\n\tcase 4:\n\t        return  AF_INET;\n\tcase 6:\n\t\treturn AF_INET6;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n/* Convert from an address parameter type to an address family.  */\nstatic inline int param_type2af(__be16 type)\n{\n\tswitch (type) {\n\tcase SCTP_PARAM_IPV4_ADDRESS:\n\t        return  AF_INET;\n\tcase SCTP_PARAM_IPV6_ADDRESS:\n\t\treturn AF_INET6;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n/* Warning: The following hash functions assume a power of two 'size'. */\n/* This is the hash function for the SCTP port hash table. */\nstatic inline int sctp_phashfn(struct net *net, __u16 lport)\n{\n\treturn (net_hash_mix(net) + lport) & (sctp_port_hashsize - 1);\n}\n\n/* This is the hash function for the endpoint hash table. */\nstatic inline int sctp_ep_hashfn(struct net *net, __u16 lport)\n{\n\treturn (net_hash_mix(net) + lport) & (sctp_ep_hashsize - 1);\n}\n\n/* This is the hash function for the association hash table. */\nstatic inline int sctp_assoc_hashfn(struct net *net, __u16 lport, __u16 rport)\n{\n\tint h = (lport << 16) + rport + net_hash_mix(net);\n\th ^= h>>8;\n\treturn h & (sctp_assoc_hashsize - 1);\n}\n\n/* This is the hash function for the association hash table.  This is\n * not used yet, but could be used as a better hash function when\n * we have a vtag.\n */\nstatic inline int sctp_vtag_hashfn(__u16 lport, __u16 rport, __u32 vtag)\n{\n\tint h = (lport << 16) + rport;\n\th ^= vtag;\n\treturn h & (sctp_assoc_hashsize - 1);\n}\n\n#define sctp_for_each_hentry(epb, head) \\\n\thlist_for_each_entry(epb, head, node)\n\n/* Is a socket of this style? */\n#define sctp_style(sk, style) __sctp_style((sk), (SCTP_SOCKET_##style))\nstatic inline int __sctp_style(const struct sock *sk, sctp_socket_type_t style)\n{\n\treturn sctp_sk(sk)->type == style;\n}\n\n/* Is the association in this state? */\n#define sctp_state(asoc, state) __sctp_state((asoc), (SCTP_STATE_##state))\nstatic inline int __sctp_state(const struct sctp_association *asoc,\n\t\t\t       sctp_state_t state)\n{\n\treturn asoc->state == state;\n}\n\n/* Is the socket in this state? */\n#define sctp_sstate(sk, state) __sctp_sstate((sk), (SCTP_SS_##state))\nstatic inline int __sctp_sstate(const struct sock *sk, sctp_sock_state_t state)\n{\n\treturn sk->sk_state == state;\n}\n\n/* Map v4-mapped v6 address back to v4 address */\nstatic inline void sctp_v6_map_v4(union sctp_addr *addr)\n{\n\taddr->v4.sin_family = AF_INET;\n\taddr->v4.sin_port = addr->v6.sin6_port;\n\taddr->v4.sin_addr.s_addr = addr->v6.sin6_addr.s6_addr32[3];\n}\n\n/* Map v4 address to v4-mapped v6 address */\nstatic inline void sctp_v4_map_v6(union sctp_addr *addr)\n{\n\taddr->v6.sin6_family = AF_INET6;\n\taddr->v6.sin6_flowinfo = 0;\n\taddr->v6.sin6_scope_id = 0;\n\taddr->v6.sin6_port = addr->v4.sin_port;\n\taddr->v6.sin6_addr.s6_addr32[3] = addr->v4.sin_addr.s_addr;\n\taddr->v6.sin6_addr.s6_addr32[0] = 0;\n\taddr->v6.sin6_addr.s6_addr32[1] = 0;\n\taddr->v6.sin6_addr.s6_addr32[2] = htonl(0x0000ffff);\n}\n\n/* The cookie is always 0 since this is how it's used in the\n * pmtu code.\n */\nstatic inline struct dst_entry *sctp_transport_dst_check(struct sctp_transport *t)\n{\n\tif (t->dst && !dst_check(t->dst, t->dst_cookie)) {\n\t\tdst_release(t->dst);\n\t\tt->dst = NULL;\n\t}\n\n\treturn t->dst;\n}\n\n#endif /* __net_sctp_h__ */\n", "/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2001, 2004\n * Copyright (c) 1999-2000 Cisco, Inc.\n * Copyright (c) 1999-2001 Motorola, Inc.\n * Copyright (c) 2001 Intel Corp.\n * Copyright (c) 2001 La Monte H.P. Yarroll\n *\n * This file is part of the SCTP kernel implementation\n *\n * This module provides the abstraction for an SCTP association.\n *\n * This SCTP implementation is free software;\n * you can redistribute it and/or modify it under the terms of\n * the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n * This SCTP implementation is distributed in the hope that it\n * will be useful, but WITHOUT ANY WARRANTY; without even the implied\n *                 ************************\n * warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n * See the GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with GNU CC; see the file COPYING.  If not, see\n * <http://www.gnu.org/licenses/>.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    La Monte H.P. Yarroll <piggy@acm.org>\n *    Karl Knutson          <karl@athena.chicago.il.us>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Xingang Guo           <xingang.guo@intel.com>\n *    Hui Huang             <hui.huang@nokia.com>\n *    Sridhar Samudrala\t    <sri@us.ibm.com>\n *    Daisy Chang\t    <daisyc@us.ibm.com>\n *    Ryan Layer\t    <rmlayer@us.ibm.com>\n *    Kevin Gao             <kevin.gao@intel.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n\n#include <linux/slab.h>\n#include <linux/in.h>\n#include <net/ipv6.h>\n#include <net/sctp/sctp.h>\n#include <net/sctp/sm.h>\n\n/* Forward declarations for internal functions. */\nstatic void sctp_select_active_and_retran_path(struct sctp_association *asoc);\nstatic void sctp_assoc_bh_rcv(struct work_struct *work);\nstatic void sctp_assoc_free_asconf_acks(struct sctp_association *asoc);\nstatic void sctp_assoc_free_asconf_queue(struct sctp_association *asoc);\n\n/* 1st Level Abstractions. */\n\n/* Initialize a new association from provided memory. */\nstatic struct sctp_association *sctp_association_init(struct sctp_association *asoc,\n\t\t\t\t\t  const struct sctp_endpoint *ep,\n\t\t\t\t\t  const struct sock *sk,\n\t\t\t\t\t  sctp_scope_t scope,\n\t\t\t\t\t  gfp_t gfp)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\tint i;\n\tsctp_paramhdr_t *p;\n\tint err;\n\n\t/* Retrieve the SCTP per socket area.  */\n\tsp = sctp_sk((struct sock *)sk);\n\n\t/* Discarding const is appropriate here.  */\n\tasoc->ep = (struct sctp_endpoint *)ep;\n\tasoc->base.sk = (struct sock *)sk;\n\n\tsctp_endpoint_hold(asoc->ep);\n\tsock_hold(asoc->base.sk);\n\n\t/* Initialize the common base substructure.  */\n\tasoc->base.type = SCTP_EP_TYPE_ASSOCIATION;\n\n\t/* Initialize the object handling fields.  */\n\tatomic_set(&asoc->base.refcnt, 1);\n\n\t/* Initialize the bind addr area.  */\n\tsctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port);\n\n\tasoc->state = SCTP_STATE_CLOSED;\n\tasoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life);\n\tasoc->user_frag = sp->user_frag;\n\n\t/* Set the association max_retrans and RTO values from the\n\t * socket values.\n\t */\n\tasoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt;\n\tasoc->pf_retrans  = net->sctp.pf_retrans;\n\n\tasoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial);\n\tasoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max);\n\tasoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min);\n\n\t/* Initialize the association's heartbeat interval based on the\n\t * sock configured value.\n\t */\n\tasoc->hbinterval = msecs_to_jiffies(sp->hbinterval);\n\n\t/* Initialize path max retrans value. */\n\tasoc->pathmaxrxt = sp->pathmaxrxt;\n\n\t/* Initialize default path MTU. */\n\tasoc->pathmtu = sp->pathmtu;\n\n\t/* Set association default SACK delay */\n\tasoc->sackdelay = msecs_to_jiffies(sp->sackdelay);\n\tasoc->sackfreq = sp->sackfreq;\n\n\t/* Set the association default flags controlling\n\t * Heartbeat, SACK delay, and Path MTU Discovery.\n\t */\n\tasoc->param_flags = sp->param_flags;\n\n\t/* Initialize the maximum number of new data packets that can be sent\n\t * in a burst.\n\t */\n\tasoc->max_burst = sp->max_burst;\n\n\t/* initialize association timers */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial;\n\n\t/* sctpimpguide Section 2.12.2\n\t * If the 'T5-shutdown-guard' timer is used, it SHOULD be set to the\n\t * recommended value of 5 times 'RTO.Max'.\n\t */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD]\n\t\t= 5 * asoc->rto_max;\n\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;\n\n\t/* Initializes the timers */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i)\n\t\tsetup_timer(&asoc->timers[i], sctp_timer_events[i],\n\t\t\t\t(unsigned long)asoc);\n\n\t/* Pull default initialization values from the sock options.\n\t * Note: This assumes that the values have already been\n\t * validated in the sock.\n\t */\n\tasoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams;\n\tasoc->c.sinit_num_ostreams  = sp->initmsg.sinit_num_ostreams;\n\tasoc->max_init_attempts\t= sp->initmsg.sinit_max_attempts;\n\n\tasoc->max_init_timeo =\n\t\t msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo);\n\n\t/* Set the local window size for receive.\n\t * This is also the rcvbuf space per association.\n\t * RFC 6 - A SCTP receiver MUST be able to receive a minimum of\n\t * 1500 bytes in one SCTP packet.\n\t */\n\tif ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW)\n\t\tasoc->rwnd = SCTP_DEFAULT_MINWINDOW;\n\telse\n\t\tasoc->rwnd = sk->sk_rcvbuf/2;\n\n\tasoc->a_rwnd = asoc->rwnd;\n\n\t/* Use my own max window until I learn something better.  */\n\tasoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW;\n\n\t/* Initialize the receive memory counter */\n\tatomic_set(&asoc->rmem_alloc, 0);\n\n\tinit_waitqueue_head(&asoc->wait);\n\n\tasoc->c.my_vtag = sctp_generate_tag(ep);\n\tasoc->c.my_port = ep->base.bind_addr.port;\n\n\tasoc->c.initial_tsn = sctp_generate_tsn(ep);\n\n\tasoc->next_tsn = asoc->c.initial_tsn;\n\n\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\tasoc->highest_sacked = asoc->ctsn_ack_point;\n\tasoc->last_cwr_tsn = asoc->ctsn_ack_point;\n\n\t/* ADDIP Section 4.1 Asconf Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) a serial number should be assigned to the chunk. The serial\n\t * number SHOULD be a monotonically increasing number. The serial\n\t * numbers SHOULD be initialized at the start of the\n\t * association to the same value as the initial TSN.\n\t */\n\tasoc->addip_serial = asoc->c.initial_tsn;\n\n\tINIT_LIST_HEAD(&asoc->addip_chunk_list);\n\tINIT_LIST_HEAD(&asoc->asconf_ack_list);\n\n\t/* Make an empty list of remote transport addresses.  */\n\tINIT_LIST_HEAD(&asoc->peer.transport_addr_list);\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * After the reception of the first data chunk in an\n\t * association the endpoint must immediately respond with a\n\t * sack to acknowledge the data chunk.  Subsequent\n\t * acknowledgements should be done as described in Section\n\t * 6.2.\n\t *\n\t * [We implement this by telling a new association that it\n\t * already received one packet.]\n\t */\n\tasoc->peer.sack_needed = 1;\n\tasoc->peer.sack_generation = 1;\n\n\t/* Assume that the peer will tell us if he recognizes ASCONF\n\t * as part of INIT exchange.\n\t * The sctp_addip_noauth option is there for backward compatibility\n\t * and will revert old behavior.\n\t */\n\tif (net->sctp.addip_noauth)\n\t\tasoc->peer.asconf_capable = 1;\n\n\t/* Create an input queue.  */\n\tsctp_inq_init(&asoc->base.inqueue);\n\tsctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv);\n\n\t/* Create an output queue.  */\n\tsctp_outq_init(asoc, &asoc->outqueue);\n\n\tif (!sctp_ulpq_init(&asoc->ulpq, asoc))\n\t\tgoto fail_init;\n\n\t/* Assume that peer would support both address types unless we are\n\t * told otherwise.\n\t */\n\tasoc->peer.ipv4_address = 1;\n\tif (asoc->base.sk->sk_family == PF_INET6)\n\t\tasoc->peer.ipv6_address = 1;\n\tINIT_LIST_HEAD(&asoc->asocs);\n\n\tasoc->default_stream = sp->default_stream;\n\tasoc->default_ppid = sp->default_ppid;\n\tasoc->default_flags = sp->default_flags;\n\tasoc->default_context = sp->default_context;\n\tasoc->default_timetolive = sp->default_timetolive;\n\tasoc->default_rcv_context = sp->default_rcv_context;\n\n\t/* AUTH related initializations */\n\tINIT_LIST_HEAD(&asoc->endpoint_shared_keys);\n\terr = sctp_auth_asoc_copy_shkeys(ep, asoc, gfp);\n\tif (err)\n\t\tgoto fail_init;\n\n\tasoc->active_key_id = ep->active_key_id;\n\n\t/* Save the hmacs and chunks list into this association */\n\tif (ep->auth_hmacs_list)\n\t\tmemcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list,\n\t\t\tntohs(ep->auth_hmacs_list->param_hdr.length));\n\tif (ep->auth_chunk_list)\n\t\tmemcpy(asoc->c.auth_chunks, ep->auth_chunk_list,\n\t\t\tntohs(ep->auth_chunk_list->param_hdr.length));\n\n\t/* Get the AUTH random number for this association */\n\tp = (sctp_paramhdr_t *)asoc->c.auth_random;\n\tp->type = SCTP_PARAM_RANDOM;\n\tp->length = htons(sizeof(sctp_paramhdr_t) + SCTP_AUTH_RANDOM_LENGTH);\n\tget_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH);\n\n\treturn asoc;\n\nfail_init:\n\tsock_put(asoc->base.sk);\n\tsctp_endpoint_put(asoc->ep);\n\treturn NULL;\n}\n\n/* Allocate and initialize a new association */\nstruct sctp_association *sctp_association_new(const struct sctp_endpoint *ep,\n\t\t\t\t\t const struct sock *sk,\n\t\t\t\t\t sctp_scope_t scope,\n\t\t\t\t\t gfp_t gfp)\n{\n\tstruct sctp_association *asoc;\n\n\tasoc = kzalloc(sizeof(*asoc), gfp);\n\tif (!asoc)\n\t\tgoto fail;\n\n\tif (!sctp_association_init(asoc, ep, sk, scope, gfp))\n\t\tgoto fail_init;\n\n\tSCTP_DBG_OBJCNT_INC(assoc);\n\n\tpr_debug(\"Created asoc %p\\n\", asoc);\n\n\treturn asoc;\n\nfail_init:\n\tkfree(asoc);\nfail:\n\treturn NULL;\n}\n\n/* Free this association if possible.  There may still be users, so\n * the actual deallocation may be delayed.\n */\nvoid sctp_association_free(struct sctp_association *asoc)\n{\n\tstruct sock *sk = asoc->base.sk;\n\tstruct sctp_transport *transport;\n\tstruct list_head *pos, *temp;\n\tint i;\n\n\t/* Only real associations count against the endpoint, so\n\t * don't bother for if this is a temporary association.\n\t */\n\tif (!list_empty(&asoc->asocs)) {\n\t\tlist_del(&asoc->asocs);\n\n\t\t/* Decrement the backlog value for a TCP-style listening\n\t\t * socket.\n\t\t */\n\t\tif (sctp_style(sk, TCP) && sctp_sstate(sk, LISTENING))\n\t\t\tsk->sk_ack_backlog--;\n\t}\n\n\t/* Mark as dead, so other users can know this structure is\n\t * going away.\n\t */\n\tasoc->base.dead = true;\n\n\t/* Dispose of any data lying around in the outqueue. */\n\tsctp_outq_free(&asoc->outqueue);\n\n\t/* Dispose of any pending messages for the upper layer. */\n\tsctp_ulpq_free(&asoc->ulpq);\n\n\t/* Dispose of any pending chunks on the inqueue. */\n\tsctp_inq_free(&asoc->base.inqueue);\n\n\tsctp_tsnmap_free(&asoc->peer.tsn_map);\n\n\t/* Free ssnmap storage. */\n\tsctp_ssnmap_free(asoc->ssnmap);\n\n\t/* Clean up the bound address list. */\n\tsctp_bind_addr_free(&asoc->base.bind_addr);\n\n\t/* Do we need to go through all of our timers and\n\t * delete them?   To be safe we will try to delete all, but we\n\t * should be able to go through and make a guess based\n\t * on our state.\n\t */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i) {\n\t\tif (del_timer(&asoc->timers[i]))\n\t\t\tsctp_association_put(asoc);\n\t}\n\n\t/* Free peer's cached cookie. */\n\tkfree(asoc->peer.cookie);\n\tkfree(asoc->peer.peer_random);\n\tkfree(asoc->peer.peer_chunks);\n\tkfree(asoc->peer.peer_hmacs);\n\n\t/* Release the transport structures. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tlist_del_rcu(pos);\n\t\tsctp_transport_free(transport);\n\t}\n\n\tasoc->peer.transport_count = 0;\n\n\tsctp_asconf_queue_teardown(asoc);\n\n\t/* Free pending address space being deleted */\n\tif (asoc->asconf_addr_del_pending != NULL)\n\t\tkfree(asoc->asconf_addr_del_pending);\n\n\t/* AUTH - Free the endpoint shared keys */\n\tsctp_auth_destroy_keys(&asoc->endpoint_shared_keys);\n\n\t/* AUTH - Free the association shared key */\n\tsctp_auth_key_put(asoc->asoc_shared_key);\n\n\tsctp_association_put(asoc);\n}\n\n/* Cleanup and free up an association. */\nstatic void sctp_association_destroy(struct sctp_association *asoc)\n{\n\tif (unlikely(!asoc->base.dead)) {\n\t\tWARN(1, \"Attempt to destroy undead association %p!\\n\", asoc);\n\t\treturn;\n\t}\n\n\tsctp_endpoint_put(asoc->ep);\n\tsock_put(asoc->base.sk);\n\n\tif (asoc->assoc_id != 0) {\n\t\tspin_lock_bh(&sctp_assocs_id_lock);\n\t\tidr_remove(&sctp_assocs_id, asoc->assoc_id);\n\t\tspin_unlock_bh(&sctp_assocs_id_lock);\n\t}\n\n\tWARN_ON(atomic_read(&asoc->rmem_alloc));\n\n\tkfree(asoc);\n\tSCTP_DBG_OBJCNT_DEC(assoc);\n}\n\n/* Change the primary destination address for the peer. */\nvoid sctp_assoc_set_primary(struct sctp_association *asoc,\n\t\t\t    struct sctp_transport *transport)\n{\n\tint changeover = 0;\n\n\t/* it's a changeover only if we already have a primary path\n\t * that we are changing\n\t */\n\tif (asoc->peer.primary_path != NULL &&\n\t    asoc->peer.primary_path != transport)\n\t\tchangeover = 1 ;\n\n\tasoc->peer.primary_path = transport;\n\n\t/* Set a default msg_name for events. */\n\tmemcpy(&asoc->peer.primary_addr, &transport->ipaddr,\n\t       sizeof(union sctp_addr));\n\n\t/* If the primary path is changing, assume that the\n\t * user wants to use this new path.\n\t */\n\tif ((transport->state == SCTP_ACTIVE) ||\n\t    (transport->state == SCTP_UNKNOWN))\n\t\tasoc->peer.active_path = transport;\n\n\t/*\n\t * SFR-CACC algorithm:\n\t * Upon the receipt of a request to change the primary\n\t * destination address, on the data structure for the new\n\t * primary destination, the sender MUST do the following:\n\t *\n\t * 1) If CHANGEOVER_ACTIVE is set, then there was a switch\n\t * to this destination address earlier. The sender MUST set\n\t * CYCLING_CHANGEOVER to indicate that this switch is a\n\t * double switch to the same destination address.\n\t *\n\t * Really, only bother is we have data queued or outstanding on\n\t * the association.\n\t */\n\tif (!asoc->outqueue.outstanding_bytes && !asoc->outqueue.out_qlen)\n\t\treturn;\n\n\tif (transport->cacc.changeover_active)\n\t\ttransport->cacc.cycling_changeover = changeover;\n\n\t/* 2) The sender MUST set CHANGEOVER_ACTIVE to indicate that\n\t * a changeover has occurred.\n\t */\n\ttransport->cacc.changeover_active = changeover;\n\n\t/* 3) The sender MUST store the next TSN to be sent in\n\t * next_tsn_at_change.\n\t */\n\ttransport->cacc.next_tsn_at_change = asoc->next_tsn;\n}\n\n/* Remove a transport from an association.  */\nvoid sctp_assoc_rm_peer(struct sctp_association *asoc,\n\t\t\tstruct sctp_transport *peer)\n{\n\tstruct list_head\t*pos;\n\tstruct sctp_transport\t*transport;\n\n\tpr_debug(\"%s: association:%p addr:%pISpc\\n\",\n\t\t __func__, asoc, &peer->ipaddr.sa);\n\n\t/* If we are to remove the current retran_path, update it\n\t * to the next peer before removing this peer from the list.\n\t */\n\tif (asoc->peer.retran_path == peer)\n\t\tsctp_assoc_update_retran_path(asoc);\n\n\t/* Remove this peer from the list. */\n\tlist_del_rcu(&peer->transports);\n\n\t/* Get the first transport of asoc. */\n\tpos = asoc->peer.transport_addr_list.next;\n\ttransport = list_entry(pos, struct sctp_transport, transports);\n\n\t/* Update any entries that match the peer to be deleted. */\n\tif (asoc->peer.primary_path == peer)\n\t\tsctp_assoc_set_primary(asoc, transport);\n\tif (asoc->peer.active_path == peer)\n\t\tasoc->peer.active_path = transport;\n\tif (asoc->peer.retran_path == peer)\n\t\tasoc->peer.retran_path = transport;\n\tif (asoc->peer.last_data_from == peer)\n\t\tasoc->peer.last_data_from = transport;\n\n\t/* If we remove the transport an INIT was last sent to, set it to\n\t * NULL. Combined with the update of the retran path above, this\n\t * will cause the next INIT to be sent to the next available\n\t * transport, maintaining the cycle.\n\t */\n\tif (asoc->init_last_sent_to == peer)\n\t\tasoc->init_last_sent_to = NULL;\n\n\t/* If we remove the transport an SHUTDOWN was last sent to, set it\n\t * to NULL. Combined with the update of the retran path above, this\n\t * will cause the next SHUTDOWN to be sent to the next available\n\t * transport, maintaining the cycle.\n\t */\n\tif (asoc->shutdown_last_sent_to == peer)\n\t\tasoc->shutdown_last_sent_to = NULL;\n\n\t/* If we remove the transport an ASCONF was last sent to, set it to\n\t * NULL.\n\t */\n\tif (asoc->addip_last_asconf &&\n\t    asoc->addip_last_asconf->transport == peer)\n\t\tasoc->addip_last_asconf->transport = NULL;\n\n\t/* If we have something on the transmitted list, we have to\n\t * save it off.  The best place is the active path.\n\t */\n\tif (!list_empty(&peer->transmitted)) {\n\t\tstruct sctp_transport *active = asoc->peer.active_path;\n\t\tstruct sctp_chunk *ch;\n\n\t\t/* Reset the transport of each chunk on this list */\n\t\tlist_for_each_entry(ch, &peer->transmitted,\n\t\t\t\t\ttransmitted_list) {\n\t\t\tch->transport = NULL;\n\t\t\tch->rtt_in_progress = 0;\n\t\t}\n\n\t\tlist_splice_tail_init(&peer->transmitted,\n\t\t\t\t\t&active->transmitted);\n\n\t\t/* Start a T3 timer here in case it wasn't running so\n\t\t * that these migrated packets have a chance to get\n\t\t * retransmitted.\n\t\t */\n\t\tif (!timer_pending(&active->T3_rtx_timer))\n\t\t\tif (!mod_timer(&active->T3_rtx_timer,\n\t\t\t\t\tjiffies + active->rto))\n\t\t\t\tsctp_transport_hold(active);\n\t}\n\n\tasoc->peer.transport_count--;\n\n\tsctp_transport_free(peer);\n}\n\n/* Add a transport address to an association.  */\nstruct sctp_transport *sctp_assoc_add_peer(struct sctp_association *asoc,\n\t\t\t\t\t   const union sctp_addr *addr,\n\t\t\t\t\t   const gfp_t gfp,\n\t\t\t\t\t   const int peer_state)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tstruct sctp_transport *peer;\n\tstruct sctp_sock *sp;\n\tunsigned short port;\n\n\tsp = sctp_sk(asoc->base.sk);\n\n\t/* AF_INET and AF_INET6 share common port field. */\n\tport = ntohs(addr->v4.sin_port);\n\n\tpr_debug(\"%s: association:%p addr:%pISpc state:%d\\n\", __func__,\n\t\t asoc, &addr->sa, peer_state);\n\n\t/* Set the port if it has not been set yet.  */\n\tif (0 == asoc->peer.port)\n\t\tasoc->peer.port = port;\n\n\t/* Check to see if this is a duplicate. */\n\tpeer = sctp_assoc_lookup_paddr(asoc, addr);\n\tif (peer) {\n\t\t/* An UNKNOWN state is only set on transports added by\n\t\t * user in sctp_connectx() call.  Such transports should be\n\t\t * considered CONFIRMED per RFC 4960, Section 5.4.\n\t\t */\n\t\tif (peer->state == SCTP_UNKNOWN) {\n\t\t\tpeer->state = SCTP_ACTIVE;\n\t\t}\n\t\treturn peer;\n\t}\n\n\tpeer = sctp_transport_new(net, addr, gfp);\n\tif (!peer)\n\t\treturn NULL;\n\n\tsctp_transport_set_owner(peer, asoc);\n\n\t/* Initialize the peer's heartbeat interval based on the\n\t * association configured value.\n\t */\n\tpeer->hbinterval = asoc->hbinterval;\n\n\t/* Set the path max_retrans.  */\n\tpeer->pathmaxrxt = asoc->pathmaxrxt;\n\n\t/* And the partial failure retrans threshold */\n\tpeer->pf_retrans = asoc->pf_retrans;\n\n\t/* Initialize the peer's SACK delay timeout based on the\n\t * association configured value.\n\t */\n\tpeer->sackdelay = asoc->sackdelay;\n\tpeer->sackfreq = asoc->sackfreq;\n\n\t/* Enable/disable heartbeat, SACK delay, and path MTU discovery\n\t * based on association setting.\n\t */\n\tpeer->param_flags = asoc->param_flags;\n\n\tsctp_transport_route(peer, NULL, sp);\n\n\t/* Initialize the pmtu of the transport. */\n\tif (peer->param_flags & SPP_PMTUD_DISABLE) {\n\t\tif (asoc->pathmtu)\n\t\t\tpeer->pathmtu = asoc->pathmtu;\n\t\telse\n\t\t\tpeer->pathmtu = SCTP_DEFAULT_MAXSEGMENT;\n\t}\n\n\t/* If this is the first transport addr on this association,\n\t * initialize the association PMTU to the peer's PMTU.\n\t * If not and the current association PMTU is higher than the new\n\t * peer's PMTU, reset the association PMTU to the new peer's PMTU.\n\t */\n\tif (asoc->pathmtu)\n\t\tasoc->pathmtu = min_t(int, peer->pathmtu, asoc->pathmtu);\n\telse\n\t\tasoc->pathmtu = peer->pathmtu;\n\n\tpr_debug(\"%s: association:%p PMTU set to %d\\n\", __func__, asoc,\n\t\t asoc->pathmtu);\n\n\tpeer->pmtu_pending = 0;\n\n\tasoc->frag_point = sctp_frag_point(asoc, asoc->pathmtu);\n\n\t/* The asoc->peer.port might not be meaningful yet, but\n\t * initialize the packet structure anyway.\n\t */\n\tsctp_packet_init(&peer->packet, peer, asoc->base.bind_addr.port,\n\t\t\t asoc->peer.port);\n\n\t/* 7.2.1 Slow-Start\n\t *\n\t * o The initial cwnd before DATA transmission or after a sufficiently\n\t *   long idle period MUST be set to\n\t *      min(4*MTU, max(2*MTU, 4380 bytes))\n\t *\n\t * o The initial value of ssthresh MAY be arbitrarily high\n\t *   (for example, implementations MAY use the size of the\n\t *   receiver advertised window).\n\t */\n\tpeer->cwnd = min(4*asoc->pathmtu, max_t(__u32, 2*asoc->pathmtu, 4380));\n\n\t/* At this point, we may not have the receiver's advertised window,\n\t * so initialize ssthresh to the default value and it will be set\n\t * later when we process the INIT.\n\t */\n\tpeer->ssthresh = SCTP_DEFAULT_MAXWINDOW;\n\n\tpeer->partial_bytes_acked = 0;\n\tpeer->flight_size = 0;\n\tpeer->burst_limited = 0;\n\n\t/* Set the transport's RTO.initial value */\n\tpeer->rto = asoc->rto_initial;\n\tsctp_max_rto(asoc, peer);\n\n\t/* Set the peer's active state. */\n\tpeer->state = peer_state;\n\n\t/* Attach the remote transport to our asoc.  */\n\tlist_add_tail_rcu(&peer->transports, &asoc->peer.transport_addr_list);\n\tasoc->peer.transport_count++;\n\n\t/* If we do not yet have a primary path, set one.  */\n\tif (!asoc->peer.primary_path) {\n\t\tsctp_assoc_set_primary(asoc, peer);\n\t\tasoc->peer.retran_path = peer;\n\t}\n\n\tif (asoc->peer.active_path == asoc->peer.retran_path &&\n\t    peer->state != SCTP_UNCONFIRMED) {\n\t\tasoc->peer.retran_path = peer;\n\t}\n\n\treturn peer;\n}\n\n/* Delete a transport address from an association.  */\nvoid sctp_assoc_del_peer(struct sctp_association *asoc,\n\t\t\t const union sctp_addr *addr)\n{\n\tstruct list_head\t*pos;\n\tstruct list_head\t*temp;\n\tstruct sctp_transport\t*transport;\n\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttransport = list_entry(pos, struct sctp_transport, transports);\n\t\tif (sctp_cmp_addr_exact(addr, &transport->ipaddr)) {\n\t\t\t/* Do book keeping for removing the peer and free it. */\n\t\t\tsctp_assoc_rm_peer(asoc, transport);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/* Lookup a transport by address. */\nstruct sctp_transport *sctp_assoc_lookup_paddr(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\tconst union sctp_addr *address)\n{\n\tstruct sctp_transport *t;\n\n\t/* Cycle through all transports searching for a peer address. */\n\n\tlist_for_each_entry(t, &asoc->peer.transport_addr_list,\n\t\t\ttransports) {\n\t\tif (sctp_cmp_addr_exact(address, &t->ipaddr))\n\t\t\treturn t;\n\t}\n\n\treturn NULL;\n}\n\n/* Remove all transports except a give one */\nvoid sctp_assoc_del_nonprimary_peers(struct sctp_association *asoc,\n\t\t\t\t     struct sctp_transport *primary)\n{\n\tstruct sctp_transport\t*temp;\n\tstruct sctp_transport\t*t;\n\n\tlist_for_each_entry_safe(t, temp, &asoc->peer.transport_addr_list,\n\t\t\t\t transports) {\n\t\t/* if the current transport is not the primary one, delete it */\n\t\tif (t != primary)\n\t\t\tsctp_assoc_rm_peer(asoc, t);\n\t}\n}\n\n/* Engage in transport control operations.\n * Mark the transport up or down and send a notification to the user.\n * Select and update the new active and retran paths.\n */\nvoid sctp_assoc_control_transport(struct sctp_association *asoc,\n\t\t\t\t  struct sctp_transport *transport,\n\t\t\t\t  sctp_transport_cmd_t command,\n\t\t\t\t  sctp_sn_error_t error)\n{\n\tstruct sctp_ulpevent *event;\n\tstruct sockaddr_storage addr;\n\tint spc_state = 0;\n\tbool ulp_notify = true;\n\n\t/* Record the transition on the transport.  */\n\tswitch (command) {\n\tcase SCTP_TRANSPORT_UP:\n\t\t/* If we are moving from UNCONFIRMED state due\n\t\t * to heartbeat success, report the SCTP_ADDR_CONFIRMED\n\t\t * state to the user, otherwise report SCTP_ADDR_AVAILABLE.\n\t\t */\n\t\tif (SCTP_UNCONFIRMED == transport->state &&\n\t\t    SCTP_HEARTBEAT_SUCCESS == error)\n\t\t\tspc_state = SCTP_ADDR_CONFIRMED;\n\t\telse\n\t\t\tspc_state = SCTP_ADDR_AVAILABLE;\n\t\t/* Don't inform ULP about transition from PF to\n\t\t * active state and set cwnd to 1 MTU, see SCTP\n\t\t * Quick failover draft section 5.1, point 5\n\t\t */\n\t\tif (transport->state == SCTP_PF) {\n\t\t\tulp_notify = false;\n\t\t\ttransport->cwnd = asoc->pathmtu;\n\t\t}\n\t\ttransport->state = SCTP_ACTIVE;\n\t\tbreak;\n\n\tcase SCTP_TRANSPORT_DOWN:\n\t\t/* If the transport was never confirmed, do not transition it\n\t\t * to inactive state.  Also, release the cached route since\n\t\t * there may be a better route next time.\n\t\t */\n\t\tif (transport->state != SCTP_UNCONFIRMED)\n\t\t\ttransport->state = SCTP_INACTIVE;\n\t\telse {\n\t\t\tdst_release(transport->dst);\n\t\t\ttransport->dst = NULL;\n\t\t\tulp_notify = false;\n\t\t}\n\n\t\tspc_state = SCTP_ADDR_UNREACHABLE;\n\t\tbreak;\n\n\tcase SCTP_TRANSPORT_PF:\n\t\ttransport->state = SCTP_PF;\n\t\tulp_notify = false;\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\n\t/* Generate and send a SCTP_PEER_ADDR_CHANGE notification\n\t * to the user.\n\t */\n\tif (ulp_notify) {\n\t\tmemset(&addr, 0, sizeof(struct sockaddr_storage));\n\t\tmemcpy(&addr, &transport->ipaddr,\n\t\t       transport->af_specific->sockaddr_len);\n\n\t\tevent = sctp_ulpevent_make_peer_addr_change(asoc, &addr,\n\t\t\t\t\t0, spc_state, error, GFP_ATOMIC);\n\t\tif (event)\n\t\t\tsctp_ulpq_tail_event(&asoc->ulpq, event);\n\t}\n\n\t/* Select new active and retran paths. */\n\tsctp_select_active_and_retran_path(asoc);\n}\n\n/* Hold a reference to an association. */\nvoid sctp_association_hold(struct sctp_association *asoc)\n{\n\tatomic_inc(&asoc->base.refcnt);\n}\n\n/* Release a reference to an association and cleanup\n * if there are no more references.\n */\nvoid sctp_association_put(struct sctp_association *asoc)\n{\n\tif (atomic_dec_and_test(&asoc->base.refcnt))\n\t\tsctp_association_destroy(asoc);\n}\n\n/* Allocate the next TSN, Transmission Sequence Number, for the given\n * association.\n */\n__u32 sctp_association_get_next_tsn(struct sctp_association *asoc)\n{\n\t/* From Section 1.6 Serial Number Arithmetic:\n\t * Transmission Sequence Numbers wrap around when they reach\n\t * 2**32 - 1.  That is, the next TSN a DATA chunk MUST use\n\t * after transmitting TSN = 2*32 - 1 is TSN = 0.\n\t */\n\t__u32 retval = asoc->next_tsn;\n\tasoc->next_tsn++;\n\tasoc->unack_data++;\n\n\treturn retval;\n}\n\n/* Compare two addresses to see if they match.  Wildcard addresses\n * only match themselves.\n */\nint sctp_cmp_addr_exact(const union sctp_addr *ss1,\n\t\t\tconst union sctp_addr *ss2)\n{\n\tstruct sctp_af *af;\n\n\taf = sctp_get_af_specific(ss1->sa.sa_family);\n\tif (unlikely(!af))\n\t\treturn 0;\n\n\treturn af->cmp_addr(ss1, ss2);\n}\n\n/* Return an ecne chunk to get prepended to a packet.\n * Note:  We are sly and return a shared, prealloced chunk.  FIXME:\n * No we don't, but we could/should.\n */\nstruct sctp_chunk *sctp_get_ecne_prepend(struct sctp_association *asoc)\n{\n\tif (!asoc->need_ecne)\n\t\treturn NULL;\n\n\t/* Send ECNE if needed.\n\t * Not being able to allocate a chunk here is not deadly.\n\t */\n\treturn sctp_make_ecne(asoc, asoc->last_ecne_tsn);\n}\n\n/*\n * Find which transport this TSN was sent on.\n */\nstruct sctp_transport *sctp_assoc_lookup_tsn(struct sctp_association *asoc,\n\t\t\t\t\t     __u32 tsn)\n{\n\tstruct sctp_transport *active;\n\tstruct sctp_transport *match;\n\tstruct sctp_transport *transport;\n\tstruct sctp_chunk *chunk;\n\t__be32 key = htonl(tsn);\n\n\tmatch = NULL;\n\n\t/*\n\t * FIXME: In general, find a more efficient data structure for\n\t * searching.\n\t */\n\n\t/*\n\t * The general strategy is to search each transport's transmitted\n\t * list.   Return which transport this TSN lives on.\n\t *\n\t * Let's be hopeful and check the active_path first.\n\t * Another optimization would be to know if there is only one\n\t * outbound path and not have to look for the TSN at all.\n\t *\n\t */\n\n\tactive = asoc->peer.active_path;\n\n\tlist_for_each_entry(chunk, &active->transmitted,\n\t\t\ttransmitted_list) {\n\n\t\tif (key == chunk->subh.data_hdr->tsn) {\n\t\t\tmatch = active;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* If not found, go search all the other transports. */\n\tlist_for_each_entry(transport, &asoc->peer.transport_addr_list,\n\t\t\ttransports) {\n\n\t\tif (transport == active)\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chunk, &transport->transmitted,\n\t\t\t\ttransmitted_list) {\n\t\t\tif (key == chunk->subh.data_hdr->tsn) {\n\t\t\t\tmatch = transport;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\treturn match;\n}\n\n/* Is this the association we are looking for? */\nstruct sctp_transport *sctp_assoc_is_match(struct sctp_association *asoc,\n\t\t\t\t\t   struct net *net,\n\t\t\t\t\t   const union sctp_addr *laddr,\n\t\t\t\t\t   const union sctp_addr *paddr)\n{\n\tstruct sctp_transport *transport;\n\n\tif ((htons(asoc->base.bind_addr.port) == laddr->v4.sin_port) &&\n\t    (htons(asoc->peer.port) == paddr->v4.sin_port) &&\n\t    net_eq(sock_net(asoc->base.sk), net)) {\n\t\ttransport = sctp_assoc_lookup_paddr(asoc, paddr);\n\t\tif (!transport)\n\t\t\tgoto out;\n\n\t\tif (sctp_bind_addr_match(&asoc->base.bind_addr, laddr,\n\t\t\t\t\t sctp_sk(asoc->base.sk)))\n\t\t\tgoto out;\n\t}\n\ttransport = NULL;\n\nout:\n\treturn transport;\n}\n\n/* Do delayed input processing.  This is scheduled by sctp_rcv(). */\nstatic void sctp_assoc_bh_rcv(struct work_struct *work)\n{\n\tstruct sctp_association *asoc =\n\t\tcontainer_of(work, struct sctp_association,\n\t\t\t     base.inqueue.immediate);\n\tstruct net *net = sock_net(asoc->base.sk);\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_chunk *chunk;\n\tstruct sctp_inq *inqueue;\n\tint state;\n\tsctp_subtype_t subtype;\n\tint error = 0;\n\n\t/* The association should be held so we should be safe. */\n\tep = asoc->ep;\n\n\tinqueue = &asoc->base.inqueue;\n\tsctp_association_hold(asoc);\n\twhile (NULL != (chunk = sctp_inq_pop(inqueue))) {\n\t\tstate = asoc->state;\n\t\tsubtype = SCTP_ST_CHUNK(chunk->chunk_hdr->type);\n\n\t\t/* SCTP-AUTH, Section 6.3:\n\t\t *    The receiver has a list of chunk types which it expects\n\t\t *    to be received only after an AUTH-chunk.  This list has\n\t\t *    been sent to the peer during the association setup.  It\n\t\t *    MUST silently discard these chunks if they are not placed\n\t\t *    after an AUTH chunk in the packet.\n\t\t */\n\t\tif (sctp_auth_recv_cid(subtype.chunk, asoc) && !chunk->auth)\n\t\t\tcontinue;\n\n\t\t/* Remember where the last DATA chunk came from so we\n\t\t * know where to send the SACK.\n\t\t */\n\t\tif (sctp_chunk_is_data(chunk))\n\t\t\tasoc->peer.last_data_from = chunk->transport;\n\t\telse {\n\t\t\tSCTP_INC_STATS(net, SCTP_MIB_INCTRLCHUNKS);\n\t\t\tasoc->stats.ictrlchunks++;\n\t\t\tif (chunk->chunk_hdr->type == SCTP_CID_SACK)\n\t\t\t\tasoc->stats.isacks++;\n\t\t}\n\n\t\tif (chunk->transport)\n\t\t\tchunk->transport->last_time_heard = ktime_get();\n\n\t\t/* Run through the state machine. */\n\t\terror = sctp_do_sm(net, SCTP_EVENT_T_CHUNK, subtype,\n\t\t\t\t   state, ep, asoc, chunk, GFP_ATOMIC);\n\n\t\t/* Check to see if the association is freed in response to\n\t\t * the incoming chunk.  If so, get out of the while loop.\n\t\t */\n\t\tif (asoc->base.dead)\n\t\t\tbreak;\n\n\t\t/* If there is an error on chunk, discard this packet. */\n\t\tif (error && chunk)\n\t\t\tchunk->pdiscard = 1;\n\t}\n\tsctp_association_put(asoc);\n}\n\n/* This routine moves an association from its old sk to a new sk.  */\nvoid sctp_assoc_migrate(struct sctp_association *assoc, struct sock *newsk)\n{\n\tstruct sctp_sock *newsp = sctp_sk(newsk);\n\tstruct sock *oldsk = assoc->base.sk;\n\n\t/* Delete the association from the old endpoint's list of\n\t * associations.\n\t */\n\tlist_del_init(&assoc->asocs);\n\n\t/* Decrement the backlog value for a TCP-style socket. */\n\tif (sctp_style(oldsk, TCP))\n\t\toldsk->sk_ack_backlog--;\n\n\t/* Release references to the old endpoint and the sock.  */\n\tsctp_endpoint_put(assoc->ep);\n\tsock_put(assoc->base.sk);\n\n\t/* Get a reference to the new endpoint.  */\n\tassoc->ep = newsp->ep;\n\tsctp_endpoint_hold(assoc->ep);\n\n\t/* Get a reference to the new sock.  */\n\tassoc->base.sk = newsk;\n\tsock_hold(assoc->base.sk);\n\n\t/* Add the association to the new endpoint's list of associations.  */\n\tsctp_endpoint_add_asoc(newsp->ep, assoc);\n}\n\n/* Update an association (possibly from unexpected COOKIE-ECHO processing).  */\nvoid sctp_assoc_update(struct sctp_association *asoc,\n\t\t       struct sctp_association *new)\n{\n\tstruct sctp_transport *trans;\n\tstruct list_head *pos, *temp;\n\n\t/* Copy in new parameters of peer. */\n\tasoc->c = new->c;\n\tasoc->peer.rwnd = new->peer.rwnd;\n\tasoc->peer.sack_needed = new->peer.sack_needed;\n\tasoc->peer.auth_capable = new->peer.auth_capable;\n\tasoc->peer.i = new->peer.i;\n\tsctp_tsnmap_init(&asoc->peer.tsn_map, SCTP_TSN_MAP_INITIAL,\n\t\t\t asoc->peer.i.initial_tsn, GFP_ATOMIC);\n\n\t/* Remove any peer addresses not present in the new association. */\n\tlist_for_each_safe(pos, temp, &asoc->peer.transport_addr_list) {\n\t\ttrans = list_entry(pos, struct sctp_transport, transports);\n\t\tif (!sctp_assoc_lookup_paddr(new, &trans->ipaddr)) {\n\t\t\tsctp_assoc_rm_peer(asoc, trans);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (asoc->state >= SCTP_STATE_ESTABLISHED)\n\t\t\tsctp_transport_reset(trans);\n\t}\n\n\t/* If the case is A (association restart), use\n\t * initial_tsn as next_tsn. If the case is B, use\n\t * current next_tsn in case data sent to peer\n\t * has been discarded and needs retransmission.\n\t */\n\tif (asoc->state >= SCTP_STATE_ESTABLISHED) {\n\t\tasoc->next_tsn = new->next_tsn;\n\t\tasoc->ctsn_ack_point = new->ctsn_ack_point;\n\t\tasoc->adv_peer_ack_point = new->adv_peer_ack_point;\n\n\t\t/* Reinitialize SSN for both local streams\n\t\t * and peer's streams.\n\t\t */\n\t\tsctp_ssnmap_clear(asoc->ssnmap);\n\n\t\t/* Flush the ULP reassembly and ordered queue.\n\t\t * Any data there will now be stale and will\n\t\t * cause problems.\n\t\t */\n\t\tsctp_ulpq_flush(&asoc->ulpq);\n\n\t\t/* reset the overall association error count so\n\t\t * that the restarted association doesn't get torn\n\t\t * down on the next retransmission timer.\n\t\t */\n\t\tasoc->overall_error_count = 0;\n\n\t} else {\n\t\t/* Add any peer addresses from the new association. */\n\t\tlist_for_each_entry(trans, &new->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\t\tif (!sctp_assoc_lookup_paddr(asoc, &trans->ipaddr))\n\t\t\t\tsctp_assoc_add_peer(asoc, &trans->ipaddr,\n\t\t\t\t\t\t    GFP_ATOMIC, trans->state);\n\t\t}\n\n\t\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\t\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\t\tif (!asoc->ssnmap) {\n\t\t\t/* Move the ssnmap. */\n\t\t\tasoc->ssnmap = new->ssnmap;\n\t\t\tnew->ssnmap = NULL;\n\t\t}\n\n\t\tif (!asoc->assoc_id) {\n\t\t\t/* get a new association id since we don't have one\n\t\t\t * yet.\n\t\t\t */\n\t\t\tsctp_assoc_set_id(asoc, GFP_ATOMIC);\n\t\t}\n\t}\n\n\t/* SCTP-AUTH: Save the peer parameters from the new associations\n\t * and also move the association shared keys over\n\t */\n\tkfree(asoc->peer.peer_random);\n\tasoc->peer.peer_random = new->peer.peer_random;\n\tnew->peer.peer_random = NULL;\n\n\tkfree(asoc->peer.peer_chunks);\n\tasoc->peer.peer_chunks = new->peer.peer_chunks;\n\tnew->peer.peer_chunks = NULL;\n\n\tkfree(asoc->peer.peer_hmacs);\n\tasoc->peer.peer_hmacs = new->peer.peer_hmacs;\n\tnew->peer.peer_hmacs = NULL;\n\n\tsctp_auth_key_put(asoc->asoc_shared_key);\n\tsctp_auth_asoc_init_active_key(asoc, GFP_ATOMIC);\n}\n\n/* Update the retran path for sending a retransmitted packet.\n * See also RFC4960, 6.4. Multi-Homed SCTP Endpoints:\n *\n *   When there is outbound data to send and the primary path\n *   becomes inactive (e.g., due to failures), or where the\n *   SCTP user explicitly requests to send data to an\n *   inactive destination transport address, before reporting\n *   an error to its ULP, the SCTP endpoint should try to send\n *   the data to an alternate active destination transport\n *   address if one exists.\n *\n *   When retransmitting data that timed out, if the endpoint\n *   is multihomed, it should consider each source-destination\n *   address pair in its retransmission selection policy.\n *   When retransmitting timed-out data, the endpoint should\n *   attempt to pick the most divergent source-destination\n *   pair from the original source-destination pair to which\n *   the packet was transmitted.\n *\n *   Note: Rules for picking the most divergent source-destination\n *   pair are an implementation decision and are not specified\n *   within this document.\n *\n * Our basic strategy is to round-robin transports in priorities\n * according to sctp_state_prio_map[] e.g., if no such\n * transport with state SCTP_ACTIVE exists, round-robin through\n * SCTP_UNKNOWN, etc. You get the picture.\n */\nstatic const u8 sctp_trans_state_to_prio_map[] = {\n\t[SCTP_ACTIVE]\t= 3,\t/* best case */\n\t[SCTP_UNKNOWN]\t= 2,\n\t[SCTP_PF]\t= 1,\n\t[SCTP_INACTIVE] = 0,\t/* worst case */\n};\n\nstatic u8 sctp_trans_score(const struct sctp_transport *trans)\n{\n\treturn sctp_trans_state_to_prio_map[trans->state];\n}\n\nstatic struct sctp_transport *sctp_trans_elect_tie(struct sctp_transport *trans1,\n\t\t\t\t\t\t   struct sctp_transport *trans2)\n{\n\tif (trans1->error_count > trans2->error_count) {\n\t\treturn trans2;\n\t} else if (trans1->error_count == trans2->error_count &&\n\t\t   ktime_after(trans2->last_time_heard,\n\t\t\t       trans1->last_time_heard)) {\n\t\treturn trans2;\n\t} else {\n\t\treturn trans1;\n\t}\n}\n\nstatic struct sctp_transport *sctp_trans_elect_best(struct sctp_transport *curr,\n\t\t\t\t\t\t    struct sctp_transport *best)\n{\n\tu8 score_curr, score_best;\n\n\tif (best == NULL || curr == best)\n\t\treturn curr;\n\n\tscore_curr = sctp_trans_score(curr);\n\tscore_best = sctp_trans_score(best);\n\n\t/* First, try a score-based selection if both transport states\n\t * differ. If we're in a tie, lets try to make a more clever\n\t * decision here based on error counts and last time heard.\n\t */\n\tif (score_curr > score_best)\n\t\treturn curr;\n\telse if (score_curr == score_best)\n\t\treturn sctp_trans_elect_tie(curr, best);\n\telse\n\t\treturn best;\n}\n\nvoid sctp_assoc_update_retran_path(struct sctp_association *asoc)\n{\n\tstruct sctp_transport *trans = asoc->peer.retran_path;\n\tstruct sctp_transport *trans_next = NULL;\n\n\t/* We're done as we only have the one and only path. */\n\tif (asoc->peer.transport_count == 1)\n\t\treturn;\n\t/* If active_path and retran_path are the same and active,\n\t * then this is the only active path. Use it.\n\t */\n\tif (asoc->peer.active_path == asoc->peer.retran_path &&\n\t    asoc->peer.active_path->state == SCTP_ACTIVE)\n\t\treturn;\n\n\t/* Iterate from retran_path's successor back to retran_path. */\n\tfor (trans = list_next_entry(trans, transports); 1;\n\t     trans = list_next_entry(trans, transports)) {\n\t\t/* Manually skip the head element. */\n\t\tif (&trans->transports == &asoc->peer.transport_addr_list)\n\t\t\tcontinue;\n\t\tif (trans->state == SCTP_UNCONFIRMED)\n\t\t\tcontinue;\n\t\ttrans_next = sctp_trans_elect_best(trans, trans_next);\n\t\t/* Active is good enough for immediate return. */\n\t\tif (trans_next->state == SCTP_ACTIVE)\n\t\t\tbreak;\n\t\t/* We've reached the end, time to update path. */\n\t\tif (trans == asoc->peer.retran_path)\n\t\t\tbreak;\n\t}\n\n\tasoc->peer.retran_path = trans_next;\n\n\tpr_debug(\"%s: association:%p updated new path to addr:%pISpc\\n\",\n\t\t __func__, asoc, &asoc->peer.retran_path->ipaddr.sa);\n}\n\nstatic void sctp_select_active_and_retran_path(struct sctp_association *asoc)\n{\n\tstruct sctp_transport *trans, *trans_pri = NULL, *trans_sec = NULL;\n\tstruct sctp_transport *trans_pf = NULL;\n\n\t/* Look for the two most recently used active transports. */\n\tlist_for_each_entry(trans, &asoc->peer.transport_addr_list,\n\t\t\t    transports) {\n\t\t/* Skip uninteresting transports. */\n\t\tif (trans->state == SCTP_INACTIVE ||\n\t\t    trans->state == SCTP_UNCONFIRMED)\n\t\t\tcontinue;\n\t\t/* Keep track of the best PF transport from our\n\t\t * list in case we don't find an active one.\n\t\t */\n\t\tif (trans->state == SCTP_PF) {\n\t\t\ttrans_pf = sctp_trans_elect_best(trans, trans_pf);\n\t\t\tcontinue;\n\t\t}\n\t\t/* For active transports, pick the most recent ones. */\n\t\tif (trans_pri == NULL ||\n\t\t    ktime_after(trans->last_time_heard,\n\t\t\t\ttrans_pri->last_time_heard)) {\n\t\t\ttrans_sec = trans_pri;\n\t\t\ttrans_pri = trans;\n\t\t} else if (trans_sec == NULL ||\n\t\t\t   ktime_after(trans->last_time_heard,\n\t\t\t\t       trans_sec->last_time_heard)) {\n\t\t\ttrans_sec = trans;\n\t\t}\n\t}\n\n\t/* RFC 2960 6.4 Multi-Homed SCTP Endpoints\n\t *\n\t * By default, an endpoint should always transmit to the primary\n\t * path, unless the SCTP user explicitly specifies the\n\t * destination transport address (and possibly source transport\n\t * address) to use. [If the primary is active but not most recent,\n\t * bump the most recently used transport.]\n\t */\n\tif ((asoc->peer.primary_path->state == SCTP_ACTIVE ||\n\t     asoc->peer.primary_path->state == SCTP_UNKNOWN) &&\n\t     asoc->peer.primary_path != trans_pri) {\n\t\ttrans_sec = trans_pri;\n\t\ttrans_pri = asoc->peer.primary_path;\n\t}\n\n\t/* We did not find anything useful for a possible retransmission\n\t * path; either primary path that we found is the the same as\n\t * the current one, or we didn't generally find an active one.\n\t */\n\tif (trans_sec == NULL)\n\t\ttrans_sec = trans_pri;\n\n\t/* If we failed to find a usable transport, just camp on the\n\t * active or pick a PF iff it's the better choice.\n\t */\n\tif (trans_pri == NULL) {\n\t\ttrans_pri = sctp_trans_elect_best(asoc->peer.active_path, trans_pf);\n\t\ttrans_sec = trans_pri;\n\t}\n\n\t/* Set the active and retran transports. */\n\tasoc->peer.active_path = trans_pri;\n\tasoc->peer.retran_path = trans_sec;\n}\n\nstruct sctp_transport *\nsctp_assoc_choose_alter_transport(struct sctp_association *asoc,\n\t\t\t\t  struct sctp_transport *last_sent_to)\n{\n\t/* If this is the first time packet is sent, use the active path,\n\t * else use the retran path. If the last packet was sent over the\n\t * retran path, update the retran path and use it.\n\t */\n\tif (last_sent_to == NULL) {\n\t\treturn asoc->peer.active_path;\n\t} else {\n\t\tif (last_sent_to == asoc->peer.retran_path)\n\t\t\tsctp_assoc_update_retran_path(asoc);\n\n\t\treturn asoc->peer.retran_path;\n\t}\n}\n\n/* Update the association's pmtu and frag_point by going through all the\n * transports. This routine is called when a transport's PMTU has changed.\n */\nvoid sctp_assoc_sync_pmtu(struct sock *sk, struct sctp_association *asoc)\n{\n\tstruct sctp_transport *t;\n\t__u32 pmtu = 0;\n\n\tif (!asoc)\n\t\treturn;\n\n\t/* Get the lowest pmtu of all the transports. */\n\tlist_for_each_entry(t, &asoc->peer.transport_addr_list,\n\t\t\t\ttransports) {\n\t\tif (t->pmtu_pending && t->dst) {\n\t\t\tsctp_transport_update_pmtu(sk, t, dst_mtu(t->dst));\n\t\t\tt->pmtu_pending = 0;\n\t\t}\n\t\tif (!pmtu || (t->pathmtu < pmtu))\n\t\t\tpmtu = t->pathmtu;\n\t}\n\n\tif (pmtu) {\n\t\tasoc->pathmtu = pmtu;\n\t\tasoc->frag_point = sctp_frag_point(asoc, pmtu);\n\t}\n\n\tpr_debug(\"%s: asoc:%p, pmtu:%d, frag_point:%d\\n\", __func__, asoc,\n\t\t asoc->pathmtu, asoc->frag_point);\n}\n\n/* Should we send a SACK to update our peer? */\nstatic inline bool sctp_peer_needs_update(struct sctp_association *asoc)\n{\n\tstruct net *net = sock_net(asoc->base.sk);\n\tswitch (asoc->state) {\n\tcase SCTP_STATE_ESTABLISHED:\n\tcase SCTP_STATE_SHUTDOWN_PENDING:\n\tcase SCTP_STATE_SHUTDOWN_RECEIVED:\n\tcase SCTP_STATE_SHUTDOWN_SENT:\n\t\tif ((asoc->rwnd > asoc->a_rwnd) &&\n\t\t    ((asoc->rwnd - asoc->a_rwnd) >= max_t(__u32,\n\t\t\t   (asoc->base.sk->sk_rcvbuf >> net->sctp.rwnd_upd_shift),\n\t\t\t   asoc->pathmtu)))\n\t\t\treturn true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n}\n\n/* Increase asoc's rwnd by len and send any window update SACK if needed. */\nvoid sctp_assoc_rwnd_increase(struct sctp_association *asoc, unsigned int len)\n{\n\tstruct sctp_chunk *sack;\n\tstruct timer_list *timer;\n\n\tif (asoc->rwnd_over) {\n\t\tif (asoc->rwnd_over >= len) {\n\t\t\tasoc->rwnd_over -= len;\n\t\t} else {\n\t\t\tasoc->rwnd += (len - asoc->rwnd_over);\n\t\t\tasoc->rwnd_over = 0;\n\t\t}\n\t} else {\n\t\tasoc->rwnd += len;\n\t}\n\n\t/* If we had window pressure, start recovering it\n\t * once our rwnd had reached the accumulated pressure\n\t * threshold.  The idea is to recover slowly, but up\n\t * to the initial advertised window.\n\t */\n\tif (asoc->rwnd_press && asoc->rwnd >= asoc->rwnd_press) {\n\t\tint change = min(asoc->pathmtu, asoc->rwnd_press);\n\t\tasoc->rwnd += change;\n\t\tasoc->rwnd_press -= change;\n\t}\n\n\tpr_debug(\"%s: asoc:%p rwnd increased by %d to (%u, %u) - %u\\n\",\n\t\t __func__, asoc, len, asoc->rwnd, asoc->rwnd_over,\n\t\t asoc->a_rwnd);\n\n\t/* Send a window update SACK if the rwnd has increased by at least the\n\t * minimum of the association's PMTU and half of the receive buffer.\n\t * The algorithm used is similar to the one described in\n\t * Section 4.2.3.3 of RFC 1122.\n\t */\n\tif (sctp_peer_needs_update(asoc)) {\n\t\tasoc->a_rwnd = asoc->rwnd;\n\n\t\tpr_debug(\"%s: sending window update SACK- asoc:%p rwnd:%u \"\n\t\t\t \"a_rwnd:%u\\n\", __func__, asoc, asoc->rwnd,\n\t\t\t asoc->a_rwnd);\n\n\t\tsack = sctp_make_sack(asoc);\n\t\tif (!sack)\n\t\t\treturn;\n\n\t\tasoc->peer.sack_needed = 0;\n\n\t\tsctp_outq_tail(&asoc->outqueue, sack);\n\n\t\t/* Stop the SACK timer.  */\n\t\ttimer = &asoc->timers[SCTP_EVENT_TIMEOUT_SACK];\n\t\tif (del_timer(timer))\n\t\t\tsctp_association_put(asoc);\n\t}\n}\n\n/* Decrease asoc's rwnd by len. */\nvoid sctp_assoc_rwnd_decrease(struct sctp_association *asoc, unsigned int len)\n{\n\tint rx_count;\n\tint over = 0;\n\n\tif (unlikely(!asoc->rwnd || asoc->rwnd_over))\n\t\tpr_debug(\"%s: association:%p has asoc->rwnd:%u, \"\n\t\t\t \"asoc->rwnd_over:%u!\\n\", __func__, asoc,\n\t\t\t asoc->rwnd, asoc->rwnd_over);\n\n\tif (asoc->ep->rcvbuf_policy)\n\t\trx_count = atomic_read(&asoc->rmem_alloc);\n\telse\n\t\trx_count = atomic_read(&asoc->base.sk->sk_rmem_alloc);\n\n\t/* If we've reached or overflowed our receive buffer, announce\n\t * a 0 rwnd if rwnd would still be positive.  Store the\n\t * the potential pressure overflow so that the window can be restored\n\t * back to original value.\n\t */\n\tif (rx_count >= asoc->base.sk->sk_rcvbuf)\n\t\tover = 1;\n\n\tif (asoc->rwnd >= len) {\n\t\tasoc->rwnd -= len;\n\t\tif (over) {\n\t\t\tasoc->rwnd_press += asoc->rwnd;\n\t\t\tasoc->rwnd = 0;\n\t\t}\n\t} else {\n\t\tasoc->rwnd_over = len - asoc->rwnd;\n\t\tasoc->rwnd = 0;\n\t}\n\n\tpr_debug(\"%s: asoc:%p rwnd decreased by %d to (%u, %u, %u)\\n\",\n\t\t __func__, asoc, len, asoc->rwnd, asoc->rwnd_over,\n\t\t asoc->rwnd_press);\n}\n\n/* Build the bind address list for the association based on info from the\n * local endpoint and the remote peer.\n */\nint sctp_assoc_set_bind_addr_from_ep(struct sctp_association *asoc,\n\t\t\t\t     sctp_scope_t scope, gfp_t gfp)\n{\n\tint flags;\n\n\t/* Use scoping rules to determine the subset of addresses from\n\t * the endpoint.\n\t */\n\tflags = (PF_INET6 == asoc->base.sk->sk_family) ? SCTP_ADDR6_ALLOWED : 0;\n\tif (asoc->peer.ipv4_address)\n\t\tflags |= SCTP_ADDR4_PEERSUPP;\n\tif (asoc->peer.ipv6_address)\n\t\tflags |= SCTP_ADDR6_PEERSUPP;\n\n\treturn sctp_bind_addr_copy(sock_net(asoc->base.sk),\n\t\t\t\t   &asoc->base.bind_addr,\n\t\t\t\t   &asoc->ep->base.bind_addr,\n\t\t\t\t   scope, gfp, flags);\n}\n\n/* Build the association's bind address list from the cookie.  */\nint sctp_assoc_set_bind_addr_from_cookie(struct sctp_association *asoc,\n\t\t\t\t\t struct sctp_cookie *cookie,\n\t\t\t\t\t gfp_t gfp)\n{\n\tint var_size2 = ntohs(cookie->peer_init->chunk_hdr.length);\n\tint var_size3 = cookie->raw_addr_list_len;\n\t__u8 *raw = (__u8 *)cookie->peer_init + var_size2;\n\n\treturn sctp_raw_to_bind_addrs(&asoc->base.bind_addr, raw, var_size3,\n\t\t\t\t      asoc->ep->base.bind_addr.port, gfp);\n}\n\n/* Lookup laddr in the bind address list of an association. */\nint sctp_assoc_lookup_laddr(struct sctp_association *asoc,\n\t\t\t    const union sctp_addr *laddr)\n{\n\tint found = 0;\n\n\tif ((asoc->base.bind_addr.port == ntohs(laddr->v4.sin_port)) &&\n\t    sctp_bind_addr_match(&asoc->base.bind_addr, laddr,\n\t\t\t\t sctp_sk(asoc->base.sk)))\n\t\tfound = 1;\n\n\treturn found;\n}\n\n/* Set an association id for a given association */\nint sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)\n{\n\tbool preload = !!(gfp & __GFP_WAIT);\n\tint ret;\n\n\t/* If the id is already assigned, keep it. */\n\tif (asoc->assoc_id)\n\t\treturn 0;\n\n\tif (preload)\n\t\tidr_preload(gfp);\n\tspin_lock_bh(&sctp_assocs_id_lock);\n\t/* 0 is not a valid assoc_id, must be >= 1 */\n\tret = idr_alloc_cyclic(&sctp_assocs_id, asoc, 1, 0, GFP_NOWAIT);\n\tspin_unlock_bh(&sctp_assocs_id_lock);\n\tif (preload)\n\t\tidr_preload_end();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tasoc->assoc_id = (sctp_assoc_t)ret;\n\treturn 0;\n}\n\n/* Free the ASCONF queue */\nstatic void sctp_assoc_free_asconf_queue(struct sctp_association *asoc)\n{\n\tstruct sctp_chunk *asconf;\n\tstruct sctp_chunk *tmp;\n\n\tlist_for_each_entry_safe(asconf, tmp, &asoc->addip_chunk_list, list) {\n\t\tlist_del_init(&asconf->list);\n\t\tsctp_chunk_free(asconf);\n\t}\n}\n\n/* Free asconf_ack cache */\nstatic void sctp_assoc_free_asconf_acks(struct sctp_association *asoc)\n{\n\tstruct sctp_chunk *ack;\n\tstruct sctp_chunk *tmp;\n\n\tlist_for_each_entry_safe(ack, tmp, &asoc->asconf_ack_list,\n\t\t\t\ttransmitted_list) {\n\t\tlist_del_init(&ack->transmitted_list);\n\t\tsctp_chunk_free(ack);\n\t}\n}\n\n/* Clean up the ASCONF_ACK queue */\nvoid sctp_assoc_clean_asconf_ack_cache(const struct sctp_association *asoc)\n{\n\tstruct sctp_chunk *ack;\n\tstruct sctp_chunk *tmp;\n\n\t/* We can remove all the entries from the queue up to\n\t * the \"Peer-Sequence-Number\".\n\t */\n\tlist_for_each_entry_safe(ack, tmp, &asoc->asconf_ack_list,\n\t\t\t\ttransmitted_list) {\n\t\tif (ack->subh.addip_hdr->serial ==\n\t\t\t\thtonl(asoc->peer.addip_serial))\n\t\t\tbreak;\n\n\t\tlist_del_init(&ack->transmitted_list);\n\t\tsctp_chunk_free(ack);\n\t}\n}\n\n/* Find the ASCONF_ACK whose serial number matches ASCONF */\nstruct sctp_chunk *sctp_assoc_lookup_asconf_ack(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\t__be32 serial)\n{\n\tstruct sctp_chunk *ack;\n\n\t/* Walk through the list of cached ASCONF-ACKs and find the\n\t * ack chunk whose serial number matches that of the request.\n\t */\n\tlist_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {\n\t\tif (sctp_chunk_pending(ack))\n\t\t\tcontinue;\n\t\tif (ack->subh.addip_hdr->serial == serial) {\n\t\t\tsctp_chunk_hold(ack);\n\t\t\treturn ack;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nvoid sctp_asconf_queue_teardown(struct sctp_association *asoc)\n{\n\t/* Free any cached ASCONF_ACK chunk. */\n\tsctp_assoc_free_asconf_acks(asoc);\n\n\t/* Free the ASCONF queue. */\n\tsctp_assoc_free_asconf_queue(asoc);\n\n\t/* Free any cached ASCONF chunk. */\n\tif (asoc->addip_last_asconf)\n\t\tsctp_chunk_free(asoc->addip_last_asconf);\n}\n"], "filenames": ["include/net/sctp/sctp.h", "net/sctp/associola.c"], "buggy_code_start_loc": [428, 1670], "buggy_code_end_loc": [428, 1670], "fixing_code_start_loc": [429, 1671], "fixing_code_end_loc": [434, 1673], "type": "CWE-400", "message": "The sctp_assoc_lookup_asconf_ack function in net/sctp/associola.c in the SCTP implementation in the Linux kernel through 3.17.2 allows remote attackers to cause a denial of service (panic) via duplicate ASCONF chunks that trigger an incorrect uncork within the side-effect interpreter.", "other": {"cve": {"id": "CVE-2014-3687", "sourceIdentifier": "secalert@redhat.com", "published": "2014-11-10T11:55:06.627", "lastModified": "2023-02-13T00:41:53.337", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The sctp_assoc_lookup_asconf_ack function in net/sctp/associola.c in the SCTP implementation in the Linux kernel through 3.17.2 allows remote attackers to cause a denial of service (panic) via duplicate ASCONF chunks that trigger an incorrect uncork within the side-effect interpreter."}, {"lang": "es", "value": "La funci\u00f3n sctp_assoc_lookup_asconf_ack en net/sctp/associola.c en la implementaci\u00f3n SCTP en el kernel de Linux hasta 3.17.2 permite a atacantes remotos causar una denegaci\u00f3n de servicio (kernel panic) a trav\u00e9s de trozos ASCONF duplicados que provocan una liberaci\u00f3n incorrecta dentro del int\u00e9rprete de efectos secundarios."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.8}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.27", "versionEndExcluding": "3.2.64", "matchCriteriaId": "DB3D4AA1-56D7-4CA2-9F6B-AFF5DC295374"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.4.107", "matchCriteriaId": "E15A5939-878C-4DD2-9BDF-308A9616CAC2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.5", "versionEndExcluding": "3.10.61", "matchCriteriaId": "6F6F5B79-847B-40A4-A86A-02ECF5222C5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.34", "matchCriteriaId": "7EE4711D-12D9-4BDA-86F5-6F6C3AF22A59"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.14.25", "matchCriteriaId": "FFEC5060-1B1C-4215-BD50-21710B905775"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.15", "versionEndExcluding": "3.16.35", "matchCriteriaId": "7DC4BA70-B111-4D2E-BC78-6601CED68F08"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.17.4", "matchCriteriaId": "C15DCCA4-4748-46FF-B6BC-C1F5C70E3B2B"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_mrg:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "C60FA8B1-1802-4522-A088-22171DCF7A93"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:novell:suse_linux_enterprise_desktop:12.0:*:*:*:*:*:*:*", "matchCriteriaId": "EA04C9F1-6257-4D82-BA0B-37DE66D94736"}, {"vulnerable": true, "criteria": "cpe:2.3:o:novell:suse_linux_enterprise_server:12.0:*:*:*:*:*:*:*", "matchCriteriaId": "C384D0B6-8A5C-45CA-8CD9-7F4E967FE4F0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:evergreen:11.4:*:*:*:*:*:*:*", "matchCriteriaId": "CCE4D64E-8C4B-4F21-A9B0-90637C85C1D0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_real_time_extension:11:sp3:*:*:*:*:*:*", "matchCriteriaId": "3DB41B45-D94D-4A58-88B0-B3EC3EC350E2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_software_development_kit:12:-:*:*:*:*:*:*", "matchCriteriaId": "1831D45A-EE6E-4220-8F8C-248B69520948"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_workstation_extension:12:*:*:*:*:*:*:*", "matchCriteriaId": "9DFA18B6-2642-470A-A350-68947529EE5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:suse:suse_linux_enterprise_server:11:sp2:*:*:ltss:*:*:*", "matchCriteriaId": "C202F75B-221A-40BB-8A0D-451335B39937"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:5:-:*:*:*:*:*:*", "matchCriteriaId": "62A2AC02-A933-4E51-810E-5D040B476B7B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:6:-:*:*:*:*:*:*", "matchCriteriaId": "D7B037A8-72A6-4DFF-94B2-D688A5F6F876"}, {"vulnerable": true, "criteria": "cpe:2.3:o:oracle:linux:7:-:*:*:*:*:*:*", "matchCriteriaId": "44B8FEDF-6CB0-46E9-9AD7-4445B001C158"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=b69040d8e39f20d5215a03502a8e8b4c6ab78395", "source": "secalert@redhat.com"}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3087.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3088.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://linux.oracle.com/errata/ELSA-2014-3089.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-01/msg00035.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00010.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00020.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-03/msg00025.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-04/msg00000.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-04/msg00015.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-09/msg00009.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://marc.info/?l=bugtraq&m=142722450701342&w=2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://marc.info/?l=bugtraq&m=142722544401658&w=2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-0062.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2015-0115.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.debian.org/security/2014/dsa-3060", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/70766", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2417-1", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.ubuntu.com/usn/USN-2418-1", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1155731", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/b69040d8e39f20d5215a03502a8e8b4c6ab78395", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b69040d8e39f20d5215a03502a8e8b4c6ab78395"}}