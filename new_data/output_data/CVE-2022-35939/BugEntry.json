{"buggy_code": ["/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REFERENCE_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REFERENCE_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include <algorithm>\n#include <cmath>\n#include <cstring>\n#include <functional>\n#include <limits>\n#include <memory>\n#include <type_traits>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"fixedpoint/fixedpoint.h\"\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/c/c_api_types.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/add.h\"\n#include \"tensorflow/lite/kernels/internal/reference/add_n.h\"\n#include \"tensorflow/lite/kernels/internal/reference/arg_min_max.h\"\n#include \"tensorflow/lite/kernels/internal/reference/batch_matmul.h\"\n#include \"tensorflow/lite/kernels/internal/reference/batch_to_space_nd.h\"\n#include \"tensorflow/lite/kernels/internal/reference/binary_function.h\"\n#include \"tensorflow/lite/kernels/internal/reference/cast.h\"\n#include \"tensorflow/lite/kernels/internal/reference/ceil.h\"\n#include \"tensorflow/lite/kernels/internal/reference/comparisons.h\"\n#include \"tensorflow/lite/kernels/internal/reference/concatenation.h\"\n#include \"tensorflow/lite/kernels/internal/reference/conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depth_to_space.h\"\n#include \"tensorflow/lite/kernels/internal/reference/dequantize.h\"\n#include \"tensorflow/lite/kernels/internal/reference/div.h\"\n#include \"tensorflow/lite/kernels/internal/reference/elu.h\"\n#include \"tensorflow/lite/kernels/internal/reference/exp.h\"\n#include \"tensorflow/lite/kernels/internal/reference/fill.h\"\n#include \"tensorflow/lite/kernels/internal/reference/floor.h\"\n#include \"tensorflow/lite/kernels/internal/reference/floor_div.h\"\n#include \"tensorflow/lite/kernels/internal/reference/floor_mod.h\"\n#include \"tensorflow/lite/kernels/internal/reference/fully_connected.h\"\n#include \"tensorflow/lite/kernels/internal/reference/gather.h\"\n#include \"tensorflow/lite/kernels/internal/reference/hard_swish.h\"\n#include \"tensorflow/lite/kernels/internal/reference/l2normalization.h\"\n#include \"tensorflow/lite/kernels/internal/reference/leaky_relu.h\"\n#include \"tensorflow/lite/kernels/internal/reference/log_softmax.h\"\n#include \"tensorflow/lite/kernels/internal/reference/logistic.h\"\n#include \"tensorflow/lite/kernels/internal/reference/lstm_cell.h\"\n#include \"tensorflow/lite/kernels/internal/reference/maximum_minimum.h\"\n#include \"tensorflow/lite/kernels/internal/reference/mul.h\"\n#include \"tensorflow/lite/kernels/internal/reference/neg.h\"\n#include \"tensorflow/lite/kernels/internal/reference/pad.h\"\n#include \"tensorflow/lite/kernels/internal/reference/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/prelu.h\"\n#include \"tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h\"\n#include \"tensorflow/lite/kernels/internal/reference/quantize.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reduce.h\"\n#include \"tensorflow/lite/kernels/internal/reference/requantize.h\"\n#include \"tensorflow/lite/kernels/internal/reference/resize_bilinear.h\"\n#include \"tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h\"\n#include \"tensorflow/lite/kernels/internal/reference/round.h\"\n#include \"tensorflow/lite/kernels/internal/reference/slice.h\"\n#include \"tensorflow/lite/kernels/internal/reference/softmax.h\"\n#include \"tensorflow/lite/kernels/internal/reference/space_to_batch_nd.h\"\n#include \"tensorflow/lite/kernels/internal/reference/space_to_depth.h\"\n#include \"tensorflow/lite/kernels/internal/reference/strided_slice.h\"\n#include \"tensorflow/lite/kernels/internal/reference/string_comparisons.h\"\n#include \"tensorflow/lite/kernels/internal/reference/sub.h\"\n#include \"tensorflow/lite/kernels/internal/reference/tanh.h\"\n#include \"tensorflow/lite/kernels/internal/reference/transpose.h\"\n#include \"tensorflow/lite/kernels/internal/reference/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/strided_slice_logic.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\nnamespace tflite {\n\nnamespace reference_ops {\n\ntemplate <typename T>\ninline void Relu(const RuntimeShape& input_shape, const T* input_data,\n                 const RuntimeShape& output_shape, T* output_data) {\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T lower = 0;\n    const T clamped = val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <typename T>\ninline void Relu0To1(const RuntimeShape& input_shape, const T* input_data,\n                     const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu0To1 (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T upper = 1;\n    const T lower = 0;\n    const T clamped = val > upper ? upper : val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <typename T>\ninline void Relu1(const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu1 (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T upper = 1;\n    const T lower = -1;\n    const T clamped = val > upper ? upper : val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ninline void Relu6(const RuntimeShape& input_shape, const float* input_data,\n                  const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu6 (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const float upper = 6;\n    const float lower = 0;\n    const float clamped = val > upper ? upper : val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <typename T>\ninline void ReluX(const tflite::ReluParams& params,\n                  const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantized ReluX (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const int32 val = static_cast<int32_t>(input_data[i]);\n    int32 clamped = params.output_offset +\n                    MultiplyByQuantizedMultiplier(val - params.input_offset,\n                                                  params.output_multiplier,\n                                                  params.output_shift);\n    clamped = std::max(params.quantized_activation_min, clamped);\n    clamped = std::min(params.quantized_activation_max, clamped);\n    output_data[i] = static_cast<T>(clamped);\n  }\n}\n\ntemplate <typename T>\ninline void ReluX(const tflite::ActivationParams& params,\n                  const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantized ReluX (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  const T max_value = params.quantized_activation_max;\n  const T min_value = params.quantized_activation_min;\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T clamped = val > max_value   ? max_value\n                      : val < min_value ? min_value\n                                        : val;\n    output_data[i] = clamped;\n  }\n}\n\n// TODO(jiawen): We can implement BroadcastMul on buffers of arbitrary\n// dimensionality if the runtime code does a single loop over one dimension\n// that handles broadcasting as the base case. The code generator would then\n// generate max(D1, D2) nested for loops.\ninline void BroadcastMulFivefold(const ArithmeticParams& unswitched_params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const uint8* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const uint8* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 uint8* output_data) {\n  ArithmeticParams switched_params = unswitched_params;\n  switched_params.input1_offset = unswitched_params.input2_offset;\n  switched_params.input2_offset = unswitched_params.input1_offset;\n\n  const bool use_unswitched =\n      unswitched_params.broadcast_category ==\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n\n  const ArithmeticParams& params =\n      use_unswitched ? unswitched_params : switched_params;\n  const uint8* input1_data =\n      use_unswitched ? unswitched_input1_data : unswitched_input2_data;\n  const uint8* input2_data =\n      use_unswitched ? unswitched_input2_data : unswitched_input1_data;\n\n  // Fivefold nested loops. The second input resets its position for each\n  // iteration of the second loop. The first input resets its position at the\n  // beginning of the fourth loop. The innermost loop is an elementwise Mul of\n  // sections of the arrays.\n  uint8* output_data_ptr = output_data;\n  const uint8* input1_data_ptr = input1_data;\n  const uint8* input2_data_reset = input2_data;\n  int y0 = params.broadcast_shape[0];\n  int y1 = params.broadcast_shape[1];\n  int y2 = params.broadcast_shape[2];\n  int y3 = params.broadcast_shape[3];\n  int y4 = params.broadcast_shape[4];\n  for (int i0 = 0; i0 < y0; ++i0) {\n    const uint8* input2_data_ptr;\n    for (int i1 = 0; i1 < y1; ++i1) {\n      input2_data_ptr = input2_data_reset;\n      for (int i2 = 0; i2 < y2; ++i2) {\n        for (int i3 = 0; i3 < y3; ++i3) {\n          MulElementwise(y4, params, input1_data_ptr, input2_data_ptr,\n                         output_data_ptr);\n          input2_data_ptr += y4;\n          output_data_ptr += y4;\n        }\n        input1_data_ptr += y4;\n      }\n    }\n    input2_data_reset = input2_data_ptr;\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16\");\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    output_data[i] = unclamped_result.raw();\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16Uint8\");\n  int32 output_offset = params.output_offset;\n  int32 output_activation_min = params.quantized_activation_min;\n  int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    int16 rescaled_result =\n        gemmlowp::RoundingDivideByPOT(unclamped_result.raw(), 8);\n    int16 clamped_result =\n        std::min<int16>(output_activation_max - output_offset, rescaled_result);\n    clamped_result =\n        std::max<int16>(output_activation_min - output_offset, clamped_result);\n    output_data[i] = output_offset + clamped_result;\n  }\n}\n\ninline void Sub16(const ArithmeticParams& params,\n                  const RuntimeShape& input1_shape, const int16_t* input1_data,\n                  const RuntimeShape& input2_shape, const int16_t* input2_data,\n                  const RuntimeShape& output_shape, int16_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Sub/Int16\");\n  const int input1_shift = params.input1_shift;\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  const int16 output_activation_min = params.quantized_activation_min;\n  const int16 output_activation_max = params.quantized_activation_max;\n\n  TFLITE_DCHECK(input1_shift == 0 || params.input2_shift == 0);\n  TFLITE_DCHECK_LE(input1_shift, 0);\n  TFLITE_DCHECK_LE(params.input2_shift, 0);\n  const int16* not_shift_input = input1_shift == 0 ? input1_data : input2_data;\n  const int16* shift_input = input1_shift == 0 ? input2_data : input1_data;\n  const int input_right_shift =\n      input1_shift == 0 ? -params.input2_shift : -input1_shift;\n\n  if (input1_shift == 0) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    for (int i = 0; i < flat_size; ++i) {\n      F0 input_ready_scaled = F0::FromRaw(not_shift_input[i]);\n      F0 scaled_input = F0::FromRaw(\n          gemmlowp::RoundingDivideByPOT(shift_input[i], input_right_shift));\n      F0 result = SaturatingSub(input_ready_scaled, scaled_input);\n      const int16 raw_output = result.raw();\n      const int16 clamped_output = std::min(\n          output_activation_max, std::max(output_activation_min, raw_output));\n      output_data[i] = clamped_output;\n    }\n  } else {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    for (int i = 0; i < flat_size; ++i) {\n      F0 input_ready_scaled = F0::FromRaw(not_shift_input[i]);\n      F0 scaled_input = F0::FromRaw(\n          gemmlowp::RoundingDivideByPOT(shift_input[i], input_right_shift));\n      F0 result = SaturatingSub(scaled_input, input_ready_scaled);\n      const int16 raw_output = result.raw();\n      const int16 clamped_output = std::min(\n          output_activation_max, std::max(output_activation_min, raw_output));\n      output_data[i] = clamped_output;\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid Pack(const PackParams& params, const RuntimeShape* const* input_shapes,\n          const Scalar* const* input_data, const RuntimeShape& output_shape,\n          Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"Pack\");\n  const int dimensions = output_shape.DimensionsCount();\n  int axis = params.axis;\n  int inputs_count = params.inputs_count;\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; i++) {\n    outer_size *= output_shape.Dims(i);\n  }\n  int copy_size = 1;\n  for (int i = params.axis + 1; i < dimensions; i++) {\n    copy_size *= output_shape.Dims(i);\n  }\n  TFLITE_DCHECK_EQ((**input_shapes).FlatSize(), copy_size * outer_size);\n\n  for (int i = 0; i < inputs_count; ++i) {\n    for (int k = 0; k < outer_size; k++) {\n      const Scalar* input_ptr = input_data[i] + copy_size * k;\n      int loc = k * inputs_count * copy_size + i * copy_size;\n      memcpy(output_data + loc, input_ptr, copy_size * sizeof(Scalar));\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid Unpack(const UnpackParams& params, const RuntimeShape& input_shape,\n            const Scalar* input_data, const RuntimeShape& output_shape,\n            Scalar* const* output_datas) {\n  ruy::profiler::ScopeLabel label(\"Unpack\");\n  const int dimensions = input_shape.DimensionsCount();\n  const int outputs_count = params.num_split;\n\n  int outer_size = 1;\n  int axis = params.axis;\n  if (axis < 0) {\n    axis += dimensions;\n  }\n  TFLITE_DCHECK_GE(axis, 0);\n  TFLITE_DCHECK_LT(axis, dimensions);\n  for (int i = 0; i < axis; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n  int copy_size = 1;\n  for (int i = axis + 1; i < dimensions; ++i) {\n    copy_size *= input_shape.Dims(i);\n  }\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(), copy_size * outer_size);\n\n  for (int i = 0; i < outputs_count; ++i) {\n    for (int k = 0; k < outer_size; k++) {\n      Scalar* output_ptr = output_datas[i] + copy_size * k;\n      int loc = k * outputs_count * copy_size + i * copy_size;\n      memcpy(output_ptr, input_data + loc, copy_size * sizeof(Scalar));\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid PackWithScaling(const PackParams& params,\n                     const RuntimeShape* const* input_shapes,\n                     const uint8* const* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"PackWithScaling\");\n  const int dimensions = output_shape.DimensionsCount();\n  int axis = params.axis;\n  const int32* input_zeropoint = params.input_zeropoint;\n  const float* input_scale = params.input_scale;\n  int inputs_count = params.inputs_count;\n  const int32 output_zeropoint = params.output_zeropoint;\n  const float output_scale = params.output_scale;\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; i++) {\n    outer_size *= output_shape.Dims(i);\n  }\n  int copy_size = 1;\n  for (int i = axis + 1; i < dimensions; i++) {\n    copy_size *= output_shape.Dims(i);\n  }\n  TFLITE_DCHECK_EQ((**input_shapes).FlatSize(), copy_size * outer_size);\n\n  Scalar* output_ptr = output_data;\n  const float inverse_output_scale = 1.f / output_scale;\n  for (int k = 0; k < outer_size; k++) {\n    for (int i = 0; i < inputs_count; ++i) {\n      if (input_zeropoint[i] == output_zeropoint &&\n          input_scale[i] == output_scale) {\n        memcpy(output_ptr, input_data[i] + k * copy_size,\n               copy_size * sizeof(Scalar));\n      } else {\n        assert(false);\n        const float scale = input_scale[i] * inverse_output_scale;\n        const float bias = -input_zeropoint[i] * scale;\n        auto input_ptr = input_data[i];\n        for (int j = 0; j < copy_size; ++j) {\n          const int32_t value =\n              static_cast<int32_t>(std::round(input_ptr[j] * scale + bias)) +\n              output_zeropoint;\n          output_ptr[j] =\n              static_cast<uint8_t>(std::max(std::min(255, value), 0));\n        }\n      }\n      output_ptr += copy_size;\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid DepthConcatenation(const ConcatenationParams& params,\n                        const RuntimeShape* const* input_shapes,\n                        const Scalar* const* input_data,\n                        const RuntimeShape& output_shape, Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"DepthConcatenation\");\n  auto params_copy = params;\n  params_copy.axis = 3;\n  Concatenation(params_copy, input_shapes, input_data, output_shape,\n                output_data);\n}\n\ntemplate <typename Scalar>\nvoid Split(const SplitParams& params, const RuntimeShape& input_shape,\n           const Scalar* input_data, const RuntimeShape* const* output_shapes,\n           Scalar* const* output_data) {\n  ruy::profiler::ScopeLabel label(\"Split\");\n  const int split_dimensions = input_shape.DimensionsCount();\n  int axis = params.axis < 0 ? params.axis + split_dimensions : params.axis;\n  int outputs_count = params.num_split;\n  TFLITE_DCHECK_LT(axis, split_dimensions);\n\n  int64_t split_size = 0;\n  for (int i = 0; i < outputs_count; i++) {\n    TFLITE_DCHECK_EQ(output_shapes[i]->DimensionsCount(), split_dimensions);\n    for (int j = 0; j < split_dimensions; j++) {\n      if (j != axis) {\n        MatchingDim(*output_shapes[i], j, input_shape, j);\n      }\n    }\n    split_size += output_shapes[i]->Dims(axis);\n  }\n  TFLITE_DCHECK_EQ(split_size, input_shape.Dims(axis));\n  int64_t outer_size = 1;\n  for (int i = 0; i < axis; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n  // For all output arrays,\n  // FlatSize() = outer_size * Dims(axis) * base_inner_size;\n  int64_t base_inner_size = 1;\n  for (int i = axis + 1; i < split_dimensions; ++i) {\n    base_inner_size *= input_shape.Dims(i);\n  }\n\n  const Scalar* input_ptr = input_data;\n  for (int k = 0; k < outer_size; k++) {\n    for (int i = 0; i < outputs_count; ++i) {\n      const int copy_size = output_shapes[i]->Dims(axis) * base_inner_size;\n      memcpy(output_data[i] + k * copy_size, input_ptr,\n             copy_size * sizeof(Scalar));\n      input_ptr += copy_size;\n    }\n  }\n}\n\ninline int NodeOffset(int b, int h, int w, int height, int width) {\n  return (b * height + h) * width + w;\n}\n\ninline void LocalResponseNormalization(\n    const tflite::LocalResponseNormalizationParams& op_params,\n    const RuntimeShape& input_shape, const float* input_data,\n    const RuntimeShape& output_shape, float* output_data) {\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int i = 0; i < outer_size; ++i) {\n    for (int c = 0; c < depth; ++c) {\n      const int begin_input_c = std::max(0, c - op_params.range);\n      const int end_input_c = std::min(depth, c + op_params.range);\n      float accum = 0.f;\n      for (int input_c = begin_input_c; input_c < end_input_c; ++input_c) {\n        const float input_val = input_data[i * depth + input_c];\n        accum += input_val * input_val;\n      }\n      const float multiplier =\n          std::pow(op_params.bias + op_params.alpha * accum, -op_params.beta);\n      output_data[i * depth + c] = input_data[i * depth + c] * multiplier;\n    }\n  }\n}\n\ninline void Dequantize(const RuntimeShape& input_shape,\n                       const Eigen::half* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; i++) {\n    output_data[i] = static_cast<float>(input_data[i]);\n  }\n}\n\ninline void FakeQuant(const tflite::FakeQuantParams& op_params,\n                      const RuntimeShape& input_shape, const float* input_data,\n                      const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"FakeQuant\");\n  float rmin = op_params.minmax.min;\n  float rmax = op_params.minmax.max;\n  int num_bits = op_params.num_bits;\n  // 0 should always be a representable value. Let's assume that the initial\n  // min,max range contains 0.\n  TFLITE_DCHECK_LE(rmin, 0.0f);\n  TFLITE_DCHECK_GE(rmax, 0.0f);\n  TFLITE_DCHECK_LT(rmin, rmax);\n\n  // Code matches tensorflow's FakeQuantWithMinMaxArgsFunctor.\n  int quant_min = 0;\n  int quant_max = (1 << num_bits) - 1;\n  float nudged_min, nudged_max, nudged_scale;\n  NudgeQuantizationRange(rmin, rmax, quant_min, quant_max, &nudged_min,\n                         &nudged_max, &nudged_scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  FakeQuantizeArray(nudged_scale, nudged_min, nudged_max, input_data,\n                    output_data, flat_size);\n}\n\n// Common subroutine for both `GatherNd` and `GatherNdString`.\nstruct GatherNdHelperResult {\n  int n_slices;\n  int slice_size;\n  int indices_nd;\n  std::vector<int> dims_to_count;\n};\n\n// Returns common values being used on both `GatherNd` and `GatherNdString`.\ninline GatherNdHelperResult GatherNdHelper(const RuntimeShape& params_shape,\n                                           const RuntimeShape& indices_shape) {\n  GatherNdHelperResult ret;\n  ret.n_slices = 1;\n  ret.slice_size = 1;\n  const int indices_dims = indices_shape.DimensionsCount();\n  ret.indices_nd = indices_shape.Dims(indices_dims - 1);\n  const int params_dims = params_shape.DimensionsCount();\n  for (int i = 0; i < indices_dims - 1; ++i) {\n    ret.n_slices *= indices_shape.Dims(i);\n  }\n  if (ret.n_slices == 0) return ret;\n\n  for (int i = ret.indices_nd; i < params_dims; ++i) {\n    ret.slice_size *= params_shape.Dims(i);\n  }\n\n  int remain_flat_size = params_shape.FlatSize();\n  ret.dims_to_count = std::vector<int>(ret.indices_nd, 0);\n  for (int i = 0; i < ret.indices_nd; ++i) {\n    ret.dims_to_count[i] = remain_flat_size / params_shape.Dims(i);\n    remain_flat_size = ret.dims_to_count[i];\n  }\n\n  return ret;\n}\n\n// Implements GatherNd.\n// Returns an error if any of the indices_data would cause an out of bounds\n// memory read.\ntemplate <typename ParamsT, typename IndicesT = int32>\ninline TfLiteStatus GatherNd(const RuntimeShape& params_shape,\n                             const ParamsT* params_data,\n                             const RuntimeShape& indices_shape,\n                             const IndicesT* indices_data,\n                             const RuntimeShape& output_shape,\n                             ParamsT* output_data) {\n  ruy::profiler::ScopeLabel label(\"GatherNd\");\n\n  const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n  for (int i = 0; i < res.n_slices; ++i) {\n    int64_t from_pos = 0;\n    for (int j = 0; j < res.indices_nd; ++j) {\n      from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n    }\n    if (from_pos < 0 || from_pos + res.slice_size > params_shape.FlatSize()) {\n      return kTfLiteError;\n    }\n    std::memcpy(output_data + i * res.slice_size, params_data + from_pos,\n                sizeof(ParamsT) * res.slice_size);\n  }\n  return kTfLiteOk;\n}\n\n#ifndef TF_LITE_STATIC_MEMORY\n// Implements GatherNd on strings.\n// Returns an error if any of the indices_data would cause an out of bounds\n// memory read.\ntemplate <typename IndicesT = int32>\ninline TfLiteStatus GatherNdString(const RuntimeShape& params_shape,\n                                   const TfLiteTensor* params_data,\n                                   const RuntimeShape& indices_shape,\n                                   const IndicesT* indices_data,\n                                   const RuntimeShape& output_shape,\n                                   TfLiteTensor* output_data) {\n  ruy::profiler::ScopeLabel label(\"GatherNdString\");\n\n  const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n  DynamicBuffer buffer;\n  for (int i = 0; i < res.n_slices; ++i) {\n    int64_t from_pos = 0;\n    for (int j = 0; j < res.indices_nd; ++j) {\n      from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n    }\n    if (from_pos < 0 || from_pos + res.slice_size > params_shape.FlatSize()) {\n      return kTfLiteError;\n    }\n    for (int j = 0; j < res.slice_size; ++j) {\n      buffer.AddString(GetString(params_data, from_pos + j));\n    }\n  }\n  buffer.WriteToTensor(output_data, /*new_shape=*/nullptr);\n  return kTfLiteOk;\n}\n#endif\n\ntemplate <typename IndicesT, typename UpdatesT>\ninline void ScatterNd(const RuntimeShape& indices_shape,\n                      const IndicesT* indices_data,\n                      const RuntimeShape& updates_shape,\n                      const UpdatesT* updates_data,\n                      const RuntimeShape& output_shape, UpdatesT* output_data) {\n  ruy::profiler::ScopeLabel label(\"ScatterNd\");\n\n  int n_slices = 1;\n  int slice_size = 1;\n  const int outer_dims = indices_shape.DimensionsCount() - 1;\n  const int indices_nd = indices_shape.Dims(outer_dims);\n  const int updates_dims = updates_shape.DimensionsCount();\n  for (int i = 0; i < outer_dims; ++i) {\n    n_slices *= indices_shape.Dims(i);\n  }\n  for (int i = outer_dims; i < updates_dims; ++i) {\n    slice_size *= updates_shape.Dims(i);\n  }\n\n  int output_flat_size = output_shape.FlatSize();\n  int remain_flat_size = output_flat_size;\n  std::vector<int> dims_to_count(indices_nd, 0);\n  for (int i = 0; i < indices_nd; ++i) {\n    dims_to_count[i] = remain_flat_size / output_shape.Dims(i);\n    remain_flat_size = dims_to_count[i];\n  }\n\n  memset(output_data, 0, sizeof(UpdatesT) * output_flat_size);\n  for (int i = 0; i < n_slices; ++i) {\n    int to_pos = 0;\n    for (int j = 0; j < indices_nd; ++j) {\n      IndicesT idx = indices_data[i * indices_nd + j];\n      TFLITE_DCHECK(0 <= idx && idx < output_shape.Dims(j));\n      to_pos += idx * dims_to_count[j];\n    }\n    for (int j = 0; j < slice_size; j++) {\n      output_data[to_pos + j] += updates_data[i * slice_size + j];\n    }\n  }\n}\n\ntemplate <typename T>\nvoid Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  const int flat_size = MatchingFlatSize(input1_shape, output_shape);\n\n  auto min_value = input2_data[0];\n  for (int i = 0; i < flat_size; i++) {\n    output_data[i] = input1_data[i] > min_value ? min_value : input1_data[i];\n  }\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Minimum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  const int flat_size = MatchingFlatSize(input1_shape, output_shape);\n\n  auto max_value = input2_data[0];\n  for (int i = 0; i < flat_size; i++) {\n    output_data[i] = input1_data[i] < max_value ? max_value : input1_data[i];\n  }\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Maximum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n            const T3* input2_data, const RuntimeShape& output_shape,\n            T2* output_data) {\n  ArgMinMax(input1_shape, input1_data, input2_data, output_shape, output_data,\n            std::greater<T1>());\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T1, typename T2, typename T3>\ninline void ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n                   const RuntimeShape& input2_shape, const T3* input2_data,\n                   const RuntimeShape& output_shape, T2* output_data) {\n  // Drop shape of second input: not needed.\n  ArgMax(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename D, typename T>\nvoid Select(const RuntimeShape& input_condition_shape,\n            const D* input_condition_data, const RuntimeShape& input_x_shape,\n            const T* input_x_data, const RuntimeShape& input_y_shape,\n            const T* input_y_data, const RuntimeShape& output_shape,\n            T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Select\");\n  int64_t flatsize;\n  // Allow select operator executions on mixed scalar tensors and one element\n  // tensors.\n  if (input_condition_shape.FlatSize() == 1 && input_x_shape.FlatSize() == 1 &&\n      input_y_shape.FlatSize() == 1 && output_shape.FlatSize() == 1) {\n    flatsize = 1;\n  } else {\n    flatsize = MatchingFlatSize(input_condition_shape, input_x_shape,\n                                input_y_shape, output_shape);\n  }\n  for (int64_t i = 0; i < flatsize; ++i) {\n    output_data[i] =\n        input_condition_data[i] ? input_x_data[i] : input_y_data[i];\n  }\n}\n\ntemplate <typename D, typename T>\nvoid RankOneSelect(const RuntimeShape& input_condition_shape,\n                   const D* input_condition_data,\n                   const RuntimeShape& input_x_shape, const T* input_x_data,\n                   const RuntimeShape& input_y_shape, const T* input_y_data,\n                   const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Select/RankOneSelect\");\n  const int64_t outer_size = input_condition_shape.FlatSize();\n  int64_t inner_size;\n  if (input_condition_shape.DimensionsCount() == 0) {\n    inner_size = MatchingFlatSize(input_x_shape, input_y_shape, output_shape);\n  } else {\n    TFLITE_DCHECK_EQ(\n        MatchingDim(input_x_shape, 0, input_y_shape, 0, output_shape, 0),\n        outer_size);\n    inner_size =\n        MatchingFlatSizeSkipDim(input_x_shape, 0, input_y_shape, output_shape);\n  }\n\n  int64_t offset = 0;\n  for (int64_t i = 0; i < outer_size; i++) {\n    const T* input_data = input_condition_data[i] ? input_x_data : input_y_data;\n    memcpy(output_data + offset, input_data + offset, inner_size * sizeof(T));\n    offset += inner_size;\n  }\n}\n\ntemplate <typename D, typename T>\nvoid BroadcastSelect5DSlow(const RuntimeShape& input_condition_shape,\n                           const D* input_condition_data,\n                           const RuntimeShape& input_x_shape,\n                           const T* input_x_data,\n                           const RuntimeShape& input_y_shape,\n                           const T* input_y_data,\n                           const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Select/BroadcastSelectSlow\");\n  TFLITE_DCHECK_LE(input_condition_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(input_x_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(input_y_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 5);\n\n  NdArrayDesc<5> desc_condition;\n  NdArrayDesc<5> desc_x;\n  NdArrayDesc<5> desc_y;\n  NdArrayDesc<5> desc_output;\n  const RuntimeShape extended_output_shape =\n      RuntimeShape::ExtendedShape(5, output_shape);\n  CopyDimsToDesc(extended_output_shape, &desc_output);\n  NdArrayDescsForElementwiseBroadcast(input_condition_shape, input_x_shape,\n                                      input_y_shape, &desc_condition, &desc_x,\n                                      &desc_y);\n\n  // In Tensorflow, the dimensions are canonically named (batch_number, row,\n  // col, channel), with extents (batches, height, width, depth), with the\n  // trailing dimension changing most rapidly (channels has the smallest\n  // stride, typically 1 element).\n  //\n  // In generated C code, we store arrays with the dimensions reversed. The\n  // first dimension has smallest stride.\n  //\n  // We name our variables by their Tensorflow convention, but generate C code\n  // nesting loops such that the innermost loop has the smallest stride for\n  // the best cache behavior.\n  for (int n = 0; n < desc_output.extents[0]; ++n) {\n    int out_idx_n = desc_output.extents[1] * n;\n    int cond_idx_n = desc_condition.strides[0] * n;\n    int in_idx1_n = desc_x.strides[0] * n;\n    int in_idx2_n = desc_y.strides[0] * n;\n    for (int b = 0; b < desc_output.extents[1]; ++b) {\n      int out_idx_b = (out_idx_n + b) * desc_output.extents[2];\n      int cond_idx_b = cond_idx_n + desc_condition.strides[1] * b;\n      int in_idx1_b = in_idx1_n + desc_x.strides[1] * b;\n      int in_idx2_b = in_idx2_n + desc_y.strides[1] * b;\n      for (int y = 0; y < desc_output.extents[2]; ++y) {\n        int out_idx_y = (out_idx_b + y) * desc_output.extents[3];\n        int cond_idx_y = cond_idx_b + desc_condition.strides[2] * y;\n        int in_idx1_y = in_idx1_b + desc_x.strides[2] * y;\n        int in_idx2_y = in_idx2_b + desc_y.strides[2] * y;\n        for (int x = 0; x < desc_output.extents[3]; ++x) {\n          int out_idx = (out_idx_y + x) * desc_output.extents[4];\n          int cond_idx = cond_idx_y + desc_condition.strides[3] * x;\n          int in_idx1 = in_idx1_y + desc_x.strides[3] * x;\n          int in_idx2 = in_idx2_y + desc_y.strides[3] * x;\n          for (int c = 0; c < desc_output.extents[4]; ++c) {\n            output_data[out_idx] = input_condition_data[cond_idx]\n                                       ? input_x_data[in_idx1]\n                                       : input_y_data[in_idx2];\n            out_idx++;\n            cond_idx += desc_condition.strides[4];\n            in_idx1 += desc_x.strides[4];\n            in_idx2 += desc_y.strides[4];\n          }\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename D, typename T>\nvoid SelectTrueCoords(const RuntimeShape& input_condition_shape,\n                      const D* input_condition_data, T* output_data) {\n  const size_t size = input_condition_shape.FlatSize();\n  if (size == 0) {\n    // Dimension is zero, in which case we don't need to output.\n    return;\n  }\n  const size_t cond_rank = input_condition_shape.DimensionsCount();\n\n  std::vector<int> dims_to_count(cond_rank, 0);\n  int cur_flat_size = size;\n  for (int i = 0; i < cond_rank; ++i) {\n    dims_to_count[i] = cur_flat_size / input_condition_shape.Dims(i);\n    cur_flat_size = dims_to_count[i];\n  }\n\n  int output_index = 0;\n  for (int i = 0; i < size; ++i) {\n    if (input_condition_data[i] != D(0)) {\n      // Insert the coordinate of the current item (row major) into output.\n      int flat_index = i;\n      for (int j = 0; j < cond_rank; ++j) {\n        int coord_j = flat_index / dims_to_count[j];\n        output_data[output_index * cond_rank + j] = coord_j;\n        flat_index %= dims_to_count[j];\n      }\n      output_index++;\n    }\n  }\n}\n\n// For easy implementation, the indices is always a vector of size-4 vectors.\ntemplate <typename T, typename TI>\ninline void SparseToDense(const std::vector<std::vector<TI>>& indices,\n                          const T* values, T default_value,\n                          bool value_is_scalar,\n                          const RuntimeShape& unextended_output_shape,\n                          T* output_data) {\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n  const int value_count = indices.size();\n\n  // First fill the output_data with default value.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; ++i) {\n    output_data[i] = default_value;\n  }\n\n  // Special handle for value is scalar case to avoid checking the boolean\n  // condition within the loop every time.\n  if (value_is_scalar) {\n    for (int i = 0; i < value_count; ++i) {\n      const std::vector<TI>& index = indices[i];\n      TFLITE_DCHECK_EQ(index.size(), 4);\n      const T value = *values;  // just use the first value.\n      output_data[Offset(output_shape, index[0], index[1], index[2],\n                         index[3])] = value;\n    }\n    return;\n  }\n\n  // Go through the values and indices to fill the sparse values.\n  for (int i = 0; i < value_count; ++i) {\n    const std::vector<TI>& index = indices[i];\n    TFLITE_DCHECK_EQ(index.size(), 4);\n    const T value = values[i];\n    output_data[Offset(output_shape, index[0], index[1], index[2], index[3])] =\n        value;\n  }\n}\n\ntemplate <typename T>\ninline void Pow(const RuntimeShape& input1_shape, const T* input1_data,\n                const RuntimeShape& input2_shape, const T* input2_data,\n                const RuntimeShape& output_shape, T* output_data) {\n  const int flat_size =\n      MatchingFlatSize(input1_shape, input2_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = std::pow(input1_data[i], input2_data[i]);\n  }\n}\n\ntemplate <typename T>\ninline void BroadcastPow4DSlow(const RuntimeShape& unextended_input1_shape,\n                               const T* input1_data,\n                               const RuntimeShape& unextended_input2_shape,\n                               const T* input2_data,\n                               const RuntimeShape& unextended_output_shape,\n                               T* output_data) {\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  NdArrayDesc<4> desc1;\n  NdArrayDesc<4> desc2;\n  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,\n                                      unextended_input2_shape, &desc1, &desc2);\n\n  for (int b = 0; b < output_shape.Dims(0); ++b) {\n    for (int y = 0; y < output_shape.Dims(1); ++y) {\n      for (int x = 0; x < output_shape.Dims(2); ++x) {\n        for (int c = 0; c < output_shape.Dims(3); ++c) {\n          auto out_idx = Offset(output_shape, b, y, x, c);\n          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);\n          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);\n          auto in1_val = input1_data[in1_idx];\n          auto in2_val = input2_data[in2_idx];\n          output_data[out_idx] = std::pow(in1_val, in2_val);\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid Reverse(int axis, const RuntimeShape& input_shape,\n             const Scalar* input_data, const RuntimeShape& output_shape,\n             Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"Reverse\");\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n\n  int copy_size = 1;\n  for (int i = axis + 1; i < input_shape.DimensionsCount(); ++i) {\n    copy_size *= input_shape.Dims(i);\n  }\n\n  const int dims_at_axis = input_shape.Dims(axis);\n  for (int i = 0; i < outer_size; ++i) {\n    for (int j = 0; j < dims_at_axis; ++j) {\n      const int start_pos = (i * dims_at_axis + j) * copy_size;\n      Scalar* output_ptr = output_data + start_pos;\n      int loc = (i * dims_at_axis + dims_at_axis - j - 1) * copy_size;\n      memcpy(output_ptr, input_data + loc, copy_size * sizeof(Scalar));\n    }\n  }\n}\n\ntemplate <typename Scalar, typename TS>\nvoid ReverseSequence(const TS* seq_lengths, const int seq_dim,\n                     const int batch_dim, const RuntimeShape& input_shape,\n                     const Scalar* input_data, const RuntimeShape& output_shape,\n                     Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"ReverseSequence\");\n\n  int outer_size = 1;\n  int outer_dim = std::min(batch_dim, seq_dim);\n  int medium_dim = std::max(batch_dim, seq_dim);\n  for (int i = 0; i < outer_dim; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n\n  int medium_size = 1;\n  for (int i = outer_dim + 1; i < medium_dim; ++i) {\n    medium_size *= input_shape.Dims(i);\n  }\n\n  int copy_size = 1;\n  for (int i = medium_dim + 1; i < input_shape.DimensionsCount(); ++i) {\n    copy_size *= input_shape.Dims(i);\n  }\n\n  const int dims_at_outer_dim = input_shape.Dims(outer_dim);\n  const int dims_at_medium_dim = input_shape.Dims(medium_dim);\n\n  Scalar* output_ptr;\n  if (batch_dim > seq_dim) {\n    for (int i = 0; i < outer_size; ++i) {\n      for (int j = 0; j < dims_at_outer_dim; ++j) {\n        const int in_pos_base = (i * dims_at_outer_dim + j) * medium_size;\n        for (int p = 0; p < medium_size; ++p) {\n          for (int q = 0; q < dims_at_medium_dim; ++q) {\n            const int in_pos =\n                ((in_pos_base + p) * dims_at_medium_dim + q) * copy_size;\n            const Scalar* in_ptr = input_data + in_pos;\n            int sl = seq_lengths[q] - 1;\n            if (j > sl) {\n              output_ptr = output_data + in_pos;\n            } else {\n              const int out_pos_base =\n                  (i * dims_at_outer_dim + sl - j) * medium_size;\n              const int out_pos =\n                  ((out_pos_base + p) * dims_at_medium_dim + q) * copy_size;\n              output_ptr = output_data + out_pos;\n            }\n            memcpy(output_ptr, in_ptr, copy_size * sizeof(Scalar));\n          }\n        }\n      }\n    }\n  } else if (batch_dim < seq_dim) {\n    for (int i = 0; i < outer_size; ++i) {\n      for (int j = 0; j < dims_at_outer_dim; ++j) {\n        const int in_pos_base = (i * dims_at_outer_dim + j) * medium_size;\n        int sl = seq_lengths[j] - 1;\n        const int out_pos_base = (i * dims_at_outer_dim + j) * medium_size;\n        for (int p = 0; p < medium_size; ++p) {\n          for (int q = 0; q < dims_at_medium_dim; ++q) {\n            const int in_pos =\n                ((in_pos_base + p) * dims_at_medium_dim + q) * copy_size;\n            const Scalar* in_ptr = input_data + in_pos;\n            if (q > sl) {\n              output_ptr = output_data + in_pos;\n            } else {\n              const int out_pos =\n                  ((out_pos_base + p) * dims_at_medium_dim + sl - q) *\n                  copy_size;\n              output_ptr = output_data + out_pos;\n            }\n            memcpy(output_ptr, in_ptr, copy_size * sizeof(Scalar));\n          }\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void SegmentSum(const RuntimeShape& input_shape, const T* input_data,\n                       const RuntimeShape& segment_ids_shape,\n                       const int32_t* segment_ids_data,\n                       const RuntimeShape& output_shape, T* output_data) {\n  const int segment_flat_size =\n      MatchingFlatSizeSkipDim(input_shape, 0, output_shape);\n\n  memset(output_data, 0, sizeof(T) * output_shape.FlatSize());\n\n  for (int i = 0; i < input_shape.Dims(0); i++) {\n    int output_index = segment_ids_data[i];\n    for (int j = 0; j < segment_flat_size; ++j) {\n      output_data[output_index * segment_flat_size + j] +=\n          input_data[i * segment_flat_size + j];\n    }\n  }\n}\n\ntemplate <typename T, template <typename T2> typename Op>\ninline void UnsortedSegmentRef(const RuntimeShape& input_shape,\n                               const T* input_data,\n                               const RuntimeShape& segment_ids_shape,\n                               const int32_t* segment_ids_data,\n                               const RuntimeShape& output_shape,\n                               T* output_data) {\n  for (int i = 0; i < output_shape.FlatSize(); ++i) {\n    output_data[i] = Op<T>::kInitialValue;\n  }\n  Op<T> op;\n  const int segment_flat_size =\n      MatchingFlatSizeSkipDim(input_shape, 0, output_shape);\n  for (int i = 0; i < input_shape.Dims(0); i++) {\n    int output_index = segment_ids_data[i];\n    if (output_index < 0) continue;\n    for (int j = 0; j < segment_flat_size; ++j) {\n      output_data[output_index * segment_flat_size + j] =\n          op(output_data[output_index * segment_flat_size + j],\n             input_data[i * segment_flat_size + j]);\n    }\n  }\n}\n\n}  // namespace reference_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REFERENCE_OPS_H_\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stdint.h>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace scatter_nd {\nconstexpr int kIndices = 0;\nconstexpr int kUpdates = 1;\nconstexpr int kShape = 2;\nconstexpr int kOutputTensor = 0;\n\ntemplate <typename IndicesT>\nTfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* shape,\n                                TfLiteTensor* output) {\n  const int shape_rank = SizeOfDimension(shape, 0);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(shape_rank);\n  const auto* shape_data = GetTensorData<IndicesT>(shape);\n\n  for (int i = 0; i < shape_rank; i++) {\n    output_shape->data[i] = shape_data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}\n\ntemplate <typename IndicesT>\nTfLiteStatus CheckShapes(TfLiteContext* context, const RuntimeShape& indices,\n                         const RuntimeShape& updates,\n                         const RuntimeShape& shape_shape,\n                         const IndicesT* shape_data) {\n  TF_LITE_ENSURE(context, (indices.DimensionsCount() >= 1) &&\n                              (updates.DimensionsCount() >= 1) &&\n                              (shape_shape.DimensionsCount() == 1));\n\n  const int outer_dims = indices.DimensionsCount() - 1;\n  for (int i = 0; i < outer_dims; ++i) {\n    TF_LITE_ENSURE_EQ(context, indices.Dims(i), updates.Dims(i));\n  }\n\n  const int ix = indices.Dims(outer_dims);\n  TF_LITE_ENSURE_EQ(context, updates.DimensionsCount() - outer_dims,\n                    shape_shape.Dims(0) - ix);\n  for (int i = 0; i + outer_dims < updates.DimensionsCount(); ++i) {\n    TF_LITE_ENSURE_EQ(context, updates.Dims(i + outer_dims),\n                      shape_data[ix + i]);\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* indices;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kIndices, &indices));\n  const TfLiteTensor* updates;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kUpdates, &updates));\n  const TfLiteTensor* shape;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kShape, &shape));\n\n  switch (updates->type) {\n    case kTfLiteFloat32:\n    case kTfLiteUInt8:\n    case kTfLiteBool:\n    case kTfLiteInt8:\n    case kTfLiteInt64:\n    case kTfLiteInt32:\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Updates of type '%s' are not supported by scatter_nd.\",\n          TfLiteTypeGetName(updates->type));\n      return kTfLiteError;\n  }\n  if (indices->type != shape->type) {\n    TF_LITE_KERNEL_LOG(context, \"Indices and shape must have the same type.\");\n    return kTfLiteError;\n  }\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  output->type = updates->type;\n\n  if (IsConstantTensor(shape)) {\n    switch (indices->type) {\n      case kTfLiteInt32:\n        TF_LITE_ENSURE_OK(\n            context,\n            CheckShapes<int32_t>(context, GetTensorShape(indices),\n                                 GetTensorShape(updates), GetTensorShape(shape),\n                                 GetTensorData<int32_t>(shape)));\n        return ResizeOutputTensor<int32_t>(context, shape, output);\n      default:\n        TF_LITE_KERNEL_LOG(\n            context, \"Indices of type '%s' are not supported by scatter_nd.\",\n            TfLiteTypeGetName(indices->type));\n        return kTfLiteError;\n    }\n  } else {\n    SetTensorToDynamic(output);\n    return kTfLiteOk;\n  }\n}\n\ntemplate <typename IndicesT, typename UpdatesT>\nTfLiteStatus ScatterNd(const TfLiteTensor* indices, const TfLiteTensor* updates,\n                       TfLiteTensor* output) {\n  reference_ops::ScatterNd(\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(updates), GetTensorData<UpdatesT>(updates),\n      GetTensorShape(output), GetTensorData<UpdatesT>(output));\n  return kTfLiteOk;\n}\n\ntemplate <typename IndicesT>\nTfLiteStatus EvalScatterNd(TfLiteContext* context, const TfLiteTensor* indices,\n                           const TfLiteTensor* updates,\n                           const TfLiteTensor* shape, TfLiteTensor* output) {\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(\n        context, CheckShapes<IndicesT>(\n                     context, GetTensorShape(indices), GetTensorShape(updates),\n                     GetTensorShape(shape), GetTensorData<IndicesT>(shape)));\n    TF_LITE_ENSURE_OK(context,\n                      ResizeOutputTensor<IndicesT>(context, shape, output));\n  }\n\n  switch (updates->type) {\n    case kTfLiteFloat32:\n      return ScatterNd<IndicesT, float>(indices, updates, output);\n    case kTfLiteUInt8:\n      return ScatterNd<IndicesT, uint8_t>(indices, updates, output);\n    case kTfLiteBool:\n      return ScatterNd<IndicesT, bool>(indices, updates, output);\n    case kTfLiteInt8:\n      return ScatterNd<IndicesT, int8_t>(indices, updates, output);\n    case kTfLiteInt32:\n      return ScatterNd<IndicesT, int32_t>(indices, updates, output);\n    case kTfLiteInt64:\n      return ScatterNd<IndicesT, int64_t>(indices, updates, output);\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Updates of type '%s' are not supported by scatter_nd.\",\n          TfLiteTypeGetName(updates->type));\n      return kTfLiteError;\n  }\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* indices;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kIndices, &indices));\n  const TfLiteTensor* updates;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kUpdates, &updates));\n  const TfLiteTensor* shape;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kShape, &shape));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  switch (indices->type) {\n    case kTfLiteInt32:\n      return EvalScatterNd<int32_t>(context, indices, updates, shape, output);\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Indices of type '%s' are not supported by scatter_nd.\",\n          TfLiteTypeGetName(indices->type));\n      return kTfLiteError;\n  }\n}\n\n}  // namespace scatter_nd\n\nTfLiteRegistration* Register_SCATTER_ND() {\n  static TfLiteRegistration r = {/*init*/ nullptr, /*free*/ nullptr,\n                                 scatter_nd::Prepare, scatter_nd::Eval};\n  return &r;\n}\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stdint.h>\n\n#include <initializer_list>\n#include <vector>\n\n#include <gtest/gtest.h>\n#include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n#include \"tensorflow/lite/kernels/test_util.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n\nnamespace tflite {\nnamespace {\n\nusing ::testing::ElementsAreArray;\n\nclass ScatterNdOpModel : public SingleOpModel {\n public:\n  ScatterNdOpModel(const TensorData& indices, const TensorData& updates,\n                   const TensorData& shape) {\n    indices_ = AddInput(indices);\n    updates_ = AddInput(updates);\n    shape_ = AddInput(shape);\n    output_ = AddOutput(updates.type);\n    SetBuiltinOp(BuiltinOperator_SCATTER_ND, BuiltinOptions_ScatterNdOptions,\n                 CreateScatterNdOptions(builder_).Union());\n    BuildInterpreter(\n        {GetShape(indices_), GetShape(updates_), GetShape(shape_)});\n  }\n\n  template <typename T>\n  void SetIndices(std::initializer_list<T> data) {\n    PopulateTensor<T>(indices_, data);\n  }\n\n  template <typename T>\n  void SetUpdates(std::initializer_list<T> data) {\n    PopulateTensor<T>(updates_, data);\n  }\n\n  template <typename T>\n  void SetShape(std::initializer_list<T> data) {\n    PopulateTensor<T>(shape_, data);\n  }\n\n  template <typename T>\n  std::vector<T> GetOutput() {\n    return ExtractVector<T>(output_);\n  }\n\n  std::vector<int> GetOutputShape() { return GetTensorShape(output_); }\n\n protected:\n  int indices_;\n  int updates_;\n  int shape_;\n  int output_;\n};\n\nTEST(ScatterNdOpTest, ScatterElementIntoVector) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 1}}, {TensorType_FLOAT32, {4}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4, 3, 1, 7});\n  m.SetUpdates<float>({9, 10, 11, 12});\n  m.SetShape<int32_t>({8});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({8}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({0, 11, 0, 10, 9, 0, 0, 12}));\n}\n\nTEST(ScatterNdOpTest, ScatterMatrixIntoRank3Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {2, 1}},\n                     {TensorType_FLOAT32, {2, 4, 4}}, {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({0, 2});\n  m.SetUpdates<float>({5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,\n                       5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8});\n  m.SetShape<int32_t>({4, 4, 4});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({4, 4, 4}));\n  EXPECT_THAT(\n      m.GetOutput<float>(),\n      ElementsAreArray({5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}));\n}\n\nTEST(ScatterNdOpTest, ScatterVectorIntoMatrix) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 1}}, {TensorType_FLOAT32, {4, 4}},\n                     {TensorType_INT32, {2}});\n  m.SetIndices<int32_t>({/*0*/ 9, /*1*/ 8, /*2*/ 0, /*3*/ 1});\n  m.SetUpdates<float>({/*0*/ 1, 2, 3, 4,\n                       /*1*/ 5, 6, 7, 8,\n                       /*2*/ 9, 10, 11, 12,\n                       /*3*/ 13, 14, 15, 16});\n  m.SetShape<int32_t>({10, 4});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({10, 4}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({/*0*/ 9,  10, 11, 12,\n                                /*1*/ 13, 14, 15, 16,\n                                /*2*/ 0,  0,  0,  0,\n                                /*3*/ 0,  0,  0,  0,\n                                /*4*/ 0,  0,  0,  0,\n                                /*5*/ 0,  0,  0,  0,\n                                /*6*/ 0,  0,  0,  0,\n                                /*7*/ 0,  0,  0,  0,\n                                /*8*/ 5,  6,  7,  8,\n                                /*9*/ 1,  2,  3,  4}));\n}\n\nTEST(ScatterNdOpTest, ScatterMatricesIntoRank4Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {2, 2, 2}},\n                     {TensorType_FLOAT32, {2, 2, 2, 2}},\n                     {TensorType_INT32, {4}});\n  m.SetIndices<int32_t>(\n      {/*0,0*/ 1, 1, /*0,1*/ 0, 1, /*1,0*/ 0, 0, /*1,1*/ 1, 0});\n  m.SetUpdates<float>({/*0,0*/ 1, 2, 3, 4, /*0,1*/ 5, 6, 7, 8,\n                       /*1,0*/ 9, 10, 11, 12, /*1,1*/ 13, 14, 15, 16});\n  m.SetShape<int32_t>({2, 2, 2, 2});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 2, 2, 2}));\n  EXPECT_THAT(m.GetOutput<float>(), ElementsAreArray({/*0, 0*/ 9, 10, 11, 12,\n                                                      /*0, 1*/ 5, 6, 7, 8,\n                                                      /*1, 0*/ 13, 14, 15, 16,\n                                                      /*1, 1*/ 1, 2, 3, 4}));\n}\n\nTEST(ScatterNdOpTest, ScatterVectorIntoRank4Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {2, 2, 3}},\n                     {TensorType_FLOAT32, {2, 2, 5}}, {TensorType_INT32, {4}});\n  m.SetIndices<int32_t>(\n      {/*0,0*/ 2, 2, 2, /*0,1*/ 1, 0, 1, /*1,0*/ 0, 2, 0, /*1,0*/ 2, 2, 0});\n  m.SetUpdates<float>(\n      {/*0,0*/ 1,  2,  3,  4,  5,  /*0,1*/ 6,  7,  8,  9,  10,\n       /*1,0*/ 11, 12, 13, 14, 15, /*1,1*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({3, 3, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({3, 3, 3, 5}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({\n                  /*0, 0, 0*/ 0,  0,  0,  0,  0,\n                  /*0, 0, 1*/ 0,  0,  0,  0,  0,\n                  /*0, 0, 2*/ 0,  0,  0,  0,  0,\n                  /*0, 1, 0*/ 0,  0,  0,  0,  0,\n                  /*0, 1, 1*/ 0,  0,  0,  0,  0,\n                  /*0, 1, 2*/ 0,  0,  0,  0,  0,\n                  /*0, 2, 0*/ 11, 12, 13, 14, 15,\n                  /*0, 2, 1*/ 0,  0,  0,  0,  0,\n                  /*0, 2, 2*/ 0,  0,  0,  0,  0,\n                  /*1, 0, 0*/ 0,  0,  0,  0,  0,\n                  /*1, 0, 1*/ 6,  7,  8,  9,  10,\n                  /*1, 0, 2*/ 0,  0,  0,  0,  0,\n                  /*1, 1, 0*/ 0,  0,  0,  0,  0,\n                  /*1, 1, 1*/ 0,  0,  0,  0,  0,\n                  /*1, 1, 2*/ 0,  0,  0,  0,  0,\n                  /*1, 2, 0*/ 0,  0,  0,  0,  0,\n                  /*1, 2, 1*/ 0,  0,  0,  0,  0,\n                  /*1, 2, 2*/ 0,  0,  0,  0,  0,\n                  /*2, 0, 0*/ 0,  0,  0,  0,  0,\n                  /*2, 0, 1*/ 0,  0,  0,  0,  0,\n                  /*2, 0, 2*/ 0,  0,  0,  0,  0,\n                  /*2, 1, 0*/ 0,  0,  0,  0,  0,\n                  /*2, 1, 1*/ 0,  0,  0,  0,  0,\n                  /*2, 1, 2*/ 0,  0,  0,  0,  0,\n                  /*2, 2, 0*/ 16, 17, 18, 19, 20,\n                  /*2, 2, 1*/ 0,  0,  0,  0,  0,\n                  /*2, 2, 2*/ 1,  2,  3,  4,  5,\n              }));\n}\n\nTEST(ScatterNdOpTest, ScatterVectorIntoRank3Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_FLOAT32, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<float>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, OverlappedIndicesSummed) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_FLOAT32, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 1, 0, /*1*/ 0, 2, /*2*/ 0, 2, /*3*/ 1, 0});\n  m.SetUpdates<float>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({/*0, 0*/ 0,  0,  0,  0,  0,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 17, 19, 21, 23, 25,\n                                /*1, 0*/ 17, 19, 21, 23, 25,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 0,  0,  0,  0,  0}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesUint8Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_UINT8, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<uint8_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<uint8_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesInt8Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT8, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int8_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int8_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesInt32Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT32, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int32_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int32_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesInt64Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT64, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int64_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int64_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesBoolUpdates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 1}}, {TensorType_BOOL, {4}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4, 3, 1, 7});\n  m.SetUpdates<bool>({true, false, true, false});\n  m.SetShape<int32_t>({8});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({8}));\n  EXPECT_THAT(\n      m.GetOutput<bool>(),\n      ElementsAreArray({false, true, false, false, true, false, false, false}));\n}\n\nTEST(ScatterNdOpTest, DynamicShape) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT64, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int64_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int64_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n\n  m.SetIndices<int32_t>({/*0*/ 2, 3, /*1*/ 1, 0, /*2*/ 2, 0, /*3*/ 1, 2});\n  m.SetShape<int32_t>({3, 4, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({3, 4, 5}));\n  EXPECT_THAT(m.GetOutput<int64_t>(),\n              ElementsAreArray({/*0, 0*/ 0,  0,  0,  0,  0,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 0,  0,  0,  0,  0,\n                                /*0, 3*/ 0,  0,  0,  0,  0,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20,\n                                /*1, 3*/ 0,  0,  0,  0,  0,\n                                /*2, 0*/ 11, 12, 13, 14, 15,\n                                /*2, 1*/ 0,  0,  0,  0,  0,\n                                /*2, 2*/ 0,  0,  0,  0,  0,\n                                /*2, 3*/ 1,  2,  3,  4,  5}));\n}\n\n}  // namespace\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#ifndef TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REFERENCE_OPS_H_\n#define TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REFERENCE_OPS_H_\n\n#include <stdint.h>\n#include <sys/types.h>\n\n#include <algorithm>\n#include <cmath>\n#include <cstring>\n#include <functional>\n#include <limits>\n#include <memory>\n#include <type_traits>\n\n#include \"third_party/eigen3/Eigen/Core\"\n#include \"fixedpoint/fixedpoint.h\"\n#include \"ruy/profiler/instrumentation.h\"  // from @ruy\n#include \"tensorflow/lite/c/c_api_types.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/common.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/add.h\"\n#include \"tensorflow/lite/kernels/internal/reference/add_n.h\"\n#include \"tensorflow/lite/kernels/internal/reference/arg_min_max.h\"\n#include \"tensorflow/lite/kernels/internal/reference/batch_matmul.h\"\n#include \"tensorflow/lite/kernels/internal/reference/batch_to_space_nd.h\"\n#include \"tensorflow/lite/kernels/internal/reference/binary_function.h\"\n#include \"tensorflow/lite/kernels/internal/reference/cast.h\"\n#include \"tensorflow/lite/kernels/internal/reference/ceil.h\"\n#include \"tensorflow/lite/kernels/internal/reference/comparisons.h\"\n#include \"tensorflow/lite/kernels/internal/reference/concatenation.h\"\n#include \"tensorflow/lite/kernels/internal/reference/conv.h\"\n#include \"tensorflow/lite/kernels/internal/reference/depth_to_space.h\"\n#include \"tensorflow/lite/kernels/internal/reference/dequantize.h\"\n#include \"tensorflow/lite/kernels/internal/reference/div.h\"\n#include \"tensorflow/lite/kernels/internal/reference/elu.h\"\n#include \"tensorflow/lite/kernels/internal/reference/exp.h\"\n#include \"tensorflow/lite/kernels/internal/reference/fill.h\"\n#include \"tensorflow/lite/kernels/internal/reference/floor.h\"\n#include \"tensorflow/lite/kernels/internal/reference/floor_div.h\"\n#include \"tensorflow/lite/kernels/internal/reference/floor_mod.h\"\n#include \"tensorflow/lite/kernels/internal/reference/fully_connected.h\"\n#include \"tensorflow/lite/kernels/internal/reference/gather.h\"\n#include \"tensorflow/lite/kernels/internal/reference/hard_swish.h\"\n#include \"tensorflow/lite/kernels/internal/reference/l2normalization.h\"\n#include \"tensorflow/lite/kernels/internal/reference/leaky_relu.h\"\n#include \"tensorflow/lite/kernels/internal/reference/log_softmax.h\"\n#include \"tensorflow/lite/kernels/internal/reference/logistic.h\"\n#include \"tensorflow/lite/kernels/internal/reference/lstm_cell.h\"\n#include \"tensorflow/lite/kernels/internal/reference/maximum_minimum.h\"\n#include \"tensorflow/lite/kernels/internal/reference/mul.h\"\n#include \"tensorflow/lite/kernels/internal/reference/neg.h\"\n#include \"tensorflow/lite/kernels/internal/reference/pad.h\"\n#include \"tensorflow/lite/kernels/internal/reference/pooling.h\"\n#include \"tensorflow/lite/kernels/internal/reference/prelu.h\"\n#include \"tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h\"\n#include \"tensorflow/lite/kernels/internal/reference/quantize.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reduce.h\"\n#include \"tensorflow/lite/kernels/internal/reference/requantize.h\"\n#include \"tensorflow/lite/kernels/internal/reference/resize_bilinear.h\"\n#include \"tensorflow/lite/kernels/internal/reference/resize_nearest_neighbor.h\"\n#include \"tensorflow/lite/kernels/internal/reference/round.h\"\n#include \"tensorflow/lite/kernels/internal/reference/slice.h\"\n#include \"tensorflow/lite/kernels/internal/reference/softmax.h\"\n#include \"tensorflow/lite/kernels/internal/reference/space_to_batch_nd.h\"\n#include \"tensorflow/lite/kernels/internal/reference/space_to_depth.h\"\n#include \"tensorflow/lite/kernels/internal/reference/strided_slice.h\"\n#include \"tensorflow/lite/kernels/internal/reference/string_comparisons.h\"\n#include \"tensorflow/lite/kernels/internal/reference/sub.h\"\n#include \"tensorflow/lite/kernels/internal/reference/tanh.h\"\n#include \"tensorflow/lite/kernels/internal/reference/transpose.h\"\n#include \"tensorflow/lite/kernels/internal/reference/transpose_conv.h\"\n#include \"tensorflow/lite/kernels/internal/strided_slice_logic.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\nnamespace tflite {\n\nnamespace reference_ops {\n\ntemplate <typename T>\ninline void Relu(const RuntimeShape& input_shape, const T* input_data,\n                 const RuntimeShape& output_shape, T* output_data) {\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T lower = 0;\n    const T clamped = val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <typename T>\ninline void Relu0To1(const RuntimeShape& input_shape, const T* input_data,\n                     const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu0To1 (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T upper = 1;\n    const T lower = 0;\n    const T clamped = val > upper ? upper : val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <typename T>\ninline void Relu1(const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu1 (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T upper = 1;\n    const T lower = -1;\n    const T clamped = val > upper ? upper : val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ninline void Relu6(const RuntimeShape& input_shape, const float* input_data,\n                  const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"Relu6 (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const float val = input_data[i];\n    const float upper = 6;\n    const float lower = 0;\n    const float clamped = val > upper ? upper : val < lower ? lower : val;\n    output_data[i] = clamped;\n  }\n}\n\ntemplate <typename T>\ninline void ReluX(const tflite::ReluParams& params,\n                  const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantized ReluX (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    const int32 val = static_cast<int32_t>(input_data[i]);\n    int32 clamped = params.output_offset +\n                    MultiplyByQuantizedMultiplier(val - params.input_offset,\n                                                  params.output_multiplier,\n                                                  params.output_shift);\n    clamped = std::max(params.quantized_activation_min, clamped);\n    clamped = std::min(params.quantized_activation_max, clamped);\n    output_data[i] = static_cast<T>(clamped);\n  }\n}\n\ntemplate <typename T>\ninline void ReluX(const tflite::ActivationParams& params,\n                  const RuntimeShape& input_shape, const T* input_data,\n                  const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Quantized ReluX (not fused)\");\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  const T max_value = params.quantized_activation_max;\n  const T min_value = params.quantized_activation_min;\n  for (int i = 0; i < flat_size; ++i) {\n    const T val = input_data[i];\n    const T clamped = val > max_value   ? max_value\n                      : val < min_value ? min_value\n                                        : val;\n    output_data[i] = clamped;\n  }\n}\n\n// TODO(jiawen): We can implement BroadcastMul on buffers of arbitrary\n// dimensionality if the runtime code does a single loop over one dimension\n// that handles broadcasting as the base case. The code generator would then\n// generate max(D1, D2) nested for loops.\ninline void BroadcastMulFivefold(const ArithmeticParams& unswitched_params,\n                                 const RuntimeShape& unswitched_input1_shape,\n                                 const uint8* unswitched_input1_data,\n                                 const RuntimeShape& unswitched_input2_shape,\n                                 const uint8* unswitched_input2_data,\n                                 const RuntimeShape& output_shape,\n                                 uint8* output_data) {\n  ArithmeticParams switched_params = unswitched_params;\n  switched_params.input1_offset = unswitched_params.input2_offset;\n  switched_params.input2_offset = unswitched_params.input1_offset;\n\n  const bool use_unswitched =\n      unswitched_params.broadcast_category ==\n      tflite::BroadcastableOpCategory::kFirstInputBroadcastsFast;\n\n  const ArithmeticParams& params =\n      use_unswitched ? unswitched_params : switched_params;\n  const uint8* input1_data =\n      use_unswitched ? unswitched_input1_data : unswitched_input2_data;\n  const uint8* input2_data =\n      use_unswitched ? unswitched_input2_data : unswitched_input1_data;\n\n  // Fivefold nested loops. The second input resets its position for each\n  // iteration of the second loop. The first input resets its position at the\n  // beginning of the fourth loop. The innermost loop is an elementwise Mul of\n  // sections of the arrays.\n  uint8* output_data_ptr = output_data;\n  const uint8* input1_data_ptr = input1_data;\n  const uint8* input2_data_reset = input2_data;\n  int y0 = params.broadcast_shape[0];\n  int y1 = params.broadcast_shape[1];\n  int y2 = params.broadcast_shape[2];\n  int y3 = params.broadcast_shape[3];\n  int y4 = params.broadcast_shape[4];\n  for (int i0 = 0; i0 < y0; ++i0) {\n    const uint8* input2_data_ptr;\n    for (int i1 = 0; i1 < y1; ++i1) {\n      input2_data_ptr = input2_data_reset;\n      for (int i2 = 0; i2 < y2; ++i2) {\n        for (int i3 = 0; i3 < y3; ++i3) {\n          MulElementwise(y4, params, input1_data_ptr, input2_data_ptr,\n                         output_data_ptr);\n          input2_data_ptr += y4;\n          output_data_ptr += y4;\n        }\n        input1_data_ptr += y4;\n      }\n    }\n    input2_data_reset = input2_data_ptr;\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, int16* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16\");\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    output_data[i] = unclamped_result.raw();\n  }\n}\n\ninline void Mul(const ArithmeticParams& params,\n                const RuntimeShape& input1_shape, const int16* input1_data,\n                const RuntimeShape& input2_shape, const int16* input2_data,\n                const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"Mul/Int16Uint8\");\n  int32 output_offset = params.output_offset;\n  int32 output_activation_min = params.quantized_activation_min;\n  int32 output_activation_max = params.quantized_activation_max;\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n\n  for (int i = 0; i < flat_size; i++) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n\n    F0 unclamped_result =\n        F0::FromRaw(input1_data[i]) * F0::FromRaw(input2_data[i]);\n    int16 rescaled_result =\n        gemmlowp::RoundingDivideByPOT(unclamped_result.raw(), 8);\n    int16 clamped_result =\n        std::min<int16>(output_activation_max - output_offset, rescaled_result);\n    clamped_result =\n        std::max<int16>(output_activation_min - output_offset, clamped_result);\n    output_data[i] = output_offset + clamped_result;\n  }\n}\n\ninline void Sub16(const ArithmeticParams& params,\n                  const RuntimeShape& input1_shape, const int16_t* input1_data,\n                  const RuntimeShape& input2_shape, const int16_t* input2_data,\n                  const RuntimeShape& output_shape, int16_t* output_data) {\n  ruy::profiler::ScopeLabel label(\"Sub/Int16\");\n  const int input1_shift = params.input1_shift;\n  const int flat_size =\n      MatchingElementsSize(input1_shape, input2_shape, output_shape);\n  const int16 output_activation_min = params.quantized_activation_min;\n  const int16 output_activation_max = params.quantized_activation_max;\n\n  TFLITE_DCHECK(input1_shift == 0 || params.input2_shift == 0);\n  TFLITE_DCHECK_LE(input1_shift, 0);\n  TFLITE_DCHECK_LE(params.input2_shift, 0);\n  const int16* not_shift_input = input1_shift == 0 ? input1_data : input2_data;\n  const int16* shift_input = input1_shift == 0 ? input2_data : input1_data;\n  const int input_right_shift =\n      input1_shift == 0 ? -params.input2_shift : -input1_shift;\n\n  if (input1_shift == 0) {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    for (int i = 0; i < flat_size; ++i) {\n      F0 input_ready_scaled = F0::FromRaw(not_shift_input[i]);\n      F0 scaled_input = F0::FromRaw(\n          gemmlowp::RoundingDivideByPOT(shift_input[i], input_right_shift));\n      F0 result = SaturatingSub(input_ready_scaled, scaled_input);\n      const int16 raw_output = result.raw();\n      const int16 clamped_output = std::min(\n          output_activation_max, std::max(output_activation_min, raw_output));\n      output_data[i] = clamped_output;\n    }\n  } else {\n    // F0 uses 0 integer bits, range [-1, 1].\n    using F0 = gemmlowp::FixedPoint<std::int16_t, 0>;\n    for (int i = 0; i < flat_size; ++i) {\n      F0 input_ready_scaled = F0::FromRaw(not_shift_input[i]);\n      F0 scaled_input = F0::FromRaw(\n          gemmlowp::RoundingDivideByPOT(shift_input[i], input_right_shift));\n      F0 result = SaturatingSub(scaled_input, input_ready_scaled);\n      const int16 raw_output = result.raw();\n      const int16 clamped_output = std::min(\n          output_activation_max, std::max(output_activation_min, raw_output));\n      output_data[i] = clamped_output;\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid Pack(const PackParams& params, const RuntimeShape* const* input_shapes,\n          const Scalar* const* input_data, const RuntimeShape& output_shape,\n          Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"Pack\");\n  const int dimensions = output_shape.DimensionsCount();\n  int axis = params.axis;\n  int inputs_count = params.inputs_count;\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; i++) {\n    outer_size *= output_shape.Dims(i);\n  }\n  int copy_size = 1;\n  for (int i = params.axis + 1; i < dimensions; i++) {\n    copy_size *= output_shape.Dims(i);\n  }\n  TFLITE_DCHECK_EQ((**input_shapes).FlatSize(), copy_size * outer_size);\n\n  for (int i = 0; i < inputs_count; ++i) {\n    for (int k = 0; k < outer_size; k++) {\n      const Scalar* input_ptr = input_data[i] + copy_size * k;\n      int loc = k * inputs_count * copy_size + i * copy_size;\n      memcpy(output_data + loc, input_ptr, copy_size * sizeof(Scalar));\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid Unpack(const UnpackParams& params, const RuntimeShape& input_shape,\n            const Scalar* input_data, const RuntimeShape& output_shape,\n            Scalar* const* output_datas) {\n  ruy::profiler::ScopeLabel label(\"Unpack\");\n  const int dimensions = input_shape.DimensionsCount();\n  const int outputs_count = params.num_split;\n\n  int outer_size = 1;\n  int axis = params.axis;\n  if (axis < 0) {\n    axis += dimensions;\n  }\n  TFLITE_DCHECK_GE(axis, 0);\n  TFLITE_DCHECK_LT(axis, dimensions);\n  for (int i = 0; i < axis; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n  int copy_size = 1;\n  for (int i = axis + 1; i < dimensions; ++i) {\n    copy_size *= input_shape.Dims(i);\n  }\n  TFLITE_DCHECK_EQ(output_shape.FlatSize(), copy_size * outer_size);\n\n  for (int i = 0; i < outputs_count; ++i) {\n    for (int k = 0; k < outer_size; k++) {\n      Scalar* output_ptr = output_datas[i] + copy_size * k;\n      int loc = k * outputs_count * copy_size + i * copy_size;\n      memcpy(output_ptr, input_data + loc, copy_size * sizeof(Scalar));\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid PackWithScaling(const PackParams& params,\n                     const RuntimeShape* const* input_shapes,\n                     const uint8* const* input_data,\n                     const RuntimeShape& output_shape, uint8* output_data) {\n  ruy::profiler::ScopeLabel label(\"PackWithScaling\");\n  const int dimensions = output_shape.DimensionsCount();\n  int axis = params.axis;\n  const int32* input_zeropoint = params.input_zeropoint;\n  const float* input_scale = params.input_scale;\n  int inputs_count = params.inputs_count;\n  const int32 output_zeropoint = params.output_zeropoint;\n  const float output_scale = params.output_scale;\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; i++) {\n    outer_size *= output_shape.Dims(i);\n  }\n  int copy_size = 1;\n  for (int i = axis + 1; i < dimensions; i++) {\n    copy_size *= output_shape.Dims(i);\n  }\n  TFLITE_DCHECK_EQ((**input_shapes).FlatSize(), copy_size * outer_size);\n\n  Scalar* output_ptr = output_data;\n  const float inverse_output_scale = 1.f / output_scale;\n  for (int k = 0; k < outer_size; k++) {\n    for (int i = 0; i < inputs_count; ++i) {\n      if (input_zeropoint[i] == output_zeropoint &&\n          input_scale[i] == output_scale) {\n        memcpy(output_ptr, input_data[i] + k * copy_size,\n               copy_size * sizeof(Scalar));\n      } else {\n        assert(false);\n        const float scale = input_scale[i] * inverse_output_scale;\n        const float bias = -input_zeropoint[i] * scale;\n        auto input_ptr = input_data[i];\n        for (int j = 0; j < copy_size; ++j) {\n          const int32_t value =\n              static_cast<int32_t>(std::round(input_ptr[j] * scale + bias)) +\n              output_zeropoint;\n          output_ptr[j] =\n              static_cast<uint8_t>(std::max(std::min(255, value), 0));\n        }\n      }\n      output_ptr += copy_size;\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid DepthConcatenation(const ConcatenationParams& params,\n                        const RuntimeShape* const* input_shapes,\n                        const Scalar* const* input_data,\n                        const RuntimeShape& output_shape, Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"DepthConcatenation\");\n  auto params_copy = params;\n  params_copy.axis = 3;\n  Concatenation(params_copy, input_shapes, input_data, output_shape,\n                output_data);\n}\n\ntemplate <typename Scalar>\nvoid Split(const SplitParams& params, const RuntimeShape& input_shape,\n           const Scalar* input_data, const RuntimeShape* const* output_shapes,\n           Scalar* const* output_data) {\n  ruy::profiler::ScopeLabel label(\"Split\");\n  const int split_dimensions = input_shape.DimensionsCount();\n  int axis = params.axis < 0 ? params.axis + split_dimensions : params.axis;\n  int outputs_count = params.num_split;\n  TFLITE_DCHECK_LT(axis, split_dimensions);\n\n  int64_t split_size = 0;\n  for (int i = 0; i < outputs_count; i++) {\n    TFLITE_DCHECK_EQ(output_shapes[i]->DimensionsCount(), split_dimensions);\n    for (int j = 0; j < split_dimensions; j++) {\n      if (j != axis) {\n        MatchingDim(*output_shapes[i], j, input_shape, j);\n      }\n    }\n    split_size += output_shapes[i]->Dims(axis);\n  }\n  TFLITE_DCHECK_EQ(split_size, input_shape.Dims(axis));\n  int64_t outer_size = 1;\n  for (int i = 0; i < axis; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n  // For all output arrays,\n  // FlatSize() = outer_size * Dims(axis) * base_inner_size;\n  int64_t base_inner_size = 1;\n  for (int i = axis + 1; i < split_dimensions; ++i) {\n    base_inner_size *= input_shape.Dims(i);\n  }\n\n  const Scalar* input_ptr = input_data;\n  for (int k = 0; k < outer_size; k++) {\n    for (int i = 0; i < outputs_count; ++i) {\n      const int copy_size = output_shapes[i]->Dims(axis) * base_inner_size;\n      memcpy(output_data[i] + k * copy_size, input_ptr,\n             copy_size * sizeof(Scalar));\n      input_ptr += copy_size;\n    }\n  }\n}\n\ninline int NodeOffset(int b, int h, int w, int height, int width) {\n  return (b * height + h) * width + w;\n}\n\ninline void LocalResponseNormalization(\n    const tflite::LocalResponseNormalizationParams& op_params,\n    const RuntimeShape& input_shape, const float* input_data,\n    const RuntimeShape& output_shape, float* output_data) {\n  const int trailing_dim = input_shape.DimensionsCount() - 1;\n  const int outer_size =\n      MatchingFlatSizeSkipDim(input_shape, trailing_dim, output_shape);\n  const int depth =\n      MatchingDim(input_shape, trailing_dim, output_shape, trailing_dim);\n\n  for (int i = 0; i < outer_size; ++i) {\n    for (int c = 0; c < depth; ++c) {\n      const int begin_input_c = std::max(0, c - op_params.range);\n      const int end_input_c = std::min(depth, c + op_params.range);\n      float accum = 0.f;\n      for (int input_c = begin_input_c; input_c < end_input_c; ++input_c) {\n        const float input_val = input_data[i * depth + input_c];\n        accum += input_val * input_val;\n      }\n      const float multiplier =\n          std::pow(op_params.bias + op_params.alpha * accum, -op_params.beta);\n      output_data[i * depth + c] = input_data[i * depth + c] * multiplier;\n    }\n  }\n}\n\ninline void Dequantize(const RuntimeShape& input_shape,\n                       const Eigen::half* input_data,\n                       const RuntimeShape& output_shape, float* output_data) {\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  for (int i = 0; i < flat_size; i++) {\n    output_data[i] = static_cast<float>(input_data[i]);\n  }\n}\n\ninline void FakeQuant(const tflite::FakeQuantParams& op_params,\n                      const RuntimeShape& input_shape, const float* input_data,\n                      const RuntimeShape& output_shape, float* output_data) {\n  ruy::profiler::ScopeLabel label(\"FakeQuant\");\n  float rmin = op_params.minmax.min;\n  float rmax = op_params.minmax.max;\n  int num_bits = op_params.num_bits;\n  // 0 should always be a representable value. Let's assume that the initial\n  // min,max range contains 0.\n  TFLITE_DCHECK_LE(rmin, 0.0f);\n  TFLITE_DCHECK_GE(rmax, 0.0f);\n  TFLITE_DCHECK_LT(rmin, rmax);\n\n  // Code matches tensorflow's FakeQuantWithMinMaxArgsFunctor.\n  int quant_min = 0;\n  int quant_max = (1 << num_bits) - 1;\n  float nudged_min, nudged_max, nudged_scale;\n  NudgeQuantizationRange(rmin, rmax, quant_min, quant_max, &nudged_min,\n                         &nudged_max, &nudged_scale);\n  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n  FakeQuantizeArray(nudged_scale, nudged_min, nudged_max, input_data,\n                    output_data, flat_size);\n}\n\n// Common subroutine for both `GatherNd` and `GatherNdString`.\nstruct GatherNdHelperResult {\n  int n_slices;\n  int slice_size;\n  int indices_nd;\n  std::vector<int> dims_to_count;\n};\n\n// Returns common values being used on both `GatherNd` and `GatherNdString`.\ninline GatherNdHelperResult GatherNdHelper(const RuntimeShape& params_shape,\n                                           const RuntimeShape& indices_shape) {\n  GatherNdHelperResult ret;\n  ret.n_slices = 1;\n  ret.slice_size = 1;\n  const int indices_dims = indices_shape.DimensionsCount();\n  ret.indices_nd = indices_shape.Dims(indices_dims - 1);\n  const int params_dims = params_shape.DimensionsCount();\n  for (int i = 0; i < indices_dims - 1; ++i) {\n    ret.n_slices *= indices_shape.Dims(i);\n  }\n  if (ret.n_slices == 0) return ret;\n\n  for (int i = ret.indices_nd; i < params_dims; ++i) {\n    ret.slice_size *= params_shape.Dims(i);\n  }\n\n  int remain_flat_size = params_shape.FlatSize();\n  ret.dims_to_count = std::vector<int>(ret.indices_nd, 0);\n  for (int i = 0; i < ret.indices_nd; ++i) {\n    ret.dims_to_count[i] = remain_flat_size / params_shape.Dims(i);\n    remain_flat_size = ret.dims_to_count[i];\n  }\n\n  return ret;\n}\n\n// Implements GatherNd.\n// Returns an error if any of the indices_data would cause an out of bounds\n// memory read.\ntemplate <typename ParamsT, typename IndicesT = int32>\ninline TfLiteStatus GatherNd(const RuntimeShape& params_shape,\n                             const ParamsT* params_data,\n                             const RuntimeShape& indices_shape,\n                             const IndicesT* indices_data,\n                             const RuntimeShape& output_shape,\n                             ParamsT* output_data) {\n  ruy::profiler::ScopeLabel label(\"GatherNd\");\n\n  const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n  for (int i = 0; i < res.n_slices; ++i) {\n    int64_t from_pos = 0;\n    for (int j = 0; j < res.indices_nd; ++j) {\n      from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n    }\n    if (from_pos < 0 || from_pos + res.slice_size > params_shape.FlatSize()) {\n      return kTfLiteError;\n    }\n    std::memcpy(output_data + i * res.slice_size, params_data + from_pos,\n                sizeof(ParamsT) * res.slice_size);\n  }\n  return kTfLiteOk;\n}\n\n#ifndef TF_LITE_STATIC_MEMORY\n// Implements GatherNd on strings.\n// Returns an error if any of the indices_data would cause an out of bounds\n// memory read.\ntemplate <typename IndicesT = int32>\ninline TfLiteStatus GatherNdString(const RuntimeShape& params_shape,\n                                   const TfLiteTensor* params_data,\n                                   const RuntimeShape& indices_shape,\n                                   const IndicesT* indices_data,\n                                   const RuntimeShape& output_shape,\n                                   TfLiteTensor* output_data) {\n  ruy::profiler::ScopeLabel label(\"GatherNdString\");\n\n  const GatherNdHelperResult res = GatherNdHelper(params_shape, indices_shape);\n  DynamicBuffer buffer;\n  for (int i = 0; i < res.n_slices; ++i) {\n    int64_t from_pos = 0;\n    for (int j = 0; j < res.indices_nd; ++j) {\n      from_pos += indices_data[i * res.indices_nd + j] * res.dims_to_count[j];\n    }\n    if (from_pos < 0 || from_pos + res.slice_size > params_shape.FlatSize()) {\n      return kTfLiteError;\n    }\n    for (int j = 0; j < res.slice_size; ++j) {\n      buffer.AddString(GetString(params_data, from_pos + j));\n    }\n  }\n  buffer.WriteToTensor(output_data, /*new_shape=*/nullptr);\n  return kTfLiteOk;\n}\n#endif\n\ntemplate <typename IndicesT, typename UpdatesT>\ninline TfLiteStatus ScatterNd(const RuntimeShape& indices_shape,\n                              const IndicesT* indices_data,\n                              const RuntimeShape& updates_shape,\n                              const UpdatesT* updates_data,\n                              const RuntimeShape& output_shape,\n                              UpdatesT* output_data) {\n  ruy::profiler::ScopeLabel label(\"ScatterNd\");\n\n  int n_slices = 1;\n  int slice_size = 1;\n  const int outer_dims = indices_shape.DimensionsCount() - 1;\n  const int indices_nd = indices_shape.Dims(outer_dims);\n  const int updates_dims = updates_shape.DimensionsCount();\n  for (int i = 0; i < outer_dims; ++i) {\n    n_slices *= indices_shape.Dims(i);\n  }\n  for (int i = outer_dims; i < updates_dims; ++i) {\n    slice_size *= updates_shape.Dims(i);\n  }\n\n  int output_flat_size = output_shape.FlatSize();\n  int remain_flat_size = output_flat_size;\n  std::vector<int> dims_to_count(indices_nd, 0);\n  for (int i = 0; i < indices_nd; ++i) {\n    dims_to_count[i] = remain_flat_size / output_shape.Dims(i);\n    remain_flat_size = dims_to_count[i];\n  }\n\n  if (n_slices * slice_size > updates_shape.FlatSize()) {\n    return kTfLiteError;\n  }\n  memset(output_data, 0, sizeof(UpdatesT) * output_flat_size);\n  for (int i = 0; i < n_slices; ++i) {\n    int to_pos = 0;\n    for (int j = 0; j < indices_nd; ++j) {\n      IndicesT idx = indices_data[i * indices_nd + j];\n      to_pos += idx * dims_to_count[j];\n    }\n    if (to_pos < 0 || to_pos + slice_size > output_flat_size) {\n      return kTfLiteError;\n    }\n    for (int j = 0; j < slice_size; j++) {\n      output_data[to_pos + j] += updates_data[i * slice_size + j];\n    }\n  }\n  return kTfLiteOk;\n}\n\ntemplate <typename T>\nvoid Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  const int flat_size = MatchingFlatSize(input1_shape, output_shape);\n\n  auto min_value = input2_data[0];\n  for (int i = 0; i < flat_size; i++) {\n    output_data[i] = input1_data[i] > min_value ? min_value : input1_data[i];\n  }\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Minimum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Minimum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n             const T* input2_data, const RuntimeShape& output_shape,\n             T* output_data) {\n  const int flat_size = MatchingFlatSize(input1_shape, output_shape);\n\n  auto max_value = input2_data[0];\n  for (int i = 0; i < flat_size; i++) {\n    output_data[i] = input1_data[i] < max_value ? max_value : input1_data[i];\n  }\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T>\ninline void Maximum(const RuntimeShape& input1_shape, const T* input1_data,\n                    const RuntimeShape&, const T* input2_data,\n                    const RuntimeShape& output_shape, T* output_data) {\n  // Drop shape of second input: not needed.\n  Maximum(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename T1, typename T2, typename T3>\nvoid ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n            const T3* input2_data, const RuntimeShape& output_shape,\n            T2* output_data) {\n  ArgMinMax(input1_shape, input1_data, input2_data, output_shape, output_data,\n            std::greater<T1>());\n}\n\n// Convenience version that allows, for example, generated-code calls to be\n// the same as other binary ops.\ntemplate <typename T1, typename T2, typename T3>\ninline void ArgMax(const RuntimeShape& input1_shape, const T1* input1_data,\n                   const RuntimeShape& input2_shape, const T3* input2_data,\n                   const RuntimeShape& output_shape, T2* output_data) {\n  // Drop shape of second input: not needed.\n  ArgMax(input1_shape, input1_data, input2_data, output_shape, output_data);\n}\n\ntemplate <typename D, typename T>\nvoid Select(const RuntimeShape& input_condition_shape,\n            const D* input_condition_data, const RuntimeShape& input_x_shape,\n            const T* input_x_data, const RuntimeShape& input_y_shape,\n            const T* input_y_data, const RuntimeShape& output_shape,\n            T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Select\");\n  int64_t flatsize;\n  // Allow select operator executions on mixed scalar tensors and one element\n  // tensors.\n  if (input_condition_shape.FlatSize() == 1 && input_x_shape.FlatSize() == 1 &&\n      input_y_shape.FlatSize() == 1 && output_shape.FlatSize() == 1) {\n    flatsize = 1;\n  } else {\n    flatsize = MatchingFlatSize(input_condition_shape, input_x_shape,\n                                input_y_shape, output_shape);\n  }\n  for (int64_t i = 0; i < flatsize; ++i) {\n    output_data[i] =\n        input_condition_data[i] ? input_x_data[i] : input_y_data[i];\n  }\n}\n\ntemplate <typename D, typename T>\nvoid RankOneSelect(const RuntimeShape& input_condition_shape,\n                   const D* input_condition_data,\n                   const RuntimeShape& input_x_shape, const T* input_x_data,\n                   const RuntimeShape& input_y_shape, const T* input_y_data,\n                   const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Select/RankOneSelect\");\n  const int64_t outer_size = input_condition_shape.FlatSize();\n  int64_t inner_size;\n  if (input_condition_shape.DimensionsCount() == 0) {\n    inner_size = MatchingFlatSize(input_x_shape, input_y_shape, output_shape);\n  } else {\n    TFLITE_DCHECK_EQ(\n        MatchingDim(input_x_shape, 0, input_y_shape, 0, output_shape, 0),\n        outer_size);\n    inner_size =\n        MatchingFlatSizeSkipDim(input_x_shape, 0, input_y_shape, output_shape);\n  }\n\n  int64_t offset = 0;\n  for (int64_t i = 0; i < outer_size; i++) {\n    const T* input_data = input_condition_data[i] ? input_x_data : input_y_data;\n    memcpy(output_data + offset, input_data + offset, inner_size * sizeof(T));\n    offset += inner_size;\n  }\n}\n\ntemplate <typename D, typename T>\nvoid BroadcastSelect5DSlow(const RuntimeShape& input_condition_shape,\n                           const D* input_condition_data,\n                           const RuntimeShape& input_x_shape,\n                           const T* input_x_data,\n                           const RuntimeShape& input_y_shape,\n                           const T* input_y_data,\n                           const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Select/BroadcastSelectSlow\");\n  TFLITE_DCHECK_LE(input_condition_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(input_x_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(input_y_shape.DimensionsCount(), 5);\n  TFLITE_DCHECK_LE(output_shape.DimensionsCount(), 5);\n\n  NdArrayDesc<5> desc_condition;\n  NdArrayDesc<5> desc_x;\n  NdArrayDesc<5> desc_y;\n  NdArrayDesc<5> desc_output;\n  const RuntimeShape extended_output_shape =\n      RuntimeShape::ExtendedShape(5, output_shape);\n  CopyDimsToDesc(extended_output_shape, &desc_output);\n  NdArrayDescsForElementwiseBroadcast(input_condition_shape, input_x_shape,\n                                      input_y_shape, &desc_condition, &desc_x,\n                                      &desc_y);\n\n  // In Tensorflow, the dimensions are canonically named (batch_number, row,\n  // col, channel), with extents (batches, height, width, depth), with the\n  // trailing dimension changing most rapidly (channels has the smallest\n  // stride, typically 1 element).\n  //\n  // In generated C code, we store arrays with the dimensions reversed. The\n  // first dimension has smallest stride.\n  //\n  // We name our variables by their Tensorflow convention, but generate C code\n  // nesting loops such that the innermost loop has the smallest stride for\n  // the best cache behavior.\n  for (int n = 0; n < desc_output.extents[0]; ++n) {\n    int out_idx_n = desc_output.extents[1] * n;\n    int cond_idx_n = desc_condition.strides[0] * n;\n    int in_idx1_n = desc_x.strides[0] * n;\n    int in_idx2_n = desc_y.strides[0] * n;\n    for (int b = 0; b < desc_output.extents[1]; ++b) {\n      int out_idx_b = (out_idx_n + b) * desc_output.extents[2];\n      int cond_idx_b = cond_idx_n + desc_condition.strides[1] * b;\n      int in_idx1_b = in_idx1_n + desc_x.strides[1] * b;\n      int in_idx2_b = in_idx2_n + desc_y.strides[1] * b;\n      for (int y = 0; y < desc_output.extents[2]; ++y) {\n        int out_idx_y = (out_idx_b + y) * desc_output.extents[3];\n        int cond_idx_y = cond_idx_b + desc_condition.strides[2] * y;\n        int in_idx1_y = in_idx1_b + desc_x.strides[2] * y;\n        int in_idx2_y = in_idx2_b + desc_y.strides[2] * y;\n        for (int x = 0; x < desc_output.extents[3]; ++x) {\n          int out_idx = (out_idx_y + x) * desc_output.extents[4];\n          int cond_idx = cond_idx_y + desc_condition.strides[3] * x;\n          int in_idx1 = in_idx1_y + desc_x.strides[3] * x;\n          int in_idx2 = in_idx2_y + desc_y.strides[3] * x;\n          for (int c = 0; c < desc_output.extents[4]; ++c) {\n            output_data[out_idx] = input_condition_data[cond_idx]\n                                       ? input_x_data[in_idx1]\n                                       : input_y_data[in_idx2];\n            out_idx++;\n            cond_idx += desc_condition.strides[4];\n            in_idx1 += desc_x.strides[4];\n            in_idx2 += desc_y.strides[4];\n          }\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename D, typename T>\nvoid SelectTrueCoords(const RuntimeShape& input_condition_shape,\n                      const D* input_condition_data, T* output_data) {\n  const size_t size = input_condition_shape.FlatSize();\n  if (size == 0) {\n    // Dimension is zero, in which case we don't need to output.\n    return;\n  }\n  const size_t cond_rank = input_condition_shape.DimensionsCount();\n\n  std::vector<int> dims_to_count(cond_rank, 0);\n  int cur_flat_size = size;\n  for (int i = 0; i < cond_rank; ++i) {\n    dims_to_count[i] = cur_flat_size / input_condition_shape.Dims(i);\n    cur_flat_size = dims_to_count[i];\n  }\n\n  int output_index = 0;\n  for (int i = 0; i < size; ++i) {\n    if (input_condition_data[i] != D(0)) {\n      // Insert the coordinate of the current item (row major) into output.\n      int flat_index = i;\n      for (int j = 0; j < cond_rank; ++j) {\n        int coord_j = flat_index / dims_to_count[j];\n        output_data[output_index * cond_rank + j] = coord_j;\n        flat_index %= dims_to_count[j];\n      }\n      output_index++;\n    }\n  }\n}\n\n// For easy implementation, the indices is always a vector of size-4 vectors.\ntemplate <typename T, typename TI>\ninline void SparseToDense(const std::vector<std::vector<TI>>& indices,\n                          const T* values, T default_value,\n                          bool value_is_scalar,\n                          const RuntimeShape& unextended_output_shape,\n                          T* output_data) {\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n  const int value_count = indices.size();\n\n  // First fill the output_data with default value.\n  const int num_elements = output_shape.FlatSize();\n  for (int i = 0; i < num_elements; ++i) {\n    output_data[i] = default_value;\n  }\n\n  // Special handle for value is scalar case to avoid checking the boolean\n  // condition within the loop every time.\n  if (value_is_scalar) {\n    for (int i = 0; i < value_count; ++i) {\n      const std::vector<TI>& index = indices[i];\n      TFLITE_DCHECK_EQ(index.size(), 4);\n      const T value = *values;  // just use the first value.\n      output_data[Offset(output_shape, index[0], index[1], index[2],\n                         index[3])] = value;\n    }\n    return;\n  }\n\n  // Go through the values and indices to fill the sparse values.\n  for (int i = 0; i < value_count; ++i) {\n    const std::vector<TI>& index = indices[i];\n    TFLITE_DCHECK_EQ(index.size(), 4);\n    const T value = values[i];\n    output_data[Offset(output_shape, index[0], index[1], index[2], index[3])] =\n        value;\n  }\n}\n\ntemplate <typename T>\ninline void Pow(const RuntimeShape& input1_shape, const T* input1_data,\n                const RuntimeShape& input2_shape, const T* input2_data,\n                const RuntimeShape& output_shape, T* output_data) {\n  const int flat_size =\n      MatchingFlatSize(input1_shape, input2_shape, output_shape);\n  for (int i = 0; i < flat_size; ++i) {\n    output_data[i] = std::pow(input1_data[i], input2_data[i]);\n  }\n}\n\ntemplate <typename T>\ninline void BroadcastPow4DSlow(const RuntimeShape& unextended_input1_shape,\n                               const T* input1_data,\n                               const RuntimeShape& unextended_input2_shape,\n                               const T* input2_data,\n                               const RuntimeShape& unextended_output_shape,\n                               T* output_data) {\n  TFLITE_DCHECK_LE(unextended_input1_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_input2_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(unextended_output_shape.DimensionsCount(), 4);\n  const RuntimeShape output_shape =\n      RuntimeShape::ExtendedShape(4, unextended_output_shape);\n\n  NdArrayDesc<4> desc1;\n  NdArrayDesc<4> desc2;\n  NdArrayDescsForElementwiseBroadcast(unextended_input1_shape,\n                                      unextended_input2_shape, &desc1, &desc2);\n\n  for (int b = 0; b < output_shape.Dims(0); ++b) {\n    for (int y = 0; y < output_shape.Dims(1); ++y) {\n      for (int x = 0; x < output_shape.Dims(2); ++x) {\n        for (int c = 0; c < output_shape.Dims(3); ++c) {\n          auto out_idx = Offset(output_shape, b, y, x, c);\n          auto in1_idx = SubscriptToIndex(desc1, b, y, x, c);\n          auto in2_idx = SubscriptToIndex(desc2, b, y, x, c);\n          auto in1_val = input1_data[in1_idx];\n          auto in2_val = input2_data[in2_idx];\n          output_data[out_idx] = std::pow(in1_val, in2_val);\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename Scalar>\nvoid Reverse(int axis, const RuntimeShape& input_shape,\n             const Scalar* input_data, const RuntimeShape& output_shape,\n             Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"Reverse\");\n\n  int outer_size = 1;\n  for (int i = 0; i < axis; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n\n  int copy_size = 1;\n  for (int i = axis + 1; i < input_shape.DimensionsCount(); ++i) {\n    copy_size *= input_shape.Dims(i);\n  }\n\n  const int dims_at_axis = input_shape.Dims(axis);\n  for (int i = 0; i < outer_size; ++i) {\n    for (int j = 0; j < dims_at_axis; ++j) {\n      const int start_pos = (i * dims_at_axis + j) * copy_size;\n      Scalar* output_ptr = output_data + start_pos;\n      int loc = (i * dims_at_axis + dims_at_axis - j - 1) * copy_size;\n      memcpy(output_ptr, input_data + loc, copy_size * sizeof(Scalar));\n    }\n  }\n}\n\ntemplate <typename Scalar, typename TS>\nvoid ReverseSequence(const TS* seq_lengths, const int seq_dim,\n                     const int batch_dim, const RuntimeShape& input_shape,\n                     const Scalar* input_data, const RuntimeShape& output_shape,\n                     Scalar* output_data) {\n  ruy::profiler::ScopeLabel label(\"ReverseSequence\");\n\n  int outer_size = 1;\n  int outer_dim = std::min(batch_dim, seq_dim);\n  int medium_dim = std::max(batch_dim, seq_dim);\n  for (int i = 0; i < outer_dim; ++i) {\n    outer_size *= input_shape.Dims(i);\n  }\n\n  int medium_size = 1;\n  for (int i = outer_dim + 1; i < medium_dim; ++i) {\n    medium_size *= input_shape.Dims(i);\n  }\n\n  int copy_size = 1;\n  for (int i = medium_dim + 1; i < input_shape.DimensionsCount(); ++i) {\n    copy_size *= input_shape.Dims(i);\n  }\n\n  const int dims_at_outer_dim = input_shape.Dims(outer_dim);\n  const int dims_at_medium_dim = input_shape.Dims(medium_dim);\n\n  Scalar* output_ptr;\n  if (batch_dim > seq_dim) {\n    for (int i = 0; i < outer_size; ++i) {\n      for (int j = 0; j < dims_at_outer_dim; ++j) {\n        const int in_pos_base = (i * dims_at_outer_dim + j) * medium_size;\n        for (int p = 0; p < medium_size; ++p) {\n          for (int q = 0; q < dims_at_medium_dim; ++q) {\n            const int in_pos =\n                ((in_pos_base + p) * dims_at_medium_dim + q) * copy_size;\n            const Scalar* in_ptr = input_data + in_pos;\n            int sl = seq_lengths[q] - 1;\n            if (j > sl) {\n              output_ptr = output_data + in_pos;\n            } else {\n              const int out_pos_base =\n                  (i * dims_at_outer_dim + sl - j) * medium_size;\n              const int out_pos =\n                  ((out_pos_base + p) * dims_at_medium_dim + q) * copy_size;\n              output_ptr = output_data + out_pos;\n            }\n            memcpy(output_ptr, in_ptr, copy_size * sizeof(Scalar));\n          }\n        }\n      }\n    }\n  } else if (batch_dim < seq_dim) {\n    for (int i = 0; i < outer_size; ++i) {\n      for (int j = 0; j < dims_at_outer_dim; ++j) {\n        const int in_pos_base = (i * dims_at_outer_dim + j) * medium_size;\n        int sl = seq_lengths[j] - 1;\n        const int out_pos_base = (i * dims_at_outer_dim + j) * medium_size;\n        for (int p = 0; p < medium_size; ++p) {\n          for (int q = 0; q < dims_at_medium_dim; ++q) {\n            const int in_pos =\n                ((in_pos_base + p) * dims_at_medium_dim + q) * copy_size;\n            const Scalar* in_ptr = input_data + in_pos;\n            if (q > sl) {\n              output_ptr = output_data + in_pos;\n            } else {\n              const int out_pos =\n                  ((out_pos_base + p) * dims_at_medium_dim + sl - q) *\n                  copy_size;\n              output_ptr = output_data + out_pos;\n            }\n            memcpy(output_ptr, in_ptr, copy_size * sizeof(Scalar));\n          }\n        }\n      }\n    }\n  }\n}\n\ntemplate <typename T>\ninline void SegmentSum(const RuntimeShape& input_shape, const T* input_data,\n                       const RuntimeShape& segment_ids_shape,\n                       const int32_t* segment_ids_data,\n                       const RuntimeShape& output_shape, T* output_data) {\n  const int segment_flat_size =\n      MatchingFlatSizeSkipDim(input_shape, 0, output_shape);\n\n  memset(output_data, 0, sizeof(T) * output_shape.FlatSize());\n\n  for (int i = 0; i < input_shape.Dims(0); i++) {\n    int output_index = segment_ids_data[i];\n    for (int j = 0; j < segment_flat_size; ++j) {\n      output_data[output_index * segment_flat_size + j] +=\n          input_data[i * segment_flat_size + j];\n    }\n  }\n}\n\ntemplate <typename T, template <typename T2> typename Op>\ninline void UnsortedSegmentRef(const RuntimeShape& input_shape,\n                               const T* input_data,\n                               const RuntimeShape& segment_ids_shape,\n                               const int32_t* segment_ids_data,\n                               const RuntimeShape& output_shape,\n                               T* output_data) {\n  for (int i = 0; i < output_shape.FlatSize(); ++i) {\n    output_data[i] = Op<T>::kInitialValue;\n  }\n  Op<T> op;\n  const int segment_flat_size =\n      MatchingFlatSizeSkipDim(input_shape, 0, output_shape);\n  for (int i = 0; i < input_shape.Dims(0); i++) {\n    int output_index = segment_ids_data[i];\n    if (output_index < 0) continue;\n    for (int j = 0; j < segment_flat_size; ++j) {\n      output_data[output_index * segment_flat_size + j] =\n          op(output_data[output_index * segment_flat_size + j],\n             input_data[i * segment_flat_size + j]);\n    }\n  }\n}\n\n}  // namespace reference_ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_INTERNAL_REFERENCE_REFERENCE_OPS_H_\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include <stdint.h>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace scatter_nd {\nconstexpr int kIndices = 0;\nconstexpr int kUpdates = 1;\nconstexpr int kShape = 2;\nconstexpr int kOutputTensor = 0;\n\ntemplate <typename IndicesT>\nTfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* shape,\n                                TfLiteTensor* output) {\n  const int shape_rank = SizeOfDimension(shape, 0);\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(shape_rank);\n  const auto* shape_data = GetTensorData<IndicesT>(shape);\n\n  for (int i = 0; i < shape_rank; i++) {\n    output_shape->data[i] = shape_data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}\n\ntemplate <typename IndicesT>\nTfLiteStatus CheckShapes(TfLiteContext* context, const RuntimeShape& indices,\n                         const RuntimeShape& updates,\n                         const RuntimeShape& shape_shape,\n                         const IndicesT* shape_data) {\n  TF_LITE_ENSURE(context, (indices.DimensionsCount() >= 1) &&\n                              (updates.DimensionsCount() >= 1) &&\n                              (shape_shape.DimensionsCount() == 1));\n\n  const int outer_dims = indices.DimensionsCount() - 1;\n  for (int i = 0; i < outer_dims; ++i) {\n    TF_LITE_ENSURE_EQ(context, indices.Dims(i), updates.Dims(i));\n  }\n\n  const int ix = indices.Dims(outer_dims);\n  TF_LITE_ENSURE_EQ(context, updates.DimensionsCount() - outer_dims,\n                    shape_shape.Dims(0) - ix);\n  for (int i = 0; i + outer_dims < updates.DimensionsCount(); ++i) {\n    TF_LITE_ENSURE_EQ(context, updates.Dims(i + outer_dims),\n                      shape_data[ix + i]);\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 3);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* indices;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kIndices, &indices));\n  const TfLiteTensor* updates;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kUpdates, &updates));\n  const TfLiteTensor* shape;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kShape, &shape));\n\n  switch (updates->type) {\n    case kTfLiteFloat32:\n    case kTfLiteUInt8:\n    case kTfLiteBool:\n    case kTfLiteInt8:\n    case kTfLiteInt64:\n    case kTfLiteInt32:\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Updates of type '%s' are not supported by scatter_nd.\",\n          TfLiteTypeGetName(updates->type));\n      return kTfLiteError;\n  }\n  if (indices->type != shape->type) {\n    TF_LITE_KERNEL_LOG(context, \"Indices and shape must have the same type.\");\n    return kTfLiteError;\n  }\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  output->type = updates->type;\n\n  if (IsConstantTensor(shape)) {\n    switch (indices->type) {\n      case kTfLiteInt32:\n        TF_LITE_ENSURE_OK(\n            context,\n            CheckShapes<int32_t>(context, GetTensorShape(indices),\n                                 GetTensorShape(updates), GetTensorShape(shape),\n                                 GetTensorData<int32_t>(shape)));\n        return ResizeOutputTensor<int32_t>(context, shape, output);\n      default:\n        TF_LITE_KERNEL_LOG(\n            context, \"Indices of type '%s' are not supported by scatter_nd.\",\n            TfLiteTypeGetName(indices->type));\n        return kTfLiteError;\n    }\n  } else {\n    SetTensorToDynamic(output);\n    return kTfLiteOk;\n  }\n}\n\ntemplate <typename IndicesT, typename UpdatesT>\nTfLiteStatus ScatterNd(const TfLiteTensor* indices, const TfLiteTensor* updates,\n                       TfLiteTensor* output) {\n  return reference_ops::ScatterNd(\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(updates), GetTensorData<UpdatesT>(updates),\n      GetTensorShape(output), GetTensorData<UpdatesT>(output));\n}\n\ntemplate <typename IndicesT>\nTfLiteStatus EvalScatterNd(TfLiteContext* context, const TfLiteTensor* indices,\n                           const TfLiteTensor* updates,\n                           const TfLiteTensor* shape, TfLiteTensor* output) {\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(\n        context, CheckShapes<IndicesT>(\n                     context, GetTensorShape(indices), GetTensorShape(updates),\n                     GetTensorShape(shape), GetTensorData<IndicesT>(shape)));\n    TF_LITE_ENSURE_OK(context,\n                      ResizeOutputTensor<IndicesT>(context, shape, output));\n  }\n\n  TfLiteStatus status = kTfLiteError;\n  switch (updates->type) {\n    case kTfLiteFloat32:\n      status = ScatterNd<IndicesT, float>(indices, updates, output);\n      break;\n    case kTfLiteUInt8:\n      status = ScatterNd<IndicesT, uint8_t>(indices, updates, output);\n      break;\n    case kTfLiteBool:\n      status = ScatterNd<IndicesT, bool>(indices, updates, output);\n      break;\n    case kTfLiteInt8:\n      status = ScatterNd<IndicesT, int8_t>(indices, updates, output);\n      break;\n    case kTfLiteInt32:\n      status = ScatterNd<IndicesT, int32_t>(indices, updates, output);\n      break;\n    case kTfLiteInt64:\n      status = ScatterNd<IndicesT, int64_t>(indices, updates, output);\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Updates of type '%s' are not supported by scatter_nd.\",\n          TfLiteTypeGetName(updates->type));\n      return kTfLiteError;\n  }\n  if (status != kTfLiteOk) {\n    TF_LITE_KERNEL_LOG(context, \"scatter_nd index out of bounds\");\n  }\n  return status;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* indices;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kIndices, &indices));\n  const TfLiteTensor* updates;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kUpdates, &updates));\n  const TfLiteTensor* shape;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kShape, &shape));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  switch (indices->type) {\n    case kTfLiteInt32:\n      return EvalScatterNd<int32_t>(context, indices, updates, shape, output);\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Indices of type '%s' are not supported by scatter_nd.\",\n          TfLiteTypeGetName(indices->type));\n      return kTfLiteError;\n  }\n}\n\n}  // namespace scatter_nd\n\nTfLiteRegistration* Register_SCATTER_ND() {\n  static TfLiteRegistration r = {/*init*/ nullptr, /*free*/ nullptr,\n                                 scatter_nd::Prepare, scatter_nd::Eval};\n  return &r;\n}\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stdint.h>\n\n#include <initializer_list>\n#include <vector>\n\n#include <gtest/gtest.h>\n#include \"flatbuffers/flatbuffers.h\"  // from @flatbuffers\n#include \"tensorflow/lite/kernels/test_util.h\"\n#include \"tensorflow/lite/schema/schema_generated.h\"\n\nnamespace tflite {\nnamespace {\n\nusing ::testing::ElementsAreArray;\n\nclass ScatterNdOpModel : public SingleOpModel {\n public:\n  ScatterNdOpModel(const TensorData& indices, const TensorData& updates,\n                   const TensorData& shape) {\n    indices_ = AddInput(indices);\n    updates_ = AddInput(updates);\n    shape_ = AddInput(shape);\n    output_ = AddOutput(updates.type);\n    SetBuiltinOp(BuiltinOperator_SCATTER_ND, BuiltinOptions_ScatterNdOptions,\n                 CreateScatterNdOptions(builder_).Union());\n    BuildInterpreter(\n        {GetShape(indices_), GetShape(updates_), GetShape(shape_)});\n  }\n\n  template <typename T>\n  void SetIndices(std::initializer_list<T> data) {\n    PopulateTensor<T>(indices_, data);\n  }\n\n  template <typename T>\n  void SetUpdates(std::initializer_list<T> data) {\n    PopulateTensor<T>(updates_, data);\n  }\n\n  template <typename T>\n  void SetShape(std::initializer_list<T> data) {\n    PopulateTensor<T>(shape_, data);\n  }\n\n  template <typename T>\n  std::vector<T> GetOutput() {\n    return ExtractVector<T>(output_);\n  }\n\n  std::vector<int> GetOutputShape() { return GetTensorShape(output_); }\n\n protected:\n  int indices_;\n  int updates_;\n  int shape_;\n  int output_;\n};\n\nTEST(ScatterNdOpTest, ScatterElementIntoVector) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 1}}, {TensorType_FLOAT32, {4}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4, 3, 1, 7});\n  m.SetUpdates<float>({9, 10, 11, 12});\n  m.SetShape<int32_t>({8});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({8}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({0, 11, 0, 10, 9, 0, 0, 12}));\n}\n\nTEST(ScatterNdOpTest, ScatterMatrixIntoRank3Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {2, 1}},\n                     {TensorType_FLOAT32, {2, 4, 4}}, {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({0, 2});\n  m.SetUpdates<float>({5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,\n                       5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8});\n  m.SetShape<int32_t>({4, 4, 4});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({4, 4, 4}));\n  EXPECT_THAT(\n      m.GetOutput<float>(),\n      ElementsAreArray({5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                        5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8,\n                        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}));\n}\n\nTEST(ScatterNdOpTest, ScatterVectorIntoMatrix) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 1}}, {TensorType_FLOAT32, {4, 4}},\n                     {TensorType_INT32, {2}});\n  m.SetIndices<int32_t>({/*0*/ 9, /*1*/ 8, /*2*/ 0, /*3*/ 1});\n  m.SetUpdates<float>({/*0*/ 1, 2, 3, 4,\n                       /*1*/ 5, 6, 7, 8,\n                       /*2*/ 9, 10, 11, 12,\n                       /*3*/ 13, 14, 15, 16});\n  m.SetShape<int32_t>({10, 4});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({10, 4}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({/*0*/ 9,  10, 11, 12,\n                                /*1*/ 13, 14, 15, 16,\n                                /*2*/ 0,  0,  0,  0,\n                                /*3*/ 0,  0,  0,  0,\n                                /*4*/ 0,  0,  0,  0,\n                                /*5*/ 0,  0,  0,  0,\n                                /*6*/ 0,  0,  0,  0,\n                                /*7*/ 0,  0,  0,  0,\n                                /*8*/ 5,  6,  7,  8,\n                                /*9*/ 1,  2,  3,  4}));\n}\n\nTEST(ScatterNdOpTest, ScatterMatricesIntoRank4Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {2, 2, 2}},\n                     {TensorType_FLOAT32, {2, 2, 2, 2}},\n                     {TensorType_INT32, {4}});\n  m.SetIndices<int32_t>(\n      {/*0,0*/ 1, 1, /*0,1*/ 0, 1, /*1,0*/ 0, 0, /*1,1*/ 1, 0});\n  m.SetUpdates<float>({/*0,0*/ 1, 2, 3, 4, /*0,1*/ 5, 6, 7, 8,\n                       /*1,0*/ 9, 10, 11, 12, /*1,1*/ 13, 14, 15, 16});\n  m.SetShape<int32_t>({2, 2, 2, 2});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 2, 2, 2}));\n  EXPECT_THAT(m.GetOutput<float>(), ElementsAreArray({/*0, 0*/ 9, 10, 11, 12,\n                                                      /*0, 1*/ 5, 6, 7, 8,\n                                                      /*1, 0*/ 13, 14, 15, 16,\n                                                      /*1, 1*/ 1, 2, 3, 4}));\n}\n\nTEST(ScatterNdOpTest, ScatterVectorIntoRank4Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {2, 2, 3}},\n                     {TensorType_FLOAT32, {2, 2, 5}}, {TensorType_INT32, {4}});\n  m.SetIndices<int32_t>(\n      {/*0,0*/ 2, 2, 2, /*0,1*/ 1, 0, 1, /*1,0*/ 0, 2, 0, /*1,0*/ 2, 2, 0});\n  m.SetUpdates<float>(\n      {/*0,0*/ 1,  2,  3,  4,  5,  /*0,1*/ 6,  7,  8,  9,  10,\n       /*1,0*/ 11, 12, 13, 14, 15, /*1,1*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({3, 3, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({3, 3, 3, 5}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({\n                  /*0, 0, 0*/ 0,  0,  0,  0,  0,\n                  /*0, 0, 1*/ 0,  0,  0,  0,  0,\n                  /*0, 0, 2*/ 0,  0,  0,  0,  0,\n                  /*0, 1, 0*/ 0,  0,  0,  0,  0,\n                  /*0, 1, 1*/ 0,  0,  0,  0,  0,\n                  /*0, 1, 2*/ 0,  0,  0,  0,  0,\n                  /*0, 2, 0*/ 11, 12, 13, 14, 15,\n                  /*0, 2, 1*/ 0,  0,  0,  0,  0,\n                  /*0, 2, 2*/ 0,  0,  0,  0,  0,\n                  /*1, 0, 0*/ 0,  0,  0,  0,  0,\n                  /*1, 0, 1*/ 6,  7,  8,  9,  10,\n                  /*1, 0, 2*/ 0,  0,  0,  0,  0,\n                  /*1, 1, 0*/ 0,  0,  0,  0,  0,\n                  /*1, 1, 1*/ 0,  0,  0,  0,  0,\n                  /*1, 1, 2*/ 0,  0,  0,  0,  0,\n                  /*1, 2, 0*/ 0,  0,  0,  0,  0,\n                  /*1, 2, 1*/ 0,  0,  0,  0,  0,\n                  /*1, 2, 2*/ 0,  0,  0,  0,  0,\n                  /*2, 0, 0*/ 0,  0,  0,  0,  0,\n                  /*2, 0, 1*/ 0,  0,  0,  0,  0,\n                  /*2, 0, 2*/ 0,  0,  0,  0,  0,\n                  /*2, 1, 0*/ 0,  0,  0,  0,  0,\n                  /*2, 1, 1*/ 0,  0,  0,  0,  0,\n                  /*2, 1, 2*/ 0,  0,  0,  0,  0,\n                  /*2, 2, 0*/ 16, 17, 18, 19, 20,\n                  /*2, 2, 1*/ 0,  0,  0,  0,  0,\n                  /*2, 2, 2*/ 1,  2,  3,  4,  5,\n              }));\n}\n\nTEST(ScatterNdOpTest, ScatterVectorIntoRank3Tensor) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_FLOAT32, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<float>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, OverlappedIndicesSummed) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_FLOAT32, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 1, 0, /*1*/ 0, 2, /*2*/ 0, 2, /*3*/ 1, 0});\n  m.SetUpdates<float>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<float>(),\n              ElementsAreArray({/*0, 0*/ 0,  0,  0,  0,  0,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 17, 19, 21, 23, 25,\n                                /*1, 0*/ 17, 19, 21, 23, 25,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 0,  0,  0,  0,  0}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesUint8Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_UINT8, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<uint8_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<uint8_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesInt8Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT8, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int8_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int8_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesInt32Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT32, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int32_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int32_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesInt64Updates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT64, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int64_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int64_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n}\n\nTEST(ScatterNdOpTest, Int32IndicesBoolUpdates) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 1}}, {TensorType_BOOL, {4}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4, 3, 1, 7});\n  m.SetUpdates<bool>({true, false, true, false});\n  m.SetShape<int32_t>({8});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({8}));\n  EXPECT_THAT(\n      m.GetOutput<bool>(),\n      ElementsAreArray({false, true, false, false, true, false, false, false}));\n}\n\nTEST(ScatterNdOpTest, DynamicShape) {\n  ScatterNdOpModel m({TensorType_INT32, {4, 2}}, {TensorType_INT64, {4, 5}},\n                     {TensorType_INT32, {3}});\n  m.SetIndices<int32_t>({/*0*/ 0, 0, /*1*/ 1, 0, /*2*/ 0, 2, /*3*/ 1, 2});\n  m.SetUpdates<int64_t>(\n      {/*0*/ 1,  2,  3,  4,  5,  /*1*/ 6,  7,  8,  9,  10,\n       /*2*/ 11, 12, 13, 14, 15, /*3*/ 16, 17, 18, 19, 20});\n  m.SetShape<int32_t>({2, 3, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({2, 3, 5}));\n  EXPECT_THAT(m.GetOutput<int64_t>(),\n              ElementsAreArray({/*0, 0*/ 1,  2,  3,  4,  5,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 11, 12, 13, 14, 15,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20}));\n\n  m.SetIndices<int32_t>({/*0*/ 2, 3, /*1*/ 1, 0, /*2*/ 2, 0, /*3*/ 1, 2});\n  m.SetShape<int32_t>({3, 4, 5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({3, 4, 5}));\n  EXPECT_THAT(m.GetOutput<int64_t>(),\n              ElementsAreArray({/*0, 0*/ 0,  0,  0,  0,  0,\n                                /*0, 1*/ 0,  0,  0,  0,  0,\n                                /*0, 2*/ 0,  0,  0,  0,  0,\n                                /*0, 3*/ 0,  0,  0,  0,  0,\n                                /*1, 0*/ 6,  7,  8,  9,  10,\n                                /*1, 1*/ 0,  0,  0,  0,  0,\n                                /*1, 2*/ 16, 17, 18, 19, 20,\n                                /*1, 3*/ 0,  0,  0,  0,  0,\n                                /*2, 0*/ 11, 12, 13, 14, 15,\n                                /*2, 1*/ 0,  0,  0,  0,  0,\n                                /*2, 2*/ 0,  0,  0,  0,  0,\n                                /*2, 3*/ 1,  2,  3,  4,  5}));\n}\n\nTEST(ScatterNdOpTest, ReadAndWriteArrayLimits) {\n  ScatterNdOpModel m({TensorType_INT32, {5, 1}}, {TensorType_INT32, {5}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4, 3, 1, 0, 2});\n  m.SetUpdates<int32_t>({1, 2, 3, 7, 9});\n  m.SetShape<int32_t>({5});\n  ASSERT_EQ(m.Invoke(), kTfLiteOk);\n  EXPECT_THAT(m.GetOutputShape(), ElementsAreArray({5}));\n  EXPECT_THAT(m.GetOutput<int32_t>(), ElementsAreArray({7, 3, 9, 2, 1}));\n}\n\nTEST(ScatterNdOpTest, OOBRead) {\n  ScatterNdOpModel m({TensorType_INT32, {1, 1}}, {TensorType_INT32, {1}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4});\n  m.SetUpdates<int32_t>({1});\n  m.SetShape<int32_t>({1});\n  ASSERT_EQ(m.Invoke(), kTfLiteError);\n}\n\nTEST(ScatterNdOpTest, OOBWrites) {\n  ScatterNdOpModel m({TensorType_INT32, {5, 1}}, {TensorType_INT32, {5}},\n                     {TensorType_INT32, {1}});\n  m.SetIndices<int32_t>({4, 3, 1, -0x38, 0x38});\n  m.SetUpdates<int32_t>({1, 2, 3, 0x44444444, 0x55555555});\n  m.SetShape<int32_t>({1});\n  ASSERT_EQ(m.Invoke(), kTfLiteError);\n}\n\n}  // namespace\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/internal/reference/reference_ops.h", "tensorflow/lite/kernels/scatter_nd.cc", "tensorflow/lite/kernels/scatter_nd_test.cc"], "buggy_code_start_loc": [659, 132, 363], "buggy_code_end_loc": [697, 170, 363], "fixing_code_start_loc": [659, 132, 364], "fixing_code_end_loc": [705, 181, 393], "type": "CWE-787", "message": "TensorFlow is an open source platform for machine learning. The `ScatterNd` function takes an input argument that determines the indices of of the output tensor. An input index greater than the output tensor or less than zero will either write content at the wrong index or trigger a crash. We have patched the issue in GitHub commit b4d4b4cb019bd7240a52daa4ba61e3cc814f0384. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35939", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T20:15:10.243", "lastModified": "2022-09-20T16:27:52.050", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. The `ScatterNd` function takes an input argument that determines the indices of of the output tensor. An input index greater than the output tensor or less than zero will either write content at the wrong index or trigger a crash. We have patched the issue in GitHub commit b4d4b4cb019bd7240a52daa4ba61e3cc814f0384. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. La funci\u00f3n \"ScatterNd\" toma un argumento de entrada que determina los \u00edndices del tensor de salida. Un \u00edndice de entrada mayor que el tensor de salida o menor que cero escribir\u00e1 el contenido en el \u00edndice equivocado o desencadenar\u00e1 un bloqueo. Hemos parcheado el problema en el commit b4d4b4cb019bd7240a52daa4ba61e3cc814f0384 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:L/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 4.7}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/266558ac4c1f361e9a178ee9d3f0ce2e648ae499/tensorflow/lite/kernels/internal/reference/reference_ops.h#L659-L698", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-ffjm-4qwc-7cmf", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/b4d4b4cb019bd7240a52daa4ba61e3cc814f0384"}}