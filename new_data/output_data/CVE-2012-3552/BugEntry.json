{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for inet_sock\n *\n * Authors:\tMany, reorganised here by\n * \t\tArnaldo Carvalho de Melo <acme@mandriva.com>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n#ifndef _INET_SOCK_H\n#define _INET_SOCK_H\n\n\n#include <linux/kmemcheck.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/jhash.h>\n#include <linux/netdevice.h>\n\n#include <net/flow.h>\n#include <net/sock.h>\n#include <net/request_sock.h>\n#include <net/netns/hash.h>\n\n/** struct ip_options - IP Options\n *\n * @faddr - Saved first hop address\n * @is_data - Options in __data, rather than skb\n * @is_strictroute - Strict source route\n * @srr_is_hit - Packet destination addr was our one\n * @is_changed - IP checksum more not valid\n * @rr_needaddr - Need to record addr of outgoing dev\n * @ts_needtime - Need to record timestamp\n * @ts_needaddr - Need to record addr of outgoing dev\n */\nstruct ip_options {\n\t__be32\t\tfaddr;\n\tunsigned char\toptlen;\n\tunsigned char\tsrr;\n\tunsigned char\trr;\n\tunsigned char\tts;\n\tunsigned char\tis_strictroute:1,\n\t\t\tsrr_is_hit:1,\n\t\t\tis_changed:1,\n\t\t\trr_needaddr:1,\n\t\t\tts_needtime:1,\n\t\t\tts_needaddr:1;\n\tunsigned char\trouter_alert;\n\tunsigned char\tcipso;\n\tunsigned char\t__pad2;\n\tunsigned char\t__data[0];\n};\n\n#define optlength(opt) (sizeof(struct ip_options) + opt->optlen)\n\nstruct inet_request_sock {\n\tstruct request_sock\treq;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tu16\t\t\tinet6_rsk_offset;\n#endif\n\t__be16\t\t\tloc_port;\n\t__be32\t\t\tloc_addr;\n\t__be32\t\t\trmt_addr;\n\t__be16\t\t\trmt_port;\n\tkmemcheck_bitfield_begin(flags);\n\tu16\t\t\tsnd_wscale : 4,\n\t\t\t\trcv_wscale : 4,\n\t\t\t\ttstamp_ok  : 1,\n\t\t\t\tsack_ok\t   : 1,\n\t\t\t\twscale_ok  : 1,\n\t\t\t\tecn_ok\t   : 1,\n\t\t\t\tacked\t   : 1,\n\t\t\t\tno_srccheck: 1;\n\tkmemcheck_bitfield_end(flags);\n\tstruct ip_options\t*opt;\n};\n\nstatic inline struct inet_request_sock *inet_rsk(const struct request_sock *sk)\n{\n\treturn (struct inet_request_sock *)sk;\n}\n\nstruct inet_cork {\n\tunsigned int\t\tflags;\n\tunsigned int\t\tfragsize;\n\tstruct ip_options\t*opt;\n\tstruct dst_entry\t*dst;\n\tint\t\t\tlength; /* Total length of all frames */\n\t__be32\t\t\taddr;\n\tstruct flowi\t\tfl;\n\tstruct page\t\t*page;\n\tu32\t\t\toff;\n\tu8\t\t\ttx_flags;\n};\n\nstruct ip_mc_socklist;\nstruct ipv6_pinfo;\nstruct rtable;\n\n/** struct inet_sock - representation of INET sockets\n *\n * @sk - ancestor class\n * @pinet6 - pointer to IPv6 control block\n * @inet_daddr - Foreign IPv4 addr\n * @inet_rcv_saddr - Bound local IPv4 addr\n * @inet_dport - Destination port\n * @inet_num - Local port\n * @inet_saddr - Sending source\n * @uc_ttl - Unicast TTL\n * @inet_sport - Source port\n * @inet_id - ID counter for DF pkts\n * @tos - TOS\n * @mc_ttl - Multicasting TTL\n * @is_icsk - is this an inet_connection_sock?\n * @mc_index - Multicast device index\n * @mc_list - Group array\n * @cork - info to build ip hdr on each ip frag while socket is corked\n */\nstruct inet_sock {\n\t/* sk and pinet6 has to be the first two members of inet_sock */\n\tstruct sock\t\tsk;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tstruct ipv6_pinfo\t*pinet6;\n#endif\n\t/* Socket demultiplex comparisons on incoming packets. */\n#define inet_daddr\t\tsk.__sk_common.skc_daddr\n#define inet_rcv_saddr\t\tsk.__sk_common.skc_rcv_saddr\n\n\t__be16\t\t\tinet_dport;\n\t__u16\t\t\tinet_num;\n\t__be32\t\t\tinet_saddr;\n\t__s16\t\t\tuc_ttl;\n\t__u16\t\t\tcmsg_flags;\n\t__be16\t\t\tinet_sport;\n\t__u16\t\t\tinet_id;\n\n\tstruct ip_options\t*opt;\n\t__u8\t\t\ttos;\n\t__u8\t\t\tmin_ttl;\n\t__u8\t\t\tmc_ttl;\n\t__u8\t\t\tpmtudisc;\n\t__u8\t\t\trecverr:1,\n\t\t\t\tis_icsk:1,\n\t\t\t\tfreebind:1,\n\t\t\t\thdrincl:1,\n\t\t\t\tmc_loop:1,\n\t\t\t\ttransparent:1,\n\t\t\t\tmc_all:1,\n\t\t\t\tnodefrag:1;\n\tint\t\t\tmc_index;\n\t__be32\t\t\tmc_addr;\n\tstruct ip_mc_socklist __rcu\t*mc_list;\n\tstruct inet_cork\tcork;\n};\n\n#define IPCORK_OPT\t1\t/* ip-options has been held in ipcork.opt */\n#define IPCORK_ALLFRAG\t2\t/* always fragment (for ipv6 for now) */\n\nstatic inline struct inet_sock *inet_sk(const struct sock *sk)\n{\n\treturn (struct inet_sock *)sk;\n}\n\nstatic inline void __inet_sk_copy_descendant(struct sock *sk_to,\n\t\t\t\t\t     const struct sock *sk_from,\n\t\t\t\t\t     const int ancestor_size)\n{\n\tmemcpy(inet_sk(sk_to) + 1, inet_sk(sk_from) + 1,\n\t       sk_from->sk_prot->obj_size - ancestor_size);\n}\n#if !(defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE))\nstatic inline void inet_sk_copy_descendant(struct sock *sk_to,\n\t\t\t\t\t   const struct sock *sk_from)\n{\n\t__inet_sk_copy_descendant(sk_to, sk_from, sizeof(struct inet_sock));\n}\n#endif\n\nextern int inet_sk_rebuild_header(struct sock *sk);\n\nextern u32 inet_ehash_secret;\nextern void build_ehash_secret(void);\n\nstatic inline unsigned int inet_ehashfn(struct net *net,\n\t\t\t\t\tconst __be32 laddr, const __u16 lport,\n\t\t\t\t\tconst __be32 faddr, const __be16 fport)\n{\n\treturn jhash_3words((__force __u32) laddr,\n\t\t\t    (__force __u32) faddr,\n\t\t\t    ((__u32) lport) << 16 | (__force __u32)fport,\n\t\t\t    inet_ehash_secret + net_hash_mix(net));\n}\n\nstatic inline int inet_sk_ehashfn(const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst __be32 laddr = inet->inet_rcv_saddr;\n\tconst __u16 lport = inet->inet_num;\n\tconst __be32 faddr = inet->inet_daddr;\n\tconst __be16 fport = inet->inet_dport;\n\tstruct net *net = sock_net(sk);\n\n\treturn inet_ehashfn(net, laddr, lport, faddr, fport);\n}\n\nstatic inline struct request_sock *inet_reqsk_alloc(struct request_sock_ops *ops)\n{\n\tstruct request_sock *req = reqsk_alloc(ops);\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\tif (req != NULL) {\n\t\tkmemcheck_annotate_bitfield(ireq, flags);\n\t\tireq->opt = NULL;\n\t}\n\n\treturn req;\n}\n\nstatic inline __u8 inet_sk_flowi_flags(const struct sock *sk)\n{\n\t__u8 flags = 0;\n\n\tif (inet_sk(sk)->transparent)\n\t\tflags |= FLOWI_FLAG_ANYSRC;\n\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\tflags |= FLOWI_FLAG_PRECOW_METRICS;\n\treturn flags;\n}\n\n#endif\t/* _INET_SOCK_H */\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the IP module.\n *\n * Version:\t@(#)ip.h\t1.0.2\t05/07/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Changes:\n *\t\tMike McLagan    :       Routing by source\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n#ifndef _IP_H\n#define _IP_H\n\n#include <linux/types.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/skbuff.h>\n\n#include <net/inet_sock.h>\n#include <net/snmp.h>\n#include <net/flow.h>\n\nstruct sock;\n\nstruct inet_skb_parm {\n\tstruct ip_options\topt;\t\t/* Compiled IP options\t\t*/\n\tunsigned char\t\tflags;\n\n#define IPSKB_FORWARDED\t\t1\n#define IPSKB_XFRM_TUNNEL_SIZE\t2\n#define IPSKB_XFRM_TRANSFORMED\t4\n#define IPSKB_FRAG_COMPLETE\t8\n#define IPSKB_REROUTED\t\t16\n};\n\nstatic inline unsigned int ip_hdrlen(const struct sk_buff *skb)\n{\n\treturn ip_hdr(skb)->ihl * 4;\n}\n\nstruct ipcm_cookie {\n\t__be32\t\t\taddr;\n\tint\t\t\toif;\n\tstruct ip_options\t*opt;\n\t__u8\t\t\ttx_flags;\n};\n\n#define IPCB(skb) ((struct inet_skb_parm*)((skb)->cb))\n\nstruct ip_ra_chain {\n\tstruct ip_ra_chain __rcu *next;\n\tstruct sock\t\t*sk;\n\tunion {\n\t\tvoid\t\t\t(*destructor)(struct sock *);\n\t\tstruct sock\t\t*saved_sk;\n\t};\n\tstruct rcu_head\t\trcu;\n};\n\nextern struct ip_ra_chain __rcu *ip_ra_chain;\n\n/* IP flags. */\n#define IP_CE\t\t0x8000\t\t/* Flag: \"Congestion\"\t\t*/\n#define IP_DF\t\t0x4000\t\t/* Flag: \"Don't Fragment\"\t*/\n#define IP_MF\t\t0x2000\t\t/* Flag: \"More Fragments\"\t*/\n#define IP_OFFSET\t0x1FFF\t\t/* \"Fragment Offset\" part\t*/\n\n#define IP_FRAG_TIME\t(30 * HZ)\t\t/* fragment lifetime\t*/\n\nstruct msghdr;\nstruct net_device;\nstruct packet_type;\nstruct rtable;\nstruct sockaddr;\n\nextern int\t\tigmp_mc_proc_init(void);\n\n/*\n *\tFunctions provided by ip.c\n */\n\nextern int\t\tip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\t      __be32 saddr, __be32 daddr,\n\t\t\t\t\t      struct ip_options *opt);\nextern int\t\tip_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t\t       struct packet_type *pt, struct net_device *orig_dev);\nextern int\t\tip_local_deliver(struct sk_buff *skb);\nextern int\t\tip_mr_input(struct sk_buff *skb);\nextern int\t\tip_output(struct sk_buff *skb);\nextern int\t\tip_mc_output(struct sk_buff *skb);\nextern int\t\tip_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *));\nextern int\t\tip_do_nat(struct sk_buff *skb);\nextern void\t\tip_send_check(struct iphdr *ip);\nextern int\t\t__ip_local_out(struct sk_buff *skb);\nextern int\t\tip_local_out(struct sk_buff *skb);\nextern int\t\tip_queue_xmit(struct sk_buff *skb);\nextern void\t\tip_init(void);\nextern int\t\tip_append_data(struct sock *sk,\n\t\t\t\t       int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\t\t\t   int odd, struct sk_buff *skb),\n\t\t\t\tvoid *from, int len, int protolen,\n\t\t\t\tstruct ipcm_cookie *ipc,\n\t\t\t\tstruct rtable **rt,\n\t\t\t\tunsigned int flags);\nextern int\t\tip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb);\nextern ssize_t\t\tip_append_page(struct sock *sk, struct page *page,\n\t\t\t\tint offset, size_t size, int flags);\nextern struct sk_buff  *__ip_make_skb(struct sock *sk,\n\t\t\t\t      struct sk_buff_head *queue,\n\t\t\t\t      struct inet_cork *cork);\nextern int\t\tip_send_skb(struct sk_buff *skb);\nextern int\t\tip_push_pending_frames(struct sock *sk);\nextern void\t\tip_flush_pending_frames(struct sock *sk);\nextern struct sk_buff  *ip_make_skb(struct sock *sk,\n\t\t\t\t    int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\t\t\tint odd, struct sk_buff *skb),\n\t\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t\t    struct ipcm_cookie *ipc,\n\t\t\t\t    struct rtable **rtp,\n\t\t\t\t    unsigned int flags);\n\nstatic inline struct sk_buff *ip_finish_skb(struct sock *sk)\n{\n\treturn __ip_make_skb(sk, &sk->sk_write_queue, &inet_sk(sk)->cork);\n}\n\n/* datagram.c */\nextern int\t\tip4_datagram_connect(struct sock *sk, \n\t\t\t\t\t     struct sockaddr *uaddr, int addr_len);\n\n/*\n *\tMap a multicast IP onto multicast MAC for type Token Ring.\n *      This conforms to RFC1469 Option 2 Multicasting i.e.\n *      using a functional address to transmit / receive \n *      multicast packets.\n */\n\nstatic inline void ip_tr_mc_map(__be32 addr, char *buf)\n{\n\tbuf[0]=0xC0;\n\tbuf[1]=0x00;\n\tbuf[2]=0x00;\n\tbuf[3]=0x04;\n\tbuf[4]=0x00;\n\tbuf[5]=0x00;\n}\n\nstruct ip_reply_arg {\n\tstruct kvec iov[1];   \n\tint\t    flags;\n\t__wsum \t    csum;\n\tint\t    csumoffset; /* u16 offset of csum in iov[0].iov_base */\n\t\t\t\t/* -1 if not needed */ \n\tint\t    bound_dev_if;\n}; \n\n#define IP_REPLY_ARG_NOSRCCHECK 1\n\nstatic inline __u8 ip_reply_arg_flowi_flags(const struct ip_reply_arg *arg)\n{\n\treturn (arg->flags & IP_REPLY_ARG_NOSRCCHECK) ? FLOWI_FLAG_ANYSRC : 0;\n}\n\nvoid ip_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg,\n\t\t   unsigned int len); \n\nstruct ipv4_config {\n\tint\tlog_martians;\n\tint\tno_pmtu_disc;\n};\n\nextern struct ipv4_config ipv4_config;\n#define IP_INC_STATS(net, field)\tSNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define IP_INC_STATS_BH(net, field)\tSNMP_INC_STATS64_BH((net)->mib.ip_statistics, field)\n#define IP_ADD_STATS(net, field, val)\tSNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_ADD_STATS_BH(net, field, val) SNMP_ADD_STATS64_BH((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS(net, field, val) SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS_BH(net, field, val) SNMP_UPD_PO_STATS64_BH((net)->mib.ip_statistics, field, val)\n#define NET_INC_STATS(net, field)\tSNMP_INC_STATS((net)->mib.net_statistics, field)\n#define NET_INC_STATS_BH(net, field)\tSNMP_INC_STATS_BH((net)->mib.net_statistics, field)\n#define NET_INC_STATS_USER(net, field) \tSNMP_INC_STATS_USER((net)->mib.net_statistics, field)\n#define NET_ADD_STATS_BH(net, field, adnd) SNMP_ADD_STATS_BH((net)->mib.net_statistics, field, adnd)\n#define NET_ADD_STATS_USER(net, field, adnd) SNMP_ADD_STATS_USER((net)->mib.net_statistics, field, adnd)\n\nextern unsigned long snmp_fold_field(void __percpu *mib[], int offt);\n#if BITS_PER_LONG==32\nextern u64 snmp_fold_field64(void __percpu *mib[], int offt, size_t sync_off);\n#else\nstatic inline u64 snmp_fold_field64(void __percpu *mib[], int offt, size_t syncp_off)\n{\n\treturn snmp_fold_field(mib, offt);\n}\n#endif\nextern int snmp_mib_init(void __percpu *ptr[2], size_t mibsize, size_t align);\nextern void snmp_mib_free(void __percpu *ptr[2]);\n\nextern struct local_ports {\n\tseqlock_t\tlock;\n\tint\t\trange[2];\n} sysctl_local_ports;\nextern void inet_get_local_port_range(int *low, int *high);\n\nextern unsigned long *sysctl_local_reserved_ports;\nstatic inline int inet_is_reserved_local_port(int port)\n{\n\treturn test_bit(port, sysctl_local_reserved_ports);\n}\n\nextern int sysctl_ip_nonlocal_bind;\n\nextern struct ctl_path net_core_path[];\nextern struct ctl_path net_ipv4_ctl_path[];\n\n/* From inetpeer.c */\nextern int inet_peer_threshold;\nextern int inet_peer_minttl;\nextern int inet_peer_maxttl;\nextern int inet_peer_gc_mintime;\nextern int inet_peer_gc_maxtime;\n\n/* From ip_output.c */\nextern int sysctl_ip_dynaddr;\n\nextern void ipfrag_init(void);\n\nextern void ip_static_sysctl_init(void);\n\n#ifdef CONFIG_INET\n#include <net/dst.h>\n\n/* The function in 2.2 was invalid, producing wrong result for\n * check=0xFEFF. It was noticed by Arthur Skawina _year_ ago. --ANK(000625) */\nstatic inline\nint ip_decrease_ttl(struct iphdr *iph)\n{\n\tu32 check = (__force u32)iph->check;\n\tcheck += (__force u32)htons(0x0100);\n\tiph->check = (__force __sum16)(check + (check>=0xFFFF));\n\treturn --iph->ttl;\n}\n\nstatic inline\nint ip_dont_fragment(struct sock *sk, struct dst_entry *dst)\n{\n\treturn  inet_sk(sk)->pmtudisc == IP_PMTUDISC_DO ||\n\t\t(inet_sk(sk)->pmtudisc == IP_PMTUDISC_WANT &&\n\t\t !(dst_metric_locked(dst, RTAX_MTU)));\n}\n\nextern void __ip_select_ident(struct iphdr *iph, struct dst_entry *dst, int more);\n\nstatic inline void ip_select_ident(struct iphdr *iph, struct dst_entry *dst, struct sock *sk)\n{\n\tif (iph->frag_off & htons(IP_DF)) {\n\t\t/* This is only to work around buggy Windows95/2000\n\t\t * VJ compression implementations.  If the ID field\n\t\t * does not change, they drop every other packet in\n\t\t * a TCP stream using header compression.\n\t\t */\n\t\tiph->id = (sk && inet_sk(sk)->inet_daddr) ?\n\t\t\t\t\thtons(inet_sk(sk)->inet_id++) : 0;\n\t} else\n\t\t__ip_select_ident(iph, dst, 0);\n}\n\nstatic inline void ip_select_ident_more(struct iphdr *iph, struct dst_entry *dst, struct sock *sk, int more)\n{\n\tif (iph->frag_off & htons(IP_DF)) {\n\t\tif (sk && inet_sk(sk)->inet_daddr) {\n\t\t\tiph->id = htons(inet_sk(sk)->inet_id);\n\t\t\tinet_sk(sk)->inet_id += 1 + more;\n\t\t} else\n\t\t\tiph->id = 0;\n\t} else\n\t\t__ip_select_ident(iph, dst, more);\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type ethernet.\n */\n\nstatic inline void ip_eth_mc_map(__be32 naddr, char *buf)\n{\n\t__u32 addr=ntohl(naddr);\n\tbuf[0]=0x01;\n\tbuf[1]=0x00;\n\tbuf[2]=0x5e;\n\tbuf[5]=addr&0xFF;\n\taddr>>=8;\n\tbuf[4]=addr&0xFF;\n\taddr>>=8;\n\tbuf[3]=addr&0x7F;\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type IP-over-InfiniBand.\n *\tLeave P_Key as 0 to be filled in by driver.\n */\n\nstatic inline void ip_ib_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\t__u32 addr;\n\tunsigned char scope = broadcast[5] & 0xF;\n\n\tbuf[0]  = 0;\t\t/* Reserved */\n\tbuf[1]  = 0xff;\t\t/* Multicast QPN */\n\tbuf[2]  = 0xff;\n\tbuf[3]  = 0xff;\n\taddr    = ntohl(naddr);\n\tbuf[4]  = 0xff;\n\tbuf[5]  = 0x10 | scope;\t/* scope from broadcast address */\n\tbuf[6]  = 0x40;\t\t/* IPv4 signature */\n\tbuf[7]  = 0x1b;\n\tbuf[8]  = broadcast[8];\t\t/* P_Key */\n\tbuf[9]  = broadcast[9];\n\tbuf[10] = 0;\n\tbuf[11] = 0;\n\tbuf[12] = 0;\n\tbuf[13] = 0;\n\tbuf[14] = 0;\n\tbuf[15] = 0;\n\tbuf[19] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[18] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[17] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[16] = addr & 0x0f;\n}\n\nstatic inline void ip_ipgre_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\tif ((broadcast[0] | broadcast[1] | broadcast[2] | broadcast[3]) != 0)\n\t\tmemcpy(buf, broadcast, 4);\n\telse\n\t\tmemcpy(buf, &naddr, sizeof(naddr));\n}\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#include <linux/ipv6.h>\n#endif\n\nstatic __inline__ void inet_reset_saddr(struct sock *sk)\n{\n\tinet_sk(sk)->inet_rcv_saddr = inet_sk(sk)->inet_saddr = 0;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tif (sk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tmemset(&np->saddr, 0, sizeof(np->saddr));\n\t\tmemset(&np->rcv_saddr, 0, sizeof(np->rcv_saddr));\n\t}\n#endif\n}\n\n#endif\n\nstatic inline int sk_mc_loop(struct sock *sk)\n{\n\tif (!sk)\n\t\treturn 1;\n\tswitch (sk->sk_family) {\n\tcase AF_INET:\n\t\treturn inet_sk(sk)->mc_loop;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tcase AF_INET6:\n\t\treturn inet6_sk(sk)->mc_loop;\n#endif\n\t}\n\tWARN_ON(1);\n\treturn 1;\n}\n\nextern int\tip_call_ra_chain(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_fragment.c\n */\n\nenum ip_defrag_users {\n\tIP_DEFRAG_LOCAL_DELIVER,\n\tIP_DEFRAG_CALL_RA_CHAIN,\n\tIP_DEFRAG_CONNTRACK_IN,\n\t__IP_DEFRAG_CONNTRACK_IN_END\t= IP_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_OUT,\n\t__IP_DEFRAG_CONNTRACK_OUT_END\t= IP_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP_DEFRAG_CONNTRACK_BRIDGE_IN = IP_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n\tIP_DEFRAG_VS_IN,\n\tIP_DEFRAG_VS_OUT,\n\tIP_DEFRAG_VS_FWD\n};\n\nint ip_defrag(struct sk_buff *skb, u32 user);\nint ip_frag_mem(struct net *net);\nint ip_frag_nqueues(struct net *net);\n\n/*\n *\tFunctions provided by ip_forward.c\n */\n \nextern int ip_forward(struct sk_buff *skb);\n \n/*\n *\tFunctions provided by ip_options.c\n */\n \nextern void ip_options_build(struct sk_buff *skb, struct ip_options *opt, __be32 daddr, struct rtable *rt, int is_frag);\nextern int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb);\nextern void ip_options_fragment(struct sk_buff *skb);\nextern int ip_options_compile(struct net *net,\n\t\t\t      struct ip_options *opt, struct sk_buff *skb);\nextern int ip_options_get(struct net *net, struct ip_options **optp,\n\t\t\t  unsigned char *data, int optlen);\nextern int ip_options_get_from_user(struct net *net, struct ip_options **optp,\n\t\t\t\t    unsigned char __user *data, int optlen);\nextern void ip_options_undo(struct ip_options * opt);\nextern void ip_forward_options(struct sk_buff *skb);\nextern int ip_options_rcv_srr(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_sockglue.c\n */\n\nextern int\tip_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);\nextern void\tip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb);\nextern int\tip_cmsg_send(struct net *net,\n\t\t\t     struct msghdr *msg, struct ipcm_cookie *ipc);\nextern int\tip_setsockopt(struct sock *sk, int level, int optname, char __user *optval, unsigned int optlen);\nextern int\tip_getsockopt(struct sock *sk, int level, int optname, char __user *optval, int __user *optlen);\nextern int\tcompat_ip_setsockopt(struct sock *sk, int level,\n\t\t\tint optname, char __user *optval, unsigned int optlen);\nextern int\tcompat_ip_getsockopt(struct sock *sk, int level,\n\t\t\tint optname, char __user *optval, int __user *optlen);\nextern int\tip_ra_control(struct sock *sk, unsigned char on, void (*destructor)(struct sock *));\n\nextern int \tip_recv_error(struct sock *sk, struct msghdr *msg, int len);\nextern void\tip_icmp_error(struct sock *sk, struct sk_buff *skb, int err, \n\t\t\t      __be16 port, u32 info, u8 *payload);\nextern void\tip_local_error(struct sock *sk, int err, __be32 daddr, __be16 dport,\n\t\t\t       u32 info);\n\n#ifdef CONFIG_PROC_FS\nextern int ip_misc_proc_init(void);\n#endif\n\n#endif\t/* _IP_H */\n", "/*\n *  net/dccp/ipv4.c\n *\n *  An implementation of the DCCP protocol\n *  Arnaldo Carvalho de Melo <acme@conectiva.com.br>\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/dccp.h>\n#include <linux/icmp.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/random.h>\n\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_sock.h>\n#include <net/protocol.h>\n#include <net/sock.h>\n#include <net/timewait_sock.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n\n#include \"ackvec.h\"\n#include \"ccid.h\"\n#include \"dccp.h\"\n#include \"feat.h\"\n\n/*\n * The per-net dccp.v4_ctl_sk socket is used for responding to\n * the Out-of-the-blue (OOTB) packets. A control sock will be created\n * for this socket at the initialization time.\n */\n\nint dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tconst struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tif (inet->opt != NULL && inet->opt->srr) {\n\t\tif (daddr == 0)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet->opt->faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_DCCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (inet->opt == NULL || !inet->opt->srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (inet->inet_saddr == 0)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet->opt != NULL)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n\t/*\n\t * Socket identity is still unknown (sport may be zero).\n\t * However we set state to DCCP_REQUESTING and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet_hash_connect(&dccp_death_row, sk);\n\tif (err != 0)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk_setup_caps(sk, &rt->dst);\n\n\tdp->dccps_iss = secure_dccp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t    inet->inet_daddr,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport);\n\tinet->inet_id = dp->dccps_iss ^ jiffies;\n\n\terr = dccp_connect(sk);\n\trt = NULL;\n\tif (err != 0)\n\t\tgoto failure;\nout:\n\treturn err;\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port, if necessary.\n\t */\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\tgoto out;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_connect);\n\n/*\n * This routine does path mtu discovery as defined in RFC1191.\n */\nstatic inline void dccp_do_pmtu_discovery(struct sock *sk,\n\t\t\t\t\t  const struct iphdr *iph,\n\t\t\t\t\t  u32 mtu)\n{\n\tstruct dst_entry *dst;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct dccp_sock *dp = dccp_sk(sk);\n\n\t/* We are not interested in DCCP_LISTEN and request_socks (RESPONSEs\n\t * send out by Linux are always < 576bytes so they should go through\n\t * unfragmented).\n\t */\n\tif (sk->sk_state == DCCP_LISTEN)\n\t\treturn;\n\n\t/* We don't check in the destentry if pmtu discovery is forbidden\n\t * on this route. We just assume that no packet_to_big packets\n\t * are send back when pmtu discovery is not active.\n\t * There is a small race when the user changes this flag in the\n\t * route, but I think that's acceptable.\n\t */\n\tif ((dst = __sk_dst_check(sk, 0)) == NULL)\n\t\treturn;\n\n\tdst->ops->update_pmtu(dst, mtu);\n\n\t/* Something is about to be wrong... Remember soft error\n\t * for the case, if this connection will not able to recover.\n\t */\n\tif (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))\n\t\tsk->sk_err_soft = EMSGSIZE;\n\n\tmtu = dst_mtu(dst);\n\n\tif (inet->pmtudisc != IP_PMTUDISC_DONT &&\n\t    inet_csk(sk)->icsk_pmtu_cookie > mtu) {\n\t\tdccp_sync_mss(sk, mtu);\n\n\t\t/*\n\t\t * From RFC 4340, sec. 14.1:\n\t\t *\n\t\t *\tDCCP-Sync packets are the best choice for upward\n\t\t *\tprobing, since DCCP-Sync probes do not risk application\n\t\t *\tdata loss.\n\t\t */\n\t\tdccp_send_sync(sk, dp->dccps_gsr, DCCP_PKT_SYNC);\n\t} /* else let the usual retransmit timer handle it */\n}\n\n/*\n * This routine is called by the ICMP module when it gets some sort of error\n * condition. If err < 0 then the socket should be closed and the error\n * returned to the user. If err > 0 it's just the icmp type << 8 | icmp code.\n * After adjustment header points to the first 8 bytes of the tcp header. We\n * need to find the appropriate port.\n *\n * The locking strategy used here is very \"optimistic\". When someone else\n * accesses the socket the ICMP is just dropped and for some paths there is no\n * check at all. A more general error queue to queue errors for later handling\n * is probably better.\n */\nstatic void dccp_v4_err(struct sk_buff *skb, u32 info)\n{\n\tconst struct iphdr *iph = (struct iphdr *)skb->data;\n\tconst u8 offset = iph->ihl << 2;\n\tconst struct dccp_hdr *dh = (struct dccp_hdr *)(skb->data + offset);\n\tstruct dccp_sock *dp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\t__u64 seq;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->len < offset + sizeof(*dh) ||\n\t    skb->len < offset + __dccp_basic_hdr_len(dh)) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = inet_lookup(net, &dccp_hashinfo,\n\t\t\tiph->daddr, dh->dccph_dport,\n\t\t\tiph->saddr, dh->dccph_sport, inet_iif(skb));\n\tif (sk == NULL) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t */\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tseq = dccp_hdr_seq(dh);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tdccp_do_pmtu_discovery(sk, iph, info);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req , **prev;\n\tcase DCCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\treq = inet_csk_search_req(sk, &prev, dh->dccph_dport,\n\t\t\t\t\t  iph->daddr, iph->saddr);\n\t\tif (!req)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * ICMPs are not backlogged, hence we cannot get an established\n\t\t * socket here.\n\t\t */\n\t\tWARN_ON(req->sk);\n\n\t\tif (seq != dccp_rsk(req)->dreq_iss) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Still in RESPOND, just remove it silently.\n\t\t * There is no good way to pass the error to the newly\n\t\t * created socket, and POSIX does not want network\n\t\t * errors returned from accept().\n\t\t */\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\nstatic inline __sum16 dccp_v4_csum_finish(struct sk_buff *skb,\n\t\t\t\t      __be32 src, __be32 dst)\n{\n\treturn csum_tcpudp_magic(src, dst, skb->len, IPPROTO_DCCP, skb->csum);\n}\n\nvoid dccp_v4_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tdccp_csum_outgoing(skb);\n\tdh->dccph_checksum = dccp_v4_csum_finish(skb,\n\t\t\t\t\t\t inet->inet_saddr,\n\t\t\t\t\t\t inet->inet_daddr);\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_send_check);\n\nstatic inline u64 dccp_v4_init_sequence(const struct sk_buff *skb)\n{\n\treturn secure_dccp_sequence_number(ip_hdr(skb)->daddr,\n\t\t\t\t\t   ip_hdr(skb)->saddr,\n\t\t\t\t\t   dccp_hdr(skb)->dccph_dport,\n\t\t\t\t\t   dccp_hdr(skb)->dccph_sport);\n}\n\n/*\n * The three way handshake has completed - we got a valid ACK or DATAACK -\n * now create the new socket.\n *\n * This is the equivalent of TCP's tcp_v4_syn_recv_sock\n */\nstruct sock *dccp_v4_request_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct sock *newsk;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (dst == NULL && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto exit_nonewsk;\n\n\tsk_setup_caps(newsk, dst);\n\n\tnewinet\t\t   = inet_sk(newsk);\n\tireq\t\t   = inet_rsk(req);\n\tnewinet->inet_daddr\t= ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t= ireq->loc_addr;\n\tnewinet->opt\t   = ireq->opt;\n\tireq->opt\t   = NULL;\n\tnewinet->mc_index  = inet_iif(skb);\n\tnewinet->mc_ttl\t   = ip_hdr(skb)->ttl;\n\tnewinet->inet_id   = jiffies;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_request_recv_sock);\n\nstatic struct sock *dccp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh = dccp_hdr(skb);\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct sock *nsk;\n\tstruct request_sock **prev;\n\t/* Find possible connection requests. */\n\tstruct request_sock *req = inet_csk_search_req(sk, &prev,\n\t\t\t\t\t\t       dh->dccph_sport,\n\t\t\t\t\t\t       iph->saddr, iph->daddr);\n\tif (req != NULL)\n\t\treturn dccp_check_req(sk, skb, req, prev);\n\n\tnsk = inet_lookup_established(sock_net(sk), &dccp_hashinfo,\n\t\t\t\t      iph->saddr, dh->dccph_sport,\n\t\t\t\t      iph->daddr, dh->dccph_dport,\n\t\t\t\t      inet_iif(skb));\n\tif (nsk != NULL) {\n\t\tif (nsk->sk_state != DCCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic struct dst_entry* dccp_v4_route_skb(struct net *net, struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct rtable *rt;\n\tstruct flowi4 fl4 = {\n\t\t.flowi4_oif = skb_rtable(skb)->rt_iif,\n\t\t.daddr = ip_hdr(skb)->saddr,\n\t\t.saddr = ip_hdr(skb)->daddr,\n\t\t.flowi4_tos = RT_CONN_FLAGS(sk),\n\t\t.flowi4_proto = sk->sk_protocol,\n\t\t.fl4_sport = dccp_hdr(skb)->dccph_dport,\n\t\t.fl4_dport = dccp_hdr(skb)->dccph_sport,\n\t};\n\n\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\t\treturn NULL;\n\t}\n\n\treturn &rt->dst;\n}\n\nstatic int dccp_v4_send_response(struct sock *sk, struct request_sock *req,\n\t\t\t\t struct request_values *rv_unused)\n{\n\tint err = -1;\n\tstruct sk_buff *skb;\n\tstruct dst_entry *dst;\n\n\tdst = inet_csk_route_req(sk, req);\n\tif (dst == NULL)\n\t\tgoto out;\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v4_csum_finish(skb, ireq->loc_addr,\n\t\t\t\t\t\t\t      ireq->rmt_addr);\n\t\terr = ip_build_and_send_pkt(skb, sk, ireq->loc_addr,\n\t\t\t\t\t    ireq->rmt_addr,\n\t\t\t\t\t    ireq->opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\nout:\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic void dccp_v4_ctl_send_reset(struct sock *sk, struct sk_buff *rxskb)\n{\n\tint err;\n\tconst struct iphdr *rxiph;\n\tstruct sk_buff *skb;\n\tstruct dst_entry *dst;\n\tstruct net *net = dev_net(skb_dst(rxskb)->dev);\n\tstruct sock *ctl_sk = net->dccp.v4_ctl_sk;\n\n\t/* Never send a reset in response to a reset. */\n\tif (dccp_hdr(rxskb)->dccph_type == DCCP_PKT_RESET)\n\t\treturn;\n\n\tif (skb_rtable(rxskb)->rt_type != RTN_LOCAL)\n\t\treturn;\n\n\tdst = dccp_v4_route_skb(net, ctl_sk, rxskb);\n\tif (dst == NULL)\n\t\treturn;\n\n\tskb = dccp_ctl_make_reset(ctl_sk, rxskb);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\trxiph = ip_hdr(rxskb);\n\tdccp_hdr(skb)->dccph_checksum = dccp_v4_csum_finish(skb, rxiph->saddr,\n\t\t\t\t\t\t\t\t rxiph->daddr);\n\tskb_dst_set(skb, dst_clone(dst));\n\n\tbh_lock_sock(ctl_sk);\n\terr = ip_build_and_send_pkt(skb, ctl_sk,\n\t\t\t\t    rxiph->daddr, rxiph->saddr, NULL);\n\tbh_unlock_sock(ctl_sk);\n\n\tif (net_xmit_eval(err) == 0) {\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTSEGS);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTRSTS);\n\t}\nout:\n\t dst_release(dst);\n}\n\nstatic void dccp_v4_reqsk_destructor(struct request_sock *req)\n{\n\tdccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);\n\tkfree(inet_rsk(req)->opt);\n}\n\nstatic struct request_sock_ops dccp_request_sock_ops __read_mostly = {\n\t.family\t\t= PF_INET,\n\t.obj_size\t= sizeof(struct dccp_request_sock),\n\t.rtx_syn_ack\t= dccp_v4_send_response,\n\t.send_ack\t= dccp_reqsk_send_ack,\n\t.destructor\t= dccp_v4_reqsk_destructor,\n\t.send_reset\t= dccp_v4_ctl_send_reset,\n};\n\nint dccp_v4_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct request_sock *req;\n\tstruct dccp_request_sock *dreq;\n\tconst __be32 service = dccp_hdr_request(skb)->dccph_req_service;\n\tstruct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);\n\n\t/* Never answer to DCCP_PKT_REQUESTs send to broadcast or multicast */\n\tif (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\treturn 0;\t/* discard, don't send a reset here */\n\n\tif (dccp_bad_service_code(sk, service)) {\n\t\tdcb->dccpd_reset_code = DCCP_RESET_CODE_BAD_SERVICE_CODE;\n\t\tgoto drop;\n\t}\n\t/*\n\t * TW buckets are converted to open requests without\n\t * limitations, they conserve resources and peer is\n\t * evidently real one.\n\t */\n\tdcb->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto drop;\n\n\t/*\n\t * Accept backlog is full. If we have already queued enough\n\t * of warm entries in syn queue, drop request. It is better than\n\t * clogging syn queue with openreqs with exponentially increasing\n\t * timeout.\n\t */\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet_reqsk_alloc(&dccp_request_sock_ops);\n\tif (req == NULL)\n\t\tgoto drop;\n\n\tif (dccp_reqsk_init(req, dccp_sk(sk), skb))\n\t\tgoto drop_and_free;\n\n\tdreq = dccp_rsk(req);\n\tif (dccp_parse_options(sk, dreq, skb))\n\t\tgoto drop_and_free;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tireq = inet_rsk(req);\n\tireq->loc_addr = ip_hdr(skb)->daddr;\n\tireq->rmt_addr = ip_hdr(skb)->saddr;\n\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t * Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookie\n\t *\n\t * In fact we defer setting S.GSR, S.SWL, S.SWH to\n\t * dccp_create_openreq_child.\n\t */\n\tdreq->dreq_isr\t   = dcb->dccpd_seq;\n\tdreq->dreq_iss\t   = dccp_v4_init_sequence(skb);\n\tdreq->dreq_service = service;\n\n\tif (dccp_v4_send_response(sk, req, NULL))\n\t\tgoto drop_and_free;\n\n\tinet_csk_reqsk_queue_hash_add(sk, req, DCCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\treturn -1;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_conn_request);\n\nint dccp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tif (sk->sk_state == DCCP_OPEN) { /* Fast path */\n\t\tif (dccp_rcv_established(sk, skb, dh, skb->len))\n\t\t\tgoto reset;\n\t\treturn 0;\n\t}\n\n\t/*\n\t *  Step 3: Process LISTEN state\n\t *\t If P.type == Request or P contains a valid Init Cookie option,\n\t *\t      (* Must scan the packet's options to check for Init\n\t *\t\t Cookies.  Only Init Cookies are processed here,\n\t *\t\t however; other options are processed in Step 8.  This\n\t *\t\t scan need only be performed if the endpoint uses Init\n\t *\t\t Cookies *)\n\t *\t      (* Generate a new socket and switch to that socket *)\n\t *\t      Set S := new socket for this port pair\n\t *\t      S.state = RESPOND\n\t *\t      Choose S.ISS (initial seqno) or set from Init Cookies\n\t *\t      Initialize S.GAR := S.ISS\n\t *\t      Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookies\n\t *\t      Continue with S.state == RESPOND\n\t *\t      (* A Response packet will be generated in Step 11 *)\n\t *\t Otherwise,\n\t *\t      Generate Reset(No Connection) unless P.type == Reset\n\t *\t      Drop packet and return\n\t *\n\t * NOTE: the check for the packet types is done in\n\t *\t dccp_rcv_state_process\n\t */\n\tif (sk->sk_state == DCCP_LISTEN) {\n\t\tstruct sock *nsk = dccp_v4_hnd_req(sk, skb);\n\n\t\tif (nsk == NULL)\n\t\t\tgoto discard;\n\n\t\tif (nsk != sk) {\n\t\t\tif (dccp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (dccp_rcv_state_process(sk, skb, dh, skb->len))\n\t\tgoto reset;\n\treturn 0;\n\nreset:\n\tdccp_v4_ctl_send_reset(sk, skb);\ndiscard:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_do_rcv);\n\n/**\n *\tdccp_invalid_packet  -  check for malformed packets\n *\tImplements RFC 4340, 8.5:  Step 1: Check header basics\n *\tPackets that fail these checks are ignored and do not receive Resets.\n */\nint dccp_invalid_packet(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tunsigned int cscov;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn 1;\n\n\t/* If the packet is shorter than 12 bytes, drop packet and return */\n\tif (!pskb_may_pull(skb, sizeof(struct dccp_hdr))) {\n\t\tDCCP_WARN(\"pskb_may_pull failed\\n\");\n\t\treturn 1;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\t/* If P.type is not understood, drop packet and return */\n\tif (dh->dccph_type >= DCCP_PKT_INVALID) {\n\t\tDCCP_WARN(\"invalid packet type\\n\");\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If P.Data Offset is too small for packet type, drop packet and return\n\t */\n\tif (dh->dccph_doff < dccp_hdr_len(skb) / sizeof(u32)) {\n\t\tDCCP_WARN(\"P.Data Offset(%u) too small\\n\", dh->dccph_doff);\n\t\treturn 1;\n\t}\n\t/*\n\t * If P.Data Offset is too too large for packet, drop packet and return\n\t */\n\tif (!pskb_may_pull(skb, dh->dccph_doff * sizeof(u32))) {\n\t\tDCCP_WARN(\"P.Data Offset(%u) too large\\n\", dh->dccph_doff);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If P.type is not Data, Ack, or DataAck and P.X == 0 (the packet\n\t * has short sequence numbers), drop packet and return\n\t */\n\tif ((dh->dccph_type < DCCP_PKT_DATA    ||\n\t    dh->dccph_type > DCCP_PKT_DATAACK) && dh->dccph_x == 0)  {\n\t\tDCCP_WARN(\"P.type (%s) not Data || [Data]Ack, while P.X == 0\\n\",\n\t\t\t  dccp_packet_name(dh->dccph_type));\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If P.CsCov is too large for the packet size, drop packet and return.\n\t * This must come _before_ checksumming (not as RFC 4340 suggests).\n\t */\n\tcscov = dccp_csum_coverage(skb);\n\tif (cscov > skb->len) {\n\t\tDCCP_WARN(\"P.CsCov %u exceeds packet length %d\\n\",\n\t\t\t  dh->dccph_cscov, skb->len);\n\t\treturn 1;\n\t}\n\n\t/* If header checksum is incorrect, drop packet and return.\n\t * (This step is completed in the AF-dependent functions.) */\n\tskb->csum = skb_checksum(skb, 0, cscov, 0);\n\n\treturn 0;\n}\n\nEXPORT_SYMBOL_GPL(dccp_invalid_packet);\n\n/* this is called when real data arrives */\nstatic int dccp_v4_rcv(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tconst struct iphdr *iph;\n\tstruct sock *sk;\n\tint min_cov;\n\n\t/* Step 1: Check header basics */\n\n\tif (dccp_invalid_packet(skb))\n\t\tgoto discard_it;\n\n\tiph = ip_hdr(skb);\n\t/* Step 1: If header checksum is incorrect, drop packet and return */\n\tif (dccp_v4_csum_finish(skb, iph->saddr, iph->daddr)) {\n\t\tDCCP_WARN(\"dropped packet with invalid checksum\\n\");\n\t\tgoto discard_it;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\tDCCP_SKB_CB(skb)->dccpd_seq  = dccp_hdr_seq(dh);\n\tDCCP_SKB_CB(skb)->dccpd_type = dh->dccph_type;\n\n\tdccp_pr_debug(\"%8.8s src=%pI4@%-5d dst=%pI4@%-5d seq=%llu\",\n\t\t      dccp_packet_name(dh->dccph_type),\n\t\t      &iph->saddr, ntohs(dh->dccph_sport),\n\t\t      &iph->daddr, ntohs(dh->dccph_dport),\n\t\t      (unsigned long long) DCCP_SKB_CB(skb)->dccpd_seq);\n\n\tif (dccp_packet_without_ack(skb)) {\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = DCCP_PKT_WITHOUT_ACK_SEQ;\n\t\tdccp_pr_debug_cat(\"\\n\");\n\t} else {\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = dccp_hdr_ack_seq(skb);\n\t\tdccp_pr_debug_cat(\", ack=%llu\\n\", (unsigned long long)\n\t\t\t\t  DCCP_SKB_CB(skb)->dccpd_ack_seq);\n\t}\n\n\t/* Step 2:\n\t *\tLook up flow ID in table and get corresponding socket */\n\tsk = __inet_lookup_skb(&dccp_hashinfo, skb,\n\t\t\t       dh->dccph_sport, dh->dccph_dport);\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t */\n\tif (sk == NULL) {\n\t\tdccp_pr_debug(\"failed to look up flow ID in table and \"\n\t\t\t      \"get corresponding socket\\n\");\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * Step 2:\n\t *\t... or S.state == TIMEWAIT,\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tdccp_pr_debug(\"sk->sk_state == DCCP_TIME_WAIT: do_time_wait\\n\");\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage\n\t *\to if MinCsCov = 0, only packets with CsCov = 0 are accepted\n\t *\to if MinCsCov > 0, also accept packets with CsCov >= MinCsCov\n\t */\n\tmin_cov = dccp_sk(sk)->dccps_pcrlen;\n\tif (dh->dccph_cscov && (min_cov == 0 || dh->dccph_cscov < min_cov))  {\n\t\tdccp_pr_debug(\"Packet CsCov %d does not satisfy MinCsCov %d\\n\",\n\t\t\t      dh->dccph_cscov, min_cov);\n\t\t/* FIXME: \"Such packets SHOULD be reported using Data Dropped\n\t\t *         options (Section 11.7) with Drop Code 0, Protocol\n\t\t *         Constraints.\"                                     */\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\tnf_reset(skb);\n\n\treturn sk_receive_skb(sk, skb, 1);\n\nno_dccp_socket:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (dh->dccph_type != DCCP_PKT_RESET) {\n\t\tDCCP_SKB_CB(skb)->dccpd_reset_code =\n\t\t\t\t\tDCCP_RESET_CODE_NO_CONNECTION;\n\t\tdccp_v4_ctl_send_reset(sk, skb);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n}\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv4_af_ops = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = dccp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = dccp_v4_conn_request,\n\t.syn_recv_sock\t   = dccp_v4_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\n\t.bind_conflict\t   = inet_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ip_setsockopt,\n\t.compat_getsockopt = compat_ip_getsockopt,\n#endif\n};\n\nstatic int dccp_v4_init_sock(struct sock *sk)\n{\n\tstatic __u8 dccp_v4_ctl_sock_initialized;\n\tint err = dccp_init_sock(sk, dccp_v4_ctl_sock_initialized);\n\n\tif (err == 0) {\n\t\tif (unlikely(!dccp_v4_ctl_sock_initialized))\n\t\t\tdccp_v4_ctl_sock_initialized = 1;\n\t\tinet_csk(sk)->icsk_af_ops = &dccp_ipv4_af_ops;\n\t}\n\n\treturn err;\n}\n\nstatic struct timewait_sock_ops dccp_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct inet_timewait_sock),\n};\n\nstatic struct proto dccp_v4_prot = {\n\t.name\t\t\t= \"DCCP\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= dccp_close,\n\t.connect\t\t= dccp_v4_connect,\n\t.disconnect\t\t= dccp_disconnect,\n\t.ioctl\t\t\t= dccp_ioctl,\n\t.init\t\t\t= dccp_v4_init_sock,\n\t.setsockopt\t\t= dccp_setsockopt,\n\t.getsockopt\t\t= dccp_getsockopt,\n\t.sendmsg\t\t= dccp_sendmsg,\n\t.recvmsg\t\t= dccp_recvmsg,\n\t.backlog_rcv\t\t= dccp_v4_do_rcv,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.accept\t\t\t= inet_csk_accept,\n\t.get_port\t\t= inet_csk_get_port,\n\t.shutdown\t\t= dccp_shutdown,\n\t.destroy\t\t= dccp_destroy_sock,\n\t.orphan_count\t\t= &dccp_orphan_count,\n\t.max_header\t\t= MAX_DCCP_HEADER,\n\t.obj_size\t\t= sizeof(struct dccp_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.rsk_prot\t\t= &dccp_request_sock_ops,\n\t.twsk_prot\t\t= &dccp_timewait_sock_ops,\n\t.h.hashinfo\t\t= &dccp_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_dccp_setsockopt,\n\t.compat_getsockopt\t= compat_dccp_getsockopt,\n#endif\n};\n\nstatic const struct net_protocol dccp_v4_protocol = {\n\t.handler\t= dccp_v4_rcv,\n\t.err_handler\t= dccp_v4_err,\n\t.no_policy\t= 1,\n\t.netns_ok\t= 1,\n};\n\nstatic const struct proto_ops inet_dccp_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet_getname,\n\t/* FIXME: work on tcp_poll to rename it to inet_csk_poll */\n\t.poll\t\t   = dccp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t/* FIXME: work on inet_listen to rename it to sock_common_listen */\n\t.listen\t\t   = inet_dccp_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw dccp_v4_protosw = {\n\t.type\t\t= SOCK_DCCP,\n\t.protocol\t= IPPROTO_DCCP,\n\t.prot\t\t= &dccp_v4_prot,\n\t.ops\t\t= &inet_dccp_ops,\n\t.no_check\t= 0,\n\t.flags\t\t= INET_PROTOSW_ICSK,\n};\n\nstatic int __net_init dccp_v4_init_net(struct net *net)\n{\n\tif (dccp_hashinfo.bhash == NULL)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\treturn inet_ctl_sock_create(&net->dccp.v4_ctl_sk, PF_INET,\n\t\t\t\t    SOCK_DCCP, IPPROTO_DCCP, net);\n}\n\nstatic void __net_exit dccp_v4_exit_net(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->dccp.v4_ctl_sk);\n}\n\nstatic struct pernet_operations dccp_v4_ops = {\n\t.init\t= dccp_v4_init_net,\n\t.exit\t= dccp_v4_exit_net,\n};\n\nstatic int __init dccp_v4_init(void)\n{\n\tint err = proto_register(&dccp_v4_prot, 1);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet_add_protocol(&dccp_v4_protocol, IPPROTO_DCCP);\n\tif (err != 0)\n\t\tgoto out_proto_unregister;\n\n\tinet_register_protosw(&dccp_v4_protosw);\n\n\terr = register_pernet_subsys(&dccp_v4_ops);\n\tif (err)\n\t\tgoto out_destroy_ctl_sock;\nout:\n\treturn err;\nout_destroy_ctl_sock:\n\tinet_unregister_protosw(&dccp_v4_protosw);\n\tinet_del_protocol(&dccp_v4_protocol, IPPROTO_DCCP);\nout_proto_unregister:\n\tproto_unregister(&dccp_v4_prot);\n\tgoto out;\n}\n\nstatic void __exit dccp_v4_exit(void)\n{\n\tunregister_pernet_subsys(&dccp_v4_ops);\n\tinet_unregister_protosw(&dccp_v4_protosw);\n\tinet_del_protocol(&dccp_v4_protocol, IPPROTO_DCCP);\n\tproto_unregister(&dccp_v4_prot);\n}\n\nmodule_init(dccp_v4_init);\nmodule_exit(dccp_v4_exit);\n\n/*\n * __stringify doesn't likes enums, so use SOCK_DCCP (6) and IPPROTO_DCCP (33)\n * values directly, Also cover the case where the protocol is not specified,\n * i.e. net-pf-PF_INET-proto-0-type-SOCK_DCCP\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET, 33, 6);\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET, 0, 6);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Arnaldo Carvalho de Melo <acme@mandriva.com>\");\nMODULE_DESCRIPTION(\"DCCP - Datagram Congestion Controlled Protocol\");\n", "/*\n *\tDCCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tBased on net/dccp6/ipv6.c\n *\n *\tArnaldo Carvalho de Melo <acme@ghostprotocols.net>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/xfrm.h>\n\n#include <net/addrconf.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_sock.h>\n#include <net/inet6_connection_sock.h>\n#include <net/inet6_hashtables.h>\n#include <net/ip6_route.h>\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include \"dccp.h\"\n#include \"ipv6.h\"\n#include \"feat.h\"\n\n/* The per-net dccp.v6_ctl_sk is used for sending RSTs and ACKs */\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops;\n\nstatic void dccp_v6_hash(struct sock *sk)\n{\n\tif (sk->sk_state != DCCP_CLOSED) {\n\t\tif (inet_csk(sk)->icsk_af_ops == &dccp_ipv6_mapped) {\n\t\t\tinet_hash(sk);\n\t\t\treturn;\n\t\t}\n\t\tlocal_bh_disable();\n\t\t__inet6_hash(sk, NULL);\n\t\tlocal_bh_enable();\n\t}\n}\n\n/* add pseudo-header to DCCP checksum stored in skb->csum */\nstatic inline __sum16 dccp_v6_csum_finish(struct sk_buff *skb,\n\t\t\t\t      const struct in6_addr *saddr,\n\t\t\t\t      const struct in6_addr *daddr)\n{\n\treturn csum_ipv6_magic(saddr, daddr, skb->len, IPPROTO_DCCP, skb->csum);\n}\n\nstatic inline void dccp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tdccp_csum_outgoing(skb);\n\tdh->dccph_checksum = dccp_v6_csum_finish(skb, &np->saddr, &np->daddr);\n}\n\nstatic inline __u32 secure_dccpv6_sequence_number(__be32 *saddr, __be32 *daddr,\n\t\t\t\t\t\t  __be16 sport, __be16 dport   )\n{\n\treturn secure_tcpv6_sequence_number(saddr, daddr, sport, dport);\n}\n\nstatic inline __u32 dccp_v6_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_dccpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t     ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_dport,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_sport     );\n\n}\n\nstatic void dccp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct dccp_hdr *dh = (struct dccp_hdr *)(skb->data + offset);\n\tstruct dccp_sock *dp;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\t__u64 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->len < offset + sizeof(*dh) ||\n\t    skb->len < offset + __dccp_basic_hdr_len(dh)) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = inet6_lookup(net, &dccp_hashinfo,\n\t\t\t&hdr->daddr, dh->dccph_dport,\n\t\t\t&hdr->saddr, dh->dccph_sport, inet6_iif(skb));\n\n\tif (sk == NULL) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tseq = dccp_hdr_seq(dh);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst = NULL;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (DCCPF_LISTEN | DCCPF_CLOSED))\n\t\t\tgoto out;\n\n\t\t/* icmp should have updated the destination cache entry */\n\t\tdst = __sk_dst_check(sk, np->dst_cookie);\n\t\tif (dst == NULL) {\n\t\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\t\tstruct flowi6 fl6;\n\n\t\t\t/* BUGGG_FUTURE: Again, it is not clear how\n\t\t\t   to handle rthdr case. Ignore this complexity\n\t\t\t   for now.\n\t\t\t */\n\t\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\t\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\t\t\tipv6_addr_copy(&fl6.saddr, &np->saddr);\n\t\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\t\tfl6.fl6_dport = inet->inet_dport;\n\t\t\tfl6.fl6_sport = inet->inet_sport;\n\t\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\t\tdst = ip6_dst_lookup_flow(sk, &fl6, NULL, false);\n\t\t\tif (IS_ERR(dst)) {\n\t\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else\n\t\t\tdst_hold(dst);\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {\n\t\t\tdccp_sync_mss(sk, dst_mtu(dst));\n\t\t} /* else let the usual retransmit timer handle it */\n\t\tdst_release(dst);\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req, **prev;\n\tcase DCCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\n\t\treq = inet6_csk_search_req(sk, &prev, dh->dccph_dport,\n\t\t\t\t\t   &hdr->daddr, &hdr->saddr,\n\t\t\t\t\t   inet6_iif(skb));\n\t\tif (req == NULL)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * ICMPs are not backlogged, hence we cannot get an established\n\t\t * socket here.\n\t\t */\n\t\tWARN_ON(req->sk != NULL);\n\n\t\tif (seq != dccp_rsk(req)->dreq_iss) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\t\t\t/*\n\t\t\t * Wake people up to see the error\n\t\t\t * (see connect in sock.c)\n\t\t\t */\n\t\t\tsk->sk_error_report(sk);\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\n\nstatic int dccp_v6_send_response(struct sock *sk, struct request_sock *req,\n\t\t\t\t struct request_values *rv_unused)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq6->iif;\n\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\topt = np->opt;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq6->loc_addr,\n\t\t\t\t\t\t\t &ireq6->rmt_addr);\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\terr = ip6_xmit(sk, skb, &fl6, opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic void dccp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tdccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);\n\tif (inet6_rsk(req)->pktopts != NULL)\n\t\tkfree_skb(inet6_rsk(req)->pktopts);\n}\n\nstatic void dccp_v6_ctl_send_reset(struct sock *sk, struct sk_buff *rxskb)\n{\n\tconst struct ipv6hdr *rxip6h;\n\tstruct sk_buff *skb;\n\tstruct flowi6 fl6;\n\tstruct net *net = dev_net(skb_dst(rxskb)->dev);\n\tstruct sock *ctl_sk = net->dccp.v6_ctl_sk;\n\tstruct dst_entry *dst;\n\n\tif (dccp_hdr(rxskb)->dccph_type == DCCP_PKT_RESET)\n\t\treturn;\n\n\tif (!ipv6_unicast_destination(rxskb))\n\t\treturn;\n\n\tskb = dccp_ctl_make_reset(ctl_sk, rxskb);\n\tif (skb == NULL)\n\t\treturn;\n\n\trxip6h = ipv6_hdr(rxskb);\n\tdccp_hdr(skb)->dccph_checksum = dccp_v6_csum_finish(skb, &rxip6h->saddr,\n\t\t\t\t\t\t\t    &rxip6h->daddr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tipv6_addr_copy(&fl6.daddr, &rxip6h->saddr);\n\tipv6_addr_copy(&fl6.saddr, &rxip6h->daddr);\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.flowi6_oif = inet6_iif(rxskb);\n\tfl6.fl6_dport = dccp_hdr(skb)->dccph_dport;\n\tfl6.fl6_sport = dccp_hdr(skb)->dccph_sport;\n\tsecurity_skb_classify_flow(rxskb, flowi6_to_flowi(&fl6));\n\n\t/* sk = NULL, but it is safe for now. RST socket required. */\n\tdst = ip6_dst_lookup_flow(ctl_sk, &fl6, NULL, false);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(skb, dst);\n\t\tip6_xmit(ctl_sk, skb, &fl6, NULL);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTSEGS);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(skb);\n}\n\nstatic struct request_sock_ops dccp6_request_sock_ops = {\n\t.family\t\t= AF_INET6,\n\t.obj_size\t= sizeof(struct dccp6_request_sock),\n\t.rtx_syn_ack\t= dccp_v6_send_response,\n\t.send_ack\t= dccp_reqsk_send_ack,\n\t.destructor\t= dccp_v6_reqsk_destructor,\n\t.send_reset\t= dccp_v6_ctl_send_reset,\n};\n\nstatic struct sock *dccp_v6_hnd_req(struct sock *sk,struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh = dccp_hdr(skb);\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\tstruct sock *nsk;\n\tstruct request_sock **prev;\n\t/* Find possible connection requests. */\n\tstruct request_sock *req = inet6_csk_search_req(sk, &prev,\n\t\t\t\t\t\t\tdh->dccph_sport,\n\t\t\t\t\t\t\t&iph->saddr,\n\t\t\t\t\t\t\t&iph->daddr,\n\t\t\t\t\t\t\tinet6_iif(skb));\n\tif (req != NULL)\n\t\treturn dccp_check_req(sk, skb, req, prev);\n\n\tnsk = __inet6_lookup_established(sock_net(sk), &dccp_hashinfo,\n\t\t\t\t\t &iph->saddr, dh->dccph_sport,\n\t\t\t\t\t &iph->daddr, ntohs(dh->dccph_dport),\n\t\t\t\t\t inet6_iif(skb));\n\tif (nsk != NULL) {\n\t\tif (nsk->sk_state != DCCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic int dccp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct request_sock *req;\n\tstruct dccp_request_sock *dreq;\n\tstruct inet6_request_sock *ireq6;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tconst __be32 service = dccp_hdr_request(skb)->dccph_req_service;\n\tstruct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\treturn 0;\t/* discard, don't send a reset here */\n\n\tif (dccp_bad_service_code(sk, service)) {\n\t\tdcb->dccpd_reset_code = DCCP_RESET_CODE_BAD_SERVICE_CODE;\n\t\tgoto drop;\n\t}\n\t/*\n\t * There are no SYN attacks on IPv6, yet...\n\t */\n\tdcb->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto drop;\n\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet6_reqsk_alloc(&dccp6_request_sock_ops);\n\tif (req == NULL)\n\t\tgoto drop;\n\n\tif (dccp_reqsk_init(req, dccp_sk(sk), skb))\n\t\tgoto drop_and_free;\n\n\tdreq = dccp_rsk(req);\n\tif (dccp_parse_options(sk, dreq, skb))\n\t\tgoto drop_and_free;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tireq6 = inet6_rsk(req);\n\tipv6_addr_copy(&ireq6->rmt_addr, &ipv6_hdr(skb)->saddr);\n\tipv6_addr_copy(&ireq6->loc_addr, &ipv6_hdr(skb)->daddr);\n\n\tif (ipv6_opt_accepted(sk, skb) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq6->pktopts = skb;\n\t}\n\tireq6->iif = sk->sk_bound_dev_if;\n\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq6->rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq6->iif = inet6_iif(skb);\n\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t *   Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookie\n\t *\n\t *   In fact we defer setting S.GSR, S.SWL, S.SWH to\n\t *   dccp_create_openreq_child.\n\t */\n\tdreq->dreq_isr\t   = dcb->dccpd_seq;\n\tdreq->dreq_iss\t   = dccp_v6_init_sequence(skb);\n\tdreq->dreq_service = service;\n\n\tif (dccp_v6_send_response(sk, req, NULL))\n\t\tgoto drop_and_free;\n\n\tinet6_csk_reqsk_queue_hash_add(sk, req, DCCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\treturn -1;\n}\n\nstatic struct sock *dccp_v6_request_recv_sock(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (dst == NULL) {\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\t\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\t\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &ireq6->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &ireq6->loc_addr);\n\tnewsk->sk_bound_dev_if = ireq6->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (ireq6->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(ireq6->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(ireq6->pktopts);\n\t\tireq6->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (opt != NULL) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\treturn NULL;\n}\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int dccp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, dccp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   dccp_v6_hnd_req and dccp_v6_ctl_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_do_rcv(sk, skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t * socket locking is here for SMP purposes as backlog rcv is currently\n\t * called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all)\n\t/*\n\t * FIXME: Add handling of IPV6_PKTOPTIONS skb. See the comments below\n\t *        (wrt ipv6_pktopions) and net/ipv6/tcp_ipv6.c for an example.\n\t */\n\t\topt_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (sk->sk_state == DCCP_OPEN) { /* Fast path */\n\t\tif (dccp_rcv_established(sk, skb, dccp_hdr(skb), skb->len))\n\t\t\tgoto reset;\n\t\tif (opt_skb) {\n\t\t\t/* XXX This is where we would goto ipv6_pktoptions. */\n\t\t\t__kfree_skb(opt_skb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/*\n\t *  Step 3: Process LISTEN state\n\t *     If S.state == LISTEN,\n\t *\t If P.type == Request or P contains a valid Init Cookie option,\n\t *\t      (* Must scan the packet's options to check for Init\n\t *\t\t Cookies.  Only Init Cookies are processed here,\n\t *\t\t however; other options are processed in Step 8.  This\n\t *\t\t scan need only be performed if the endpoint uses Init\n\t *\t\t Cookies *)\n\t *\t      (* Generate a new socket and switch to that socket *)\n\t *\t      Set S := new socket for this port pair\n\t *\t      S.state = RESPOND\n\t *\t      Choose S.ISS (initial seqno) or set from Init Cookies\n\t *\t      Initialize S.GAR := S.ISS\n\t *\t      Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookies\n\t *\t      Continue with S.state == RESPOND\n\t *\t      (* A Response packet will be generated in Step 11 *)\n\t *\t Otherwise,\n\t *\t      Generate Reset(No Connection) unless P.type == Reset\n\t *\t      Drop packet and return\n\t *\n\t * NOTE: the check for the packet types is done in\n\t *\t dccp_rcv_state_process\n\t */\n\tif (sk->sk_state == DCCP_LISTEN) {\n\t\tstruct sock *nsk = dccp_v6_hnd_req(sk, skb);\n\n\t\tif (nsk == NULL)\n\t\t\tgoto discard;\n\t\t/*\n\t\t * Queue it on the new socket if the new socket is active,\n\t\t * otherwise we just shortcircuit this and continue with\n\t\t * the new socket..\n\t\t */\n\t\tif (nsk != sk) {\n\t\t\tif (dccp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\tif (opt_skb != NULL)\n\t\t\t\t__kfree_skb(opt_skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (dccp_rcv_state_process(sk, skb, dccp_hdr(skb), skb->len))\n\t\tgoto reset;\n\tif (opt_skb) {\n\t\t/* XXX This is where we would goto ipv6_pktoptions. */\n\t\t__kfree_skb(opt_skb);\n\t}\n\treturn 0;\n\nreset:\n\tdccp_v6_ctl_send_reset(sk, skb);\ndiscard:\n\tif (opt_skb != NULL)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int dccp_v6_rcv(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tstruct sock *sk;\n\tint min_cov;\n\n\t/* Step 1: Check header basics */\n\n\tif (dccp_invalid_packet(skb))\n\t\tgoto discard_it;\n\n\t/* Step 1: If header checksum is incorrect, drop packet and return. */\n\tif (dccp_v6_csum_finish(skb, &ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr)) {\n\t\tDCCP_WARN(\"dropped packet with invalid checksum\\n\");\n\t\tgoto discard_it;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\tDCCP_SKB_CB(skb)->dccpd_seq  = dccp_hdr_seq(dh);\n\tDCCP_SKB_CB(skb)->dccpd_type = dh->dccph_type;\n\n\tif (dccp_packet_without_ack(skb))\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = DCCP_PKT_WITHOUT_ACK_SEQ;\n\telse\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = dccp_hdr_ack_seq(skb);\n\n\t/* Step 2:\n\t *\tLook up flow ID in table and get corresponding socket */\n\tsk = __inet6_lookup_skb(&dccp_hashinfo, skb,\n\t\t\t        dh->dccph_sport, dh->dccph_dport);\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t */\n\tif (sk == NULL) {\n\t\tdccp_pr_debug(\"failed to look up flow ID in table and \"\n\t\t\t      \"get corresponding socket\\n\");\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * Step 2:\n\t *\t... or S.state == TIMEWAIT,\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tdccp_pr_debug(\"sk->sk_state == DCCP_TIME_WAIT: do_time_wait\\n\");\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage\n\t *\to if MinCsCov = 0, only packets with CsCov = 0 are accepted\n\t *\to if MinCsCov > 0, also accept packets with CsCov >= MinCsCov\n\t */\n\tmin_cov = dccp_sk(sk)->dccps_pcrlen;\n\tif (dh->dccph_cscov  &&  (min_cov == 0 || dh->dccph_cscov < min_cov))  {\n\t\tdccp_pr_debug(\"Packet CsCov %d does not satisfy MinCsCov %d\\n\",\n\t\t\t      dh->dccph_cscov, min_cov);\n\t\t/* FIXME: send Data Dropped option (see also dccp_v4_rcv) */\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\treturn sk_receive_skb(sk, skb, 1) ? -1 : 0;\n\nno_dccp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (dh->dccph_type != DCCP_PKT_RESET) {\n\t\tDCCP_SKB_CB(skb)->dccpd_reset_code =\n\t\t\t\t\tDCCP_RESET_CODE_NO_CONNECTION;\n\t\tdccp_v6_ctl_send_reset(sk, skb);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n}\n\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tipv6_addr_copy(&usin->sin6_addr, &flowlabel->dst);\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tipv6_addr_copy(&np->daddr, &usin->sin6_addr);\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr, &np->rcv_saddr);\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&np->rcv_saddr))\n\t\tsaddr = &np->rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\tipv6_addr_copy(&fl6.saddr, saddr ? saddr : &np->saddr);\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, true);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tipv6_addr_copy(&np->rcv_saddr, saddr);\n\t}\n\n\t/* set the source address */\n\tipv6_addr_copy(&np->saddr, saddr);\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      np->daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = dccp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n/*\n *\tDCCP over IPv4 via INET6 API\n */\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = dccp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int dccp_v6_init_sock(struct sock *sk)\n{\n\tstatic __u8 dccp_v6_ctl_sock_initialized;\n\tint err = dccp_init_sock(sk, dccp_v6_ctl_sock_initialized);\n\n\tif (err == 0) {\n\t\tif (unlikely(!dccp_v6_ctl_sock_initialized))\n\t\t\tdccp_v6_ctl_sock_initialized = 1;\n\t\tinet_csk(sk)->icsk_af_ops = &dccp_ipv6_af_ops;\n\t}\n\n\treturn err;\n}\n\nstatic void dccp_v6_destroy_sock(struct sock *sk)\n{\n\tdccp_destroy_sock(sk);\n\tinet6_destroy_sock(sk);\n}\n\nstatic struct timewait_sock_ops dccp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct dccp6_timewait_sock),\n};\n\nstatic struct proto dccp_v6_prot = {\n\t.name\t\t   = \"DCCPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = dccp_close,\n\t.connect\t   = dccp_v6_connect,\n\t.disconnect\t   = dccp_disconnect,\n\t.ioctl\t\t   = dccp_ioctl,\n\t.init\t\t   = dccp_v6_init_sock,\n\t.setsockopt\t   = dccp_setsockopt,\n\t.getsockopt\t   = dccp_getsockopt,\n\t.sendmsg\t   = dccp_sendmsg,\n\t.recvmsg\t   = dccp_recvmsg,\n\t.backlog_rcv\t   = dccp_v6_do_rcv,\n\t.hash\t\t   = dccp_v6_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.accept\t\t   = inet_csk_accept,\n\t.get_port\t   = inet_csk_get_port,\n\t.shutdown\t   = dccp_shutdown,\n\t.destroy\t   = dccp_v6_destroy_sock,\n\t.orphan_count\t   = &dccp_orphan_count,\n\t.max_header\t   = MAX_DCCP_HEADER,\n\t.obj_size\t   = sizeof(struct dccp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.rsk_prot\t   = &dccp6_request_sock_ops,\n\t.twsk_prot\t   = &dccp6_timewait_sock_ops,\n\t.h.hashinfo\t   = &dccp_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_dccp_setsockopt,\n\t.compat_getsockopt = compat_dccp_getsockopt,\n#endif\n};\n\nstatic const struct inet6_protocol dccp_v6_protocol = {\n\t.handler\t= dccp_v6_rcv,\n\t.err_handler\t= dccp_v6_err,\n\t.flags\t\t= INET6_PROTO_NOPOLICY | INET6_PROTO_FINAL,\n};\n\nstatic const struct proto_ops inet6_dccp_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = dccp_poll,\n\t.ioctl\t\t   = inet6_ioctl,\n\t.listen\t\t   = inet_dccp_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw dccp_v6_protosw = {\n\t.type\t\t= SOCK_DCCP,\n\t.protocol\t= IPPROTO_DCCP,\n\t.prot\t\t= &dccp_v6_prot,\n\t.ops\t\t= &inet6_dccp_ops,\n\t.flags\t\t= INET_PROTOSW_ICSK,\n};\n\nstatic int __net_init dccp_v6_init_net(struct net *net)\n{\n\tif (dccp_hashinfo.bhash == NULL)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\treturn inet_ctl_sock_create(&net->dccp.v6_ctl_sk, PF_INET6,\n\t\t\t\t    SOCK_DCCP, IPPROTO_DCCP, net);\n}\n\nstatic void __net_exit dccp_v6_exit_net(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->dccp.v6_ctl_sk);\n}\n\nstatic struct pernet_operations dccp_v6_ops = {\n\t.init   = dccp_v6_init_net,\n\t.exit   = dccp_v6_exit_net,\n};\n\nstatic int __init dccp_v6_init(void)\n{\n\tint err = proto_register(&dccp_v6_prot, 1);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet6_add_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tif (err != 0)\n\t\tgoto out_unregister_proto;\n\n\tinet6_register_protosw(&dccp_v6_protosw);\n\n\terr = register_pernet_subsys(&dccp_v6_ops);\n\tif (err != 0)\n\t\tgoto out_destroy_ctl_sock;\nout:\n\treturn err;\n\nout_destroy_ctl_sock:\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\nout_unregister_proto:\n\tproto_unregister(&dccp_v6_prot);\n\tgoto out;\n}\n\nstatic void __exit dccp_v6_exit(void)\n{\n\tunregister_pernet_subsys(&dccp_v6_ops);\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\n\tproto_unregister(&dccp_v6_prot);\n}\n\nmodule_init(dccp_v6_init);\nmodule_exit(dccp_v6_exit);\n\n/*\n * __stringify doesn't likes enums, so use SOCK_DCCP (6) and IPPROTO_DCCP (33)\n * values directly, Also cover the case where the protocol is not specified,\n * i.e. net-pf-PF_INET6-proto-0-type-SOCK_DCCP\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 33, 6);\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 0, 6);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Arnaldo Carvalho de Melo <acme@mandriva.com>\");\nMODULE_DESCRIPTION(\"DCCPv6 - Datagram Congestion Controlled Protocol\");\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPF_INET protocol family socket handler.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Changes (see also sock.c)\n *\n *\t\tpiggy,\n *\t\tKarl Knutson\t:\tSocket protocol table\n *\t\tA.N.Kuznetsov\t:\tSocket death error in accept().\n *\t\tJohn Richardson :\tFix non blocking error in connect()\n *\t\t\t\t\tso sockets that fail to connect\n *\t\t\t\t\tdon't return -EINPROGRESS.\n *\t\tAlan Cox\t:\tAsynchronous I/O support\n *\t\tAlan Cox\t:\tKeep correct socket pointer on sock\n *\t\t\t\t\tstructures\n *\t\t\t\t\twhen accept() ed\n *\t\tAlan Cox\t:\tSemantics of SO_LINGER aren't state\n *\t\t\t\t\tmoved to close when you look carefully.\n *\t\t\t\t\tWith this fixed and the accept bug fixed\n *\t\t\t\t\tsome RPC stuff seems happier.\n *\t\tNiibe Yutaka\t:\t4.4BSD style write async I/O\n *\t\tAlan Cox,\n *\t\tTony Gale \t:\tFixed reuse semantics.\n *\t\tAlan Cox\t:\tbind() shouldn't abort existing but dead\n *\t\t\t\t\tsockets. Stops FTP netin:.. I hope.\n *\t\tAlan Cox\t:\tbind() works correctly for RAW sockets.\n *\t\t\t\t\tNote that FreeBSD at least was broken\n *\t\t\t\t\tin this respect so be careful with\n *\t\t\t\t\tcompatibility tests...\n *\t\tAlan Cox\t:\trouting cache support\n *\t\tAlan Cox\t:\tmemzero the socket structure for\n *\t\t\t\t\tcompactness.\n *\t\tMatt Day\t:\tnonblock connect error handler\n *\t\tAlan Cox\t:\tAllow large numbers of pending sockets\n *\t\t\t\t\t(eg for big web sites), but only if\n *\t\t\t\t\tspecifically application requested.\n *\t\tAlan Cox\t:\tNew buffering throughout IP. Used\n *\t\t\t\t\tdumbly.\n *\t\tAlan Cox\t:\tNew buffering now used smartly.\n *\t\tAlan Cox\t:\tBSD rather than common sense\n *\t\t\t\t\tinterpretation of listen.\n *\t\tGermano Caronni\t:\tAssorted small races.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg basic support.\n *\t\tAlan Cox\t:\tOnly sendmsg/recvmsg now supported.\n *\t\tAlan Cox\t:\tLocked down bind (see security list).\n *\t\tAlan Cox\t:\tLoosened bind a little.\n *\t\tMike McLagan\t:\tADD/DEL DLCI Ioctls\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tSome other random speedups.\n *\t\tCyrus Durgin\t:\tCleaned up file for kmod hacks.\n *\t\tAndi Kleen\t:\tFix inet_stream_connect TCP race.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n\n#include <asm/uaccess.h>\n#include <asm/system.h>\n\n#include <linux/inet.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <net/checksum.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/arp.h>\n#include <net/route.h>\n#include <net/ip_fib.h>\n#include <net/inet_connection_sock.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/raw.h>\n#include <net/icmp.h>\n#include <net/ipip.h>\n#include <net/inet_common.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#ifdef CONFIG_IP_MROUTE\n#include <linux/mroute.h>\n#endif\n\n\n/* The inetsw table contains everything that inet_create needs to\n * build a new socket.\n */\nstatic struct list_head inetsw[SOCK_MAX];\nstatic DEFINE_SPINLOCK(inetsw_lock);\n\nstruct ipv4_config ipv4_config;\nEXPORT_SYMBOL(ipv4_config);\n\n/* New destruction routine */\n\nvoid inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(inet->opt);\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tsk_refcnt_debug_dec(sk);\n}\nEXPORT_SYMBOL(inet_sock_destruct);\n\n/*\n *\tThe routines beyond this point handle the behaviour of an AF_INET\n *\tsocket object. Mostly it punts to the subprotocols of IP to do\n *\tthe work.\n */\n\n/*\n *\tAutomatically bind an unbound socket.\n */\n\nstatic int inet_autobind(struct sock *sk)\n{\n\tstruct inet_sock *inet;\n\t/* We may need to bind the socket. */\n\tlock_sock(sk);\n\tinet = inet_sk(sk);\n\tif (!inet->inet_num) {\n\t\tif (sk->sk_prot->get_port(sk, 0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t}\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/*\n *\tMove a socket into listening state.\n */\nint inet_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\tunsigned char old_state;\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)\n\t\tgoto out;\n\n\told_state = sk->sk_state;\n\tif (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\tgoto out;\n\n\t/* Really, if the socket is already in listen state\n\t * we can only allow the backlog to be adjusted.\n\t */\n\tif (old_state != TCP_LISTEN) {\n\t\terr = inet_csk_listen_start(sk, backlog);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\tsk->sk_max_ack_backlog = backlog;\n\terr = 0;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_listen);\n\nu32 inet_ehash_secret __read_mostly;\nEXPORT_SYMBOL(inet_ehash_secret);\n\n/*\n * inet_ehash_secret must be set exactly once\n */\nvoid build_ehash_secret(void)\n{\n\tu32 rnd;\n\n\tdo {\n\t\tget_random_bytes(&rnd, sizeof(rnd));\n\t} while (rnd == 0);\n\n\tcmpxchg(&inet_ehash_secret, 0, rnd);\n}\nEXPORT_SYMBOL(build_ehash_secret);\n\nstatic inline int inet_netns_ok(struct net *net, int protocol)\n{\n\tint hash;\n\tconst struct net_protocol *ipprot;\n\n\tif (net_eq(net, &init_net))\n\t\treturn 1;\n\n\thash = protocol & (MAX_INET_PROTOS - 1);\n\tipprot = rcu_dereference(inet_protos[hash]);\n\n\tif (ipprot == NULL)\n\t\t/* raw IP is OK */\n\t\treturn 1;\n\treturn ipprot->netns_ok;\n}\n\n/*\n *\tCreate an inet socket.\n */\n\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\n\t\t       int kern)\n{\n\tstruct sock *sk;\n\tstruct inet_protosw *answer;\n\tstruct inet_sock *inet;\n\tstruct proto *answer_prot;\n\tunsigned char answer_flags;\n\tchar answer_no_check;\n\tint try_loading_module = 0;\n\tint err;\n\n\tif (unlikely(!inet_ehash_secret))\n\t\tif (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)\n\t\t\tbuild_ehash_secret();\n\n\tsock->state = SS_UNCONNECTED;\n\n\t/* Look for the requested type/protocol pair. */\nlookup_protocol:\n\terr = -ESOCKTNOSUPPORT;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(answer, &inetsw[sock->type], list) {\n\n\t\terr = 0;\n\t\t/* Check the non-wild match. */\n\t\tif (protocol == answer->protocol) {\n\t\t\tif (protocol != IPPROTO_IP)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* Check for the two wild cases. */\n\t\t\tif (IPPROTO_IP == protocol) {\n\t\t\t\tprotocol = answer->protocol;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IPPROTO_IP == answer->protocol)\n\t\t\t\tbreak;\n\t\t}\n\t\terr = -EPROTONOSUPPORT;\n\t}\n\n\tif (unlikely(err)) {\n\t\tif (try_loading_module < 2) {\n\t\t\trcu_read_unlock();\n\t\t\t/*\n\t\t\t * Be more specific, e.g. net-pf-2-proto-132-type-1\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)\n\t\t\t */\n\t\t\tif (++try_loading_module == 1)\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d-type-%d\",\n\t\t\t\t\t       PF_INET, protocol, sock->type);\n\t\t\t/*\n\t\t\t * Fall back to generic, e.g. net-pf-2-proto-132\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP)\n\t\t\t */\n\t\t\telse\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d\",\n\t\t\t\t\t       PF_INET, protocol);\n\t\t\tgoto lookup_protocol;\n\t\t} else\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\terr = -EPERM;\n\tif (sock->type == SOCK_RAW && !kern && !capable(CAP_NET_RAW))\n\t\tgoto out_rcu_unlock;\n\n\terr = -EAFNOSUPPORT;\n\tif (!inet_netns_ok(net, protocol))\n\t\tgoto out_rcu_unlock;\n\n\tsock->ops = answer->ops;\n\tanswer_prot = answer->prot;\n\tanswer_no_check = answer->no_check;\n\tanswer_flags = answer->flags;\n\trcu_read_unlock();\n\n\tWARN_ON(answer_prot->slab == NULL);\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\terr = 0;\n\tsk->sk_no_check = answer_no_check;\n\tif (INET_PROTOSW_REUSE & answer_flags)\n\t\tsk->sk_reuse = 1;\n\n\tinet = inet_sk(sk);\n\tinet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;\n\n\tinet->nodefrag = 0;\n\n\tif (SOCK_RAW == sock->type) {\n\t\tinet->inet_num = protocol;\n\t\tif (IPPROTO_RAW == protocol)\n\t\t\tinet->hdrincl = 1;\n\t}\n\n\tif (ipv4_config.no_pmtu_disc)\n\t\tinet->pmtudisc = IP_PMTUDISC_DONT;\n\telse\n\t\tinet->pmtudisc = IP_PMTUDISC_WANT;\n\n\tinet->inet_id = 0;\n\n\tsock_init_data(sock, sk);\n\n\tsk->sk_destruct\t   = inet_sock_destruct;\n\tsk->sk_protocol\t   = protocol;\n\tsk->sk_backlog_rcv = sk->sk_prot->backlog_rcv;\n\n\tinet->uc_ttl\t= -1;\n\tinet->mc_loop\t= 1;\n\tinet->mc_ttl\t= 1;\n\tinet->mc_all\t= 1;\n\tinet->mc_index\t= 0;\n\tinet->mc_list\t= NULL;\n\n\tsk_refcnt_debug_inc(sk);\n\n\tif (inet->inet_num) {\n\t\t/* It assumes that any protocol which allows\n\t\t * the user to assign a number at socket\n\t\t * creation time automatically\n\t\t * shares.\n\t\t */\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t\t/* Add to protocol hash chains. */\n\t\tsk->sk_prot->hash(sk);\n\t}\n\n\tif (sk->sk_prot->init) {\n\t\terr = sk->sk_prot->init(sk);\n\t\tif (err)\n\t\t\tsk_common_release(sk);\n\t}\nout:\n\treturn err;\nout_rcu_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\n\n\n/*\n *\tThe peer socket should always be NULL (or else). When we call this\n *\tfunction we are destroying the object and from then on nobody\n *\tshould refer to it.\n */\nint inet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk) {\n\t\tlong timeout;\n\n\t\tsock_rps_reset_flow(sk);\n\n\t\t/* Applications forget to leave groups before exiting */\n\t\tip_mc_drop_socket(sk);\n\n\t\t/* If linger is set, we don't return until the close\n\t\t * is complete.  Otherwise we return immediately. The\n\t\t * actually closing is done the same either way.\n\t\t *\n\t\t * If the close is due to the process exiting, we never\n\t\t * linger..\n\t\t */\n\t\ttimeout = 0;\n\t\tif (sock_flag(sk, SOCK_LINGER) &&\n\t\t    !(current->flags & PF_EXITING))\n\t\t\ttimeout = sk->sk_lingertime;\n\t\tsock->sk = NULL;\n\t\tsk->sk_prot->close(sk, timeout);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_release);\n\n/* It is off by default, see below. */\nint sysctl_ip_nonlocal_bind __read_mostly;\nEXPORT_SYMBOL(sysctl_ip_nonlocal_bind);\n\nint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *addr = (struct sockaddr_in *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tunsigned short snum;\n\tint chk_addr_ret;\n\tint err;\n\n\t/* If the socket has its own bind function then use it. (RAW) */\n\tif (sk->sk_prot->bind) {\n\t\terr = sk->sk_prot->bind(sk, uaddr, addr_len);\n\t\tgoto out;\n\t}\n\terr = -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(sock_net(sk), addr->sin_addr.s_addr);\n\n\t/* Not specified by any standard per-se, however it breaks too\n\t * many applications when removed.  It is unfortunate since\n\t * allowing applications to make a non-local bind solves\n\t * several problems with systems using dynamic addressing.\n\t * (ie. your servers still start up even if your ISDN link\n\t *  is temporarily down)\n\t */\n\terr = -EADDRNOTAVAIL;\n\tif (!sysctl_ip_nonlocal_bind &&\n\t    !(inet->freebind || inet->transparent) &&\n\t    addr->sin_addr.s_addr != htonl(INADDR_ANY) &&\n\t    chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST &&\n\t    chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tsnum = ntohs(addr->sin_port);\n\terr = -EACCES;\n\tif (snum && snum < PROT_SOCK && !capable(CAP_NET_BIND_SERVICE))\n\t\tgoto out;\n\n\t/*      We keep a pair of addresses. rcv_saddr is the one\n\t *      used by hash lookups, and saddr is used for transmit.\n\t *\n\t *      In the BSD API these are the same except where it\n\t *      would be illegal to use them (multicast/broadcast) in\n\t *      which case the sending device address is used.\n\t */\n\tlock_sock(sk);\n\n\t/* Check these errors (active socket, double bind). */\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE || inet->inet_num)\n\t\tgoto out_release_sock;\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->sin_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\n\t/* Make sure we are allowed to bind here. */\n\tif (sk->sk_prot->get_port(sk, snum)) {\n\t\tinet->inet_saddr = inet->inet_rcv_saddr = 0;\n\t\terr = -EADDRINUSE;\n\t\tgoto out_release_sock;\n\t}\n\n\tif (inet->inet_rcv_saddr)\n\t\tsk->sk_userlocks |= SOCK_BINDADDR_LOCK;\n\tif (snum)\n\t\tsk->sk_userlocks |= SOCK_BINDPORT_LOCK;\n\tinet->inet_sport = htons(inet->inet_num);\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsk_dst_reset(sk);\n\terr = 0;\nout_release_sock:\n\trelease_sock(sk);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_bind);\n\nint inet_dgram_connect(struct socket *sock, struct sockaddr * uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn sk->sk_prot->disconnect(sk, flags);\n\n\tif (!inet_sk(sk)->inet_num && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn sk->sk_prot->connect(sk, (struct sockaddr *)uaddr, addr_len);\n}\nEXPORT_SYMBOL(inet_dgram_connect);\n\nstatic long inet_wait_for_connect(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t/* Basic assumption: if someone sets sk->sk_err, he _must_\n\t * change state of the socket from TCP_SYN_*.\n\t * Connect() does not allow to get error notifications\n\t * without closing the socket.\n\t */\n\twhile ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\trelease_sock(sk);\n\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock(sk);\n\t\tif (signal_pending(current) || !timeo)\n\t\t\tbreak;\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n/*\n *\tConnect to a remote host. There is regrettably still a little\n *\tTCP 'magic' in here.\n */\nint inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\tlong timeo;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (uaddr->sa_family == AF_UNSPEC) {\n\t\terr = sk->sk_prot->disconnect(sk, flags);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tgoto out;\n\t}\n\n\tswitch (sock->state) {\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\tcase SS_CONNECTED:\n\t\terr = -EISCONN;\n\t\tgoto out;\n\tcase SS_CONNECTING:\n\t\terr = -EALREADY;\n\t\t/* Fall out of switch with err, set for this state */\n\t\tbreak;\n\tcase SS_UNCONNECTED:\n\t\terr = -EISCONN;\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\tgoto out;\n\n\t\terr = sk->sk_prot->connect(sk, uaddr, addr_len);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tsock->state = SS_CONNECTING;\n\n\t\t/* Just entered SS_CONNECTING state; the only\n\t\t * difference is that return value in non-blocking\n\t\t * case is EINPROGRESS, rather than EALREADY.\n\t\t */\n\t\terr = -EINPROGRESS;\n\t\tbreak;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\t/* Error code is set above */\n\t\tif (!timeo || !inet_wait_for_connect(sk, timeo))\n\t\t\tgoto out;\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/* Connection was closed by RST, timeout, ICMP error\n\t * or another process disconnected us.\n\t */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto sock_error;\n\n\t/* sk->sk_err may be not zero now, if RECVERR was ordered by user\n\t * and error was received after socket entered established state.\n\t * Hence, it is handled normally after connect() return successfully.\n\t */\n\n\tsock->state = SS_CONNECTED;\n\terr = 0;\nout:\n\trelease_sock(sk);\n\treturn err;\n\nsock_error:\n\terr = sock_error(sk) ? : -ECONNABORTED;\n\tsock->state = SS_UNCONNECTED;\n\tif (sk->sk_prot->disconnect(sk, flags))\n\t\tsock->state = SS_DISCONNECTING;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_stream_connect);\n\n/*\n *\tAccept a pending connection. The TCP layer now gives BSD semantics.\n */\n\nint inet_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_accept);\n\n\n/*\n *\tThis does both peername and sockname.\n */\nint inet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint *uaddr_len, int peer)\n{\n\tstruct sock *sk\t\t= sock->sk;\n\tstruct inet_sock *inet\t= inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, uaddr);\n\n\tsin->sin_family = AF_INET;\n\tif (peer) {\n\t\tif (!inet->inet_dport ||\n\t\t    (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&\n\t\t     peer == 1))\n\t\t\treturn -ENOTCONN;\n\t\tsin->sin_port = inet->inet_dport;\n\t\tsin->sin_addr.s_addr = inet->inet_daddr;\n\t} else {\n\t\t__be32 addr = inet->inet_rcv_saddr;\n\t\tif (!addr)\n\t\t\taddr = inet->inet_saddr;\n\t\tsin->sin_port = inet->inet_sport;\n\t\tsin->sin_addr.s_addr = addr;\n\t}\n\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t*uaddr_len = sizeof(*sin);\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_getname);\n\nint inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\t size_t size)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\treturn sk->sk_prot->sendmsg(iocb, sk, msg, size);\n}\nEXPORT_SYMBOL(inet_sendmsg);\n\nssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\tif (sk->sk_prot->sendpage)\n\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}\nEXPORT_SYMBOL(inet_sendpage);\n\nint inet_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\t size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\tsock_rps_record_flow(sk);\n\n\terr = sk->sk_prot->recvmsg(iocb, sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_recvmsg);\n\nint inet_shutdown(struct socket *sock, int how)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\t/* This should really check to make sure\n\t * the socket is a TCP socket. (WHY AC...)\n\t */\n\thow++; /* maps 0->1 has the advantage of making bit 1 rcvs and\n\t\t       1->2 bit 2 snds.\n\t\t       2->3 */\n\tif ((how & ~SHUTDOWN_MASK) || !how)\t/* MAXINT->0 */\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tif (sock->state == SS_CONNECTING) {\n\t\tif ((1 << sk->sk_state) &\n\t\t    (TCPF_SYN_SENT | TCPF_SYN_RECV | TCPF_CLOSE))\n\t\t\tsock->state = SS_DISCONNECTING;\n\t\telse\n\t\t\tsock->state = SS_CONNECTED;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\terr = -ENOTCONN;\n\t\t/* Hack to wake up other listeners, who can poll for\n\t\t   POLLHUP, even on eg. unconnected UDP sockets -- RR */\n\tdefault:\n\t\tsk->sk_shutdown |= how;\n\t\tif (sk->sk_prot->shutdown)\n\t\t\tsk->sk_prot->shutdown(sk, how);\n\t\tbreak;\n\n\t/* Remaining two branches are temporary solution for missing\n\t * close() in multithreaded environment. It is _not_ a good idea,\n\t * but we have no choice until close() is repaired at VFS level.\n\t */\n\tcase TCP_LISTEN:\n\t\tif (!(how & RCV_SHUTDOWN))\n\t\t\tbreak;\n\t\t/* Fall through */\n\tcase TCP_SYN_SENT:\n\t\terr = sk->sk_prot->disconnect(sk, O_NONBLOCK);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tbreak;\n\t}\n\n\t/* Wake up anyone sleeping in poll. */\n\tsk->sk_state_change(sk);\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_shutdown);\n\n/*\n *\tioctl() calls you can issue on an INET socket. Most of these are\n *\tdevice configuration and stuff and very rarely used. Some ioctls\n *\tpass on to the socket itself.\n *\n *\tNOTE: I like the idea of a module for the config stuff. ie ifconfig\n *\tloads the devconfigure module does its configuring and unloads it.\n *\tThere's a good 20K of config code hanging around the kernel.\n */\n\nint inet_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\tstruct net *net = sock_net(sk);\n\n\tswitch (cmd) {\n\tcase SIOCGSTAMP:\n\t\terr = sock_get_timestamp(sk, (struct timeval __user *)arg);\n\t\tbreak;\n\tcase SIOCGSTAMPNS:\n\t\terr = sock_get_timestampns(sk, (struct timespec __user *)arg);\n\t\tbreak;\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCRTMSG:\n\t\terr = ip_rt_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\t\terr = arp_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFPFLAGS:\n\tcase SIOCGIFPFLAGS:\n\tcase SIOCSIFFLAGS:\n\t\terr = devinet_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tif (sk->sk_prot->ioctl)\n\t\t\terr = sk->sk_prot->ioctl(sk, cmd, arg);\n\t\telse\n\t\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(inet_ioctl);\n\n#ifdef CONFIG_COMPAT\nint inet_compat_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = -ENOIOCTLCMD;\n\n\tif (sk->sk_prot->compat_ioctl)\n\t\terr = sk->sk_prot->compat_ioctl(sk, cmd, arg);\n\n\treturn err;\n}\n#endif\n\nconst struct proto_ops inet_stream_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = tcp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = inet_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n\t.splice_read\t   = tcp_splice_read,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_stream_ops);\n\nconst struct proto_ops inet_dgram_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = udp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_dgram_ops);\n\n/*\n * For SOCK_RAW sockets; should be the same as inet_dgram_ops but without\n * udp_poll\n */\nstatic const struct proto_ops inet_sockraw_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\n\nstatic const struct net_proto_family inet_family_ops = {\n\t.family = PF_INET,\n\t.create = inet_create,\n\t.owner\t= THIS_MODULE,\n};\n\n/* Upon startup we insert all the elements in inetsw_array[] into\n * the linked list inetsw.\n */\nstatic struct inet_protosw inetsw_array[] =\n{\n\t{\n\t\t.type =       SOCK_STREAM,\n\t\t.protocol =   IPPROTO_TCP,\n\t\t.prot =       &tcp_prot,\n\t\t.ops =        &inet_stream_ops,\n\t\t.no_check =   0,\n\t\t.flags =      INET_PROTOSW_PERMANENT |\n\t\t\t      INET_PROTOSW_ICSK,\n\t},\n\n\t{\n\t\t.type =       SOCK_DGRAM,\n\t\t.protocol =   IPPROTO_UDP,\n\t\t.prot =       &udp_prot,\n\t\t.ops =        &inet_dgram_ops,\n\t\t.no_check =   UDP_CSUM_DEFAULT,\n\t\t.flags =      INET_PROTOSW_PERMANENT,\n       },\n\n\n       {\n\t       .type =       SOCK_RAW,\n\t       .protocol =   IPPROTO_IP,\t/* wild card */\n\t       .prot =       &raw_prot,\n\t       .ops =        &inet_sockraw_ops,\n\t       .no_check =   UDP_CSUM_DEFAULT,\n\t       .flags =      INET_PROTOSW_REUSE,\n       }\n};\n\n#define INETSW_ARRAY_LEN ARRAY_SIZE(inetsw_array)\n\nvoid inet_register_protosw(struct inet_protosw *p)\n{\n\tstruct list_head *lh;\n\tstruct inet_protosw *answer;\n\tint protocol = p->protocol;\n\tstruct list_head *last_perm;\n\n\tspin_lock_bh(&inetsw_lock);\n\n\tif (p->type >= SOCK_MAX)\n\t\tgoto out_illegal;\n\n\t/* If we are trying to override a permanent protocol, bail. */\n\tanswer = NULL;\n\tlast_perm = &inetsw[p->type];\n\tlist_for_each(lh, &inetsw[p->type]) {\n\t\tanswer = list_entry(lh, struct inet_protosw, list);\n\n\t\t/* Check only the non-wild match. */\n\t\tif (INET_PROTOSW_PERMANENT & answer->flags) {\n\t\t\tif (protocol == answer->protocol)\n\t\t\t\tbreak;\n\t\t\tlast_perm = lh;\n\t\t}\n\n\t\tanswer = NULL;\n\t}\n\tif (answer)\n\t\tgoto out_permanent;\n\n\t/* Add the new entry after the last permanent entry if any, so that\n\t * the new entry does not override a permanent entry when matched with\n\t * a wild-card protocol. But it is allowed to override any existing\n\t * non-permanent entry.  This means that when we remove this entry, the\n\t * system automatically returns to the old behavior.\n\t */\n\tlist_add_rcu(&p->list, last_perm);\nout:\n\tspin_unlock_bh(&inetsw_lock);\n\n\treturn;\n\nout_permanent:\n\tprintk(KERN_ERR \"Attempt to override permanent protocol %d.\\n\",\n\t       protocol);\n\tgoto out;\n\nout_illegal:\n\tprintk(KERN_ERR\n\t       \"Ignoring attempt to register invalid socket type %d.\\n\",\n\t       p->type);\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_register_protosw);\n\nvoid inet_unregister_protosw(struct inet_protosw *p)\n{\n\tif (INET_PROTOSW_PERMANENT & p->flags) {\n\t\tprintk(KERN_ERR\n\t\t       \"Attempt to unregister permanent protocol %d.\\n\",\n\t\t       p->protocol);\n\t} else {\n\t\tspin_lock_bh(&inetsw_lock);\n\t\tlist_del_rcu(&p->list);\n\t\tspin_unlock_bh(&inetsw_lock);\n\n\t\tsynchronize_net();\n\t}\n}\nEXPORT_SYMBOL(inet_unregister_protosw);\n\n/*\n *      Shall we try to damage output packets if routing dev changes?\n */\n\nint sysctl_ip_dynaddr __read_mostly;\n\nstatic int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\n\tif (inet->opt && inet->opt->srr)\n\t\tdaddr = inet->opt->faddr;\n\n\t/* Query new route. */\n\trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk, false);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = rt->rt_src;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sysctl_ip_dynaddr > 1) {\n\t\tprintk(KERN_INFO \"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t       __func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\t__sk_prot_rehash(sk);\n\treturn 0;\n}\n\nint inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\tdaddr = inet->inet_daddr;\n\tif (inet->opt && inet->opt->srr)\n\t\tdaddr = inet->opt->faddr;\n\trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(inet_sk_rebuild_header);\n\nstatic int inet_gso_send_check(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tconst struct net_protocol *ops;\n\tint proto;\n\tint ihl;\n\tint err = -EINVAL;\n\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*iph))))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\tif (ihl < sizeof(*iph))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, ihl)))\n\t\tgoto out;\n\n\t__skb_pull(skb, ihl);\n\tskb_reset_transport_header(skb);\n\tiph = ip_hdr(skb);\n\tproto = iph->protocol & (MAX_INET_PROTOS - 1);\n\terr = -EPROTONOSUPPORT;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (likely(ops && ops->gso_send_check))\n\t\terr = ops->gso_send_check(skb);\n\trcu_read_unlock();\n\nout:\n\treturn err;\n}\n\nstatic struct sk_buff *inet_gso_segment(struct sk_buff *skb, u32 features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct iphdr *iph;\n\tconst struct net_protocol *ops;\n\tint proto;\n\tint ihl;\n\tint id;\n\tunsigned int offset = 0;\n\n\tif (!(features & NETIF_F_V4_CSUM))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t     ~(SKB_GSO_TCPV4 |\n\t\t       SKB_GSO_UDP |\n\t\t       SKB_GSO_DODGY |\n\t\t       SKB_GSO_TCP_ECN |\n\t\t       0)))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*iph))))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\tif (ihl < sizeof(*iph))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, ihl)))\n\t\tgoto out;\n\n\t__skb_pull(skb, ihl);\n\tskb_reset_transport_header(skb);\n\tiph = ip_hdr(skb);\n\tid = ntohs(iph->id);\n\tproto = iph->protocol & (MAX_INET_PROTOS - 1);\n\tsegs = ERR_PTR(-EPROTONOSUPPORT);\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (likely(ops && ops->gso_segment))\n\t\tsegs = ops->gso_segment(skb, features);\n\trcu_read_unlock();\n\n\tif (!segs || IS_ERR(segs))\n\t\tgoto out;\n\n\tskb = segs;\n\tdo {\n\t\tiph = ip_hdr(skb);\n\t\tif (proto == IPPROTO_UDP) {\n\t\t\tiph->id = htons(id);\n\t\t\tiph->frag_off = htons(offset >> 3);\n\t\t\tif (skb->next != NULL)\n\t\t\t\tiph->frag_off |= htons(IP_MF);\n\t\t\toffset += (skb->len - skb->mac_len - iph->ihl * 4);\n\t\t} else\n\t\t\tiph->id = htons(id++);\n\t\tiph->tot_len = htons(skb->len - skb->mac_len);\n\t\tiph->check = 0;\n\t\tiph->check = ip_fast_csum(skb_network_header(skb), iph->ihl);\n\t} while ((skb = skb->next));\n\nout:\n\treturn segs;\n}\n\nstatic struct sk_buff **inet_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct net_protocol *ops;\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct iphdr *iph;\n\tunsigned int hlen;\n\tunsigned int off;\n\tunsigned int id;\n\tint flush = 1;\n\tint proto;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*iph);\n\tiph = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tiph = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!iph))\n\t\t\tgoto out;\n\t}\n\n\tproto = iph->protocol & (MAX_INET_PROTOS - 1);\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (!ops || !ops->gro_receive)\n\t\tgoto out_unlock;\n\n\tif (*(u8 *)iph != 0x45)\n\t\tgoto out_unlock;\n\n\tif (unlikely(ip_fast_csum((u8 *)iph, iph->ihl)))\n\t\tgoto out_unlock;\n\n\tid = ntohl(*(__be32 *)&iph->id);\n\tflush = (u16)((ntohl(*(__be32 *)iph) ^ skb_gro_len(skb)) | (id ^ IP_DF));\n\tid >>= 16;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tstruct iphdr *iph2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tiph2 = ip_hdr(p);\n\n\t\tif ((iph->protocol ^ iph2->protocol) |\n\t\t    (iph->tos ^ iph2->tos) |\n\t\t    ((__force u32)iph->saddr ^ (__force u32)iph2->saddr) |\n\t\t    ((__force u32)iph->daddr ^ (__force u32)iph2->daddr)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* All fields must match except length and checksum. */\n\t\tNAPI_GRO_CB(p)->flush |=\n\t\t\t(iph->ttl ^ iph2->ttl) |\n\t\t\t((u16)(ntohs(iph2->id) + NAPI_GRO_CB(p)->count) ^ id);\n\n\t\tNAPI_GRO_CB(p)->flush |= flush;\n\t}\n\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\tskb_gro_pull(skb, sizeof(*iph));\n\tskb_set_transport_header(skb, skb_gro_offset(skb));\n\n\tpp = ops->gro_receive(head, skb);\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic int inet_gro_complete(struct sk_buff *skb)\n{\n\tconst struct net_protocol *ops;\n\tstruct iphdr *iph = ip_hdr(skb);\n\tint proto = iph->protocol & (MAX_INET_PROTOS - 1);\n\tint err = -ENOSYS;\n\t__be16 newlen = htons(skb->len - skb_network_offset(skb));\n\n\tcsum_replace2(&iph->check, iph->tot_len, newlen);\n\tiph->tot_len = newlen;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (WARN_ON(!ops || !ops->gro_complete))\n\t\tgoto out_unlock;\n\n\terr = ops->gro_complete(skb);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nint inet_ctl_sock_create(struct sock **sk, unsigned short family,\n\t\t\t unsigned short type, unsigned char protocol,\n\t\t\t struct net *net)\n{\n\tstruct socket *sock;\n\tint rc = sock_create_kern(family, type, protocol, &sock);\n\n\tif (rc == 0) {\n\t\t*sk = sock->sk;\n\t\t(*sk)->sk_allocation = GFP_ATOMIC;\n\t\t/*\n\t\t * Unhash it so that IP input processing does not even see it,\n\t\t * we do not wish this socket to see incoming packets.\n\t\t */\n\t\t(*sk)->sk_prot->unhash(*sk);\n\n\t\tsk_change_net(*sk, net);\n\t}\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(inet_ctl_sock_create);\n\nunsigned long snmp_fold_field(void __percpu *mib[], int offt)\n{\n\tunsigned long res = 0;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tres += *(((unsigned long *) per_cpu_ptr(mib[0], i)) + offt);\n\t\tres += *(((unsigned long *) per_cpu_ptr(mib[1], i)) + offt);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field);\n\n#if BITS_PER_LONG==32\n\nu64 snmp_fold_field64(void __percpu *mib[], int offt, size_t syncp_offset)\n{\n\tu64 res = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *bhptr, *userptr;\n\t\tstruct u64_stats_sync *syncp;\n\t\tu64 v_bh, v_user;\n\t\tunsigned int start;\n\n\t\t/* first mib used by softirq context, we must use _bh() accessors */\n\t\tbhptr = per_cpu_ptr(SNMP_STAT_BHPTR(mib), cpu);\n\t\tsyncp = (struct u64_stats_sync *)(bhptr + syncp_offset);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_bh(syncp);\n\t\t\tv_bh = *(((u64 *) bhptr) + offt);\n\t\t} while (u64_stats_fetch_retry_bh(syncp, start));\n\n\t\t/* second mib used in USER context */\n\t\tuserptr = per_cpu_ptr(SNMP_STAT_USRPTR(mib), cpu);\n\t\tsyncp = (struct u64_stats_sync *)(userptr + syncp_offset);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(syncp);\n\t\t\tv_user = *(((u64 *) userptr) + offt);\n\t\t} while (u64_stats_fetch_retry(syncp, start));\n\n\t\tres += v_bh + v_user;\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field64);\n#endif\n\nint snmp_mib_init(void __percpu *ptr[2], size_t mibsize, size_t align)\n{\n\tBUG_ON(ptr == NULL);\n\tptr[0] = __alloc_percpu(mibsize, align);\n\tif (!ptr[0])\n\t\tgoto err0;\n\tptr[1] = __alloc_percpu(mibsize, align);\n\tif (!ptr[1])\n\t\tgoto err1;\n\treturn 0;\nerr1:\n\tfree_percpu(ptr[0]);\n\tptr[0] = NULL;\nerr0:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(snmp_mib_init);\n\nvoid snmp_mib_free(void __percpu *ptr[2])\n{\n\tBUG_ON(ptr == NULL);\n\tfree_percpu(ptr[0]);\n\tfree_percpu(ptr[1]);\n\tptr[0] = ptr[1] = NULL;\n}\nEXPORT_SYMBOL_GPL(snmp_mib_free);\n\n#ifdef CONFIG_IP_MULTICAST\nstatic const struct net_protocol igmp_protocol = {\n\t.handler =\tigmp_rcv,\n\t.netns_ok =\t1,\n};\n#endif\n\nstatic const struct net_protocol tcp_protocol = {\n\t.handler =\ttcp_v4_rcv,\n\t.err_handler =\ttcp_v4_err,\n\t.gso_send_check = tcp_v4_gso_send_check,\n\t.gso_segment =\ttcp_tso_segment,\n\t.gro_receive =\ttcp4_gro_receive,\n\t.gro_complete =\ttcp4_gro_complete,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic const struct net_protocol udp_protocol = {\n\t.handler =\tudp_rcv,\n\t.err_handler =\tudp_err,\n\t.gso_send_check = udp4_ufo_send_check,\n\t.gso_segment = udp4_ufo_fragment,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic const struct net_protocol icmp_protocol = {\n\t.handler =\ticmp_rcv,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic __net_init int ipv4_mib_init_net(struct net *net)\n{\n\tif (snmp_mib_init((void __percpu **)net->mib.tcp_statistics,\n\t\t\t  sizeof(struct tcp_mib),\n\t\t\t  __alignof__(struct tcp_mib)) < 0)\n\t\tgoto err_tcp_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.ip_statistics,\n\t\t\t  sizeof(struct ipstats_mib),\n\t\t\t  __alignof__(struct ipstats_mib)) < 0)\n\t\tgoto err_ip_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.net_statistics,\n\t\t\t  sizeof(struct linux_mib),\n\t\t\t  __alignof__(struct linux_mib)) < 0)\n\t\tgoto err_net_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.udp_statistics,\n\t\t\t  sizeof(struct udp_mib),\n\t\t\t  __alignof__(struct udp_mib)) < 0)\n\t\tgoto err_udp_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.udplite_statistics,\n\t\t\t  sizeof(struct udp_mib),\n\t\t\t  __alignof__(struct udp_mib)) < 0)\n\t\tgoto err_udplite_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.icmp_statistics,\n\t\t\t  sizeof(struct icmp_mib),\n\t\t\t  __alignof__(struct icmp_mib)) < 0)\n\t\tgoto err_icmp_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.icmpmsg_statistics,\n\t\t\t  sizeof(struct icmpmsg_mib),\n\t\t\t  __alignof__(struct icmpmsg_mib)) < 0)\n\t\tgoto err_icmpmsg_mib;\n\n\ttcp_mib_init(net);\n\treturn 0;\n\nerr_icmpmsg_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.icmp_statistics);\nerr_icmp_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.udplite_statistics);\nerr_udplite_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.udp_statistics);\nerr_udp_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.net_statistics);\nerr_net_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.ip_statistics);\nerr_ip_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.tcp_statistics);\nerr_tcp_mib:\n\treturn -ENOMEM;\n}\n\nstatic __net_exit void ipv4_mib_exit_net(struct net *net)\n{\n\tsnmp_mib_free((void __percpu **)net->mib.icmpmsg_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.icmp_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.udplite_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.udp_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.net_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.ip_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.tcp_statistics);\n}\n\nstatic __net_initdata struct pernet_operations ipv4_mib_ops = {\n\t.init = ipv4_mib_init_net,\n\t.exit = ipv4_mib_exit_net,\n};\n\nstatic int __init init_ipv4_mibs(void)\n{\n\treturn register_pernet_subsys(&ipv4_mib_ops);\n}\n\nstatic int ipv4_proc_init(void);\n\n/*\n *\tIP protocol layer initialiser\n */\n\nstatic struct packet_type ip_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.func = ip_rcv,\n\t.gso_send_check = inet_gso_send_check,\n\t.gso_segment = inet_gso_segment,\n\t.gro_receive = inet_gro_receive,\n\t.gro_complete = inet_gro_complete,\n};\n\nstatic int __init inet_init(void)\n{\n\tstruct sk_buff *dummy_skb;\n\tstruct inet_protosw *q;\n\tstruct list_head *r;\n\tint rc = -EINVAL;\n\n\tBUILD_BUG_ON(sizeof(struct inet_skb_parm) > sizeof(dummy_skb->cb));\n\n\tsysctl_local_reserved_ports = kzalloc(65536 / 8, GFP_KERNEL);\n\tif (!sysctl_local_reserved_ports)\n\t\tgoto out;\n\n\trc = proto_register(&tcp_prot, 1);\n\tif (rc)\n\t\tgoto out_free_reserved_ports;\n\n\trc = proto_register(&udp_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_tcp_proto;\n\n\trc = proto_register(&raw_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_udp_proto;\n\n\t/*\n\t *\tTell SOCKET that we are alive...\n\t */\n\n\t(void)sock_register(&inet_family_ops);\n\n#ifdef CONFIG_SYSCTL\n\tip_static_sysctl_init();\n#endif\n\n\t/*\n\t *\tAdd all the base protocols.\n\t */\n\n\tif (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add ICMP protocol\\n\");\n\tif (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add UDP protocol\\n\");\n\tif (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add TCP protocol\\n\");\n#ifdef CONFIG_IP_MULTICAST\n\tif (inet_add_protocol(&igmp_protocol, IPPROTO_IGMP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add IGMP protocol\\n\");\n#endif\n\n\t/* Register the socket-side information for inet_create. */\n\tfor (r = &inetsw[0]; r < &inetsw[SOCK_MAX]; ++r)\n\t\tINIT_LIST_HEAD(r);\n\n\tfor (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)\n\t\tinet_register_protosw(q);\n\n\t/*\n\t *\tSet the ARP module up\n\t */\n\n\tarp_init();\n\n\t/*\n\t *\tSet the IP module up\n\t */\n\n\tip_init();\n\n\ttcp_v4_init();\n\n\t/* Setup TCP slab cache for open requests. */\n\ttcp_init();\n\n\t/* Setup UDP memory threshold */\n\tudp_init();\n\n\t/* Add UDP-Lite (RFC 3828) */\n\tudplite4_register();\n\n\t/*\n\t *\tSet the ICMP layer up\n\t */\n\n\tif (icmp_init() < 0)\n\t\tpanic(\"Failed to create the ICMP control socket.\\n\");\n\n\t/*\n\t *\tInitialise the multicast router\n\t */\n#if defined(CONFIG_IP_MROUTE)\n\tif (ip_mr_init())\n\t\tprintk(KERN_CRIT \"inet_init: Cannot init ipv4 mroute\\n\");\n#endif\n\t/*\n\t *\tInitialise per-cpu ipv4 mibs\n\t */\n\n\tif (init_ipv4_mibs())\n\t\tprintk(KERN_CRIT \"inet_init: Cannot init ipv4 mibs\\n\");\n\n\tipv4_proc_init();\n\n\tipfrag_init();\n\n\tdev_add_pack(&ip_packet_type);\n\n\trc = 0;\nout:\n\treturn rc;\nout_unregister_udp_proto:\n\tproto_unregister(&udp_prot);\nout_unregister_tcp_proto:\n\tproto_unregister(&tcp_prot);\nout_free_reserved_ports:\n\tkfree(sysctl_local_reserved_ports);\n\tgoto out;\n}\n\nfs_initcall(inet_init);\n\n/* ------------------------------------------------------------------------ */\n\n#ifdef CONFIG_PROC_FS\nstatic int __init ipv4_proc_init(void)\n{\n\tint rc = 0;\n\n\tif (raw_proc_init())\n\t\tgoto out_raw;\n\tif (tcp4_proc_init())\n\t\tgoto out_tcp;\n\tif (udp4_proc_init())\n\t\tgoto out_udp;\n\tif (ip_misc_proc_init())\n\t\tgoto out_misc;\nout:\n\treturn rc;\nout_misc:\n\tudp4_proc_exit();\nout_udp:\n\ttcp4_proc_exit();\nout_tcp:\n\traw_proc_exit();\nout_raw:\n\trc = -ENOMEM;\n\tgoto out;\n}\n\n#else /* CONFIG_PROC_FS */\nstatic int __init ipv4_proc_init(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_PROC_FS */\n\nMODULE_ALIAS_NETPROTO(PF_INET);\n\n", "/*\n * CIPSO - Commercial IP Security Option\n *\n * This is an implementation of the CIPSO 2.2 protocol as specified in\n * draft-ietf-cipso-ipsecurity-01.txt with additional tag types as found in\n * FIPS-188.  While CIPSO never became a full IETF RFC standard many vendors\n * have chosen to adopt the protocol and over the years it has become a\n * de-facto standard for labeled networking.\n *\n * The CIPSO draft specification can be found in the kernel's Documentation\n * directory as well as the following URL:\n *   http://tools.ietf.org/id/draft-ietf-cipso-ipsecurity-01.txt\n * The FIPS-188 specification can be found at the following URL:\n *   http://www.itl.nist.gov/fipspubs/fip188.htm\n *\n * Author: Paul Moore <paul.moore@hp.com>\n *\n */\n\n/*\n * (c) Copyright Hewlett-Packard Development Company, L.P., 2006, 2008\n *\n * This program is free software;  you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY;  without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See\n * the GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program;  if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n *\n */\n\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/rcupdate.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/jhash.h>\n#include <linux/audit.h>\n#include <linux/slab.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/tcp.h>\n#include <net/netlabel.h>\n#include <net/cipso_ipv4.h>\n#include <asm/atomic.h>\n#include <asm/bug.h>\n#include <asm/unaligned.h>\n\n/* List of available DOI definitions */\n/* XXX - This currently assumes a minimal number of different DOIs in use,\n * if in practice there are a lot of different DOIs this list should\n * probably be turned into a hash table or something similar so we\n * can do quick lookups. */\nstatic DEFINE_SPINLOCK(cipso_v4_doi_list_lock);\nstatic LIST_HEAD(cipso_v4_doi_list);\n\n/* Label mapping cache */\nint cipso_v4_cache_enabled = 1;\nint cipso_v4_cache_bucketsize = 10;\n#define CIPSO_V4_CACHE_BUCKETBITS     7\n#define CIPSO_V4_CACHE_BUCKETS        (1 << CIPSO_V4_CACHE_BUCKETBITS)\n#define CIPSO_V4_CACHE_REORDERLIMIT   10\nstruct cipso_v4_map_cache_bkt {\n\tspinlock_t lock;\n\tu32 size;\n\tstruct list_head list;\n};\nstruct cipso_v4_map_cache_entry {\n\tu32 hash;\n\tunsigned char *key;\n\tsize_t key_len;\n\n\tstruct netlbl_lsm_cache *lsm_data;\n\n\tu32 activity;\n\tstruct list_head list;\n};\nstatic struct cipso_v4_map_cache_bkt *cipso_v4_cache = NULL;\n\n/* Restricted bitmap (tag #1) flags */\nint cipso_v4_rbm_optfmt = 0;\nint cipso_v4_rbm_strictvalid = 1;\n\n/*\n * Protocol Constants\n */\n\n/* Maximum size of the CIPSO IP option, derived from the fact that the maximum\n * IPv4 header size is 60 bytes and the base IPv4 header is 20 bytes long. */\n#define CIPSO_V4_OPT_LEN_MAX          40\n\n/* Length of the base CIPSO option, this includes the option type (1 byte), the\n * option length (1 byte), and the DOI (4 bytes). */\n#define CIPSO_V4_HDR_LEN              6\n\n/* Base length of the restrictive category bitmap tag (tag #1). */\n#define CIPSO_V4_TAG_RBM_BLEN         4\n\n/* Base length of the enumerated category tag (tag #2). */\n#define CIPSO_V4_TAG_ENUM_BLEN        4\n\n/* Base length of the ranged categories bitmap tag (tag #5). */\n#define CIPSO_V4_TAG_RNG_BLEN         4\n/* The maximum number of category ranges permitted in the ranged category tag\n * (tag #5).  You may note that the IETF draft states that the maximum number\n * of category ranges is 7, but if the low end of the last category range is\n * zero then it is possible to fit 8 category ranges because the zero should\n * be omitted. */\n#define CIPSO_V4_TAG_RNG_CAT_MAX      8\n\n/* Base length of the local tag (non-standard tag).\n *  Tag definition (may change between kernel versions)\n *\n * 0          8          16         24         32\n * +----------+----------+----------+----------+\n * | 10000000 | 00000110 | 32-bit secid value  |\n * +----------+----------+----------+----------+\n * | in (host byte order)|\n * +----------+----------+\n *\n */\n#define CIPSO_V4_TAG_LOC_BLEN         6\n\n/*\n * Helper Functions\n */\n\n/**\n * cipso_v4_bitmap_walk - Walk a bitmap looking for a bit\n * @bitmap: the bitmap\n * @bitmap_len: length in bits\n * @offset: starting offset\n * @state: if non-zero, look for a set (1) bit else look for a cleared (0) bit\n *\n * Description:\n * Starting at @offset, walk the bitmap from left to right until either the\n * desired bit is found or we reach the end.  Return the bit offset, -1 if\n * not found, or -2 if error.\n */\nstatic int cipso_v4_bitmap_walk(const unsigned char *bitmap,\n\t\t\t\tu32 bitmap_len,\n\t\t\t\tu32 offset,\n\t\t\t\tu8 state)\n{\n\tu32 bit_spot;\n\tu32 byte_offset;\n\tunsigned char bitmask;\n\tunsigned char byte;\n\n\t/* gcc always rounds to zero when doing integer division */\n\tbyte_offset = offset / 8;\n\tbyte = bitmap[byte_offset];\n\tbit_spot = offset;\n\tbitmask = 0x80 >> (offset % 8);\n\n\twhile (bit_spot < bitmap_len) {\n\t\tif ((state && (byte & bitmask) == bitmask) ||\n\t\t    (state == 0 && (byte & bitmask) == 0))\n\t\t\treturn bit_spot;\n\n\t\tbit_spot++;\n\t\tbitmask >>= 1;\n\t\tif (bitmask == 0) {\n\t\t\tbyte = bitmap[++byte_offset];\n\t\t\tbitmask = 0x80;\n\t\t}\n\t}\n\n\treturn -1;\n}\n\n/**\n * cipso_v4_bitmap_setbit - Sets a single bit in a bitmap\n * @bitmap: the bitmap\n * @bit: the bit\n * @state: if non-zero, set the bit (1) else clear the bit (0)\n *\n * Description:\n * Set a single bit in the bitmask.  Returns zero on success, negative values\n * on error.\n */\nstatic void cipso_v4_bitmap_setbit(unsigned char *bitmap,\n\t\t\t\t   u32 bit,\n\t\t\t\t   u8 state)\n{\n\tu32 byte_spot;\n\tu8 bitmask;\n\n\t/* gcc always rounds to zero when doing integer division */\n\tbyte_spot = bit / 8;\n\tbitmask = 0x80 >> (bit % 8);\n\tif (state)\n\t\tbitmap[byte_spot] |= bitmask;\n\telse\n\t\tbitmap[byte_spot] &= ~bitmask;\n}\n\n/**\n * cipso_v4_cache_entry_free - Frees a cache entry\n * @entry: the entry to free\n *\n * Description:\n * This function frees the memory associated with a cache entry including the\n * LSM cache data if there are no longer any users, i.e. reference count == 0.\n *\n */\nstatic void cipso_v4_cache_entry_free(struct cipso_v4_map_cache_entry *entry)\n{\n\tif (entry->lsm_data)\n\t\tnetlbl_secattr_cache_free(entry->lsm_data);\n\tkfree(entry->key);\n\tkfree(entry);\n}\n\n/**\n * cipso_v4_map_cache_hash - Hashing function for the CIPSO cache\n * @key: the hash key\n * @key_len: the length of the key in bytes\n *\n * Description:\n * The CIPSO tag hashing function.  Returns a 32-bit hash value.\n *\n */\nstatic u32 cipso_v4_map_cache_hash(const unsigned char *key, u32 key_len)\n{\n\treturn jhash(key, key_len, 0);\n}\n\n/*\n * Label Mapping Cache Functions\n */\n\n/**\n * cipso_v4_cache_init - Initialize the CIPSO cache\n *\n * Description:\n * Initializes the CIPSO label mapping cache, this function should be called\n * before any of the other functions defined in this file.  Returns zero on\n * success, negative values on error.\n *\n */\nstatic int cipso_v4_cache_init(void)\n{\n\tu32 iter;\n\n\tcipso_v4_cache = kcalloc(CIPSO_V4_CACHE_BUCKETS,\n\t\t\t\t sizeof(struct cipso_v4_map_cache_bkt),\n\t\t\t\t GFP_KERNEL);\n\tif (cipso_v4_cache == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (iter = 0; iter < CIPSO_V4_CACHE_BUCKETS; iter++) {\n\t\tspin_lock_init(&cipso_v4_cache[iter].lock);\n\t\tcipso_v4_cache[iter].size = 0;\n\t\tINIT_LIST_HEAD(&cipso_v4_cache[iter].list);\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_cache_invalidate - Invalidates the current CIPSO cache\n *\n * Description:\n * Invalidates and frees any entries in the CIPSO cache.  Returns zero on\n * success and negative values on failure.\n *\n */\nvoid cipso_v4_cache_invalidate(void)\n{\n\tstruct cipso_v4_map_cache_entry *entry, *tmp_entry;\n\tu32 iter;\n\n\tfor (iter = 0; iter < CIPSO_V4_CACHE_BUCKETS; iter++) {\n\t\tspin_lock_bh(&cipso_v4_cache[iter].lock);\n\t\tlist_for_each_entry_safe(entry,\n\t\t\t\t\t tmp_entry,\n\t\t\t\t\t &cipso_v4_cache[iter].list, list) {\n\t\t\tlist_del(&entry->list);\n\t\t\tcipso_v4_cache_entry_free(entry);\n\t\t}\n\t\tcipso_v4_cache[iter].size = 0;\n\t\tspin_unlock_bh(&cipso_v4_cache[iter].lock);\n\t}\n}\n\n/**\n * cipso_v4_cache_check - Check the CIPSO cache for a label mapping\n * @key: the buffer to check\n * @key_len: buffer length in bytes\n * @secattr: the security attribute struct to use\n *\n * Description:\n * This function checks the cache to see if a label mapping already exists for\n * the given key.  If there is a match then the cache is adjusted and the\n * @secattr struct is populated with the correct LSM security attributes.  The\n * cache is adjusted in the following manner if the entry is not already the\n * first in the cache bucket:\n *\n *  1. The cache entry's activity counter is incremented\n *  2. The previous (higher ranking) entry's activity counter is decremented\n *  3. If the difference between the two activity counters is geater than\n *     CIPSO_V4_CACHE_REORDERLIMIT the two entries are swapped\n *\n * Returns zero on success, -ENOENT for a cache miss, and other negative values\n * on error.\n *\n */\nstatic int cipso_v4_cache_check(const unsigned char *key,\n\t\t\t\tu32 key_len,\n\t\t\t\tstruct netlbl_lsm_secattr *secattr)\n{\n\tu32 bkt;\n\tstruct cipso_v4_map_cache_entry *entry;\n\tstruct cipso_v4_map_cache_entry *prev_entry = NULL;\n\tu32 hash;\n\n\tif (!cipso_v4_cache_enabled)\n\t\treturn -ENOENT;\n\n\thash = cipso_v4_map_cache_hash(key, key_len);\n\tbkt = hash & (CIPSO_V4_CACHE_BUCKETS - 1);\n\tspin_lock_bh(&cipso_v4_cache[bkt].lock);\n\tlist_for_each_entry(entry, &cipso_v4_cache[bkt].list, list) {\n\t\tif (entry->hash == hash &&\n\t\t    entry->key_len == key_len &&\n\t\t    memcmp(entry->key, key, key_len) == 0) {\n\t\t\tentry->activity += 1;\n\t\t\tatomic_inc(&entry->lsm_data->refcount);\n\t\t\tsecattr->cache = entry->lsm_data;\n\t\t\tsecattr->flags |= NETLBL_SECATTR_CACHE;\n\t\t\tsecattr->type = NETLBL_NLTYPE_CIPSOV4;\n\t\t\tif (prev_entry == NULL) {\n\t\t\t\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\tif (prev_entry->activity > 0)\n\t\t\t\tprev_entry->activity -= 1;\n\t\t\tif (entry->activity > prev_entry->activity &&\n\t\t\t    entry->activity - prev_entry->activity >\n\t\t\t    CIPSO_V4_CACHE_REORDERLIMIT) {\n\t\t\t\t__list_del(entry->list.prev, entry->list.next);\n\t\t\t\t__list_add(&entry->list,\n\t\t\t\t\t   prev_entry->list.prev,\n\t\t\t\t\t   &prev_entry->list);\n\t\t\t}\n\n\t\t\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\t\t\treturn 0;\n\t\t}\n\t\tprev_entry = entry;\n\t}\n\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\n\treturn -ENOENT;\n}\n\n/**\n * cipso_v4_cache_add - Add an entry to the CIPSO cache\n * @skb: the packet\n * @secattr: the packet's security attributes\n *\n * Description:\n * Add a new entry into the CIPSO label mapping cache.  Add the new entry to\n * head of the cache bucket's list, if the cache bucket is out of room remove\n * the last entry in the list first.  It is important to note that there is\n * currently no checking for duplicate keys.  Returns zero on success,\n * negative values on failure.\n *\n */\nint cipso_v4_cache_add(const struct sk_buff *skb,\n\t\t       const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tu32 bkt;\n\tstruct cipso_v4_map_cache_entry *entry = NULL;\n\tstruct cipso_v4_map_cache_entry *old_entry = NULL;\n\tunsigned char *cipso_ptr;\n\tu32 cipso_ptr_len;\n\n\tif (!cipso_v4_cache_enabled || cipso_v4_cache_bucketsize <= 0)\n\t\treturn 0;\n\n\tcipso_ptr = CIPSO_V4_OPTPTR(skb);\n\tcipso_ptr_len = cipso_ptr[1];\n\n\tentry = kzalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (entry == NULL)\n\t\treturn -ENOMEM;\n\tentry->key = kmemdup(cipso_ptr, cipso_ptr_len, GFP_ATOMIC);\n\tif (entry->key == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto cache_add_failure;\n\t}\n\tentry->key_len = cipso_ptr_len;\n\tentry->hash = cipso_v4_map_cache_hash(cipso_ptr, cipso_ptr_len);\n\tatomic_inc(&secattr->cache->refcount);\n\tentry->lsm_data = secattr->cache;\n\n\tbkt = entry->hash & (CIPSO_V4_CACHE_BUCKETS - 1);\n\tspin_lock_bh(&cipso_v4_cache[bkt].lock);\n\tif (cipso_v4_cache[bkt].size < cipso_v4_cache_bucketsize) {\n\t\tlist_add(&entry->list, &cipso_v4_cache[bkt].list);\n\t\tcipso_v4_cache[bkt].size += 1;\n\t} else {\n\t\told_entry = list_entry(cipso_v4_cache[bkt].list.prev,\n\t\t\t\t       struct cipso_v4_map_cache_entry, list);\n\t\tlist_del(&old_entry->list);\n\t\tlist_add(&entry->list, &cipso_v4_cache[bkt].list);\n\t\tcipso_v4_cache_entry_free(old_entry);\n\t}\n\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\n\treturn 0;\n\ncache_add_failure:\n\tif (entry)\n\t\tcipso_v4_cache_entry_free(entry);\n\treturn ret_val;\n}\n\n/*\n * DOI List Functions\n */\n\n/**\n * cipso_v4_doi_search - Searches for a DOI definition\n * @doi: the DOI to search for\n *\n * Description:\n * Search the DOI definition list for a DOI definition with a DOI value that\n * matches @doi.  The caller is responsible for calling rcu_read_[un]lock().\n * Returns a pointer to the DOI definition on success and NULL on failure.\n */\nstatic struct cipso_v4_doi *cipso_v4_doi_search(u32 doi)\n{\n\tstruct cipso_v4_doi *iter;\n\n\tlist_for_each_entry_rcu(iter, &cipso_v4_doi_list, list)\n\t\tif (iter->doi == doi && atomic_read(&iter->refcount))\n\t\t\treturn iter;\n\treturn NULL;\n}\n\n/**\n * cipso_v4_doi_add - Add a new DOI to the CIPSO protocol engine\n * @doi_def: the DOI structure\n * @audit_info: NetLabel audit information\n *\n * Description:\n * The caller defines a new DOI for use by the CIPSO engine and calls this\n * function to add it to the list of acceptable domains.  The caller must\n * ensure that the mapping table specified in @doi_def->map meets all of the\n * requirements of the mapping type (see cipso_ipv4.h for details).  Returns\n * zero on success and non-zero on failure.\n *\n */\nint cipso_v4_doi_add(struct cipso_v4_doi *doi_def,\n\t\t     struct netlbl_audit *audit_info)\n{\n\tint ret_val = -EINVAL;\n\tu32 iter;\n\tu32 doi;\n\tu32 doi_type;\n\tstruct audit_buffer *audit_buf;\n\n\tdoi = doi_def->doi;\n\tdoi_type = doi_def->type;\n\n\tif (doi_def == NULL || doi_def->doi == CIPSO_V4_DOI_UNKNOWN)\n\t\tgoto doi_add_return;\n\tfor (iter = 0; iter < CIPSO_V4_TAG_MAXCNT; iter++) {\n\t\tswitch (doi_def->tags[iter]) {\n\t\tcase CIPSO_V4_TAG_RBITMAP:\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_RANGE:\n\t\tcase CIPSO_V4_TAG_ENUM:\n\t\t\tif (doi_def->type != CIPSO_V4_MAP_PASS)\n\t\t\t\tgoto doi_add_return;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_LOCAL:\n\t\t\tif (doi_def->type != CIPSO_V4_MAP_LOCAL)\n\t\t\t\tgoto doi_add_return;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_INVALID:\n\t\t\tif (iter == 0)\n\t\t\t\tgoto doi_add_return;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto doi_add_return;\n\t\t}\n\t}\n\n\tatomic_set(&doi_def->refcount, 1);\n\n\tspin_lock(&cipso_v4_doi_list_lock);\n\tif (cipso_v4_doi_search(doi_def->doi) != NULL) {\n\t\tspin_unlock(&cipso_v4_doi_list_lock);\n\t\tret_val = -EEXIST;\n\t\tgoto doi_add_return;\n\t}\n\tlist_add_tail_rcu(&doi_def->list, &cipso_v4_doi_list);\n\tspin_unlock(&cipso_v4_doi_list_lock);\n\tret_val = 0;\n\ndoi_add_return:\n\taudit_buf = netlbl_audit_start(AUDIT_MAC_CIPSOV4_ADD, audit_info);\n\tif (audit_buf != NULL) {\n\t\tconst char *type_str;\n\t\tswitch (doi_type) {\n\t\tcase CIPSO_V4_MAP_TRANS:\n\t\t\ttype_str = \"trans\";\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_PASS:\n\t\t\ttype_str = \"pass\";\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_LOCAL:\n\t\t\ttype_str = \"local\";\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttype_str = \"(unknown)\";\n\t\t}\n\t\taudit_log_format(audit_buf,\n\t\t\t\t \" cipso_doi=%u cipso_type=%s res=%u\",\n\t\t\t\t doi, type_str, ret_val == 0 ? 1 : 0);\n\t\taudit_log_end(audit_buf);\n\t}\n\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_doi_free - Frees a DOI definition\n * @entry: the entry's RCU field\n *\n * Description:\n * This function frees all of the memory associated with a DOI definition.\n *\n */\nvoid cipso_v4_doi_free(struct cipso_v4_doi *doi_def)\n{\n\tif (doi_def == NULL)\n\t\treturn;\n\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tkfree(doi_def->map.std->lvl.cipso);\n\t\tkfree(doi_def->map.std->lvl.local);\n\t\tkfree(doi_def->map.std->cat.cipso);\n\t\tkfree(doi_def->map.std->cat.local);\n\t\tbreak;\n\t}\n\tkfree(doi_def);\n}\n\n/**\n * cipso_v4_doi_free_rcu - Frees a DOI definition via the RCU pointer\n * @entry: the entry's RCU field\n *\n * Description:\n * This function is designed to be used as a callback to the call_rcu()\n * function so that the memory allocated to the DOI definition can be released\n * safely.\n *\n */\nstatic void cipso_v4_doi_free_rcu(struct rcu_head *entry)\n{\n\tstruct cipso_v4_doi *doi_def;\n\n\tdoi_def = container_of(entry, struct cipso_v4_doi, rcu);\n\tcipso_v4_doi_free(doi_def);\n}\n\n/**\n * cipso_v4_doi_remove - Remove an existing DOI from the CIPSO protocol engine\n * @doi: the DOI value\n * @audit_secid: the LSM secid to use in the audit message\n *\n * Description:\n * Removes a DOI definition from the CIPSO engine.  The NetLabel routines will\n * be called to release their own LSM domain mappings as well as our own\n * domain list.  Returns zero on success and negative values on failure.\n *\n */\nint cipso_v4_doi_remove(u32 doi, struct netlbl_audit *audit_info)\n{\n\tint ret_val;\n\tstruct cipso_v4_doi *doi_def;\n\tstruct audit_buffer *audit_buf;\n\n\tspin_lock(&cipso_v4_doi_list_lock);\n\tdoi_def = cipso_v4_doi_search(doi);\n\tif (doi_def == NULL) {\n\t\tspin_unlock(&cipso_v4_doi_list_lock);\n\t\tret_val = -ENOENT;\n\t\tgoto doi_remove_return;\n\t}\n\tif (!atomic_dec_and_test(&doi_def->refcount)) {\n\t\tspin_unlock(&cipso_v4_doi_list_lock);\n\t\tret_val = -EBUSY;\n\t\tgoto doi_remove_return;\n\t}\n\tlist_del_rcu(&doi_def->list);\n\tspin_unlock(&cipso_v4_doi_list_lock);\n\n\tcipso_v4_cache_invalidate();\n\tcall_rcu(&doi_def->rcu, cipso_v4_doi_free_rcu);\n\tret_val = 0;\n\ndoi_remove_return:\n\taudit_buf = netlbl_audit_start(AUDIT_MAC_CIPSOV4_DEL, audit_info);\n\tif (audit_buf != NULL) {\n\t\taudit_log_format(audit_buf,\n\t\t\t\t \" cipso_doi=%u res=%u\",\n\t\t\t\t doi, ret_val == 0 ? 1 : 0);\n\t\taudit_log_end(audit_buf);\n\t}\n\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_doi_getdef - Returns a reference to a valid DOI definition\n * @doi: the DOI value\n *\n * Description:\n * Searches for a valid DOI definition and if one is found it is returned to\n * the caller.  Otherwise NULL is returned.  The caller must ensure that\n * rcu_read_lock() is held while accessing the returned definition and the DOI\n * definition reference count is decremented when the caller is done.\n *\n */\nstruct cipso_v4_doi *cipso_v4_doi_getdef(u32 doi)\n{\n\tstruct cipso_v4_doi *doi_def;\n\n\trcu_read_lock();\n\tdoi_def = cipso_v4_doi_search(doi);\n\tif (doi_def == NULL)\n\t\tgoto doi_getdef_return;\n\tif (!atomic_inc_not_zero(&doi_def->refcount))\n\t\tdoi_def = NULL;\n\ndoi_getdef_return:\n\trcu_read_unlock();\n\treturn doi_def;\n}\n\n/**\n * cipso_v4_doi_putdef - Releases a reference for the given DOI definition\n * @doi_def: the DOI definition\n *\n * Description:\n * Releases a DOI definition reference obtained from cipso_v4_doi_getdef().\n *\n */\nvoid cipso_v4_doi_putdef(struct cipso_v4_doi *doi_def)\n{\n\tif (doi_def == NULL)\n\t\treturn;\n\n\tif (!atomic_dec_and_test(&doi_def->refcount))\n\t\treturn;\n\tspin_lock(&cipso_v4_doi_list_lock);\n\tlist_del_rcu(&doi_def->list);\n\tspin_unlock(&cipso_v4_doi_list_lock);\n\n\tcipso_v4_cache_invalidate();\n\tcall_rcu(&doi_def->rcu, cipso_v4_doi_free_rcu);\n}\n\n/**\n * cipso_v4_doi_walk - Iterate through the DOI definitions\n * @skip_cnt: skip past this number of DOI definitions, updated\n * @callback: callback for each DOI definition\n * @cb_arg: argument for the callback function\n *\n * Description:\n * Iterate over the DOI definition list, skipping the first @skip_cnt entries.\n * For each entry call @callback, if @callback returns a negative value stop\n * 'walking' through the list and return.  Updates the value in @skip_cnt upon\n * return.  Returns zero on success, negative values on failure.\n *\n */\nint cipso_v4_doi_walk(u32 *skip_cnt,\n\t\t     int (*callback) (struct cipso_v4_doi *doi_def, void *arg),\n\t\t     void *cb_arg)\n{\n\tint ret_val = -ENOENT;\n\tu32 doi_cnt = 0;\n\tstruct cipso_v4_doi *iter_doi;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(iter_doi, &cipso_v4_doi_list, list)\n\t\tif (atomic_read(&iter_doi->refcount) > 0) {\n\t\t\tif (doi_cnt++ < *skip_cnt)\n\t\t\t\tcontinue;\n\t\t\tret_val = callback(iter_doi, cb_arg);\n\t\t\tif (ret_val < 0) {\n\t\t\t\tdoi_cnt--;\n\t\t\t\tgoto doi_walk_return;\n\t\t\t}\n\t\t}\n\ndoi_walk_return:\n\trcu_read_unlock();\n\t*skip_cnt = doi_cnt;\n\treturn ret_val;\n}\n\n/*\n * Label Mapping Functions\n */\n\n/**\n * cipso_v4_map_lvl_valid - Checks to see if the given level is understood\n * @doi_def: the DOI definition\n * @level: the level to check\n *\n * Description:\n * Checks the given level against the given DOI definition and returns a\n * negative value if the level does not have a valid mapping and a zero value\n * if the level is defined by the DOI.\n *\n */\nstatic int cipso_v4_map_lvl_valid(const struct cipso_v4_doi *doi_def, u8 level)\n{\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tif (doi_def->map.std->lvl.cipso[level] < CIPSO_V4_INV_LVL)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -EFAULT;\n}\n\n/**\n * cipso_v4_map_lvl_hton - Perform a level mapping from the host to the network\n * @doi_def: the DOI definition\n * @host_lvl: the host MLS level\n * @net_lvl: the network/CIPSO MLS level\n *\n * Description:\n * Perform a label mapping to translate a local MLS level to the correct\n * CIPSO level using the given DOI definition.  Returns zero on success,\n * negative values otherwise.\n *\n */\nstatic int cipso_v4_map_lvl_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t u32 host_lvl,\n\t\t\t\t u32 *net_lvl)\n{\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\t*net_lvl = host_lvl;\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tif (host_lvl < doi_def->map.std->lvl.local_size &&\n\t\t    doi_def->map.std->lvl.local[host_lvl] < CIPSO_V4_INV_LVL) {\n\t\t\t*net_lvl = doi_def->map.std->lvl.local[host_lvl];\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EPERM;\n\t}\n\n\treturn -EINVAL;\n}\n\n/**\n * cipso_v4_map_lvl_ntoh - Perform a level mapping from the network to the host\n * @doi_def: the DOI definition\n * @net_lvl: the network/CIPSO MLS level\n * @host_lvl: the host MLS level\n *\n * Description:\n * Perform a label mapping to translate a CIPSO level to the correct local MLS\n * level using the given DOI definition.  Returns zero on success, negative\n * values otherwise.\n *\n */\nstatic int cipso_v4_map_lvl_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t u32 net_lvl,\n\t\t\t\t u32 *host_lvl)\n{\n\tstruct cipso_v4_std_map_tbl *map_tbl;\n\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\t*host_lvl = net_lvl;\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tmap_tbl = doi_def->map.std;\n\t\tif (net_lvl < map_tbl->lvl.cipso_size &&\n\t\t    map_tbl->lvl.cipso[net_lvl] < CIPSO_V4_INV_LVL) {\n\t\t\t*host_lvl = doi_def->map.std->lvl.cipso[net_lvl];\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EPERM;\n\t}\n\n\treturn -EINVAL;\n}\n\n/**\n * cipso_v4_map_cat_rbm_valid - Checks to see if the category bitmap is valid\n * @doi_def: the DOI definition\n * @bitmap: category bitmap\n * @bitmap_len: bitmap length in bytes\n *\n * Description:\n * Checks the given category bitmap against the given DOI definition and\n * returns a negative value if any of the categories in the bitmap do not have\n * a valid mapping and a zero value if all of the categories are valid.\n *\n */\nstatic int cipso_v4_map_cat_rbm_valid(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const unsigned char *bitmap,\n\t\t\t\t      u32 bitmap_len)\n{\n\tint cat = -1;\n\tu32 bitmap_len_bits = bitmap_len * 8;\n\tu32 cipso_cat_size;\n\tu32 *cipso_array;\n\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tcipso_cat_size = doi_def->map.std->cat.cipso_size;\n\t\tcipso_array = doi_def->map.std->cat.cipso;\n\t\tfor (;;) {\n\t\t\tcat = cipso_v4_bitmap_walk(bitmap,\n\t\t\t\t\t\t   bitmap_len_bits,\n\t\t\t\t\t\t   cat + 1,\n\t\t\t\t\t\t   1);\n\t\t\tif (cat < 0)\n\t\t\t\tbreak;\n\t\t\tif (cat >= cipso_cat_size ||\n\t\t\t    cipso_array[cat] >= CIPSO_V4_INV_CAT)\n\t\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (cat == -1)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -EFAULT;\n}\n\n/**\n * cipso_v4_map_cat_rbm_hton - Perform a category mapping from host to network\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @net_cat: the zero'd out category bitmap in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n *\n * Description:\n * Perform a label mapping to translate a local MLS category bitmap to the\n * correct CIPSO bitmap using the given DOI definition.  Returns the minimum\n * size in bytes of the network bitmap on success, negative values otherwise.\n *\n */\nstatic int cipso_v4_map_cat_rbm_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const struct netlbl_lsm_secattr *secattr,\n\t\t\t\t     unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len)\n{\n\tint host_spot = -1;\n\tu32 net_spot = CIPSO_V4_INV_CAT;\n\tu32 net_spot_max = 0;\n\tu32 net_clen_bits = net_cat_len * 8;\n\tu32 host_cat_size = 0;\n\tu32 *host_cat_array = NULL;\n\n\tif (doi_def->type == CIPSO_V4_MAP_TRANS) {\n\t\thost_cat_size = doi_def->map.std->cat.local_size;\n\t\thost_cat_array = doi_def->map.std->cat.local;\n\t}\n\n\tfor (;;) {\n\t\thost_spot = netlbl_secattr_catmap_walk(secattr->attr.mls.cat,\n\t\t\t\t\t\t       host_spot + 1);\n\t\tif (host_spot < 0)\n\t\t\tbreak;\n\n\t\tswitch (doi_def->type) {\n\t\tcase CIPSO_V4_MAP_PASS:\n\t\t\tnet_spot = host_spot;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_TRANS:\n\t\t\tif (host_spot >= host_cat_size)\n\t\t\t\treturn -EPERM;\n\t\t\tnet_spot = host_cat_array[host_spot];\n\t\t\tif (net_spot >= CIPSO_V4_INV_CAT)\n\t\t\t\treturn -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (net_spot >= net_clen_bits)\n\t\t\treturn -ENOSPC;\n\t\tcipso_v4_bitmap_setbit(net_cat, net_spot, 1);\n\n\t\tif (net_spot > net_spot_max)\n\t\t\tnet_spot_max = net_spot;\n\t}\n\n\tif (++net_spot_max % 8)\n\t\treturn net_spot_max / 8 + 1;\n\treturn net_spot_max / 8;\n}\n\n/**\n * cipso_v4_map_cat_rbm_ntoh - Perform a category mapping from network to host\n * @doi_def: the DOI definition\n * @net_cat: the category bitmap in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n * @secattr: the security attributes\n *\n * Description:\n * Perform a label mapping to translate a CIPSO bitmap to the correct local\n * MLS category bitmap using the given DOI definition.  Returns zero on\n * success, negative values on failure.\n *\n */\nstatic int cipso_v4_map_cat_rbm_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len,\n\t\t\t\t     struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tint net_spot = -1;\n\tu32 host_spot = CIPSO_V4_INV_CAT;\n\tu32 net_clen_bits = net_cat_len * 8;\n\tu32 net_cat_size = 0;\n\tu32 *net_cat_array = NULL;\n\n\tif (doi_def->type == CIPSO_V4_MAP_TRANS) {\n\t\tnet_cat_size = doi_def->map.std->cat.cipso_size;\n\t\tnet_cat_array = doi_def->map.std->cat.cipso;\n\t}\n\n\tfor (;;) {\n\t\tnet_spot = cipso_v4_bitmap_walk(net_cat,\n\t\t\t\t\t\tnet_clen_bits,\n\t\t\t\t\t\tnet_spot + 1,\n\t\t\t\t\t\t1);\n\t\tif (net_spot < 0) {\n\t\t\tif (net_spot == -2)\n\t\t\t\treturn -EFAULT;\n\t\t\treturn 0;\n\t\t}\n\n\t\tswitch (doi_def->type) {\n\t\tcase CIPSO_V4_MAP_PASS:\n\t\t\thost_spot = net_spot;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_TRANS:\n\t\t\tif (net_spot >= net_cat_size)\n\t\t\t\treturn -EPERM;\n\t\t\thost_spot = net_cat_array[net_spot];\n\t\t\tif (host_spot >= CIPSO_V4_INV_CAT)\n\t\t\t\treturn -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tret_val = netlbl_secattr_catmap_setbit(secattr->attr.mls.cat,\n\t\t\t\t\t\t       host_spot,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\tif (ret_val != 0)\n\t\t\treturn ret_val;\n\t}\n\n\treturn -EINVAL;\n}\n\n/**\n * cipso_v4_map_cat_enum_valid - Checks to see if the categories are valid\n * @doi_def: the DOI definition\n * @enumcat: category list\n * @enumcat_len: length of the category list in bytes\n *\n * Description:\n * Checks the given categories against the given DOI definition and returns a\n * negative value if any of the categories do not have a valid mapping and a\n * zero value if all of the categories are valid.\n *\n */\nstatic int cipso_v4_map_cat_enum_valid(const struct cipso_v4_doi *doi_def,\n\t\t\t\t       const unsigned char *enumcat,\n\t\t\t\t       u32 enumcat_len)\n{\n\tu16 cat;\n\tint cat_prev = -1;\n\tu32 iter;\n\n\tif (doi_def->type != CIPSO_V4_MAP_PASS || enumcat_len & 0x01)\n\t\treturn -EFAULT;\n\n\tfor (iter = 0; iter < enumcat_len; iter += 2) {\n\t\tcat = get_unaligned_be16(&enumcat[iter]);\n\t\tif (cat <= cat_prev)\n\t\t\treturn -EFAULT;\n\t\tcat_prev = cat;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_map_cat_enum_hton - Perform a category mapping from host to network\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @net_cat: the zero'd out category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO category list in bytes\n *\n * Description:\n * Perform a label mapping to translate a local MLS category bitmap to the\n * correct CIPSO category list using the given DOI definition.   Returns the\n * size in bytes of the network category bitmap on success, negative values\n * otherwise.\n *\n */\nstatic int cipso_v4_map_cat_enum_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const struct netlbl_lsm_secattr *secattr,\n\t\t\t\t      unsigned char *net_cat,\n\t\t\t\t      u32 net_cat_len)\n{\n\tint cat = -1;\n\tu32 cat_iter = 0;\n\n\tfor (;;) {\n\t\tcat = netlbl_secattr_catmap_walk(secattr->attr.mls.cat,\n\t\t\t\t\t\t cat + 1);\n\t\tif (cat < 0)\n\t\t\tbreak;\n\t\tif ((cat_iter + 2) > net_cat_len)\n\t\t\treturn -ENOSPC;\n\n\t\t*((__be16 *)&net_cat[cat_iter]) = htons(cat);\n\t\tcat_iter += 2;\n\t}\n\n\treturn cat_iter;\n}\n\n/**\n * cipso_v4_map_cat_enum_ntoh - Perform a category mapping from network to host\n * @doi_def: the DOI definition\n * @net_cat: the category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n * @secattr: the security attributes\n *\n * Description:\n * Perform a label mapping to translate a CIPSO category list to the correct\n * local MLS category bitmap using the given DOI definition.  Returns zero on\n * success, negative values on failure.\n *\n */\nstatic int cipso_v4_map_cat_enum_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const unsigned char *net_cat,\n\t\t\t\t      u32 net_cat_len,\n\t\t\t\t      struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu32 iter;\n\n\tfor (iter = 0; iter < net_cat_len; iter += 2) {\n\t\tret_val = netlbl_secattr_catmap_setbit(secattr->attr.mls.cat,\n\t\t\t\tget_unaligned_be16(&net_cat[iter]),\n\t\t\t\tGFP_ATOMIC);\n\t\tif (ret_val != 0)\n\t\t\treturn ret_val;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_map_cat_rng_valid - Checks to see if the categories are valid\n * @doi_def: the DOI definition\n * @rngcat: category list\n * @rngcat_len: length of the category list in bytes\n *\n * Description:\n * Checks the given categories against the given DOI definition and returns a\n * negative value if any of the categories do not have a valid mapping and a\n * zero value if all of the categories are valid.\n *\n */\nstatic int cipso_v4_map_cat_rng_valid(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const unsigned char *rngcat,\n\t\t\t\t      u32 rngcat_len)\n{\n\tu16 cat_high;\n\tu16 cat_low;\n\tu32 cat_prev = CIPSO_V4_MAX_REM_CATS + 1;\n\tu32 iter;\n\n\tif (doi_def->type != CIPSO_V4_MAP_PASS || rngcat_len & 0x01)\n\t\treturn -EFAULT;\n\n\tfor (iter = 0; iter < rngcat_len; iter += 4) {\n\t\tcat_high = get_unaligned_be16(&rngcat[iter]);\n\t\tif ((iter + 4) <= rngcat_len)\n\t\t\tcat_low = get_unaligned_be16(&rngcat[iter + 2]);\n\t\telse\n\t\t\tcat_low = 0;\n\n\t\tif (cat_high > cat_prev)\n\t\t\treturn -EFAULT;\n\n\t\tcat_prev = cat_low;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_map_cat_rng_hton - Perform a category mapping from host to network\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @net_cat: the zero'd out category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO category list in bytes\n *\n * Description:\n * Perform a label mapping to translate a local MLS category bitmap to the\n * correct CIPSO category list using the given DOI definition.   Returns the\n * size in bytes of the network category bitmap on success, negative values\n * otherwise.\n *\n */\nstatic int cipso_v4_map_cat_rng_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const struct netlbl_lsm_secattr *secattr,\n\t\t\t\t     unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len)\n{\n\tint iter = -1;\n\tu16 array[CIPSO_V4_TAG_RNG_CAT_MAX * 2];\n\tu32 array_cnt = 0;\n\tu32 cat_size = 0;\n\n\t/* make sure we don't overflow the 'array[]' variable */\n\tif (net_cat_len >\n\t    (CIPSO_V4_OPT_LEN_MAX - CIPSO_V4_HDR_LEN - CIPSO_V4_TAG_RNG_BLEN))\n\t\treturn -ENOSPC;\n\n\tfor (;;) {\n\t\titer = netlbl_secattr_catmap_walk(secattr->attr.mls.cat,\n\t\t\t\t\t\t  iter + 1);\n\t\tif (iter < 0)\n\t\t\tbreak;\n\t\tcat_size += (iter == 0 ? 0 : sizeof(u16));\n\t\tif (cat_size > net_cat_len)\n\t\t\treturn -ENOSPC;\n\t\tarray[array_cnt++] = iter;\n\n\t\titer = netlbl_secattr_catmap_walk_rng(secattr->attr.mls.cat,\n\t\t\t\t\t\t      iter);\n\t\tif (iter < 0)\n\t\t\treturn -EFAULT;\n\t\tcat_size += sizeof(u16);\n\t\tif (cat_size > net_cat_len)\n\t\t\treturn -ENOSPC;\n\t\tarray[array_cnt++] = iter;\n\t}\n\n\tfor (iter = 0; array_cnt > 0;) {\n\t\t*((__be16 *)&net_cat[iter]) = htons(array[--array_cnt]);\n\t\titer += 2;\n\t\tarray_cnt--;\n\t\tif (array[array_cnt] != 0) {\n\t\t\t*((__be16 *)&net_cat[iter]) = htons(array[array_cnt]);\n\t\t\titer += 2;\n\t\t}\n\t}\n\n\treturn cat_size;\n}\n\n/**\n * cipso_v4_map_cat_rng_ntoh - Perform a category mapping from network to host\n * @doi_def: the DOI definition\n * @net_cat: the category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n * @secattr: the security attributes\n *\n * Description:\n * Perform a label mapping to translate a CIPSO category list to the correct\n * local MLS category bitmap using the given DOI definition.  Returns zero on\n * success, negative values on failure.\n *\n */\nstatic int cipso_v4_map_cat_rng_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len,\n\t\t\t\t     struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu32 net_iter;\n\tu16 cat_low;\n\tu16 cat_high;\n\n\tfor (net_iter = 0; net_iter < net_cat_len; net_iter += 4) {\n\t\tcat_high = get_unaligned_be16(&net_cat[net_iter]);\n\t\tif ((net_iter + 4) <= net_cat_len)\n\t\t\tcat_low = get_unaligned_be16(&net_cat[net_iter + 2]);\n\t\telse\n\t\t\tcat_low = 0;\n\n\t\tret_val = netlbl_secattr_catmap_setrng(secattr->attr.mls.cat,\n\t\t\t\t\t\t       cat_low,\n\t\t\t\t\t\t       cat_high,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\tif (ret_val != 0)\n\t\t\treturn ret_val;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Protocol Handling Functions\n */\n\n/**\n * cipso_v4_gentag_hdr - Generate a CIPSO option header\n * @doi_def: the DOI definition\n * @len: the total tag length in bytes, not including this header\n * @buf: the CIPSO option buffer\n *\n * Description:\n * Write a CIPSO header into the beginning of @buffer.\n *\n */\nstatic void cipso_v4_gentag_hdr(const struct cipso_v4_doi *doi_def,\n\t\t\t\tunsigned char *buf,\n\t\t\t\tu32 len)\n{\n\tbuf[0] = IPOPT_CIPSO;\n\tbuf[1] = CIPSO_V4_HDR_LEN + len;\n\t*(__be32 *)&buf[2] = htonl(doi_def->doi);\n}\n\n/**\n * cipso_v4_gentag_rbm - Generate a CIPSO restricted bitmap tag (type #1)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the restricted bitmap tag, tag type #1.  The\n * actual buffer length may be larger than the indicated size due to\n * translation between host and network category bitmaps.  Returns the size of\n * the tag on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t       const struct netlbl_lsm_secattr *secattr,\n\t\t\t       unsigned char *buffer,\n\t\t\t       u32 buffer_len)\n{\n\tint ret_val;\n\tu32 tag_len;\n\tu32 level;\n\n\tif ((secattr->flags & NETLBL_SECATTR_MLS_LVL) == 0)\n\t\treturn -EPERM;\n\n\tret_val = cipso_v4_map_lvl_hton(doi_def,\n\t\t\t\t\tsecattr->attr.mls.lvl,\n\t\t\t\t\t&level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\n\tif (secattr->flags & NETLBL_SECATTR_MLS_CAT) {\n\t\tret_val = cipso_v4_map_cat_rbm_hton(doi_def,\n\t\t\t\t\t\t    secattr,\n\t\t\t\t\t\t    &buffer[4],\n\t\t\t\t\t\t    buffer_len - 4);\n\t\tif (ret_val < 0)\n\t\t\treturn ret_val;\n\n\t\t/* This will send packets using the \"optimized\" format when\n\t\t * possible as specified in  section 3.4.2.6 of the\n\t\t * CIPSO draft. */\n\t\tif (cipso_v4_rbm_optfmt && ret_val > 0 && ret_val <= 10)\n\t\t\ttag_len = 14;\n\t\telse\n\t\t\ttag_len = 4 + ret_val;\n\t} else\n\t\ttag_len = 4;\n\n\tbuffer[0] = CIPSO_V4_TAG_RBITMAP;\n\tbuffer[1] = tag_len;\n\tbuffer[3] = level;\n\n\treturn tag_len;\n}\n\n/**\n * cipso_v4_parsetag_rbm - Parse a CIPSO restricted bitmap tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO restricted bitmap tag (tag type #1) and return the security\n * attributes in @secattr.  Return zero on success, negatives values on\n * failure.\n *\n */\nstatic int cipso_v4_parsetag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tsecattr->attr.mls.cat =\n\t\t                       netlbl_secattr_catmap_alloc(GFP_ATOMIC);\n\t\tif (secattr->attr.mls.cat == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret_val = cipso_v4_map_cat_rbm_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_secattr_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_gentag_enum - Generate a CIPSO enumerated tag (type #2)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the enumerated tag, tag type #2.  Returns the\n * size of the tag on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_enum(const struct cipso_v4_doi *doi_def,\n\t\t\t\tconst struct netlbl_lsm_secattr *secattr,\n\t\t\t\tunsigned char *buffer,\n\t\t\t\tu32 buffer_len)\n{\n\tint ret_val;\n\tu32 tag_len;\n\tu32 level;\n\n\tif (!(secattr->flags & NETLBL_SECATTR_MLS_LVL))\n\t\treturn -EPERM;\n\n\tret_val = cipso_v4_map_lvl_hton(doi_def,\n\t\t\t\t\tsecattr->attr.mls.lvl,\n\t\t\t\t\t&level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\n\tif (secattr->flags & NETLBL_SECATTR_MLS_CAT) {\n\t\tret_val = cipso_v4_map_cat_enum_hton(doi_def,\n\t\t\t\t\t\t     secattr,\n\t\t\t\t\t\t     &buffer[4],\n\t\t\t\t\t\t     buffer_len - 4);\n\t\tif (ret_val < 0)\n\t\t\treturn ret_val;\n\n\t\ttag_len = 4 + ret_val;\n\t} else\n\t\ttag_len = 4;\n\n\tbuffer[0] = CIPSO_V4_TAG_ENUM;\n\tbuffer[1] = tag_len;\n\tbuffer[3] = level;\n\n\treturn tag_len;\n}\n\n/**\n * cipso_v4_parsetag_enum - Parse a CIPSO enumerated tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO enumerated tag (tag type #2) and return the security\n * attributes in @secattr.  Return zero on success, negatives values on\n * failure.\n *\n */\nstatic int cipso_v4_parsetag_enum(const struct cipso_v4_doi *doi_def,\n\t\t\t\t  const unsigned char *tag,\n\t\t\t\t  struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tsecattr->attr.mls.cat =\n\t\t\t               netlbl_secattr_catmap_alloc(GFP_ATOMIC);\n\t\tif (secattr->attr.mls.cat == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret_val = cipso_v4_map_cat_enum_ntoh(doi_def,\n\t\t\t\t\t\t     &tag[4],\n\t\t\t\t\t\t     tag_len - 4,\n\t\t\t\t\t\t     secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_secattr_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_gentag_rng - Generate a CIPSO ranged tag (type #5)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the ranged tag, tag type #5.  Returns the\n * size of the tag on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t       const struct netlbl_lsm_secattr *secattr,\n\t\t\t       unsigned char *buffer,\n\t\t\t       u32 buffer_len)\n{\n\tint ret_val;\n\tu32 tag_len;\n\tu32 level;\n\n\tif (!(secattr->flags & NETLBL_SECATTR_MLS_LVL))\n\t\treturn -EPERM;\n\n\tret_val = cipso_v4_map_lvl_hton(doi_def,\n\t\t\t\t\tsecattr->attr.mls.lvl,\n\t\t\t\t\t&level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\n\tif (secattr->flags & NETLBL_SECATTR_MLS_CAT) {\n\t\tret_val = cipso_v4_map_cat_rng_hton(doi_def,\n\t\t\t\t\t\t    secattr,\n\t\t\t\t\t\t    &buffer[4],\n\t\t\t\t\t\t    buffer_len - 4);\n\t\tif (ret_val < 0)\n\t\t\treturn ret_val;\n\n\t\ttag_len = 4 + ret_val;\n\t} else\n\t\ttag_len = 4;\n\n\tbuffer[0] = CIPSO_V4_TAG_RANGE;\n\tbuffer[1] = tag_len;\n\tbuffer[3] = level;\n\n\treturn tag_len;\n}\n\n/**\n * cipso_v4_parsetag_rng - Parse a CIPSO ranged tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO ranged tag (tag type #5) and return the security attributes\n * in @secattr.  Return zero on success, negatives values on failure.\n *\n */\nstatic int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tsecattr->attr.mls.cat =\n\t\t\t               netlbl_secattr_catmap_alloc(GFP_ATOMIC);\n\t\tif (secattr->attr.mls.cat == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_secattr_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_gentag_loc - Generate a CIPSO local tag (non-standard)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the local tag.  Returns the size of the tag\n * on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_loc(const struct cipso_v4_doi *doi_def,\n\t\t\t       const struct netlbl_lsm_secattr *secattr,\n\t\t\t       unsigned char *buffer,\n\t\t\t       u32 buffer_len)\n{\n\tif (!(secattr->flags & NETLBL_SECATTR_SECID))\n\t\treturn -EPERM;\n\n\tbuffer[0] = CIPSO_V4_TAG_LOCAL;\n\tbuffer[1] = CIPSO_V4_TAG_LOC_BLEN;\n\t*(u32 *)&buffer[2] = secattr->attr.secid;\n\n\treturn CIPSO_V4_TAG_LOC_BLEN;\n}\n\n/**\n * cipso_v4_parsetag_loc - Parse a CIPSO local tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO local tag and return the security attributes in @secattr.\n * Return zero on success, negatives values on failure.\n *\n */\nstatic int cipso_v4_parsetag_loc(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tsecattr->attr.secid = *(u32 *)&tag[2];\n\tsecattr->flags |= NETLBL_SECATTR_SECID;\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_validate - Validate a CIPSO option\n * @option: the start of the option, on error it is set to point to the error\n *\n * Description:\n * This routine is called to validate a CIPSO option, it checks all of the\n * fields to ensure that they are at least valid, see the draft snippet below\n * for details.  If the option is valid then a zero value is returned and\n * the value of @option is unchanged.  If the option is invalid then a\n * non-zero value is returned and @option is adjusted to point to the\n * offending portion of the option.  From the IETF draft ...\n *\n *  \"If any field within the CIPSO options, such as the DOI identifier, is not\n *   recognized the IP datagram is discarded and an ICMP 'parameter problem'\n *   (type 12) is generated and returned.  The ICMP code field is set to 'bad\n *   parameter' (code 0) and the pointer is set to the start of the CIPSO field\n *   that is unrecognized.\"\n *\n */\nint cipso_v4_validate(const struct sk_buff *skb, unsigned char **option)\n{\n\tunsigned char *opt = *option;\n\tunsigned char *tag;\n\tunsigned char opt_iter;\n\tunsigned char err_offset = 0;\n\tu8 opt_len;\n\tu8 tag_len;\n\tstruct cipso_v4_doi *doi_def = NULL;\n\tu32 tag_iter;\n\n\t/* caller already checks for length values that are too large */\n\topt_len = opt[1];\n\tif (opt_len < 8) {\n\t\terr_offset = 1;\n\t\tgoto validate_return;\n\t}\n\n\trcu_read_lock();\n\tdoi_def = cipso_v4_doi_search(get_unaligned_be32(&opt[2]));\n\tif (doi_def == NULL) {\n\t\terr_offset = 2;\n\t\tgoto validate_return_locked;\n\t}\n\n\topt_iter = CIPSO_V4_HDR_LEN;\n\ttag = opt + opt_iter;\n\twhile (opt_iter < opt_len) {\n\t\tfor (tag_iter = 0; doi_def->tags[tag_iter] != tag[0];)\n\t\t\tif (doi_def->tags[tag_iter] == CIPSO_V4_TAG_INVALID ||\n\t\t\t    ++tag_iter == CIPSO_V4_TAG_MAXCNT) {\n\t\t\t\terr_offset = opt_iter;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\ttag_len = tag[1];\n\t\tif (tag_len > (opt_len - opt_iter)) {\n\t\t\terr_offset = opt_iter + 1;\n\t\t\tgoto validate_return_locked;\n\t\t}\n\n\t\tswitch (tag[0]) {\n\t\tcase CIPSO_V4_TAG_RBITMAP:\n\t\t\tif (tag_len < CIPSO_V4_TAG_RBM_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\t\t/* We are already going to do all the verification\n\t\t\t * necessary at the socket layer so from our point of\n\t\t\t * view it is safe to turn these checks off (and less\n\t\t\t * work), however, the CIPSO draft says we should do\n\t\t\t * all the CIPSO validations here but it doesn't\n\t\t\t * really specify _exactly_ what we need to validate\n\t\t\t * ... so, just make it a sysctl tunable. */\n\t\t\tif (cipso_v4_rbm_strictvalid) {\n\t\t\t\tif (cipso_v4_map_lvl_valid(doi_def,\n\t\t\t\t\t\t\t   tag[3]) < 0) {\n\t\t\t\t\terr_offset = opt_iter + 3;\n\t\t\t\t\tgoto validate_return_locked;\n\t\t\t\t}\n\t\t\t\tif (tag_len > CIPSO_V4_TAG_RBM_BLEN &&\n\t\t\t\t    cipso_v4_map_cat_rbm_valid(doi_def,\n\t\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t\t    tag_len - 4) < 0) {\n\t\t\t\t\terr_offset = opt_iter + 4;\n\t\t\t\t\tgoto validate_return_locked;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_ENUM:\n\t\t\tif (tag_len < CIPSO_V4_TAG_ENUM_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\t\tif (cipso_v4_map_lvl_valid(doi_def,\n\t\t\t\t\t\t   tag[3]) < 0) {\n\t\t\t\terr_offset = opt_iter + 3;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tif (tag_len > CIPSO_V4_TAG_ENUM_BLEN &&\n\t\t\t    cipso_v4_map_cat_enum_valid(doi_def,\n\t\t\t\t\t\t\t&tag[4],\n\t\t\t\t\t\t\ttag_len - 4) < 0) {\n\t\t\t\terr_offset = opt_iter + 4;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_RANGE:\n\t\t\tif (tag_len < CIPSO_V4_TAG_RNG_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\t\tif (cipso_v4_map_lvl_valid(doi_def,\n\t\t\t\t\t\t   tag[3]) < 0) {\n\t\t\t\terr_offset = opt_iter + 3;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tif (tag_len > CIPSO_V4_TAG_RNG_BLEN &&\n\t\t\t    cipso_v4_map_cat_rng_valid(doi_def,\n\t\t\t\t\t\t       &tag[4],\n\t\t\t\t\t\t       tag_len - 4) < 0) {\n\t\t\t\terr_offset = opt_iter + 4;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_LOCAL:\n\t\t\t/* This is a non-standard tag that we only allow for\n\t\t\t * local connections, so if the incoming interface is\n\t\t\t * not the loopback device drop the packet. */\n\t\t\tif (!(skb->dev->flags & IFF_LOOPBACK)) {\n\t\t\t\terr_offset = opt_iter;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tif (tag_len != CIPSO_V4_TAG_LOC_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr_offset = opt_iter;\n\t\t\tgoto validate_return_locked;\n\t\t}\n\n\t\ttag += tag_len;\n\t\topt_iter += tag_len;\n\t}\n\nvalidate_return_locked:\n\trcu_read_unlock();\nvalidate_return:\n\t*option = opt + err_offset;\n\treturn err_offset;\n}\n\n/**\n * cipso_v4_error - Send the correct response for a bad packet\n * @skb: the packet\n * @error: the error code\n * @gateway: CIPSO gateway flag\n *\n * Description:\n * Based on the error code given in @error, send an ICMP error message back to\n * the originating host.  From the IETF draft ...\n *\n *  \"If the contents of the CIPSO [option] are valid but the security label is\n *   outside of the configured host or port label range, the datagram is\n *   discarded and an ICMP 'destination unreachable' (type 3) is generated and\n *   returned.  The code field of the ICMP is set to 'communication with\n *   destination network administratively prohibited' (code 9) or to\n *   'communication with destination host administratively prohibited'\n *   (code 10).  The value of the code is dependent on whether the originator\n *   of the ICMP message is acting as a CIPSO host or a CIPSO gateway.  The\n *   recipient of the ICMP message MUST be able to handle either value.  The\n *   same procedure is performed if a CIPSO [option] can not be added to an\n *   IP packet because it is too large to fit in the IP options area.\"\n *\n *  \"If the error is triggered by receipt of an ICMP message, the message is\n *   discarded and no response is permitted (consistent with general ICMP\n *   processing rules).\"\n *\n */\nvoid cipso_v4_error(struct sk_buff *skb, int error, u32 gateway)\n{\n\tif (ip_hdr(skb)->protocol == IPPROTO_ICMP || error != -EACCES)\n\t\treturn;\n\n\tif (gateway)\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_NET_ANO, 0);\n\telse\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_HOST_ANO, 0);\n}\n\n/**\n * cipso_v4_genopt - Generate a CIPSO option\n * @buf: the option buffer\n * @buf_len: the size of opt_buf\n * @doi_def: the CIPSO DOI to use\n * @secattr: the security attributes\n *\n * Description:\n * Generate a CIPSO option using the DOI definition and security attributes\n * passed to the function.  Returns the length of the option on success and\n * negative values on failure.\n *\n */\nstatic int cipso_v4_genopt(unsigned char *buf, u32 buf_len,\n\t\t\t   const struct cipso_v4_doi *doi_def,\n\t\t\t   const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu32 iter;\n\n\tif (buf_len <= CIPSO_V4_HDR_LEN)\n\t\treturn -ENOSPC;\n\n\t/* XXX - This code assumes only one tag per CIPSO option which isn't\n\t * really a good assumption to make but since we only support the MAC\n\t * tags right now it is a safe assumption. */\n\titer = 0;\n\tdo {\n\t\tmemset(buf, 0, buf_len);\n\t\tswitch (doi_def->tags[iter]) {\n\t\tcase CIPSO_V4_TAG_RBITMAP:\n\t\t\tret_val = cipso_v4_gentag_rbm(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_ENUM:\n\t\t\tret_val = cipso_v4_gentag_enum(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_RANGE:\n\t\t\tret_val = cipso_v4_gentag_rng(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_LOCAL:\n\t\t\tret_val = cipso_v4_gentag_loc(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\n\t\titer++;\n\t} while (ret_val < 0 &&\n\t\t iter < CIPSO_V4_TAG_MAXCNT &&\n\t\t doi_def->tags[iter] != CIPSO_V4_TAG_INVALID);\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\tcipso_v4_gentag_hdr(doi_def, buf, ret_val);\n\treturn CIPSO_V4_HDR_LEN + ret_val;\n}\n\n/**\n * cipso_v4_sock_setattr - Add a CIPSO option to a socket\n * @sk: the socket\n * @doi_def: the CIPSO DOI to use\n * @secattr: the specific security attributes of the socket\n *\n * Description:\n * Set the CIPSO option on the given socket using the DOI definition and\n * security attributes passed to the function.  This function requires\n * exclusive access to @sk, which means it either needs to be in the\n * process of being created or locked.  Returns zero on success and negative\n * values on failure.\n *\n */\nint cipso_v4_sock_setattr(struct sock *sk,\n\t\t\t  const struct cipso_v4_doi *doi_def,\n\t\t\t  const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_sock *sk_inet;\n\tstruct inet_connection_sock *sk_conn;\n\n\t/* In the case of sock_create_lite(), the sock->sk field is not\n\t * defined yet but it is not a problem as the only users of these\n\t * \"lite\" PF_INET sockets are functions which do an accept() call\n\t * afterwards so we will label the socket as part of the accept(). */\n\tif (sk == NULL)\n\t\treturn 0;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto socket_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\tsk_inet = inet_sk(sk);\n\tif (sk_inet->is_icsk) {\n\t\tsk_conn = inet_csk(sk);\n\t\tif (sk_inet->opt)\n\t\t\tsk_conn->icsk_ext_hdr_len -= sk_inet->opt->optlen;\n\t\tsk_conn->icsk_ext_hdr_len += opt->optlen;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n\topt = xchg(&sk_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nsocket_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_req_setattr - Add a CIPSO option to a connection request socket\n * @req: the connection request socket\n * @doi_def: the CIPSO DOI to use\n * @secattr: the specific security attributes of the socket\n *\n * Description:\n * Set the CIPSO option on the given socket using the DOI definition and\n * security attributes passed to the function.  Returns zero on success and\n * negative values on failure.\n *\n */\nint cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_delopt - Delete the CIPSO option from a set of IP options\n * @opt_ptr: IP option pointer\n *\n * Description:\n * Deletes the CIPSO IP option from a set of IP options and makes the necessary\n * adjustments to the IP option structure.  Returns zero on success, negative\n * values on failure.\n *\n */\nstatic int cipso_v4_delopt(struct ip_options **opt_ptr)\n{\n\tint hdr_delta = 0;\n\tstruct ip_options *opt = *opt_ptr;\n\n\tif (opt->srr || opt->rr || opt->ts || opt->router_alert) {\n\t\tu8 cipso_len;\n\t\tu8 cipso_off;\n\t\tunsigned char *cipso_ptr;\n\t\tint iter;\n\t\tint optlen_new;\n\n\t\tcipso_off = opt->cipso - sizeof(struct iphdr);\n\t\tcipso_ptr = &opt->__data[cipso_off];\n\t\tcipso_len = cipso_ptr[1];\n\n\t\tif (opt->srr > opt->cipso)\n\t\t\topt->srr -= cipso_len;\n\t\tif (opt->rr > opt->cipso)\n\t\t\topt->rr -= cipso_len;\n\t\tif (opt->ts > opt->cipso)\n\t\t\topt->ts -= cipso_len;\n\t\tif (opt->router_alert > opt->cipso)\n\t\t\topt->router_alert -= cipso_len;\n\t\topt->cipso = 0;\n\n\t\tmemmove(cipso_ptr, cipso_ptr + cipso_len,\n\t\t\topt->optlen - cipso_off - cipso_len);\n\n\t\t/* determining the new total option length is tricky because of\n\t\t * the padding necessary, the only thing i can think to do at\n\t\t * this point is walk the options one-by-one, skipping the\n\t\t * padding at the end to determine the actual option size and\n\t\t * from there we can determine the new total option length */\n\t\titer = 0;\n\t\toptlen_new = 0;\n\t\twhile (iter < opt->optlen)\n\t\t\tif (opt->__data[iter] != IPOPT_NOP) {\n\t\t\t\titer += opt->__data[iter + 1];\n\t\t\t\toptlen_new = iter;\n\t\t\t} else\n\t\t\t\titer++;\n\t\thdr_delta = opt->optlen;\n\t\topt->optlen = (optlen_new + 3) & ~3;\n\t\thdr_delta -= opt->optlen;\n\t} else {\n\t\t/* only the cipso option was present on the socket so we can\n\t\t * remove the entire option struct */\n\t\t*opt_ptr = NULL;\n\t\thdr_delta = opt->optlen;\n\t\tkfree(opt);\n\t}\n\n\treturn hdr_delta;\n}\n\n/**\n * cipso_v4_sock_delattr - Delete the CIPSO option from a socket\n * @sk: the socket\n *\n * Description:\n * Removes the CIPSO option from a socket, if present.\n *\n */\nvoid cipso_v4_sock_delattr(struct sock *sk)\n{\n\tint hdr_delta;\n\tstruct ip_options *opt;\n\tstruct inet_sock *sk_inet;\n\n\tsk_inet = inet_sk(sk);\n\topt = sk_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\thdr_delta = cipso_v4_delopt(&sk_inet->opt);\n\tif (sk_inet->is_icsk && hdr_delta > 0) {\n\t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n\t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n}\n\n/**\n * cipso_v4_req_delattr - Delete the CIPSO option from a request socket\n * @reg: the request socket\n *\n * Description:\n * Removes the CIPSO option from a request socket, if present.\n *\n */\nvoid cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}\n\n/**\n * cipso_v4_getattr - Helper function for the cipso_v4_*_getattr functions\n * @cipso: the CIPSO v4 option\n * @secattr: the security attributes\n *\n * Description:\n * Inspect @cipso and return the security attributes in @secattr.  Returns zero\n * on success and negative values on failure.\n *\n */\nstatic int cipso_v4_getattr(const unsigned char *cipso,\n\t\t\t    struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -ENOMSG;\n\tu32 doi;\n\tstruct cipso_v4_doi *doi_def;\n\n\tif (cipso_v4_cache_check(cipso, cipso[1], secattr) == 0)\n\t\treturn 0;\n\n\tdoi = get_unaligned_be32(&cipso[2]);\n\trcu_read_lock();\n\tdoi_def = cipso_v4_doi_search(doi);\n\tif (doi_def == NULL)\n\t\tgoto getattr_return;\n\t/* XXX - This code assumes only one tag per CIPSO option which isn't\n\t * really a good assumption to make but since we only support the MAC\n\t * tags right now it is a safe assumption. */\n\tswitch (cipso[6]) {\n\tcase CIPSO_V4_TAG_RBITMAP:\n\t\tret_val = cipso_v4_parsetag_rbm(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\tcase CIPSO_V4_TAG_ENUM:\n\t\tret_val = cipso_v4_parsetag_enum(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\tcase CIPSO_V4_TAG_RANGE:\n\t\tret_val = cipso_v4_parsetag_rng(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\tcase CIPSO_V4_TAG_LOCAL:\n\t\tret_val = cipso_v4_parsetag_loc(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\t}\n\tif (ret_val == 0)\n\t\tsecattr->type = NETLBL_NLTYPE_CIPSOV4;\n\ngetattr_return:\n\trcu_read_unlock();\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_sock_getattr - Get the security attributes from a sock\n * @sk: the sock\n * @secattr: the security attributes\n *\n * Description:\n * Query @sk to see if there is a CIPSO option attached to the sock and if\n * there is return the CIPSO security attributes in @secattr.  This function\n * requires that @sk be locked, or privately held, but it does not do any\n * locking itself.  Returns zero on success and negative values on failure.\n *\n */\nint cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n{\n\tstruct ip_options *opt;\n\n\topt = inet_sk(sk)->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn -ENOMSG;\n\n\treturn cipso_v4_getattr(opt->__data + opt->cipso - sizeof(struct iphdr),\n\t\t\t\tsecattr);\n}\n\n/**\n * cipso_v4_skbuff_setattr - Set the CIPSO option on a packet\n * @skb: the packet\n * @secattr: the security attributes\n *\n * Description:\n * Set the CIPSO option on the given packet based on the security attributes.\n * Returns a pointer to the IP header on success and NULL on failure.\n *\n */\nint cipso_v4_skbuff_setattr(struct sk_buff *skb,\n\t\t\t    const struct cipso_v4_doi *doi_def,\n\t\t\t    const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tstruct iphdr *iph;\n\tstruct ip_options *opt = &IPCB(skb)->opt;\n\tunsigned char buf[CIPSO_V4_OPT_LEN_MAX];\n\tu32 buf_len = CIPSO_V4_OPT_LEN_MAX;\n\tu32 opt_len;\n\tint len_delta;\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\tbuf_len = ret_val;\n\topt_len = (buf_len + 3) & ~3;\n\n\t/* we overwrite any existing options to ensure that we have enough\n\t * room for the CIPSO option, the reason is that we _need_ to guarantee\n\t * that the security label is applied to the packet - we do the same\n\t * thing when using the socket options and it hasn't caused a problem,\n\t * if we need to we can always revisit this choice later */\n\n\tlen_delta = opt_len - opt->optlen;\n\t/* if we don't ensure enough headroom we could panic on the skb_push()\n\t * call below so make sure we have enough, we are also \"mangling\" the\n\t * packet so we should probably do a copy-on-write call anyway */\n\tret_val = skb_cow(skb, skb_headroom(skb) + len_delta);\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\n\tif (len_delta > 0) {\n\t\t/* we assume that the header + opt->optlen have already been\n\t\t * \"pushed\" in ip_options_build() or similar */\n\t\tiph = ip_hdr(skb);\n\t\tskb_push(skb, len_delta);\n\t\tmemmove((char *)iph - len_delta, iph, iph->ihl << 2);\n\t\tskb_reset_network_header(skb);\n\t\tiph = ip_hdr(skb);\n\t} else if (len_delta < 0) {\n\t\tiph = ip_hdr(skb);\n\t\tmemset(iph + 1, IPOPT_NOP, opt->optlen);\n\t} else\n\t\tiph = ip_hdr(skb);\n\n\tif (opt->optlen > 0)\n\t\tmemset(opt, 0, sizeof(*opt));\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\topt->is_changed = 1;\n\n\t/* we have to do the following because we are being called from a\n\t * netfilter hook which means the packet already has had the header\n\t * fields populated and the checksum calculated - yes this means we\n\t * are doing more work than needed but we do it to keep the core\n\t * stack clean and tidy */\n\tmemcpy(iph + 1, buf, buf_len);\n\tif (opt_len > buf_len)\n\t\tmemset((char *)(iph + 1) + buf_len, 0, opt_len - buf_len);\n\tif (len_delta != 0) {\n\t\tiph->ihl = 5 + (opt_len >> 2);\n\t\tiph->tot_len = htons(skb->len);\n\t}\n\tip_send_check(iph);\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_skbuff_delattr - Delete any CIPSO options from a packet\n * @skb: the packet\n *\n * Description:\n * Removes any and all CIPSO options from the given packet.  Returns zero on\n * success, negative values on failure.\n *\n */\nint cipso_v4_skbuff_delattr(struct sk_buff *skb)\n{\n\tint ret_val;\n\tstruct iphdr *iph;\n\tstruct ip_options *opt = &IPCB(skb)->opt;\n\tunsigned char *cipso_ptr;\n\n\tif (opt->cipso == 0)\n\t\treturn 0;\n\n\t/* since we are changing the packet we should make a copy */\n\tret_val = skb_cow(skb, skb_headroom(skb));\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\n\t/* the easiest thing to do is just replace the cipso option with noop\n\t * options since we don't change the size of the packet, although we\n\t * still need to recalculate the checksum */\n\n\tiph = ip_hdr(skb);\n\tcipso_ptr = (unsigned char *)iph + opt->cipso;\n\tmemset(cipso_ptr, IPOPT_NOOP, cipso_ptr[1]);\n\topt->cipso = 0;\n\topt->is_changed = 1;\n\n\tip_send_check(iph);\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_skbuff_getattr - Get the security attributes from the CIPSO option\n * @skb: the packet\n * @secattr: the security attributes\n *\n * Description:\n * Parse the given packet's CIPSO option and return the security attributes.\n * Returns zero on success and negative values on failure.\n *\n */\nint cipso_v4_skbuff_getattr(const struct sk_buff *skb,\n\t\t\t    struct netlbl_lsm_secattr *secattr)\n{\n\treturn cipso_v4_getattr(CIPSO_V4_OPTPTR(skb), secattr);\n}\n\n/*\n * Setup Functions\n */\n\n/**\n * cipso_v4_init - Initialize the CIPSO module\n *\n * Description:\n * Initialize the CIPSO module and prepare it for use.  Returns zero on success\n * and negative values on failure.\n *\n */\nstatic int __init cipso_v4_init(void)\n{\n\tint ret_val;\n\n\tret_val = cipso_v4_cache_init();\n\tif (ret_val != 0)\n\t\tpanic(\"Failed to initialize the CIPSO/IPv4 cache (%d)\\n\",\n\t\t      ret_val);\n\n\treturn 0;\n}\n\nsubsys_initcall(cipso_v4_init);\n", "/*\n *\tNET3:\tImplementation of the ICMP protocol layer.\n *\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *\tSome of the function names and the icmp unreach table for this\n *\tmodule were derived from [icmp.c 1.0.11 06/02/93] by\n *\tRoss Biro, Fred N. van Kempen, Mark Evans, Alan Cox, Gerhard Koerting.\n *\tOther than that this module is a complete rewrite.\n *\n *\tFixes:\n *\tClemens Fruhwirth\t:\tintroduce global icmp rate limiting\n *\t\t\t\t\twith icmp type masking ability instead\n *\t\t\t\t\tof broken per type icmp timeouts.\n *\t\tMike Shaver\t:\tRFC1122 checks.\n *\t\tAlan Cox\t:\tMulticast ping reply as self.\n *\t\tAlan Cox\t:\tFix atomicity lockup in ip_build_xmit\n *\t\t\t\t\tcall.\n *\t\tAlan Cox\t:\tAdded 216,128 byte paths to the MTU\n *\t\t\t\t\tcode.\n *\t\tMartin Mares\t:\tRFC1812 checks.\n *\t\tMartin Mares\t:\tCan be configured to follow redirects\n *\t\t\t\t\tif acting as a router _without_ a\n *\t\t\t\t\trouting protocol (RFC 1812).\n *\t\tMartin Mares\t:\tEcho requests may be configured to\n *\t\t\t\t\tbe ignored (RFC 1812).\n *\t\tMartin Mares\t:\tLimitation of ICMP error message\n *\t\t\t\t\ttransmit rate (RFC 1812).\n *\t\tMartin Mares\t:\tTOS and Precedence set correctly\n *\t\t\t\t\t(RFC 1812).\n *\t\tMartin Mares\t:\tNow copying as much data from the\n *\t\t\t\t\toriginal packet as we can without\n *\t\t\t\t\texceeding 576 bytes (RFC 1812).\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tKeith Owens\t:\tRFC1191 correction for 4.2BSD based\n *\t\t\t\t\tpath MTU bug.\n *\t\tThomas Quinot\t:\tICMP Dest Unreach codes up to 15 are\n *\t\t\t\t\tvalid (RFC 1812).\n *\t\tAndi Kleen\t:\tCheck all packet lengths properly\n *\t\t\t\t\tand moved all kfree_skb() up to\n *\t\t\t\t\ticmp_rcv.\n *\t\tAndi Kleen\t:\tMove the rate limit bookkeeping\n *\t\t\t\t\tinto the dest entry and use a token\n *\t\t\t\t\tbucket filter (thanks to ANK). Make\n *\t\t\t\t\tthe rates sysctl configurable.\n *\t\tYu Tianli\t:\tFixed two ugly bugs in icmp_send\n *\t\t\t\t\t- IP option length was accounted wrongly\n *\t\t\t\t\t- ICMP header length was not accounted\n *\t\t\t\t\t  at all.\n *              Tristan Greaves :       Added sysctl option to ignore bogus\n *              \t\t\tbroadcast responses from broken routers.\n *\n * To Fix:\n *\n *\t- Should use skb_pull() instead of all the manual checking.\n *\t  This would also greatly simply some upper layer error handlers. --AK\n *\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <linux/string.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/slab.h>\n#include <net/snmp.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/protocol.h>\n#include <net/icmp.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/raw.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/init.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <net/inet_common.h>\n\n/*\n *\tBuild xmit assembly blocks\n */\n\nstruct icmp_bxm {\n\tstruct sk_buff *skb;\n\tint offset;\n\tint data_len;\n\n\tstruct {\n\t\tstruct icmphdr icmph;\n\t\t__be32\t       times[3];\n\t} data;\n\tint head_len;\n\tstruct ip_options replyopts;\n\tunsigned char  optbuf[40];\n};\n\n/* An array of errno for error messages from dest unreach. */\n/* RFC 1122: 3.2.2.1 States that NET_UNREACH, HOST_UNREACH and SR_FAILED MUST be considered 'transient errs'. */\n\nconst struct icmp_err icmp_err_convert[] = {\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_UNREACH */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_HOST_UNREACH */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = ENOPROTOOPT\t/* ICMP_PROT_UNREACH */,\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ECONNREFUSED,\t/* ICMP_PORT_UNREACH */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EMSGSIZE,\t/* ICMP_FRAG_NEEDED */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EOPNOTSUPP,\t/* ICMP_SR_FAILED */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_UNKNOWN */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTDOWN,\t/* ICMP_HOST_UNKNOWN */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ENONET,\t/* ICMP_HOST_ISOLATED */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_ANO\t*/\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_HOST_ANO */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_UNR_TOS */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_HOST_UNR_TOS */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_PKT_FILTERED */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_PREC_VIOLATION */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_PREC_CUTOFF */\n\t\t.fatal = 1,\n\t},\n};\nEXPORT_SYMBOL(icmp_err_convert);\n\n/*\n *\tICMP control array. This specifies what to do with each ICMP.\n */\n\nstruct icmp_control {\n\tvoid (*handler)(struct sk_buff *skb);\n\tshort   error;\t\t/* This ICMP is classed as an error message */\n};\n\nstatic const struct icmp_control icmp_pointers[NR_ICMP_TYPES+1];\n\n/*\n *\tThe ICMP socket(s). This is the most convenient way to flow control\n *\tour ICMP output as well as maintain a clean interface throughout\n *\tall layers. All Socketless IP sends will soon be gone.\n *\n *\tOn SMP we have one ICMP socket per-cpu.\n */\nstatic struct sock *icmp_sk(struct net *net)\n{\n\treturn net->ipv4.icmp_sk[smp_processor_id()];\n}\n\nstatic inline struct sock *icmp_xmit_lock(struct net *net)\n{\n\tstruct sock *sk;\n\n\tlocal_bh_disable();\n\n\tsk = icmp_sk(net);\n\n\tif (unlikely(!spin_trylock(&sk->sk_lock.slock))) {\n\t\t/* This can happen if the output path signals a\n\t\t * dst_link_failure() for an outgoing ICMP packet.\n\t\t */\n\t\tlocal_bh_enable();\n\t\treturn NULL;\n\t}\n\treturn sk;\n}\n\nstatic inline void icmp_xmit_unlock(struct sock *sk)\n{\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\n\n/*\n *\tSend an ICMP frame.\n */\n\nstatic inline bool icmpv4_xrlim_allow(struct net *net, struct rtable *rt,\n\t\tint type, int code)\n{\n\tstruct dst_entry *dst = &rt->dst;\n\tbool rc = true;\n\n\tif (type > NR_ICMP_TYPES)\n\t\tgoto out;\n\n\t/* Don't limit PMTU discovery. */\n\tif (type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED)\n\t\tgoto out;\n\n\t/* No rate limit on loopback */\n\tif (dst->dev && (dst->dev->flags&IFF_LOOPBACK))\n\t\tgoto out;\n\n\t/* Limit if icmp type is enabled in ratemask. */\n\tif ((1 << type) & net->ipv4.sysctl_icmp_ratemask) {\n\t\tif (!rt->peer)\n\t\t\trt_bind_peer(rt, 1);\n\t\trc = inet_peer_xrlim_allow(rt->peer,\n\t\t\t\t\t   net->ipv4.sysctl_icmp_ratelimit);\n\t}\nout:\n\treturn rc;\n}\n\n/*\n *\tMaintain the counters used in the SNMP statistics for outgoing ICMP\n */\nvoid icmp_out_count(struct net *net, unsigned char type)\n{\n\tICMPMSGOUT_INC_STATS(net, type);\n\tICMP_INC_STATS(net, ICMP_MIB_OUTMSGS);\n}\n\n/*\n *\tChecksum each fragment, and on the first include the headers and final\n *\tchecksum.\n */\nstatic int icmp_glue_bits(void *from, char *to, int offset, int len, int odd,\n\t\t\t  struct sk_buff *skb)\n{\n\tstruct icmp_bxm *icmp_param = (struct icmp_bxm *)from;\n\t__wsum csum;\n\n\tcsum = skb_copy_and_csum_bits(icmp_param->skb,\n\t\t\t\t      icmp_param->offset + offset,\n\t\t\t\t      to, len, 0);\n\n\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\tif (icmp_pointers[icmp_param->data.icmph.type].error)\n\t\tnf_ct_attach(skb, icmp_param->skb);\n\treturn 0;\n}\n\nstatic void icmp_push_reply(struct icmp_bxm *icmp_param,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rt)\n{\n\tstruct sock *sk;\n\tstruct sk_buff *skb;\n\n\tsk = icmp_sk(dev_net((*rt)->dst.dev));\n\tif (ip_append_data(sk, icmp_glue_bits, icmp_param,\n\t\t\t   icmp_param->data_len+icmp_param->head_len,\n\t\t\t   icmp_param->head_len,\n\t\t\t   ipc, rt, MSG_DONTWAIT) < 0) {\n\t\tICMP_INC_STATS_BH(sock_net(sk), ICMP_MIB_OUTERRORS);\n\t\tip_flush_pending_frames(sk);\n\t} else if ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {\n\t\tstruct icmphdr *icmph = icmp_hdr(skb);\n\t\t__wsum csum = 0;\n\t\tstruct sk_buff *skb1;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb1) {\n\t\t\tcsum = csum_add(csum, skb1->csum);\n\t\t}\n\t\tcsum = csum_partial_copy_nocheck((void *)&icmp_param->data,\n\t\t\t\t\t\t (char *)icmph,\n\t\t\t\t\t\t icmp_param->head_len, csum);\n\t\ticmph->checksum = csum_fold(csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk);\n\t}\n}\n\n/*\n *\tDriving logic for building and sending ICMP messages.\n */\n\nstatic void icmp_reply(struct icmp_bxm *icmp_param, struct sk_buff *skb)\n{\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\tstruct sock *sk;\n\tstruct inet_sock *inet;\n\t__be32 daddr;\n\n\tif (ip_options_echo(&icmp_param->replyopts, skb))\n\t\treturn;\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\tinet = inet_sk(sk);\n\n\ticmp_param->data.icmph.checksum = 0;\n\n\tinet->tos = ip_hdr(skb)->tos;\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tif (icmp_param->replyopts.optlen) {\n\t\tipc.opt = &icmp_param->replyopts;\n\t\tif (ipc.opt->srr)\n\t\t\tdaddr = icmp_param->replyopts.faddr;\n\t}\n\t{\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = daddr,\n\t\t\t.saddr = rt->rt_spec_dst,\n\t\t\t.flowi4_tos = RT_TOS(ip_hdr(skb)->tos),\n\t\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t};\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_unlock;\n\t}\n\tif (icmpv4_xrlim_allow(net, rt, icmp_param->data.icmph.type,\n\t\t\t       icmp_param->data.icmph.code))\n\t\ticmp_push_reply(icmp_param, &ipc, &rt);\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\n}\n\nstatic struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.srr ?\n\t\t\t  param->replyopts.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}\n\n/*\n *\tSend an ICMP message in response to a situation\n *\n *\tRFC 1122: 3.2.2\tMUST send at least the IP header and 8 bytes of header.\n *\t\t  MAY send more (we do).\n *\t\t\tMUST NOT change this header information.\n *\t\t\tMUST NOT reply to a multicast/broadcast IP address.\n *\t\t\tMUST NOT reply to a multicast/broadcast MAC address.\n *\t\t\tMUST reply to only the first fragment.\n */\n\nvoid icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info)\n{\n\tstruct iphdr *iph;\n\tint room;\n\tstruct icmp_bxm icmp_param;\n\tstruct rtable *rt = skb_rtable(skb_in);\n\tstruct ipcm_cookie ipc;\n\t__be32 saddr;\n\tu8  tos;\n\tstruct net *net;\n\tstruct sock *sk;\n\n\tif (!rt)\n\t\tgoto out;\n\tnet = dev_net(rt->dst.dev);\n\n\t/*\n\t *\tFind the original header. It is expected to be valid, of course.\n\t *\tCheck this, icmp_send is called from the most obscure devices\n\t *\tsometimes.\n\t */\n\tiph = ip_hdr(skb_in);\n\n\tif ((u8 *)iph < skb_in->head ||\n\t    (skb_in->network_header + sizeof(*iph)) > skb_in->tail)\n\t\tgoto out;\n\n\t/*\n\t *\tNo replies to physical multicast/broadcast\n\t */\n\tif (skb_in->pkt_type != PACKET_HOST)\n\t\tgoto out;\n\n\t/*\n\t *\tNow check at the protocol level\n\t */\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto out;\n\n\t/*\n\t *\tOnly reply to fragment 0. We byte re-order the constant\n\t *\tmask for efficiency.\n\t */\n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\tgoto out;\n\n\t/*\n\t *\tIf we send an ICMP error to an ICMP error a mess would result..\n\t */\n\tif (icmp_pointers[type].error) {\n\t\t/*\n\t\t *\tWe are an error, check if we are replying to an\n\t\t *\tICMP error\n\t\t */\n\t\tif (iph->protocol == IPPROTO_ICMP) {\n\t\t\tu8 _inner_type, *itp;\n\n\t\t\titp = skb_header_pointer(skb_in,\n\t\t\t\t\t\t skb_network_header(skb_in) +\n\t\t\t\t\t\t (iph->ihl << 2) +\n\t\t\t\t\t\t offsetof(struct icmphdr,\n\t\t\t\t\t\t\t  type) -\n\t\t\t\t\t\t skb_in->data,\n\t\t\t\t\t\t sizeof(_inner_type),\n\t\t\t\t\t\t &_inner_type);\n\t\t\tif (itp == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t/*\n\t\t\t *\tAssume any unknown ICMP type is an error. This\n\t\t\t *\tisn't specified by the RFC, but think about it..\n\t\t\t */\n\t\t\tif (*itp > NR_ICMP_TYPES ||\n\t\t\t    icmp_pointers[*itp].error)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\n\t/*\n\t *\tConstruct source address and options.\n\t */\n\n\tsaddr = iph->daddr;\n\tif (!(rt->rt_flags & RTCF_LOCAL)) {\n\t\tstruct net_device *dev = NULL;\n\n\t\trcu_read_lock();\n\t\tif (rt_is_input_route(rt) &&\n\t\t    net->ipv4.sysctl_icmp_errors_use_inbound_ifaddr)\n\t\t\tdev = dev_get_by_index_rcu(net, rt->rt_iif);\n\n\t\tif (dev)\n\t\t\tsaddr = inet_select_addr(dev, 0, RT_SCOPE_LINK);\n\t\telse\n\t\t\tsaddr = 0;\n\t\trcu_read_unlock();\n\t}\n\n\ttos = icmp_pointers[type].error ? ((iph->tos & IPTOS_TOS_MASK) |\n\t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n\t\t\t\t\t  iph->tos;\n\n\tif (ip_options_echo(&icmp_param.replyopts, skb_in))\n\t\tgoto out_unlock;\n\n\n\t/*\n\t *\tPrepare data for ICMP header.\n\t */\n\n\ticmp_param.data.icmph.type\t = type;\n\ticmp_param.data.icmph.code\t = code;\n\ticmp_param.data.icmph.un.gateway = info;\n\ticmp_param.data.icmph.checksum\t = 0;\n\ticmp_param.skb\t  = skb_in;\n\ticmp_param.offset = skb_network_offset(skb_in);\n\tinet_sk(sk)->tos = tos;\n\tipc.addr = iph->saddr;\n\tipc.opt = &icmp_param.replyopts;\n\tipc.tx_flags = 0;\n\n\trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n\t\t\t       type, code, &icmp_param);\n\tif (IS_ERR(rt))\n\t\tgoto out_unlock;\n\n\tif (!icmpv4_xrlim_allow(net, rt, type, code))\n\t\tgoto ende;\n\n\t/* RFC says return as much as we can without exceeding 576 bytes. */\n\n\troom = dst_mtu(&rt->dst);\n\tif (room > 576)\n\t\troom = 576;\n\troom -= sizeof(struct iphdr) + icmp_param.replyopts.optlen;\n\troom -= sizeof(struct icmphdr);\n\n\ticmp_param.data_len = skb_in->len - icmp_param.offset;\n\tif (icmp_param.data_len > room)\n\t\ticmp_param.data_len = room;\n\ticmp_param.head_len = sizeof(struct icmphdr);\n\n\ticmp_push_reply(&icmp_param, &ipc, &rt);\nende:\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\nout:;\n}\nEXPORT_SYMBOL(icmp_send);\n\n\n/*\n *\tHandle ICMP_DEST_UNREACH, ICMP_TIME_EXCEED, and ICMP_QUENCH.\n */\n\nstatic void icmp_unreach(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct icmphdr *icmph;\n\tint hash, protocol;\n\tconst struct net_protocol *ipprot;\n\tu32 info = 0;\n\tstruct net *net;\n\n\tnet = dev_net(skb_dst(skb)->dev);\n\n\t/*\n\t *\tIncomplete header ?\n\t * \tOnly checks for the IP header, there should be an\n\t *\tadditional check for longer headers in upper levels.\n\t */\n\n\tif (!pskb_may_pull(skb, sizeof(struct iphdr)))\n\t\tgoto out_err;\n\n\ticmph = icmp_hdr(skb);\n\tiph   = (const struct iphdr *)skb->data;\n\n\tif (iph->ihl < 5) /* Mangled header, drop. */\n\t\tgoto out_err;\n\n\tif (icmph->type == ICMP_DEST_UNREACH) {\n\t\tswitch (icmph->code & 15) {\n\t\tcase ICMP_NET_UNREACH:\n\t\tcase ICMP_HOST_UNREACH:\n\t\tcase ICMP_PROT_UNREACH:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\tbreak;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\tif (ipv4_config.no_pmtu_disc) {\n\t\t\t\tLIMIT_NETDEBUG(KERN_INFO \"ICMP: %pI4: fragmentation needed and DF set.\\n\",\n\t\t\t\t\t       &iph->daddr);\n\t\t\t} else {\n\t\t\t\tinfo = ip_rt_frag_needed(net, iph,\n\t\t\t\t\t\t\t ntohs(icmph->un.frag.mtu),\n\t\t\t\t\t\t\t skb->dev);\n\t\t\t\tif (!info)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ICMP_SR_FAILED:\n\t\t\tLIMIT_NETDEBUG(KERN_INFO \"ICMP: %pI4: Source Route Failed.\\n\",\n\t\t\t\t       &iph->daddr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tif (icmph->code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\t} else if (icmph->type == ICMP_PARAMETERPROB)\n\t\tinfo = ntohl(icmph->un.gateway) >> 24;\n\n\t/*\n\t *\tThrow it at our lower layers\n\t *\n\t *\tRFC 1122: 3.2.2 MUST extract the protocol ID from the passed\n\t *\t\t  header.\n\t *\tRFC 1122: 3.2.2.1 MUST pass ICMP unreach messages to the\n\t *\t\t  transport layer.\n\t *\tRFC 1122: 3.2.2.2 MUST pass ICMP time expired messages to\n\t *\t\t  transport layer.\n\t */\n\n\t/*\n\t *\tCheck the other end isn't violating RFC 1122. Some routers send\n\t *\tbogus responses to broadcast frames. If you see this message\n\t *\tfirst check your netmask matches at both ends, if it does then\n\t *\tget the other vendor to fix their kit.\n\t */\n\n\tif (!net->ipv4.sysctl_icmp_ignore_bogus_error_responses &&\n\t    inet_addr_type(net, iph->daddr) == RTN_BROADCAST) {\n\t\tif (net_ratelimit())\n\t\t\tprintk(KERN_WARNING \"%pI4 sent an invalid ICMP \"\n\t\t\t\t\t    \"type %u, code %u \"\n\t\t\t\t\t    \"error to a broadcast: %pI4 on %s\\n\",\n\t\t\t       &ip_hdr(skb)->saddr,\n\t\t\t       icmph->type, icmph->code,\n\t\t\t       &iph->daddr,\n\t\t\t       skb->dev->name);\n\t\tgoto out;\n\t}\n\n\t/* Checkin full IP header plus 8 bytes of protocol to\n\t * avoid additional coding at protocol handlers.\n\t */\n\tif (!pskb_may_pull(skb, iph->ihl * 4 + 8))\n\t\tgoto out;\n\n\tiph = (const struct iphdr *)skb->data;\n\tprotocol = iph->protocol;\n\n\t/*\n\t *\tDeliver ICMP message to raw sockets. Pretty useless feature?\n\t */\n\traw_icmp_error(skb, protocol, info);\n\n\thash = protocol & (MAX_INET_PROTOS - 1);\n\trcu_read_lock();\n\tipprot = rcu_dereference(inet_protos[hash]);\n\tif (ipprot && ipprot->err_handler)\n\t\tipprot->err_handler(skb, info);\n\trcu_read_unlock();\n\nout:\n\treturn;\nout_err:\n\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\tgoto out;\n}\n\n\n/*\n *\tHandle ICMP_REDIRECT.\n */\n\nstatic void icmp_redirect(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\n\tif (skb->len < sizeof(struct iphdr))\n\t\tgoto out_err;\n\n\t/*\n\t *\tGet the copied header of the packet that caused the redirect\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct iphdr)))\n\t\tgoto out;\n\n\tiph = (const struct iphdr *)skb->data;\n\n\tswitch (icmp_hdr(skb)->code & 7) {\n\tcase ICMP_REDIR_NET:\n\tcase ICMP_REDIR_NETTOS:\n\t\t/*\n\t\t * As per RFC recommendations now handle it as a host redirect.\n\t\t */\n\tcase ICMP_REDIR_HOST:\n\tcase ICMP_REDIR_HOSTTOS:\n\t\tip_rt_redirect(ip_hdr(skb)->saddr, iph->daddr,\n\t\t\t       icmp_hdr(skb)->un.gateway,\n\t\t\t       iph->saddr, skb->dev);\n\t\tbreak;\n\t}\nout:\n\treturn;\nout_err:\n\tICMP_INC_STATS_BH(dev_net(skb->dev), ICMP_MIB_INERRORS);\n\tgoto out;\n}\n\n/*\n *\tHandle ICMP_ECHO (\"ping\") requests.\n *\n *\tRFC 1122: 3.2.2.6 MUST have an echo server that answers ICMP echo\n *\t\t  requests.\n *\tRFC 1122: 3.2.2.6 Data received in the ICMP_ECHO request MUST be\n *\t\t  included in the reply.\n *\tRFC 1812: 4.3.3.6 SHOULD have a config option for silently ignoring\n *\t\t  echo requests, MUST have default=NOT.\n *\tSee also WRT handling of options once they are done and working.\n */\n\nstatic void icmp_echo(struct sk_buff *skb)\n{\n\tstruct net *net;\n\n\tnet = dev_net(skb_dst(skb)->dev);\n\tif (!net->ipv4.sysctl_icmp_echo_ignore_all) {\n\t\tstruct icmp_bxm icmp_param;\n\n\t\ticmp_param.data.icmph\t   = *icmp_hdr(skb);\n\t\ticmp_param.data.icmph.type = ICMP_ECHOREPLY;\n\t\ticmp_param.skb\t\t   = skb;\n\t\ticmp_param.offset\t   = 0;\n\t\ticmp_param.data_len\t   = skb->len;\n\t\ticmp_param.head_len\t   = sizeof(struct icmphdr);\n\t\ticmp_reply(&icmp_param, skb);\n\t}\n}\n\n/*\n *\tHandle ICMP Timestamp requests.\n *\tRFC 1122: 3.2.2.8 MAY implement ICMP timestamp requests.\n *\t\t  SHOULD be in the kernel for minimum random latency.\n *\t\t  MUST be accurate to a few minutes.\n *\t\t  MUST be updated at least at 15Hz.\n */\nstatic void icmp_timestamp(struct sk_buff *skb)\n{\n\tstruct timespec tv;\n\tstruct icmp_bxm icmp_param;\n\t/*\n\t *\tToo short.\n\t */\n\tif (skb->len < 4)\n\t\tgoto out_err;\n\n\t/*\n\t *\tFill in the current time as ms since midnight UT:\n\t */\n\tgetnstimeofday(&tv);\n\ticmp_param.data.times[1] = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC +\n\t\t\t\t\t tv.tv_nsec / NSEC_PER_MSEC);\n\ticmp_param.data.times[2] = icmp_param.data.times[1];\n\tif (skb_copy_bits(skb, 0, &icmp_param.data.times[0], 4))\n\t\tBUG();\n\ticmp_param.data.icmph\t   = *icmp_hdr(skb);\n\ticmp_param.data.icmph.type = ICMP_TIMESTAMPREPLY;\n\ticmp_param.data.icmph.code = 0;\n\ticmp_param.skb\t\t   = skb;\n\ticmp_param.offset\t   = 0;\n\ticmp_param.data_len\t   = 0;\n\ticmp_param.head_len\t   = sizeof(struct icmphdr) + 12;\n\ticmp_reply(&icmp_param, skb);\nout:\n\treturn;\nout_err:\n\tICMP_INC_STATS_BH(dev_net(skb_dst(skb)->dev), ICMP_MIB_INERRORS);\n\tgoto out;\n}\n\n\n/*\n *\tHandle ICMP_ADDRESS_MASK requests.  (RFC950)\n *\n * RFC1122 (3.2.2.9).  A host MUST only send replies to\n * ADDRESS_MASK requests if it's been configured as an address mask\n * agent.  Receiving a request doesn't constitute implicit permission to\n * act as one. Of course, implementing this correctly requires (SHOULD)\n * a way to turn the functionality on and off.  Another one for sysctl(),\n * I guess. -- MS\n *\n * RFC1812 (4.3.3.9).\tA router MUST implement it.\n *\t\t\tA router SHOULD have switch turning it on/off.\n *\t\t      \tThis switch MUST be ON by default.\n *\n * Gratuitous replies, zero-source replies are not implemented,\n * that complies with RFC. DO NOT implement them!!! All the idea\n * of broadcast addrmask replies as specified in RFC950 is broken.\n * The problem is that it is not uncommon to have several prefixes\n * on one physical interface. Moreover, addrmask agent can even be\n * not aware of existing another prefixes.\n * If source is zero, addrmask agent cannot choose correct prefix.\n * Gratuitous mask announcements suffer from the same problem.\n * RFC1812 explains it, but still allows to use ADDRMASK,\n * that is pretty silly. --ANK\n *\n * All these rules are so bizarre, that I removed kernel addrmask\n * support at all. It is wrong, it is obsolete, nobody uses it in\n * any case. --ANK\n *\n * Furthermore you can do it with a usermode address agent program\n * anyway...\n */\n\nstatic void icmp_address(struct sk_buff *skb)\n{\n#if 0\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"a guy asks for address mask. Who is it?\\n\");\n#endif\n}\n\n/*\n * RFC1812 (4.3.3.9).\tA router SHOULD listen all replies, and complain\n *\t\t\tloudly if an inconsistency is found.\n * called with rcu_read_lock()\n */\n\nstatic void icmp_address_reply(struct sk_buff *skb)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct in_device *in_dev;\n\tstruct in_ifaddr *ifa;\n\n\tif (skb->len < 4 || !(rt->rt_flags&RTCF_DIRECTSRC))\n\t\treturn;\n\n\tin_dev = __in_dev_get_rcu(dev);\n\tif (!in_dev)\n\t\treturn;\n\n\tif (in_dev->ifa_list &&\n\t    IN_DEV_LOG_MARTIANS(in_dev) &&\n\t    IN_DEV_FORWARD(in_dev)) {\n\t\t__be32 _mask, *mp;\n\n\t\tmp = skb_header_pointer(skb, 0, sizeof(_mask), &_mask);\n\t\tBUG_ON(mp == NULL);\n\t\tfor (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {\n\t\t\tif (*mp == ifa->ifa_mask &&\n\t\t\t    inet_ifa_match(rt->rt_src, ifa))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!ifa && net_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"Wrong address mask %pI4 from %s/%pI4\\n\",\n\t\t\t       mp, dev->name, &rt->rt_src);\n\t\t}\n\t}\n}\n\nstatic void icmp_discard(struct sk_buff *skb)\n{\n}\n\n/*\n *\tDeal with incoming ICMP packets.\n */\nint icmp_rcv(struct sk_buff *skb)\n{\n\tstruct icmphdr *icmph;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\t\tint nh;\n\n\t\tif (!(sp && sp->xvec[sp->len - 1]->props.flags &\n\t\t\t\t XFRM_STATE_ICMP))\n\t\t\tgoto drop;\n\n\t\tif (!pskb_may_pull(skb, sizeof(*icmph) + sizeof(struct iphdr)))\n\t\t\tgoto drop;\n\n\t\tnh = skb_network_offset(skb);\n\t\tskb_set_network_header(skb, sizeof(*icmph));\n\n\t\tif (!xfrm4_policy_check_reverse(NULL, XFRM_POLICY_IN, skb))\n\t\t\tgoto drop;\n\n\t\tskb_set_network_header(skb, nh);\n\t}\n\n\tICMP_INC_STATS_BH(net, ICMP_MIB_INMSGS);\n\n\tswitch (skb->ip_summed) {\n\tcase CHECKSUM_COMPLETE:\n\t\tif (!csum_fold(skb->csum))\n\t\t\tbreak;\n\t\t/* fall through */\n\tcase CHECKSUM_NONE:\n\t\tskb->csum = 0;\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto error;\n\t}\n\n\tif (!pskb_pull(skb, sizeof(*icmph)))\n\t\tgoto error;\n\n\ticmph = icmp_hdr(skb);\n\n\tICMPMSGIN_INC_STATS_BH(net, icmph->type);\n\t/*\n\t *\t18 is the highest 'known' ICMP type. Anything else is a mystery\n\t *\n\t *\tRFC 1122: 3.2.2  Unknown ICMP messages types MUST be silently\n\t *\t\t  discarded.\n\t */\n\tif (icmph->type > NR_ICMP_TYPES)\n\t\tgoto error;\n\n\n\t/*\n\t *\tParse the ICMP message\n\t */\n\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST)) {\n\t\t/*\n\t\t *\tRFC 1122: 3.2.2.6 An ICMP_ECHO to broadcast MAY be\n\t\t *\t  silently ignored (we let user decide with a sysctl).\n\t\t *\tRFC 1122: 3.2.2.8 An ICMP_TIMESTAMP MAY be silently\n\t\t *\t  discarded if to broadcast/multicast.\n\t\t */\n\t\tif ((icmph->type == ICMP_ECHO ||\n\t\t     icmph->type == ICMP_TIMESTAMP) &&\n\t\t    net->ipv4.sysctl_icmp_echo_ignore_broadcasts) {\n\t\t\tgoto error;\n\t\t}\n\t\tif (icmph->type != ICMP_ECHO &&\n\t\t    icmph->type != ICMP_TIMESTAMP &&\n\t\t    icmph->type != ICMP_ADDRESS &&\n\t\t    icmph->type != ICMP_ADDRESSREPLY) {\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\ticmp_pointers[icmph->type].handler(skb);\n\ndrop:\n\tkfree_skb(skb);\n\treturn 0;\nerror:\n\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\tgoto drop;\n}\n\n/*\n *\tThis table is the definition of how we handle ICMP.\n */\nstatic const struct icmp_control icmp_pointers[NR_ICMP_TYPES + 1] = {\n\t[ICMP_ECHOREPLY] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[1] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[2] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[ICMP_DEST_UNREACH] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_SOURCE_QUENCH] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_REDIRECT] = {\n\t\t.handler = icmp_redirect,\n\t\t.error = 1,\n\t},\n\t[6] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[7] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[ICMP_ECHO] = {\n\t\t.handler = icmp_echo,\n\t},\n\t[9] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[10] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[ICMP_TIME_EXCEEDED] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_PARAMETERPROB] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_TIMESTAMP] = {\n\t\t.handler = icmp_timestamp,\n\t},\n\t[ICMP_TIMESTAMPREPLY] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[ICMP_INFO_REQUEST] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[ICMP_INFO_REPLY] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[ICMP_ADDRESS] = {\n\t\t.handler = icmp_address,\n\t},\n\t[ICMP_ADDRESSREPLY] = {\n\t\t.handler = icmp_address_reply,\n\t},\n};\n\nstatic void __net_exit icmp_sk_exit(struct net *net)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tinet_ctl_sock_destroy(net->ipv4.icmp_sk[i]);\n\tkfree(net->ipv4.icmp_sk);\n\tnet->ipv4.icmp_sk = NULL;\n}\n\nstatic int __net_init icmp_sk_init(struct net *net)\n{\n\tint i, err;\n\n\tnet->ipv4.icmp_sk =\n\t\tkzalloc(nr_cpu_ids * sizeof(struct sock *), GFP_KERNEL);\n\tif (net->ipv4.icmp_sk == NULL)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct sock *sk;\n\n\t\terr = inet_ctl_sock_create(&sk, PF_INET,\n\t\t\t\t\t   SOCK_RAW, IPPROTO_ICMP, net);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\n\t\tnet->ipv4.icmp_sk[i] = sk;\n\n\t\t/* Enough space for 2 64K ICMP packets, including\n\t\t * sk_buff struct overhead.\n\t\t */\n\t\tsk->sk_sndbuf =\n\t\t\t(2 * ((64 * 1024) + sizeof(struct sk_buff)));\n\n\t\t/*\n\t\t * Speedup sock_wfree()\n\t\t */\n\t\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\t\tinet_sk(sk)->pmtudisc = IP_PMTUDISC_DONT;\n\t}\n\n\t/* Control parameters for ECHO replies. */\n\tnet->ipv4.sysctl_icmp_echo_ignore_all = 0;\n\tnet->ipv4.sysctl_icmp_echo_ignore_broadcasts = 1;\n\n\t/* Control parameter - ignore bogus broadcast responses? */\n\tnet->ipv4.sysctl_icmp_ignore_bogus_error_responses = 1;\n\n\t/*\n\t * \tConfigurable global rate limit.\n\t *\n\t *\tratelimit defines tokens/packet consumed for dst->rate_token\n\t *\tbucket ratemask defines which icmp types are ratelimited by\n\t *\tsetting\tit's bit position.\n\t *\n\t *\tdefault:\n\t *\tdest unreachable (3), source quench (4),\n\t *\ttime exceeded (11), parameter problem (12)\n\t */\n\n\tnet->ipv4.sysctl_icmp_ratelimit = 1 * HZ;\n\tnet->ipv4.sysctl_icmp_ratemask = 0x1818;\n\tnet->ipv4.sysctl_icmp_errors_use_inbound_ifaddr = 0;\n\n\treturn 0;\n\nfail:\n\tfor_each_possible_cpu(i)\n\t\tinet_ctl_sock_destroy(net->ipv4.icmp_sk[i]);\n\tkfree(net->ipv4.icmp_sk);\n\treturn err;\n}\n\nstatic struct pernet_operations __net_initdata icmp_sk_ops = {\n       .init = icmp_sk_init,\n       .exit = icmp_sk_exit,\n};\n\nint __init icmp_init(void)\n{\n\treturn register_pernet_subsys(&icmp_sk_ops);\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tSupport for INET connection oriented protocols.\n *\n * Authors:\tSee the TCP sources\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/jhash.h>\n\n#include <net/inet_connection_sock.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_timewait_sock.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n\n#ifdef INET_CSK_DEBUG\nconst char inet_csk_timer_bug_msg[] = \"inet_csk BUG: unknown timer value\\n\";\nEXPORT_SYMBOL(inet_csk_timer_bug_msg);\n#endif\n\n/*\n * This struct holds the first and last local port number.\n */\nstruct local_ports sysctl_local_ports __read_mostly = {\n\t.lock = SEQLOCK_UNLOCKED,\n\t.range = { 32768, 61000 },\n};\n\nunsigned long *sysctl_local_reserved_ports;\nEXPORT_SYMBOL(sysctl_local_reserved_ports);\n\nvoid inet_get_local_port_range(int *low, int *high)\n{\n\tunsigned seq;\n\tdo {\n\t\tseq = read_seqbegin(&sysctl_local_ports.lock);\n\n\t\t*low = sysctl_local_ports.range[0];\n\t\t*high = sysctl_local_ports.range[1];\n\t} while (read_seqretry(&sysctl_local_ports.lock, seq));\n}\nEXPORT_SYMBOL(inet_get_local_port_range);\n\nint inet_csk_bind_conflict(const struct sock *sk,\n\t\t\t   const struct inet_bind_bucket *tb)\n{\n\tstruct sock *sk2;\n\tstruct hlist_node *node;\n\tint reuse = sk->sk_reuse;\n\n\t/*\n\t * Unlike other sk lookup places we do not check\n\t * for sk_net here, since _all_ the socks listed\n\t * in tb->owners list belong to the same net - the\n\t * one this bucket belongs to.\n\t */\n\n\tsk_for_each_bound(sk2, node, &tb->owners) {\n\t\tif (sk != sk2 &&\n\t\t    !inet_v6_ipv6only(sk2) &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     !sk2->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\n\t\t\tif (!reuse || !sk2->sk_reuse ||\n\t\t\t    sk2->sk_state == TCP_LISTEN) {\n\t\t\t\tconst __be32 sk2_rcv_saddr = sk_rcv_saddr(sk2);\n\t\t\t\tif (!sk2_rcv_saddr || !sk_rcv_saddr(sk) ||\n\t\t\t\t    sk2_rcv_saddr == sk_rcv_saddr(sk))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn node != NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_bind_conflict);\n\n/* Obtain a reference to a local port for the given sock,\n * if snum is zero it means select any available local port.\n */\nint inet_csk_get_port(struct sock *sk, unsigned short snum)\n{\n\tstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\n\tstruct inet_bind_hashbucket *head;\n\tstruct hlist_node *node;\n\tstruct inet_bind_bucket *tb;\n\tint ret, attempts = 5;\n\tstruct net *net = sock_net(sk);\n\tint smallest_size = -1, smallest_rover;\n\n\tlocal_bh_disable();\n\tif (!snum) {\n\t\tint remaining, rover, low, high;\n\nagain:\n\t\tinet_get_local_port_range(&low, &high);\n\t\tremaining = (high - low) + 1;\n\t\tsmallest_rover = rover = net_random() % remaining + low;\n\n\t\tsmallest_size = -1;\n\t\tdo {\n\t\t\tif (inet_is_reserved_local_port(rover))\n\t\t\t\tgoto next_nolock;\n\t\t\thead = &hashinfo->bhash[inet_bhashfn(net, rover,\n\t\t\t\t\thashinfo->bhash_size)];\n\t\t\tspin_lock(&head->lock);\n\t\t\tinet_bind_bucket_for_each(tb, node, &head->chain)\n\t\t\t\tif (net_eq(ib_net(tb), net) && tb->port == rover) {\n\t\t\t\t\tif (tb->fastreuse > 0 &&\n\t\t\t\t\t    sk->sk_reuse &&\n\t\t\t\t\t    sk->sk_state != TCP_LISTEN &&\n\t\t\t\t\t    (tb->num_owners < smallest_size || smallest_size == -1)) {\n\t\t\t\t\t\tsmallest_size = tb->num_owners;\n\t\t\t\t\t\tsmallest_rover = rover;\n\t\t\t\t\t\tif (atomic_read(&hashinfo->bsockets) > (high - low) + 1) {\n\t\t\t\t\t\t\tspin_unlock(&head->lock);\n\t\t\t\t\t\t\tsnum = smallest_rover;\n\t\t\t\t\t\t\tgoto have_snum;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\tbreak;\n\t\tnext:\n\t\t\tspin_unlock(&head->lock);\n\t\tnext_nolock:\n\t\t\tif (++rover > high)\n\t\t\t\trover = low;\n\t\t} while (--remaining > 0);\n\n\t\t/* Exhausted local port range during search?  It is not\n\t\t * possible for us to be holding one of the bind hash\n\t\t * locks if this test triggers, because if 'remaining'\n\t\t * drops to zero, we broke out of the do/while loop at\n\t\t * the top level, not from the 'break;' statement.\n\t\t */\n\t\tret = 1;\n\t\tif (remaining <= 0) {\n\t\t\tif (smallest_size != -1) {\n\t\t\t\tsnum = smallest_rover;\n\t\t\t\tgoto have_snum;\n\t\t\t}\n\t\t\tgoto fail;\n\t\t}\n\t\t/* OK, here is the one we will use.  HEAD is\n\t\t * non-NULL and we hold it's mutex.\n\t\t */\n\t\tsnum = rover;\n\t} else {\nhave_snum:\n\t\thead = &hashinfo->bhash[inet_bhashfn(net, snum,\n\t\t\t\thashinfo->bhash_size)];\n\t\tspin_lock(&head->lock);\n\t\tinet_bind_bucket_for_each(tb, node, &head->chain)\n\t\t\tif (net_eq(ib_net(tb), net) && tb->port == snum)\n\t\t\t\tgoto tb_found;\n\t}\n\ttb = NULL;\n\tgoto tb_not_found;\ntb_found:\n\tif (!hlist_empty(&tb->owners)) {\n\t\tif (tb->fastreuse > 0 &&\n\t\t    sk->sk_reuse && sk->sk_state != TCP_LISTEN &&\n\t\t    smallest_size == -1) {\n\t\t\tgoto success;\n\t\t} else {\n\t\t\tret = 1;\n\t\t\tif (inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb)) {\n\t\t\t\tif (sk->sk_reuse && sk->sk_state != TCP_LISTEN &&\n\t\t\t\t    smallest_size != -1 && --attempts >= 0) {\n\t\t\t\t\tspin_unlock(&head->lock);\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\t\t\t\tgoto fail_unlock;\n\t\t\t}\n\t\t}\n\t}\ntb_not_found:\n\tret = 1;\n\tif (!tb && (tb = inet_bind_bucket_create(hashinfo->bind_bucket_cachep,\n\t\t\t\t\tnet, head, snum)) == NULL)\n\t\tgoto fail_unlock;\n\tif (hlist_empty(&tb->owners)) {\n\t\tif (sk->sk_reuse && sk->sk_state != TCP_LISTEN)\n\t\t\ttb->fastreuse = 1;\n\t\telse\n\t\t\ttb->fastreuse = 0;\n\t} else if (tb->fastreuse &&\n\t\t   (!sk->sk_reuse || sk->sk_state == TCP_LISTEN))\n\t\ttb->fastreuse = 0;\nsuccess:\n\tif (!inet_csk(sk)->icsk_bind_hash)\n\t\tinet_bind_hash(sk, tb, snum);\n\tWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\n\tret = 0;\n\nfail_unlock:\n\tspin_unlock(&head->lock);\nfail:\n\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(inet_csk_get_port);\n\n/*\n * Wait for an incoming connection, avoid race conditions. This must be called\n * with the socket locked.\n */\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tDEFINE_WAIT(wait);\n\tint err;\n\n\t/*\n\t * True wake-one mechanism for incoming connections: only\n\t * one process gets woken up, not the 'whole herd'.\n\t * Since we do not 'race & poll' for established sockets\n\t * anymore, the common case will execute the loop only once.\n\t *\n\t * Subtle issue: \"add_wait_queue_exclusive()\" will be added\n\t * after any current non-exclusive waiters, and we know that\n\t * it will always _stay_ after any new non-exclusive waiters\n\t * because all non-exclusive waiters are added at the\n\t * beginning of the wait-queue. As such, it's ok to \"drop\"\n\t * our exclusiveness temporarily when we get woken up without\n\t * having to remove and re-insert us on the wait queue.\n\t */\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\trelease_sock(sk);\n\t\tif (reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock(sk);\n\t\terr = 0;\n\t\tif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\tbreak;\n\t\terr = -EINVAL;\n\t\tif (sk->sk_state != TCP_LISTEN)\n\t\t\tbreak;\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tbreak;\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn err;\n}\n\n/*\n * This will accept the next outstanding connection.\n */\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sock *newsk;\n\tint error;\n\n\tlock_sock(sk);\n\n\t/* We need to make sure that this socket is listening,\n\t * and that it has something pending.\n\t */\n\terror = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out_err;\n\n\t/* Find already established connection */\n\tif (reqsk_queue_empty(&icsk->icsk_accept_queue)) {\n\t\tlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\n\n\t\t/* If this is a non blocking socket don't sleep */\n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_err;\n\n\t\terror = inet_csk_wait_for_connect(sk, timeo);\n\t\tif (error)\n\t\t\tgoto out_err;\n\t}\n\n\tnewsk = reqsk_queue_get_child(&icsk->icsk_accept_queue, sk);\n\tWARN_ON(newsk->sk_state == TCP_SYN_RECV);\nout:\n\trelease_sock(sk);\n\treturn newsk;\nout_err:\n\tnewsk = NULL;\n\t*err = error;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_csk_accept);\n\n/*\n * Using different timers for retransmit, delayed acks and probes\n * We may wish use just one timer maintaining a list of expire jiffies\n * to optimize.\n */\nvoid inet_csk_init_xmit_timers(struct sock *sk,\n\t\t\t       void (*retransmit_handler)(unsigned long),\n\t\t\t       void (*delack_handler)(unsigned long),\n\t\t\t       void (*keepalive_handler)(unsigned long))\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tsetup_timer(&icsk->icsk_retransmit_timer, retransmit_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&icsk->icsk_delack_timer, delack_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&sk->sk_timer, keepalive_handler, (unsigned long)sk);\n\ticsk->icsk_pending = icsk->icsk_ack.pending = 0;\n}\nEXPORT_SYMBOL(inet_csk_init_xmit_timers);\n\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_pending = icsk->icsk_ack.pending = icsk->icsk_ack.blocked = 0;\n\n\tsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\n\tsk_stop_timer(sk, &icsk->icsk_delack_timer);\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_clear_xmit_timers);\n\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_delete_keepalive_timer);\n\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\n{\n\tsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\n}\nEXPORT_SYMBOL(inet_csk_reset_keepalive_timer);\n\nstruct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_req);\n\nstatic inline u32 inet_synq_hash(const __be32 raddr, const __be16 rport,\n\t\t\t\t const u32 rnd, const u32 synq_hsize)\n{\n\treturn jhash_2words((__force u32)raddr, (__force u32)rport, rnd) & (synq_hsize - 1);\n}\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#define AF_INET_FAMILY(fam) ((fam) == AF_INET)\n#else\n#define AF_INET_FAMILY(fam) 1\n#endif\n\nstruct request_sock *inet_csk_search_req(const struct sock *sk,\n\t\t\t\t\t struct request_sock ***prevp,\n\t\t\t\t\t const __be16 rport, const __be32 raddr,\n\t\t\t\t\t const __be32 laddr)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\n\tstruct request_sock *req, **prev;\n\n\tfor (prev = &lopt->syn_table[inet_synq_hash(raddr, rport, lopt->hash_rnd,\n\t\t\t\t\t\t    lopt->nr_table_entries)];\n\t     (req = *prev) != NULL;\n\t     prev = &req->dl_next) {\n\t\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\n\t\tif (ireq->rmt_port == rport &&\n\t\t    ireq->rmt_addr == raddr &&\n\t\t    ireq->loc_addr == laddr &&\n\t\t    AF_INET_FAMILY(req->rsk_ops->family)) {\n\t\t\tWARN_ON(req->sk);\n\t\t\t*prevp = prev;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL_GPL(inet_csk_search_req);\n\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\n\t\t\t\t   unsigned long timeout)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\n\tconst u32 h = inet_synq_hash(inet_rsk(req)->rmt_addr, inet_rsk(req)->rmt_port,\n\t\t\t\t     lopt->hash_rnd, lopt->nr_table_entries);\n\n\treqsk_queue_hash_req(&icsk->icsk_accept_queue, h, req, timeout);\n\tinet_csk_reqsk_queue_added(sk, timeout);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_hash_add);\n\n/* Only thing we need from tcp.h */\nextern int sysctl_tcp_synack_retries;\n\n\n/* Decide when to expire the request and when to resend SYN-ACK */\nstatic inline void syn_ack_recalc(struct request_sock *req, const int thresh,\n\t\t\t\t  const int max_retries,\n\t\t\t\t  const u8 rskq_defer_accept,\n\t\t\t\t  int *expire, int *resend)\n{\n\tif (!rskq_defer_accept) {\n\t\t*expire = req->retrans >= thresh;\n\t\t*resend = 1;\n\t\treturn;\n\t}\n\t*expire = req->retrans >= thresh &&\n\t\t  (!inet_rsk(req)->acked || req->retrans >= max_retries);\n\t/*\n\t * Do not resend while waiting for data after ACK,\n\t * start to resend on end of deferring period to give\n\t * last chance for data or ACK to create established socket.\n\t */\n\t*resend = !inet_rsk(req)->acked ||\n\t\t  req->retrans >= rskq_defer_accept - 1;\n}\n\nvoid inet_csk_reqsk_queue_prune(struct sock *parent,\n\t\t\t\tconst unsigned long interval,\n\t\t\t\tconst unsigned long timeout,\n\t\t\t\tconst unsigned long max_rto)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(parent);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct listen_sock *lopt = queue->listen_opt;\n\tint max_retries = icsk->icsk_syn_retries ? : sysctl_tcp_synack_retries;\n\tint thresh = max_retries;\n\tunsigned long now = jiffies;\n\tstruct request_sock **reqp, *req;\n\tint i, budget;\n\n\tif (lopt == NULL || lopt->qlen == 0)\n\t\treturn;\n\n\t/* Normally all the openreqs are young and become mature\n\t * (i.e. converted to established socket) for first timeout.\n\t * If synack was not acknowledged for 3 seconds, it means\n\t * one of the following things: synack was lost, ack was lost,\n\t * rtt is high or nobody planned to ack (i.e. synflood).\n\t * When server is a bit loaded, queue is populated with old\n\t * open requests, reducing effective size of queue.\n\t * When server is well loaded, queue size reduces to zero\n\t * after several minutes of work. It is not synflood,\n\t * it is normal operation. The solution is pruning\n\t * too old entries overriding normal timeout, when\n\t * situation becomes dangerous.\n\t *\n\t * Essentially, we reserve half of room for young\n\t * embrions; and abort old ones without pity, if old\n\t * ones are about to clog our table.\n\t */\n\tif (lopt->qlen>>(lopt->max_qlen_log-1)) {\n\t\tint young = (lopt->qlen_young<<1);\n\n\t\twhile (thresh > 2) {\n\t\t\tif (lopt->qlen < young)\n\t\t\t\tbreak;\n\t\t\tthresh--;\n\t\t\tyoung <<= 1;\n\t\t}\n\t}\n\n\tif (queue->rskq_defer_accept)\n\t\tmax_retries = queue->rskq_defer_accept;\n\n\tbudget = 2 * (lopt->nr_table_entries / (timeout / interval));\n\ti = lopt->clock_hand;\n\n\tdo {\n\t\treqp=&lopt->syn_table[i];\n\t\twhile ((req = *reqp) != NULL) {\n\t\t\tif (time_after_eq(now, req->expires)) {\n\t\t\t\tint expire = 0, resend = 0;\n\n\t\t\t\tsyn_ack_recalc(req, thresh, max_retries,\n\t\t\t\t\t       queue->rskq_defer_accept,\n\t\t\t\t\t       &expire, &resend);\n\t\t\t\tif (req->rsk_ops->syn_ack_timeout)\n\t\t\t\t\treq->rsk_ops->syn_ack_timeout(parent, req);\n\t\t\t\tif (!expire &&\n\t\t\t\t    (!resend ||\n\t\t\t\t     !req->rsk_ops->rtx_syn_ack(parent, req, NULL) ||\n\t\t\t\t     inet_rsk(req)->acked)) {\n\t\t\t\t\tunsigned long timeo;\n\n\t\t\t\t\tif (req->retrans++ == 0)\n\t\t\t\t\t\tlopt->qlen_young--;\n\t\t\t\t\ttimeo = min((timeout << req->retrans), max_rto);\n\t\t\t\t\treq->expires = now + timeo;\n\t\t\t\t\treqp = &req->dl_next;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t/* Drop this request */\n\t\t\t\tinet_csk_reqsk_queue_unlink(parent, req, reqp);\n\t\t\t\treqsk_queue_removed(queue, req);\n\t\t\t\treqsk_free(req);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treqp = &req->dl_next;\n\t\t}\n\n\t\ti = (i + 1) & (lopt->nr_table_entries - 1);\n\n\t} while (--budget > 0);\n\n\tlopt->clock_hand = i;\n\n\tif (lopt->qlen)\n\t\tinet_csk_reset_keepalive_timer(parent, interval);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_prune);\n\nstruct sock *inet_csk_clone(struct sock *sk, const struct request_sock *req,\n\t\t\t    const gfp_t priority)\n{\n\tstruct sock *newsk = sk_clone(sk, priority);\n\n\tif (newsk != NULL) {\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\n\t\tnewsk->sk_state = TCP_SYN_RECV;\n\t\tnewicsk->icsk_bind_hash = NULL;\n\n\t\tinet_sk(newsk)->inet_dport = inet_rsk(req)->rmt_port;\n\t\tinet_sk(newsk)->inet_num = ntohs(inet_rsk(req)->loc_port);\n\t\tinet_sk(newsk)->inet_sport = inet_rsk(req)->loc_port;\n\t\tnewsk->sk_write_space = sk_stream_write_space;\n\n\t\tnewicsk->icsk_retransmits = 0;\n\t\tnewicsk->icsk_backoff\t  = 0;\n\t\tnewicsk->icsk_probes_out  = 0;\n\n\t\t/* Deinitialize accept_queue to trap illegal accesses. */\n\t\tmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\n\n\t\tsecurity_inet_csk_clone(newsk, req);\n\t}\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(inet_csk_clone);\n\n/*\n * At this point, there should be no process reference to this\n * socket, and thus no user references at all.  Therefore we\n * can assume the socket waitqueue is inactive and nobody will\n * try to jump onto it.\n */\nvoid inet_csk_destroy_sock(struct sock *sk)\n{\n\tWARN_ON(sk->sk_state != TCP_CLOSE);\n\tWARN_ON(!sock_flag(sk, SOCK_DEAD));\n\n\t/* It cannot be in hash table! */\n\tWARN_ON(!sk_unhashed(sk));\n\n\t/* If it has not 0 inet_sk(sk)->inet_num, it must be bound */\n\tWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\n\n\tsk->sk_prot->destroy(sk);\n\n\tsk_stream_kill_queues(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\n\tpercpu_counter_dec(sk->sk_prot->orphan_count);\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(inet_csk_destroy_sock);\n\nint inet_csk_listen_start(struct sock *sk, const int nr_table_entries)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint rc = reqsk_queue_alloc(&icsk->icsk_accept_queue, nr_table_entries);\n\n\tif (rc != 0)\n\t\treturn rc;\n\n\tsk->sk_max_ack_backlog = 0;\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tsk->sk_state = TCP_LISTEN;\n\tif (!sk->sk_prot->get_port(sk, inet->inet_num)) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\tsk->sk_prot->hash(sk);\n\n\t\treturn 0;\n\t}\n\n\tsk->sk_state = TCP_CLOSE;\n\t__reqsk_queue_destroy(&icsk->icsk_accept_queue);\n\treturn -EADDRINUSE;\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_start);\n\n/*\n *\tThis routine closes sockets which have been at least partially\n *\topened, but not yet accepted.\n */\nvoid inet_csk_listen_stop(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock *acc_req;\n\tstruct request_sock *req;\n\n\tinet_csk_delete_keepalive_timer(sk);\n\n\t/* make all the listen_opt local to us */\n\tacc_req = reqsk_queue_yank_acceptq(&icsk->icsk_accept_queue);\n\n\t/* Following specs, it would be better either to send FIN\n\t * (and enter FIN-WAIT-1, it is normal close)\n\t * or to send active reset (abort).\n\t * Certainly, it is pretty dangerous while synflood, but it is\n\t * bad justification for our negligence 8)\n\t * To be honest, we are not able to make either\n\t * of the variants now.\t\t\t--ANK\n\t */\n\treqsk_queue_destroy(&icsk->icsk_accept_queue);\n\n\twhile ((req = acc_req) != NULL) {\n\t\tstruct sock *child = req->sk;\n\n\t\tacc_req = req->dl_next;\n\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(child);\n\t\tWARN_ON(sock_owned_by_user(child));\n\t\tsock_hold(child);\n\n\t\tsk->sk_prot->disconnect(child, O_NONBLOCK);\n\n\t\tsock_orphan(child);\n\n\t\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\t\tinet_csk_destroy_sock(child);\n\n\t\tbh_unlock_sock(child);\n\t\tlocal_bh_enable();\n\t\tsock_put(child);\n\n\t\tsk_acceptq_removed(sk);\n\t\t__reqsk_free(req);\n\t}\n\tWARN_ON(sk->sk_ack_backlog);\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_stop);\n\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tsin->sin_family\t\t= AF_INET;\n\tsin->sin_addr.s_addr\t= inet->inet_daddr;\n\tsin->sin_port\t\t= inet->inet_dport;\n}\nEXPORT_SYMBOL_GPL(inet_csk_addr2sockaddr);\n\n#ifdef CONFIG_COMPAT\nint inet_csk_compat_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_getsockopt != NULL)\n\t\treturn icsk->icsk_af_ops->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_getsockopt);\n\nint inet_csk_compat_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_setsockopt != NULL)\n\t\treturn icsk->icsk_af_ops->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_setsockopt);\n#endif\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe options processing module for ip.c\n *\n * Authors:\tA.N.Kuznetsov\n *\n */\n\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/ip.h>\n#include <linux/icmp.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/route.h>\n#include <net/cipso_ipv4.h>\n\n/*\n * Write options to IP header, record destination address to\n * source route option, address of outgoing interface\n * (we should already know it, so that this  function is allowed be\n * called only after routing decision) and timestamp,\n * if we originate this datagram.\n *\n * daddr is real destination address, next hop is recorded in IP header.\n * saddr is address of outgoing interface.\n */\n\nvoid ip_options_build(struct sk_buff * skb, struct ip_options * opt,\n\t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n{\n\tunsigned char *iph = skb_network_header(skb);\n\n\tmemcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));\n\tmemcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);\n\topt = &(IPCB(skb)->opt);\n\n\tif (opt->srr)\n\t\tmemcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);\n\n\tif (!is_frag) {\n\t\tif (opt->rr_needaddr)\n\t\t\tip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);\n\t\tif (opt->ts_needaddr)\n\t\t\tip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);\n\t\tif (opt->ts_needtime) {\n\t\t\tstruct timespec tv;\n\t\t\t__be32 midtime;\n\t\t\tgetnstimeofday(&tv);\n\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\tmemcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);\n\t\t}\n\t\treturn;\n\t}\n\tif (opt->rr) {\n\t\tmemset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);\n\t\topt->rr = 0;\n\t\topt->rr_needaddr = 0;\n\t}\n\tif (opt->ts) {\n\t\tmemset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);\n\t\topt->ts = 0;\n\t\topt->ts_needaddr = opt->ts_needtime = 0;\n\t}\n}\n\n/*\n * Provided (sopt, skb) points to received options,\n * build in dopt compiled option set appropriate for answering.\n * i.e. invert SRR option, copy anothers,\n * and grab room in RR/TS options.\n *\n * NOTE: dopt cannot point to skb.\n */\n\nint ip_options_echo(struct ip_options * dopt, struct sk_buff * skb)\n{\n\tstruct ip_options *sopt;\n\tunsigned char *sptr, *dptr;\n\tint soffset, doffset;\n\tint\toptlen;\n\t__be32\tdaddr;\n\n\tmemset(dopt, 0, sizeof(struct ip_options));\n\n\tsopt = &(IPCB(skb)->opt);\n\n\tif (sopt->optlen == 0) {\n\t\tdopt->optlen = 0;\n\t\treturn 0;\n\t}\n\n\tsptr = skb_network_header(skb);\n\tdptr = dopt->__data;\n\n\tdaddr = skb_rtable(skb)->rt_spec_dst;\n\n\tif (sopt->rr) {\n\t\toptlen  = sptr[sopt->rr+1];\n\t\tsoffset = sptr[sopt->rr+2];\n\t\tdopt->rr = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->rr, optlen);\n\t\tif (sopt->rr_needaddr && soffset <= optlen) {\n\t\t\tif (soffset + 3 > optlen)\n\t\t\t\treturn -EINVAL;\n\t\t\tdptr[2] = soffset + 4;\n\t\t\tdopt->rr_needaddr = 1;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->ts) {\n\t\toptlen = sptr[sopt->ts+1];\n\t\tsoffset = sptr[sopt->ts+2];\n\t\tdopt->ts = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->ts, optlen);\n\t\tif (soffset <= optlen) {\n\t\t\tif (sopt->ts_needaddr) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tdopt->ts_needaddr = 1;\n\t\t\t\tsoffset += 4;\n\t\t\t}\n\t\t\tif (sopt->ts_needtime) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif ((dptr[3]&0xF) != IPOPT_TS_PRESPEC) {\n\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\tsoffset += 4;\n\t\t\t\t} else {\n\t\t\t\t\tdopt->ts_needtime = 0;\n\n\t\t\t\t\tif (soffset + 7 <= optlen) {\n\t\t\t\t\t\t__be32 addr;\n\n\t\t\t\t\t\tmemcpy(&addr, dptr+soffset-1, 4);\n\t\t\t\t\t\tif (inet_addr_type(dev_net(skb_dst(skb)->dev), addr) != RTN_UNICAST) {\n\t\t\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\t\t\tsoffset += 8;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tdptr[2] = soffset;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->srr) {\n\t\tunsigned char * start = sptr+sopt->srr;\n\t\t__be32 faddr;\n\n\t\toptlen  = start[1];\n\t\tsoffset = start[2];\n\t\tdoffset = 0;\n\t\tif (soffset > optlen)\n\t\t\tsoffset = optlen + 1;\n\t\tsoffset -= 4;\n\t\tif (soffset > 3) {\n\t\t\tmemcpy(&faddr, &start[soffset-1], 4);\n\t\t\tfor (soffset-=4, doffset=4; soffset > 3; soffset-=4, doffset+=4)\n\t\t\t\tmemcpy(&dptr[doffset-1], &start[soffset-1], 4);\n\t\t\t/*\n\t\t\t * RFC1812 requires to fix illegal source routes.\n\t\t\t */\n\t\t\tif (memcmp(&ip_hdr(skb)->saddr,\n\t\t\t\t   &start[soffset + 3], 4) == 0)\n\t\t\t\tdoffset -= 4;\n\t\t}\n\t\tif (doffset > 3) {\n\t\t\tmemcpy(&start[doffset-1], &daddr, 4);\n\t\t\tdopt->faddr = faddr;\n\t\t\tdptr[0] = start[0];\n\t\t\tdptr[1] = doffset+3;\n\t\t\tdptr[2] = 4;\n\t\t\tdptr += doffset+3;\n\t\t\tdopt->srr = dopt->optlen + sizeof(struct iphdr);\n\t\t\tdopt->optlen += doffset+3;\n\t\t\tdopt->is_strictroute = sopt->is_strictroute;\n\t\t}\n\t}\n\tif (sopt->cipso) {\n\t\toptlen  = sptr[sopt->cipso+1];\n\t\tdopt->cipso = dopt->optlen+sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->cipso, optlen);\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\twhile (dopt->optlen & 3) {\n\t\t*dptr++ = IPOPT_END;\n\t\tdopt->optlen++;\n\t}\n\treturn 0;\n}\n\n/*\n *\tOptions \"fragmenting\", just fill options not\n *\tallowed in fragments with NOOPs.\n *\tSimple and stupid 8), but the most efficient way.\n */\n\nvoid ip_options_fragment(struct sk_buff * skb)\n{\n\tunsigned char *optptr = skb_network_header(skb) + sizeof(struct iphdr);\n\tstruct ip_options * opt = &(IPCB(skb)->opt);\n\tint  l = opt->optlen;\n\tint  optlen;\n\n\twhile (l > 0) {\n\t\tswitch (*optptr) {\n\t\tcase IPOPT_END:\n\t\t\treturn;\n\t\tcase IPOPT_NOOP:\n\t\t\tl--;\n\t\t\toptptr++;\n\t\t\tcontinue;\n\t\t}\n\t\toptlen = optptr[1];\n\t\tif (optlen<2 || optlen>l)\n\t\t  return;\n\t\tif (!IPOPT_COPIED(*optptr))\n\t\t\tmemset(optptr, IPOPT_NOOP, optlen);\n\t\tl -= optlen;\n\t\toptptr += optlen;\n\t}\n\topt->ts = 0;\n\topt->rr = 0;\n\topt->rr_needaddr = 0;\n\topt->ts_needaddr = 0;\n\topt->ts_needtime = 0;\n}\n\n/*\n * Verify options and fill pointers in struct options.\n * Caller should clear *opt, and set opt->data.\n * If opt == NULL, then skb->data should point to IP header.\n */\n\nint ip_options_compile(struct net *net,\n\t\t       struct ip_options * opt, struct sk_buff * skb)\n{\n\tint l;\n\tunsigned char * iph;\n\tunsigned char * optptr;\n\tint optlen;\n\tunsigned char * pp_ptr = NULL;\n\tstruct rtable *rt = NULL;\n\n\tif (skb != NULL) {\n\t\trt = skb_rtable(skb);\n\t\toptptr = (unsigned char *)&(ip_hdr(skb)[1]);\n\t} else\n\t\toptptr = opt->__data;\n\tiph = optptr - sizeof(struct iphdr);\n\n\tfor (l = opt->optlen; l > 0; ) {\n\t\tswitch (*optptr) {\n\t\t      case IPOPT_END:\n\t\t\tfor (optptr++, l--; l>0; optptr++, l--) {\n\t\t\t\tif (*optptr != IPOPT_END) {\n\t\t\t\t\t*optptr = IPOPT_END;\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgoto eol;\n\t\t      case IPOPT_NOOP:\n\t\t\tl--;\n\t\t\toptptr++;\n\t\t\tcontinue;\n\t\t}\n\t\toptlen = optptr[1];\n\t\tif (optlen<2 || optlen>l) {\n\t\t\tpp_ptr = optptr;\n\t\t\tgoto error;\n\t\t}\n\t\tswitch (*optptr) {\n\t\t      case IPOPT_SSRR:\n\t\t      case IPOPT_LSRR:\n\t\t\tif (optlen < 3) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] < 4) {\n\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/* NB: cf RFC-1812 5.2.4.1 */\n\t\t\tif (opt->srr) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (!skb) {\n\t\t\t\tif (optptr[2] != 4 || optlen < 7 || ((optlen-3) & 3)) {\n\t\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tmemcpy(&opt->faddr, &optptr[3], 4);\n\t\t\t\tif (optlen > 7)\n\t\t\t\t\tmemmove(&optptr[3], &optptr[7], optlen-7);\n\t\t\t}\n\t\t\topt->is_strictroute = (optptr[0] == IPOPT_SSRR);\n\t\t\topt->srr = optptr - iph;\n\t\t\tbreak;\n\t\t      case IPOPT_RR:\n\t\t\tif (opt->rr) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optlen < 3) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] < 4) {\n\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] <= optlen) {\n\t\t\t\tif (optptr[2]+3 > optlen) {\n\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tif (rt) {\n\t\t\t\t\tmemcpy(&optptr[optptr[2]-1], &rt->rt_spec_dst, 4);\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t\toptptr[2] += 4;\n\t\t\t\topt->rr_needaddr = 1;\n\t\t\t}\n\t\t\topt->rr = optptr - iph;\n\t\t\tbreak;\n\t\t      case IPOPT_TIMESTAMP:\n\t\t\tif (opt->ts) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optlen < 4) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] < 5) {\n\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] <= optlen) {\n\t\t\t\t__be32 *timeptr = NULL;\n\t\t\t\tif (optptr[2]+3 > optptr[1]) {\n\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tswitch (optptr[3]&0xF) {\n\t\t\t\t      case IPOPT_TS_TSONLY:\n\t\t\t\t\topt->ts = optptr - iph;\n\t\t\t\t\tif (skb)\n\t\t\t\t\t\ttimeptr = (__be32*)&optptr[optptr[2]-1];\n\t\t\t\t\topt->ts_needtime = 1;\n\t\t\t\t\toptptr[2] += 4;\n\t\t\t\t\tbreak;\n\t\t\t\t      case IPOPT_TS_TSANDADDR:\n\t\t\t\t\tif (optptr[2]+7 > optptr[1]) {\n\t\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\topt->ts = optptr - iph;\n\t\t\t\t\tif (rt)  {\n\t\t\t\t\t\tmemcpy(&optptr[optptr[2]-1], &rt->rt_spec_dst, 4);\n\t\t\t\t\t\ttimeptr = (__be32*)&optptr[optptr[2]+3];\n\t\t\t\t\t}\n\t\t\t\t\topt->ts_needaddr = 1;\n\t\t\t\t\topt->ts_needtime = 1;\n\t\t\t\t\toptptr[2] += 8;\n\t\t\t\t\tbreak;\n\t\t\t\t      case IPOPT_TS_PRESPEC:\n\t\t\t\t\tif (optptr[2]+7 > optptr[1]) {\n\t\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\topt->ts = optptr - iph;\n\t\t\t\t\t{\n\t\t\t\t\t\t__be32 addr;\n\t\t\t\t\t\tmemcpy(&addr, &optptr[optptr[2]-1], 4);\n\t\t\t\t\t\tif (inet_addr_type(net, addr) == RTN_UNICAST)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tif (skb)\n\t\t\t\t\t\t\ttimeptr = (__be32*)&optptr[optptr[2]+3];\n\t\t\t\t\t}\n\t\t\t\t\topt->ts_needtime = 1;\n\t\t\t\t\toptptr[2] += 8;\n\t\t\t\t\tbreak;\n\t\t\t\t      default:\n\t\t\t\t\tif (!skb && !capable(CAP_NET_RAW)) {\n\t\t\t\t\t\tpp_ptr = optptr + 3;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (timeptr) {\n\t\t\t\t\tstruct timespec tv;\n\t\t\t\t\t__be32  midtime;\n\t\t\t\t\tgetnstimeofday(&tv);\n\t\t\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\t\t\tmemcpy(timeptr, &midtime, sizeof(__be32));\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tunsigned overflow = optptr[3]>>4;\n\t\t\t\tif (overflow == 15) {\n\t\t\t\t\tpp_ptr = optptr + 3;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\topt->ts = optptr - iph;\n\t\t\t\tif (skb) {\n\t\t\t\t\toptptr[3] = (optptr[3]&0xF)|((overflow+1)<<4);\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t      case IPOPT_RA:\n\t\t\tif (optlen < 4) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] == 0 && optptr[3] == 0)\n\t\t\t\topt->router_alert = optptr - iph;\n\t\t\tbreak;\n\t\t      case IPOPT_CIPSO:\n\t\t\tif ((!skb && !capable(CAP_NET_RAW)) || opt->cipso) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\topt->cipso = optptr - iph;\n\t\t\tif (cipso_v4_validate(skb, &optptr)) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\t      case IPOPT_SEC:\n\t\t      case IPOPT_SID:\n\t\t      default:\n\t\t\tif (!skb && !capable(CAP_NET_RAW)) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tl -= optlen;\n\t\toptptr += optlen;\n\t}\n\neol:\n\tif (!pp_ptr)\n\t\treturn 0;\n\nerror:\n\tif (skb) {\n\t\ticmp_send(skb, ICMP_PARAMETERPROB, 0, htonl((pp_ptr-iph)<<24));\n\t}\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(ip_options_compile);\n\n/*\n *\tUndo all the changes done by ip_options_compile().\n */\n\nvoid ip_options_undo(struct ip_options * opt)\n{\n\tif (opt->srr) {\n\t\tunsigned  char * optptr = opt->__data+opt->srr-sizeof(struct  iphdr);\n\t\tmemmove(optptr+7, optptr+3, optptr[1]-7);\n\t\tmemcpy(optptr+3, &opt->faddr, 4);\n\t}\n\tif (opt->rr_needaddr) {\n\t\tunsigned  char * optptr = opt->__data+opt->rr-sizeof(struct  iphdr);\n\t\toptptr[2] -= 4;\n\t\tmemset(&optptr[optptr[2]-1], 0, 4);\n\t}\n\tif (opt->ts) {\n\t\tunsigned  char * optptr = opt->__data+opt->ts-sizeof(struct  iphdr);\n\t\tif (opt->ts_needtime) {\n\t\t\toptptr[2] -= 4;\n\t\t\tmemset(&optptr[optptr[2]-1], 0, 4);\n\t\t\tif ((optptr[3]&0xF) == IPOPT_TS_PRESPEC)\n\t\t\t\toptptr[2] -= 4;\n\t\t}\n\t\tif (opt->ts_needaddr) {\n\t\t\toptptr[2] -= 4;\n\t\t\tmemset(&optptr[optptr[2]-1], 0, 4);\n\t\t}\n\t}\n}\n\nstatic struct ip_options *ip_options_get_alloc(const int optlen)\n{\n\treturn kzalloc(sizeof(struct ip_options) + ((optlen + 3) & ~3),\n\t\t       GFP_KERNEL);\n}\n\nstatic int ip_options_get_finish(struct net *net, struct ip_options **optp,\n\t\t\t\t struct ip_options *opt, int optlen)\n{\n\twhile (optlen & 3)\n\t\topt->__data[optlen++] = IPOPT_END;\n\topt->optlen = optlen;\n\tif (optlen && ip_options_compile(net, opt, NULL)) {\n\t\tkfree(opt);\n\t\treturn -EINVAL;\n\t}\n\tkfree(*optp);\n\t*optp = opt;\n\treturn 0;\n}\n\nint ip_options_get_from_user(struct net *net, struct ip_options **optp,\n\t\t\t     unsigned char __user *data, int optlen)\n{\n\tstruct ip_options *opt = ip_options_get_alloc(optlen);\n\n\tif (!opt)\n\t\treturn -ENOMEM;\n\tif (optlen && copy_from_user(opt->__data, data, optlen)) {\n\t\tkfree(opt);\n\t\treturn -EFAULT;\n\t}\n\treturn ip_options_get_finish(net, optp, opt, optlen);\n}\n\nint ip_options_get(struct net *net, struct ip_options **optp,\n\t\t   unsigned char *data, int optlen)\n{\n\tstruct ip_options *opt = ip_options_get_alloc(optlen);\n\n\tif (!opt)\n\t\treturn -ENOMEM;\n\tif (optlen)\n\t\tmemcpy(opt->__data, data, optlen);\n\treturn ip_options_get_finish(net, optp, opt, optlen);\n}\n\nvoid ip_forward_options(struct sk_buff *skb)\n{\n\tstruct   ip_options * opt\t= &(IPCB(skb)->opt);\n\tunsigned char * optptr;\n\tstruct rtable *rt = skb_rtable(skb);\n\tunsigned char *raw = skb_network_header(skb);\n\n\tif (opt->rr_needaddr) {\n\t\toptptr = (unsigned char *)raw + opt->rr;\n\t\tip_rt_get_source(&optptr[optptr[2]-5], rt);\n\t\topt->is_changed = 1;\n\t}\n\tif (opt->srr_is_hit) {\n\t\tint srrptr, srrspace;\n\n\t\toptptr = raw + opt->srr;\n\n\t\tfor ( srrptr=optptr[2], srrspace = optptr[1];\n\t\t     srrptr <= srrspace;\n\t\t     srrptr += 4\n\t\t     ) {\n\t\t\tif (srrptr + 3 > srrspace)\n\t\t\t\tbreak;\n\t\t\tif (memcmp(&rt->rt_dst, &optptr[srrptr-1], 4) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (srrptr + 3 <= srrspace) {\n\t\t\topt->is_changed = 1;\n\t\t\tip_rt_get_source(&optptr[srrptr-1], rt);\n\t\t\tip_hdr(skb)->daddr = rt->rt_dst;\n\t\t\toptptr[2] = srrptr+4;\n\t\t} else if (net_ratelimit())\n\t\t\tprintk(KERN_CRIT \"ip_forward(): Argh! Destination lost!\\n\");\n\t\tif (opt->ts_needaddr) {\n\t\t\toptptr = raw + opt->ts;\n\t\t\tip_rt_get_source(&optptr[optptr[2]-9], rt);\n\t\t\topt->is_changed = 1;\n\t\t}\n\t}\n\tif (opt->is_changed) {\n\t\topt->is_changed = 0;\n\t\tip_send_check(ip_hdr(skb));\n\t}\n}\n\nint ip_options_rcv_srr(struct sk_buff *skb)\n{\n\tstruct ip_options *opt = &(IPCB(skb)->opt);\n\tint srrspace, srrptr;\n\t__be32 nexthop;\n\tstruct iphdr *iph = ip_hdr(skb);\n\tunsigned char *optptr = skb_network_header(skb) + opt->srr;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct rtable *rt2;\n\tunsigned long orefdst;\n\tint err;\n\n\tif (!opt->srr || !rt)\n\t\treturn 0;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn -EINVAL;\n\tif (rt->rt_type == RTN_UNICAST) {\n\t\tif (!opt->is_strictroute)\n\t\t\treturn 0;\n\t\ticmp_send(skb, ICMP_PARAMETERPROB, 0, htonl(16<<24));\n\t\treturn -EINVAL;\n\t}\n\tif (rt->rt_type != RTN_LOCAL)\n\t\treturn -EINVAL;\n\n\tfor (srrptr=optptr[2], srrspace = optptr[1]; srrptr <= srrspace; srrptr += 4) {\n\t\tif (srrptr + 3 > srrspace) {\n\t\t\ticmp_send(skb, ICMP_PARAMETERPROB, 0, htonl((opt->srr+2)<<24));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmemcpy(&nexthop, &optptr[srrptr-1], 4);\n\n\t\torefdst = skb->_skb_refdst;\n\t\tskb_dst_set(skb, NULL);\n\t\terr = ip_route_input(skb, nexthop, iph->saddr, iph->tos, skb->dev);\n\t\trt2 = skb_rtable(skb);\n\t\tif (err || (rt2->rt_type != RTN_UNICAST && rt2->rt_type != RTN_LOCAL)) {\n\t\t\tskb_dst_drop(skb);\n\t\t\tskb->_skb_refdst = orefdst;\n\t\t\treturn -EINVAL;\n\t\t}\n\t\trefdst_drop(orefdst);\n\t\tif (rt2->rt_type != RTN_LOCAL)\n\t\t\tbreak;\n\t\t/* Superfast 8) loopback forward */\n\t\tmemcpy(&iph->daddr, &optptr[srrptr-1], 4);\n\t\topt->is_changed = 1;\n\t}\n\tif (srrptr <= srrspace) {\n\t\topt->srr_is_hit = 1;\n\t\topt->is_changed = 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ip_options_rcv_srr);\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe Internet Protocol (IP) output module.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tDonald Becker, <becker@super.org>\n *\t\tAlan Cox, <Alan.Cox@linux.org>\n *\t\tRichard Underwood\n *\t\tStefan Becker, <stefanb@yello.ping.de>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n *\tSee ip_input.c for original log\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tMissing nonblock feature in ip_build_xmit.\n *\t\tMike Kilburn\t:\thtons() missing in ip_build_xmit.\n *\t\tBradford Johnson:\tFix faulty handling of some frames when\n *\t\t\t\t\tno route is found.\n *\t\tAlexander Demenshin:\tMissing sk/skb free in ip_queue_xmit\n *\t\t\t\t\t(in case if packet not accepted by\n *\t\t\t\t\toutput firewall rules)\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tAlexey Kuznetsov:\tuse new route cache\n *\t\tAndi Kleen:\t\tFix broken PMTU recovery and remove\n *\t\t\t\t\tsome redundant tests.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tAndi Kleen\t: \tReplace ip_reply with ip_send_reply.\n *\t\tAndi Kleen\t:\tSplit fast and slow ip_build_xmit path\n *\t\t\t\t\tfor decreased register pressure on x86\n *\t\t\t\t\tand more readibility.\n *\t\tMarc Boucher\t:\tWhen call_out_firewall returns FW_QUEUE,\n *\t\t\t\t\tsilently drop skb instead of failing with -EPERM.\n *\t\tDetlev Wengorz\t:\tCopy protocol for fragments.\n *\t\tHirokazu Takahashi:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\t\tHirokazu Takahashi:\tsendfile() on UDP works now.\n */\n\n#include <asm/uaccess.h>\n#include <asm/system.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n\n#include <net/snmp.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/route.h>\n#include <net/xfrm.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/arp.h>\n#include <net/icmp.h>\n#include <net/checksum.h>\n#include <net/inetpeer.h>\n#include <linux/igmp.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/netfilter_bridge.h>\n#include <linux/mroute.h>\n#include <linux/netlink.h>\n#include <linux/tcp.h>\n\nint sysctl_ip_default_ttl __read_mostly = IPDEFTTL;\nEXPORT_SYMBOL(sysctl_ip_default_ttl);\n\n/* Generate a checksum for an outgoing IP datagram. */\n__inline__ void ip_send_check(struct iphdr *iph)\n{\n\tiph->check = 0;\n\tiph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);\n}\nEXPORT_SYMBOL(ip_send_check);\n\nint __ip_local_out(struct sk_buff *skb)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tiph->tot_len = htons(skb->len);\n\tip_send_check(iph);\n\treturn nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t       skb_dst(skb)->dev, dst_output);\n}\n\nint ip_local_out(struct sk_buff *skb)\n{\n\tint err;\n\n\terr = __ip_local_out(skb);\n\tif (likely(err == 1))\n\t\terr = dst_output(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip_local_out);\n\n/* dev_loopback_xmit for use with netfilter. */\nstatic int ip_dev_loopback_xmit(struct sk_buff *newskb)\n{\n\tskb_reset_mac_header(newskb);\n\t__skb_pull(newskb, skb_network_offset(newskb));\n\tnewskb->pkt_type = PACKET_LOOPBACK;\n\tnewskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(newskb));\n\tnetif_rx_ni(newskb);\n\treturn 0;\n}\n\nstatic inline int ip_select_ttl(struct inet_sock *inet, struct dst_entry *dst)\n{\n\tint ttl = inet->uc_ttl;\n\n\tif (ttl < 0)\n\t\tttl = ip4_dst_hoplimit(dst);\n\treturn ttl;\n}\n\n/*\n *\t\tAdd an ip header to a skbuff and send it out.\n *\n */\nint ip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,\n\t\t\t  __be32 saddr, __be32 daddr, struct ip_options *opt)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct iphdr *iph;\n\n\t/* Build the IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tiph->version  = 4;\n\tiph->ihl      = 5;\n\tiph->tos      = inet->tos;\n\tif (ip_dont_fragment(sk, &rt->dst))\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->daddr    = rt->rt_dst;\n\tiph->saddr    = rt->rt_src;\n\tiph->protocol = sk->sk_protocol;\n\tip_select_ident(iph, &rt->dst, sk);\n\n\tif (opt && opt->optlen) {\n\t\tiph->ihl += opt->optlen>>2;\n\t\tip_options_build(skb, opt, daddr, rt, 0);\n\t}\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\t/* Send it out. */\n\treturn ip_local_out(skb);\n}\nEXPORT_SYMBOL_GPL(ip_build_and_send_pkt);\n\nstatic inline int ip_finish_output2(struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct rtable *rt = (struct rtable *)dst;\n\tstruct net_device *dev = dst->dev;\n\tunsigned int hh_len = LL_RESERVED_SPACE(dev);\n\n\tif (rt->rt_type == RTN_MULTICAST) {\n\t\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTMCAST, skb->len);\n\t} else if (rt->rt_type == RTN_BROADCAST)\n\t\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTBCAST, skb->len);\n\n\t/* Be paranoid, rather than too clever. */\n\tif (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {\n\t\tstruct sk_buff *skb2;\n\n\t\tskb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(dev));\n\t\tif (skb2 == NULL) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(skb2, skb->sk);\n\t\tkfree_skb(skb);\n\t\tskb = skb2;\n\t}\n\n\tif (dst->hh)\n\t\treturn neigh_hh_output(dst->hh, skb);\n\telse if (dst->neighbour)\n\t\treturn dst->neighbour->output(skb);\n\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"ip_finish_output2: No header cache and no neighbour!\\n\");\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic inline int ip_skb_dst_mtu(struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = skb->sk ? inet_sk(skb->sk) : NULL;\n\n\treturn (inet && inet->pmtudisc == IP_PMTUDISC_PROBE) ?\n\t       skb_dst(skb)->dev->mtu : dst_mtu(skb_dst(skb));\n}\n\nstatic int ip_finish_output(struct sk_buff *skb)\n{\n#if defined(CONFIG_NETFILTER) && defined(CONFIG_XFRM)\n\t/* Policy lookup after SNAT yielded a new policy */\n\tif (skb_dst(skb)->xfrm != NULL) {\n\t\tIPCB(skb)->flags |= IPSKB_REROUTED;\n\t\treturn dst_output(skb);\n\t}\n#endif\n\tif (skb->len > ip_skb_dst_mtu(skb) && !skb_is_gso(skb))\n\t\treturn ip_fragment(skb, ip_finish_output2);\n\telse\n\t\treturn ip_finish_output2(skb);\n}\n\nint ip_mc_output(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net_device *dev = rt->dst.dev;\n\n\t/*\n\t *\tIf the indicated interface is up and running, send the packet.\n\t */\n\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb->len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_IP);\n\n\t/*\n\t *\tMulticasts are looped back for other local users\n\t */\n\n\tif (rt->rt_flags&RTCF_MULTICAST) {\n\t\tif (sk_mc_loop(sk)\n#ifdef CONFIG_IP_MROUTE\n\t\t/* Small optimization: do not loopback not local frames,\n\t\t   which returned after forwarding; they will be  dropped\n\t\t   by ip_mr_input in any case.\n\t\t   Note, that local frames are looped back to be delivered\n\t\t   to local recipients.\n\n\t\t   This check is duplicated in ip_mr_input at the moment.\n\t\t */\n\t\t    &&\n\t\t    ((rt->rt_flags & RTCF_LOCAL) ||\n\t\t     !(IPCB(skb)->flags & IPSKB_FORWARDED))\n#endif\n\t\t   ) {\n\t\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\t\t\tif (newskb)\n\t\t\t\tNF_HOOK(NFPROTO_IPV4, NF_INET_POST_ROUTING,\n\t\t\t\t\tnewskb, NULL, newskb->dev,\n\t\t\t\t\tip_dev_loopback_xmit);\n\t\t}\n\n\t\t/* Multicasts with ttl 0 must not go beyond the host */\n\n\t\tif (ip_hdr(skb)->ttl == 0) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (rt->rt_flags&RTCF_BROADCAST) {\n\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (newskb)\n\t\t\tNF_HOOK(NFPROTO_IPV4, NF_INET_POST_ROUTING, newskb,\n\t\t\t\tNULL, newskb->dev, ip_dev_loopback_xmit);\n\t}\n\n\treturn NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL,\n\t\t\t    skb->dev, ip_finish_output,\n\t\t\t    !(IPCB(skb)->flags & IPSKB_REROUTED));\n}\n\nint ip_output(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb_dst(skb)->dev;\n\n\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb->len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_IP);\n\n\treturn NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,\n\t\t\t    ip_finish_output,\n\t\t\t    !(IPCB(skb)->flags & IPSKB_REROUTED));\n}\n\nint ip_queue_xmit(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options *opt = inet->opt;\n\tstruct rtable *rt;\n\tstruct iphdr *iph;\n\tint res;\n\n\t/* Skip all of this if the packet is already routed,\n\t * f.e. by something like SCTP.\n\t */\n\trcu_read_lock();\n\trt = skb_rtable(skb);\n\tif (rt != NULL)\n\t\tgoto packet_routed;\n\n\t/* Make sure we can route this packet. */\n\trt = (struct rtable *)__sk_dst_check(sk, 0);\n\tif (rt == NULL) {\n\t\t__be32 daddr;\n\n\t\t/* Use correct destination address if we have options. */\n\t\tdaddr = inet->inet_daddr;\n\t\tif(opt && opt->srr)\n\t\t\tdaddr = opt->faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport,\n\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol,\n\t\t\t\t\t   RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set_noref(skb, &rt->dst);\n\npacket_routed:\n\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto no_route;\n\n\t/* OK, we know where to send it, allocate and build IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\t*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (inet->tos & 0xff));\n\tif (ip_dont_fragment(sk, &rt->dst) && !skb->local_df)\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->protocol = sk->sk_protocol;\n\tiph->saddr    = rt->rt_src;\n\tiph->daddr    = rt->rt_dst;\n\t/* Transport layer set skb->h.foo itself. */\n\n\tif (opt && opt->optlen) {\n\t\tiph->ihl += opt->optlen >> 2;\n\t\tip_options_build(skb, opt, inet->inet_daddr, rt, 0);\n\t}\n\n\tip_select_ident_more(iph, &rt->dst, sk,\n\t\t\t     (skb_shinfo(skb)->gso_segs ?: 1) - 1);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tres = ip_local_out(skb);\n\trcu_read_unlock();\n\treturn res;\n\nno_route:\n\trcu_read_unlock();\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}\nEXPORT_SYMBOL(ip_queue_xmit);\n\n\nstatic void ip_copy_metadata(struct sk_buff *to, struct sk_buff *from)\n{\n\tto->pkt_type = from->pkt_type;\n\tto->priority = from->priority;\n\tto->protocol = from->protocol;\n\tskb_dst_drop(to);\n\tskb_dst_copy(to, from);\n\tto->dev = from->dev;\n\tto->mark = from->mark;\n\n\t/* Copy the flags to each fragment. */\n\tIPCB(to)->flags = IPCB(from)->flags;\n\n#ifdef CONFIG_NET_SCHED\n\tto->tc_index = from->tc_index;\n#endif\n\tnf_copy(to, from);\n#if defined(CONFIG_NETFILTER_XT_TARGET_TRACE) || \\\n    defined(CONFIG_NETFILTER_XT_TARGET_TRACE_MODULE)\n\tto->nf_trace = from->nf_trace;\n#endif\n#if defined(CONFIG_IP_VS) || defined(CONFIG_IP_VS_MODULE)\n\tto->ipvs_property = from->ipvs_property;\n#endif\n\tskb_copy_secmark(to, from);\n}\n\n/*\n *\tThis IP datagram is too large to be sent in one piece.  Break it up into\n *\tsmaller pieces (each of size equal to IP header plus\n *\ta block of the data of the original IP data part) that will yet fit in a\n *\tsingle device frame, and queue such a frame for sending.\n */\n\nint ip_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *))\n{\n\tstruct iphdr *iph;\n\tint ptr;\n\tstruct net_device *dev;\n\tstruct sk_buff *skb2;\n\tunsigned int mtu, hlen, left, len, ll_rs;\n\tint offset;\n\t__be16 not_last_frag;\n\tstruct rtable *rt = skb_rtable(skb);\n\tint err = 0;\n\n\tdev = rt->dst.dev;\n\n\t/*\n\t *\tPoint into the IP datagram header.\n\t */\n\n\tiph = ip_hdr(skb);\n\n\tif (unlikely((iph->frag_off & htons(IP_DF)) && !skb->local_df)) {\n\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,\n\t\t\t  htonl(ip_skb_dst_mtu(skb)));\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t *\tSetup starting values.\n\t */\n\n\thlen = iph->ihl * 4;\n\tmtu = dst_mtu(&rt->dst) - hlen;\t/* Size of data space */\n#ifdef CONFIG_BRIDGE_NETFILTER\n\tif (skb->nf_bridge)\n\t\tmtu -= nf_bridge_mtu_reduction(skb);\n#endif\n\tIPCB(skb)->flags |= IPSKB_FRAG_COMPLETE;\n\n\t/* When frag_list is given, use it. First, check its validity:\n\t * some transformers could create wrong frag_list or break existing\n\t * one, it is not prohibited. In this case fall back to copying.\n\t *\n\t * LATER: this step can be merged to real generation of fragments,\n\t * we can switch to copy when see the first bad fragment.\n\t */\n\tif (skb_has_frag_list(skb)) {\n\t\tstruct sk_buff *frag, *frag2;\n\t\tint first_len = skb_pagelen(skb);\n\n\t\tif (first_len - hlen > mtu ||\n\t\t    ((first_len - hlen) & 7) ||\n\t\t    (iph->frag_off & htons(IP_MF|IP_OFFSET)) ||\n\t\t    skb_cloned(skb))\n\t\t\tgoto slow_path;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\t/* Correct geometry. */\n\t\t\tif (frag->len > mtu ||\n\t\t\t    ((frag->len & 7) && frag->next) ||\n\t\t\t    skb_headroom(frag) < hlen)\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\t/* Partially cloned skb? */\n\t\t\tif (skb_shared(frag))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\tBUG_ON(frag->sk);\n\t\t\tif (skb->sk) {\n\t\t\t\tfrag->sk = skb->sk;\n\t\t\t\tfrag->destructor = sock_wfree;\n\t\t\t}\n\t\t\tskb->truesize -= frag->truesize;\n\t\t}\n\n\t\t/* Everything is OK. Generate! */\n\n\t\terr = 0;\n\t\toffset = 0;\n\t\tfrag = skb_shinfo(skb)->frag_list;\n\t\tskb_frag_list_init(skb);\n\t\tskb->data_len = first_len - skb_headlen(skb);\n\t\tskb->len = first_len;\n\t\tiph->tot_len = htons(first_len);\n\t\tiph->frag_off = htons(IP_MF);\n\t\tip_send_check(iph);\n\n\t\tfor (;;) {\n\t\t\t/* Prepare header of the next frame,\n\t\t\t * before previous one went down. */\n\t\t\tif (frag) {\n\t\t\t\tfrag->ip_summed = CHECKSUM_NONE;\n\t\t\t\tskb_reset_transport_header(frag);\n\t\t\t\t__skb_push(frag, hlen);\n\t\t\t\tskb_reset_network_header(frag);\n\t\t\t\tmemcpy(skb_network_header(frag), iph, hlen);\n\t\t\t\tiph = ip_hdr(frag);\n\t\t\t\tiph->tot_len = htons(frag->len);\n\t\t\t\tip_copy_metadata(frag, skb);\n\t\t\t\tif (offset == 0)\n\t\t\t\t\tip_options_fragment(frag);\n\t\t\t\toffset += skb->len - hlen;\n\t\t\t\tiph->frag_off = htons(offset>>3);\n\t\t\t\tif (frag->next != NULL)\n\t\t\t\t\tiph->frag_off |= htons(IP_MF);\n\t\t\t\t/* Ready, complete checksum */\n\t\t\t\tip_send_check(iph);\n\t\t\t}\n\n\t\t\terr = output(skb);\n\n\t\t\tif (!err)\n\t\t\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGCREATES);\n\t\t\tif (err || !frag)\n\t\t\t\tbreak;\n\n\t\t\tskb = frag;\n\t\t\tfrag = skb->next;\n\t\t\tskb->next = NULL;\n\t\t}\n\n\t\tif (err == 0) {\n\t\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGOKS);\n\t\t\treturn 0;\n\t\t}\n\n\t\twhile (frag) {\n\t\t\tskb = frag->next;\n\t\t\tkfree_skb(frag);\n\t\t\tfrag = skb;\n\t\t}\n\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);\n\t\treturn err;\n\nslow_path_clean:\n\t\tskb_walk_frags(skb, frag2) {\n\t\t\tif (frag2 == frag)\n\t\t\t\tbreak;\n\t\t\tfrag2->sk = NULL;\n\t\t\tfrag2->destructor = NULL;\n\t\t\tskb->truesize += frag2->truesize;\n\t\t}\n\t}\n\nslow_path:\n\tleft = skb->len - hlen;\t\t/* Space per frame */\n\tptr = hlen;\t\t/* Where to start from */\n\n\t/* for bridged IP traffic encapsulated inside f.e. a vlan header,\n\t * we need to make room for the encapsulating header\n\t */\n\tll_rs = LL_RESERVED_SPACE_EXTRA(rt->dst.dev, nf_bridge_pad(skb));\n\n\t/*\n\t *\tFragment the datagram.\n\t */\n\n\toffset = (ntohs(iph->frag_off) & IP_OFFSET) << 3;\n\tnot_last_frag = iph->frag_off & htons(IP_MF);\n\n\t/*\n\t *\tKeep copying data until we run out.\n\t */\n\n\twhile (left > 0) {\n\t\tlen = left;\n\t\t/* IF: it doesn't fit, use 'mtu' - the data space left */\n\t\tif (len > mtu)\n\t\t\tlen = mtu;\n\t\t/* IF: we are not sending up to and including the packet end\n\t\t   then align the next start on an eight byte boundary */\n\t\tif (len < left)\t{\n\t\t\tlen &= ~7;\n\t\t}\n\t\t/*\n\t\t *\tAllocate buffer.\n\t\t */\n\n\t\tif ((skb2 = alloc_skb(len+hlen+ll_rs, GFP_ATOMIC)) == NULL) {\n\t\t\tNETDEBUG(KERN_INFO \"IP: frag: no memory for new fragment!\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t *\tSet up data on packet\n\t\t */\n\n\t\tip_copy_metadata(skb2, skb);\n\t\tskb_reserve(skb2, ll_rs);\n\t\tskb_put(skb2, len + hlen);\n\t\tskb_reset_network_header(skb2);\n\t\tskb2->transport_header = skb2->network_header + hlen;\n\n\t\t/*\n\t\t *\tCharge the memory for the fragment to any owner\n\t\t *\tit might possess\n\t\t */\n\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(skb2, skb->sk);\n\n\t\t/*\n\t\t *\tCopy the packet header into the new buffer.\n\t\t */\n\n\t\tskb_copy_from_linear_data(skb, skb_network_header(skb2), hlen);\n\n\t\t/*\n\t\t *\tCopy a block of the IP datagram.\n\t\t */\n\t\tif (skb_copy_bits(skb, ptr, skb_transport_header(skb2), len))\n\t\t\tBUG();\n\t\tleft -= len;\n\n\t\t/*\n\t\t *\tFill in the new header fields.\n\t\t */\n\t\tiph = ip_hdr(skb2);\n\t\tiph->frag_off = htons((offset >> 3));\n\n\t\t/* ANK: dirty, but effective trick. Upgrade options only if\n\t\t * the segment to be fragmented was THE FIRST (otherwise,\n\t\t * options are already fixed) and make it ONCE\n\t\t * on the initial skb, so that all the following fragments\n\t\t * will inherit fixed options.\n\t\t */\n\t\tif (offset == 0)\n\t\t\tip_options_fragment(skb);\n\n\t\t/*\n\t\t *\tAdded AC : If we are fragmenting a fragment that's not the\n\t\t *\t\t   last fragment then keep MF on each bit\n\t\t */\n\t\tif (left > 0 || not_last_frag)\n\t\t\tiph->frag_off |= htons(IP_MF);\n\t\tptr += len;\n\t\toffset += len;\n\n\t\t/*\n\t\t *\tPut this fragment into the sending queue.\n\t\t */\n\t\tiph->tot_len = htons(len + hlen);\n\n\t\tip_send_check(iph);\n\n\t\terr = output(skb2);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGCREATES);\n\t}\n\tkfree_skb(skb);\n\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGOKS);\n\treturn err;\n\nfail:\n\tkfree_skb(skb);\n\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);\n\treturn err;\n}\nEXPORT_SYMBOL(ip_fragment);\n\nint\nip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb)\n{\n\tstruct iovec *iov = from;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tif (memcpy_fromiovecend(to, iov, offset, len) < 0)\n\t\t\treturn -EFAULT;\n\t} else {\n\t\t__wsum csum = 0;\n\t\tif (csum_partial_copy_fromiovecend(to, iov, offset, len, &csum) < 0)\n\t\t\treturn -EFAULT;\n\t\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ip_generic_getfrag);\n\nstatic inline __wsum\ncsum_page(struct page *page, int offset, int copy)\n{\n\tchar *kaddr;\n\t__wsum csum;\n\tkaddr = kmap(page);\n\tcsum = csum_partial(kaddr + offset, copy, 0);\n\tkunmap(page);\n\treturn csum;\n}\n\nstatic inline int ip_ufo_append_data(struct sock *sk,\n\t\t\tstruct sk_buff_head *queue,\n\t\t\tint getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length, int hh_len, int fragheaderlen,\n\t\t\tint transhdrlen, int mtu, unsigned int flags)\n{\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* There is support for UDP fragmentation offload by network\n\t * device, so create one single skb packet containing complete\n\t * udp datagram\n\t */\n\tif ((skb = skb_peek_tail(queue)) == NULL) {\n\t\tskb = sock_alloc_send_skb(sk,\n\t\t\thh_len + fragheaderlen + transhdrlen + 20,\n\t\t\t(flags & MSG_DONTWAIT), &err);\n\n\t\tif (skb == NULL)\n\t\t\treturn err;\n\n\t\t/* reserve space for Hardware header */\n\t\tskb_reserve(skb, hh_len);\n\n\t\t/* create space for UDP/IP header */\n\t\tskb_put(skb, fragheaderlen + transhdrlen);\n\n\t\t/* initialize network header pointer */\n\t\tskb_reset_network_header(skb);\n\n\t\t/* initialize protocol header pointer */\n\t\tskb->transport_header = skb->network_header + fragheaderlen;\n\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum = 0;\n\n\t\t/* specify the length of each IP datagram fragment */\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t\t__skb_queue_tail(queue, skb);\n\t}\n\n\treturn skb_append_datato_frags(sk, skb, getfrag, from,\n\t\t\t\t       (length - transhdrlen));\n}\n\nstatic int __ip_append_data(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t    struct inet_cork *cork,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\n\tstruct ip_options *opt = cork->opt;\n\tint hh_len;\n\tint exthdrlen;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tunsigned int maxfraglen, fragheaderlen;\n\tint csummode = CHECKSUM_NONE;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\n\texthdrlen = transhdrlen ? rt->dst.header_len : 0;\n\tlength += exthdrlen;\n\ttranshdrlen += exthdrlen;\n\tmtu = cork->fragsize;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\n\tif (cork->length + length > 0xFFFF - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, rt->rt_dst, inet->inet_dport,\n\t\t\t       mtu-exthdrlen);\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t * transhdrlen > 0 means that this is the first fragment and we wish\n\t * it won't be fragmented in the future.\n\t */\n\tif (transhdrlen &&\n\t    length + fragheaderlen <= mtu &&\n\t    rt->dst.dev->features & NETIF_F_V4_CSUM &&\n\t    !exthdrlen)\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tskb = skb_peek_tail(queue);\n\n\tcork->length += length;\n\tif (((length > mtu) || (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n\t\t\t\t\t mtu, flags);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\t/* So, what's going on in the loop below?\n\t *\n\t * We use calculated fragment length to generate chained skb,\n\t * each of segments is IP fragment ready for sending to network after\n\t * adding appropriate IP header.\n\t */\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = mtu - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > mtu - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = fraglen;\n\n\t\t\t/* The last fragment gets additional space at tail.\n\t\t\t * Note, with MSG_MORE we overallocate on fragments,\n\t\t\t * because we have no idea what fragment will be\n\t\t\t * the last.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap) {\n\t\t\t\talloclen += rt->dst.trailer_len;\n\t\t\t\t/* make sure mtu is not reached */\n\t\t\t\tif (datalen > mtu - fragheaderlen - rt->dst.trailer_len)\n\t\t\t\t\tdatalen -= ALIGN(rt->dst.trailer_len, 8);\n\t\t\t}\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len + 15,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (atomic_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len + 15, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(skb == NULL))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\telse\n\t\t\t\t\t/* only the initial fragment is\n\t\t\t\t\t   time stamped */\n\t\t\t\t\tcork->tx_flags = 0;\n\t\t\t}\n\t\t\tif (skb == NULL)\n\t\t\t\tgoto error;\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\t\t\tskb_shinfo(skb)->tx_flags = cork->tx_flags;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tdata += fragheaderlen;\n\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i-1];\n\t\t\tstruct page *page = cork->page;\n\t\t\tint off = cork->off;\n\t\t\tunsigned int left;\n\n\t\t\tif (page && (left = PAGE_SIZE - off) > 0) {\n\t\t\t\tif (copy >= left)\n\t\t\t\t\tcopy = left;\n\t\t\t\tif (page != frag->page) {\n\t\t\t\t\tif (i == MAX_SKB_FRAGS) {\n\t\t\t\t\t\terr = -EMSGSIZE;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tget_page(page);\n\t\t\t\t\tskb_fill_page_desc(skb, i, page, off, 0);\n\t\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t\t}\n\t\t\t} else if (i < MAX_SKB_FRAGS) {\n\t\t\t\tif (copy > PAGE_SIZE)\n\t\t\t\t\tcopy = PAGE_SIZE;\n\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\tif (page == NULL)  {\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tcork->page = page;\n\t\t\t\tcork->off = 0;\n\n\t\t\t\tskb_fill_page_desc(skb, i, page, 0, 0);\n\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t} else {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (getfrag(from, page_address(frag->page)+frag->page_offset+frag->size, offset, copy, skb->len, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tcork->off += copy;\n\t\t\tfrag->size += copy;\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror:\n\tcork->length -= length;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic int ip_setup_cork(struct sock *sk, struct inet_cork *cork,\n\t\t\t struct ipcm_cookie *ipc, struct rtable **rtp)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options *opt;\n\tstruct rtable *rt;\n\n\t/*\n\t * setup for corking.\n\t */\n\topt = ipc->opt;\n\tif (opt) {\n\t\tif (cork->opt == NULL) {\n\t\t\tcork->opt = kmalloc(sizeof(struct ip_options) + 40,\n\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (unlikely(cork->opt == NULL))\n\t\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tmemcpy(cork->opt, opt, sizeof(struct ip_options) + opt->optlen);\n\t\tcork->flags |= IPCORK_OPT;\n\t\tcork->addr = ipc->addr;\n\t}\n\trt = *rtp;\n\tif (unlikely(!rt))\n\t\treturn -EFAULT;\n\t/*\n\t * We steal reference to this route, caller should not release it\n\t */\n\t*rtp = NULL;\n\tcork->fragsize = inet->pmtudisc == IP_PMTUDISC_PROBE ?\n\t\t\t rt->dst.dev->mtu : dst_mtu(rt->dst.path);\n\tcork->dst = &rt->dst;\n\tcork->length = 0;\n\tcork->tx_flags = ipc->tx_flags;\n\tcork->page = NULL;\n\tcork->off = 0;\n\n\treturn 0;\n}\n\n/*\n *\tip_append_data() and ip_append_page() can make one large IP datagram\n *\tfrom many pieces of data. Each pieces will be holded on the socket\n *\tuntil ip_push_pending_frames() is called. Each piece can be a page\n *\tor non-page data.\n *\n *\tNot only UDP, other transport protocols - e.g. raw sockets - can use\n *\tthis interface potentially.\n *\n *\tLATER: length must be adjusted by pad at tail, when it is required.\n */\nint ip_append_data(struct sock *sk,\n\t\t   int getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t   void *from, int length, int transhdrlen,\n\t\t   struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t   unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue)) {\n\t\terr = ip_setup_cork(sk, &inet->cork, ipc, rtp);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\ttranshdrlen = 0;\n\t}\n\n\treturn __ip_append_data(sk, &sk->sk_write_queue, &inet->cork, getfrag,\n\t\t\t\tfrom, length, transhdrlen, flags);\n}\n\nssize_t\tip_append_page(struct sock *sk, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\trt = (struct rtable *)inet->cork.dst;\n\tif (inet->cork.flags & IPCORK_OPT)\n\t\topt = inet->cork.opt;\n\n\tif (!(rt->dst.dev->features&NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = inet->cork.fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\n\tif (inet->cork.length + size > 0xFFFF - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, rt->rt_dst, inet->inet_dport, mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL)\n\t\treturn -EINVAL;\n\n\tinet->cork.length += size;\n\tif ((size + skb->len > mtu) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t}\n\n\n\twhile (size > 0) {\n\t\tint i;\n\n\t\tif (skb_is_gso(skb))\n\t\t\tlen = size;\n\t\telse {\n\n\t\t\t/* Check if the remaining data fits into current packet. */\n\t\t\tlen = mtu - skb->len;\n\t\t\tif (len < size)\n\t\t\t\tlen = maxfraglen - skb->len;\n\t\t}\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\ti = skb_shinfo(skb)->nr_frags;\n\t\tif (len > size)\n\t\t\tlen = size;\n\t\tif (skb_can_coalesce(skb, i, page, offset)) {\n\t\t\tskb_shinfo(skb)->frags[i-1].size += len;\n\t\t} else if (i < MAX_SKB_FRAGS) {\n\t\t\tget_page(page);\n\t\t\tskb_fill_page_desc(skb, i, page, offset, len);\n\t\t} else {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\tatomic_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tinet->cork.length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic void ip_cork_release(struct inet_cork *cork)\n{\n\tcork->flags &= ~IPCORK_OPT;\n\tkfree(cork->opt);\n\tcork->opt = NULL;\n\tdst_release(cork->dst);\n\tcork->dst = NULL;\n}\n\n/*\n *\tCombined all pending IP fragments on the socket as one IP datagram\n *\tand push them out.\n */\nstruct sk_buff *__ip_make_skb(struct sock *sk,\n\t\t\t      struct sk_buff_head *queue,\n\t\t\t      struct inet_cork *cork)\n{\n\tstruct sk_buff *skb, *tmp_skb;\n\tstruct sk_buff **tail_skb;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ip_options *opt = NULL;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\tstruct iphdr *iph;\n\t__be16 df = 0;\n\t__u8 ttl;\n\n\tif ((skb = __skb_dequeue(queue)) == NULL)\n\t\tgoto out;\n\ttail_skb = &(skb_shinfo(skb)->frag_list);\n\n\t/* move skb->data to ip header from ext header */\n\tif (skb->data < skb_network_header(skb))\n\t\t__skb_pull(skb, skb_network_offset(skb));\n\twhile ((tmp_skb = __skb_dequeue(queue)) != NULL) {\n\t\t__skb_pull(tmp_skb, skb_network_header_len(skb));\n\t\t*tail_skb = tmp_skb;\n\t\ttail_skb = &(tmp_skb->next);\n\t\tskb->len += tmp_skb->len;\n\t\tskb->data_len += tmp_skb->len;\n\t\tskb->truesize += tmp_skb->truesize;\n\t\ttmp_skb->destructor = NULL;\n\t\ttmp_skb->sk = NULL;\n\t}\n\n\t/* Unless user demanded real pmtu discovery (IP_PMTUDISC_DO), we allow\n\t * to fragment the frame generated here. No matter, what transforms\n\t * how transforms change size of the packet, it will come out.\n\t */\n\tif (inet->pmtudisc < IP_PMTUDISC_DO)\n\t\tskb->local_df = 1;\n\n\t/* DF bit is set when we want to see DF on outgoing frames.\n\t * If local_df is set too, we still allow to fragment this frame\n\t * locally. */\n\tif (inet->pmtudisc >= IP_PMTUDISC_DO ||\n\t    (skb->len <= dst_mtu(&rt->dst) &&\n\t     ip_dont_fragment(sk, &rt->dst)))\n\t\tdf = htons(IP_DF);\n\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (rt->rt_type == RTN_MULTICAST)\n\t\tttl = inet->mc_ttl;\n\telse\n\t\tttl = ip_select_ttl(inet, &rt->dst);\n\n\tiph = (struct iphdr *)skb->data;\n\tiph->version = 4;\n\tiph->ihl = 5;\n\tif (opt) {\n\t\tiph->ihl += opt->optlen>>2;\n\t\tip_options_build(skb, opt, cork->addr, rt, 0);\n\t}\n\tiph->tos = inet->tos;\n\tiph->frag_off = df;\n\tip_select_ident(iph, &rt->dst, sk);\n\tiph->ttl = ttl;\n\tiph->protocol = sk->sk_protocol;\n\tiph->saddr = rt->rt_src;\n\tiph->daddr = rt->rt_dst;\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\t/*\n\t * Steal rt from cork.dst to avoid a pair of atomic_inc/atomic_dec\n\t * on dst refcount\n\t */\n\tcork->dst = NULL;\n\tskb_dst_set(skb, &rt->dst);\n\n\tif (iph->protocol == IPPROTO_ICMP)\n\t\ticmp_out_count(net, ((struct icmphdr *)\n\t\t\tskb_transport_header(skb))->type);\n\n\tip_cork_release(cork);\nout:\n\treturn skb;\n}\n\nint ip_send_skb(struct sk_buff *skb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tint err;\n\n\terr = ip_local_out(skb);\n\tif (err) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);\n\t}\n\n\treturn err;\n}\n\nint ip_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\tskb = ip_finish_skb(sk);\n\tif (!skb)\n\t\treturn 0;\n\n\t/* Netfilter gets whole the not fragmented skb. */\n\treturn ip_send_skb(skb);\n}\n\n/*\n *\tThrow away all pending data on the socket.\n */\nstatic void __ip_flush_pending_frames(struct sock *sk,\n\t\t\t\t      struct sk_buff_head *queue,\n\t\t\t\t      struct inet_cork *cork)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(queue)) != NULL)\n\t\tkfree_skb(skb);\n\n\tip_cork_release(cork);\n}\n\nvoid ip_flush_pending_frames(struct sock *sk)\n{\n\t__ip_flush_pending_frames(sk, &sk->sk_write_queue, &inet_sk(sk)->cork);\n}\n\nstruct sk_buff *ip_make_skb(struct sock *sk,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_cork cork = {};\n\tstruct sk_buff_head queue;\n\tint err;\n\n\tif (flags & MSG_PROBE)\n\t\treturn NULL;\n\n\t__skb_queue_head_init(&queue);\n\n\terr = ip_setup_cork(sk, &cork, ipc, rtp);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\terr = __ip_append_data(sk, &queue, &cork, getfrag,\n\t\t\t       from, length, transhdrlen, flags);\n\tif (err) {\n\t\t__ip_flush_pending_frames(sk, &queue, &cork);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn __ip_make_skb(sk, &queue, &cork);\n}\n\n/*\n *\tFetch data from kernel space and fill in checksum if needed.\n */\nstatic int ip_reply_glue_bits(void *dptr, char *to, int offset,\n\t\t\t      int len, int odd, struct sk_buff *skb)\n{\n\t__wsum csum;\n\n\tcsum = csum_partial_copy_nocheck(dptr+offset, to, len, 0);\n\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\treturn 0;\n}\n\n/*\n *\tGeneric function to send a packet as reply to another packet.\n *\tUsed to send TCP resets so far. ICMP should use this function too.\n *\n *\tShould run single threaded per socket because it uses the sock\n *     \tstructure to pass arguments.\n */\nvoid ip_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg,\n\t\t   unsigned int len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct {\n\t\tstruct ip_options\topt;\n\t\tchar\t\t\tdata[40];\n\t} replyopts;\n\tstruct ipcm_cookie ipc;\n\t__be32 daddr;\n\tstruct rtable *rt = skb_rtable(skb);\n\n\tif (ip_options_echo(&replyopts.opt, skb))\n\t\treturn;\n\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tif (replyopts.opt.optlen) {\n\t\tipc.opt = &replyopts.opt;\n\n\t\tif (ipc.opt->srr)\n\t\t\tdaddr = replyopts.opt.faddr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, arg->bound_dev_if, 0,\n\t\t\t\t   RT_TOS(ip_hdr(skb)->tos),\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   ip_reply_arg_flowi_flags(arg),\n\t\t\t\t   daddr, rt->rt_spec_dst,\n\t\t\t\t   tcp_hdr(skb)->source, tcp_hdr(skb)->dest);\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\treturn;\n\t}\n\n\t/* And let IP do all the hard work.\n\n\t   This chunk is not reenterable, hence spinlock.\n\t   Note that it uses the fact, that this function is called\n\t   with locally disabled BH and that sk cannot be already spinlocked.\n\t */\n\tbh_lock_sock(sk);\n\tinet->tos = ip_hdr(skb)->tos;\n\tsk->sk_priority = skb->priority;\n\tsk->sk_protocol = ip_hdr(skb)->protocol;\n\tsk->sk_bound_dev_if = arg->bound_dev_if;\n\tip_append_data(sk, ip_reply_glue_bits, arg->iov->iov_base, len, 0,\n\t\t       &ipc, &rt, MSG_DONTWAIT);\n\tif ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {\n\t\tif (arg->csumoffset >= 0)\n\t\t\t*((__sum16 *)skb_transport_header(skb) +\n\t\t\t  arg->csumoffset) = csum_fold(csum_add(skb->csum,\n\t\t\t\t\t\t\t\targ->csum));\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\n\tip_rt_put(rt);\n}\n\nvoid __init ip_init(void)\n{\n\tip_rt_init();\n\tinet_initpeers();\n\n#if defined(CONFIG_IP_MULTICAST) && defined(CONFIG_PROC_FS)\n\tigmp_mc_proc_init();\n#endif\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe IP to API glue.\n *\n * Authors:\tsee ip.c\n *\n * Fixes:\n *\t\tMany\t\t:\tSplit from ip.c , see ip.c for history.\n *\t\tMartin Mares\t:\tTOS setting fixed.\n *\t\tAlan Cox\t:\tFixed a couple of oopses in Martin's\n *\t\t\t\t\tTOS tweaks.\n *\t\tMike McLagan\t:\tRouting by source\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/skbuff.h>\n#include <linux/ip.h>\n#include <linux/icmp.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/tcp_states.h>\n#include <linux/udp.h>\n#include <linux/igmp.h>\n#include <linux/netfilter.h>\n#include <linux/route.h>\n#include <linux/mroute.h>\n#include <net/route.h>\n#include <net/xfrm.h>\n#include <net/compat.h>\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#include <net/transp_v6.h>\n#endif\n\n#include <linux/errqueue.h>\n#include <asm/uaccess.h>\n\n#define IP_CMSG_PKTINFO\t\t1\n#define IP_CMSG_TTL\t\t2\n#define IP_CMSG_TOS\t\t4\n#define IP_CMSG_RECVOPTS\t8\n#define IP_CMSG_RETOPTS\t\t16\n#define IP_CMSG_PASSSEC\t\t32\n#define IP_CMSG_ORIGDSTADDR     64\n\n/*\n *\tSOL_IP control messages.\n */\n\nstatic void ip_cmsg_recv_pktinfo(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct in_pktinfo info;\n\tstruct rtable *rt = skb_rtable(skb);\n\n\tinfo.ipi_addr.s_addr = ip_hdr(skb)->daddr;\n\tif (rt) {\n\t\tinfo.ipi_ifindex = rt->rt_iif;\n\t\tinfo.ipi_spec_dst.s_addr = rt->rt_spec_dst;\n\t} else {\n\t\tinfo.ipi_ifindex = 0;\n\t\tinfo.ipi_spec_dst.s_addr = 0;\n\t}\n\n\tput_cmsg(msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);\n}\n\nstatic void ip_cmsg_recv_ttl(struct msghdr *msg, struct sk_buff *skb)\n{\n\tint ttl = ip_hdr(skb)->ttl;\n\tput_cmsg(msg, SOL_IP, IP_TTL, sizeof(int), &ttl);\n}\n\nstatic void ip_cmsg_recv_tos(struct msghdr *msg, struct sk_buff *skb)\n{\n\tput_cmsg(msg, SOL_IP, IP_TOS, 1, &ip_hdr(skb)->tos);\n}\n\nstatic void ip_cmsg_recv_opts(struct msghdr *msg, struct sk_buff *skb)\n{\n\tif (IPCB(skb)->opt.optlen == 0)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_IP, IP_RECVOPTS, IPCB(skb)->opt.optlen,\n\t\t ip_hdr(skb) + 1);\n}\n\n\nstatic void ip_cmsg_recv_retopts(struct msghdr *msg, struct sk_buff *skb)\n{\n\tunsigned char optbuf[sizeof(struct ip_options) + 40];\n\tstruct ip_options * opt = (struct ip_options *)optbuf;\n\n\tif (IPCB(skb)->opt.optlen == 0)\n\t\treturn;\n\n\tif (ip_options_echo(opt, skb)) {\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\treturn;\n\t}\n\tip_options_undo(opt);\n\n\tput_cmsg(msg, SOL_IP, IP_RETOPTS, opt->optlen, opt->__data);\n}\n\nstatic void ip_cmsg_recv_security(struct msghdr *msg, struct sk_buff *skb)\n{\n\tchar *secdata;\n\tu32 seclen, secid;\n\tint err;\n\n\terr = security_socket_getpeersec_dgram(NULL, skb, &secid);\n\tif (err)\n\t\treturn;\n\n\terr = security_secid_to_secctx(secid, &secdata, &seclen);\n\tif (err)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_IP, SCM_SECURITY, seclen, secdata);\n\tsecurity_release_secctx(secdata, seclen);\n}\n\nstatic void ip_cmsg_recv_dstaddr(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct sockaddr_in sin;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\t__be16 *ports = (__be16 *)skb_transport_header(skb);\n\n\tif (skb_transport_offset(skb) + 4 > skb->len)\n\t\treturn;\n\n\t/* All current transport protocols have the port numbers in the\n\t * first four bytes of the transport header and this function is\n\t * written with this assumption in mind.\n\t */\n\n\tsin.sin_family = AF_INET;\n\tsin.sin_addr.s_addr = iph->daddr;\n\tsin.sin_port = ports[1];\n\tmemset(sin.sin_zero, 0, sizeof(sin.sin_zero));\n\n\tput_cmsg(msg, SOL_IP, IP_ORIGDSTADDR, sizeof(sin), &sin);\n}\n\nvoid ip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(skb->sk);\n\tunsigned flags = inet->cmsg_flags;\n\n\t/* Ordered by supposed usage frequency */\n\tif (flags & 1)\n\t\tip_cmsg_recv_pktinfo(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_ttl(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_tos(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_opts(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_retopts(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_security(msg, skb);\n\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\tif (flags & 1)\n\t\tip_cmsg_recv_dstaddr(msg, skb);\n\n}\nEXPORT_SYMBOL(ip_cmsg_recv);\n\nint ip_cmsg_send(struct net *net, struct msghdr *msg, struct ipcm_cookie *ipc)\n{\n\tint err;\n\tstruct cmsghdr *cmsg;\n\n\tfor (cmsg = CMSG_FIRSTHDR(msg); cmsg; cmsg = CMSG_NXTHDR(msg, cmsg)) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_level != SOL_IP)\n\t\t\tcontinue;\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase IP_RETOPTS:\n\t\t\terr = cmsg->cmsg_len - CMSG_ALIGN(sizeof(struct cmsghdr));\n\t\t\terr = ip_options_get(net, &ipc->opt, CMSG_DATA(cmsg),\n\t\t\t\t\t     err < 40 ? err : 40);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\t\tcase IP_PKTINFO:\n\t\t{\n\t\t\tstruct in_pktinfo *info;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(struct in_pktinfo)))\n\t\t\t\treturn -EINVAL;\n\t\t\tinfo = (struct in_pktinfo *)CMSG_DATA(cmsg);\n\t\t\tipc->oif = info->ipi_ifindex;\n\t\t\tipc->addr = info->ipi_spec_dst.s_addr;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\n\n/* Special input handler for packets caught by router alert option.\n   They are selected only by protocol field, and then processed likely\n   local ones; but only if someone wants them! Otherwise, router\n   not running rsvpd will kill RSVP.\n\n   It is user level problem, what it will make with them.\n   I have no idea, how it will masquearde or NAT them (it is joke, joke :-)),\n   but receiver should be enough clever f.e. to forward mtrace requests,\n   sent to multicast group to reach destination designated router.\n */\nstruct ip_ra_chain __rcu *ip_ra_chain;\nstatic DEFINE_SPINLOCK(ip_ra_lock);\n\n\nstatic void ip_ra_destroy_rcu(struct rcu_head *head)\n{\n\tstruct ip_ra_chain *ra = container_of(head, struct ip_ra_chain, rcu);\n\n\tsock_put(ra->saved_sk);\n\tkfree(ra);\n}\n\nint ip_ra_control(struct sock *sk, unsigned char on,\n\t\t  void (*destructor)(struct sock *))\n{\n\tstruct ip_ra_chain *ra, *new_ra;\n\tstruct ip_ra_chain __rcu **rap;\n\n\tif (sk->sk_type != SOCK_RAW || inet_sk(sk)->inet_num == IPPROTO_RAW)\n\t\treturn -EINVAL;\n\n\tnew_ra = on ? kmalloc(sizeof(*new_ra), GFP_KERNEL) : NULL;\n\n\tspin_lock_bh(&ip_ra_lock);\n\tfor (rap = &ip_ra_chain;\n\t     (ra = rcu_dereference_protected(*rap,\n\t\t\tlockdep_is_held(&ip_ra_lock))) != NULL;\n\t     rap = &ra->next) {\n\t\tif (ra->sk == sk) {\n\t\t\tif (on) {\n\t\t\t\tspin_unlock_bh(&ip_ra_lock);\n\t\t\t\tkfree(new_ra);\n\t\t\t\treturn -EADDRINUSE;\n\t\t\t}\n\t\t\t/* dont let ip_call_ra_chain() use sk again */\n\t\t\tra->sk = NULL;\n\t\t\trcu_assign_pointer(*rap, ra->next);\n\t\t\tspin_unlock_bh(&ip_ra_lock);\n\n\t\t\tif (ra->destructor)\n\t\t\t\tra->destructor(sk);\n\t\t\t/*\n\t\t\t * Delay sock_put(sk) and kfree(ra) after one rcu grace\n\t\t\t * period. This guarantee ip_call_ra_chain() dont need\n\t\t\t * to mess with socket refcounts.\n\t\t\t */\n\t\t\tra->saved_sk = sk;\n\t\t\tcall_rcu(&ra->rcu, ip_ra_destroy_rcu);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (new_ra == NULL) {\n\t\tspin_unlock_bh(&ip_ra_lock);\n\t\treturn -ENOBUFS;\n\t}\n\tnew_ra->sk = sk;\n\tnew_ra->destructor = destructor;\n\n\tnew_ra->next = ra;\n\trcu_assign_pointer(*rap, new_ra);\n\tsock_hold(sk);\n\tspin_unlock_bh(&ip_ra_lock);\n\n\treturn 0;\n}\n\nvoid ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err,\n\t\t   __be16 port, u32 info, u8 *payload)\n{\n\tstruct sock_exterr_skb *serr;\n\n\tskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_ICMP;\n\tserr->ee.ee_type = icmp_hdr(skb)->type;\n\tserr->ee.ee_code = icmp_hdr(skb)->code;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&(((struct iphdr *)(icmp_hdr(skb) + 1))->daddr) -\n\t\t\t\t   skb_network_header(skb);\n\tserr->port = port;\n\n\tif (skb_pull(skb, payload - skb->data) != NULL) {\n\t\tskb_reset_transport_header(skb);\n\t\tif (sock_queue_err_skb(sk, skb) == 0)\n\t\t\treturn;\n\t}\n\tkfree_skb(skb);\n}\n\nvoid ip_local_error(struct sock *sk, int err, __be32 daddr, __be16 port, u32 info)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sock_exterr_skb *serr;\n\tstruct iphdr *iph;\n\tstruct sk_buff *skb;\n\n\tif (!inet->recverr)\n\t\treturn;\n\n\tskb = alloc_skb(sizeof(struct iphdr), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb_put(skb, sizeof(struct iphdr));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tiph->daddr = daddr;\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_LOCAL;\n\tserr->ee.ee_type = 0;\n\tserr->ee.ee_code = 0;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&iph->daddr - skb_network_header(skb);\n\tserr->port = port;\n\n\t__skb_pull(skb, skb_tail_pointer(skb) - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tif (sock_queue_err_skb(sk, skb))\n\t\tkfree_skb(skb);\n}\n\n/*\n *\tHandle MSG_ERRQUEUE\n */\nint ip_recv_error(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb, *skb2;\n\tstruct sockaddr_in *sin;\n\tstruct {\n\t\tstruct sock_extended_err ee;\n\t\tstruct sockaddr_in\t offender;\n\t} errhdr;\n\tint err;\n\tint copied;\n\n\terr = -EAGAIN;\n\tskb = skb_dequeue(&sk->sk_error_queue);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\n\tsin = (struct sockaddr_in *)msg->msg_name;\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = *(__be32 *)(skb_network_header(skb) +\n\t\t\t\t\t\t   serr->addr_offset);\n\t\tsin->sin_port = serr->port;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\n\tmemcpy(&errhdr.ee, &serr->ee, sizeof(struct sock_extended_err));\n\tsin = &errhdr.offender;\n\tsin->sin_family = AF_UNSPEC;\n\tif (serr->ee.ee_origin == SO_EE_ORIGIN_ICMP) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t}\n\n\tput_cmsg(msg, SOL_IP, IP_RECVERR, sizeof(errhdr), &errhdr);\n\n\t/* Now we could try to dump offended packet options */\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\n\t/* Reset and regenerate socket error */\n\tspin_lock_bh(&sk->sk_error_queue.lock);\n\tsk->sk_err = 0;\n\tskb2 = skb_peek(&sk->sk_error_queue);\n\tif (skb2 != NULL) {\n\t\tsk->sk_err = SKB_EXT_ERR(skb2)->ee.ee_errno;\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\n\n\n/*\n *\tSocket option code for IP. This is the end of the line after any\n *\tTCP,UDP etc options on an IP socket.\n */\n\nstatic int do_ip_setsockopt(struct sock *sk, int level,\n\t\t\t    int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val = 0, err;\n\n\tif (((1<<optname) & ((1<<IP_PKTINFO) | (1<<IP_RECVTTL) |\n\t\t\t     (1<<IP_RECVOPTS) | (1<<IP_RECVTOS) |\n\t\t\t     (1<<IP_RETOPTS) | (1<<IP_TOS) |\n\t\t\t     (1<<IP_TTL) | (1<<IP_HDRINCL) |\n\t\t\t     (1<<IP_MTU_DISCOVER) | (1<<IP_RECVERR) |\n\t\t\t     (1<<IP_ROUTER_ALERT) | (1<<IP_FREEBIND) |\n\t\t\t     (1<<IP_PASSSEC) | (1<<IP_TRANSPARENT) |\n\t\t\t     (1<<IP_MINTTL) | (1<<IP_NODEFRAG))) ||\n\t    optname == IP_MULTICAST_TTL ||\n\t    optname == IP_MULTICAST_ALL ||\n\t    optname == IP_MULTICAST_LOOP ||\n\t    optname == IP_RECVORIGDSTADDR) {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else if (optlen >= sizeof(char)) {\n\t\t\tunsigned char ucval;\n\n\t\t\tif (get_user(ucval, (unsigned char __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t\tval = (int) ucval;\n\t\t}\n\t}\n\n\t/* If optlen==0, it is equivalent to val == 0 */\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_setsockopt(sk, optname, optval, optlen);\n\n\terr = 0;\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tstruct ip_options *opt = NULL;\n\t\tif (optlen > 40)\n\t\t\tgoto e_inval;\n\t\terr = ip_options_get_from_user(sock_net(sk), &opt,\n\t\t\t\t\t       optval, optlen);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (inet->is_icsk) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\tif (sk->sk_family == PF_INET ||\n\t\t\t    (!((1 << sk->sk_state) &\n\t\t\t       (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t\t     inet->inet_daddr != LOOPBACK4_IPV6)) {\n#endif\n\t\t\t\tif (inet->opt)\n\t\t\t\t\ticsk->icsk_ext_hdr_len -= inet->opt->optlen;\n\t\t\t\tif (opt)\n\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->optlen;\n\t\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\t}\n#endif\n\t\t}\n\t\topt = xchg(&inet->opt, opt);\n\t\tkfree(opt);\n\t\tbreak;\n\t}\n\tcase IP_PKTINFO:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PKTINFO;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PKTINFO;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TTL;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TTL;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TOS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TOS;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_RECVOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RECVOPTS;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_RETOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RETOPTS;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PASSSEC;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PASSSEC;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_ORIGDSTADDR;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_ORIGDSTADDR;\n\t\tbreak;\n\tcase IP_TOS:\t/* This sets both TOS and Precedence */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~3;\n\t\t\tval |= inet->tos & 3;\n\t\t}\n\t\tif (inet->tos != val) {\n\t\t\tinet->tos = val;\n\t\t\tsk->sk_priority = rt_tos2priority(val);\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tbreak;\n\tcase IP_TTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != -1 && (val < 0 || val > 255))\n\t\t\tgoto e_inval;\n\t\tinet->uc_ttl = val;\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->hdrincl = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->nodefrag = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tif (val < IP_PMTUDISC_DONT || val > IP_PMTUDISC_PROBE)\n\t\t\tgoto e_inval;\n\t\tinet->pmtudisc = val;\n\t\tbreak;\n\tcase IP_RECVERR:\n\t\tinet->recverr = !!val;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val == -1)\n\t\t\tval = 1;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->mc_ttl = val;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_loop = !!val;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct ip_mreqn mreq;\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\t/*\n\t\t *\tCheck the arguments are allowable\n\t\t */\n\n\t\tif (optlen < sizeof(struct in_addr))\n\t\t\tgoto e_inval;\n\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (optlen >= sizeof(struct in_addr) &&\n\t\t\t    copy_from_user(&mreq.imr_address, optval,\n\t\t\t\t\t   sizeof(struct in_addr)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!mreq.imr_ifindex) {\n\t\t\tif (mreq.imr_address.s_addr == htonl(INADDR_ANY)) {\n\t\t\t\tinet->mc_index = 0;\n\t\t\t\tinet->mc_addr  = 0;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev = ip_dev_find(sock_net(sk), mreq.imr_address.s_addr);\n\t\t\tif (dev)\n\t\t\t\tmreq.imr_ifindex = dev->ifindex;\n\t\t} else\n\t\t\tdev = dev_get_by_index(sock_net(sk), mreq.imr_ifindex);\n\n\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\terr = -EINVAL;\n\t\tif (sk->sk_bound_dev_if &&\n\t\t    mreq.imr_ifindex != sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tinet->mc_index = mreq.imr_ifindex;\n\t\tinet->mc_addr  = mreq.imr_address.s_addr;\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\tcase IP_ADD_MEMBERSHIP:\n\tcase IP_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreqn mreq;\n\n\t\terr = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tif (optlen < sizeof(struct ip_mreq))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(struct ip_mreq)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (optname == IP_ADD_MEMBERSHIP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter *msf;\n\n\t\tif (optlen < IP_MSFILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tmsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(msf, optval, optlen)) {\n\t\t\tkfree(msf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (1G-4) overflow in 32 bits */\n\t\tif (msf->imsf_numsrc >= 0x3ffffffcU ||\n\t\t    msf->imsf_numsrc > sysctl_igmp_max_msf) {\n\t\t\tkfree(msf);\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (IP_MSFILTER_SIZE(msf->imsf_numsrc) > optlen) {\n\t\t\tkfree(msf);\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\terr = ip_mc_msfilter(sk, msf, 0);\n\t\tkfree(msf);\n\t\tbreak;\n\t}\n\tcase IP_BLOCK_SOURCE:\n\tcase IP_UNBLOCK_SOURCE:\n\tcase IP_ADD_SOURCE_MEMBERSHIP:\n\tcase IP_DROP_SOURCE_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreq_source mreqs;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct ip_mreq_source))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&mreqs, optval, sizeof(mreqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == IP_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == IP_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == IP_ADD_SOURCE_MEMBERSHIP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tmreq.imr_multiaddr.s_addr = mreqs.imr_multiaddr;\n\t\t\tmreq.imr_address.s_addr = mreqs.imr_interface;\n\t\t\tmreq.imr_ifindex = 0;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* IP_DROP_SOURCE_MEMBERSHIP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs, 0);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_mreqn mreq;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(greq)))\n\t\t\tbreak;\n\t\tpsin = (struct sockaddr_in *)&greq.gr_group;\n\t\tif (psin->sin_family != AF_INET)\n\t\t\tgoto e_inval;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\tmreq.imr_ifindex = greq.gr_interface;\n\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tstruct ip_mreq_source mreqs;\n\t\tstruct sockaddr_in *psin;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET ||\n\t\t    greqs.gsr_source.ss_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\tmreqs.imr_multiaddr = psin->sin_addr.s_addr;\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_source;\n\t\tmreqs.imr_sourceaddr = psin->sin_addr.s_addr;\n\t\tmreqs.imr_interface = 0; /* use index for mc_source */\n\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\t\tmreq.imr_address.s_addr = 0;\n\t\t\tmreq.imr_ifindex = greqs.gsr_interface;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tgreqs.gsr_interface = mreq.imr_ifindex;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs,\n\t\t\t\t   greqs.gsr_interface);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_msfilter *msf = NULL;\n\t\tstruct group_filter *gsf = NULL;\n\t\tint msize, i, ifindex;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen))\n\t\t\tgoto mc_msf_out;\n\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffff ||\n\t\t    gsf->gf_numsrc > sysctl_igmp_max_msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsize = IP_MSFILTER_SIZE(gsf->gf_numsrc);\n\t\tmsf = kmalloc(msize, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tifindex = gsf->gf_interface;\n\t\tpsin = (struct sockaddr_in *)&gsf->gf_group;\n\t\tif (psin->sin_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsf->imsf_multiaddr = psin->sin_addr.s_addr;\n\t\tmsf->imsf_interface = 0;\n\t\tmsf->imsf_fmode = gsf->gf_fmode;\n\t\tmsf->imsf_numsrc = gsf->gf_numsrc;\n\t\terr = -EADDRNOTAVAIL;\n\t\tfor (i = 0; i < gsf->gf_numsrc; ++i) {\n\t\t\tpsin = (struct sockaddr_in *)&gsf->gf_slist[i];\n\n\t\t\tif (psin->sin_family != AF_INET)\n\t\t\t\tgoto mc_msf_out;\n\t\t\tmsf->imsf_slist[i] = psin->sin_addr.s_addr;\n\t\t}\n\t\tkfree(gsf);\n\t\tgsf = NULL;\n\n\t\terr = ip_mc_msfilter(sk, msf, ifindex);\nmc_msf_out:\n\t\tkfree(msf);\n\t\tkfree(gsf);\n\t\tbreak;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != 0 && val != 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_all = val;\n\t\tbreak;\n\tcase IP_ROUTER_ALERT:\n\t\terr = ip_ra_control(sk, val ? 1 : 0, NULL);\n\t\tbreak;\n\n\tcase IP_FREEBIND:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->freebind = !!val;\n\t\tbreak;\n\n\tcase IP_IPSEC_POLICY:\n\tcase IP_XFRM_POLICY:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\terr = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IP_TRANSPARENT:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->transparent = !!val;\n\t\tbreak;\n\n\tcase IP_MINTTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->min_ttl = val;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn err;\n\ne_inval:\n\trelease_sock(sk);\n\treturn -EINVAL;\n}\n\n/**\n * ip_queue_rcv_skb - Queue an skb into sock receive queue\n * @sk: socket\n * @skb: buffer\n *\n * Queues an skb into socket receive queue. If IP_CMSG_PKTINFO option\n * is not set, we drop skb dst entry now, while dst cache line is hot.\n */\nint ip_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif (!(inet_sk(sk)->cmsg_flags & IP_CMSG_PKTINFO))\n\t\tskb_dst_drop(skb);\n\treturn sock_queue_rcv_skb(sk, skb);\n}\nEXPORT_SYMBOL(ip_queue_rcv_skb);\n\nint ip_setsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, unsigned int optlen)\n{\n\tint err;\n\n\tif (level != SOL_IP)\n\t\treturn -ENOPROTOOPT;\n\n\terr = do_ip_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_HDRINCL &&\n\t\t\toptname != IP_IPSEC_POLICY &&\n\t\t\toptname != IP_XFRM_POLICY &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tlock_sock(sk);\n\t\terr = nf_setsockopt(sk, PF_INET, optname, optval, optlen);\n\t\trelease_sock(sk);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ip_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_ip_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t char __user *optval, unsigned int optlen)\n{\n\tint err;\n\n\tif (level != SOL_IP)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optname >= MCAST_JOIN_GROUP && optname <= MCAST_MSFILTER)\n\t\treturn compat_mc_setsockopt(sk, level, optname, optval, optlen,\n\t\t\tip_setsockopt);\n\n\terr = do_ip_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_HDRINCL &&\n\t\t\toptname != IP_IPSEC_POLICY &&\n\t\t\toptname != IP_XFRM_POLICY &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tlock_sock(sk);\n\t\terr = compat_nf_setsockopt(sk, PF_INET, optname,\n\t\t\t\t\t   optval, optlen);\n\t\trelease_sock(sk);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(compat_ip_setsockopt);\n#endif\n\n/*\n *\tGet the options. Note for future reference. The GET of IP options gets\n *\tthe _received_ ones. The set sets the _sent_ ones.\n */\n\nstatic int do_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val;\n\tint len;\n\n\tif (level != SOL_IP)\n\t\treturn -EOPNOTSUPP;\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tunsigned char optbuf[sizeof(struct ip_options)+40];\n\t\tstruct ip_options * opt = (struct ip_options *)optbuf;\n\t\topt->optlen = 0;\n\t\tif (inet->opt)\n\t\t\tmemcpy(optbuf, inet->opt,\n\t\t\t       sizeof(struct ip_options)+\n\t\t\t       inet->opt->optlen);\n\t\trelease_sock(sk);\n\n\t\tif (opt->optlen == 0)\n\t\t\treturn put_user(0, optlen);\n\n\t\tip_options_undo(opt);\n\n\t\tlen = min_t(unsigned int, len, opt->optlen);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, opt->__data, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_PKTINFO:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PKTINFO) != 0;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TTL) != 0;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TOS) != 0;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RECVOPTS) != 0;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RETOPTS) != 0;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PASSSEC) != 0;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tval = (inet->cmsg_flags & IP_CMSG_ORIGDSTADDR) != 0;\n\t\tbreak;\n\tcase IP_TOS:\n\t\tval = inet->tos;\n\t\tbreak;\n\tcase IP_TTL:\n\t\tval = (inet->uc_ttl == -1 ?\n\t\t       sysctl_ip_default_ttl :\n\t\t       inet->uc_ttl);\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tval = inet->hdrincl;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tval = inet->nodefrag;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tval = inet->pmtudisc;\n\t\tbreak;\n\tcase IP_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tval = 0;\n\t\tdst = sk_dst_get(sk);\n\t\tif (dst) {\n\t\t\tval = dst_mtu(dst);\n\t\t\tdst_release(dst);\n\t\t}\n\t\tif (!val) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\tbreak;\n\t}\n\tcase IP_RECVERR:\n\t\tval = inet->recverr;\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tval = inet->mc_ttl;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tval = inet->mc_loop;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct in_addr addr;\n\t\tlen = min_t(unsigned int, len, sizeof(struct in_addr));\n\t\taddr.s_addr = inet->mc_addr;\n\t\trelease_sock(sk);\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &addr, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter msf;\n\t\tint err;\n\n\t\tif (len < IP_MSFILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&msf, optval, IP_MSFILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_msfget(sk, &msf,\n\t\t\t\t   (struct ip_msfilter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_gsfget(sk, &gsf,\n\t\t\t\t   (struct group_filter __user *)optval,\n\t\t\t\t   optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tval = inet->mc_all;\n\t\tbreak;\n\tcase IP_PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\n\t\trelease_sock(sk);\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = 0;\n\n\t\tif (inet->cmsg_flags & IP_CMSG_PKTINFO) {\n\t\t\tstruct in_pktinfo info;\n\n\t\t\tinfo.ipi_addr.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_spec_dst.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_ifindex = inet->mc_index;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);\n\t\t}\n\t\tif (inet->cmsg_flags & IP_CMSG_TTL) {\n\t\t\tint hlim = inet->mc_ttl;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_TTL, sizeof(hlim), &hlim);\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IP_FREEBIND:\n\t\tval = inet->freebind;\n\t\tbreak;\n\tcase IP_TRANSPARENT:\n\t\tval = inet->transparent;\n\t\tbreak;\n\tcase IP_MINTTL:\n\t\tval = inet->min_ttl;\n\t\tbreak;\n\tdefault:\n\t\trelease_sock(sk);\n\t\treturn -ENOPROTOOPT;\n\t}\n\trelease_sock(sk);\n\n\tif (len < sizeof(int) && len > 0 && val >= 0 && val <= 255) {\n\t\tunsigned char ucval = (unsigned char)val;\n\t\tlen = 1;\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &ucval, 1))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tlen = min_t(unsigned int, sizeof(int), len);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &val, len))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\nint ip_getsockopt(struct sock *sk, int level,\n\t\t  int optname, char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\terr = do_ip_getsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_PKTOPTIONS &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\terr = nf_getsockopt(sk, PF_INET, optname, optval,\n\t\t\t\t&len);\n\t\trelease_sock(sk);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t\treturn err;\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ip_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\tif (optname == MCAST_MSFILTER)\n\t\treturn compat_mc_getsockopt(sk, level, optname, optval, optlen,\n\t\t\tip_getsockopt);\n\n\terr = do_ip_getsockopt(sk, level, optname, optval, optlen);\n\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_PKTOPTIONS &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\terr = compat_nf_getsockopt(sk, PF_INET, optname, optval, &len);\n\t\trelease_sock(sk);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t\treturn err;\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(compat_ip_getsockopt);\n#endif\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tRAW - implementation of IP \"raw\" sockets.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() fixed up\n *\t\tAlan Cox\t:\tICMP error handling\n *\t\tAlan Cox\t:\tEMSGSIZE if you send too big a packet\n *\t\tAlan Cox\t: \tNow uses generic datagrams and shared\n *\t\t\t\t\tskbuff library. No more peek crashes,\n *\t\t\t\t\tno more backlogs\n *\t\tAlan Cox\t:\tChecks sk->broadcast.\n *\t\tAlan Cox\t:\tUses skb_free_datagram/skb_copy_datagram\n *\t\tAlan Cox\t:\tRaw passes ip options too\n *\t\tAlan Cox\t:\tSetsocketopt added\n *\t\tAlan Cox\t:\tFixed error return for broadcasts\n *\t\tAlan Cox\t:\tRemoved wake_up calls\n *\t\tAlan Cox\t:\tUse ttl/tos\n *\t\tAlan Cox\t:\tCleaned up old debugging\n *\t\tAlan Cox\t:\tUse new kernel side addresses\n *\tArnt Gulbrandsen\t:\tFixed MSG_DONTROUTE in raw sockets.\n *\t\tAlan Cox\t:\tBSD style RAW socket demultiplexing.\n *\t\tAlan Cox\t:\tBeginnings of mrouted support.\n *\t\tAlan Cox\t:\tAdded IP_HDRINCL option.\n *\t\tAlan Cox\t:\tSkip broadcast check if BSDism set.\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/types.h>\n#include <asm/atomic.h>\n#include <asm/byteorder.h>\n#include <asm/current.h>\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/stddef.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/aio.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/sockios.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/mroute.h>\n#include <linux/netdevice.h>\n#include <linux/in_route.h>\n#include <linux/route.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <linux/ip.h>\n#include <linux/net.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/udp.h>\n#include <net/raw.h>\n#include <net/snmp.h>\n#include <net/tcp_states.h>\n#include <net/inet_common.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/compat.h>\n\nstatic struct raw_hashinfo raw_v4_hashinfo = {\n\t.lock = __RW_LOCK_UNLOCKED(raw_v4_hashinfo.lock),\n};\n\nvoid raw_hash_sk(struct sock *sk)\n{\n\tstruct raw_hashinfo *h = sk->sk_prot->h.raw_hash;\n\tstruct hlist_head *head;\n\n\thead = &h->ht[inet_sk(sk)->inet_num & (RAW_HTABLE_SIZE - 1)];\n\n\twrite_lock_bh(&h->lock);\n\tsk_add_node(sk, head);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\twrite_unlock_bh(&h->lock);\n}\nEXPORT_SYMBOL_GPL(raw_hash_sk);\n\nvoid raw_unhash_sk(struct sock *sk)\n{\n\tstruct raw_hashinfo *h = sk->sk_prot->h.raw_hash;\n\n\twrite_lock_bh(&h->lock);\n\tif (sk_del_node_init(sk))\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\twrite_unlock_bh(&h->lock);\n}\nEXPORT_SYMBOL_GPL(raw_unhash_sk);\n\nstatic struct sock *__raw_v4_lookup(struct net *net, struct sock *sk,\n\t\tunsigned short num, __be32 raddr, __be32 laddr, int dif)\n{\n\tstruct hlist_node *node;\n\n\tsk_for_each_from(sk, node) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (net_eq(sock_net(sk), net) && inet->inet_num == num\t&&\n\t\t    !(inet->inet_daddr && inet->inet_daddr != raddr) \t&&\n\t\t    !(inet->inet_rcv_saddr && inet->inet_rcv_saddr != laddr) &&\n\t\t    !(sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\t\tgoto found; /* gotcha */\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\n/*\n *\t0 - deliver\n *\t1 - block\n */\nstatic __inline__ int icmp_filter(struct sock *sk, struct sk_buff *skb)\n{\n\tint type;\n\n\tif (!pskb_may_pull(skb, sizeof(struct icmphdr)))\n\t\treturn 1;\n\n\ttype = icmp_hdr(skb)->type;\n\tif (type < 32) {\n\t\t__u32 data = raw_sk(sk)->filter.data;\n\n\t\treturn ((1 << type) & data) != 0;\n\t}\n\n\t/* Do not block unknown ICMP types */\n\treturn 0;\n}\n\n/* IP input processing comes here for RAW socket delivery.\n * Caller owns SKB, so we must make clones.\n *\n * RFC 1122: SHOULD pass TOS value up to the transport layer.\n * -> It does. And not only TOS, but all IP header.\n */\nstatic int raw_v4_input(struct sk_buff *skb, const struct iphdr *iph, int hash)\n{\n\tstruct sock *sk;\n\tstruct hlist_head *head;\n\tint delivered = 0;\n\tstruct net *net;\n\n\tread_lock(&raw_v4_hashinfo.lock);\n\thead = &raw_v4_hashinfo.ht[hash];\n\tif (hlist_empty(head))\n\t\tgoto out;\n\n\tnet = dev_net(skb->dev);\n\tsk = __raw_v4_lookup(net, __sk_head(head), iph->protocol,\n\t\t\t     iph->saddr, iph->daddr,\n\t\t\t     skb->dev->ifindex);\n\n\twhile (sk) {\n\t\tdelivered = 1;\n\t\tif (iph->protocol != IPPROTO_ICMP || !icmp_filter(sk, skb)) {\n\t\t\tstruct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Not releasing hash table! */\n\t\t\tif (clone)\n\t\t\t\traw_rcv(sk, clone);\n\t\t}\n\t\tsk = __raw_v4_lookup(net, sk_next(sk), iph->protocol,\n\t\t\t\t     iph->saddr, iph->daddr,\n\t\t\t\t     skb->dev->ifindex);\n\t}\nout:\n\tread_unlock(&raw_v4_hashinfo.lock);\n\treturn delivered;\n}\n\nint raw_local_deliver(struct sk_buff *skb, int protocol)\n{\n\tint hash;\n\tstruct sock *raw_sk;\n\n\thash = protocol & (RAW_HTABLE_SIZE - 1);\n\traw_sk = sk_head(&raw_v4_hashinfo.ht[hash]);\n\n\t/* If there maybe a raw socket we must check - if not we\n\t * don't care less\n\t */\n\tif (raw_sk && !raw_v4_input(skb, ip_hdr(skb), hash))\n\t\traw_sk = NULL;\n\n\treturn raw_sk != NULL;\n\n}\n\nstatic void raw_err(struct sock *sk, struct sk_buff *skb, u32 info)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tint err = 0;\n\tint harderr = 0;\n\n\t/* Report error on raw socket, if:\n\t   1. User requested ip_recverr.\n\t   2. Socket is connected (otherwise the error indication\n\t      is useless without ip_recverr and error is hard.\n\t */\n\tif (!inet->recverr && sk->sk_state != TCP_ESTABLISHED)\n\t\treturn;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\treturn;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\terr = EHOSTUNREACH;\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tbreak;\n\t\terr = icmp_err_convert[code].errno;\n\t\tharderr = icmp_err_convert[code].fatal;\n\t\tif (code == ICMP_FRAG_NEEDED) {\n\t\t\tharderr = inet->pmtudisc != IP_PMTUDISC_DONT;\n\t\t\terr = EMSGSIZE;\n\t\t}\n\t}\n\n\tif (inet->recverr) {\n\t\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\t\tu8 *payload = skb->data + (iph->ihl << 2);\n\n\t\tif (inet->hdrincl)\n\t\t\tpayload = skb->data;\n\t\tip_icmp_error(sk, skb, err, 0, info, payload);\n\t}\n\n\tif (inet->recverr || harderr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t}\n}\n\nvoid raw_icmp_error(struct sk_buff *skb, int protocol, u32 info)\n{\n\tint hash;\n\tstruct sock *raw_sk;\n\tconst struct iphdr *iph;\n\tstruct net *net;\n\n\thash = protocol & (RAW_HTABLE_SIZE - 1);\n\n\tread_lock(&raw_v4_hashinfo.lock);\n\traw_sk = sk_head(&raw_v4_hashinfo.ht[hash]);\n\tif (raw_sk != NULL) {\n\t\tiph = (const struct iphdr *)skb->data;\n\t\tnet = dev_net(skb->dev);\n\n\t\twhile ((raw_sk = __raw_v4_lookup(net, raw_sk, protocol,\n\t\t\t\t\t\tiph->daddr, iph->saddr,\n\t\t\t\t\t\tskb->dev->ifindex)) != NULL) {\n\t\t\traw_err(raw_sk, skb, info);\n\t\t\traw_sk = sk_next(raw_sk);\n\t\t\tiph = (const struct iphdr *)skb->data;\n\t\t}\n\t}\n\tread_unlock(&raw_v4_hashinfo.lock);\n}\n\nstatic int raw_rcv_skb(struct sock * sk, struct sk_buff * skb)\n{\n\t/* Charge it to the socket. */\n\n\tif (ip_queue_rcv_skb(sk, skb) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\treturn NET_RX_SUCCESS;\n}\n\nint raw_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\tnf_reset(skb);\n\n\tskb_push(skb, skb->data - skb_network_header(skb));\n\n\traw_rcv_skb(sk, skb);\n\treturn 0;\n}\n\nstatic int raw_send_hdrinc(struct sock *sk, void *from, size_t length,\n\t\t\tstruct rtable **rtp,\n\t\t\tunsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct iphdr *iph;\n\tstruct sk_buff *skb;\n\tunsigned int iphlen;\n\tint err;\n\tstruct rtable *rt = *rtp;\n\n\tif (length > rt->dst.dev->mtu) {\n\t\tip_local_error(sk, EMSGSIZE, rt->rt_dst, inet->inet_dport,\n\t\t\t       rt->dst.dev->mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (flags&MSG_PROBE)\n\t\tgoto out;\n\n\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t  length + LL_ALLOCATED_SPACE(rt->dst.dev) + 15,\n\t\t\t\t  flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto error;\n\tskb_reserve(skb, LL_RESERVED_SPACE(rt->dst.dev));\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\tskb_dst_set(skb, &rt->dst);\n\t*rtp = NULL;\n\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tskb_put(skb, length);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb->transport_header = skb->network_header;\n\terr = -EFAULT;\n\tif (memcpy_fromiovecend((void *)iph, from, 0, length))\n\t\tgoto error_free;\n\n\tiphlen = iph->ihl * 4;\n\n\t/*\n\t * We don't want to modify the ip header, but we do need to\n\t * be sure that it won't cause problems later along the network\n\t * stack.  Specifically we want to make sure that iph->ihl is a\n\t * sane value.  If ihl points beyond the length of the buffer passed\n\t * in, reject the frame as invalid\n\t */\n\terr = -EINVAL;\n\tif (iphlen > length)\n\t\tgoto error_free;\n\n\tif (iphlen >= sizeof(*iph)) {\n\t\tif (!iph->saddr)\n\t\t\tiph->saddr = rt->rt_src;\n\t\tiph->check   = 0;\n\t\tiph->tot_len = htons(length);\n\t\tif (!iph->id)\n\t\t\tip_select_ident(iph, &rt->dst, NULL);\n\n\t\tiph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);\n\t}\n\tif (iph->protocol == IPPROTO_ICMP)\n\t\ticmp_out_count(net, ((struct icmphdr *)\n\t\t\tskb_transport_header(skb))->type);\n\n\terr = NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t      rt->dst.dev, dst_output);\n\tif (err > 0)\n\t\terr = net_xmit_errno(err);\n\tif (err)\n\t\tgoto error;\nout:\n\treturn 0;\n\nerror_free:\n\tkfree_skb(skb);\nerror:\n\tIP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);\n\tif (err == -ENOBUFS && !inet->recverr)\n\t\terr = 0;\n\treturn err;\n}\n\nstatic int raw_probe_proto_opt(struct flowi4 *fl4, struct msghdr *msg)\n{\n\tstruct iovec *iov;\n\tu8 __user *type = NULL;\n\tu8 __user *code = NULL;\n\tint probed = 0;\n\tunsigned int i;\n\n\tif (!msg->msg_iov)\n\t\treturn 0;\n\n\tfor (i = 0; i < msg->msg_iovlen; i++) {\n\t\tiov = &msg->msg_iov[i];\n\t\tif (!iov)\n\t\t\tcontinue;\n\n\t\tswitch (fl4->flowi4_proto) {\n\t\tcase IPPROTO_ICMP:\n\t\t\t/* check if one-byte field is readable or not. */\n\t\t\tif (iov->iov_base && iov->iov_len < 1)\n\t\t\t\tbreak;\n\n\t\t\tif (!type) {\n\t\t\t\ttype = iov->iov_base;\n\t\t\t\t/* check if code field is readable or not. */\n\t\t\t\tif (iov->iov_len > 1)\n\t\t\t\t\tcode = type + 1;\n\t\t\t} else if (!code)\n\t\t\t\tcode = iov->iov_base;\n\n\t\t\tif (type && code) {\n\t\t\t\tif (get_user(fl4->fl4_icmp_type, type) ||\n\t\t\t\t    get_user(fl4->fl4_icmp_code, code))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\tprobed = 1;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprobed = 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (probed)\n\t\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int raw_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tstatic int complained;\n\t\t\tif (!complained++)\n\t\t\t\tprintk(KERN_INFO \"%s forgot to set AF_INET in \"\n\t\t\t\t\t\t \"raw sendmsg. Fix it!\\n\",\n\t\t\t\t\t\t current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt)\n\t\tipc.opt = inet->opt;\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->faddr;\n\t\t}\n\t}\n\ttos = RT_CONN_FLAGS(sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t\t   FLOWI_FLAG_CAN_SLEEP, daddr, saddr, 0, 0);\n\n\t\tif (!inet->hdrincl) {\n\t\t\terr = raw_probe_proto_opt(&fl4, msg);\n\t\t\tif (err)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, msg->msg_iov, len,\n\t\t\t\t\t&rt, msg->msg_flags);\n\n\t else {\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = rt->rt_dst;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, ip_generic_getfrag, msg->msg_iov, len, 0,\n\t\t\t\t\t&ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic void raw_close(struct sock *sk, long timeout)\n{\n\t/*\n\t * Raw sockets may have direct kernel references. Kill them.\n\t */\n\tip_ra_control(sk, 0, NULL);\n\n\tsk_common_release(sk);\n}\n\nstatic void raw_destroy(struct sock *sk)\n{\n\tlock_sock(sk);\n\tip_flush_pending_frames(sk);\n\trelease_sock(sk);\n}\n\n/* This gets rid of all the nasties in af_inet. -DaveM */\nstatic int raw_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_in *addr = (struct sockaddr_in *) uaddr;\n\tint ret = -EINVAL;\n\tint chk_addr_ret;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_in))\n\t\tgoto out;\n\tchk_addr_ret = inet_addr_type(sock_net(sk), addr->sin_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->sin_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->sin_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\tret = 0;\nout:\treturn ret;\n}\n\n/*\n *\tThis should be easy, if there is something there\n *\twe return it, otherwise we block.\n */\n\nstatic int raw_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = ip_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}\n\nstatic int raw_init(struct sock *sk)\n{\n\tstruct raw_sock *rp = raw_sk(sk);\n\n\tif (inet_sk(sk)->inet_num == IPPROTO_ICMP)\n\t\tmemset(&rp->filter, 0, sizeof(rp->filter));\n\treturn 0;\n}\n\nstatic int raw_seticmpfilter(struct sock *sk, char __user *optval, int optlen)\n{\n\tif (optlen > sizeof(struct icmp_filter))\n\t\toptlen = sizeof(struct icmp_filter);\n\tif (copy_from_user(&raw_sk(sk)->filter, optval, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int raw_geticmpfilter(struct sock *sk, char __user *optval, int __user *optlen)\n{\n\tint len, ret = -EFAULT;\n\n\tif (get_user(len, optlen))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (len < 0)\n\t\tgoto out;\n\tif (len > sizeof(struct icmp_filter))\n\t\tlen = sizeof(struct icmp_filter);\n\tret = -EFAULT;\n\tif (put_user(len, optlen) ||\n\t    copy_to_user(optval, &raw_sk(sk)->filter, len))\n\t\tgoto out;\n\tret = 0;\nout:\treturn ret;\n}\n\nstatic int do_raw_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (optname == ICMP_FILTER) {\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMP)\n\t\t\treturn -EOPNOTSUPP;\n\t\telse\n\t\t\treturn raw_seticmpfilter(sk, optval, optlen);\n\t}\n\treturn -ENOPROTOOPT;\n}\n\nstatic int raw_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_raw_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn compat_ip_setsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int do_raw_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (optname == ICMP_FILTER) {\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMP)\n\t\t\treturn -EOPNOTSUPP;\n\t\telse\n\t\t\treturn raw_geticmpfilter(sk, optval, optlen);\n\t}\n\treturn -ENOPROTOOPT;\n}\n\nstatic int raw_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_raw_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn compat_ip_getsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int raw_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\t\tcase SIOCOUTQ: {\n\t\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\t\treturn put_user(amount, (int __user *)arg);\n\t\t}\n\t\tcase SIOCINQ: {\n\t\t\tstruct sk_buff *skb;\n\t\t\tint amount = 0;\n\n\t\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\t\tif (skb != NULL)\n\t\t\t\tamount = skb->len;\n\t\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\t\treturn put_user(amount, (int __user *)arg);\n\t\t}\n\n\t\tdefault:\n#ifdef CONFIG_IP_MROUTE\n\t\t\treturn ipmr_ioctl(sk, cmd, (void __user *)arg);\n#else\n\t\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_raw_ioctl(struct sock *sk, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\tcase SIOCINQ:\n\t\treturn -ENOIOCTLCMD;\n\tdefault:\n#ifdef CONFIG_IP_MROUTE\n\t\treturn ipmr_compat_ioctl(sk, cmd, compat_ptr(arg));\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n#endif\n\nstruct proto raw_prot = {\n\t.name\t\t   = \"RAW\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = raw_close,\n\t.destroy\t   = raw_destroy,\n\t.connect\t   = ip4_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = raw_ioctl,\n\t.init\t\t   = raw_init,\n\t.setsockopt\t   = raw_setsockopt,\n\t.getsockopt\t   = raw_getsockopt,\n\t.sendmsg\t   = raw_sendmsg,\n\t.recvmsg\t   = raw_recvmsg,\n\t.bind\t\t   = raw_bind,\n\t.backlog_rcv\t   = raw_rcv_skb,\n\t.hash\t\t   = raw_hash_sk,\n\t.unhash\t\t   = raw_unhash_sk,\n\t.obj_size\t   = sizeof(struct raw_sock),\n\t.h.raw_hash\t   = &raw_v4_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_raw_setsockopt,\n\t.compat_getsockopt = compat_raw_getsockopt,\n\t.compat_ioctl\t   = compat_raw_ioctl,\n#endif\n};\n\n#ifdef CONFIG_PROC_FS\nstatic struct sock *raw_get_first(struct seq_file *seq)\n{\n\tstruct sock *sk;\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tfor (state->bucket = 0; state->bucket < RAW_HTABLE_SIZE;\n\t\t\t++state->bucket) {\n\t\tstruct hlist_node *node;\n\n\t\tsk_for_each(sk, node, &state->h->ht[state->bucket])\n\t\t\tif (sock_net(sk) == seq_file_net(seq))\n\t\t\t\tgoto found;\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *raw_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tdo {\n\t\tsk = sk_next(sk);\ntry_again:\n\t\t;\n\t} while (sk && sock_net(sk) != seq_file_net(seq));\n\n\tif (!sk && ++state->bucket < RAW_HTABLE_SIZE) {\n\t\tsk = sk_head(&state->h->ht[state->bucket]);\n\t\tgoto try_again;\n\t}\n\treturn sk;\n}\n\nstatic struct sock *raw_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = raw_get_first(seq);\n\n\tif (sk)\n\t\twhile (pos && (sk = raw_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nvoid *raw_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tread_lock(&state->h->lock);\n\treturn *pos ? raw_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;\n}\nEXPORT_SYMBOL_GPL(raw_seq_start);\n\nvoid *raw_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = raw_get_first(seq);\n\telse\n\t\tsk = raw_get_next(seq, v);\n\t++*pos;\n\treturn sk;\n}\nEXPORT_SYMBOL_GPL(raw_seq_next);\n\nvoid raw_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tread_unlock(&state->h->lock);\n}\nEXPORT_SYMBOL_GPL(raw_seq_stop);\n\nstatic void raw_sock_seq_show(struct seq_file *seq, struct sock *sp, int i)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr,\n\t       src = inet->inet_rcv_saddr;\n\t__u16 destp = 0,\n\t      srcp  = inet->inet_num;\n\n\tseq_printf(seq, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %p %d\\n\",\n\t\ti, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tsk_rmem_alloc_get(sp),\n\t\t0, 0L, 0, sock_i_uid(sp), 0, sock_i_ino(sp),\n\t\tatomic_read(&sp->sk_refcnt), sp, atomic_read(&sp->sk_drops));\n}\n\nstatic int raw_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq, \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t\t\"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t\t\"inode ref pointer drops\\n\");\n\telse\n\t\traw_sock_seq_show(seq, v, raw_seq_private(seq)->bucket);\n\treturn 0;\n}\n\nstatic const struct seq_operations raw_seq_ops = {\n\t.start = raw_seq_start,\n\t.next  = raw_seq_next,\n\t.stop  = raw_seq_stop,\n\t.show  = raw_seq_show,\n};\n\nint raw_seq_open(struct inode *ino, struct file *file,\n\t\t struct raw_hashinfo *h, const struct seq_operations *ops)\n{\n\tint err;\n\tstruct raw_iter_state *i;\n\n\terr = seq_open_net(ino, file, ops, sizeof(struct raw_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ti = raw_seq_private((struct seq_file *)file->private_data);\n\ti->h = h;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(raw_seq_open);\n\nstatic int raw_v4_seq_open(struct inode *inode, struct file *file)\n{\n\treturn raw_seq_open(inode, file, &raw_v4_hashinfo, &raw_seq_ops);\n}\n\nstatic const struct file_operations raw_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open\t = raw_v4_seq_open,\n\t.read\t = seq_read,\n\t.llseek\t = seq_lseek,\n\t.release = seq_release_net,\n};\n\nstatic __net_init int raw_init_net(struct net *net)\n{\n\tif (!proc_net_fops_create(net, \"raw\", S_IRUGO, &raw_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void raw_exit_net(struct net *net)\n{\n\tproc_net_remove(net, \"raw\");\n}\n\nstatic __net_initdata struct pernet_operations raw_net_ops = {\n\t.init = raw_init_net,\n\t.exit = raw_exit_net,\n};\n\nint __init raw_proc_init(void)\n{\n\treturn register_pernet_subsys(&raw_net_ops);\n}\n\nvoid __init raw_proc_exit(void)\n{\n\tunregister_pernet_subsys(&raw_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n", "/*\n *  Syncookies implementation for the Linux kernel\n *\n *  Copyright (C) 1997 Andi Kleen\n *  Based on ideas by D.J.Bernstein and Eric Schenk.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/tcp.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/cryptohash.h>\n#include <linux/kernel.h>\n#include <net/tcp.h>\n#include <net/route.h>\n\n/* Timestamps: lowest bits store TCP options */\n#define TSBITS 6\n#define TSMASK (((__u32)1 << TSBITS) - 1)\n\nextern int sysctl_tcp_syncookies;\n\n__u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];\nEXPORT_SYMBOL(syncookie_secret);\n\nstatic __init int init_syncookies(void)\n{\n\tget_random_bytes(syncookie_secret, sizeof(syncookie_secret));\n\treturn 0;\n}\n__initcall(init_syncookies);\n\n#define COOKIEBITS 24\t/* Upper bits store count */\n#define COOKIEMASK (((__u32)1 << COOKIEBITS) - 1)\n\nstatic DEFINE_PER_CPU(__u32 [16 + 5 + SHA_WORKSPACE_WORDS],\n\t\t      ipv4_cookie_scratch);\n\nstatic u32 cookie_hash(__be32 saddr, __be32 daddr, __be16 sport, __be16 dport,\n\t\t       u32 count, int c)\n{\n\t__u32 *tmp = __get_cpu_var(ipv4_cookie_scratch);\n\n\tmemcpy(tmp + 4, syncookie_secret[c], sizeof(syncookie_secret[c]));\n\ttmp[0] = (__force u32)saddr;\n\ttmp[1] = (__force u32)daddr;\n\ttmp[2] = ((__force u32)sport << 16) + (__force u32)dport;\n\ttmp[3] = count;\n\tsha_transform(tmp + 16, (__u8 *)tmp, tmp + 16 + 5);\n\n\treturn tmp[17];\n}\n\n\n/*\n * when syncookies are in effect and tcp timestamps are enabled we encode\n * tcp options in the lower bits of the timestamp value that will be\n * sent in the syn-ack.\n * Since subsequent timestamps use the normal tcp_time_stamp value, we\n * must make sure that the resulting initial timestamp is <= tcp_time_stamp.\n */\n__u32 cookie_init_timestamp(struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq;\n\tu32 ts, ts_now = tcp_time_stamp;\n\tu32 options = 0;\n\n\tireq = inet_rsk(req);\n\n\toptions = ireq->wscale_ok ? ireq->snd_wscale : 0xf;\n\toptions |= ireq->sack_ok << 4;\n\toptions |= ireq->ecn_ok << 5;\n\n\tts = ts_now & ~TSMASK;\n\tts |= options;\n\tif (ts > ts_now) {\n\t\tts >>= TSBITS;\n\t\tts--;\n\t\tts <<= TSBITS;\n\t\tts |= options;\n\t}\n\treturn ts;\n}\n\n\nstatic __u32 secure_tcp_syn_cookie(__be32 saddr, __be32 daddr, __be16 sport,\n\t\t\t\t   __be16 dport, __u32 sseq, __u32 count,\n\t\t\t\t   __u32 data)\n{\n\t/*\n\t * Compute the secure sequence number.\n\t * The output should be:\n\t *   HASH(sec1,saddr,sport,daddr,dport,sec1) + sseq + (count * 2^24)\n\t *      + (HASH(sec2,saddr,sport,daddr,dport,count,sec2) % 2^24).\n\t * Where sseq is their sequence number and count increases every\n\t * minute by 1.\n\t * As an extra hack, we add a small \"data\" value that encodes the\n\t * MSS into the second hash value.\n\t */\n\n\treturn (cookie_hash(saddr, daddr, sport, dport, 0, 0) +\n\t\tsseq + (count << COOKIEBITS) +\n\t\t((cookie_hash(saddr, daddr, sport, dport, count, 1) + data)\n\t\t & COOKIEMASK));\n}\n\n/*\n * This retrieves the small \"data\" value from the syncookie.\n * If the syncookie is bad, the data returned will be out of\n * range.  This must be checked by the caller.\n *\n * The count value used to generate the cookie must be within\n * \"maxdiff\" if the current (passed-in) \"count\".  The return value\n * is (__u32)-1 if this test fails.\n */\nstatic __u32 check_tcp_syn_cookie(__u32 cookie, __be32 saddr, __be32 daddr,\n\t\t\t\t  __be16 sport, __be16 dport, __u32 sseq,\n\t\t\t\t  __u32 count, __u32 maxdiff)\n{\n\t__u32 diff;\n\n\t/* Strip away the layers from the cookie */\n\tcookie -= cookie_hash(saddr, daddr, sport, dport, 0, 0) + sseq;\n\n\t/* Cookie is now reduced to (count * 2^24) ^ (hash % 2^24) */\n\tdiff = (count - (cookie >> COOKIEBITS)) & ((__u32) - 1 >> COOKIEBITS);\n\tif (diff >= maxdiff)\n\t\treturn (__u32)-1;\n\n\treturn (cookie -\n\t\tcookie_hash(saddr, daddr, sport, dport, count - diff, 1))\n\t\t& COOKIEMASK;\t/* Leaving the data behind */\n}\n\n/*\n * MSS Values are taken from the 2009 paper\n * 'Measuring TCP Maximum Segment Size' by S. Alcock and R. Nelson:\n *  - values 1440 to 1460 accounted for 80% of observed mss values\n *  - values outside the 536-1460 range are rare (<0.2%).\n *\n * Table must be sorted.\n */\nstatic __u16 const msstab[] = {\n\t64,\n\t512,\n\t536,\n\t1024,\n\t1440,\n\t1460,\n\t4312,\n\t8960,\n};\n\n/*\n * Generate a syncookie.  mssp points to the mss, which is returned\n * rounded down to the value encoded in the cookie.\n */\n__u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, __u16 *mssp)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint mssind;\n\tconst __u16 mss = *mssp;\n\n\ttcp_synq_overflow(sk);\n\n\tfor (mssind = ARRAY_SIZE(msstab) - 1; mssind ; mssind--)\n\t\tif (mss >= msstab[mssind])\n\t\t\tbreak;\n\t*mssp = msstab[mssind];\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);\n\n\treturn secure_tcp_syn_cookie(iph->saddr, iph->daddr,\n\t\t\t\t     th->source, th->dest, ntohl(th->seq),\n\t\t\t\t     jiffies / (HZ * 60), mssind);\n}\n\n/*\n * This (misnamed) value is the age of syncookie which is permitted.\n * Its ideal value should be dependent on TCP_TIMEOUT_INIT and\n * sysctl_tcp_retries1. It's a rather complicated formula (exponential\n * backoff) to compute at runtime so it's currently hardcoded here.\n */\n#define COUNTER_TRIES 4\n/*\n * Check if a ack sequence number is a valid syncookie.\n * Return the decoded mss if it is, or 0 if not.\n */\nstatic inline int cookie_check(struct sk_buff *skb, __u32 cookie)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 seq = ntohl(th->seq) - 1;\n\t__u32 mssind = check_tcp_syn_cookie(cookie, iph->saddr, iph->daddr,\n\t\t\t\t\t    th->source, th->dest, seq,\n\t\t\t\t\t    jiffies / (HZ * 60),\n\t\t\t\t\t    COUNTER_TRIES);\n\n\treturn mssind < ARRAY_SIZE(msstab) ? msstab[mssind] : 0;\n}\n\nstatic inline struct sock *get_cookie_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t   struct request_sock *req,\n\t\t\t\t\t   struct dst_entry *dst)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sock *child;\n\n\tchild = icsk->icsk_af_ops->syn_recv_sock(sk, skb, req, dst);\n\tif (child)\n\t\tinet_csk_reqsk_queue_add(sk, req, child);\n\telse\n\t\treqsk_free(req);\n\n\treturn child;\n}\n\n\n/*\n * when syncookies are in effect and tcp timestamps are enabled we stored\n * additional tcp options in the timestamp.\n * This extracts these options from the timestamp echo.\n *\n * The lowest 4 bits store snd_wscale.\n * next 2 bits indicate SACK and ECN support.\n *\n * return false if we decode an option that should not be.\n */\nbool cookie_check_timestamp(struct tcp_options_received *tcp_opt, bool *ecn_ok)\n{\n\t/* echoed timestamp, lowest bits contain options */\n\tu32 options = tcp_opt->rcv_tsecr & TSMASK;\n\n\tif (!tcp_opt->saw_tstamp)  {\n\t\ttcp_clear_options(tcp_opt);\n\t\treturn true;\n\t}\n\n\tif (!sysctl_tcp_timestamps)\n\t\treturn false;\n\n\ttcp_opt->sack_ok = (options >> 4) & 0x1;\n\t*ecn_ok = (options >> 5) & 1;\n\tif (*ecn_ok && !sysctl_tcp_ecn)\n\t\treturn false;\n\n\tif (tcp_opt->sack_ok && !sysctl_tcp_sack)\n\t\treturn false;\n\n\tif ((options & 0xf) == 0xf)\n\t\treturn true; /* no window scaling */\n\n\ttcp_opt->wscale_ok = 1;\n\ttcp_opt->snd_wscale = options & 0xf;\n\treturn sysctl_tcp_window_scaling != 0;\n}\nEXPORT_SYMBOL(cookie_check_timestamp);\n\nstruct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct ip_options *opt)\n{\n\tstruct tcp_options_received tcp_opt;\n\tu8 *hash_location;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct rtable *rt;\n\t__u8 rcv_wscale;\n\tbool ecn_ok;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk) ||\n\t    (mss = cookie_check(skb, cookie)) == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, &hash_location, 0);\n\n\tif (!cookie_check_timestamp(&tcp_opt, &ecn_ok))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp_request_sock_ops); /* for safety */\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->rcv_isn\t\t= ntohl(th->seq) - 1;\n\ttreq->snt_isn\t\t= cookie;\n\treq->mss\t\t= mss;\n\tireq->loc_port\t\t= th->dest;\n\tireq->rmt_port\t\t= th->source;\n\tireq->loc_addr\t\t= ip_hdr(skb)->daddr;\n\tireq->rmt_addr\t\t= ip_hdr(skb)->saddr;\n\tireq->ecn_ok\t\t= ecn_ok;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\n\t/* We throwed the options of the initial SYN away, so we hope\n\t * the ACK carries the same options again (see RFC1122 4.2.3.8)\n\t */\n\tif (opt && opt->optlen) {\n\t\tint opt_size = sizeof(struct ip_options) + opt->optlen;\n\n\t\tireq->opt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (ireq->opt != NULL && ip_options_echo(ireq->opt, skb)) {\n\t\t\tkfree(ireq->opt);\n\t\t\tireq->opt = NULL;\n\t\t}\n\t}\n\n\tif (security_inet_conn_request(sk, skb, req)) {\n\t\treqsk_free(req);\n\t\tgoto out;\n\t}\n\n\treq->expires\t= 0UL;\n\treq->retrans\t= 0;\n\n\t/*\n\t * We need to lookup the route here to get at the correct\n\t * window size. We should better make sure that the window size\n\t * hasn't changed since we received the original syn, but I see\n\t * no easy way to do this.\n\t */\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, 0, sk->sk_mark, RT_CONN_FLAGS(sk),\n\t\t\t\t   RT_SCOPE_UNIVERSE, IPPROTO_TCP,\n\t\t\t\t   inet_sk_flowi_flags(sk),\n\t\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t\t   ireq->loc_addr, th->source, th->dest);\n\t\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt)) {\n\t\t\treqsk_free(req);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Try to redo what tcp_v4_send_synack did. */\n\treq->window_clamp = tp->window_clamp ? :dst_metric(&rt->dst, RTAX_WINDOW);\n\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rcv_wnd, &req->window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(&rt->dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale  = rcv_wscale;\n\n\tret = get_cookie_sock(sk, skb, req, &rt->dst);\nout:\treturn ret;\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n *\t\tIPv4 specific functions\n *\n *\n *\t\tcode split from:\n *\t\tlinux/ipv4/tcp.c\n *\t\tlinux/ipv4/tcp_input.c\n *\t\tlinux/ipv4/tcp_output.c\n *\n *\t\tSee tcp.c for author information\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n/*\n * Changes:\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tThis code is dedicated to John Dyson.\n *\t\tDavid S. Miller :\tChange semantics of established hash,\n *\t\t\t\t\thalf is devoted to TIME_WAIT sockets\n *\t\t\t\t\tand the rest go in the other half.\n *\t\tAndi Kleen :\t\tAdd support for syncookies and fixed\n *\t\t\t\t\tsome bugs: ip options weren't passed to\n *\t\t\t\t\tthe TCP layer, missed a check for an\n *\t\t\t\t\tACK bit.\n *\t\tAndi Kleen :\t\tImplemented fast path mtu discovery.\n *\t     \t\t\t\tFixed many serious bugs in the\n *\t\t\t\t\trequest_sock handling and moved\n *\t\t\t\t\tmost of it into the af independent code.\n *\t\t\t\t\tAdded tail drop and some other bugfixes.\n *\t\t\t\t\tAdded new listen semantics.\n *\t\tMike McLagan\t:\tRouting by source\n *\tJuan Jose Ciarlante:\t\tip_dynaddr bits\n *\t\tAndi Kleen:\t\tvarious fixes.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year\n *\t\t\t\t\tcoma.\n *\tAndi Kleen\t\t:\tFix new listen.\n *\tAndi Kleen\t\t:\tFix accept error reporting.\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n */\n\n\n#include <linux/bottom_half.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/cache.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/inet_hashtables.h>\n#include <net/tcp.h>\n#include <net/transp_v6.h>\n#include <net/ipv6.h>\n#include <net/inet_common.h>\n#include <net/timewait_sock.h>\n#include <net/xfrm.h>\n#include <net/netdma.h>\n\n#include <linux/inet.h>\n#include <linux/ipv6.h>\n#include <linux/stddef.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n\n#include <linux/crypto.h>\n#include <linux/scatterlist.h>\n\nint sysctl_tcp_tw_reuse __read_mostly;\nint sysctl_tcp_low_latency __read_mostly;\nEXPORT_SYMBOL(sysctl_tcp_low_latency);\n\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic struct tcp_md5sig_key *tcp_v4_md5_do_lookup(struct sock *sk,\n\t\t\t\t\t\t   __be32 addr);\nstatic int tcp_v4_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       __be32 daddr, __be32 saddr, struct tcphdr *th);\n#else\nstatic inline\nstruct tcp_md5sig_key *tcp_v4_md5_do_lookup(struct sock *sk, __be32 addr)\n{\n\treturn NULL;\n}\n#endif\n\nstruct inet_hashinfo tcp_hashinfo;\nEXPORT_SYMBOL(tcp_hashinfo);\n\nstatic inline __u32 tcp_v4_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_tcp_sequence_number(ip_hdr(skb)->daddr,\n\t\t\t\t\t  ip_hdr(skb)->saddr,\n\t\t\t\t\t  tcp_hdr(skb)->dest,\n\t\t\t\t\t  tcp_hdr(skb)->source);\n}\n\nint tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)\n{\n\tconst struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* With PAWS, it is safe from the viewpoint\n\t   of data integrity. Even without PAWS it is safe provided sequence\n\t   spaces do not overlap i.e. at data rates <= 80Mbit/sec.\n\n\t   Actually, the idea is close to VJ's one, only timestamp cache is\n\t   held not per host, but per port pair and TW bucket is used as state\n\t   holder.\n\n\t   If TW bucket has been already destroyed we fall back to VJ's scheme\n\t   and use initial timestamp retrieved from peer table.\n\t */\n\tif (tcptw->tw_ts_recent_stamp &&\n\t    (twp == NULL || (sysctl_tcp_tw_reuse &&\n\t\t\t     get_seconds() - tcptw->tw_ts_recent_stamp > 1))) {\n\t\ttp->write_seq = tcptw->tw_snd_nxt + 65535 + 2;\n\t\tif (tp->write_seq == 0)\n\t\t\ttp->write_seq = 1;\n\t\ttp->rx_opt.ts_recent\t   = tcptw->tw_ts_recent;\n\t\ttp->rx_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;\n\t\tsock_hold(sktw);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(tcp_twsk_unique);\n\n/* This will initiate an outgoing connection. */\nint tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tif (inet->opt && inet->opt->srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet->opt->faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_TCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet->opt || !inet->opt->srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t/* Reset inherited state */\n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq\t\t   = 0;\n\t}\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {\n\t\tstruct inet_peer *peer = rt_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet->opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t/* Socket identity is still unknown (sport may be zero).\n\t * However we set state to SYN-SENT and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t\t   inet->inet_daddr,\n\t\t\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t\t\t   usin->sin_port);\n\n\tinet->inet_id = tp->write_seq ^ jiffies;\n\n\terr = tcp_connect(sk);\n\trt = NULL;\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port,\n\t * if necessary.\n\t */\n\ttcp_set_state(sk, TCP_CLOSE);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_v4_connect);\n\n/*\n * This routine does path mtu discovery as defined in RFC1191.\n */\nstatic void do_pmtu_discovery(struct sock *sk, const struct iphdr *iph, u32 mtu)\n{\n\tstruct dst_entry *dst;\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t/* We are not interested in TCP_LISTEN and open_requests (SYN-ACKs\n\t * send out by Linux are always <576bytes so they should go through\n\t * unfragmented).\n\t */\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn;\n\n\t/* We don't check in the destentry if pmtu discovery is forbidden\n\t * on this route. We just assume that no packet_to_big packets\n\t * are send back when pmtu discovery is not active.\n\t * There is a small race when the user changes this flag in the\n\t * route, but I think that's acceptable.\n\t */\n\tif ((dst = __sk_dst_check(sk, 0)) == NULL)\n\t\treturn;\n\n\tdst->ops->update_pmtu(dst, mtu);\n\n\t/* Something is about to be wrong... Remember soft error\n\t * for the case, if this connection will not able to recover.\n\t */\n\tif (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))\n\t\tsk->sk_err_soft = EMSGSIZE;\n\n\tmtu = dst_mtu(dst);\n\n\tif (inet->pmtudisc != IP_PMTUDISC_DONT &&\n\t    inet_csk(sk)->icsk_pmtu_cookie > mtu) {\n\t\ttcp_sync_mss(sk, mtu);\n\n\t\t/* Resend the TCP packet because it's\n\t\t * clear that the old packet has been\n\t\t * dropped. This is the new \"fast\" path mtu\n\t\t * discovery.\n\t\t */\n\t\ttcp_simple_retransmit(sk);\n\t} /* else let the usual retransmit timer handle it */\n}\n\n/*\n * This routine is called by the ICMP module when it gets some\n * sort of error condition.  If err < 0 then the socket should\n * be closed and the error returned to the user.  If err > 0\n * it's just the icmp type << 8 | icmp code.  After adjustment\n * header points to the first 8 bytes of the tcp header.  We need\n * to find the appropriate port.\n *\n * The locking strategy used here is very \"optimistic\". When\n * someone else accesses the socket the ICMP is just dropped\n * and for some paths there is no check at all.\n * A more general error queue to queue errors for later handling\n * is probably better.\n *\n */\n\nvoid tcp_v4_err(struct sk_buff *icmp_skb, u32 info)\n{\n\tconst struct iphdr *iph = (const struct iphdr *)icmp_skb->data;\n\tstruct tcphdr *th = (struct tcphdr *)(icmp_skb->data + (iph->ihl << 2));\n\tstruct inet_connection_sock *icsk;\n\tstruct tcp_sock *tp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(icmp_skb)->type;\n\tconst int code = icmp_hdr(icmp_skb)->code;\n\tstruct sock *sk;\n\tstruct sk_buff *skb;\n\t__u32 seq;\n\t__u32 remaining;\n\tint err;\n\tstruct net *net = dev_net(icmp_skb->dev);\n\n\tif (icmp_skb->len < (iph->ihl << 2) + 8) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = inet_lookup(net, &tcp_hashinfo, iph->daddr, th->dest,\n\t\t\tiph->saddr, th->source, inet_iif(icmp_skb));\n\tif (!sk) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t */\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ticsk = inet_csk(sk);\n\ttp = tcp_sk(sk);\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, tp->snd_una, tp->snd_nxt)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tdo_pmtu_discovery(sk, iph, info);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\t/* check if icmp_skb allows revert of backoff\n\t\t * (see draft-zimmermann-tcp-lcd) */\n\t\tif (code != ICMP_NET_UNREACH && code != ICMP_HOST_UNREACH)\n\t\t\tbreak;\n\t\tif (seq != tp->snd_una  || !icsk->icsk_retransmits ||\n\t\t    !icsk->icsk_backoff)\n\t\t\tbreak;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tbreak;\n\n\t\ticsk->icsk_backoff--;\n\t\tinet_csk(sk)->icsk_rto = __tcp_set_rto(tp) <<\n\t\t\t\t\t icsk->icsk_backoff;\n\t\ttcp_bound_rto(sk);\n\n\t\tskb = tcp_write_queue_head(sk);\n\t\tBUG_ON(!skb);\n\n\t\tremaining = icsk->icsk_rto - min(icsk->icsk_rto,\n\t\t\t\ttcp_time_stamp - TCP_SKB_CB(skb)->when);\n\n\t\tif (remaining) {\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t  remaining, TCP_RTO_MAX);\n\t\t} else {\n\t\t\t/* RTO revert clocked out retransmission.\n\t\t\t * Will retransmit now */\n\t\t\ttcp_retransmit_timer(sk);\n\t\t}\n\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req, **prev;\n\tcase TCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\n\t\treq = inet_csk_search_req(sk, &prev, th->dest,\n\t\t\t\t\t  iph->daddr, iph->saddr);\n\t\tif (!req)\n\t\t\tgoto out;\n\n\t\t/* ICMPs are not backlogged, hence we cannot get\n\t\t   an established socket here.\n\t\t */\n\t\tWARN_ON(req->sk);\n\n\t\tif (seq != tcp_rsk(req)->snt_isn) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * Still in SYN_RECV, just remove it silently.\n\t\t * There is no good way to pass the error to the newly\n\t\t * created socket, and POSIX does not want network\n\t\t * errors returned from accept().\n\t\t */\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:  /* Cannot happen.\n\t\t\t       It can f.e. if SYNs crossed.\n\t\t\t     */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\ttcp_done(sk);\n\t\t} else {\n\t\t\tsk->sk_err_soft = err;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\t{ /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\n\t}\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\nstatic void __tcp_v4_send_check(struct sk_buff *skb,\n\t\t\t\t__be32 saddr, __be32 daddr)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tth->check = ~tcp_v4_check(skb->len, saddr, daddr, 0);\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct tcphdr, check);\n\t} else {\n\t\tth->check = tcp_v4_check(skb->len, saddr, daddr,\n\t\t\t\t\t csum_partial(th,\n\t\t\t\t\t\t      th->doff << 2,\n\t\t\t\t\t\t      skb->csum));\n\t}\n}\n\n/* This routine computes an IPv4 TCP checksum. */\nvoid tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__tcp_v4_send_check(skb, inet->inet_saddr, inet->inet_daddr);\n}\nEXPORT_SYMBOL(tcp_v4_send_check);\n\nint tcp_v4_gso_send_check(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct tcphdr *th;\n\n\tif (!pskb_may_pull(skb, sizeof(*th)))\n\t\treturn -EINVAL;\n\n\tiph = ip_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tth->check = 0;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t__tcp_v4_send_check(skb, iph->saddr, iph->daddr);\n\treturn 0;\n}\n\n/*\n *\tThis routine will send an RST to the other tcp.\n *\n *\tSomeone asks: why I NEVER use socket parameters (TOS, TTL etc.)\n *\t\t      for reset.\n *\tAnswer: if a packet caused RST, it is not for a socket\n *\t\texisting in our system, if it is matched to a socket,\n *\t\tit is just duplicate segment or bug in other side's TCP.\n *\t\tSo that we build reply only basing on parameters\n *\t\tarrived with segment.\n *\tException: precedence violation. We do not implement it in any case.\n */\n\nstatic void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tstruct {\n\t\tstruct tcphdr th;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t__be32 opt[(TCPOLEN_MD5SIG_ALIGNED >> 2)];\n#endif\n\t} rep;\n\tstruct ip_reply_arg arg;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct net *net;\n\n\t/* Never send a reset in response to a reset. */\n\tif (th->rst)\n\t\treturn;\n\n\tif (skb_rtable(skb)->rt_type != RTN_LOCAL)\n\t\treturn;\n\n\t/* Swap the send and the receive. */\n\tmemset(&rep, 0, sizeof(rep));\n\trep.th.dest   = th->source;\n\trep.th.source = th->dest;\n\trep.th.doff   = sizeof(struct tcphdr) / 4;\n\trep.th.rst    = 1;\n\n\tif (th->ack) {\n\t\trep.th.seq = th->ack_seq;\n\t} else {\n\t\trep.th.ack = 1;\n\t\trep.th.ack_seq = htonl(ntohl(th->seq) + th->syn + th->fin +\n\t\t\t\t       skb->len - (th->doff << 2));\n\t}\n\n\tmemset(&arg, 0, sizeof(arg));\n\targ.iov[0].iov_base = (unsigned char *)&rep;\n\targ.iov[0].iov_len  = sizeof(rep.th);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tkey = sk ? tcp_v4_md5_do_lookup(sk, ip_hdr(skb)->daddr) : NULL;\n\tif (key) {\n\t\trep.opt[0] = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t   (TCPOPT_NOP << 16) |\n\t\t\t\t   (TCPOPT_MD5SIG << 8) |\n\t\t\t\t   TCPOLEN_MD5SIG);\n\t\t/* Update length and the length the header thinks exists */\n\t\targ.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;\n\t\trep.th.doff = arg.iov[0].iov_len / 4;\n\n\t\ttcp_v4_md5_hash_hdr((__u8 *) &rep.opt[1],\n\t\t\t\t     key, ip_hdr(skb)->saddr,\n\t\t\t\t     ip_hdr(skb)->daddr, &rep.th);\n\t}\n#endif\n\targ.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,\n\t\t\t\t      ip_hdr(skb)->saddr, /* XXX */\n\t\t\t\t      arg.iov[0].iov_len, IPPROTO_TCP, 0);\n\targ.csumoffset = offsetof(struct tcphdr, check) / 2;\n\targ.flags = (sk && inet_sk(sk)->transparent) ? IP_REPLY_ARG_NOSRCCHECK : 0;\n\n\tnet = dev_net(skb_dst(skb)->dev);\n\tip_send_reply(net->ipv4.tcp_sock, skb,\n\t\t      &arg, arg.iov[0].iov_len);\n\n\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n\tTCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);\n}\n\n/* The code following below sending ACKs in SYN-RECV and TIME-WAIT states\n   outside socket context is ugly, certainly. What can I do?\n */\n\nstatic void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,\n\t\t\t    u32 win, u32 ts, int oif,\n\t\t\t    struct tcp_md5sig_key *key,\n\t\t\t    int reply_flags)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tstruct {\n\t\tstruct tcphdr th;\n\t\t__be32 opt[(TCPOLEN_TSTAMP_ALIGNED >> 2)\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\t   + (TCPOLEN_MD5SIG_ALIGNED >> 2)\n#endif\n\t\t\t];\n\t} rep;\n\tstruct ip_reply_arg arg;\n\tstruct net *net = dev_net(skb_dst(skb)->dev);\n\n\tmemset(&rep.th, 0, sizeof(struct tcphdr));\n\tmemset(&arg, 0, sizeof(arg));\n\n\targ.iov[0].iov_base = (unsigned char *)&rep;\n\targ.iov[0].iov_len  = sizeof(rep.th);\n\tif (ts) {\n\t\trep.opt[0] = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t   (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t   TCPOLEN_TIMESTAMP);\n\t\trep.opt[1] = htonl(tcp_time_stamp);\n\t\trep.opt[2] = htonl(ts);\n\t\targ.iov[0].iov_len += TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\n\t/* Swap the send and the receive. */\n\trep.th.dest    = th->source;\n\trep.th.source  = th->dest;\n\trep.th.doff    = arg.iov[0].iov_len / 4;\n\trep.th.seq     = htonl(seq);\n\trep.th.ack_seq = htonl(ack);\n\trep.th.ack     = 1;\n\trep.th.window  = htons(win);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key) {\n\t\tint offset = (ts) ? 3 : 0;\n\n\t\trep.opt[offset++] = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t\t  (TCPOPT_NOP << 16) |\n\t\t\t\t\t  (TCPOPT_MD5SIG << 8) |\n\t\t\t\t\t  TCPOLEN_MD5SIG);\n\t\targ.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;\n\t\trep.th.doff = arg.iov[0].iov_len/4;\n\n\t\ttcp_v4_md5_hash_hdr((__u8 *) &rep.opt[offset],\n\t\t\t\t    key, ip_hdr(skb)->saddr,\n\t\t\t\t    ip_hdr(skb)->daddr, &rep.th);\n\t}\n#endif\n\targ.flags = reply_flags;\n\targ.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,\n\t\t\t\t      ip_hdr(skb)->saddr, /* XXX */\n\t\t\t\t      arg.iov[0].iov_len, IPPROTO_TCP, 0);\n\targ.csumoffset = offsetof(struct tcphdr, check) / 2;\n\tif (oif)\n\t\targ.bound_dev_if = oif;\n\n\tip_send_reply(net->ipv4.tcp_sock, skb,\n\t\t      &arg, arg.iov[0].iov_len);\n\n\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n}\n\nstatic void tcp_v4_timewait_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\n\ttcp_v4_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcptw->tw_ts_recent,\n\t\t\ttw->tw_bound_dev_if,\n\t\t\ttcp_twsk_md5_key(tcptw),\n\t\t\ttw->tw_transparent ? IP_REPLY_ARG_NOSRCCHECK : 0\n\t\t\t);\n\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v4_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\ttcp_v4_send_ack(skb, tcp_rsk(req)->snt_isn + 1,\n\t\t\ttcp_rsk(req)->rcv_isn + 1, req->rcv_wnd,\n\t\t\treq->ts_recent,\n\t\t\t0,\n\t\t\ttcp_v4_md5_do_lookup(sk, ip_hdr(skb)->daddr),\n\t\t\tinet_rsk(req)->no_srccheck ? IP_REPLY_ARG_NOSRCCHECK : 0);\n}\n\n/*\n *\tSend a SYN-ACK after having received a SYN.\n *\tThis still operates on a request_sock only, not on a big\n *\tsocket.\n */\nstatic int tcp_v4_send_synack(struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct request_values *rvp)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tint err = -1;\n\tstruct sk_buff * skb;\n\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\treturn -1;\n\n\tskb = tcp_make_synack(sk, dst, req, rvp);\n\n\tif (skb) {\n\t\t__tcp_v4_send_check(skb, ireq->loc_addr, ireq->rmt_addr);\n\n\t\terr = ip_build_and_send_pkt(skb, sk, ireq->loc_addr,\n\t\t\t\t\t    ireq->rmt_addr,\n\t\t\t\t\t    ireq->opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic int tcp_v4_rtx_synack(struct sock *sk, struct request_sock *req,\n\t\t\t      struct request_values *rvp)\n{\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\treturn tcp_v4_send_synack(sk, NULL, req, rvp);\n}\n\n/*\n *\tIPv4 request_sock destructor.\n */\nstatic void tcp_v4_reqsk_destructor(struct request_sock *req)\n{\n\tkfree(inet_rsk(req)->opt);\n}\n\nstatic void syn_flood_warning(const struct sk_buff *skb)\n{\n\tconst char *msg;\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (sysctl_tcp_syncookies)\n\t\tmsg = \"Sending cookies\";\n\telse\n#endif\n\t\tmsg = \"Dropping request\";\n\n\tpr_info(\"TCP: Possible SYN flooding on port %d. %s.\\n\",\n\t\t\t\tntohs(tcp_hdr(skb)->dest), msg);\n}\n\n/*\n * Save and compile IPv4 options into the request_sock if needed.\n */\nstatic struct ip_options *tcp_v4_save_options(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct ip_options *opt = &(IPCB(skb)->opt);\n\tstruct ip_options *dopt = NULL;\n\n\tif (opt && opt->optlen) {\n\t\tint opt_size = optlength(opt);\n\t\tdopt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (dopt) {\n\t\t\tif (ip_options_echo(dopt, skb)) {\n\t\t\t\tkfree(dopt);\n\t\t\t\tdopt = NULL;\n\t\t\t}\n\t\t}\n\t}\n\treturn dopt;\n}\n\n#ifdef CONFIG_TCP_MD5SIG\n/*\n * RFC2385 MD5 checksumming requires a mapping of\n * IP address->MD5 Key.\n * We need to maintain these in the sk structure.\n */\n\n/* Find the Key structure for an address.  */\nstatic struct tcp_md5sig_key *\n\t\t\ttcp_v4_md5_do_lookup(struct sock *sk, __be32 addr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tif (!tp->md5sig_info || !tp->md5sig_info->entries4)\n\t\treturn NULL;\n\tfor (i = 0; i < tp->md5sig_info->entries4; i++) {\n\t\tif (tp->md5sig_info->keys4[i].addr == addr)\n\t\t\treturn &tp->md5sig_info->keys4[i].base;\n\t}\n\treturn NULL;\n}\n\nstruct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,\n\t\t\t\t\t struct sock *addr_sk)\n{\n\treturn tcp_v4_md5_do_lookup(sk, inet_sk(addr_sk)->inet_daddr);\n}\nEXPORT_SYMBOL(tcp_v4_md5_lookup);\n\nstatic struct tcp_md5sig_key *tcp_v4_reqsk_md5_lookup(struct sock *sk,\n\t\t\t\t\t\t      struct request_sock *req)\n{\n\treturn tcp_v4_md5_do_lookup(sk, inet_rsk(req)->rmt_addr);\n}\n\n/* This can be called on a newly created socket, from other files */\nint tcp_v4_md5_do_add(struct sock *sk, __be32 addr,\n\t\t      u8 *newkey, u8 newkeylen)\n{\n\t/* Add Key to the list */\n\tstruct tcp_md5sig_key *key;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp4_md5sig_key *keys;\n\n\tkey = tcp_v4_md5_do_lookup(sk, addr);\n\tif (key) {\n\t\t/* Pre-existing entry - just update that one. */\n\t\tkfree(key->key);\n\t\tkey->key = newkey;\n\t\tkey->keylen = newkeylen;\n\t} else {\n\t\tstruct tcp_md5sig_info *md5sig;\n\n\t\tif (!tp->md5sig_info) {\n\t\t\ttp->md5sig_info = kzalloc(sizeof(*tp->md5sig_info),\n\t\t\t\t\t\t  GFP_ATOMIC);\n\t\t\tif (!tp->md5sig_info) {\n\t\t\t\tkfree(newkey);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t\t}\n\t\tif (tcp_alloc_md5sig_pool(sk) == NULL) {\n\t\t\tkfree(newkey);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmd5sig = tp->md5sig_info;\n\n\t\tif (md5sig->alloced4 == md5sig->entries4) {\n\t\t\tkeys = kmalloc((sizeof(*keys) *\n\t\t\t\t\t(md5sig->entries4 + 1)), GFP_ATOMIC);\n\t\t\tif (!keys) {\n\t\t\t\tkfree(newkey);\n\t\t\t\ttcp_free_md5sig_pool();\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tif (md5sig->entries4)\n\t\t\t\tmemcpy(keys, md5sig->keys4,\n\t\t\t\t       sizeof(*keys) * md5sig->entries4);\n\n\t\t\t/* Free old key list, and reference new one */\n\t\t\tkfree(md5sig->keys4);\n\t\t\tmd5sig->keys4 = keys;\n\t\t\tmd5sig->alloced4++;\n\t\t}\n\t\tmd5sig->entries4++;\n\t\tmd5sig->keys4[md5sig->entries4 - 1].addr        = addr;\n\t\tmd5sig->keys4[md5sig->entries4 - 1].base.key    = newkey;\n\t\tmd5sig->keys4[md5sig->entries4 - 1].base.keylen = newkeylen;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_v4_md5_do_add);\n\nstatic int tcp_v4_md5_add_func(struct sock *sk, struct sock *addr_sk,\n\t\t\t       u8 *newkey, u8 newkeylen)\n{\n\treturn tcp_v4_md5_do_add(sk, inet_sk(addr_sk)->inet_daddr,\n\t\t\t\t newkey, newkeylen);\n}\n\nint tcp_v4_md5_do_del(struct sock *sk, __be32 addr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tfor (i = 0; i < tp->md5sig_info->entries4; i++) {\n\t\tif (tp->md5sig_info->keys4[i].addr == addr) {\n\t\t\t/* Free the key */\n\t\t\tkfree(tp->md5sig_info->keys4[i].base.key);\n\t\t\ttp->md5sig_info->entries4--;\n\n\t\t\tif (tp->md5sig_info->entries4 == 0) {\n\t\t\t\tkfree(tp->md5sig_info->keys4);\n\t\t\t\ttp->md5sig_info->keys4 = NULL;\n\t\t\t\ttp->md5sig_info->alloced4 = 0;\n\t\t\t} else if (tp->md5sig_info->entries4 != i) {\n\t\t\t\t/* Need to do some manipulation */\n\t\t\t\tmemmove(&tp->md5sig_info->keys4[i],\n\t\t\t\t\t&tp->md5sig_info->keys4[i+1],\n\t\t\t\t\t(tp->md5sig_info->entries4 - i) *\n\t\t\t\t\t sizeof(struct tcp4_md5sig_key));\n\t\t\t}\n\t\t\ttcp_free_md5sig_pool();\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\nEXPORT_SYMBOL(tcp_v4_md5_do_del);\n\nstatic void tcp_v4_clear_md5_list(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Free each key, then the set of key keys,\n\t * the crypto element, and then decrement our\n\t * hold on the last resort crypto.\n\t */\n\tif (tp->md5sig_info->entries4) {\n\t\tint i;\n\t\tfor (i = 0; i < tp->md5sig_info->entries4; i++)\n\t\t\tkfree(tp->md5sig_info->keys4[i].base.key);\n\t\ttp->md5sig_info->entries4 = 0;\n\t\ttcp_free_md5sig_pool();\n\t}\n\tif (tp->md5sig_info->keys4) {\n\t\tkfree(tp->md5sig_info->keys4);\n\t\ttp->md5sig_info->keys4 = NULL;\n\t\ttp->md5sig_info->alloced4  = 0;\n\t}\n}\n\nstatic int tcp_v4_parse_md5_keys(struct sock *sk, char __user *optval,\n\t\t\t\t int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)&cmd.tcpm_addr;\n\tu8 *newkey;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin->sin_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tif (!cmd.tcpm_key || !cmd.tcpm_keylen) {\n\t\tif (!tcp_sk(sk)->md5sig_info)\n\t\t\treturn -ENOENT;\n\t\treturn tcp_v4_md5_do_del(sk, sin->sin_addr.s_addr);\n\t}\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\tif (!tcp_sk(sk)->md5sig_info) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tstruct tcp_md5sig_info *p;\n\n\t\tp = kzalloc(sizeof(*p), sk->sk_allocation);\n\t\tif (!p)\n\t\t\treturn -EINVAL;\n\n\t\ttp->md5sig_info = p;\n\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t}\n\n\tnewkey = kmemdup(cmd.tcpm_key, cmd.tcpm_keylen, sk->sk_allocation);\n\tif (!newkey)\n\t\treturn -ENOMEM;\n\treturn tcp_v4_md5_do_add(sk, sin->sin_addr.s_addr,\n\t\t\t\t newkey, cmd.tcpm_keylen);\n}\n\nstatic int tcp_v4_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,\n\t\t\t\t\t__be32 daddr, __be32 saddr, int nbytes)\n{\n\tstruct tcp4_pseudohdr *bp;\n\tstruct scatterlist sg;\n\n\tbp = &hp->md5_blk.ip4;\n\n\t/*\n\t * 1. the TCP pseudo-header (in the order: source IP address,\n\t * destination IP address, zero-padded protocol number, and\n\t * segment length)\n\t */\n\tbp->saddr = saddr;\n\tbp->daddr = daddr;\n\tbp->pad = 0;\n\tbp->protocol = IPPROTO_TCP;\n\tbp->len = cpu_to_be16(nbytes);\n\n\tsg_init_one(&sg, bp, sizeof(*bp));\n\treturn crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));\n}\n\nstatic int tcp_v4_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       __be32 daddr, __be32 saddr, struct tcphdr *th)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\tif (tcp_v4_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nint tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\tstruct sock *sk, struct request_sock *req,\n\t\t\tstruct sk_buff *skb)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\tstruct tcphdr *th = tcp_hdr(skb);\n\t__be32 saddr, daddr;\n\n\tif (sk) {\n\t\tsaddr = inet_sk(sk)->inet_saddr;\n\t\tdaddr = inet_sk(sk)->inet_daddr;\n\t} else if (req) {\n\t\tsaddr = inet_rsk(req)->loc_addr;\n\t\tdaddr = inet_rsk(req)->rmt_addr;\n\t} else {\n\t\tconst struct iphdr *iph = ip_hdr(skb);\n\t\tsaddr = iph->saddr;\n\t\tdaddr = iph->daddr;\n\t}\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\n\tif (tcp_v4_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\nEXPORT_SYMBOL(tcp_v4_md5_hash_skb);\n\nstatic int tcp_v4_inbound_md5_hash(struct sock *sk, struct sk_buff *skb)\n{\n\t/*\n\t * This gets called for each TCP segment that arrives\n\t * so we want to be efficient.\n\t * We have 3 drop cases:\n\t * o No MD5 hash and one expected.\n\t * o MD5 hash and we're not expecting one.\n\t * o MD5 hash and its wrong.\n\t */\n\t__u8 *hash_location = NULL;\n\tstruct tcp_md5sig_key *hash_expected;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tint genhash;\n\tunsigned char newhash[16];\n\n\thash_expected = tcp_v4_md5_do_lookup(sk, iph->saddr);\n\thash_location = tcp_parse_md5sig_option(th);\n\n\t/* We've parsed the options - do we have a hash? */\n\tif (!hash_expected && !hash_location)\n\t\treturn 0;\n\n\tif (hash_expected && !hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);\n\t\treturn 1;\n\t}\n\n\tif (!hash_expected && hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);\n\t\treturn 1;\n\t}\n\n\t/* Okay, so this is hash_expected and hash_location -\n\t * so we need to calculate the checksum.\n\t */\n\tgenhash = tcp_v4_md5_hash_skb(newhash,\n\t\t\t\t      hash_expected,\n\t\t\t\t      NULL, NULL, skb);\n\n\tif (genhash || memcmp(hash_location, newhash, 16) != 0) {\n\t\tif (net_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"MD5 Hash failed for (%pI4, %d)->(%pI4, %d)%s\\n\",\n\t\t\t       &iph->saddr, ntohs(th->source),\n\t\t\t       &iph->daddr, ntohs(th->dest),\n\t\t\t       genhash ? \" tcp_v4_calc_md5_hash failed\" : \"\");\n\t\t}\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n#endif\n\nstruct request_sock_ops tcp_request_sock_ops __read_mostly = {\n\t.family\t\t=\tPF_INET,\n\t.obj_size\t=\tsizeof(struct tcp_request_sock),\n\t.rtx_syn_ack\t=\ttcp_v4_rtx_synack,\n\t.send_ack\t=\ttcp_v4_reqsk_send_ack,\n\t.destructor\t=\ttcp_v4_reqsk_destructor,\n\t.send_reset\t=\ttcp_v4_send_reset,\n\t.syn_ack_timeout = \ttcp_syn_ack_timeout,\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {\n\t.md5_lookup\t=\ttcp_v4_reqsk_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n};\n#endif\n\nint tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_extend_values tmp_ext;\n\tstruct tcp_options_received tmp_opt;\n\tu8 *hash_location;\n\tstruct request_sock *req;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct dst_entry *dst = NULL;\n\t__be32 saddr = ip_hdr(skb)->saddr;\n\t__be32 daddr = ip_hdr(skb)->daddr;\n\t__u32 isn = TCP_SKB_CB(skb)->when;\n#ifdef CONFIG_SYN_COOKIES\n\tint want_cookie = 0;\n#else\n#define want_cookie 0 /* Argh, why doesn't gcc optimize this :( */\n#endif\n\n\t/* Never answer to SYNs send to broadcast or multicast */\n\tif (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto drop;\n\n\t/* TW buckets are converted to open requests without\n\t * limitations, they conserve resources and peer is\n\t * evidently real one.\n\t */\n\tif (inet_csk_reqsk_queue_is_full(sk) && !isn) {\n\t\tif (net_ratelimit())\n\t\t\tsyn_flood_warning(skb);\n#ifdef CONFIG_SYN_COOKIES\n\t\tif (sysctl_tcp_syncookies) {\n\t\t\twant_cookie = 1;\n\t\t} else\n#endif\n\t\tgoto drop;\n\t}\n\n\t/* Accept backlog is full. If we have already queued enough\n\t * of warm entries in syn queue, drop request. It is better than\n\t * clogging syn queue with openreqs with exponentially increasing\n\t * timeout.\n\t */\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet_reqsk_alloc(&tcp_request_sock_ops);\n\tif (!req)\n\t\tgoto drop;\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttcp_rsk(req)->af_specific = &tcp_request_sock_ipv4_ops;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = TCP_MSS_DEFAULT;\n\ttmp_opt.user_mss  = tp->rx_opt.user_mss;\n\ttcp_parse_options(skb, &tmp_opt, &hash_location, 0);\n\n\tif (tmp_opt.cookie_plus > 0 &&\n\t    tmp_opt.saw_tstamp &&\n\t    !tp->rx_opt.cookie_out_never &&\n\t    (sysctl_tcp_cookie_size > 0 ||\n\t     (tp->cookie_values != NULL &&\n\t      tp->cookie_values->cookie_desired > 0))) {\n\t\tu8 *c;\n\t\tu32 *mess = &tmp_ext.cookie_bakery[COOKIE_DIGEST_WORDS];\n\t\tint l = tmp_opt.cookie_plus - TCPOLEN_COOKIE_BASE;\n\n\t\tif (tcp_cookie_generator(&tmp_ext.cookie_bakery[0]) != 0)\n\t\t\tgoto drop_and_release;\n\n\t\t/* Secret recipe starts with IP addresses */\n\t\t*mess++ ^= (__force u32)daddr;\n\t\t*mess++ ^= (__force u32)saddr;\n\n\t\t/* plus variable length Initiator Cookie */\n\t\tc = (u8 *)mess;\n\t\twhile (l-- > 0)\n\t\t\t*c++ ^= *hash_location++;\n\n#ifdef CONFIG_SYN_COOKIES\n\t\twant_cookie = 0;\t/* not our kind of cookie */\n#endif\n\t\ttmp_ext.cookie_out_never = 0; /* false */\n\t\ttmp_ext.cookie_plus = tmp_opt.cookie_plus;\n\t} else if (!tp->rx_opt.cookie_in_always) {\n\t\t/* redundant indications, but ensure initialization. */\n\t\ttmp_ext.cookie_out_never = 1; /* true */\n\t\ttmp_ext.cookie_plus = 0;\n\t} else {\n\t\tgoto drop_and_release;\n\t}\n\ttmp_ext.cookie_in_always = tp->rx_opt.cookie_in_always;\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb);\n\n\tireq = inet_rsk(req);\n\tireq->loc_addr = daddr;\n\tireq->rmt_addr = saddr;\n\tireq->no_srccheck = inet_sk(sk)->transparent;\n\tireq->opt = tcp_v4_save_options(sk, skb);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tif (!want_cookie || tmp_opt.tstamp_ok)\n\t\tTCP_ECN_create_request(req, tcp_hdr(skb));\n\n\tif (want_cookie) {\n\t\tisn = cookie_v4_init_sequence(sk, skb, &req->mss);\n\t\treq->cookie_ts = tmp_opt.tstamp_ok;\n\t} else if (!isn) {\n\t\tstruct inet_peer *peer = NULL;\n\n\t\t/* VJ's idea. We save last timestamp seen\n\t\t * from the destination in peer table, when entering\n\t\t * state TIME-WAIT, and check against it before\n\t\t * accepting new connection request.\n\t\t *\n\t\t * If \"isn\" is not zero, this request hit alive\n\t\t * timewait bucket, so that all the necessary checks\n\t\t * are made in the function processing timewait state.\n\t\t */\n\t\tif (tmp_opt.saw_tstamp &&\n\t\t    tcp_death_row.sysctl_tw_recycle &&\n\t\t    (dst = inet_csk_route_req(sk, req)) != NULL &&\n\t\t    (peer = rt_get_peer((struct rtable *)dst)) != NULL &&\n\t\t    peer->daddr.addr.a4 == saddr) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp < TCP_PAWS_MSL &&\n\t\t\t    (s32)(peer->tcp_ts - req->ts_recent) >\n\t\t\t\t\t\t\tTCP_PAWS_WINDOW) {\n\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);\n\t\t\t\tgoto drop_and_release;\n\t\t\t}\n\t\t}\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\telse if (!sysctl_tcp_syncookies &&\n\t\t\t (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t\t  (sysctl_max_syn_backlog >> 2)) &&\n\t\t\t (!peer || !peer->tcp_ts_stamp) &&\n\t\t\t (!dst || !dst_metric(dst, RTAX_RTT))) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tLIMIT_NETDEBUG(KERN_DEBUG \"TCP: drop open request from %pI4/%u\\n\",\n\t\t\t\t       &saddr, ntohs(tcp_hdr(skb)->source));\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = tcp_v4_init_sequence(skb);\n\t}\n\ttcp_rsk(req)->snt_isn = isn;\n\n\tif (tcp_v4_send_synack(sk, dst, req,\n\t\t\t       (struct request_values *)&tmp_ext) ||\n\t    want_cookie)\n\t\tgoto drop_and_free;\n\n\tinet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_v4_conn_request);\n\n\n/*\n * The three way handshake has completed - we got a valid synack -\n * now create the new socket.\n */\nstruct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto exit_nonewsk;\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(newsk, dst);\n\n\tnewtp\t\t      = tcp_sk(newsk);\n\tnewinet\t\t      = inet_sk(newsk);\n\tireq\t\t      = inet_rsk(req);\n\tnewinet->inet_daddr   = ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t      = ireq->loc_addr;\n\tnewinet->opt\t      = ireq->opt;\n\tireq->opt\t      = NULL;\n\tnewinet->mc_index     = inet_iif(skb);\n\tnewinet->mc_ttl\t      = ip_hdr(skb)->ttl;\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newinet->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = newinet->opt->optlen;\n\tnewinet->inet_id = newtp->write_seq ^ jiffies;\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v4_md5_do_lookup(sk, newinet->inet_daddr);\n\tif (key != NULL) {\n\t\t/*\n\t\t * We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v4_md5_do_add(newsk, newinet->inet_daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t\tsk_nocaps_add(newsk, NETIF_F_GSO_MASK);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_v4_syn_recv_sock);\n\nstatic struct sock *tcp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct sock *nsk;\n\tstruct request_sock **prev;\n\t/* Find possible connection requests. */\n\tstruct request_sock *req = inet_csk_search_req(sk, &prev, th->source,\n\t\t\t\t\t\t       iph->saddr, iph->daddr);\n\tif (req)\n\t\treturn tcp_check_req(sk, skb, req, prev);\n\n\tnsk = inet_lookup_established(sock_net(sk), &tcp_hashinfo, iph->saddr,\n\t\t\tth->source, iph->daddr, th->dest, inet_iif(skb));\n\n\tif (nsk) {\n\t\tif (nsk->sk_state != TCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (!th->syn)\n\t\tsk = cookie_v4_check(sk, skb, &(IPCB(skb)->opt));\n#endif\n\treturn sk;\n}\n\nstatic __sum16 tcp_v4_checksum_init(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!tcp_v4_check(skb->len, iph->saddr,\n\t\t\t\t  iph->daddr, skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t       skb->len, IPPROTO_TCP, 0);\n\n\tif (skb->len <= 76) {\n\t\treturn __skb_checksum_complete(skb);\n\t}\n\treturn 0;\n}\n\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nint tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sock *rsk;\n#ifdef CONFIG_TCP_MD5SIG\n\t/*\n\t * We really want to reject the packet as early as possible\n\t * if:\n\t *  o We're expecting an MD5'd packet and this is no MD5 tcp option\n\t *  o There is an MD5 option and we're not expecting one\n\t */\n\tif (tcp_v4_inbound_md5_hash(sk, skb))\n\t\tgoto discard;\n#endif\n\n\tif (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\t\tif (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len)) {\n\t\t\trsk = sk;\n\t\t\tgoto reset;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v4_hnd_req(sk, skb);\n\t\tif (!nsk)\n\t\t\tgoto discard;\n\n\t\tif (nsk != sk) {\n\t\t\tif (tcp_child_process(sk, nsk, skb)) {\n\t\t\t\trsk = nsk;\n\t\t\t\tgoto reset;\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\tif (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb->len)) {\n\t\trsk = sk;\n\t\tgoto reset;\n\t}\n\treturn 0;\n\nreset:\n\ttcp_v4_send_reset(rsk, skb);\ndiscard:\n\tkfree_skb(skb);\n\t/* Be careful here. If this function gets more complicated and\n\t * gcc suffers from register pressure on the x86, sk (in %ebx)\n\t * might be destroyed here. This current version compiles correctly,\n\t * but you have been warned.\n\t */\n\treturn 0;\n\ncsum_err:\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n}\nEXPORT_SYMBOL(tcp_v4_do_rcv);\n\n/*\n *\tFrom tcp_input.c\n */\n\nint tcp_v4_rcv(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct tcphdr *th;\n\tstruct sock *sk;\n\tint ret;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/* Count it even if it's bad */\n\tTCP_INC_STATS_BH(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr) / 4)\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff * 4))\n\t\tgoto discard_it;\n\n\t/* An explanation is required here, I think.\n\t * Packet length and doff are validated by header prediction,\n\t * provided case of th->doff==0 is eliminated.\n\t * So, we defer the checks. */\n\tif (!skb_csum_unnecessary(skb) && tcp_v4_checksum_init(skb))\n\t\tgoto bad_packet;\n\n\tth = tcp_hdr(skb);\n\tiph = ip_hdr(skb);\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff * 4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->when\t = 0;\n\tTCP_SKB_CB(skb)->flags\t = iph->tos;\n\tTCP_SKB_CB(skb)->sacked\t = 0;\n\n\tsk = __inet_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\tnf_reset(skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tbh_lock_sock_nested(sk);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n#ifdef CONFIG_NET_DMA\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)\n\t\t\ttp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);\n\t\tif (tp->ucopy.dma_chan)\n\t\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\telse\n#endif\n\t\t{\n\t\t\tif (!tcp_prequeue(sk, skb))\n\t\t\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\t}\n\t} else if (unlikely(sk_add_backlog(sk, skb))) {\n\t\tbh_unlock_sock(sk);\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\n\tsock_put(sk);\n\n\treturn ret;\n\nno_tcp_socket:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\tif (skb->len < (th->doff << 2) || tcp_checksum_complete(skb)) {\nbad_packet:\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v4_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\t/* Discard frame. */\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tif (skb->len < (th->doff << 2) || tcp_checksum_complete(skb)) {\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN: {\n\t\tstruct sock *sk2 = inet_lookup_listener(dev_net(skb->dev),\n\t\t\t\t\t\t\t&tcp_hashinfo,\n\t\t\t\t\t\t\tiph->daddr, th->dest,\n\t\t\t\t\t\t\tinet_iif(skb));\n\t\tif (sk2) {\n\t\t\tinet_twsk_deschedule(inet_twsk(sk), &tcp_death_row);\n\t\t\tinet_twsk_put(inet_twsk(sk));\n\t\t\tsk = sk2;\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v4_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\tgoto no_tcp_socket;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n\nstruct inet_peer *tcp_v4_get_peer(struct sock *sk, bool *release_it)\n{\n\tstruct rtable *rt = (struct rtable *) __sk_dst_get(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_peer *peer;\n\n\tif (!rt || rt->rt_dst != inet->inet_daddr) {\n\t\tpeer = inet_getpeer_v4(inet->inet_daddr, 1);\n\t\t*release_it = true;\n\t} else {\n\t\tif (!rt->peer)\n\t\t\trt_bind_peer(rt, 1);\n\t\tpeer = rt->peer;\n\t\t*release_it = false;\n\t}\n\n\treturn peer;\n}\nEXPORT_SYMBOL(tcp_v4_get_peer);\n\nvoid *tcp_v4_tw_get_peer(struct sock *sk)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\n\treturn inet_getpeer_v4(tw->tw_daddr, 1);\n}\nEXPORT_SYMBOL(tcp_v4_tw_get_peer);\n\nstatic struct timewait_sock_ops tcp_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp_timewait_sock),\n\t.twsk_unique\t= tcp_twsk_unique,\n\t.twsk_destructor= tcp_twsk_destructor,\n\t.twsk_getpeer\t= tcp_v4_tw_get_peer,\n};\n\nconst struct inet_connection_sock_af_ops ipv4_specific = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = tcp_v4_conn_request,\n\t.syn_recv_sock\t   = tcp_v4_syn_recv_sock,\n\t.get_peer\t   = tcp_v4_get_peer,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\n\t.bind_conflict\t   = inet_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ip_setsockopt,\n\t.compat_getsockopt = compat_ip_getsockopt,\n#endif\n};\nEXPORT_SYMBOL(ipv4_specific);\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv4_specific = {\n\t.md5_lookup\t\t= tcp_v4_md5_lookup,\n\t.calc_md5_hash\t\t= tcp_v4_md5_hash_skb,\n\t.md5_add\t\t= tcp_v4_md5_add_func,\n\t.md5_parse\t\t= tcp_v4_parse_md5_keys,\n};\n#endif\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int tcp_v4_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tskb_queue_head_init(&tp->out_of_order_queue);\n\ttcp_init_xmit_timers(sk);\n\ttcp_prequeue_init(tp);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ttp->mdev = TCP_TIMEOUT_INIT;\n\n\t/* So many TCP implementations out there (incorrectly) count the\n\t * initial SYN frame in their delayed-ACK and congestion control\n\t * algorithms that we must have the following bandaid to talk\n\t * efficiently to them.  -DaveM\n\t */\n\ttp->snd_cwnd = 2;\n\n\t/* See draft-stevens-tcpca-spec-01 for discussion of the\n\t * initialization of these values.\n\t */\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = sysctl_tcp_reordering;\n\ticsk->icsk_ca_ops = &tcp_init_congestion_ops;\n\n\tsk->sk_state = TCP_CLOSE;\n\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n\ticsk->icsk_af_ops = &ipv4_specific;\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n#ifdef CONFIG_TCP_MD5SIG\n\ttp->af_specific = &tcp_sock_ipv4_specific;\n#endif\n\n\t/* TCP Cookie Transactions */\n\tif (sysctl_tcp_cookie_size > 0) {\n\t\t/* Default, cookies without s_data_payload. */\n\t\ttp->cookie_values =\n\t\t\tkzalloc(sizeof(*tp->cookie_values),\n\t\t\t\tsk->sk_allocation);\n\t\tif (tp->cookie_values != NULL)\n\t\t\tkref_init(&tp->cookie_values->kref);\n\t}\n\t/* Presumed zeroed, in order of appearance:\n\t *\tcookie_in_always, cookie_out_never,\n\t *\ts_data_constant, s_data_in, s_data_out\n\t */\n\tsk->sk_sndbuf = sysctl_tcp_wmem[1];\n\tsk->sk_rcvbuf = sysctl_tcp_rmem[1];\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&tcp_sockets_allocated);\n\tlocal_bh_enable();\n\n\treturn 0;\n}\n\nvoid tcp_v4_destroy_sock(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_clear_xmit_timers(sk);\n\n\ttcp_cleanup_congestion_control(sk);\n\n\t/* Cleanup up the write buffer. */\n\ttcp_write_queue_purge(sk);\n\n\t/* Cleans up our, hopefully empty, out_of_order_queue. */\n\t__skb_queue_purge(&tp->out_of_order_queue);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Clean up the MD5 key list, if any */\n\tif (tp->md5sig_info) {\n\t\ttcp_v4_clear_md5_list(sk);\n\t\tkfree(tp->md5sig_info);\n\t\ttp->md5sig_info = NULL;\n\t}\n#endif\n\n#ifdef CONFIG_NET_DMA\n\t/* Cleans up our sk_async_wait_queue */\n\t__skb_queue_purge(&sk->sk_async_wait_queue);\n#endif\n\n\t/* Clean prequeue, it must be empty really */\n\t__skb_queue_purge(&tp->ucopy.prequeue);\n\n\t/* Clean up a referenced TCP bind bucket. */\n\tif (inet_csk(sk)->icsk_bind_hash)\n\t\tinet_put_port(sk);\n\n\t/*\n\t * If sendmsg cached page exists, toss it.\n\t */\n\tif (sk->sk_sndmsg_page) {\n\t\t__free_page(sk->sk_sndmsg_page);\n\t\tsk->sk_sndmsg_page = NULL;\n\t}\n\n\t/* TCP Cookie Transactions */\n\tif (tp->cookie_values != NULL) {\n\t\tkref_put(&tp->cookie_values->kref,\n\t\t\t tcp_cookie_values_release);\n\t\ttp->cookie_values = NULL;\n\t}\n\n\tpercpu_counter_dec(&tcp_sockets_allocated);\n}\nEXPORT_SYMBOL(tcp_v4_destroy_sock);\n\n#ifdef CONFIG_PROC_FS\n/* Proc filesystem TCP sock list dumping. */\n\nstatic inline struct inet_timewait_sock *tw_head(struct hlist_nulls_head *head)\n{\n\treturn hlist_nulls_empty(head) ? NULL :\n\t\tlist_entry(head->first, struct inet_timewait_sock, tw_node);\n}\n\nstatic inline struct inet_timewait_sock *tw_next(struct inet_timewait_sock *tw)\n{\n\treturn !is_a_nulls(tw->tw_node.next) ?\n\t\thlist_nulls_entry(tw->tw_node.next, typeof(*tw), tw_node) : NULL;\n}\n\n/*\n * Get next listener socket follow cur.  If cur is NULL, get first socket\n * starting from bucket given in st->bucket; when st->bucket is zero the\n * very first socket in the hash table is returned.\n */\nstatic void *listening_get_next(struct seq_file *seq, void *cur)\n{\n\tstruct inet_connection_sock *icsk;\n\tstruct hlist_nulls_node *node;\n\tstruct sock *sk = cur;\n\tstruct inet_listen_hashbucket *ilb;\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tif (!sk) {\n\t\tilb = &tcp_hashinfo.listening_hash[st->bucket];\n\t\tspin_lock_bh(&ilb->lock);\n\t\tsk = sk_nulls_head(&ilb->head);\n\t\tst->offset = 0;\n\t\tgoto get_sk;\n\t}\n\tilb = &tcp_hashinfo.listening_hash[st->bucket];\n\t++st->num;\n\t++st->offset;\n\n\tif (st->state == TCP_SEQ_STATE_OPENREQ) {\n\t\tstruct request_sock *req = cur;\n\n\t\ticsk = inet_csk(st->syn_wait_sk);\n\t\treq = req->dl_next;\n\t\twhile (1) {\n\t\t\twhile (req) {\n\t\t\t\tif (req->rsk_ops->family == st->family) {\n\t\t\t\t\tcur = req;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\treq = req->dl_next;\n\t\t\t}\n\t\t\tif (++st->sbucket >= icsk->icsk_accept_queue.listen_opt->nr_table_entries)\n\t\t\t\tbreak;\nget_req:\n\t\t\treq = icsk->icsk_accept_queue.listen_opt->syn_table[st->sbucket];\n\t\t}\n\t\tsk\t  = sk_nulls_next(st->syn_wait_sk);\n\t\tst->state = TCP_SEQ_STATE_LISTENING;\n\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t} else {\n\t\ticsk = inet_csk(sk);\n\t\tread_lock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\tif (reqsk_queue_len(&icsk->icsk_accept_queue))\n\t\t\tgoto start_req;\n\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\tsk = sk_nulls_next(sk);\n\t}\nget_sk:\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\t\tif (sk->sk_family == st->family) {\n\t\t\tcur = sk;\n\t\t\tgoto out;\n\t\t}\n\t\ticsk = inet_csk(sk);\n\t\tread_lock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\tif (reqsk_queue_len(&icsk->icsk_accept_queue)) {\nstart_req:\n\t\t\tst->uid\t\t= sock_i_uid(sk);\n\t\t\tst->syn_wait_sk = sk;\n\t\t\tst->state\t= TCP_SEQ_STATE_OPENREQ;\n\t\t\tst->sbucket\t= 0;\n\t\t\tgoto get_req;\n\t\t}\n\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t}\n\tspin_unlock_bh(&ilb->lock);\n\tst->offset = 0;\n\tif (++st->bucket < INET_LHTABLE_SIZE) {\n\t\tilb = &tcp_hashinfo.listening_hash[st->bucket];\n\t\tspin_lock_bh(&ilb->lock);\n\t\tsk = sk_nulls_head(&ilb->head);\n\t\tgoto get_sk;\n\t}\n\tcur = NULL;\nout:\n\treturn cur;\n}\n\nstatic void *listening_get_idx(struct seq_file *seq, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tst->bucket = 0;\n\tst->offset = 0;\n\trc = listening_get_next(seq, NULL);\n\n\twhile (rc && *pos) {\n\t\trc = listening_get_next(seq, rc);\n\t\t--*pos;\n\t}\n\treturn rc;\n}\n\nstatic inline int empty_bucket(struct tcp_iter_state *st)\n{\n\treturn hlist_nulls_empty(&tcp_hashinfo.ehash[st->bucket].chain) &&\n\t\thlist_nulls_empty(&tcp_hashinfo.ehash[st->bucket].twchain);\n}\n\n/*\n * Get first established socket starting from bucket given in st->bucket.\n * If st->bucket is zero, the very first socket in the hash is returned.\n */\nstatic void *established_get_first(struct seq_file *seq)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\tvoid *rc = NULL;\n\n\tst->offset = 0;\n\tfor (; st->bucket <= tcp_hashinfo.ehash_mask; ++st->bucket) {\n\t\tstruct sock *sk;\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct inet_timewait_sock *tw;\n\t\tspinlock_t *lock = inet_ehash_lockp(&tcp_hashinfo, st->bucket);\n\n\t\t/* Lockless fast path for the common case of empty buckets */\n\t\tif (empty_bucket(st))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(lock);\n\t\tsk_nulls_for_each(sk, node, &tcp_hashinfo.ehash[st->bucket].chain) {\n\t\t\tif (sk->sk_family != st->family ||\n\t\t\t    !net_eq(sock_net(sk), net)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\trc = sk;\n\t\t\tgoto out;\n\t\t}\n\t\tst->state = TCP_SEQ_STATE_TIME_WAIT;\n\t\tinet_twsk_for_each(tw, node,\n\t\t\t\t   &tcp_hashinfo.ehash[st->bucket].twchain) {\n\t\t\tif (tw->tw_family != st->family ||\n\t\t\t    !net_eq(twsk_net(tw), net)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\trc = tw;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock_bh(lock);\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t}\nout:\n\treturn rc;\n}\n\nstatic void *established_get_next(struct seq_file *seq, void *cur)\n{\n\tstruct sock *sk = cur;\n\tstruct inet_timewait_sock *tw;\n\tstruct hlist_nulls_node *node;\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\t++st->num;\n\t++st->offset;\n\n\tif (st->state == TCP_SEQ_STATE_TIME_WAIT) {\n\t\ttw = cur;\n\t\ttw = tw_next(tw);\nget_tw:\n\t\twhile (tw && (tw->tw_family != st->family || !net_eq(twsk_net(tw), net))) {\n\t\t\ttw = tw_next(tw);\n\t\t}\n\t\tif (tw) {\n\t\t\tcur = tw;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\n\t\t/* Look for next non empty bucket */\n\t\tst->offset = 0;\n\t\twhile (++st->bucket <= tcp_hashinfo.ehash_mask &&\n\t\t\t\tempty_bucket(st))\n\t\t\t;\n\t\tif (st->bucket > tcp_hashinfo.ehash_mask)\n\t\t\treturn NULL;\n\n\t\tspin_lock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));\n\t\tsk = sk_nulls_head(&tcp_hashinfo.ehash[st->bucket].chain);\n\t} else\n\t\tsk = sk_nulls_next(sk);\n\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (sk->sk_family == st->family && net_eq(sock_net(sk), net))\n\t\t\tgoto found;\n\t}\n\n\tst->state = TCP_SEQ_STATE_TIME_WAIT;\n\ttw = tw_head(&tcp_hashinfo.ehash[st->bucket].twchain);\n\tgoto get_tw;\nfound:\n\tcur = sk;\nout:\n\treturn cur;\n}\n\nstatic void *established_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tst->bucket = 0;\n\trc = established_get_first(seq);\n\n\twhile (rc && pos) {\n\t\trc = established_get_next(seq, rc);\n\t\t--pos;\n\t}\n\treturn rc;\n}\n\nstatic void *tcp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tvoid *rc;\n\tstruct tcp_iter_state *st = seq->private;\n\n\tst->state = TCP_SEQ_STATE_LISTENING;\n\trc\t  = listening_get_idx(seq, &pos);\n\n\tif (!rc) {\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\trc\t  = established_get_idx(seq, pos);\n\t}\n\n\treturn rc;\n}\n\nstatic void *tcp_seek_last_pos(struct seq_file *seq)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tint offset = st->offset;\n\tint orig_num = st->num;\n\tvoid *rc = NULL;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_OPENREQ:\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\tif (st->bucket >= INET_LHTABLE_SIZE)\n\t\t\tbreak;\n\t\tst->state = TCP_SEQ_STATE_LISTENING;\n\t\trc = listening_get_next(seq, NULL);\n\t\twhile (offset-- && rc)\n\t\t\trc = listening_get_next(seq, rc);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tst->bucket = 0;\n\t\t/* Fallthrough */\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\tif (st->bucket > tcp_hashinfo.ehash_mask)\n\t\t\tbreak;\n\t\trc = established_get_first(seq);\n\t\twhile (offset-- && rc)\n\t\t\trc = established_get_next(seq, rc);\n\t}\n\n\tst->num = orig_num;\n\n\treturn rc;\n}\n\nstatic void *tcp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tif (*pos && *pos == st->last_pos) {\n\t\trc = tcp_seek_last_pos(seq);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\tst->state = TCP_SEQ_STATE_LISTENING;\n\tst->num = 0;\n\tst->bucket = 0;\n\tst->offset = 0;\n\trc = *pos ? tcp_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;\n\nout:\n\tst->last_pos = *pos;\n\treturn rc;\n}\n\nstatic void *tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc = NULL;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\trc = tcp_get_idx(seq, 0);\n\t\tgoto out;\n\t}\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_OPENREQ:\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\trc = listening_get_next(seq, v);\n\t\tif (!rc) {\n\t\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\t\tst->bucket = 0;\n\t\t\tst->offset = 0;\n\t\t\trc\t  = established_get_first(seq);\n\t\t}\n\t\tbreak;\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\trc = established_get_next(seq, v);\n\t\tbreak;\n\t}\nout:\n\t++*pos;\n\tst->last_pos = *pos;\n\treturn rc;\n}\n\nstatic void tcp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_OPENREQ:\n\t\tif (v) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(st->syn_wait_sk);\n\t\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\t}\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\tif (v != SEQ_START_TOKEN)\n\t\t\tspin_unlock_bh(&tcp_hashinfo.listening_hash[st->bucket].lock);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tif (v)\n\t\t\tspin_unlock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));\n\t\tbreak;\n\t}\n}\n\nstatic int tcp_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct tcp_seq_afinfo *afinfo = PDE(inode)->data;\n\tstruct tcp_iter_state *s;\n\tint err;\n\n\terr = seq_open_net(inode, file, &afinfo->seq_ops,\n\t\t\t  sizeof(struct tcp_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ts = ((struct seq_file *)file->private_data)->private;\n\ts->family\t\t= afinfo->family;\n\ts->last_pos \t\t= 0;\n\treturn 0;\n}\n\nint tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo)\n{\n\tint rc = 0;\n\tstruct proc_dir_entry *p;\n\n\tafinfo->seq_fops.open\t\t= tcp_seq_open;\n\tafinfo->seq_fops.read\t\t= seq_read;\n\tafinfo->seq_fops.llseek\t\t= seq_lseek;\n\tafinfo->seq_fops.release\t= seq_release_net;\n\n\tafinfo->seq_ops.start\t\t= tcp_seq_start;\n\tafinfo->seq_ops.next\t\t= tcp_seq_next;\n\tafinfo->seq_ops.stop\t\t= tcp_seq_stop;\n\n\tp = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,\n\t\t\t     &afinfo->seq_fops, afinfo);\n\tif (!p)\n\t\trc = -ENOMEM;\n\treturn rc;\n}\nEXPORT_SYMBOL(tcp_proc_register);\n\nvoid tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo)\n{\n\tproc_net_remove(net, afinfo->name);\n}\nEXPORT_SYMBOL(tcp_proc_unregister);\n\nstatic void get_openreq4(struct sock *sk, struct request_sock *req,\n\t\t\t struct seq_file *f, int i, int uid, int *len)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tint ttd = req->expires - jiffies;\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %u %d %p%n\",\n\t\ti,\n\t\tireq->loc_addr,\n\t\tntohs(inet_sk(sk)->inet_sport),\n\t\tireq->rmt_addr,\n\t\tntohs(ireq->rmt_port),\n\t\tTCP_SYN_RECV,\n\t\t0, 0, /* could print option size, but that is af dependent. */\n\t\t1,    /* timers active (only the expire timer) */\n\t\tjiffies_to_clock_t(ttd),\n\t\treq->retrans,\n\t\tuid,\n\t\t0,  /* non standard timer */\n\t\t0, /* open_requests have no inode */\n\t\tatomic_read(&sk->sk_refcnt),\n\t\treq,\n\t\tlen);\n}\n\nstatic void get_tcp4_sock(struct sock *sk, struct seq_file *f, int i, int *len)\n{\n\tint timer_active;\n\tunsigned long timer_expires;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src = inet->inet_rcv_saddr;\n\t__u16 destp = ntohs(inet->inet_dport);\n\t__u16 srcp = ntohs(inet->inet_sport);\n\tint rx_queue;\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (timer_pending(&sk->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sk->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\trx_queue = sk->sk_ack_backlog;\n\telse\n\t\t/*\n\t\t * because we dont lock socket, we might find a transient negative value\n\t\t */\n\t\trx_queue = max_t(int, tp->rcv_nxt - tp->copied_seq, 0);\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X %02X %08X:%08X %02X:%08lX \"\n\t\t\t\"%08X %5d %8d %lu %d %p %lu %lu %u %u %d%n\",\n\t\ti, src, srcp, dest, destp, sk->sk_state,\n\t\ttp->write_seq - tp->snd_una,\n\t\trx_queue,\n\t\ttimer_active,\n\t\tjiffies_to_clock_t(timer_expires - jiffies),\n\t\ticsk->icsk_retransmits,\n\t\tsock_i_uid(sk),\n\t\ticsk->icsk_probes_out,\n\t\tsock_i_ino(sk),\n\t\tatomic_read(&sk->sk_refcnt), sk,\n\t\tjiffies_to_clock_t(icsk->icsk_rto),\n\t\tjiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t(icsk->icsk_ack.quick << 1) | icsk->icsk_ack.pingpong,\n\t\ttp->snd_cwnd,\n\t\ttcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh,\n\t\tlen);\n}\n\nstatic void get_timewait4_sock(struct inet_timewait_sock *tw,\n\t\t\t       struct seq_file *f, int i, int *len)\n{\n\t__be32 dest, src;\n\t__u16 destp, srcp;\n\tint ttd = tw->tw_ttd - jiffies;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tdest  = tw->tw_daddr;\n\tsrc   = tw->tw_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p%n\",\n\t\ti, src, srcp, dest, destp, tw->tw_substate, 0, 0,\n\t\t3, jiffies_to_clock_t(ttd), 0, 0, 0, 0,\n\t\tatomic_read(&tw->tw_refcnt), tw, len);\n}\n\n#define TMPSZ 150\n\nstatic int tcp4_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\tint len;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_printf(seq, \"%-*s\\n\", TMPSZ - 1,\n\t\t\t   \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tget_tcp4_sock(v, seq, st->num, &len);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_OPENREQ:\n\t\tget_openreq4(st->syn_wait_sk, v, seq, st->num, st->uid, &len);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\tget_timewait4_sock(v, seq, st->num, &len);\n\t\tbreak;\n\t}\n\tseq_printf(seq, \"%*s\\n\", TMPSZ - 1 - len, \"\");\nout:\n\treturn 0;\n}\n\nstatic struct tcp_seq_afinfo tcp4_seq_afinfo = {\n\t.name\t\t= \"tcp\",\n\t.family\t\t= AF_INET,\n\t.seq_fops\t= {\n\t\t.owner\t\t= THIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= tcp4_seq_show,\n\t},\n};\n\nstatic int __net_init tcp4_proc_init_net(struct net *net)\n{\n\treturn tcp_proc_register(net, &tcp4_seq_afinfo);\n}\n\nstatic void __net_exit tcp4_proc_exit_net(struct net *net)\n{\n\ttcp_proc_unregister(net, &tcp4_seq_afinfo);\n}\n\nstatic struct pernet_operations tcp4_net_ops = {\n\t.init = tcp4_proc_init_net,\n\t.exit = tcp4_proc_exit_net,\n};\n\nint __init tcp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&tcp4_net_ops);\n}\n\nvoid tcp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&tcp4_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n\nstruct sk_buff **tcp4_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = skb_gro_network_header(skb);\n\n\tswitch (skb->ip_summed) {\n\tcase CHECKSUM_COMPLETE:\n\t\tif (!tcp_v4_check(skb_gro_len(skb), iph->saddr, iph->daddr,\n\t\t\t\t  skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* fall through */\n\tcase CHECKSUM_NONE:\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\treturn tcp_gro_receive(head, skb);\n}\n\nint tcp4_gro_complete(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tth->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  iph->saddr, iph->daddr, 0);\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\n\treturn tcp_gro_complete(skb);\n}\n\nstruct proto tcp_prot = {\n\t.name\t\t\t= \"TCP\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.connect\t\t= tcp_v4_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v4_init_sock,\n\t.destroy\t\t= tcp_v4_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.sendpage\t\t= tcp_sendpage,\n\t.backlog_rcv\t\t= tcp_v4_do_rcv,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\t.orphan_count\t\t= &tcp_orphan_count,\n\t.memory_allocated\t= &tcp_memory_allocated,\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem\t\t= sysctl_tcp_wmem,\n\t.sysctl_rmem\t\t= sysctl_tcp_rmem,\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.twsk_prot\t\t= &tcp_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp_request_sock_ops,\n\t.h.hashinfo\t\t= &tcp_hashinfo,\n\t.no_autobind\t\t= true,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_tcp_setsockopt,\n\t.compat_getsockopt\t= compat_tcp_getsockopt,\n#endif\n};\nEXPORT_SYMBOL(tcp_prot);\n\n\nstatic int __net_init tcp_sk_init(struct net *net)\n{\n\treturn inet_ctl_sock_create(&net->ipv4.tcp_sock,\n\t\t\t\t    PF_INET, SOCK_RAW, IPPROTO_TCP, net);\n}\n\nstatic void __net_exit tcp_sk_exit(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->ipv4.tcp_sock);\n}\n\nstatic void __net_exit tcp_sk_exit_batch(struct list_head *net_exit_list)\n{\n\tinet_twsk_purge(&tcp_hashinfo, &tcp_death_row, AF_INET);\n}\n\nstatic struct pernet_operations __net_initdata tcp_sk_ops = {\n       .init\t   = tcp_sk_init,\n       .exit\t   = tcp_sk_exit,\n       .exit_batch = tcp_sk_exit_batch,\n};\n\nvoid __init tcp_v4_init(void)\n{\n\tinet_hashinfo_init(&tcp_hashinfo);\n\tif (register_pernet_subsys(&tcp_sk_ops))\n\t\tpanic(\"Failed to create the TCP control socket.\\n\");\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe User Datagram Protocol (UDP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() calls\n *\t\tAlan Cox\t: \tstopped close while in use off icmp\n *\t\t\t\t\tmessages. Not a fix but a botch that\n *\t\t\t\t\tfor udp at least is 'valid'.\n *\t\tAlan Cox\t:\tFixed icmp handling properly\n *\t\tAlan Cox\t: \tCorrect error for oversized datagrams\n *\t\tAlan Cox\t:\tTidied select() semantics.\n *\t\tAlan Cox\t:\tudp_err() fixed properly, also now\n *\t\t\t\t\tselect and read wake correctly on errors\n *\t\tAlan Cox\t:\tudp_send verify_area moved to avoid mem leak\n *\t\tAlan Cox\t:\tUDP can count its memory\n *\t\tAlan Cox\t:\tsend to an unknown connection causes\n *\t\t\t\t\tan ECONNREFUSED off the icmp, but\n *\t\t\t\t\tdoes NOT close.\n *\t\tAlan Cox\t:\tSwitched to new sk_buff handlers. No more backlog!\n *\t\tAlan Cox\t:\tUsing generic datagram code. Even smaller and the PEEK\n *\t\t\t\t\tbug no longer crashes it.\n *\t\tFred Van Kempen\t: \tNet2e support for sk->broadcast.\n *\t\tAlan Cox\t:\tUses skb_free_datagram\n *\t\tAlan Cox\t:\tAdded get/set sockopt support.\n *\t\tAlan Cox\t:\tBroadcasting without option set returns EACCES.\n *\t\tAlan Cox\t:\tNo wakeup calls. Instead we now use the callbacks.\n *\t\tAlan Cox\t:\tUse ip_tos and ip_ttl\n *\t\tAlan Cox\t:\tSNMP Mibs\n *\t\tAlan Cox\t:\tMSG_DONTROUTE, and 0.0.0.0 support.\n *\t\tMatt Dillon\t:\tUDP length checks.\n *\t\tAlan Cox\t:\tSmarter af_inet used properly.\n *\t\tAlan Cox\t:\tUse new kernel side addressing.\n *\t\tAlan Cox\t:\tIncorrect return on truncated datagram receive.\n *\tArnt Gulbrandsen \t:\tNew udp_send and stuff\n *\t\tAlan Cox\t:\tCache last socket\n *\t\tAlan Cox\t:\tRoute cache\n *\t\tJon Peatfield\t:\tMinor efficiency fix to sendto().\n *\t\tMike Shaver\t:\tRFC1122 checks.\n *\t\tAlan Cox\t:\tNonblocking error fix.\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tLast socket cache retained as it\n *\t\t\t\t\tdoes have a high hit rate.\n *\t\tOlaf Kirch\t:\tDon't linearise iovec on sendmsg.\n *\t\tAndi Kleen\t:\tSome cleanups, cache destination entry\n *\t\t\t\t\tfor connect.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tMelvin Smith\t:\tCheck msg_name not msg_namelen in sendto(),\n *\t\t\t\t\treturn ENOTCONN for unconnected sockets (POSIX)\n *\t\tJanos Farkas\t:\tdon't deliver multi/broadcasts to a different\n *\t\t\t\t\tbound-to-device socket\n *\tHirokazu Takahashi\t:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\tHirokazu Takahashi\t:\tsendfile() on UDP works now.\n *\t\tArnaldo C. Melo :\tconvert /proc/net/udp to seq_file\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov:\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tDerek Atkins <derek@ihtfp.com>: Add Encapulation Support\n *\tJames Chapman\t\t:\tAdd L2TP encapsulation type.\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/igmp.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/tcp_states.h>\n#include <linux/skbuff.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/route.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include \"udp_impl.h\"\n\nstruct udp_table udp_table __read_mostly;\nEXPORT_SYMBOL(udp_table);\n\nlong sysctl_udp_mem[3] __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_mem);\n\nint sysctl_udp_rmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_rmem_min);\n\nint sysctl_udp_wmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_wmem_min);\n\natomic_long_t udp_memory_allocated;\nEXPORT_SYMBOL(udp_memory_allocated);\n\n#define MAX_UDP_PORTS 65536\n#define PORTS_PER_CHAIN (MAX_UDP_PORTS / UDP_HTABLE_SIZE_MIN)\n\nstatic int udp_lib_lport_inuse(struct net *net, __u16 num,\n\t\t\t       const struct udp_hslot *hslot,\n\t\t\t       unsigned long *bitmap,\n\t\t\t       struct sock *sk,\n\t\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t const struct sock *sk2),\n\t\t\t       unsigned int log)\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\n\tsk_nulls_for_each(sk2, node, &hslot->head)\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (bitmap || udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (*saddr_comp)(sk, sk2)) {\n\t\t\tif (bitmap)\n\t\t\t\t__set_bit(udp_sk(sk2)->udp_port_hash >> log,\n\t\t\t\t\t  bitmap);\n\t\t\telse\n\t\t\t\treturn 1;\n\t\t}\n\treturn 0;\n}\n\n/*\n * Note: we still hold spinlock of primary hash chain, so no other writer\n * can insert/delete a socket with local_port == num\n */\nstatic int udp_lib_lport_inuse2(struct net *net, __u16 num,\n\t\t\t       struct udp_hslot *hslot2,\n\t\t\t       struct sock *sk,\n\t\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t const struct sock *sk2))\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\tint res = 0;\n\n\tspin_lock(&hslot2->lock);\n\tudp_portaddr_for_each_entry(sk2, node, &hslot2->head)\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (*saddr_comp)(sk, sk2)) {\n\t\t\tres = 1;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&hslot2->lock);\n\treturn res;\n}\n\n/**\n *  udp_lib_get_port  -  UDP/-Lite port lookup for IPv4 and IPv6\n *\n *  @sk:          socket struct in question\n *  @snum:        port number to look up\n *  @saddr_comp:  AF-dependent comparison of bound local IP addresses\n *  @hash2_nulladdr: AF-dependent hash value in secondary hash chains,\n *                   with NULL address\n */\nint udp_lib_get_port(struct sock *sk, unsigned short snum,\n\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t const struct sock *sk2),\n\t\t     unsigned int hash2_nulladdr)\n{\n\tstruct udp_hslot *hslot, *hslot2;\n\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\tint    error = 1;\n\tstruct net *net = sock_net(sk);\n\n\tif (!snum) {\n\t\tint low, high, remaining;\n\t\tunsigned rand;\n\t\tunsigned short first, last;\n\t\tDECLARE_BITMAP(bitmap, PORTS_PER_CHAIN);\n\n\t\tinet_get_local_port_range(&low, &high);\n\t\tremaining = (high - low) + 1;\n\n\t\trand = net_random();\n\t\tfirst = (((u64)rand * remaining) >> 32) + low;\n\t\t/*\n\t\t * force rand to be an odd multiple of UDP_HTABLE_SIZE\n\t\t */\n\t\trand = (rand | 1) * (udptable->mask + 1);\n\t\tlast = first + udptable->mask + 1;\n\t\tdo {\n\t\t\thslot = udp_hashslot(udptable, net, first);\n\t\t\tbitmap_zero(bitmap, PORTS_PER_CHAIN);\n\t\t\tspin_lock_bh(&hslot->lock);\n\t\t\tudp_lib_lport_inuse(net, snum, hslot, bitmap, sk,\n\t\t\t\t\t    saddr_comp, udptable->log);\n\n\t\t\tsnum = first;\n\t\t\t/*\n\t\t\t * Iterate on all possible values of snum for this hash.\n\t\t\t * Using steps of an odd multiple of UDP_HTABLE_SIZE\n\t\t\t * give us randomization and full range coverage.\n\t\t\t */\n\t\t\tdo {\n\t\t\t\tif (low <= snum && snum <= high &&\n\t\t\t\t    !test_bit(snum >> udptable->log, bitmap) &&\n\t\t\t\t    !inet_is_reserved_local_port(snum))\n\t\t\t\t\tgoto found;\n\t\t\t\tsnum += rand;\n\t\t\t} while (snum != first);\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t} while (++first != last);\n\t\tgoto fail;\n\t} else {\n\t\thslot = udp_hashslot(udptable, net, snum);\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (hslot->count > 10) {\n\t\t\tint exist;\n\t\t\tunsigned int slot2 = udp_sk(sk)->udp_portaddr_hash ^ snum;\n\n\t\t\tslot2          &= udptable->mask;\n\t\t\thash2_nulladdr &= udptable->mask;\n\n\t\t\thslot2 = udp_hashslot2(udptable, slot2);\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto scan_primary_hash;\n\n\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\tif (!exist && (hash2_nulladdr != slot2)) {\n\t\t\t\thslot2 = udp_hashslot2(udptable, hash2_nulladdr);\n\t\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\t}\n\t\t\tif (exist)\n\t\t\t\tgoto fail_unlock;\n\t\t\telse\n\t\t\t\tgoto found;\n\t\t}\nscan_primary_hash:\n\t\tif (udp_lib_lport_inuse(net, snum, hslot, NULL, sk,\n\t\t\t\t\tsaddr_comp, 0))\n\t\t\tgoto fail_unlock;\n\t}\nfound:\n\tinet_sk(sk)->inet_num = snum;\n\tudp_sk(sk)->udp_port_hash = snum;\n\tudp_sk(sk)->udp_portaddr_hash ^= snum;\n\tif (sk_unhashed(sk)) {\n\t\tsk_nulls_add_node_rcu(sk, &hslot->head);\n\t\thslot->count++;\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tspin_lock(&hslot2->lock);\n\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t &hslot2->head);\n\t\thslot2->count++;\n\t\tspin_unlock(&hslot2->lock);\n\t}\n\terror = 0;\nfail_unlock:\n\tspin_unlock_bh(&hslot->lock);\nfail:\n\treturn error;\n}\nEXPORT_SYMBOL(udp_lib_get_port);\n\nstatic int ipv4_rcv_saddr_equal(const struct sock *sk1, const struct sock *sk2)\n{\n\tstruct inet_sock *inet1 = inet_sk(sk1), *inet2 = inet_sk(sk2);\n\n\treturn \t(!ipv6_only_sock(sk2)  &&\n\t\t (!inet1->inet_rcv_saddr || !inet2->inet_rcv_saddr ||\n\t\t   inet1->inet_rcv_saddr == inet2->inet_rcv_saddr));\n}\n\nstatic unsigned int udp4_portaddr_hash(struct net *net, __be32 saddr,\n\t\t\t\t       unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nint udp_v4_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp4_portaddr_hash(sock_net(sk), htonl(INADDR_ANY), snum);\n\tunsigned int hash2_partial =\n\t\tudp4_portaddr_hash(sock_net(sk), inet_sk(sk)->inet_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv4_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net, __be32 saddr,\n\t\t\t unsigned short hnum,\n\t\t\t __be16 sport, __be32 daddr, __be16 dport, int dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && udp_sk(sk)->udp_port_hash == hnum &&\n\t\t\t!ipv6_only_sock(sk)) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tscore = (sk->sk_family == PF_INET ? 1 : 0);\n\t\tif (inet->inet_rcv_saddr) {\n\t\t\tif (inet->inet_rcv_saddr != daddr)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (inet->inet_daddr) {\n\t\t\tif (inet->inet_daddr != saddr)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t}\n\treturn score;\n}\n\n/*\n * In this second variant, we check (daddr, dport) matches (inet_rcv_sadd, inet_num)\n */\n#define SCORE2_MAX (1 + 2 + 2 + 2)\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t __be32 saddr, __be16 sport,\n\t\t\t\t __be32 daddr, unsigned int hnum, int dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && !ipv6_only_sock(sk)) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (inet->inet_rcv_saddr != daddr)\n\t\t\treturn -1;\n\t\tif (inet->inet_num != hnum)\n\t\t\treturn -1;\n\n\t\tscore = (sk->sk_family == PF_INET ? 1 : 0);\n\t\tif (inet->inet_daddr) {\n\t\t\tif (inet->inet_daddr != saddr)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t}\n\treturn score;\n}\n\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp4_lib_lookup2(struct net *net,\n\t\t__be32 saddr, __be16 sport,\n\t\t__be32 daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\tif (score == SCORE2_MAX)\n\t\t\t\tgoto exact_match;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\nexact_match:\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\n/* UDP is nearly always wildcards out the wazoo, it makes no sense to try\n * harder than this. -DaveM\n */\nstatic struct sock *__udp4_lib_lookup(struct net *net, __be32 saddr,\n\t\t__be16 sport, __be32 daddr, __be16 dport,\n\t\tint dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp4_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp4_portaddr_hash(net, htonl(INADDR_ANY), hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  htonl(INADDR_ANY), hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, saddr, hnum, sport,\n\t\t\t\t      daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, saddr, hnum, sport,\n\t\t\t\t  daddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\nstatic inline struct sock *__udp4_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t\t __be16 sport, __be16 dport,\n\t\t\t\t\t\t struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\tif (unlikely(sk = skb_steal_sock(skb)))\n\t\treturn sk;\n\telse\n\t\treturn __udp4_lib_lookup(dev_net(skb_dst(skb)->dev), iph->saddr, sport,\n\t\t\t\t\t iph->daddr, dport, inet_iif(skb),\n\t\t\t\t\t udptable);\n}\n\nstruct sock *udp4_lib_lookup(struct net *net, __be32 saddr, __be16 sport,\n\t\t\t     __be32 daddr, __be16 dport, int dif)\n{\n\treturn __udp4_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp4_lib_lookup);\n\nstatic inline struct sock *udp_v4_mcast_next(struct net *net, struct sock *sk,\n\t\t\t\t\t     __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t     __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t     int dif)\n{\n\tstruct hlist_nulls_node *node;\n\tstruct sock *s = sk;\n\tunsigned short hnum = ntohs(loc_port);\n\n\tsk_nulls_for_each_from(s, node) {\n\t\tstruct inet_sock *inet = inet_sk(s);\n\n\t\tif (!net_eq(sock_net(s), net) ||\n\t\t    udp_sk(s)->udp_port_hash != hnum ||\n\t\t    (inet->inet_daddr && inet->inet_daddr != rmt_addr) ||\n\t\t    (inet->inet_dport != rmt_port && inet->inet_dport) ||\n\t\t    (inet->inet_rcv_saddr &&\n\t\t     inet->inet_rcv_saddr != loc_addr) ||\n\t\t    ipv6_only_sock(s) ||\n\t\t    (s->sk_bound_dev_if && s->sk_bound_dev_if != dif))\n\t\t\tcontinue;\n\t\tif (!ip_mc_sf_allow(s, loc_addr, rmt_addr, dif))\n\t\t\tcontinue;\n\t\tgoto found;\n\t}\n\ts = NULL;\nfound:\n\treturn s;\n}\n\n/*\n * This routine is called by the ICMP module when it gets some\n * sort of error condition.  If err < 0 then the socket should\n * be closed and the error returned to the user.  If err > 0\n * it's just the icmp type << 8 | icmp code.\n * Header points to the ip header of the error packet. We move\n * on past this. Then (as it used to claim before adjustment)\n * header points to the first 8 bytes of the udp header.  We need\n * to find the appropriate port.\n */\n\nvoid __udp4_lib_err(struct sk_buff *skb, u32 info, struct udp_table *udptable)\n{\n\tstruct inet_sock *inet;\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+(iph->ihl<<2));\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\tint harderr;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp4_lib_lookup(net, iph->daddr, uh->dest,\n\t\t\tiph->saddr, uh->source, skb->dev->ifindex, udptable);\n\tif (sk == NULL) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\t/* No socket for error */\n\t}\n\n\terr = 0;\n\tharderr = 0;\n\tinet = inet_sk(sk);\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code == ICMP_FRAG_NEEDED) { /* Path MTU discovery */\n\t\t\tif (inet->pmtudisc != IP_PMTUDISC_DONT) {\n\t\t\t\terr = EMSGSIZE;\n\t\t\t\tharderr = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\terr = EHOSTUNREACH;\n\t\tif (code <= NR_ICMP_UNREACH) {\n\t\t\tharderr = icmp_err_convert[code].fatal;\n\t\t\terr = icmp_err_convert[code].errno;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *      RFC1122: OK.  Passes ICMP errors back to application, as per\n\t *\t4.1.3.3.\n\t */\n\tif (!inet->recverr) {\n\t\tif (!harderr || sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t} else\n\t\tip_icmp_error(sk, skb, err, uh->dest, info, (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nvoid udp_err(struct sk_buff *skb, u32 info)\n{\n\t__udp4_lib_err(skb, info, &udp_table);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nvoid udp_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip_flush_pending_frames(sk);\n\t}\n}\nEXPORT_SYMBOL(udp_flush_pending_frames);\n\n/**\n * \tudp4_hwcsum  -  handle outgoing HW checksumming\n * \t@skb: \tsk_buff containing the filled-in UDP header\n * \t        (checksum field must be zeroed out)\n *\t@src:\tsource IP address\n *\t@dst:\tdestination IP address\n */\nstatic void udp4_hwcsum(struct sk_buff *skb, __be32 src, __be32 dst)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *frags = skb_shinfo(skb)->frag_list;\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint hlen = len;\n\t__wsum csum = 0;\n\n\tif (!frags) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_tcpudp_magic(src, dst, len,\n\t\t\t\t\t       IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\tdo {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t\thlen -= frags->len;\n\t\t} while ((frags = frags->next));\n\n\t\tcsum = skb_checksum(skb, offset, hlen, csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tuh->check = csum_tcpudp_magic(src, dst, len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic int udp_send_skb(struct sk_buff *skb, __be32 daddr, __be32 dport)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tstruct rtable *rt = (struct rtable *)skb_dst(skb);\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\t__wsum csum = 0;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check == UDP_CSUM_NOXMIT) {   /* UDP csum disabled */\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\n\t\tudp4_hwcsum(skb, rt->rt_src, daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_tcpudp_magic(rt->rt_src, daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet->recverr) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t   UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\n/*\n * Push out all pending data as one UDP datagram. Socket is locked.\n */\nstatic int udp_push_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 *fl4 = &inet->cork.fl.u.ip4;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tskb = ip_finish_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_send_skb(skb, fl4->daddr, fl4->fl4_dport);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\t__be16 dport;\n\tu8  tos;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\tif (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */\n\t\treturn -EOPNOTSUPP;\n\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in * usin = (struct sockaddr_in *)msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t/* Open fast path for connected socket.\n\t\t   Route will not be used, if at least one option is set.\n\t\t */\n\t\tconnected = 1;\n\t}\n\tipc.addr = inet->inet_saddr;\n\n\tipc.oif = sk->sk_bound_dev_if;\n\terr = sock_tx_timestamp(sk, &ipc.tx_flags);\n\tif (err)\n\t\treturn err;\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt)\n\t\tipc.opt = inet->opt;\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tfaddr = ipc.opt->faddr;\n\t\tconnected = 0;\n\t}\n\ttos = RT_TOS(inet->tos);\n\tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n\t    (msg->msg_flags & MSG_DONTROUTE) ||\n\t    (ipc.opt && ipc.opt->is_strictroute)) {\n\t\ttos |= RTO_ONLINK;\n\t\tconnected = 0;\n\t}\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\tstruct flowi4 fl4;\n\t\tstruct net *net = sock_net(sk);\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP,\n\t\t\t\t   faddr, saddr, dport, inet->inet_sport);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(net, &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = rt->rt_src;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = rt->rt_dst;\n\n\t/* Lockless fast path for the non-corking case. */\n\tif (!corkreq) {\n\t\tskb = ip_make_skb(sk, getfrag, msg->msg_iov, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (skb && !IS_ERR(skb))\n\t\t\terr = udp_send_skb(skb, daddr, dport);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t/*\n\t *\tNow cork the socket to pend data.\n\t */\n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tup->pending = AF_INET;\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, getfrag, msg->msg_iov, ulen,\n\t\t\tsizeof(struct udphdr), &ipc, &rt,\n\t\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\nEXPORT_SYMBOL(udp_sendmsg);\n\nint udp_sendpage(struct sock *sk, struct page *page, int offset,\n\t\t size_t size, int flags)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint ret;\n\n\tif (!up->pending) {\n\t\tstruct msghdr msg = {\t.msg_flags = flags|MSG_MORE };\n\n\t\t/* Call udp_sendmsg to specify destination address which\n\t\t * sendpage interface can't pass.\n\t\t * This will succeed only when the socket is connected.\n\t\t */\n\t\tret = udp_sendmsg(NULL, sk, &msg, 0);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tlock_sock(sk);\n\n\tif (unlikely(!up->pending)) {\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 3\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = ip_append_page(sk, page, offset, size, flags);\n\tif (ret == -EOPNOTSUPP) {\n\t\trelease_sock(sk);\n\t\treturn sock_no_sendpage(sk->sk_socket, page, offset,\n\t\t\t\t\tsize, flags);\n\t}\n\tif (ret < 0) {\n\t\tudp_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\tup->len += size;\n\tif (!(up->corkflag || (flags&MSG_MORE)))\n\t\tret = udp_push_pending_frames(sk);\n\tif (!ret)\n\t\tret = size;\nout:\n\trelease_sock(sk);\n\treturn ret;\n}\n\n\n/**\n *\tfirst_packet_length\t- return length of first packet in receive queue\n *\t@sk: socket\n *\n *\tDrops all bad checksum frames, until a valid one is found.\n *\tReturns the length of found skb, or 0 if none is found.\n */\nstatic unsigned int first_packet_length(struct sock *sk)\n{\n\tstruct sk_buff_head list_kill, *rcvq = &sk->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tunsigned int res;\n\n\t__skb_queue_head_init(&list_kill);\n\n\tspin_lock_bh(&rcvq->lock);\n\twhile ((skb = skb_peek(rcvq)) != NULL &&\n\t\tudp_lib_checksum_complete(skb)) {\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t IS_UDPLITE(sk));\n\t\tatomic_inc(&sk->sk_drops);\n\t\t__skb_unlink(skb, rcvq);\n\t\t__skb_queue_tail(&list_kill, skb);\n\t}\n\tres = skb ? skb->len : 0;\n\tspin_unlock_bh(&rcvq->lock);\n\n\tif (!skb_queue_empty(&list_kill)) {\n\t\tbool slow = lock_sock_fast(sk);\n\n\t\t__skb_queue_purge(&list_kill);\n\t\tsk_mem_reclaim_partial(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t}\n\treturn res;\n}\n\n/*\n *\tIOCTL requests applicable to the UDP protocol\n */\n\nint udp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tcase SIOCINQ:\n\t{\n\t\tunsigned int amount = first_packet_length(sk);\n\n\t\tif (amount)\n\t\t\t/*\n\t\t\t * We will only return the amount\n\t\t\t * of this packet since that is all\n\t\t\t * that will be read.\n\t\t\t */\n\t\t\tamount -= sizeof(struct udphdr);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_ioctl);\n\n/*\n * \tThis should be easy, if there is something there we\n * \treturn it, otherwise we block.\n */\n\nint udp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tunsigned int ulen;\n\tint peeked;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool slow;\n\n\t/*\n\t *\tCheck any passed addresses\n\t */\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tif (len > ulen)\n\t\tlen = ulen;\n\telse if (len < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (len < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, len);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb,\n\t\t\t\t\t\t       sizeof(struct udphdr),\n\t\t\t\t\t\t       msg->msg_iov);\n\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (err)\n\t\tgoto out_free;\n\n\tif (!peeked)\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\n\terr = len;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags))\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\tgoto try_again;\n}\n\n\nint udp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t/*\n\t *\t1003.1g - break association.\n\t */\n\n\tsk->sk_state = TCP_CLOSE;\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsock_rps_save_rxhash(sk, 0);\n\tsk->sk_bound_dev_if = 0;\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tif (!(sk->sk_userlocks & SOCK_BINDPORT_LOCK)) {\n\t\tsk->sk_prot->unhash(sk);\n\t\tinet->inet_sport = 0;\n\t}\n\tsk_dst_reset(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_disconnect);\n\nvoid udp_lib_unhash(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2;\n\n\t\thslot  = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t      udp_sk(sk)->udp_port_hash);\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (sk_nulls_del_node_init_rcu(sk)) {\n\t\t\thslot->count--;\n\t\t\tinet_sk(sk)->inet_num = 0;\n\t\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n}\nEXPORT_SYMBOL(udp_lib_unhash);\n\n/*\n * inet_rcv_saddr was changed, we must rehash secondary hash\n */\nvoid udp_lib_rehash(struct sock *sk, u16 newhash)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2, *nhslot2;\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tnhslot2 = udp_hashslot2(udptable, newhash);\n\t\tudp_sk(sk)->udp_portaddr_hash = newhash;\n\t\tif (hslot2 != nhslot2) {\n\t\t\thslot = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t\t     udp_sk(sk)->udp_port_hash);\n\t\t\t/* we must lock primary chain too */\n\t\t\tspin_lock_bh(&hslot->lock);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\n\t\t\tspin_lock(&nhslot2->lock);\n\t\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t\t &nhslot2->head);\n\t\t\tnhslot2->count++;\n\t\t\tspin_unlock(&nhslot2->lock);\n\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(udp_lib_rehash);\n\nstatic void udp_v4_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp4_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  inet_sk(sk)->inet_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic int __udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (inet_sk(sk)->inet_daddr)\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\trc = ip_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t is_udplite);\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n\n}\n\n/* returns:\n *  -1: error\n *   0: success\n *  >0: \"udp encap\" protocol resubmission\n *\n * Note that in the success and error cases, the skb is assumed to\n * have either been requeued or freed.\n */\nint udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (up->encap_type) {\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tif (skb->len > sizeof(struct udphdr) &&\n\t\t    up->encap_rcv != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = (*up->encap_rcv)(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE: partial coverage \"\n\t\t\t\t\"%d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING\n\t\t\t\t\"UDPLITE: coverage %d too small, need min %d\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_dereference_raw(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\n\tif (sk_rcvqueues_full(sk, skb))\n\t\tgoto drop;\n\n\trc = 0;\n\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tunsigned int i;\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(skb1 == NULL))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udp_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\n/*\n *\tMulticasts and broadcasts go to each listener.\n *\n *\tNote: called only from the BH handler context.\n */\nstatic int __udp4_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\t\t\t    struct udphdr  *uh,\n\t\t\t\t    __be32 saddr, __be32 daddr,\n\t\t\t\t    struct udp_table *udptable)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, ntohs(uh->dest));\n\tint dif;\n\tunsigned int i, count = 0;\n\n\tspin_lock(&hslot->lock);\n\tsk = sk_nulls_head(&hslot->head);\n\tdif = skb->dev->ifindex;\n\tsk = udp_v4_mcast_next(net, sk, uh->dest, daddr, uh->source, saddr, dif);\n\twhile (sk) {\n\t\tstack[count++] = sk;\n\t\tsk = udp_v4_mcast_next(net, sk_nulls_next(sk), uh->dest,\n\t\t\t\t       daddr, uh->source, saddr, dif);\n\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\tif (!sk)\n\t\t\t\tbreak;\n\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\tcount = 0;\n\t\t}\n\t}\n\t/*\n\t * before releasing chain lock, we must take a reference on sockets\n\t */\n\tfor (i = 0; i < count; i++)\n\t\tsock_hold(stack[i]);\n\n\tspin_unlock(&hslot->lock);\n\n\t/*\n\t * do the slow work with no lock held\n\t */\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tsock_put(stack[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\treturn 0;\n}\n\n/* Initialize UDP checksum. If exited with zero value (success),\n * CHECKSUM_UNNECESSARY means, that no more checks are required.\n * Otherwise, csum completion requires chacksumming packet body,\n * including udp header and folding it to skb->csum.\n */\nstatic inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tconst struct iphdr *iph;\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tiph = ip_hdr(skb);\n\tif (uh->check == 0) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t} else if (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_tcpudp_magic(iph->saddr, iph->daddr, skb->len,\n\t\t\t\t      proto, skb->csum))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t\t       skb->len, proto, 0);\n\t/* Probably, we should checksum udp header (it should be in cache\n\t * in any case) and data in tiny packets (< rx copybreak).\n\t */\n\n\treturn 0;\n}\n\n/*\n *\tAll we need to do is get the socket, and then do a checksum.\n */\n\nint __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tunsigned short ulen;\n\tstruct rtable *rt = skb_rtable(skb);\n\t__be32 saddr, daddr;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/*\n\t *  Validate the packet.\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto drop;\t\t/* No space for header. */\n\n\tuh   = udp_hdr(skb);\n\tulen = ntohs(uh->len);\n\tsaddr = ip_hdr(skb)->saddr;\n\tdaddr = ip_hdr(skb)->daddr;\n\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\t\tif (ulen < sizeof(*uh) || pskb_trim_rcsum(skb, ulen))\n\t\t\tgoto short_packet;\n\t\tuh = udp_hdr(skb);\n\t}\n\n\tif (udp4_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\tif (rt->rt_flags & (RTCF_BROADCAST|RTCF_MULTICAST))\n\t\treturn __udp4_lib_mcast_deliver(net, skb, uh,\n\t\t\t\tsaddr, daddr, udptable);\n\n\tsk = __udp4_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\n\tif (sk != NULL) {\n\t\tint ret = udp_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\t\treturn 0;\n\t}\n\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\t/* No socket. Drop packet silently, if checksum is wrong */\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\n\t/*\n\t * Hmm.  We got an UDP packet to a port to which we\n\t * don't wanna listen.  Ignore it.\n\t */\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tLIMIT_NETDEBUG(KERN_DEBUG \"UDP%s: short packet: From %pI4:%u %d/%d to %pI4:%u\\n\",\n\t\t       proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t       &saddr,\n\t\t       ntohs(uh->source),\n\t\t       ulen,\n\t\t       skb->len,\n\t\t       &daddr,\n\t\t       ntohs(uh->dest));\n\tgoto drop;\n\ncsum_error:\n\t/*\n\t * RFC1122: OK.  Discards the bad packet silently (as far as\n\t * the network is concerned, anyway) as per 4.1.3.4 (MUST).\n\t */\n\tLIMIT_NETDEBUG(KERN_DEBUG \"UDP%s: bad checksum. From %pI4:%u to %pI4:%u ulen %d\\n\",\n\t\t       proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t       &saddr,\n\t\t       ntohs(uh->source),\n\t\t       &daddr,\n\t\t       ntohs(uh->dest),\n\t\t       ulen);\ndrop:\n\tUDP_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nint udp_rcv(struct sk_buff *skb)\n{\n\treturn __udp4_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\nvoid udp_destroy_sock(struct sock *sk)\n{\n\tbool slow = lock_sock_fast(sk);\n\tudp_flush_pending_frames(sk);\n\tunlock_sock_fast(sk, slow);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udp_lib_setsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, unsigned int optlen,\n\t\t       int (*push_pending_frames)(struct sock *))\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tif (val != 0) {\n\t\t\tup->corkflag = 1;\n\t\t} else {\n\t\t\tup->corkflag = 0;\n\t\t\tlock_sock(sk);\n\t\t\t(*push_pending_frames)(sk);\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tswitch (val) {\n\t\tcase 0:\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tup->encap_rcv = xfrm4_udp_encap_rcv;\n\t\t\t/* FALLTHROUGH */\n\t\tcase UDP_ENCAP_L2TPINUDP:\n\t\t\tup->encap_type = val;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t/*\n\t * \tUDP-Lite's partial checksum coverage (RFC 3828).\n\t */\n\t/* The sender sets actual checksum coverage length via this option.\n\t * The case coverage > packet length is handled by send module. */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Illegal coverage: use default (8) */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcslen = val;\n\t\tup->pcflag |= UDPLITE_SEND_CC;\n\t\tbreak;\n\n\t/* The receiver specifies a minimum checksum coverage value. To make\n\t * sense, this should be set to at least 8 (as done below). If zero is\n\t * used, this again means full checksum coverage.                     */\n\tcase UDPLITE_RECV_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Avoid silly minimal values.       */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcrlen = val;\n\t\tup->pcflag |= UDPLITE_RECV_CC;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(udp_lib_setsockopt);\n\nint udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn compat_ip_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udp_lib_getsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tval = up->corkflag;\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tval = up->encap_type;\n\t\tbreak;\n\n\t/* The following two cannot be changed on UDP sockets, the return is\n\t * always 0 (which corresponds to the full checksum coverage of UDP). */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tval = up->pcslen;\n\t\tbreak;\n\n\tcase UDPLITE_RECV_CSCOV:\n\t\tval = up->pcrlen;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_lib_getsockopt);\n\nint udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ip_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n/**\n * \tudp_poll - wait for a UDP event.\n *\t@file - file struct\n *\t@sock - socket\n *\t@wait - poll table\n *\n *\tThis is same as datagram poll, except for the special case of\n *\tblocking sockets. If application is using a blocking fd\n *\tand a packet with checksum error is in the queue;\n *\tthen it could get return from select indicating data available\n *\tbut then block when reading it. Add special case code\n *\tto work around these arguably broken applications.\n */\nunsigned int udp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\tstruct sock *sk = sock->sk;\n\n\t/* Check for false positives due to checksum errors */\n\tif ((mask & POLLRDNORM) && !(file->f_flags & O_NONBLOCK) &&\n\t    !(sk->sk_shutdown & RCV_SHUTDOWN) && !first_packet_length(sk))\n\t\tmask &= ~(POLLIN | POLLRDNORM);\n\n\treturn mask;\n\n}\nEXPORT_SYMBOL(udp_poll);\n\nstruct proto udp_prot = {\n\t.name\t\t   = \"UDP\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip4_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udp_destroy_sock,\n\t.setsockopt\t   = udp_setsockopt,\n\t.getsockopt\t   = udp_getsockopt,\n\t.sendmsg\t   = udp_sendmsg,\n\t.recvmsg\t   = udp_recvmsg,\n\t.sendpage\t   = udp_sendpage,\n\t.backlog_rcv\t   = __udp_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v4_rehash,\n\t.get_port\t   = udp_v4_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udp_setsockopt,\n\t.compat_getsockopt = compat_udp_getsockopt,\n#endif\n\t.clear_sk\t   = sk_prot_clear_portaddr_nulls,\n};\nEXPORT_SYMBOL(udp_prot);\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\n\nstatic struct sock *udp_get_first(struct seq_file *seq, int start)\n{\n\tstruct sock *sk;\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tfor (state->bucket = start; state->bucket <= state->udp_table->mask;\n\t     ++state->bucket) {\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct udp_hslot *hslot = &state->udp_table->hash[state->bucket];\n\n\t\tif (hlist_nulls_empty(&hslot->head))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tsk_nulls_for_each(sk, node, &hslot->head) {\n\t\t\tif (!net_eq(sock_net(sk), net))\n\t\t\t\tcontinue;\n\t\t\tif (sk->sk_family == state->family)\n\t\t\t\tgoto found;\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tdo {\n\t\tsk = sk_nulls_next(sk);\n\t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n\n\tif (!sk) {\n\t\tif (state->bucket <= state->udp_table->mask)\n\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}\n\nstatic struct sock *udp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = udp_get_first(seq, 0);\n\n\tif (sk)\n\t\twhile (pos && (sk = udp_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nstatic void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstate->bucket = MAX_UDP_PORTS;\n\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}\n\nstatic void *udp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = udp_get_idx(seq, 0);\n\telse\n\t\tsk = udp_get_next(seq, v);\n\n\t++*pos;\n\treturn sk;\n}\n\nstatic void udp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct udp_iter_state *state = seq->private;\n\n\tif (state->bucket <= state->udp_table->mask)\n\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n}\n\nstatic int udp_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct udp_seq_afinfo *afinfo = PDE(inode)->data;\n\tstruct udp_iter_state *s;\n\tint err;\n\n\terr = seq_open_net(inode, file, &afinfo->seq_ops,\n\t\t\t   sizeof(struct udp_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ts = ((struct seq_file *)file->private_data)->private;\n\ts->family\t\t= afinfo->family;\n\ts->udp_table\t\t= afinfo->udp_table;\n\treturn err;\n}\n\n/* ------------------------------------------------------------------------ */\nint udp_proc_register(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tstruct proc_dir_entry *p;\n\tint rc = 0;\n\n\tafinfo->seq_fops.open\t\t= udp_seq_open;\n\tafinfo->seq_fops.read\t\t= seq_read;\n\tafinfo->seq_fops.llseek\t\t= seq_lseek;\n\tafinfo->seq_fops.release\t= seq_release_net;\n\n\tafinfo->seq_ops.start\t\t= udp_seq_start;\n\tafinfo->seq_ops.next\t\t= udp_seq_next;\n\tafinfo->seq_ops.stop\t\t= udp_seq_stop;\n\n\tp = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,\n\t\t\t     &afinfo->seq_fops, afinfo);\n\tif (!p)\n\t\trc = -ENOMEM;\n\treturn rc;\n}\nEXPORT_SYMBOL(udp_proc_register);\n\nvoid udp_proc_unregister(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tproc_net_remove(net, afinfo->name);\n}\nEXPORT_SYMBOL(udp_proc_unregister);\n\n/* ------------------------------------------------------------------------ */\nstatic void udp4_format_sock(struct sock *sp, struct seq_file *f,\n\t\tint bucket, int *len)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src  = inet->inet_rcv_saddr;\n\t__u16 destp\t  = ntohs(inet->inet_dport);\n\t__u16 srcp\t  = ntohs(inet->inet_sport);\n\n\tseq_printf(f, \"%5d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %p %d%n\",\n\t\tbucket, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tsk_rmem_alloc_get(sp),\n\t\t0, 0L, 0, sock_i_uid(sp), 0, sock_i_ino(sp),\n\t\tatomic_read(&sp->sk_refcnt), sp,\n\t\tatomic_read(&sp->sk_drops), len);\n}\n\nint udp4_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq, \"%-127s\\n\",\n\t\t\t   \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode ref pointer drops\");\n\telse {\n\t\tstruct udp_iter_state *state = seq->private;\n\t\tint len;\n\n\t\tudp4_format_sock(v, seq, state->bucket, &len);\n\t\tseq_printf(seq, \"%*s\\n\", 127 - len, \"\");\n\t}\n\treturn 0;\n}\n\n/* ------------------------------------------------------------------------ */\nstatic struct udp_seq_afinfo udp4_seq_afinfo = {\n\t.name\t\t= \"udp\",\n\t.family\t\t= AF_INET,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= {\n\t\t.owner\t=\tTHIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= udp4_seq_show,\n\t},\n};\n\nstatic int __net_init udp4_proc_init_net(struct net *net)\n{\n\treturn udp_proc_register(net, &udp4_seq_afinfo);\n}\n\nstatic void __net_exit udp4_proc_exit_net(struct net *net)\n{\n\tudp_proc_unregister(net, &udp4_seq_afinfo);\n}\n\nstatic struct pernet_operations udp4_net_ops = {\n\t.init = udp4_proc_init_net,\n\t.exit = udp4_proc_exit_net,\n};\n\nint __init udp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&udp4_net_ops);\n}\n\nvoid udp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&udp4_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n\nstatic __initdata unsigned long uhash_entries;\nstatic int __init set_uhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tuhash_entries = simple_strtoul(str, &str, 0);\n\tif (uhash_entries && uhash_entries < UDP_HTABLE_SIZE_MIN)\n\t\tuhash_entries = UDP_HTABLE_SIZE_MIN;\n\treturn 1;\n}\n__setup(\"uhash_entries=\", set_uhash_entries);\n\nvoid __init udp_table_init(struct udp_table *table, const char *name)\n{\n\tunsigned int i;\n\n\tif (!CONFIG_BASE_SMALL)\n\t\ttable->hash = alloc_large_system_hash(name,\n\t\t\t2 * sizeof(struct udp_hslot),\n\t\t\tuhash_entries,\n\t\t\t21, /* one slot per 2 MB */\n\t\t\t0,\n\t\t\t&table->log,\n\t\t\t&table->mask,\n\t\t\t64 * 1024);\n\t/*\n\t * Make sure hash table has the minimum size\n\t */\n\tif (CONFIG_BASE_SMALL || table->mask < UDP_HTABLE_SIZE_MIN - 1) {\n\t\ttable->hash = kmalloc(UDP_HTABLE_SIZE_MIN *\n\t\t\t\t      2 * sizeof(struct udp_hslot), GFP_KERNEL);\n\t\tif (!table->hash)\n\t\t\tpanic(name);\n\t\ttable->log = ilog2(UDP_HTABLE_SIZE_MIN);\n\t\ttable->mask = UDP_HTABLE_SIZE_MIN - 1;\n\t}\n\ttable->hash2 = table->hash + (table->mask + 1);\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash[i].head, i);\n\t\ttable->hash[i].count = 0;\n\t\tspin_lock_init(&table->hash[i].lock);\n\t}\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash2[i].head, i);\n\t\ttable->hash2[i].count = 0;\n\t\tspin_lock_init(&table->hash2[i].lock);\n\t}\n}\n\nvoid __init udp_init(void)\n{\n\tunsigned long nr_pages, limit;\n\n\tudp_table_init(&udp_table, \"UDP\");\n\t/* Set the pressure threshold up by the same strategy of TCP. It is a\n\t * fraction of global memory that is up to 1/2 at 256 MB, decreasing\n\t * toward zero with the amount of memory, with a floor of 128 pages.\n\t */\n\tnr_pages = totalram_pages - totalhigh_pages;\n\tlimit = min(nr_pages, 1UL<<(28-PAGE_SHIFT)) >> (20-PAGE_SHIFT);\n\tlimit = (limit * (nr_pages >> (20-PAGE_SHIFT))) >> (PAGE_SHIFT-11);\n\tlimit = max(limit, 128UL);\n\tsysctl_udp_mem[0] = limit / 4 * 3;\n\tsysctl_udp_mem[1] = limit;\n\tsysctl_udp_mem[2] = sysctl_udp_mem[0] * 2;\n\n\tsysctl_udp_rmem_min = SK_MEM_QUANTUM;\n\tsysctl_udp_wmem_min = SK_MEM_QUANTUM;\n}\n\nint udp4_ufo_send_check(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct udphdr *uh;\n\n\tif (!pskb_may_pull(skb, sizeof(*uh)))\n\t\treturn -EINVAL;\n\n\tiph = ip_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tuh->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, skb->len,\n\t\t\t\t       IPPROTO_UDP, 0);\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct udphdr, check);\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\treturn 0;\n}\n\nstruct sk_buff *udp4_ufo_fragment(struct sk_buff *skb, u32 features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tint offset;\n\t__wsum csum;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP | SKB_GSO_DODGY) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t * do checksum of UDP packets sent as multiple IP fragments.\n\t */\n\toffset = skb_checksum_start_offset(skb);\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\toffset += skb->csum_offset;\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t/* Fragment the skb. IP headers of the fragments are updated in\n\t * inet_gso_segment()\n\t */\n\tsegs = skb_segment(skb, features);\nout:\n\treturn segs;\n}\n\n", "/*\n *\tTCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on:\n *\tlinux/net/ipv4/tcp.c\n *\tlinux/net/ipv4/tcp_input.c\n *\tlinux/net/ipv4/tcp_output.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tYOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/tcp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/bottom_half.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/jiffies.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/init.h>\n#include <linux/jhash.h>\n#include <linux/ipsec.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/random.h>\n\n#include <net/tcp.h>\n#include <net/ndisc.h>\n#include <net/inet6_hashtables.h>\n#include <net/inet6_connection_sock.h>\n#include <net/ipv6.h>\n#include <net/transp_v6.h>\n#include <net/addrconf.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/inet_ecn.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n#include <net/snmp.h>\n#include <net/dsfield.h>\n#include <net/timewait_sock.h>\n#include <net/netdma.h>\n#include <net/inet_common.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n\n#include <linux/crypto.h>\n#include <linux/scatterlist.h>\n\nstatic void\ttcp_v6_send_reset(struct sock *sk, struct sk_buff *skb);\nstatic void\ttcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      struct request_sock *req);\n\nstatic int\ttcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);\nstatic void\t__tcp_v6_send_check(struct sk_buff *skb,\n\t\t\t\t    const struct in6_addr *saddr,\n\t\t\t\t    const struct in6_addr *daddr);\n\nstatic const struct inet_connection_sock_af_ops ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops ipv6_specific;\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific;\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;\n#else\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr)\n{\n\treturn NULL;\n}\n#endif\n\nstatic void tcp_v6_hash(struct sock *sk)\n{\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tif (inet_csk(sk)->icsk_af_ops == &ipv6_mapped) {\n\t\t\ttcp_prot.hash(sk);\n\t\t\treturn;\n\t\t}\n\t\tlocal_bh_disable();\n\t\t__inet6_hash(sk, NULL);\n\t\tlocal_bh_enable();\n\t}\n}\n\nstatic __inline__ __sum16 tcp_v6_check(int len,\n\t\t\t\t   const struct in6_addr *saddr,\n\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t   __wsum base)\n{\n\treturn csum_ipv6_magic(saddr, daddr, len, IPPROTO_TCP, base);\n}\n\nstatic __u32 tcp_v6_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_tcpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t    ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t    tcp_hdr(skb)->dest,\n\t\t\t\t\t    tcp_hdr(skb)->source);\n}\n\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct rt6_info *rt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tipv6_addr_copy(&usin->sin6_addr, &flowlabel->dst);\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif(ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif(addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&np->daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tipv6_addr_copy(&np->daddr, &usin->sin6_addr);\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t} else {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &np->rcv_saddr);\n\t\t}\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&np->rcv_saddr))\n\t\tsaddr = &np->rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\tipv6_addr_copy(&fl6.saddr,\n\t\t       (saddr ? saddr : &np->saddr));\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, true);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tipv6_addr_copy(&np->rcv_saddr, saddr);\n\t}\n\n\t/* set the source address */\n\tipv6_addr_copy(&np->saddr, saddr);\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\trt = (struct rt6_info *) dst;\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&rt->rt6i_dst.addr, &np->daddr)) {\n\t\tstruct inet_peer *peer = rt6_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     np->daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr*)skb->data;\n\tconst struct tcphdr *th = (struct tcphdr *)(skb->data+offset);\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\tstruct tcp_sock *tp;\n\t__u32 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = inet6_lookup(net, &tcp_hashinfo, &hdr->daddr,\n\t\t\tth->dest, &hdr->saddr, th->source, skb->dev->ifindex);\n\n\tif (sk == NULL) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (ipv6_hdr(skb)->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ttp = tcp_sk(sk);\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, tp->snd_una, tp->snd_nxt)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))\n\t\t\tgoto out;\n\n\t\t/* icmp should have updated the destination cache entry */\n\t\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\tif (dst == NULL) {\n\t\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\t\tstruct flowi6 fl6;\n\n\t\t\t/* BUGGG_FUTURE: Again, it is not clear how\n\t\t\t   to handle rthdr case. Ignore this complexity\n\t\t\t   for now.\n\t\t\t */\n\t\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\t\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\t\t\tipv6_addr_copy(&fl6.saddr, &np->saddr);\n\t\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\t\tfl6.fl6_dport = inet->inet_dport;\n\t\t\tfl6.fl6_sport = inet->inet_sport;\n\t\t\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\t\t\tdst = ip6_dst_lookup_flow(sk, &fl6, NULL, false);\n\t\t\tif (IS_ERR(dst)) {\n\t\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t} else\n\t\t\tdst_hold(dst);\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {\n\t\t\ttcp_sync_mss(sk, dst_mtu(dst));\n\t\t\ttcp_simple_retransmit(sk);\n\t\t} /* else let the usual retransmit timer handle it */\n\t\tdst_release(dst);\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req, **prev;\n\tcase TCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\n\t\treq = inet6_csk_search_req(sk, &prev, th->dest, &hdr->daddr,\n\t\t\t\t\t   &hdr->saddr, inet6_iif(skb));\n\t\tif (!req)\n\t\t\tgoto out;\n\n\t\t/* ICMPs are not backlogged, hence we cannot get\n\t\t * an established socket here.\n\t\t */\n\t\tWARN_ON(req->sk != NULL);\n\n\t\tif (seq != tcp_rsk(req)->snt_isn) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\t\t\tsk->sk_error_report(sk);\t\t/* Wake people up to see the error (see connect in sock.c) */\n\n\t\t\ttcp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\n\nstatic int tcp_v6_send_synack(struct sock *sk, struct request_sock *req,\n\t\t\t      struct request_values *rvp)\n{\n\tstruct inet6_request_sock *treq = inet6_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff * skb;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct in6_addr * final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint err;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tipv6_addr_copy(&fl6.daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&fl6.saddr, &treq->loc_addr);\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = treq->iif;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\topt = np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\tskb = tcp_make_synack(sk, dst, req, rvp);\n\terr = -ENOMEM;\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &treq->loc_addr, &treq->rmt_addr);\n\n\t\tipv6_addr_copy(&fl6.daddr, &treq->rmt_addr);\n\t\terr = ip6_xmit(sk, skb, &fl6, opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic int tcp_v6_rtx_synack(struct sock *sk, struct request_sock *req,\n\t\t\t     struct request_values *rvp)\n{\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\treturn tcp_v6_send_synack(sk, req, rvp);\n}\n\nstatic inline void syn_flood_warning(struct sk_buff *skb)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tif (sysctl_tcp_syncookies)\n\t\tprintk(KERN_INFO\n\t\t       \"TCPv6: Possible SYN flooding on port %d. \"\n\t\t       \"Sending cookies.\\n\", ntohs(tcp_hdr(skb)->dest));\n\telse\n#endif\n\t\tprintk(KERN_INFO\n\t\t       \"TCPv6: Possible SYN flooding on port %d. \"\n\t\t       \"Dropping request.\\n\", ntohs(tcp_hdr(skb)->dest));\n}\n\nstatic void tcp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tkfree_skb(inet6_rsk(req)->pktopts);\n}\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tBUG_ON(tp == NULL);\n\n\tif (!tp->md5sig_info || !tp->md5sig_info->entries6)\n\t\treturn NULL;\n\n\tfor (i = 0; i < tp->md5sig_info->entries6; i++) {\n\t\tif (ipv6_addr_equal(&tp->md5sig_info->keys6[i].addr, addr))\n\t\t\treturn &tp->md5sig_info->keys6[i].base;\n\t}\n\treturn NULL;\n}\n\nstatic struct tcp_md5sig_key *tcp_v6_md5_lookup(struct sock *sk,\n\t\t\t\t\t\tstruct sock *addr_sk)\n{\n\treturn tcp_v6_md5_do_lookup(sk, &inet6_sk(addr_sk)->daddr);\n}\n\nstatic struct tcp_md5sig_key *tcp_v6_reqsk_md5_lookup(struct sock *sk,\n\t\t\t\t\t\t      struct request_sock *req)\n{\n\treturn tcp_v6_md5_do_lookup(sk, &inet6_rsk(req)->rmt_addr);\n}\n\nstatic int tcp_v6_md5_do_add(struct sock *sk, const struct in6_addr *peer,\n\t\t\t     char *newkey, u8 newkeylen)\n{\n\t/* Add key to the list */\n\tstruct tcp_md5sig_key *key;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp6_md5sig_key *keys;\n\n\tkey = tcp_v6_md5_do_lookup(sk, peer);\n\tif (key) {\n\t\t/* modify existing entry - just update that one */\n\t\tkfree(key->key);\n\t\tkey->key = newkey;\n\t\tkey->keylen = newkeylen;\n\t} else {\n\t\t/* reallocate new list if current one is full. */\n\t\tif (!tp->md5sig_info) {\n\t\t\ttp->md5sig_info = kzalloc(sizeof(*tp->md5sig_info), GFP_ATOMIC);\n\t\t\tif (!tp->md5sig_info) {\n\t\t\t\tkfree(newkey);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t\t}\n\t\tif (tcp_alloc_md5sig_pool(sk) == NULL) {\n\t\t\tkfree(newkey);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (tp->md5sig_info->alloced6 == tp->md5sig_info->entries6) {\n\t\t\tkeys = kmalloc((sizeof (tp->md5sig_info->keys6[0]) *\n\t\t\t\t       (tp->md5sig_info->entries6 + 1)), GFP_ATOMIC);\n\n\t\t\tif (!keys) {\n\t\t\t\ttcp_free_md5sig_pool();\n\t\t\t\tkfree(newkey);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tif (tp->md5sig_info->entries6)\n\t\t\t\tmemmove(keys, tp->md5sig_info->keys6,\n\t\t\t\t\t(sizeof (tp->md5sig_info->keys6[0]) *\n\t\t\t\t\t tp->md5sig_info->entries6));\n\n\t\t\tkfree(tp->md5sig_info->keys6);\n\t\t\ttp->md5sig_info->keys6 = keys;\n\t\t\ttp->md5sig_info->alloced6++;\n\t\t}\n\n\t\tipv6_addr_copy(&tp->md5sig_info->keys6[tp->md5sig_info->entries6].addr,\n\t\t\t       peer);\n\t\ttp->md5sig_info->keys6[tp->md5sig_info->entries6].base.key = newkey;\n\t\ttp->md5sig_info->keys6[tp->md5sig_info->entries6].base.keylen = newkeylen;\n\n\t\ttp->md5sig_info->entries6++;\n\t}\n\treturn 0;\n}\n\nstatic int tcp_v6_md5_add_func(struct sock *sk, struct sock *addr_sk,\n\t\t\t       u8 *newkey, __u8 newkeylen)\n{\n\treturn tcp_v6_md5_do_add(sk, &inet6_sk(addr_sk)->daddr,\n\t\t\t\t newkey, newkeylen);\n}\n\nstatic int tcp_v6_md5_do_del(struct sock *sk, const struct in6_addr *peer)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tfor (i = 0; i < tp->md5sig_info->entries6; i++) {\n\t\tif (ipv6_addr_equal(&tp->md5sig_info->keys6[i].addr, peer)) {\n\t\t\t/* Free the key */\n\t\t\tkfree(tp->md5sig_info->keys6[i].base.key);\n\t\t\ttp->md5sig_info->entries6--;\n\n\t\t\tif (tp->md5sig_info->entries6 == 0) {\n\t\t\t\tkfree(tp->md5sig_info->keys6);\n\t\t\t\ttp->md5sig_info->keys6 = NULL;\n\t\t\t\ttp->md5sig_info->alloced6 = 0;\n\t\t\t} else {\n\t\t\t\t/* shrink the database */\n\t\t\t\tif (tp->md5sig_info->entries6 != i)\n\t\t\t\t\tmemmove(&tp->md5sig_info->keys6[i],\n\t\t\t\t\t\t&tp->md5sig_info->keys6[i+1],\n\t\t\t\t\t\t(tp->md5sig_info->entries6 - i)\n\t\t\t\t\t\t* sizeof (tp->md5sig_info->keys6[0]));\n\t\t\t}\n\t\t\ttcp_free_md5sig_pool();\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\n\nstatic void tcp_v6_clear_md5_list (struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tif (tp->md5sig_info->entries6) {\n\t\tfor (i = 0; i < tp->md5sig_info->entries6; i++)\n\t\t\tkfree(tp->md5sig_info->keys6[i].base.key);\n\t\ttp->md5sig_info->entries6 = 0;\n\t\ttcp_free_md5sig_pool();\n\t}\n\n\tkfree(tp->md5sig_info->keys6);\n\ttp->md5sig_info->keys6 = NULL;\n\ttp->md5sig_info->alloced6 = 0;\n\n\tif (tp->md5sig_info->entries4) {\n\t\tfor (i = 0; i < tp->md5sig_info->entries4; i++)\n\t\t\tkfree(tp->md5sig_info->keys4[i].base.key);\n\t\ttp->md5sig_info->entries4 = 0;\n\t\ttcp_free_md5sig_pool();\n\t}\n\n\tkfree(tp->md5sig_info->keys4);\n\ttp->md5sig_info->keys4 = NULL;\n\ttp->md5sig_info->alloced4 = 0;\n}\n\nstatic int tcp_v6_parse_md5_keys (struct sock *sk, char __user *optval,\n\t\t\t\t  int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)&cmd.tcpm_addr;\n\tu8 *newkey;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin6->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\tif (!cmd.tcpm_keylen) {\n\t\tif (!tcp_sk(sk)->md5sig_info)\n\t\t\treturn -ENOENT;\n\t\tif (ipv6_addr_v4mapped(&sin6->sin6_addr))\n\t\t\treturn tcp_v4_md5_do_del(sk, sin6->sin6_addr.s6_addr32[3]);\n\t\treturn tcp_v6_md5_do_del(sk, &sin6->sin6_addr);\n\t}\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\tif (!tcp_sk(sk)->md5sig_info) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tstruct tcp_md5sig_info *p;\n\n\t\tp = kzalloc(sizeof(struct tcp_md5sig_info), GFP_KERNEL);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\n\t\ttp->md5sig_info = p;\n\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t}\n\n\tnewkey = kmemdup(cmd.tcpm_key, cmd.tcpm_keylen, GFP_KERNEL);\n\tif (!newkey)\n\t\treturn -ENOMEM;\n\tif (ipv6_addr_v4mapped(&sin6->sin6_addr)) {\n\t\treturn tcp_v4_md5_do_add(sk, sin6->sin6_addr.s6_addr32[3],\n\t\t\t\t\t newkey, cmd.tcpm_keylen);\n\t}\n\treturn tcp_v6_md5_do_add(sk, &sin6->sin6_addr, newkey, cmd.tcpm_keylen);\n}\n\nstatic int tcp_v6_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,\n\t\t\t\t\tconst struct in6_addr *daddr,\n\t\t\t\t\tconst struct in6_addr *saddr, int nbytes)\n{\n\tstruct tcp6_pseudohdr *bp;\n\tstruct scatterlist sg;\n\n\tbp = &hp->md5_blk.ip6;\n\t/* 1. TCP pseudo-header (RFC2460) */\n\tipv6_addr_copy(&bp->saddr, saddr);\n\tipv6_addr_copy(&bp->daddr, daddr);\n\tbp->protocol = cpu_to_be32(IPPROTO_TCP);\n\tbp->len = cpu_to_be32(nbytes);\n\n\tsg_init_one(&sg, bp, sizeof(*bp));\n\treturn crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));\n}\n\nstatic int tcp_v6_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       const struct in6_addr *daddr, struct in6_addr *saddr,\n\t\t\t       struct tcphdr *th)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\tif (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nstatic int tcp_v6_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       struct sock *sk, struct request_sock *req,\n\t\t\t       struct sk_buff *skb)\n{\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tif (sk) {\n\t\tsaddr = &inet6_sk(sk)->saddr;\n\t\tdaddr = &inet6_sk(sk)->daddr;\n\t} else if (req) {\n\t\tsaddr = &inet6_rsk(req)->loc_addr;\n\t\tdaddr = &inet6_rsk(req)->rmt_addr;\n\t} else {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tsaddr = &ip6h->saddr;\n\t\tdaddr = &ip6h->daddr;\n\t}\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\n\tif (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nstatic int tcp_v6_inbound_md5_hash (struct sock *sk, struct sk_buff *skb)\n{\n\t__u8 *hash_location = NULL;\n\tstruct tcp_md5sig_key *hash_expected;\n\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tint genhash;\n\tu8 newhash[16];\n\n\thash_expected = tcp_v6_md5_do_lookup(sk, &ip6h->saddr);\n\thash_location = tcp_parse_md5sig_option(th);\n\n\t/* We've parsed the options - do we have a hash? */\n\tif (!hash_expected && !hash_location)\n\t\treturn 0;\n\n\tif (hash_expected && !hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);\n\t\treturn 1;\n\t}\n\n\tif (!hash_expected && hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);\n\t\treturn 1;\n\t}\n\n\t/* check the signature */\n\tgenhash = tcp_v6_md5_hash_skb(newhash,\n\t\t\t\t      hash_expected,\n\t\t\t\t      NULL, NULL, skb);\n\n\tif (genhash || memcmp(hash_location, newhash, 16) != 0) {\n\t\tif (net_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"MD5 Hash %s for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t       genhash ? \"failed\" : \"mismatch\",\n\t\t\t       &ip6h->saddr, ntohs(th->source),\n\t\t\t       &ip6h->daddr, ntohs(th->dest));\n\t\t}\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n#endif\n\nstruct request_sock_ops tcp6_request_sock_ops __read_mostly = {\n\t.family\t\t=\tAF_INET6,\n\t.obj_size\t=\tsizeof(struct tcp6_request_sock),\n\t.rtx_syn_ack\t=\ttcp_v6_rtx_synack,\n\t.send_ack\t=\ttcp_v6_reqsk_send_ack,\n\t.destructor\t=\ttcp_v6_reqsk_destructor,\n\t.send_reset\t=\ttcp_v6_send_reset,\n\t.syn_ack_timeout = \ttcp_syn_ack_timeout,\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {\n\t.md5_lookup\t=\ttcp_v6_reqsk_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n};\n#endif\n\nstatic void __tcp_v6_send_check(struct sk_buff *skb,\n\t\t\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tth->check = ~tcp_v6_check(skb->len, saddr, daddr, 0);\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct tcphdr, check);\n\t} else {\n\t\tth->check = tcp_v6_check(skb->len, saddr, daddr,\n\t\t\t\t\t csum_partial(th, th->doff << 2,\n\t\t\t\t\t\t      skb->csum));\n\t}\n}\n\nstatic void tcp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t__tcp_v6_send_check(skb, &np->saddr, &np->daddr);\n}\n\nstatic int tcp_v6_gso_send_check(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *ipv6h;\n\tstruct tcphdr *th;\n\n\tif (!pskb_may_pull(skb, sizeof(*th)))\n\t\treturn -EINVAL;\n\n\tipv6h = ipv6_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tth->check = 0;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t__tcp_v6_send_check(skb, &ipv6h->saddr, &ipv6h->daddr);\n\treturn 0;\n}\n\nstatic struct sk_buff **tcp6_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *iph = skb_gro_network_header(skb);\n\n\tswitch (skb->ip_summed) {\n\tcase CHECKSUM_COMPLETE:\n\t\tif (!tcp_v6_check(skb_gro_len(skb), &iph->saddr, &iph->daddr,\n\t\t\t\t  skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* fall through */\n\tcase CHECKSUM_NONE:\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\treturn tcp_gro_receive(head, skb);\n}\n\nstatic int tcp6_gro_complete(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tth->check = ~tcp_v6_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  &iph->saddr, &iph->daddr, 0);\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\n\n\treturn tcp_gro_complete(skb);\n}\n\nstatic void tcp_v6_send_response(struct sk_buff *skb, u32 seq, u32 ack, u32 win,\n\t\t\t\t u32 ts, struct tcp_md5sig_key *key, int rst)\n{\n\tstruct tcphdr *th = tcp_hdr(skb), *t1;\n\tstruct sk_buff *buff;\n\tstruct flowi6 fl6;\n\tstruct net *net = dev_net(skb_dst(skb)->dev);\n\tstruct sock *ctl_sk = net->ipv6.tcp_sk;\n\tunsigned int tot_len = sizeof(struct tcphdr);\n\tstruct dst_entry *dst;\n\t__be32 *topt;\n\n\tif (ts)\n\t\ttot_len += TCPOLEN_TSTAMP_ALIGNED;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key)\n\t\ttot_len += TCPOLEN_MD5SIG_ALIGNED;\n#endif\n\n\tbuff = alloc_skb(MAX_HEADER + sizeof(struct ipv6hdr) + tot_len,\n\t\t\t GFP_ATOMIC);\n\tif (buff == NULL)\n\t\treturn;\n\n\tskb_reserve(buff, MAX_HEADER + sizeof(struct ipv6hdr) + tot_len);\n\n\tt1 = (struct tcphdr *) skb_push(buff, tot_len);\n\tskb_reset_transport_header(buff);\n\n\t/* Swap the send and the receive. */\n\tmemset(t1, 0, sizeof(*t1));\n\tt1->dest = th->source;\n\tt1->source = th->dest;\n\tt1->doff = tot_len / 4;\n\tt1->seq = htonl(seq);\n\tt1->ack_seq = htonl(ack);\n\tt1->ack = !rst || !th->ack;\n\tt1->rst = rst;\n\tt1->window = htons(win);\n\n\ttopt = (__be32 *)(t1 + 1);\n\n\tif (ts) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);\n\t\t*topt++ = htonl(tcp_time_stamp);\n\t\t*topt++ = htonl(ts);\n\t}\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\ttcp_v6_md5_hash_hdr((__u8 *)topt, key,\n\t\t\t\t    &ipv6_hdr(skb)->saddr,\n\t\t\t\t    &ipv6_hdr(skb)->daddr, t1);\n\t}\n#endif\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tipv6_addr_copy(&fl6.daddr, &ipv6_hdr(skb)->saddr);\n\tipv6_addr_copy(&fl6.saddr, &ipv6_hdr(skb)->daddr);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tbuff->csum = 0;\n\n\t__tcp_v6_send_check(buff, &fl6.saddr, &fl6.daddr);\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.flowi6_oif = inet6_iif(skb);\n\tfl6.fl6_dport = t1->dest;\n\tfl6.fl6_sport = t1->source;\n\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\t/* Pass a socket to ip6_dst_lookup either it is for RST\n\t * Underlying function will use this to retrieve the network\n\t * namespace\n\t */\n\tdst = ip6_dst_lookup_flow(ctl_sk, &fl6, NULL, false);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(buff, dst);\n\t\tip6_xmit(ctl_sk, buff, &fl6, NULL);\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n\t\tif (rst)\n\t\t\tTCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(buff);\n}\n\nstatic void tcp_v6_send_reset(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tu32 seq = 0, ack_seq = 0;\n\tstruct tcp_md5sig_key *key = NULL;\n\n\tif (th->rst)\n\t\treturn;\n\n\tif (!ipv6_unicast_destination(skb))\n\t\treturn;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (sk)\n\t\tkey = tcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr);\n#endif\n\n\tif (th->ack)\n\t\tseq = ntohl(th->ack_seq);\n\telse\n\t\tack_seq = ntohl(th->seq) + th->syn + th->fin + skb->len -\n\t\t\t  (th->doff << 2);\n\n\ttcp_v6_send_response(skb, seq, ack_seq, 0, 0, key, 1);\n}\n\nstatic void tcp_v6_send_ack(struct sk_buff *skb, u32 seq, u32 ack, u32 win, u32 ts,\n\t\t\t    struct tcp_md5sig_key *key)\n{\n\ttcp_v6_send_response(skb, seq, ack, win, ts, key, 0);\n}\n\nstatic void tcp_v6_timewait_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\n\ttcp_v6_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcptw->tw_ts_recent, tcp_twsk_md5_key(tcptw));\n\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\ttcp_v6_send_ack(skb, tcp_rsk(req)->snt_isn + 1, tcp_rsk(req)->rcv_isn + 1, req->rcv_wnd, req->ts_recent,\n\t\t\ttcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr));\n}\n\n\nstatic struct sock *tcp_v6_hnd_req(struct sock *sk,struct sk_buff *skb)\n{\n\tstruct request_sock *req, **prev;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct sock *nsk;\n\n\t/* Find possible connection requests. */\n\treq = inet6_csk_search_req(sk, &prev, th->source,\n\t\t\t\t   &ipv6_hdr(skb)->saddr,\n\t\t\t\t   &ipv6_hdr(skb)->daddr, inet6_iif(skb));\n\tif (req)\n\t\treturn tcp_check_req(sk, skb, req, prev);\n\n\tnsk = __inet6_lookup_established(sock_net(sk), &tcp_hashinfo,\n\t\t\t&ipv6_hdr(skb)->saddr, th->source,\n\t\t\t&ipv6_hdr(skb)->daddr, ntohs(th->dest), inet6_iif(skb));\n\n\tif (nsk) {\n\t\tif (nsk->sk_state != TCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (!th->syn)\n\t\tsk = cookie_v6_check(sk, skb);\n#endif\n\treturn sk;\n}\n\n/* FIXME: this is substantially similar to the ipv4 code.\n * Can some kind of merge be done? -- erics\n */\nstatic int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_extend_values tmp_ext;\n\tstruct tcp_options_received tmp_opt;\n\tu8 *hash_location;\n\tstruct request_sock *req;\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 isn = TCP_SKB_CB(skb)->when;\n\tstruct dst_entry *dst = NULL;\n#ifdef CONFIG_SYN_COOKIES\n\tint want_cookie = 0;\n#else\n#define want_cookie 0\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\tgoto drop;\n\n\tif (inet_csk_reqsk_queue_is_full(sk) && !isn) {\n\t\tif (net_ratelimit())\n\t\t\tsyn_flood_warning(skb);\n#ifdef CONFIG_SYN_COOKIES\n\t\tif (sysctl_tcp_syncookies)\n\t\t\twant_cookie = 1;\n\t\telse\n#endif\n\t\tgoto drop;\n\t}\n\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet6_reqsk_alloc(&tcp6_request_sock_ops);\n\tif (req == NULL)\n\t\tgoto drop;\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttcp_rsk(req)->af_specific = &tcp_request_sock_ipv6_ops;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\ttmp_opt.user_mss = tp->rx_opt.user_mss;\n\ttcp_parse_options(skb, &tmp_opt, &hash_location, 0);\n\n\tif (tmp_opt.cookie_plus > 0 &&\n\t    tmp_opt.saw_tstamp &&\n\t    !tp->rx_opt.cookie_out_never &&\n\t    (sysctl_tcp_cookie_size > 0 ||\n\t     (tp->cookie_values != NULL &&\n\t      tp->cookie_values->cookie_desired > 0))) {\n\t\tu8 *c;\n\t\tu32 *d;\n\t\tu32 *mess = &tmp_ext.cookie_bakery[COOKIE_DIGEST_WORDS];\n\t\tint l = tmp_opt.cookie_plus - TCPOLEN_COOKIE_BASE;\n\n\t\tif (tcp_cookie_generator(&tmp_ext.cookie_bakery[0]) != 0)\n\t\t\tgoto drop_and_free;\n\n\t\t/* Secret recipe starts with IP addresses */\n\t\td = (__force u32 *)&ipv6_hdr(skb)->daddr.s6_addr32[0];\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\td = (__force u32 *)&ipv6_hdr(skb)->saddr.s6_addr32[0];\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\n\t\t/* plus variable length Initiator Cookie */\n\t\tc = (u8 *)mess;\n\t\twhile (l-- > 0)\n\t\t\t*c++ ^= *hash_location++;\n\n#ifdef CONFIG_SYN_COOKIES\n\t\twant_cookie = 0;\t/* not our kind of cookie */\n#endif\n\t\ttmp_ext.cookie_out_never = 0; /* false */\n\t\ttmp_ext.cookie_plus = tmp_opt.cookie_plus;\n\t} else if (!tp->rx_opt.cookie_in_always) {\n\t\t/* redundant indications, but ensure initialization. */\n\t\ttmp_ext.cookie_out_never = 1; /* true */\n\t\ttmp_ext.cookie_plus = 0;\n\t} else {\n\t\tgoto drop_and_free;\n\t}\n\ttmp_ext.cookie_in_always = tp->rx_opt.cookie_in_always;\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb);\n\n\ttreq = inet6_rsk(req);\n\tipv6_addr_copy(&treq->rmt_addr, &ipv6_hdr(skb)->saddr);\n\tipv6_addr_copy(&treq->loc_addr, &ipv6_hdr(skb)->daddr);\n\tif (!want_cookie || tmp_opt.tstamp_ok)\n\t\tTCP_ECN_create_request(req, tcp_hdr(skb));\n\n\tif (!isn) {\n\t\tstruct inet_peer *peer = NULL;\n\n\t\tif (ipv6_opt_accepted(sk, skb) ||\n\t\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\t\tatomic_inc(&skb->users);\n\t\t\ttreq->pktopts = skb;\n\t\t}\n\t\ttreq->iif = sk->sk_bound_dev_if;\n\n\t\t/* So that link locals have meaning */\n\t\tif (!sk->sk_bound_dev_if &&\n\t\t    ipv6_addr_type(&treq->rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\ttreq->iif = inet6_iif(skb);\n\n\t\tif (want_cookie) {\n\t\t\tisn = cookie_v6_init_sequence(sk, skb, &req->mss);\n\t\t\treq->cookie_ts = tmp_opt.tstamp_ok;\n\t\t\tgoto have_isn;\n\t\t}\n\n\t\t/* VJ's idea. We save last timestamp seen\n\t\t * from the destination in peer table, when entering\n\t\t * state TIME-WAIT, and check against it before\n\t\t * accepting new connection request.\n\t\t *\n\t\t * If \"isn\" is not zero, this request hit alive\n\t\t * timewait bucket, so that all the necessary checks\n\t\t * are made in the function processing timewait state.\n\t\t */\n\t\tif (tmp_opt.saw_tstamp &&\n\t\t    tcp_death_row.sysctl_tw_recycle &&\n\t\t    (dst = inet6_csk_route_req(sk, req)) != NULL &&\n\t\t    (peer = rt6_get_peer((struct rt6_info *)dst)) != NULL &&\n\t\t    ipv6_addr_equal((struct in6_addr *)peer->daddr.addr.a6,\n\t\t\t\t    &treq->rmt_addr)) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp < TCP_PAWS_MSL &&\n\t\t\t    (s32)(peer->tcp_ts - req->ts_recent) >\n\t\t\t\t\t\t\tTCP_PAWS_WINDOW) {\n\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);\n\t\t\t\tgoto drop_and_release;\n\t\t\t}\n\t\t}\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\telse if (!sysctl_tcp_syncookies &&\n\t\t\t (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t\t  (sysctl_max_syn_backlog >> 2)) &&\n\t\t\t (!peer || !peer->tcp_ts_stamp) &&\n\t\t\t (!dst || !dst_metric(dst, RTAX_RTT))) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tLIMIT_NETDEBUG(KERN_DEBUG \"TCP: drop open request from %pI6/%u\\n\",\n\t\t\t\t       &treq->rmt_addr, ntohs(tcp_hdr(skb)->source));\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = tcp_v6_init_sequence(skb);\n\t}\nhave_isn:\n\ttcp_rsk(req)->snt_isn = isn;\n\n\tsecurity_inet_conn_request(sk, skb, req);\n\n\tif (tcp_v6_send_synack(sk, req,\n\t\t\t       (struct request_values *)&tmp_ext) ||\n\t    want_cookie)\n\t\tgoto drop_and_free;\n\n\tinet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\treturn 0; /* don't send reset */\n}\n\nstatic struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);\n\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\ttreq = inet6_rsk(req);\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, req);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &treq->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);\n\tnewsk->sk_bound_dev_if = treq->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->opt = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (treq->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(treq->pktopts);\n\t\ttreq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (opt) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tif ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v6_md5_do_add(newsk, &newnp->daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\nstatic __sum16 tcp_v6_checksum_init(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!tcp_v6_check(skb->len, &ipv6_hdr(skb)->saddr,\n\t\t\t\t  &ipv6_hdr(skb)->daddr, skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = ~csum_unfold(tcp_v6_check(skb->len,\n\t\t\t\t\t      &ipv6_hdr(skb)->saddr,\n\t\t\t\t\t      &ipv6_hdr(skb)->daddr, 0));\n\n\tif (skb->len <= 76) {\n\t\treturn __skb_checksum_complete(skb);\n\t}\n\treturn 0;\n}\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp;\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, tcp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   tcp_v6_hnd_req and tcp_v6_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_do_rcv(sk, skb);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (tcp_v6_inbound_md5_hash (sk, skb))\n\t\tgoto discard;\n#endif\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t *\tsocket locking is here for SMP purposes as backlog rcv\n\t *\tis currently called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all)\n\t\topt_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\t\tif (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len))\n\t\t\tgoto reset;\n\t\tif (opt_skb)\n\t\t\tgoto ipv6_pktoptions;\n\t\treturn 0;\n\t}\n\n\tif (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v6_hnd_req(sk, skb);\n\t\tif (!nsk)\n\t\t\tgoto discard;\n\n\t\t/*\n\t\t * Queue it on the new socket if the new socket is active,\n\t\t * otherwise we just shortcircuit this and continue with\n\t\t * the new socket..\n\t\t */\n\t\tif(nsk != sk) {\n\t\t\tif (tcp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\tif (opt_skb)\n\t\t\t\t__kfree_skb(opt_skb);\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\tif (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb->len))\n\t\tgoto reset;\n\tif (opt_skb)\n\t\tgoto ipv6_pktoptions;\n\treturn 0;\n\nreset:\n\ttcp_v6_send_reset(sk, skb);\ndiscard:\n\tif (opt_skb)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\ncsum_err:\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n\n\nipv6_pktoptions:\n\t/* Do you ask, what is it?\n\n\t   1. skb was enqueued by tcp.\n\t   2. skb is added to tail of read queue, rather than out of order.\n\t   3. socket is not in passive state.\n\t   4. Finally, it really contains options, which user wants to receive.\n\t */\n\ttp = tcp_sk(sk);\n\tif (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt &&\n\t    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\tif (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo)\n\t\t\tnp->mcast_oif = inet6_iif(opt_skb);\n\t\tif (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim)\n\t\t\tnp->mcast_hops = ipv6_hdr(opt_skb)->hop_limit;\n\t\tif (ipv6_opt_accepted(sk, opt_skb)) {\n\t\t\tskb_set_owner_r(opt_skb, sk);\n\t\t\topt_skb = xchg(&np->pktoptions, opt_skb);\n\t\t} else {\n\t\t\t__kfree_skb(opt_skb);\n\t\t\topt_skb = xchg(&np->pktoptions, NULL);\n\t\t}\n\t}\n\n\tkfree_skb(opt_skb);\n\treturn 0;\n}\n\nstatic int tcp_v6_rcv(struct sk_buff *skb)\n{\n\tstruct tcphdr *th;\n\tconst struct ipv6hdr *hdr;\n\tstruct sock *sk;\n\tint ret;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/*\n\t *\tCount it even if it's bad.\n\t */\n\tTCP_INC_STATS_BH(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr)/4)\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff*4))\n\t\tgoto discard_it;\n\n\tif (!skb_csum_unnecessary(skb) && tcp_v6_checksum_init(skb))\n\t\tgoto bad_packet;\n\n\tth = tcp_hdr(skb);\n\thdr = ipv6_hdr(skb);\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff*4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->when = 0;\n\tTCP_SKB_CB(skb)->flags = ipv6_get_dsfield(hdr);\n\tTCP_SKB_CB(skb)->sacked = 0;\n\n\tsk = __inet6_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (hdr->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tbh_lock_sock_nested(sk);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n#ifdef CONFIG_NET_DMA\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)\n\t\t\ttp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);\n\t\tif (tp->ucopy.dma_chan)\n\t\t\tret = tcp_v6_do_rcv(sk, skb);\n\t\telse\n#endif\n\t\t{\n\t\t\tif (!tcp_prequeue(sk, skb))\n\t\t\t\tret = tcp_v6_do_rcv(sk, skb);\n\t\t}\n\t} else if (unlikely(sk_add_backlog(sk, skb))) {\n\t\tbh_unlock_sock(sk);\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\n\tsock_put(sk);\n\treturn ret ? -1 : 0;\n\nno_tcp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\tif (skb->len < (th->doff<<2) || tcp_checksum_complete(skb)) {\nbad_packet:\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v6_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\n\t/*\n\t *\tDiscard frame\n\t */\n\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tif (skb->len < (th->doff<<2) || tcp_checksum_complete(skb)) {\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN:\n\t{\n\t\tstruct sock *sk2;\n\n\t\tsk2 = inet6_lookup_listener(dev_net(skb->dev), &tcp_hashinfo,\n\t\t\t\t\t    &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t    ntohs(th->dest), inet6_iif(skb));\n\t\tif (sk2 != NULL) {\n\t\t\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\t\t\tinet_twsk_deschedule(tw, &tcp_death_row);\n\t\t\tinet_twsk_put(tw);\n\t\t\tsk = sk2;\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v6_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\tgoto no_tcp_socket;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n\nstatic struct inet_peer *tcp_v6_get_peer(struct sock *sk, bool *release_it)\n{\n\tstruct rt6_info *rt = (struct rt6_info *) __sk_dst_get(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_peer *peer;\n\n\tif (!rt ||\n\t    !ipv6_addr_equal(&np->daddr, &rt->rt6i_dst.addr)) {\n\t\tpeer = inet_getpeer_v6(&np->daddr, 1);\n\t\t*release_it = true;\n\t} else {\n\t\tif (!rt->rt6i_peer)\n\t\t\trt6_bind_peer(rt, 1);\n\t\tpeer = rt->rt6i_peer;\n\t\t*release_it = false;\n\t}\n\n\treturn peer;\n}\n\nstatic void *tcp_v6_tw_get_peer(struct sock *sk)\n{\n\tstruct inet6_timewait_sock *tw6 = inet6_twsk(sk);\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\n\tif (tw->tw_family == AF_INET)\n\t\treturn tcp_v4_tw_get_peer(sk);\n\n\treturn inet_getpeer_v6(&tw6->tw_v6_daddr, 1);\n}\n\nstatic struct timewait_sock_ops tcp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp6_timewait_sock),\n\t.twsk_unique\t= tcp_twsk_unique,\n\t.twsk_destructor= tcp_twsk_destructor,\n\t.twsk_getpeer\t= tcp_v6_tw_get_peer,\n};\n\nstatic const struct inet_connection_sock_af_ops ipv6_specific = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = tcp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.get_peer\t   = tcp_v6_get_peer,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {\n\t.md5_lookup\t=\ttcp_v6_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n\t.md5_add\t=\ttcp_v6_md5_add_func,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n};\n#endif\n\n/*\n *\tTCP over IPv4 via INET6 API\n */\n\nstatic const struct inet_connection_sock_af_ops ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.get_peer\t   = tcp_v4_get_peer,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {\n\t.md5_lookup\t=\ttcp_v4_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n\t.md5_add\t=\ttcp_v6_md5_add_func,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n};\n#endif\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int tcp_v6_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tskb_queue_head_init(&tp->out_of_order_queue);\n\ttcp_init_xmit_timers(sk);\n\ttcp_prequeue_init(tp);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ttp->mdev = TCP_TIMEOUT_INIT;\n\n\t/* So many TCP implementations out there (incorrectly) count the\n\t * initial SYN frame in their delayed-ACK and congestion control\n\t * algorithms that we must have the following bandaid to talk\n\t * efficiently to them.  -DaveM\n\t */\n\ttp->snd_cwnd = 2;\n\n\t/* See draft-stevens-tcpca-spec-01 for discussion of the\n\t * initialization of these values.\n\t */\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = sysctl_tcp_reordering;\n\n\tsk->sk_state = TCP_CLOSE;\n\n\ticsk->icsk_af_ops = &ipv6_specific;\n\ticsk->icsk_ca_ops = &tcp_init_congestion_ops;\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\n\t/* TCP Cookie Transactions */\n\tif (sysctl_tcp_cookie_size > 0) {\n\t\t/* Default, cookies without s_data_payload. */\n\t\ttp->cookie_values =\n\t\t\tkzalloc(sizeof(*tp->cookie_values),\n\t\t\t\tsk->sk_allocation);\n\t\tif (tp->cookie_values != NULL)\n\t\t\tkref_init(&tp->cookie_values->kref);\n\t}\n\t/* Presumed zeroed, in order of appearance:\n\t *\tcookie_in_always, cookie_out_never,\n\t *\ts_data_constant, s_data_in, s_data_out\n\t */\n\tsk->sk_sndbuf = sysctl_tcp_wmem[1];\n\tsk->sk_rcvbuf = sysctl_tcp_rmem[1];\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&tcp_sockets_allocated);\n\tlocal_bh_enable();\n\n\treturn 0;\n}\n\nstatic void tcp_v6_destroy_sock(struct sock *sk)\n{\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Clean up the MD5 key list */\n\tif (tcp_sk(sk)->md5sig_info)\n\t\ttcp_v6_clear_md5_list(sk);\n#endif\n\ttcp_v4_destroy_sock(sk);\n\tinet6_destroy_sock(sk);\n}\n\n#ifdef CONFIG_PROC_FS\n/* Proc filesystem TCPv6 sock list dumping. */\nstatic void get_openreq6(struct seq_file *seq,\n\t\t\t struct sock *sk, struct request_sock *req, int i, int uid)\n{\n\tint ttd = req->expires - jiffies;\n\tconst struct in6_addr *src = &inet6_rsk(req)->loc_addr;\n\tconst struct in6_addr *dest = &inet6_rsk(req)->rmt_addr;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3],\n\t\t   ntohs(inet_rsk(req)->loc_port),\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3],\n\t\t   ntohs(inet_rsk(req)->rmt_port),\n\t\t   TCP_SYN_RECV,\n\t\t   0,0, /* could print option size, but that is af dependent. */\n\t\t   1,   /* timers active (only the expire timer) */\n\t\t   jiffies_to_clock_t(ttd),\n\t\t   req->retrans,\n\t\t   uid,\n\t\t   0,  /* non standard timer */\n\t\t   0, /* open_requests have no inode */\n\t\t   0, req);\n}\n\nstatic void get_tcp6_sock(struct seq_file *seq, struct sock *sp, int i)\n{\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\tint timer_active;\n\tunsigned long timer_expires;\n\tstruct inet_sock *inet = inet_sk(sp);\n\tstruct tcp_sock *tp = tcp_sk(sp);\n\tconst struct inet_connection_sock *icsk = inet_csk(sp);\n\tstruct ipv6_pinfo *np = inet6_sk(sp);\n\n\tdest  = &np->daddr;\n\tsrc   = &np->rcv_saddr;\n\tdestp = ntohs(inet->inet_dport);\n\tsrcp  = ntohs(inet->inet_sport);\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (timer_pending(&sp->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sp->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %p %lu %lu %u %u %d\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   sp->sk_state,\n\t\t   tp->write_seq-tp->snd_una,\n\t\t   (sp->sk_state == TCP_LISTEN) ? sp->sk_ack_backlog : (tp->rcv_nxt - tp->copied_seq),\n\t\t   timer_active,\n\t\t   jiffies_to_clock_t(timer_expires - jiffies),\n\t\t   icsk->icsk_retransmits,\n\t\t   sock_i_uid(sp),\n\t\t   icsk->icsk_probes_out,\n\t\t   sock_i_ino(sp),\n\t\t   atomic_read(&sp->sk_refcnt), sp,\n\t\t   jiffies_to_clock_t(icsk->icsk_rto),\n\t\t   jiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t   (icsk->icsk_ack.quick << 1 ) | icsk->icsk_ack.pingpong,\n\t\t   tp->snd_cwnd,\n\t\t   tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh\n\t\t   );\n}\n\nstatic void get_timewait6_sock(struct seq_file *seq,\n\t\t\t       struct inet_timewait_sock *tw, int i)\n{\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\tstruct inet6_timewait_sock *tw6 = inet6_twsk((struct sock *)tw);\n\tint ttd = tw->tw_ttd - jiffies;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tdest = &tw6->tw_v6_daddr;\n\tsrc  = &tw6->tw_v6_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   tw->tw_substate, 0, 0,\n\t\t   3, jiffies_to_clock_t(ttd), 0, 0, 0, 0,\n\t\t   atomic_read(&tw->tw_refcnt), tw);\n}\n\nstatic int tcp6_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"  sl  \"\n\t\t\t \"local_address                         \"\n\t\t\t \"remote_address                        \"\n\t\t\t \"st tx_queue rx_queue tr tm->when retrnsmt\"\n\t\t\t \"   uid  timeout inode\\n\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tget_tcp6_sock(seq, v, st->num);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_OPENREQ:\n\t\tget_openreq6(seq, st->syn_wait_sk, v, st->num, st->uid);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\tget_timewait6_sock(seq, v, st->num);\n\t\tbreak;\n\t}\nout:\n\treturn 0;\n}\n\nstatic struct tcp_seq_afinfo tcp6_seq_afinfo = {\n\t.name\t\t= \"tcp6\",\n\t.family\t\t= AF_INET6,\n\t.seq_fops\t= {\n\t\t.owner\t\t= THIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= tcp6_seq_show,\n\t},\n};\n\nint __net_init tcp6_proc_init(struct net *net)\n{\n\treturn tcp_proc_register(net, &tcp6_seq_afinfo);\n}\n\nvoid tcp6_proc_exit(struct net *net)\n{\n\ttcp_proc_unregister(net, &tcp6_seq_afinfo);\n}\n#endif\n\nstruct proto tcpv6_prot = {\n\t.name\t\t\t= \"TCPv6\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.connect\t\t= tcp_v6_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v6_init_sock,\n\t.destroy\t\t= tcp_v6_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.sendpage\t\t= tcp_sendpage,\n\t.backlog_rcv\t\t= tcp_v6_do_rcv,\n\t.hash\t\t\t= tcp_v6_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\t.memory_allocated\t= &tcp_memory_allocated,\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.orphan_count\t\t= &tcp_orphan_count,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem\t\t= sysctl_tcp_wmem,\n\t.sysctl_rmem\t\t= sysctl_tcp_rmem,\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp6_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.twsk_prot\t\t= &tcp6_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp6_request_sock_ops,\n\t.h.hashinfo\t\t= &tcp_hashinfo,\n\t.no_autobind\t\t= true,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_tcp_setsockopt,\n\t.compat_getsockopt\t= compat_tcp_getsockopt,\n#endif\n};\n\nstatic const struct inet6_protocol tcpv6_protocol = {\n\t.handler\t=\ttcp_v6_rcv,\n\t.err_handler\t=\ttcp_v6_err,\n\t.gso_send_check\t=\ttcp_v6_gso_send_check,\n\t.gso_segment\t=\ttcp_tso_segment,\n\t.gro_receive\t=\ttcp6_gro_receive,\n\t.gro_complete\t=\ttcp6_gro_complete,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\nstatic struct inet_protosw tcpv6_protosw = {\n\t.type\t\t=\tSOCK_STREAM,\n\t.protocol\t=\tIPPROTO_TCP,\n\t.prot\t\t=\t&tcpv6_prot,\n\t.ops\t\t=\t&inet6_stream_ops,\n\t.no_check\t=\t0,\n\t.flags\t\t=\tINET_PROTOSW_PERMANENT |\n\t\t\t\tINET_PROTOSW_ICSK,\n};\n\nstatic int __net_init tcpv6_net_init(struct net *net)\n{\n\treturn inet_ctl_sock_create(&net->ipv6.tcp_sk, PF_INET6,\n\t\t\t\t    SOCK_RAW, IPPROTO_TCP, net);\n}\n\nstatic void __net_exit tcpv6_net_exit(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->ipv6.tcp_sk);\n}\n\nstatic void __net_exit tcpv6_net_exit_batch(struct list_head *net_exit_list)\n{\n\tinet_twsk_purge(&tcp_hashinfo, &tcp_death_row, AF_INET6);\n}\n\nstatic struct pernet_operations tcpv6_net_ops = {\n\t.init\t    = tcpv6_net_init,\n\t.exit\t    = tcpv6_net_exit,\n\t.exit_batch = tcpv6_net_exit_batch,\n};\n\nint __init tcpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&tcpv6_protocol, IPPROTO_TCP);\n\tif (ret)\n\t\tgoto out;\n\n\t/* register inet6 protocol */\n\tret = inet6_register_protosw(&tcpv6_protosw);\n\tif (ret)\n\t\tgoto out_tcpv6_protocol;\n\n\tret = register_pernet_subsys(&tcpv6_net_ops);\n\tif (ret)\n\t\tgoto out_tcpv6_protosw;\nout:\n\treturn ret;\n\nout_tcpv6_protocol:\n\tinet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);\nout_tcpv6_protosw:\n\tinet6_unregister_protosw(&tcpv6_protosw);\n\tgoto out;\n}\n\nvoid tcpv6_exit(void)\n{\n\tunregister_pernet_subsys(&tcpv6_net_ops);\n\tinet6_unregister_protosw(&tcpv6_protosw);\n\tinet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);\n}\n", "/*\n * L2TPv3 IP encapsulation support\n *\n * Copyright (c) 2008,2009,2010 Katalix Systems Ltd\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/icmp.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/random.h>\n#include <linux/socket.h>\n#include <linux/l2tp.h>\n#include <linux/in.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/tcp_states.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n\n#include \"l2tp_core.h\"\n\nstruct l2tp_ip_sock {\n\t/* inet_sock has to be the first member of l2tp_ip_sock */\n\tstruct inet_sock\tinet;\n\n\t__u32\t\t\tconn_id;\n\t__u32\t\t\tpeer_conn_id;\n\n\t__u64\t\t\ttx_packets;\n\t__u64\t\t\ttx_bytes;\n\t__u64\t\t\ttx_errors;\n\t__u64\t\t\trx_packets;\n\t__u64\t\t\trx_bytes;\n\t__u64\t\t\trx_errors;\n};\n\nstatic DEFINE_RWLOCK(l2tp_ip_lock);\nstatic struct hlist_head l2tp_ip_table;\nstatic struct hlist_head l2tp_ip_bind_table;\n\nstatic inline struct l2tp_ip_sock *l2tp_ip_sk(const struct sock *sk)\n{\n\treturn (struct l2tp_ip_sock *)sk;\n}\n\nstatic struct sock *__l2tp_ip_bind_lookup(struct net *net, __be32 laddr, int dif, u32 tunnel_id)\n{\n\tstruct hlist_node *node;\n\tstruct sock *sk;\n\n\tsk_for_each_bound(sk, node, &l2tp_ip_bind_table) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct l2tp_ip_sock *l2tp = l2tp_ip_sk(sk);\n\n\t\tif (l2tp == NULL)\n\t\t\tcontinue;\n\n\t\tif ((l2tp->conn_id == tunnel_id) &&\n\t\t    net_eq(sock_net(sk), net) &&\n\t\t    !(inet->inet_rcv_saddr && inet->inet_rcv_saddr != laddr) &&\n\t\t    !(sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\t\tgoto found;\n\t}\n\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic inline struct sock *l2tp_ip_bind_lookup(struct net *net, __be32 laddr, int dif, u32 tunnel_id)\n{\n\tstruct sock *sk = __l2tp_ip_bind_lookup(net, laddr, dif, tunnel_id);\n\tif (sk)\n\t\tsock_hold(sk);\n\n\treturn sk;\n}\n\n/* When processing receive frames, there are two cases to\n * consider. Data frames consist of a non-zero session-id and an\n * optional cookie. Control frames consist of a regular L2TP header\n * preceded by 32-bits of zeros.\n *\n * L2TPv3 Session Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                           Session ID                          |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Cookie (optional, maximum 64 bits)...\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *                                                                 |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * L2TPv3 Control Message Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                      (32 bits of zeros)                       |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |T|L|x|x|S|x|x|x|x|x|x|x|  Ver  |             Length            |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                     Control Connection ID                     |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Ns              |               Nr              |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * All control frames are passed to userspace.\n */\nstatic int l2tp_ip_recv(struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tu32 session_id;\n\tu32 tunnel_id;\n\tunsigned char *ptr, *optr;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel = NULL;\n\tint length;\n\tint offset;\n\n\t/* Point to L2TP header */\n\toptr = ptr = skb->data;\n\n\tif (!pskb_may_pull(skb, 4))\n\t\tgoto discard;\n\n\tsession_id = ntohl(*((__be32 *) ptr));\n\tptr += 4;\n\n\t/* RFC3931: L2TP/IP packets have the first 4 bytes containing\n\t * the session_id. If it is 0, the packet is a L2TP control\n\t * frame and the session_id value can be discarded.\n\t */\n\tif (session_id == 0) {\n\t\t__skb_pull(skb, 4);\n\t\tgoto pass_up;\n\t}\n\n\t/* Ok, this is a data packet. Lookup the session. */\n\tsession = l2tp_session_find(&init_net, NULL, session_id);\n\tif (session == NULL)\n\t\tgoto discard;\n\n\ttunnel = session->tunnel;\n\tif (tunnel == NULL)\n\t\tgoto discard;\n\n\t/* Trace packet contents, if enabled */\n\tif (tunnel->debug & L2TP_MSG_DATA) {\n\t\tlength = min(32u, skb->len);\n\t\tif (!pskb_may_pull(skb, length))\n\t\t\tgoto discard;\n\n\t\tprintk(KERN_DEBUG \"%s: ip recv: \", tunnel->name);\n\n\t\toffset = 0;\n\t\tdo {\n\t\t\tprintk(\" %02X\", ptr[offset]);\n\t\t} while (++offset < length);\n\n\t\tprintk(\"\\n\");\n\t}\n\n\tl2tp_recv_common(session, skb, ptr, optr, 0, skb->len, tunnel->recv_payload_hook);\n\n\treturn 0;\n\npass_up:\n\t/* Get the tunnel_id from the L2TP header */\n\tif (!pskb_may_pull(skb, 12))\n\t\tgoto discard;\n\n\tif ((skb->data[0] & 0xc0) != 0xc0)\n\t\tgoto discard;\n\n\ttunnel_id = ntohl(*(__be32 *) &skb->data[4]);\n\ttunnel = l2tp_tunnel_find(&init_net, tunnel_id);\n\tif (tunnel != NULL)\n\t\tsk = tunnel->sock;\n\telse {\n\t\tstruct iphdr *iph = (struct iphdr *) skb_network_header(skb);\n\n\t\tread_lock_bh(&l2tp_ip_lock);\n\t\tsk = __l2tp_ip_bind_lookup(&init_net, iph->daddr, 0, tunnel_id);\n\t\tread_unlock_bh(&l2tp_ip_lock);\n\t}\n\n\tif (sk == NULL)\n\t\tgoto discard;\n\n\tsock_hold(sk);\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_put;\n\n\tnf_reset(skb);\n\n\treturn sk_receive_skb(sk, skb, 1);\n\ndiscard_put:\n\tsock_put(sk);\n\ndiscard:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int l2tp_ip_open(struct sock *sk)\n{\n\t/* Prevent autobind. We don't have ports. */\n\tinet_sk(sk)->inet_num = IPPROTO_L2TP;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_node(sk, &l2tp_ip_table);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\n\treturn 0;\n}\n\nstatic void l2tp_ip_close(struct sock *sk, long timeout)\n{\n\twrite_lock_bh(&l2tp_ip_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\thlist_del_init(&sk->sk_node);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tsk_common_release(sk);\n}\n\nstatic void l2tp_ip_destroy_sock(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(&sk->sk_write_queue)) != NULL)\n\t\tkfree_skb(skb);\n\n\tsk_refcnt_debug_dec(sk);\n}\n\nstatic int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tint ret = -EINVAL;\n\tint chk_addr_ret;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(&init_net, addr->l2tp_addr.s_addr, sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(&init_net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}\n\nstatic int l2tp_ip_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_l2tpip *lsa = (struct sockaddr_l2tpip *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 saddr;\n\tint oif, rc;\n\n\trc = -EINVAL;\n\tif (addr_len < sizeof(*lsa))\n\t\tgoto out;\n\n\trc = -EAFNOSUPPORT;\n\tif (lsa->l2tp_family != AF_INET)\n\t\tgoto out;\n\n\tsk_dst_reset(sk);\n\n\toif = sk->sk_bound_dev_if;\n\tsaddr = inet->inet_saddr;\n\n\trc = -EINVAL;\n\tif (ipv4_is_multicast(lsa->l2tp_addr.s_addr))\n\t\tgoto out;\n\n\trt = ip_route_connect(&fl4, lsa->l2tp_addr.s_addr, saddr,\n\t\t\t      RT_CONN_FLAGS(sk), oif,\n\t\t\t      IPPROTO_L2TP,\n\t\t\t      0, 0, sk, true);\n\tif (IS_ERR(rt)) {\n\t\trc = PTR_ERR(rt);\n\t\tif (rc == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(&init_net, IPSTATS_MIB_OUTNOROUTES);\n\t\tgoto out;\n\t}\n\n\trc = -ENETUNREACH;\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\tgoto out;\n\t}\n\n\tl2tp_ip_sk(sk)->peer_conn_id = lsa->l2tp_conn_id;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tif (!inet->inet_rcv_saddr)\n\t\tinet->inet_rcv_saddr = rt->rt_src;\n\tinet->inet_daddr = rt->rt_dst;\n\tsk->sk_state = TCP_ESTABLISHED;\n\tinet->inet_id = jiffies;\n\n\tsk_dst_set(sk, &rt->dst);\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\n\trc = 0;\nout:\n\treturn rc;\n}\n\nstatic int l2tp_ip_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t   int *uaddr_len, int peer)\n{\n\tstruct sock *sk\t\t= sock->sk;\n\tstruct inet_sock *inet\t= inet_sk(sk);\n\tstruct l2tp_ip_sock *lsk = l2tp_ip_sk(sk);\n\tstruct sockaddr_l2tpip *lsa = (struct sockaddr_l2tpip *)uaddr;\n\n\tmemset(lsa, 0, sizeof(*lsa));\n\tlsa->l2tp_family = AF_INET;\n\tif (peer) {\n\t\tif (!inet->inet_dport)\n\t\t\treturn -ENOTCONN;\n\t\tlsa->l2tp_conn_id = lsk->peer_conn_id;\n\t\tlsa->l2tp_addr.s_addr = inet->inet_daddr;\n\t} else {\n\t\t__be32 addr = inet->inet_rcv_saddr;\n\t\tif (!addr)\n\t\t\taddr = inet->inet_saddr;\n\t\tlsa->l2tp_conn_id = lsk->conn_id;\n\t\tlsa->l2tp_addr.s_addr = addr;\n\t}\n\t*uaddr_len = sizeof(*lsa);\n\treturn 0;\n}\n\nstatic int l2tp_ip_backlog_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tnf_reset(skb);\n\n\t/* Charge it to the socket, dropping if the queue is full. */\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0)\n\t\tgoto drop;\n\n\treturn 0;\n\ndrop:\n\tIP_INC_STATS(&init_net, IPSTATS_MIB_INDISCARDS);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\n/* Userspace will call sendmsg() on the tunnel socket to send L2TP\n * control frames.\n */\nstatic int l2tp_ip_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct sk_buff *skb;\n\tint rc;\n\tstruct l2tp_ip_sock *lsa = l2tp_ip_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options *opt = inet->opt;\n\tstruct rtable *rt = NULL;\n\tint connected = 0;\n\t__be32 daddr;\n\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\treturn -ENOTCONN;\n\n\t/* Get and verify the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_l2tpip *lip = (struct sockaddr_l2tpip *) msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*lip))\n\t\t\treturn -EINVAL;\n\n\t\tif (lip->l2tp_family != AF_INET) {\n\t\t\tif (lip->l2tp_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = lip->l2tp_addr.s_addr;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = inet->inet_daddr;\n\t\tconnected = 1;\n\t}\n\n\t/* Allocate a socket buffer */\n\trc = -ENOMEM;\n\tskb = sock_wmalloc(sk, 2 + NET_SKB_PAD + sizeof(struct iphdr) +\n\t\t\t   4 + len, 0, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto error;\n\n\t/* Reserve space for headers, putting IP header on 4-byte boundary. */\n\tskb_reserve(skb, 2 + NET_SKB_PAD);\n\tskb_reset_network_header(skb);\n\tskb_reserve(skb, sizeof(struct iphdr));\n\tskb_reset_transport_header(skb);\n\n\t/* Insert 0 session_id */\n\t*((__be32 *) skb_put(skb, 4)) = 0;\n\n\t/* Copy user data into skb */\n\trc = memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len);\n\tif (rc < 0) {\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *) __sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\t/* Use correct destination address if we have options. */\n\t\tif (opt && opt->srr)\n\t\t\tdaddr = opt->faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\n\t/* Queue the packet to IP for output */\n\trc = ip_queue_xmit(skb);\n\nerror:\n\t/* Update stats */\n\tif (rc >= 0) {\n\t\tlsa->tx_packets++;\n\t\tlsa->tx_bytes += len;\n\t\trc = len;\n\t} else {\n\t\tlsa->tx_errors++;\n\t}\n\n\treturn rc;\n\nno_route:\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}\n\nstatic int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct l2tp_ip_sock *lsk = l2tp_ip_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err) {\n\t\tlsk->rx_errors++;\n\t\treturn err;\n\t}\n\n\tlsk->rx_packets++;\n\tlsk->rx_bytes += copied;\n\n\treturn copied;\n}\n\nstatic struct proto l2tp_ip_prot = {\n\t.name\t\t   = \"L2TP/IP\",\n\t.owner\t\t   = THIS_MODULE,\n\t.init\t\t   = l2tp_ip_open,\n\t.close\t\t   = l2tp_ip_close,\n\t.bind\t\t   = l2tp_ip_bind,\n\t.connect\t   = l2tp_ip_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = l2tp_ip_destroy_sock,\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.sendmsg\t   = l2tp_ip_sendmsg,\n\t.recvmsg\t   = l2tp_ip_recvmsg,\n\t.backlog_rcv\t   = l2tp_ip_backlog_recv,\n\t.hash\t\t   = inet_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.obj_size\t   = sizeof(struct l2tp_ip_sock),\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ip_setsockopt,\n\t.compat_getsockopt = compat_ip_getsockopt,\n#endif\n};\n\nstatic const struct proto_ops l2tp_ip_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = l2tp_ip_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw l2tp_ip_protosw = {\n\t.type\t\t= SOCK_DGRAM,\n\t.protocol\t= IPPROTO_L2TP,\n\t.prot\t\t= &l2tp_ip_prot,\n\t.ops\t\t= &l2tp_ip_ops,\n\t.no_check\t= 0,\n};\n\nstatic struct net_protocol l2tp_ip_protocol __read_mostly = {\n\t.handler\t= l2tp_ip_recv,\n};\n\nstatic int __init l2tp_ip_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"L2TP IP encapsulation support (L2TPv3)\\n\");\n\n\terr = proto_register(&l2tp_ip_prot, 1);\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet_add_protocol(&l2tp_ip_protocol, IPPROTO_L2TP);\n\tif (err)\n\t\tgoto out1;\n\n\tinet_register_protosw(&l2tp_ip_protosw);\n\treturn 0;\n\nout1:\n\tproto_unregister(&l2tp_ip_prot);\nout:\n\treturn err;\n}\n\nstatic void __exit l2tp_ip_exit(void)\n{\n\tinet_unregister_protosw(&l2tp_ip_protosw);\n\tinet_del_protocol(&l2tp_ip_protocol, IPPROTO_L2TP);\n\tproto_unregister(&l2tp_ip_prot);\n}\n\nmodule_init(l2tp_ip_init);\nmodule_exit(l2tp_ip_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"James Chapman <jchapman@katalix.com>\");\nMODULE_DESCRIPTION(\"L2TP over IP\");\nMODULE_VERSION(\"1.0\");\n\n/* Use the value of SOCK_DGRAM (2) directory, because __stringify does't like\n * enums\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET, 2, IPPROTO_L2TP);\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for inet_sock\n *\n * Authors:\tMany, reorganised here by\n * \t\tArnaldo Carvalho de Melo <acme@mandriva.com>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n#ifndef _INET_SOCK_H\n#define _INET_SOCK_H\n\n\n#include <linux/kmemcheck.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/jhash.h>\n#include <linux/netdevice.h>\n\n#include <net/flow.h>\n#include <net/sock.h>\n#include <net/request_sock.h>\n#include <net/netns/hash.h>\n\n/** struct ip_options - IP Options\n *\n * @faddr - Saved first hop address\n * @is_data - Options in __data, rather than skb\n * @is_strictroute - Strict source route\n * @srr_is_hit - Packet destination addr was our one\n * @is_changed - IP checksum more not valid\n * @rr_needaddr - Need to record addr of outgoing dev\n * @ts_needtime - Need to record timestamp\n * @ts_needaddr - Need to record addr of outgoing dev\n */\nstruct ip_options {\n\t__be32\t\tfaddr;\n\tunsigned char\toptlen;\n\tunsigned char\tsrr;\n\tunsigned char\trr;\n\tunsigned char\tts;\n\tunsigned char\tis_strictroute:1,\n\t\t\tsrr_is_hit:1,\n\t\t\tis_changed:1,\n\t\t\trr_needaddr:1,\n\t\t\tts_needtime:1,\n\t\t\tts_needaddr:1;\n\tunsigned char\trouter_alert;\n\tunsigned char\tcipso;\n\tunsigned char\t__pad2;\n\tunsigned char\t__data[0];\n};\n\nstruct ip_options_rcu {\n\tstruct rcu_head rcu;\n\tstruct ip_options opt;\n};\n\nstruct ip_options_data {\n\tstruct ip_options_rcu\topt;\n\tchar\t\t\tdata[40];\n};\n\nstruct inet_request_sock {\n\tstruct request_sock\treq;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tu16\t\t\tinet6_rsk_offset;\n#endif\n\t__be16\t\t\tloc_port;\n\t__be32\t\t\tloc_addr;\n\t__be32\t\t\trmt_addr;\n\t__be16\t\t\trmt_port;\n\tkmemcheck_bitfield_begin(flags);\n\tu16\t\t\tsnd_wscale : 4,\n\t\t\t\trcv_wscale : 4,\n\t\t\t\ttstamp_ok  : 1,\n\t\t\t\tsack_ok\t   : 1,\n\t\t\t\twscale_ok  : 1,\n\t\t\t\tecn_ok\t   : 1,\n\t\t\t\tacked\t   : 1,\n\t\t\t\tno_srccheck: 1;\n\tkmemcheck_bitfield_end(flags);\n\tstruct ip_options_rcu\t*opt;\n};\n\nstatic inline struct inet_request_sock *inet_rsk(const struct request_sock *sk)\n{\n\treturn (struct inet_request_sock *)sk;\n}\n\nstruct inet_cork {\n\tunsigned int\t\tflags;\n\tunsigned int\t\tfragsize;\n\tstruct ip_options\t*opt;\n\tstruct dst_entry\t*dst;\n\tint\t\t\tlength; /* Total length of all frames */\n\t__be32\t\t\taddr;\n\tstruct flowi\t\tfl;\n\tstruct page\t\t*page;\n\tu32\t\t\toff;\n\tu8\t\t\ttx_flags;\n};\n\nstruct ip_mc_socklist;\nstruct ipv6_pinfo;\nstruct rtable;\n\n/** struct inet_sock - representation of INET sockets\n *\n * @sk - ancestor class\n * @pinet6 - pointer to IPv6 control block\n * @inet_daddr - Foreign IPv4 addr\n * @inet_rcv_saddr - Bound local IPv4 addr\n * @inet_dport - Destination port\n * @inet_num - Local port\n * @inet_saddr - Sending source\n * @uc_ttl - Unicast TTL\n * @inet_sport - Source port\n * @inet_id - ID counter for DF pkts\n * @tos - TOS\n * @mc_ttl - Multicasting TTL\n * @is_icsk - is this an inet_connection_sock?\n * @mc_index - Multicast device index\n * @mc_list - Group array\n * @cork - info to build ip hdr on each ip frag while socket is corked\n */\nstruct inet_sock {\n\t/* sk and pinet6 has to be the first two members of inet_sock */\n\tstruct sock\t\tsk;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tstruct ipv6_pinfo\t*pinet6;\n#endif\n\t/* Socket demultiplex comparisons on incoming packets. */\n#define inet_daddr\t\tsk.__sk_common.skc_daddr\n#define inet_rcv_saddr\t\tsk.__sk_common.skc_rcv_saddr\n\n\t__be16\t\t\tinet_dport;\n\t__u16\t\t\tinet_num;\n\t__be32\t\t\tinet_saddr;\n\t__s16\t\t\tuc_ttl;\n\t__u16\t\t\tcmsg_flags;\n\t__be16\t\t\tinet_sport;\n\t__u16\t\t\tinet_id;\n\n\tstruct ip_options_rcu __rcu\t*inet_opt;\n\t__u8\t\t\ttos;\n\t__u8\t\t\tmin_ttl;\n\t__u8\t\t\tmc_ttl;\n\t__u8\t\t\tpmtudisc;\n\t__u8\t\t\trecverr:1,\n\t\t\t\tis_icsk:1,\n\t\t\t\tfreebind:1,\n\t\t\t\thdrincl:1,\n\t\t\t\tmc_loop:1,\n\t\t\t\ttransparent:1,\n\t\t\t\tmc_all:1,\n\t\t\t\tnodefrag:1;\n\tint\t\t\tmc_index;\n\t__be32\t\t\tmc_addr;\n\tstruct ip_mc_socklist __rcu\t*mc_list;\n\tstruct inet_cork\tcork;\n};\n\n#define IPCORK_OPT\t1\t/* ip-options has been held in ipcork.opt */\n#define IPCORK_ALLFRAG\t2\t/* always fragment (for ipv6 for now) */\n\nstatic inline struct inet_sock *inet_sk(const struct sock *sk)\n{\n\treturn (struct inet_sock *)sk;\n}\n\nstatic inline void __inet_sk_copy_descendant(struct sock *sk_to,\n\t\t\t\t\t     const struct sock *sk_from,\n\t\t\t\t\t     const int ancestor_size)\n{\n\tmemcpy(inet_sk(sk_to) + 1, inet_sk(sk_from) + 1,\n\t       sk_from->sk_prot->obj_size - ancestor_size);\n}\n#if !(defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE))\nstatic inline void inet_sk_copy_descendant(struct sock *sk_to,\n\t\t\t\t\t   const struct sock *sk_from)\n{\n\t__inet_sk_copy_descendant(sk_to, sk_from, sizeof(struct inet_sock));\n}\n#endif\n\nextern int inet_sk_rebuild_header(struct sock *sk);\n\nextern u32 inet_ehash_secret;\nextern void build_ehash_secret(void);\n\nstatic inline unsigned int inet_ehashfn(struct net *net,\n\t\t\t\t\tconst __be32 laddr, const __u16 lport,\n\t\t\t\t\tconst __be32 faddr, const __be16 fport)\n{\n\treturn jhash_3words((__force __u32) laddr,\n\t\t\t    (__force __u32) faddr,\n\t\t\t    ((__u32) lport) << 16 | (__force __u32)fport,\n\t\t\t    inet_ehash_secret + net_hash_mix(net));\n}\n\nstatic inline int inet_sk_ehashfn(const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst __be32 laddr = inet->inet_rcv_saddr;\n\tconst __u16 lport = inet->inet_num;\n\tconst __be32 faddr = inet->inet_daddr;\n\tconst __be16 fport = inet->inet_dport;\n\tstruct net *net = sock_net(sk);\n\n\treturn inet_ehashfn(net, laddr, lport, faddr, fport);\n}\n\nstatic inline struct request_sock *inet_reqsk_alloc(struct request_sock_ops *ops)\n{\n\tstruct request_sock *req = reqsk_alloc(ops);\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\tif (req != NULL) {\n\t\tkmemcheck_annotate_bitfield(ireq, flags);\n\t\tireq->opt = NULL;\n\t}\n\n\treturn req;\n}\n\nstatic inline __u8 inet_sk_flowi_flags(const struct sock *sk)\n{\n\t__u8 flags = 0;\n\n\tif (inet_sk(sk)->transparent)\n\t\tflags |= FLOWI_FLAG_ANYSRC;\n\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\tflags |= FLOWI_FLAG_PRECOW_METRICS;\n\treturn flags;\n}\n\n#endif\t/* _INET_SOCK_H */\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the IP module.\n *\n * Version:\t@(#)ip.h\t1.0.2\t05/07/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Changes:\n *\t\tMike McLagan    :       Routing by source\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n#ifndef _IP_H\n#define _IP_H\n\n#include <linux/types.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/skbuff.h>\n\n#include <net/inet_sock.h>\n#include <net/snmp.h>\n#include <net/flow.h>\n\nstruct sock;\n\nstruct inet_skb_parm {\n\tstruct ip_options\topt;\t\t/* Compiled IP options\t\t*/\n\tunsigned char\t\tflags;\n\n#define IPSKB_FORWARDED\t\t1\n#define IPSKB_XFRM_TUNNEL_SIZE\t2\n#define IPSKB_XFRM_TRANSFORMED\t4\n#define IPSKB_FRAG_COMPLETE\t8\n#define IPSKB_REROUTED\t\t16\n};\n\nstatic inline unsigned int ip_hdrlen(const struct sk_buff *skb)\n{\n\treturn ip_hdr(skb)->ihl * 4;\n}\n\nstruct ipcm_cookie {\n\t__be32\t\t\taddr;\n\tint\t\t\toif;\n\tstruct ip_options_rcu\t*opt;\n\t__u8\t\t\ttx_flags;\n};\n\n#define IPCB(skb) ((struct inet_skb_parm*)((skb)->cb))\n\nstruct ip_ra_chain {\n\tstruct ip_ra_chain __rcu *next;\n\tstruct sock\t\t*sk;\n\tunion {\n\t\tvoid\t\t\t(*destructor)(struct sock *);\n\t\tstruct sock\t\t*saved_sk;\n\t};\n\tstruct rcu_head\t\trcu;\n};\n\nextern struct ip_ra_chain __rcu *ip_ra_chain;\n\n/* IP flags. */\n#define IP_CE\t\t0x8000\t\t/* Flag: \"Congestion\"\t\t*/\n#define IP_DF\t\t0x4000\t\t/* Flag: \"Don't Fragment\"\t*/\n#define IP_MF\t\t0x2000\t\t/* Flag: \"More Fragments\"\t*/\n#define IP_OFFSET\t0x1FFF\t\t/* \"Fragment Offset\" part\t*/\n\n#define IP_FRAG_TIME\t(30 * HZ)\t\t/* fragment lifetime\t*/\n\nstruct msghdr;\nstruct net_device;\nstruct packet_type;\nstruct rtable;\nstruct sockaddr;\n\nextern int\t\tigmp_mc_proc_init(void);\n\n/*\n *\tFunctions provided by ip.c\n */\n\nextern int\t\tip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\t      __be32 saddr, __be32 daddr,\n\t\t\t\t\t      struct ip_options_rcu *opt);\nextern int\t\tip_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t\t       struct packet_type *pt, struct net_device *orig_dev);\nextern int\t\tip_local_deliver(struct sk_buff *skb);\nextern int\t\tip_mr_input(struct sk_buff *skb);\nextern int\t\tip_output(struct sk_buff *skb);\nextern int\t\tip_mc_output(struct sk_buff *skb);\nextern int\t\tip_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *));\nextern int\t\tip_do_nat(struct sk_buff *skb);\nextern void\t\tip_send_check(struct iphdr *ip);\nextern int\t\t__ip_local_out(struct sk_buff *skb);\nextern int\t\tip_local_out(struct sk_buff *skb);\nextern int\t\tip_queue_xmit(struct sk_buff *skb);\nextern void\t\tip_init(void);\nextern int\t\tip_append_data(struct sock *sk,\n\t\t\t\t       int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\t\t\t   int odd, struct sk_buff *skb),\n\t\t\t\tvoid *from, int len, int protolen,\n\t\t\t\tstruct ipcm_cookie *ipc,\n\t\t\t\tstruct rtable **rt,\n\t\t\t\tunsigned int flags);\nextern int\t\tip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb);\nextern ssize_t\t\tip_append_page(struct sock *sk, struct page *page,\n\t\t\t\tint offset, size_t size, int flags);\nextern struct sk_buff  *__ip_make_skb(struct sock *sk,\n\t\t\t\t      struct sk_buff_head *queue,\n\t\t\t\t      struct inet_cork *cork);\nextern int\t\tip_send_skb(struct sk_buff *skb);\nextern int\t\tip_push_pending_frames(struct sock *sk);\nextern void\t\tip_flush_pending_frames(struct sock *sk);\nextern struct sk_buff  *ip_make_skb(struct sock *sk,\n\t\t\t\t    int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\t\t\tint odd, struct sk_buff *skb),\n\t\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t\t    struct ipcm_cookie *ipc,\n\t\t\t\t    struct rtable **rtp,\n\t\t\t\t    unsigned int flags);\n\nstatic inline struct sk_buff *ip_finish_skb(struct sock *sk)\n{\n\treturn __ip_make_skb(sk, &sk->sk_write_queue, &inet_sk(sk)->cork);\n}\n\n/* datagram.c */\nextern int\t\tip4_datagram_connect(struct sock *sk, \n\t\t\t\t\t     struct sockaddr *uaddr, int addr_len);\n\n/*\n *\tMap a multicast IP onto multicast MAC for type Token Ring.\n *      This conforms to RFC1469 Option 2 Multicasting i.e.\n *      using a functional address to transmit / receive \n *      multicast packets.\n */\n\nstatic inline void ip_tr_mc_map(__be32 addr, char *buf)\n{\n\tbuf[0]=0xC0;\n\tbuf[1]=0x00;\n\tbuf[2]=0x00;\n\tbuf[3]=0x04;\n\tbuf[4]=0x00;\n\tbuf[5]=0x00;\n}\n\nstruct ip_reply_arg {\n\tstruct kvec iov[1];   \n\tint\t    flags;\n\t__wsum \t    csum;\n\tint\t    csumoffset; /* u16 offset of csum in iov[0].iov_base */\n\t\t\t\t/* -1 if not needed */ \n\tint\t    bound_dev_if;\n}; \n\n#define IP_REPLY_ARG_NOSRCCHECK 1\n\nstatic inline __u8 ip_reply_arg_flowi_flags(const struct ip_reply_arg *arg)\n{\n\treturn (arg->flags & IP_REPLY_ARG_NOSRCCHECK) ? FLOWI_FLAG_ANYSRC : 0;\n}\n\nvoid ip_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg,\n\t\t   unsigned int len); \n\nstruct ipv4_config {\n\tint\tlog_martians;\n\tint\tno_pmtu_disc;\n};\n\nextern struct ipv4_config ipv4_config;\n#define IP_INC_STATS(net, field)\tSNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define IP_INC_STATS_BH(net, field)\tSNMP_INC_STATS64_BH((net)->mib.ip_statistics, field)\n#define IP_ADD_STATS(net, field, val)\tSNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_ADD_STATS_BH(net, field, val) SNMP_ADD_STATS64_BH((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS(net, field, val) SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS_BH(net, field, val) SNMP_UPD_PO_STATS64_BH((net)->mib.ip_statistics, field, val)\n#define NET_INC_STATS(net, field)\tSNMP_INC_STATS((net)->mib.net_statistics, field)\n#define NET_INC_STATS_BH(net, field)\tSNMP_INC_STATS_BH((net)->mib.net_statistics, field)\n#define NET_INC_STATS_USER(net, field) \tSNMP_INC_STATS_USER((net)->mib.net_statistics, field)\n#define NET_ADD_STATS_BH(net, field, adnd) SNMP_ADD_STATS_BH((net)->mib.net_statistics, field, adnd)\n#define NET_ADD_STATS_USER(net, field, adnd) SNMP_ADD_STATS_USER((net)->mib.net_statistics, field, adnd)\n\nextern unsigned long snmp_fold_field(void __percpu *mib[], int offt);\n#if BITS_PER_LONG==32\nextern u64 snmp_fold_field64(void __percpu *mib[], int offt, size_t sync_off);\n#else\nstatic inline u64 snmp_fold_field64(void __percpu *mib[], int offt, size_t syncp_off)\n{\n\treturn snmp_fold_field(mib, offt);\n}\n#endif\nextern int snmp_mib_init(void __percpu *ptr[2], size_t mibsize, size_t align);\nextern void snmp_mib_free(void __percpu *ptr[2]);\n\nextern struct local_ports {\n\tseqlock_t\tlock;\n\tint\t\trange[2];\n} sysctl_local_ports;\nextern void inet_get_local_port_range(int *low, int *high);\n\nextern unsigned long *sysctl_local_reserved_ports;\nstatic inline int inet_is_reserved_local_port(int port)\n{\n\treturn test_bit(port, sysctl_local_reserved_ports);\n}\n\nextern int sysctl_ip_nonlocal_bind;\n\nextern struct ctl_path net_core_path[];\nextern struct ctl_path net_ipv4_ctl_path[];\n\n/* From inetpeer.c */\nextern int inet_peer_threshold;\nextern int inet_peer_minttl;\nextern int inet_peer_maxttl;\nextern int inet_peer_gc_mintime;\nextern int inet_peer_gc_maxtime;\n\n/* From ip_output.c */\nextern int sysctl_ip_dynaddr;\n\nextern void ipfrag_init(void);\n\nextern void ip_static_sysctl_init(void);\n\n#ifdef CONFIG_INET\n#include <net/dst.h>\n\n/* The function in 2.2 was invalid, producing wrong result for\n * check=0xFEFF. It was noticed by Arthur Skawina _year_ ago. --ANK(000625) */\nstatic inline\nint ip_decrease_ttl(struct iphdr *iph)\n{\n\tu32 check = (__force u32)iph->check;\n\tcheck += (__force u32)htons(0x0100);\n\tiph->check = (__force __sum16)(check + (check>=0xFFFF));\n\treturn --iph->ttl;\n}\n\nstatic inline\nint ip_dont_fragment(struct sock *sk, struct dst_entry *dst)\n{\n\treturn  inet_sk(sk)->pmtudisc == IP_PMTUDISC_DO ||\n\t\t(inet_sk(sk)->pmtudisc == IP_PMTUDISC_WANT &&\n\t\t !(dst_metric_locked(dst, RTAX_MTU)));\n}\n\nextern void __ip_select_ident(struct iphdr *iph, struct dst_entry *dst, int more);\n\nstatic inline void ip_select_ident(struct iphdr *iph, struct dst_entry *dst, struct sock *sk)\n{\n\tif (iph->frag_off & htons(IP_DF)) {\n\t\t/* This is only to work around buggy Windows95/2000\n\t\t * VJ compression implementations.  If the ID field\n\t\t * does not change, they drop every other packet in\n\t\t * a TCP stream using header compression.\n\t\t */\n\t\tiph->id = (sk && inet_sk(sk)->inet_daddr) ?\n\t\t\t\t\thtons(inet_sk(sk)->inet_id++) : 0;\n\t} else\n\t\t__ip_select_ident(iph, dst, 0);\n}\n\nstatic inline void ip_select_ident_more(struct iphdr *iph, struct dst_entry *dst, struct sock *sk, int more)\n{\n\tif (iph->frag_off & htons(IP_DF)) {\n\t\tif (sk && inet_sk(sk)->inet_daddr) {\n\t\t\tiph->id = htons(inet_sk(sk)->inet_id);\n\t\t\tinet_sk(sk)->inet_id += 1 + more;\n\t\t} else\n\t\t\tiph->id = 0;\n\t} else\n\t\t__ip_select_ident(iph, dst, more);\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type ethernet.\n */\n\nstatic inline void ip_eth_mc_map(__be32 naddr, char *buf)\n{\n\t__u32 addr=ntohl(naddr);\n\tbuf[0]=0x01;\n\tbuf[1]=0x00;\n\tbuf[2]=0x5e;\n\tbuf[5]=addr&0xFF;\n\taddr>>=8;\n\tbuf[4]=addr&0xFF;\n\taddr>>=8;\n\tbuf[3]=addr&0x7F;\n}\n\n/*\n *\tMap a multicast IP onto multicast MAC for type IP-over-InfiniBand.\n *\tLeave P_Key as 0 to be filled in by driver.\n */\n\nstatic inline void ip_ib_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\t__u32 addr;\n\tunsigned char scope = broadcast[5] & 0xF;\n\n\tbuf[0]  = 0;\t\t/* Reserved */\n\tbuf[1]  = 0xff;\t\t/* Multicast QPN */\n\tbuf[2]  = 0xff;\n\tbuf[3]  = 0xff;\n\taddr    = ntohl(naddr);\n\tbuf[4]  = 0xff;\n\tbuf[5]  = 0x10 | scope;\t/* scope from broadcast address */\n\tbuf[6]  = 0x40;\t\t/* IPv4 signature */\n\tbuf[7]  = 0x1b;\n\tbuf[8]  = broadcast[8];\t\t/* P_Key */\n\tbuf[9]  = broadcast[9];\n\tbuf[10] = 0;\n\tbuf[11] = 0;\n\tbuf[12] = 0;\n\tbuf[13] = 0;\n\tbuf[14] = 0;\n\tbuf[15] = 0;\n\tbuf[19] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[18] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[17] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[16] = addr & 0x0f;\n}\n\nstatic inline void ip_ipgre_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\tif ((broadcast[0] | broadcast[1] | broadcast[2] | broadcast[3]) != 0)\n\t\tmemcpy(buf, broadcast, 4);\n\telse\n\t\tmemcpy(buf, &naddr, sizeof(naddr));\n}\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#include <linux/ipv6.h>\n#endif\n\nstatic __inline__ void inet_reset_saddr(struct sock *sk)\n{\n\tinet_sk(sk)->inet_rcv_saddr = inet_sk(sk)->inet_saddr = 0;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tif (sk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tmemset(&np->saddr, 0, sizeof(np->saddr));\n\t\tmemset(&np->rcv_saddr, 0, sizeof(np->rcv_saddr));\n\t}\n#endif\n}\n\n#endif\n\nstatic inline int sk_mc_loop(struct sock *sk)\n{\n\tif (!sk)\n\t\treturn 1;\n\tswitch (sk->sk_family) {\n\tcase AF_INET:\n\t\treturn inet_sk(sk)->mc_loop;\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\tcase AF_INET6:\n\t\treturn inet6_sk(sk)->mc_loop;\n#endif\n\t}\n\tWARN_ON(1);\n\treturn 1;\n}\n\nextern int\tip_call_ra_chain(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_fragment.c\n */\n\nenum ip_defrag_users {\n\tIP_DEFRAG_LOCAL_DELIVER,\n\tIP_DEFRAG_CALL_RA_CHAIN,\n\tIP_DEFRAG_CONNTRACK_IN,\n\t__IP_DEFRAG_CONNTRACK_IN_END\t= IP_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_OUT,\n\t__IP_DEFRAG_CONNTRACK_OUT_END\t= IP_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP_DEFRAG_CONNTRACK_BRIDGE_IN = IP_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n\tIP_DEFRAG_VS_IN,\n\tIP_DEFRAG_VS_OUT,\n\tIP_DEFRAG_VS_FWD\n};\n\nint ip_defrag(struct sk_buff *skb, u32 user);\nint ip_frag_mem(struct net *net);\nint ip_frag_nqueues(struct net *net);\n\n/*\n *\tFunctions provided by ip_forward.c\n */\n \nextern int ip_forward(struct sk_buff *skb);\n \n/*\n *\tFunctions provided by ip_options.c\n */\n \nextern void ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t\t     __be32 daddr, struct rtable *rt, int is_frag);\nextern int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb);\nextern void ip_options_fragment(struct sk_buff *skb);\nextern int ip_options_compile(struct net *net,\n\t\t\t      struct ip_options *opt, struct sk_buff *skb);\nextern int ip_options_get(struct net *net, struct ip_options_rcu **optp,\n\t\t\t  unsigned char *data, int optlen);\nextern int ip_options_get_from_user(struct net *net, struct ip_options_rcu **optp,\n\t\t\t\t    unsigned char __user *data, int optlen);\nextern void ip_options_undo(struct ip_options * opt);\nextern void ip_forward_options(struct sk_buff *skb);\nextern int ip_options_rcv_srr(struct sk_buff *skb);\n\n/*\n *\tFunctions provided by ip_sockglue.c\n */\n\nextern int\tip_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);\nextern void\tip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb);\nextern int\tip_cmsg_send(struct net *net,\n\t\t\t     struct msghdr *msg, struct ipcm_cookie *ipc);\nextern int\tip_setsockopt(struct sock *sk, int level, int optname, char __user *optval, unsigned int optlen);\nextern int\tip_getsockopt(struct sock *sk, int level, int optname, char __user *optval, int __user *optlen);\nextern int\tcompat_ip_setsockopt(struct sock *sk, int level,\n\t\t\tint optname, char __user *optval, unsigned int optlen);\nextern int\tcompat_ip_getsockopt(struct sock *sk, int level,\n\t\t\tint optname, char __user *optval, int __user *optlen);\nextern int\tip_ra_control(struct sock *sk, unsigned char on, void (*destructor)(struct sock *));\n\nextern int \tip_recv_error(struct sock *sk, struct msghdr *msg, int len);\nextern void\tip_icmp_error(struct sock *sk, struct sk_buff *skb, int err, \n\t\t\t      __be16 port, u32 info, u8 *payload);\nextern void\tip_local_error(struct sock *sk, int err, __be32 daddr, __be16 dport,\n\t\t\t       u32 info);\n\n#ifdef CONFIG_PROC_FS\nextern int ip_misc_proc_init(void);\n#endif\n\n#endif\t/* _IP_H */\n", "/*\n *  net/dccp/ipv4.c\n *\n *  An implementation of the DCCP protocol\n *  Arnaldo Carvalho de Melo <acme@conectiva.com.br>\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/dccp.h>\n#include <linux/icmp.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/random.h>\n\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_sock.h>\n#include <net/protocol.h>\n#include <net/sock.h>\n#include <net/timewait_sock.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n\n#include \"ackvec.h\"\n#include \"ccid.h\"\n#include \"dccp.h\"\n#include \"feat.h\"\n\n/*\n * The per-net dccp.v4_ctl_sk socket is used for responding to\n * the Out-of-the-blue (OOTB) packets. A control sock will be created\n * for this socket at the initialization time.\n */\n\nint dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tconst struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\tstruct ip_options_rcu *inet_opt;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt != NULL && inet_opt->opt.srr) {\n\t\tif (daddr == 0)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_DCCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (inet_opt == NULL || !inet_opt->opt.srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (inet->inet_saddr == 0)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\t/*\n\t * Socket identity is still unknown (sport may be zero).\n\t * However we set state to DCCP_REQUESTING and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet_hash_connect(&dccp_death_row, sk);\n\tif (err != 0)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk_setup_caps(sk, &rt->dst);\n\n\tdp->dccps_iss = secure_dccp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t    inet->inet_daddr,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport);\n\tinet->inet_id = dp->dccps_iss ^ jiffies;\n\n\terr = dccp_connect(sk);\n\trt = NULL;\n\tif (err != 0)\n\t\tgoto failure;\nout:\n\treturn err;\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port, if necessary.\n\t */\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\tgoto out;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_connect);\n\n/*\n * This routine does path mtu discovery as defined in RFC1191.\n */\nstatic inline void dccp_do_pmtu_discovery(struct sock *sk,\n\t\t\t\t\t  const struct iphdr *iph,\n\t\t\t\t\t  u32 mtu)\n{\n\tstruct dst_entry *dst;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct dccp_sock *dp = dccp_sk(sk);\n\n\t/* We are not interested in DCCP_LISTEN and request_socks (RESPONSEs\n\t * send out by Linux are always < 576bytes so they should go through\n\t * unfragmented).\n\t */\n\tif (sk->sk_state == DCCP_LISTEN)\n\t\treturn;\n\n\t/* We don't check in the destentry if pmtu discovery is forbidden\n\t * on this route. We just assume that no packet_to_big packets\n\t * are send back when pmtu discovery is not active.\n\t * There is a small race when the user changes this flag in the\n\t * route, but I think that's acceptable.\n\t */\n\tif ((dst = __sk_dst_check(sk, 0)) == NULL)\n\t\treturn;\n\n\tdst->ops->update_pmtu(dst, mtu);\n\n\t/* Something is about to be wrong... Remember soft error\n\t * for the case, if this connection will not able to recover.\n\t */\n\tif (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))\n\t\tsk->sk_err_soft = EMSGSIZE;\n\n\tmtu = dst_mtu(dst);\n\n\tif (inet->pmtudisc != IP_PMTUDISC_DONT &&\n\t    inet_csk(sk)->icsk_pmtu_cookie > mtu) {\n\t\tdccp_sync_mss(sk, mtu);\n\n\t\t/*\n\t\t * From RFC 4340, sec. 14.1:\n\t\t *\n\t\t *\tDCCP-Sync packets are the best choice for upward\n\t\t *\tprobing, since DCCP-Sync probes do not risk application\n\t\t *\tdata loss.\n\t\t */\n\t\tdccp_send_sync(sk, dp->dccps_gsr, DCCP_PKT_SYNC);\n\t} /* else let the usual retransmit timer handle it */\n}\n\n/*\n * This routine is called by the ICMP module when it gets some sort of error\n * condition. If err < 0 then the socket should be closed and the error\n * returned to the user. If err > 0 it's just the icmp type << 8 | icmp code.\n * After adjustment header points to the first 8 bytes of the tcp header. We\n * need to find the appropriate port.\n *\n * The locking strategy used here is very \"optimistic\". When someone else\n * accesses the socket the ICMP is just dropped and for some paths there is no\n * check at all. A more general error queue to queue errors for later handling\n * is probably better.\n */\nstatic void dccp_v4_err(struct sk_buff *skb, u32 info)\n{\n\tconst struct iphdr *iph = (struct iphdr *)skb->data;\n\tconst u8 offset = iph->ihl << 2;\n\tconst struct dccp_hdr *dh = (struct dccp_hdr *)(skb->data + offset);\n\tstruct dccp_sock *dp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\t__u64 seq;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->len < offset + sizeof(*dh) ||\n\t    skb->len < offset + __dccp_basic_hdr_len(dh)) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = inet_lookup(net, &dccp_hashinfo,\n\t\t\tiph->daddr, dh->dccph_dport,\n\t\t\tiph->saddr, dh->dccph_sport, inet_iif(skb));\n\tif (sk == NULL) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t */\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tseq = dccp_hdr_seq(dh);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tdccp_do_pmtu_discovery(sk, iph, info);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req , **prev;\n\tcase DCCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\treq = inet_csk_search_req(sk, &prev, dh->dccph_dport,\n\t\t\t\t\t  iph->daddr, iph->saddr);\n\t\tif (!req)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * ICMPs are not backlogged, hence we cannot get an established\n\t\t * socket here.\n\t\t */\n\t\tWARN_ON(req->sk);\n\n\t\tif (seq != dccp_rsk(req)->dreq_iss) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Still in RESPOND, just remove it silently.\n\t\t * There is no good way to pass the error to the newly\n\t\t * created socket, and POSIX does not want network\n\t\t * errors returned from accept().\n\t\t */\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\nstatic inline __sum16 dccp_v4_csum_finish(struct sk_buff *skb,\n\t\t\t\t      __be32 src, __be32 dst)\n{\n\treturn csum_tcpudp_magic(src, dst, skb->len, IPPROTO_DCCP, skb->csum);\n}\n\nvoid dccp_v4_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tdccp_csum_outgoing(skb);\n\tdh->dccph_checksum = dccp_v4_csum_finish(skb,\n\t\t\t\t\t\t inet->inet_saddr,\n\t\t\t\t\t\t inet->inet_daddr);\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_send_check);\n\nstatic inline u64 dccp_v4_init_sequence(const struct sk_buff *skb)\n{\n\treturn secure_dccp_sequence_number(ip_hdr(skb)->daddr,\n\t\t\t\t\t   ip_hdr(skb)->saddr,\n\t\t\t\t\t   dccp_hdr(skb)->dccph_dport,\n\t\t\t\t\t   dccp_hdr(skb)->dccph_sport);\n}\n\n/*\n * The three way handshake has completed - we got a valid ACK or DATAACK -\n * now create the new socket.\n *\n * This is the equivalent of TCP's tcp_v4_syn_recv_sock\n */\nstruct sock *dccp_v4_request_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t       struct request_sock *req,\n\t\t\t\t       struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct sock *newsk;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (dst == NULL && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto exit_nonewsk;\n\n\tsk_setup_caps(newsk, dst);\n\n\tnewinet\t\t   = inet_sk(newsk);\n\tireq\t\t   = inet_rsk(req);\n\tnewinet->inet_daddr\t= ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t= ireq->loc_addr;\n\tnewinet->inet_opt\t= ireq->opt;\n\tireq->opt\t   = NULL;\n\tnewinet->mc_index  = inet_iif(skb);\n\tnewinet->mc_ttl\t   = ip_hdr(skb)->ttl;\n\tnewinet->inet_id   = jiffies;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_request_recv_sock);\n\nstatic struct sock *dccp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh = dccp_hdr(skb);\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct sock *nsk;\n\tstruct request_sock **prev;\n\t/* Find possible connection requests. */\n\tstruct request_sock *req = inet_csk_search_req(sk, &prev,\n\t\t\t\t\t\t       dh->dccph_sport,\n\t\t\t\t\t\t       iph->saddr, iph->daddr);\n\tif (req != NULL)\n\t\treturn dccp_check_req(sk, skb, req, prev);\n\n\tnsk = inet_lookup_established(sock_net(sk), &dccp_hashinfo,\n\t\t\t\t      iph->saddr, dh->dccph_sport,\n\t\t\t\t      iph->daddr, dh->dccph_dport,\n\t\t\t\t      inet_iif(skb));\n\tif (nsk != NULL) {\n\t\tif (nsk->sk_state != DCCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic struct dst_entry* dccp_v4_route_skb(struct net *net, struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct rtable *rt;\n\tstruct flowi4 fl4 = {\n\t\t.flowi4_oif = skb_rtable(skb)->rt_iif,\n\t\t.daddr = ip_hdr(skb)->saddr,\n\t\t.saddr = ip_hdr(skb)->daddr,\n\t\t.flowi4_tos = RT_CONN_FLAGS(sk),\n\t\t.flowi4_proto = sk->sk_protocol,\n\t\t.fl4_sport = dccp_hdr(skb)->dccph_dport,\n\t\t.fl4_dport = dccp_hdr(skb)->dccph_sport,\n\t};\n\n\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt)) {\n\t\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\t\treturn NULL;\n\t}\n\n\treturn &rt->dst;\n}\n\nstatic int dccp_v4_send_response(struct sock *sk, struct request_sock *req,\n\t\t\t\t struct request_values *rv_unused)\n{\n\tint err = -1;\n\tstruct sk_buff *skb;\n\tstruct dst_entry *dst;\n\n\tdst = inet_csk_route_req(sk, req);\n\tif (dst == NULL)\n\t\tgoto out;\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v4_csum_finish(skb, ireq->loc_addr,\n\t\t\t\t\t\t\t      ireq->rmt_addr);\n\t\terr = ip_build_and_send_pkt(skb, sk, ireq->loc_addr,\n\t\t\t\t\t    ireq->rmt_addr,\n\t\t\t\t\t    ireq->opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\nout:\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic void dccp_v4_ctl_send_reset(struct sock *sk, struct sk_buff *rxskb)\n{\n\tint err;\n\tconst struct iphdr *rxiph;\n\tstruct sk_buff *skb;\n\tstruct dst_entry *dst;\n\tstruct net *net = dev_net(skb_dst(rxskb)->dev);\n\tstruct sock *ctl_sk = net->dccp.v4_ctl_sk;\n\n\t/* Never send a reset in response to a reset. */\n\tif (dccp_hdr(rxskb)->dccph_type == DCCP_PKT_RESET)\n\t\treturn;\n\n\tif (skb_rtable(rxskb)->rt_type != RTN_LOCAL)\n\t\treturn;\n\n\tdst = dccp_v4_route_skb(net, ctl_sk, rxskb);\n\tif (dst == NULL)\n\t\treturn;\n\n\tskb = dccp_ctl_make_reset(ctl_sk, rxskb);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\trxiph = ip_hdr(rxskb);\n\tdccp_hdr(skb)->dccph_checksum = dccp_v4_csum_finish(skb, rxiph->saddr,\n\t\t\t\t\t\t\t\t rxiph->daddr);\n\tskb_dst_set(skb, dst_clone(dst));\n\n\tbh_lock_sock(ctl_sk);\n\terr = ip_build_and_send_pkt(skb, ctl_sk,\n\t\t\t\t    rxiph->daddr, rxiph->saddr, NULL);\n\tbh_unlock_sock(ctl_sk);\n\n\tif (net_xmit_eval(err) == 0) {\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTSEGS);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTRSTS);\n\t}\nout:\n\t dst_release(dst);\n}\n\nstatic void dccp_v4_reqsk_destructor(struct request_sock *req)\n{\n\tdccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);\n\tkfree(inet_rsk(req)->opt);\n}\n\nstatic struct request_sock_ops dccp_request_sock_ops __read_mostly = {\n\t.family\t\t= PF_INET,\n\t.obj_size\t= sizeof(struct dccp_request_sock),\n\t.rtx_syn_ack\t= dccp_v4_send_response,\n\t.send_ack\t= dccp_reqsk_send_ack,\n\t.destructor\t= dccp_v4_reqsk_destructor,\n\t.send_reset\t= dccp_v4_ctl_send_reset,\n};\n\nint dccp_v4_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct request_sock *req;\n\tstruct dccp_request_sock *dreq;\n\tconst __be32 service = dccp_hdr_request(skb)->dccph_req_service;\n\tstruct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);\n\n\t/* Never answer to DCCP_PKT_REQUESTs send to broadcast or multicast */\n\tif (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\treturn 0;\t/* discard, don't send a reset here */\n\n\tif (dccp_bad_service_code(sk, service)) {\n\t\tdcb->dccpd_reset_code = DCCP_RESET_CODE_BAD_SERVICE_CODE;\n\t\tgoto drop;\n\t}\n\t/*\n\t * TW buckets are converted to open requests without\n\t * limitations, they conserve resources and peer is\n\t * evidently real one.\n\t */\n\tdcb->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto drop;\n\n\t/*\n\t * Accept backlog is full. If we have already queued enough\n\t * of warm entries in syn queue, drop request. It is better than\n\t * clogging syn queue with openreqs with exponentially increasing\n\t * timeout.\n\t */\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet_reqsk_alloc(&dccp_request_sock_ops);\n\tif (req == NULL)\n\t\tgoto drop;\n\n\tif (dccp_reqsk_init(req, dccp_sk(sk), skb))\n\t\tgoto drop_and_free;\n\n\tdreq = dccp_rsk(req);\n\tif (dccp_parse_options(sk, dreq, skb))\n\t\tgoto drop_and_free;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tireq = inet_rsk(req);\n\tireq->loc_addr = ip_hdr(skb)->daddr;\n\tireq->rmt_addr = ip_hdr(skb)->saddr;\n\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t * Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookie\n\t *\n\t * In fact we defer setting S.GSR, S.SWL, S.SWH to\n\t * dccp_create_openreq_child.\n\t */\n\tdreq->dreq_isr\t   = dcb->dccpd_seq;\n\tdreq->dreq_iss\t   = dccp_v4_init_sequence(skb);\n\tdreq->dreq_service = service;\n\n\tif (dccp_v4_send_response(sk, req, NULL))\n\t\tgoto drop_and_free;\n\n\tinet_csk_reqsk_queue_hash_add(sk, req, DCCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\treturn -1;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_conn_request);\n\nint dccp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tif (sk->sk_state == DCCP_OPEN) { /* Fast path */\n\t\tif (dccp_rcv_established(sk, skb, dh, skb->len))\n\t\t\tgoto reset;\n\t\treturn 0;\n\t}\n\n\t/*\n\t *  Step 3: Process LISTEN state\n\t *\t If P.type == Request or P contains a valid Init Cookie option,\n\t *\t      (* Must scan the packet's options to check for Init\n\t *\t\t Cookies.  Only Init Cookies are processed here,\n\t *\t\t however; other options are processed in Step 8.  This\n\t *\t\t scan need only be performed if the endpoint uses Init\n\t *\t\t Cookies *)\n\t *\t      (* Generate a new socket and switch to that socket *)\n\t *\t      Set S := new socket for this port pair\n\t *\t      S.state = RESPOND\n\t *\t      Choose S.ISS (initial seqno) or set from Init Cookies\n\t *\t      Initialize S.GAR := S.ISS\n\t *\t      Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookies\n\t *\t      Continue with S.state == RESPOND\n\t *\t      (* A Response packet will be generated in Step 11 *)\n\t *\t Otherwise,\n\t *\t      Generate Reset(No Connection) unless P.type == Reset\n\t *\t      Drop packet and return\n\t *\n\t * NOTE: the check for the packet types is done in\n\t *\t dccp_rcv_state_process\n\t */\n\tif (sk->sk_state == DCCP_LISTEN) {\n\t\tstruct sock *nsk = dccp_v4_hnd_req(sk, skb);\n\n\t\tif (nsk == NULL)\n\t\t\tgoto discard;\n\n\t\tif (nsk != sk) {\n\t\t\tif (dccp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (dccp_rcv_state_process(sk, skb, dh, skb->len))\n\t\tgoto reset;\n\treturn 0;\n\nreset:\n\tdccp_v4_ctl_send_reset(sk, skb);\ndiscard:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nEXPORT_SYMBOL_GPL(dccp_v4_do_rcv);\n\n/**\n *\tdccp_invalid_packet  -  check for malformed packets\n *\tImplements RFC 4340, 8.5:  Step 1: Check header basics\n *\tPackets that fail these checks are ignored and do not receive Resets.\n */\nint dccp_invalid_packet(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tunsigned int cscov;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn 1;\n\n\t/* If the packet is shorter than 12 bytes, drop packet and return */\n\tif (!pskb_may_pull(skb, sizeof(struct dccp_hdr))) {\n\t\tDCCP_WARN(\"pskb_may_pull failed\\n\");\n\t\treturn 1;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\t/* If P.type is not understood, drop packet and return */\n\tif (dh->dccph_type >= DCCP_PKT_INVALID) {\n\t\tDCCP_WARN(\"invalid packet type\\n\");\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If P.Data Offset is too small for packet type, drop packet and return\n\t */\n\tif (dh->dccph_doff < dccp_hdr_len(skb) / sizeof(u32)) {\n\t\tDCCP_WARN(\"P.Data Offset(%u) too small\\n\", dh->dccph_doff);\n\t\treturn 1;\n\t}\n\t/*\n\t * If P.Data Offset is too too large for packet, drop packet and return\n\t */\n\tif (!pskb_may_pull(skb, dh->dccph_doff * sizeof(u32))) {\n\t\tDCCP_WARN(\"P.Data Offset(%u) too large\\n\", dh->dccph_doff);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If P.type is not Data, Ack, or DataAck and P.X == 0 (the packet\n\t * has short sequence numbers), drop packet and return\n\t */\n\tif ((dh->dccph_type < DCCP_PKT_DATA    ||\n\t    dh->dccph_type > DCCP_PKT_DATAACK) && dh->dccph_x == 0)  {\n\t\tDCCP_WARN(\"P.type (%s) not Data || [Data]Ack, while P.X == 0\\n\",\n\t\t\t  dccp_packet_name(dh->dccph_type));\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If P.CsCov is too large for the packet size, drop packet and return.\n\t * This must come _before_ checksumming (not as RFC 4340 suggests).\n\t */\n\tcscov = dccp_csum_coverage(skb);\n\tif (cscov > skb->len) {\n\t\tDCCP_WARN(\"P.CsCov %u exceeds packet length %d\\n\",\n\t\t\t  dh->dccph_cscov, skb->len);\n\t\treturn 1;\n\t}\n\n\t/* If header checksum is incorrect, drop packet and return.\n\t * (This step is completed in the AF-dependent functions.) */\n\tskb->csum = skb_checksum(skb, 0, cscov, 0);\n\n\treturn 0;\n}\n\nEXPORT_SYMBOL_GPL(dccp_invalid_packet);\n\n/* this is called when real data arrives */\nstatic int dccp_v4_rcv(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tconst struct iphdr *iph;\n\tstruct sock *sk;\n\tint min_cov;\n\n\t/* Step 1: Check header basics */\n\n\tif (dccp_invalid_packet(skb))\n\t\tgoto discard_it;\n\n\tiph = ip_hdr(skb);\n\t/* Step 1: If header checksum is incorrect, drop packet and return */\n\tif (dccp_v4_csum_finish(skb, iph->saddr, iph->daddr)) {\n\t\tDCCP_WARN(\"dropped packet with invalid checksum\\n\");\n\t\tgoto discard_it;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\tDCCP_SKB_CB(skb)->dccpd_seq  = dccp_hdr_seq(dh);\n\tDCCP_SKB_CB(skb)->dccpd_type = dh->dccph_type;\n\n\tdccp_pr_debug(\"%8.8s src=%pI4@%-5d dst=%pI4@%-5d seq=%llu\",\n\t\t      dccp_packet_name(dh->dccph_type),\n\t\t      &iph->saddr, ntohs(dh->dccph_sport),\n\t\t      &iph->daddr, ntohs(dh->dccph_dport),\n\t\t      (unsigned long long) DCCP_SKB_CB(skb)->dccpd_seq);\n\n\tif (dccp_packet_without_ack(skb)) {\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = DCCP_PKT_WITHOUT_ACK_SEQ;\n\t\tdccp_pr_debug_cat(\"\\n\");\n\t} else {\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = dccp_hdr_ack_seq(skb);\n\t\tdccp_pr_debug_cat(\", ack=%llu\\n\", (unsigned long long)\n\t\t\t\t  DCCP_SKB_CB(skb)->dccpd_ack_seq);\n\t}\n\n\t/* Step 2:\n\t *\tLook up flow ID in table and get corresponding socket */\n\tsk = __inet_lookup_skb(&dccp_hashinfo, skb,\n\t\t\t       dh->dccph_sport, dh->dccph_dport);\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t */\n\tif (sk == NULL) {\n\t\tdccp_pr_debug(\"failed to look up flow ID in table and \"\n\t\t\t      \"get corresponding socket\\n\");\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * Step 2:\n\t *\t... or S.state == TIMEWAIT,\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tdccp_pr_debug(\"sk->sk_state == DCCP_TIME_WAIT: do_time_wait\\n\");\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage\n\t *\to if MinCsCov = 0, only packets with CsCov = 0 are accepted\n\t *\to if MinCsCov > 0, also accept packets with CsCov >= MinCsCov\n\t */\n\tmin_cov = dccp_sk(sk)->dccps_pcrlen;\n\tif (dh->dccph_cscov && (min_cov == 0 || dh->dccph_cscov < min_cov))  {\n\t\tdccp_pr_debug(\"Packet CsCov %d does not satisfy MinCsCov %d\\n\",\n\t\t\t      dh->dccph_cscov, min_cov);\n\t\t/* FIXME: \"Such packets SHOULD be reported using Data Dropped\n\t\t *         options (Section 11.7) with Drop Code 0, Protocol\n\t\t *         Constraints.\"                                     */\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\tnf_reset(skb);\n\n\treturn sk_receive_skb(sk, skb, 1);\n\nno_dccp_socket:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (dh->dccph_type != DCCP_PKT_RESET) {\n\t\tDCCP_SKB_CB(skb)->dccpd_reset_code =\n\t\t\t\t\tDCCP_RESET_CODE_NO_CONNECTION;\n\t\tdccp_v4_ctl_send_reset(sk, skb);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n}\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv4_af_ops = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = dccp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = dccp_v4_conn_request,\n\t.syn_recv_sock\t   = dccp_v4_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\n\t.bind_conflict\t   = inet_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ip_setsockopt,\n\t.compat_getsockopt = compat_ip_getsockopt,\n#endif\n};\n\nstatic int dccp_v4_init_sock(struct sock *sk)\n{\n\tstatic __u8 dccp_v4_ctl_sock_initialized;\n\tint err = dccp_init_sock(sk, dccp_v4_ctl_sock_initialized);\n\n\tif (err == 0) {\n\t\tif (unlikely(!dccp_v4_ctl_sock_initialized))\n\t\t\tdccp_v4_ctl_sock_initialized = 1;\n\t\tinet_csk(sk)->icsk_af_ops = &dccp_ipv4_af_ops;\n\t}\n\n\treturn err;\n}\n\nstatic struct timewait_sock_ops dccp_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct inet_timewait_sock),\n};\n\nstatic struct proto dccp_v4_prot = {\n\t.name\t\t\t= \"DCCP\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= dccp_close,\n\t.connect\t\t= dccp_v4_connect,\n\t.disconnect\t\t= dccp_disconnect,\n\t.ioctl\t\t\t= dccp_ioctl,\n\t.init\t\t\t= dccp_v4_init_sock,\n\t.setsockopt\t\t= dccp_setsockopt,\n\t.getsockopt\t\t= dccp_getsockopt,\n\t.sendmsg\t\t= dccp_sendmsg,\n\t.recvmsg\t\t= dccp_recvmsg,\n\t.backlog_rcv\t\t= dccp_v4_do_rcv,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.accept\t\t\t= inet_csk_accept,\n\t.get_port\t\t= inet_csk_get_port,\n\t.shutdown\t\t= dccp_shutdown,\n\t.destroy\t\t= dccp_destroy_sock,\n\t.orphan_count\t\t= &dccp_orphan_count,\n\t.max_header\t\t= MAX_DCCP_HEADER,\n\t.obj_size\t\t= sizeof(struct dccp_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.rsk_prot\t\t= &dccp_request_sock_ops,\n\t.twsk_prot\t\t= &dccp_timewait_sock_ops,\n\t.h.hashinfo\t\t= &dccp_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_dccp_setsockopt,\n\t.compat_getsockopt\t= compat_dccp_getsockopt,\n#endif\n};\n\nstatic const struct net_protocol dccp_v4_protocol = {\n\t.handler\t= dccp_v4_rcv,\n\t.err_handler\t= dccp_v4_err,\n\t.no_policy\t= 1,\n\t.netns_ok\t= 1,\n};\n\nstatic const struct proto_ops inet_dccp_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet_getname,\n\t/* FIXME: work on tcp_poll to rename it to inet_csk_poll */\n\t.poll\t\t   = dccp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t/* FIXME: work on inet_listen to rename it to sock_common_listen */\n\t.listen\t\t   = inet_dccp_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw dccp_v4_protosw = {\n\t.type\t\t= SOCK_DCCP,\n\t.protocol\t= IPPROTO_DCCP,\n\t.prot\t\t= &dccp_v4_prot,\n\t.ops\t\t= &inet_dccp_ops,\n\t.no_check\t= 0,\n\t.flags\t\t= INET_PROTOSW_ICSK,\n};\n\nstatic int __net_init dccp_v4_init_net(struct net *net)\n{\n\tif (dccp_hashinfo.bhash == NULL)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\treturn inet_ctl_sock_create(&net->dccp.v4_ctl_sk, PF_INET,\n\t\t\t\t    SOCK_DCCP, IPPROTO_DCCP, net);\n}\n\nstatic void __net_exit dccp_v4_exit_net(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->dccp.v4_ctl_sk);\n}\n\nstatic struct pernet_operations dccp_v4_ops = {\n\t.init\t= dccp_v4_init_net,\n\t.exit\t= dccp_v4_exit_net,\n};\n\nstatic int __init dccp_v4_init(void)\n{\n\tint err = proto_register(&dccp_v4_prot, 1);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet_add_protocol(&dccp_v4_protocol, IPPROTO_DCCP);\n\tif (err != 0)\n\t\tgoto out_proto_unregister;\n\n\tinet_register_protosw(&dccp_v4_protosw);\n\n\terr = register_pernet_subsys(&dccp_v4_ops);\n\tif (err)\n\t\tgoto out_destroy_ctl_sock;\nout:\n\treturn err;\nout_destroy_ctl_sock:\n\tinet_unregister_protosw(&dccp_v4_protosw);\n\tinet_del_protocol(&dccp_v4_protocol, IPPROTO_DCCP);\nout_proto_unregister:\n\tproto_unregister(&dccp_v4_prot);\n\tgoto out;\n}\n\nstatic void __exit dccp_v4_exit(void)\n{\n\tunregister_pernet_subsys(&dccp_v4_ops);\n\tinet_unregister_protosw(&dccp_v4_protosw);\n\tinet_del_protocol(&dccp_v4_protocol, IPPROTO_DCCP);\n\tproto_unregister(&dccp_v4_prot);\n}\n\nmodule_init(dccp_v4_init);\nmodule_exit(dccp_v4_exit);\n\n/*\n * __stringify doesn't likes enums, so use SOCK_DCCP (6) and IPPROTO_DCCP (33)\n * values directly, Also cover the case where the protocol is not specified,\n * i.e. net-pf-PF_INET-proto-0-type-SOCK_DCCP\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET, 33, 6);\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET, 0, 6);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Arnaldo Carvalho de Melo <acme@mandriva.com>\");\nMODULE_DESCRIPTION(\"DCCP - Datagram Congestion Controlled Protocol\");\n", "/*\n *\tDCCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tBased on net/dccp6/ipv6.c\n *\n *\tArnaldo Carvalho de Melo <acme@ghostprotocols.net>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/xfrm.h>\n\n#include <net/addrconf.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_sock.h>\n#include <net/inet6_connection_sock.h>\n#include <net/inet6_hashtables.h>\n#include <net/ip6_route.h>\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include \"dccp.h\"\n#include \"ipv6.h\"\n#include \"feat.h\"\n\n/* The per-net dccp.v6_ctl_sk is used for sending RSTs and ACKs */\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops;\n\nstatic void dccp_v6_hash(struct sock *sk)\n{\n\tif (sk->sk_state != DCCP_CLOSED) {\n\t\tif (inet_csk(sk)->icsk_af_ops == &dccp_ipv6_mapped) {\n\t\t\tinet_hash(sk);\n\t\t\treturn;\n\t\t}\n\t\tlocal_bh_disable();\n\t\t__inet6_hash(sk, NULL);\n\t\tlocal_bh_enable();\n\t}\n}\n\n/* add pseudo-header to DCCP checksum stored in skb->csum */\nstatic inline __sum16 dccp_v6_csum_finish(struct sk_buff *skb,\n\t\t\t\t      const struct in6_addr *saddr,\n\t\t\t\t      const struct in6_addr *daddr)\n{\n\treturn csum_ipv6_magic(saddr, daddr, skb->len, IPPROTO_DCCP, skb->csum);\n}\n\nstatic inline void dccp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tdccp_csum_outgoing(skb);\n\tdh->dccph_checksum = dccp_v6_csum_finish(skb, &np->saddr, &np->daddr);\n}\n\nstatic inline __u32 secure_dccpv6_sequence_number(__be32 *saddr, __be32 *daddr,\n\t\t\t\t\t\t  __be16 sport, __be16 dport   )\n{\n\treturn secure_tcpv6_sequence_number(saddr, daddr, sport, dport);\n}\n\nstatic inline __u32 dccp_v6_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_dccpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t     ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_dport,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_sport     );\n\n}\n\nstatic void dccp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct dccp_hdr *dh = (struct dccp_hdr *)(skb->data + offset);\n\tstruct dccp_sock *dp;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\t__u64 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->len < offset + sizeof(*dh) ||\n\t    skb->len < offset + __dccp_basic_hdr_len(dh)) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = inet6_lookup(net, &dccp_hashinfo,\n\t\t\t&hdr->daddr, dh->dccph_dport,\n\t\t\t&hdr->saddr, dh->dccph_sport, inet6_iif(skb));\n\n\tif (sk == NULL) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tseq = dccp_hdr_seq(dh);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst = NULL;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (DCCPF_LISTEN | DCCPF_CLOSED))\n\t\t\tgoto out;\n\n\t\t/* icmp should have updated the destination cache entry */\n\t\tdst = __sk_dst_check(sk, np->dst_cookie);\n\t\tif (dst == NULL) {\n\t\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\t\tstruct flowi6 fl6;\n\n\t\t\t/* BUGGG_FUTURE: Again, it is not clear how\n\t\t\t   to handle rthdr case. Ignore this complexity\n\t\t\t   for now.\n\t\t\t */\n\t\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\t\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\t\t\tipv6_addr_copy(&fl6.saddr, &np->saddr);\n\t\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\t\tfl6.fl6_dport = inet->inet_dport;\n\t\t\tfl6.fl6_sport = inet->inet_sport;\n\t\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\t\tdst = ip6_dst_lookup_flow(sk, &fl6, NULL, false);\n\t\t\tif (IS_ERR(dst)) {\n\t\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else\n\t\t\tdst_hold(dst);\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {\n\t\t\tdccp_sync_mss(sk, dst_mtu(dst));\n\t\t} /* else let the usual retransmit timer handle it */\n\t\tdst_release(dst);\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req, **prev;\n\tcase DCCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\n\t\treq = inet6_csk_search_req(sk, &prev, dh->dccph_dport,\n\t\t\t\t\t   &hdr->daddr, &hdr->saddr,\n\t\t\t\t\t   inet6_iif(skb));\n\t\tif (req == NULL)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * ICMPs are not backlogged, hence we cannot get an established\n\t\t * socket here.\n\t\t */\n\t\tWARN_ON(req->sk != NULL);\n\n\t\tif (seq != dccp_rsk(req)->dreq_iss) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\t\t\t/*\n\t\t\t * Wake people up to see the error\n\t\t\t * (see connect in sock.c)\n\t\t\t */\n\t\t\tsk->sk_error_report(sk);\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\n\nstatic int dccp_v6_send_response(struct sock *sk, struct request_sock *req,\n\t\t\t\t struct request_values *rv_unused)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq6->iif;\n\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\topt = np->opt;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq6->loc_addr,\n\t\t\t\t\t\t\t &ireq6->rmt_addr);\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\terr = ip6_xmit(sk, skb, &fl6, opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic void dccp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tdccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);\n\tif (inet6_rsk(req)->pktopts != NULL)\n\t\tkfree_skb(inet6_rsk(req)->pktopts);\n}\n\nstatic void dccp_v6_ctl_send_reset(struct sock *sk, struct sk_buff *rxskb)\n{\n\tconst struct ipv6hdr *rxip6h;\n\tstruct sk_buff *skb;\n\tstruct flowi6 fl6;\n\tstruct net *net = dev_net(skb_dst(rxskb)->dev);\n\tstruct sock *ctl_sk = net->dccp.v6_ctl_sk;\n\tstruct dst_entry *dst;\n\n\tif (dccp_hdr(rxskb)->dccph_type == DCCP_PKT_RESET)\n\t\treturn;\n\n\tif (!ipv6_unicast_destination(rxskb))\n\t\treturn;\n\n\tskb = dccp_ctl_make_reset(ctl_sk, rxskb);\n\tif (skb == NULL)\n\t\treturn;\n\n\trxip6h = ipv6_hdr(rxskb);\n\tdccp_hdr(skb)->dccph_checksum = dccp_v6_csum_finish(skb, &rxip6h->saddr,\n\t\t\t\t\t\t\t    &rxip6h->daddr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tipv6_addr_copy(&fl6.daddr, &rxip6h->saddr);\n\tipv6_addr_copy(&fl6.saddr, &rxip6h->daddr);\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.flowi6_oif = inet6_iif(rxskb);\n\tfl6.fl6_dport = dccp_hdr(skb)->dccph_dport;\n\tfl6.fl6_sport = dccp_hdr(skb)->dccph_sport;\n\tsecurity_skb_classify_flow(rxskb, flowi6_to_flowi(&fl6));\n\n\t/* sk = NULL, but it is safe for now. RST socket required. */\n\tdst = ip6_dst_lookup_flow(ctl_sk, &fl6, NULL, false);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(skb, dst);\n\t\tip6_xmit(ctl_sk, skb, &fl6, NULL);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTSEGS);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(skb);\n}\n\nstatic struct request_sock_ops dccp6_request_sock_ops = {\n\t.family\t\t= AF_INET6,\n\t.obj_size\t= sizeof(struct dccp6_request_sock),\n\t.rtx_syn_ack\t= dccp_v6_send_response,\n\t.send_ack\t= dccp_reqsk_send_ack,\n\t.destructor\t= dccp_v6_reqsk_destructor,\n\t.send_reset\t= dccp_v6_ctl_send_reset,\n};\n\nstatic struct sock *dccp_v6_hnd_req(struct sock *sk,struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh = dccp_hdr(skb);\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\tstruct sock *nsk;\n\tstruct request_sock **prev;\n\t/* Find possible connection requests. */\n\tstruct request_sock *req = inet6_csk_search_req(sk, &prev,\n\t\t\t\t\t\t\tdh->dccph_sport,\n\t\t\t\t\t\t\t&iph->saddr,\n\t\t\t\t\t\t\t&iph->daddr,\n\t\t\t\t\t\t\tinet6_iif(skb));\n\tif (req != NULL)\n\t\treturn dccp_check_req(sk, skb, req, prev);\n\n\tnsk = __inet6_lookup_established(sock_net(sk), &dccp_hashinfo,\n\t\t\t\t\t &iph->saddr, dh->dccph_sport,\n\t\t\t\t\t &iph->daddr, ntohs(dh->dccph_dport),\n\t\t\t\t\t inet6_iif(skb));\n\tif (nsk != NULL) {\n\t\tif (nsk->sk_state != DCCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic int dccp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct request_sock *req;\n\tstruct dccp_request_sock *dreq;\n\tstruct inet6_request_sock *ireq6;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tconst __be32 service = dccp_hdr_request(skb)->dccph_req_service;\n\tstruct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\treturn 0;\t/* discard, don't send a reset here */\n\n\tif (dccp_bad_service_code(sk, service)) {\n\t\tdcb->dccpd_reset_code = DCCP_RESET_CODE_BAD_SERVICE_CODE;\n\t\tgoto drop;\n\t}\n\t/*\n\t * There are no SYN attacks on IPv6, yet...\n\t */\n\tdcb->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto drop;\n\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet6_reqsk_alloc(&dccp6_request_sock_ops);\n\tif (req == NULL)\n\t\tgoto drop;\n\n\tif (dccp_reqsk_init(req, dccp_sk(sk), skb))\n\t\tgoto drop_and_free;\n\n\tdreq = dccp_rsk(req);\n\tif (dccp_parse_options(sk, dreq, skb))\n\t\tgoto drop_and_free;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tireq6 = inet6_rsk(req);\n\tipv6_addr_copy(&ireq6->rmt_addr, &ipv6_hdr(skb)->saddr);\n\tipv6_addr_copy(&ireq6->loc_addr, &ipv6_hdr(skb)->daddr);\n\n\tif (ipv6_opt_accepted(sk, skb) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq6->pktopts = skb;\n\t}\n\tireq6->iif = sk->sk_bound_dev_if;\n\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq6->rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq6->iif = inet6_iif(skb);\n\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t *   Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookie\n\t *\n\t *   In fact we defer setting S.GSR, S.SWL, S.SWH to\n\t *   dccp_create_openreq_child.\n\t */\n\tdreq->dreq_isr\t   = dcb->dccpd_seq;\n\tdreq->dreq_iss\t   = dccp_v6_init_sequence(skb);\n\tdreq->dreq_service = service;\n\n\tif (dccp_v6_send_response(sk, req, NULL))\n\t\tgoto drop_and_free;\n\n\tinet6_csk_reqsk_queue_hash_add(sk, req, DCCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\treturn -1;\n}\n\nstatic struct sock *dccp_v6_request_recv_sock(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (dst == NULL) {\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\t\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\t\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &ireq6->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &ireq6->loc_addr);\n\tnewsk->sk_bound_dev_if = ireq6->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (ireq6->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(ireq6->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(ireq6->pktopts);\n\t\tireq6->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (opt != NULL) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\treturn NULL;\n}\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int dccp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, dccp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   dccp_v6_hnd_req and dccp_v6_ctl_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_do_rcv(sk, skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t * socket locking is here for SMP purposes as backlog rcv is currently\n\t * called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all)\n\t/*\n\t * FIXME: Add handling of IPV6_PKTOPTIONS skb. See the comments below\n\t *        (wrt ipv6_pktopions) and net/ipv6/tcp_ipv6.c for an example.\n\t */\n\t\topt_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (sk->sk_state == DCCP_OPEN) { /* Fast path */\n\t\tif (dccp_rcv_established(sk, skb, dccp_hdr(skb), skb->len))\n\t\t\tgoto reset;\n\t\tif (opt_skb) {\n\t\t\t/* XXX This is where we would goto ipv6_pktoptions. */\n\t\t\t__kfree_skb(opt_skb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/*\n\t *  Step 3: Process LISTEN state\n\t *     If S.state == LISTEN,\n\t *\t If P.type == Request or P contains a valid Init Cookie option,\n\t *\t      (* Must scan the packet's options to check for Init\n\t *\t\t Cookies.  Only Init Cookies are processed here,\n\t *\t\t however; other options are processed in Step 8.  This\n\t *\t\t scan need only be performed if the endpoint uses Init\n\t *\t\t Cookies *)\n\t *\t      (* Generate a new socket and switch to that socket *)\n\t *\t      Set S := new socket for this port pair\n\t *\t      S.state = RESPOND\n\t *\t      Choose S.ISS (initial seqno) or set from Init Cookies\n\t *\t      Initialize S.GAR := S.ISS\n\t *\t      Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookies\n\t *\t      Continue with S.state == RESPOND\n\t *\t      (* A Response packet will be generated in Step 11 *)\n\t *\t Otherwise,\n\t *\t      Generate Reset(No Connection) unless P.type == Reset\n\t *\t      Drop packet and return\n\t *\n\t * NOTE: the check for the packet types is done in\n\t *\t dccp_rcv_state_process\n\t */\n\tif (sk->sk_state == DCCP_LISTEN) {\n\t\tstruct sock *nsk = dccp_v6_hnd_req(sk, skb);\n\n\t\tif (nsk == NULL)\n\t\t\tgoto discard;\n\t\t/*\n\t\t * Queue it on the new socket if the new socket is active,\n\t\t * otherwise we just shortcircuit this and continue with\n\t\t * the new socket..\n\t\t */\n\t\tif (nsk != sk) {\n\t\t\tif (dccp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\tif (opt_skb != NULL)\n\t\t\t\t__kfree_skb(opt_skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (dccp_rcv_state_process(sk, skb, dccp_hdr(skb), skb->len))\n\t\tgoto reset;\n\tif (opt_skb) {\n\t\t/* XXX This is where we would goto ipv6_pktoptions. */\n\t\t__kfree_skb(opt_skb);\n\t}\n\treturn 0;\n\nreset:\n\tdccp_v6_ctl_send_reset(sk, skb);\ndiscard:\n\tif (opt_skb != NULL)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int dccp_v6_rcv(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tstruct sock *sk;\n\tint min_cov;\n\n\t/* Step 1: Check header basics */\n\n\tif (dccp_invalid_packet(skb))\n\t\tgoto discard_it;\n\n\t/* Step 1: If header checksum is incorrect, drop packet and return. */\n\tif (dccp_v6_csum_finish(skb, &ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr)) {\n\t\tDCCP_WARN(\"dropped packet with invalid checksum\\n\");\n\t\tgoto discard_it;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\tDCCP_SKB_CB(skb)->dccpd_seq  = dccp_hdr_seq(dh);\n\tDCCP_SKB_CB(skb)->dccpd_type = dh->dccph_type;\n\n\tif (dccp_packet_without_ack(skb))\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = DCCP_PKT_WITHOUT_ACK_SEQ;\n\telse\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = dccp_hdr_ack_seq(skb);\n\n\t/* Step 2:\n\t *\tLook up flow ID in table and get corresponding socket */\n\tsk = __inet6_lookup_skb(&dccp_hashinfo, skb,\n\t\t\t        dh->dccph_sport, dh->dccph_dport);\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t */\n\tif (sk == NULL) {\n\t\tdccp_pr_debug(\"failed to look up flow ID in table and \"\n\t\t\t      \"get corresponding socket\\n\");\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * Step 2:\n\t *\t... or S.state == TIMEWAIT,\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tdccp_pr_debug(\"sk->sk_state == DCCP_TIME_WAIT: do_time_wait\\n\");\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage\n\t *\to if MinCsCov = 0, only packets with CsCov = 0 are accepted\n\t *\to if MinCsCov > 0, also accept packets with CsCov >= MinCsCov\n\t */\n\tmin_cov = dccp_sk(sk)->dccps_pcrlen;\n\tif (dh->dccph_cscov  &&  (min_cov == 0 || dh->dccph_cscov < min_cov))  {\n\t\tdccp_pr_debug(\"Packet CsCov %d does not satisfy MinCsCov %d\\n\",\n\t\t\t      dh->dccph_cscov, min_cov);\n\t\t/* FIXME: send Data Dropped option (see also dccp_v4_rcv) */\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\treturn sk_receive_skb(sk, skb, 1) ? -1 : 0;\n\nno_dccp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (dh->dccph_type != DCCP_PKT_RESET) {\n\t\tDCCP_SKB_CB(skb)->dccpd_reset_code =\n\t\t\t\t\tDCCP_RESET_CODE_NO_CONNECTION;\n\t\tdccp_v6_ctl_send_reset(sk, skb);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n}\n\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tipv6_addr_copy(&usin->sin6_addr, &flowlabel->dst);\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tipv6_addr_copy(&np->daddr, &usin->sin6_addr);\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr, &np->rcv_saddr);\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&np->rcv_saddr))\n\t\tsaddr = &np->rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\tipv6_addr_copy(&fl6.saddr, saddr ? saddr : &np->saddr);\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, true);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tipv6_addr_copy(&np->rcv_saddr, saddr);\n\t}\n\n\t/* set the source address */\n\tipv6_addr_copy(&np->saddr, saddr);\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      np->daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = dccp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n/*\n *\tDCCP over IPv4 via INET6 API\n */\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = dccp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int dccp_v6_init_sock(struct sock *sk)\n{\n\tstatic __u8 dccp_v6_ctl_sock_initialized;\n\tint err = dccp_init_sock(sk, dccp_v6_ctl_sock_initialized);\n\n\tif (err == 0) {\n\t\tif (unlikely(!dccp_v6_ctl_sock_initialized))\n\t\t\tdccp_v6_ctl_sock_initialized = 1;\n\t\tinet_csk(sk)->icsk_af_ops = &dccp_ipv6_af_ops;\n\t}\n\n\treturn err;\n}\n\nstatic void dccp_v6_destroy_sock(struct sock *sk)\n{\n\tdccp_destroy_sock(sk);\n\tinet6_destroy_sock(sk);\n}\n\nstatic struct timewait_sock_ops dccp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct dccp6_timewait_sock),\n};\n\nstatic struct proto dccp_v6_prot = {\n\t.name\t\t   = \"DCCPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = dccp_close,\n\t.connect\t   = dccp_v6_connect,\n\t.disconnect\t   = dccp_disconnect,\n\t.ioctl\t\t   = dccp_ioctl,\n\t.init\t\t   = dccp_v6_init_sock,\n\t.setsockopt\t   = dccp_setsockopt,\n\t.getsockopt\t   = dccp_getsockopt,\n\t.sendmsg\t   = dccp_sendmsg,\n\t.recvmsg\t   = dccp_recvmsg,\n\t.backlog_rcv\t   = dccp_v6_do_rcv,\n\t.hash\t\t   = dccp_v6_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.accept\t\t   = inet_csk_accept,\n\t.get_port\t   = inet_csk_get_port,\n\t.shutdown\t   = dccp_shutdown,\n\t.destroy\t   = dccp_v6_destroy_sock,\n\t.orphan_count\t   = &dccp_orphan_count,\n\t.max_header\t   = MAX_DCCP_HEADER,\n\t.obj_size\t   = sizeof(struct dccp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.rsk_prot\t   = &dccp6_request_sock_ops,\n\t.twsk_prot\t   = &dccp6_timewait_sock_ops,\n\t.h.hashinfo\t   = &dccp_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_dccp_setsockopt,\n\t.compat_getsockopt = compat_dccp_getsockopt,\n#endif\n};\n\nstatic const struct inet6_protocol dccp_v6_protocol = {\n\t.handler\t= dccp_v6_rcv,\n\t.err_handler\t= dccp_v6_err,\n\t.flags\t\t= INET6_PROTO_NOPOLICY | INET6_PROTO_FINAL,\n};\n\nstatic const struct proto_ops inet6_dccp_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = dccp_poll,\n\t.ioctl\t\t   = inet6_ioctl,\n\t.listen\t\t   = inet_dccp_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw dccp_v6_protosw = {\n\t.type\t\t= SOCK_DCCP,\n\t.protocol\t= IPPROTO_DCCP,\n\t.prot\t\t= &dccp_v6_prot,\n\t.ops\t\t= &inet6_dccp_ops,\n\t.flags\t\t= INET_PROTOSW_ICSK,\n};\n\nstatic int __net_init dccp_v6_init_net(struct net *net)\n{\n\tif (dccp_hashinfo.bhash == NULL)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\treturn inet_ctl_sock_create(&net->dccp.v6_ctl_sk, PF_INET6,\n\t\t\t\t    SOCK_DCCP, IPPROTO_DCCP, net);\n}\n\nstatic void __net_exit dccp_v6_exit_net(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->dccp.v6_ctl_sk);\n}\n\nstatic struct pernet_operations dccp_v6_ops = {\n\t.init   = dccp_v6_init_net,\n\t.exit   = dccp_v6_exit_net,\n};\n\nstatic int __init dccp_v6_init(void)\n{\n\tint err = proto_register(&dccp_v6_prot, 1);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet6_add_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tif (err != 0)\n\t\tgoto out_unregister_proto;\n\n\tinet6_register_protosw(&dccp_v6_protosw);\n\n\terr = register_pernet_subsys(&dccp_v6_ops);\n\tif (err != 0)\n\t\tgoto out_destroy_ctl_sock;\nout:\n\treturn err;\n\nout_destroy_ctl_sock:\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\nout_unregister_proto:\n\tproto_unregister(&dccp_v6_prot);\n\tgoto out;\n}\n\nstatic void __exit dccp_v6_exit(void)\n{\n\tunregister_pernet_subsys(&dccp_v6_ops);\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\n\tproto_unregister(&dccp_v6_prot);\n}\n\nmodule_init(dccp_v6_init);\nmodule_exit(dccp_v6_exit);\n\n/*\n * __stringify doesn't likes enums, so use SOCK_DCCP (6) and IPPROTO_DCCP (33)\n * values directly, Also cover the case where the protocol is not specified,\n * i.e. net-pf-PF_INET6-proto-0-type-SOCK_DCCP\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 33, 6);\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 0, 6);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Arnaldo Carvalho de Melo <acme@mandriva.com>\");\nMODULE_DESCRIPTION(\"DCCPv6 - Datagram Congestion Controlled Protocol\");\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPF_INET protocol family socket handler.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Changes (see also sock.c)\n *\n *\t\tpiggy,\n *\t\tKarl Knutson\t:\tSocket protocol table\n *\t\tA.N.Kuznetsov\t:\tSocket death error in accept().\n *\t\tJohn Richardson :\tFix non blocking error in connect()\n *\t\t\t\t\tso sockets that fail to connect\n *\t\t\t\t\tdon't return -EINPROGRESS.\n *\t\tAlan Cox\t:\tAsynchronous I/O support\n *\t\tAlan Cox\t:\tKeep correct socket pointer on sock\n *\t\t\t\t\tstructures\n *\t\t\t\t\twhen accept() ed\n *\t\tAlan Cox\t:\tSemantics of SO_LINGER aren't state\n *\t\t\t\t\tmoved to close when you look carefully.\n *\t\t\t\t\tWith this fixed and the accept bug fixed\n *\t\t\t\t\tsome RPC stuff seems happier.\n *\t\tNiibe Yutaka\t:\t4.4BSD style write async I/O\n *\t\tAlan Cox,\n *\t\tTony Gale \t:\tFixed reuse semantics.\n *\t\tAlan Cox\t:\tbind() shouldn't abort existing but dead\n *\t\t\t\t\tsockets. Stops FTP netin:.. I hope.\n *\t\tAlan Cox\t:\tbind() works correctly for RAW sockets.\n *\t\t\t\t\tNote that FreeBSD at least was broken\n *\t\t\t\t\tin this respect so be careful with\n *\t\t\t\t\tcompatibility tests...\n *\t\tAlan Cox\t:\trouting cache support\n *\t\tAlan Cox\t:\tmemzero the socket structure for\n *\t\t\t\t\tcompactness.\n *\t\tMatt Day\t:\tnonblock connect error handler\n *\t\tAlan Cox\t:\tAllow large numbers of pending sockets\n *\t\t\t\t\t(eg for big web sites), but only if\n *\t\t\t\t\tspecifically application requested.\n *\t\tAlan Cox\t:\tNew buffering throughout IP. Used\n *\t\t\t\t\tdumbly.\n *\t\tAlan Cox\t:\tNew buffering now used smartly.\n *\t\tAlan Cox\t:\tBSD rather than common sense\n *\t\t\t\t\tinterpretation of listen.\n *\t\tGermano Caronni\t:\tAssorted small races.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg basic support.\n *\t\tAlan Cox\t:\tOnly sendmsg/recvmsg now supported.\n *\t\tAlan Cox\t:\tLocked down bind (see security list).\n *\t\tAlan Cox\t:\tLoosened bind a little.\n *\t\tMike McLagan\t:\tADD/DEL DLCI Ioctls\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tSome other random speedups.\n *\t\tCyrus Durgin\t:\tCleaned up file for kmod hacks.\n *\t\tAndi Kleen\t:\tFix inet_stream_connect TCP race.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n\n#include <asm/uaccess.h>\n#include <asm/system.h>\n\n#include <linux/inet.h>\n#include <linux/igmp.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <net/checksum.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/arp.h>\n#include <net/route.h>\n#include <net/ip_fib.h>\n#include <net/inet_connection_sock.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/raw.h>\n#include <net/icmp.h>\n#include <net/ipip.h>\n#include <net/inet_common.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#ifdef CONFIG_IP_MROUTE\n#include <linux/mroute.h>\n#endif\n\n\n/* The inetsw table contains everything that inet_create needs to\n * build a new socket.\n */\nstatic struct list_head inetsw[SOCK_MAX];\nstatic DEFINE_SPINLOCK(inetsw_lock);\n\nstruct ipv4_config ipv4_config;\nEXPORT_SYMBOL(ipv4_config);\n\n/* New destruction routine */\n\nvoid inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(rcu_dereference_protected(inet->inet_opt, 1));\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tsk_refcnt_debug_dec(sk);\n}\nEXPORT_SYMBOL(inet_sock_destruct);\n\n/*\n *\tThe routines beyond this point handle the behaviour of an AF_INET\n *\tsocket object. Mostly it punts to the subprotocols of IP to do\n *\tthe work.\n */\n\n/*\n *\tAutomatically bind an unbound socket.\n */\n\nstatic int inet_autobind(struct sock *sk)\n{\n\tstruct inet_sock *inet;\n\t/* We may need to bind the socket. */\n\tlock_sock(sk);\n\tinet = inet_sk(sk);\n\tif (!inet->inet_num) {\n\t\tif (sk->sk_prot->get_port(sk, 0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t}\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/*\n *\tMove a socket into listening state.\n */\nint inet_listen(struct socket *sock, int backlog)\n{\n\tstruct sock *sk = sock->sk;\n\tunsigned char old_state;\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sock->state != SS_UNCONNECTED || sock->type != SOCK_STREAM)\n\t\tgoto out;\n\n\told_state = sk->sk_state;\n\tif (!((1 << old_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\tgoto out;\n\n\t/* Really, if the socket is already in listen state\n\t * we can only allow the backlog to be adjusted.\n\t */\n\tif (old_state != TCP_LISTEN) {\n\t\terr = inet_csk_listen_start(sk, backlog);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\tsk->sk_max_ack_backlog = backlog;\n\terr = 0;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_listen);\n\nu32 inet_ehash_secret __read_mostly;\nEXPORT_SYMBOL(inet_ehash_secret);\n\n/*\n * inet_ehash_secret must be set exactly once\n */\nvoid build_ehash_secret(void)\n{\n\tu32 rnd;\n\n\tdo {\n\t\tget_random_bytes(&rnd, sizeof(rnd));\n\t} while (rnd == 0);\n\n\tcmpxchg(&inet_ehash_secret, 0, rnd);\n}\nEXPORT_SYMBOL(build_ehash_secret);\n\nstatic inline int inet_netns_ok(struct net *net, int protocol)\n{\n\tint hash;\n\tconst struct net_protocol *ipprot;\n\n\tif (net_eq(net, &init_net))\n\t\treturn 1;\n\n\thash = protocol & (MAX_INET_PROTOS - 1);\n\tipprot = rcu_dereference(inet_protos[hash]);\n\n\tif (ipprot == NULL)\n\t\t/* raw IP is OK */\n\t\treturn 1;\n\treturn ipprot->netns_ok;\n}\n\n/*\n *\tCreate an inet socket.\n */\n\nstatic int inet_create(struct net *net, struct socket *sock, int protocol,\n\t\t       int kern)\n{\n\tstruct sock *sk;\n\tstruct inet_protosw *answer;\n\tstruct inet_sock *inet;\n\tstruct proto *answer_prot;\n\tunsigned char answer_flags;\n\tchar answer_no_check;\n\tint try_loading_module = 0;\n\tint err;\n\n\tif (unlikely(!inet_ehash_secret))\n\t\tif (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)\n\t\t\tbuild_ehash_secret();\n\n\tsock->state = SS_UNCONNECTED;\n\n\t/* Look for the requested type/protocol pair. */\nlookup_protocol:\n\terr = -ESOCKTNOSUPPORT;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(answer, &inetsw[sock->type], list) {\n\n\t\terr = 0;\n\t\t/* Check the non-wild match. */\n\t\tif (protocol == answer->protocol) {\n\t\t\tif (protocol != IPPROTO_IP)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* Check for the two wild cases. */\n\t\t\tif (IPPROTO_IP == protocol) {\n\t\t\t\tprotocol = answer->protocol;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IPPROTO_IP == answer->protocol)\n\t\t\t\tbreak;\n\t\t}\n\t\terr = -EPROTONOSUPPORT;\n\t}\n\n\tif (unlikely(err)) {\n\t\tif (try_loading_module < 2) {\n\t\t\trcu_read_unlock();\n\t\t\t/*\n\t\t\t * Be more specific, e.g. net-pf-2-proto-132-type-1\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP-type-SOCK_STREAM)\n\t\t\t */\n\t\t\tif (++try_loading_module == 1)\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d-type-%d\",\n\t\t\t\t\t       PF_INET, protocol, sock->type);\n\t\t\t/*\n\t\t\t * Fall back to generic, e.g. net-pf-2-proto-132\n\t\t\t * (net-pf-PF_INET-proto-IPPROTO_SCTP)\n\t\t\t */\n\t\t\telse\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d\",\n\t\t\t\t\t       PF_INET, protocol);\n\t\t\tgoto lookup_protocol;\n\t\t} else\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\terr = -EPERM;\n\tif (sock->type == SOCK_RAW && !kern && !capable(CAP_NET_RAW))\n\t\tgoto out_rcu_unlock;\n\n\terr = -EAFNOSUPPORT;\n\tif (!inet_netns_ok(net, protocol))\n\t\tgoto out_rcu_unlock;\n\n\tsock->ops = answer->ops;\n\tanswer_prot = answer->prot;\n\tanswer_no_check = answer->no_check;\n\tanswer_flags = answer->flags;\n\trcu_read_unlock();\n\n\tWARN_ON(answer_prot->slab == NULL);\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_INET, GFP_KERNEL, answer_prot);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\terr = 0;\n\tsk->sk_no_check = answer_no_check;\n\tif (INET_PROTOSW_REUSE & answer_flags)\n\t\tsk->sk_reuse = 1;\n\n\tinet = inet_sk(sk);\n\tinet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;\n\n\tinet->nodefrag = 0;\n\n\tif (SOCK_RAW == sock->type) {\n\t\tinet->inet_num = protocol;\n\t\tif (IPPROTO_RAW == protocol)\n\t\t\tinet->hdrincl = 1;\n\t}\n\n\tif (ipv4_config.no_pmtu_disc)\n\t\tinet->pmtudisc = IP_PMTUDISC_DONT;\n\telse\n\t\tinet->pmtudisc = IP_PMTUDISC_WANT;\n\n\tinet->inet_id = 0;\n\n\tsock_init_data(sock, sk);\n\n\tsk->sk_destruct\t   = inet_sock_destruct;\n\tsk->sk_protocol\t   = protocol;\n\tsk->sk_backlog_rcv = sk->sk_prot->backlog_rcv;\n\n\tinet->uc_ttl\t= -1;\n\tinet->mc_loop\t= 1;\n\tinet->mc_ttl\t= 1;\n\tinet->mc_all\t= 1;\n\tinet->mc_index\t= 0;\n\tinet->mc_list\t= NULL;\n\n\tsk_refcnt_debug_inc(sk);\n\n\tif (inet->inet_num) {\n\t\t/* It assumes that any protocol which allows\n\t\t * the user to assign a number at socket\n\t\t * creation time automatically\n\t\t * shares.\n\t\t */\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t\t/* Add to protocol hash chains. */\n\t\tsk->sk_prot->hash(sk);\n\t}\n\n\tif (sk->sk_prot->init) {\n\t\terr = sk->sk_prot->init(sk);\n\t\tif (err)\n\t\t\tsk_common_release(sk);\n\t}\nout:\n\treturn err;\nout_rcu_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\n\n\n/*\n *\tThe peer socket should always be NULL (or else). When we call this\n *\tfunction we are destroying the object and from then on nobody\n *\tshould refer to it.\n */\nint inet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk) {\n\t\tlong timeout;\n\n\t\tsock_rps_reset_flow(sk);\n\n\t\t/* Applications forget to leave groups before exiting */\n\t\tip_mc_drop_socket(sk);\n\n\t\t/* If linger is set, we don't return until the close\n\t\t * is complete.  Otherwise we return immediately. The\n\t\t * actually closing is done the same either way.\n\t\t *\n\t\t * If the close is due to the process exiting, we never\n\t\t * linger..\n\t\t */\n\t\ttimeout = 0;\n\t\tif (sock_flag(sk, SOCK_LINGER) &&\n\t\t    !(current->flags & PF_EXITING))\n\t\t\ttimeout = sk->sk_lingertime;\n\t\tsock->sk = NULL;\n\t\tsk->sk_prot->close(sk, timeout);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_release);\n\n/* It is off by default, see below. */\nint sysctl_ip_nonlocal_bind __read_mostly;\nEXPORT_SYMBOL(sysctl_ip_nonlocal_bind);\n\nint inet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *addr = (struct sockaddr_in *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tunsigned short snum;\n\tint chk_addr_ret;\n\tint err;\n\n\t/* If the socket has its own bind function then use it. (RAW) */\n\tif (sk->sk_prot->bind) {\n\t\terr = sk->sk_prot->bind(sk, uaddr, addr_len);\n\t\tgoto out;\n\t}\n\terr = -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(sock_net(sk), addr->sin_addr.s_addr);\n\n\t/* Not specified by any standard per-se, however it breaks too\n\t * many applications when removed.  It is unfortunate since\n\t * allowing applications to make a non-local bind solves\n\t * several problems with systems using dynamic addressing.\n\t * (ie. your servers still start up even if your ISDN link\n\t *  is temporarily down)\n\t */\n\terr = -EADDRNOTAVAIL;\n\tif (!sysctl_ip_nonlocal_bind &&\n\t    !(inet->freebind || inet->transparent) &&\n\t    addr->sin_addr.s_addr != htonl(INADDR_ANY) &&\n\t    chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST &&\n\t    chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tsnum = ntohs(addr->sin_port);\n\terr = -EACCES;\n\tif (snum && snum < PROT_SOCK && !capable(CAP_NET_BIND_SERVICE))\n\t\tgoto out;\n\n\t/*      We keep a pair of addresses. rcv_saddr is the one\n\t *      used by hash lookups, and saddr is used for transmit.\n\t *\n\t *      In the BSD API these are the same except where it\n\t *      would be illegal to use them (multicast/broadcast) in\n\t *      which case the sending device address is used.\n\t */\n\tlock_sock(sk);\n\n\t/* Check these errors (active socket, double bind). */\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE || inet->inet_num)\n\t\tgoto out_release_sock;\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->sin_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\n\t/* Make sure we are allowed to bind here. */\n\tif (sk->sk_prot->get_port(sk, snum)) {\n\t\tinet->inet_saddr = inet->inet_rcv_saddr = 0;\n\t\terr = -EADDRINUSE;\n\t\tgoto out_release_sock;\n\t}\n\n\tif (inet->inet_rcv_saddr)\n\t\tsk->sk_userlocks |= SOCK_BINDADDR_LOCK;\n\tif (snum)\n\t\tsk->sk_userlocks |= SOCK_BINDPORT_LOCK;\n\tinet->inet_sport = htons(inet->inet_num);\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsk_dst_reset(sk);\n\terr = 0;\nout_release_sock:\n\trelease_sock(sk);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_bind);\n\nint inet_dgram_connect(struct socket *sock, struct sockaddr * uaddr,\n\t\t       int addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\tif (uaddr->sa_family == AF_UNSPEC)\n\t\treturn sk->sk_prot->disconnect(sk, flags);\n\n\tif (!inet_sk(sk)->inet_num && inet_autobind(sk))\n\t\treturn -EAGAIN;\n\treturn sk->sk_prot->connect(sk, (struct sockaddr *)uaddr, addr_len);\n}\nEXPORT_SYMBOL(inet_dgram_connect);\n\nstatic long inet_wait_for_connect(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t/* Basic assumption: if someone sets sk->sk_err, he _must_\n\t * change state of the socket from TCP_SYN_*.\n\t * Connect() does not allow to get error notifications\n\t * without closing the socket.\n\t */\n\twhile ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\trelease_sock(sk);\n\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock(sk);\n\t\tif (signal_pending(current) || !timeo)\n\t\t\tbreak;\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n/*\n *\tConnect to a remote host. There is regrettably still a little\n *\tTCP 'magic' in here.\n */\nint inet_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint addr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint err;\n\tlong timeo;\n\n\tif (addr_len < sizeof(uaddr->sa_family))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tif (uaddr->sa_family == AF_UNSPEC) {\n\t\terr = sk->sk_prot->disconnect(sk, flags);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tgoto out;\n\t}\n\n\tswitch (sock->state) {\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out;\n\tcase SS_CONNECTED:\n\t\terr = -EISCONN;\n\t\tgoto out;\n\tcase SS_CONNECTING:\n\t\terr = -EALREADY;\n\t\t/* Fall out of switch with err, set for this state */\n\t\tbreak;\n\tcase SS_UNCONNECTED:\n\t\terr = -EISCONN;\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\tgoto out;\n\n\t\terr = sk->sk_prot->connect(sk, uaddr, addr_len);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\tsock->state = SS_CONNECTING;\n\n\t\t/* Just entered SS_CONNECTING state; the only\n\t\t * difference is that return value in non-blocking\n\t\t * case is EINPROGRESS, rather than EALREADY.\n\t\t */\n\t\terr = -EINPROGRESS;\n\t\tbreak;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\t/* Error code is set above */\n\t\tif (!timeo || !inet_wait_for_connect(sk, timeo))\n\t\t\tgoto out;\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/* Connection was closed by RST, timeout, ICMP error\n\t * or another process disconnected us.\n\t */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto sock_error;\n\n\t/* sk->sk_err may be not zero now, if RECVERR was ordered by user\n\t * and error was received after socket entered established state.\n\t * Hence, it is handled normally after connect() return successfully.\n\t */\n\n\tsock->state = SS_CONNECTED;\n\terr = 0;\nout:\n\trelease_sock(sk);\n\treturn err;\n\nsock_error:\n\terr = sock_error(sk) ? : -ECONNABORTED;\n\tsock->state = SS_UNCONNECTED;\n\tif (sk->sk_prot->disconnect(sk, flags))\n\t\tsock->state = SS_DISCONNECTING;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_stream_connect);\n\n/*\n *\tAccept a pending connection. The TCP layer now gives BSD semantics.\n */\n\nint inet_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}\nEXPORT_SYMBOL(inet_accept);\n\n\n/*\n *\tThis does both peername and sockname.\n */\nint inet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\tint *uaddr_len, int peer)\n{\n\tstruct sock *sk\t\t= sock->sk;\n\tstruct inet_sock *inet\t= inet_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, sin, uaddr);\n\n\tsin->sin_family = AF_INET;\n\tif (peer) {\n\t\tif (!inet->inet_dport ||\n\t\t    (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&\n\t\t     peer == 1))\n\t\t\treturn -ENOTCONN;\n\t\tsin->sin_port = inet->inet_dport;\n\t\tsin->sin_addr.s_addr = inet->inet_daddr;\n\t} else {\n\t\t__be32 addr = inet->inet_rcv_saddr;\n\t\tif (!addr)\n\t\t\taddr = inet->inet_saddr;\n\t\tsin->sin_port = inet->inet_sport;\n\t\tsin->sin_addr.s_addr = addr;\n\t}\n\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t*uaddr_len = sizeof(*sin);\n\treturn 0;\n}\nEXPORT_SYMBOL(inet_getname);\n\nint inet_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\t size_t size)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\treturn sk->sk_prot->sendmsg(iocb, sk, msg, size);\n}\nEXPORT_SYMBOL(inet_sendmsg);\n\nssize_t inet_sendpage(struct socket *sock, struct page *page, int offset,\n\t\t      size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tsock_rps_record_flow(sk);\n\n\t/* We may need to bind the socket. */\n\tif (!inet_sk(sk)->inet_num && !sk->sk_prot->no_autobind &&\n\t    inet_autobind(sk))\n\t\treturn -EAGAIN;\n\n\tif (sk->sk_prot->sendpage)\n\t\treturn sk->sk_prot->sendpage(sk, page, offset, size, flags);\n\treturn sock_no_sendpage(sock, page, offset, size, flags);\n}\nEXPORT_SYMBOL(inet_sendpage);\n\nint inet_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,\n\t\t size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\tsock_rps_record_flow(sk);\n\n\terr = sk->sk_prot->recvmsg(iocb, sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(inet_recvmsg);\n\nint inet_shutdown(struct socket *sock, int how)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\n\t/* This should really check to make sure\n\t * the socket is a TCP socket. (WHY AC...)\n\t */\n\thow++; /* maps 0->1 has the advantage of making bit 1 rcvs and\n\t\t       1->2 bit 2 snds.\n\t\t       2->3 */\n\tif ((how & ~SHUTDOWN_MASK) || !how)\t/* MAXINT->0 */\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tif (sock->state == SS_CONNECTING) {\n\t\tif ((1 << sk->sk_state) &\n\t\t    (TCPF_SYN_SENT | TCPF_SYN_RECV | TCPF_CLOSE))\n\t\t\tsock->state = SS_DISCONNECTING;\n\t\telse\n\t\t\tsock->state = SS_CONNECTED;\n\t}\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\terr = -ENOTCONN;\n\t\t/* Hack to wake up other listeners, who can poll for\n\t\t   POLLHUP, even on eg. unconnected UDP sockets -- RR */\n\tdefault:\n\t\tsk->sk_shutdown |= how;\n\t\tif (sk->sk_prot->shutdown)\n\t\t\tsk->sk_prot->shutdown(sk, how);\n\t\tbreak;\n\n\t/* Remaining two branches are temporary solution for missing\n\t * close() in multithreaded environment. It is _not_ a good idea,\n\t * but we have no choice until close() is repaired at VFS level.\n\t */\n\tcase TCP_LISTEN:\n\t\tif (!(how & RCV_SHUTDOWN))\n\t\t\tbreak;\n\t\t/* Fall through */\n\tcase TCP_SYN_SENT:\n\t\terr = sk->sk_prot->disconnect(sk, O_NONBLOCK);\n\t\tsock->state = err ? SS_DISCONNECTING : SS_UNCONNECTED;\n\t\tbreak;\n\t}\n\n\t/* Wake up anyone sleeping in poll. */\n\tsk->sk_state_change(sk);\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(inet_shutdown);\n\n/*\n *\tioctl() calls you can issue on an INET socket. Most of these are\n *\tdevice configuration and stuff and very rarely used. Some ioctls\n *\tpass on to the socket itself.\n *\n *\tNOTE: I like the idea of a module for the config stuff. ie ifconfig\n *\tloads the devconfigure module does its configuring and unloads it.\n *\tThere's a good 20K of config code hanging around the kernel.\n */\n\nint inet_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = 0;\n\tstruct net *net = sock_net(sk);\n\n\tswitch (cmd) {\n\tcase SIOCGSTAMP:\n\t\terr = sock_get_timestamp(sk, (struct timeval __user *)arg);\n\t\tbreak;\n\tcase SIOCGSTAMPNS:\n\t\terr = sock_get_timestampns(sk, (struct timespec __user *)arg);\n\t\tbreak;\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCRTMSG:\n\t\terr = ip_rt_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\t\terr = arp_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFPFLAGS:\n\tcase SIOCGIFPFLAGS:\n\tcase SIOCSIFFLAGS:\n\t\terr = devinet_ioctl(net, cmd, (void __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\tif (sk->sk_prot->ioctl)\n\t\t\terr = sk->sk_prot->ioctl(sk, cmd, arg);\n\t\telse\n\t\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(inet_ioctl);\n\n#ifdef CONFIG_COMPAT\nint inet_compat_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tint err = -ENOIOCTLCMD;\n\n\tif (sk->sk_prot->compat_ioctl)\n\t\terr = sk->sk_prot->compat_ioctl(sk, cmd, arg);\n\n\treturn err;\n}\n#endif\n\nconst struct proto_ops inet_stream_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = tcp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = inet_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n\t.splice_read\t   = tcp_splice_read,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_stream_ops);\n\nconst struct proto_ops inet_dgram_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = udp_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\nEXPORT_SYMBOL(inet_dgram_ops);\n\n/*\n * For SOCK_RAW sockets; should be the same as inet_dgram_ops but without\n * udp_poll\n */\nstatic const struct proto_ops inet_sockraw_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = inet_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = inet_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n\t.compat_ioctl\t   = inet_compat_ioctl,\n#endif\n};\n\nstatic const struct net_proto_family inet_family_ops = {\n\t.family = PF_INET,\n\t.create = inet_create,\n\t.owner\t= THIS_MODULE,\n};\n\n/* Upon startup we insert all the elements in inetsw_array[] into\n * the linked list inetsw.\n */\nstatic struct inet_protosw inetsw_array[] =\n{\n\t{\n\t\t.type =       SOCK_STREAM,\n\t\t.protocol =   IPPROTO_TCP,\n\t\t.prot =       &tcp_prot,\n\t\t.ops =        &inet_stream_ops,\n\t\t.no_check =   0,\n\t\t.flags =      INET_PROTOSW_PERMANENT |\n\t\t\t      INET_PROTOSW_ICSK,\n\t},\n\n\t{\n\t\t.type =       SOCK_DGRAM,\n\t\t.protocol =   IPPROTO_UDP,\n\t\t.prot =       &udp_prot,\n\t\t.ops =        &inet_dgram_ops,\n\t\t.no_check =   UDP_CSUM_DEFAULT,\n\t\t.flags =      INET_PROTOSW_PERMANENT,\n       },\n\n\n       {\n\t       .type =       SOCK_RAW,\n\t       .protocol =   IPPROTO_IP,\t/* wild card */\n\t       .prot =       &raw_prot,\n\t       .ops =        &inet_sockraw_ops,\n\t       .no_check =   UDP_CSUM_DEFAULT,\n\t       .flags =      INET_PROTOSW_REUSE,\n       }\n};\n\n#define INETSW_ARRAY_LEN ARRAY_SIZE(inetsw_array)\n\nvoid inet_register_protosw(struct inet_protosw *p)\n{\n\tstruct list_head *lh;\n\tstruct inet_protosw *answer;\n\tint protocol = p->protocol;\n\tstruct list_head *last_perm;\n\n\tspin_lock_bh(&inetsw_lock);\n\n\tif (p->type >= SOCK_MAX)\n\t\tgoto out_illegal;\n\n\t/* If we are trying to override a permanent protocol, bail. */\n\tanswer = NULL;\n\tlast_perm = &inetsw[p->type];\n\tlist_for_each(lh, &inetsw[p->type]) {\n\t\tanswer = list_entry(lh, struct inet_protosw, list);\n\n\t\t/* Check only the non-wild match. */\n\t\tif (INET_PROTOSW_PERMANENT & answer->flags) {\n\t\t\tif (protocol == answer->protocol)\n\t\t\t\tbreak;\n\t\t\tlast_perm = lh;\n\t\t}\n\n\t\tanswer = NULL;\n\t}\n\tif (answer)\n\t\tgoto out_permanent;\n\n\t/* Add the new entry after the last permanent entry if any, so that\n\t * the new entry does not override a permanent entry when matched with\n\t * a wild-card protocol. But it is allowed to override any existing\n\t * non-permanent entry.  This means that when we remove this entry, the\n\t * system automatically returns to the old behavior.\n\t */\n\tlist_add_rcu(&p->list, last_perm);\nout:\n\tspin_unlock_bh(&inetsw_lock);\n\n\treturn;\n\nout_permanent:\n\tprintk(KERN_ERR \"Attempt to override permanent protocol %d.\\n\",\n\t       protocol);\n\tgoto out;\n\nout_illegal:\n\tprintk(KERN_ERR\n\t       \"Ignoring attempt to register invalid socket type %d.\\n\",\n\t       p->type);\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_register_protosw);\n\nvoid inet_unregister_protosw(struct inet_protosw *p)\n{\n\tif (INET_PROTOSW_PERMANENT & p->flags) {\n\t\tprintk(KERN_ERR\n\t\t       \"Attempt to unregister permanent protocol %d.\\n\",\n\t\t       p->protocol);\n\t} else {\n\t\tspin_lock_bh(&inetsw_lock);\n\t\tlist_del_rcu(&p->list);\n\t\tspin_unlock_bh(&inetsw_lock);\n\n\t\tsynchronize_net();\n\t}\n}\nEXPORT_SYMBOL(inet_unregister_protosw);\n\n/*\n *      Shall we try to damage output packets if routing dev changes?\n */\n\nint sysctl_ip_dynaddr __read_mostly;\n\nstatic int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\tstruct ip_options_rcu *inet_opt;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\n\t/* Query new route. */\n\trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk, false);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = rt->rt_src;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sysctl_ip_dynaddr > 1) {\n\t\tprintk(KERN_INFO \"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t       __func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\t__sk_prot_rehash(sk);\n\treturn 0;\n}\n\nint inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tstruct ip_options_rcu *inet_opt;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tdaddr = inet->inet_daddr;\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trcu_read_unlock();\n\trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(inet_sk_rebuild_header);\n\nstatic int inet_gso_send_check(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tconst struct net_protocol *ops;\n\tint proto;\n\tint ihl;\n\tint err = -EINVAL;\n\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*iph))))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\tif (ihl < sizeof(*iph))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, ihl)))\n\t\tgoto out;\n\n\t__skb_pull(skb, ihl);\n\tskb_reset_transport_header(skb);\n\tiph = ip_hdr(skb);\n\tproto = iph->protocol & (MAX_INET_PROTOS - 1);\n\terr = -EPROTONOSUPPORT;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (likely(ops && ops->gso_send_check))\n\t\terr = ops->gso_send_check(skb);\n\trcu_read_unlock();\n\nout:\n\treturn err;\n}\n\nstatic struct sk_buff *inet_gso_segment(struct sk_buff *skb, u32 features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tstruct iphdr *iph;\n\tconst struct net_protocol *ops;\n\tint proto;\n\tint ihl;\n\tint id;\n\tunsigned int offset = 0;\n\n\tif (!(features & NETIF_F_V4_CSUM))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\tif (unlikely(skb_shinfo(skb)->gso_type &\n\t\t     ~(SKB_GSO_TCPV4 |\n\t\t       SKB_GSO_UDP |\n\t\t       SKB_GSO_DODGY |\n\t\t       SKB_GSO_TCP_ECN |\n\t\t       0)))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, sizeof(*iph))))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tihl = iph->ihl * 4;\n\tif (ihl < sizeof(*iph))\n\t\tgoto out;\n\n\tif (unlikely(!pskb_may_pull(skb, ihl)))\n\t\tgoto out;\n\n\t__skb_pull(skb, ihl);\n\tskb_reset_transport_header(skb);\n\tiph = ip_hdr(skb);\n\tid = ntohs(iph->id);\n\tproto = iph->protocol & (MAX_INET_PROTOS - 1);\n\tsegs = ERR_PTR(-EPROTONOSUPPORT);\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (likely(ops && ops->gso_segment))\n\t\tsegs = ops->gso_segment(skb, features);\n\trcu_read_unlock();\n\n\tif (!segs || IS_ERR(segs))\n\t\tgoto out;\n\n\tskb = segs;\n\tdo {\n\t\tiph = ip_hdr(skb);\n\t\tif (proto == IPPROTO_UDP) {\n\t\t\tiph->id = htons(id);\n\t\t\tiph->frag_off = htons(offset >> 3);\n\t\t\tif (skb->next != NULL)\n\t\t\t\tiph->frag_off |= htons(IP_MF);\n\t\t\toffset += (skb->len - skb->mac_len - iph->ihl * 4);\n\t\t} else\n\t\t\tiph->id = htons(id++);\n\t\tiph->tot_len = htons(skb->len - skb->mac_len);\n\t\tiph->check = 0;\n\t\tiph->check = ip_fast_csum(skb_network_header(skb), iph->ihl);\n\t} while ((skb = skb->next));\n\nout:\n\treturn segs;\n}\n\nstatic struct sk_buff **inet_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct net_protocol *ops;\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct iphdr *iph;\n\tunsigned int hlen;\n\tunsigned int off;\n\tunsigned int id;\n\tint flush = 1;\n\tint proto;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*iph);\n\tiph = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tiph = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!iph))\n\t\t\tgoto out;\n\t}\n\n\tproto = iph->protocol & (MAX_INET_PROTOS - 1);\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (!ops || !ops->gro_receive)\n\t\tgoto out_unlock;\n\n\tif (*(u8 *)iph != 0x45)\n\t\tgoto out_unlock;\n\n\tif (unlikely(ip_fast_csum((u8 *)iph, iph->ihl)))\n\t\tgoto out_unlock;\n\n\tid = ntohl(*(__be32 *)&iph->id);\n\tflush = (u16)((ntohl(*(__be32 *)iph) ^ skb_gro_len(skb)) | (id ^ IP_DF));\n\tid >>= 16;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tstruct iphdr *iph2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tiph2 = ip_hdr(p);\n\n\t\tif ((iph->protocol ^ iph2->protocol) |\n\t\t    (iph->tos ^ iph2->tos) |\n\t\t    ((__force u32)iph->saddr ^ (__force u32)iph2->saddr) |\n\t\t    ((__force u32)iph->daddr ^ (__force u32)iph2->daddr)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* All fields must match except length and checksum. */\n\t\tNAPI_GRO_CB(p)->flush |=\n\t\t\t(iph->ttl ^ iph2->ttl) |\n\t\t\t((u16)(ntohs(iph2->id) + NAPI_GRO_CB(p)->count) ^ id);\n\n\t\tNAPI_GRO_CB(p)->flush |= flush;\n\t}\n\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\tskb_gro_pull(skb, sizeof(*iph));\n\tskb_set_transport_header(skb, skb_gro_offset(skb));\n\n\tpp = ops->gro_receive(head, skb);\n\nout_unlock:\n\trcu_read_unlock();\n\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}\n\nstatic int inet_gro_complete(struct sk_buff *skb)\n{\n\tconst struct net_protocol *ops;\n\tstruct iphdr *iph = ip_hdr(skb);\n\tint proto = iph->protocol & (MAX_INET_PROTOS - 1);\n\tint err = -ENOSYS;\n\t__be16 newlen = htons(skb->len - skb_network_offset(skb));\n\n\tcsum_replace2(&iph->check, iph->tot_len, newlen);\n\tiph->tot_len = newlen;\n\n\trcu_read_lock();\n\tops = rcu_dereference(inet_protos[proto]);\n\tif (WARN_ON(!ops || !ops->gro_complete))\n\t\tgoto out_unlock;\n\n\terr = ops->gro_complete(skb);\n\nout_unlock:\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nint inet_ctl_sock_create(struct sock **sk, unsigned short family,\n\t\t\t unsigned short type, unsigned char protocol,\n\t\t\t struct net *net)\n{\n\tstruct socket *sock;\n\tint rc = sock_create_kern(family, type, protocol, &sock);\n\n\tif (rc == 0) {\n\t\t*sk = sock->sk;\n\t\t(*sk)->sk_allocation = GFP_ATOMIC;\n\t\t/*\n\t\t * Unhash it so that IP input processing does not even see it,\n\t\t * we do not wish this socket to see incoming packets.\n\t\t */\n\t\t(*sk)->sk_prot->unhash(*sk);\n\n\t\tsk_change_net(*sk, net);\n\t}\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(inet_ctl_sock_create);\n\nunsigned long snmp_fold_field(void __percpu *mib[], int offt)\n{\n\tunsigned long res = 0;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tres += *(((unsigned long *) per_cpu_ptr(mib[0], i)) + offt);\n\t\tres += *(((unsigned long *) per_cpu_ptr(mib[1], i)) + offt);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field);\n\n#if BITS_PER_LONG==32\n\nu64 snmp_fold_field64(void __percpu *mib[], int offt, size_t syncp_offset)\n{\n\tu64 res = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *bhptr, *userptr;\n\t\tstruct u64_stats_sync *syncp;\n\t\tu64 v_bh, v_user;\n\t\tunsigned int start;\n\n\t\t/* first mib used by softirq context, we must use _bh() accessors */\n\t\tbhptr = per_cpu_ptr(SNMP_STAT_BHPTR(mib), cpu);\n\t\tsyncp = (struct u64_stats_sync *)(bhptr + syncp_offset);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_bh(syncp);\n\t\t\tv_bh = *(((u64 *) bhptr) + offt);\n\t\t} while (u64_stats_fetch_retry_bh(syncp, start));\n\n\t\t/* second mib used in USER context */\n\t\tuserptr = per_cpu_ptr(SNMP_STAT_USRPTR(mib), cpu);\n\t\tsyncp = (struct u64_stats_sync *)(userptr + syncp_offset);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(syncp);\n\t\t\tv_user = *(((u64 *) userptr) + offt);\n\t\t} while (u64_stats_fetch_retry(syncp, start));\n\n\t\tres += v_bh + v_user;\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(snmp_fold_field64);\n#endif\n\nint snmp_mib_init(void __percpu *ptr[2], size_t mibsize, size_t align)\n{\n\tBUG_ON(ptr == NULL);\n\tptr[0] = __alloc_percpu(mibsize, align);\n\tif (!ptr[0])\n\t\tgoto err0;\n\tptr[1] = __alloc_percpu(mibsize, align);\n\tif (!ptr[1])\n\t\tgoto err1;\n\treturn 0;\nerr1:\n\tfree_percpu(ptr[0]);\n\tptr[0] = NULL;\nerr0:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(snmp_mib_init);\n\nvoid snmp_mib_free(void __percpu *ptr[2])\n{\n\tBUG_ON(ptr == NULL);\n\tfree_percpu(ptr[0]);\n\tfree_percpu(ptr[1]);\n\tptr[0] = ptr[1] = NULL;\n}\nEXPORT_SYMBOL_GPL(snmp_mib_free);\n\n#ifdef CONFIG_IP_MULTICAST\nstatic const struct net_protocol igmp_protocol = {\n\t.handler =\tigmp_rcv,\n\t.netns_ok =\t1,\n};\n#endif\n\nstatic const struct net_protocol tcp_protocol = {\n\t.handler =\ttcp_v4_rcv,\n\t.err_handler =\ttcp_v4_err,\n\t.gso_send_check = tcp_v4_gso_send_check,\n\t.gso_segment =\ttcp_tso_segment,\n\t.gro_receive =\ttcp4_gro_receive,\n\t.gro_complete =\ttcp4_gro_complete,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic const struct net_protocol udp_protocol = {\n\t.handler =\tudp_rcv,\n\t.err_handler =\tudp_err,\n\t.gso_send_check = udp4_ufo_send_check,\n\t.gso_segment = udp4_ufo_fragment,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic const struct net_protocol icmp_protocol = {\n\t.handler =\ticmp_rcv,\n\t.no_policy =\t1,\n\t.netns_ok =\t1,\n};\n\nstatic __net_init int ipv4_mib_init_net(struct net *net)\n{\n\tif (snmp_mib_init((void __percpu **)net->mib.tcp_statistics,\n\t\t\t  sizeof(struct tcp_mib),\n\t\t\t  __alignof__(struct tcp_mib)) < 0)\n\t\tgoto err_tcp_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.ip_statistics,\n\t\t\t  sizeof(struct ipstats_mib),\n\t\t\t  __alignof__(struct ipstats_mib)) < 0)\n\t\tgoto err_ip_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.net_statistics,\n\t\t\t  sizeof(struct linux_mib),\n\t\t\t  __alignof__(struct linux_mib)) < 0)\n\t\tgoto err_net_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.udp_statistics,\n\t\t\t  sizeof(struct udp_mib),\n\t\t\t  __alignof__(struct udp_mib)) < 0)\n\t\tgoto err_udp_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.udplite_statistics,\n\t\t\t  sizeof(struct udp_mib),\n\t\t\t  __alignof__(struct udp_mib)) < 0)\n\t\tgoto err_udplite_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.icmp_statistics,\n\t\t\t  sizeof(struct icmp_mib),\n\t\t\t  __alignof__(struct icmp_mib)) < 0)\n\t\tgoto err_icmp_mib;\n\tif (snmp_mib_init((void __percpu **)net->mib.icmpmsg_statistics,\n\t\t\t  sizeof(struct icmpmsg_mib),\n\t\t\t  __alignof__(struct icmpmsg_mib)) < 0)\n\t\tgoto err_icmpmsg_mib;\n\n\ttcp_mib_init(net);\n\treturn 0;\n\nerr_icmpmsg_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.icmp_statistics);\nerr_icmp_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.udplite_statistics);\nerr_udplite_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.udp_statistics);\nerr_udp_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.net_statistics);\nerr_net_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.ip_statistics);\nerr_ip_mib:\n\tsnmp_mib_free((void __percpu **)net->mib.tcp_statistics);\nerr_tcp_mib:\n\treturn -ENOMEM;\n}\n\nstatic __net_exit void ipv4_mib_exit_net(struct net *net)\n{\n\tsnmp_mib_free((void __percpu **)net->mib.icmpmsg_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.icmp_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.udplite_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.udp_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.net_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.ip_statistics);\n\tsnmp_mib_free((void __percpu **)net->mib.tcp_statistics);\n}\n\nstatic __net_initdata struct pernet_operations ipv4_mib_ops = {\n\t.init = ipv4_mib_init_net,\n\t.exit = ipv4_mib_exit_net,\n};\n\nstatic int __init init_ipv4_mibs(void)\n{\n\treturn register_pernet_subsys(&ipv4_mib_ops);\n}\n\nstatic int ipv4_proc_init(void);\n\n/*\n *\tIP protocol layer initialiser\n */\n\nstatic struct packet_type ip_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IP),\n\t.func = ip_rcv,\n\t.gso_send_check = inet_gso_send_check,\n\t.gso_segment = inet_gso_segment,\n\t.gro_receive = inet_gro_receive,\n\t.gro_complete = inet_gro_complete,\n};\n\nstatic int __init inet_init(void)\n{\n\tstruct sk_buff *dummy_skb;\n\tstruct inet_protosw *q;\n\tstruct list_head *r;\n\tint rc = -EINVAL;\n\n\tBUILD_BUG_ON(sizeof(struct inet_skb_parm) > sizeof(dummy_skb->cb));\n\n\tsysctl_local_reserved_ports = kzalloc(65536 / 8, GFP_KERNEL);\n\tif (!sysctl_local_reserved_ports)\n\t\tgoto out;\n\n\trc = proto_register(&tcp_prot, 1);\n\tif (rc)\n\t\tgoto out_free_reserved_ports;\n\n\trc = proto_register(&udp_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_tcp_proto;\n\n\trc = proto_register(&raw_prot, 1);\n\tif (rc)\n\t\tgoto out_unregister_udp_proto;\n\n\t/*\n\t *\tTell SOCKET that we are alive...\n\t */\n\n\t(void)sock_register(&inet_family_ops);\n\n#ifdef CONFIG_SYSCTL\n\tip_static_sysctl_init();\n#endif\n\n\t/*\n\t *\tAdd all the base protocols.\n\t */\n\n\tif (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add ICMP protocol\\n\");\n\tif (inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add UDP protocol\\n\");\n\tif (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add TCP protocol\\n\");\n#ifdef CONFIG_IP_MULTICAST\n\tif (inet_add_protocol(&igmp_protocol, IPPROTO_IGMP) < 0)\n\t\tprintk(KERN_CRIT \"inet_init: Cannot add IGMP protocol\\n\");\n#endif\n\n\t/* Register the socket-side information for inet_create. */\n\tfor (r = &inetsw[0]; r < &inetsw[SOCK_MAX]; ++r)\n\t\tINIT_LIST_HEAD(r);\n\n\tfor (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)\n\t\tinet_register_protosw(q);\n\n\t/*\n\t *\tSet the ARP module up\n\t */\n\n\tarp_init();\n\n\t/*\n\t *\tSet the IP module up\n\t */\n\n\tip_init();\n\n\ttcp_v4_init();\n\n\t/* Setup TCP slab cache for open requests. */\n\ttcp_init();\n\n\t/* Setup UDP memory threshold */\n\tudp_init();\n\n\t/* Add UDP-Lite (RFC 3828) */\n\tudplite4_register();\n\n\t/*\n\t *\tSet the ICMP layer up\n\t */\n\n\tif (icmp_init() < 0)\n\t\tpanic(\"Failed to create the ICMP control socket.\\n\");\n\n\t/*\n\t *\tInitialise the multicast router\n\t */\n#if defined(CONFIG_IP_MROUTE)\n\tif (ip_mr_init())\n\t\tprintk(KERN_CRIT \"inet_init: Cannot init ipv4 mroute\\n\");\n#endif\n\t/*\n\t *\tInitialise per-cpu ipv4 mibs\n\t */\n\n\tif (init_ipv4_mibs())\n\t\tprintk(KERN_CRIT \"inet_init: Cannot init ipv4 mibs\\n\");\n\n\tipv4_proc_init();\n\n\tipfrag_init();\n\n\tdev_add_pack(&ip_packet_type);\n\n\trc = 0;\nout:\n\treturn rc;\nout_unregister_udp_proto:\n\tproto_unregister(&udp_prot);\nout_unregister_tcp_proto:\n\tproto_unregister(&tcp_prot);\nout_free_reserved_ports:\n\tkfree(sysctl_local_reserved_ports);\n\tgoto out;\n}\n\nfs_initcall(inet_init);\n\n/* ------------------------------------------------------------------------ */\n\n#ifdef CONFIG_PROC_FS\nstatic int __init ipv4_proc_init(void)\n{\n\tint rc = 0;\n\n\tif (raw_proc_init())\n\t\tgoto out_raw;\n\tif (tcp4_proc_init())\n\t\tgoto out_tcp;\n\tif (udp4_proc_init())\n\t\tgoto out_udp;\n\tif (ip_misc_proc_init())\n\t\tgoto out_misc;\nout:\n\treturn rc;\nout_misc:\n\tudp4_proc_exit();\nout_udp:\n\ttcp4_proc_exit();\nout_tcp:\n\traw_proc_exit();\nout_raw:\n\trc = -ENOMEM;\n\tgoto out;\n}\n\n#else /* CONFIG_PROC_FS */\nstatic int __init ipv4_proc_init(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_PROC_FS */\n\nMODULE_ALIAS_NETPROTO(PF_INET);\n\n", "/*\n * CIPSO - Commercial IP Security Option\n *\n * This is an implementation of the CIPSO 2.2 protocol as specified in\n * draft-ietf-cipso-ipsecurity-01.txt with additional tag types as found in\n * FIPS-188.  While CIPSO never became a full IETF RFC standard many vendors\n * have chosen to adopt the protocol and over the years it has become a\n * de-facto standard for labeled networking.\n *\n * The CIPSO draft specification can be found in the kernel's Documentation\n * directory as well as the following URL:\n *   http://tools.ietf.org/id/draft-ietf-cipso-ipsecurity-01.txt\n * The FIPS-188 specification can be found at the following URL:\n *   http://www.itl.nist.gov/fipspubs/fip188.htm\n *\n * Author: Paul Moore <paul.moore@hp.com>\n *\n */\n\n/*\n * (c) Copyright Hewlett-Packard Development Company, L.P., 2006, 2008\n *\n * This program is free software;  you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY;  without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See\n * the GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program;  if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n *\n */\n\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/rcupdate.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/jhash.h>\n#include <linux/audit.h>\n#include <linux/slab.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/tcp.h>\n#include <net/netlabel.h>\n#include <net/cipso_ipv4.h>\n#include <asm/atomic.h>\n#include <asm/bug.h>\n#include <asm/unaligned.h>\n\n/* List of available DOI definitions */\n/* XXX - This currently assumes a minimal number of different DOIs in use,\n * if in practice there are a lot of different DOIs this list should\n * probably be turned into a hash table or something similar so we\n * can do quick lookups. */\nstatic DEFINE_SPINLOCK(cipso_v4_doi_list_lock);\nstatic LIST_HEAD(cipso_v4_doi_list);\n\n/* Label mapping cache */\nint cipso_v4_cache_enabled = 1;\nint cipso_v4_cache_bucketsize = 10;\n#define CIPSO_V4_CACHE_BUCKETBITS     7\n#define CIPSO_V4_CACHE_BUCKETS        (1 << CIPSO_V4_CACHE_BUCKETBITS)\n#define CIPSO_V4_CACHE_REORDERLIMIT   10\nstruct cipso_v4_map_cache_bkt {\n\tspinlock_t lock;\n\tu32 size;\n\tstruct list_head list;\n};\nstruct cipso_v4_map_cache_entry {\n\tu32 hash;\n\tunsigned char *key;\n\tsize_t key_len;\n\n\tstruct netlbl_lsm_cache *lsm_data;\n\n\tu32 activity;\n\tstruct list_head list;\n};\nstatic struct cipso_v4_map_cache_bkt *cipso_v4_cache = NULL;\n\n/* Restricted bitmap (tag #1) flags */\nint cipso_v4_rbm_optfmt = 0;\nint cipso_v4_rbm_strictvalid = 1;\n\n/*\n * Protocol Constants\n */\n\n/* Maximum size of the CIPSO IP option, derived from the fact that the maximum\n * IPv4 header size is 60 bytes and the base IPv4 header is 20 bytes long. */\n#define CIPSO_V4_OPT_LEN_MAX          40\n\n/* Length of the base CIPSO option, this includes the option type (1 byte), the\n * option length (1 byte), and the DOI (4 bytes). */\n#define CIPSO_V4_HDR_LEN              6\n\n/* Base length of the restrictive category bitmap tag (tag #1). */\n#define CIPSO_V4_TAG_RBM_BLEN         4\n\n/* Base length of the enumerated category tag (tag #2). */\n#define CIPSO_V4_TAG_ENUM_BLEN        4\n\n/* Base length of the ranged categories bitmap tag (tag #5). */\n#define CIPSO_V4_TAG_RNG_BLEN         4\n/* The maximum number of category ranges permitted in the ranged category tag\n * (tag #5).  You may note that the IETF draft states that the maximum number\n * of category ranges is 7, but if the low end of the last category range is\n * zero then it is possible to fit 8 category ranges because the zero should\n * be omitted. */\n#define CIPSO_V4_TAG_RNG_CAT_MAX      8\n\n/* Base length of the local tag (non-standard tag).\n *  Tag definition (may change between kernel versions)\n *\n * 0          8          16         24         32\n * +----------+----------+----------+----------+\n * | 10000000 | 00000110 | 32-bit secid value  |\n * +----------+----------+----------+----------+\n * | in (host byte order)|\n * +----------+----------+\n *\n */\n#define CIPSO_V4_TAG_LOC_BLEN         6\n\n/*\n * Helper Functions\n */\n\n/**\n * cipso_v4_bitmap_walk - Walk a bitmap looking for a bit\n * @bitmap: the bitmap\n * @bitmap_len: length in bits\n * @offset: starting offset\n * @state: if non-zero, look for a set (1) bit else look for a cleared (0) bit\n *\n * Description:\n * Starting at @offset, walk the bitmap from left to right until either the\n * desired bit is found or we reach the end.  Return the bit offset, -1 if\n * not found, or -2 if error.\n */\nstatic int cipso_v4_bitmap_walk(const unsigned char *bitmap,\n\t\t\t\tu32 bitmap_len,\n\t\t\t\tu32 offset,\n\t\t\t\tu8 state)\n{\n\tu32 bit_spot;\n\tu32 byte_offset;\n\tunsigned char bitmask;\n\tunsigned char byte;\n\n\t/* gcc always rounds to zero when doing integer division */\n\tbyte_offset = offset / 8;\n\tbyte = bitmap[byte_offset];\n\tbit_spot = offset;\n\tbitmask = 0x80 >> (offset % 8);\n\n\twhile (bit_spot < bitmap_len) {\n\t\tif ((state && (byte & bitmask) == bitmask) ||\n\t\t    (state == 0 && (byte & bitmask) == 0))\n\t\t\treturn bit_spot;\n\n\t\tbit_spot++;\n\t\tbitmask >>= 1;\n\t\tif (bitmask == 0) {\n\t\t\tbyte = bitmap[++byte_offset];\n\t\t\tbitmask = 0x80;\n\t\t}\n\t}\n\n\treturn -1;\n}\n\n/**\n * cipso_v4_bitmap_setbit - Sets a single bit in a bitmap\n * @bitmap: the bitmap\n * @bit: the bit\n * @state: if non-zero, set the bit (1) else clear the bit (0)\n *\n * Description:\n * Set a single bit in the bitmask.  Returns zero on success, negative values\n * on error.\n */\nstatic void cipso_v4_bitmap_setbit(unsigned char *bitmap,\n\t\t\t\t   u32 bit,\n\t\t\t\t   u8 state)\n{\n\tu32 byte_spot;\n\tu8 bitmask;\n\n\t/* gcc always rounds to zero when doing integer division */\n\tbyte_spot = bit / 8;\n\tbitmask = 0x80 >> (bit % 8);\n\tif (state)\n\t\tbitmap[byte_spot] |= bitmask;\n\telse\n\t\tbitmap[byte_spot] &= ~bitmask;\n}\n\n/**\n * cipso_v4_cache_entry_free - Frees a cache entry\n * @entry: the entry to free\n *\n * Description:\n * This function frees the memory associated with a cache entry including the\n * LSM cache data if there are no longer any users, i.e. reference count == 0.\n *\n */\nstatic void cipso_v4_cache_entry_free(struct cipso_v4_map_cache_entry *entry)\n{\n\tif (entry->lsm_data)\n\t\tnetlbl_secattr_cache_free(entry->lsm_data);\n\tkfree(entry->key);\n\tkfree(entry);\n}\n\n/**\n * cipso_v4_map_cache_hash - Hashing function for the CIPSO cache\n * @key: the hash key\n * @key_len: the length of the key in bytes\n *\n * Description:\n * The CIPSO tag hashing function.  Returns a 32-bit hash value.\n *\n */\nstatic u32 cipso_v4_map_cache_hash(const unsigned char *key, u32 key_len)\n{\n\treturn jhash(key, key_len, 0);\n}\n\n/*\n * Label Mapping Cache Functions\n */\n\n/**\n * cipso_v4_cache_init - Initialize the CIPSO cache\n *\n * Description:\n * Initializes the CIPSO label mapping cache, this function should be called\n * before any of the other functions defined in this file.  Returns zero on\n * success, negative values on error.\n *\n */\nstatic int cipso_v4_cache_init(void)\n{\n\tu32 iter;\n\n\tcipso_v4_cache = kcalloc(CIPSO_V4_CACHE_BUCKETS,\n\t\t\t\t sizeof(struct cipso_v4_map_cache_bkt),\n\t\t\t\t GFP_KERNEL);\n\tif (cipso_v4_cache == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (iter = 0; iter < CIPSO_V4_CACHE_BUCKETS; iter++) {\n\t\tspin_lock_init(&cipso_v4_cache[iter].lock);\n\t\tcipso_v4_cache[iter].size = 0;\n\t\tINIT_LIST_HEAD(&cipso_v4_cache[iter].list);\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_cache_invalidate - Invalidates the current CIPSO cache\n *\n * Description:\n * Invalidates and frees any entries in the CIPSO cache.  Returns zero on\n * success and negative values on failure.\n *\n */\nvoid cipso_v4_cache_invalidate(void)\n{\n\tstruct cipso_v4_map_cache_entry *entry, *tmp_entry;\n\tu32 iter;\n\n\tfor (iter = 0; iter < CIPSO_V4_CACHE_BUCKETS; iter++) {\n\t\tspin_lock_bh(&cipso_v4_cache[iter].lock);\n\t\tlist_for_each_entry_safe(entry,\n\t\t\t\t\t tmp_entry,\n\t\t\t\t\t &cipso_v4_cache[iter].list, list) {\n\t\t\tlist_del(&entry->list);\n\t\t\tcipso_v4_cache_entry_free(entry);\n\t\t}\n\t\tcipso_v4_cache[iter].size = 0;\n\t\tspin_unlock_bh(&cipso_v4_cache[iter].lock);\n\t}\n}\n\n/**\n * cipso_v4_cache_check - Check the CIPSO cache for a label mapping\n * @key: the buffer to check\n * @key_len: buffer length in bytes\n * @secattr: the security attribute struct to use\n *\n * Description:\n * This function checks the cache to see if a label mapping already exists for\n * the given key.  If there is a match then the cache is adjusted and the\n * @secattr struct is populated with the correct LSM security attributes.  The\n * cache is adjusted in the following manner if the entry is not already the\n * first in the cache bucket:\n *\n *  1. The cache entry's activity counter is incremented\n *  2. The previous (higher ranking) entry's activity counter is decremented\n *  3. If the difference between the two activity counters is geater than\n *     CIPSO_V4_CACHE_REORDERLIMIT the two entries are swapped\n *\n * Returns zero on success, -ENOENT for a cache miss, and other negative values\n * on error.\n *\n */\nstatic int cipso_v4_cache_check(const unsigned char *key,\n\t\t\t\tu32 key_len,\n\t\t\t\tstruct netlbl_lsm_secattr *secattr)\n{\n\tu32 bkt;\n\tstruct cipso_v4_map_cache_entry *entry;\n\tstruct cipso_v4_map_cache_entry *prev_entry = NULL;\n\tu32 hash;\n\n\tif (!cipso_v4_cache_enabled)\n\t\treturn -ENOENT;\n\n\thash = cipso_v4_map_cache_hash(key, key_len);\n\tbkt = hash & (CIPSO_V4_CACHE_BUCKETS - 1);\n\tspin_lock_bh(&cipso_v4_cache[bkt].lock);\n\tlist_for_each_entry(entry, &cipso_v4_cache[bkt].list, list) {\n\t\tif (entry->hash == hash &&\n\t\t    entry->key_len == key_len &&\n\t\t    memcmp(entry->key, key, key_len) == 0) {\n\t\t\tentry->activity += 1;\n\t\t\tatomic_inc(&entry->lsm_data->refcount);\n\t\t\tsecattr->cache = entry->lsm_data;\n\t\t\tsecattr->flags |= NETLBL_SECATTR_CACHE;\n\t\t\tsecattr->type = NETLBL_NLTYPE_CIPSOV4;\n\t\t\tif (prev_entry == NULL) {\n\t\t\t\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\tif (prev_entry->activity > 0)\n\t\t\t\tprev_entry->activity -= 1;\n\t\t\tif (entry->activity > prev_entry->activity &&\n\t\t\t    entry->activity - prev_entry->activity >\n\t\t\t    CIPSO_V4_CACHE_REORDERLIMIT) {\n\t\t\t\t__list_del(entry->list.prev, entry->list.next);\n\t\t\t\t__list_add(&entry->list,\n\t\t\t\t\t   prev_entry->list.prev,\n\t\t\t\t\t   &prev_entry->list);\n\t\t\t}\n\n\t\t\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\t\t\treturn 0;\n\t\t}\n\t\tprev_entry = entry;\n\t}\n\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\n\treturn -ENOENT;\n}\n\n/**\n * cipso_v4_cache_add - Add an entry to the CIPSO cache\n * @skb: the packet\n * @secattr: the packet's security attributes\n *\n * Description:\n * Add a new entry into the CIPSO label mapping cache.  Add the new entry to\n * head of the cache bucket's list, if the cache bucket is out of room remove\n * the last entry in the list first.  It is important to note that there is\n * currently no checking for duplicate keys.  Returns zero on success,\n * negative values on failure.\n *\n */\nint cipso_v4_cache_add(const struct sk_buff *skb,\n\t\t       const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tu32 bkt;\n\tstruct cipso_v4_map_cache_entry *entry = NULL;\n\tstruct cipso_v4_map_cache_entry *old_entry = NULL;\n\tunsigned char *cipso_ptr;\n\tu32 cipso_ptr_len;\n\n\tif (!cipso_v4_cache_enabled || cipso_v4_cache_bucketsize <= 0)\n\t\treturn 0;\n\n\tcipso_ptr = CIPSO_V4_OPTPTR(skb);\n\tcipso_ptr_len = cipso_ptr[1];\n\n\tentry = kzalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (entry == NULL)\n\t\treturn -ENOMEM;\n\tentry->key = kmemdup(cipso_ptr, cipso_ptr_len, GFP_ATOMIC);\n\tif (entry->key == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto cache_add_failure;\n\t}\n\tentry->key_len = cipso_ptr_len;\n\tentry->hash = cipso_v4_map_cache_hash(cipso_ptr, cipso_ptr_len);\n\tatomic_inc(&secattr->cache->refcount);\n\tentry->lsm_data = secattr->cache;\n\n\tbkt = entry->hash & (CIPSO_V4_CACHE_BUCKETS - 1);\n\tspin_lock_bh(&cipso_v4_cache[bkt].lock);\n\tif (cipso_v4_cache[bkt].size < cipso_v4_cache_bucketsize) {\n\t\tlist_add(&entry->list, &cipso_v4_cache[bkt].list);\n\t\tcipso_v4_cache[bkt].size += 1;\n\t} else {\n\t\told_entry = list_entry(cipso_v4_cache[bkt].list.prev,\n\t\t\t\t       struct cipso_v4_map_cache_entry, list);\n\t\tlist_del(&old_entry->list);\n\t\tlist_add(&entry->list, &cipso_v4_cache[bkt].list);\n\t\tcipso_v4_cache_entry_free(old_entry);\n\t}\n\tspin_unlock_bh(&cipso_v4_cache[bkt].lock);\n\n\treturn 0;\n\ncache_add_failure:\n\tif (entry)\n\t\tcipso_v4_cache_entry_free(entry);\n\treturn ret_val;\n}\n\n/*\n * DOI List Functions\n */\n\n/**\n * cipso_v4_doi_search - Searches for a DOI definition\n * @doi: the DOI to search for\n *\n * Description:\n * Search the DOI definition list for a DOI definition with a DOI value that\n * matches @doi.  The caller is responsible for calling rcu_read_[un]lock().\n * Returns a pointer to the DOI definition on success and NULL on failure.\n */\nstatic struct cipso_v4_doi *cipso_v4_doi_search(u32 doi)\n{\n\tstruct cipso_v4_doi *iter;\n\n\tlist_for_each_entry_rcu(iter, &cipso_v4_doi_list, list)\n\t\tif (iter->doi == doi && atomic_read(&iter->refcount))\n\t\t\treturn iter;\n\treturn NULL;\n}\n\n/**\n * cipso_v4_doi_add - Add a new DOI to the CIPSO protocol engine\n * @doi_def: the DOI structure\n * @audit_info: NetLabel audit information\n *\n * Description:\n * The caller defines a new DOI for use by the CIPSO engine and calls this\n * function to add it to the list of acceptable domains.  The caller must\n * ensure that the mapping table specified in @doi_def->map meets all of the\n * requirements of the mapping type (see cipso_ipv4.h for details).  Returns\n * zero on success and non-zero on failure.\n *\n */\nint cipso_v4_doi_add(struct cipso_v4_doi *doi_def,\n\t\t     struct netlbl_audit *audit_info)\n{\n\tint ret_val = -EINVAL;\n\tu32 iter;\n\tu32 doi;\n\tu32 doi_type;\n\tstruct audit_buffer *audit_buf;\n\n\tdoi = doi_def->doi;\n\tdoi_type = doi_def->type;\n\n\tif (doi_def == NULL || doi_def->doi == CIPSO_V4_DOI_UNKNOWN)\n\t\tgoto doi_add_return;\n\tfor (iter = 0; iter < CIPSO_V4_TAG_MAXCNT; iter++) {\n\t\tswitch (doi_def->tags[iter]) {\n\t\tcase CIPSO_V4_TAG_RBITMAP:\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_RANGE:\n\t\tcase CIPSO_V4_TAG_ENUM:\n\t\t\tif (doi_def->type != CIPSO_V4_MAP_PASS)\n\t\t\t\tgoto doi_add_return;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_LOCAL:\n\t\t\tif (doi_def->type != CIPSO_V4_MAP_LOCAL)\n\t\t\t\tgoto doi_add_return;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_INVALID:\n\t\t\tif (iter == 0)\n\t\t\t\tgoto doi_add_return;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto doi_add_return;\n\t\t}\n\t}\n\n\tatomic_set(&doi_def->refcount, 1);\n\n\tspin_lock(&cipso_v4_doi_list_lock);\n\tif (cipso_v4_doi_search(doi_def->doi) != NULL) {\n\t\tspin_unlock(&cipso_v4_doi_list_lock);\n\t\tret_val = -EEXIST;\n\t\tgoto doi_add_return;\n\t}\n\tlist_add_tail_rcu(&doi_def->list, &cipso_v4_doi_list);\n\tspin_unlock(&cipso_v4_doi_list_lock);\n\tret_val = 0;\n\ndoi_add_return:\n\taudit_buf = netlbl_audit_start(AUDIT_MAC_CIPSOV4_ADD, audit_info);\n\tif (audit_buf != NULL) {\n\t\tconst char *type_str;\n\t\tswitch (doi_type) {\n\t\tcase CIPSO_V4_MAP_TRANS:\n\t\t\ttype_str = \"trans\";\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_PASS:\n\t\t\ttype_str = \"pass\";\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_LOCAL:\n\t\t\ttype_str = \"local\";\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttype_str = \"(unknown)\";\n\t\t}\n\t\taudit_log_format(audit_buf,\n\t\t\t\t \" cipso_doi=%u cipso_type=%s res=%u\",\n\t\t\t\t doi, type_str, ret_val == 0 ? 1 : 0);\n\t\taudit_log_end(audit_buf);\n\t}\n\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_doi_free - Frees a DOI definition\n * @entry: the entry's RCU field\n *\n * Description:\n * This function frees all of the memory associated with a DOI definition.\n *\n */\nvoid cipso_v4_doi_free(struct cipso_v4_doi *doi_def)\n{\n\tif (doi_def == NULL)\n\t\treturn;\n\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tkfree(doi_def->map.std->lvl.cipso);\n\t\tkfree(doi_def->map.std->lvl.local);\n\t\tkfree(doi_def->map.std->cat.cipso);\n\t\tkfree(doi_def->map.std->cat.local);\n\t\tbreak;\n\t}\n\tkfree(doi_def);\n}\n\n/**\n * cipso_v4_doi_free_rcu - Frees a DOI definition via the RCU pointer\n * @entry: the entry's RCU field\n *\n * Description:\n * This function is designed to be used as a callback to the call_rcu()\n * function so that the memory allocated to the DOI definition can be released\n * safely.\n *\n */\nstatic void cipso_v4_doi_free_rcu(struct rcu_head *entry)\n{\n\tstruct cipso_v4_doi *doi_def;\n\n\tdoi_def = container_of(entry, struct cipso_v4_doi, rcu);\n\tcipso_v4_doi_free(doi_def);\n}\n\n/**\n * cipso_v4_doi_remove - Remove an existing DOI from the CIPSO protocol engine\n * @doi: the DOI value\n * @audit_secid: the LSM secid to use in the audit message\n *\n * Description:\n * Removes a DOI definition from the CIPSO engine.  The NetLabel routines will\n * be called to release their own LSM domain mappings as well as our own\n * domain list.  Returns zero on success and negative values on failure.\n *\n */\nint cipso_v4_doi_remove(u32 doi, struct netlbl_audit *audit_info)\n{\n\tint ret_val;\n\tstruct cipso_v4_doi *doi_def;\n\tstruct audit_buffer *audit_buf;\n\n\tspin_lock(&cipso_v4_doi_list_lock);\n\tdoi_def = cipso_v4_doi_search(doi);\n\tif (doi_def == NULL) {\n\t\tspin_unlock(&cipso_v4_doi_list_lock);\n\t\tret_val = -ENOENT;\n\t\tgoto doi_remove_return;\n\t}\n\tif (!atomic_dec_and_test(&doi_def->refcount)) {\n\t\tspin_unlock(&cipso_v4_doi_list_lock);\n\t\tret_val = -EBUSY;\n\t\tgoto doi_remove_return;\n\t}\n\tlist_del_rcu(&doi_def->list);\n\tspin_unlock(&cipso_v4_doi_list_lock);\n\n\tcipso_v4_cache_invalidate();\n\tcall_rcu(&doi_def->rcu, cipso_v4_doi_free_rcu);\n\tret_val = 0;\n\ndoi_remove_return:\n\taudit_buf = netlbl_audit_start(AUDIT_MAC_CIPSOV4_DEL, audit_info);\n\tif (audit_buf != NULL) {\n\t\taudit_log_format(audit_buf,\n\t\t\t\t \" cipso_doi=%u res=%u\",\n\t\t\t\t doi, ret_val == 0 ? 1 : 0);\n\t\taudit_log_end(audit_buf);\n\t}\n\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_doi_getdef - Returns a reference to a valid DOI definition\n * @doi: the DOI value\n *\n * Description:\n * Searches for a valid DOI definition and if one is found it is returned to\n * the caller.  Otherwise NULL is returned.  The caller must ensure that\n * rcu_read_lock() is held while accessing the returned definition and the DOI\n * definition reference count is decremented when the caller is done.\n *\n */\nstruct cipso_v4_doi *cipso_v4_doi_getdef(u32 doi)\n{\n\tstruct cipso_v4_doi *doi_def;\n\n\trcu_read_lock();\n\tdoi_def = cipso_v4_doi_search(doi);\n\tif (doi_def == NULL)\n\t\tgoto doi_getdef_return;\n\tif (!atomic_inc_not_zero(&doi_def->refcount))\n\t\tdoi_def = NULL;\n\ndoi_getdef_return:\n\trcu_read_unlock();\n\treturn doi_def;\n}\n\n/**\n * cipso_v4_doi_putdef - Releases a reference for the given DOI definition\n * @doi_def: the DOI definition\n *\n * Description:\n * Releases a DOI definition reference obtained from cipso_v4_doi_getdef().\n *\n */\nvoid cipso_v4_doi_putdef(struct cipso_v4_doi *doi_def)\n{\n\tif (doi_def == NULL)\n\t\treturn;\n\n\tif (!atomic_dec_and_test(&doi_def->refcount))\n\t\treturn;\n\tspin_lock(&cipso_v4_doi_list_lock);\n\tlist_del_rcu(&doi_def->list);\n\tspin_unlock(&cipso_v4_doi_list_lock);\n\n\tcipso_v4_cache_invalidate();\n\tcall_rcu(&doi_def->rcu, cipso_v4_doi_free_rcu);\n}\n\n/**\n * cipso_v4_doi_walk - Iterate through the DOI definitions\n * @skip_cnt: skip past this number of DOI definitions, updated\n * @callback: callback for each DOI definition\n * @cb_arg: argument for the callback function\n *\n * Description:\n * Iterate over the DOI definition list, skipping the first @skip_cnt entries.\n * For each entry call @callback, if @callback returns a negative value stop\n * 'walking' through the list and return.  Updates the value in @skip_cnt upon\n * return.  Returns zero on success, negative values on failure.\n *\n */\nint cipso_v4_doi_walk(u32 *skip_cnt,\n\t\t     int (*callback) (struct cipso_v4_doi *doi_def, void *arg),\n\t\t     void *cb_arg)\n{\n\tint ret_val = -ENOENT;\n\tu32 doi_cnt = 0;\n\tstruct cipso_v4_doi *iter_doi;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(iter_doi, &cipso_v4_doi_list, list)\n\t\tif (atomic_read(&iter_doi->refcount) > 0) {\n\t\t\tif (doi_cnt++ < *skip_cnt)\n\t\t\t\tcontinue;\n\t\t\tret_val = callback(iter_doi, cb_arg);\n\t\t\tif (ret_val < 0) {\n\t\t\t\tdoi_cnt--;\n\t\t\t\tgoto doi_walk_return;\n\t\t\t}\n\t\t}\n\ndoi_walk_return:\n\trcu_read_unlock();\n\t*skip_cnt = doi_cnt;\n\treturn ret_val;\n}\n\n/*\n * Label Mapping Functions\n */\n\n/**\n * cipso_v4_map_lvl_valid - Checks to see if the given level is understood\n * @doi_def: the DOI definition\n * @level: the level to check\n *\n * Description:\n * Checks the given level against the given DOI definition and returns a\n * negative value if the level does not have a valid mapping and a zero value\n * if the level is defined by the DOI.\n *\n */\nstatic int cipso_v4_map_lvl_valid(const struct cipso_v4_doi *doi_def, u8 level)\n{\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tif (doi_def->map.std->lvl.cipso[level] < CIPSO_V4_INV_LVL)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -EFAULT;\n}\n\n/**\n * cipso_v4_map_lvl_hton - Perform a level mapping from the host to the network\n * @doi_def: the DOI definition\n * @host_lvl: the host MLS level\n * @net_lvl: the network/CIPSO MLS level\n *\n * Description:\n * Perform a label mapping to translate a local MLS level to the correct\n * CIPSO level using the given DOI definition.  Returns zero on success,\n * negative values otherwise.\n *\n */\nstatic int cipso_v4_map_lvl_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t u32 host_lvl,\n\t\t\t\t u32 *net_lvl)\n{\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\t*net_lvl = host_lvl;\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tif (host_lvl < doi_def->map.std->lvl.local_size &&\n\t\t    doi_def->map.std->lvl.local[host_lvl] < CIPSO_V4_INV_LVL) {\n\t\t\t*net_lvl = doi_def->map.std->lvl.local[host_lvl];\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EPERM;\n\t}\n\n\treturn -EINVAL;\n}\n\n/**\n * cipso_v4_map_lvl_ntoh - Perform a level mapping from the network to the host\n * @doi_def: the DOI definition\n * @net_lvl: the network/CIPSO MLS level\n * @host_lvl: the host MLS level\n *\n * Description:\n * Perform a label mapping to translate a CIPSO level to the correct local MLS\n * level using the given DOI definition.  Returns zero on success, negative\n * values otherwise.\n *\n */\nstatic int cipso_v4_map_lvl_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t u32 net_lvl,\n\t\t\t\t u32 *host_lvl)\n{\n\tstruct cipso_v4_std_map_tbl *map_tbl;\n\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\t*host_lvl = net_lvl;\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tmap_tbl = doi_def->map.std;\n\t\tif (net_lvl < map_tbl->lvl.cipso_size &&\n\t\t    map_tbl->lvl.cipso[net_lvl] < CIPSO_V4_INV_LVL) {\n\t\t\t*host_lvl = doi_def->map.std->lvl.cipso[net_lvl];\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EPERM;\n\t}\n\n\treturn -EINVAL;\n}\n\n/**\n * cipso_v4_map_cat_rbm_valid - Checks to see if the category bitmap is valid\n * @doi_def: the DOI definition\n * @bitmap: category bitmap\n * @bitmap_len: bitmap length in bytes\n *\n * Description:\n * Checks the given category bitmap against the given DOI definition and\n * returns a negative value if any of the categories in the bitmap do not have\n * a valid mapping and a zero value if all of the categories are valid.\n *\n */\nstatic int cipso_v4_map_cat_rbm_valid(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const unsigned char *bitmap,\n\t\t\t\t      u32 bitmap_len)\n{\n\tint cat = -1;\n\tu32 bitmap_len_bits = bitmap_len * 8;\n\tu32 cipso_cat_size;\n\tu32 *cipso_array;\n\n\tswitch (doi_def->type) {\n\tcase CIPSO_V4_MAP_PASS:\n\t\treturn 0;\n\tcase CIPSO_V4_MAP_TRANS:\n\t\tcipso_cat_size = doi_def->map.std->cat.cipso_size;\n\t\tcipso_array = doi_def->map.std->cat.cipso;\n\t\tfor (;;) {\n\t\t\tcat = cipso_v4_bitmap_walk(bitmap,\n\t\t\t\t\t\t   bitmap_len_bits,\n\t\t\t\t\t\t   cat + 1,\n\t\t\t\t\t\t   1);\n\t\t\tif (cat < 0)\n\t\t\t\tbreak;\n\t\t\tif (cat >= cipso_cat_size ||\n\t\t\t    cipso_array[cat] >= CIPSO_V4_INV_CAT)\n\t\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (cat == -1)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -EFAULT;\n}\n\n/**\n * cipso_v4_map_cat_rbm_hton - Perform a category mapping from host to network\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @net_cat: the zero'd out category bitmap in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n *\n * Description:\n * Perform a label mapping to translate a local MLS category bitmap to the\n * correct CIPSO bitmap using the given DOI definition.  Returns the minimum\n * size in bytes of the network bitmap on success, negative values otherwise.\n *\n */\nstatic int cipso_v4_map_cat_rbm_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const struct netlbl_lsm_secattr *secattr,\n\t\t\t\t     unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len)\n{\n\tint host_spot = -1;\n\tu32 net_spot = CIPSO_V4_INV_CAT;\n\tu32 net_spot_max = 0;\n\tu32 net_clen_bits = net_cat_len * 8;\n\tu32 host_cat_size = 0;\n\tu32 *host_cat_array = NULL;\n\n\tif (doi_def->type == CIPSO_V4_MAP_TRANS) {\n\t\thost_cat_size = doi_def->map.std->cat.local_size;\n\t\thost_cat_array = doi_def->map.std->cat.local;\n\t}\n\n\tfor (;;) {\n\t\thost_spot = netlbl_secattr_catmap_walk(secattr->attr.mls.cat,\n\t\t\t\t\t\t       host_spot + 1);\n\t\tif (host_spot < 0)\n\t\t\tbreak;\n\n\t\tswitch (doi_def->type) {\n\t\tcase CIPSO_V4_MAP_PASS:\n\t\t\tnet_spot = host_spot;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_TRANS:\n\t\t\tif (host_spot >= host_cat_size)\n\t\t\t\treturn -EPERM;\n\t\t\tnet_spot = host_cat_array[host_spot];\n\t\t\tif (net_spot >= CIPSO_V4_INV_CAT)\n\t\t\t\treturn -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (net_spot >= net_clen_bits)\n\t\t\treturn -ENOSPC;\n\t\tcipso_v4_bitmap_setbit(net_cat, net_spot, 1);\n\n\t\tif (net_spot > net_spot_max)\n\t\t\tnet_spot_max = net_spot;\n\t}\n\n\tif (++net_spot_max % 8)\n\t\treturn net_spot_max / 8 + 1;\n\treturn net_spot_max / 8;\n}\n\n/**\n * cipso_v4_map_cat_rbm_ntoh - Perform a category mapping from network to host\n * @doi_def: the DOI definition\n * @net_cat: the category bitmap in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n * @secattr: the security attributes\n *\n * Description:\n * Perform a label mapping to translate a CIPSO bitmap to the correct local\n * MLS category bitmap using the given DOI definition.  Returns zero on\n * success, negative values on failure.\n *\n */\nstatic int cipso_v4_map_cat_rbm_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len,\n\t\t\t\t     struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tint net_spot = -1;\n\tu32 host_spot = CIPSO_V4_INV_CAT;\n\tu32 net_clen_bits = net_cat_len * 8;\n\tu32 net_cat_size = 0;\n\tu32 *net_cat_array = NULL;\n\n\tif (doi_def->type == CIPSO_V4_MAP_TRANS) {\n\t\tnet_cat_size = doi_def->map.std->cat.cipso_size;\n\t\tnet_cat_array = doi_def->map.std->cat.cipso;\n\t}\n\n\tfor (;;) {\n\t\tnet_spot = cipso_v4_bitmap_walk(net_cat,\n\t\t\t\t\t\tnet_clen_bits,\n\t\t\t\t\t\tnet_spot + 1,\n\t\t\t\t\t\t1);\n\t\tif (net_spot < 0) {\n\t\t\tif (net_spot == -2)\n\t\t\t\treturn -EFAULT;\n\t\t\treturn 0;\n\t\t}\n\n\t\tswitch (doi_def->type) {\n\t\tcase CIPSO_V4_MAP_PASS:\n\t\t\thost_spot = net_spot;\n\t\t\tbreak;\n\t\tcase CIPSO_V4_MAP_TRANS:\n\t\t\tif (net_spot >= net_cat_size)\n\t\t\t\treturn -EPERM;\n\t\t\thost_spot = net_cat_array[net_spot];\n\t\t\tif (host_spot >= CIPSO_V4_INV_CAT)\n\t\t\t\treturn -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tret_val = netlbl_secattr_catmap_setbit(secattr->attr.mls.cat,\n\t\t\t\t\t\t       host_spot,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\tif (ret_val != 0)\n\t\t\treturn ret_val;\n\t}\n\n\treturn -EINVAL;\n}\n\n/**\n * cipso_v4_map_cat_enum_valid - Checks to see if the categories are valid\n * @doi_def: the DOI definition\n * @enumcat: category list\n * @enumcat_len: length of the category list in bytes\n *\n * Description:\n * Checks the given categories against the given DOI definition and returns a\n * negative value if any of the categories do not have a valid mapping and a\n * zero value if all of the categories are valid.\n *\n */\nstatic int cipso_v4_map_cat_enum_valid(const struct cipso_v4_doi *doi_def,\n\t\t\t\t       const unsigned char *enumcat,\n\t\t\t\t       u32 enumcat_len)\n{\n\tu16 cat;\n\tint cat_prev = -1;\n\tu32 iter;\n\n\tif (doi_def->type != CIPSO_V4_MAP_PASS || enumcat_len & 0x01)\n\t\treturn -EFAULT;\n\n\tfor (iter = 0; iter < enumcat_len; iter += 2) {\n\t\tcat = get_unaligned_be16(&enumcat[iter]);\n\t\tif (cat <= cat_prev)\n\t\t\treturn -EFAULT;\n\t\tcat_prev = cat;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_map_cat_enum_hton - Perform a category mapping from host to network\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @net_cat: the zero'd out category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO category list in bytes\n *\n * Description:\n * Perform a label mapping to translate a local MLS category bitmap to the\n * correct CIPSO category list using the given DOI definition.   Returns the\n * size in bytes of the network category bitmap on success, negative values\n * otherwise.\n *\n */\nstatic int cipso_v4_map_cat_enum_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const struct netlbl_lsm_secattr *secattr,\n\t\t\t\t      unsigned char *net_cat,\n\t\t\t\t      u32 net_cat_len)\n{\n\tint cat = -1;\n\tu32 cat_iter = 0;\n\n\tfor (;;) {\n\t\tcat = netlbl_secattr_catmap_walk(secattr->attr.mls.cat,\n\t\t\t\t\t\t cat + 1);\n\t\tif (cat < 0)\n\t\t\tbreak;\n\t\tif ((cat_iter + 2) > net_cat_len)\n\t\t\treturn -ENOSPC;\n\n\t\t*((__be16 *)&net_cat[cat_iter]) = htons(cat);\n\t\tcat_iter += 2;\n\t}\n\n\treturn cat_iter;\n}\n\n/**\n * cipso_v4_map_cat_enum_ntoh - Perform a category mapping from network to host\n * @doi_def: the DOI definition\n * @net_cat: the category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n * @secattr: the security attributes\n *\n * Description:\n * Perform a label mapping to translate a CIPSO category list to the correct\n * local MLS category bitmap using the given DOI definition.  Returns zero on\n * success, negative values on failure.\n *\n */\nstatic int cipso_v4_map_cat_enum_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const unsigned char *net_cat,\n\t\t\t\t      u32 net_cat_len,\n\t\t\t\t      struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu32 iter;\n\n\tfor (iter = 0; iter < net_cat_len; iter += 2) {\n\t\tret_val = netlbl_secattr_catmap_setbit(secattr->attr.mls.cat,\n\t\t\t\tget_unaligned_be16(&net_cat[iter]),\n\t\t\t\tGFP_ATOMIC);\n\t\tif (ret_val != 0)\n\t\t\treturn ret_val;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_map_cat_rng_valid - Checks to see if the categories are valid\n * @doi_def: the DOI definition\n * @rngcat: category list\n * @rngcat_len: length of the category list in bytes\n *\n * Description:\n * Checks the given categories against the given DOI definition and returns a\n * negative value if any of the categories do not have a valid mapping and a\n * zero value if all of the categories are valid.\n *\n */\nstatic int cipso_v4_map_cat_rng_valid(const struct cipso_v4_doi *doi_def,\n\t\t\t\t      const unsigned char *rngcat,\n\t\t\t\t      u32 rngcat_len)\n{\n\tu16 cat_high;\n\tu16 cat_low;\n\tu32 cat_prev = CIPSO_V4_MAX_REM_CATS + 1;\n\tu32 iter;\n\n\tif (doi_def->type != CIPSO_V4_MAP_PASS || rngcat_len & 0x01)\n\t\treturn -EFAULT;\n\n\tfor (iter = 0; iter < rngcat_len; iter += 4) {\n\t\tcat_high = get_unaligned_be16(&rngcat[iter]);\n\t\tif ((iter + 4) <= rngcat_len)\n\t\t\tcat_low = get_unaligned_be16(&rngcat[iter + 2]);\n\t\telse\n\t\t\tcat_low = 0;\n\n\t\tif (cat_high > cat_prev)\n\t\t\treturn -EFAULT;\n\n\t\tcat_prev = cat_low;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_map_cat_rng_hton - Perform a category mapping from host to network\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @net_cat: the zero'd out category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO category list in bytes\n *\n * Description:\n * Perform a label mapping to translate a local MLS category bitmap to the\n * correct CIPSO category list using the given DOI definition.   Returns the\n * size in bytes of the network category bitmap on success, negative values\n * otherwise.\n *\n */\nstatic int cipso_v4_map_cat_rng_hton(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const struct netlbl_lsm_secattr *secattr,\n\t\t\t\t     unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len)\n{\n\tint iter = -1;\n\tu16 array[CIPSO_V4_TAG_RNG_CAT_MAX * 2];\n\tu32 array_cnt = 0;\n\tu32 cat_size = 0;\n\n\t/* make sure we don't overflow the 'array[]' variable */\n\tif (net_cat_len >\n\t    (CIPSO_V4_OPT_LEN_MAX - CIPSO_V4_HDR_LEN - CIPSO_V4_TAG_RNG_BLEN))\n\t\treturn -ENOSPC;\n\n\tfor (;;) {\n\t\titer = netlbl_secattr_catmap_walk(secattr->attr.mls.cat,\n\t\t\t\t\t\t  iter + 1);\n\t\tif (iter < 0)\n\t\t\tbreak;\n\t\tcat_size += (iter == 0 ? 0 : sizeof(u16));\n\t\tif (cat_size > net_cat_len)\n\t\t\treturn -ENOSPC;\n\t\tarray[array_cnt++] = iter;\n\n\t\titer = netlbl_secattr_catmap_walk_rng(secattr->attr.mls.cat,\n\t\t\t\t\t\t      iter);\n\t\tif (iter < 0)\n\t\t\treturn -EFAULT;\n\t\tcat_size += sizeof(u16);\n\t\tif (cat_size > net_cat_len)\n\t\t\treturn -ENOSPC;\n\t\tarray[array_cnt++] = iter;\n\t}\n\n\tfor (iter = 0; array_cnt > 0;) {\n\t\t*((__be16 *)&net_cat[iter]) = htons(array[--array_cnt]);\n\t\titer += 2;\n\t\tarray_cnt--;\n\t\tif (array[array_cnt] != 0) {\n\t\t\t*((__be16 *)&net_cat[iter]) = htons(array[array_cnt]);\n\t\t\titer += 2;\n\t\t}\n\t}\n\n\treturn cat_size;\n}\n\n/**\n * cipso_v4_map_cat_rng_ntoh - Perform a category mapping from network to host\n * @doi_def: the DOI definition\n * @net_cat: the category list in network/CIPSO format\n * @net_cat_len: the length of the CIPSO bitmap in bytes\n * @secattr: the security attributes\n *\n * Description:\n * Perform a label mapping to translate a CIPSO category list to the correct\n * local MLS category bitmap using the given DOI definition.  Returns zero on\n * success, negative values on failure.\n *\n */\nstatic int cipso_v4_map_cat_rng_ntoh(const struct cipso_v4_doi *doi_def,\n\t\t\t\t     const unsigned char *net_cat,\n\t\t\t\t     u32 net_cat_len,\n\t\t\t\t     struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu32 net_iter;\n\tu16 cat_low;\n\tu16 cat_high;\n\n\tfor (net_iter = 0; net_iter < net_cat_len; net_iter += 4) {\n\t\tcat_high = get_unaligned_be16(&net_cat[net_iter]);\n\t\tif ((net_iter + 4) <= net_cat_len)\n\t\t\tcat_low = get_unaligned_be16(&net_cat[net_iter + 2]);\n\t\telse\n\t\t\tcat_low = 0;\n\n\t\tret_val = netlbl_secattr_catmap_setrng(secattr->attr.mls.cat,\n\t\t\t\t\t\t       cat_low,\n\t\t\t\t\t\t       cat_high,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\tif (ret_val != 0)\n\t\t\treturn ret_val;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Protocol Handling Functions\n */\n\n/**\n * cipso_v4_gentag_hdr - Generate a CIPSO option header\n * @doi_def: the DOI definition\n * @len: the total tag length in bytes, not including this header\n * @buf: the CIPSO option buffer\n *\n * Description:\n * Write a CIPSO header into the beginning of @buffer.\n *\n */\nstatic void cipso_v4_gentag_hdr(const struct cipso_v4_doi *doi_def,\n\t\t\t\tunsigned char *buf,\n\t\t\t\tu32 len)\n{\n\tbuf[0] = IPOPT_CIPSO;\n\tbuf[1] = CIPSO_V4_HDR_LEN + len;\n\t*(__be32 *)&buf[2] = htonl(doi_def->doi);\n}\n\n/**\n * cipso_v4_gentag_rbm - Generate a CIPSO restricted bitmap tag (type #1)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the restricted bitmap tag, tag type #1.  The\n * actual buffer length may be larger than the indicated size due to\n * translation between host and network category bitmaps.  Returns the size of\n * the tag on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t       const struct netlbl_lsm_secattr *secattr,\n\t\t\t       unsigned char *buffer,\n\t\t\t       u32 buffer_len)\n{\n\tint ret_val;\n\tu32 tag_len;\n\tu32 level;\n\n\tif ((secattr->flags & NETLBL_SECATTR_MLS_LVL) == 0)\n\t\treturn -EPERM;\n\n\tret_val = cipso_v4_map_lvl_hton(doi_def,\n\t\t\t\t\tsecattr->attr.mls.lvl,\n\t\t\t\t\t&level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\n\tif (secattr->flags & NETLBL_SECATTR_MLS_CAT) {\n\t\tret_val = cipso_v4_map_cat_rbm_hton(doi_def,\n\t\t\t\t\t\t    secattr,\n\t\t\t\t\t\t    &buffer[4],\n\t\t\t\t\t\t    buffer_len - 4);\n\t\tif (ret_val < 0)\n\t\t\treturn ret_val;\n\n\t\t/* This will send packets using the \"optimized\" format when\n\t\t * possible as specified in  section 3.4.2.6 of the\n\t\t * CIPSO draft. */\n\t\tif (cipso_v4_rbm_optfmt && ret_val > 0 && ret_val <= 10)\n\t\t\ttag_len = 14;\n\t\telse\n\t\t\ttag_len = 4 + ret_val;\n\t} else\n\t\ttag_len = 4;\n\n\tbuffer[0] = CIPSO_V4_TAG_RBITMAP;\n\tbuffer[1] = tag_len;\n\tbuffer[3] = level;\n\n\treturn tag_len;\n}\n\n/**\n * cipso_v4_parsetag_rbm - Parse a CIPSO restricted bitmap tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO restricted bitmap tag (tag type #1) and return the security\n * attributes in @secattr.  Return zero on success, negatives values on\n * failure.\n *\n */\nstatic int cipso_v4_parsetag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tsecattr->attr.mls.cat =\n\t\t                       netlbl_secattr_catmap_alloc(GFP_ATOMIC);\n\t\tif (secattr->attr.mls.cat == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret_val = cipso_v4_map_cat_rbm_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_secattr_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_gentag_enum - Generate a CIPSO enumerated tag (type #2)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the enumerated tag, tag type #2.  Returns the\n * size of the tag on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_enum(const struct cipso_v4_doi *doi_def,\n\t\t\t\tconst struct netlbl_lsm_secattr *secattr,\n\t\t\t\tunsigned char *buffer,\n\t\t\t\tu32 buffer_len)\n{\n\tint ret_val;\n\tu32 tag_len;\n\tu32 level;\n\n\tif (!(secattr->flags & NETLBL_SECATTR_MLS_LVL))\n\t\treturn -EPERM;\n\n\tret_val = cipso_v4_map_lvl_hton(doi_def,\n\t\t\t\t\tsecattr->attr.mls.lvl,\n\t\t\t\t\t&level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\n\tif (secattr->flags & NETLBL_SECATTR_MLS_CAT) {\n\t\tret_val = cipso_v4_map_cat_enum_hton(doi_def,\n\t\t\t\t\t\t     secattr,\n\t\t\t\t\t\t     &buffer[4],\n\t\t\t\t\t\t     buffer_len - 4);\n\t\tif (ret_val < 0)\n\t\t\treturn ret_val;\n\n\t\ttag_len = 4 + ret_val;\n\t} else\n\t\ttag_len = 4;\n\n\tbuffer[0] = CIPSO_V4_TAG_ENUM;\n\tbuffer[1] = tag_len;\n\tbuffer[3] = level;\n\n\treturn tag_len;\n}\n\n/**\n * cipso_v4_parsetag_enum - Parse a CIPSO enumerated tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO enumerated tag (tag type #2) and return the security\n * attributes in @secattr.  Return zero on success, negatives values on\n * failure.\n *\n */\nstatic int cipso_v4_parsetag_enum(const struct cipso_v4_doi *doi_def,\n\t\t\t\t  const unsigned char *tag,\n\t\t\t\t  struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tsecattr->attr.mls.cat =\n\t\t\t               netlbl_secattr_catmap_alloc(GFP_ATOMIC);\n\t\tif (secattr->attr.mls.cat == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret_val = cipso_v4_map_cat_enum_ntoh(doi_def,\n\t\t\t\t\t\t     &tag[4],\n\t\t\t\t\t\t     tag_len - 4,\n\t\t\t\t\t\t     secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_secattr_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_gentag_rng - Generate a CIPSO ranged tag (type #5)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the ranged tag, tag type #5.  Returns the\n * size of the tag on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t       const struct netlbl_lsm_secattr *secattr,\n\t\t\t       unsigned char *buffer,\n\t\t\t       u32 buffer_len)\n{\n\tint ret_val;\n\tu32 tag_len;\n\tu32 level;\n\n\tif (!(secattr->flags & NETLBL_SECATTR_MLS_LVL))\n\t\treturn -EPERM;\n\n\tret_val = cipso_v4_map_lvl_hton(doi_def,\n\t\t\t\t\tsecattr->attr.mls.lvl,\n\t\t\t\t\t&level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\n\tif (secattr->flags & NETLBL_SECATTR_MLS_CAT) {\n\t\tret_val = cipso_v4_map_cat_rng_hton(doi_def,\n\t\t\t\t\t\t    secattr,\n\t\t\t\t\t\t    &buffer[4],\n\t\t\t\t\t\t    buffer_len - 4);\n\t\tif (ret_val < 0)\n\t\t\treturn ret_val;\n\n\t\ttag_len = 4 + ret_val;\n\t} else\n\t\ttag_len = 4;\n\n\tbuffer[0] = CIPSO_V4_TAG_RANGE;\n\tbuffer[1] = tag_len;\n\tbuffer[3] = level;\n\n\treturn tag_len;\n}\n\n/**\n * cipso_v4_parsetag_rng - Parse a CIPSO ranged tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO ranged tag (tag type #5) and return the security attributes\n * in @secattr.  Return zero on success, negatives values on failure.\n *\n */\nstatic int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tsecattr->attr.mls.cat =\n\t\t\t               netlbl_secattr_catmap_alloc(GFP_ATOMIC);\n\t\tif (secattr->attr.mls.cat == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_secattr_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_gentag_loc - Generate a CIPSO local tag (non-standard)\n * @doi_def: the DOI definition\n * @secattr: the security attributes\n * @buffer: the option buffer\n * @buffer_len: length of buffer in bytes\n *\n * Description:\n * Generate a CIPSO option using the local tag.  Returns the size of the tag\n * on success, negative values on failure.\n *\n */\nstatic int cipso_v4_gentag_loc(const struct cipso_v4_doi *doi_def,\n\t\t\t       const struct netlbl_lsm_secattr *secattr,\n\t\t\t       unsigned char *buffer,\n\t\t\t       u32 buffer_len)\n{\n\tif (!(secattr->flags & NETLBL_SECATTR_SECID))\n\t\treturn -EPERM;\n\n\tbuffer[0] = CIPSO_V4_TAG_LOCAL;\n\tbuffer[1] = CIPSO_V4_TAG_LOC_BLEN;\n\t*(u32 *)&buffer[2] = secattr->attr.secid;\n\n\treturn CIPSO_V4_TAG_LOC_BLEN;\n}\n\n/**\n * cipso_v4_parsetag_loc - Parse a CIPSO local tag\n * @doi_def: the DOI definition\n * @tag: the CIPSO tag\n * @secattr: the security attributes\n *\n * Description:\n * Parse a CIPSO local tag and return the security attributes in @secattr.\n * Return zero on success, negatives values on failure.\n *\n */\nstatic int cipso_v4_parsetag_loc(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tsecattr->attr.secid = *(u32 *)&tag[2];\n\tsecattr->flags |= NETLBL_SECATTR_SECID;\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_validate - Validate a CIPSO option\n * @option: the start of the option, on error it is set to point to the error\n *\n * Description:\n * This routine is called to validate a CIPSO option, it checks all of the\n * fields to ensure that they are at least valid, see the draft snippet below\n * for details.  If the option is valid then a zero value is returned and\n * the value of @option is unchanged.  If the option is invalid then a\n * non-zero value is returned and @option is adjusted to point to the\n * offending portion of the option.  From the IETF draft ...\n *\n *  \"If any field within the CIPSO options, such as the DOI identifier, is not\n *   recognized the IP datagram is discarded and an ICMP 'parameter problem'\n *   (type 12) is generated and returned.  The ICMP code field is set to 'bad\n *   parameter' (code 0) and the pointer is set to the start of the CIPSO field\n *   that is unrecognized.\"\n *\n */\nint cipso_v4_validate(const struct sk_buff *skb, unsigned char **option)\n{\n\tunsigned char *opt = *option;\n\tunsigned char *tag;\n\tunsigned char opt_iter;\n\tunsigned char err_offset = 0;\n\tu8 opt_len;\n\tu8 tag_len;\n\tstruct cipso_v4_doi *doi_def = NULL;\n\tu32 tag_iter;\n\n\t/* caller already checks for length values that are too large */\n\topt_len = opt[1];\n\tif (opt_len < 8) {\n\t\terr_offset = 1;\n\t\tgoto validate_return;\n\t}\n\n\trcu_read_lock();\n\tdoi_def = cipso_v4_doi_search(get_unaligned_be32(&opt[2]));\n\tif (doi_def == NULL) {\n\t\terr_offset = 2;\n\t\tgoto validate_return_locked;\n\t}\n\n\topt_iter = CIPSO_V4_HDR_LEN;\n\ttag = opt + opt_iter;\n\twhile (opt_iter < opt_len) {\n\t\tfor (tag_iter = 0; doi_def->tags[tag_iter] != tag[0];)\n\t\t\tif (doi_def->tags[tag_iter] == CIPSO_V4_TAG_INVALID ||\n\t\t\t    ++tag_iter == CIPSO_V4_TAG_MAXCNT) {\n\t\t\t\terr_offset = opt_iter;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\ttag_len = tag[1];\n\t\tif (tag_len > (opt_len - opt_iter)) {\n\t\t\terr_offset = opt_iter + 1;\n\t\t\tgoto validate_return_locked;\n\t\t}\n\n\t\tswitch (tag[0]) {\n\t\tcase CIPSO_V4_TAG_RBITMAP:\n\t\t\tif (tag_len < CIPSO_V4_TAG_RBM_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\t\t/* We are already going to do all the verification\n\t\t\t * necessary at the socket layer so from our point of\n\t\t\t * view it is safe to turn these checks off (and less\n\t\t\t * work), however, the CIPSO draft says we should do\n\t\t\t * all the CIPSO validations here but it doesn't\n\t\t\t * really specify _exactly_ what we need to validate\n\t\t\t * ... so, just make it a sysctl tunable. */\n\t\t\tif (cipso_v4_rbm_strictvalid) {\n\t\t\t\tif (cipso_v4_map_lvl_valid(doi_def,\n\t\t\t\t\t\t\t   tag[3]) < 0) {\n\t\t\t\t\terr_offset = opt_iter + 3;\n\t\t\t\t\tgoto validate_return_locked;\n\t\t\t\t}\n\t\t\t\tif (tag_len > CIPSO_V4_TAG_RBM_BLEN &&\n\t\t\t\t    cipso_v4_map_cat_rbm_valid(doi_def,\n\t\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t\t    tag_len - 4) < 0) {\n\t\t\t\t\terr_offset = opt_iter + 4;\n\t\t\t\t\tgoto validate_return_locked;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_ENUM:\n\t\t\tif (tag_len < CIPSO_V4_TAG_ENUM_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\t\tif (cipso_v4_map_lvl_valid(doi_def,\n\t\t\t\t\t\t   tag[3]) < 0) {\n\t\t\t\terr_offset = opt_iter + 3;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tif (tag_len > CIPSO_V4_TAG_ENUM_BLEN &&\n\t\t\t    cipso_v4_map_cat_enum_valid(doi_def,\n\t\t\t\t\t\t\t&tag[4],\n\t\t\t\t\t\t\ttag_len - 4) < 0) {\n\t\t\t\terr_offset = opt_iter + 4;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_RANGE:\n\t\t\tif (tag_len < CIPSO_V4_TAG_RNG_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\n\t\t\tif (cipso_v4_map_lvl_valid(doi_def,\n\t\t\t\t\t\t   tag[3]) < 0) {\n\t\t\t\terr_offset = opt_iter + 3;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tif (tag_len > CIPSO_V4_TAG_RNG_BLEN &&\n\t\t\t    cipso_v4_map_cat_rng_valid(doi_def,\n\t\t\t\t\t\t       &tag[4],\n\t\t\t\t\t\t       tag_len - 4) < 0) {\n\t\t\t\terr_offset = opt_iter + 4;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_LOCAL:\n\t\t\t/* This is a non-standard tag that we only allow for\n\t\t\t * local connections, so if the incoming interface is\n\t\t\t * not the loopback device drop the packet. */\n\t\t\tif (!(skb->dev->flags & IFF_LOOPBACK)) {\n\t\t\t\terr_offset = opt_iter;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tif (tag_len != CIPSO_V4_TAG_LOC_BLEN) {\n\t\t\t\terr_offset = opt_iter + 1;\n\t\t\t\tgoto validate_return_locked;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr_offset = opt_iter;\n\t\t\tgoto validate_return_locked;\n\t\t}\n\n\t\ttag += tag_len;\n\t\topt_iter += tag_len;\n\t}\n\nvalidate_return_locked:\n\trcu_read_unlock();\nvalidate_return:\n\t*option = opt + err_offset;\n\treturn err_offset;\n}\n\n/**\n * cipso_v4_error - Send the correct response for a bad packet\n * @skb: the packet\n * @error: the error code\n * @gateway: CIPSO gateway flag\n *\n * Description:\n * Based on the error code given in @error, send an ICMP error message back to\n * the originating host.  From the IETF draft ...\n *\n *  \"If the contents of the CIPSO [option] are valid but the security label is\n *   outside of the configured host or port label range, the datagram is\n *   discarded and an ICMP 'destination unreachable' (type 3) is generated and\n *   returned.  The code field of the ICMP is set to 'communication with\n *   destination network administratively prohibited' (code 9) or to\n *   'communication with destination host administratively prohibited'\n *   (code 10).  The value of the code is dependent on whether the originator\n *   of the ICMP message is acting as a CIPSO host or a CIPSO gateway.  The\n *   recipient of the ICMP message MUST be able to handle either value.  The\n *   same procedure is performed if a CIPSO [option] can not be added to an\n *   IP packet because it is too large to fit in the IP options area.\"\n *\n *  \"If the error is triggered by receipt of an ICMP message, the message is\n *   discarded and no response is permitted (consistent with general ICMP\n *   processing rules).\"\n *\n */\nvoid cipso_v4_error(struct sk_buff *skb, int error, u32 gateway)\n{\n\tif (ip_hdr(skb)->protocol == IPPROTO_ICMP || error != -EACCES)\n\t\treturn;\n\n\tif (gateway)\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_NET_ANO, 0);\n\telse\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_HOST_ANO, 0);\n}\n\n/**\n * cipso_v4_genopt - Generate a CIPSO option\n * @buf: the option buffer\n * @buf_len: the size of opt_buf\n * @doi_def: the CIPSO DOI to use\n * @secattr: the security attributes\n *\n * Description:\n * Generate a CIPSO option using the DOI definition and security attributes\n * passed to the function.  Returns the length of the option on success and\n * negative values on failure.\n *\n */\nstatic int cipso_v4_genopt(unsigned char *buf, u32 buf_len,\n\t\t\t   const struct cipso_v4_doi *doi_def,\n\t\t\t   const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu32 iter;\n\n\tif (buf_len <= CIPSO_V4_HDR_LEN)\n\t\treturn -ENOSPC;\n\n\t/* XXX - This code assumes only one tag per CIPSO option which isn't\n\t * really a good assumption to make but since we only support the MAC\n\t * tags right now it is a safe assumption. */\n\titer = 0;\n\tdo {\n\t\tmemset(buf, 0, buf_len);\n\t\tswitch (doi_def->tags[iter]) {\n\t\tcase CIPSO_V4_TAG_RBITMAP:\n\t\t\tret_val = cipso_v4_gentag_rbm(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_ENUM:\n\t\t\tret_val = cipso_v4_gentag_enum(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_RANGE:\n\t\t\tret_val = cipso_v4_gentag_rng(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tcase CIPSO_V4_TAG_LOCAL:\n\t\t\tret_val = cipso_v4_gentag_loc(doi_def,\n\t\t\t\t\t\t   secattr,\n\t\t\t\t\t\t   &buf[CIPSO_V4_HDR_LEN],\n\t\t\t\t\t\t   buf_len - CIPSO_V4_HDR_LEN);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EPERM;\n\t\t}\n\n\t\titer++;\n\t} while (ret_val < 0 &&\n\t\t iter < CIPSO_V4_TAG_MAXCNT &&\n\t\t doi_def->tags[iter] != CIPSO_V4_TAG_INVALID);\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\tcipso_v4_gentag_hdr(doi_def, buf, ret_val);\n\treturn CIPSO_V4_HDR_LEN + ret_val;\n}\n\nstatic void opt_kfree_rcu(struct rcu_head *head)\n{\n\tkfree(container_of(head, struct ip_options_rcu, rcu));\n}\n\n/**\n * cipso_v4_sock_setattr - Add a CIPSO option to a socket\n * @sk: the socket\n * @doi_def: the CIPSO DOI to use\n * @secattr: the specific security attributes of the socket\n *\n * Description:\n * Set the CIPSO option on the given socket using the DOI definition and\n * security attributes passed to the function.  This function requires\n * exclusive access to @sk, which means it either needs to be in the\n * process of being created or locked.  Returns zero on success and negative\n * values on failure.\n *\n */\nint cipso_v4_sock_setattr(struct sock *sk,\n\t\t\t  const struct cipso_v4_doi *doi_def,\n\t\t\t  const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *old, *opt = NULL;\n\tstruct inet_sock *sk_inet;\n\tstruct inet_connection_sock *sk_conn;\n\n\t/* In the case of sock_create_lite(), the sock->sk field is not\n\t * defined yet but it is not a problem as the only users of these\n\t * \"lite\" PF_INET sockets are functions which do an accept() call\n\t * afterwards so we will label the socket as part of the accept(). */\n\tif (sk == NULL)\n\t\treturn 0;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto socket_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\tsk_inet = inet_sk(sk);\n\n\told = rcu_dereference_protected(sk_inet->inet_opt, sock_owned_by_user(sk));\n\tif (sk_inet->is_icsk) {\n\t\tsk_conn = inet_csk(sk);\n\t\tif (old)\n\t\t\tsk_conn->icsk_ext_hdr_len -= old->opt.optlen;\n\t\tsk_conn->icsk_ext_hdr_len += opt->opt.optlen;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n\trcu_assign_pointer(sk_inet->inet_opt, opt);\n\tif (old)\n\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nsocket_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_req_setattr - Add a CIPSO option to a connection request socket\n * @req: the connection request socket\n * @doi_def: the CIPSO DOI to use\n * @secattr: the specific security attributes of the socket\n *\n * Description:\n * Set the CIPSO option on the given socket using the DOI definition and\n * security attributes passed to the function.  Returns zero on success and\n * negative values on failure.\n *\n */\nint cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tif (opt)\n\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_delopt - Delete the CIPSO option from a set of IP options\n * @opt_ptr: IP option pointer\n *\n * Description:\n * Deletes the CIPSO IP option from a set of IP options and makes the necessary\n * adjustments to the IP option structure.  Returns zero on success, negative\n * values on failure.\n *\n */\nstatic int cipso_v4_delopt(struct ip_options_rcu **opt_ptr)\n{\n\tint hdr_delta = 0;\n\tstruct ip_options_rcu *opt = *opt_ptr;\n\n\tif (opt->opt.srr || opt->opt.rr || opt->opt.ts || opt->opt.router_alert) {\n\t\tu8 cipso_len;\n\t\tu8 cipso_off;\n\t\tunsigned char *cipso_ptr;\n\t\tint iter;\n\t\tint optlen_new;\n\n\t\tcipso_off = opt->opt.cipso - sizeof(struct iphdr);\n\t\tcipso_ptr = &opt->opt.__data[cipso_off];\n\t\tcipso_len = cipso_ptr[1];\n\n\t\tif (opt->opt.srr > opt->opt.cipso)\n\t\t\topt->opt.srr -= cipso_len;\n\t\tif (opt->opt.rr > opt->opt.cipso)\n\t\t\topt->opt.rr -= cipso_len;\n\t\tif (opt->opt.ts > opt->opt.cipso)\n\t\t\topt->opt.ts -= cipso_len;\n\t\tif (opt->opt.router_alert > opt->opt.cipso)\n\t\t\topt->opt.router_alert -= cipso_len;\n\t\topt->opt.cipso = 0;\n\n\t\tmemmove(cipso_ptr, cipso_ptr + cipso_len,\n\t\t\topt->opt.optlen - cipso_off - cipso_len);\n\n\t\t/* determining the new total option length is tricky because of\n\t\t * the padding necessary, the only thing i can think to do at\n\t\t * this point is walk the options one-by-one, skipping the\n\t\t * padding at the end to determine the actual option size and\n\t\t * from there we can determine the new total option length */\n\t\titer = 0;\n\t\toptlen_new = 0;\n\t\twhile (iter < opt->opt.optlen)\n\t\t\tif (opt->opt.__data[iter] != IPOPT_NOP) {\n\t\t\t\titer += opt->opt.__data[iter + 1];\n\t\t\t\toptlen_new = iter;\n\t\t\t} else\n\t\t\t\titer++;\n\t\thdr_delta = opt->opt.optlen;\n\t\topt->opt.optlen = (optlen_new + 3) & ~3;\n\t\thdr_delta -= opt->opt.optlen;\n\t} else {\n\t\t/* only the cipso option was present on the socket so we can\n\t\t * remove the entire option struct */\n\t\t*opt_ptr = NULL;\n\t\thdr_delta = opt->opt.optlen;\n\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n\t}\n\n\treturn hdr_delta;\n}\n\n/**\n * cipso_v4_sock_delattr - Delete the CIPSO option from a socket\n * @sk: the socket\n *\n * Description:\n * Removes the CIPSO option from a socket, if present.\n *\n */\nvoid cipso_v4_sock_delattr(struct sock *sk)\n{\n\tint hdr_delta;\n\tstruct ip_options_rcu *opt;\n\tstruct inet_sock *sk_inet;\n\n\tsk_inet = inet_sk(sk);\n\topt = rcu_dereference_protected(sk_inet->inet_opt, 1);\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\thdr_delta = cipso_v4_delopt(&sk_inet->inet_opt);\n\tif (sk_inet->is_icsk && hdr_delta > 0) {\n\t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n\t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n}\n\n/**\n * cipso_v4_req_delattr - Delete the CIPSO option from a request socket\n * @reg: the request socket\n *\n * Description:\n * Removes the CIPSO option from a request socket, if present.\n *\n */\nvoid cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options_rcu *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}\n\n/**\n * cipso_v4_getattr - Helper function for the cipso_v4_*_getattr functions\n * @cipso: the CIPSO v4 option\n * @secattr: the security attributes\n *\n * Description:\n * Inspect @cipso and return the security attributes in @secattr.  Returns zero\n * on success and negative values on failure.\n *\n */\nstatic int cipso_v4_getattr(const unsigned char *cipso,\n\t\t\t    struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -ENOMSG;\n\tu32 doi;\n\tstruct cipso_v4_doi *doi_def;\n\n\tif (cipso_v4_cache_check(cipso, cipso[1], secattr) == 0)\n\t\treturn 0;\n\n\tdoi = get_unaligned_be32(&cipso[2]);\n\trcu_read_lock();\n\tdoi_def = cipso_v4_doi_search(doi);\n\tif (doi_def == NULL)\n\t\tgoto getattr_return;\n\t/* XXX - This code assumes only one tag per CIPSO option which isn't\n\t * really a good assumption to make but since we only support the MAC\n\t * tags right now it is a safe assumption. */\n\tswitch (cipso[6]) {\n\tcase CIPSO_V4_TAG_RBITMAP:\n\t\tret_val = cipso_v4_parsetag_rbm(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\tcase CIPSO_V4_TAG_ENUM:\n\t\tret_val = cipso_v4_parsetag_enum(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\tcase CIPSO_V4_TAG_RANGE:\n\t\tret_val = cipso_v4_parsetag_rng(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\tcase CIPSO_V4_TAG_LOCAL:\n\t\tret_val = cipso_v4_parsetag_loc(doi_def, &cipso[6], secattr);\n\t\tbreak;\n\t}\n\tif (ret_val == 0)\n\t\tsecattr->type = NETLBL_NLTYPE_CIPSOV4;\n\ngetattr_return:\n\trcu_read_unlock();\n\treturn ret_val;\n}\n\n/**\n * cipso_v4_sock_getattr - Get the security attributes from a sock\n * @sk: the sock\n * @secattr: the security attributes\n *\n * Description:\n * Query @sk to see if there is a CIPSO option attached to the sock and if\n * there is return the CIPSO security attributes in @secattr.  This function\n * requires that @sk be locked, or privately held, but it does not do any\n * locking itself.  Returns zero on success and negative values on failure.\n *\n */\nint cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n{\n\tstruct ip_options_rcu *opt;\n\tint res = -ENOMSG;\n\n\trcu_read_lock();\n\topt = rcu_dereference(inet_sk(sk)->inet_opt);\n\tif (opt && opt->opt.cipso)\n\t\tres = cipso_v4_getattr(opt->opt.__data +\n\t\t\t\t\t\topt->opt.cipso -\n\t\t\t\t\t\tsizeof(struct iphdr),\n\t\t\t\t       secattr);\n\trcu_read_unlock();\n\treturn res;\n}\n\n/**\n * cipso_v4_skbuff_setattr - Set the CIPSO option on a packet\n * @skb: the packet\n * @secattr: the security attributes\n *\n * Description:\n * Set the CIPSO option on the given packet based on the security attributes.\n * Returns a pointer to the IP header on success and NULL on failure.\n *\n */\nint cipso_v4_skbuff_setattr(struct sk_buff *skb,\n\t\t\t    const struct cipso_v4_doi *doi_def,\n\t\t\t    const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tstruct iphdr *iph;\n\tstruct ip_options *opt = &IPCB(skb)->opt;\n\tunsigned char buf[CIPSO_V4_OPT_LEN_MAX];\n\tu32 buf_len = CIPSO_V4_OPT_LEN_MAX;\n\tu32 opt_len;\n\tint len_delta;\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\tbuf_len = ret_val;\n\topt_len = (buf_len + 3) & ~3;\n\n\t/* we overwrite any existing options to ensure that we have enough\n\t * room for the CIPSO option, the reason is that we _need_ to guarantee\n\t * that the security label is applied to the packet - we do the same\n\t * thing when using the socket options and it hasn't caused a problem,\n\t * if we need to we can always revisit this choice later */\n\n\tlen_delta = opt_len - opt->optlen;\n\t/* if we don't ensure enough headroom we could panic on the skb_push()\n\t * call below so make sure we have enough, we are also \"mangling\" the\n\t * packet so we should probably do a copy-on-write call anyway */\n\tret_val = skb_cow(skb, skb_headroom(skb) + len_delta);\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\n\tif (len_delta > 0) {\n\t\t/* we assume that the header + opt->optlen have already been\n\t\t * \"pushed\" in ip_options_build() or similar */\n\t\tiph = ip_hdr(skb);\n\t\tskb_push(skb, len_delta);\n\t\tmemmove((char *)iph - len_delta, iph, iph->ihl << 2);\n\t\tskb_reset_network_header(skb);\n\t\tiph = ip_hdr(skb);\n\t} else if (len_delta < 0) {\n\t\tiph = ip_hdr(skb);\n\t\tmemset(iph + 1, IPOPT_NOP, opt->optlen);\n\t} else\n\t\tiph = ip_hdr(skb);\n\n\tif (opt->optlen > 0)\n\t\tmemset(opt, 0, sizeof(*opt));\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\topt->is_changed = 1;\n\n\t/* we have to do the following because we are being called from a\n\t * netfilter hook which means the packet already has had the header\n\t * fields populated and the checksum calculated - yes this means we\n\t * are doing more work than needed but we do it to keep the core\n\t * stack clean and tidy */\n\tmemcpy(iph + 1, buf, buf_len);\n\tif (opt_len > buf_len)\n\t\tmemset((char *)(iph + 1) + buf_len, 0, opt_len - buf_len);\n\tif (len_delta != 0) {\n\t\tiph->ihl = 5 + (opt_len >> 2);\n\t\tiph->tot_len = htons(skb->len);\n\t}\n\tip_send_check(iph);\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_skbuff_delattr - Delete any CIPSO options from a packet\n * @skb: the packet\n *\n * Description:\n * Removes any and all CIPSO options from the given packet.  Returns zero on\n * success, negative values on failure.\n *\n */\nint cipso_v4_skbuff_delattr(struct sk_buff *skb)\n{\n\tint ret_val;\n\tstruct iphdr *iph;\n\tstruct ip_options *opt = &IPCB(skb)->opt;\n\tunsigned char *cipso_ptr;\n\n\tif (opt->cipso == 0)\n\t\treturn 0;\n\n\t/* since we are changing the packet we should make a copy */\n\tret_val = skb_cow(skb, skb_headroom(skb));\n\tif (ret_val < 0)\n\t\treturn ret_val;\n\n\t/* the easiest thing to do is just replace the cipso option with noop\n\t * options since we don't change the size of the packet, although we\n\t * still need to recalculate the checksum */\n\n\tiph = ip_hdr(skb);\n\tcipso_ptr = (unsigned char *)iph + opt->cipso;\n\tmemset(cipso_ptr, IPOPT_NOOP, cipso_ptr[1]);\n\topt->cipso = 0;\n\topt->is_changed = 1;\n\n\tip_send_check(iph);\n\n\treturn 0;\n}\n\n/**\n * cipso_v4_skbuff_getattr - Get the security attributes from the CIPSO option\n * @skb: the packet\n * @secattr: the security attributes\n *\n * Description:\n * Parse the given packet's CIPSO option and return the security attributes.\n * Returns zero on success and negative values on failure.\n *\n */\nint cipso_v4_skbuff_getattr(const struct sk_buff *skb,\n\t\t\t    struct netlbl_lsm_secattr *secattr)\n{\n\treturn cipso_v4_getattr(CIPSO_V4_OPTPTR(skb), secattr);\n}\n\n/*\n * Setup Functions\n */\n\n/**\n * cipso_v4_init - Initialize the CIPSO module\n *\n * Description:\n * Initialize the CIPSO module and prepare it for use.  Returns zero on success\n * and negative values on failure.\n *\n */\nstatic int __init cipso_v4_init(void)\n{\n\tint ret_val;\n\n\tret_val = cipso_v4_cache_init();\n\tif (ret_val != 0)\n\t\tpanic(\"Failed to initialize the CIPSO/IPv4 cache (%d)\\n\",\n\t\t      ret_val);\n\n\treturn 0;\n}\n\nsubsys_initcall(cipso_v4_init);\n", "/*\n *\tNET3:\tImplementation of the ICMP protocol layer.\n *\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *\tSome of the function names and the icmp unreach table for this\n *\tmodule were derived from [icmp.c 1.0.11 06/02/93] by\n *\tRoss Biro, Fred N. van Kempen, Mark Evans, Alan Cox, Gerhard Koerting.\n *\tOther than that this module is a complete rewrite.\n *\n *\tFixes:\n *\tClemens Fruhwirth\t:\tintroduce global icmp rate limiting\n *\t\t\t\t\twith icmp type masking ability instead\n *\t\t\t\t\tof broken per type icmp timeouts.\n *\t\tMike Shaver\t:\tRFC1122 checks.\n *\t\tAlan Cox\t:\tMulticast ping reply as self.\n *\t\tAlan Cox\t:\tFix atomicity lockup in ip_build_xmit\n *\t\t\t\t\tcall.\n *\t\tAlan Cox\t:\tAdded 216,128 byte paths to the MTU\n *\t\t\t\t\tcode.\n *\t\tMartin Mares\t:\tRFC1812 checks.\n *\t\tMartin Mares\t:\tCan be configured to follow redirects\n *\t\t\t\t\tif acting as a router _without_ a\n *\t\t\t\t\trouting protocol (RFC 1812).\n *\t\tMartin Mares\t:\tEcho requests may be configured to\n *\t\t\t\t\tbe ignored (RFC 1812).\n *\t\tMartin Mares\t:\tLimitation of ICMP error message\n *\t\t\t\t\ttransmit rate (RFC 1812).\n *\t\tMartin Mares\t:\tTOS and Precedence set correctly\n *\t\t\t\t\t(RFC 1812).\n *\t\tMartin Mares\t:\tNow copying as much data from the\n *\t\t\t\t\toriginal packet as we can without\n *\t\t\t\t\texceeding 576 bytes (RFC 1812).\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tKeith Owens\t:\tRFC1191 correction for 4.2BSD based\n *\t\t\t\t\tpath MTU bug.\n *\t\tThomas Quinot\t:\tICMP Dest Unreach codes up to 15 are\n *\t\t\t\t\tvalid (RFC 1812).\n *\t\tAndi Kleen\t:\tCheck all packet lengths properly\n *\t\t\t\t\tand moved all kfree_skb() up to\n *\t\t\t\t\ticmp_rcv.\n *\t\tAndi Kleen\t:\tMove the rate limit bookkeeping\n *\t\t\t\t\tinto the dest entry and use a token\n *\t\t\t\t\tbucket filter (thanks to ANK). Make\n *\t\t\t\t\tthe rates sysctl configurable.\n *\t\tYu Tianli\t:\tFixed two ugly bugs in icmp_send\n *\t\t\t\t\t- IP option length was accounted wrongly\n *\t\t\t\t\t- ICMP header length was not accounted\n *\t\t\t\t\t  at all.\n *              Tristan Greaves :       Added sysctl option to ignore bogus\n *              \t\t\tbroadcast responses from broken routers.\n *\n * To Fix:\n *\n *\t- Should use skb_pull() instead of all the manual checking.\n *\t  This would also greatly simply some upper layer error handlers. --AK\n *\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <linux/string.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/slab.h>\n#include <net/snmp.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/protocol.h>\n#include <net/icmp.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/raw.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/init.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <net/inet_common.h>\n\n/*\n *\tBuild xmit assembly blocks\n */\n\nstruct icmp_bxm {\n\tstruct sk_buff *skb;\n\tint offset;\n\tint data_len;\n\n\tstruct {\n\t\tstruct icmphdr icmph;\n\t\t__be32\t       times[3];\n\t} data;\n\tint head_len;\n\tstruct ip_options_data replyopts;\n};\n\n/* An array of errno for error messages from dest unreach. */\n/* RFC 1122: 3.2.2.1 States that NET_UNREACH, HOST_UNREACH and SR_FAILED MUST be considered 'transient errs'. */\n\nconst struct icmp_err icmp_err_convert[] = {\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_UNREACH */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_HOST_UNREACH */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = ENOPROTOOPT\t/* ICMP_PROT_UNREACH */,\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ECONNREFUSED,\t/* ICMP_PORT_UNREACH */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EMSGSIZE,\t/* ICMP_FRAG_NEEDED */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EOPNOTSUPP,\t/* ICMP_SR_FAILED */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_UNKNOWN */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTDOWN,\t/* ICMP_HOST_UNKNOWN */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ENONET,\t/* ICMP_HOST_ISOLATED */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_ANO\t*/\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_HOST_ANO */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = ENETUNREACH,\t/* ICMP_NET_UNR_TOS */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_HOST_UNR_TOS */\n\t\t.fatal = 0,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_PKT_FILTERED */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_PREC_VIOLATION */\n\t\t.fatal = 1,\n\t},\n\t{\n\t\t.errno = EHOSTUNREACH,\t/* ICMP_PREC_CUTOFF */\n\t\t.fatal = 1,\n\t},\n};\nEXPORT_SYMBOL(icmp_err_convert);\n\n/*\n *\tICMP control array. This specifies what to do with each ICMP.\n */\n\nstruct icmp_control {\n\tvoid (*handler)(struct sk_buff *skb);\n\tshort   error;\t\t/* This ICMP is classed as an error message */\n};\n\nstatic const struct icmp_control icmp_pointers[NR_ICMP_TYPES+1];\n\n/*\n *\tThe ICMP socket(s). This is the most convenient way to flow control\n *\tour ICMP output as well as maintain a clean interface throughout\n *\tall layers. All Socketless IP sends will soon be gone.\n *\n *\tOn SMP we have one ICMP socket per-cpu.\n */\nstatic struct sock *icmp_sk(struct net *net)\n{\n\treturn net->ipv4.icmp_sk[smp_processor_id()];\n}\n\nstatic inline struct sock *icmp_xmit_lock(struct net *net)\n{\n\tstruct sock *sk;\n\n\tlocal_bh_disable();\n\n\tsk = icmp_sk(net);\n\n\tif (unlikely(!spin_trylock(&sk->sk_lock.slock))) {\n\t\t/* This can happen if the output path signals a\n\t\t * dst_link_failure() for an outgoing ICMP packet.\n\t\t */\n\t\tlocal_bh_enable();\n\t\treturn NULL;\n\t}\n\treturn sk;\n}\n\nstatic inline void icmp_xmit_unlock(struct sock *sk)\n{\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\n\n/*\n *\tSend an ICMP frame.\n */\n\nstatic inline bool icmpv4_xrlim_allow(struct net *net, struct rtable *rt,\n\t\tint type, int code)\n{\n\tstruct dst_entry *dst = &rt->dst;\n\tbool rc = true;\n\n\tif (type > NR_ICMP_TYPES)\n\t\tgoto out;\n\n\t/* Don't limit PMTU discovery. */\n\tif (type == ICMP_DEST_UNREACH && code == ICMP_FRAG_NEEDED)\n\t\tgoto out;\n\n\t/* No rate limit on loopback */\n\tif (dst->dev && (dst->dev->flags&IFF_LOOPBACK))\n\t\tgoto out;\n\n\t/* Limit if icmp type is enabled in ratemask. */\n\tif ((1 << type) & net->ipv4.sysctl_icmp_ratemask) {\n\t\tif (!rt->peer)\n\t\t\trt_bind_peer(rt, 1);\n\t\trc = inet_peer_xrlim_allow(rt->peer,\n\t\t\t\t\t   net->ipv4.sysctl_icmp_ratelimit);\n\t}\nout:\n\treturn rc;\n}\n\n/*\n *\tMaintain the counters used in the SNMP statistics for outgoing ICMP\n */\nvoid icmp_out_count(struct net *net, unsigned char type)\n{\n\tICMPMSGOUT_INC_STATS(net, type);\n\tICMP_INC_STATS(net, ICMP_MIB_OUTMSGS);\n}\n\n/*\n *\tChecksum each fragment, and on the first include the headers and final\n *\tchecksum.\n */\nstatic int icmp_glue_bits(void *from, char *to, int offset, int len, int odd,\n\t\t\t  struct sk_buff *skb)\n{\n\tstruct icmp_bxm *icmp_param = (struct icmp_bxm *)from;\n\t__wsum csum;\n\n\tcsum = skb_copy_and_csum_bits(icmp_param->skb,\n\t\t\t\t      icmp_param->offset + offset,\n\t\t\t\t      to, len, 0);\n\n\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\tif (icmp_pointers[icmp_param->data.icmph.type].error)\n\t\tnf_ct_attach(skb, icmp_param->skb);\n\treturn 0;\n}\n\nstatic void icmp_push_reply(struct icmp_bxm *icmp_param,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rt)\n{\n\tstruct sock *sk;\n\tstruct sk_buff *skb;\n\n\tsk = icmp_sk(dev_net((*rt)->dst.dev));\n\tif (ip_append_data(sk, icmp_glue_bits, icmp_param,\n\t\t\t   icmp_param->data_len+icmp_param->head_len,\n\t\t\t   icmp_param->head_len,\n\t\t\t   ipc, rt, MSG_DONTWAIT) < 0) {\n\t\tICMP_INC_STATS_BH(sock_net(sk), ICMP_MIB_OUTERRORS);\n\t\tip_flush_pending_frames(sk);\n\t} else if ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {\n\t\tstruct icmphdr *icmph = icmp_hdr(skb);\n\t\t__wsum csum = 0;\n\t\tstruct sk_buff *skb1;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb1) {\n\t\t\tcsum = csum_add(csum, skb1->csum);\n\t\t}\n\t\tcsum = csum_partial_copy_nocheck((void *)&icmp_param->data,\n\t\t\t\t\t\t (char *)icmph,\n\t\t\t\t\t\t icmp_param->head_len, csum);\n\t\ticmph->checksum = csum_fold(csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk);\n\t}\n}\n\n/*\n *\tDriving logic for building and sending ICMP messages.\n */\n\nstatic void icmp_reply(struct icmp_bxm *icmp_param, struct sk_buff *skb)\n{\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\tstruct sock *sk;\n\tstruct inet_sock *inet;\n\t__be32 daddr;\n\n\tif (ip_options_echo(&icmp_param->replyopts.opt.opt, skb))\n\t\treturn;\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\tinet = inet_sk(sk);\n\n\ticmp_param->data.icmph.checksum = 0;\n\n\tinet->tos = ip_hdr(skb)->tos;\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tif (icmp_param->replyopts.opt.opt.optlen) {\n\t\tipc.opt = &icmp_param->replyopts.opt;\n\t\tif (ipc.opt->opt.srr)\n\t\t\tdaddr = icmp_param->replyopts.opt.opt.faddr;\n\t}\n\t{\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = daddr,\n\t\t\t.saddr = rt->rt_spec_dst,\n\t\t\t.flowi4_tos = RT_TOS(ip_hdr(skb)->tos),\n\t\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t};\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_unlock;\n\t}\n\tif (icmpv4_xrlim_allow(net, rt, icmp_param->data.icmph.type,\n\t\t\t       icmp_param->data.icmph.code))\n\t\ticmp_push_reply(icmp_param, &ipc, &rt);\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\n}\n\nstatic struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.opt.opt.srr ?\n\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}\n\n/*\n *\tSend an ICMP message in response to a situation\n *\n *\tRFC 1122: 3.2.2\tMUST send at least the IP header and 8 bytes of header.\n *\t\t  MAY send more (we do).\n *\t\t\tMUST NOT change this header information.\n *\t\t\tMUST NOT reply to a multicast/broadcast IP address.\n *\t\t\tMUST NOT reply to a multicast/broadcast MAC address.\n *\t\t\tMUST reply to only the first fragment.\n */\n\nvoid icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info)\n{\n\tstruct iphdr *iph;\n\tint room;\n\tstruct icmp_bxm icmp_param;\n\tstruct rtable *rt = skb_rtable(skb_in);\n\tstruct ipcm_cookie ipc;\n\t__be32 saddr;\n\tu8  tos;\n\tstruct net *net;\n\tstruct sock *sk;\n\n\tif (!rt)\n\t\tgoto out;\n\tnet = dev_net(rt->dst.dev);\n\n\t/*\n\t *\tFind the original header. It is expected to be valid, of course.\n\t *\tCheck this, icmp_send is called from the most obscure devices\n\t *\tsometimes.\n\t */\n\tiph = ip_hdr(skb_in);\n\n\tif ((u8 *)iph < skb_in->head ||\n\t    (skb_in->network_header + sizeof(*iph)) > skb_in->tail)\n\t\tgoto out;\n\n\t/*\n\t *\tNo replies to physical multicast/broadcast\n\t */\n\tif (skb_in->pkt_type != PACKET_HOST)\n\t\tgoto out;\n\n\t/*\n\t *\tNow check at the protocol level\n\t */\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto out;\n\n\t/*\n\t *\tOnly reply to fragment 0. We byte re-order the constant\n\t *\tmask for efficiency.\n\t */\n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\tgoto out;\n\n\t/*\n\t *\tIf we send an ICMP error to an ICMP error a mess would result..\n\t */\n\tif (icmp_pointers[type].error) {\n\t\t/*\n\t\t *\tWe are an error, check if we are replying to an\n\t\t *\tICMP error\n\t\t */\n\t\tif (iph->protocol == IPPROTO_ICMP) {\n\t\t\tu8 _inner_type, *itp;\n\n\t\t\titp = skb_header_pointer(skb_in,\n\t\t\t\t\t\t skb_network_header(skb_in) +\n\t\t\t\t\t\t (iph->ihl << 2) +\n\t\t\t\t\t\t offsetof(struct icmphdr,\n\t\t\t\t\t\t\t  type) -\n\t\t\t\t\t\t skb_in->data,\n\t\t\t\t\t\t sizeof(_inner_type),\n\t\t\t\t\t\t &_inner_type);\n\t\t\tif (itp == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t/*\n\t\t\t *\tAssume any unknown ICMP type is an error. This\n\t\t\t *\tisn't specified by the RFC, but think about it..\n\t\t\t */\n\t\t\tif (*itp > NR_ICMP_TYPES ||\n\t\t\t    icmp_pointers[*itp].error)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\n\t/*\n\t *\tConstruct source address and options.\n\t */\n\n\tsaddr = iph->daddr;\n\tif (!(rt->rt_flags & RTCF_LOCAL)) {\n\t\tstruct net_device *dev = NULL;\n\n\t\trcu_read_lock();\n\t\tif (rt_is_input_route(rt) &&\n\t\t    net->ipv4.sysctl_icmp_errors_use_inbound_ifaddr)\n\t\t\tdev = dev_get_by_index_rcu(net, rt->rt_iif);\n\n\t\tif (dev)\n\t\t\tsaddr = inet_select_addr(dev, 0, RT_SCOPE_LINK);\n\t\telse\n\t\t\tsaddr = 0;\n\t\trcu_read_unlock();\n\t}\n\n\ttos = icmp_pointers[type].error ? ((iph->tos & IPTOS_TOS_MASK) |\n\t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n\t\t\t\t\t  iph->tos;\n\n\tif (ip_options_echo(&icmp_param.replyopts.opt.opt, skb_in))\n\t\tgoto out_unlock;\n\n\n\t/*\n\t *\tPrepare data for ICMP header.\n\t */\n\n\ticmp_param.data.icmph.type\t = type;\n\ticmp_param.data.icmph.code\t = code;\n\ticmp_param.data.icmph.un.gateway = info;\n\ticmp_param.data.icmph.checksum\t = 0;\n\ticmp_param.skb\t  = skb_in;\n\ticmp_param.offset = skb_network_offset(skb_in);\n\tinet_sk(sk)->tos = tos;\n\tipc.addr = iph->saddr;\n\tipc.opt = &icmp_param.replyopts.opt;\n\tipc.tx_flags = 0;\n\n\trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n\t\t\t       type, code, &icmp_param);\n\tif (IS_ERR(rt))\n\t\tgoto out_unlock;\n\n\tif (!icmpv4_xrlim_allow(net, rt, type, code))\n\t\tgoto ende;\n\n\t/* RFC says return as much as we can without exceeding 576 bytes. */\n\n\troom = dst_mtu(&rt->dst);\n\tif (room > 576)\n\t\troom = 576;\n\troom -= sizeof(struct iphdr) + icmp_param.replyopts.opt.opt.optlen;\n\troom -= sizeof(struct icmphdr);\n\n\ticmp_param.data_len = skb_in->len - icmp_param.offset;\n\tif (icmp_param.data_len > room)\n\t\ticmp_param.data_len = room;\n\ticmp_param.head_len = sizeof(struct icmphdr);\n\n\ticmp_push_reply(&icmp_param, &ipc, &rt);\nende:\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\nout:;\n}\nEXPORT_SYMBOL(icmp_send);\n\n\n/*\n *\tHandle ICMP_DEST_UNREACH, ICMP_TIME_EXCEED, and ICMP_QUENCH.\n */\n\nstatic void icmp_unreach(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct icmphdr *icmph;\n\tint hash, protocol;\n\tconst struct net_protocol *ipprot;\n\tu32 info = 0;\n\tstruct net *net;\n\n\tnet = dev_net(skb_dst(skb)->dev);\n\n\t/*\n\t *\tIncomplete header ?\n\t * \tOnly checks for the IP header, there should be an\n\t *\tadditional check for longer headers in upper levels.\n\t */\n\n\tif (!pskb_may_pull(skb, sizeof(struct iphdr)))\n\t\tgoto out_err;\n\n\ticmph = icmp_hdr(skb);\n\tiph   = (const struct iphdr *)skb->data;\n\n\tif (iph->ihl < 5) /* Mangled header, drop. */\n\t\tgoto out_err;\n\n\tif (icmph->type == ICMP_DEST_UNREACH) {\n\t\tswitch (icmph->code & 15) {\n\t\tcase ICMP_NET_UNREACH:\n\t\tcase ICMP_HOST_UNREACH:\n\t\tcase ICMP_PROT_UNREACH:\n\t\tcase ICMP_PORT_UNREACH:\n\t\t\tbreak;\n\t\tcase ICMP_FRAG_NEEDED:\n\t\t\tif (ipv4_config.no_pmtu_disc) {\n\t\t\t\tLIMIT_NETDEBUG(KERN_INFO \"ICMP: %pI4: fragmentation needed and DF set.\\n\",\n\t\t\t\t\t       &iph->daddr);\n\t\t\t} else {\n\t\t\t\tinfo = ip_rt_frag_needed(net, iph,\n\t\t\t\t\t\t\t ntohs(icmph->un.frag.mtu),\n\t\t\t\t\t\t\t skb->dev);\n\t\t\t\tif (!info)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ICMP_SR_FAILED:\n\t\t\tLIMIT_NETDEBUG(KERN_INFO \"ICMP: %pI4: Source Route Failed.\\n\",\n\t\t\t\t       &iph->daddr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tif (icmph->code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\t} else if (icmph->type == ICMP_PARAMETERPROB)\n\t\tinfo = ntohl(icmph->un.gateway) >> 24;\n\n\t/*\n\t *\tThrow it at our lower layers\n\t *\n\t *\tRFC 1122: 3.2.2 MUST extract the protocol ID from the passed\n\t *\t\t  header.\n\t *\tRFC 1122: 3.2.2.1 MUST pass ICMP unreach messages to the\n\t *\t\t  transport layer.\n\t *\tRFC 1122: 3.2.2.2 MUST pass ICMP time expired messages to\n\t *\t\t  transport layer.\n\t */\n\n\t/*\n\t *\tCheck the other end isn't violating RFC 1122. Some routers send\n\t *\tbogus responses to broadcast frames. If you see this message\n\t *\tfirst check your netmask matches at both ends, if it does then\n\t *\tget the other vendor to fix their kit.\n\t */\n\n\tif (!net->ipv4.sysctl_icmp_ignore_bogus_error_responses &&\n\t    inet_addr_type(net, iph->daddr) == RTN_BROADCAST) {\n\t\tif (net_ratelimit())\n\t\t\tprintk(KERN_WARNING \"%pI4 sent an invalid ICMP \"\n\t\t\t\t\t    \"type %u, code %u \"\n\t\t\t\t\t    \"error to a broadcast: %pI4 on %s\\n\",\n\t\t\t       &ip_hdr(skb)->saddr,\n\t\t\t       icmph->type, icmph->code,\n\t\t\t       &iph->daddr,\n\t\t\t       skb->dev->name);\n\t\tgoto out;\n\t}\n\n\t/* Checkin full IP header plus 8 bytes of protocol to\n\t * avoid additional coding at protocol handlers.\n\t */\n\tif (!pskb_may_pull(skb, iph->ihl * 4 + 8))\n\t\tgoto out;\n\n\tiph = (const struct iphdr *)skb->data;\n\tprotocol = iph->protocol;\n\n\t/*\n\t *\tDeliver ICMP message to raw sockets. Pretty useless feature?\n\t */\n\traw_icmp_error(skb, protocol, info);\n\n\thash = protocol & (MAX_INET_PROTOS - 1);\n\trcu_read_lock();\n\tipprot = rcu_dereference(inet_protos[hash]);\n\tif (ipprot && ipprot->err_handler)\n\t\tipprot->err_handler(skb, info);\n\trcu_read_unlock();\n\nout:\n\treturn;\nout_err:\n\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\tgoto out;\n}\n\n\n/*\n *\tHandle ICMP_REDIRECT.\n */\n\nstatic void icmp_redirect(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\n\tif (skb->len < sizeof(struct iphdr))\n\t\tgoto out_err;\n\n\t/*\n\t *\tGet the copied header of the packet that caused the redirect\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct iphdr)))\n\t\tgoto out;\n\n\tiph = (const struct iphdr *)skb->data;\n\n\tswitch (icmp_hdr(skb)->code & 7) {\n\tcase ICMP_REDIR_NET:\n\tcase ICMP_REDIR_NETTOS:\n\t\t/*\n\t\t * As per RFC recommendations now handle it as a host redirect.\n\t\t */\n\tcase ICMP_REDIR_HOST:\n\tcase ICMP_REDIR_HOSTTOS:\n\t\tip_rt_redirect(ip_hdr(skb)->saddr, iph->daddr,\n\t\t\t       icmp_hdr(skb)->un.gateway,\n\t\t\t       iph->saddr, skb->dev);\n\t\tbreak;\n\t}\nout:\n\treturn;\nout_err:\n\tICMP_INC_STATS_BH(dev_net(skb->dev), ICMP_MIB_INERRORS);\n\tgoto out;\n}\n\n/*\n *\tHandle ICMP_ECHO (\"ping\") requests.\n *\n *\tRFC 1122: 3.2.2.6 MUST have an echo server that answers ICMP echo\n *\t\t  requests.\n *\tRFC 1122: 3.2.2.6 Data received in the ICMP_ECHO request MUST be\n *\t\t  included in the reply.\n *\tRFC 1812: 4.3.3.6 SHOULD have a config option for silently ignoring\n *\t\t  echo requests, MUST have default=NOT.\n *\tSee also WRT handling of options once they are done and working.\n */\n\nstatic void icmp_echo(struct sk_buff *skb)\n{\n\tstruct net *net;\n\n\tnet = dev_net(skb_dst(skb)->dev);\n\tif (!net->ipv4.sysctl_icmp_echo_ignore_all) {\n\t\tstruct icmp_bxm icmp_param;\n\n\t\ticmp_param.data.icmph\t   = *icmp_hdr(skb);\n\t\ticmp_param.data.icmph.type = ICMP_ECHOREPLY;\n\t\ticmp_param.skb\t\t   = skb;\n\t\ticmp_param.offset\t   = 0;\n\t\ticmp_param.data_len\t   = skb->len;\n\t\ticmp_param.head_len\t   = sizeof(struct icmphdr);\n\t\ticmp_reply(&icmp_param, skb);\n\t}\n}\n\n/*\n *\tHandle ICMP Timestamp requests.\n *\tRFC 1122: 3.2.2.8 MAY implement ICMP timestamp requests.\n *\t\t  SHOULD be in the kernel for minimum random latency.\n *\t\t  MUST be accurate to a few minutes.\n *\t\t  MUST be updated at least at 15Hz.\n */\nstatic void icmp_timestamp(struct sk_buff *skb)\n{\n\tstruct timespec tv;\n\tstruct icmp_bxm icmp_param;\n\t/*\n\t *\tToo short.\n\t */\n\tif (skb->len < 4)\n\t\tgoto out_err;\n\n\t/*\n\t *\tFill in the current time as ms since midnight UT:\n\t */\n\tgetnstimeofday(&tv);\n\ticmp_param.data.times[1] = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC +\n\t\t\t\t\t tv.tv_nsec / NSEC_PER_MSEC);\n\ticmp_param.data.times[2] = icmp_param.data.times[1];\n\tif (skb_copy_bits(skb, 0, &icmp_param.data.times[0], 4))\n\t\tBUG();\n\ticmp_param.data.icmph\t   = *icmp_hdr(skb);\n\ticmp_param.data.icmph.type = ICMP_TIMESTAMPREPLY;\n\ticmp_param.data.icmph.code = 0;\n\ticmp_param.skb\t\t   = skb;\n\ticmp_param.offset\t   = 0;\n\ticmp_param.data_len\t   = 0;\n\ticmp_param.head_len\t   = sizeof(struct icmphdr) + 12;\n\ticmp_reply(&icmp_param, skb);\nout:\n\treturn;\nout_err:\n\tICMP_INC_STATS_BH(dev_net(skb_dst(skb)->dev), ICMP_MIB_INERRORS);\n\tgoto out;\n}\n\n\n/*\n *\tHandle ICMP_ADDRESS_MASK requests.  (RFC950)\n *\n * RFC1122 (3.2.2.9).  A host MUST only send replies to\n * ADDRESS_MASK requests if it's been configured as an address mask\n * agent.  Receiving a request doesn't constitute implicit permission to\n * act as one. Of course, implementing this correctly requires (SHOULD)\n * a way to turn the functionality on and off.  Another one for sysctl(),\n * I guess. -- MS\n *\n * RFC1812 (4.3.3.9).\tA router MUST implement it.\n *\t\t\tA router SHOULD have switch turning it on/off.\n *\t\t      \tThis switch MUST be ON by default.\n *\n * Gratuitous replies, zero-source replies are not implemented,\n * that complies with RFC. DO NOT implement them!!! All the idea\n * of broadcast addrmask replies as specified in RFC950 is broken.\n * The problem is that it is not uncommon to have several prefixes\n * on one physical interface. Moreover, addrmask agent can even be\n * not aware of existing another prefixes.\n * If source is zero, addrmask agent cannot choose correct prefix.\n * Gratuitous mask announcements suffer from the same problem.\n * RFC1812 explains it, but still allows to use ADDRMASK,\n * that is pretty silly. --ANK\n *\n * All these rules are so bizarre, that I removed kernel addrmask\n * support at all. It is wrong, it is obsolete, nobody uses it in\n * any case. --ANK\n *\n * Furthermore you can do it with a usermode address agent program\n * anyway...\n */\n\nstatic void icmp_address(struct sk_buff *skb)\n{\n#if 0\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"a guy asks for address mask. Who is it?\\n\");\n#endif\n}\n\n/*\n * RFC1812 (4.3.3.9).\tA router SHOULD listen all replies, and complain\n *\t\t\tloudly if an inconsistency is found.\n * called with rcu_read_lock()\n */\n\nstatic void icmp_address_reply(struct sk_buff *skb)\n{\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct in_device *in_dev;\n\tstruct in_ifaddr *ifa;\n\n\tif (skb->len < 4 || !(rt->rt_flags&RTCF_DIRECTSRC))\n\t\treturn;\n\n\tin_dev = __in_dev_get_rcu(dev);\n\tif (!in_dev)\n\t\treturn;\n\n\tif (in_dev->ifa_list &&\n\t    IN_DEV_LOG_MARTIANS(in_dev) &&\n\t    IN_DEV_FORWARD(in_dev)) {\n\t\t__be32 _mask, *mp;\n\n\t\tmp = skb_header_pointer(skb, 0, sizeof(_mask), &_mask);\n\t\tBUG_ON(mp == NULL);\n\t\tfor (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {\n\t\t\tif (*mp == ifa->ifa_mask &&\n\t\t\t    inet_ifa_match(rt->rt_src, ifa))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (!ifa && net_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"Wrong address mask %pI4 from %s/%pI4\\n\",\n\t\t\t       mp, dev->name, &rt->rt_src);\n\t\t}\n\t}\n}\n\nstatic void icmp_discard(struct sk_buff *skb)\n{\n}\n\n/*\n *\tDeal with incoming ICMP packets.\n */\nint icmp_rcv(struct sk_buff *skb)\n{\n\tstruct icmphdr *icmph;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tstruct sec_path *sp = skb_sec_path(skb);\n\t\tint nh;\n\n\t\tif (!(sp && sp->xvec[sp->len - 1]->props.flags &\n\t\t\t\t XFRM_STATE_ICMP))\n\t\t\tgoto drop;\n\n\t\tif (!pskb_may_pull(skb, sizeof(*icmph) + sizeof(struct iphdr)))\n\t\t\tgoto drop;\n\n\t\tnh = skb_network_offset(skb);\n\t\tskb_set_network_header(skb, sizeof(*icmph));\n\n\t\tif (!xfrm4_policy_check_reverse(NULL, XFRM_POLICY_IN, skb))\n\t\t\tgoto drop;\n\n\t\tskb_set_network_header(skb, nh);\n\t}\n\n\tICMP_INC_STATS_BH(net, ICMP_MIB_INMSGS);\n\n\tswitch (skb->ip_summed) {\n\tcase CHECKSUM_COMPLETE:\n\t\tif (!csum_fold(skb->csum))\n\t\t\tbreak;\n\t\t/* fall through */\n\tcase CHECKSUM_NONE:\n\t\tskb->csum = 0;\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto error;\n\t}\n\n\tif (!pskb_pull(skb, sizeof(*icmph)))\n\t\tgoto error;\n\n\ticmph = icmp_hdr(skb);\n\n\tICMPMSGIN_INC_STATS_BH(net, icmph->type);\n\t/*\n\t *\t18 is the highest 'known' ICMP type. Anything else is a mystery\n\t *\n\t *\tRFC 1122: 3.2.2  Unknown ICMP messages types MUST be silently\n\t *\t\t  discarded.\n\t */\n\tif (icmph->type > NR_ICMP_TYPES)\n\t\tgoto error;\n\n\n\t/*\n\t *\tParse the ICMP message\n\t */\n\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST)) {\n\t\t/*\n\t\t *\tRFC 1122: 3.2.2.6 An ICMP_ECHO to broadcast MAY be\n\t\t *\t  silently ignored (we let user decide with a sysctl).\n\t\t *\tRFC 1122: 3.2.2.8 An ICMP_TIMESTAMP MAY be silently\n\t\t *\t  discarded if to broadcast/multicast.\n\t\t */\n\t\tif ((icmph->type == ICMP_ECHO ||\n\t\t     icmph->type == ICMP_TIMESTAMP) &&\n\t\t    net->ipv4.sysctl_icmp_echo_ignore_broadcasts) {\n\t\t\tgoto error;\n\t\t}\n\t\tif (icmph->type != ICMP_ECHO &&\n\t\t    icmph->type != ICMP_TIMESTAMP &&\n\t\t    icmph->type != ICMP_ADDRESS &&\n\t\t    icmph->type != ICMP_ADDRESSREPLY) {\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\ticmp_pointers[icmph->type].handler(skb);\n\ndrop:\n\tkfree_skb(skb);\n\treturn 0;\nerror:\n\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\tgoto drop;\n}\n\n/*\n *\tThis table is the definition of how we handle ICMP.\n */\nstatic const struct icmp_control icmp_pointers[NR_ICMP_TYPES + 1] = {\n\t[ICMP_ECHOREPLY] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[1] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[2] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[ICMP_DEST_UNREACH] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_SOURCE_QUENCH] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_REDIRECT] = {\n\t\t.handler = icmp_redirect,\n\t\t.error = 1,\n\t},\n\t[6] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[7] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[ICMP_ECHO] = {\n\t\t.handler = icmp_echo,\n\t},\n\t[9] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[10] = {\n\t\t.handler = icmp_discard,\n\t\t.error = 1,\n\t},\n\t[ICMP_TIME_EXCEEDED] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_PARAMETERPROB] = {\n\t\t.handler = icmp_unreach,\n\t\t.error = 1,\n\t},\n\t[ICMP_TIMESTAMP] = {\n\t\t.handler = icmp_timestamp,\n\t},\n\t[ICMP_TIMESTAMPREPLY] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[ICMP_INFO_REQUEST] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[ICMP_INFO_REPLY] = {\n\t\t.handler = icmp_discard,\n\t},\n\t[ICMP_ADDRESS] = {\n\t\t.handler = icmp_address,\n\t},\n\t[ICMP_ADDRESSREPLY] = {\n\t\t.handler = icmp_address_reply,\n\t},\n};\n\nstatic void __net_exit icmp_sk_exit(struct net *net)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tinet_ctl_sock_destroy(net->ipv4.icmp_sk[i]);\n\tkfree(net->ipv4.icmp_sk);\n\tnet->ipv4.icmp_sk = NULL;\n}\n\nstatic int __net_init icmp_sk_init(struct net *net)\n{\n\tint i, err;\n\n\tnet->ipv4.icmp_sk =\n\t\tkzalloc(nr_cpu_ids * sizeof(struct sock *), GFP_KERNEL);\n\tif (net->ipv4.icmp_sk == NULL)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct sock *sk;\n\n\t\terr = inet_ctl_sock_create(&sk, PF_INET,\n\t\t\t\t\t   SOCK_RAW, IPPROTO_ICMP, net);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\n\t\tnet->ipv4.icmp_sk[i] = sk;\n\n\t\t/* Enough space for 2 64K ICMP packets, including\n\t\t * sk_buff struct overhead.\n\t\t */\n\t\tsk->sk_sndbuf =\n\t\t\t(2 * ((64 * 1024) + sizeof(struct sk_buff)));\n\n\t\t/*\n\t\t * Speedup sock_wfree()\n\t\t */\n\t\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\t\tinet_sk(sk)->pmtudisc = IP_PMTUDISC_DONT;\n\t}\n\n\t/* Control parameters for ECHO replies. */\n\tnet->ipv4.sysctl_icmp_echo_ignore_all = 0;\n\tnet->ipv4.sysctl_icmp_echo_ignore_broadcasts = 1;\n\n\t/* Control parameter - ignore bogus broadcast responses? */\n\tnet->ipv4.sysctl_icmp_ignore_bogus_error_responses = 1;\n\n\t/*\n\t * \tConfigurable global rate limit.\n\t *\n\t *\tratelimit defines tokens/packet consumed for dst->rate_token\n\t *\tbucket ratemask defines which icmp types are ratelimited by\n\t *\tsetting\tit's bit position.\n\t *\n\t *\tdefault:\n\t *\tdest unreachable (3), source quench (4),\n\t *\ttime exceeded (11), parameter problem (12)\n\t */\n\n\tnet->ipv4.sysctl_icmp_ratelimit = 1 * HZ;\n\tnet->ipv4.sysctl_icmp_ratemask = 0x1818;\n\tnet->ipv4.sysctl_icmp_errors_use_inbound_ifaddr = 0;\n\n\treturn 0;\n\nfail:\n\tfor_each_possible_cpu(i)\n\t\tinet_ctl_sock_destroy(net->ipv4.icmp_sk[i]);\n\tkfree(net->ipv4.icmp_sk);\n\treturn err;\n}\n\nstatic struct pernet_operations __net_initdata icmp_sk_ops = {\n       .init = icmp_sk_init,\n       .exit = icmp_sk_exit,\n};\n\nint __init icmp_init(void)\n{\n\treturn register_pernet_subsys(&icmp_sk_ops);\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tSupport for INET connection oriented protocols.\n *\n * Authors:\tSee the TCP sources\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/jhash.h>\n\n#include <net/inet_connection_sock.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_timewait_sock.h>\n#include <net/ip.h>\n#include <net/route.h>\n#include <net/tcp_states.h>\n#include <net/xfrm.h>\n\n#ifdef INET_CSK_DEBUG\nconst char inet_csk_timer_bug_msg[] = \"inet_csk BUG: unknown timer value\\n\";\nEXPORT_SYMBOL(inet_csk_timer_bug_msg);\n#endif\n\n/*\n * This struct holds the first and last local port number.\n */\nstruct local_ports sysctl_local_ports __read_mostly = {\n\t.lock = SEQLOCK_UNLOCKED,\n\t.range = { 32768, 61000 },\n};\n\nunsigned long *sysctl_local_reserved_ports;\nEXPORT_SYMBOL(sysctl_local_reserved_ports);\n\nvoid inet_get_local_port_range(int *low, int *high)\n{\n\tunsigned seq;\n\tdo {\n\t\tseq = read_seqbegin(&sysctl_local_ports.lock);\n\n\t\t*low = sysctl_local_ports.range[0];\n\t\t*high = sysctl_local_ports.range[1];\n\t} while (read_seqretry(&sysctl_local_ports.lock, seq));\n}\nEXPORT_SYMBOL(inet_get_local_port_range);\n\nint inet_csk_bind_conflict(const struct sock *sk,\n\t\t\t   const struct inet_bind_bucket *tb)\n{\n\tstruct sock *sk2;\n\tstruct hlist_node *node;\n\tint reuse = sk->sk_reuse;\n\n\t/*\n\t * Unlike other sk lookup places we do not check\n\t * for sk_net here, since _all_ the socks listed\n\t * in tb->owners list belong to the same net - the\n\t * one this bucket belongs to.\n\t */\n\n\tsk_for_each_bound(sk2, node, &tb->owners) {\n\t\tif (sk != sk2 &&\n\t\t    !inet_v6_ipv6only(sk2) &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     !sk2->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\n\t\t\tif (!reuse || !sk2->sk_reuse ||\n\t\t\t    sk2->sk_state == TCP_LISTEN) {\n\t\t\t\tconst __be32 sk2_rcv_saddr = sk_rcv_saddr(sk2);\n\t\t\t\tif (!sk2_rcv_saddr || !sk_rcv_saddr(sk) ||\n\t\t\t\t    sk2_rcv_saddr == sk_rcv_saddr(sk))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn node != NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_bind_conflict);\n\n/* Obtain a reference to a local port for the given sock,\n * if snum is zero it means select any available local port.\n */\nint inet_csk_get_port(struct sock *sk, unsigned short snum)\n{\n\tstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\n\tstruct inet_bind_hashbucket *head;\n\tstruct hlist_node *node;\n\tstruct inet_bind_bucket *tb;\n\tint ret, attempts = 5;\n\tstruct net *net = sock_net(sk);\n\tint smallest_size = -1, smallest_rover;\n\n\tlocal_bh_disable();\n\tif (!snum) {\n\t\tint remaining, rover, low, high;\n\nagain:\n\t\tinet_get_local_port_range(&low, &high);\n\t\tremaining = (high - low) + 1;\n\t\tsmallest_rover = rover = net_random() % remaining + low;\n\n\t\tsmallest_size = -1;\n\t\tdo {\n\t\t\tif (inet_is_reserved_local_port(rover))\n\t\t\t\tgoto next_nolock;\n\t\t\thead = &hashinfo->bhash[inet_bhashfn(net, rover,\n\t\t\t\t\thashinfo->bhash_size)];\n\t\t\tspin_lock(&head->lock);\n\t\t\tinet_bind_bucket_for_each(tb, node, &head->chain)\n\t\t\t\tif (net_eq(ib_net(tb), net) && tb->port == rover) {\n\t\t\t\t\tif (tb->fastreuse > 0 &&\n\t\t\t\t\t    sk->sk_reuse &&\n\t\t\t\t\t    sk->sk_state != TCP_LISTEN &&\n\t\t\t\t\t    (tb->num_owners < smallest_size || smallest_size == -1)) {\n\t\t\t\t\t\tsmallest_size = tb->num_owners;\n\t\t\t\t\t\tsmallest_rover = rover;\n\t\t\t\t\t\tif (atomic_read(&hashinfo->bsockets) > (high - low) + 1) {\n\t\t\t\t\t\t\tspin_unlock(&head->lock);\n\t\t\t\t\t\t\tsnum = smallest_rover;\n\t\t\t\t\t\t\tgoto have_snum;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\tbreak;\n\t\tnext:\n\t\t\tspin_unlock(&head->lock);\n\t\tnext_nolock:\n\t\t\tif (++rover > high)\n\t\t\t\trover = low;\n\t\t} while (--remaining > 0);\n\n\t\t/* Exhausted local port range during search?  It is not\n\t\t * possible for us to be holding one of the bind hash\n\t\t * locks if this test triggers, because if 'remaining'\n\t\t * drops to zero, we broke out of the do/while loop at\n\t\t * the top level, not from the 'break;' statement.\n\t\t */\n\t\tret = 1;\n\t\tif (remaining <= 0) {\n\t\t\tif (smallest_size != -1) {\n\t\t\t\tsnum = smallest_rover;\n\t\t\t\tgoto have_snum;\n\t\t\t}\n\t\t\tgoto fail;\n\t\t}\n\t\t/* OK, here is the one we will use.  HEAD is\n\t\t * non-NULL and we hold it's mutex.\n\t\t */\n\t\tsnum = rover;\n\t} else {\nhave_snum:\n\t\thead = &hashinfo->bhash[inet_bhashfn(net, snum,\n\t\t\t\thashinfo->bhash_size)];\n\t\tspin_lock(&head->lock);\n\t\tinet_bind_bucket_for_each(tb, node, &head->chain)\n\t\t\tif (net_eq(ib_net(tb), net) && tb->port == snum)\n\t\t\t\tgoto tb_found;\n\t}\n\ttb = NULL;\n\tgoto tb_not_found;\ntb_found:\n\tif (!hlist_empty(&tb->owners)) {\n\t\tif (tb->fastreuse > 0 &&\n\t\t    sk->sk_reuse && sk->sk_state != TCP_LISTEN &&\n\t\t    smallest_size == -1) {\n\t\t\tgoto success;\n\t\t} else {\n\t\t\tret = 1;\n\t\t\tif (inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb)) {\n\t\t\t\tif (sk->sk_reuse && sk->sk_state != TCP_LISTEN &&\n\t\t\t\t    smallest_size != -1 && --attempts >= 0) {\n\t\t\t\t\tspin_unlock(&head->lock);\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\t\t\t\tgoto fail_unlock;\n\t\t\t}\n\t\t}\n\t}\ntb_not_found:\n\tret = 1;\n\tif (!tb && (tb = inet_bind_bucket_create(hashinfo->bind_bucket_cachep,\n\t\t\t\t\tnet, head, snum)) == NULL)\n\t\tgoto fail_unlock;\n\tif (hlist_empty(&tb->owners)) {\n\t\tif (sk->sk_reuse && sk->sk_state != TCP_LISTEN)\n\t\t\ttb->fastreuse = 1;\n\t\telse\n\t\t\ttb->fastreuse = 0;\n\t} else if (tb->fastreuse &&\n\t\t   (!sk->sk_reuse || sk->sk_state == TCP_LISTEN))\n\t\ttb->fastreuse = 0;\nsuccess:\n\tif (!inet_csk(sk)->icsk_bind_hash)\n\t\tinet_bind_hash(sk, tb, snum);\n\tWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\n\tret = 0;\n\nfail_unlock:\n\tspin_unlock(&head->lock);\nfail:\n\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(inet_csk_get_port);\n\n/*\n * Wait for an incoming connection, avoid race conditions. This must be called\n * with the socket locked.\n */\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tDEFINE_WAIT(wait);\n\tint err;\n\n\t/*\n\t * True wake-one mechanism for incoming connections: only\n\t * one process gets woken up, not the 'whole herd'.\n\t * Since we do not 'race & poll' for established sockets\n\t * anymore, the common case will execute the loop only once.\n\t *\n\t * Subtle issue: \"add_wait_queue_exclusive()\" will be added\n\t * after any current non-exclusive waiters, and we know that\n\t * it will always _stay_ after any new non-exclusive waiters\n\t * because all non-exclusive waiters are added at the\n\t * beginning of the wait-queue. As such, it's ok to \"drop\"\n\t * our exclusiveness temporarily when we get woken up without\n\t * having to remove and re-insert us on the wait queue.\n\t */\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(sk_sleep(sk), &wait,\n\t\t\t\t\t  TASK_INTERRUPTIBLE);\n\t\trelease_sock(sk);\n\t\tif (reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\ttimeo = schedule_timeout(timeo);\n\t\tlock_sock(sk);\n\t\terr = 0;\n\t\tif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\n\t\t\tbreak;\n\t\terr = -EINVAL;\n\t\tif (sk->sk_state != TCP_LISTEN)\n\t\t\tbreak;\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tbreak;\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn err;\n}\n\n/*\n * This will accept the next outstanding connection.\n */\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sock *newsk;\n\tint error;\n\n\tlock_sock(sk);\n\n\t/* We need to make sure that this socket is listening,\n\t * and that it has something pending.\n\t */\n\terror = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out_err;\n\n\t/* Find already established connection */\n\tif (reqsk_queue_empty(&icsk->icsk_accept_queue)) {\n\t\tlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\n\n\t\t/* If this is a non blocking socket don't sleep */\n\t\terror = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_err;\n\n\t\terror = inet_csk_wait_for_connect(sk, timeo);\n\t\tif (error)\n\t\t\tgoto out_err;\n\t}\n\n\tnewsk = reqsk_queue_get_child(&icsk->icsk_accept_queue, sk);\n\tWARN_ON(newsk->sk_state == TCP_SYN_RECV);\nout:\n\trelease_sock(sk);\n\treturn newsk;\nout_err:\n\tnewsk = NULL;\n\t*err = error;\n\tgoto out;\n}\nEXPORT_SYMBOL(inet_csk_accept);\n\n/*\n * Using different timers for retransmit, delayed acks and probes\n * We may wish use just one timer maintaining a list of expire jiffies\n * to optimize.\n */\nvoid inet_csk_init_xmit_timers(struct sock *sk,\n\t\t\t       void (*retransmit_handler)(unsigned long),\n\t\t\t       void (*delack_handler)(unsigned long),\n\t\t\t       void (*keepalive_handler)(unsigned long))\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tsetup_timer(&icsk->icsk_retransmit_timer, retransmit_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&icsk->icsk_delack_timer, delack_handler,\n\t\t\t(unsigned long)sk);\n\tsetup_timer(&sk->sk_timer, keepalive_handler, (unsigned long)sk);\n\ticsk->icsk_pending = icsk->icsk_ack.pending = 0;\n}\nEXPORT_SYMBOL(inet_csk_init_xmit_timers);\n\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_pending = icsk->icsk_ack.pending = icsk->icsk_ack.blocked = 0;\n\n\tsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\n\tsk_stop_timer(sk, &icsk->icsk_delack_timer);\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_clear_xmit_timers);\n\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n}\nEXPORT_SYMBOL(inet_csk_delete_keepalive_timer);\n\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\n{\n\tsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\n}\nEXPORT_SYMBOL(inet_csk_reset_keepalive_timer);\n\nstruct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(inet_csk_route_req);\n\nstatic inline u32 inet_synq_hash(const __be32 raddr, const __be16 rport,\n\t\t\t\t const u32 rnd, const u32 synq_hsize)\n{\n\treturn jhash_2words((__force u32)raddr, (__force u32)rport, rnd) & (synq_hsize - 1);\n}\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#define AF_INET_FAMILY(fam) ((fam) == AF_INET)\n#else\n#define AF_INET_FAMILY(fam) 1\n#endif\n\nstruct request_sock *inet_csk_search_req(const struct sock *sk,\n\t\t\t\t\t struct request_sock ***prevp,\n\t\t\t\t\t const __be16 rport, const __be32 raddr,\n\t\t\t\t\t const __be32 laddr)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\n\tstruct request_sock *req, **prev;\n\n\tfor (prev = &lopt->syn_table[inet_synq_hash(raddr, rport, lopt->hash_rnd,\n\t\t\t\t\t\t    lopt->nr_table_entries)];\n\t     (req = *prev) != NULL;\n\t     prev = &req->dl_next) {\n\t\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\n\t\tif (ireq->rmt_port == rport &&\n\t\t    ireq->rmt_addr == raddr &&\n\t\t    ireq->loc_addr == laddr &&\n\t\t    AF_INET_FAMILY(req->rsk_ops->family)) {\n\t\t\tWARN_ON(req->sk);\n\t\t\t*prevp = prev;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL_GPL(inet_csk_search_req);\n\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\n\t\t\t\t   unsigned long timeout)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\n\tconst u32 h = inet_synq_hash(inet_rsk(req)->rmt_addr, inet_rsk(req)->rmt_port,\n\t\t\t\t     lopt->hash_rnd, lopt->nr_table_entries);\n\n\treqsk_queue_hash_req(&icsk->icsk_accept_queue, h, req, timeout);\n\tinet_csk_reqsk_queue_added(sk, timeout);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_hash_add);\n\n/* Only thing we need from tcp.h */\nextern int sysctl_tcp_synack_retries;\n\n\n/* Decide when to expire the request and when to resend SYN-ACK */\nstatic inline void syn_ack_recalc(struct request_sock *req, const int thresh,\n\t\t\t\t  const int max_retries,\n\t\t\t\t  const u8 rskq_defer_accept,\n\t\t\t\t  int *expire, int *resend)\n{\n\tif (!rskq_defer_accept) {\n\t\t*expire = req->retrans >= thresh;\n\t\t*resend = 1;\n\t\treturn;\n\t}\n\t*expire = req->retrans >= thresh &&\n\t\t  (!inet_rsk(req)->acked || req->retrans >= max_retries);\n\t/*\n\t * Do not resend while waiting for data after ACK,\n\t * start to resend on end of deferring period to give\n\t * last chance for data or ACK to create established socket.\n\t */\n\t*resend = !inet_rsk(req)->acked ||\n\t\t  req->retrans >= rskq_defer_accept - 1;\n}\n\nvoid inet_csk_reqsk_queue_prune(struct sock *parent,\n\t\t\t\tconst unsigned long interval,\n\t\t\t\tconst unsigned long timeout,\n\t\t\t\tconst unsigned long max_rto)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(parent);\n\tstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\n\tstruct listen_sock *lopt = queue->listen_opt;\n\tint max_retries = icsk->icsk_syn_retries ? : sysctl_tcp_synack_retries;\n\tint thresh = max_retries;\n\tunsigned long now = jiffies;\n\tstruct request_sock **reqp, *req;\n\tint i, budget;\n\n\tif (lopt == NULL || lopt->qlen == 0)\n\t\treturn;\n\n\t/* Normally all the openreqs are young and become mature\n\t * (i.e. converted to established socket) for first timeout.\n\t * If synack was not acknowledged for 3 seconds, it means\n\t * one of the following things: synack was lost, ack was lost,\n\t * rtt is high or nobody planned to ack (i.e. synflood).\n\t * When server is a bit loaded, queue is populated with old\n\t * open requests, reducing effective size of queue.\n\t * When server is well loaded, queue size reduces to zero\n\t * after several minutes of work. It is not synflood,\n\t * it is normal operation. The solution is pruning\n\t * too old entries overriding normal timeout, when\n\t * situation becomes dangerous.\n\t *\n\t * Essentially, we reserve half of room for young\n\t * embrions; and abort old ones without pity, if old\n\t * ones are about to clog our table.\n\t */\n\tif (lopt->qlen>>(lopt->max_qlen_log-1)) {\n\t\tint young = (lopt->qlen_young<<1);\n\n\t\twhile (thresh > 2) {\n\t\t\tif (lopt->qlen < young)\n\t\t\t\tbreak;\n\t\t\tthresh--;\n\t\t\tyoung <<= 1;\n\t\t}\n\t}\n\n\tif (queue->rskq_defer_accept)\n\t\tmax_retries = queue->rskq_defer_accept;\n\n\tbudget = 2 * (lopt->nr_table_entries / (timeout / interval));\n\ti = lopt->clock_hand;\n\n\tdo {\n\t\treqp=&lopt->syn_table[i];\n\t\twhile ((req = *reqp) != NULL) {\n\t\t\tif (time_after_eq(now, req->expires)) {\n\t\t\t\tint expire = 0, resend = 0;\n\n\t\t\t\tsyn_ack_recalc(req, thresh, max_retries,\n\t\t\t\t\t       queue->rskq_defer_accept,\n\t\t\t\t\t       &expire, &resend);\n\t\t\t\tif (req->rsk_ops->syn_ack_timeout)\n\t\t\t\t\treq->rsk_ops->syn_ack_timeout(parent, req);\n\t\t\t\tif (!expire &&\n\t\t\t\t    (!resend ||\n\t\t\t\t     !req->rsk_ops->rtx_syn_ack(parent, req, NULL) ||\n\t\t\t\t     inet_rsk(req)->acked)) {\n\t\t\t\t\tunsigned long timeo;\n\n\t\t\t\t\tif (req->retrans++ == 0)\n\t\t\t\t\t\tlopt->qlen_young--;\n\t\t\t\t\ttimeo = min((timeout << req->retrans), max_rto);\n\t\t\t\t\treq->expires = now + timeo;\n\t\t\t\t\treqp = &req->dl_next;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t/* Drop this request */\n\t\t\t\tinet_csk_reqsk_queue_unlink(parent, req, reqp);\n\t\t\t\treqsk_queue_removed(queue, req);\n\t\t\t\treqsk_free(req);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treqp = &req->dl_next;\n\t\t}\n\n\t\ti = (i + 1) & (lopt->nr_table_entries - 1);\n\n\t} while (--budget > 0);\n\n\tlopt->clock_hand = i;\n\n\tif (lopt->qlen)\n\t\tinet_csk_reset_keepalive_timer(parent, interval);\n}\nEXPORT_SYMBOL_GPL(inet_csk_reqsk_queue_prune);\n\nstruct sock *inet_csk_clone(struct sock *sk, const struct request_sock *req,\n\t\t\t    const gfp_t priority)\n{\n\tstruct sock *newsk = sk_clone(sk, priority);\n\n\tif (newsk != NULL) {\n\t\tstruct inet_connection_sock *newicsk = inet_csk(newsk);\n\n\t\tnewsk->sk_state = TCP_SYN_RECV;\n\t\tnewicsk->icsk_bind_hash = NULL;\n\n\t\tinet_sk(newsk)->inet_dport = inet_rsk(req)->rmt_port;\n\t\tinet_sk(newsk)->inet_num = ntohs(inet_rsk(req)->loc_port);\n\t\tinet_sk(newsk)->inet_sport = inet_rsk(req)->loc_port;\n\t\tnewsk->sk_write_space = sk_stream_write_space;\n\n\t\tnewicsk->icsk_retransmits = 0;\n\t\tnewicsk->icsk_backoff\t  = 0;\n\t\tnewicsk->icsk_probes_out  = 0;\n\n\t\t/* Deinitialize accept_queue to trap illegal accesses. */\n\t\tmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\n\n\t\tsecurity_inet_csk_clone(newsk, req);\n\t}\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(inet_csk_clone);\n\n/*\n * At this point, there should be no process reference to this\n * socket, and thus no user references at all.  Therefore we\n * can assume the socket waitqueue is inactive and nobody will\n * try to jump onto it.\n */\nvoid inet_csk_destroy_sock(struct sock *sk)\n{\n\tWARN_ON(sk->sk_state != TCP_CLOSE);\n\tWARN_ON(!sock_flag(sk, SOCK_DEAD));\n\n\t/* It cannot be in hash table! */\n\tWARN_ON(!sk_unhashed(sk));\n\n\t/* If it has not 0 inet_sk(sk)->inet_num, it must be bound */\n\tWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\n\n\tsk->sk_prot->destroy(sk);\n\n\tsk_stream_kill_queues(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\n\tpercpu_counter_dec(sk->sk_prot->orphan_count);\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(inet_csk_destroy_sock);\n\nint inet_csk_listen_start(struct sock *sk, const int nr_table_entries)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint rc = reqsk_queue_alloc(&icsk->icsk_accept_queue, nr_table_entries);\n\n\tif (rc != 0)\n\t\treturn rc;\n\n\tsk->sk_max_ack_backlog = 0;\n\tsk->sk_ack_backlog = 0;\n\tinet_csk_delack_init(sk);\n\n\t/* There is race window here: we announce ourselves listening,\n\t * but this transition is still not validated by get_port().\n\t * It is OK, because this socket enters to hash table only\n\t * after validation is complete.\n\t */\n\tsk->sk_state = TCP_LISTEN;\n\tif (!sk->sk_prot->get_port(sk, inet->inet_num)) {\n\t\tinet->inet_sport = htons(inet->inet_num);\n\n\t\tsk_dst_reset(sk);\n\t\tsk->sk_prot->hash(sk);\n\n\t\treturn 0;\n\t}\n\n\tsk->sk_state = TCP_CLOSE;\n\t__reqsk_queue_destroy(&icsk->icsk_accept_queue);\n\treturn -EADDRINUSE;\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_start);\n\n/*\n *\tThis routine closes sockets which have been at least partially\n *\topened, but not yet accepted.\n */\nvoid inet_csk_listen_stop(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct request_sock *acc_req;\n\tstruct request_sock *req;\n\n\tinet_csk_delete_keepalive_timer(sk);\n\n\t/* make all the listen_opt local to us */\n\tacc_req = reqsk_queue_yank_acceptq(&icsk->icsk_accept_queue);\n\n\t/* Following specs, it would be better either to send FIN\n\t * (and enter FIN-WAIT-1, it is normal close)\n\t * or to send active reset (abort).\n\t * Certainly, it is pretty dangerous while synflood, but it is\n\t * bad justification for our negligence 8)\n\t * To be honest, we are not able to make either\n\t * of the variants now.\t\t\t--ANK\n\t */\n\treqsk_queue_destroy(&icsk->icsk_accept_queue);\n\n\twhile ((req = acc_req) != NULL) {\n\t\tstruct sock *child = req->sk;\n\n\t\tacc_req = req->dl_next;\n\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(child);\n\t\tWARN_ON(sock_owned_by_user(child));\n\t\tsock_hold(child);\n\n\t\tsk->sk_prot->disconnect(child, O_NONBLOCK);\n\n\t\tsock_orphan(child);\n\n\t\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\t\tinet_csk_destroy_sock(child);\n\n\t\tbh_unlock_sock(child);\n\t\tlocal_bh_enable();\n\t\tsock_put(child);\n\n\t\tsk_acceptq_removed(sk);\n\t\t__reqsk_free(req);\n\t}\n\tWARN_ON(sk->sk_ack_backlog);\n}\nEXPORT_SYMBOL_GPL(inet_csk_listen_stop);\n\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tsin->sin_family\t\t= AF_INET;\n\tsin->sin_addr.s_addr\t= inet->inet_daddr;\n\tsin->sin_port\t\t= inet->inet_dport;\n}\nEXPORT_SYMBOL_GPL(inet_csk_addr2sockaddr);\n\n#ifdef CONFIG_COMPAT\nint inet_csk_compat_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_getsockopt != NULL)\n\t\treturn icsk->icsk_af_ops->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_getsockopt);\n\nint inet_csk_compat_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_af_ops->compat_setsockopt != NULL)\n\t\treturn icsk->icsk_af_ops->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t    optval, optlen);\n\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t     optval, optlen);\n}\nEXPORT_SYMBOL_GPL(inet_csk_compat_setsockopt);\n#endif\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe options processing module for ip.c\n *\n * Authors:\tA.N.Kuznetsov\n *\n */\n\n#include <linux/capability.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/ip.h>\n#include <linux/icmp.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/route.h>\n#include <net/cipso_ipv4.h>\n\n/*\n * Write options to IP header, record destination address to\n * source route option, address of outgoing interface\n * (we should already know it, so that this  function is allowed be\n * called only after routing decision) and timestamp,\n * if we originate this datagram.\n *\n * daddr is real destination address, next hop is recorded in IP header.\n * saddr is address of outgoing interface.\n */\n\nvoid ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n{\n\tunsigned char *iph = skb_network_header(skb);\n\n\tmemcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));\n\tmemcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);\n\topt = &(IPCB(skb)->opt);\n\n\tif (opt->srr)\n\t\tmemcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);\n\n\tif (!is_frag) {\n\t\tif (opt->rr_needaddr)\n\t\t\tip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);\n\t\tif (opt->ts_needaddr)\n\t\t\tip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);\n\t\tif (opt->ts_needtime) {\n\t\t\tstruct timespec tv;\n\t\t\t__be32 midtime;\n\t\t\tgetnstimeofday(&tv);\n\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\tmemcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);\n\t\t}\n\t\treturn;\n\t}\n\tif (opt->rr) {\n\t\tmemset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);\n\t\topt->rr = 0;\n\t\topt->rr_needaddr = 0;\n\t}\n\tif (opt->ts) {\n\t\tmemset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);\n\t\topt->ts = 0;\n\t\topt->ts_needaddr = opt->ts_needtime = 0;\n\t}\n}\n\n/*\n * Provided (sopt, skb) points to received options,\n * build in dopt compiled option set appropriate for answering.\n * i.e. invert SRR option, copy anothers,\n * and grab room in RR/TS options.\n *\n * NOTE: dopt cannot point to skb.\n */\n\nint ip_options_echo(struct ip_options *dopt, struct sk_buff *skb)\n{\n\tconst struct ip_options *sopt;\n\tunsigned char *sptr, *dptr;\n\tint soffset, doffset;\n\tint\toptlen;\n\t__be32\tdaddr;\n\n\tmemset(dopt, 0, sizeof(struct ip_options));\n\n\tsopt = &(IPCB(skb)->opt);\n\n\tif (sopt->optlen == 0)\n\t\treturn 0;\n\n\tsptr = skb_network_header(skb);\n\tdptr = dopt->__data;\n\n\tdaddr = skb_rtable(skb)->rt_spec_dst;\n\n\tif (sopt->rr) {\n\t\toptlen  = sptr[sopt->rr+1];\n\t\tsoffset = sptr[sopt->rr+2];\n\t\tdopt->rr = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->rr, optlen);\n\t\tif (sopt->rr_needaddr && soffset <= optlen) {\n\t\t\tif (soffset + 3 > optlen)\n\t\t\t\treturn -EINVAL;\n\t\t\tdptr[2] = soffset + 4;\n\t\t\tdopt->rr_needaddr = 1;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->ts) {\n\t\toptlen = sptr[sopt->ts+1];\n\t\tsoffset = sptr[sopt->ts+2];\n\t\tdopt->ts = dopt->optlen + sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->ts, optlen);\n\t\tif (soffset <= optlen) {\n\t\t\tif (sopt->ts_needaddr) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tdopt->ts_needaddr = 1;\n\t\t\t\tsoffset += 4;\n\t\t\t}\n\t\t\tif (sopt->ts_needtime) {\n\t\t\t\tif (soffset + 3 > optlen)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif ((dptr[3]&0xF) != IPOPT_TS_PRESPEC) {\n\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\tsoffset += 4;\n\t\t\t\t} else {\n\t\t\t\t\tdopt->ts_needtime = 0;\n\n\t\t\t\t\tif (soffset + 7 <= optlen) {\n\t\t\t\t\t\t__be32 addr;\n\n\t\t\t\t\t\tmemcpy(&addr, dptr+soffset-1, 4);\n\t\t\t\t\t\tif (inet_addr_type(dev_net(skb_dst(skb)->dev), addr) != RTN_UNICAST) {\n\t\t\t\t\t\t\tdopt->ts_needtime = 1;\n\t\t\t\t\t\t\tsoffset += 8;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tdptr[2] = soffset;\n\t\t}\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\tif (sopt->srr) {\n\t\tunsigned char *start = sptr+sopt->srr;\n\t\t__be32 faddr;\n\n\t\toptlen  = start[1];\n\t\tsoffset = start[2];\n\t\tdoffset = 0;\n\t\tif (soffset > optlen)\n\t\t\tsoffset = optlen + 1;\n\t\tsoffset -= 4;\n\t\tif (soffset > 3) {\n\t\t\tmemcpy(&faddr, &start[soffset-1], 4);\n\t\t\tfor (soffset-=4, doffset=4; soffset > 3; soffset-=4, doffset+=4)\n\t\t\t\tmemcpy(&dptr[doffset-1], &start[soffset-1], 4);\n\t\t\t/*\n\t\t\t * RFC1812 requires to fix illegal source routes.\n\t\t\t */\n\t\t\tif (memcmp(&ip_hdr(skb)->saddr,\n\t\t\t\t   &start[soffset + 3], 4) == 0)\n\t\t\t\tdoffset -= 4;\n\t\t}\n\t\tif (doffset > 3) {\n\t\t\tmemcpy(&start[doffset-1], &daddr, 4);\n\t\t\tdopt->faddr = faddr;\n\t\t\tdptr[0] = start[0];\n\t\t\tdptr[1] = doffset+3;\n\t\t\tdptr[2] = 4;\n\t\t\tdptr += doffset+3;\n\t\t\tdopt->srr = dopt->optlen + sizeof(struct iphdr);\n\t\t\tdopt->optlen += doffset+3;\n\t\t\tdopt->is_strictroute = sopt->is_strictroute;\n\t\t}\n\t}\n\tif (sopt->cipso) {\n\t\toptlen  = sptr[sopt->cipso+1];\n\t\tdopt->cipso = dopt->optlen+sizeof(struct iphdr);\n\t\tmemcpy(dptr, sptr+sopt->cipso, optlen);\n\t\tdptr += optlen;\n\t\tdopt->optlen += optlen;\n\t}\n\twhile (dopt->optlen & 3) {\n\t\t*dptr++ = IPOPT_END;\n\t\tdopt->optlen++;\n\t}\n\treturn 0;\n}\n\n/*\n *\tOptions \"fragmenting\", just fill options not\n *\tallowed in fragments with NOOPs.\n *\tSimple and stupid 8), but the most efficient way.\n */\n\nvoid ip_options_fragment(struct sk_buff * skb)\n{\n\tunsigned char *optptr = skb_network_header(skb) + sizeof(struct iphdr);\n\tstruct ip_options * opt = &(IPCB(skb)->opt);\n\tint  l = opt->optlen;\n\tint  optlen;\n\n\twhile (l > 0) {\n\t\tswitch (*optptr) {\n\t\tcase IPOPT_END:\n\t\t\treturn;\n\t\tcase IPOPT_NOOP:\n\t\t\tl--;\n\t\t\toptptr++;\n\t\t\tcontinue;\n\t\t}\n\t\toptlen = optptr[1];\n\t\tif (optlen<2 || optlen>l)\n\t\t  return;\n\t\tif (!IPOPT_COPIED(*optptr))\n\t\t\tmemset(optptr, IPOPT_NOOP, optlen);\n\t\tl -= optlen;\n\t\toptptr += optlen;\n\t}\n\topt->ts = 0;\n\topt->rr = 0;\n\topt->rr_needaddr = 0;\n\topt->ts_needaddr = 0;\n\topt->ts_needtime = 0;\n}\n\n/*\n * Verify options and fill pointers in struct options.\n * Caller should clear *opt, and set opt->data.\n * If opt == NULL, then skb->data should point to IP header.\n */\n\nint ip_options_compile(struct net *net,\n\t\t       struct ip_options * opt, struct sk_buff * skb)\n{\n\tint l;\n\tunsigned char * iph;\n\tunsigned char * optptr;\n\tint optlen;\n\tunsigned char * pp_ptr = NULL;\n\tstruct rtable *rt = NULL;\n\n\tif (skb != NULL) {\n\t\trt = skb_rtable(skb);\n\t\toptptr = (unsigned char *)&(ip_hdr(skb)[1]);\n\t} else\n\t\toptptr = opt->__data;\n\tiph = optptr - sizeof(struct iphdr);\n\n\tfor (l = opt->optlen; l > 0; ) {\n\t\tswitch (*optptr) {\n\t\t      case IPOPT_END:\n\t\t\tfor (optptr++, l--; l>0; optptr++, l--) {\n\t\t\t\tif (*optptr != IPOPT_END) {\n\t\t\t\t\t*optptr = IPOPT_END;\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgoto eol;\n\t\t      case IPOPT_NOOP:\n\t\t\tl--;\n\t\t\toptptr++;\n\t\t\tcontinue;\n\t\t}\n\t\toptlen = optptr[1];\n\t\tif (optlen<2 || optlen>l) {\n\t\t\tpp_ptr = optptr;\n\t\t\tgoto error;\n\t\t}\n\t\tswitch (*optptr) {\n\t\t      case IPOPT_SSRR:\n\t\t      case IPOPT_LSRR:\n\t\t\tif (optlen < 3) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] < 4) {\n\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t/* NB: cf RFC-1812 5.2.4.1 */\n\t\t\tif (opt->srr) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (!skb) {\n\t\t\t\tif (optptr[2] != 4 || optlen < 7 || ((optlen-3) & 3)) {\n\t\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tmemcpy(&opt->faddr, &optptr[3], 4);\n\t\t\t\tif (optlen > 7)\n\t\t\t\t\tmemmove(&optptr[3], &optptr[7], optlen-7);\n\t\t\t}\n\t\t\topt->is_strictroute = (optptr[0] == IPOPT_SSRR);\n\t\t\topt->srr = optptr - iph;\n\t\t\tbreak;\n\t\t      case IPOPT_RR:\n\t\t\tif (opt->rr) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optlen < 3) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] < 4) {\n\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] <= optlen) {\n\t\t\t\tif (optptr[2]+3 > optlen) {\n\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tif (rt) {\n\t\t\t\t\tmemcpy(&optptr[optptr[2]-1], &rt->rt_spec_dst, 4);\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t\toptptr[2] += 4;\n\t\t\t\topt->rr_needaddr = 1;\n\t\t\t}\n\t\t\topt->rr = optptr - iph;\n\t\t\tbreak;\n\t\t      case IPOPT_TIMESTAMP:\n\t\t\tif (opt->ts) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optlen < 4) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] < 5) {\n\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] <= optlen) {\n\t\t\t\t__be32 *timeptr = NULL;\n\t\t\t\tif (optptr[2]+3 > optptr[1]) {\n\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tswitch (optptr[3]&0xF) {\n\t\t\t\t      case IPOPT_TS_TSONLY:\n\t\t\t\t\topt->ts = optptr - iph;\n\t\t\t\t\tif (skb)\n\t\t\t\t\t\ttimeptr = (__be32*)&optptr[optptr[2]-1];\n\t\t\t\t\topt->ts_needtime = 1;\n\t\t\t\t\toptptr[2] += 4;\n\t\t\t\t\tbreak;\n\t\t\t\t      case IPOPT_TS_TSANDADDR:\n\t\t\t\t\tif (optptr[2]+7 > optptr[1]) {\n\t\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\topt->ts = optptr - iph;\n\t\t\t\t\tif (rt)  {\n\t\t\t\t\t\tmemcpy(&optptr[optptr[2]-1], &rt->rt_spec_dst, 4);\n\t\t\t\t\t\ttimeptr = (__be32*)&optptr[optptr[2]+3];\n\t\t\t\t\t}\n\t\t\t\t\topt->ts_needaddr = 1;\n\t\t\t\t\topt->ts_needtime = 1;\n\t\t\t\t\toptptr[2] += 8;\n\t\t\t\t\tbreak;\n\t\t\t\t      case IPOPT_TS_PRESPEC:\n\t\t\t\t\tif (optptr[2]+7 > optptr[1]) {\n\t\t\t\t\t\tpp_ptr = optptr + 2;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\topt->ts = optptr - iph;\n\t\t\t\t\t{\n\t\t\t\t\t\t__be32 addr;\n\t\t\t\t\t\tmemcpy(&addr, &optptr[optptr[2]-1], 4);\n\t\t\t\t\t\tif (inet_addr_type(net, addr) == RTN_UNICAST)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tif (skb)\n\t\t\t\t\t\t\ttimeptr = (__be32*)&optptr[optptr[2]+3];\n\t\t\t\t\t}\n\t\t\t\t\topt->ts_needtime = 1;\n\t\t\t\t\toptptr[2] += 8;\n\t\t\t\t\tbreak;\n\t\t\t\t      default:\n\t\t\t\t\tif (!skb && !capable(CAP_NET_RAW)) {\n\t\t\t\t\t\tpp_ptr = optptr + 3;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (timeptr) {\n\t\t\t\t\tstruct timespec tv;\n\t\t\t\t\t__be32  midtime;\n\t\t\t\t\tgetnstimeofday(&tv);\n\t\t\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\t\t\tmemcpy(timeptr, &midtime, sizeof(__be32));\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tunsigned overflow = optptr[3]>>4;\n\t\t\t\tif (overflow == 15) {\n\t\t\t\t\tpp_ptr = optptr + 3;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\topt->ts = optptr - iph;\n\t\t\t\tif (skb) {\n\t\t\t\t\toptptr[3] = (optptr[3]&0xF)|((overflow+1)<<4);\n\t\t\t\t\topt->is_changed = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t      case IPOPT_RA:\n\t\t\tif (optlen < 4) {\n\t\t\t\tpp_ptr = optptr + 1;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (optptr[2] == 0 && optptr[3] == 0)\n\t\t\t\topt->router_alert = optptr - iph;\n\t\t\tbreak;\n\t\t      case IPOPT_CIPSO:\n\t\t\tif ((!skb && !capable(CAP_NET_RAW)) || opt->cipso) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\topt->cipso = optptr - iph;\n\t\t\tif (cipso_v4_validate(skb, &optptr)) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\t      case IPOPT_SEC:\n\t\t      case IPOPT_SID:\n\t\t      default:\n\t\t\tif (!skb && !capable(CAP_NET_RAW)) {\n\t\t\t\tpp_ptr = optptr;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tl -= optlen;\n\t\toptptr += optlen;\n\t}\n\neol:\n\tif (!pp_ptr)\n\t\treturn 0;\n\nerror:\n\tif (skb) {\n\t\ticmp_send(skb, ICMP_PARAMETERPROB, 0, htonl((pp_ptr-iph)<<24));\n\t}\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(ip_options_compile);\n\n/*\n *\tUndo all the changes done by ip_options_compile().\n */\n\nvoid ip_options_undo(struct ip_options * opt)\n{\n\tif (opt->srr) {\n\t\tunsigned  char * optptr = opt->__data+opt->srr-sizeof(struct  iphdr);\n\t\tmemmove(optptr+7, optptr+3, optptr[1]-7);\n\t\tmemcpy(optptr+3, &opt->faddr, 4);\n\t}\n\tif (opt->rr_needaddr) {\n\t\tunsigned  char * optptr = opt->__data+opt->rr-sizeof(struct  iphdr);\n\t\toptptr[2] -= 4;\n\t\tmemset(&optptr[optptr[2]-1], 0, 4);\n\t}\n\tif (opt->ts) {\n\t\tunsigned  char * optptr = opt->__data+opt->ts-sizeof(struct  iphdr);\n\t\tif (opt->ts_needtime) {\n\t\t\toptptr[2] -= 4;\n\t\t\tmemset(&optptr[optptr[2]-1], 0, 4);\n\t\t\tif ((optptr[3]&0xF) == IPOPT_TS_PRESPEC)\n\t\t\t\toptptr[2] -= 4;\n\t\t}\n\t\tif (opt->ts_needaddr) {\n\t\t\toptptr[2] -= 4;\n\t\t\tmemset(&optptr[optptr[2]-1], 0, 4);\n\t\t}\n\t}\n}\n\nstatic struct ip_options_rcu *ip_options_get_alloc(const int optlen)\n{\n\treturn kzalloc(sizeof(struct ip_options_rcu) + ((optlen + 3) & ~3),\n\t\t       GFP_KERNEL);\n}\n\nstatic int ip_options_get_finish(struct net *net, struct ip_options_rcu **optp,\n\t\t\t\t struct ip_options_rcu *opt, int optlen)\n{\n\twhile (optlen & 3)\n\t\topt->opt.__data[optlen++] = IPOPT_END;\n\topt->opt.optlen = optlen;\n\tif (optlen && ip_options_compile(net, &opt->opt, NULL)) {\n\t\tkfree(opt);\n\t\treturn -EINVAL;\n\t}\n\tkfree(*optp);\n\t*optp = opt;\n\treturn 0;\n}\n\nint ip_options_get_from_user(struct net *net, struct ip_options_rcu **optp,\n\t\t\t     unsigned char __user *data, int optlen)\n{\n\tstruct ip_options_rcu *opt = ip_options_get_alloc(optlen);\n\n\tif (!opt)\n\t\treturn -ENOMEM;\n\tif (optlen && copy_from_user(opt->opt.__data, data, optlen)) {\n\t\tkfree(opt);\n\t\treturn -EFAULT;\n\t}\n\treturn ip_options_get_finish(net, optp, opt, optlen);\n}\n\nint ip_options_get(struct net *net, struct ip_options_rcu **optp,\n\t\t   unsigned char *data, int optlen)\n{\n\tstruct ip_options_rcu *opt = ip_options_get_alloc(optlen);\n\n\tif (!opt)\n\t\treturn -ENOMEM;\n\tif (optlen)\n\t\tmemcpy(opt->opt.__data, data, optlen);\n\treturn ip_options_get_finish(net, optp, opt, optlen);\n}\n\nvoid ip_forward_options(struct sk_buff *skb)\n{\n\tstruct   ip_options * opt\t= &(IPCB(skb)->opt);\n\tunsigned char * optptr;\n\tstruct rtable *rt = skb_rtable(skb);\n\tunsigned char *raw = skb_network_header(skb);\n\n\tif (opt->rr_needaddr) {\n\t\toptptr = (unsigned char *)raw + opt->rr;\n\t\tip_rt_get_source(&optptr[optptr[2]-5], rt);\n\t\topt->is_changed = 1;\n\t}\n\tif (opt->srr_is_hit) {\n\t\tint srrptr, srrspace;\n\n\t\toptptr = raw + opt->srr;\n\n\t\tfor ( srrptr=optptr[2], srrspace = optptr[1];\n\t\t     srrptr <= srrspace;\n\t\t     srrptr += 4\n\t\t     ) {\n\t\t\tif (srrptr + 3 > srrspace)\n\t\t\t\tbreak;\n\t\t\tif (memcmp(&rt->rt_dst, &optptr[srrptr-1], 4) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (srrptr + 3 <= srrspace) {\n\t\t\topt->is_changed = 1;\n\t\t\tip_rt_get_source(&optptr[srrptr-1], rt);\n\t\t\tip_hdr(skb)->daddr = rt->rt_dst;\n\t\t\toptptr[2] = srrptr+4;\n\t\t} else if (net_ratelimit())\n\t\t\tprintk(KERN_CRIT \"ip_forward(): Argh! Destination lost!\\n\");\n\t\tif (opt->ts_needaddr) {\n\t\t\toptptr = raw + opt->ts;\n\t\t\tip_rt_get_source(&optptr[optptr[2]-9], rt);\n\t\t\topt->is_changed = 1;\n\t\t}\n\t}\n\tif (opt->is_changed) {\n\t\topt->is_changed = 0;\n\t\tip_send_check(ip_hdr(skb));\n\t}\n}\n\nint ip_options_rcv_srr(struct sk_buff *skb)\n{\n\tstruct ip_options *opt = &(IPCB(skb)->opt);\n\tint srrspace, srrptr;\n\t__be32 nexthop;\n\tstruct iphdr *iph = ip_hdr(skb);\n\tunsigned char *optptr = skb_network_header(skb) + opt->srr;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct rtable *rt2;\n\tunsigned long orefdst;\n\tint err;\n\n\tif (!opt->srr || !rt)\n\t\treturn 0;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn -EINVAL;\n\tif (rt->rt_type == RTN_UNICAST) {\n\t\tif (!opt->is_strictroute)\n\t\t\treturn 0;\n\t\ticmp_send(skb, ICMP_PARAMETERPROB, 0, htonl(16<<24));\n\t\treturn -EINVAL;\n\t}\n\tif (rt->rt_type != RTN_LOCAL)\n\t\treturn -EINVAL;\n\n\tfor (srrptr=optptr[2], srrspace = optptr[1]; srrptr <= srrspace; srrptr += 4) {\n\t\tif (srrptr + 3 > srrspace) {\n\t\t\ticmp_send(skb, ICMP_PARAMETERPROB, 0, htonl((opt->srr+2)<<24));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmemcpy(&nexthop, &optptr[srrptr-1], 4);\n\n\t\torefdst = skb->_skb_refdst;\n\t\tskb_dst_set(skb, NULL);\n\t\terr = ip_route_input(skb, nexthop, iph->saddr, iph->tos, skb->dev);\n\t\trt2 = skb_rtable(skb);\n\t\tif (err || (rt2->rt_type != RTN_UNICAST && rt2->rt_type != RTN_LOCAL)) {\n\t\t\tskb_dst_drop(skb);\n\t\t\tskb->_skb_refdst = orefdst;\n\t\t\treturn -EINVAL;\n\t\t}\n\t\trefdst_drop(orefdst);\n\t\tif (rt2->rt_type != RTN_LOCAL)\n\t\t\tbreak;\n\t\t/* Superfast 8) loopback forward */\n\t\tmemcpy(&iph->daddr, &optptr[srrptr-1], 4);\n\t\topt->is_changed = 1;\n\t}\n\tif (srrptr <= srrspace) {\n\t\topt->srr_is_hit = 1;\n\t\topt->is_changed = 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ip_options_rcv_srr);\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe Internet Protocol (IP) output module.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tDonald Becker, <becker@super.org>\n *\t\tAlan Cox, <Alan.Cox@linux.org>\n *\t\tRichard Underwood\n *\t\tStefan Becker, <stefanb@yello.ping.de>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n *\tSee ip_input.c for original log\n *\n *\tFixes:\n *\t\tAlan Cox\t:\tMissing nonblock feature in ip_build_xmit.\n *\t\tMike Kilburn\t:\thtons() missing in ip_build_xmit.\n *\t\tBradford Johnson:\tFix faulty handling of some frames when\n *\t\t\t\t\tno route is found.\n *\t\tAlexander Demenshin:\tMissing sk/skb free in ip_queue_xmit\n *\t\t\t\t\t(in case if packet not accepted by\n *\t\t\t\t\toutput firewall rules)\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tAlexey Kuznetsov:\tuse new route cache\n *\t\tAndi Kleen:\t\tFix broken PMTU recovery and remove\n *\t\t\t\t\tsome redundant tests.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tAndi Kleen\t: \tReplace ip_reply with ip_send_reply.\n *\t\tAndi Kleen\t:\tSplit fast and slow ip_build_xmit path\n *\t\t\t\t\tfor decreased register pressure on x86\n *\t\t\t\t\tand more readibility.\n *\t\tMarc Boucher\t:\tWhen call_out_firewall returns FW_QUEUE,\n *\t\t\t\t\tsilently drop skb instead of failing with -EPERM.\n *\t\tDetlev Wengorz\t:\tCopy protocol for fragments.\n *\t\tHirokazu Takahashi:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\t\tHirokazu Takahashi:\tsendfile() on UDP works now.\n */\n\n#include <asm/uaccess.h>\n#include <asm/system.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n\n#include <net/snmp.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/route.h>\n#include <net/xfrm.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/arp.h>\n#include <net/icmp.h>\n#include <net/checksum.h>\n#include <net/inetpeer.h>\n#include <linux/igmp.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/netfilter_bridge.h>\n#include <linux/mroute.h>\n#include <linux/netlink.h>\n#include <linux/tcp.h>\n\nint sysctl_ip_default_ttl __read_mostly = IPDEFTTL;\nEXPORT_SYMBOL(sysctl_ip_default_ttl);\n\n/* Generate a checksum for an outgoing IP datagram. */\n__inline__ void ip_send_check(struct iphdr *iph)\n{\n\tiph->check = 0;\n\tiph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);\n}\nEXPORT_SYMBOL(ip_send_check);\n\nint __ip_local_out(struct sk_buff *skb)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tiph->tot_len = htons(skb->len);\n\tip_send_check(iph);\n\treturn nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t       skb_dst(skb)->dev, dst_output);\n}\n\nint ip_local_out(struct sk_buff *skb)\n{\n\tint err;\n\n\terr = __ip_local_out(skb);\n\tif (likely(err == 1))\n\t\terr = dst_output(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip_local_out);\n\n/* dev_loopback_xmit for use with netfilter. */\nstatic int ip_dev_loopback_xmit(struct sk_buff *newskb)\n{\n\tskb_reset_mac_header(newskb);\n\t__skb_pull(newskb, skb_network_offset(newskb));\n\tnewskb->pkt_type = PACKET_LOOPBACK;\n\tnewskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(newskb));\n\tnetif_rx_ni(newskb);\n\treturn 0;\n}\n\nstatic inline int ip_select_ttl(struct inet_sock *inet, struct dst_entry *dst)\n{\n\tint ttl = inet->uc_ttl;\n\n\tif (ttl < 0)\n\t\tttl = ip4_dst_hoplimit(dst);\n\treturn ttl;\n}\n\n/*\n *\t\tAdd an ip header to a skbuff and send it out.\n *\n */\nint ip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,\n\t\t\t  __be32 saddr, __be32 daddr, struct ip_options_rcu *opt)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct iphdr *iph;\n\n\t/* Build the IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (opt ? opt->opt.optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tiph->version  = 4;\n\tiph->ihl      = 5;\n\tiph->tos      = inet->tos;\n\tif (ip_dont_fragment(sk, &rt->dst))\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->daddr    = rt->rt_dst;\n\tiph->saddr    = rt->rt_src;\n\tiph->protocol = sk->sk_protocol;\n\tip_select_ident(iph, &rt->dst, sk);\n\n\tif (opt && opt->opt.optlen) {\n\t\tiph->ihl += opt->opt.optlen>>2;\n\t\tip_options_build(skb, &opt->opt, daddr, rt, 0);\n\t}\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\t/* Send it out. */\n\treturn ip_local_out(skb);\n}\nEXPORT_SYMBOL_GPL(ip_build_and_send_pkt);\n\nstatic inline int ip_finish_output2(struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct rtable *rt = (struct rtable *)dst;\n\tstruct net_device *dev = dst->dev;\n\tunsigned int hh_len = LL_RESERVED_SPACE(dev);\n\n\tif (rt->rt_type == RTN_MULTICAST) {\n\t\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTMCAST, skb->len);\n\t} else if (rt->rt_type == RTN_BROADCAST)\n\t\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUTBCAST, skb->len);\n\n\t/* Be paranoid, rather than too clever. */\n\tif (unlikely(skb_headroom(skb) < hh_len && dev->header_ops)) {\n\t\tstruct sk_buff *skb2;\n\n\t\tskb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(dev));\n\t\tif (skb2 == NULL) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(skb2, skb->sk);\n\t\tkfree_skb(skb);\n\t\tskb = skb2;\n\t}\n\n\tif (dst->hh)\n\t\treturn neigh_hh_output(dst->hh, skb);\n\telse if (dst->neighbour)\n\t\treturn dst->neighbour->output(skb);\n\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"ip_finish_output2: No header cache and no neighbour!\\n\");\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic inline int ip_skb_dst_mtu(struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = skb->sk ? inet_sk(skb->sk) : NULL;\n\n\treturn (inet && inet->pmtudisc == IP_PMTUDISC_PROBE) ?\n\t       skb_dst(skb)->dev->mtu : dst_mtu(skb_dst(skb));\n}\n\nstatic int ip_finish_output(struct sk_buff *skb)\n{\n#if defined(CONFIG_NETFILTER) && defined(CONFIG_XFRM)\n\t/* Policy lookup after SNAT yielded a new policy */\n\tif (skb_dst(skb)->xfrm != NULL) {\n\t\tIPCB(skb)->flags |= IPSKB_REROUTED;\n\t\treturn dst_output(skb);\n\t}\n#endif\n\tif (skb->len > ip_skb_dst_mtu(skb) && !skb_is_gso(skb))\n\t\treturn ip_fragment(skb, ip_finish_output2);\n\telse\n\t\treturn ip_finish_output2(skb);\n}\n\nint ip_mc_output(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net_device *dev = rt->dst.dev;\n\n\t/*\n\t *\tIf the indicated interface is up and running, send the packet.\n\t */\n\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb->len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_IP);\n\n\t/*\n\t *\tMulticasts are looped back for other local users\n\t */\n\n\tif (rt->rt_flags&RTCF_MULTICAST) {\n\t\tif (sk_mc_loop(sk)\n#ifdef CONFIG_IP_MROUTE\n\t\t/* Small optimization: do not loopback not local frames,\n\t\t   which returned after forwarding; they will be  dropped\n\t\t   by ip_mr_input in any case.\n\t\t   Note, that local frames are looped back to be delivered\n\t\t   to local recipients.\n\n\t\t   This check is duplicated in ip_mr_input at the moment.\n\t\t */\n\t\t    &&\n\t\t    ((rt->rt_flags & RTCF_LOCAL) ||\n\t\t     !(IPCB(skb)->flags & IPSKB_FORWARDED))\n#endif\n\t\t   ) {\n\t\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\t\t\tif (newskb)\n\t\t\t\tNF_HOOK(NFPROTO_IPV4, NF_INET_POST_ROUTING,\n\t\t\t\t\tnewskb, NULL, newskb->dev,\n\t\t\t\t\tip_dev_loopback_xmit);\n\t\t}\n\n\t\t/* Multicasts with ttl 0 must not go beyond the host */\n\n\t\tif (ip_hdr(skb)->ttl == 0) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (rt->rt_flags&RTCF_BROADCAST) {\n\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (newskb)\n\t\t\tNF_HOOK(NFPROTO_IPV4, NF_INET_POST_ROUTING, newskb,\n\t\t\t\tNULL, newskb->dev, ip_dev_loopback_xmit);\n\t}\n\n\treturn NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL,\n\t\t\t    skb->dev, ip_finish_output,\n\t\t\t    !(IPCB(skb)->flags & IPSKB_REROUTED));\n}\n\nint ip_output(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb_dst(skb)->dev;\n\n\tIP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb->len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_IP);\n\n\treturn NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,\n\t\t\t    ip_finish_output,\n\t\t\t    !(IPCB(skb)->flags & IPSKB_REROUTED));\n}\n\nint ip_queue_xmit(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options_rcu *inet_opt;\n\tstruct rtable *rt;\n\tstruct iphdr *iph;\n\tint res;\n\n\t/* Skip all of this if the packet is already routed,\n\t * f.e. by something like SCTP.\n\t */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\trt = skb_rtable(skb);\n\tif (rt != NULL)\n\t\tgoto packet_routed;\n\n\t/* Make sure we can route this packet. */\n\trt = (struct rtable *)__sk_dst_check(sk, 0);\n\tif (rt == NULL) {\n\t\t__be32 daddr;\n\n\t\t/* Use correct destination address if we have options. */\n\t\tdaddr = inet->inet_daddr;\n\t\tif (inet_opt && inet_opt->opt.srr)\n\t\t\tdaddr = inet_opt->opt.faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport,\n\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol,\n\t\t\t\t\t   RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set_noref(skb, &rt->dst);\n\npacket_routed:\n\tif (inet_opt && inet_opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto no_route;\n\n\t/* OK, we know where to send it, allocate and build IP header. */\n\tskb_push(skb, sizeof(struct iphdr) + (inet_opt ? inet_opt->opt.optlen : 0));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\t*((__be16 *)iph) = htons((4 << 12) | (5 << 8) | (inet->tos & 0xff));\n\tif (ip_dont_fragment(sk, &rt->dst) && !skb->local_df)\n\t\tiph->frag_off = htons(IP_DF);\n\telse\n\t\tiph->frag_off = 0;\n\tiph->ttl      = ip_select_ttl(inet, &rt->dst);\n\tiph->protocol = sk->sk_protocol;\n\tiph->saddr    = rt->rt_src;\n\tiph->daddr    = rt->rt_dst;\n\t/* Transport layer set skb->h.foo itself. */\n\n\tif (inet_opt && inet_opt->opt.optlen) {\n\t\tiph->ihl += inet_opt->opt.optlen >> 2;\n\t\tip_options_build(skb, &inet_opt->opt, inet->inet_daddr, rt, 0);\n\t}\n\n\tip_select_ident_more(iph, &rt->dst, sk,\n\t\t\t     (skb_shinfo(skb)->gso_segs ?: 1) - 1);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tres = ip_local_out(skb);\n\trcu_read_unlock();\n\treturn res;\n\nno_route:\n\trcu_read_unlock();\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}\nEXPORT_SYMBOL(ip_queue_xmit);\n\n\nstatic void ip_copy_metadata(struct sk_buff *to, struct sk_buff *from)\n{\n\tto->pkt_type = from->pkt_type;\n\tto->priority = from->priority;\n\tto->protocol = from->protocol;\n\tskb_dst_drop(to);\n\tskb_dst_copy(to, from);\n\tto->dev = from->dev;\n\tto->mark = from->mark;\n\n\t/* Copy the flags to each fragment. */\n\tIPCB(to)->flags = IPCB(from)->flags;\n\n#ifdef CONFIG_NET_SCHED\n\tto->tc_index = from->tc_index;\n#endif\n\tnf_copy(to, from);\n#if defined(CONFIG_NETFILTER_XT_TARGET_TRACE) || \\\n    defined(CONFIG_NETFILTER_XT_TARGET_TRACE_MODULE)\n\tto->nf_trace = from->nf_trace;\n#endif\n#if defined(CONFIG_IP_VS) || defined(CONFIG_IP_VS_MODULE)\n\tto->ipvs_property = from->ipvs_property;\n#endif\n\tskb_copy_secmark(to, from);\n}\n\n/*\n *\tThis IP datagram is too large to be sent in one piece.  Break it up into\n *\tsmaller pieces (each of size equal to IP header plus\n *\ta block of the data of the original IP data part) that will yet fit in a\n *\tsingle device frame, and queue such a frame for sending.\n */\n\nint ip_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *))\n{\n\tstruct iphdr *iph;\n\tint ptr;\n\tstruct net_device *dev;\n\tstruct sk_buff *skb2;\n\tunsigned int mtu, hlen, left, len, ll_rs;\n\tint offset;\n\t__be16 not_last_frag;\n\tstruct rtable *rt = skb_rtable(skb);\n\tint err = 0;\n\n\tdev = rt->dst.dev;\n\n\t/*\n\t *\tPoint into the IP datagram header.\n\t */\n\n\tiph = ip_hdr(skb);\n\n\tif (unlikely((iph->frag_off & htons(IP_DF)) && !skb->local_df)) {\n\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);\n\t\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,\n\t\t\t  htonl(ip_skb_dst_mtu(skb)));\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t *\tSetup starting values.\n\t */\n\n\thlen = iph->ihl * 4;\n\tmtu = dst_mtu(&rt->dst) - hlen;\t/* Size of data space */\n#ifdef CONFIG_BRIDGE_NETFILTER\n\tif (skb->nf_bridge)\n\t\tmtu -= nf_bridge_mtu_reduction(skb);\n#endif\n\tIPCB(skb)->flags |= IPSKB_FRAG_COMPLETE;\n\n\t/* When frag_list is given, use it. First, check its validity:\n\t * some transformers could create wrong frag_list or break existing\n\t * one, it is not prohibited. In this case fall back to copying.\n\t *\n\t * LATER: this step can be merged to real generation of fragments,\n\t * we can switch to copy when see the first bad fragment.\n\t */\n\tif (skb_has_frag_list(skb)) {\n\t\tstruct sk_buff *frag, *frag2;\n\t\tint first_len = skb_pagelen(skb);\n\n\t\tif (first_len - hlen > mtu ||\n\t\t    ((first_len - hlen) & 7) ||\n\t\t    (iph->frag_off & htons(IP_MF|IP_OFFSET)) ||\n\t\t    skb_cloned(skb))\n\t\t\tgoto slow_path;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\t/* Correct geometry. */\n\t\t\tif (frag->len > mtu ||\n\t\t\t    ((frag->len & 7) && frag->next) ||\n\t\t\t    skb_headroom(frag) < hlen)\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\t/* Partially cloned skb? */\n\t\t\tif (skb_shared(frag))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\tBUG_ON(frag->sk);\n\t\t\tif (skb->sk) {\n\t\t\t\tfrag->sk = skb->sk;\n\t\t\t\tfrag->destructor = sock_wfree;\n\t\t\t}\n\t\t\tskb->truesize -= frag->truesize;\n\t\t}\n\n\t\t/* Everything is OK. Generate! */\n\n\t\terr = 0;\n\t\toffset = 0;\n\t\tfrag = skb_shinfo(skb)->frag_list;\n\t\tskb_frag_list_init(skb);\n\t\tskb->data_len = first_len - skb_headlen(skb);\n\t\tskb->len = first_len;\n\t\tiph->tot_len = htons(first_len);\n\t\tiph->frag_off = htons(IP_MF);\n\t\tip_send_check(iph);\n\n\t\tfor (;;) {\n\t\t\t/* Prepare header of the next frame,\n\t\t\t * before previous one went down. */\n\t\t\tif (frag) {\n\t\t\t\tfrag->ip_summed = CHECKSUM_NONE;\n\t\t\t\tskb_reset_transport_header(frag);\n\t\t\t\t__skb_push(frag, hlen);\n\t\t\t\tskb_reset_network_header(frag);\n\t\t\t\tmemcpy(skb_network_header(frag), iph, hlen);\n\t\t\t\tiph = ip_hdr(frag);\n\t\t\t\tiph->tot_len = htons(frag->len);\n\t\t\t\tip_copy_metadata(frag, skb);\n\t\t\t\tif (offset == 0)\n\t\t\t\t\tip_options_fragment(frag);\n\t\t\t\toffset += skb->len - hlen;\n\t\t\t\tiph->frag_off = htons(offset>>3);\n\t\t\t\tif (frag->next != NULL)\n\t\t\t\t\tiph->frag_off |= htons(IP_MF);\n\t\t\t\t/* Ready, complete checksum */\n\t\t\t\tip_send_check(iph);\n\t\t\t}\n\n\t\t\terr = output(skb);\n\n\t\t\tif (!err)\n\t\t\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGCREATES);\n\t\t\tif (err || !frag)\n\t\t\t\tbreak;\n\n\t\t\tskb = frag;\n\t\t\tfrag = skb->next;\n\t\t\tskb->next = NULL;\n\t\t}\n\n\t\tif (err == 0) {\n\t\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGOKS);\n\t\t\treturn 0;\n\t\t}\n\n\t\twhile (frag) {\n\t\t\tskb = frag->next;\n\t\t\tkfree_skb(frag);\n\t\t\tfrag = skb;\n\t\t}\n\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);\n\t\treturn err;\n\nslow_path_clean:\n\t\tskb_walk_frags(skb, frag2) {\n\t\t\tif (frag2 == frag)\n\t\t\t\tbreak;\n\t\t\tfrag2->sk = NULL;\n\t\t\tfrag2->destructor = NULL;\n\t\t\tskb->truesize += frag2->truesize;\n\t\t}\n\t}\n\nslow_path:\n\tleft = skb->len - hlen;\t\t/* Space per frame */\n\tptr = hlen;\t\t/* Where to start from */\n\n\t/* for bridged IP traffic encapsulated inside f.e. a vlan header,\n\t * we need to make room for the encapsulating header\n\t */\n\tll_rs = LL_RESERVED_SPACE_EXTRA(rt->dst.dev, nf_bridge_pad(skb));\n\n\t/*\n\t *\tFragment the datagram.\n\t */\n\n\toffset = (ntohs(iph->frag_off) & IP_OFFSET) << 3;\n\tnot_last_frag = iph->frag_off & htons(IP_MF);\n\n\t/*\n\t *\tKeep copying data until we run out.\n\t */\n\n\twhile (left > 0) {\n\t\tlen = left;\n\t\t/* IF: it doesn't fit, use 'mtu' - the data space left */\n\t\tif (len > mtu)\n\t\t\tlen = mtu;\n\t\t/* IF: we are not sending up to and including the packet end\n\t\t   then align the next start on an eight byte boundary */\n\t\tif (len < left)\t{\n\t\t\tlen &= ~7;\n\t\t}\n\t\t/*\n\t\t *\tAllocate buffer.\n\t\t */\n\n\t\tif ((skb2 = alloc_skb(len+hlen+ll_rs, GFP_ATOMIC)) == NULL) {\n\t\t\tNETDEBUG(KERN_INFO \"IP: frag: no memory for new fragment!\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t *\tSet up data on packet\n\t\t */\n\n\t\tip_copy_metadata(skb2, skb);\n\t\tskb_reserve(skb2, ll_rs);\n\t\tskb_put(skb2, len + hlen);\n\t\tskb_reset_network_header(skb2);\n\t\tskb2->transport_header = skb2->network_header + hlen;\n\n\t\t/*\n\t\t *\tCharge the memory for the fragment to any owner\n\t\t *\tit might possess\n\t\t */\n\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(skb2, skb->sk);\n\n\t\t/*\n\t\t *\tCopy the packet header into the new buffer.\n\t\t */\n\n\t\tskb_copy_from_linear_data(skb, skb_network_header(skb2), hlen);\n\n\t\t/*\n\t\t *\tCopy a block of the IP datagram.\n\t\t */\n\t\tif (skb_copy_bits(skb, ptr, skb_transport_header(skb2), len))\n\t\t\tBUG();\n\t\tleft -= len;\n\n\t\t/*\n\t\t *\tFill in the new header fields.\n\t\t */\n\t\tiph = ip_hdr(skb2);\n\t\tiph->frag_off = htons((offset >> 3));\n\n\t\t/* ANK: dirty, but effective trick. Upgrade options only if\n\t\t * the segment to be fragmented was THE FIRST (otherwise,\n\t\t * options are already fixed) and make it ONCE\n\t\t * on the initial skb, so that all the following fragments\n\t\t * will inherit fixed options.\n\t\t */\n\t\tif (offset == 0)\n\t\t\tip_options_fragment(skb);\n\n\t\t/*\n\t\t *\tAdded AC : If we are fragmenting a fragment that's not the\n\t\t *\t\t   last fragment then keep MF on each bit\n\t\t */\n\t\tif (left > 0 || not_last_frag)\n\t\t\tiph->frag_off |= htons(IP_MF);\n\t\tptr += len;\n\t\toffset += len;\n\n\t\t/*\n\t\t *\tPut this fragment into the sending queue.\n\t\t */\n\t\tiph->tot_len = htons(len + hlen);\n\n\t\tip_send_check(iph);\n\n\t\terr = output(skb2);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGCREATES);\n\t}\n\tkfree_skb(skb);\n\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGOKS);\n\treturn err;\n\nfail:\n\tkfree_skb(skb);\n\tIP_INC_STATS(dev_net(dev), IPSTATS_MIB_FRAGFAILS);\n\treturn err;\n}\nEXPORT_SYMBOL(ip_fragment);\n\nint\nip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb)\n{\n\tstruct iovec *iov = from;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tif (memcpy_fromiovecend(to, iov, offset, len) < 0)\n\t\t\treturn -EFAULT;\n\t} else {\n\t\t__wsum csum = 0;\n\t\tif (csum_partial_copy_fromiovecend(to, iov, offset, len, &csum) < 0)\n\t\t\treturn -EFAULT;\n\t\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ip_generic_getfrag);\n\nstatic inline __wsum\ncsum_page(struct page *page, int offset, int copy)\n{\n\tchar *kaddr;\n\t__wsum csum;\n\tkaddr = kmap(page);\n\tcsum = csum_partial(kaddr + offset, copy, 0);\n\tkunmap(page);\n\treturn csum;\n}\n\nstatic inline int ip_ufo_append_data(struct sock *sk,\n\t\t\tstruct sk_buff_head *queue,\n\t\t\tint getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length, int hh_len, int fragheaderlen,\n\t\t\tint transhdrlen, int mtu, unsigned int flags)\n{\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* There is support for UDP fragmentation offload by network\n\t * device, so create one single skb packet containing complete\n\t * udp datagram\n\t */\n\tif ((skb = skb_peek_tail(queue)) == NULL) {\n\t\tskb = sock_alloc_send_skb(sk,\n\t\t\thh_len + fragheaderlen + transhdrlen + 20,\n\t\t\t(flags & MSG_DONTWAIT), &err);\n\n\t\tif (skb == NULL)\n\t\t\treturn err;\n\n\t\t/* reserve space for Hardware header */\n\t\tskb_reserve(skb, hh_len);\n\n\t\t/* create space for UDP/IP header */\n\t\tskb_put(skb, fragheaderlen + transhdrlen);\n\n\t\t/* initialize network header pointer */\n\t\tskb_reset_network_header(skb);\n\n\t\t/* initialize protocol header pointer */\n\t\tskb->transport_header = skb->network_header + fragheaderlen;\n\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum = 0;\n\n\t\t/* specify the length of each IP datagram fragment */\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t\t__skb_queue_tail(queue, skb);\n\t}\n\n\treturn skb_append_datato_frags(sk, skb, getfrag, from,\n\t\t\t\t       (length - transhdrlen));\n}\n\nstatic int __ip_append_data(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t    struct inet_cork *cork,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\n\tstruct ip_options *opt = cork->opt;\n\tint hh_len;\n\tint exthdrlen;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tunsigned int maxfraglen, fragheaderlen;\n\tint csummode = CHECKSUM_NONE;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\n\texthdrlen = transhdrlen ? rt->dst.header_len : 0;\n\tlength += exthdrlen;\n\ttranshdrlen += exthdrlen;\n\tmtu = cork->fragsize;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\n\tif (cork->length + length > 0xFFFF - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, rt->rt_dst, inet->inet_dport,\n\t\t\t       mtu-exthdrlen);\n\t\treturn -EMSGSIZE;\n\t}\n\n\t/*\n\t * transhdrlen > 0 means that this is the first fragment and we wish\n\t * it won't be fragmented in the future.\n\t */\n\tif (transhdrlen &&\n\t    length + fragheaderlen <= mtu &&\n\t    rt->dst.dev->features & NETIF_F_V4_CSUM &&\n\t    !exthdrlen)\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tskb = skb_peek_tail(queue);\n\n\tcork->length += length;\n\tif (((length > mtu) || (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n\t\t\t\t\t mtu, flags);\n\t\tif (err)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\t/* So, what's going on in the loop below?\n\t *\n\t * We use calculated fragment length to generate chained skb,\n\t * each of segments is IP fragment ready for sending to network after\n\t * adding appropriate IP header.\n\t */\n\n\tif (!skb)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = mtu - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > mtu - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\t\t\tfraglen = datalen + fragheaderlen;\n\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = fraglen;\n\n\t\t\t/* The last fragment gets additional space at tail.\n\t\t\t * Note, with MSG_MORE we overallocate on fragments,\n\t\t\t * because we have no idea what fragment will be\n\t\t\t * the last.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap) {\n\t\t\t\talloclen += rt->dst.trailer_len;\n\t\t\t\t/* make sure mtu is not reached */\n\t\t\t\tif (datalen > mtu - fragheaderlen - rt->dst.trailer_len)\n\t\t\t\t\tdatalen -= ALIGN(rt->dst.trailer_len, 8);\n\t\t\t}\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len + 15,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (atomic_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len + 15, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(skb == NULL))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\telse\n\t\t\t\t\t/* only the initial fragment is\n\t\t\t\t\t   time stamped */\n\t\t\t\t\tcork->tx_flags = 0;\n\t\t\t}\n\t\t\tif (skb == NULL)\n\t\t\t\tgoto error;\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\t\t\tskb_shinfo(skb)->tx_flags = cork->tx_flags;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tdata += fragheaderlen;\n\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i-1];\n\t\t\tstruct page *page = cork->page;\n\t\t\tint off = cork->off;\n\t\t\tunsigned int left;\n\n\t\t\tif (page && (left = PAGE_SIZE - off) > 0) {\n\t\t\t\tif (copy >= left)\n\t\t\t\t\tcopy = left;\n\t\t\t\tif (page != frag->page) {\n\t\t\t\t\tif (i == MAX_SKB_FRAGS) {\n\t\t\t\t\t\terr = -EMSGSIZE;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tget_page(page);\n\t\t\t\t\tskb_fill_page_desc(skb, i, page, off, 0);\n\t\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t\t}\n\t\t\t} else if (i < MAX_SKB_FRAGS) {\n\t\t\t\tif (copy > PAGE_SIZE)\n\t\t\t\t\tcopy = PAGE_SIZE;\n\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\tif (page == NULL)  {\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tcork->page = page;\n\t\t\t\tcork->off = 0;\n\n\t\t\t\tskb_fill_page_desc(skb, i, page, 0, 0);\n\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t} else {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (getfrag(from, page_address(frag->page)+frag->page_offset+frag->size, offset, copy, skb->len, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tcork->off += copy;\n\t\t\tfrag->size += copy;\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\n\treturn 0;\n\nerror:\n\tcork->length -= length;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic int ip_setup_cork(struct sock *sk, struct inet_cork *cork,\n\t\t\t struct ipcm_cookie *ipc, struct rtable **rtp)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options_rcu *opt;\n\tstruct rtable *rt;\n\n\t/*\n\t * setup for corking.\n\t */\n\topt = ipc->opt;\n\tif (opt) {\n\t\tif (cork->opt == NULL) {\n\t\t\tcork->opt = kmalloc(sizeof(struct ip_options) + 40,\n\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (unlikely(cork->opt == NULL))\n\t\t\t\treturn -ENOBUFS;\n\t\t}\n\t\tmemcpy(cork->opt, &opt->opt, sizeof(struct ip_options) + opt->opt.optlen);\n\t\tcork->flags |= IPCORK_OPT;\n\t\tcork->addr = ipc->addr;\n\t}\n\trt = *rtp;\n\tif (unlikely(!rt))\n\t\treturn -EFAULT;\n\t/*\n\t * We steal reference to this route, caller should not release it\n\t */\n\t*rtp = NULL;\n\tcork->fragsize = inet->pmtudisc == IP_PMTUDISC_PROBE ?\n\t\t\t rt->dst.dev->mtu : dst_mtu(rt->dst.path);\n\tcork->dst = &rt->dst;\n\tcork->length = 0;\n\tcork->tx_flags = ipc->tx_flags;\n\tcork->page = NULL;\n\tcork->off = 0;\n\n\treturn 0;\n}\n\n/*\n *\tip_append_data() and ip_append_page() can make one large IP datagram\n *\tfrom many pieces of data. Each pieces will be holded on the socket\n *\tuntil ip_push_pending_frames() is called. Each piece can be a page\n *\tor non-page data.\n *\n *\tNot only UDP, other transport protocols - e.g. raw sockets - can use\n *\tthis interface potentially.\n *\n *\tLATER: length must be adjusted by pad at tail, when it is required.\n */\nint ip_append_data(struct sock *sk,\n\t\t   int getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t   void *from, int length, int transhdrlen,\n\t\t   struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t   unsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue)) {\n\t\terr = ip_setup_cork(sk, &inet->cork, ipc, rtp);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\ttranshdrlen = 0;\n\t}\n\n\treturn __ip_append_data(sk, &sk->sk_write_queue, &inet->cork, getfrag,\n\t\t\t\tfrom, length, transhdrlen, flags);\n}\n\nssize_t\tip_append_page(struct sock *sk, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\trt = (struct rtable *)inet->cork.dst;\n\tif (inet->cork.flags & IPCORK_OPT)\n\t\topt = inet->cork.opt;\n\n\tif (!(rt->dst.dev->features&NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = inet->cork.fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\n\tif (inet->cork.length + size > 0xFFFF - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, rt->rt_dst, inet->inet_dport, mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL)\n\t\treturn -EINVAL;\n\n\tinet->cork.length += size;\n\tif ((size + skb->len > mtu) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t}\n\n\n\twhile (size > 0) {\n\t\tint i;\n\n\t\tif (skb_is_gso(skb))\n\t\t\tlen = size;\n\t\telse {\n\n\t\t\t/* Check if the remaining data fits into current packet. */\n\t\t\tlen = mtu - skb->len;\n\t\t\tif (len < size)\n\t\t\t\tlen = maxfraglen - skb->len;\n\t\t}\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\ti = skb_shinfo(skb)->nr_frags;\n\t\tif (len > size)\n\t\t\tlen = size;\n\t\tif (skb_can_coalesce(skb, i, page, offset)) {\n\t\t\tskb_shinfo(skb)->frags[i-1].size += len;\n\t\t} else if (i < MAX_SKB_FRAGS) {\n\t\t\tget_page(page);\n\t\t\tskb_fill_page_desc(skb, i, page, offset, len);\n\t\t} else {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\tatomic_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tinet->cork.length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic void ip_cork_release(struct inet_cork *cork)\n{\n\tcork->flags &= ~IPCORK_OPT;\n\tkfree(cork->opt);\n\tcork->opt = NULL;\n\tdst_release(cork->dst);\n\tcork->dst = NULL;\n}\n\n/*\n *\tCombined all pending IP fragments on the socket as one IP datagram\n *\tand push them out.\n */\nstruct sk_buff *__ip_make_skb(struct sock *sk,\n\t\t\t      struct sk_buff_head *queue,\n\t\t\t      struct inet_cork *cork)\n{\n\tstruct sk_buff *skb, *tmp_skb;\n\tstruct sk_buff **tail_skb;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ip_options *opt = NULL;\n\tstruct rtable *rt = (struct rtable *)cork->dst;\n\tstruct iphdr *iph;\n\t__be16 df = 0;\n\t__u8 ttl;\n\n\tif ((skb = __skb_dequeue(queue)) == NULL)\n\t\tgoto out;\n\ttail_skb = &(skb_shinfo(skb)->frag_list);\n\n\t/* move skb->data to ip header from ext header */\n\tif (skb->data < skb_network_header(skb))\n\t\t__skb_pull(skb, skb_network_offset(skb));\n\twhile ((tmp_skb = __skb_dequeue(queue)) != NULL) {\n\t\t__skb_pull(tmp_skb, skb_network_header_len(skb));\n\t\t*tail_skb = tmp_skb;\n\t\ttail_skb = &(tmp_skb->next);\n\t\tskb->len += tmp_skb->len;\n\t\tskb->data_len += tmp_skb->len;\n\t\tskb->truesize += tmp_skb->truesize;\n\t\ttmp_skb->destructor = NULL;\n\t\ttmp_skb->sk = NULL;\n\t}\n\n\t/* Unless user demanded real pmtu discovery (IP_PMTUDISC_DO), we allow\n\t * to fragment the frame generated here. No matter, what transforms\n\t * how transforms change size of the packet, it will come out.\n\t */\n\tif (inet->pmtudisc < IP_PMTUDISC_DO)\n\t\tskb->local_df = 1;\n\n\t/* DF bit is set when we want to see DF on outgoing frames.\n\t * If local_df is set too, we still allow to fragment this frame\n\t * locally. */\n\tif (inet->pmtudisc >= IP_PMTUDISC_DO ||\n\t    (skb->len <= dst_mtu(&rt->dst) &&\n\t     ip_dont_fragment(sk, &rt->dst)))\n\t\tdf = htons(IP_DF);\n\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (rt->rt_type == RTN_MULTICAST)\n\t\tttl = inet->mc_ttl;\n\telse\n\t\tttl = ip_select_ttl(inet, &rt->dst);\n\n\tiph = (struct iphdr *)skb->data;\n\tiph->version = 4;\n\tiph->ihl = 5;\n\tif (opt) {\n\t\tiph->ihl += opt->optlen>>2;\n\t\tip_options_build(skb, opt, cork->addr, rt, 0);\n\t}\n\tiph->tos = inet->tos;\n\tiph->frag_off = df;\n\tip_select_ident(iph, &rt->dst, sk);\n\tiph->ttl = ttl;\n\tiph->protocol = sk->sk_protocol;\n\tiph->saddr = rt->rt_src;\n\tiph->daddr = rt->rt_dst;\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\t/*\n\t * Steal rt from cork.dst to avoid a pair of atomic_inc/atomic_dec\n\t * on dst refcount\n\t */\n\tcork->dst = NULL;\n\tskb_dst_set(skb, &rt->dst);\n\n\tif (iph->protocol == IPPROTO_ICMP)\n\t\ticmp_out_count(net, ((struct icmphdr *)\n\t\t\tskb_transport_header(skb))->type);\n\n\tip_cork_release(cork);\nout:\n\treturn skb;\n}\n\nint ip_send_skb(struct sk_buff *skb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tint err;\n\n\terr = ip_local_out(skb);\n\tif (err) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tIP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);\n\t}\n\n\treturn err;\n}\n\nint ip_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\tskb = ip_finish_skb(sk);\n\tif (!skb)\n\t\treturn 0;\n\n\t/* Netfilter gets whole the not fragmented skb. */\n\treturn ip_send_skb(skb);\n}\n\n/*\n *\tThrow away all pending data on the socket.\n */\nstatic void __ip_flush_pending_frames(struct sock *sk,\n\t\t\t\t      struct sk_buff_head *queue,\n\t\t\t\t      struct inet_cork *cork)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(queue)) != NULL)\n\t\tkfree_skb(skb);\n\n\tip_cork_release(cork);\n}\n\nvoid ip_flush_pending_frames(struct sock *sk)\n{\n\t__ip_flush_pending_frames(sk, &sk->sk_write_queue, &inet_sk(sk)->cork);\n}\n\nstruct sk_buff *ip_make_skb(struct sock *sk,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t\t    unsigned int flags)\n{\n\tstruct inet_cork cork = {};\n\tstruct sk_buff_head queue;\n\tint err;\n\n\tif (flags & MSG_PROBE)\n\t\treturn NULL;\n\n\t__skb_queue_head_init(&queue);\n\n\terr = ip_setup_cork(sk, &cork, ipc, rtp);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\terr = __ip_append_data(sk, &queue, &cork, getfrag,\n\t\t\t       from, length, transhdrlen, flags);\n\tif (err) {\n\t\t__ip_flush_pending_frames(sk, &queue, &cork);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn __ip_make_skb(sk, &queue, &cork);\n}\n\n/*\n *\tFetch data from kernel space and fill in checksum if needed.\n */\nstatic int ip_reply_glue_bits(void *dptr, char *to, int offset,\n\t\t\t      int len, int odd, struct sk_buff *skb)\n{\n\t__wsum csum;\n\n\tcsum = csum_partial_copy_nocheck(dptr+offset, to, len, 0);\n\tskb->csum = csum_block_add(skb->csum, csum, odd);\n\treturn 0;\n}\n\n/*\n *\tGeneric function to send a packet as reply to another packet.\n *\tUsed to send TCP resets so far. ICMP should use this function too.\n *\n *\tShould run single threaded per socket because it uses the sock\n *     \tstructure to pass arguments.\n */\nvoid ip_send_reply(struct sock *sk, struct sk_buff *skb, struct ip_reply_arg *arg,\n\t\t   unsigned int len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ip_options_data replyopts;\n\tstruct ipcm_cookie ipc;\n\t__be32 daddr;\n\tstruct rtable *rt = skb_rtable(skb);\n\n\tif (ip_options_echo(&replyopts.opt.opt, skb))\n\t\treturn;\n\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tif (replyopts.opt.opt.optlen) {\n\t\tipc.opt = &replyopts.opt;\n\n\t\tif (replyopts.opt.opt.srr)\n\t\t\tdaddr = replyopts.opt.opt.faddr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, arg->bound_dev_if, 0,\n\t\t\t\t   RT_TOS(ip_hdr(skb)->tos),\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   ip_reply_arg_flowi_flags(arg),\n\t\t\t\t   daddr, rt->rt_spec_dst,\n\t\t\t\t   tcp_hdr(skb)->source, tcp_hdr(skb)->dest);\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\treturn;\n\t}\n\n\t/* And let IP do all the hard work.\n\n\t   This chunk is not reenterable, hence spinlock.\n\t   Note that it uses the fact, that this function is called\n\t   with locally disabled BH and that sk cannot be already spinlocked.\n\t */\n\tbh_lock_sock(sk);\n\tinet->tos = ip_hdr(skb)->tos;\n\tsk->sk_priority = skb->priority;\n\tsk->sk_protocol = ip_hdr(skb)->protocol;\n\tsk->sk_bound_dev_if = arg->bound_dev_if;\n\tip_append_data(sk, ip_reply_glue_bits, arg->iov->iov_base, len, 0,\n\t\t       &ipc, &rt, MSG_DONTWAIT);\n\tif ((skb = skb_peek(&sk->sk_write_queue)) != NULL) {\n\t\tif (arg->csumoffset >= 0)\n\t\t\t*((__sum16 *)skb_transport_header(skb) +\n\t\t\t  arg->csumoffset) = csum_fold(csum_add(skb->csum,\n\t\t\t\t\t\t\t\targ->csum));\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tip_push_pending_frames(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\n\tip_rt_put(rt);\n}\n\nvoid __init ip_init(void)\n{\n\tip_rt_init();\n\tinet_initpeers();\n\n#if defined(CONFIG_IP_MULTICAST) && defined(CONFIG_PROC_FS)\n\tigmp_mc_proc_init();\n#endif\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe IP to API glue.\n *\n * Authors:\tsee ip.c\n *\n * Fixes:\n *\t\tMany\t\t:\tSplit from ip.c , see ip.c for history.\n *\t\tMartin Mares\t:\tTOS setting fixed.\n *\t\tAlan Cox\t:\tFixed a couple of oopses in Martin's\n *\t\t\t\t\tTOS tweaks.\n *\t\tMike McLagan\t:\tRouting by source\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/skbuff.h>\n#include <linux/ip.h>\n#include <linux/icmp.h>\n#include <linux/inetdevice.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/tcp_states.h>\n#include <linux/udp.h>\n#include <linux/igmp.h>\n#include <linux/netfilter.h>\n#include <linux/route.h>\n#include <linux/mroute.h>\n#include <net/route.h>\n#include <net/xfrm.h>\n#include <net/compat.h>\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n#include <net/transp_v6.h>\n#endif\n\n#include <linux/errqueue.h>\n#include <asm/uaccess.h>\n\n#define IP_CMSG_PKTINFO\t\t1\n#define IP_CMSG_TTL\t\t2\n#define IP_CMSG_TOS\t\t4\n#define IP_CMSG_RECVOPTS\t8\n#define IP_CMSG_RETOPTS\t\t16\n#define IP_CMSG_PASSSEC\t\t32\n#define IP_CMSG_ORIGDSTADDR     64\n\n/*\n *\tSOL_IP control messages.\n */\n\nstatic void ip_cmsg_recv_pktinfo(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct in_pktinfo info;\n\tstruct rtable *rt = skb_rtable(skb);\n\n\tinfo.ipi_addr.s_addr = ip_hdr(skb)->daddr;\n\tif (rt) {\n\t\tinfo.ipi_ifindex = rt->rt_iif;\n\t\tinfo.ipi_spec_dst.s_addr = rt->rt_spec_dst;\n\t} else {\n\t\tinfo.ipi_ifindex = 0;\n\t\tinfo.ipi_spec_dst.s_addr = 0;\n\t}\n\n\tput_cmsg(msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);\n}\n\nstatic void ip_cmsg_recv_ttl(struct msghdr *msg, struct sk_buff *skb)\n{\n\tint ttl = ip_hdr(skb)->ttl;\n\tput_cmsg(msg, SOL_IP, IP_TTL, sizeof(int), &ttl);\n}\n\nstatic void ip_cmsg_recv_tos(struct msghdr *msg, struct sk_buff *skb)\n{\n\tput_cmsg(msg, SOL_IP, IP_TOS, 1, &ip_hdr(skb)->tos);\n}\n\nstatic void ip_cmsg_recv_opts(struct msghdr *msg, struct sk_buff *skb)\n{\n\tif (IPCB(skb)->opt.optlen == 0)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_IP, IP_RECVOPTS, IPCB(skb)->opt.optlen,\n\t\t ip_hdr(skb) + 1);\n}\n\n\nstatic void ip_cmsg_recv_retopts(struct msghdr *msg, struct sk_buff *skb)\n{\n\tunsigned char optbuf[sizeof(struct ip_options) + 40];\n\tstruct ip_options * opt = (struct ip_options *)optbuf;\n\n\tif (IPCB(skb)->opt.optlen == 0)\n\t\treturn;\n\n\tif (ip_options_echo(opt, skb)) {\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\treturn;\n\t}\n\tip_options_undo(opt);\n\n\tput_cmsg(msg, SOL_IP, IP_RETOPTS, opt->optlen, opt->__data);\n}\n\nstatic void ip_cmsg_recv_security(struct msghdr *msg, struct sk_buff *skb)\n{\n\tchar *secdata;\n\tu32 seclen, secid;\n\tint err;\n\n\terr = security_socket_getpeersec_dgram(NULL, skb, &secid);\n\tif (err)\n\t\treturn;\n\n\terr = security_secid_to_secctx(secid, &secdata, &seclen);\n\tif (err)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_IP, SCM_SECURITY, seclen, secdata);\n\tsecurity_release_secctx(secdata, seclen);\n}\n\nstatic void ip_cmsg_recv_dstaddr(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct sockaddr_in sin;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\t__be16 *ports = (__be16 *)skb_transport_header(skb);\n\n\tif (skb_transport_offset(skb) + 4 > skb->len)\n\t\treturn;\n\n\t/* All current transport protocols have the port numbers in the\n\t * first four bytes of the transport header and this function is\n\t * written with this assumption in mind.\n\t */\n\n\tsin.sin_family = AF_INET;\n\tsin.sin_addr.s_addr = iph->daddr;\n\tsin.sin_port = ports[1];\n\tmemset(sin.sin_zero, 0, sizeof(sin.sin_zero));\n\n\tput_cmsg(msg, SOL_IP, IP_ORIGDSTADDR, sizeof(sin), &sin);\n}\n\nvoid ip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(skb->sk);\n\tunsigned flags = inet->cmsg_flags;\n\n\t/* Ordered by supposed usage frequency */\n\tif (flags & 1)\n\t\tip_cmsg_recv_pktinfo(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_ttl(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_tos(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_opts(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_retopts(msg, skb);\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\n\tif (flags & 1)\n\t\tip_cmsg_recv_security(msg, skb);\n\n\tif ((flags >>= 1) == 0)\n\t\treturn;\n\tif (flags & 1)\n\t\tip_cmsg_recv_dstaddr(msg, skb);\n\n}\nEXPORT_SYMBOL(ip_cmsg_recv);\n\nint ip_cmsg_send(struct net *net, struct msghdr *msg, struct ipcm_cookie *ipc)\n{\n\tint err;\n\tstruct cmsghdr *cmsg;\n\n\tfor (cmsg = CMSG_FIRSTHDR(msg); cmsg; cmsg = CMSG_NXTHDR(msg, cmsg)) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_level != SOL_IP)\n\t\t\tcontinue;\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase IP_RETOPTS:\n\t\t\terr = cmsg->cmsg_len - CMSG_ALIGN(sizeof(struct cmsghdr));\n\t\t\terr = ip_options_get(net, &ipc->opt, CMSG_DATA(cmsg),\n\t\t\t\t\t     err < 40 ? err : 40);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\t\tcase IP_PKTINFO:\n\t\t{\n\t\t\tstruct in_pktinfo *info;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(struct in_pktinfo)))\n\t\t\t\treturn -EINVAL;\n\t\t\tinfo = (struct in_pktinfo *)CMSG_DATA(cmsg);\n\t\t\tipc->oif = info->ipi_ifindex;\n\t\t\tipc->addr = info->ipi_spec_dst.s_addr;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\n\n/* Special input handler for packets caught by router alert option.\n   They are selected only by protocol field, and then processed likely\n   local ones; but only if someone wants them! Otherwise, router\n   not running rsvpd will kill RSVP.\n\n   It is user level problem, what it will make with them.\n   I have no idea, how it will masquearde or NAT them (it is joke, joke :-)),\n   but receiver should be enough clever f.e. to forward mtrace requests,\n   sent to multicast group to reach destination designated router.\n */\nstruct ip_ra_chain __rcu *ip_ra_chain;\nstatic DEFINE_SPINLOCK(ip_ra_lock);\n\n\nstatic void ip_ra_destroy_rcu(struct rcu_head *head)\n{\n\tstruct ip_ra_chain *ra = container_of(head, struct ip_ra_chain, rcu);\n\n\tsock_put(ra->saved_sk);\n\tkfree(ra);\n}\n\nint ip_ra_control(struct sock *sk, unsigned char on,\n\t\t  void (*destructor)(struct sock *))\n{\n\tstruct ip_ra_chain *ra, *new_ra;\n\tstruct ip_ra_chain __rcu **rap;\n\n\tif (sk->sk_type != SOCK_RAW || inet_sk(sk)->inet_num == IPPROTO_RAW)\n\t\treturn -EINVAL;\n\n\tnew_ra = on ? kmalloc(sizeof(*new_ra), GFP_KERNEL) : NULL;\n\n\tspin_lock_bh(&ip_ra_lock);\n\tfor (rap = &ip_ra_chain;\n\t     (ra = rcu_dereference_protected(*rap,\n\t\t\tlockdep_is_held(&ip_ra_lock))) != NULL;\n\t     rap = &ra->next) {\n\t\tif (ra->sk == sk) {\n\t\t\tif (on) {\n\t\t\t\tspin_unlock_bh(&ip_ra_lock);\n\t\t\t\tkfree(new_ra);\n\t\t\t\treturn -EADDRINUSE;\n\t\t\t}\n\t\t\t/* dont let ip_call_ra_chain() use sk again */\n\t\t\tra->sk = NULL;\n\t\t\trcu_assign_pointer(*rap, ra->next);\n\t\t\tspin_unlock_bh(&ip_ra_lock);\n\n\t\t\tif (ra->destructor)\n\t\t\t\tra->destructor(sk);\n\t\t\t/*\n\t\t\t * Delay sock_put(sk) and kfree(ra) after one rcu grace\n\t\t\t * period. This guarantee ip_call_ra_chain() dont need\n\t\t\t * to mess with socket refcounts.\n\t\t\t */\n\t\t\tra->saved_sk = sk;\n\t\t\tcall_rcu(&ra->rcu, ip_ra_destroy_rcu);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (new_ra == NULL) {\n\t\tspin_unlock_bh(&ip_ra_lock);\n\t\treturn -ENOBUFS;\n\t}\n\tnew_ra->sk = sk;\n\tnew_ra->destructor = destructor;\n\n\tnew_ra->next = ra;\n\trcu_assign_pointer(*rap, new_ra);\n\tsock_hold(sk);\n\tspin_unlock_bh(&ip_ra_lock);\n\n\treturn 0;\n}\n\nvoid ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err,\n\t\t   __be16 port, u32 info, u8 *payload)\n{\n\tstruct sock_exterr_skb *serr;\n\n\tskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_ICMP;\n\tserr->ee.ee_type = icmp_hdr(skb)->type;\n\tserr->ee.ee_code = icmp_hdr(skb)->code;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&(((struct iphdr *)(icmp_hdr(skb) + 1))->daddr) -\n\t\t\t\t   skb_network_header(skb);\n\tserr->port = port;\n\n\tif (skb_pull(skb, payload - skb->data) != NULL) {\n\t\tskb_reset_transport_header(skb);\n\t\tif (sock_queue_err_skb(sk, skb) == 0)\n\t\t\treturn;\n\t}\n\tkfree_skb(skb);\n}\n\nvoid ip_local_error(struct sock *sk, int err, __be32 daddr, __be16 port, u32 info)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sock_exterr_skb *serr;\n\tstruct iphdr *iph;\n\tstruct sk_buff *skb;\n\n\tif (!inet->recverr)\n\t\treturn;\n\n\tskb = alloc_skb(sizeof(struct iphdr), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb_put(skb, sizeof(struct iphdr));\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tiph->daddr = daddr;\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_LOCAL;\n\tserr->ee.ee_type = 0;\n\tserr->ee.ee_code = 0;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&iph->daddr - skb_network_header(skb);\n\tserr->port = port;\n\n\t__skb_pull(skb, skb_tail_pointer(skb) - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tif (sock_queue_err_skb(sk, skb))\n\t\tkfree_skb(skb);\n}\n\n/*\n *\tHandle MSG_ERRQUEUE\n */\nint ip_recv_error(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb, *skb2;\n\tstruct sockaddr_in *sin;\n\tstruct {\n\t\tstruct sock_extended_err ee;\n\t\tstruct sockaddr_in\t offender;\n\t} errhdr;\n\tint err;\n\tint copied;\n\n\terr = -EAGAIN;\n\tskb = skb_dequeue(&sk->sk_error_queue);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\n\tsin = (struct sockaddr_in *)msg->msg_name;\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = *(__be32 *)(skb_network_header(skb) +\n\t\t\t\t\t\t   serr->addr_offset);\n\t\tsin->sin_port = serr->port;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\n\tmemcpy(&errhdr.ee, &serr->ee, sizeof(struct sock_extended_err));\n\tsin = &errhdr.offender;\n\tsin->sin_family = AF_UNSPEC;\n\tif (serr->ee.ee_origin == SO_EE_ORIGIN_ICMP) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t}\n\n\tput_cmsg(msg, SOL_IP, IP_RECVERR, sizeof(errhdr), &errhdr);\n\n\t/* Now we could try to dump offended packet options */\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\n\t/* Reset and regenerate socket error */\n\tspin_lock_bh(&sk->sk_error_queue.lock);\n\tsk->sk_err = 0;\n\tskb2 = skb_peek(&sk->sk_error_queue);\n\tif (skb2 != NULL) {\n\t\tsk->sk_err = SKB_EXT_ERR(skb2)->ee.ee_errno;\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\n\n\nstatic void opt_kfree_rcu(struct rcu_head *head)\n{\n\tkfree(container_of(head, struct ip_options_rcu, rcu));\n}\n\n/*\n *\tSocket option code for IP. This is the end of the line after any\n *\tTCP,UDP etc options on an IP socket.\n */\n\nstatic int do_ip_setsockopt(struct sock *sk, int level,\n\t\t\t    int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val = 0, err;\n\n\tif (((1<<optname) & ((1<<IP_PKTINFO) | (1<<IP_RECVTTL) |\n\t\t\t     (1<<IP_RECVOPTS) | (1<<IP_RECVTOS) |\n\t\t\t     (1<<IP_RETOPTS) | (1<<IP_TOS) |\n\t\t\t     (1<<IP_TTL) | (1<<IP_HDRINCL) |\n\t\t\t     (1<<IP_MTU_DISCOVER) | (1<<IP_RECVERR) |\n\t\t\t     (1<<IP_ROUTER_ALERT) | (1<<IP_FREEBIND) |\n\t\t\t     (1<<IP_PASSSEC) | (1<<IP_TRANSPARENT) |\n\t\t\t     (1<<IP_MINTTL) | (1<<IP_NODEFRAG))) ||\n\t    optname == IP_MULTICAST_TTL ||\n\t    optname == IP_MULTICAST_ALL ||\n\t    optname == IP_MULTICAST_LOOP ||\n\t    optname == IP_RECVORIGDSTADDR) {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else if (optlen >= sizeof(char)) {\n\t\t\tunsigned char ucval;\n\n\t\t\tif (get_user(ucval, (unsigned char __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t\tval = (int) ucval;\n\t\t}\n\t}\n\n\t/* If optlen==0, it is equivalent to val == 0 */\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_setsockopt(sk, optname, optval, optlen);\n\n\terr = 0;\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tstruct ip_options_rcu *old, *opt = NULL;\n\n\t\tif (optlen > 40)\n\t\t\tgoto e_inval;\n\t\terr = ip_options_get_from_user(sock_net(sk), &opt,\n\t\t\t\t\t       optval, optlen);\n\t\tif (err)\n\t\t\tbreak;\n\t\told = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t\tsock_owned_by_user(sk));\n\t\tif (inet->is_icsk) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\tif (sk->sk_family == PF_INET ||\n\t\t\t    (!((1 << sk->sk_state) &\n\t\t\t       (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t\t     inet->inet_daddr != LOOPBACK4_IPV6)) {\n#endif\n\t\t\t\tif (old)\n\t\t\t\t\ticsk->icsk_ext_hdr_len -= old->opt.optlen;\n\t\t\t\tif (opt)\n\t\t\t\t\ticsk->icsk_ext_hdr_len += opt->opt.optlen;\n\t\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\t\t\t}\n#endif\n\t\t}\n\t\trcu_assign_pointer(inet->inet_opt, opt);\n\t\tif (old)\n\t\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n\t\tbreak;\n\t}\n\tcase IP_PKTINFO:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PKTINFO;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PKTINFO;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TTL;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TTL;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_TOS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_TOS;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |=  IP_CMSG_RECVOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RECVOPTS;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_RETOPTS;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_RETOPTS;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_PASSSEC;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_PASSSEC;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tif (val)\n\t\t\tinet->cmsg_flags |= IP_CMSG_ORIGDSTADDR;\n\t\telse\n\t\t\tinet->cmsg_flags &= ~IP_CMSG_ORIGDSTADDR;\n\t\tbreak;\n\tcase IP_TOS:\t/* This sets both TOS and Precedence */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~3;\n\t\t\tval |= inet->tos & 3;\n\t\t}\n\t\tif (inet->tos != val) {\n\t\t\tinet->tos = val;\n\t\t\tsk->sk_priority = rt_tos2priority(val);\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tbreak;\n\tcase IP_TTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != -1 && (val < 0 || val > 255))\n\t\t\tgoto e_inval;\n\t\tinet->uc_ttl = val;\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->hdrincl = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tif (sk->sk_type != SOCK_RAW) {\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tinet->nodefrag = val ? 1 : 0;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tif (val < IP_PMTUDISC_DONT || val > IP_PMTUDISC_PROBE)\n\t\t\tgoto e_inval;\n\t\tinet->pmtudisc = val;\n\t\tbreak;\n\tcase IP_RECVERR:\n\t\tinet->recverr = !!val;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val == -1)\n\t\t\tval = 1;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->mc_ttl = val;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_loop = !!val;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct ip_mreqn mreq;\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tgoto e_inval;\n\t\t/*\n\t\t *\tCheck the arguments are allowable\n\t\t */\n\n\t\tif (optlen < sizeof(struct in_addr))\n\t\t\tgoto e_inval;\n\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (optlen >= sizeof(struct in_addr) &&\n\t\t\t    copy_from_user(&mreq.imr_address, optval,\n\t\t\t\t\t   sizeof(struct in_addr)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!mreq.imr_ifindex) {\n\t\t\tif (mreq.imr_address.s_addr == htonl(INADDR_ANY)) {\n\t\t\t\tinet->mc_index = 0;\n\t\t\t\tinet->mc_addr  = 0;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev = ip_dev_find(sock_net(sk), mreq.imr_address.s_addr);\n\t\t\tif (dev)\n\t\t\t\tmreq.imr_ifindex = dev->ifindex;\n\t\t} else\n\t\t\tdev = dev_get_by_index(sock_net(sk), mreq.imr_ifindex);\n\n\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\terr = -EINVAL;\n\t\tif (sk->sk_bound_dev_if &&\n\t\t    mreq.imr_ifindex != sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tinet->mc_index = mreq.imr_ifindex;\n\t\tinet->mc_addr  = mreq.imr_address.s_addr;\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\tcase IP_ADD_MEMBERSHIP:\n\tcase IP_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreqn mreq;\n\n\t\terr = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tif (optlen < sizeof(struct ip_mreq))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (optlen >= sizeof(struct ip_mreqn)) {\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(mreq)))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\t\tif (copy_from_user(&mreq, optval, sizeof(struct ip_mreq)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (optname == IP_ADD_MEMBERSHIP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter *msf;\n\n\t\tif (optlen < IP_MSFILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tmsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(msf, optval, optlen)) {\n\t\t\tkfree(msf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (1G-4) overflow in 32 bits */\n\t\tif (msf->imsf_numsrc >= 0x3ffffffcU ||\n\t\t    msf->imsf_numsrc > sysctl_igmp_max_msf) {\n\t\t\tkfree(msf);\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (IP_MSFILTER_SIZE(msf->imsf_numsrc) > optlen) {\n\t\t\tkfree(msf);\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\terr = ip_mc_msfilter(sk, msf, 0);\n\t\tkfree(msf);\n\t\tbreak;\n\t}\n\tcase IP_BLOCK_SOURCE:\n\tcase IP_UNBLOCK_SOURCE:\n\tcase IP_ADD_SOURCE_MEMBERSHIP:\n\tcase IP_DROP_SOURCE_MEMBERSHIP:\n\t{\n\t\tstruct ip_mreq_source mreqs;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct ip_mreq_source))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&mreqs, optval, sizeof(mreqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == IP_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == IP_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == IP_ADD_SOURCE_MEMBERSHIP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tmreq.imr_multiaddr.s_addr = mreqs.imr_multiaddr;\n\t\t\tmreq.imr_address.s_addr = mreqs.imr_interface;\n\t\t\tmreq.imr_ifindex = 0;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* IP_DROP_SOURCE_MEMBERSHIP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs, 0);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_mreqn mreq;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(greq)))\n\t\t\tbreak;\n\t\tpsin = (struct sockaddr_in *)&greq.gr_group;\n\t\tif (psin->sin_family != AF_INET)\n\t\t\tgoto e_inval;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\tmreq.imr_ifindex = greq.gr_interface;\n\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\telse\n\t\t\terr = ip_mc_leave_group(sk, &mreq);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tstruct ip_mreq_source mreqs;\n\t\tstruct sockaddr_in *psin;\n\t\tint omode, add;\n\n\t\tif (optlen != sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET ||\n\t\t    greqs.gsr_source.ss_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\tmreqs.imr_multiaddr = psin->sin_addr.s_addr;\n\t\tpsin = (struct sockaddr_in *)&greqs.gsr_source;\n\t\tmreqs.imr_sourceaddr = psin->sin_addr.s_addr;\n\t\tmreqs.imr_interface = 0; /* use index for mc_source */\n\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct ip_mreqn mreq;\n\n\t\t\tpsin = (struct sockaddr_in *)&greqs.gsr_group;\n\t\t\tmreq.imr_multiaddr = psin->sin_addr;\n\t\t\tmreq.imr_address.s_addr = 0;\n\t\t\tmreq.imr_ifindex = greqs.gsr_interface;\n\t\t\terr = ip_mc_join_group(sk, &mreq);\n\t\t\tif (err && err != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tgreqs.gsr_interface = mreq.imr_ifindex;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\terr = ip_mc_source(add, omode, sk, &mreqs,\n\t\t\t\t   greqs.gsr_interface);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct sockaddr_in *psin;\n\t\tstruct ip_msfilter *msf = NULL;\n\t\tstruct group_filter *gsf = NULL;\n\t\tint msize, i, ifindex;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen))\n\t\t\tgoto mc_msf_out;\n\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffff ||\n\t\t    gsf->gf_numsrc > sysctl_igmp_max_msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsize = IP_MSFILTER_SIZE(gsf->gf_numsrc);\n\t\tmsf = kmalloc(msize, GFP_KERNEL);\n\t\tif (!msf) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tifindex = gsf->gf_interface;\n\t\tpsin = (struct sockaddr_in *)&gsf->gf_group;\n\t\tif (psin->sin_family != AF_INET) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto mc_msf_out;\n\t\t}\n\t\tmsf->imsf_multiaddr = psin->sin_addr.s_addr;\n\t\tmsf->imsf_interface = 0;\n\t\tmsf->imsf_fmode = gsf->gf_fmode;\n\t\tmsf->imsf_numsrc = gsf->gf_numsrc;\n\t\terr = -EADDRNOTAVAIL;\n\t\tfor (i = 0; i < gsf->gf_numsrc; ++i) {\n\t\t\tpsin = (struct sockaddr_in *)&gsf->gf_slist[i];\n\n\t\t\tif (psin->sin_family != AF_INET)\n\t\t\t\tgoto mc_msf_out;\n\t\t\tmsf->imsf_slist[i] = psin->sin_addr.s_addr;\n\t\t}\n\t\tkfree(gsf);\n\t\tgsf = NULL;\n\n\t\terr = ip_mc_msfilter(sk, msf, ifindex);\nmc_msf_out:\n\t\tkfree(msf);\n\t\tkfree(gsf);\n\t\tbreak;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val != 0 && val != 1)\n\t\t\tgoto e_inval;\n\t\tinet->mc_all = val;\n\t\tbreak;\n\tcase IP_ROUTER_ALERT:\n\t\terr = ip_ra_control(sk, val ? 1 : 0, NULL);\n\t\tbreak;\n\n\tcase IP_FREEBIND:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->freebind = !!val;\n\t\tbreak;\n\n\tcase IP_IPSEC_POLICY:\n\tcase IP_XFRM_POLICY:\n\t\terr = -EPERM;\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\terr = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IP_TRANSPARENT:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tinet->transparent = !!val;\n\t\tbreak;\n\n\tcase IP_MINTTL:\n\t\tif (optlen < 1)\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tinet->min_ttl = val;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn err;\n\ne_inval:\n\trelease_sock(sk);\n\treturn -EINVAL;\n}\n\n/**\n * ip_queue_rcv_skb - Queue an skb into sock receive queue\n * @sk: socket\n * @skb: buffer\n *\n * Queues an skb into socket receive queue. If IP_CMSG_PKTINFO option\n * is not set, we drop skb dst entry now, while dst cache line is hot.\n */\nint ip_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif (!(inet_sk(sk)->cmsg_flags & IP_CMSG_PKTINFO))\n\t\tskb_dst_drop(skb);\n\treturn sock_queue_rcv_skb(sk, skb);\n}\nEXPORT_SYMBOL(ip_queue_rcv_skb);\n\nint ip_setsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, unsigned int optlen)\n{\n\tint err;\n\n\tif (level != SOL_IP)\n\t\treturn -ENOPROTOOPT;\n\n\terr = do_ip_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_HDRINCL &&\n\t\t\toptname != IP_IPSEC_POLICY &&\n\t\t\toptname != IP_XFRM_POLICY &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tlock_sock(sk);\n\t\terr = nf_setsockopt(sk, PF_INET, optname, optval, optlen);\n\t\trelease_sock(sk);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ip_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_ip_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t char __user *optval, unsigned int optlen)\n{\n\tint err;\n\n\tif (level != SOL_IP)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optname >= MCAST_JOIN_GROUP && optname <= MCAST_MSFILTER)\n\t\treturn compat_mc_setsockopt(sk, level, optname, optval, optlen,\n\t\t\tip_setsockopt);\n\n\terr = do_ip_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_HDRINCL &&\n\t\t\toptname != IP_IPSEC_POLICY &&\n\t\t\toptname != IP_XFRM_POLICY &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tlock_sock(sk);\n\t\terr = compat_nf_setsockopt(sk, PF_INET, optname,\n\t\t\t\t\t   optval, optlen);\n\t\trelease_sock(sk);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(compat_ip_setsockopt);\n#endif\n\n/*\n *\tGet the options. Note for future reference. The GET of IP options gets\n *\tthe _received_ ones. The set sets the _sent_ ones.\n */\n\nstatic int do_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint val;\n\tint len;\n\n\tif (level != SOL_IP)\n\t\treturn -EOPNOTSUPP;\n\n\tif (ip_mroute_opt(optname))\n\t\treturn ip_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase IP_OPTIONS:\n\t{\n\t\tunsigned char optbuf[sizeof(struct ip_options)+40];\n\t\tstruct ip_options *opt = (struct ip_options *)optbuf;\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t\t     sock_owned_by_user(sk));\n\t\topt->optlen = 0;\n\t\tif (inet_opt)\n\t\t\tmemcpy(optbuf, &inet_opt->opt,\n\t\t\t       sizeof(struct ip_options) +\n\t\t\t       inet_opt->opt.optlen);\n\t\trelease_sock(sk);\n\n\t\tif (opt->optlen == 0)\n\t\t\treturn put_user(0, optlen);\n\n\t\tip_options_undo(opt);\n\n\t\tlen = min_t(unsigned int, len, opt->optlen);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, opt->__data, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_PKTINFO:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PKTINFO) != 0;\n\t\tbreak;\n\tcase IP_RECVTTL:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TTL) != 0;\n\t\tbreak;\n\tcase IP_RECVTOS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_TOS) != 0;\n\t\tbreak;\n\tcase IP_RECVOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RECVOPTS) != 0;\n\t\tbreak;\n\tcase IP_RETOPTS:\n\t\tval = (inet->cmsg_flags & IP_CMSG_RETOPTS) != 0;\n\t\tbreak;\n\tcase IP_PASSSEC:\n\t\tval = (inet->cmsg_flags & IP_CMSG_PASSSEC) != 0;\n\t\tbreak;\n\tcase IP_RECVORIGDSTADDR:\n\t\tval = (inet->cmsg_flags & IP_CMSG_ORIGDSTADDR) != 0;\n\t\tbreak;\n\tcase IP_TOS:\n\t\tval = inet->tos;\n\t\tbreak;\n\tcase IP_TTL:\n\t\tval = (inet->uc_ttl == -1 ?\n\t\t       sysctl_ip_default_ttl :\n\t\t       inet->uc_ttl);\n\t\tbreak;\n\tcase IP_HDRINCL:\n\t\tval = inet->hdrincl;\n\t\tbreak;\n\tcase IP_NODEFRAG:\n\t\tval = inet->nodefrag;\n\t\tbreak;\n\tcase IP_MTU_DISCOVER:\n\t\tval = inet->pmtudisc;\n\t\tbreak;\n\tcase IP_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tval = 0;\n\t\tdst = sk_dst_get(sk);\n\t\tif (dst) {\n\t\t\tval = dst_mtu(dst);\n\t\t\tdst_release(dst);\n\t\t}\n\t\tif (!val) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\tbreak;\n\t}\n\tcase IP_RECVERR:\n\t\tval = inet->recverr;\n\t\tbreak;\n\tcase IP_MULTICAST_TTL:\n\t\tval = inet->mc_ttl;\n\t\tbreak;\n\tcase IP_MULTICAST_LOOP:\n\t\tval = inet->mc_loop;\n\t\tbreak;\n\tcase IP_MULTICAST_IF:\n\t{\n\t\tstruct in_addr addr;\n\t\tlen = min_t(unsigned int, len, sizeof(struct in_addr));\n\t\taddr.s_addr = inet->mc_addr;\n\t\trelease_sock(sk);\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &addr, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase IP_MSFILTER:\n\t{\n\t\tstruct ip_msfilter msf;\n\t\tint err;\n\n\t\tif (len < IP_MSFILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&msf, optval, IP_MSFILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_msfget(sk, &msf,\n\t\t\t\t   (struct ip_msfilter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0))) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\terr = ip_mc_gsfget(sk, &gsf,\n\t\t\t\t   (struct group_filter __user *)optval,\n\t\t\t\t   optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tcase IP_MULTICAST_ALL:\n\t\tval = inet->mc_all;\n\t\tbreak;\n\tcase IP_PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\n\t\trelease_sock(sk);\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = 0;\n\n\t\tif (inet->cmsg_flags & IP_CMSG_PKTINFO) {\n\t\t\tstruct in_pktinfo info;\n\n\t\t\tinfo.ipi_addr.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_spec_dst.s_addr = inet->inet_rcv_saddr;\n\t\t\tinfo.ipi_ifindex = inet->mc_index;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_PKTINFO, sizeof(info), &info);\n\t\t}\n\t\tif (inet->cmsg_flags & IP_CMSG_TTL) {\n\t\t\tint hlim = inet->mc_ttl;\n\t\t\tput_cmsg(&msg, SOL_IP, IP_TTL, sizeof(hlim), &hlim);\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IP_FREEBIND:\n\t\tval = inet->freebind;\n\t\tbreak;\n\tcase IP_TRANSPARENT:\n\t\tval = inet->transparent;\n\t\tbreak;\n\tcase IP_MINTTL:\n\t\tval = inet->min_ttl;\n\t\tbreak;\n\tdefault:\n\t\trelease_sock(sk);\n\t\treturn -ENOPROTOOPT;\n\t}\n\trelease_sock(sk);\n\n\tif (len < sizeof(int) && len > 0 && val >= 0 && val <= 255) {\n\t\tunsigned char ucval = (unsigned char)val;\n\t\tlen = 1;\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &ucval, 1))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tlen = min_t(unsigned int, sizeof(int), len);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &val, len))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\nint ip_getsockopt(struct sock *sk, int level,\n\t\t  int optname, char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\terr = do_ip_getsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_PKTOPTIONS &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\terr = nf_getsockopt(sk, PF_INET, optname, optval,\n\t\t\t\t&len);\n\t\trelease_sock(sk);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t\treturn err;\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ip_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\tif (optname == MCAST_MSFILTER)\n\t\treturn compat_mc_getsockopt(sk, level, optname, optval, optlen,\n\t\t\tip_getsockopt);\n\n\terr = do_ip_getsockopt(sk, level, optname, optval, optlen);\n\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IP_PKTOPTIONS &&\n\t\t\t!ip_mroute_opt(optname)) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\terr = compat_nf_getsockopt(sk, PF_INET, optname, optval, &len);\n\t\trelease_sock(sk);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t\treturn err;\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(compat_ip_getsockopt);\n#endif\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tRAW - implementation of IP \"raw\" sockets.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() fixed up\n *\t\tAlan Cox\t:\tICMP error handling\n *\t\tAlan Cox\t:\tEMSGSIZE if you send too big a packet\n *\t\tAlan Cox\t: \tNow uses generic datagrams and shared\n *\t\t\t\t\tskbuff library. No more peek crashes,\n *\t\t\t\t\tno more backlogs\n *\t\tAlan Cox\t:\tChecks sk->broadcast.\n *\t\tAlan Cox\t:\tUses skb_free_datagram/skb_copy_datagram\n *\t\tAlan Cox\t:\tRaw passes ip options too\n *\t\tAlan Cox\t:\tSetsocketopt added\n *\t\tAlan Cox\t:\tFixed error return for broadcasts\n *\t\tAlan Cox\t:\tRemoved wake_up calls\n *\t\tAlan Cox\t:\tUse ttl/tos\n *\t\tAlan Cox\t:\tCleaned up old debugging\n *\t\tAlan Cox\t:\tUse new kernel side addresses\n *\tArnt Gulbrandsen\t:\tFixed MSG_DONTROUTE in raw sockets.\n *\t\tAlan Cox\t:\tBSD style RAW socket demultiplexing.\n *\t\tAlan Cox\t:\tBeginnings of mrouted support.\n *\t\tAlan Cox\t:\tAdded IP_HDRINCL option.\n *\t\tAlan Cox\t:\tSkip broadcast check if BSDism set.\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/types.h>\n#include <asm/atomic.h>\n#include <asm/byteorder.h>\n#include <asm/current.h>\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/stddef.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/aio.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/sockios.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/mroute.h>\n#include <linux/netdevice.h>\n#include <linux/in_route.h>\n#include <linux/route.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/dst.h>\n#include <net/sock.h>\n#include <linux/ip.h>\n#include <linux/net.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/udp.h>\n#include <net/raw.h>\n#include <net/snmp.h>\n#include <net/tcp_states.h>\n#include <net/inet_common.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/compat.h>\n\nstatic struct raw_hashinfo raw_v4_hashinfo = {\n\t.lock = __RW_LOCK_UNLOCKED(raw_v4_hashinfo.lock),\n};\n\nvoid raw_hash_sk(struct sock *sk)\n{\n\tstruct raw_hashinfo *h = sk->sk_prot->h.raw_hash;\n\tstruct hlist_head *head;\n\n\thead = &h->ht[inet_sk(sk)->inet_num & (RAW_HTABLE_SIZE - 1)];\n\n\twrite_lock_bh(&h->lock);\n\tsk_add_node(sk, head);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\twrite_unlock_bh(&h->lock);\n}\nEXPORT_SYMBOL_GPL(raw_hash_sk);\n\nvoid raw_unhash_sk(struct sock *sk)\n{\n\tstruct raw_hashinfo *h = sk->sk_prot->h.raw_hash;\n\n\twrite_lock_bh(&h->lock);\n\tif (sk_del_node_init(sk))\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\twrite_unlock_bh(&h->lock);\n}\nEXPORT_SYMBOL_GPL(raw_unhash_sk);\n\nstatic struct sock *__raw_v4_lookup(struct net *net, struct sock *sk,\n\t\tunsigned short num, __be32 raddr, __be32 laddr, int dif)\n{\n\tstruct hlist_node *node;\n\n\tsk_for_each_from(sk, node) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (net_eq(sock_net(sk), net) && inet->inet_num == num\t&&\n\t\t    !(inet->inet_daddr && inet->inet_daddr != raddr) \t&&\n\t\t    !(inet->inet_rcv_saddr && inet->inet_rcv_saddr != laddr) &&\n\t\t    !(sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\t\tgoto found; /* gotcha */\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\n/*\n *\t0 - deliver\n *\t1 - block\n */\nstatic __inline__ int icmp_filter(struct sock *sk, struct sk_buff *skb)\n{\n\tint type;\n\n\tif (!pskb_may_pull(skb, sizeof(struct icmphdr)))\n\t\treturn 1;\n\n\ttype = icmp_hdr(skb)->type;\n\tif (type < 32) {\n\t\t__u32 data = raw_sk(sk)->filter.data;\n\n\t\treturn ((1 << type) & data) != 0;\n\t}\n\n\t/* Do not block unknown ICMP types */\n\treturn 0;\n}\n\n/* IP input processing comes here for RAW socket delivery.\n * Caller owns SKB, so we must make clones.\n *\n * RFC 1122: SHOULD pass TOS value up to the transport layer.\n * -> It does. And not only TOS, but all IP header.\n */\nstatic int raw_v4_input(struct sk_buff *skb, const struct iphdr *iph, int hash)\n{\n\tstruct sock *sk;\n\tstruct hlist_head *head;\n\tint delivered = 0;\n\tstruct net *net;\n\n\tread_lock(&raw_v4_hashinfo.lock);\n\thead = &raw_v4_hashinfo.ht[hash];\n\tif (hlist_empty(head))\n\t\tgoto out;\n\n\tnet = dev_net(skb->dev);\n\tsk = __raw_v4_lookup(net, __sk_head(head), iph->protocol,\n\t\t\t     iph->saddr, iph->daddr,\n\t\t\t     skb->dev->ifindex);\n\n\twhile (sk) {\n\t\tdelivered = 1;\n\t\tif (iph->protocol != IPPROTO_ICMP || !icmp_filter(sk, skb)) {\n\t\t\tstruct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Not releasing hash table! */\n\t\t\tif (clone)\n\t\t\t\traw_rcv(sk, clone);\n\t\t}\n\t\tsk = __raw_v4_lookup(net, sk_next(sk), iph->protocol,\n\t\t\t\t     iph->saddr, iph->daddr,\n\t\t\t\t     skb->dev->ifindex);\n\t}\nout:\n\tread_unlock(&raw_v4_hashinfo.lock);\n\treturn delivered;\n}\n\nint raw_local_deliver(struct sk_buff *skb, int protocol)\n{\n\tint hash;\n\tstruct sock *raw_sk;\n\n\thash = protocol & (RAW_HTABLE_SIZE - 1);\n\traw_sk = sk_head(&raw_v4_hashinfo.ht[hash]);\n\n\t/* If there maybe a raw socket we must check - if not we\n\t * don't care less\n\t */\n\tif (raw_sk && !raw_v4_input(skb, ip_hdr(skb), hash))\n\t\traw_sk = NULL;\n\n\treturn raw_sk != NULL;\n\n}\n\nstatic void raw_err(struct sock *sk, struct sk_buff *skb, u32 info)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tint err = 0;\n\tint harderr = 0;\n\n\t/* Report error on raw socket, if:\n\t   1. User requested ip_recverr.\n\t   2. Socket is connected (otherwise the error indication\n\t      is useless without ip_recverr and error is hard.\n\t */\n\tif (!inet->recverr && sk->sk_state != TCP_ESTABLISHED)\n\t\treturn;\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\treturn;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\terr = EHOSTUNREACH;\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tbreak;\n\t\terr = icmp_err_convert[code].errno;\n\t\tharderr = icmp_err_convert[code].fatal;\n\t\tif (code == ICMP_FRAG_NEEDED) {\n\t\t\tharderr = inet->pmtudisc != IP_PMTUDISC_DONT;\n\t\t\terr = EMSGSIZE;\n\t\t}\n\t}\n\n\tif (inet->recverr) {\n\t\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\t\tu8 *payload = skb->data + (iph->ihl << 2);\n\n\t\tif (inet->hdrincl)\n\t\t\tpayload = skb->data;\n\t\tip_icmp_error(sk, skb, err, 0, info, payload);\n\t}\n\n\tif (inet->recverr || harderr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t}\n}\n\nvoid raw_icmp_error(struct sk_buff *skb, int protocol, u32 info)\n{\n\tint hash;\n\tstruct sock *raw_sk;\n\tconst struct iphdr *iph;\n\tstruct net *net;\n\n\thash = protocol & (RAW_HTABLE_SIZE - 1);\n\n\tread_lock(&raw_v4_hashinfo.lock);\n\traw_sk = sk_head(&raw_v4_hashinfo.ht[hash]);\n\tif (raw_sk != NULL) {\n\t\tiph = (const struct iphdr *)skb->data;\n\t\tnet = dev_net(skb->dev);\n\n\t\twhile ((raw_sk = __raw_v4_lookup(net, raw_sk, protocol,\n\t\t\t\t\t\tiph->daddr, iph->saddr,\n\t\t\t\t\t\tskb->dev->ifindex)) != NULL) {\n\t\t\traw_err(raw_sk, skb, info);\n\t\t\traw_sk = sk_next(raw_sk);\n\t\t\tiph = (const struct iphdr *)skb->data;\n\t\t}\n\t}\n\tread_unlock(&raw_v4_hashinfo.lock);\n}\n\nstatic int raw_rcv_skb(struct sock * sk, struct sk_buff * skb)\n{\n\t/* Charge it to the socket. */\n\n\tif (ip_queue_rcv_skb(sk, skb) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\treturn NET_RX_SUCCESS;\n}\n\nint raw_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\tnf_reset(skb);\n\n\tskb_push(skb, skb->data - skb_network_header(skb));\n\n\traw_rcv_skb(sk, skb);\n\treturn 0;\n}\n\nstatic int raw_send_hdrinc(struct sock *sk, void *from, size_t length,\n\t\t\tstruct rtable **rtp,\n\t\t\tunsigned int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct iphdr *iph;\n\tstruct sk_buff *skb;\n\tunsigned int iphlen;\n\tint err;\n\tstruct rtable *rt = *rtp;\n\n\tif (length > rt->dst.dev->mtu) {\n\t\tip_local_error(sk, EMSGSIZE, rt->rt_dst, inet->inet_dport,\n\t\t\t       rt->dst.dev->mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (flags&MSG_PROBE)\n\t\tgoto out;\n\n\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t  length + LL_ALLOCATED_SPACE(rt->dst.dev) + 15,\n\t\t\t\t  flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto error;\n\tskb_reserve(skb, LL_RESERVED_SPACE(rt->dst.dev));\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\tskb_dst_set(skb, &rt->dst);\n\t*rtp = NULL;\n\n\tskb_reset_network_header(skb);\n\tiph = ip_hdr(skb);\n\tskb_put(skb, length);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb->transport_header = skb->network_header;\n\terr = -EFAULT;\n\tif (memcpy_fromiovecend((void *)iph, from, 0, length))\n\t\tgoto error_free;\n\n\tiphlen = iph->ihl * 4;\n\n\t/*\n\t * We don't want to modify the ip header, but we do need to\n\t * be sure that it won't cause problems later along the network\n\t * stack.  Specifically we want to make sure that iph->ihl is a\n\t * sane value.  If ihl points beyond the length of the buffer passed\n\t * in, reject the frame as invalid\n\t */\n\terr = -EINVAL;\n\tif (iphlen > length)\n\t\tgoto error_free;\n\n\tif (iphlen >= sizeof(*iph)) {\n\t\tif (!iph->saddr)\n\t\t\tiph->saddr = rt->rt_src;\n\t\tiph->check   = 0;\n\t\tiph->tot_len = htons(length);\n\t\tif (!iph->id)\n\t\t\tip_select_ident(iph, &rt->dst, NULL);\n\n\t\tiph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);\n\t}\n\tif (iph->protocol == IPPROTO_ICMP)\n\t\ticmp_out_count(net, ((struct icmphdr *)\n\t\t\tskb_transport_header(skb))->type);\n\n\terr = NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t      rt->dst.dev, dst_output);\n\tif (err > 0)\n\t\terr = net_xmit_errno(err);\n\tif (err)\n\t\tgoto error;\nout:\n\treturn 0;\n\nerror_free:\n\tkfree_skb(skb);\nerror:\n\tIP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);\n\tif (err == -ENOBUFS && !inet->recverr)\n\t\terr = 0;\n\treturn err;\n}\n\nstatic int raw_probe_proto_opt(struct flowi4 *fl4, struct msghdr *msg)\n{\n\tstruct iovec *iov;\n\tu8 __user *type = NULL;\n\tu8 __user *code = NULL;\n\tint probed = 0;\n\tunsigned int i;\n\n\tif (!msg->msg_iov)\n\t\treturn 0;\n\n\tfor (i = 0; i < msg->msg_iovlen; i++) {\n\t\tiov = &msg->msg_iov[i];\n\t\tif (!iov)\n\t\t\tcontinue;\n\n\t\tswitch (fl4->flowi4_proto) {\n\t\tcase IPPROTO_ICMP:\n\t\t\t/* check if one-byte field is readable or not. */\n\t\t\tif (iov->iov_base && iov->iov_len < 1)\n\t\t\t\tbreak;\n\n\t\t\tif (!type) {\n\t\t\t\ttype = iov->iov_base;\n\t\t\t\t/* check if code field is readable or not. */\n\t\t\t\tif (iov->iov_len > 1)\n\t\t\t\t\tcode = type + 1;\n\t\t\t} else if (!code)\n\t\t\t\tcode = iov->iov_base;\n\n\t\t\tif (type && code) {\n\t\t\t\tif (get_user(fl4->fl4_icmp_type, type) ||\n\t\t\t\t    get_user(fl4->fl4_icmp_code, code))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\tprobed = 1;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tprobed = 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (probed)\n\t\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int raw_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\t__be32 daddr;\n\t__be32 saddr;\n\tu8  tos;\n\tint err;\n\tstruct ip_options_data opt_copy;\n\n\terr = -EMSGSIZE;\n\tif (len > 0xFFFF)\n\t\tgoto out;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags & MSG_OOB)\t/* Mirror BSD error message */\n\t\tgoto out;               /* compatibility */\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (msg->msg_namelen) {\n\t\tstruct sockaddr_in *usin = (struct sockaddr_in *)msg->msg_name;\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\tgoto out;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tstatic int complained;\n\t\t\tif (!complained++)\n\t\t\t\tprintk(KERN_INFO \"%s forgot to set AF_INET in \"\n\t\t\t\t\t\t \"raw sendmsg. Fix it!\\n\",\n\t\t\t\t\t\t current->comm);\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tif (usin->sin_family)\n\t\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\t/* ANK: I did not forget to get protocol from port field.\n\t\t * I just do not know, who uses this weirdness.\n\t\t * IP_HDRINCL is much more convenient.\n\t\t */\n\t} else {\n\t\terr = -EDESTADDRREQ;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t\tdaddr = inet->inet_daddr;\n\t}\n\n\tipc.addr = inet->inet_saddr;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tipc.oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = daddr;\n\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (ipc.opt) {\n\t\terr = -EINVAL;\n\t\t/* Linux does not mangle headers on raw sockets,\n\t\t * so that IP options + IP_HDRINCL is non-sense.\n\t\t */\n\t\tif (inet->hdrincl)\n\t\t\tgoto done;\n\t\tif (ipc.opt->opt.srr) {\n\t\t\tif (!daddr)\n\t\t\t\tgoto done;\n\t\t\tdaddr = ipc.opt->opt.faddr;\n\t\t}\n\t}\n\ttos = RT_CONN_FLAGS(sk);\n\tif (msg->msg_flags & MSG_DONTROUTE)\n\t\ttos |= RTO_ONLINK;\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t}\n\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE,\n\t\t\t\t   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,\n\t\t\t\t   FLOWI_FLAG_CAN_SLEEP, daddr, saddr, 0, 0);\n\n\t\tif (!inet->hdrincl) {\n\t\t\terr = raw_probe_proto_opt(&fl4, msg);\n\t\t\tif (err)\n\t\t\t\tgoto done;\n\t\t}\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(sock_net(sk), &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\terr = -EACCES;\n\tif (rt->rt_flags & RTCF_BROADCAST && !sock_flag(sk, SOCK_BROADCAST))\n\t\tgoto done;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tif (inet->hdrincl)\n\t\terr = raw_send_hdrinc(sk, msg->msg_iov, len,\n\t\t\t\t\t&rt, msg->msg_flags);\n\n\t else {\n\t\tif (!ipc.addr)\n\t\t\tipc.addr = rt->rt_dst;\n\t\tlock_sock(sk);\n\t\terr = ip_append_data(sk, ip_generic_getfrag, msg->msg_iov, len, 0,\n\t\t\t\t\t&ipc, &rt, msg->msg_flags);\n\t\tif (err)\n\t\t\tip_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE)) {\n\t\t\terr = ip_push_pending_frames(sk);\n\t\t\tif (err == -ENOBUFS && !inet->recverr)\n\t\t\t\terr = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t}\ndone:\n\tif (free)\n\t\tkfree(ipc.opt);\n\tip_rt_put(rt);\n\nout:\n\tif (err < 0)\n\t\treturn err;\n\treturn len;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic void raw_close(struct sock *sk, long timeout)\n{\n\t/*\n\t * Raw sockets may have direct kernel references. Kill them.\n\t */\n\tip_ra_control(sk, 0, NULL);\n\n\tsk_common_release(sk);\n}\n\nstatic void raw_destroy(struct sock *sk)\n{\n\tlock_sock(sk);\n\tip_flush_pending_frames(sk);\n\trelease_sock(sk);\n}\n\n/* This gets rid of all the nasties in af_inet. -DaveM */\nstatic int raw_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_in *addr = (struct sockaddr_in *) uaddr;\n\tint ret = -EINVAL;\n\tint chk_addr_ret;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_in))\n\t\tgoto out;\n\tchk_addr_ret = inet_addr_type(sock_net(sk), addr->sin_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->sin_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->sin_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\tret = 0;\nout:\treturn ret;\n}\n\n/*\n *\tThis should be easy, if there is something there\n *\twe return it, otherwise we block.\n */\n\nstatic int raw_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t       size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = ip_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err)\n\t\treturn err;\n\treturn copied;\n}\n\nstatic int raw_init(struct sock *sk)\n{\n\tstruct raw_sock *rp = raw_sk(sk);\n\n\tif (inet_sk(sk)->inet_num == IPPROTO_ICMP)\n\t\tmemset(&rp->filter, 0, sizeof(rp->filter));\n\treturn 0;\n}\n\nstatic int raw_seticmpfilter(struct sock *sk, char __user *optval, int optlen)\n{\n\tif (optlen > sizeof(struct icmp_filter))\n\t\toptlen = sizeof(struct icmp_filter);\n\tif (copy_from_user(&raw_sk(sk)->filter, optval, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int raw_geticmpfilter(struct sock *sk, char __user *optval, int __user *optlen)\n{\n\tint len, ret = -EFAULT;\n\n\tif (get_user(len, optlen))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (len < 0)\n\t\tgoto out;\n\tif (len > sizeof(struct icmp_filter))\n\t\tlen = sizeof(struct icmp_filter);\n\tret = -EFAULT;\n\tif (put_user(len, optlen) ||\n\t    copy_to_user(optval, &raw_sk(sk)->filter, len))\n\t\tgoto out;\n\tret = 0;\nout:\treturn ret;\n}\n\nstatic int do_raw_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (optname == ICMP_FILTER) {\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMP)\n\t\t\treturn -EOPNOTSUPP;\n\t\telse\n\t\t\treturn raw_seticmpfilter(sk, optval, optlen);\n\t}\n\treturn -ENOPROTOOPT;\n}\n\nstatic int raw_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_raw_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn compat_ip_setsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int do_raw_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (optname == ICMP_FILTER) {\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMP)\n\t\t\treturn -EOPNOTSUPP;\n\t\telse\n\t\t\treturn raw_geticmpfilter(sk, optval, optlen);\n\t}\n\treturn -ENOPROTOOPT;\n}\n\nstatic int raw_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_raw_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_RAW)\n\t\treturn compat_ip_getsockopt(sk, level, optname, optval, optlen);\n\treturn do_raw_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int raw_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\t\tcase SIOCOUTQ: {\n\t\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\t\treturn put_user(amount, (int __user *)arg);\n\t\t}\n\t\tcase SIOCINQ: {\n\t\t\tstruct sk_buff *skb;\n\t\t\tint amount = 0;\n\n\t\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\t\tif (skb != NULL)\n\t\t\t\tamount = skb->len;\n\t\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\t\treturn put_user(amount, (int __user *)arg);\n\t\t}\n\n\t\tdefault:\n#ifdef CONFIG_IP_MROUTE\n\t\t\treturn ipmr_ioctl(sk, cmd, (void __user *)arg);\n#else\n\t\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_raw_ioctl(struct sock *sk, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\tcase SIOCINQ:\n\t\treturn -ENOIOCTLCMD;\n\tdefault:\n#ifdef CONFIG_IP_MROUTE\n\t\treturn ipmr_compat_ioctl(sk, cmd, compat_ptr(arg));\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n#endif\n\nstruct proto raw_prot = {\n\t.name\t\t   = \"RAW\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = raw_close,\n\t.destroy\t   = raw_destroy,\n\t.connect\t   = ip4_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = raw_ioctl,\n\t.init\t\t   = raw_init,\n\t.setsockopt\t   = raw_setsockopt,\n\t.getsockopt\t   = raw_getsockopt,\n\t.sendmsg\t   = raw_sendmsg,\n\t.recvmsg\t   = raw_recvmsg,\n\t.bind\t\t   = raw_bind,\n\t.backlog_rcv\t   = raw_rcv_skb,\n\t.hash\t\t   = raw_hash_sk,\n\t.unhash\t\t   = raw_unhash_sk,\n\t.obj_size\t   = sizeof(struct raw_sock),\n\t.h.raw_hash\t   = &raw_v4_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_raw_setsockopt,\n\t.compat_getsockopt = compat_raw_getsockopt,\n\t.compat_ioctl\t   = compat_raw_ioctl,\n#endif\n};\n\n#ifdef CONFIG_PROC_FS\nstatic struct sock *raw_get_first(struct seq_file *seq)\n{\n\tstruct sock *sk;\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tfor (state->bucket = 0; state->bucket < RAW_HTABLE_SIZE;\n\t\t\t++state->bucket) {\n\t\tstruct hlist_node *node;\n\n\t\tsk_for_each(sk, node, &state->h->ht[state->bucket])\n\t\t\tif (sock_net(sk) == seq_file_net(seq))\n\t\t\t\tgoto found;\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *raw_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tdo {\n\t\tsk = sk_next(sk);\ntry_again:\n\t\t;\n\t} while (sk && sock_net(sk) != seq_file_net(seq));\n\n\tif (!sk && ++state->bucket < RAW_HTABLE_SIZE) {\n\t\tsk = sk_head(&state->h->ht[state->bucket]);\n\t\tgoto try_again;\n\t}\n\treturn sk;\n}\n\nstatic struct sock *raw_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = raw_get_first(seq);\n\n\tif (sk)\n\t\twhile (pos && (sk = raw_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nvoid *raw_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tread_lock(&state->h->lock);\n\treturn *pos ? raw_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;\n}\nEXPORT_SYMBOL_GPL(raw_seq_start);\n\nvoid *raw_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = raw_get_first(seq);\n\telse\n\t\tsk = raw_get_next(seq, v);\n\t++*pos;\n\treturn sk;\n}\nEXPORT_SYMBOL_GPL(raw_seq_next);\n\nvoid raw_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct raw_iter_state *state = raw_seq_private(seq);\n\n\tread_unlock(&state->h->lock);\n}\nEXPORT_SYMBOL_GPL(raw_seq_stop);\n\nstatic void raw_sock_seq_show(struct seq_file *seq, struct sock *sp, int i)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr,\n\t       src = inet->inet_rcv_saddr;\n\t__u16 destp = 0,\n\t      srcp  = inet->inet_num;\n\n\tseq_printf(seq, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %p %d\\n\",\n\t\ti, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tsk_rmem_alloc_get(sp),\n\t\t0, 0L, 0, sock_i_uid(sp), 0, sock_i_ino(sp),\n\t\tatomic_read(&sp->sk_refcnt), sp, atomic_read(&sp->sk_drops));\n}\n\nstatic int raw_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq, \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t\t\"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t\t\"inode ref pointer drops\\n\");\n\telse\n\t\traw_sock_seq_show(seq, v, raw_seq_private(seq)->bucket);\n\treturn 0;\n}\n\nstatic const struct seq_operations raw_seq_ops = {\n\t.start = raw_seq_start,\n\t.next  = raw_seq_next,\n\t.stop  = raw_seq_stop,\n\t.show  = raw_seq_show,\n};\n\nint raw_seq_open(struct inode *ino, struct file *file,\n\t\t struct raw_hashinfo *h, const struct seq_operations *ops)\n{\n\tint err;\n\tstruct raw_iter_state *i;\n\n\terr = seq_open_net(ino, file, ops, sizeof(struct raw_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ti = raw_seq_private((struct seq_file *)file->private_data);\n\ti->h = h;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(raw_seq_open);\n\nstatic int raw_v4_seq_open(struct inode *inode, struct file *file)\n{\n\treturn raw_seq_open(inode, file, &raw_v4_hashinfo, &raw_seq_ops);\n}\n\nstatic const struct file_operations raw_seq_fops = {\n\t.owner\t = THIS_MODULE,\n\t.open\t = raw_v4_seq_open,\n\t.read\t = seq_read,\n\t.llseek\t = seq_lseek,\n\t.release = seq_release_net,\n};\n\nstatic __net_init int raw_init_net(struct net *net)\n{\n\tif (!proc_net_fops_create(net, \"raw\", S_IRUGO, &raw_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void raw_exit_net(struct net *net)\n{\n\tproc_net_remove(net, \"raw\");\n}\n\nstatic __net_initdata struct pernet_operations raw_net_ops = {\n\t.init = raw_init_net,\n\t.exit = raw_exit_net,\n};\n\nint __init raw_proc_init(void)\n{\n\treturn register_pernet_subsys(&raw_net_ops);\n}\n\nvoid __init raw_proc_exit(void)\n{\n\tunregister_pernet_subsys(&raw_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n", "/*\n *  Syncookies implementation for the Linux kernel\n *\n *  Copyright (C) 1997 Andi Kleen\n *  Based on ideas by D.J.Bernstein and Eric Schenk.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/tcp.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/cryptohash.h>\n#include <linux/kernel.h>\n#include <net/tcp.h>\n#include <net/route.h>\n\n/* Timestamps: lowest bits store TCP options */\n#define TSBITS 6\n#define TSMASK (((__u32)1 << TSBITS) - 1)\n\nextern int sysctl_tcp_syncookies;\n\n__u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];\nEXPORT_SYMBOL(syncookie_secret);\n\nstatic __init int init_syncookies(void)\n{\n\tget_random_bytes(syncookie_secret, sizeof(syncookie_secret));\n\treturn 0;\n}\n__initcall(init_syncookies);\n\n#define COOKIEBITS 24\t/* Upper bits store count */\n#define COOKIEMASK (((__u32)1 << COOKIEBITS) - 1)\n\nstatic DEFINE_PER_CPU(__u32 [16 + 5 + SHA_WORKSPACE_WORDS],\n\t\t      ipv4_cookie_scratch);\n\nstatic u32 cookie_hash(__be32 saddr, __be32 daddr, __be16 sport, __be16 dport,\n\t\t       u32 count, int c)\n{\n\t__u32 *tmp = __get_cpu_var(ipv4_cookie_scratch);\n\n\tmemcpy(tmp + 4, syncookie_secret[c], sizeof(syncookie_secret[c]));\n\ttmp[0] = (__force u32)saddr;\n\ttmp[1] = (__force u32)daddr;\n\ttmp[2] = ((__force u32)sport << 16) + (__force u32)dport;\n\ttmp[3] = count;\n\tsha_transform(tmp + 16, (__u8 *)tmp, tmp + 16 + 5);\n\n\treturn tmp[17];\n}\n\n\n/*\n * when syncookies are in effect and tcp timestamps are enabled we encode\n * tcp options in the lower bits of the timestamp value that will be\n * sent in the syn-ack.\n * Since subsequent timestamps use the normal tcp_time_stamp value, we\n * must make sure that the resulting initial timestamp is <= tcp_time_stamp.\n */\n__u32 cookie_init_timestamp(struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq;\n\tu32 ts, ts_now = tcp_time_stamp;\n\tu32 options = 0;\n\n\tireq = inet_rsk(req);\n\n\toptions = ireq->wscale_ok ? ireq->snd_wscale : 0xf;\n\toptions |= ireq->sack_ok << 4;\n\toptions |= ireq->ecn_ok << 5;\n\n\tts = ts_now & ~TSMASK;\n\tts |= options;\n\tif (ts > ts_now) {\n\t\tts >>= TSBITS;\n\t\tts--;\n\t\tts <<= TSBITS;\n\t\tts |= options;\n\t}\n\treturn ts;\n}\n\n\nstatic __u32 secure_tcp_syn_cookie(__be32 saddr, __be32 daddr, __be16 sport,\n\t\t\t\t   __be16 dport, __u32 sseq, __u32 count,\n\t\t\t\t   __u32 data)\n{\n\t/*\n\t * Compute the secure sequence number.\n\t * The output should be:\n\t *   HASH(sec1,saddr,sport,daddr,dport,sec1) + sseq + (count * 2^24)\n\t *      + (HASH(sec2,saddr,sport,daddr,dport,count,sec2) % 2^24).\n\t * Where sseq is their sequence number and count increases every\n\t * minute by 1.\n\t * As an extra hack, we add a small \"data\" value that encodes the\n\t * MSS into the second hash value.\n\t */\n\n\treturn (cookie_hash(saddr, daddr, sport, dport, 0, 0) +\n\t\tsseq + (count << COOKIEBITS) +\n\t\t((cookie_hash(saddr, daddr, sport, dport, count, 1) + data)\n\t\t & COOKIEMASK));\n}\n\n/*\n * This retrieves the small \"data\" value from the syncookie.\n * If the syncookie is bad, the data returned will be out of\n * range.  This must be checked by the caller.\n *\n * The count value used to generate the cookie must be within\n * \"maxdiff\" if the current (passed-in) \"count\".  The return value\n * is (__u32)-1 if this test fails.\n */\nstatic __u32 check_tcp_syn_cookie(__u32 cookie, __be32 saddr, __be32 daddr,\n\t\t\t\t  __be16 sport, __be16 dport, __u32 sseq,\n\t\t\t\t  __u32 count, __u32 maxdiff)\n{\n\t__u32 diff;\n\n\t/* Strip away the layers from the cookie */\n\tcookie -= cookie_hash(saddr, daddr, sport, dport, 0, 0) + sseq;\n\n\t/* Cookie is now reduced to (count * 2^24) ^ (hash % 2^24) */\n\tdiff = (count - (cookie >> COOKIEBITS)) & ((__u32) - 1 >> COOKIEBITS);\n\tif (diff >= maxdiff)\n\t\treturn (__u32)-1;\n\n\treturn (cookie -\n\t\tcookie_hash(saddr, daddr, sport, dport, count - diff, 1))\n\t\t& COOKIEMASK;\t/* Leaving the data behind */\n}\n\n/*\n * MSS Values are taken from the 2009 paper\n * 'Measuring TCP Maximum Segment Size' by S. Alcock and R. Nelson:\n *  - values 1440 to 1460 accounted for 80% of observed mss values\n *  - values outside the 536-1460 range are rare (<0.2%).\n *\n * Table must be sorted.\n */\nstatic __u16 const msstab[] = {\n\t64,\n\t512,\n\t536,\n\t1024,\n\t1440,\n\t1460,\n\t4312,\n\t8960,\n};\n\n/*\n * Generate a syncookie.  mssp points to the mss, which is returned\n * rounded down to the value encoded in the cookie.\n */\n__u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, __u16 *mssp)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint mssind;\n\tconst __u16 mss = *mssp;\n\n\ttcp_synq_overflow(sk);\n\n\tfor (mssind = ARRAY_SIZE(msstab) - 1; mssind ; mssind--)\n\t\tif (mss >= msstab[mssind])\n\t\t\tbreak;\n\t*mssp = msstab[mssind];\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);\n\n\treturn secure_tcp_syn_cookie(iph->saddr, iph->daddr,\n\t\t\t\t     th->source, th->dest, ntohl(th->seq),\n\t\t\t\t     jiffies / (HZ * 60), mssind);\n}\n\n/*\n * This (misnamed) value is the age of syncookie which is permitted.\n * Its ideal value should be dependent on TCP_TIMEOUT_INIT and\n * sysctl_tcp_retries1. It's a rather complicated formula (exponential\n * backoff) to compute at runtime so it's currently hardcoded here.\n */\n#define COUNTER_TRIES 4\n/*\n * Check if a ack sequence number is a valid syncookie.\n * Return the decoded mss if it is, or 0 if not.\n */\nstatic inline int cookie_check(struct sk_buff *skb, __u32 cookie)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 seq = ntohl(th->seq) - 1;\n\t__u32 mssind = check_tcp_syn_cookie(cookie, iph->saddr, iph->daddr,\n\t\t\t\t\t    th->source, th->dest, seq,\n\t\t\t\t\t    jiffies / (HZ * 60),\n\t\t\t\t\t    COUNTER_TRIES);\n\n\treturn mssind < ARRAY_SIZE(msstab) ? msstab[mssind] : 0;\n}\n\nstatic inline struct sock *get_cookie_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t   struct request_sock *req,\n\t\t\t\t\t   struct dst_entry *dst)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sock *child;\n\n\tchild = icsk->icsk_af_ops->syn_recv_sock(sk, skb, req, dst);\n\tif (child)\n\t\tinet_csk_reqsk_queue_add(sk, req, child);\n\telse\n\t\treqsk_free(req);\n\n\treturn child;\n}\n\n\n/*\n * when syncookies are in effect and tcp timestamps are enabled we stored\n * additional tcp options in the timestamp.\n * This extracts these options from the timestamp echo.\n *\n * The lowest 4 bits store snd_wscale.\n * next 2 bits indicate SACK and ECN support.\n *\n * return false if we decode an option that should not be.\n */\nbool cookie_check_timestamp(struct tcp_options_received *tcp_opt, bool *ecn_ok)\n{\n\t/* echoed timestamp, lowest bits contain options */\n\tu32 options = tcp_opt->rcv_tsecr & TSMASK;\n\n\tif (!tcp_opt->saw_tstamp)  {\n\t\ttcp_clear_options(tcp_opt);\n\t\treturn true;\n\t}\n\n\tif (!sysctl_tcp_timestamps)\n\t\treturn false;\n\n\ttcp_opt->sack_ok = (options >> 4) & 0x1;\n\t*ecn_ok = (options >> 5) & 1;\n\tif (*ecn_ok && !sysctl_tcp_ecn)\n\t\treturn false;\n\n\tif (tcp_opt->sack_ok && !sysctl_tcp_sack)\n\t\treturn false;\n\n\tif ((options & 0xf) == 0xf)\n\t\treturn true; /* no window scaling */\n\n\ttcp_opt->wscale_ok = 1;\n\ttcp_opt->snd_wscale = options & 0xf;\n\treturn sysctl_tcp_window_scaling != 0;\n}\nEXPORT_SYMBOL(cookie_check_timestamp);\n\nstruct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct ip_options *opt)\n{\n\tstruct tcp_options_received tcp_opt;\n\tu8 *hash_location;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct rtable *rt;\n\t__u8 rcv_wscale;\n\tbool ecn_ok;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk) ||\n\t    (mss = cookie_check(skb, cookie)) == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, &hash_location, 0);\n\n\tif (!cookie_check_timestamp(&tcp_opt, &ecn_ok))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp_request_sock_ops); /* for safety */\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->rcv_isn\t\t= ntohl(th->seq) - 1;\n\ttreq->snt_isn\t\t= cookie;\n\treq->mss\t\t= mss;\n\tireq->loc_port\t\t= th->dest;\n\tireq->rmt_port\t\t= th->source;\n\tireq->loc_addr\t\t= ip_hdr(skb)->daddr;\n\tireq->rmt_addr\t\t= ip_hdr(skb)->saddr;\n\tireq->ecn_ok\t\t= ecn_ok;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\n\t/* We throwed the options of the initial SYN away, so we hope\n\t * the ACK carries the same options again (see RFC1122 4.2.3.8)\n\t */\n\tif (opt && opt->optlen) {\n\t\tint opt_size = sizeof(struct ip_options_rcu) + opt->optlen;\n\n\t\tireq->opt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (ireq->opt != NULL && ip_options_echo(&ireq->opt->opt, skb)) {\n\t\t\tkfree(ireq->opt);\n\t\t\tireq->opt = NULL;\n\t\t}\n\t}\n\n\tif (security_inet_conn_request(sk, skb, req)) {\n\t\treqsk_free(req);\n\t\tgoto out;\n\t}\n\n\treq->expires\t= 0UL;\n\treq->retrans\t= 0;\n\n\t/*\n\t * We need to lookup the route here to get at the correct\n\t * window size. We should better make sure that the window size\n\t * hasn't changed since we received the original syn, but I see\n\t * no easy way to do this.\n\t */\n\t{\n\t\tstruct flowi4 fl4;\n\n\t\tflowi4_init_output(&fl4, 0, sk->sk_mark, RT_CONN_FLAGS(sk),\n\t\t\t\t   RT_SCOPE_UNIVERSE, IPPROTO_TCP,\n\t\t\t\t   inet_sk_flowi_flags(sk),\n\t\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t\t   ireq->loc_addr, th->source, th->dest);\n\t\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(sock_net(sk), &fl4);\n\t\tif (IS_ERR(rt)) {\n\t\t\treqsk_free(req);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Try to redo what tcp_v4_send_synack did. */\n\treq->window_clamp = tp->window_clamp ? :dst_metric(&rt->dst, RTAX_WINDOW);\n\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rcv_wnd, &req->window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(&rt->dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale  = rcv_wscale;\n\n\tret = get_cookie_sock(sk, skb, req, &rt->dst);\nout:\treturn ret;\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n *\t\tIPv4 specific functions\n *\n *\n *\t\tcode split from:\n *\t\tlinux/ipv4/tcp.c\n *\t\tlinux/ipv4/tcp_input.c\n *\t\tlinux/ipv4/tcp_output.c\n *\n *\t\tSee tcp.c for author information\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n/*\n * Changes:\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tThis code is dedicated to John Dyson.\n *\t\tDavid S. Miller :\tChange semantics of established hash,\n *\t\t\t\t\thalf is devoted to TIME_WAIT sockets\n *\t\t\t\t\tand the rest go in the other half.\n *\t\tAndi Kleen :\t\tAdd support for syncookies and fixed\n *\t\t\t\t\tsome bugs: ip options weren't passed to\n *\t\t\t\t\tthe TCP layer, missed a check for an\n *\t\t\t\t\tACK bit.\n *\t\tAndi Kleen :\t\tImplemented fast path mtu discovery.\n *\t     \t\t\t\tFixed many serious bugs in the\n *\t\t\t\t\trequest_sock handling and moved\n *\t\t\t\t\tmost of it into the af independent code.\n *\t\t\t\t\tAdded tail drop and some other bugfixes.\n *\t\t\t\t\tAdded new listen semantics.\n *\t\tMike McLagan\t:\tRouting by source\n *\tJuan Jose Ciarlante:\t\tip_dynaddr bits\n *\t\tAndi Kleen:\t\tvarious fixes.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year\n *\t\t\t\t\tcoma.\n *\tAndi Kleen\t\t:\tFix new listen.\n *\tAndi Kleen\t\t:\tFix accept error reporting.\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n */\n\n\n#include <linux/bottom_half.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/cache.h>\n#include <linux/jhash.h>\n#include <linux/init.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/inet_hashtables.h>\n#include <net/tcp.h>\n#include <net/transp_v6.h>\n#include <net/ipv6.h>\n#include <net/inet_common.h>\n#include <net/timewait_sock.h>\n#include <net/xfrm.h>\n#include <net/netdma.h>\n\n#include <linux/inet.h>\n#include <linux/ipv6.h>\n#include <linux/stddef.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n\n#include <linux/crypto.h>\n#include <linux/scatterlist.h>\n\nint sysctl_tcp_tw_reuse __read_mostly;\nint sysctl_tcp_low_latency __read_mostly;\nEXPORT_SYMBOL(sysctl_tcp_low_latency);\n\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic struct tcp_md5sig_key *tcp_v4_md5_do_lookup(struct sock *sk,\n\t\t\t\t\t\t   __be32 addr);\nstatic int tcp_v4_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       __be32 daddr, __be32 saddr, struct tcphdr *th);\n#else\nstatic inline\nstruct tcp_md5sig_key *tcp_v4_md5_do_lookup(struct sock *sk, __be32 addr)\n{\n\treturn NULL;\n}\n#endif\n\nstruct inet_hashinfo tcp_hashinfo;\nEXPORT_SYMBOL(tcp_hashinfo);\n\nstatic inline __u32 tcp_v4_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_tcp_sequence_number(ip_hdr(skb)->daddr,\n\t\t\t\t\t  ip_hdr(skb)->saddr,\n\t\t\t\t\t  tcp_hdr(skb)->dest,\n\t\t\t\t\t  tcp_hdr(skb)->source);\n}\n\nint tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)\n{\n\tconst struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* With PAWS, it is safe from the viewpoint\n\t   of data integrity. Even without PAWS it is safe provided sequence\n\t   spaces do not overlap i.e. at data rates <= 80Mbit/sec.\n\n\t   Actually, the idea is close to VJ's one, only timestamp cache is\n\t   held not per host, but per port pair and TW bucket is used as state\n\t   holder.\n\n\t   If TW bucket has been already destroyed we fall back to VJ's scheme\n\t   and use initial timestamp retrieved from peer table.\n\t */\n\tif (tcptw->tw_ts_recent_stamp &&\n\t    (twp == NULL || (sysctl_tcp_tw_reuse &&\n\t\t\t     get_seconds() - tcptw->tw_ts_recent_stamp > 1))) {\n\t\ttp->write_seq = tcptw->tw_snd_nxt + 65535 + 2;\n\t\tif (tp->write_seq == 0)\n\t\t\ttp->write_seq = 1;\n\t\ttp->rx_opt.ts_recent\t   = tcptw->tw_ts_recent;\n\t\ttp->rx_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;\n\t\tsock_hold(sktw);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(tcp_twsk_unique);\n\n/* This will initiate an outgoing connection. */\nint tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\tstruct ip_options_rcu *inet_opt;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_TCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\tif (err == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\t\treturn err;\n\t}\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (!inet_opt || !inet_opt->opt.srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tif (tp->rx_opt.ts_recent_stamp && inet->inet_daddr != daddr) {\n\t\t/* Reset inherited state */\n\t\ttp->rx_opt.ts_recent\t   = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq\t\t   = 0;\n\t}\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp && rt->rt_dst == daddr) {\n\t\tstruct inet_peer *peer = rt_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\n\ttp->rx_opt.mss_clamp = TCP_MSS_DEFAULT;\n\n\t/* Socket identity is still unknown (sport may be zero).\n\t * However we set state to SYN-SENT and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\terr = PTR_ERR(rt);\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(sk, &rt->dst);\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t\t   inet->inet_daddr,\n\t\t\t\t\t\t\t   inet->inet_sport,\n\t\t\t\t\t\t\t   usin->sin_port);\n\n\tinet->inet_id = tp->write_seq ^ jiffies;\n\n\terr = tcp_connect(sk);\n\trt = NULL;\n\tif (err)\n\t\tgoto failure;\n\n\treturn 0;\n\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port,\n\t * if necessary.\n\t */\n\ttcp_set_state(sk, TCP_CLOSE);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_v4_connect);\n\n/*\n * This routine does path mtu discovery as defined in RFC1191.\n */\nstatic void do_pmtu_discovery(struct sock *sk, const struct iphdr *iph, u32 mtu)\n{\n\tstruct dst_entry *dst;\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t/* We are not interested in TCP_LISTEN and open_requests (SYN-ACKs\n\t * send out by Linux are always <576bytes so they should go through\n\t * unfragmented).\n\t */\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn;\n\n\t/* We don't check in the destentry if pmtu discovery is forbidden\n\t * on this route. We just assume that no packet_to_big packets\n\t * are send back when pmtu discovery is not active.\n\t * There is a small race when the user changes this flag in the\n\t * route, but I think that's acceptable.\n\t */\n\tif ((dst = __sk_dst_check(sk, 0)) == NULL)\n\t\treturn;\n\n\tdst->ops->update_pmtu(dst, mtu);\n\n\t/* Something is about to be wrong... Remember soft error\n\t * for the case, if this connection will not able to recover.\n\t */\n\tif (mtu < dst_mtu(dst) && ip_dont_fragment(sk, dst))\n\t\tsk->sk_err_soft = EMSGSIZE;\n\n\tmtu = dst_mtu(dst);\n\n\tif (inet->pmtudisc != IP_PMTUDISC_DONT &&\n\t    inet_csk(sk)->icsk_pmtu_cookie > mtu) {\n\t\ttcp_sync_mss(sk, mtu);\n\n\t\t/* Resend the TCP packet because it's\n\t\t * clear that the old packet has been\n\t\t * dropped. This is the new \"fast\" path mtu\n\t\t * discovery.\n\t\t */\n\t\ttcp_simple_retransmit(sk);\n\t} /* else let the usual retransmit timer handle it */\n}\n\n/*\n * This routine is called by the ICMP module when it gets some\n * sort of error condition.  If err < 0 then the socket should\n * be closed and the error returned to the user.  If err > 0\n * it's just the icmp type << 8 | icmp code.  After adjustment\n * header points to the first 8 bytes of the tcp header.  We need\n * to find the appropriate port.\n *\n * The locking strategy used here is very \"optimistic\". When\n * someone else accesses the socket the ICMP is just dropped\n * and for some paths there is no check at all.\n * A more general error queue to queue errors for later handling\n * is probably better.\n *\n */\n\nvoid tcp_v4_err(struct sk_buff *icmp_skb, u32 info)\n{\n\tconst struct iphdr *iph = (const struct iphdr *)icmp_skb->data;\n\tstruct tcphdr *th = (struct tcphdr *)(icmp_skb->data + (iph->ihl << 2));\n\tstruct inet_connection_sock *icsk;\n\tstruct tcp_sock *tp;\n\tstruct inet_sock *inet;\n\tconst int type = icmp_hdr(icmp_skb)->type;\n\tconst int code = icmp_hdr(icmp_skb)->code;\n\tstruct sock *sk;\n\tstruct sk_buff *skb;\n\t__u32 seq;\n\t__u32 remaining;\n\tint err;\n\tstruct net *net = dev_net(icmp_skb->dev);\n\n\tif (icmp_skb->len < (iph->ihl << 2) + 8) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = inet_lookup(net, &tcp_hashinfo, iph->daddr, th->dest,\n\t\t\tiph->saddr, th->source, inet_iif(icmp_skb));\n\tif (!sk) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\n\t}\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\t/* If too many ICMPs get dropped on busy\n\t * servers this needs to be solved differently.\n\t */\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ticsk = inet_csk(sk);\n\ttp = tcp_sk(sk);\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, tp->snd_una, tp->snd_nxt)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tswitch (type) {\n\tcase ICMP_SOURCE_QUENCH:\n\t\t/* Just silently ignore these. */\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code > NR_ICMP_UNREACH)\n\t\t\tgoto out;\n\n\t\tif (code == ICMP_FRAG_NEEDED) { /* PMTU discovery (RFC1191) */\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tdo_pmtu_discovery(sk, iph, info);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = icmp_err_convert[code].errno;\n\t\t/* check if icmp_skb allows revert of backoff\n\t\t * (see draft-zimmermann-tcp-lcd) */\n\t\tif (code != ICMP_NET_UNREACH && code != ICMP_HOST_UNREACH)\n\t\t\tbreak;\n\t\tif (seq != tp->snd_una  || !icsk->icsk_retransmits ||\n\t\t    !icsk->icsk_backoff)\n\t\t\tbreak;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tbreak;\n\n\t\ticsk->icsk_backoff--;\n\t\tinet_csk(sk)->icsk_rto = __tcp_set_rto(tp) <<\n\t\t\t\t\t icsk->icsk_backoff;\n\t\ttcp_bound_rto(sk);\n\n\t\tskb = tcp_write_queue_head(sk);\n\t\tBUG_ON(!skb);\n\n\t\tremaining = icsk->icsk_rto - min(icsk->icsk_rto,\n\t\t\t\ttcp_time_stamp - TCP_SKB_CB(skb)->when);\n\n\t\tif (remaining) {\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t  remaining, TCP_RTO_MAX);\n\t\t} else {\n\t\t\t/* RTO revert clocked out retransmission.\n\t\t\t * Will retransmit now */\n\t\t\ttcp_retransmit_timer(sk);\n\t\t}\n\n\t\tbreak;\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req, **prev;\n\tcase TCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\n\t\treq = inet_csk_search_req(sk, &prev, th->dest,\n\t\t\t\t\t  iph->daddr, iph->saddr);\n\t\tif (!req)\n\t\t\tgoto out;\n\n\t\t/* ICMPs are not backlogged, hence we cannot get\n\t\t   an established socket here.\n\t\t */\n\t\tWARN_ON(req->sk);\n\n\t\tif (seq != tcp_rsk(req)->snt_isn) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * Still in SYN_RECV, just remove it silently.\n\t\t * There is no good way to pass the error to the newly\n\t\t * created socket, and POSIX does not want network\n\t\t * errors returned from accept().\n\t\t */\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:  /* Cannot happen.\n\t\t\t       It can f.e. if SYNs crossed.\n\t\t\t     */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\n\t\t\tsk->sk_error_report(sk);\n\n\t\t\ttcp_done(sk);\n\t\t} else {\n\t\t\tsk->sk_err_soft = err;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* If we've already connected we will keep trying\n\t * until we time out, or the user gives up.\n\t *\n\t * rfc1122 4.2.3.9 allows to consider as hard errors\n\t * only PROTO_UNREACH and PORT_UNREACH (well, FRAG_FAILED too,\n\t * but it is obsoleted by pmtu discovery).\n\t *\n\t * Note, that in modern internet, where routing is unreliable\n\t * and in each dark corner broken firewalls sit, sending random\n\t * errors ordered by their masters even this two messages finally lose\n\t * their original sense (even Linux sends invalid PORT_UNREACHs)\n\t *\n\t * Now we are in compliance with RFCs.\n\t *\t\t\t\t\t\t\t--ANK (980905)\n\t */\n\n\tinet = inet_sk(sk);\n\tif (!sock_owned_by_user(sk) && inet->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\t{ /* Only an error on timeout */\n\t\tsk->sk_err_soft = err;\n\t}\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\nstatic void __tcp_v4_send_check(struct sk_buff *skb,\n\t\t\t\t__be32 saddr, __be32 daddr)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tth->check = ~tcp_v4_check(skb->len, saddr, daddr, 0);\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct tcphdr, check);\n\t} else {\n\t\tth->check = tcp_v4_check(skb->len, saddr, daddr,\n\t\t\t\t\t csum_partial(th,\n\t\t\t\t\t\t      th->doff << 2,\n\t\t\t\t\t\t      skb->csum));\n\t}\n}\n\n/* This routine computes an IPv4 TCP checksum. */\nvoid tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__tcp_v4_send_check(skb, inet->inet_saddr, inet->inet_daddr);\n}\nEXPORT_SYMBOL(tcp_v4_send_check);\n\nint tcp_v4_gso_send_check(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct tcphdr *th;\n\n\tif (!pskb_may_pull(skb, sizeof(*th)))\n\t\treturn -EINVAL;\n\n\tiph = ip_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tth->check = 0;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t__tcp_v4_send_check(skb, iph->saddr, iph->daddr);\n\treturn 0;\n}\n\n/*\n *\tThis routine will send an RST to the other tcp.\n *\n *\tSomeone asks: why I NEVER use socket parameters (TOS, TTL etc.)\n *\t\t      for reset.\n *\tAnswer: if a packet caused RST, it is not for a socket\n *\t\texisting in our system, if it is matched to a socket,\n *\t\tit is just duplicate segment or bug in other side's TCP.\n *\t\tSo that we build reply only basing on parameters\n *\t\tarrived with segment.\n *\tException: precedence violation. We do not implement it in any case.\n */\n\nstatic void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tstruct {\n\t\tstruct tcphdr th;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t__be32 opt[(TCPOLEN_MD5SIG_ALIGNED >> 2)];\n#endif\n\t} rep;\n\tstruct ip_reply_arg arg;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct net *net;\n\n\t/* Never send a reset in response to a reset. */\n\tif (th->rst)\n\t\treturn;\n\n\tif (skb_rtable(skb)->rt_type != RTN_LOCAL)\n\t\treturn;\n\n\t/* Swap the send and the receive. */\n\tmemset(&rep, 0, sizeof(rep));\n\trep.th.dest   = th->source;\n\trep.th.source = th->dest;\n\trep.th.doff   = sizeof(struct tcphdr) / 4;\n\trep.th.rst    = 1;\n\n\tif (th->ack) {\n\t\trep.th.seq = th->ack_seq;\n\t} else {\n\t\trep.th.ack = 1;\n\t\trep.th.ack_seq = htonl(ntohl(th->seq) + th->syn + th->fin +\n\t\t\t\t       skb->len - (th->doff << 2));\n\t}\n\n\tmemset(&arg, 0, sizeof(arg));\n\targ.iov[0].iov_base = (unsigned char *)&rep;\n\targ.iov[0].iov_len  = sizeof(rep.th);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tkey = sk ? tcp_v4_md5_do_lookup(sk, ip_hdr(skb)->daddr) : NULL;\n\tif (key) {\n\t\trep.opt[0] = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t   (TCPOPT_NOP << 16) |\n\t\t\t\t   (TCPOPT_MD5SIG << 8) |\n\t\t\t\t   TCPOLEN_MD5SIG);\n\t\t/* Update length and the length the header thinks exists */\n\t\targ.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;\n\t\trep.th.doff = arg.iov[0].iov_len / 4;\n\n\t\ttcp_v4_md5_hash_hdr((__u8 *) &rep.opt[1],\n\t\t\t\t     key, ip_hdr(skb)->saddr,\n\t\t\t\t     ip_hdr(skb)->daddr, &rep.th);\n\t}\n#endif\n\targ.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,\n\t\t\t\t      ip_hdr(skb)->saddr, /* XXX */\n\t\t\t\t      arg.iov[0].iov_len, IPPROTO_TCP, 0);\n\targ.csumoffset = offsetof(struct tcphdr, check) / 2;\n\targ.flags = (sk && inet_sk(sk)->transparent) ? IP_REPLY_ARG_NOSRCCHECK : 0;\n\n\tnet = dev_net(skb_dst(skb)->dev);\n\tip_send_reply(net->ipv4.tcp_sock, skb,\n\t\t      &arg, arg.iov[0].iov_len);\n\n\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n\tTCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);\n}\n\n/* The code following below sending ACKs in SYN-RECV and TIME-WAIT states\n   outside socket context is ugly, certainly. What can I do?\n */\n\nstatic void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,\n\t\t\t    u32 win, u32 ts, int oif,\n\t\t\t    struct tcp_md5sig_key *key,\n\t\t\t    int reply_flags)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tstruct {\n\t\tstruct tcphdr th;\n\t\t__be32 opt[(TCPOLEN_TSTAMP_ALIGNED >> 2)\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\t   + (TCPOLEN_MD5SIG_ALIGNED >> 2)\n#endif\n\t\t\t];\n\t} rep;\n\tstruct ip_reply_arg arg;\n\tstruct net *net = dev_net(skb_dst(skb)->dev);\n\n\tmemset(&rep.th, 0, sizeof(struct tcphdr));\n\tmemset(&arg, 0, sizeof(arg));\n\n\targ.iov[0].iov_base = (unsigned char *)&rep;\n\targ.iov[0].iov_len  = sizeof(rep.th);\n\tif (ts) {\n\t\trep.opt[0] = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t   (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t   TCPOLEN_TIMESTAMP);\n\t\trep.opt[1] = htonl(tcp_time_stamp);\n\t\trep.opt[2] = htonl(ts);\n\t\targ.iov[0].iov_len += TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\n\t/* Swap the send and the receive. */\n\trep.th.dest    = th->source;\n\trep.th.source  = th->dest;\n\trep.th.doff    = arg.iov[0].iov_len / 4;\n\trep.th.seq     = htonl(seq);\n\trep.th.ack_seq = htonl(ack);\n\trep.th.ack     = 1;\n\trep.th.window  = htons(win);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key) {\n\t\tint offset = (ts) ? 3 : 0;\n\n\t\trep.opt[offset++] = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t\t  (TCPOPT_NOP << 16) |\n\t\t\t\t\t  (TCPOPT_MD5SIG << 8) |\n\t\t\t\t\t  TCPOLEN_MD5SIG);\n\t\targ.iov[0].iov_len += TCPOLEN_MD5SIG_ALIGNED;\n\t\trep.th.doff = arg.iov[0].iov_len/4;\n\n\t\ttcp_v4_md5_hash_hdr((__u8 *) &rep.opt[offset],\n\t\t\t\t    key, ip_hdr(skb)->saddr,\n\t\t\t\t    ip_hdr(skb)->daddr, &rep.th);\n\t}\n#endif\n\targ.flags = reply_flags;\n\targ.csum = csum_tcpudp_nofold(ip_hdr(skb)->daddr,\n\t\t\t\t      ip_hdr(skb)->saddr, /* XXX */\n\t\t\t\t      arg.iov[0].iov_len, IPPROTO_TCP, 0);\n\targ.csumoffset = offsetof(struct tcphdr, check) / 2;\n\tif (oif)\n\t\targ.bound_dev_if = oif;\n\n\tip_send_reply(net->ipv4.tcp_sock, skb,\n\t\t      &arg, arg.iov[0].iov_len);\n\n\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n}\n\nstatic void tcp_v4_timewait_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\n\ttcp_v4_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcptw->tw_ts_recent,\n\t\t\ttw->tw_bound_dev_if,\n\t\t\ttcp_twsk_md5_key(tcptw),\n\t\t\ttw->tw_transparent ? IP_REPLY_ARG_NOSRCCHECK : 0\n\t\t\t);\n\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v4_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\ttcp_v4_send_ack(skb, tcp_rsk(req)->snt_isn + 1,\n\t\t\ttcp_rsk(req)->rcv_isn + 1, req->rcv_wnd,\n\t\t\treq->ts_recent,\n\t\t\t0,\n\t\t\ttcp_v4_md5_do_lookup(sk, ip_hdr(skb)->daddr),\n\t\t\tinet_rsk(req)->no_srccheck ? IP_REPLY_ARG_NOSRCCHECK : 0);\n}\n\n/*\n *\tSend a SYN-ACK after having received a SYN.\n *\tThis still operates on a request_sock only, not on a big\n *\tsocket.\n */\nstatic int tcp_v4_send_synack(struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct request_values *rvp)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tint err = -1;\n\tstruct sk_buff * skb;\n\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\treturn -1;\n\n\tskb = tcp_make_synack(sk, dst, req, rvp);\n\n\tif (skb) {\n\t\t__tcp_v4_send_check(skb, ireq->loc_addr, ireq->rmt_addr);\n\n\t\terr = ip_build_and_send_pkt(skb, sk, ireq->loc_addr,\n\t\t\t\t\t    ireq->rmt_addr,\n\t\t\t\t\t    ireq->opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic int tcp_v4_rtx_synack(struct sock *sk, struct request_sock *req,\n\t\t\t      struct request_values *rvp)\n{\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\treturn tcp_v4_send_synack(sk, NULL, req, rvp);\n}\n\n/*\n *\tIPv4 request_sock destructor.\n */\nstatic void tcp_v4_reqsk_destructor(struct request_sock *req)\n{\n\tkfree(inet_rsk(req)->opt);\n}\n\nstatic void syn_flood_warning(const struct sk_buff *skb)\n{\n\tconst char *msg;\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (sysctl_tcp_syncookies)\n\t\tmsg = \"Sending cookies\";\n\telse\n#endif\n\t\tmsg = \"Dropping request\";\n\n\tpr_info(\"TCP: Possible SYN flooding on port %d. %s.\\n\",\n\t\t\t\tntohs(tcp_hdr(skb)->dest), msg);\n}\n\n/*\n * Save and compile IPv4 options into the request_sock if needed.\n */\nstatic struct ip_options_rcu *tcp_v4_save_options(struct sock *sk,\n\t\t\t\t\t\t  struct sk_buff *skb)\n{\n\tconst struct ip_options *opt = &(IPCB(skb)->opt);\n\tstruct ip_options_rcu *dopt = NULL;\n\n\tif (opt && opt->optlen) {\n\t\tint opt_size = sizeof(*dopt) + opt->optlen;\n\n\t\tdopt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (dopt) {\n\t\t\tif (ip_options_echo(&dopt->opt, skb)) {\n\t\t\t\tkfree(dopt);\n\t\t\t\tdopt = NULL;\n\t\t\t}\n\t\t}\n\t}\n\treturn dopt;\n}\n\n#ifdef CONFIG_TCP_MD5SIG\n/*\n * RFC2385 MD5 checksumming requires a mapping of\n * IP address->MD5 Key.\n * We need to maintain these in the sk structure.\n */\n\n/* Find the Key structure for an address.  */\nstatic struct tcp_md5sig_key *\n\t\t\ttcp_v4_md5_do_lookup(struct sock *sk, __be32 addr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tif (!tp->md5sig_info || !tp->md5sig_info->entries4)\n\t\treturn NULL;\n\tfor (i = 0; i < tp->md5sig_info->entries4; i++) {\n\t\tif (tp->md5sig_info->keys4[i].addr == addr)\n\t\t\treturn &tp->md5sig_info->keys4[i].base;\n\t}\n\treturn NULL;\n}\n\nstruct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,\n\t\t\t\t\t struct sock *addr_sk)\n{\n\treturn tcp_v4_md5_do_lookup(sk, inet_sk(addr_sk)->inet_daddr);\n}\nEXPORT_SYMBOL(tcp_v4_md5_lookup);\n\nstatic struct tcp_md5sig_key *tcp_v4_reqsk_md5_lookup(struct sock *sk,\n\t\t\t\t\t\t      struct request_sock *req)\n{\n\treturn tcp_v4_md5_do_lookup(sk, inet_rsk(req)->rmt_addr);\n}\n\n/* This can be called on a newly created socket, from other files */\nint tcp_v4_md5_do_add(struct sock *sk, __be32 addr,\n\t\t      u8 *newkey, u8 newkeylen)\n{\n\t/* Add Key to the list */\n\tstruct tcp_md5sig_key *key;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp4_md5sig_key *keys;\n\n\tkey = tcp_v4_md5_do_lookup(sk, addr);\n\tif (key) {\n\t\t/* Pre-existing entry - just update that one. */\n\t\tkfree(key->key);\n\t\tkey->key = newkey;\n\t\tkey->keylen = newkeylen;\n\t} else {\n\t\tstruct tcp_md5sig_info *md5sig;\n\n\t\tif (!tp->md5sig_info) {\n\t\t\ttp->md5sig_info = kzalloc(sizeof(*tp->md5sig_info),\n\t\t\t\t\t\t  GFP_ATOMIC);\n\t\t\tif (!tp->md5sig_info) {\n\t\t\t\tkfree(newkey);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t\t}\n\t\tif (tcp_alloc_md5sig_pool(sk) == NULL) {\n\t\t\tkfree(newkey);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmd5sig = tp->md5sig_info;\n\n\t\tif (md5sig->alloced4 == md5sig->entries4) {\n\t\t\tkeys = kmalloc((sizeof(*keys) *\n\t\t\t\t\t(md5sig->entries4 + 1)), GFP_ATOMIC);\n\t\t\tif (!keys) {\n\t\t\t\tkfree(newkey);\n\t\t\t\ttcp_free_md5sig_pool();\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tif (md5sig->entries4)\n\t\t\t\tmemcpy(keys, md5sig->keys4,\n\t\t\t\t       sizeof(*keys) * md5sig->entries4);\n\n\t\t\t/* Free old key list, and reference new one */\n\t\t\tkfree(md5sig->keys4);\n\t\t\tmd5sig->keys4 = keys;\n\t\t\tmd5sig->alloced4++;\n\t\t}\n\t\tmd5sig->entries4++;\n\t\tmd5sig->keys4[md5sig->entries4 - 1].addr        = addr;\n\t\tmd5sig->keys4[md5sig->entries4 - 1].base.key    = newkey;\n\t\tmd5sig->keys4[md5sig->entries4 - 1].base.keylen = newkeylen;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_v4_md5_do_add);\n\nstatic int tcp_v4_md5_add_func(struct sock *sk, struct sock *addr_sk,\n\t\t\t       u8 *newkey, u8 newkeylen)\n{\n\treturn tcp_v4_md5_do_add(sk, inet_sk(addr_sk)->inet_daddr,\n\t\t\t\t newkey, newkeylen);\n}\n\nint tcp_v4_md5_do_del(struct sock *sk, __be32 addr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tfor (i = 0; i < tp->md5sig_info->entries4; i++) {\n\t\tif (tp->md5sig_info->keys4[i].addr == addr) {\n\t\t\t/* Free the key */\n\t\t\tkfree(tp->md5sig_info->keys4[i].base.key);\n\t\t\ttp->md5sig_info->entries4--;\n\n\t\t\tif (tp->md5sig_info->entries4 == 0) {\n\t\t\t\tkfree(tp->md5sig_info->keys4);\n\t\t\t\ttp->md5sig_info->keys4 = NULL;\n\t\t\t\ttp->md5sig_info->alloced4 = 0;\n\t\t\t} else if (tp->md5sig_info->entries4 != i) {\n\t\t\t\t/* Need to do some manipulation */\n\t\t\t\tmemmove(&tp->md5sig_info->keys4[i],\n\t\t\t\t\t&tp->md5sig_info->keys4[i+1],\n\t\t\t\t\t(tp->md5sig_info->entries4 - i) *\n\t\t\t\t\t sizeof(struct tcp4_md5sig_key));\n\t\t\t}\n\t\t\ttcp_free_md5sig_pool();\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\nEXPORT_SYMBOL(tcp_v4_md5_do_del);\n\nstatic void tcp_v4_clear_md5_list(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Free each key, then the set of key keys,\n\t * the crypto element, and then decrement our\n\t * hold on the last resort crypto.\n\t */\n\tif (tp->md5sig_info->entries4) {\n\t\tint i;\n\t\tfor (i = 0; i < tp->md5sig_info->entries4; i++)\n\t\t\tkfree(tp->md5sig_info->keys4[i].base.key);\n\t\ttp->md5sig_info->entries4 = 0;\n\t\ttcp_free_md5sig_pool();\n\t}\n\tif (tp->md5sig_info->keys4) {\n\t\tkfree(tp->md5sig_info->keys4);\n\t\ttp->md5sig_info->keys4 = NULL;\n\t\ttp->md5sig_info->alloced4  = 0;\n\t}\n}\n\nstatic int tcp_v4_parse_md5_keys(struct sock *sk, char __user *optval,\n\t\t\t\t int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)&cmd.tcpm_addr;\n\tu8 *newkey;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin->sin_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tif (!cmd.tcpm_key || !cmd.tcpm_keylen) {\n\t\tif (!tcp_sk(sk)->md5sig_info)\n\t\t\treturn -ENOENT;\n\t\treturn tcp_v4_md5_do_del(sk, sin->sin_addr.s_addr);\n\t}\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\tif (!tcp_sk(sk)->md5sig_info) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tstruct tcp_md5sig_info *p;\n\n\t\tp = kzalloc(sizeof(*p), sk->sk_allocation);\n\t\tif (!p)\n\t\t\treturn -EINVAL;\n\n\t\ttp->md5sig_info = p;\n\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t}\n\n\tnewkey = kmemdup(cmd.tcpm_key, cmd.tcpm_keylen, sk->sk_allocation);\n\tif (!newkey)\n\t\treturn -ENOMEM;\n\treturn tcp_v4_md5_do_add(sk, sin->sin_addr.s_addr,\n\t\t\t\t newkey, cmd.tcpm_keylen);\n}\n\nstatic int tcp_v4_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,\n\t\t\t\t\t__be32 daddr, __be32 saddr, int nbytes)\n{\n\tstruct tcp4_pseudohdr *bp;\n\tstruct scatterlist sg;\n\n\tbp = &hp->md5_blk.ip4;\n\n\t/*\n\t * 1. the TCP pseudo-header (in the order: source IP address,\n\t * destination IP address, zero-padded protocol number, and\n\t * segment length)\n\t */\n\tbp->saddr = saddr;\n\tbp->daddr = daddr;\n\tbp->pad = 0;\n\tbp->protocol = IPPROTO_TCP;\n\tbp->len = cpu_to_be16(nbytes);\n\n\tsg_init_one(&sg, bp, sizeof(*bp));\n\treturn crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));\n}\n\nstatic int tcp_v4_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       __be32 daddr, __be32 saddr, struct tcphdr *th)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\tif (tcp_v4_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nint tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\tstruct sock *sk, struct request_sock *req,\n\t\t\tstruct sk_buff *skb)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\tstruct tcphdr *th = tcp_hdr(skb);\n\t__be32 saddr, daddr;\n\n\tif (sk) {\n\t\tsaddr = inet_sk(sk)->inet_saddr;\n\t\tdaddr = inet_sk(sk)->inet_daddr;\n\t} else if (req) {\n\t\tsaddr = inet_rsk(req)->loc_addr;\n\t\tdaddr = inet_rsk(req)->rmt_addr;\n\t} else {\n\t\tconst struct iphdr *iph = ip_hdr(skb);\n\t\tsaddr = iph->saddr;\n\t\tdaddr = iph->daddr;\n\t}\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\n\tif (tcp_v4_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\nEXPORT_SYMBOL(tcp_v4_md5_hash_skb);\n\nstatic int tcp_v4_inbound_md5_hash(struct sock *sk, struct sk_buff *skb)\n{\n\t/*\n\t * This gets called for each TCP segment that arrives\n\t * so we want to be efficient.\n\t * We have 3 drop cases:\n\t * o No MD5 hash and one expected.\n\t * o MD5 hash and we're not expecting one.\n\t * o MD5 hash and its wrong.\n\t */\n\t__u8 *hash_location = NULL;\n\tstruct tcp_md5sig_key *hash_expected;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tint genhash;\n\tunsigned char newhash[16];\n\n\thash_expected = tcp_v4_md5_do_lookup(sk, iph->saddr);\n\thash_location = tcp_parse_md5sig_option(th);\n\n\t/* We've parsed the options - do we have a hash? */\n\tif (!hash_expected && !hash_location)\n\t\treturn 0;\n\n\tif (hash_expected && !hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);\n\t\treturn 1;\n\t}\n\n\tif (!hash_expected && hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);\n\t\treturn 1;\n\t}\n\n\t/* Okay, so this is hash_expected and hash_location -\n\t * so we need to calculate the checksum.\n\t */\n\tgenhash = tcp_v4_md5_hash_skb(newhash,\n\t\t\t\t      hash_expected,\n\t\t\t\t      NULL, NULL, skb);\n\n\tif (genhash || memcmp(hash_location, newhash, 16) != 0) {\n\t\tif (net_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"MD5 Hash failed for (%pI4, %d)->(%pI4, %d)%s\\n\",\n\t\t\t       &iph->saddr, ntohs(th->source),\n\t\t\t       &iph->daddr, ntohs(th->dest),\n\t\t\t       genhash ? \" tcp_v4_calc_md5_hash failed\" : \"\");\n\t\t}\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n#endif\n\nstruct request_sock_ops tcp_request_sock_ops __read_mostly = {\n\t.family\t\t=\tPF_INET,\n\t.obj_size\t=\tsizeof(struct tcp_request_sock),\n\t.rtx_syn_ack\t=\ttcp_v4_rtx_synack,\n\t.send_ack\t=\ttcp_v4_reqsk_send_ack,\n\t.destructor\t=\ttcp_v4_reqsk_destructor,\n\t.send_reset\t=\ttcp_v4_send_reset,\n\t.syn_ack_timeout = \ttcp_syn_ack_timeout,\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops = {\n\t.md5_lookup\t=\ttcp_v4_reqsk_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n};\n#endif\n\nint tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_extend_values tmp_ext;\n\tstruct tcp_options_received tmp_opt;\n\tu8 *hash_location;\n\tstruct request_sock *req;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct dst_entry *dst = NULL;\n\t__be32 saddr = ip_hdr(skb)->saddr;\n\t__be32 daddr = ip_hdr(skb)->daddr;\n\t__u32 isn = TCP_SKB_CB(skb)->when;\n#ifdef CONFIG_SYN_COOKIES\n\tint want_cookie = 0;\n#else\n#define want_cookie 0 /* Argh, why doesn't gcc optimize this :( */\n#endif\n\n\t/* Never answer to SYNs send to broadcast or multicast */\n\tif (skb_rtable(skb)->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto drop;\n\n\t/* TW buckets are converted to open requests without\n\t * limitations, they conserve resources and peer is\n\t * evidently real one.\n\t */\n\tif (inet_csk_reqsk_queue_is_full(sk) && !isn) {\n\t\tif (net_ratelimit())\n\t\t\tsyn_flood_warning(skb);\n#ifdef CONFIG_SYN_COOKIES\n\t\tif (sysctl_tcp_syncookies) {\n\t\t\twant_cookie = 1;\n\t\t} else\n#endif\n\t\tgoto drop;\n\t}\n\n\t/* Accept backlog is full. If we have already queued enough\n\t * of warm entries in syn queue, drop request. It is better than\n\t * clogging syn queue with openreqs with exponentially increasing\n\t * timeout.\n\t */\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet_reqsk_alloc(&tcp_request_sock_ops);\n\tif (!req)\n\t\tgoto drop;\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttcp_rsk(req)->af_specific = &tcp_request_sock_ipv4_ops;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = TCP_MSS_DEFAULT;\n\ttmp_opt.user_mss  = tp->rx_opt.user_mss;\n\ttcp_parse_options(skb, &tmp_opt, &hash_location, 0);\n\n\tif (tmp_opt.cookie_plus > 0 &&\n\t    tmp_opt.saw_tstamp &&\n\t    !tp->rx_opt.cookie_out_never &&\n\t    (sysctl_tcp_cookie_size > 0 ||\n\t     (tp->cookie_values != NULL &&\n\t      tp->cookie_values->cookie_desired > 0))) {\n\t\tu8 *c;\n\t\tu32 *mess = &tmp_ext.cookie_bakery[COOKIE_DIGEST_WORDS];\n\t\tint l = tmp_opt.cookie_plus - TCPOLEN_COOKIE_BASE;\n\n\t\tif (tcp_cookie_generator(&tmp_ext.cookie_bakery[0]) != 0)\n\t\t\tgoto drop_and_release;\n\n\t\t/* Secret recipe starts with IP addresses */\n\t\t*mess++ ^= (__force u32)daddr;\n\t\t*mess++ ^= (__force u32)saddr;\n\n\t\t/* plus variable length Initiator Cookie */\n\t\tc = (u8 *)mess;\n\t\twhile (l-- > 0)\n\t\t\t*c++ ^= *hash_location++;\n\n#ifdef CONFIG_SYN_COOKIES\n\t\twant_cookie = 0;\t/* not our kind of cookie */\n#endif\n\t\ttmp_ext.cookie_out_never = 0; /* false */\n\t\ttmp_ext.cookie_plus = tmp_opt.cookie_plus;\n\t} else if (!tp->rx_opt.cookie_in_always) {\n\t\t/* redundant indications, but ensure initialization. */\n\t\ttmp_ext.cookie_out_never = 1; /* true */\n\t\ttmp_ext.cookie_plus = 0;\n\t} else {\n\t\tgoto drop_and_release;\n\t}\n\ttmp_ext.cookie_in_always = tp->rx_opt.cookie_in_always;\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb);\n\n\tireq = inet_rsk(req);\n\tireq->loc_addr = daddr;\n\tireq->rmt_addr = saddr;\n\tireq->no_srccheck = inet_sk(sk)->transparent;\n\tireq->opt = tcp_v4_save_options(sk, skb);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tif (!want_cookie || tmp_opt.tstamp_ok)\n\t\tTCP_ECN_create_request(req, tcp_hdr(skb));\n\n\tif (want_cookie) {\n\t\tisn = cookie_v4_init_sequence(sk, skb, &req->mss);\n\t\treq->cookie_ts = tmp_opt.tstamp_ok;\n\t} else if (!isn) {\n\t\tstruct inet_peer *peer = NULL;\n\n\t\t/* VJ's idea. We save last timestamp seen\n\t\t * from the destination in peer table, when entering\n\t\t * state TIME-WAIT, and check against it before\n\t\t * accepting new connection request.\n\t\t *\n\t\t * If \"isn\" is not zero, this request hit alive\n\t\t * timewait bucket, so that all the necessary checks\n\t\t * are made in the function processing timewait state.\n\t\t */\n\t\tif (tmp_opt.saw_tstamp &&\n\t\t    tcp_death_row.sysctl_tw_recycle &&\n\t\t    (dst = inet_csk_route_req(sk, req)) != NULL &&\n\t\t    (peer = rt_get_peer((struct rtable *)dst)) != NULL &&\n\t\t    peer->daddr.addr.a4 == saddr) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp < TCP_PAWS_MSL &&\n\t\t\t    (s32)(peer->tcp_ts - req->ts_recent) >\n\t\t\t\t\t\t\tTCP_PAWS_WINDOW) {\n\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);\n\t\t\t\tgoto drop_and_release;\n\t\t\t}\n\t\t}\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\telse if (!sysctl_tcp_syncookies &&\n\t\t\t (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t\t  (sysctl_max_syn_backlog >> 2)) &&\n\t\t\t (!peer || !peer->tcp_ts_stamp) &&\n\t\t\t (!dst || !dst_metric(dst, RTAX_RTT))) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tLIMIT_NETDEBUG(KERN_DEBUG \"TCP: drop open request from %pI4/%u\\n\",\n\t\t\t\t       &saddr, ntohs(tcp_hdr(skb)->source));\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = tcp_v4_init_sequence(skb);\n\t}\n\ttcp_rsk(req)->snt_isn = isn;\n\n\tif (tcp_v4_send_synack(sk, dst, req,\n\t\t\t       (struct request_values *)&tmp_ext) ||\n\t    want_cookie)\n\t\tgoto drop_and_free;\n\n\tinet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_v4_conn_request);\n\n\n/*\n * The three way handshake has completed - we got a valid synack -\n * now create the new socket.\n */\nstruct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct ip_options_rcu *inet_opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (!dst && (dst = inet_csk_route_req(sk, req)) == NULL)\n\t\tgoto exit;\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto exit_nonewsk;\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV4;\n\tsk_setup_caps(newsk, dst);\n\n\tnewtp\t\t      = tcp_sk(newsk);\n\tnewinet\t\t      = inet_sk(newsk);\n\tireq\t\t      = inet_rsk(req);\n\tnewinet->inet_daddr   = ireq->rmt_addr;\n\tnewinet->inet_rcv_saddr = ireq->loc_addr;\n\tnewinet->inet_saddr\t      = ireq->loc_addr;\n\tinet_opt\t      = ireq->opt;\n\trcu_assign_pointer(newinet->inet_opt, inet_opt);\n\tireq->opt\t      = NULL;\n\tnewinet->mc_index     = inet_iif(skb);\n\tnewinet->mc_ttl\t      = ip_hdr(skb)->ttl;\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\tnewinet->inet_id = newtp->write_seq ^ jiffies;\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v4_md5_do_lookup(sk, newinet->inet_daddr);\n\tif (key != NULL) {\n\t\t/*\n\t\t * We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v4_md5_do_add(newsk, newinet->inet_daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t\tsk_nocaps_add(newsk, NETIF_F_GSO_MASK);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto exit;\n\t}\n\t__inet_hash_nolisten(newsk, NULL);\n\n\treturn newsk;\n\nexit_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_v4_syn_recv_sock);\n\nstatic struct sock *tcp_v4_hnd_req(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct sock *nsk;\n\tstruct request_sock **prev;\n\t/* Find possible connection requests. */\n\tstruct request_sock *req = inet_csk_search_req(sk, &prev, th->source,\n\t\t\t\t\t\t       iph->saddr, iph->daddr);\n\tif (req)\n\t\treturn tcp_check_req(sk, skb, req, prev);\n\n\tnsk = inet_lookup_established(sock_net(sk), &tcp_hashinfo, iph->saddr,\n\t\t\tth->source, iph->daddr, th->dest, inet_iif(skb));\n\n\tif (nsk) {\n\t\tif (nsk->sk_state != TCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (!th->syn)\n\t\tsk = cookie_v4_check(sk, skb, &(IPCB(skb)->opt));\n#endif\n\treturn sk;\n}\n\nstatic __sum16 tcp_v4_checksum_init(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!tcp_v4_check(skb->len, iph->saddr,\n\t\t\t\t  iph->daddr, skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t       skb->len, IPPROTO_TCP, 0);\n\n\tif (skb->len <= 76) {\n\t\treturn __skb_checksum_complete(skb);\n\t}\n\treturn 0;\n}\n\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nint tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sock *rsk;\n#ifdef CONFIG_TCP_MD5SIG\n\t/*\n\t * We really want to reject the packet as early as possible\n\t * if:\n\t *  o We're expecting an MD5'd packet and this is no MD5 tcp option\n\t *  o There is an MD5 option and we're not expecting one\n\t */\n\tif (tcp_v4_inbound_md5_hash(sk, skb))\n\t\tgoto discard;\n#endif\n\n\tif (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\t\tif (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len)) {\n\t\t\trsk = sk;\n\t\t\tgoto reset;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v4_hnd_req(sk, skb);\n\t\tif (!nsk)\n\t\t\tgoto discard;\n\n\t\tif (nsk != sk) {\n\t\t\tif (tcp_child_process(sk, nsk, skb)) {\n\t\t\t\trsk = nsk;\n\t\t\t\tgoto reset;\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\tif (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb->len)) {\n\t\trsk = sk;\n\t\tgoto reset;\n\t}\n\treturn 0;\n\nreset:\n\ttcp_v4_send_reset(rsk, skb);\ndiscard:\n\tkfree_skb(skb);\n\t/* Be careful here. If this function gets more complicated and\n\t * gcc suffers from register pressure on the x86, sk (in %ebx)\n\t * might be destroyed here. This current version compiles correctly,\n\t * but you have been warned.\n\t */\n\treturn 0;\n\ncsum_err:\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n}\nEXPORT_SYMBOL(tcp_v4_do_rcv);\n\n/*\n *\tFrom tcp_input.c\n */\n\nint tcp_v4_rcv(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct tcphdr *th;\n\tstruct sock *sk;\n\tint ret;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/* Count it even if it's bad */\n\tTCP_INC_STATS_BH(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr) / 4)\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff * 4))\n\t\tgoto discard_it;\n\n\t/* An explanation is required here, I think.\n\t * Packet length and doff are validated by header prediction,\n\t * provided case of th->doff==0 is eliminated.\n\t * So, we defer the checks. */\n\tif (!skb_csum_unnecessary(skb) && tcp_v4_checksum_init(skb))\n\t\tgoto bad_packet;\n\n\tth = tcp_hdr(skb);\n\tiph = ip_hdr(skb);\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff * 4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->when\t = 0;\n\tTCP_SKB_CB(skb)->flags\t = iph->tos;\n\tTCP_SKB_CB(skb)->sacked\t = 0;\n\n\tsk = __inet_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (unlikely(iph->ttl < inet_sk(sk)->min_ttl)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\tnf_reset(skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tbh_lock_sock_nested(sk);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n#ifdef CONFIG_NET_DMA\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)\n\t\t\ttp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);\n\t\tif (tp->ucopy.dma_chan)\n\t\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\telse\n#endif\n\t\t{\n\t\t\tif (!tcp_prequeue(sk, skb))\n\t\t\t\tret = tcp_v4_do_rcv(sk, skb);\n\t\t}\n\t} else if (unlikely(sk_add_backlog(sk, skb))) {\n\t\tbh_unlock_sock(sk);\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\n\tsock_put(sk);\n\n\treturn ret;\n\nno_tcp_socket:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\tif (skb->len < (th->doff << 2) || tcp_checksum_complete(skb)) {\nbad_packet:\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v4_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\t/* Discard frame. */\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tif (skb->len < (th->doff << 2) || tcp_checksum_complete(skb)) {\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN: {\n\t\tstruct sock *sk2 = inet_lookup_listener(dev_net(skb->dev),\n\t\t\t\t\t\t\t&tcp_hashinfo,\n\t\t\t\t\t\t\tiph->daddr, th->dest,\n\t\t\t\t\t\t\tinet_iif(skb));\n\t\tif (sk2) {\n\t\t\tinet_twsk_deschedule(inet_twsk(sk), &tcp_death_row);\n\t\t\tinet_twsk_put(inet_twsk(sk));\n\t\t\tsk = sk2;\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v4_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\tgoto no_tcp_socket;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n\nstruct inet_peer *tcp_v4_get_peer(struct sock *sk, bool *release_it)\n{\n\tstruct rtable *rt = (struct rtable *) __sk_dst_get(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_peer *peer;\n\n\tif (!rt || rt->rt_dst != inet->inet_daddr) {\n\t\tpeer = inet_getpeer_v4(inet->inet_daddr, 1);\n\t\t*release_it = true;\n\t} else {\n\t\tif (!rt->peer)\n\t\t\trt_bind_peer(rt, 1);\n\t\tpeer = rt->peer;\n\t\t*release_it = false;\n\t}\n\n\treturn peer;\n}\nEXPORT_SYMBOL(tcp_v4_get_peer);\n\nvoid *tcp_v4_tw_get_peer(struct sock *sk)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\n\treturn inet_getpeer_v4(tw->tw_daddr, 1);\n}\nEXPORT_SYMBOL(tcp_v4_tw_get_peer);\n\nstatic struct timewait_sock_ops tcp_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp_timewait_sock),\n\t.twsk_unique\t= tcp_twsk_unique,\n\t.twsk_destructor= tcp_twsk_destructor,\n\t.twsk_getpeer\t= tcp_v4_tw_get_peer,\n};\n\nconst struct inet_connection_sock_af_ops ipv4_specific = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = tcp_v4_conn_request,\n\t.syn_recv_sock\t   = tcp_v4_syn_recv_sock,\n\t.get_peer\t   = tcp_v4_get_peer,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.addr2sockaddr\t   = inet_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in),\n\t.bind_conflict\t   = inet_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ip_setsockopt,\n\t.compat_getsockopt = compat_ip_getsockopt,\n#endif\n};\nEXPORT_SYMBOL(ipv4_specific);\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv4_specific = {\n\t.md5_lookup\t\t= tcp_v4_md5_lookup,\n\t.calc_md5_hash\t\t= tcp_v4_md5_hash_skb,\n\t.md5_add\t\t= tcp_v4_md5_add_func,\n\t.md5_parse\t\t= tcp_v4_parse_md5_keys,\n};\n#endif\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int tcp_v4_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tskb_queue_head_init(&tp->out_of_order_queue);\n\ttcp_init_xmit_timers(sk);\n\ttcp_prequeue_init(tp);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ttp->mdev = TCP_TIMEOUT_INIT;\n\n\t/* So many TCP implementations out there (incorrectly) count the\n\t * initial SYN frame in their delayed-ACK and congestion control\n\t * algorithms that we must have the following bandaid to talk\n\t * efficiently to them.  -DaveM\n\t */\n\ttp->snd_cwnd = 2;\n\n\t/* See draft-stevens-tcpca-spec-01 for discussion of the\n\t * initialization of these values.\n\t */\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = sysctl_tcp_reordering;\n\ticsk->icsk_ca_ops = &tcp_init_congestion_ops;\n\n\tsk->sk_state = TCP_CLOSE;\n\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n\ticsk->icsk_af_ops = &ipv4_specific;\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n#ifdef CONFIG_TCP_MD5SIG\n\ttp->af_specific = &tcp_sock_ipv4_specific;\n#endif\n\n\t/* TCP Cookie Transactions */\n\tif (sysctl_tcp_cookie_size > 0) {\n\t\t/* Default, cookies without s_data_payload. */\n\t\ttp->cookie_values =\n\t\t\tkzalloc(sizeof(*tp->cookie_values),\n\t\t\t\tsk->sk_allocation);\n\t\tif (tp->cookie_values != NULL)\n\t\t\tkref_init(&tp->cookie_values->kref);\n\t}\n\t/* Presumed zeroed, in order of appearance:\n\t *\tcookie_in_always, cookie_out_never,\n\t *\ts_data_constant, s_data_in, s_data_out\n\t */\n\tsk->sk_sndbuf = sysctl_tcp_wmem[1];\n\tsk->sk_rcvbuf = sysctl_tcp_rmem[1];\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&tcp_sockets_allocated);\n\tlocal_bh_enable();\n\n\treturn 0;\n}\n\nvoid tcp_v4_destroy_sock(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_clear_xmit_timers(sk);\n\n\ttcp_cleanup_congestion_control(sk);\n\n\t/* Cleanup up the write buffer. */\n\ttcp_write_queue_purge(sk);\n\n\t/* Cleans up our, hopefully empty, out_of_order_queue. */\n\t__skb_queue_purge(&tp->out_of_order_queue);\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Clean up the MD5 key list, if any */\n\tif (tp->md5sig_info) {\n\t\ttcp_v4_clear_md5_list(sk);\n\t\tkfree(tp->md5sig_info);\n\t\ttp->md5sig_info = NULL;\n\t}\n#endif\n\n#ifdef CONFIG_NET_DMA\n\t/* Cleans up our sk_async_wait_queue */\n\t__skb_queue_purge(&sk->sk_async_wait_queue);\n#endif\n\n\t/* Clean prequeue, it must be empty really */\n\t__skb_queue_purge(&tp->ucopy.prequeue);\n\n\t/* Clean up a referenced TCP bind bucket. */\n\tif (inet_csk(sk)->icsk_bind_hash)\n\t\tinet_put_port(sk);\n\n\t/*\n\t * If sendmsg cached page exists, toss it.\n\t */\n\tif (sk->sk_sndmsg_page) {\n\t\t__free_page(sk->sk_sndmsg_page);\n\t\tsk->sk_sndmsg_page = NULL;\n\t}\n\n\t/* TCP Cookie Transactions */\n\tif (tp->cookie_values != NULL) {\n\t\tkref_put(&tp->cookie_values->kref,\n\t\t\t tcp_cookie_values_release);\n\t\ttp->cookie_values = NULL;\n\t}\n\n\tpercpu_counter_dec(&tcp_sockets_allocated);\n}\nEXPORT_SYMBOL(tcp_v4_destroy_sock);\n\n#ifdef CONFIG_PROC_FS\n/* Proc filesystem TCP sock list dumping. */\n\nstatic inline struct inet_timewait_sock *tw_head(struct hlist_nulls_head *head)\n{\n\treturn hlist_nulls_empty(head) ? NULL :\n\t\tlist_entry(head->first, struct inet_timewait_sock, tw_node);\n}\n\nstatic inline struct inet_timewait_sock *tw_next(struct inet_timewait_sock *tw)\n{\n\treturn !is_a_nulls(tw->tw_node.next) ?\n\t\thlist_nulls_entry(tw->tw_node.next, typeof(*tw), tw_node) : NULL;\n}\n\n/*\n * Get next listener socket follow cur.  If cur is NULL, get first socket\n * starting from bucket given in st->bucket; when st->bucket is zero the\n * very first socket in the hash table is returned.\n */\nstatic void *listening_get_next(struct seq_file *seq, void *cur)\n{\n\tstruct inet_connection_sock *icsk;\n\tstruct hlist_nulls_node *node;\n\tstruct sock *sk = cur;\n\tstruct inet_listen_hashbucket *ilb;\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tif (!sk) {\n\t\tilb = &tcp_hashinfo.listening_hash[st->bucket];\n\t\tspin_lock_bh(&ilb->lock);\n\t\tsk = sk_nulls_head(&ilb->head);\n\t\tst->offset = 0;\n\t\tgoto get_sk;\n\t}\n\tilb = &tcp_hashinfo.listening_hash[st->bucket];\n\t++st->num;\n\t++st->offset;\n\n\tif (st->state == TCP_SEQ_STATE_OPENREQ) {\n\t\tstruct request_sock *req = cur;\n\n\t\ticsk = inet_csk(st->syn_wait_sk);\n\t\treq = req->dl_next;\n\t\twhile (1) {\n\t\t\twhile (req) {\n\t\t\t\tif (req->rsk_ops->family == st->family) {\n\t\t\t\t\tcur = req;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\treq = req->dl_next;\n\t\t\t}\n\t\t\tif (++st->sbucket >= icsk->icsk_accept_queue.listen_opt->nr_table_entries)\n\t\t\t\tbreak;\nget_req:\n\t\t\treq = icsk->icsk_accept_queue.listen_opt->syn_table[st->sbucket];\n\t\t}\n\t\tsk\t  = sk_nulls_next(st->syn_wait_sk);\n\t\tst->state = TCP_SEQ_STATE_LISTENING;\n\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t} else {\n\t\ticsk = inet_csk(sk);\n\t\tread_lock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\tif (reqsk_queue_len(&icsk->icsk_accept_queue))\n\t\t\tgoto start_req;\n\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\tsk = sk_nulls_next(sk);\n\t}\nget_sk:\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\t\tif (sk->sk_family == st->family) {\n\t\t\tcur = sk;\n\t\t\tgoto out;\n\t\t}\n\t\ticsk = inet_csk(sk);\n\t\tread_lock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\tif (reqsk_queue_len(&icsk->icsk_accept_queue)) {\nstart_req:\n\t\t\tst->uid\t\t= sock_i_uid(sk);\n\t\t\tst->syn_wait_sk = sk;\n\t\t\tst->state\t= TCP_SEQ_STATE_OPENREQ;\n\t\t\tst->sbucket\t= 0;\n\t\t\tgoto get_req;\n\t\t}\n\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t}\n\tspin_unlock_bh(&ilb->lock);\n\tst->offset = 0;\n\tif (++st->bucket < INET_LHTABLE_SIZE) {\n\t\tilb = &tcp_hashinfo.listening_hash[st->bucket];\n\t\tspin_lock_bh(&ilb->lock);\n\t\tsk = sk_nulls_head(&ilb->head);\n\t\tgoto get_sk;\n\t}\n\tcur = NULL;\nout:\n\treturn cur;\n}\n\nstatic void *listening_get_idx(struct seq_file *seq, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tst->bucket = 0;\n\tst->offset = 0;\n\trc = listening_get_next(seq, NULL);\n\n\twhile (rc && *pos) {\n\t\trc = listening_get_next(seq, rc);\n\t\t--*pos;\n\t}\n\treturn rc;\n}\n\nstatic inline int empty_bucket(struct tcp_iter_state *st)\n{\n\treturn hlist_nulls_empty(&tcp_hashinfo.ehash[st->bucket].chain) &&\n\t\thlist_nulls_empty(&tcp_hashinfo.ehash[st->bucket].twchain);\n}\n\n/*\n * Get first established socket starting from bucket given in st->bucket.\n * If st->bucket is zero, the very first socket in the hash is returned.\n */\nstatic void *established_get_first(struct seq_file *seq)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\tvoid *rc = NULL;\n\n\tst->offset = 0;\n\tfor (; st->bucket <= tcp_hashinfo.ehash_mask; ++st->bucket) {\n\t\tstruct sock *sk;\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct inet_timewait_sock *tw;\n\t\tspinlock_t *lock = inet_ehash_lockp(&tcp_hashinfo, st->bucket);\n\n\t\t/* Lockless fast path for the common case of empty buckets */\n\t\tif (empty_bucket(st))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(lock);\n\t\tsk_nulls_for_each(sk, node, &tcp_hashinfo.ehash[st->bucket].chain) {\n\t\t\tif (sk->sk_family != st->family ||\n\t\t\t    !net_eq(sock_net(sk), net)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\trc = sk;\n\t\t\tgoto out;\n\t\t}\n\t\tst->state = TCP_SEQ_STATE_TIME_WAIT;\n\t\tinet_twsk_for_each(tw, node,\n\t\t\t\t   &tcp_hashinfo.ehash[st->bucket].twchain) {\n\t\t\tif (tw->tw_family != st->family ||\n\t\t\t    !net_eq(twsk_net(tw), net)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\trc = tw;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock_bh(lock);\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t}\nout:\n\treturn rc;\n}\n\nstatic void *established_get_next(struct seq_file *seq, void *cur)\n{\n\tstruct sock *sk = cur;\n\tstruct inet_timewait_sock *tw;\n\tstruct hlist_nulls_node *node;\n\tstruct tcp_iter_state *st = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\t++st->num;\n\t++st->offset;\n\n\tif (st->state == TCP_SEQ_STATE_TIME_WAIT) {\n\t\ttw = cur;\n\t\ttw = tw_next(tw);\nget_tw:\n\t\twhile (tw && (tw->tw_family != st->family || !net_eq(twsk_net(tw), net))) {\n\t\t\ttw = tw_next(tw);\n\t\t}\n\t\tif (tw) {\n\t\t\tcur = tw;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\n\t\t/* Look for next non empty bucket */\n\t\tst->offset = 0;\n\t\twhile (++st->bucket <= tcp_hashinfo.ehash_mask &&\n\t\t\t\tempty_bucket(st))\n\t\t\t;\n\t\tif (st->bucket > tcp_hashinfo.ehash_mask)\n\t\t\treturn NULL;\n\n\t\tspin_lock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));\n\t\tsk = sk_nulls_head(&tcp_hashinfo.ehash[st->bucket].chain);\n\t} else\n\t\tsk = sk_nulls_next(sk);\n\n\tsk_nulls_for_each_from(sk, node) {\n\t\tif (sk->sk_family == st->family && net_eq(sock_net(sk), net))\n\t\t\tgoto found;\n\t}\n\n\tst->state = TCP_SEQ_STATE_TIME_WAIT;\n\ttw = tw_head(&tcp_hashinfo.ehash[st->bucket].twchain);\n\tgoto get_tw;\nfound:\n\tcur = sk;\nout:\n\treturn cur;\n}\n\nstatic void *established_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tst->bucket = 0;\n\trc = established_get_first(seq);\n\n\twhile (rc && pos) {\n\t\trc = established_get_next(seq, rc);\n\t\t--pos;\n\t}\n\treturn rc;\n}\n\nstatic void *tcp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tvoid *rc;\n\tstruct tcp_iter_state *st = seq->private;\n\n\tst->state = TCP_SEQ_STATE_LISTENING;\n\trc\t  = listening_get_idx(seq, &pos);\n\n\tif (!rc) {\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\trc\t  = established_get_idx(seq, pos);\n\t}\n\n\treturn rc;\n}\n\nstatic void *tcp_seek_last_pos(struct seq_file *seq)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tint offset = st->offset;\n\tint orig_num = st->num;\n\tvoid *rc = NULL;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_OPENREQ:\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\tif (st->bucket >= INET_LHTABLE_SIZE)\n\t\t\tbreak;\n\t\tst->state = TCP_SEQ_STATE_LISTENING;\n\t\trc = listening_get_next(seq, NULL);\n\t\twhile (offset-- && rc)\n\t\t\trc = listening_get_next(seq, rc);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tst->bucket = 0;\n\t\t/* Fallthrough */\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\tif (st->bucket > tcp_hashinfo.ehash_mask)\n\t\t\tbreak;\n\t\trc = established_get_first(seq);\n\t\twhile (offset-- && rc)\n\t\t\trc = established_get_next(seq, rc);\n\t}\n\n\tst->num = orig_num;\n\n\treturn rc;\n}\n\nstatic void *tcp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc;\n\n\tif (*pos && *pos == st->last_pos) {\n\t\trc = tcp_seek_last_pos(seq);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\tst->state = TCP_SEQ_STATE_LISTENING;\n\tst->num = 0;\n\tst->bucket = 0;\n\tst->offset = 0;\n\trc = *pos ? tcp_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;\n\nout:\n\tst->last_pos = *pos;\n\treturn rc;\n}\n\nstatic void *tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\tvoid *rc = NULL;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\trc = tcp_get_idx(seq, 0);\n\t\tgoto out;\n\t}\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_OPENREQ:\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\trc = listening_get_next(seq, v);\n\t\tif (!rc) {\n\t\t\tst->state = TCP_SEQ_STATE_ESTABLISHED;\n\t\t\tst->bucket = 0;\n\t\t\tst->offset = 0;\n\t\t\trc\t  = established_get_first(seq);\n\t\t}\n\t\tbreak;\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\trc = established_get_next(seq, v);\n\t\tbreak;\n\t}\nout:\n\t++*pos;\n\tst->last_pos = *pos;\n\treturn rc;\n}\n\nstatic void tcp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_OPENREQ:\n\t\tif (v) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(st->syn_wait_sk);\n\t\t\tread_unlock_bh(&icsk->icsk_accept_queue.syn_wait_lock);\n\t\t}\n\tcase TCP_SEQ_STATE_LISTENING:\n\t\tif (v != SEQ_START_TOKEN)\n\t\t\tspin_unlock_bh(&tcp_hashinfo.listening_hash[st->bucket].lock);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tif (v)\n\t\t\tspin_unlock_bh(inet_ehash_lockp(&tcp_hashinfo, st->bucket));\n\t\tbreak;\n\t}\n}\n\nstatic int tcp_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct tcp_seq_afinfo *afinfo = PDE(inode)->data;\n\tstruct tcp_iter_state *s;\n\tint err;\n\n\terr = seq_open_net(inode, file, &afinfo->seq_ops,\n\t\t\t  sizeof(struct tcp_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ts = ((struct seq_file *)file->private_data)->private;\n\ts->family\t\t= afinfo->family;\n\ts->last_pos \t\t= 0;\n\treturn 0;\n}\n\nint tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo)\n{\n\tint rc = 0;\n\tstruct proc_dir_entry *p;\n\n\tafinfo->seq_fops.open\t\t= tcp_seq_open;\n\tafinfo->seq_fops.read\t\t= seq_read;\n\tafinfo->seq_fops.llseek\t\t= seq_lseek;\n\tafinfo->seq_fops.release\t= seq_release_net;\n\n\tafinfo->seq_ops.start\t\t= tcp_seq_start;\n\tafinfo->seq_ops.next\t\t= tcp_seq_next;\n\tafinfo->seq_ops.stop\t\t= tcp_seq_stop;\n\n\tp = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,\n\t\t\t     &afinfo->seq_fops, afinfo);\n\tif (!p)\n\t\trc = -ENOMEM;\n\treturn rc;\n}\nEXPORT_SYMBOL(tcp_proc_register);\n\nvoid tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo)\n{\n\tproc_net_remove(net, afinfo->name);\n}\nEXPORT_SYMBOL(tcp_proc_unregister);\n\nstatic void get_openreq4(struct sock *sk, struct request_sock *req,\n\t\t\t struct seq_file *f, int i, int uid, int *len)\n{\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tint ttd = req->expires - jiffies;\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %u %d %p%n\",\n\t\ti,\n\t\tireq->loc_addr,\n\t\tntohs(inet_sk(sk)->inet_sport),\n\t\tireq->rmt_addr,\n\t\tntohs(ireq->rmt_port),\n\t\tTCP_SYN_RECV,\n\t\t0, 0, /* could print option size, but that is af dependent. */\n\t\t1,    /* timers active (only the expire timer) */\n\t\tjiffies_to_clock_t(ttd),\n\t\treq->retrans,\n\t\tuid,\n\t\t0,  /* non standard timer */\n\t\t0, /* open_requests have no inode */\n\t\tatomic_read(&sk->sk_refcnt),\n\t\treq,\n\t\tlen);\n}\n\nstatic void get_tcp4_sock(struct sock *sk, struct seq_file *f, int i, int *len)\n{\n\tint timer_active;\n\tunsigned long timer_expires;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src = inet->inet_rcv_saddr;\n\t__u16 destp = ntohs(inet->inet_dport);\n\t__u16 srcp = ntohs(inet->inet_sport);\n\tint rx_queue;\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (timer_pending(&sk->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sk->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\trx_queue = sk->sk_ack_backlog;\n\telse\n\t\t/*\n\t\t * because we dont lock socket, we might find a transient negative value\n\t\t */\n\t\trx_queue = max_t(int, tp->rcv_nxt - tp->copied_seq, 0);\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X %02X %08X:%08X %02X:%08lX \"\n\t\t\t\"%08X %5d %8d %lu %d %p %lu %lu %u %u %d%n\",\n\t\ti, src, srcp, dest, destp, sk->sk_state,\n\t\ttp->write_seq - tp->snd_una,\n\t\trx_queue,\n\t\ttimer_active,\n\t\tjiffies_to_clock_t(timer_expires - jiffies),\n\t\ticsk->icsk_retransmits,\n\t\tsock_i_uid(sk),\n\t\ticsk->icsk_probes_out,\n\t\tsock_i_ino(sk),\n\t\tatomic_read(&sk->sk_refcnt), sk,\n\t\tjiffies_to_clock_t(icsk->icsk_rto),\n\t\tjiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t(icsk->icsk_ack.quick << 1) | icsk->icsk_ack.pingpong,\n\t\ttp->snd_cwnd,\n\t\ttcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh,\n\t\tlen);\n}\n\nstatic void get_timewait4_sock(struct inet_timewait_sock *tw,\n\t\t\t       struct seq_file *f, int i, int *len)\n{\n\t__be32 dest, src;\n\t__u16 destp, srcp;\n\tint ttd = tw->tw_ttd - jiffies;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tdest  = tw->tw_daddr;\n\tsrc   = tw->tw_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(f, \"%4d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p%n\",\n\t\ti, src, srcp, dest, destp, tw->tw_substate, 0, 0,\n\t\t3, jiffies_to_clock_t(ttd), 0, 0, 0, 0,\n\t\tatomic_read(&tw->tw_refcnt), tw, len);\n}\n\n#define TMPSZ 150\n\nstatic int tcp4_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\tint len;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_printf(seq, \"%-*s\\n\", TMPSZ - 1,\n\t\t\t   \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tget_tcp4_sock(v, seq, st->num, &len);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_OPENREQ:\n\t\tget_openreq4(st->syn_wait_sk, v, seq, st->num, st->uid, &len);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\tget_timewait4_sock(v, seq, st->num, &len);\n\t\tbreak;\n\t}\n\tseq_printf(seq, \"%*s\\n\", TMPSZ - 1 - len, \"\");\nout:\n\treturn 0;\n}\n\nstatic struct tcp_seq_afinfo tcp4_seq_afinfo = {\n\t.name\t\t= \"tcp\",\n\t.family\t\t= AF_INET,\n\t.seq_fops\t= {\n\t\t.owner\t\t= THIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= tcp4_seq_show,\n\t},\n};\n\nstatic int __net_init tcp4_proc_init_net(struct net *net)\n{\n\treturn tcp_proc_register(net, &tcp4_seq_afinfo);\n}\n\nstatic void __net_exit tcp4_proc_exit_net(struct net *net)\n{\n\ttcp_proc_unregister(net, &tcp4_seq_afinfo);\n}\n\nstatic struct pernet_operations tcp4_net_ops = {\n\t.init = tcp4_proc_init_net,\n\t.exit = tcp4_proc_exit_net,\n};\n\nint __init tcp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&tcp4_net_ops);\n}\n\nvoid tcp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&tcp4_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n\nstruct sk_buff **tcp4_gro_receive(struct sk_buff **head, struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = skb_gro_network_header(skb);\n\n\tswitch (skb->ip_summed) {\n\tcase CHECKSUM_COMPLETE:\n\t\tif (!tcp_v4_check(skb_gro_len(skb), iph->saddr, iph->daddr,\n\t\t\t\t  skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* fall through */\n\tcase CHECKSUM_NONE:\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\treturn tcp_gro_receive(head, skb);\n}\n\nint tcp4_gro_complete(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tth->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  iph->saddr, iph->daddr, 0);\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\n\treturn tcp_gro_complete(skb);\n}\n\nstruct proto tcp_prot = {\n\t.name\t\t\t= \"TCP\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.connect\t\t= tcp_v4_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v4_init_sock,\n\t.destroy\t\t= tcp_v4_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.sendpage\t\t= tcp_sendpage,\n\t.backlog_rcv\t\t= tcp_v4_do_rcv,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\t.orphan_count\t\t= &tcp_orphan_count,\n\t.memory_allocated\t= &tcp_memory_allocated,\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem\t\t= sysctl_tcp_wmem,\n\t.sysctl_rmem\t\t= sysctl_tcp_rmem,\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.twsk_prot\t\t= &tcp_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp_request_sock_ops,\n\t.h.hashinfo\t\t= &tcp_hashinfo,\n\t.no_autobind\t\t= true,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_tcp_setsockopt,\n\t.compat_getsockopt\t= compat_tcp_getsockopt,\n#endif\n};\nEXPORT_SYMBOL(tcp_prot);\n\n\nstatic int __net_init tcp_sk_init(struct net *net)\n{\n\treturn inet_ctl_sock_create(&net->ipv4.tcp_sock,\n\t\t\t\t    PF_INET, SOCK_RAW, IPPROTO_TCP, net);\n}\n\nstatic void __net_exit tcp_sk_exit(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->ipv4.tcp_sock);\n}\n\nstatic void __net_exit tcp_sk_exit_batch(struct list_head *net_exit_list)\n{\n\tinet_twsk_purge(&tcp_hashinfo, &tcp_death_row, AF_INET);\n}\n\nstatic struct pernet_operations __net_initdata tcp_sk_ops = {\n       .init\t   = tcp_sk_init,\n       .exit\t   = tcp_sk_exit,\n       .exit_batch = tcp_sk_exit_batch,\n};\n\nvoid __init tcp_v4_init(void)\n{\n\tinet_hashinfo_init(&tcp_hashinfo);\n\tif (register_pernet_subsys(&tcp_sk_ops))\n\t\tpanic(\"Failed to create the TCP control socket.\\n\");\n}\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tThe User Datagram Protocol (UDP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tHirokazu Takahashi, <taka@valinux.co.jp>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() calls\n *\t\tAlan Cox\t: \tstopped close while in use off icmp\n *\t\t\t\t\tmessages. Not a fix but a botch that\n *\t\t\t\t\tfor udp at least is 'valid'.\n *\t\tAlan Cox\t:\tFixed icmp handling properly\n *\t\tAlan Cox\t: \tCorrect error for oversized datagrams\n *\t\tAlan Cox\t:\tTidied select() semantics.\n *\t\tAlan Cox\t:\tudp_err() fixed properly, also now\n *\t\t\t\t\tselect and read wake correctly on errors\n *\t\tAlan Cox\t:\tudp_send verify_area moved to avoid mem leak\n *\t\tAlan Cox\t:\tUDP can count its memory\n *\t\tAlan Cox\t:\tsend to an unknown connection causes\n *\t\t\t\t\tan ECONNREFUSED off the icmp, but\n *\t\t\t\t\tdoes NOT close.\n *\t\tAlan Cox\t:\tSwitched to new sk_buff handlers. No more backlog!\n *\t\tAlan Cox\t:\tUsing generic datagram code. Even smaller and the PEEK\n *\t\t\t\t\tbug no longer crashes it.\n *\t\tFred Van Kempen\t: \tNet2e support for sk->broadcast.\n *\t\tAlan Cox\t:\tUses skb_free_datagram\n *\t\tAlan Cox\t:\tAdded get/set sockopt support.\n *\t\tAlan Cox\t:\tBroadcasting without option set returns EACCES.\n *\t\tAlan Cox\t:\tNo wakeup calls. Instead we now use the callbacks.\n *\t\tAlan Cox\t:\tUse ip_tos and ip_ttl\n *\t\tAlan Cox\t:\tSNMP Mibs\n *\t\tAlan Cox\t:\tMSG_DONTROUTE, and 0.0.0.0 support.\n *\t\tMatt Dillon\t:\tUDP length checks.\n *\t\tAlan Cox\t:\tSmarter af_inet used properly.\n *\t\tAlan Cox\t:\tUse new kernel side addressing.\n *\t\tAlan Cox\t:\tIncorrect return on truncated datagram receive.\n *\tArnt Gulbrandsen \t:\tNew udp_send and stuff\n *\t\tAlan Cox\t:\tCache last socket\n *\t\tAlan Cox\t:\tRoute cache\n *\t\tJon Peatfield\t:\tMinor efficiency fix to sendto().\n *\t\tMike Shaver\t:\tRFC1122 checks.\n *\t\tAlan Cox\t:\tNonblocking error fix.\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\t\tMike McLagan\t:\tRouting by source\n *\t\tDavid S. Miller\t:\tNew socket lookup architecture.\n *\t\t\t\t\tLast socket cache retained as it\n *\t\t\t\t\tdoes have a high hit rate.\n *\t\tOlaf Kirch\t:\tDon't linearise iovec on sendmsg.\n *\t\tAndi Kleen\t:\tSome cleanups, cache destination entry\n *\t\t\t\t\tfor connect.\n *\tVitaly E. Lavrov\t:\tTransparent proxy revived after year coma.\n *\t\tMelvin Smith\t:\tCheck msg_name not msg_namelen in sendto(),\n *\t\t\t\t\treturn ENOTCONN for unconnected sockets (POSIX)\n *\t\tJanos Farkas\t:\tdon't deliver multi/broadcasts to a different\n *\t\t\t\t\tbound-to-device socket\n *\tHirokazu Takahashi\t:\tHW checksumming for outgoing UDP\n *\t\t\t\t\tdatagrams.\n *\tHirokazu Takahashi\t:\tsendfile() on UDP works now.\n *\t\tArnaldo C. Melo :\tconvert /proc/net/udp to seq_file\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov:\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tDerek Atkins <derek@ihtfp.com>: Add Encapulation Support\n *\tJames Chapman\t\t:\tAdd L2TP encapsulation type.\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/module.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/igmp.h>\n#include <linux/in.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/mm.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <net/tcp_states.h>\n#include <linux/skbuff.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/net_namespace.h>\n#include <net/icmp.h>\n#include <net/route.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include \"udp_impl.h\"\n\nstruct udp_table udp_table __read_mostly;\nEXPORT_SYMBOL(udp_table);\n\nlong sysctl_udp_mem[3] __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_mem);\n\nint sysctl_udp_rmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_rmem_min);\n\nint sysctl_udp_wmem_min __read_mostly;\nEXPORT_SYMBOL(sysctl_udp_wmem_min);\n\natomic_long_t udp_memory_allocated;\nEXPORT_SYMBOL(udp_memory_allocated);\n\n#define MAX_UDP_PORTS 65536\n#define PORTS_PER_CHAIN (MAX_UDP_PORTS / UDP_HTABLE_SIZE_MIN)\n\nstatic int udp_lib_lport_inuse(struct net *net, __u16 num,\n\t\t\t       const struct udp_hslot *hslot,\n\t\t\t       unsigned long *bitmap,\n\t\t\t       struct sock *sk,\n\t\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t const struct sock *sk2),\n\t\t\t       unsigned int log)\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\n\tsk_nulls_for_each(sk2, node, &hslot->head)\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (bitmap || udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (*saddr_comp)(sk, sk2)) {\n\t\t\tif (bitmap)\n\t\t\t\t__set_bit(udp_sk(sk2)->udp_port_hash >> log,\n\t\t\t\t\t  bitmap);\n\t\t\telse\n\t\t\t\treturn 1;\n\t\t}\n\treturn 0;\n}\n\n/*\n * Note: we still hold spinlock of primary hash chain, so no other writer\n * can insert/delete a socket with local_port == num\n */\nstatic int udp_lib_lport_inuse2(struct net *net, __u16 num,\n\t\t\t       struct udp_hslot *hslot2,\n\t\t\t       struct sock *sk,\n\t\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t\t const struct sock *sk2))\n{\n\tstruct sock *sk2;\n\tstruct hlist_nulls_node *node;\n\tint res = 0;\n\n\tspin_lock(&hslot2->lock);\n\tudp_portaddr_for_each_entry(sk2, node, &hslot2->head)\n\t\tif (net_eq(sock_net(sk2), net) &&\n\t\t    sk2 != sk &&\n\t\t    (udp_sk(sk2)->udp_port_hash == num) &&\n\t\t    (!sk2->sk_reuse || !sk->sk_reuse) &&\n\t\t    (!sk2->sk_bound_dev_if || !sk->sk_bound_dev_if ||\n\t\t     sk2->sk_bound_dev_if == sk->sk_bound_dev_if) &&\n\t\t    (*saddr_comp)(sk, sk2)) {\n\t\t\tres = 1;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&hslot2->lock);\n\treturn res;\n}\n\n/**\n *  udp_lib_get_port  -  UDP/-Lite port lookup for IPv4 and IPv6\n *\n *  @sk:          socket struct in question\n *  @snum:        port number to look up\n *  @saddr_comp:  AF-dependent comparison of bound local IP addresses\n *  @hash2_nulladdr: AF-dependent hash value in secondary hash chains,\n *                   with NULL address\n */\nint udp_lib_get_port(struct sock *sk, unsigned short snum,\n\t\t       int (*saddr_comp)(const struct sock *sk1,\n\t\t\t\t\t const struct sock *sk2),\n\t\t     unsigned int hash2_nulladdr)\n{\n\tstruct udp_hslot *hslot, *hslot2;\n\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\tint    error = 1;\n\tstruct net *net = sock_net(sk);\n\n\tif (!snum) {\n\t\tint low, high, remaining;\n\t\tunsigned rand;\n\t\tunsigned short first, last;\n\t\tDECLARE_BITMAP(bitmap, PORTS_PER_CHAIN);\n\n\t\tinet_get_local_port_range(&low, &high);\n\t\tremaining = (high - low) + 1;\n\n\t\trand = net_random();\n\t\tfirst = (((u64)rand * remaining) >> 32) + low;\n\t\t/*\n\t\t * force rand to be an odd multiple of UDP_HTABLE_SIZE\n\t\t */\n\t\trand = (rand | 1) * (udptable->mask + 1);\n\t\tlast = first + udptable->mask + 1;\n\t\tdo {\n\t\t\thslot = udp_hashslot(udptable, net, first);\n\t\t\tbitmap_zero(bitmap, PORTS_PER_CHAIN);\n\t\t\tspin_lock_bh(&hslot->lock);\n\t\t\tudp_lib_lport_inuse(net, snum, hslot, bitmap, sk,\n\t\t\t\t\t    saddr_comp, udptable->log);\n\n\t\t\tsnum = first;\n\t\t\t/*\n\t\t\t * Iterate on all possible values of snum for this hash.\n\t\t\t * Using steps of an odd multiple of UDP_HTABLE_SIZE\n\t\t\t * give us randomization and full range coverage.\n\t\t\t */\n\t\t\tdo {\n\t\t\t\tif (low <= snum && snum <= high &&\n\t\t\t\t    !test_bit(snum >> udptable->log, bitmap) &&\n\t\t\t\t    !inet_is_reserved_local_port(snum))\n\t\t\t\t\tgoto found;\n\t\t\t\tsnum += rand;\n\t\t\t} while (snum != first);\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t} while (++first != last);\n\t\tgoto fail;\n\t} else {\n\t\thslot = udp_hashslot(udptable, net, snum);\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (hslot->count > 10) {\n\t\t\tint exist;\n\t\t\tunsigned int slot2 = udp_sk(sk)->udp_portaddr_hash ^ snum;\n\n\t\t\tslot2          &= udptable->mask;\n\t\t\thash2_nulladdr &= udptable->mask;\n\n\t\t\thslot2 = udp_hashslot2(udptable, slot2);\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto scan_primary_hash;\n\n\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\tif (!exist && (hash2_nulladdr != slot2)) {\n\t\t\t\thslot2 = udp_hashslot2(udptable, hash2_nulladdr);\n\t\t\t\texist = udp_lib_lport_inuse2(net, snum, hslot2,\n\t\t\t\t\t\t\t     sk, saddr_comp);\n\t\t\t}\n\t\t\tif (exist)\n\t\t\t\tgoto fail_unlock;\n\t\t\telse\n\t\t\t\tgoto found;\n\t\t}\nscan_primary_hash:\n\t\tif (udp_lib_lport_inuse(net, snum, hslot, NULL, sk,\n\t\t\t\t\tsaddr_comp, 0))\n\t\t\tgoto fail_unlock;\n\t}\nfound:\n\tinet_sk(sk)->inet_num = snum;\n\tudp_sk(sk)->udp_port_hash = snum;\n\tudp_sk(sk)->udp_portaddr_hash ^= snum;\n\tif (sk_unhashed(sk)) {\n\t\tsk_nulls_add_node_rcu(sk, &hslot->head);\n\t\thslot->count++;\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tspin_lock(&hslot2->lock);\n\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t &hslot2->head);\n\t\thslot2->count++;\n\t\tspin_unlock(&hslot2->lock);\n\t}\n\terror = 0;\nfail_unlock:\n\tspin_unlock_bh(&hslot->lock);\nfail:\n\treturn error;\n}\nEXPORT_SYMBOL(udp_lib_get_port);\n\nstatic int ipv4_rcv_saddr_equal(const struct sock *sk1, const struct sock *sk2)\n{\n\tstruct inet_sock *inet1 = inet_sk(sk1), *inet2 = inet_sk(sk2);\n\n\treturn \t(!ipv6_only_sock(sk2)  &&\n\t\t (!inet1->inet_rcv_saddr || !inet2->inet_rcv_saddr ||\n\t\t   inet1->inet_rcv_saddr == inet2->inet_rcv_saddr));\n}\n\nstatic unsigned int udp4_portaddr_hash(struct net *net, __be32 saddr,\n\t\t\t\t       unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nint udp_v4_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp4_portaddr_hash(sock_net(sk), htonl(INADDR_ANY), snum);\n\tunsigned int hash2_partial =\n\t\tudp4_portaddr_hash(sock_net(sk), inet_sk(sk)->inet_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv4_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net, __be32 saddr,\n\t\t\t unsigned short hnum,\n\t\t\t __be16 sport, __be32 daddr, __be16 dport, int dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && udp_sk(sk)->udp_port_hash == hnum &&\n\t\t\t!ipv6_only_sock(sk)) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tscore = (sk->sk_family == PF_INET ? 1 : 0);\n\t\tif (inet->inet_rcv_saddr) {\n\t\t\tif (inet->inet_rcv_saddr != daddr)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (inet->inet_daddr) {\n\t\t\tif (inet->inet_daddr != saddr)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t}\n\treturn score;\n}\n\n/*\n * In this second variant, we check (daddr, dport) matches (inet_rcv_sadd, inet_num)\n */\n#define SCORE2_MAX (1 + 2 + 2 + 2)\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t __be32 saddr, __be16 sport,\n\t\t\t\t __be32 daddr, unsigned int hnum, int dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && !ipv6_only_sock(sk)) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (inet->inet_rcv_saddr != daddr)\n\t\t\treturn -1;\n\t\tif (inet->inet_num != hnum)\n\t\t\treturn -1;\n\n\t\tscore = (sk->sk_family == PF_INET ? 1 : 0);\n\t\tif (inet->inet_daddr) {\n\t\t\tif (inet->inet_daddr != saddr)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore += 2;\n\t\t}\n\t}\n\treturn score;\n}\n\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp4_lib_lookup2(struct net *net,\n\t\t__be32 saddr, __be16 sport,\n\t\t__be32 daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\tif (score == SCORE2_MAX)\n\t\t\t\tgoto exact_match;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\nexact_match:\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\n/* UDP is nearly always wildcards out the wazoo, it makes no sense to try\n * harder than this. -DaveM\n */\nstatic struct sock *__udp4_lib_lookup(struct net *net, __be32 saddr,\n\t\t__be16 sport, __be32 daddr, __be16 dport,\n\t\tint dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp4_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp4_portaddr_hash(net, htonl(INADDR_ANY), hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp4_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  htonl(INADDR_ANY), hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, saddr, hnum, sport,\n\t\t\t\t      daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, saddr, hnum, sport,\n\t\t\t\t  daddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\nstatic inline struct sock *__udp4_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t\t __be16 sport, __be16 dport,\n\t\t\t\t\t\t struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\tif (unlikely(sk = skb_steal_sock(skb)))\n\t\treturn sk;\n\telse\n\t\treturn __udp4_lib_lookup(dev_net(skb_dst(skb)->dev), iph->saddr, sport,\n\t\t\t\t\t iph->daddr, dport, inet_iif(skb),\n\t\t\t\t\t udptable);\n}\n\nstruct sock *udp4_lib_lookup(struct net *net, __be32 saddr, __be16 sport,\n\t\t\t     __be32 daddr, __be16 dport, int dif)\n{\n\treturn __udp4_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp4_lib_lookup);\n\nstatic inline struct sock *udp_v4_mcast_next(struct net *net, struct sock *sk,\n\t\t\t\t\t     __be16 loc_port, __be32 loc_addr,\n\t\t\t\t\t     __be16 rmt_port, __be32 rmt_addr,\n\t\t\t\t\t     int dif)\n{\n\tstruct hlist_nulls_node *node;\n\tstruct sock *s = sk;\n\tunsigned short hnum = ntohs(loc_port);\n\n\tsk_nulls_for_each_from(s, node) {\n\t\tstruct inet_sock *inet = inet_sk(s);\n\n\t\tif (!net_eq(sock_net(s), net) ||\n\t\t    udp_sk(s)->udp_port_hash != hnum ||\n\t\t    (inet->inet_daddr && inet->inet_daddr != rmt_addr) ||\n\t\t    (inet->inet_dport != rmt_port && inet->inet_dport) ||\n\t\t    (inet->inet_rcv_saddr &&\n\t\t     inet->inet_rcv_saddr != loc_addr) ||\n\t\t    ipv6_only_sock(s) ||\n\t\t    (s->sk_bound_dev_if && s->sk_bound_dev_if != dif))\n\t\t\tcontinue;\n\t\tif (!ip_mc_sf_allow(s, loc_addr, rmt_addr, dif))\n\t\t\tcontinue;\n\t\tgoto found;\n\t}\n\ts = NULL;\nfound:\n\treturn s;\n}\n\n/*\n * This routine is called by the ICMP module when it gets some\n * sort of error condition.  If err < 0 then the socket should\n * be closed and the error returned to the user.  If err > 0\n * it's just the icmp type << 8 | icmp code.\n * Header points to the ip header of the error packet. We move\n * on past this. Then (as it used to claim before adjustment)\n * header points to the first 8 bytes of the udp header.  We need\n * to find the appropriate port.\n */\n\nvoid __udp4_lib_err(struct sk_buff *skb, u32 info, struct udp_table *udptable)\n{\n\tstruct inet_sock *inet;\n\tconst struct iphdr *iph = (const struct iphdr *)skb->data;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+(iph->ihl<<2));\n\tconst int type = icmp_hdr(skb)->type;\n\tconst int code = icmp_hdr(skb)->code;\n\tstruct sock *sk;\n\tint harderr;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp4_lib_lookup(net, iph->daddr, uh->dest,\n\t\t\tiph->saddr, uh->source, skb->dev->ifindex, udptable);\n\tif (sk == NULL) {\n\t\tICMP_INC_STATS_BH(net, ICMP_MIB_INERRORS);\n\t\treturn;\t/* No socket for error */\n\t}\n\n\terr = 0;\n\tharderr = 0;\n\tinet = inet_sk(sk);\n\n\tswitch (type) {\n\tdefault:\n\tcase ICMP_TIME_EXCEEDED:\n\t\terr = EHOSTUNREACH;\n\t\tbreak;\n\tcase ICMP_SOURCE_QUENCH:\n\t\tgoto out;\n\tcase ICMP_PARAMETERPROB:\n\t\terr = EPROTO;\n\t\tharderr = 1;\n\t\tbreak;\n\tcase ICMP_DEST_UNREACH:\n\t\tif (code == ICMP_FRAG_NEEDED) { /* Path MTU discovery */\n\t\t\tif (inet->pmtudisc != IP_PMTUDISC_DONT) {\n\t\t\t\terr = EMSGSIZE;\n\t\t\t\tharderr = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\terr = EHOSTUNREACH;\n\t\tif (code <= NR_ICMP_UNREACH) {\n\t\t\tharderr = icmp_err_convert[code].fatal;\n\t\t\terr = icmp_err_convert[code].errno;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/*\n\t *      RFC1122: OK.  Passes ICMP errors back to application, as per\n\t *\t4.1.3.3.\n\t */\n\tif (!inet->recverr) {\n\t\tif (!harderr || sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t} else\n\t\tip_icmp_error(sk, skb, err, uh->dest, info, (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nvoid udp_err(struct sk_buff *skb, u32 info)\n{\n\t__udp4_lib_err(skb, info, &udp_table);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nvoid udp_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip_flush_pending_frames(sk);\n\t}\n}\nEXPORT_SYMBOL(udp_flush_pending_frames);\n\n/**\n * \tudp4_hwcsum  -  handle outgoing HW checksumming\n * \t@skb: \tsk_buff containing the filled-in UDP header\n * \t        (checksum field must be zeroed out)\n *\t@src:\tsource IP address\n *\t@dst:\tdestination IP address\n */\nstatic void udp4_hwcsum(struct sk_buff *skb, __be32 src, __be32 dst)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *frags = skb_shinfo(skb)->frag_list;\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint hlen = len;\n\t__wsum csum = 0;\n\n\tif (!frags) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_tcpudp_magic(src, dst, len,\n\t\t\t\t\t       IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\tdo {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t\thlen -= frags->len;\n\t\t} while ((frags = frags->next));\n\n\t\tcsum = skb_checksum(skb, offset, hlen, csum);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tuh->check = csum_tcpudp_magic(src, dst, len, IPPROTO_UDP, csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\nstatic int udp_send_skb(struct sk_buff *skb, __be32 daddr, __be32 dport)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udphdr *uh;\n\tstruct rtable *rt = (struct rtable *)skb_dst(skb);\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\t__wsum csum = 0;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = inet->inet_sport;\n\tuh->dest = dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)  \t\t\t\t /*     UDP-Lite      */\n\t\tcsum = udplite_csum(skb);\n\n\telse if (sk->sk_no_check == UDP_CSUM_NOXMIT) {   /* UDP csum disabled */\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\n\t\tudp4_hwcsum(skb, rt->rt_src, daddr);\n\t\tgoto send;\n\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_tcpudp_magic(rt->rt_src, daddr, len,\n\t\t\t\t      sk->sk_protocol, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip_send_skb(skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet->recverr) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t   UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t   UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\n/*\n * Push out all pending data as one UDP datagram. Socket is locked.\n */\nstatic int udp_push_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 *fl4 = &inet->cork.fl.u.ip4;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\n\tskb = ip_finish_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_send_skb(skb, fl4->daddr, fl4->fl4_dport);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct flowi4 *fl4;\n\tint ulen = len;\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = NULL;\n\tint free = 0;\n\tint connected = 0;\n\t__be32 daddr, faddr, saddr;\n\t__be16 dport;\n\tu8  tos;\n\tint err, is_udplite = IS_UDPLITE(sk);\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\tstruct sk_buff *skb;\n\tstruct ip_options_data opt_copy;\n\n\tif (len > 0xFFFF)\n\t\treturn -EMSGSIZE;\n\n\t/*\n\t *\tCheck the flags.\n\t */\n\n\tif (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */\n\t\treturn -EOPNOTSUPP;\n\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\n\tgetfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;\n\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in * usin = (struct sockaddr_in *)msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*usin))\n\t\t\treturn -EINVAL;\n\t\tif (usin->sin_family != AF_INET) {\n\t\t\tif (usin->sin_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t\tif (dport == 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = inet->inet_daddr;\n\t\tdport = inet->inet_dport;\n\t\t/* Open fast path for connected socket.\n\t\t   Route will not be used, if at least one option is set.\n\t\t */\n\t\tconnected = 1;\n\t}\n\tipc.addr = inet->inet_saddr;\n\n\tipc.oif = sk->sk_bound_dev_if;\n\terr = sock_tx_timestamp(sk, &ipc.tx_flags);\n\tif (err)\n\t\treturn err;\n\tif (msg->msg_controllen) {\n\t\terr = ip_cmsg_send(sock_net(sk), msg, &ipc);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (ipc.opt)\n\t\t\tfree = 1;\n\t\tconnected = 0;\n\t}\n\tif (!ipc.opt) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\trcu_read_lock();\n\t\tinet_opt = rcu_dereference(inet->inet_opt);\n\t\tif (inet_opt) {\n\t\t\tmemcpy(&opt_copy, inet_opt,\n\t\t\t       sizeof(*inet_opt) + inet_opt->opt.optlen);\n\t\t\tipc.opt = &opt_copy.opt;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tsaddr = ipc.addr;\n\tipc.addr = faddr = daddr;\n\n\tif (ipc.opt && ipc.opt->opt.srr) {\n\t\tif (!daddr)\n\t\t\treturn -EINVAL;\n\t\tfaddr = ipc.opt->opt.faddr;\n\t\tconnected = 0;\n\t}\n\ttos = RT_TOS(inet->tos);\n\tif (sock_flag(sk, SOCK_LOCALROUTE) ||\n\t    (msg->msg_flags & MSG_DONTROUTE) ||\n\t    (ipc.opt && ipc.opt->opt.is_strictroute)) {\n\t\ttos |= RTO_ONLINK;\n\t\tconnected = 0;\n\t}\n\n\tif (ipv4_is_multicast(daddr)) {\n\t\tif (!ipc.oif)\n\t\t\tipc.oif = inet->mc_index;\n\t\tif (!saddr)\n\t\t\tsaddr = inet->mc_addr;\n\t\tconnected = 0;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *)sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\tstruct flowi4 fl4;\n\t\tstruct net *net = sock_net(sk);\n\n\t\tflowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,\n\t\t\t\t   RT_SCOPE_UNIVERSE, sk->sk_protocol,\n\t\t\t\t   inet_sk_flowi_flags(sk)|FLOWI_FLAG_CAN_SLEEP,\n\t\t\t\t   faddr, saddr, dport, inet->inet_sport);\n\n\t\tsecurity_sk_classify_flow(sk, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_flow(net, &fl4, sk);\n\t\tif (IS_ERR(rt)) {\n\t\t\terr = PTR_ERR(rt);\n\t\t\trt = NULL;\n\t\t\tif (err == -ENETUNREACH)\n\t\t\t\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = -EACCES;\n\t\tif ((rt->rt_flags & RTCF_BROADCAST) &&\n\t\t    !sock_flag(sk, SOCK_BROADCAST))\n\t\t\tgoto out;\n\t\tif (connected)\n\t\t\tsk_dst_set(sk, dst_clone(&rt->dst));\n\t}\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tsaddr = rt->rt_src;\n\tif (!ipc.addr)\n\t\tdaddr = ipc.addr = rt->rt_dst;\n\n\t/* Lockless fast path for the non-corking case. */\n\tif (!corkreq) {\n\t\tskb = ip_make_skb(sk, getfrag, msg->msg_iov, ulen,\n\t\t\t\t  sizeof(struct udphdr), &ipc, &rt,\n\t\t\t\t  msg->msg_flags);\n\t\terr = PTR_ERR(skb);\n\t\tif (skb && !IS_ERR(skb))\n\t\t\terr = udp_send_skb(skb, daddr, dport);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\t/*\n\t *\tNow cork the socket to pend data.\n\t */\n\tfl4 = &inet->cork.fl.u.ip4;\n\tfl4->daddr = daddr;\n\tfl4->saddr = saddr;\n\tfl4->fl4_dport = dport;\n\tfl4->fl4_sport = inet->inet_sport;\n\tup->pending = AF_INET;\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip_append_data(sk, getfrag, msg->msg_iov, ulen,\n\t\t\tsizeof(struct udphdr), &ipc, &rt,\n\t\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\trelease_sock(sk);\n\nout:\n\tip_rt_put(rt);\n\tif (free)\n\t\tkfree(ipc.opt);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(&rt->dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\nEXPORT_SYMBOL(udp_sendmsg);\n\nint udp_sendpage(struct sock *sk, struct page *page, int offset,\n\t\t size_t size, int flags)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint ret;\n\n\tif (!up->pending) {\n\t\tstruct msghdr msg = {\t.msg_flags = flags|MSG_MORE };\n\n\t\t/* Call udp_sendmsg to specify destination address which\n\t\t * sendpage interface can't pass.\n\t\t * This will succeed only when the socket is connected.\n\t\t */\n\t\tret = udp_sendmsg(NULL, sk, &msg, 0);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tlock_sock(sk);\n\n\tif (unlikely(!up->pending)) {\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 3\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = ip_append_page(sk, page, offset, size, flags);\n\tif (ret == -EOPNOTSUPP) {\n\t\trelease_sock(sk);\n\t\treturn sock_no_sendpage(sk->sk_socket, page, offset,\n\t\t\t\t\tsize, flags);\n\t}\n\tif (ret < 0) {\n\t\tudp_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\tup->len += size;\n\tif (!(up->corkflag || (flags&MSG_MORE)))\n\t\tret = udp_push_pending_frames(sk);\n\tif (!ret)\n\t\tret = size;\nout:\n\trelease_sock(sk);\n\treturn ret;\n}\n\n\n/**\n *\tfirst_packet_length\t- return length of first packet in receive queue\n *\t@sk: socket\n *\n *\tDrops all bad checksum frames, until a valid one is found.\n *\tReturns the length of found skb, or 0 if none is found.\n */\nstatic unsigned int first_packet_length(struct sock *sk)\n{\n\tstruct sk_buff_head list_kill, *rcvq = &sk->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tunsigned int res;\n\n\t__skb_queue_head_init(&list_kill);\n\n\tspin_lock_bh(&rcvq->lock);\n\twhile ((skb = skb_peek(rcvq)) != NULL &&\n\t\tudp_lib_checksum_complete(skb)) {\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t IS_UDPLITE(sk));\n\t\tatomic_inc(&sk->sk_drops);\n\t\t__skb_unlink(skb, rcvq);\n\t\t__skb_queue_tail(&list_kill, skb);\n\t}\n\tres = skb ? skb->len : 0;\n\tspin_unlock_bh(&rcvq->lock);\n\n\tif (!skb_queue_empty(&list_kill)) {\n\t\tbool slow = lock_sock_fast(sk);\n\n\t\t__skb_queue_purge(&list_kill);\n\t\tsk_mem_reclaim_partial(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t}\n\treturn res;\n}\n\n/*\n *\tIOCTL requests applicable to the UDP protocol\n */\n\nint udp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tcase SIOCINQ:\n\t{\n\t\tunsigned int amount = first_packet_length(sk);\n\n\t\tif (amount)\n\t\t\t/*\n\t\t\t * We will only return the amount\n\t\t\t * of this packet since that is all\n\t\t\t * that will be read.\n\t\t\t */\n\t\t\tamount -= sizeof(struct udphdr);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_ioctl);\n\n/*\n * \tThis should be easy, if there is something there we\n * \treturn it, otherwise we block.\n */\n\nint udp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\tsize_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tunsigned int ulen;\n\tint peeked;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tbool slow;\n\n\t/*\n\t *\tCheck any passed addresses\n\t */\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ip_recv_error(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tif (len > ulen)\n\t\tlen = ulen;\n\telse if (len < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (len < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov, len);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb,\n\t\t\t\t\t\t       sizeof(struct udphdr),\n\t\t\t\t\t\t       msg->msg_iov);\n\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (err)\n\t\tgoto out_free;\n\n\tif (!peeked)\n\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = udp_hdr(skb)->source;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\n\terr = len;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags))\n\t\tUDP_INC_STATS_USER(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\tgoto try_again;\n}\n\n\nint udp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t/*\n\t *\t1003.1g - break association.\n\t */\n\n\tsk->sk_state = TCP_CLOSE;\n\tinet->inet_daddr = 0;\n\tinet->inet_dport = 0;\n\tsock_rps_save_rxhash(sk, 0);\n\tsk->sk_bound_dev_if = 0;\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tif (!(sk->sk_userlocks & SOCK_BINDPORT_LOCK)) {\n\t\tsk->sk_prot->unhash(sk);\n\t\tinet->inet_sport = 0;\n\t}\n\tsk_dst_reset(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_disconnect);\n\nvoid udp_lib_unhash(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2;\n\n\t\thslot  = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t      udp_sk(sk)->udp_port_hash);\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tif (sk_nulls_del_node_init_rcu(sk)) {\n\t\t\thslot->count--;\n\t\t\tinet_sk(sk)->inet_num = 0;\n\t\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n}\nEXPORT_SYMBOL(udp_lib_unhash);\n\n/*\n * inet_rcv_saddr was changed, we must rehash secondary hash\n */\nvoid udp_lib_rehash(struct sock *sk, u16 newhash)\n{\n\tif (sk_hashed(sk)) {\n\t\tstruct udp_table *udptable = sk->sk_prot->h.udp_table;\n\t\tstruct udp_hslot *hslot, *hslot2, *nhslot2;\n\n\t\thslot2 = udp_hashslot2(udptable, udp_sk(sk)->udp_portaddr_hash);\n\t\tnhslot2 = udp_hashslot2(udptable, newhash);\n\t\tudp_sk(sk)->udp_portaddr_hash = newhash;\n\t\tif (hslot2 != nhslot2) {\n\t\t\thslot = udp_hashslot(udptable, sock_net(sk),\n\t\t\t\t\t     udp_sk(sk)->udp_port_hash);\n\t\t\t/* we must lock primary chain too */\n\t\t\tspin_lock_bh(&hslot->lock);\n\n\t\t\tspin_lock(&hslot2->lock);\n\t\t\thlist_nulls_del_init_rcu(&udp_sk(sk)->udp_portaddr_node);\n\t\t\thslot2->count--;\n\t\t\tspin_unlock(&hslot2->lock);\n\n\t\t\tspin_lock(&nhslot2->lock);\n\t\t\thlist_nulls_add_head_rcu(&udp_sk(sk)->udp_portaddr_node,\n\t\t\t\t\t\t &nhslot2->head);\n\t\t\tnhslot2->count++;\n\t\t\tspin_unlock(&nhslot2->lock);\n\n\t\t\tspin_unlock_bh(&hslot->lock);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(udp_lib_rehash);\n\nstatic void udp_v4_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp4_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  inet_sk(sk)->inet_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic int __udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (inet_sk(sk)->inet_daddr)\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\trc = ip_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t is_udplite);\n\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n\n}\n\n/* returns:\n *  -1: error\n *   0: success\n *  >0: \"udp encap\" protocol resubmission\n *\n * Note that in the success and error cases, the skb is assumed to\n * have either been requeued or freed.\n */\nint udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (up->encap_type) {\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tif (skb->len > sizeof(struct udphdr) &&\n\t\t    up->encap_rcv != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = (*up->encap_rcv)(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE: partial coverage \"\n\t\t\t\t\"%d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING\n\t\t\t\t\"UDPLITE: coverage %d too small, need min %d\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_dereference_raw(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\n\tif (sk_rcvqueues_full(sk, skb))\n\t\tgoto drop;\n\n\trc = 0;\n\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tunsigned int i;\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(skb1 == NULL))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udp_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\n/*\n *\tMulticasts and broadcasts go to each listener.\n *\n *\tNote: called only from the BH handler context.\n */\nstatic int __udp4_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\t\t\t    struct udphdr  *uh,\n\t\t\t\t    __be32 saddr, __be32 daddr,\n\t\t\t\t    struct udp_table *udptable)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, ntohs(uh->dest));\n\tint dif;\n\tunsigned int i, count = 0;\n\n\tspin_lock(&hslot->lock);\n\tsk = sk_nulls_head(&hslot->head);\n\tdif = skb->dev->ifindex;\n\tsk = udp_v4_mcast_next(net, sk, uh->dest, daddr, uh->source, saddr, dif);\n\twhile (sk) {\n\t\tstack[count++] = sk;\n\t\tsk = udp_v4_mcast_next(net, sk_nulls_next(sk), uh->dest,\n\t\t\t\t       daddr, uh->source, saddr, dif);\n\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\tif (!sk)\n\t\t\t\tbreak;\n\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\tcount = 0;\n\t\t}\n\t}\n\t/*\n\t * before releasing chain lock, we must take a reference on sockets\n\t */\n\tfor (i = 0; i < count; i++)\n\t\tsock_hold(stack[i]);\n\n\tspin_unlock(&hslot->lock);\n\n\t/*\n\t * do the slow work with no lock held\n\t */\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tsock_put(stack[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\treturn 0;\n}\n\n/* Initialize UDP checksum. If exited with zero value (success),\n * CHECKSUM_UNNECESSARY means, that no more checks are required.\n * Otherwise, csum completion requires chacksumming packet body,\n * including udp header and folding it to skb->csum.\n */\nstatic inline int udp4_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tconst struct iphdr *iph;\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tiph = ip_hdr(skb);\n\tif (uh->check == 0) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t} else if (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_tcpudp_magic(iph->saddr, iph->daddr, skb->len,\n\t\t\t\t      proto, skb->csum))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t\t       skb->len, proto, 0);\n\t/* Probably, we should checksum udp header (it should be in cache\n\t * in any case) and data in tiny packets (< rx copybreak).\n\t */\n\n\treturn 0;\n}\n\n/*\n *\tAll we need to do is get the socket, and then do a checksum.\n */\n\nint __udp4_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tunsigned short ulen;\n\tstruct rtable *rt = skb_rtable(skb);\n\t__be32 saddr, daddr;\n\tstruct net *net = dev_net(skb->dev);\n\n\t/*\n\t *  Validate the packet.\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto drop;\t\t/* No space for header. */\n\n\tuh   = udp_hdr(skb);\n\tulen = ntohs(uh->len);\n\tsaddr = ip_hdr(skb)->saddr;\n\tdaddr = ip_hdr(skb)->daddr;\n\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\t\tif (ulen < sizeof(*uh) || pskb_trim_rcsum(skb, ulen))\n\t\t\tgoto short_packet;\n\t\tuh = udp_hdr(skb);\n\t}\n\n\tif (udp4_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\tif (rt->rt_flags & (RTCF_BROADCAST|RTCF_MULTICAST))\n\t\treturn __udp4_lib_mcast_deliver(net, skb, uh,\n\t\t\t\tsaddr, daddr, udptable);\n\n\tsk = __udp4_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\n\tif (sk != NULL) {\n\t\tint ret = udp_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\t\treturn 0;\n\t}\n\n\tif (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\t/* No socket. Drop packet silently, if checksum is wrong */\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);\n\n\t/*\n\t * Hmm.  We got an UDP packet to a port to which we\n\t * don't wanna listen.  Ignore it.\n\t */\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tLIMIT_NETDEBUG(KERN_DEBUG \"UDP%s: short packet: From %pI4:%u %d/%d to %pI4:%u\\n\",\n\t\t       proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t       &saddr,\n\t\t       ntohs(uh->source),\n\t\t       ulen,\n\t\t       skb->len,\n\t\t       &daddr,\n\t\t       ntohs(uh->dest));\n\tgoto drop;\n\ncsum_error:\n\t/*\n\t * RFC1122: OK.  Discards the bad packet silently (as far as\n\t * the network is concerned, anyway) as per 4.1.3.4 (MUST).\n\t */\n\tLIMIT_NETDEBUG(KERN_DEBUG \"UDP%s: bad checksum. From %pI4:%u to %pI4:%u ulen %d\\n\",\n\t\t       proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t       &saddr,\n\t\t       ntohs(uh->source),\n\t\t       &daddr,\n\t\t       ntohs(uh->dest),\n\t\t       ulen);\ndrop:\n\tUDP_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nint udp_rcv(struct sk_buff *skb)\n{\n\treturn __udp4_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\nvoid udp_destroy_sock(struct sock *sk)\n{\n\tbool slow = lock_sock_fast(sk);\n\tudp_flush_pending_frames(sk);\n\tunlock_sock_fast(sk, slow);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udp_lib_setsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, unsigned int optlen,\n\t\t       int (*push_pending_frames)(struct sock *))\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tif (val != 0) {\n\t\t\tup->corkflag = 1;\n\t\t} else {\n\t\t\tup->corkflag = 0;\n\t\t\tlock_sock(sk);\n\t\t\t(*push_pending_frames)(sk);\n\t\t\trelease_sock(sk);\n\t\t}\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tswitch (val) {\n\t\tcase 0:\n\t\tcase UDP_ENCAP_ESPINUDP:\n\t\tcase UDP_ENCAP_ESPINUDP_NON_IKE:\n\t\t\tup->encap_rcv = xfrm4_udp_encap_rcv;\n\t\t\t/* FALLTHROUGH */\n\t\tcase UDP_ENCAP_L2TPINUDP:\n\t\t\tup->encap_type = val;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOPROTOOPT;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t/*\n\t * \tUDP-Lite's partial checksum coverage (RFC 3828).\n\t */\n\t/* The sender sets actual checksum coverage length via this option.\n\t * The case coverage > packet length is handled by send module. */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Illegal coverage: use default (8) */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcslen = val;\n\t\tup->pcflag |= UDPLITE_SEND_CC;\n\t\tbreak;\n\n\t/* The receiver specifies a minimum checksum coverage value. To make\n\t * sense, this should be set to at least 8 (as done below). If zero is\n\t * used, this again means full checksum coverage.                     */\n\tcase UDPLITE_RECV_CSCOV:\n\t\tif (!is_udplite)         /* Disable the option on UDP sockets */\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (val != 0 && val < 8) /* Avoid silly minimal values.       */\n\t\t\tval = 8;\n\t\telse if (val > USHRT_MAX)\n\t\t\tval = USHRT_MAX;\n\t\tup->pcrlen = val;\n\t\tup->pcflag |= UDPLITE_RECV_CC;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(udp_lib_setsockopt);\n\nint udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_push_pending_frames);\n\treturn compat_ip_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udp_lib_getsockopt(struct sock *sk, int level, int optname,\n\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase UDP_CORK:\n\t\tval = up->corkflag;\n\t\tbreak;\n\n\tcase UDP_ENCAP:\n\t\tval = up->encap_type;\n\t\tbreak;\n\n\t/* The following two cannot be changed on UDP sockets, the return is\n\t * always 0 (which corresponds to the full checksum coverage of UDP). */\n\tcase UDPLITE_SEND_CSCOV:\n\t\tval = up->pcslen;\n\t\tbreak;\n\n\tcase UDPLITE_RECV_CSCOV:\n\t\tval = up->pcrlen;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\nEXPORT_SYMBOL(udp_lib_getsockopt);\n\nint udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ip_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n/**\n * \tudp_poll - wait for a UDP event.\n *\t@file - file struct\n *\t@sock - socket\n *\t@wait - poll table\n *\n *\tThis is same as datagram poll, except for the special case of\n *\tblocking sockets. If application is using a blocking fd\n *\tand a packet with checksum error is in the queue;\n *\tthen it could get return from select indicating data available\n *\tbut then block when reading it. Add special case code\n *\tto work around these arguably broken applications.\n */\nunsigned int udp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\tstruct sock *sk = sock->sk;\n\n\t/* Check for false positives due to checksum errors */\n\tif ((mask & POLLRDNORM) && !(file->f_flags & O_NONBLOCK) &&\n\t    !(sk->sk_shutdown & RCV_SHUTDOWN) && !first_packet_length(sk))\n\t\tmask &= ~(POLLIN | POLLRDNORM);\n\n\treturn mask;\n\n}\nEXPORT_SYMBOL(udp_poll);\n\nstruct proto udp_prot = {\n\t.name\t\t   = \"UDP\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip4_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udp_destroy_sock,\n\t.setsockopt\t   = udp_setsockopt,\n\t.getsockopt\t   = udp_getsockopt,\n\t.sendmsg\t   = udp_sendmsg,\n\t.recvmsg\t   = udp_recvmsg,\n\t.sendpage\t   = udp_sendpage,\n\t.backlog_rcv\t   = __udp_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v4_rehash,\n\t.get_port\t   = udp_v4_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udp_setsockopt,\n\t.compat_getsockopt = compat_udp_getsockopt,\n#endif\n\t.clear_sk\t   = sk_prot_clear_portaddr_nulls,\n};\nEXPORT_SYMBOL(udp_prot);\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\n\nstatic struct sock *udp_get_first(struct seq_file *seq, int start)\n{\n\tstruct sock *sk;\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tfor (state->bucket = start; state->bucket <= state->udp_table->mask;\n\t     ++state->bucket) {\n\t\tstruct hlist_nulls_node *node;\n\t\tstruct udp_hslot *hslot = &state->udp_table->hash[state->bucket];\n\n\t\tif (hlist_nulls_empty(&hslot->head))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&hslot->lock);\n\t\tsk_nulls_for_each(sk, node, &hslot->head) {\n\t\t\tif (!net_eq(sock_net(sk), net))\n\t\t\t\tcontinue;\n\t\t\tif (sk->sk_family == state->family)\n\t\t\t\tgoto found;\n\t\t}\n\t\tspin_unlock_bh(&hslot->lock);\n\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tdo {\n\t\tsk = sk_nulls_next(sk);\n\t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n\n\tif (!sk) {\n\t\tif (state->bucket <= state->udp_table->mask)\n\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}\n\nstatic struct sock *udp_get_idx(struct seq_file *seq, loff_t pos)\n{\n\tstruct sock *sk = udp_get_first(seq, 0);\n\n\tif (sk)\n\t\twhile (pos && (sk = udp_get_next(seq, sk)) != NULL)\n\t\t\t--pos;\n\treturn pos ? NULL : sk;\n}\n\nstatic void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstate->bucket = MAX_UDP_PORTS;\n\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}\n\nstatic void *udp_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct sock *sk;\n\n\tif (v == SEQ_START_TOKEN)\n\t\tsk = udp_get_idx(seq, 0);\n\telse\n\t\tsk = udp_get_next(seq, v);\n\n\t++*pos;\n\treturn sk;\n}\n\nstatic void udp_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct udp_iter_state *state = seq->private;\n\n\tif (state->bucket <= state->udp_table->mask)\n\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n}\n\nstatic int udp_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct udp_seq_afinfo *afinfo = PDE(inode)->data;\n\tstruct udp_iter_state *s;\n\tint err;\n\n\terr = seq_open_net(inode, file, &afinfo->seq_ops,\n\t\t\t   sizeof(struct udp_iter_state));\n\tif (err < 0)\n\t\treturn err;\n\n\ts = ((struct seq_file *)file->private_data)->private;\n\ts->family\t\t= afinfo->family;\n\ts->udp_table\t\t= afinfo->udp_table;\n\treturn err;\n}\n\n/* ------------------------------------------------------------------------ */\nint udp_proc_register(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tstruct proc_dir_entry *p;\n\tint rc = 0;\n\n\tafinfo->seq_fops.open\t\t= udp_seq_open;\n\tafinfo->seq_fops.read\t\t= seq_read;\n\tafinfo->seq_fops.llseek\t\t= seq_lseek;\n\tafinfo->seq_fops.release\t= seq_release_net;\n\n\tafinfo->seq_ops.start\t\t= udp_seq_start;\n\tafinfo->seq_ops.next\t\t= udp_seq_next;\n\tafinfo->seq_ops.stop\t\t= udp_seq_stop;\n\n\tp = proc_create_data(afinfo->name, S_IRUGO, net->proc_net,\n\t\t\t     &afinfo->seq_fops, afinfo);\n\tif (!p)\n\t\trc = -ENOMEM;\n\treturn rc;\n}\nEXPORT_SYMBOL(udp_proc_register);\n\nvoid udp_proc_unregister(struct net *net, struct udp_seq_afinfo *afinfo)\n{\n\tproc_net_remove(net, afinfo->name);\n}\nEXPORT_SYMBOL(udp_proc_unregister);\n\n/* ------------------------------------------------------------------------ */\nstatic void udp4_format_sock(struct sock *sp, struct seq_file *f,\n\t\tint bucket, int *len)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\t__be32 dest = inet->inet_daddr;\n\t__be32 src  = inet->inet_rcv_saddr;\n\t__u16 destp\t  = ntohs(inet->inet_dport);\n\t__u16 srcp\t  = ntohs(inet->inet_sport);\n\n\tseq_printf(f, \"%5d: %08X:%04X %08X:%04X\"\n\t\t\" %02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %p %d%n\",\n\t\tbucket, src, srcp, dest, destp, sp->sk_state,\n\t\tsk_wmem_alloc_get(sp),\n\t\tsk_rmem_alloc_get(sp),\n\t\t0, 0L, 0, sock_i_uid(sp), 0, sock_i_ino(sp),\n\t\tatomic_read(&sp->sk_refcnt), sp,\n\t\tatomic_read(&sp->sk_drops), len);\n}\n\nint udp4_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq, \"%-127s\\n\",\n\t\t\t   \"  sl  local_address rem_address   st tx_queue \"\n\t\t\t   \"rx_queue tr tm->when retrnsmt   uid  timeout \"\n\t\t\t   \"inode ref pointer drops\");\n\telse {\n\t\tstruct udp_iter_state *state = seq->private;\n\t\tint len;\n\n\t\tudp4_format_sock(v, seq, state->bucket, &len);\n\t\tseq_printf(seq, \"%*s\\n\", 127 - len, \"\");\n\t}\n\treturn 0;\n}\n\n/* ------------------------------------------------------------------------ */\nstatic struct udp_seq_afinfo udp4_seq_afinfo = {\n\t.name\t\t= \"udp\",\n\t.family\t\t= AF_INET,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= {\n\t\t.owner\t=\tTHIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= udp4_seq_show,\n\t},\n};\n\nstatic int __net_init udp4_proc_init_net(struct net *net)\n{\n\treturn udp_proc_register(net, &udp4_seq_afinfo);\n}\n\nstatic void __net_exit udp4_proc_exit_net(struct net *net)\n{\n\tudp_proc_unregister(net, &udp4_seq_afinfo);\n}\n\nstatic struct pernet_operations udp4_net_ops = {\n\t.init = udp4_proc_init_net,\n\t.exit = udp4_proc_exit_net,\n};\n\nint __init udp4_proc_init(void)\n{\n\treturn register_pernet_subsys(&udp4_net_ops);\n}\n\nvoid udp4_proc_exit(void)\n{\n\tunregister_pernet_subsys(&udp4_net_ops);\n}\n#endif /* CONFIG_PROC_FS */\n\nstatic __initdata unsigned long uhash_entries;\nstatic int __init set_uhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tuhash_entries = simple_strtoul(str, &str, 0);\n\tif (uhash_entries && uhash_entries < UDP_HTABLE_SIZE_MIN)\n\t\tuhash_entries = UDP_HTABLE_SIZE_MIN;\n\treturn 1;\n}\n__setup(\"uhash_entries=\", set_uhash_entries);\n\nvoid __init udp_table_init(struct udp_table *table, const char *name)\n{\n\tunsigned int i;\n\n\tif (!CONFIG_BASE_SMALL)\n\t\ttable->hash = alloc_large_system_hash(name,\n\t\t\t2 * sizeof(struct udp_hslot),\n\t\t\tuhash_entries,\n\t\t\t21, /* one slot per 2 MB */\n\t\t\t0,\n\t\t\t&table->log,\n\t\t\t&table->mask,\n\t\t\t64 * 1024);\n\t/*\n\t * Make sure hash table has the minimum size\n\t */\n\tif (CONFIG_BASE_SMALL || table->mask < UDP_HTABLE_SIZE_MIN - 1) {\n\t\ttable->hash = kmalloc(UDP_HTABLE_SIZE_MIN *\n\t\t\t\t      2 * sizeof(struct udp_hslot), GFP_KERNEL);\n\t\tif (!table->hash)\n\t\t\tpanic(name);\n\t\ttable->log = ilog2(UDP_HTABLE_SIZE_MIN);\n\t\ttable->mask = UDP_HTABLE_SIZE_MIN - 1;\n\t}\n\ttable->hash2 = table->hash + (table->mask + 1);\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash[i].head, i);\n\t\ttable->hash[i].count = 0;\n\t\tspin_lock_init(&table->hash[i].lock);\n\t}\n\tfor (i = 0; i <= table->mask; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&table->hash2[i].head, i);\n\t\ttable->hash2[i].count = 0;\n\t\tspin_lock_init(&table->hash2[i].lock);\n\t}\n}\n\nvoid __init udp_init(void)\n{\n\tunsigned long nr_pages, limit;\n\n\tudp_table_init(&udp_table, \"UDP\");\n\t/* Set the pressure threshold up by the same strategy of TCP. It is a\n\t * fraction of global memory that is up to 1/2 at 256 MB, decreasing\n\t * toward zero with the amount of memory, with a floor of 128 pages.\n\t */\n\tnr_pages = totalram_pages - totalhigh_pages;\n\tlimit = min(nr_pages, 1UL<<(28-PAGE_SHIFT)) >> (20-PAGE_SHIFT);\n\tlimit = (limit * (nr_pages >> (20-PAGE_SHIFT))) >> (PAGE_SHIFT-11);\n\tlimit = max(limit, 128UL);\n\tsysctl_udp_mem[0] = limit / 4 * 3;\n\tsysctl_udp_mem[1] = limit;\n\tsysctl_udp_mem[2] = sysctl_udp_mem[0] * 2;\n\n\tsysctl_udp_rmem_min = SK_MEM_QUANTUM;\n\tsysctl_udp_wmem_min = SK_MEM_QUANTUM;\n}\n\nint udp4_ufo_send_check(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph;\n\tstruct udphdr *uh;\n\n\tif (!pskb_may_pull(skb, sizeof(*uh)))\n\t\treturn -EINVAL;\n\n\tiph = ip_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tuh->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, skb->len,\n\t\t\t\t       IPPROTO_UDP, 0);\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct udphdr, check);\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\treturn 0;\n}\n\nstruct sk_buff *udp4_ufo_fragment(struct sk_buff *skb, u32 features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tint offset;\n\t__wsum csum;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP | SKB_GSO_DODGY) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t * do checksum of UDP packets sent as multiple IP fragments.\n\t */\n\toffset = skb_checksum_start_offset(skb);\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\toffset += skb->csum_offset;\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t/* Fragment the skb. IP headers of the fragments are updated in\n\t * inet_gso_segment()\n\t */\n\tsegs = skb_segment(skb, features);\nout:\n\treturn segs;\n}\n\n", "/*\n *\tTCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on:\n *\tlinux/net/ipv4/tcp.c\n *\tlinux/net/ipv4/tcp_input.c\n *\tlinux/net/ipv4/tcp_output.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tYOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/tcp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/bottom_half.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/jiffies.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/init.h>\n#include <linux/jhash.h>\n#include <linux/ipsec.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/random.h>\n\n#include <net/tcp.h>\n#include <net/ndisc.h>\n#include <net/inet6_hashtables.h>\n#include <net/inet6_connection_sock.h>\n#include <net/ipv6.h>\n#include <net/transp_v6.h>\n#include <net/addrconf.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/inet_ecn.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n#include <net/snmp.h>\n#include <net/dsfield.h>\n#include <net/timewait_sock.h>\n#include <net/netdma.h>\n#include <net/inet_common.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n\n#include <linux/crypto.h>\n#include <linux/scatterlist.h>\n\nstatic void\ttcp_v6_send_reset(struct sock *sk, struct sk_buff *skb);\nstatic void\ttcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      struct request_sock *req);\n\nstatic int\ttcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);\nstatic void\t__tcp_v6_send_check(struct sk_buff *skb,\n\t\t\t\t    const struct in6_addr *saddr,\n\t\t\t\t    const struct in6_addr *daddr);\n\nstatic const struct inet_connection_sock_af_ops ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops ipv6_specific;\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific;\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;\n#else\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr)\n{\n\treturn NULL;\n}\n#endif\n\nstatic void tcp_v6_hash(struct sock *sk)\n{\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tif (inet_csk(sk)->icsk_af_ops == &ipv6_mapped) {\n\t\t\ttcp_prot.hash(sk);\n\t\t\treturn;\n\t\t}\n\t\tlocal_bh_disable();\n\t\t__inet6_hash(sk, NULL);\n\t\tlocal_bh_enable();\n\t}\n}\n\nstatic __inline__ __sum16 tcp_v6_check(int len,\n\t\t\t\t   const struct in6_addr *saddr,\n\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t   __wsum base)\n{\n\treturn csum_ipv6_magic(saddr, daddr, len, IPPROTO_TCP, base);\n}\n\nstatic __u32 tcp_v6_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_tcpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t    ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t    tcp_hdr(skb)->dest,\n\t\t\t\t\t    tcp_hdr(skb)->source);\n}\n\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct rt6_info *rt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tipv6_addr_copy(&usin->sin6_addr, &flowlabel->dst);\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif(ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif(addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&np->daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tipv6_addr_copy(&np->daddr, &usin->sin6_addr);\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t} else {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &np->rcv_saddr);\n\t\t}\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&np->rcv_saddr))\n\t\tsaddr = &np->rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\tipv6_addr_copy(&fl6.saddr,\n\t\t       (saddr ? saddr : &np->saddr));\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, true);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tipv6_addr_copy(&np->rcv_saddr, saddr);\n\t}\n\n\t/* set the source address */\n\tipv6_addr_copy(&np->saddr, saddr);\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\trt = (struct rt6_info *) dst;\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&rt->rt6i_dst.addr, &np->daddr)) {\n\t\tstruct inet_peer *peer = rt6_get_peer(rt);\n\t\t/*\n\t\t * VJ's idea. We save last timestamp seen from\n\t\t * the destination in peer table, when entering state\n\t\t * TIME-WAIT * and initialize rx_opt.ts_recent from it,\n\t\t * when trying new connection.\n\t\t */\n\t\tif (peer) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp <= TCP_PAWS_MSL) {\n\t\t\t\ttp->rx_opt.ts_recent_stamp = peer->tcp_ts_stamp;\n\t\t\t\ttp->rx_opt.ts_recent = peer->tcp_ts;\n\t\t\t}\n\t\t}\n\t}\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tif (!tp->write_seq)\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     np->daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr*)skb->data;\n\tconst struct tcphdr *th = (struct tcphdr *)(skb->data+offset);\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\tstruct tcp_sock *tp;\n\t__u32 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = inet6_lookup(net, &tcp_hashinfo, &hdr->daddr,\n\t\t\tth->dest, &hdr->saddr, th->source, skb->dev->ifindex);\n\n\tif (sk == NULL) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (ipv6_hdr(skb)->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ttp = tcp_sk(sk);\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, tp->snd_una, tp->snd_nxt)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))\n\t\t\tgoto out;\n\n\t\t/* icmp should have updated the destination cache entry */\n\t\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\tif (dst == NULL) {\n\t\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\t\tstruct flowi6 fl6;\n\n\t\t\t/* BUGGG_FUTURE: Again, it is not clear how\n\t\t\t   to handle rthdr case. Ignore this complexity\n\t\t\t   for now.\n\t\t\t */\n\t\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\t\tipv6_addr_copy(&fl6.daddr, &np->daddr);\n\t\t\tipv6_addr_copy(&fl6.saddr, &np->saddr);\n\t\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\t\tfl6.fl6_dport = inet->inet_dport;\n\t\t\tfl6.fl6_sport = inet->inet_sport;\n\t\t\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\t\t\tdst = ip6_dst_lookup_flow(sk, &fl6, NULL, false);\n\t\t\tif (IS_ERR(dst)) {\n\t\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t} else\n\t\t\tdst_hold(dst);\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {\n\t\t\ttcp_sync_mss(sk, dst_mtu(dst));\n\t\t\ttcp_simple_retransmit(sk);\n\t\t} /* else let the usual retransmit timer handle it */\n\t\tdst_release(dst);\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\t\tstruct request_sock *req, **prev;\n\tcase TCP_LISTEN:\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\n\t\treq = inet6_csk_search_req(sk, &prev, th->dest, &hdr->daddr,\n\t\t\t\t\t   &hdr->saddr, inet6_iif(skb));\n\t\tif (!req)\n\t\t\tgoto out;\n\n\t\t/* ICMPs are not backlogged, hence we cannot get\n\t\t * an established socket here.\n\t\t */\n\t\tWARN_ON(req->sk != NULL);\n\n\t\tif (seq != tcp_rsk(req)->snt_isn) {\n\t\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\t\tgoto out;\n\t\t}\n\n\t\tinet_csk_reqsk_queue_drop(sk, req, prev);\n\t\tgoto out;\n\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\t\t\tsk->sk_error_report(sk);\t\t/* Wake people up to see the error (see connect in sock.c) */\n\n\t\t\ttcp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\n\nstatic int tcp_v6_send_synack(struct sock *sk, struct request_sock *req,\n\t\t\t      struct request_values *rvp)\n{\n\tstruct inet6_request_sock *treq = inet6_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff * skb;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct in6_addr * final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint err;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tipv6_addr_copy(&fl6.daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&fl6.saddr, &treq->loc_addr);\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = treq->iif;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\topt = np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\tskb = tcp_make_synack(sk, dst, req, rvp);\n\terr = -ENOMEM;\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &treq->loc_addr, &treq->rmt_addr);\n\n\t\tipv6_addr_copy(&fl6.daddr, &treq->rmt_addr);\n\t\terr = ip6_xmit(sk, skb, &fl6, opt);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic int tcp_v6_rtx_synack(struct sock *sk, struct request_sock *req,\n\t\t\t     struct request_values *rvp)\n{\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\treturn tcp_v6_send_synack(sk, req, rvp);\n}\n\nstatic inline void syn_flood_warning(struct sk_buff *skb)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tif (sysctl_tcp_syncookies)\n\t\tprintk(KERN_INFO\n\t\t       \"TCPv6: Possible SYN flooding on port %d. \"\n\t\t       \"Sending cookies.\\n\", ntohs(tcp_hdr(skb)->dest));\n\telse\n#endif\n\t\tprintk(KERN_INFO\n\t\t       \"TCPv6: Possible SYN flooding on port %d. \"\n\t\t       \"Dropping request.\\n\", ntohs(tcp_hdr(skb)->dest));\n}\n\nstatic void tcp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tkfree_skb(inet6_rsk(req)->pktopts);\n}\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tBUG_ON(tp == NULL);\n\n\tif (!tp->md5sig_info || !tp->md5sig_info->entries6)\n\t\treturn NULL;\n\n\tfor (i = 0; i < tp->md5sig_info->entries6; i++) {\n\t\tif (ipv6_addr_equal(&tp->md5sig_info->keys6[i].addr, addr))\n\t\t\treturn &tp->md5sig_info->keys6[i].base;\n\t}\n\treturn NULL;\n}\n\nstatic struct tcp_md5sig_key *tcp_v6_md5_lookup(struct sock *sk,\n\t\t\t\t\t\tstruct sock *addr_sk)\n{\n\treturn tcp_v6_md5_do_lookup(sk, &inet6_sk(addr_sk)->daddr);\n}\n\nstatic struct tcp_md5sig_key *tcp_v6_reqsk_md5_lookup(struct sock *sk,\n\t\t\t\t\t\t      struct request_sock *req)\n{\n\treturn tcp_v6_md5_do_lookup(sk, &inet6_rsk(req)->rmt_addr);\n}\n\nstatic int tcp_v6_md5_do_add(struct sock *sk, const struct in6_addr *peer,\n\t\t\t     char *newkey, u8 newkeylen)\n{\n\t/* Add key to the list */\n\tstruct tcp_md5sig_key *key;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp6_md5sig_key *keys;\n\n\tkey = tcp_v6_md5_do_lookup(sk, peer);\n\tif (key) {\n\t\t/* modify existing entry - just update that one */\n\t\tkfree(key->key);\n\t\tkey->key = newkey;\n\t\tkey->keylen = newkeylen;\n\t} else {\n\t\t/* reallocate new list if current one is full. */\n\t\tif (!tp->md5sig_info) {\n\t\t\ttp->md5sig_info = kzalloc(sizeof(*tp->md5sig_info), GFP_ATOMIC);\n\t\t\tif (!tp->md5sig_info) {\n\t\t\t\tkfree(newkey);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t\t}\n\t\tif (tcp_alloc_md5sig_pool(sk) == NULL) {\n\t\t\tkfree(newkey);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (tp->md5sig_info->alloced6 == tp->md5sig_info->entries6) {\n\t\t\tkeys = kmalloc((sizeof (tp->md5sig_info->keys6[0]) *\n\t\t\t\t       (tp->md5sig_info->entries6 + 1)), GFP_ATOMIC);\n\n\t\t\tif (!keys) {\n\t\t\t\ttcp_free_md5sig_pool();\n\t\t\t\tkfree(newkey);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tif (tp->md5sig_info->entries6)\n\t\t\t\tmemmove(keys, tp->md5sig_info->keys6,\n\t\t\t\t\t(sizeof (tp->md5sig_info->keys6[0]) *\n\t\t\t\t\t tp->md5sig_info->entries6));\n\n\t\t\tkfree(tp->md5sig_info->keys6);\n\t\t\ttp->md5sig_info->keys6 = keys;\n\t\t\ttp->md5sig_info->alloced6++;\n\t\t}\n\n\t\tipv6_addr_copy(&tp->md5sig_info->keys6[tp->md5sig_info->entries6].addr,\n\t\t\t       peer);\n\t\ttp->md5sig_info->keys6[tp->md5sig_info->entries6].base.key = newkey;\n\t\ttp->md5sig_info->keys6[tp->md5sig_info->entries6].base.keylen = newkeylen;\n\n\t\ttp->md5sig_info->entries6++;\n\t}\n\treturn 0;\n}\n\nstatic int tcp_v6_md5_add_func(struct sock *sk, struct sock *addr_sk,\n\t\t\t       u8 *newkey, __u8 newkeylen)\n{\n\treturn tcp_v6_md5_do_add(sk, &inet6_sk(addr_sk)->daddr,\n\t\t\t\t newkey, newkeylen);\n}\n\nstatic int tcp_v6_md5_do_del(struct sock *sk, const struct in6_addr *peer)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tfor (i = 0; i < tp->md5sig_info->entries6; i++) {\n\t\tif (ipv6_addr_equal(&tp->md5sig_info->keys6[i].addr, peer)) {\n\t\t\t/* Free the key */\n\t\t\tkfree(tp->md5sig_info->keys6[i].base.key);\n\t\t\ttp->md5sig_info->entries6--;\n\n\t\t\tif (tp->md5sig_info->entries6 == 0) {\n\t\t\t\tkfree(tp->md5sig_info->keys6);\n\t\t\t\ttp->md5sig_info->keys6 = NULL;\n\t\t\t\ttp->md5sig_info->alloced6 = 0;\n\t\t\t} else {\n\t\t\t\t/* shrink the database */\n\t\t\t\tif (tp->md5sig_info->entries6 != i)\n\t\t\t\t\tmemmove(&tp->md5sig_info->keys6[i],\n\t\t\t\t\t\t&tp->md5sig_info->keys6[i+1],\n\t\t\t\t\t\t(tp->md5sig_info->entries6 - i)\n\t\t\t\t\t\t* sizeof (tp->md5sig_info->keys6[0]));\n\t\t\t}\n\t\t\ttcp_free_md5sig_pool();\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\n\nstatic void tcp_v6_clear_md5_list (struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint i;\n\n\tif (tp->md5sig_info->entries6) {\n\t\tfor (i = 0; i < tp->md5sig_info->entries6; i++)\n\t\t\tkfree(tp->md5sig_info->keys6[i].base.key);\n\t\ttp->md5sig_info->entries6 = 0;\n\t\ttcp_free_md5sig_pool();\n\t}\n\n\tkfree(tp->md5sig_info->keys6);\n\ttp->md5sig_info->keys6 = NULL;\n\ttp->md5sig_info->alloced6 = 0;\n\n\tif (tp->md5sig_info->entries4) {\n\t\tfor (i = 0; i < tp->md5sig_info->entries4; i++)\n\t\t\tkfree(tp->md5sig_info->keys4[i].base.key);\n\t\ttp->md5sig_info->entries4 = 0;\n\t\ttcp_free_md5sig_pool();\n\t}\n\n\tkfree(tp->md5sig_info->keys4);\n\ttp->md5sig_info->keys4 = NULL;\n\ttp->md5sig_info->alloced4 = 0;\n}\n\nstatic int tcp_v6_parse_md5_keys (struct sock *sk, char __user *optval,\n\t\t\t\t  int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)&cmd.tcpm_addr;\n\tu8 *newkey;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin6->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\tif (!cmd.tcpm_keylen) {\n\t\tif (!tcp_sk(sk)->md5sig_info)\n\t\t\treturn -ENOENT;\n\t\tif (ipv6_addr_v4mapped(&sin6->sin6_addr))\n\t\t\treturn tcp_v4_md5_do_del(sk, sin6->sin6_addr.s6_addr32[3]);\n\t\treturn tcp_v6_md5_do_del(sk, &sin6->sin6_addr);\n\t}\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\tif (!tcp_sk(sk)->md5sig_info) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tstruct tcp_md5sig_info *p;\n\n\t\tp = kzalloc(sizeof(struct tcp_md5sig_info), GFP_KERNEL);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\n\t\ttp->md5sig_info = p;\n\t\tsk_nocaps_add(sk, NETIF_F_GSO_MASK);\n\t}\n\n\tnewkey = kmemdup(cmd.tcpm_key, cmd.tcpm_keylen, GFP_KERNEL);\n\tif (!newkey)\n\t\treturn -ENOMEM;\n\tif (ipv6_addr_v4mapped(&sin6->sin6_addr)) {\n\t\treturn tcp_v4_md5_do_add(sk, sin6->sin6_addr.s6_addr32[3],\n\t\t\t\t\t newkey, cmd.tcpm_keylen);\n\t}\n\treturn tcp_v6_md5_do_add(sk, &sin6->sin6_addr, newkey, cmd.tcpm_keylen);\n}\n\nstatic int tcp_v6_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,\n\t\t\t\t\tconst struct in6_addr *daddr,\n\t\t\t\t\tconst struct in6_addr *saddr, int nbytes)\n{\n\tstruct tcp6_pseudohdr *bp;\n\tstruct scatterlist sg;\n\n\tbp = &hp->md5_blk.ip6;\n\t/* 1. TCP pseudo-header (RFC2460) */\n\tipv6_addr_copy(&bp->saddr, saddr);\n\tipv6_addr_copy(&bp->daddr, daddr);\n\tbp->protocol = cpu_to_be32(IPPROTO_TCP);\n\tbp->len = cpu_to_be32(nbytes);\n\n\tsg_init_one(&sg, bp, sizeof(*bp));\n\treturn crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));\n}\n\nstatic int tcp_v6_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       const struct in6_addr *daddr, struct in6_addr *saddr,\n\t\t\t       struct tcphdr *th)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\tif (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nstatic int tcp_v6_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       struct sock *sk, struct request_sock *req,\n\t\t\t       struct sk_buff *skb)\n{\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tif (sk) {\n\t\tsaddr = &inet6_sk(sk)->saddr;\n\t\tdaddr = &inet6_sk(sk)->daddr;\n\t} else if (req) {\n\t\tsaddr = &inet6_rsk(req)->loc_addr;\n\t\tdaddr = &inet6_rsk(req)->rmt_addr;\n\t} else {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tsaddr = &ip6h->saddr;\n\t\tdaddr = &ip6h->daddr;\n\t}\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\n\tif (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nstatic int tcp_v6_inbound_md5_hash (struct sock *sk, struct sk_buff *skb)\n{\n\t__u8 *hash_location = NULL;\n\tstruct tcp_md5sig_key *hash_expected;\n\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tint genhash;\n\tu8 newhash[16];\n\n\thash_expected = tcp_v6_md5_do_lookup(sk, &ip6h->saddr);\n\thash_location = tcp_parse_md5sig_option(th);\n\n\t/* We've parsed the options - do we have a hash? */\n\tif (!hash_expected && !hash_location)\n\t\treturn 0;\n\n\tif (hash_expected && !hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);\n\t\treturn 1;\n\t}\n\n\tif (!hash_expected && hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);\n\t\treturn 1;\n\t}\n\n\t/* check the signature */\n\tgenhash = tcp_v6_md5_hash_skb(newhash,\n\t\t\t\t      hash_expected,\n\t\t\t\t      NULL, NULL, skb);\n\n\tif (genhash || memcmp(hash_location, newhash, 16) != 0) {\n\t\tif (net_ratelimit()) {\n\t\t\tprintk(KERN_INFO \"MD5 Hash %s for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t       genhash ? \"failed\" : \"mismatch\",\n\t\t\t       &ip6h->saddr, ntohs(th->source),\n\t\t\t       &ip6h->daddr, ntohs(th->dest));\n\t\t}\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n#endif\n\nstruct request_sock_ops tcp6_request_sock_ops __read_mostly = {\n\t.family\t\t=\tAF_INET6,\n\t.obj_size\t=\tsizeof(struct tcp6_request_sock),\n\t.rtx_syn_ack\t=\ttcp_v6_rtx_synack,\n\t.send_ack\t=\ttcp_v6_reqsk_send_ack,\n\t.destructor\t=\ttcp_v6_reqsk_destructor,\n\t.send_reset\t=\ttcp_v6_send_reset,\n\t.syn_ack_timeout = \ttcp_syn_ack_timeout,\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {\n\t.md5_lookup\t=\ttcp_v6_reqsk_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n};\n#endif\n\nstatic void __tcp_v6_send_check(struct sk_buff *skb,\n\t\t\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tth->check = ~tcp_v6_check(skb->len, saddr, daddr, 0);\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct tcphdr, check);\n\t} else {\n\t\tth->check = tcp_v6_check(skb->len, saddr, daddr,\n\t\t\t\t\t csum_partial(th, th->doff << 2,\n\t\t\t\t\t\t      skb->csum));\n\t}\n}\n\nstatic void tcp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t__tcp_v6_send_check(skb, &np->saddr, &np->daddr);\n}\n\nstatic int tcp_v6_gso_send_check(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *ipv6h;\n\tstruct tcphdr *th;\n\n\tif (!pskb_may_pull(skb, sizeof(*th)))\n\t\treturn -EINVAL;\n\n\tipv6h = ipv6_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tth->check = 0;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t__tcp_v6_send_check(skb, &ipv6h->saddr, &ipv6h->daddr);\n\treturn 0;\n}\n\nstatic struct sk_buff **tcp6_gro_receive(struct sk_buff **head,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *iph = skb_gro_network_header(skb);\n\n\tswitch (skb->ip_summed) {\n\tcase CHECKSUM_COMPLETE:\n\t\tif (!tcp_v6_check(skb_gro_len(skb), &iph->saddr, &iph->daddr,\n\t\t\t\t  skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* fall through */\n\tcase CHECKSUM_NONE:\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\treturn tcp_gro_receive(head, skb);\n}\n\nstatic int tcp6_gro_complete(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\tstruct tcphdr *th = tcp_hdr(skb);\n\n\tth->check = ~tcp_v6_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  &iph->saddr, &iph->daddr, 0);\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\n\n\treturn tcp_gro_complete(skb);\n}\n\nstatic void tcp_v6_send_response(struct sk_buff *skb, u32 seq, u32 ack, u32 win,\n\t\t\t\t u32 ts, struct tcp_md5sig_key *key, int rst)\n{\n\tstruct tcphdr *th = tcp_hdr(skb), *t1;\n\tstruct sk_buff *buff;\n\tstruct flowi6 fl6;\n\tstruct net *net = dev_net(skb_dst(skb)->dev);\n\tstruct sock *ctl_sk = net->ipv6.tcp_sk;\n\tunsigned int tot_len = sizeof(struct tcphdr);\n\tstruct dst_entry *dst;\n\t__be32 *topt;\n\n\tif (ts)\n\t\ttot_len += TCPOLEN_TSTAMP_ALIGNED;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key)\n\t\ttot_len += TCPOLEN_MD5SIG_ALIGNED;\n#endif\n\n\tbuff = alloc_skb(MAX_HEADER + sizeof(struct ipv6hdr) + tot_len,\n\t\t\t GFP_ATOMIC);\n\tif (buff == NULL)\n\t\treturn;\n\n\tskb_reserve(buff, MAX_HEADER + sizeof(struct ipv6hdr) + tot_len);\n\n\tt1 = (struct tcphdr *) skb_push(buff, tot_len);\n\tskb_reset_transport_header(buff);\n\n\t/* Swap the send and the receive. */\n\tmemset(t1, 0, sizeof(*t1));\n\tt1->dest = th->source;\n\tt1->source = th->dest;\n\tt1->doff = tot_len / 4;\n\tt1->seq = htonl(seq);\n\tt1->ack_seq = htonl(ack);\n\tt1->ack = !rst || !th->ack;\n\tt1->rst = rst;\n\tt1->window = htons(win);\n\n\ttopt = (__be32 *)(t1 + 1);\n\n\tif (ts) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);\n\t\t*topt++ = htonl(tcp_time_stamp);\n\t\t*topt++ = htonl(ts);\n\t}\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\ttcp_v6_md5_hash_hdr((__u8 *)topt, key,\n\t\t\t\t    &ipv6_hdr(skb)->saddr,\n\t\t\t\t    &ipv6_hdr(skb)->daddr, t1);\n\t}\n#endif\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tipv6_addr_copy(&fl6.daddr, &ipv6_hdr(skb)->saddr);\n\tipv6_addr_copy(&fl6.saddr, &ipv6_hdr(skb)->daddr);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tbuff->csum = 0;\n\n\t__tcp_v6_send_check(buff, &fl6.saddr, &fl6.daddr);\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.flowi6_oif = inet6_iif(skb);\n\tfl6.fl6_dport = t1->dest;\n\tfl6.fl6_sport = t1->source;\n\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\t/* Pass a socket to ip6_dst_lookup either it is for RST\n\t * Underlying function will use this to retrieve the network\n\t * namespace\n\t */\n\tdst = ip6_dst_lookup_flow(ctl_sk, &fl6, NULL, false);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(buff, dst);\n\t\tip6_xmit(ctl_sk, buff, &fl6, NULL);\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n\t\tif (rst)\n\t\t\tTCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(buff);\n}\n\nstatic void tcp_v6_send_reset(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tu32 seq = 0, ack_seq = 0;\n\tstruct tcp_md5sig_key *key = NULL;\n\n\tif (th->rst)\n\t\treturn;\n\n\tif (!ipv6_unicast_destination(skb))\n\t\treturn;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (sk)\n\t\tkey = tcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr);\n#endif\n\n\tif (th->ack)\n\t\tseq = ntohl(th->ack_seq);\n\telse\n\t\tack_seq = ntohl(th->seq) + th->syn + th->fin + skb->len -\n\t\t\t  (th->doff << 2);\n\n\ttcp_v6_send_response(skb, seq, ack_seq, 0, 0, key, 1);\n}\n\nstatic void tcp_v6_send_ack(struct sk_buff *skb, u32 seq, u32 ack, u32 win, u32 ts,\n\t\t\t    struct tcp_md5sig_key *key)\n{\n\ttcp_v6_send_response(skb, seq, ack, win, ts, key, 0);\n}\n\nstatic void tcp_v6_timewait_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\n\ttcp_v6_send_ack(skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcptw->tw_ts_recent, tcp_twsk_md5_key(tcptw));\n\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v6_reqsk_send_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\ttcp_v6_send_ack(skb, tcp_rsk(req)->snt_isn + 1, tcp_rsk(req)->rcv_isn + 1, req->rcv_wnd, req->ts_recent,\n\t\t\ttcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr));\n}\n\n\nstatic struct sock *tcp_v6_hnd_req(struct sock *sk,struct sk_buff *skb)\n{\n\tstruct request_sock *req, **prev;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct sock *nsk;\n\n\t/* Find possible connection requests. */\n\treq = inet6_csk_search_req(sk, &prev, th->source,\n\t\t\t\t   &ipv6_hdr(skb)->saddr,\n\t\t\t\t   &ipv6_hdr(skb)->daddr, inet6_iif(skb));\n\tif (req)\n\t\treturn tcp_check_req(sk, skb, req, prev);\n\n\tnsk = __inet6_lookup_established(sock_net(sk), &tcp_hashinfo,\n\t\t\t&ipv6_hdr(skb)->saddr, th->source,\n\t\t\t&ipv6_hdr(skb)->daddr, ntohs(th->dest), inet6_iif(skb));\n\n\tif (nsk) {\n\t\tif (nsk->sk_state != TCP_TIME_WAIT) {\n\t\t\tbh_lock_sock(nsk);\n\t\t\treturn nsk;\n\t\t}\n\t\tinet_twsk_put(inet_twsk(nsk));\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (!th->syn)\n\t\tsk = cookie_v6_check(sk, skb);\n#endif\n\treturn sk;\n}\n\n/* FIXME: this is substantially similar to the ipv4 code.\n * Can some kind of merge be done? -- erics\n */\nstatic int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_extend_values tmp_ext;\n\tstruct tcp_options_received tmp_opt;\n\tu8 *hash_location;\n\tstruct request_sock *req;\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 isn = TCP_SKB_CB(skb)->when;\n\tstruct dst_entry *dst = NULL;\n#ifdef CONFIG_SYN_COOKIES\n\tint want_cookie = 0;\n#else\n#define want_cookie 0\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\tgoto drop;\n\n\tif (inet_csk_reqsk_queue_is_full(sk) && !isn) {\n\t\tif (net_ratelimit())\n\t\t\tsyn_flood_warning(skb);\n#ifdef CONFIG_SYN_COOKIES\n\t\tif (sysctl_tcp_syncookies)\n\t\t\twant_cookie = 1;\n\t\telse\n#endif\n\t\tgoto drop;\n\t}\n\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet6_reqsk_alloc(&tcp6_request_sock_ops);\n\tif (req == NULL)\n\t\tgoto drop;\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttcp_rsk(req)->af_specific = &tcp_request_sock_ipv6_ops;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\ttmp_opt.user_mss = tp->rx_opt.user_mss;\n\ttcp_parse_options(skb, &tmp_opt, &hash_location, 0);\n\n\tif (tmp_opt.cookie_plus > 0 &&\n\t    tmp_opt.saw_tstamp &&\n\t    !tp->rx_opt.cookie_out_never &&\n\t    (sysctl_tcp_cookie_size > 0 ||\n\t     (tp->cookie_values != NULL &&\n\t      tp->cookie_values->cookie_desired > 0))) {\n\t\tu8 *c;\n\t\tu32 *d;\n\t\tu32 *mess = &tmp_ext.cookie_bakery[COOKIE_DIGEST_WORDS];\n\t\tint l = tmp_opt.cookie_plus - TCPOLEN_COOKIE_BASE;\n\n\t\tif (tcp_cookie_generator(&tmp_ext.cookie_bakery[0]) != 0)\n\t\t\tgoto drop_and_free;\n\n\t\t/* Secret recipe starts with IP addresses */\n\t\td = (__force u32 *)&ipv6_hdr(skb)->daddr.s6_addr32[0];\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\td = (__force u32 *)&ipv6_hdr(skb)->saddr.s6_addr32[0];\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\t\t*mess++ ^= *d++;\n\n\t\t/* plus variable length Initiator Cookie */\n\t\tc = (u8 *)mess;\n\t\twhile (l-- > 0)\n\t\t\t*c++ ^= *hash_location++;\n\n#ifdef CONFIG_SYN_COOKIES\n\t\twant_cookie = 0;\t/* not our kind of cookie */\n#endif\n\t\ttmp_ext.cookie_out_never = 0; /* false */\n\t\ttmp_ext.cookie_plus = tmp_opt.cookie_plus;\n\t} else if (!tp->rx_opt.cookie_in_always) {\n\t\t/* redundant indications, but ensure initialization. */\n\t\ttmp_ext.cookie_out_never = 1; /* true */\n\t\ttmp_ext.cookie_plus = 0;\n\t} else {\n\t\tgoto drop_and_free;\n\t}\n\ttmp_ext.cookie_in_always = tp->rx_opt.cookie_in_always;\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb);\n\n\ttreq = inet6_rsk(req);\n\tipv6_addr_copy(&treq->rmt_addr, &ipv6_hdr(skb)->saddr);\n\tipv6_addr_copy(&treq->loc_addr, &ipv6_hdr(skb)->daddr);\n\tif (!want_cookie || tmp_opt.tstamp_ok)\n\t\tTCP_ECN_create_request(req, tcp_hdr(skb));\n\n\tif (!isn) {\n\t\tstruct inet_peer *peer = NULL;\n\n\t\tif (ipv6_opt_accepted(sk, skb) ||\n\t\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\t\tatomic_inc(&skb->users);\n\t\t\ttreq->pktopts = skb;\n\t\t}\n\t\ttreq->iif = sk->sk_bound_dev_if;\n\n\t\t/* So that link locals have meaning */\n\t\tif (!sk->sk_bound_dev_if &&\n\t\t    ipv6_addr_type(&treq->rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\ttreq->iif = inet6_iif(skb);\n\n\t\tif (want_cookie) {\n\t\t\tisn = cookie_v6_init_sequence(sk, skb, &req->mss);\n\t\t\treq->cookie_ts = tmp_opt.tstamp_ok;\n\t\t\tgoto have_isn;\n\t\t}\n\n\t\t/* VJ's idea. We save last timestamp seen\n\t\t * from the destination in peer table, when entering\n\t\t * state TIME-WAIT, and check against it before\n\t\t * accepting new connection request.\n\t\t *\n\t\t * If \"isn\" is not zero, this request hit alive\n\t\t * timewait bucket, so that all the necessary checks\n\t\t * are made in the function processing timewait state.\n\t\t */\n\t\tif (tmp_opt.saw_tstamp &&\n\t\t    tcp_death_row.sysctl_tw_recycle &&\n\t\t    (dst = inet6_csk_route_req(sk, req)) != NULL &&\n\t\t    (peer = rt6_get_peer((struct rt6_info *)dst)) != NULL &&\n\t\t    ipv6_addr_equal((struct in6_addr *)peer->daddr.addr.a6,\n\t\t\t\t    &treq->rmt_addr)) {\n\t\t\tinet_peer_refcheck(peer);\n\t\t\tif ((u32)get_seconds() - peer->tcp_ts_stamp < TCP_PAWS_MSL &&\n\t\t\t    (s32)(peer->tcp_ts - req->ts_recent) >\n\t\t\t\t\t\t\tTCP_PAWS_WINDOW) {\n\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);\n\t\t\t\tgoto drop_and_release;\n\t\t\t}\n\t\t}\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\telse if (!sysctl_tcp_syncookies &&\n\t\t\t (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t\t  (sysctl_max_syn_backlog >> 2)) &&\n\t\t\t (!peer || !peer->tcp_ts_stamp) &&\n\t\t\t (!dst || !dst_metric(dst, RTAX_RTT))) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tLIMIT_NETDEBUG(KERN_DEBUG \"TCP: drop open request from %pI6/%u\\n\",\n\t\t\t\t       &treq->rmt_addr, ntohs(tcp_hdr(skb)->source));\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = tcp_v6_init_sequence(skb);\n\t}\nhave_isn:\n\ttcp_rsk(req)->snt_isn = isn;\n\n\tsecurity_inet_conn_request(sk, skb, req);\n\n\tif (tcp_v6_send_synack(sk, req,\n\t\t\t       (struct request_values *)&tmp_ext) ||\n\t    want_cookie)\n\t\tgoto drop_and_free;\n\n\tinet6_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\treturn 0; /* don't send reset */\n}\n\nstatic struct sock * tcp_v6_syn_recv_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *treq;\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst);\n\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\ttreq = inet6_rsk(req);\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, req);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &treq->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &treq->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &treq->loc_addr);\n\tnewsk->sk_bound_dev_if = treq->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (treq->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(treq->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(treq->pktopts);\n\t\ttreq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (opt) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_mtup_init(newsk);\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tif ((key = tcp_v6_md5_do_lookup(sk, &newnp->daddr)) != NULL) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\tchar *newkey = kmemdup(key->key, key->keylen, GFP_ATOMIC);\n\t\tif (newkey != NULL)\n\t\t\ttcp_v6_md5_do_add(newsk, &newnp->daddr,\n\t\t\t\t\t  newkey, key->keylen);\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tif (opt && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\nstatic __sum16 tcp_v6_checksum_init(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!tcp_v6_check(skb->len, &ipv6_hdr(skb)->saddr,\n\t\t\t\t  &ipv6_hdr(skb)->daddr, skb->csum)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = ~csum_unfold(tcp_v6_check(skb->len,\n\t\t\t\t\t      &ipv6_hdr(skb)->saddr,\n\t\t\t\t\t      &ipv6_hdr(skb)->daddr, 0));\n\n\tif (skb->len <= 76) {\n\t\treturn __skb_checksum_complete(skb);\n\t}\n\treturn 0;\n}\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp;\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, tcp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   tcp_v6_hnd_req and tcp_v6_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_do_rcv(sk, skb);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (tcp_v6_inbound_md5_hash (sk, skb))\n\t\tgoto discard;\n#endif\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t *\tsocket locking is here for SMP purposes as backlog rcv\n\t *\tis currently called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all)\n\t\topt_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\t\tif (tcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len))\n\t\t\tgoto reset;\n\t\tif (opt_skb)\n\t\t\tgoto ipv6_pktoptions;\n\t\treturn 0;\n\t}\n\n\tif (skb->len < tcp_hdrlen(skb) || tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v6_hnd_req(sk, skb);\n\t\tif (!nsk)\n\t\t\tgoto discard;\n\n\t\t/*\n\t\t * Queue it on the new socket if the new socket is active,\n\t\t * otherwise we just shortcircuit this and continue with\n\t\t * the new socket..\n\t\t */\n\t\tif(nsk != sk) {\n\t\t\tif (tcp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\tif (opt_skb)\n\t\t\t\t__kfree_skb(opt_skb);\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\tif (tcp_rcv_state_process(sk, skb, tcp_hdr(skb), skb->len))\n\t\tgoto reset;\n\tif (opt_skb)\n\t\tgoto ipv6_pktoptions;\n\treturn 0;\n\nreset:\n\ttcp_v6_send_reset(sk, skb);\ndiscard:\n\tif (opt_skb)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\ncsum_err:\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n\n\nipv6_pktoptions:\n\t/* Do you ask, what is it?\n\n\t   1. skb was enqueued by tcp.\n\t   2. skb is added to tail of read queue, rather than out of order.\n\t   3. socket is not in passive state.\n\t   4. Finally, it really contains options, which user wants to receive.\n\t */\n\ttp = tcp_sk(sk);\n\tif (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt &&\n\t    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\tif (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo)\n\t\t\tnp->mcast_oif = inet6_iif(opt_skb);\n\t\tif (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim)\n\t\t\tnp->mcast_hops = ipv6_hdr(opt_skb)->hop_limit;\n\t\tif (ipv6_opt_accepted(sk, opt_skb)) {\n\t\t\tskb_set_owner_r(opt_skb, sk);\n\t\t\topt_skb = xchg(&np->pktoptions, opt_skb);\n\t\t} else {\n\t\t\t__kfree_skb(opt_skb);\n\t\t\topt_skb = xchg(&np->pktoptions, NULL);\n\t\t}\n\t}\n\n\tkfree_skb(opt_skb);\n\treturn 0;\n}\n\nstatic int tcp_v6_rcv(struct sk_buff *skb)\n{\n\tstruct tcphdr *th;\n\tconst struct ipv6hdr *hdr;\n\tstruct sock *sk;\n\tint ret;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/*\n\t *\tCount it even if it's bad.\n\t */\n\tTCP_INC_STATS_BH(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr)/4)\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff*4))\n\t\tgoto discard_it;\n\n\tif (!skb_csum_unnecessary(skb) && tcp_v6_checksum_init(skb))\n\t\tgoto bad_packet;\n\n\tth = tcp_hdr(skb);\n\thdr = ipv6_hdr(skb);\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff*4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->when = 0;\n\tTCP_SKB_CB(skb)->flags = ipv6_get_dsfield(hdr);\n\tTCP_SKB_CB(skb)->sacked = 0;\n\n\tsk = __inet6_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (hdr->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tbh_lock_sock_nested(sk);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n#ifdef CONFIG_NET_DMA\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (!tp->ucopy.dma_chan && tp->ucopy.pinned_list)\n\t\t\ttp->ucopy.dma_chan = dma_find_channel(DMA_MEMCPY);\n\t\tif (tp->ucopy.dma_chan)\n\t\t\tret = tcp_v6_do_rcv(sk, skb);\n\t\telse\n#endif\n\t\t{\n\t\t\tif (!tcp_prequeue(sk, skb))\n\t\t\t\tret = tcp_v6_do_rcv(sk, skb);\n\t\t}\n\t} else if (unlikely(sk_add_backlog(sk, skb))) {\n\t\tbh_unlock_sock(sk);\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\n\tsock_put(sk);\n\treturn ret ? -1 : 0;\n\nno_tcp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\tif (skb->len < (th->doff<<2) || tcp_checksum_complete(skb)) {\nbad_packet:\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v6_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\n\t/*\n\t *\tDiscard frame\n\t */\n\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tif (skb->len < (th->doff<<2) || tcp_checksum_complete(skb)) {\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN:\n\t{\n\t\tstruct sock *sk2;\n\n\t\tsk2 = inet6_lookup_listener(dev_net(skb->dev), &tcp_hashinfo,\n\t\t\t\t\t    &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t    ntohs(th->dest), inet6_iif(skb));\n\t\tif (sk2 != NULL) {\n\t\t\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\t\t\tinet_twsk_deschedule(tw, &tcp_death_row);\n\t\t\tinet_twsk_put(tw);\n\t\t\tsk = sk2;\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v6_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\tgoto no_tcp_socket;\n\tcase TCP_TW_SUCCESS:;\n\t}\n\tgoto discard_it;\n}\n\nstatic struct inet_peer *tcp_v6_get_peer(struct sock *sk, bool *release_it)\n{\n\tstruct rt6_info *rt = (struct rt6_info *) __sk_dst_get(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_peer *peer;\n\n\tif (!rt ||\n\t    !ipv6_addr_equal(&np->daddr, &rt->rt6i_dst.addr)) {\n\t\tpeer = inet_getpeer_v6(&np->daddr, 1);\n\t\t*release_it = true;\n\t} else {\n\t\tif (!rt->rt6i_peer)\n\t\t\trt6_bind_peer(rt, 1);\n\t\tpeer = rt->rt6i_peer;\n\t\t*release_it = false;\n\t}\n\n\treturn peer;\n}\n\nstatic void *tcp_v6_tw_get_peer(struct sock *sk)\n{\n\tstruct inet6_timewait_sock *tw6 = inet6_twsk(sk);\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\n\tif (tw->tw_family == AF_INET)\n\t\treturn tcp_v4_tw_get_peer(sk);\n\n\treturn inet_getpeer_v6(&tw6->tw_v6_daddr, 1);\n}\n\nstatic struct timewait_sock_ops tcp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp6_timewait_sock),\n\t.twsk_unique\t= tcp_twsk_unique,\n\t.twsk_destructor= tcp_twsk_destructor,\n\t.twsk_getpeer\t= tcp_v6_tw_get_peer,\n};\n\nstatic const struct inet_connection_sock_af_ops ipv6_specific = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = tcp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.get_peer\t   = tcp_v6_get_peer,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {\n\t.md5_lookup\t=\ttcp_v6_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n\t.md5_add\t=\ttcp_v6_md5_add_func,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n};\n#endif\n\n/*\n *\tTCP over IPv4 via INET6 API\n */\n\nstatic const struct inet_connection_sock_af_ops ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.get_peer\t   = tcp_v4_get_peer,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {\n\t.md5_lookup\t=\ttcp_v4_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n\t.md5_add\t=\ttcp_v6_md5_add_func,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n};\n#endif\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int tcp_v6_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tskb_queue_head_init(&tp->out_of_order_queue);\n\ttcp_init_xmit_timers(sk);\n\ttcp_prequeue_init(tp);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ttp->mdev = TCP_TIMEOUT_INIT;\n\n\t/* So many TCP implementations out there (incorrectly) count the\n\t * initial SYN frame in their delayed-ACK and congestion control\n\t * algorithms that we must have the following bandaid to talk\n\t * efficiently to them.  -DaveM\n\t */\n\ttp->snd_cwnd = 2;\n\n\t/* See draft-stevens-tcpca-spec-01 for discussion of the\n\t * initialization of these values.\n\t */\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = sysctl_tcp_reordering;\n\n\tsk->sk_state = TCP_CLOSE;\n\n\ticsk->icsk_af_ops = &ipv6_specific;\n\ticsk->icsk_ca_ops = &tcp_init_congestion_ops;\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\n\t/* TCP Cookie Transactions */\n\tif (sysctl_tcp_cookie_size > 0) {\n\t\t/* Default, cookies without s_data_payload. */\n\t\ttp->cookie_values =\n\t\t\tkzalloc(sizeof(*tp->cookie_values),\n\t\t\t\tsk->sk_allocation);\n\t\tif (tp->cookie_values != NULL)\n\t\t\tkref_init(&tp->cookie_values->kref);\n\t}\n\t/* Presumed zeroed, in order of appearance:\n\t *\tcookie_in_always, cookie_out_never,\n\t *\ts_data_constant, s_data_in, s_data_out\n\t */\n\tsk->sk_sndbuf = sysctl_tcp_wmem[1];\n\tsk->sk_rcvbuf = sysctl_tcp_rmem[1];\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&tcp_sockets_allocated);\n\tlocal_bh_enable();\n\n\treturn 0;\n}\n\nstatic void tcp_v6_destroy_sock(struct sock *sk)\n{\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Clean up the MD5 key list */\n\tif (tcp_sk(sk)->md5sig_info)\n\t\ttcp_v6_clear_md5_list(sk);\n#endif\n\ttcp_v4_destroy_sock(sk);\n\tinet6_destroy_sock(sk);\n}\n\n#ifdef CONFIG_PROC_FS\n/* Proc filesystem TCPv6 sock list dumping. */\nstatic void get_openreq6(struct seq_file *seq,\n\t\t\t struct sock *sk, struct request_sock *req, int i, int uid)\n{\n\tint ttd = req->expires - jiffies;\n\tconst struct in6_addr *src = &inet6_rsk(req)->loc_addr;\n\tconst struct in6_addr *dest = &inet6_rsk(req)->rmt_addr;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3],\n\t\t   ntohs(inet_rsk(req)->loc_port),\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3],\n\t\t   ntohs(inet_rsk(req)->rmt_port),\n\t\t   TCP_SYN_RECV,\n\t\t   0,0, /* could print option size, but that is af dependent. */\n\t\t   1,   /* timers active (only the expire timer) */\n\t\t   jiffies_to_clock_t(ttd),\n\t\t   req->retrans,\n\t\t   uid,\n\t\t   0,  /* non standard timer */\n\t\t   0, /* open_requests have no inode */\n\t\t   0, req);\n}\n\nstatic void get_tcp6_sock(struct seq_file *seq, struct sock *sp, int i)\n{\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\tint timer_active;\n\tunsigned long timer_expires;\n\tstruct inet_sock *inet = inet_sk(sp);\n\tstruct tcp_sock *tp = tcp_sk(sp);\n\tconst struct inet_connection_sock *icsk = inet_csk(sp);\n\tstruct ipv6_pinfo *np = inet6_sk(sp);\n\n\tdest  = &np->daddr;\n\tsrc   = &np->rcv_saddr;\n\tdestp = ntohs(inet->inet_dport);\n\tsrcp  = ntohs(inet->inet_sport);\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (timer_pending(&sp->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sp->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %p %lu %lu %u %u %d\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   sp->sk_state,\n\t\t   tp->write_seq-tp->snd_una,\n\t\t   (sp->sk_state == TCP_LISTEN) ? sp->sk_ack_backlog : (tp->rcv_nxt - tp->copied_seq),\n\t\t   timer_active,\n\t\t   jiffies_to_clock_t(timer_expires - jiffies),\n\t\t   icsk->icsk_retransmits,\n\t\t   sock_i_uid(sp),\n\t\t   icsk->icsk_probes_out,\n\t\t   sock_i_ino(sp),\n\t\t   atomic_read(&sp->sk_refcnt), sp,\n\t\t   jiffies_to_clock_t(icsk->icsk_rto),\n\t\t   jiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t   (icsk->icsk_ack.quick << 1 ) | icsk->icsk_ack.pingpong,\n\t\t   tp->snd_cwnd,\n\t\t   tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh\n\t\t   );\n}\n\nstatic void get_timewait6_sock(struct seq_file *seq,\n\t\t\t       struct inet_timewait_sock *tw, int i)\n{\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\tstruct inet6_timewait_sock *tw6 = inet6_twsk((struct sock *)tw);\n\tint ttd = tw->tw_ttd - jiffies;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tdest = &tw6->tw_v6_daddr;\n\tsrc  = &tw6->tw_v6_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %p\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   tw->tw_substate, 0, 0,\n\t\t   3, jiffies_to_clock_t(ttd), 0, 0, 0, 0,\n\t\t   atomic_read(&tw->tw_refcnt), tw);\n}\n\nstatic int tcp6_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"  sl  \"\n\t\t\t \"local_address                         \"\n\t\t\t \"remote_address                        \"\n\t\t\t \"st tx_queue rx_queue tr tm->when retrnsmt\"\n\t\t\t \"   uid  timeout inode\\n\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tswitch (st->state) {\n\tcase TCP_SEQ_STATE_LISTENING:\n\tcase TCP_SEQ_STATE_ESTABLISHED:\n\t\tget_tcp6_sock(seq, v, st->num);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_OPENREQ:\n\t\tget_openreq6(seq, st->syn_wait_sk, v, st->num, st->uid);\n\t\tbreak;\n\tcase TCP_SEQ_STATE_TIME_WAIT:\n\t\tget_timewait6_sock(seq, v, st->num);\n\t\tbreak;\n\t}\nout:\n\treturn 0;\n}\n\nstatic struct tcp_seq_afinfo tcp6_seq_afinfo = {\n\t.name\t\t= \"tcp6\",\n\t.family\t\t= AF_INET6,\n\t.seq_fops\t= {\n\t\t.owner\t\t= THIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= tcp6_seq_show,\n\t},\n};\n\nint __net_init tcp6_proc_init(struct net *net)\n{\n\treturn tcp_proc_register(net, &tcp6_seq_afinfo);\n}\n\nvoid tcp6_proc_exit(struct net *net)\n{\n\ttcp_proc_unregister(net, &tcp6_seq_afinfo);\n}\n#endif\n\nstruct proto tcpv6_prot = {\n\t.name\t\t\t= \"TCPv6\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.connect\t\t= tcp_v6_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v6_init_sock,\n\t.destroy\t\t= tcp_v6_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.sendpage\t\t= tcp_sendpage,\n\t.backlog_rcv\t\t= tcp_v6_do_rcv,\n\t.hash\t\t\t= tcp_v6_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\t.memory_allocated\t= &tcp_memory_allocated,\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.orphan_count\t\t= &tcp_orphan_count,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem\t\t= sysctl_tcp_wmem,\n\t.sysctl_rmem\t\t= sysctl_tcp_rmem,\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp6_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.twsk_prot\t\t= &tcp6_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp6_request_sock_ops,\n\t.h.hashinfo\t\t= &tcp_hashinfo,\n\t.no_autobind\t\t= true,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_tcp_setsockopt,\n\t.compat_getsockopt\t= compat_tcp_getsockopt,\n#endif\n};\n\nstatic const struct inet6_protocol tcpv6_protocol = {\n\t.handler\t=\ttcp_v6_rcv,\n\t.err_handler\t=\ttcp_v6_err,\n\t.gso_send_check\t=\ttcp_v6_gso_send_check,\n\t.gso_segment\t=\ttcp_tso_segment,\n\t.gro_receive\t=\ttcp6_gro_receive,\n\t.gro_complete\t=\ttcp6_gro_complete,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\nstatic struct inet_protosw tcpv6_protosw = {\n\t.type\t\t=\tSOCK_STREAM,\n\t.protocol\t=\tIPPROTO_TCP,\n\t.prot\t\t=\t&tcpv6_prot,\n\t.ops\t\t=\t&inet6_stream_ops,\n\t.no_check\t=\t0,\n\t.flags\t\t=\tINET_PROTOSW_PERMANENT |\n\t\t\t\tINET_PROTOSW_ICSK,\n};\n\nstatic int __net_init tcpv6_net_init(struct net *net)\n{\n\treturn inet_ctl_sock_create(&net->ipv6.tcp_sk, PF_INET6,\n\t\t\t\t    SOCK_RAW, IPPROTO_TCP, net);\n}\n\nstatic void __net_exit tcpv6_net_exit(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->ipv6.tcp_sk);\n}\n\nstatic void __net_exit tcpv6_net_exit_batch(struct list_head *net_exit_list)\n{\n\tinet_twsk_purge(&tcp_hashinfo, &tcp_death_row, AF_INET6);\n}\n\nstatic struct pernet_operations tcpv6_net_ops = {\n\t.init\t    = tcpv6_net_init,\n\t.exit\t    = tcpv6_net_exit,\n\t.exit_batch = tcpv6_net_exit_batch,\n};\n\nint __init tcpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&tcpv6_protocol, IPPROTO_TCP);\n\tif (ret)\n\t\tgoto out;\n\n\t/* register inet6 protocol */\n\tret = inet6_register_protosw(&tcpv6_protosw);\n\tif (ret)\n\t\tgoto out_tcpv6_protocol;\n\n\tret = register_pernet_subsys(&tcpv6_net_ops);\n\tif (ret)\n\t\tgoto out_tcpv6_protosw;\nout:\n\treturn ret;\n\nout_tcpv6_protocol:\n\tinet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);\nout_tcpv6_protosw:\n\tinet6_unregister_protosw(&tcpv6_protosw);\n\tgoto out;\n}\n\nvoid tcpv6_exit(void)\n{\n\tunregister_pernet_subsys(&tcpv6_net_ops);\n\tinet6_unregister_protosw(&tcpv6_protosw);\n\tinet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);\n}\n", "/*\n * L2TPv3 IP encapsulation support\n *\n * Copyright (c) 2008,2009,2010 Katalix Systems Ltd\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/icmp.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/random.h>\n#include <linux/socket.h>\n#include <linux/l2tp.h>\n#include <linux/in.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/tcp_states.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n\n#include \"l2tp_core.h\"\n\nstruct l2tp_ip_sock {\n\t/* inet_sock has to be the first member of l2tp_ip_sock */\n\tstruct inet_sock\tinet;\n\n\t__u32\t\t\tconn_id;\n\t__u32\t\t\tpeer_conn_id;\n\n\t__u64\t\t\ttx_packets;\n\t__u64\t\t\ttx_bytes;\n\t__u64\t\t\ttx_errors;\n\t__u64\t\t\trx_packets;\n\t__u64\t\t\trx_bytes;\n\t__u64\t\t\trx_errors;\n};\n\nstatic DEFINE_RWLOCK(l2tp_ip_lock);\nstatic struct hlist_head l2tp_ip_table;\nstatic struct hlist_head l2tp_ip_bind_table;\n\nstatic inline struct l2tp_ip_sock *l2tp_ip_sk(const struct sock *sk)\n{\n\treturn (struct l2tp_ip_sock *)sk;\n}\n\nstatic struct sock *__l2tp_ip_bind_lookup(struct net *net, __be32 laddr, int dif, u32 tunnel_id)\n{\n\tstruct hlist_node *node;\n\tstruct sock *sk;\n\n\tsk_for_each_bound(sk, node, &l2tp_ip_bind_table) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct l2tp_ip_sock *l2tp = l2tp_ip_sk(sk);\n\n\t\tif (l2tp == NULL)\n\t\t\tcontinue;\n\n\t\tif ((l2tp->conn_id == tunnel_id) &&\n\t\t    net_eq(sock_net(sk), net) &&\n\t\t    !(inet->inet_rcv_saddr && inet->inet_rcv_saddr != laddr) &&\n\t\t    !(sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\t\tgoto found;\n\t}\n\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic inline struct sock *l2tp_ip_bind_lookup(struct net *net, __be32 laddr, int dif, u32 tunnel_id)\n{\n\tstruct sock *sk = __l2tp_ip_bind_lookup(net, laddr, dif, tunnel_id);\n\tif (sk)\n\t\tsock_hold(sk);\n\n\treturn sk;\n}\n\n/* When processing receive frames, there are two cases to\n * consider. Data frames consist of a non-zero session-id and an\n * optional cookie. Control frames consist of a regular L2TP header\n * preceded by 32-bits of zeros.\n *\n * L2TPv3 Session Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                           Session ID                          |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Cookie (optional, maximum 64 bits)...\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *                                                                 |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * L2TPv3 Control Message Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                      (32 bits of zeros)                       |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |T|L|x|x|S|x|x|x|x|x|x|x|  Ver  |             Length            |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                     Control Connection ID                     |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Ns              |               Nr              |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * All control frames are passed to userspace.\n */\nstatic int l2tp_ip_recv(struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tu32 session_id;\n\tu32 tunnel_id;\n\tunsigned char *ptr, *optr;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel = NULL;\n\tint length;\n\tint offset;\n\n\t/* Point to L2TP header */\n\toptr = ptr = skb->data;\n\n\tif (!pskb_may_pull(skb, 4))\n\t\tgoto discard;\n\n\tsession_id = ntohl(*((__be32 *) ptr));\n\tptr += 4;\n\n\t/* RFC3931: L2TP/IP packets have the first 4 bytes containing\n\t * the session_id. If it is 0, the packet is a L2TP control\n\t * frame and the session_id value can be discarded.\n\t */\n\tif (session_id == 0) {\n\t\t__skb_pull(skb, 4);\n\t\tgoto pass_up;\n\t}\n\n\t/* Ok, this is a data packet. Lookup the session. */\n\tsession = l2tp_session_find(&init_net, NULL, session_id);\n\tif (session == NULL)\n\t\tgoto discard;\n\n\ttunnel = session->tunnel;\n\tif (tunnel == NULL)\n\t\tgoto discard;\n\n\t/* Trace packet contents, if enabled */\n\tif (tunnel->debug & L2TP_MSG_DATA) {\n\t\tlength = min(32u, skb->len);\n\t\tif (!pskb_may_pull(skb, length))\n\t\t\tgoto discard;\n\n\t\tprintk(KERN_DEBUG \"%s: ip recv: \", tunnel->name);\n\n\t\toffset = 0;\n\t\tdo {\n\t\t\tprintk(\" %02X\", ptr[offset]);\n\t\t} while (++offset < length);\n\n\t\tprintk(\"\\n\");\n\t}\n\n\tl2tp_recv_common(session, skb, ptr, optr, 0, skb->len, tunnel->recv_payload_hook);\n\n\treturn 0;\n\npass_up:\n\t/* Get the tunnel_id from the L2TP header */\n\tif (!pskb_may_pull(skb, 12))\n\t\tgoto discard;\n\n\tif ((skb->data[0] & 0xc0) != 0xc0)\n\t\tgoto discard;\n\n\ttunnel_id = ntohl(*(__be32 *) &skb->data[4]);\n\ttunnel = l2tp_tunnel_find(&init_net, tunnel_id);\n\tif (tunnel != NULL)\n\t\tsk = tunnel->sock;\n\telse {\n\t\tstruct iphdr *iph = (struct iphdr *) skb_network_header(skb);\n\n\t\tread_lock_bh(&l2tp_ip_lock);\n\t\tsk = __l2tp_ip_bind_lookup(&init_net, iph->daddr, 0, tunnel_id);\n\t\tread_unlock_bh(&l2tp_ip_lock);\n\t}\n\n\tif (sk == NULL)\n\t\tgoto discard;\n\n\tsock_hold(sk);\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_put;\n\n\tnf_reset(skb);\n\n\treturn sk_receive_skb(sk, skb, 1);\n\ndiscard_put:\n\tsock_put(sk);\n\ndiscard:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int l2tp_ip_open(struct sock *sk)\n{\n\t/* Prevent autobind. We don't have ports. */\n\tinet_sk(sk)->inet_num = IPPROTO_L2TP;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_node(sk, &l2tp_ip_table);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\n\treturn 0;\n}\n\nstatic void l2tp_ip_close(struct sock *sk, long timeout)\n{\n\twrite_lock_bh(&l2tp_ip_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\thlist_del_init(&sk->sk_node);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tsk_common_release(sk);\n}\n\nstatic void l2tp_ip_destroy_sock(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(&sk->sk_write_queue)) != NULL)\n\t\tkfree_skb(skb);\n\n\tsk_refcnt_debug_dec(sk);\n}\n\nstatic int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tint ret = -EINVAL;\n\tint chk_addr_ret;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(&init_net, addr->l2tp_addr.s_addr, sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(&init_net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}\n\nstatic int l2tp_ip_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_l2tpip *lsa = (struct sockaddr_l2tpip *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 saddr;\n\tint oif, rc;\n\n\trc = -EINVAL;\n\tif (addr_len < sizeof(*lsa))\n\t\tgoto out;\n\n\trc = -EAFNOSUPPORT;\n\tif (lsa->l2tp_family != AF_INET)\n\t\tgoto out;\n\n\tsk_dst_reset(sk);\n\n\toif = sk->sk_bound_dev_if;\n\tsaddr = inet->inet_saddr;\n\n\trc = -EINVAL;\n\tif (ipv4_is_multicast(lsa->l2tp_addr.s_addr))\n\t\tgoto out;\n\n\trt = ip_route_connect(&fl4, lsa->l2tp_addr.s_addr, saddr,\n\t\t\t      RT_CONN_FLAGS(sk), oif,\n\t\t\t      IPPROTO_L2TP,\n\t\t\t      0, 0, sk, true);\n\tif (IS_ERR(rt)) {\n\t\trc = PTR_ERR(rt);\n\t\tif (rc == -ENETUNREACH)\n\t\t\tIP_INC_STATS_BH(&init_net, IPSTATS_MIB_OUTNOROUTES);\n\t\tgoto out;\n\t}\n\n\trc = -ENETUNREACH;\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\tgoto out;\n\t}\n\n\tl2tp_ip_sk(sk)->peer_conn_id = lsa->l2tp_conn_id;\n\n\tif (!inet->inet_saddr)\n\t\tinet->inet_saddr = rt->rt_src;\n\tif (!inet->inet_rcv_saddr)\n\t\tinet->inet_rcv_saddr = rt->rt_src;\n\tinet->inet_daddr = rt->rt_dst;\n\tsk->sk_state = TCP_ESTABLISHED;\n\tinet->inet_id = jiffies;\n\n\tsk_dst_set(sk, &rt->dst);\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\n\trc = 0;\nout:\n\treturn rc;\n}\n\nstatic int l2tp_ip_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t   int *uaddr_len, int peer)\n{\n\tstruct sock *sk\t\t= sock->sk;\n\tstruct inet_sock *inet\t= inet_sk(sk);\n\tstruct l2tp_ip_sock *lsk = l2tp_ip_sk(sk);\n\tstruct sockaddr_l2tpip *lsa = (struct sockaddr_l2tpip *)uaddr;\n\n\tmemset(lsa, 0, sizeof(*lsa));\n\tlsa->l2tp_family = AF_INET;\n\tif (peer) {\n\t\tif (!inet->inet_dport)\n\t\t\treturn -ENOTCONN;\n\t\tlsa->l2tp_conn_id = lsk->peer_conn_id;\n\t\tlsa->l2tp_addr.s_addr = inet->inet_daddr;\n\t} else {\n\t\t__be32 addr = inet->inet_rcv_saddr;\n\t\tif (!addr)\n\t\t\taddr = inet->inet_saddr;\n\t\tlsa->l2tp_conn_id = lsk->conn_id;\n\t\tlsa->l2tp_addr.s_addr = addr;\n\t}\n\t*uaddr_len = sizeof(*lsa);\n\treturn 0;\n}\n\nstatic int l2tp_ip_backlog_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tnf_reset(skb);\n\n\t/* Charge it to the socket, dropping if the queue is full. */\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0)\n\t\tgoto drop;\n\n\treturn 0;\n\ndrop:\n\tIP_INC_STATS(&init_net, IPSTATS_MIB_INDISCARDS);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\n/* Userspace will call sendmsg() on the tunnel socket to send L2TP\n * control frames.\n */\nstatic int l2tp_ip_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct sk_buff *skb;\n\tint rc;\n\tstruct l2tp_ip_sock *lsa = l2tp_ip_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = NULL;\n\tint connected = 0;\n\t__be32 daddr;\n\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\treturn -ENOTCONN;\n\n\t/* Get and verify the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_l2tpip *lip = (struct sockaddr_l2tpip *) msg->msg_name;\n\t\tif (msg->msg_namelen < sizeof(*lip))\n\t\t\treturn -EINVAL;\n\n\t\tif (lip->l2tp_family != AF_INET) {\n\t\t\tif (lip->l2tp_family != AF_UNSPEC)\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t}\n\n\t\tdaddr = lip->l2tp_addr.s_addr;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = inet->inet_daddr;\n\t\tconnected = 1;\n\t}\n\n\t/* Allocate a socket buffer */\n\trc = -ENOMEM;\n\tskb = sock_wmalloc(sk, 2 + NET_SKB_PAD + sizeof(struct iphdr) +\n\t\t\t   4 + len, 0, GFP_KERNEL);\n\tif (!skb)\n\t\tgoto error;\n\n\t/* Reserve space for headers, putting IP header on 4-byte boundary. */\n\tskb_reserve(skb, 2 + NET_SKB_PAD);\n\tskb_reset_network_header(skb);\n\tskb_reserve(skb, sizeof(struct iphdr));\n\tskb_reset_transport_header(skb);\n\n\t/* Insert 0 session_id */\n\t*((__be32 *) skb_put(skb, 4)) = 0;\n\n\t/* Copy user data into skb */\n\trc = memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len);\n\tif (rc < 0) {\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tif (connected)\n\t\trt = (struct rtable *) __sk_dst_check(sk, 0);\n\n\tif (rt == NULL) {\n\t\tstruct ip_options_rcu *inet_opt;\n\n\t\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t\t     sock_owned_by_user(sk));\n\n\t\t/* Use correct destination address if we have options. */\n\t\tif (inet_opt && inet_opt->opt.srr)\n\t\t\tdaddr = inet_opt->opt.faddr;\n\n\t\t/* If this fails, retransmit mechanism of transport layer will\n\t\t * keep trying until route appears or the connection times\n\t\t * itself out.\n\t\t */\n\t\trt = ip_route_output_ports(sock_net(sk), sk,\n\t\t\t\t\t   daddr, inet->inet_saddr,\n\t\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto no_route;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t}\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\n\t/* Queue the packet to IP for output */\n\trc = ip_queue_xmit(skb);\n\nerror:\n\t/* Update stats */\n\tif (rc >= 0) {\n\t\tlsa->tx_packets++;\n\t\tlsa->tx_bytes += len;\n\t\trc = len;\n\t} else {\n\t\tlsa->tx_errors++;\n\t}\n\n\treturn rc;\n\nno_route:\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EHOSTUNREACH;\n}\n\nstatic int l2tp_ip_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,\n\t\t\t   size_t len, int noblock, int flags, int *addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct l2tp_ip_sock *lsk = l2tp_ip_sk(sk);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*sin);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (sin) {\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n\t\tsin->sin_port = 0;\n\t\tmemset(&sin->sin_zero, 0, sizeof(sin->sin_zero));\n\t}\n\tif (inet->cmsg_flags)\n\t\tip_cmsg_recv(msg, skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\tif (err) {\n\t\tlsk->rx_errors++;\n\t\treturn err;\n\t}\n\n\tlsk->rx_packets++;\n\tlsk->rx_bytes += copied;\n\n\treturn copied;\n}\n\nstatic struct proto l2tp_ip_prot = {\n\t.name\t\t   = \"L2TP/IP\",\n\t.owner\t\t   = THIS_MODULE,\n\t.init\t\t   = l2tp_ip_open,\n\t.close\t\t   = l2tp_ip_close,\n\t.bind\t\t   = l2tp_ip_bind,\n\t.connect\t   = l2tp_ip_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = l2tp_ip_destroy_sock,\n\t.setsockopt\t   = ip_setsockopt,\n\t.getsockopt\t   = ip_getsockopt,\n\t.sendmsg\t   = l2tp_ip_sendmsg,\n\t.recvmsg\t   = l2tp_ip_recvmsg,\n\t.backlog_rcv\t   = l2tp_ip_backlog_recv,\n\t.hash\t\t   = inet_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.obj_size\t   = sizeof(struct l2tp_ip_sock),\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ip_setsockopt,\n\t.compat_getsockopt = compat_ip_getsockopt,\n#endif\n};\n\nstatic const struct proto_ops l2tp_ip_ops = {\n\t.family\t\t   = PF_INET,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet_release,\n\t.bind\t\t   = inet_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = l2tp_ip_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw l2tp_ip_protosw = {\n\t.type\t\t= SOCK_DGRAM,\n\t.protocol\t= IPPROTO_L2TP,\n\t.prot\t\t= &l2tp_ip_prot,\n\t.ops\t\t= &l2tp_ip_ops,\n\t.no_check\t= 0,\n};\n\nstatic struct net_protocol l2tp_ip_protocol __read_mostly = {\n\t.handler\t= l2tp_ip_recv,\n};\n\nstatic int __init l2tp_ip_init(void)\n{\n\tint err;\n\n\tprintk(KERN_INFO \"L2TP IP encapsulation support (L2TPv3)\\n\");\n\n\terr = proto_register(&l2tp_ip_prot, 1);\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet_add_protocol(&l2tp_ip_protocol, IPPROTO_L2TP);\n\tif (err)\n\t\tgoto out1;\n\n\tinet_register_protosw(&l2tp_ip_protosw);\n\treturn 0;\n\nout1:\n\tproto_unregister(&l2tp_ip_prot);\nout:\n\treturn err;\n}\n\nstatic void __exit l2tp_ip_exit(void)\n{\n\tinet_unregister_protosw(&l2tp_ip_protosw);\n\tinet_del_protocol(&l2tp_ip_protocol, IPPROTO_L2TP);\n\tproto_unregister(&l2tp_ip_prot);\n}\n\nmodule_init(l2tp_ip_init);\nmodule_exit(l2tp_ip_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"James Chapman <jchapman@katalix.com>\");\nMODULE_DESCRIPTION(\"L2TP over IP\");\nMODULE_VERSION(\"1.0\");\n\n/* Use the value of SOCK_DGRAM (2) directory, because __stringify does't like\n * enums\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET, 2, IPPROTO_L2TP);\n"], "filenames": ["include/net/inet_sock.h", "include/net/ip.h", "net/dccp/ipv4.c", "net/dccp/ipv6.c", "net/ipv4/af_inet.c", "net/ipv4/cipso_ipv4.c", "net/ipv4/icmp.c", "net/ipv4/inet_connection_sock.c", "net/ipv4/ip_options.c", "net/ipv4/ip_output.c", "net/ipv4/ip_sockglue.c", "net/ipv4/raw.c", "net/ipv4/syncookies.c", "net/ipv4/tcp_ipv4.c", "net/ipv4/udp.c", "net/ipv6/tcp_ipv6.c", "net/l2tp/l2tp_ip.c"], "buggy_code_start_loc": [60, 55, 50, 576, 156, 1859, 111, 357, 39, 143, 453, 462, 324, 156, 806, 1472, 419], "buggy_code_end_loc": [144, 427, 409, 577, 1161, 2195, 617, 371, 546, 1474, 1090, 537, 328, 1441, 896, 1473, 477], "fixing_code_start_loc": [60, 55, 51, 576, 156, 1860, 111, 357, 39, 143, 454, 463, 324, 157, 807, 1472, 418], "fixing_code_end_loc": [152, 428, 413, 577, 1168, 2208, 616, 371, 544, 1472, 1103, 548, 328, 1447, 907, 1473, 481], "type": "CWE-362", "message": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic.", "other": {"cve": {"id": "CVE-2012-3552", "sourceIdentifier": "secalert@redhat.com", "published": "2012-10-03T11:02:57.220", "lastModified": "2023-02-13T00:25:52.087", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Race condition in the IP implementation in the Linux kernel before 3.0 might allow remote attackers to cause a denial of service (slab corruption and system crash) by sending packets to an application that sets socket options during the handling of network traffic."}, {"lang": "es", "value": "La aplicaci\u00f3n IP en el kernel de Linux antes de v3.0 podr\u00eda permitir a atacantes remotos provocar una denegaci\u00f3n de servicio mediante el env\u00edo de paquetes a una aplicaci\u00f3n que configura las opciones de socket durante el manejo de tr\u00e1fico de red."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.1}, "baseSeverity": "HIGH", "exploitabilityScore": 8.6, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.0", "matchCriteriaId": "E0135A6D-9FB7-4E1B-B471-914E37494942"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_eus:6.2:*:*:*:*:*:*:*", "matchCriteriaId": "C0554C89-3716-49F3-BFAE-E008D5E4E29C"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=f6d8bd051c391c1c0458a30b2a7abcd939329259", "source": "secalert@redhat.com"}, {"url": "http://rhn.redhat.com/errata/RHSA-2012-1540.html", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2012/08/31/11", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=853465", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/f6d8bd051c391c1c0458a30b2a7abcd939329259", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/f6d8bd051c391c1c0458a30b2a7abcd939329259"}}