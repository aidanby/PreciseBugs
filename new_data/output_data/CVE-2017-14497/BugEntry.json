{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPACKET - implements raw packet sockets.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() now used correctly\n *\t\tAlan Cox\t:\tnew skbuff lists, look ma no backlogs!\n *\t\tAlan Cox\t:\ttidied skbuff lists.\n *\t\tAlan Cox\t:\tNow uses generic datagram routines I\n *\t\t\t\t\tadded. Also fixed the peek/read crash\n *\t\t\t\t\tfrom all old Linux datagram code.\n *\t\tAlan Cox\t:\tUses the improved datagram code.\n *\t\tAlan Cox\t:\tAdded NULL's for socket options.\n *\t\tAlan Cox\t:\tRe-commented the code.\n *\t\tAlan Cox\t:\tUse new kernel side addressing\n *\t\tRob Janssen\t:\tCorrect MTU usage.\n *\t\tDave Platt\t:\tCounter leaks caused by incorrect\n *\t\t\t\t\tinterrupt locking and some slightly\n *\t\t\t\t\tdubious gcc output. Can you read\n *\t\t\t\t\tcompiler: it said _VOLATILE_\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tNew buffers. Use sk->mac.raw.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg support.\n *\t\tAlan Cox\t:\tProtocol setting support\n *\tAlexey Kuznetsov\t:\tUntied from IPv4 stack.\n *\tCyrus Durgin\t\t:\tFixed kerneld for kmod.\n *\tMichal Ostrowski        :       Module initialization cleanup.\n *         Ulises Alonso        :       Frame number limit removal and\n *                                      packet_set_ring memory leak.\n *\t\tEric Biederman\t:\tAllow for > 8 byte hardware addresses.\n *\t\t\t\t\tThe convention is that longer addresses\n *\t\t\t\t\twill simply extend the hardware address\n *\t\t\t\t\tbyte arrays at the end of sockaddr_ll\n *\t\t\t\t\tand packet_mreq.\n *\t\tJohann Baudy\t:\tAdded TX RING.\n *\t\tChetan Loke\t:\tImplemented TPACKET_V3 block abstraction\n *\t\t\t\t\tlayer.\n *\t\t\t\t\tCopyright (C) 2011, <lokec@ccs.neu.edu>\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/wireless.h>\n#include <linux/kernel.h>\n#include <linux/kmod.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <asm/page.h>\n#include <asm/cacheflush.h>\n#include <asm/io.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/mutex.h>\n#include <linux/if_vlan.h>\n#include <linux/virtio_net.h>\n#include <linux/errqueue.h>\n#include <linux/net_tstamp.h>\n#include <linux/percpu.h>\n#ifdef CONFIG_INET\n#include <net/inet_common.h>\n#endif\n#include <linux/bpf.h>\n#include <net/compat.h>\n\n#include \"internal.h\"\n\n/*\n   Assumptions:\n   - if device has no dev->hard_header routine, it adds and removes ll header\n     inside itself. In this case ll header is invisible outside of device,\n     but higher levels still should reserve dev->hard_header_len.\n     Some devices are enough clever to reallocate skb, when header\n     will not fit to reserved space (tunnel), another ones are silly\n     (PPP).\n   - packet socket receives packets with pulled ll header,\n     so that SOCK_RAW should push it back.\n\nOn receive:\n-----------\n\nIncoming, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> data\n\nOutgoing, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> ll header\n\nIncoming, dev->hard_header==NULL\n   mac_header -> UNKNOWN position. It is very likely, that it points to ll\n\t\t header.  PPP makes it, that is wrong, because introduce\n\t\t assymetry between rx and tx paths.\n   data       -> data\n\nOutgoing, dev->hard_header==NULL\n   mac_header -> data. ll header is still not built!\n   data       -> data\n\nResume\n  If dev->hard_header==NULL we are unlikely to restore sensible ll header.\n\n\nOn transmit:\n------------\n\ndev->hard_header != NULL\n   mac_header -> ll header\n   data       -> ll header\n\ndev->hard_header == NULL (ll header is added by device, we cannot control it)\n   mac_header -> data\n   data       -> data\n\n   We should set nh.raw on output to correct posistion,\n   packet classifier depends on it.\n */\n\n/* Private packet socket structures. */\n\n/* identical to struct packet_mreq except it has\n * a longer address field.\n */\nstruct packet_mreq_max {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[MAX_ADDR_LEN];\n};\n\nunion tpacket_uhdr {\n\tstruct tpacket_hdr  *h1;\n\tstruct tpacket2_hdr *h2;\n\tstruct tpacket3_hdr *h3;\n\tvoid *raw;\n};\n\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring);\n\n#define V3_ALIGNMENT\t(8)\n\n#define BLK_HDR_LEN\t(ALIGN(sizeof(struct tpacket_block_desc), V3_ALIGNMENT))\n\n#define BLK_PLUS_PRIV(sz_of_priv) \\\n\t(BLK_HDR_LEN + ALIGN((sz_of_priv), V3_ALIGNMENT))\n\n#define PGV_FROM_VMALLOC 1\n\n#define BLOCK_STATUS(x)\t((x)->hdr.bh1.block_status)\n#define BLOCK_NUM_PKTS(x)\t((x)->hdr.bh1.num_pkts)\n#define BLOCK_O2FP(x)\t\t((x)->hdr.bh1.offset_to_first_pkt)\n#define BLOCK_LEN(x)\t\t((x)->hdr.bh1.blk_len)\n#define BLOCK_SNUM(x)\t\t((x)->hdr.bh1.seq_num)\n#define BLOCK_O2PRIV(x)\t((x)->offset_to_priv)\n#define BLOCK_PRIV(x)\t\t((void *)((char *)(x) + BLOCK_O2PRIV(x)))\n\nstruct packet_sock;\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev);\n\nstatic void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status);\nstatic void packet_increment_head(struct packet_ring_buffer *buff);\nstatic int prb_curr_blk_in_use(struct tpacket_block_desc *);\nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *,\n\t\t\tstruct packet_sock *);\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *,\n\t\tstruct packet_sock *, unsigned int status);\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *);\nstatic void prb_open_block(struct tpacket_kbdq_core *,\n\t\tstruct tpacket_block_desc *);\nstatic void prb_retire_rx_blk_timer_expired(unsigned long);\nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *);\nstatic void prb_init_blk_timer(struct packet_sock *,\n\t\tstruct tpacket_kbdq_core *,\n\t\tvoid (*func) (unsigned long));\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *, struct tpacket3_hdr *);\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *,\n\t\tstruct tpacket3_hdr *);\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *,\n\t\tstruct tpacket3_hdr *);\nstatic void packet_flush_mclist(struct sock *sk);\nstatic void packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb);\n\nstruct packet_skb_cb {\n\tunion {\n\t\tstruct sockaddr_pkt pkt;\n\t\tunion {\n\t\t\t/* Trick: alias skb original length with\n\t\t\t * ll.sll_family and ll.protocol in order\n\t\t\t * to save room.\n\t\t\t */\n\t\t\tunsigned int origlen;\n\t\t\tstruct sockaddr_ll ll;\n\t\t};\n\t} sa;\n};\n\n#define vio_le() virtio_legacy_is_little_endian()\n\n#define PACKET_SKB_CB(__skb)\t((struct packet_skb_cb *)((__skb)->cb))\n\n#define GET_PBDQC_FROM_RB(x)\t((struct tpacket_kbdq_core *)(&(x)->prb_bdqc))\n#define GET_PBLOCK_DESC(x, bid)\t\\\n\t((struct tpacket_block_desc *)((x)->pkbdq[(bid)].buffer))\n#define GET_CURR_PBLOCK_DESC_FROM_CORE(x)\t\\\n\t((struct tpacket_block_desc *)((x)->pkbdq[(x)->kactive_blk_num].buffer))\n#define GET_NEXT_PRB_BLK_NUM(x) \\\n\t(((x)->kactive_blk_num < ((x)->knum_blocks-1)) ? \\\n\t((x)->kactive_blk_num+1) : 0)\n\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po);\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po);\n\nstatic int packet_direct_xmit(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tpacket_pick_tx_queue(dev, skb);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\tlocal_bh_disable();\n\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\n\tlocal_bh_enable();\n\n\tif (!dev_xmit_complete(ret))\n\t\tkfree_skb(skb);\n\n\treturn ret;\ndrop:\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\n\nstatic struct net_device *packet_cached_dev_get(struct packet_sock *po)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = rcu_dereference(po->cached_dev);\n\tif (likely(dev))\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\n\treturn dev;\n}\n\nstatic void packet_cached_dev_assign(struct packet_sock *po,\n\t\t\t\t     struct net_device *dev)\n{\n\trcu_assign_pointer(po->cached_dev, dev);\n}\n\nstatic void packet_cached_dev_reset(struct packet_sock *po)\n{\n\tRCU_INIT_POINTER(po->cached_dev, NULL);\n}\n\nstatic bool packet_use_direct_xmit(const struct packet_sock *po)\n{\n\treturn po->xmit == packet_direct_xmit;\n}\n\nstatic u16 __packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn (u16) raw_smp_processor_id() % dev->real_num_tx_queues;\n}\n\nstatic void packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tu16 queue_index;\n\n\tif (ops->ndo_select_queue) {\n\t\tqueue_index = ops->ndo_select_queue(dev, skb, NULL,\n\t\t\t\t\t\t    __packet_pick_tx_queue);\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t} else {\n\t\tqueue_index = __packet_pick_tx_queue(dev, skb);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n}\n\n/* register_prot_hook must be invoked with the po->bind_lock held,\n * or from a context in which asynchronous accesses to the packet\n * socket is not possible (packet_create()).\n */\nstatic void register_prot_hook(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (!po->running) {\n\t\tif (po->fanout)\n\t\t\t__fanout_link(sk, po);\n\t\telse\n\t\t\tdev_add_pack(&po->prot_hook);\n\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t}\n}\n\n/* {,__}unregister_prot_hook() must be invoked with the po->bind_lock\n * held.   If the sync parameter is true, we will temporarily drop\n * the po->bind_lock and do a synchronize_net to make sure no\n * asynchronous packet processing paths still refer to the elements\n * of po->prot_hook.  If the sync parameter is false, it is the\n * callers responsibility to take care of this.\n */\nstatic void __unregister_prot_hook(struct sock *sk, bool sync)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tpo->running = 0;\n\n\tif (po->fanout)\n\t\t__fanout_unlink(sk, po);\n\telse\n\t\t__dev_remove_pack(&po->prot_hook);\n\n\t__sock_put(sk);\n\n\tif (sync) {\n\t\tspin_unlock(&po->bind_lock);\n\t\tsynchronize_net();\n\t\tspin_lock(&po->bind_lock);\n\t}\n}\n\nstatic void unregister_prot_hook(struct sock *sk, bool sync)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (po->running)\n\t\t__unregister_prot_hook(sk, sync);\n}\n\nstatic inline struct page * __pure pgv_to_page(void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\treturn vmalloc_to_page(addr);\n\treturn virt_to_page(addr);\n}\n\nstatic void __packet_set_status(struct packet_sock *po, void *frame, int status)\n{\n\tunion tpacket_uhdr h;\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\th.h3->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h3->tp_status));\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t}\n\n\tsmp_wmb();\n}\n\nstatic int __packet_get_status(struct packet_sock *po, void *frame)\n{\n\tunion tpacket_uhdr h;\n\n\tsmp_rmb();\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\treturn h.h1->tp_status;\n\tcase TPACKET_V2:\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\treturn h.h2->tp_status;\n\tcase TPACKET_V3:\n\t\tflush_dcache_page(pgv_to_page(&h.h3->tp_status));\n\t\treturn h.h3->tp_status;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t\treturn 0;\n\t}\n}\n\nstatic __u32 tpacket_get_timestamp(struct sk_buff *skb, struct timespec *ts,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\n\tif (shhwtstamps &&\n\t    (flags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec_cond(shhwtstamps->hwtstamp, ts))\n\t\treturn TP_STATUS_TS_RAW_HARDWARE;\n\n\tif (ktime_to_timespec_cond(skb->tstamp, ts))\n\t\treturn TP_STATUS_TS_SOFTWARE;\n\n\treturn 0;\n}\n\nstatic __u32 __packet_set_timestamp(struct packet_sock *po, void *frame,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tunion tpacket_uhdr h;\n\tstruct timespec ts;\n\t__u32 ts_status;\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\n\t\treturn 0;\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\th.h3->tp_sec = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t}\n\n\t/* one flush is safe, as both fields always lie on the same cacheline */\n\tflush_dcache_page(pgv_to_page(&h.h1->tp_sec));\n\tsmp_wmb();\n\n\treturn ts_status;\n}\n\nstatic void *packet_lookup_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tunsigned int position,\n\t\tint status)\n{\n\tunsigned int pg_vec_pos, frame_offset;\n\tunion tpacket_uhdr h;\n\n\tpg_vec_pos = position / rb->frames_per_block;\n\tframe_offset = position % rb->frames_per_block;\n\n\th.raw = rb->pg_vec[pg_vec_pos].buffer +\n\t\t(frame_offset * rb->frame_size);\n\n\tif (status != __packet_get_status(po, h.raw))\n\t\treturn NULL;\n\n\treturn h.raw;\n}\n\nstatic void *packet_current_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\treturn packet_lookup_frame(po, rb, rb->head, status);\n}\n\nstatic void prb_del_retire_blk_timer(struct tpacket_kbdq_core *pkc)\n{\n\tdel_timer_sync(&pkc->retire_blk_timer);\n}\n\nstatic void prb_shutdown_retire_blk_timer(struct packet_sock *po,\n\t\tstruct sk_buff_head *rb_queue)\n{\n\tstruct tpacket_kbdq_core *pkc;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\n\tspin_lock_bh(&rb_queue->lock);\n\tpkc->delete_blk_timer = 1;\n\tspin_unlock_bh(&rb_queue->lock);\n\n\tprb_del_retire_blk_timer(pkc);\n}\n\nstatic void prb_init_blk_timer(struct packet_sock *po,\n\t\tstruct tpacket_kbdq_core *pkc,\n\t\tvoid (*func) (unsigned long))\n{\n\tinit_timer(&pkc->retire_blk_timer);\n\tpkc->retire_blk_timer.data = (long)po;\n\tpkc->retire_blk_timer.function = func;\n\tpkc->retire_blk_timer.expires = jiffies;\n}\n\nstatic void prb_setup_retire_blk_timer(struct packet_sock *po)\n{\n\tstruct tpacket_kbdq_core *pkc;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tprb_init_blk_timer(po, pkc, prb_retire_rx_blk_timer_expired);\n}\n\nstatic int prb_calc_retire_blk_tmo(struct packet_sock *po,\n\t\t\t\tint blk_size_in_bytes)\n{\n\tstruct net_device *dev;\n\tunsigned int mbits = 0, msec = 0, div = 0, tmo = 0;\n\tstruct ethtool_link_ksettings ecmd;\n\tint err;\n\n\trtnl_lock();\n\tdev = __dev_get_by_index(sock_net(&po->sk), po->ifindex);\n\tif (unlikely(!dev)) {\n\t\trtnl_unlock();\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t}\n\terr = __ethtool_get_link_ksettings(dev, &ecmd);\n\trtnl_unlock();\n\tif (!err) {\n\t\t/*\n\t\t * If the link speed is so slow you don't really\n\t\t * need to worry about perf anyways\n\t\t */\n\t\tif (ecmd.base.speed < SPEED_1000 ||\n\t\t    ecmd.base.speed == SPEED_UNKNOWN) {\n\t\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t\t} else {\n\t\t\tmsec = 1;\n\t\t\tdiv = ecmd.base.speed / 1000;\n\t\t}\n\t}\n\n\tmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\n\n\tif (div)\n\t\tmbits /= div;\n\n\ttmo = mbits * msec;\n\n\tif (div)\n\t\treturn tmo+1;\n\treturn tmo;\n}\n\nstatic void prb_init_ft_ops(struct tpacket_kbdq_core *p1,\n\t\t\tunion tpacket_req_u *req_u)\n{\n\tp1->feature_req_word = req_u->req3.tp_feature_req_word;\n}\n\nstatic void init_prb_bdqc(struct packet_sock *po,\n\t\t\tstruct packet_ring_buffer *rb,\n\t\t\tstruct pgv *pg_vec,\n\t\t\tunion tpacket_req_u *req_u)\n{\n\tstruct tpacket_kbdq_core *p1 = GET_PBDQC_FROM_RB(rb);\n\tstruct tpacket_block_desc *pbd;\n\n\tmemset(p1, 0x0, sizeof(*p1));\n\n\tp1->knxt_seq_num = 1;\n\tp1->pkbdq = pg_vec;\n\tpbd = (struct tpacket_block_desc *)pg_vec[0].buffer;\n\tp1->pkblk_start\t= pg_vec[0].buffer;\n\tp1->kblk_size = req_u->req3.tp_block_size;\n\tp1->knum_blocks\t= req_u->req3.tp_block_nr;\n\tp1->hdrlen = po->tp_hdrlen;\n\tp1->version = po->tp_version;\n\tp1->last_kactive_blk_num = 0;\n\tpo->stats.stats3.tp_freeze_q_cnt = 0;\n\tif (req_u->req3.tp_retire_blk_tov)\n\t\tp1->retire_blk_tov = req_u->req3.tp_retire_blk_tov;\n\telse\n\t\tp1->retire_blk_tov = prb_calc_retire_blk_tmo(po,\n\t\t\t\t\t\treq_u->req3.tp_block_size);\n\tp1->tov_in_jiffies = msecs_to_jiffies(p1->retire_blk_tov);\n\tp1->blk_sizeof_priv = req_u->req3.tp_sizeof_priv;\n\n\tp1->max_frame_len = p1->kblk_size - BLK_PLUS_PRIV(p1->blk_sizeof_priv);\n\tprb_init_ft_ops(p1, req_u);\n\tprb_setup_retire_blk_timer(po);\n\tprb_open_block(p1, pbd);\n}\n\n/*  Do NOT update the last_blk_num first.\n *  Assumes sk_buff_head lock is held.\n */\nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *pkc)\n{\n\tmod_timer(&pkc->retire_blk_timer,\n\t\t\tjiffies + pkc->tov_in_jiffies);\n\tpkc->last_kactive_blk_num = pkc->kactive_blk_num;\n}\n\n/*\n * Timer logic:\n * 1) We refresh the timer only when we open a block.\n *    By doing this we don't waste cycles refreshing the timer\n *\t  on packet-by-packet basis.\n *\n * With a 1MB block-size, on a 1Gbps line, it will take\n * i) ~8 ms to fill a block + ii) memcpy etc.\n * In this cut we are not accounting for the memcpy time.\n *\n * So, if the user sets the 'tmo' to 10ms then the timer\n * will never fire while the block is still getting filled\n * (which is what we want). However, the user could choose\n * to close a block early and that's fine.\n *\n * But when the timer does fire, we check whether or not to refresh it.\n * Since the tmo granularity is in msecs, it is not too expensive\n * to refresh the timer, lets say every '8' msecs.\n * Either the user can set the 'tmo' or we can derive it based on\n * a) line-speed and b) block-size.\n * prb_calc_retire_blk_tmo() calculates the tmo.\n *\n */\nstatic void prb_retire_rx_blk_timer_expired(unsigned long data)\n{\n\tstruct packet_sock *po = (struct packet_sock *)data;\n\tstruct tpacket_kbdq_core *pkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tunsigned int frozen;\n\tstruct tpacket_block_desc *pbd;\n\n\tspin_lock(&po->sk.sk_receive_queue.lock);\n\n\tfrozen = prb_queue_frozen(pkc);\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\tif (unlikely(pkc->delete_blk_timer))\n\t\tgoto out;\n\n\t/* We only need to plug the race when the block is partially filled.\n\t * tpacket_rcv:\n\t *\t\tlock(); increment BLOCK_NUM_PKTS; unlock()\n\t *\t\tcopy_bits() is in progress ...\n\t *\t\ttimer fires on other cpu:\n\t *\t\twe can't retire the current block because copy_bits\n\t *\t\tis in progress.\n\t *\n\t */\n\tif (BLOCK_NUM_PKTS(pbd)) {\n\t\twhile (atomic_read(&pkc->blk_fill_in_prog)) {\n\t\t\t/* Waiting for skb_copy_bits to finish... */\n\t\t\tcpu_relax();\n\t\t}\n\t}\n\n\tif (pkc->last_kactive_blk_num == pkc->kactive_blk_num) {\n\t\tif (!frozen) {\n\t\t\tif (!BLOCK_NUM_PKTS(pbd)) {\n\t\t\t\t/* An empty block. Just refresh the timer. */\n\t\t\t\tgoto refresh_timer;\n\t\t\t}\n\t\t\tprb_retire_current_block(pkc, po, TP_STATUS_BLK_TMO);\n\t\t\tif (!prb_dispatch_next_block(pkc, po))\n\t\t\t\tgoto refresh_timer;\n\t\t\telse\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\t/* Case 1. Queue was frozen because user-space was\n\t\t\t *\t   lagging behind.\n\t\t\t */\n\t\t\tif (prb_curr_blk_in_use(pbd)) {\n\t\t\t\t/*\n\t\t\t\t * Ok, user-space is still behind.\n\t\t\t\t * So just refresh the timer.\n\t\t\t\t */\n\t\t\t\tgoto refresh_timer;\n\t\t\t} else {\n\t\t\t       /* Case 2. queue was frozen,user-space caught up,\n\t\t\t\t* now the link went idle && the timer fired.\n\t\t\t\t* We don't have a block to close.So we open this\n\t\t\t\t* block and restart the timer.\n\t\t\t\t* opening a block thaws the queue,restarts timer\n\t\t\t\t* Thawing/timer-refresh is a side effect.\n\t\t\t\t*/\n\t\t\t\tprb_open_block(pkc, pbd);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\nrefresh_timer:\n\t_prb_refresh_rx_retire_blk_timer(pkc);\n\nout:\n\tspin_unlock(&po->sk.sk_receive_queue.lock);\n}\n\nstatic void prb_flush_block(struct tpacket_kbdq_core *pkc1,\n\t\tstruct tpacket_block_desc *pbd1, __u32 status)\n{\n\t/* Flush everything minus the block header */\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tu8 *start, *end;\n\n\tstart = (u8 *)pbd1;\n\n\t/* Skip the block header(we know header WILL fit in 4K) */\n\tstart += PAGE_SIZE;\n\n\tend = (u8 *)PAGE_ALIGN((unsigned long)pkc1->pkblk_end);\n\tfor (; start < end; start += PAGE_SIZE)\n\t\tflush_dcache_page(pgv_to_page(start));\n\n\tsmp_wmb();\n#endif\n\n\t/* Now update the block status. */\n\n\tBLOCK_STATUS(pbd1) = status;\n\n\t/* Flush the block header */\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tstart = (u8 *)pbd1;\n\tflush_dcache_page(pgv_to_page(start));\n\n\tsmp_wmb();\n#endif\n}\n\n/*\n * Side effect:\n *\n * 1) flush the block\n * 2) Increment active_blk_num\n *\n * Note:We DONT refresh the timer on purpose.\n *\tBecause almost always the next block will be opened.\n */\nstatic void prb_close_block(struct tpacket_kbdq_core *pkc1,\n\t\tstruct tpacket_block_desc *pbd1,\n\t\tstruct packet_sock *po, unsigned int stat)\n{\n\t__u32 status = TP_STATUS_USER | stat;\n\n\tstruct tpacket3_hdr *last_pkt;\n\tstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\n\tstruct sock *sk = &po->sk;\n\n\tif (po->stats.stats3.tp_drops)\n\t\tstatus |= TP_STATUS_LOSING;\n\n\tlast_pkt = (struct tpacket3_hdr *)pkc1->prev;\n\tlast_pkt->tp_next_offset = 0;\n\n\t/* Get the ts of the last pkt */\n\tif (BLOCK_NUM_PKTS(pbd1)) {\n\t\th1->ts_last_pkt.ts_sec = last_pkt->tp_sec;\n\t\th1->ts_last_pkt.ts_nsec\t= last_pkt->tp_nsec;\n\t} else {\n\t\t/* Ok, we tmo'd - so get the current time.\n\t\t *\n\t\t * It shouldn't really happen as we don't close empty\n\t\t * blocks. See prb_retire_rx_blk_timer_expired().\n\t\t */\n\t\tstruct timespec ts;\n\t\tgetnstimeofday(&ts);\n\t\th1->ts_last_pkt.ts_sec = ts.tv_sec;\n\t\th1->ts_last_pkt.ts_nsec\t= ts.tv_nsec;\n\t}\n\n\tsmp_wmb();\n\n\t/* Flush the block */\n\tprb_flush_block(pkc1, pbd1, status);\n\n\tsk->sk_data_ready(sk);\n\n\tpkc1->kactive_blk_num = GET_NEXT_PRB_BLK_NUM(pkc1);\n}\n\nstatic void prb_thaw_queue(struct tpacket_kbdq_core *pkc)\n{\n\tpkc->reset_pending_on_curr_blk = 0;\n}\n\n/*\n * Side effect of opening a block:\n *\n * 1) prb_queue is thawed.\n * 2) retire_blk_timer is refreshed.\n *\n */\nstatic void prb_open_block(struct tpacket_kbdq_core *pkc1,\n\tstruct tpacket_block_desc *pbd1)\n{\n\tstruct timespec ts;\n\tstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\n\n\tsmp_rmb();\n\n\t/* We could have just memset this but we will lose the\n\t * flexibility of making the priv area sticky\n\t */\n\n\tBLOCK_SNUM(pbd1) = pkc1->knxt_seq_num++;\n\tBLOCK_NUM_PKTS(pbd1) = 0;\n\tBLOCK_LEN(pbd1) = BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\n\tgetnstimeofday(&ts);\n\n\th1->ts_first_pkt.ts_sec = ts.tv_sec;\n\th1->ts_first_pkt.ts_nsec = ts.tv_nsec;\n\n\tpkc1->pkblk_start = (char *)pbd1;\n\tpkc1->nxt_offset = pkc1->pkblk_start + BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\n\tBLOCK_O2FP(pbd1) = (__u32)BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\tBLOCK_O2PRIV(pbd1) = BLK_HDR_LEN;\n\n\tpbd1->version = pkc1->version;\n\tpkc1->prev = pkc1->nxt_offset;\n\tpkc1->pkblk_end = pkc1->pkblk_start + pkc1->kblk_size;\n\n\tprb_thaw_queue(pkc1);\n\t_prb_refresh_rx_retire_blk_timer(pkc1);\n\n\tsmp_wmb();\n}\n\n/*\n * Queue freeze logic:\n * 1) Assume tp_block_nr = 8 blocks.\n * 2) At time 't0', user opens Rx ring.\n * 3) Some time past 't0', kernel starts filling blocks starting from 0 .. 7\n * 4) user-space is either sleeping or processing block '0'.\n * 5) tpacket_rcv is currently filling block '7', since there is no space left,\n *    it will close block-7,loop around and try to fill block '0'.\n *    call-flow:\n *    __packet_lookup_frame_in_block\n *      prb_retire_current_block()\n *      prb_dispatch_next_block()\n *        |->(BLOCK_STATUS == USER) evaluates to true\n *    5.1) Since block-0 is currently in-use, we just freeze the queue.\n * 6) Now there are two cases:\n *    6.1) Link goes idle right after the queue is frozen.\n *         But remember, the last open_block() refreshed the timer.\n *         When this timer expires,it will refresh itself so that we can\n *         re-open block-0 in near future.\n *    6.2) Link is busy and keeps on receiving packets. This is a simple\n *         case and __packet_lookup_frame_in_block will check if block-0\n *         is free and can now be re-used.\n */\nstatic void prb_freeze_queue(struct tpacket_kbdq_core *pkc,\n\t\t\t\t  struct packet_sock *po)\n{\n\tpkc->reset_pending_on_curr_blk = 1;\n\tpo->stats.stats3.tp_freeze_q_cnt++;\n}\n\n#define TOTAL_PKT_LEN_INCL_ALIGN(length) (ALIGN((length), V3_ALIGNMENT))\n\n/*\n * If the next block is free then we will dispatch it\n * and return a good offset.\n * Else, we will freeze the queue.\n * So, caller must check the return value.\n */\nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *pkc,\n\t\tstruct packet_sock *po)\n{\n\tstruct tpacket_block_desc *pbd;\n\n\tsmp_rmb();\n\n\t/* 1. Get current block num */\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t/* 2. If this block is currently in_use then freeze the queue */\n\tif (TP_STATUS_USER & BLOCK_STATUS(pbd)) {\n\t\tprb_freeze_queue(pkc, po);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * 3.\n\t * open this block and return the offset where the first packet\n\t * needs to get stored.\n\t */\n\tprb_open_block(pkc, pbd);\n\treturn (void *)pkc->nxt_offset;\n}\n\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *pkc,\n\t\tstruct packet_sock *po, unsigned int status)\n{\n\tstruct tpacket_block_desc *pbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t/* retire/close the current block */\n\tif (likely(TP_STATUS_KERNEL == BLOCK_STATUS(pbd))) {\n\t\t/*\n\t\t * Plug the case where copy_bits() is in progress on\n\t\t * cpu-0 and tpacket_rcv() got invoked on cpu-1, didn't\n\t\t * have space to copy the pkt in the current block and\n\t\t * called prb_retire_current_block()\n\t\t *\n\t\t * We don't need to worry about the TMO case because\n\t\t * the timer-handler already handled this case.\n\t\t */\n\t\tif (!(status & TP_STATUS_BLK_TMO)) {\n\t\t\twhile (atomic_read(&pkc->blk_fill_in_prog)) {\n\t\t\t\t/* Waiting for skb_copy_bits to finish... */\n\t\t\t\tcpu_relax();\n\t\t\t}\n\t\t}\n\t\tprb_close_block(pkc, pbd, po, status);\n\t\treturn;\n\t}\n}\n\nstatic int prb_curr_blk_in_use(struct tpacket_block_desc *pbd)\n{\n\treturn TP_STATUS_USER & BLOCK_STATUS(pbd);\n}\n\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *pkc)\n{\n\treturn pkc->reset_pending_on_curr_blk;\n}\n\nstatic void prb_clear_blk_fill_status(struct packet_ring_buffer *rb)\n{\n\tstruct tpacket_kbdq_core *pkc  = GET_PBDQC_FROM_RB(rb);\n\tatomic_dec(&pkc->blk_fill_in_prog);\n}\n\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_rxhash = skb_get_hash(pkc->skb);\n}\n\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_rxhash = 0;\n}\n\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tif (skb_vlan_tag_present(pkc->skb)) {\n\t\tppd->hv1.tp_vlan_tci = skb_vlan_tag_get(pkc->skb);\n\t\tppd->hv1.tp_vlan_tpid = ntohs(pkc->skb->vlan_proto);\n\t\tppd->tp_status = TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t} else {\n\t\tppd->hv1.tp_vlan_tci = 0;\n\t\tppd->hv1.tp_vlan_tpid = 0;\n\t\tppd->tp_status = TP_STATUS_AVAILABLE;\n\t}\n}\n\nstatic void prb_run_all_ft_ops(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_padding = 0;\n\tprb_fill_vlan_info(pkc, ppd);\n\n\tif (pkc->feature_req_word & TP_FT_REQ_FILL_RXHASH)\n\t\tprb_fill_rxhash(pkc, ppd);\n\telse\n\t\tprb_clear_rxhash(pkc, ppd);\n}\n\nstatic void prb_fill_curr_block(char *curr,\n\t\t\t\tstruct tpacket_kbdq_core *pkc,\n\t\t\t\tstruct tpacket_block_desc *pbd,\n\t\t\t\tunsigned int len)\n{\n\tstruct tpacket3_hdr *ppd;\n\n\tppd  = (struct tpacket3_hdr *)curr;\n\tppd->tp_next_offset = TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tpkc->prev = curr;\n\tpkc->nxt_offset += TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tBLOCK_LEN(pbd) += TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tBLOCK_NUM_PKTS(pbd) += 1;\n\tatomic_inc(&pkc->blk_fill_in_prog);\n\tprb_run_all_ft_ops(pkc, ppd);\n}\n\n/* Assumes caller has the sk->rx_queue.lock */\nstatic void *__packet_lookup_frame_in_block(struct packet_sock *po,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\tint status,\n\t\t\t\t\t    unsigned int len\n\t\t\t\t\t    )\n{\n\tstruct tpacket_kbdq_core *pkc;\n\tstruct tpacket_block_desc *pbd;\n\tchar *curr, *end;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t/* Queue is frozen when user space is lagging behind */\n\tif (prb_queue_frozen(pkc)) {\n\t\t/*\n\t\t * Check if that last block which caused the queue to freeze,\n\t\t * is still in_use by user-space.\n\t\t */\n\t\tif (prb_curr_blk_in_use(pbd)) {\n\t\t\t/* Can't record this packet */\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Ok, the block was released by user-space.\n\t\t\t * Now let's open that block.\n\t\t\t * opening a block also thaws the queue.\n\t\t\t * Thawing is a side effect.\n\t\t\t */\n\t\t\tprb_open_block(pkc, pbd);\n\t\t}\n\t}\n\n\tsmp_mb();\n\tcurr = pkc->nxt_offset;\n\tpkc->skb = skb;\n\tend = (char *)pbd + pkc->kblk_size;\n\n\t/* first try the current block */\n\tif (curr+TOTAL_PKT_LEN_INCL_ALIGN(len) < end) {\n\t\tprb_fill_curr_block(curr, pkc, pbd, len);\n\t\treturn (void *)curr;\n\t}\n\n\t/* Ok, close the current block */\n\tprb_retire_current_block(pkc, po, 0);\n\n\t/* Now, try to dispatch the next block */\n\tcurr = (char *)prb_dispatch_next_block(pkc, po);\n\tif (curr) {\n\t\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\t\tprb_fill_curr_block(curr, pkc, pbd, len);\n\t\treturn (void *)curr;\n\t}\n\n\t/*\n\t * No free blocks are available.user_space hasn't caught up yet.\n\t * Queue was just frozen and now this packet will get dropped.\n\t */\n\treturn NULL;\n}\n\nstatic void *packet_current_rx_frame(struct packet_sock *po,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t    int status, unsigned int len)\n{\n\tchar *curr = NULL;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\tcase TPACKET_V2:\n\t\tcurr = packet_lookup_frame(po, &po->rx_ring,\n\t\t\t\t\tpo->rx_ring.head, status);\n\t\treturn curr;\n\tcase TPACKET_V3:\n\t\treturn __packet_lookup_frame_in_block(po, skb, status, len);\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported\\n\");\n\t\tBUG();\n\t\treturn NULL;\n\t}\n}\n\nstatic void *prb_lookup_block(struct packet_sock *po,\n\t\t\t\t     struct packet_ring_buffer *rb,\n\t\t\t\t     unsigned int idx,\n\t\t\t\t     int status)\n{\n\tstruct tpacket_kbdq_core *pkc  = GET_PBDQC_FROM_RB(rb);\n\tstruct tpacket_block_desc *pbd = GET_PBLOCK_DESC(pkc, idx);\n\n\tif (status != BLOCK_STATUS(pbd))\n\t\treturn NULL;\n\treturn pbd;\n}\n\nstatic int prb_previous_blk_num(struct packet_ring_buffer *rb)\n{\n\tunsigned int prev;\n\tif (rb->prb_bdqc.kactive_blk_num)\n\t\tprev = rb->prb_bdqc.kactive_blk_num-1;\n\telse\n\t\tprev = rb->prb_bdqc.knum_blocks-1;\n\treturn prev;\n}\n\n/* Assumes caller has held the rx_queue.lock */\nstatic void *__prb_previous_block(struct packet_sock *po,\n\t\t\t\t\t struct packet_ring_buffer *rb,\n\t\t\t\t\t int status)\n{\n\tunsigned int previous = prb_previous_blk_num(rb);\n\treturn prb_lookup_block(po, rb, previous, status);\n}\n\nstatic void *packet_previous_rx_frame(struct packet_sock *po,\n\t\t\t\t\t     struct packet_ring_buffer *rb,\n\t\t\t\t\t     int status)\n{\n\tif (po->tp_version <= TPACKET_V2)\n\t\treturn packet_previous_frame(po, rb, status);\n\n\treturn __prb_previous_block(po, rb, status);\n}\n\nstatic void packet_increment_rx_head(struct packet_sock *po,\n\t\t\t\t\t    struct packet_ring_buffer *rb)\n{\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\tcase TPACKET_V2:\n\t\treturn packet_increment_head(rb);\n\tcase TPACKET_V3:\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t\treturn;\n\t}\n}\n\nstatic void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\tunsigned int previous = rb->head ? rb->head - 1 : rb->frame_max;\n\treturn packet_lookup_frame(po, rb, previous, status);\n}\n\nstatic void packet_increment_head(struct packet_ring_buffer *buff)\n{\n\tbuff->head = buff->head != buff->frame_max ? buff->head+1 : 0;\n}\n\nstatic void packet_inc_pending(struct packet_ring_buffer *rb)\n{\n\tthis_cpu_inc(*rb->pending_refcnt);\n}\n\nstatic void packet_dec_pending(struct packet_ring_buffer *rb)\n{\n\tthis_cpu_dec(*rb->pending_refcnt);\n}\n\nstatic unsigned int packet_read_pending(const struct packet_ring_buffer *rb)\n{\n\tunsigned int refcnt = 0;\n\tint cpu;\n\n\t/* We don't use pending refcount in rx_ring. */\n\tif (rb->pending_refcnt == NULL)\n\t\treturn 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\trefcnt += *per_cpu_ptr(rb->pending_refcnt, cpu);\n\n\treturn refcnt;\n}\n\nstatic int packet_alloc_pending(struct packet_sock *po)\n{\n\tpo->rx_ring.pending_refcnt = NULL;\n\n\tpo->tx_ring.pending_refcnt = alloc_percpu(unsigned int);\n\tif (unlikely(po->tx_ring.pending_refcnt == NULL))\n\t\treturn -ENOBUFS;\n\n\treturn 0;\n}\n\nstatic void packet_free_pending(struct packet_sock *po)\n{\n\tfree_percpu(po->tx_ring.pending_refcnt);\n}\n\n#define ROOM_POW_OFF\t2\n#define ROOM_NONE\t0x0\n#define ROOM_LOW\t0x1\n#define ROOM_NORMAL\t0x2\n\nstatic bool __tpacket_has_room(struct packet_sock *po, int pow_off)\n{\n\tint idx, len;\n\n\tlen = po->rx_ring.frame_max + 1;\n\tidx = po->rx_ring.head;\n\tif (pow_off)\n\t\tidx += len >> pow_off;\n\tif (idx >= len)\n\t\tidx -= len;\n\treturn packet_lookup_frame(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\n}\n\nstatic bool __tpacket_v3_has_room(struct packet_sock *po, int pow_off)\n{\n\tint idx, len;\n\n\tlen = po->rx_ring.prb_bdqc.knum_blocks;\n\tidx = po->rx_ring.prb_bdqc.kactive_blk_num;\n\tif (pow_off)\n\t\tidx += len >> pow_off;\n\tif (idx >= len)\n\t\tidx -= len;\n\treturn prb_lookup_block(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\n}\n\nstatic int __packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\n{\n\tstruct sock *sk = &po->sk;\n\tint ret = ROOM_NONE;\n\n\tif (po->prot_hook.func != tpacket_rcv) {\n\t\tint avail = sk->sk_rcvbuf - atomic_read(&sk->sk_rmem_alloc)\n\t\t\t\t\t  - (skb ? skb->truesize : 0);\n\t\tif (avail > (sk->sk_rcvbuf >> ROOM_POW_OFF))\n\t\t\treturn ROOM_NORMAL;\n\t\telse if (avail > 0)\n\t\t\treturn ROOM_LOW;\n\t\telse\n\t\t\treturn ROOM_NONE;\n\t}\n\n\tif (po->tp_version == TPACKET_V3) {\n\t\tif (__tpacket_v3_has_room(po, ROOM_POW_OFF))\n\t\t\tret = ROOM_NORMAL;\n\t\telse if (__tpacket_v3_has_room(po, 0))\n\t\t\tret = ROOM_LOW;\n\t} else {\n\t\tif (__tpacket_has_room(po, ROOM_POW_OFF))\n\t\t\tret = ROOM_NORMAL;\n\t\telse if (__tpacket_has_room(po, 0))\n\t\t\tret = ROOM_LOW;\n\t}\n\n\treturn ret;\n}\n\nstatic int packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\n{\n\tint ret;\n\tbool has_room;\n\n\tspin_lock_bh(&po->sk.sk_receive_queue.lock);\n\tret = __packet_rcv_has_room(po, skb);\n\thas_room = ret == ROOM_NORMAL;\n\tif (po->pressure == has_room)\n\t\tpo->pressure = !has_room;\n\tspin_unlock_bh(&po->sk.sk_receive_queue.lock);\n\n\treturn ret;\n}\n\nstatic void packet_sock_destruct(struct sock *sk)\n{\n\tskb_queue_purge(&sk->sk_error_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(refcount_read(&sk->sk_wmem_alloc));\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive packet socket: %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tsk_refcnt_debug_dec(sk);\n}\n\nstatic bool fanout_flow_is_huge(struct packet_sock *po, struct sk_buff *skb)\n{\n\tu32 rxhash;\n\tint i, count = 0;\n\n\trxhash = skb_get_hash(skb);\n\tfor (i = 0; i < ROLLOVER_HLEN; i++)\n\t\tif (po->rollover->history[i] == rxhash)\n\t\t\tcount++;\n\n\tpo->rollover->history[prandom_u32() % ROLLOVER_HLEN] = rxhash;\n\treturn count > (ROLLOVER_HLEN >> 1);\n}\n\nstatic unsigned int fanout_demux_hash(struct packet_fanout *f,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      unsigned int num)\n{\n\treturn reciprocal_scale(__skb_get_hash_symmetric(skb), num);\n}\n\nstatic unsigned int fanout_demux_lb(struct packet_fanout *f,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    unsigned int num)\n{\n\tunsigned int val = atomic_inc_return(&f->rr_cur);\n\n\treturn val % num;\n}\n\nstatic unsigned int fanout_demux_cpu(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\treturn smp_processor_id() % num;\n}\n\nstatic unsigned int fanout_demux_rnd(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\treturn prandom_u32_max(num);\n}\n\nstatic unsigned int fanout_demux_rollover(struct packet_fanout *f,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  unsigned int idx, bool try_self,\n\t\t\t\t\t  unsigned int num)\n{\n\tstruct packet_sock *po, *po_next, *po_skip = NULL;\n\tunsigned int i, j, room = ROOM_NONE;\n\n\tpo = pkt_sk(f->arr[idx]);\n\n\tif (try_self) {\n\t\troom = packet_rcv_has_room(po, skb);\n\t\tif (room == ROOM_NORMAL ||\n\t\t    (room == ROOM_LOW && !fanout_flow_is_huge(po, skb)))\n\t\t\treturn idx;\n\t\tpo_skip = po;\n\t}\n\n\ti = j = min_t(int, po->rollover->sock, num - 1);\n\tdo {\n\t\tpo_next = pkt_sk(f->arr[i]);\n\t\tif (po_next != po_skip && !po_next->pressure &&\n\t\t    packet_rcv_has_room(po_next, skb) == ROOM_NORMAL) {\n\t\t\tif (i != j)\n\t\t\t\tpo->rollover->sock = i;\n\t\t\tatomic_long_inc(&po->rollover->num);\n\t\t\tif (room == ROOM_LOW)\n\t\t\t\tatomic_long_inc(&po->rollover->num_huge);\n\t\t\treturn i;\n\t\t}\n\n\t\tif (++i == num)\n\t\t\ti = 0;\n\t} while (i != j);\n\n\tatomic_long_inc(&po->rollover->num_failed);\n\treturn idx;\n}\n\nstatic unsigned int fanout_demux_qm(struct packet_fanout *f,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    unsigned int num)\n{\n\treturn skb_get_queue_mapping(skb) % num;\n}\n\nstatic unsigned int fanout_demux_bpf(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\tstruct bpf_prog *prog;\n\tunsigned int ret = 0;\n\n\trcu_read_lock();\n\tprog = rcu_dereference(f->bpf_prog);\n\tif (prog)\n\t\tret = bpf_prog_run_clear_cb(prog, skb) % num;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic bool fanout_has_flag(struct packet_fanout *f, u16 flag)\n{\n\treturn f->flags & (flag >> 8);\n}\n\nstatic int packet_rcv_fanout(struct sk_buff *skb, struct net_device *dev,\n\t\t\t     struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct packet_fanout *f = pt->af_packet_priv;\n\tunsigned int num = READ_ONCE(f->num_members);\n\tstruct net *net = read_pnet(&f->net);\n\tstruct packet_sock *po;\n\tunsigned int idx;\n\n\tif (!net_eq(dev_net(dev), net) || !num) {\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\tif (fanout_has_flag(f, PACKET_FANOUT_FLAG_DEFRAG)) {\n\t\tskb = ip_check_defrag(net, skb, IP_DEFRAG_AF_PACKET);\n\t\tif (!skb)\n\t\t\treturn 0;\n\t}\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_HASH:\n\tdefault:\n\t\tidx = fanout_demux_hash(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_LB:\n\t\tidx = fanout_demux_lb(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_CPU:\n\t\tidx = fanout_demux_cpu(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_RND:\n\t\tidx = fanout_demux_rnd(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_QM:\n\t\tidx = fanout_demux_qm(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tidx = fanout_demux_rollover(f, skb, 0, false, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tidx = fanout_demux_bpf(f, skb, num);\n\t\tbreak;\n\t}\n\n\tif (fanout_has_flag(f, PACKET_FANOUT_FLAG_ROLLOVER))\n\t\tidx = fanout_demux_rollover(f, skb, idx, true, num);\n\n\tpo = pkt_sk(f->arr[idx]);\n\treturn po->prot_hook.func(skb, dev, &po->prot_hook, orig_dev);\n}\n\nDEFINE_MUTEX(fanout_mutex);\nEXPORT_SYMBOL_GPL(fanout_mutex);\nstatic LIST_HEAD(fanout_list);\nstatic u16 fanout_next_id;\n\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po)\n{\n\tstruct packet_fanout *f = po->fanout;\n\n\tspin_lock(&f->lock);\n\tf->arr[f->num_members] = sk;\n\tsmp_wmb();\n\tf->num_members++;\n\tif (f->num_members == 1)\n\t\tdev_add_pack(&f->prot_hook);\n\tspin_unlock(&f->lock);\n}\n\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po)\n{\n\tstruct packet_fanout *f = po->fanout;\n\tint i;\n\n\tspin_lock(&f->lock);\n\tfor (i = 0; i < f->num_members; i++) {\n\t\tif (f->arr[i] == sk)\n\t\t\tbreak;\n\t}\n\tBUG_ON(i >= f->num_members);\n\tf->arr[i] = f->arr[f->num_members - 1];\n\tf->num_members--;\n\tif (f->num_members == 0)\n\t\t__dev_remove_pack(&f->prot_hook);\n\tspin_unlock(&f->lock);\n}\n\nstatic bool match_fanout_group(struct packet_type *ptype, struct sock *sk)\n{\n\tif (sk->sk_family != PF_PACKET)\n\t\treturn false;\n\n\treturn ptype->af_packet_priv == pkt_sk(sk)->fanout;\n}\n\nstatic void fanout_init_data(struct packet_fanout *f)\n{\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_LB:\n\t\tatomic_set(&f->rr_cur, 0);\n\t\tbreak;\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tRCU_INIT_POINTER(f->bpf_prog, NULL);\n\t\tbreak;\n\t}\n}\n\nstatic void __fanout_set_data_bpf(struct packet_fanout *f, struct bpf_prog *new)\n{\n\tstruct bpf_prog *old;\n\n\tspin_lock(&f->lock);\n\told = rcu_dereference_protected(f->bpf_prog, lockdep_is_held(&f->lock));\n\trcu_assign_pointer(f->bpf_prog, new);\n\tspin_unlock(&f->lock);\n\n\tif (old) {\n\t\tsynchronize_net();\n\t\tbpf_prog_destroy(old);\n\t}\n}\n\nstatic int fanout_set_data_cbpf(struct packet_sock *po, char __user *data,\n\t\t\t\tunsigned int len)\n{\n\tstruct bpf_prog *new;\n\tstruct sock_fprog fprog;\n\tint ret;\n\n\tif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\tif (len != sizeof(fprog))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&fprog, data, len))\n\t\treturn -EFAULT;\n\n\tret = bpf_prog_create_from_user(&new, &fprog, NULL, false);\n\tif (ret)\n\t\treturn ret;\n\n\t__fanout_set_data_bpf(po->fanout, new);\n\treturn 0;\n}\n\nstatic int fanout_set_data_ebpf(struct packet_sock *po, char __user *data,\n\t\t\t\tunsigned int len)\n{\n\tstruct bpf_prog *new;\n\tu32 fd;\n\n\tif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\tif (len != sizeof(fd))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&fd, data, len))\n\t\treturn -EFAULT;\n\n\tnew = bpf_prog_get_type(fd, BPF_PROG_TYPE_SOCKET_FILTER);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\t__fanout_set_data_bpf(po->fanout, new);\n\treturn 0;\n}\n\nstatic int fanout_set_data(struct packet_sock *po, char __user *data,\n\t\t\t   unsigned int len)\n{\n\tswitch (po->fanout->type) {\n\tcase PACKET_FANOUT_CBPF:\n\t\treturn fanout_set_data_cbpf(po, data, len);\n\tcase PACKET_FANOUT_EBPF:\n\t\treturn fanout_set_data_ebpf(po, data, len);\n\tdefault:\n\t\treturn -EINVAL;\n\t};\n}\n\nstatic void fanout_release_data(struct packet_fanout *f)\n{\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\t__fanout_set_data_bpf(f, NULL);\n\t};\n}\n\nstatic bool __fanout_id_is_free(struct sock *sk, u16 candidate_id)\n{\n\tstruct packet_fanout *f;\n\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == candidate_id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\nstatic bool fanout_find_new_id(struct sock *sk, u16 *new_id)\n{\n\tu16 id = fanout_next_id;\n\n\tdo {\n\t\tif (__fanout_id_is_free(sk, id)) {\n\t\t\t*new_id = id;\n\t\t\tfanout_next_id = id + 1;\n\t\t\treturn true;\n\t\t}\n\n\t\tid++;\n\t} while (id != fanout_next_id);\n\n\treturn false;\n}\n\nstatic int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EINVAL;\n\tif (!po->running)\n\t\tgoto out;\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tif (type_flags & PACKET_FANOUT_FLAG_UNIQUEID) {\n\t\tif (id != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!fanout_find_new_id(sk, &id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/* ephemeral flag for the first socket in the group: drop it */\n\t\tflags &= ~(PACKET_FANOUT_FLAG_UNIQUEID >> 8);\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\trefcount_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (refcount_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\trefcount_set(&match->sk_ref, refcount_read(&match->sk_ref) + 1);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}\n\n/* If pkt_sk(sk)->fanout->sk_ref is zero, this function removes\n * pkt_sk(sk)->fanout from fanout_list and returns pkt_sk(sk)->fanout.\n * It is the responsibility of the caller to call fanout_release_data() and\n * free the returned packet_fanout (after synchronize_net())\n */\nstatic struct packet_fanout *fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (refcount_dec_and_test(&f->sk_ref))\n\t\t\tlist_del(&f->list);\n\t\telse\n\t\t\tf = NULL;\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\treturn f;\n}\n\nstatic bool packet_extra_vlan_len_allowed(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\t/* Earlier code assumed this would be a VLAN pkt, double-check\n\t * this now that we have the actual packet in hand. We can only\n\t * do this check on Ethernet devices.\n\t */\n\tif (unlikely(dev->type != ARPHRD_ETHER))\n\t\treturn false;\n\n\tskb_reset_mac_header(skb);\n\treturn likely(eth_hdr(skb)->h_proto == htons(ETH_P_8021Q));\n}\n\nstatic const struct proto_ops packet_ops;\n\nstatic const struct proto_ops packet_ops_spkt;\n\nstatic int packet_rcv_spkt(struct sk_buff *skb, struct net_device *dev,\n\t\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_pkt *spkt;\n\n\t/*\n\t *\tWhen we registered the protocol we saved the socket in the data\n\t *\tfield for just this event.\n\t */\n\n\tsk = pt->af_packet_priv;\n\n\t/*\n\t *\tYank back the headers [hope the device set this\n\t *\tright or kerboom...]\n\t *\n\t *\tIncoming packets have ll header pulled,\n\t *\tpush it back.\n\t *\n\t *\tFor outgoing ones skb->data == skb_mac_header(skb)\n\t *\tso that this procedure is noop.\n\t */\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto out;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto out;\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (skb == NULL)\n\t\tgoto oom;\n\n\t/* drop any routing info */\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspkt = &PACKET_SKB_CB(skb)->sa.pkt;\n\n\tskb_push(skb, skb->data - skb_mac_header(skb));\n\n\t/*\n\t *\tThe SOCK_PACKET socket receives _all_ frames.\n\t */\n\n\tspkt->spkt_family = dev->type;\n\tstrlcpy(spkt->spkt_device, dev->name, sizeof(spkt->spkt_device));\n\tspkt->spkt_protocol = skb->protocol;\n\n\t/*\n\t *\tCharge the memory to the socket. This is done specifically\n\t *\tto prevent sockets using all the memory up.\n\t */\n\n\tif (sock_queue_rcv_skb(sk, skb) == 0)\n\t\treturn 0;\n\nout:\n\tkfree_skb(skb);\noom:\n\treturn 0;\n}\n\n\n/*\n *\tOutput a raw packet to a device layer. This bypasses all the other\n *\tprotocol layers and you must therefore supply it with a complete frame\n */\n\nstatic int packet_sendmsg_spkt(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tDECLARE_SOCKADDR(struct sockaddr_pkt *, saddr, msg->msg_name);\n\tstruct sk_buff *skb = NULL;\n\tstruct net_device *dev;\n\tstruct sockcm_cookie sockc;\n\t__be16 proto = 0;\n\tint err;\n\tint extra_len = 0;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (saddr) {\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr))\n\t\t\treturn -EINVAL;\n\t\tif (msg->msg_namelen == sizeof(struct sockaddr_pkt))\n\t\t\tproto = saddr->spkt_protocol;\n\t} else\n\t\treturn -ENOTCONN;\t/* SOCK_PACKET must be sent giving an address */\n\n\t/*\n\t *\tFind the device first to size check it\n\t */\n\n\tsaddr->spkt_device[sizeof(saddr->spkt_device) - 1] = 0;\nretry:\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device);\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\t/*\n\t * You may not queue a frame bigger than the mtu. This is the lowest level\n\t * raw protocol and you must do your own fragmentation at this level.\n\t */\n\n\tif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\n\t\tif (!netif_supports_nofcs(dev)) {\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\textra_len = 4; /* We're doing our own CRC */\n\t}\n\n\terr = -EMSGSIZE;\n\tif (len > dev->mtu + dev->hard_header_len + VLAN_HLEN + extra_len)\n\t\tgoto out_unlock;\n\n\tif (!skb) {\n\t\tsize_t reserved = LL_RESERVED_SPACE(dev);\n\t\tint tlen = dev->needed_tailroom;\n\t\tunsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;\n\n\t\trcu_read_unlock();\n\t\tskb = sock_wmalloc(sk, len + reserved + tlen, 0, GFP_KERNEL);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOBUFS;\n\t\t/* FIXME: Save some space for broken drivers that write a hard\n\t\t * header at transmission time by themselves. PPP is the notable\n\t\t * one here. This should really be fixed at the driver level.\n\t\t */\n\t\tskb_reserve(skb, reserved);\n\t\tskb_reset_network_header(skb);\n\n\t\t/* Try to align data part correctly */\n\t\tif (hhlen) {\n\t\t\tskb->data -= hhlen;\n\t\t\tskb->tail -= hhlen;\n\t\t\tif (len < hhlen)\n\t\t\t\tskb_reset_network_header(skb);\n\t\t}\n\t\terr = memcpy_from_msg(skb_put(skb, len), msg, len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto retry;\n\t}\n\n\tif (!dev_validate_header(dev, skb->data, len)) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tif (len > (dev->mtu + dev->hard_header_len + extra_len) &&\n\t    !packet_extra_vlan_len_allowed(dev, skb)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_unlock;\n\t}\n\n\tsockc.tsflags = sk->sk_tsflags;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_unlock;\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tsock_tx_timestamp(sk, sockc.tsflags, &skb_shinfo(skb)->tx_flags);\n\n\tif (unlikely(extra_len == 4))\n\t\tskb->no_fcs = 1;\n\n\tskb_probe_transport_header(skb, 0);\n\n\tdev_queue_xmit(skb);\n\trcu_read_unlock();\n\treturn len;\n\nout_unlock:\n\trcu_read_unlock();\nout_free:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic unsigned int run_filter(struct sk_buff *skb,\n\t\t\t       const struct sock *sk,\n\t\t\t       unsigned int res)\n{\n\tstruct sk_filter *filter;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter != NULL)\n\t\tres = bpf_prog_run_clear_cb(filter->prog, skb);\n\trcu_read_unlock();\n\n\treturn res;\n}\n\nstatic int packet_rcv_vnet(struct msghdr *msg, const struct sk_buff *skb,\n\t\t\t   size_t *len)\n{\n\tstruct virtio_net_hdr vnet_hdr;\n\n\tif (*len < sizeof(vnet_hdr))\n\t\treturn -EINVAL;\n\t*len -= sizeof(vnet_hdr);\n\n\tif (virtio_net_hdr_from_skb(skb, &vnet_hdr, vio_le(), true))\n\t\treturn -EINVAL;\n\n\treturn memcpy_to_msg(msg, (void *)&vnet_hdr, sizeof(vnet_hdr));\n}\n\n/*\n * This function makes lazy skb cloning in hope that most of packets\n * are discarded by BPF.\n *\n * Note tricky part: we DO mangle shared skb! skb->data, skb->len\n * and skb->cb are mangled. It works because (and until) packets\n * falling here are owned by current CPU. Output packets are cloned\n * by dev_queue_xmit_nit(), input packets are processed by net_bh\n * sequencially, so that if we return skb to original state on exit,\n * we will not harm anyone.\n */\n\nstatic int packet_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t      struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_ll *sll;\n\tstruct packet_sock *po;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tbool is_drop_n_account = false;\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tskb->dev = dev;\n\n\tif (dev->header_ops) {\n\t\t/* The device has an explicit notion of ll header,\n\t\t * exported to higher levels.\n\t\t *\n\t\t * Otherwise, the device hides details of its frame\n\t\t * structure, so that corresponding packet head is\n\t\t * never delivered to user.\n\t\t */\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto drop_n_acct;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (nskb == NULL)\n\t\t\tgoto drop_n_acct;\n\n\t\tif (skb_head != skb->data) {\n\t\t\tskb->data = skb_head;\n\t\t\tskb->len = skb_len;\n\t\t}\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tsock_skb_cb_check_size(sizeof(*PACKET_SKB_CB(skb)) + MAX_ADDR_LEN - 8);\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\n\t/* sll->sll_family and sll->sll_protocol are set in packet_recvmsg().\n\t * Use their space for storing the original skb length.\n\t */\n\tPACKET_SKB_CB(skb)->sa.origlen = skb->len;\n\n\tif (pskb_trim(skb, snaplen))\n\t\tgoto drop_n_acct;\n\n\tskb_set_owner_r(skb, sk);\n\tskb->dev = NULL;\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.stats1.tp_packets++;\n\tsock_skb_set_dropcount(sk, skb);\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tsk->sk_data_ready(sk);\n\treturn 0;\n\ndrop_n_acct:\n\tis_drop_n_account = true;\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.stats1.tp_drops++;\n\tatomic_inc(&sk->sk_drops);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr)\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0)\n\t\t\t\tsnaplen = 0;\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (po->stats.stats1.tp_drops)\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tif (po->has_vnet_hdr) {\n\t\tif (virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t\t    vio_le(), true)) {\n\t\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t\tgoto drop_n_account;\n\t\t}\n\t}\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\n\t\tgetnstimeofday(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\t__packet_set_status(po, h.raw, status);\n\t\tsk->sk_data_ready(sk);\n\t} else {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tis_drop_n_account = true;\n\tpo->stats.stats1.tp_drops++;\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}\n\nstatic void tpacket_destruct_skb(struct sk_buff *skb)\n{\n\tstruct packet_sock *po = pkt_sk(skb->sk);\n\n\tif (likely(po->tx_ring.pg_vec)) {\n\t\tvoid *ph;\n\t\t__u32 ts;\n\n\t\tph = skb_shinfo(skb)->destructor_arg;\n\t\tpacket_dec_pending(&po->tx_ring);\n\n\t\tts = __packet_set_timestamp(po, ph, skb);\n\t\t__packet_set_status(po, ph, TP_STATUS_AVAILABLE | ts);\n\t}\n\n\tsock_wfree(skb);\n}\n\nstatic void tpacket_set_protocol(const struct net_device *dev,\n\t\t\t\t struct sk_buff *skb)\n{\n\tif (dev->type == ARPHRD_ETHER) {\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = eth_hdr(skb)->h_proto;\n\t}\n}\n\nstatic int __packet_snd_vnet_parse(struct virtio_net_hdr *vnet_hdr, size_t len)\n{\n\tif ((vnet_hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t    (__virtio16_to_cpu(vio_le(), vnet_hdr->csum_start) +\n\t     __virtio16_to_cpu(vio_le(), vnet_hdr->csum_offset) + 2 >\n\t      __virtio16_to_cpu(vio_le(), vnet_hdr->hdr_len)))\n\t\tvnet_hdr->hdr_len = __cpu_to_virtio16(vio_le(),\n\t\t\t __virtio16_to_cpu(vio_le(), vnet_hdr->csum_start) +\n\t\t\t__virtio16_to_cpu(vio_le(), vnet_hdr->csum_offset) + 2);\n\n\tif (__virtio16_to_cpu(vio_le(), vnet_hdr->hdr_len) > len)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int packet_snd_vnet_parse(struct msghdr *msg, size_t *len,\n\t\t\t\t struct virtio_net_hdr *vnet_hdr)\n{\n\tif (*len < sizeof(*vnet_hdr))\n\t\treturn -EINVAL;\n\t*len -= sizeof(*vnet_hdr);\n\n\tif (!copy_from_iter_full(vnet_hdr, sizeof(*vnet_hdr), &msg->msg_iter))\n\t\treturn -EFAULT;\n\n\treturn __packet_snd_vnet_parse(vnet_hdr, *len);\n}\n\nstatic int tpacket_fill_skb(struct packet_sock *po, struct sk_buff *skb,\n\t\tvoid *frame, struct net_device *dev, void *data, int tp_len,\n\t\t__be16 proto, unsigned char *addr, int hlen, int copylen,\n\t\tconst struct sockcm_cookie *sockc)\n{\n\tunion tpacket_uhdr ph;\n\tint to_write, offset, len, nr_frags, len_max;\n\tstruct socket *sock = po->sk.sk_socket;\n\tstruct page *page;\n\tint err;\n\n\tph.raw = frame;\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = po->sk.sk_priority;\n\tskb->mark = po->sk.sk_mark;\n\tsock_tx_timestamp(&po->sk, sockc->tsflags, &skb_shinfo(skb)->tx_flags);\n\tskb_shinfo(skb)->destructor_arg = ph.raw;\n\n\tskb_reserve(skb, hlen);\n\tskb_reset_network_header(skb);\n\n\tto_write = tp_len;\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\terr = dev_hard_header(skb, dev, ntohs(proto), addr,\n\t\t\t\tNULL, tp_len);\n\t\tif (unlikely(err < 0))\n\t\t\treturn -EINVAL;\n\t} else if (copylen) {\n\t\tint hdrlen = min_t(int, copylen, tp_len);\n\n\t\tskb_push(skb, dev->hard_header_len);\n\t\tskb_put(skb, copylen - dev->hard_header_len);\n\t\terr = skb_store_bits(skb, 0, data, hdrlen);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t\tif (!dev_validate_header(dev, skb->data, hdrlen))\n\t\t\treturn -EINVAL;\n\t\tif (!skb->protocol)\n\t\t\ttpacket_set_protocol(dev, skb);\n\n\t\tdata += hdrlen;\n\t\tto_write -= hdrlen;\n\t}\n\n\toffset = offset_in_page(data);\n\tlen_max = PAGE_SIZE - offset;\n\tlen = ((to_write > len_max) ? len_max : to_write);\n\n\tskb->data_len = to_write;\n\tskb->len += to_write;\n\tskb->truesize += to_write;\n\trefcount_add(to_write, &po->sk.sk_wmem_alloc);\n\n\twhile (likely(to_write)) {\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\tpr_err(\"Packet exceed the number of skb frags(%lu)\\n\",\n\t\t\t       MAX_SKB_FRAGS);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tpage = pgv_to_page(data);\n\t\tdata += len;\n\t\tflush_dcache_page(page);\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, nr_frags, page, offset, len);\n\t\tto_write -= len;\n\t\toffset = 0;\n\t\tlen_max = PAGE_SIZE;\n\t\tlen = ((to_write > len_max) ? len_max : to_write);\n\t}\n\n\tskb_probe_transport_header(skb, 0);\n\n\treturn tp_len;\n}\n\nstatic int tpacket_parse_header(struct packet_sock *po, void *frame,\n\t\t\t\tint size_max, void **data)\n{\n\tunion tpacket_uhdr ph;\n\tint tp_len, off;\n\n\tph.raw = frame;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V3:\n\t\tif (ph.h3->tp_next_offset != 0) {\n\t\t\tpr_warn_once(\"variable sized slot not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttp_len = ph.h3->tp_len;\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\ttp_len = ph.h2->tp_len;\n\t\tbreak;\n\tdefault:\n\t\ttp_len = ph.h1->tp_len;\n\t\tbreak;\n\t}\n\tif (unlikely(tp_len > size_max)) {\n\t\tpr_err(\"packet size is too long (%d > %d)\\n\", tp_len, size_max);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (unlikely(po->tp_tx_has_off)) {\n\t\tint off_min, off_max;\n\n\t\toff_min = po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\t\toff_max = po->tx_ring.frame_size - tp_len;\n\t\tif (po->sk.sk_type == SOCK_DGRAM) {\n\t\t\tswitch (po->tp_version) {\n\t\t\tcase TPACKET_V3:\n\t\t\t\toff = ph.h3->tp_net;\n\t\t\t\tbreak;\n\t\t\tcase TPACKET_V2:\n\t\t\t\toff = ph.h2->tp_net;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\toff = ph.h1->tp_net;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tswitch (po->tp_version) {\n\t\t\tcase TPACKET_V3:\n\t\t\t\toff = ph.h3->tp_mac;\n\t\t\t\tbreak;\n\t\t\tcase TPACKET_V2:\n\t\t\t\toff = ph.h2->tp_mac;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\toff = ph.h1->tp_mac;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (unlikely((off < off_min) || (off_max < off)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\toff = po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\t}\n\n\t*data = frame + off;\n\treturn tp_len;\n}\n\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg)\n{\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\tstruct virtio_net_hdr *vnet_hdr = NULL;\n\tstruct sockcm_cookie sockc;\n\t__be16 proto;\n\tint err, reserve = 0;\n\tvoid *ph;\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\n\tbool need_wait = !(msg->msg_flags & MSG_DONTWAIT);\n\tint tp_len, size_max;\n\tunsigned char *addr;\n\tvoid *data;\n\tint len_sum = 0;\n\tint status = TP_STATUS_AVAILABLE;\n\tint hlen, tlen, copylen = 0;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\tif (likely(saddr == NULL)) {\n\t\tdev\t= packet_cached_dev_get(po);\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen\n\t\t\t\t\t+ offsetof(struct sockaddr_ll,\n\t\t\t\t\t\tsll_addr)))\n\t\t\tgoto out;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t\tdev = dev_get_by_index(sock_net(&po->sk), saddr->sll_ifindex);\n\t}\n\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out;\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_put;\n\n\tsockc.tsflags = po->sk.sk_tsflags;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(&po->sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_put;\n\t}\n\n\tif (po->sk.sk_socket->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\tsize_max = po->tx_ring.frame_size\n\t\t- (po->tp_hdrlen - sizeof(struct sockaddr_ll));\n\n\tif ((size_max > dev->mtu + reserve + VLAN_HLEN) && !po->has_vnet_hdr)\n\t\tsize_max = dev->mtu + reserve + VLAN_HLEN;\n\n\tdo {\n\t\tph = packet_current_frame(po, &po->tx_ring,\n\t\t\t\t\t  TP_STATUS_SEND_REQUEST);\n\t\tif (unlikely(ph == NULL)) {\n\t\t\tif (need_wait && need_resched())\n\t\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = NULL;\n\t\ttp_len = tpacket_parse_header(po, ph, size_max, &data);\n\t\tif (tp_len < 0)\n\t\t\tgoto tpacket_error;\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\thlen = LL_RESERVED_SPACE(dev);\n\t\ttlen = dev->needed_tailroom;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tvnet_hdr = data;\n\t\t\tdata += sizeof(*vnet_hdr);\n\t\t\ttp_len -= sizeof(*vnet_hdr);\n\t\t\tif (tp_len < 0 ||\n\t\t\t    __packet_snd_vnet_parse(vnet_hdr, tp_len)) {\n\t\t\t\ttp_len = -EINVAL;\n\t\t\t\tgoto tpacket_error;\n\t\t\t}\n\t\t\tcopylen = __virtio16_to_cpu(vio_le(),\n\t\t\t\t\t\t    vnet_hdr->hdr_len);\n\t\t}\n\t\tcopylen = max_t(int, copylen, dev->hard_header_len);\n\t\tskb = sock_alloc_send_skb(&po->sk,\n\t\t\t\thlen + tlen + sizeof(struct sockaddr_ll) +\n\t\t\t\t(copylen - dev->hard_header_len),\n\t\t\t\t!need_wait, &err);\n\n\t\tif (unlikely(skb == NULL)) {\n\t\t\t/* we assume the socket was initially writeable ... */\n\t\t\tif (likely(len_sum > 0))\n\t\t\t\terr = len_sum;\n\t\t\tgoto out_status;\n\t\t}\n\t\ttp_len = tpacket_fill_skb(po, skb, ph, dev, data, tp_len, proto,\n\t\t\t\t\t  addr, hlen, copylen, &sockc);\n\t\tif (likely(tp_len >= 0) &&\n\t\t    tp_len > dev->mtu + reserve &&\n\t\t    !po->has_vnet_hdr &&\n\t\t    !packet_extra_vlan_len_allowed(dev, skb))\n\t\t\ttp_len = -EMSGSIZE;\n\n\t\tif (unlikely(tp_len < 0)) {\ntpacket_error:\n\t\t\tif (po->tp_loss) {\n\t\t\t\t__packet_set_status(po, ph,\n\t\t\t\t\t\tTP_STATUS_AVAILABLE);\n\t\t\t\tpacket_increment_head(&po->tx_ring);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tstatus = TP_STATUS_WRONG_FORMAT;\n\t\t\t\terr = tp_len;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t}\n\n\t\tif (po->has_vnet_hdr && virtio_net_hdr_to_skb(skb, vnet_hdr,\n\t\t\t\t\t\t\t      vio_le())) {\n\t\t\ttp_len = -EINVAL;\n\t\t\tgoto tpacket_error;\n\t\t}\n\n\t\tskb->destructor = tpacket_destruct_skb;\n\t\t__packet_set_status(po, ph, TP_STATUS_SENDING);\n\t\tpacket_inc_pending(&po->tx_ring);\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\terr = po->xmit(skb);\n\t\tif (unlikely(err > 0)) {\n\t\t\terr = net_xmit_errno(err);\n\t\t\tif (err && __packet_get_status(po, ph) ==\n\t\t\t\t   TP_STATUS_AVAILABLE) {\n\t\t\t\t/* skb was destructed already */\n\t\t\t\tskb = NULL;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t\t/*\n\t\t\t * skb was dropped but not destructed yet;\n\t\t\t * let's treat it like congestion or err < 0\n\t\t\t */\n\t\t\terr = 0;\n\t\t}\n\t\tpacket_increment_head(&po->tx_ring);\n\t\tlen_sum += tp_len;\n\t} while (likely((ph != NULL) ||\n\t\t/* Note: packet_read_pending() might be slow if we have\n\t\t * to call it as it's per_cpu variable, but in fast-path\n\t\t * we already short-circuit the loop with the first\n\t\t * condition, and luckily don't have to go that path\n\t\t * anyway.\n\t\t */\n\t\t (need_wait && packet_read_pending(&po->tx_ring))));\n\n\terr = len_sum;\n\tgoto out_put;\n\nout_status:\n\t__packet_set_status(po, ph, status);\n\tkfree_skb(skb);\nout_put:\n\tdev_put(dev);\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic struct sk_buff *packet_alloc_skb(struct sock *sk, size_t prepad,\n\t\t\t\t        size_t reserve, size_t len,\n\t\t\t\t        size_t linear, int noblock,\n\t\t\t\t        int *err)\n{\n\tstruct sk_buff *skb;\n\n\t/* Under a page?  Don't bother with paged skb. */\n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   err, 0);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, reserve);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic int packet_snd(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tunsigned char *addr;\n\tint err, reserve = 0;\n\tstruct sockcm_cookie sockc;\n\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\tint offset = 0;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint hlen, tlen, linear;\n\tint extra_len = 0;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (likely(saddr == NULL)) {\n\t\tdev\t= packet_cached_dev_get(po);\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))\n\t\t\tgoto out;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t\tdev = dev_get_by_index(sock_net(sk), saddr->sll_ifindex);\n\t}\n\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out_unlock;\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_unlock;\n\n\tsockc.tsflags = sk->sk_tsflags;\n\tsockc.mark = sk->sk_mark;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (sock->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\tif (po->has_vnet_hdr) {\n\t\terr = packet_snd_vnet_parse(msg, &len, &vnet_hdr);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\n\t\tif (!netif_supports_nofcs(dev)) {\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\textra_len = 4; /* We're doing our own CRC */\n\t}\n\n\terr = -EMSGSIZE;\n\tif (!vnet_hdr.gso_type &&\n\t    (len > dev->mtu + reserve + VLAN_HLEN + extra_len))\n\t\tgoto out_unlock;\n\n\terr = -ENOBUFS;\n\thlen = LL_RESERVED_SPACE(dev);\n\ttlen = dev->needed_tailroom;\n\tlinear = __virtio16_to_cpu(vio_le(), vnet_hdr.hdr_len);\n\tlinear = max(linear, min_t(int, len, dev->hard_header_len));\n\tskb = packet_alloc_skb(sk, hlen + tlen, hlen, len, linear,\n\t\t\t       msg->msg_flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out_unlock;\n\n\tskb_set_network_header(skb, reserve);\n\n\terr = -EINVAL;\n\tif (sock->type == SOCK_DGRAM) {\n\t\toffset = dev_hard_header(skb, dev, ntohs(proto), addr, NULL, len);\n\t\tif (unlikely(offset < 0))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Returns -EFAULT on error */\n\terr = skb_copy_datagram_from_iter(skb, offset, &msg->msg_iter, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type == SOCK_RAW &&\n\t    !dev_validate_header(dev, skb->data, len)) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tsock_tx_timestamp(sk, sockc.tsflags, &skb_shinfo(skb)->tx_flags);\n\n\tif (!vnet_hdr.gso_type && (len > dev->mtu + reserve + extra_len) &&\n\t    !packet_extra_vlan_len_allowed(dev, skb)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sockc.mark;\n\n\tif (po->has_vnet_hdr) {\n\t\terr = virtio_net_hdr_to_skb(skb, &vnet_hdr, vio_le());\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tlen += sizeof(vnet_hdr);\n\t}\n\n\tskb_probe_transport_header(skb, reserve);\n\n\tif (unlikely(extra_len == 4))\n\t\tskb->no_fcs = 1;\n\n\terr = po->xmit(skb);\n\tif (err > 0 && (err = net_xmit_errno(err)) != 0)\n\t\tgoto out_unlock;\n\n\tdev_put(dev);\n\n\treturn len;\n\nout_free:\n\tkfree_skb(skb);\nout_unlock:\n\tif (dev)\n\t\tdev_put(dev);\nout:\n\treturn err;\n}\n\nstatic int packet_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (po->tx_ring.pg_vec)\n\t\treturn tpacket_snd(po, msg);\n\telse\n\t\treturn packet_snd(sock, msg, len);\n}\n\n/*\n *\tClose a PACKET socket. This is fairly simple. We immediately go\n *\tto 'closed' state and remove our protocol entry in the device list.\n */\n\nstatic int packet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po;\n\tstruct packet_fanout *f;\n\tstruct net *net;\n\tunion tpacket_req_u req_u;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tpo = pkt_sk(sk);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_del_node_init_rcu(sk);\n\tmutex_unlock(&net->packet.sklist_lock);\n\n\tpreempt_disable();\n\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\tpreempt_enable();\n\n\tspin_lock(&po->bind_lock);\n\tunregister_prot_hook(sk, false);\n\tpacket_cached_dev_reset(po);\n\n\tif (po->prot_hook.dev) {\n\t\tdev_put(po->prot_hook.dev);\n\t\tpo->prot_hook.dev = NULL;\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tpacket_flush_mclist(sk);\n\n\tif (po->rx_ring.pg_vec) {\n\t\tmemset(&req_u, 0, sizeof(req_u));\n\t\tpacket_set_ring(sk, &req_u, 1, 0);\n\t}\n\n\tif (po->tx_ring.pg_vec) {\n\t\tmemset(&req_u, 0, sizeof(req_u));\n\t\tpacket_set_ring(sk, &req_u, 1, 1);\n\t}\n\n\tf = fanout_release(sk);\n\n\tsynchronize_net();\n\n\tif (f) {\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\t/*\n\t *\tNow the socket is dead. No more input will appear.\n\t */\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\t/* Purge queues */\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tpacket_free_pending(po);\n\tsk_refcnt_debug_release(sk);\n\n\tsock_put(sk);\n\treturn 0;\n}\n\n/*\n *\tAttach a packet hook.\n */\n\nstatic int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tif (po->fanout)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}\n\n/*\n *\tBind a packet socket to a device\n */\n\nstatic int packet_bind_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tchar name[sizeof(uaddr->sa_data) + 1];\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len != sizeof(struct sockaddr))\n\t\treturn -EINVAL;\n\t/* uaddr->sa_data comes from the userspace, it's not guaranteed to be\n\t * zero-terminated.\n\t */\n\tmemcpy(name, uaddr->sa_data, sizeof(uaddr->sa_data));\n\tname[sizeof(uaddr->sa_data)] = 0;\n\n\treturn packet_do_bind(sk, name, 0, pkt_sk(sk)->num);\n}\n\nstatic int packet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_ll *sll = (struct sockaddr_ll *)uaddr;\n\tstruct sock *sk = sock->sk;\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len < sizeof(struct sockaddr_ll))\n\t\treturn -EINVAL;\n\tif (sll->sll_family != AF_PACKET)\n\t\treturn -EINVAL;\n\n\treturn packet_do_bind(sk, NULL, sll->sll_ifindex,\n\t\t\t      sll->sll_protocol ? : pkt_sk(sk)->num);\n}\n\nstatic struct proto packet_proto = {\n\t.name\t  = \"PACKET\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct packet_sock),\n};\n\n/*\n *\tCreate a packet of type SOCK_PACKET.\n */\n\nstatic int packet_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t int kern)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\t__be16 proto = (__force __be16)protocol; /* weird, but documented */\n\tint err;\n\n\tif (!ns_capable(net->user_ns, CAP_NET_RAW))\n\t\treturn -EPERM;\n\tif (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW &&\n\t    sock->type != SOCK_PACKET)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto, kern);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\tsock->ops = &packet_ops;\n\tif (sock->type == SOCK_PACKET)\n\t\tsock->ops = &packet_ops_spkt;\n\n\tsock_init_data(sock, sk);\n\n\tpo = pkt_sk(sk);\n\tsk->sk_family = PF_PACKET;\n\tpo->num = proto;\n\tpo->xmit = dev_queue_xmit;\n\n\terr = packet_alloc_pending(po);\n\tif (err)\n\t\tgoto out2;\n\n\tpacket_cached_dev_reset(po);\n\n\tsk->sk_destruct = packet_sock_destruct;\n\tsk_refcnt_debug_inc(sk);\n\n\t/*\n\t *\tAttach a protocol block\n\t */\n\n\tspin_lock_init(&po->bind_lock);\n\tmutex_init(&po->pg_vec_lock);\n\tpo->rollover = NULL;\n\tpo->prot_hook.func = packet_rcv;\n\n\tif (sock->type == SOCK_PACKET)\n\t\tpo->prot_hook.func = packet_rcv_spkt;\n\n\tpo->prot_hook.af_packet_priv = sk;\n\n\tif (proto) {\n\t\tpo->prot_hook.type = proto;\n\t\tregister_prot_hook(sk);\n\t}\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_add_node_rcu(sk, &net->packet.sklist);\n\tmutex_unlock(&net->packet.sklist_lock);\n\n\tpreempt_disable();\n\tsock_prot_inuse_add(net, &packet_proto, 1);\n\tpreempt_enable();\n\n\treturn 0;\nout2:\n\tsk_free(sk);\nout:\n\treturn err;\n}\n\n/*\n *\tPull a packet from our receive queue and hand it to the user.\n *\tIf necessary we block.\n */\n\nstatic int packet_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t  int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tint vnet_hdr_len = 0;\n\tunsigned int origlen = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tif (pkt_sk(sk)->pressure)\n\t\tpacket_rcv_has_room(pkt_sk(sk), NULL);\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\terr = packet_rcv_vnet(msg, skb, &len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tvnet_hdr_len = sizeof(struct virtio_net_hdr);\n\t}\n\n\t/* You lose any data beyond the buffer you gave. If it worries\n\t * a user program they can ask the device for its MTU\n\t * anyway.\n\t */\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type != SOCK_PACKET) {\n\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t/* Original length was stored in sockaddr_ll fields */\n\t\toriglen = PACKET_SKB_CB(skb)->sa.origlen;\n\t\tsll->sll_family = AF_PACKET;\n\t\tsll->sll_protocol = skb->protocol;\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name) {\n\t\t/* If the address length field is there to be filled\n\t\t * in, we fill it in now.\n\t\t */\n\t\tif (sock->type == SOCK_PACKET) {\n\t\t\t__sockaddr_check_size(sizeof(struct sockaddr_pkt));\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\t\t} else {\n\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t\tmsg->msg_namelen = sll->sll_halen +\n\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n\t\t}\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n\t\t       msg->msg_namelen);\n\t}\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t\t  skb_csum_unnecessary(skb)))\n\t\t\taux.tp_status |= TP_STATUS_CSUM_VALID;\n\n\t\taux.tp_len = origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\taux.tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t\taux.tp_vlan_tpid = 0;\n\t\t}\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}\n\nstatic int packet_getname_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk\t= sock->sk;\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tuaddr->sa_family = AF_PACKET;\n\tmemset(uaddr->sa_data, 0, sizeof(uaddr->sa_data));\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), pkt_sk(sk)->ifindex);\n\tif (dev)\n\t\tstrlcpy(uaddr->sa_data, dev->name, sizeof(uaddr->sa_data));\n\trcu_read_unlock();\n\t*uaddr_len = sizeof(*uaddr);\n\n\treturn 0;\n}\n\nstatic int packet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, sll, uaddr);\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_ifindex = po->ifindex;\n\tsll->sll_protocol = po->num;\n\tsll->sll_pkttype = 0;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), po->ifindex);\n\tif (dev) {\n\t\tsll->sll_hatype = dev->type;\n\t\tsll->sll_halen = dev->addr_len;\n\t\tmemcpy(sll->sll_addr, dev->dev_addr, dev->addr_len);\n\t} else {\n\t\tsll->sll_hatype = 0;\t/* Bad: we have no ARPHRD_UNSPEC */\n\t\tsll->sll_halen = 0;\n\t}\n\trcu_read_unlock();\n\t*uaddr_len = offsetof(struct sockaddr_ll, sll_addr) + sll->sll_halen;\n\n\treturn 0;\n}\n\nstatic int packet_dev_mc(struct net_device *dev, struct packet_mclist *i,\n\t\t\t int what)\n{\n\tswitch (i->type) {\n\tcase PACKET_MR_MULTICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_mc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_mc_del(dev, i->addr);\n\t\tbreak;\n\tcase PACKET_MR_PROMISC:\n\t\treturn dev_set_promiscuity(dev, what);\n\tcase PACKET_MR_ALLMULTI:\n\t\treturn dev_set_allmulti(dev, what);\n\tcase PACKET_MR_UNICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_uc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_uc_del(dev, i->addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void packet_dev_mclist_delete(struct net_device *dev,\n\t\t\t\t     struct packet_mclist **mlp)\n{\n\tstruct packet_mclist *ml;\n\n\twhile ((ml = *mlp) != NULL) {\n\t\tif (ml->ifindex == dev->ifindex) {\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t*mlp = ml->next;\n\t\t\tkfree(ml);\n\t\t} else\n\t\t\tmlp = &ml->next;\n\t}\n}\n\nstatic int packet_mc_add(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml, *i;\n\tstruct net_device *dev;\n\tint err;\n\n\trtnl_lock();\n\n\terr = -ENODEV;\n\tdev = __dev_get_by_index(sock_net(sk), mreq->mr_ifindex);\n\tif (!dev)\n\t\tgoto done;\n\n\terr = -EINVAL;\n\tif (mreq->mr_alen > dev->addr_len)\n\t\tgoto done;\n\n\terr = -ENOBUFS;\n\ti = kmalloc(sizeof(*i), GFP_KERNEL);\n\tif (i == NULL)\n\t\tgoto done;\n\n\terr = 0;\n\tfor (ml = po->mclist; ml; ml = ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tml->count++;\n\t\t\t/* Free the new element ... */\n\t\t\tkfree(i);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\ti->type = mreq->mr_type;\n\ti->ifindex = mreq->mr_ifindex;\n\ti->alen = mreq->mr_alen;\n\tmemcpy(i->addr, mreq->mr_address, i->alen);\n\tmemset(i->addr + i->alen, 0, sizeof(i->addr) - i->alen);\n\ti->count = 1;\n\ti->next = po->mclist;\n\tpo->mclist = i;\n\terr = packet_dev_mc(dev, i, 1);\n\tif (err) {\n\t\tpo->mclist = i->next;\n\t\tkfree(i);\n\t}\n\ndone:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int packet_mc_drop(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_mclist *ml, **mlp;\n\n\trtnl_lock();\n\n\tfor (mlp = &pkt_sk(sk)->mclist; (ml = *mlp) != NULL; mlp = &ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tif (--ml->count == 0) {\n\t\t\t\tstruct net_device *dev;\n\t\t\t\t*mlp = ml->next;\n\t\t\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\t\t\tif (dev)\n\t\t\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t\tkfree(ml);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trtnl_unlock();\n\treturn 0;\n}\n\nstatic void packet_flush_mclist(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml;\n\n\tif (!po->mclist)\n\t\treturn;\n\n\trtnl_lock();\n\twhile ((ml = po->mclist) != NULL) {\n\t\tstruct net_device *dev;\n\n\t\tpo->mclist = ml->next;\n\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\tif (dev != NULL)\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\tkfree(ml);\n\t}\n\trtnl_unlock();\n}\n\nstatic int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tif (val > INT_MAX)\n\t\t\treturn -EINVAL;\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_reserve = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int packet_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint len;\n\tint val, lv = sizeof(val);\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tvoid *data = &val;\n\tunion tpacket_stats_u st;\n\tstruct tpacket_rollover_stats rstats;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase PACKET_STATISTICS:\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tmemcpy(&st, &po->stats, sizeof(st));\n\t\tmemset(&po->stats, 0, sizeof(po->stats));\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\n\t\tif (po->tp_version == TPACKET_V3) {\n\t\t\tlv = sizeof(struct tpacket_stats_v3);\n\t\t\tst.stats3.tp_packets += st.stats3.tp_drops;\n\t\t\tdata = &st.stats3;\n\t\t} else {\n\t\t\tlv = sizeof(struct tpacket_stats);\n\t\t\tst.stats1.tp_packets += st.stats1.tp_drops;\n\t\t\tdata = &st.stats1;\n\t\t}\n\n\t\tbreak;\n\tcase PACKET_AUXDATA:\n\t\tval = po->auxdata;\n\t\tbreak;\n\tcase PACKET_ORIGDEV:\n\t\tval = po->origdev;\n\t\tbreak;\n\tcase PACKET_VNET_HDR:\n\t\tval = po->has_vnet_hdr;\n\t\tbreak;\n\tcase PACKET_VERSION:\n\t\tval = po->tp_version;\n\t\tbreak;\n\tcase PACKET_HDRLEN:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, len))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\t\tval = sizeof(struct tpacket_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tval = sizeof(struct tpacket2_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tval = sizeof(struct tpacket3_hdr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase PACKET_RESERVE:\n\t\tval = po->tp_reserve;\n\t\tbreak;\n\tcase PACKET_LOSS:\n\t\tval = po->tp_loss;\n\t\tbreak;\n\tcase PACKET_TIMESTAMP:\n\t\tval = po->tp_tstamp;\n\t\tbreak;\n\tcase PACKET_FANOUT:\n\t\tval = (po->fanout ?\n\t\t       ((u32)po->fanout->id |\n\t\t\t((u32)po->fanout->type << 16) |\n\t\t\t((u32)po->fanout->flags << 24)) :\n\t\t       0);\n\t\tbreak;\n\tcase PACKET_ROLLOVER_STATS:\n\t\tif (!po->rollover)\n\t\t\treturn -EINVAL;\n\t\trstats.tp_all = atomic_long_read(&po->rollover->num);\n\t\trstats.tp_huge = atomic_long_read(&po->rollover->num_huge);\n\t\trstats.tp_failed = atomic_long_read(&po->rollover->num_failed);\n\t\tdata = &rstats;\n\t\tlv = sizeof(rstats);\n\t\tbreak;\n\tcase PACKET_TX_HAS_OFF:\n\t\tval = po->tp_tx_has_off;\n\t\tbreak;\n\tcase PACKET_QDISC_BYPASS:\n\t\tval = packet_use_direct_xmit(po);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, data, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n\n#ifdef CONFIG_COMPAT\nstatic int compat_packet_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct packet_sock *po = pkt_sk(sock->sk);\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optname == PACKET_FANOUT_DATA &&\n\t    po->fanout && po->fanout->type == PACKET_FANOUT_CBPF) {\n\t\toptval = (char __user *)get_compat_bpf_fprog(optval);\n\t\tif (!optval)\n\t\t\treturn -EFAULT;\n\t\toptlen = sizeof(struct sock_fprog);\n\t}\n\n\treturn packet_setsockopt(sock, level, optname, optval, optlen);\n}\n#endif\n\nstatic int packet_notifier(struct notifier_block *this,\n\t\t\t   unsigned long msg, void *ptr)\n{\n\tstruct sock *sk;\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, &net->packet.sklist) {\n\t\tstruct packet_sock *po = pkt_sk(sk);\n\n\t\tswitch (msg) {\n\t\tcase NETDEV_UNREGISTER:\n\t\t\tif (po->mclist)\n\t\t\t\tpacket_dev_mclist_delete(dev, &po->mclist);\n\t\t\t/* fallthrough */\n\n\t\tcase NETDEV_DOWN:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->running) {\n\t\t\t\t\t__unregister_prot_hook(sk, false);\n\t\t\t\t\tsk->sk_err = ENETDOWN;\n\t\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t\tsk->sk_error_report(sk);\n\t\t\t\t}\n\t\t\t\tif (msg == NETDEV_UNREGISTER) {\n\t\t\t\t\tpacket_cached_dev_reset(po);\n\t\t\t\t\tpo->ifindex = -1;\n\t\t\t\t\tif (po->prot_hook.dev)\n\t\t\t\t\t\tdev_put(po->prot_hook.dev);\n\t\t\t\t\tpo->prot_hook.dev = NULL;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NETDEV_UP:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->num)\n\t\t\t\t\tregister_prot_hook(sk);\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NOTIFY_DONE;\n}\n\n\nstatic int packet_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCGSTAMP:\n\t\treturn sock_get_timestamp(sk, (struct timeval __user *)arg);\n\tcase SIOCGSTAMPNS:\n\t\treturn sock_get_timestampns(sk, (struct timespec __user *)arg);\n\n#ifdef CONFIG_INET\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFFLAGS:\n\t\treturn inet_dgram_ops.ioctl(sock, cmd, arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\treturn 0;\n}\n\nstatic unsigned int packet_poll(struct file *file, struct socket *sock,\n\t\t\t\tpoll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\n\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\tif (po->rx_ring.pg_vec) {\n\t\tif (!packet_previous_rx_frame(po, &po->rx_ring,\n\t\t\tTP_STATUS_KERNEL))\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\t}\n\tif (po->pressure && __packet_rcv_has_room(po, NULL) == ROOM_NORMAL)\n\t\tpo->pressure = 0;\n\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\tspin_lock_bh(&sk->sk_write_queue.lock);\n\tif (po->tx_ring.pg_vec) {\n\t\tif (packet_current_frame(po, &po->tx_ring, TP_STATUS_AVAILABLE))\n\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_write_queue.lock);\n\treturn mask;\n}\n\n\n/* Dirty? Well, I still did not learn better way to account\n * for user mmaps.\n */\n\nstatic void packet_mm_open(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_inc(&pkt_sk(sk)->mapped);\n}\n\nstatic void packet_mm_close(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_dec(&pkt_sk(sk)->mapped);\n}\n\nstatic const struct vm_operations_struct packet_mmap_ops = {\n\t.open\t=\tpacket_mm_open,\n\t.close\t=\tpacket_mm_close,\n};\n\nstatic void free_pg_vec(struct pgv *pg_vec, unsigned int order,\n\t\t\tunsigned int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (likely(pg_vec[i].buffer)) {\n\t\t\tif (is_vmalloc_addr(pg_vec[i].buffer))\n\t\t\t\tvfree(pg_vec[i].buffer);\n\t\t\telse\n\t\t\t\tfree_pages((unsigned long)pg_vec[i].buffer,\n\t\t\t\t\t   order);\n\t\t\tpg_vec[i].buffer = NULL;\n\t\t}\n\t}\n\tkfree(pg_vec);\n}\n\nstatic char *alloc_one_pg_vec_page(unsigned long order)\n{\n\tchar *buffer;\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_COMP |\n\t\t\t  __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY;\n\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/* __get_free_pages failed, fall back to vmalloc */\n\tbuffer = vzalloc((1 << order) * PAGE_SIZE);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/* vmalloc failed, lets dig into swap here */\n\tgfp_flags &= ~__GFP_NORETRY;\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/* complete and utter failure */\n\treturn NULL;\n}\n\nstatic struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)\n{\n\tunsigned int block_nr = req->tp_block_nr;\n\tstruct pgv *pg_vec;\n\tint i;\n\n\tpg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL);\n\tif (unlikely(!pg_vec))\n\t\tgoto out;\n\n\tfor (i = 0; i < block_nr; i++) {\n\t\tpg_vec[i].buffer = alloc_one_pg_vec_page(order);\n\t\tif (unlikely(!pg_vec[i].buffer))\n\t\t\tgoto out_free_pgvec;\n\t}\n\nout:\n\treturn pg_vec;\n\nout_free_pgvec:\n\tfree_pg_vec(pg_vec, order, block_nr);\n\tpg_vec = NULL;\n\tgoto out;\n}\n\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    req->tp_block_size <=\n\t\t\t  BLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_block_size > UINT_MAX / req->tp_block_nr))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t\t/* Block transmit is not supported yet */\n\t\t\tif (!tx_ring) {\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\t} else {\n\t\t\t\tstruct tpacket_req3 *req3 = &req_u->req3;\n\n\t\t\t\tif (req3->tp_retire_blk_tov ||\n\t\t\t\t    req3->tp_sizeof_priv ||\n\t\t\t\t    req3->tp_feature_req_word) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (pg_vec && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic int packet_mmap(struct file *file, struct socket *sock,\n\t\tstruct vm_area_struct *vma)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned long size, expected_size;\n\tstruct packet_ring_buffer *rb;\n\tunsigned long start;\n\tint err = -EINVAL;\n\tint i;\n\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\texpected_size = 0;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec) {\n\t\t\texpected_size += rb->pg_vec_len\n\t\t\t\t\t\t* rb->pg_vec_pages\n\t\t\t\t\t\t* PAGE_SIZE;\n\t\t}\n\t}\n\n\tif (expected_size == 0)\n\t\tgoto out;\n\n\tsize = vma->vm_end - vma->vm_start;\n\tif (size != expected_size)\n\t\tgoto out;\n\n\tstart = vma->vm_start;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec == NULL)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < rb->pg_vec_len; i++) {\n\t\t\tstruct page *page;\n\t\t\tvoid *kaddr = rb->pg_vec[i].buffer;\n\t\t\tint pg_num;\n\n\t\t\tfor (pg_num = 0; pg_num < rb->pg_vec_pages; pg_num++) {\n\t\t\t\tpage = pgv_to_page(kaddr);\n\t\t\t\terr = vm_insert_page(vma, start, page);\n\t\t\t\tif (unlikely(err))\n\t\t\t\t\tgoto out;\n\t\t\t\tstart += PAGE_SIZE;\n\t\t\t\tkaddr += PAGE_SIZE;\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_inc(&po->mapped);\n\tvma->vm_ops = &packet_mmap_ops;\n\terr = 0;\n\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic const struct proto_ops packet_ops_spkt = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind_spkt,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname_spkt,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tpacket_sendmsg_spkt,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct proto_ops packet_ops = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname,\n\t.poll =\t\tpacket_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tpacket_setsockopt,\n\t.getsockopt =\tpacket_getsockopt,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_packet_setsockopt,\n#endif\n\t.sendmsg =\tpacket_sendmsg,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tpacket_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct net_proto_family packet_family_ops = {\n\t.family =\tPF_PACKET,\n\t.create =\tpacket_create,\n\t.owner\t=\tTHIS_MODULE,\n};\n\nstatic struct notifier_block packet_netdev_notifier = {\n\t.notifier_call =\tpacket_notifier,\n};\n\n#ifdef CONFIG_PROC_FS\n\nstatic void *packet_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\n\trcu_read_lock();\n\treturn seq_hlist_start_head_rcu(&net->packet.sklist, *pos);\n}\n\nstatic void *packet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net *net = seq_file_net(seq);\n\treturn seq_hlist_next_rcu(v, &net->packet.sklist, pos);\n}\n\nstatic void packet_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int packet_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"sk       RefCnt Type Proto  Iface R Rmem   User   Inode\\n\");\n\telse {\n\t\tstruct sock *s = sk_entry(v);\n\t\tconst struct packet_sock *po = pkt_sk(s);\n\n\t\tseq_printf(seq,\n\t\t\t   \"%pK %-6d %-4d %04x   %-5d %1d %-6u %-6u %-6lu\\n\",\n\t\t\t   s,\n\t\t\t   refcount_read(&s->sk_refcnt),\n\t\t\t   s->sk_type,\n\t\t\t   ntohs(po->num),\n\t\t\t   po->ifindex,\n\t\t\t   po->running,\n\t\t\t   atomic_read(&s->sk_rmem_alloc),\n\t\t\t   from_kuid_munged(seq_user_ns(seq), sock_i_uid(s)),\n\t\t\t   sock_i_ino(s));\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations packet_seq_ops = {\n\t.start\t= packet_seq_start,\n\t.next\t= packet_seq_next,\n\t.stop\t= packet_seq_stop,\n\t.show\t= packet_seq_show,\n};\n\nstatic int packet_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &packet_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations packet_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= packet_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nstatic int __net_init packet_net_init(struct net *net)\n{\n\tmutex_init(&net->packet.sklist_lock);\n\tINIT_HLIST_HEAD(&net->packet.sklist);\n\n\tif (!proc_create(\"packet\", 0, net->proc_net, &packet_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit packet_net_exit(struct net *net)\n{\n\tremove_proc_entry(\"packet\", net->proc_net);\n}\n\nstatic struct pernet_operations packet_net_ops = {\n\t.init = packet_net_init,\n\t.exit = packet_net_exit,\n};\n\n\nstatic void __exit packet_exit(void)\n{\n\tunregister_netdevice_notifier(&packet_netdev_notifier);\n\tunregister_pernet_subsys(&packet_net_ops);\n\tsock_unregister(PF_PACKET);\n\tproto_unregister(&packet_proto);\n}\n\nstatic int __init packet_init(void)\n{\n\tint rc = proto_register(&packet_proto, 0);\n\n\tif (rc != 0)\n\t\tgoto out;\n\n\tsock_register(&packet_family_ops);\n\tregister_pernet_subsys(&packet_net_ops);\n\tregister_netdevice_notifier(&packet_netdev_notifier);\nout:\n\treturn rc;\n}\n\nmodule_init(packet_init);\nmodule_exit(packet_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_PACKET);\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPACKET - implements raw packet sockets.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() now used correctly\n *\t\tAlan Cox\t:\tnew skbuff lists, look ma no backlogs!\n *\t\tAlan Cox\t:\ttidied skbuff lists.\n *\t\tAlan Cox\t:\tNow uses generic datagram routines I\n *\t\t\t\t\tadded. Also fixed the peek/read crash\n *\t\t\t\t\tfrom all old Linux datagram code.\n *\t\tAlan Cox\t:\tUses the improved datagram code.\n *\t\tAlan Cox\t:\tAdded NULL's for socket options.\n *\t\tAlan Cox\t:\tRe-commented the code.\n *\t\tAlan Cox\t:\tUse new kernel side addressing\n *\t\tRob Janssen\t:\tCorrect MTU usage.\n *\t\tDave Platt\t:\tCounter leaks caused by incorrect\n *\t\t\t\t\tinterrupt locking and some slightly\n *\t\t\t\t\tdubious gcc output. Can you read\n *\t\t\t\t\tcompiler: it said _VOLATILE_\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tNew buffers. Use sk->mac.raw.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg support.\n *\t\tAlan Cox\t:\tProtocol setting support\n *\tAlexey Kuznetsov\t:\tUntied from IPv4 stack.\n *\tCyrus Durgin\t\t:\tFixed kerneld for kmod.\n *\tMichal Ostrowski        :       Module initialization cleanup.\n *         Ulises Alonso        :       Frame number limit removal and\n *                                      packet_set_ring memory leak.\n *\t\tEric Biederman\t:\tAllow for > 8 byte hardware addresses.\n *\t\t\t\t\tThe convention is that longer addresses\n *\t\t\t\t\twill simply extend the hardware address\n *\t\t\t\t\tbyte arrays at the end of sockaddr_ll\n *\t\t\t\t\tand packet_mreq.\n *\t\tJohann Baudy\t:\tAdded TX RING.\n *\t\tChetan Loke\t:\tImplemented TPACKET_V3 block abstraction\n *\t\t\t\t\tlayer.\n *\t\t\t\t\tCopyright (C) 2011, <lokec@ccs.neu.edu>\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/wireless.h>\n#include <linux/kernel.h>\n#include <linux/kmod.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <asm/page.h>\n#include <asm/cacheflush.h>\n#include <asm/io.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/mutex.h>\n#include <linux/if_vlan.h>\n#include <linux/virtio_net.h>\n#include <linux/errqueue.h>\n#include <linux/net_tstamp.h>\n#include <linux/percpu.h>\n#ifdef CONFIG_INET\n#include <net/inet_common.h>\n#endif\n#include <linux/bpf.h>\n#include <net/compat.h>\n\n#include \"internal.h\"\n\n/*\n   Assumptions:\n   - if device has no dev->hard_header routine, it adds and removes ll header\n     inside itself. In this case ll header is invisible outside of device,\n     but higher levels still should reserve dev->hard_header_len.\n     Some devices are enough clever to reallocate skb, when header\n     will not fit to reserved space (tunnel), another ones are silly\n     (PPP).\n   - packet socket receives packets with pulled ll header,\n     so that SOCK_RAW should push it back.\n\nOn receive:\n-----------\n\nIncoming, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> data\n\nOutgoing, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> ll header\n\nIncoming, dev->hard_header==NULL\n   mac_header -> UNKNOWN position. It is very likely, that it points to ll\n\t\t header.  PPP makes it, that is wrong, because introduce\n\t\t assymetry between rx and tx paths.\n   data       -> data\n\nOutgoing, dev->hard_header==NULL\n   mac_header -> data. ll header is still not built!\n   data       -> data\n\nResume\n  If dev->hard_header==NULL we are unlikely to restore sensible ll header.\n\n\nOn transmit:\n------------\n\ndev->hard_header != NULL\n   mac_header -> ll header\n   data       -> ll header\n\ndev->hard_header == NULL (ll header is added by device, we cannot control it)\n   mac_header -> data\n   data       -> data\n\n   We should set nh.raw on output to correct posistion,\n   packet classifier depends on it.\n */\n\n/* Private packet socket structures. */\n\n/* identical to struct packet_mreq except it has\n * a longer address field.\n */\nstruct packet_mreq_max {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[MAX_ADDR_LEN];\n};\n\nunion tpacket_uhdr {\n\tstruct tpacket_hdr  *h1;\n\tstruct tpacket2_hdr *h2;\n\tstruct tpacket3_hdr *h3;\n\tvoid *raw;\n};\n\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring);\n\n#define V3_ALIGNMENT\t(8)\n\n#define BLK_HDR_LEN\t(ALIGN(sizeof(struct tpacket_block_desc), V3_ALIGNMENT))\n\n#define BLK_PLUS_PRIV(sz_of_priv) \\\n\t(BLK_HDR_LEN + ALIGN((sz_of_priv), V3_ALIGNMENT))\n\n#define PGV_FROM_VMALLOC 1\n\n#define BLOCK_STATUS(x)\t((x)->hdr.bh1.block_status)\n#define BLOCK_NUM_PKTS(x)\t((x)->hdr.bh1.num_pkts)\n#define BLOCK_O2FP(x)\t\t((x)->hdr.bh1.offset_to_first_pkt)\n#define BLOCK_LEN(x)\t\t((x)->hdr.bh1.blk_len)\n#define BLOCK_SNUM(x)\t\t((x)->hdr.bh1.seq_num)\n#define BLOCK_O2PRIV(x)\t((x)->offset_to_priv)\n#define BLOCK_PRIV(x)\t\t((void *)((char *)(x) + BLOCK_O2PRIV(x)))\n\nstruct packet_sock;\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev);\n\nstatic void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status);\nstatic void packet_increment_head(struct packet_ring_buffer *buff);\nstatic int prb_curr_blk_in_use(struct tpacket_block_desc *);\nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *,\n\t\t\tstruct packet_sock *);\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *,\n\t\tstruct packet_sock *, unsigned int status);\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *);\nstatic void prb_open_block(struct tpacket_kbdq_core *,\n\t\tstruct tpacket_block_desc *);\nstatic void prb_retire_rx_blk_timer_expired(unsigned long);\nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *);\nstatic void prb_init_blk_timer(struct packet_sock *,\n\t\tstruct tpacket_kbdq_core *,\n\t\tvoid (*func) (unsigned long));\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *, struct tpacket3_hdr *);\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *,\n\t\tstruct tpacket3_hdr *);\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *,\n\t\tstruct tpacket3_hdr *);\nstatic void packet_flush_mclist(struct sock *sk);\nstatic void packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb);\n\nstruct packet_skb_cb {\n\tunion {\n\t\tstruct sockaddr_pkt pkt;\n\t\tunion {\n\t\t\t/* Trick: alias skb original length with\n\t\t\t * ll.sll_family and ll.protocol in order\n\t\t\t * to save room.\n\t\t\t */\n\t\t\tunsigned int origlen;\n\t\t\tstruct sockaddr_ll ll;\n\t\t};\n\t} sa;\n};\n\n#define vio_le() virtio_legacy_is_little_endian()\n\n#define PACKET_SKB_CB(__skb)\t((struct packet_skb_cb *)((__skb)->cb))\n\n#define GET_PBDQC_FROM_RB(x)\t((struct tpacket_kbdq_core *)(&(x)->prb_bdqc))\n#define GET_PBLOCK_DESC(x, bid)\t\\\n\t((struct tpacket_block_desc *)((x)->pkbdq[(bid)].buffer))\n#define GET_CURR_PBLOCK_DESC_FROM_CORE(x)\t\\\n\t((struct tpacket_block_desc *)((x)->pkbdq[(x)->kactive_blk_num].buffer))\n#define GET_NEXT_PRB_BLK_NUM(x) \\\n\t(((x)->kactive_blk_num < ((x)->knum_blocks-1)) ? \\\n\t((x)->kactive_blk_num+1) : 0)\n\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po);\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po);\n\nstatic int packet_direct_xmit(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tpacket_pick_tx_queue(dev, skb);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\tlocal_bh_disable();\n\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\n\tlocal_bh_enable();\n\n\tif (!dev_xmit_complete(ret))\n\t\tkfree_skb(skb);\n\n\treturn ret;\ndrop:\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\n\nstatic struct net_device *packet_cached_dev_get(struct packet_sock *po)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = rcu_dereference(po->cached_dev);\n\tif (likely(dev))\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\n\treturn dev;\n}\n\nstatic void packet_cached_dev_assign(struct packet_sock *po,\n\t\t\t\t     struct net_device *dev)\n{\n\trcu_assign_pointer(po->cached_dev, dev);\n}\n\nstatic void packet_cached_dev_reset(struct packet_sock *po)\n{\n\tRCU_INIT_POINTER(po->cached_dev, NULL);\n}\n\nstatic bool packet_use_direct_xmit(const struct packet_sock *po)\n{\n\treturn po->xmit == packet_direct_xmit;\n}\n\nstatic u16 __packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn (u16) raw_smp_processor_id() % dev->real_num_tx_queues;\n}\n\nstatic void packet_pick_tx_queue(struct net_device *dev, struct sk_buff *skb)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tu16 queue_index;\n\n\tif (ops->ndo_select_queue) {\n\t\tqueue_index = ops->ndo_select_queue(dev, skb, NULL,\n\t\t\t\t\t\t    __packet_pick_tx_queue);\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t} else {\n\t\tqueue_index = __packet_pick_tx_queue(dev, skb);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n}\n\n/* register_prot_hook must be invoked with the po->bind_lock held,\n * or from a context in which asynchronous accesses to the packet\n * socket is not possible (packet_create()).\n */\nstatic void register_prot_hook(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (!po->running) {\n\t\tif (po->fanout)\n\t\t\t__fanout_link(sk, po);\n\t\telse\n\t\t\tdev_add_pack(&po->prot_hook);\n\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t}\n}\n\n/* {,__}unregister_prot_hook() must be invoked with the po->bind_lock\n * held.   If the sync parameter is true, we will temporarily drop\n * the po->bind_lock and do a synchronize_net to make sure no\n * asynchronous packet processing paths still refer to the elements\n * of po->prot_hook.  If the sync parameter is false, it is the\n * callers responsibility to take care of this.\n */\nstatic void __unregister_prot_hook(struct sock *sk, bool sync)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tpo->running = 0;\n\n\tif (po->fanout)\n\t\t__fanout_unlink(sk, po);\n\telse\n\t\t__dev_remove_pack(&po->prot_hook);\n\n\t__sock_put(sk);\n\n\tif (sync) {\n\t\tspin_unlock(&po->bind_lock);\n\t\tsynchronize_net();\n\t\tspin_lock(&po->bind_lock);\n\t}\n}\n\nstatic void unregister_prot_hook(struct sock *sk, bool sync)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (po->running)\n\t\t__unregister_prot_hook(sk, sync);\n}\n\nstatic inline struct page * __pure pgv_to_page(void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\treturn vmalloc_to_page(addr);\n\treturn virt_to_page(addr);\n}\n\nstatic void __packet_set_status(struct packet_sock *po, void *frame, int status)\n{\n\tunion tpacket_uhdr h;\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\th.h3->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h3->tp_status));\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t}\n\n\tsmp_wmb();\n}\n\nstatic int __packet_get_status(struct packet_sock *po, void *frame)\n{\n\tunion tpacket_uhdr h;\n\n\tsmp_rmb();\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\treturn h.h1->tp_status;\n\tcase TPACKET_V2:\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\treturn h.h2->tp_status;\n\tcase TPACKET_V3:\n\t\tflush_dcache_page(pgv_to_page(&h.h3->tp_status));\n\t\treturn h.h3->tp_status;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t\treturn 0;\n\t}\n}\n\nstatic __u32 tpacket_get_timestamp(struct sk_buff *skb, struct timespec *ts,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\n\tif (shhwtstamps &&\n\t    (flags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec_cond(shhwtstamps->hwtstamp, ts))\n\t\treturn TP_STATUS_TS_RAW_HARDWARE;\n\n\tif (ktime_to_timespec_cond(skb->tstamp, ts))\n\t\treturn TP_STATUS_TS_SOFTWARE;\n\n\treturn 0;\n}\n\nstatic __u32 __packet_set_timestamp(struct packet_sock *po, void *frame,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tunion tpacket_uhdr h;\n\tstruct timespec ts;\n\t__u32 ts_status;\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\n\t\treturn 0;\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\th.h3->tp_sec = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t}\n\n\t/* one flush is safe, as both fields always lie on the same cacheline */\n\tflush_dcache_page(pgv_to_page(&h.h1->tp_sec));\n\tsmp_wmb();\n\n\treturn ts_status;\n}\n\nstatic void *packet_lookup_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tunsigned int position,\n\t\tint status)\n{\n\tunsigned int pg_vec_pos, frame_offset;\n\tunion tpacket_uhdr h;\n\n\tpg_vec_pos = position / rb->frames_per_block;\n\tframe_offset = position % rb->frames_per_block;\n\n\th.raw = rb->pg_vec[pg_vec_pos].buffer +\n\t\t(frame_offset * rb->frame_size);\n\n\tif (status != __packet_get_status(po, h.raw))\n\t\treturn NULL;\n\n\treturn h.raw;\n}\n\nstatic void *packet_current_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\treturn packet_lookup_frame(po, rb, rb->head, status);\n}\n\nstatic void prb_del_retire_blk_timer(struct tpacket_kbdq_core *pkc)\n{\n\tdel_timer_sync(&pkc->retire_blk_timer);\n}\n\nstatic void prb_shutdown_retire_blk_timer(struct packet_sock *po,\n\t\tstruct sk_buff_head *rb_queue)\n{\n\tstruct tpacket_kbdq_core *pkc;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\n\tspin_lock_bh(&rb_queue->lock);\n\tpkc->delete_blk_timer = 1;\n\tspin_unlock_bh(&rb_queue->lock);\n\n\tprb_del_retire_blk_timer(pkc);\n}\n\nstatic void prb_init_blk_timer(struct packet_sock *po,\n\t\tstruct tpacket_kbdq_core *pkc,\n\t\tvoid (*func) (unsigned long))\n{\n\tinit_timer(&pkc->retire_blk_timer);\n\tpkc->retire_blk_timer.data = (long)po;\n\tpkc->retire_blk_timer.function = func;\n\tpkc->retire_blk_timer.expires = jiffies;\n}\n\nstatic void prb_setup_retire_blk_timer(struct packet_sock *po)\n{\n\tstruct tpacket_kbdq_core *pkc;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tprb_init_blk_timer(po, pkc, prb_retire_rx_blk_timer_expired);\n}\n\nstatic int prb_calc_retire_blk_tmo(struct packet_sock *po,\n\t\t\t\tint blk_size_in_bytes)\n{\n\tstruct net_device *dev;\n\tunsigned int mbits = 0, msec = 0, div = 0, tmo = 0;\n\tstruct ethtool_link_ksettings ecmd;\n\tint err;\n\n\trtnl_lock();\n\tdev = __dev_get_by_index(sock_net(&po->sk), po->ifindex);\n\tif (unlikely(!dev)) {\n\t\trtnl_unlock();\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t}\n\terr = __ethtool_get_link_ksettings(dev, &ecmd);\n\trtnl_unlock();\n\tif (!err) {\n\t\t/*\n\t\t * If the link speed is so slow you don't really\n\t\t * need to worry about perf anyways\n\t\t */\n\t\tif (ecmd.base.speed < SPEED_1000 ||\n\t\t    ecmd.base.speed == SPEED_UNKNOWN) {\n\t\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t\t} else {\n\t\t\tmsec = 1;\n\t\t\tdiv = ecmd.base.speed / 1000;\n\t\t}\n\t}\n\n\tmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\n\n\tif (div)\n\t\tmbits /= div;\n\n\ttmo = mbits * msec;\n\n\tif (div)\n\t\treturn tmo+1;\n\treturn tmo;\n}\n\nstatic void prb_init_ft_ops(struct tpacket_kbdq_core *p1,\n\t\t\tunion tpacket_req_u *req_u)\n{\n\tp1->feature_req_word = req_u->req3.tp_feature_req_word;\n}\n\nstatic void init_prb_bdqc(struct packet_sock *po,\n\t\t\tstruct packet_ring_buffer *rb,\n\t\t\tstruct pgv *pg_vec,\n\t\t\tunion tpacket_req_u *req_u)\n{\n\tstruct tpacket_kbdq_core *p1 = GET_PBDQC_FROM_RB(rb);\n\tstruct tpacket_block_desc *pbd;\n\n\tmemset(p1, 0x0, sizeof(*p1));\n\n\tp1->knxt_seq_num = 1;\n\tp1->pkbdq = pg_vec;\n\tpbd = (struct tpacket_block_desc *)pg_vec[0].buffer;\n\tp1->pkblk_start\t= pg_vec[0].buffer;\n\tp1->kblk_size = req_u->req3.tp_block_size;\n\tp1->knum_blocks\t= req_u->req3.tp_block_nr;\n\tp1->hdrlen = po->tp_hdrlen;\n\tp1->version = po->tp_version;\n\tp1->last_kactive_blk_num = 0;\n\tpo->stats.stats3.tp_freeze_q_cnt = 0;\n\tif (req_u->req3.tp_retire_blk_tov)\n\t\tp1->retire_blk_tov = req_u->req3.tp_retire_blk_tov;\n\telse\n\t\tp1->retire_blk_tov = prb_calc_retire_blk_tmo(po,\n\t\t\t\t\t\treq_u->req3.tp_block_size);\n\tp1->tov_in_jiffies = msecs_to_jiffies(p1->retire_blk_tov);\n\tp1->blk_sizeof_priv = req_u->req3.tp_sizeof_priv;\n\n\tp1->max_frame_len = p1->kblk_size - BLK_PLUS_PRIV(p1->blk_sizeof_priv);\n\tprb_init_ft_ops(p1, req_u);\n\tprb_setup_retire_blk_timer(po);\n\tprb_open_block(p1, pbd);\n}\n\n/*  Do NOT update the last_blk_num first.\n *  Assumes sk_buff_head lock is held.\n */\nstatic void _prb_refresh_rx_retire_blk_timer(struct tpacket_kbdq_core *pkc)\n{\n\tmod_timer(&pkc->retire_blk_timer,\n\t\t\tjiffies + pkc->tov_in_jiffies);\n\tpkc->last_kactive_blk_num = pkc->kactive_blk_num;\n}\n\n/*\n * Timer logic:\n * 1) We refresh the timer only when we open a block.\n *    By doing this we don't waste cycles refreshing the timer\n *\t  on packet-by-packet basis.\n *\n * With a 1MB block-size, on a 1Gbps line, it will take\n * i) ~8 ms to fill a block + ii) memcpy etc.\n * In this cut we are not accounting for the memcpy time.\n *\n * So, if the user sets the 'tmo' to 10ms then the timer\n * will never fire while the block is still getting filled\n * (which is what we want). However, the user could choose\n * to close a block early and that's fine.\n *\n * But when the timer does fire, we check whether or not to refresh it.\n * Since the tmo granularity is in msecs, it is not too expensive\n * to refresh the timer, lets say every '8' msecs.\n * Either the user can set the 'tmo' or we can derive it based on\n * a) line-speed and b) block-size.\n * prb_calc_retire_blk_tmo() calculates the tmo.\n *\n */\nstatic void prb_retire_rx_blk_timer_expired(unsigned long data)\n{\n\tstruct packet_sock *po = (struct packet_sock *)data;\n\tstruct tpacket_kbdq_core *pkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tunsigned int frozen;\n\tstruct tpacket_block_desc *pbd;\n\n\tspin_lock(&po->sk.sk_receive_queue.lock);\n\n\tfrozen = prb_queue_frozen(pkc);\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\tif (unlikely(pkc->delete_blk_timer))\n\t\tgoto out;\n\n\t/* We only need to plug the race when the block is partially filled.\n\t * tpacket_rcv:\n\t *\t\tlock(); increment BLOCK_NUM_PKTS; unlock()\n\t *\t\tcopy_bits() is in progress ...\n\t *\t\ttimer fires on other cpu:\n\t *\t\twe can't retire the current block because copy_bits\n\t *\t\tis in progress.\n\t *\n\t */\n\tif (BLOCK_NUM_PKTS(pbd)) {\n\t\twhile (atomic_read(&pkc->blk_fill_in_prog)) {\n\t\t\t/* Waiting for skb_copy_bits to finish... */\n\t\t\tcpu_relax();\n\t\t}\n\t}\n\n\tif (pkc->last_kactive_blk_num == pkc->kactive_blk_num) {\n\t\tif (!frozen) {\n\t\t\tif (!BLOCK_NUM_PKTS(pbd)) {\n\t\t\t\t/* An empty block. Just refresh the timer. */\n\t\t\t\tgoto refresh_timer;\n\t\t\t}\n\t\t\tprb_retire_current_block(pkc, po, TP_STATUS_BLK_TMO);\n\t\t\tif (!prb_dispatch_next_block(pkc, po))\n\t\t\t\tgoto refresh_timer;\n\t\t\telse\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\t/* Case 1. Queue was frozen because user-space was\n\t\t\t *\t   lagging behind.\n\t\t\t */\n\t\t\tif (prb_curr_blk_in_use(pbd)) {\n\t\t\t\t/*\n\t\t\t\t * Ok, user-space is still behind.\n\t\t\t\t * So just refresh the timer.\n\t\t\t\t */\n\t\t\t\tgoto refresh_timer;\n\t\t\t} else {\n\t\t\t       /* Case 2. queue was frozen,user-space caught up,\n\t\t\t\t* now the link went idle && the timer fired.\n\t\t\t\t* We don't have a block to close.So we open this\n\t\t\t\t* block and restart the timer.\n\t\t\t\t* opening a block thaws the queue,restarts timer\n\t\t\t\t* Thawing/timer-refresh is a side effect.\n\t\t\t\t*/\n\t\t\t\tprb_open_block(pkc, pbd);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\nrefresh_timer:\n\t_prb_refresh_rx_retire_blk_timer(pkc);\n\nout:\n\tspin_unlock(&po->sk.sk_receive_queue.lock);\n}\n\nstatic void prb_flush_block(struct tpacket_kbdq_core *pkc1,\n\t\tstruct tpacket_block_desc *pbd1, __u32 status)\n{\n\t/* Flush everything minus the block header */\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tu8 *start, *end;\n\n\tstart = (u8 *)pbd1;\n\n\t/* Skip the block header(we know header WILL fit in 4K) */\n\tstart += PAGE_SIZE;\n\n\tend = (u8 *)PAGE_ALIGN((unsigned long)pkc1->pkblk_end);\n\tfor (; start < end; start += PAGE_SIZE)\n\t\tflush_dcache_page(pgv_to_page(start));\n\n\tsmp_wmb();\n#endif\n\n\t/* Now update the block status. */\n\n\tBLOCK_STATUS(pbd1) = status;\n\n\t/* Flush the block header */\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tstart = (u8 *)pbd1;\n\tflush_dcache_page(pgv_to_page(start));\n\n\tsmp_wmb();\n#endif\n}\n\n/*\n * Side effect:\n *\n * 1) flush the block\n * 2) Increment active_blk_num\n *\n * Note:We DONT refresh the timer on purpose.\n *\tBecause almost always the next block will be opened.\n */\nstatic void prb_close_block(struct tpacket_kbdq_core *pkc1,\n\t\tstruct tpacket_block_desc *pbd1,\n\t\tstruct packet_sock *po, unsigned int stat)\n{\n\t__u32 status = TP_STATUS_USER | stat;\n\n\tstruct tpacket3_hdr *last_pkt;\n\tstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\n\tstruct sock *sk = &po->sk;\n\n\tif (po->stats.stats3.tp_drops)\n\t\tstatus |= TP_STATUS_LOSING;\n\n\tlast_pkt = (struct tpacket3_hdr *)pkc1->prev;\n\tlast_pkt->tp_next_offset = 0;\n\n\t/* Get the ts of the last pkt */\n\tif (BLOCK_NUM_PKTS(pbd1)) {\n\t\th1->ts_last_pkt.ts_sec = last_pkt->tp_sec;\n\t\th1->ts_last_pkt.ts_nsec\t= last_pkt->tp_nsec;\n\t} else {\n\t\t/* Ok, we tmo'd - so get the current time.\n\t\t *\n\t\t * It shouldn't really happen as we don't close empty\n\t\t * blocks. See prb_retire_rx_blk_timer_expired().\n\t\t */\n\t\tstruct timespec ts;\n\t\tgetnstimeofday(&ts);\n\t\th1->ts_last_pkt.ts_sec = ts.tv_sec;\n\t\th1->ts_last_pkt.ts_nsec\t= ts.tv_nsec;\n\t}\n\n\tsmp_wmb();\n\n\t/* Flush the block */\n\tprb_flush_block(pkc1, pbd1, status);\n\n\tsk->sk_data_ready(sk);\n\n\tpkc1->kactive_blk_num = GET_NEXT_PRB_BLK_NUM(pkc1);\n}\n\nstatic void prb_thaw_queue(struct tpacket_kbdq_core *pkc)\n{\n\tpkc->reset_pending_on_curr_blk = 0;\n}\n\n/*\n * Side effect of opening a block:\n *\n * 1) prb_queue is thawed.\n * 2) retire_blk_timer is refreshed.\n *\n */\nstatic void prb_open_block(struct tpacket_kbdq_core *pkc1,\n\tstruct tpacket_block_desc *pbd1)\n{\n\tstruct timespec ts;\n\tstruct tpacket_hdr_v1 *h1 = &pbd1->hdr.bh1;\n\n\tsmp_rmb();\n\n\t/* We could have just memset this but we will lose the\n\t * flexibility of making the priv area sticky\n\t */\n\n\tBLOCK_SNUM(pbd1) = pkc1->knxt_seq_num++;\n\tBLOCK_NUM_PKTS(pbd1) = 0;\n\tBLOCK_LEN(pbd1) = BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\n\tgetnstimeofday(&ts);\n\n\th1->ts_first_pkt.ts_sec = ts.tv_sec;\n\th1->ts_first_pkt.ts_nsec = ts.tv_nsec;\n\n\tpkc1->pkblk_start = (char *)pbd1;\n\tpkc1->nxt_offset = pkc1->pkblk_start + BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\n\tBLOCK_O2FP(pbd1) = (__u32)BLK_PLUS_PRIV(pkc1->blk_sizeof_priv);\n\tBLOCK_O2PRIV(pbd1) = BLK_HDR_LEN;\n\n\tpbd1->version = pkc1->version;\n\tpkc1->prev = pkc1->nxt_offset;\n\tpkc1->pkblk_end = pkc1->pkblk_start + pkc1->kblk_size;\n\n\tprb_thaw_queue(pkc1);\n\t_prb_refresh_rx_retire_blk_timer(pkc1);\n\n\tsmp_wmb();\n}\n\n/*\n * Queue freeze logic:\n * 1) Assume tp_block_nr = 8 blocks.\n * 2) At time 't0', user opens Rx ring.\n * 3) Some time past 't0', kernel starts filling blocks starting from 0 .. 7\n * 4) user-space is either sleeping or processing block '0'.\n * 5) tpacket_rcv is currently filling block '7', since there is no space left,\n *    it will close block-7,loop around and try to fill block '0'.\n *    call-flow:\n *    __packet_lookup_frame_in_block\n *      prb_retire_current_block()\n *      prb_dispatch_next_block()\n *        |->(BLOCK_STATUS == USER) evaluates to true\n *    5.1) Since block-0 is currently in-use, we just freeze the queue.\n * 6) Now there are two cases:\n *    6.1) Link goes idle right after the queue is frozen.\n *         But remember, the last open_block() refreshed the timer.\n *         When this timer expires,it will refresh itself so that we can\n *         re-open block-0 in near future.\n *    6.2) Link is busy and keeps on receiving packets. This is a simple\n *         case and __packet_lookup_frame_in_block will check if block-0\n *         is free and can now be re-used.\n */\nstatic void prb_freeze_queue(struct tpacket_kbdq_core *pkc,\n\t\t\t\t  struct packet_sock *po)\n{\n\tpkc->reset_pending_on_curr_blk = 1;\n\tpo->stats.stats3.tp_freeze_q_cnt++;\n}\n\n#define TOTAL_PKT_LEN_INCL_ALIGN(length) (ALIGN((length), V3_ALIGNMENT))\n\n/*\n * If the next block is free then we will dispatch it\n * and return a good offset.\n * Else, we will freeze the queue.\n * So, caller must check the return value.\n */\nstatic void *prb_dispatch_next_block(struct tpacket_kbdq_core *pkc,\n\t\tstruct packet_sock *po)\n{\n\tstruct tpacket_block_desc *pbd;\n\n\tsmp_rmb();\n\n\t/* 1. Get current block num */\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t/* 2. If this block is currently in_use then freeze the queue */\n\tif (TP_STATUS_USER & BLOCK_STATUS(pbd)) {\n\t\tprb_freeze_queue(pkc, po);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * 3.\n\t * open this block and return the offset where the first packet\n\t * needs to get stored.\n\t */\n\tprb_open_block(pkc, pbd);\n\treturn (void *)pkc->nxt_offset;\n}\n\nstatic void prb_retire_current_block(struct tpacket_kbdq_core *pkc,\n\t\tstruct packet_sock *po, unsigned int status)\n{\n\tstruct tpacket_block_desc *pbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t/* retire/close the current block */\n\tif (likely(TP_STATUS_KERNEL == BLOCK_STATUS(pbd))) {\n\t\t/*\n\t\t * Plug the case where copy_bits() is in progress on\n\t\t * cpu-0 and tpacket_rcv() got invoked on cpu-1, didn't\n\t\t * have space to copy the pkt in the current block and\n\t\t * called prb_retire_current_block()\n\t\t *\n\t\t * We don't need to worry about the TMO case because\n\t\t * the timer-handler already handled this case.\n\t\t */\n\t\tif (!(status & TP_STATUS_BLK_TMO)) {\n\t\t\twhile (atomic_read(&pkc->blk_fill_in_prog)) {\n\t\t\t\t/* Waiting for skb_copy_bits to finish... */\n\t\t\t\tcpu_relax();\n\t\t\t}\n\t\t}\n\t\tprb_close_block(pkc, pbd, po, status);\n\t\treturn;\n\t}\n}\n\nstatic int prb_curr_blk_in_use(struct tpacket_block_desc *pbd)\n{\n\treturn TP_STATUS_USER & BLOCK_STATUS(pbd);\n}\n\nstatic int prb_queue_frozen(struct tpacket_kbdq_core *pkc)\n{\n\treturn pkc->reset_pending_on_curr_blk;\n}\n\nstatic void prb_clear_blk_fill_status(struct packet_ring_buffer *rb)\n{\n\tstruct tpacket_kbdq_core *pkc  = GET_PBDQC_FROM_RB(rb);\n\tatomic_dec(&pkc->blk_fill_in_prog);\n}\n\nstatic void prb_fill_rxhash(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_rxhash = skb_get_hash(pkc->skb);\n}\n\nstatic void prb_clear_rxhash(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_rxhash = 0;\n}\n\nstatic void prb_fill_vlan_info(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tif (skb_vlan_tag_present(pkc->skb)) {\n\t\tppd->hv1.tp_vlan_tci = skb_vlan_tag_get(pkc->skb);\n\t\tppd->hv1.tp_vlan_tpid = ntohs(pkc->skb->vlan_proto);\n\t\tppd->tp_status = TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t} else {\n\t\tppd->hv1.tp_vlan_tci = 0;\n\t\tppd->hv1.tp_vlan_tpid = 0;\n\t\tppd->tp_status = TP_STATUS_AVAILABLE;\n\t}\n}\n\nstatic void prb_run_all_ft_ops(struct tpacket_kbdq_core *pkc,\n\t\t\tstruct tpacket3_hdr *ppd)\n{\n\tppd->hv1.tp_padding = 0;\n\tprb_fill_vlan_info(pkc, ppd);\n\n\tif (pkc->feature_req_word & TP_FT_REQ_FILL_RXHASH)\n\t\tprb_fill_rxhash(pkc, ppd);\n\telse\n\t\tprb_clear_rxhash(pkc, ppd);\n}\n\nstatic void prb_fill_curr_block(char *curr,\n\t\t\t\tstruct tpacket_kbdq_core *pkc,\n\t\t\t\tstruct tpacket_block_desc *pbd,\n\t\t\t\tunsigned int len)\n{\n\tstruct tpacket3_hdr *ppd;\n\n\tppd  = (struct tpacket3_hdr *)curr;\n\tppd->tp_next_offset = TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tpkc->prev = curr;\n\tpkc->nxt_offset += TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tBLOCK_LEN(pbd) += TOTAL_PKT_LEN_INCL_ALIGN(len);\n\tBLOCK_NUM_PKTS(pbd) += 1;\n\tatomic_inc(&pkc->blk_fill_in_prog);\n\tprb_run_all_ft_ops(pkc, ppd);\n}\n\n/* Assumes caller has the sk->rx_queue.lock */\nstatic void *__packet_lookup_frame_in_block(struct packet_sock *po,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\tint status,\n\t\t\t\t\t    unsigned int len\n\t\t\t\t\t    )\n{\n\tstruct tpacket_kbdq_core *pkc;\n\tstruct tpacket_block_desc *pbd;\n\tchar *curr, *end;\n\n\tpkc = GET_PBDQC_FROM_RB(&po->rx_ring);\n\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\n\t/* Queue is frozen when user space is lagging behind */\n\tif (prb_queue_frozen(pkc)) {\n\t\t/*\n\t\t * Check if that last block which caused the queue to freeze,\n\t\t * is still in_use by user-space.\n\t\t */\n\t\tif (prb_curr_blk_in_use(pbd)) {\n\t\t\t/* Can't record this packet */\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Ok, the block was released by user-space.\n\t\t\t * Now let's open that block.\n\t\t\t * opening a block also thaws the queue.\n\t\t\t * Thawing is a side effect.\n\t\t\t */\n\t\t\tprb_open_block(pkc, pbd);\n\t\t}\n\t}\n\n\tsmp_mb();\n\tcurr = pkc->nxt_offset;\n\tpkc->skb = skb;\n\tend = (char *)pbd + pkc->kblk_size;\n\n\t/* first try the current block */\n\tif (curr+TOTAL_PKT_LEN_INCL_ALIGN(len) < end) {\n\t\tprb_fill_curr_block(curr, pkc, pbd, len);\n\t\treturn (void *)curr;\n\t}\n\n\t/* Ok, close the current block */\n\tprb_retire_current_block(pkc, po, 0);\n\n\t/* Now, try to dispatch the next block */\n\tcurr = (char *)prb_dispatch_next_block(pkc, po);\n\tif (curr) {\n\t\tpbd = GET_CURR_PBLOCK_DESC_FROM_CORE(pkc);\n\t\tprb_fill_curr_block(curr, pkc, pbd, len);\n\t\treturn (void *)curr;\n\t}\n\n\t/*\n\t * No free blocks are available.user_space hasn't caught up yet.\n\t * Queue was just frozen and now this packet will get dropped.\n\t */\n\treturn NULL;\n}\n\nstatic void *packet_current_rx_frame(struct packet_sock *po,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t    int status, unsigned int len)\n{\n\tchar *curr = NULL;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\tcase TPACKET_V2:\n\t\tcurr = packet_lookup_frame(po, &po->rx_ring,\n\t\t\t\t\tpo->rx_ring.head, status);\n\t\treturn curr;\n\tcase TPACKET_V3:\n\t\treturn __packet_lookup_frame_in_block(po, skb, status, len);\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported\\n\");\n\t\tBUG();\n\t\treturn NULL;\n\t}\n}\n\nstatic void *prb_lookup_block(struct packet_sock *po,\n\t\t\t\t     struct packet_ring_buffer *rb,\n\t\t\t\t     unsigned int idx,\n\t\t\t\t     int status)\n{\n\tstruct tpacket_kbdq_core *pkc  = GET_PBDQC_FROM_RB(rb);\n\tstruct tpacket_block_desc *pbd = GET_PBLOCK_DESC(pkc, idx);\n\n\tif (status != BLOCK_STATUS(pbd))\n\t\treturn NULL;\n\treturn pbd;\n}\n\nstatic int prb_previous_blk_num(struct packet_ring_buffer *rb)\n{\n\tunsigned int prev;\n\tif (rb->prb_bdqc.kactive_blk_num)\n\t\tprev = rb->prb_bdqc.kactive_blk_num-1;\n\telse\n\t\tprev = rb->prb_bdqc.knum_blocks-1;\n\treturn prev;\n}\n\n/* Assumes caller has held the rx_queue.lock */\nstatic void *__prb_previous_block(struct packet_sock *po,\n\t\t\t\t\t struct packet_ring_buffer *rb,\n\t\t\t\t\t int status)\n{\n\tunsigned int previous = prb_previous_blk_num(rb);\n\treturn prb_lookup_block(po, rb, previous, status);\n}\n\nstatic void *packet_previous_rx_frame(struct packet_sock *po,\n\t\t\t\t\t     struct packet_ring_buffer *rb,\n\t\t\t\t\t     int status)\n{\n\tif (po->tp_version <= TPACKET_V2)\n\t\treturn packet_previous_frame(po, rb, status);\n\n\treturn __prb_previous_block(po, rb, status);\n}\n\nstatic void packet_increment_rx_head(struct packet_sock *po,\n\t\t\t\t\t    struct packet_ring_buffer *rb)\n{\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\tcase TPACKET_V2:\n\t\treturn packet_increment_head(rb);\n\tcase TPACKET_V3:\n\tdefault:\n\t\tWARN(1, \"TPACKET version not supported.\\n\");\n\t\tBUG();\n\t\treturn;\n\t}\n}\n\nstatic void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\tunsigned int previous = rb->head ? rb->head - 1 : rb->frame_max;\n\treturn packet_lookup_frame(po, rb, previous, status);\n}\n\nstatic void packet_increment_head(struct packet_ring_buffer *buff)\n{\n\tbuff->head = buff->head != buff->frame_max ? buff->head+1 : 0;\n}\n\nstatic void packet_inc_pending(struct packet_ring_buffer *rb)\n{\n\tthis_cpu_inc(*rb->pending_refcnt);\n}\n\nstatic void packet_dec_pending(struct packet_ring_buffer *rb)\n{\n\tthis_cpu_dec(*rb->pending_refcnt);\n}\n\nstatic unsigned int packet_read_pending(const struct packet_ring_buffer *rb)\n{\n\tunsigned int refcnt = 0;\n\tint cpu;\n\n\t/* We don't use pending refcount in rx_ring. */\n\tif (rb->pending_refcnt == NULL)\n\t\treturn 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\trefcnt += *per_cpu_ptr(rb->pending_refcnt, cpu);\n\n\treturn refcnt;\n}\n\nstatic int packet_alloc_pending(struct packet_sock *po)\n{\n\tpo->rx_ring.pending_refcnt = NULL;\n\n\tpo->tx_ring.pending_refcnt = alloc_percpu(unsigned int);\n\tif (unlikely(po->tx_ring.pending_refcnt == NULL))\n\t\treturn -ENOBUFS;\n\n\treturn 0;\n}\n\nstatic void packet_free_pending(struct packet_sock *po)\n{\n\tfree_percpu(po->tx_ring.pending_refcnt);\n}\n\n#define ROOM_POW_OFF\t2\n#define ROOM_NONE\t0x0\n#define ROOM_LOW\t0x1\n#define ROOM_NORMAL\t0x2\n\nstatic bool __tpacket_has_room(struct packet_sock *po, int pow_off)\n{\n\tint idx, len;\n\n\tlen = po->rx_ring.frame_max + 1;\n\tidx = po->rx_ring.head;\n\tif (pow_off)\n\t\tidx += len >> pow_off;\n\tif (idx >= len)\n\t\tidx -= len;\n\treturn packet_lookup_frame(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\n}\n\nstatic bool __tpacket_v3_has_room(struct packet_sock *po, int pow_off)\n{\n\tint idx, len;\n\n\tlen = po->rx_ring.prb_bdqc.knum_blocks;\n\tidx = po->rx_ring.prb_bdqc.kactive_blk_num;\n\tif (pow_off)\n\t\tidx += len >> pow_off;\n\tif (idx >= len)\n\t\tidx -= len;\n\treturn prb_lookup_block(po, &po->rx_ring, idx, TP_STATUS_KERNEL);\n}\n\nstatic int __packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\n{\n\tstruct sock *sk = &po->sk;\n\tint ret = ROOM_NONE;\n\n\tif (po->prot_hook.func != tpacket_rcv) {\n\t\tint avail = sk->sk_rcvbuf - atomic_read(&sk->sk_rmem_alloc)\n\t\t\t\t\t  - (skb ? skb->truesize : 0);\n\t\tif (avail > (sk->sk_rcvbuf >> ROOM_POW_OFF))\n\t\t\treturn ROOM_NORMAL;\n\t\telse if (avail > 0)\n\t\t\treturn ROOM_LOW;\n\t\telse\n\t\t\treturn ROOM_NONE;\n\t}\n\n\tif (po->tp_version == TPACKET_V3) {\n\t\tif (__tpacket_v3_has_room(po, ROOM_POW_OFF))\n\t\t\tret = ROOM_NORMAL;\n\t\telse if (__tpacket_v3_has_room(po, 0))\n\t\t\tret = ROOM_LOW;\n\t} else {\n\t\tif (__tpacket_has_room(po, ROOM_POW_OFF))\n\t\t\tret = ROOM_NORMAL;\n\t\telse if (__tpacket_has_room(po, 0))\n\t\t\tret = ROOM_LOW;\n\t}\n\n\treturn ret;\n}\n\nstatic int packet_rcv_has_room(struct packet_sock *po, struct sk_buff *skb)\n{\n\tint ret;\n\tbool has_room;\n\n\tspin_lock_bh(&po->sk.sk_receive_queue.lock);\n\tret = __packet_rcv_has_room(po, skb);\n\thas_room = ret == ROOM_NORMAL;\n\tif (po->pressure == has_room)\n\t\tpo->pressure = !has_room;\n\tspin_unlock_bh(&po->sk.sk_receive_queue.lock);\n\n\treturn ret;\n}\n\nstatic void packet_sock_destruct(struct sock *sk)\n{\n\tskb_queue_purge(&sk->sk_error_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(refcount_read(&sk->sk_wmem_alloc));\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive packet socket: %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tsk_refcnt_debug_dec(sk);\n}\n\nstatic bool fanout_flow_is_huge(struct packet_sock *po, struct sk_buff *skb)\n{\n\tu32 rxhash;\n\tint i, count = 0;\n\n\trxhash = skb_get_hash(skb);\n\tfor (i = 0; i < ROLLOVER_HLEN; i++)\n\t\tif (po->rollover->history[i] == rxhash)\n\t\t\tcount++;\n\n\tpo->rollover->history[prandom_u32() % ROLLOVER_HLEN] = rxhash;\n\treturn count > (ROLLOVER_HLEN >> 1);\n}\n\nstatic unsigned int fanout_demux_hash(struct packet_fanout *f,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      unsigned int num)\n{\n\treturn reciprocal_scale(__skb_get_hash_symmetric(skb), num);\n}\n\nstatic unsigned int fanout_demux_lb(struct packet_fanout *f,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    unsigned int num)\n{\n\tunsigned int val = atomic_inc_return(&f->rr_cur);\n\n\treturn val % num;\n}\n\nstatic unsigned int fanout_demux_cpu(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\treturn smp_processor_id() % num;\n}\n\nstatic unsigned int fanout_demux_rnd(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\treturn prandom_u32_max(num);\n}\n\nstatic unsigned int fanout_demux_rollover(struct packet_fanout *f,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  unsigned int idx, bool try_self,\n\t\t\t\t\t  unsigned int num)\n{\n\tstruct packet_sock *po, *po_next, *po_skip = NULL;\n\tunsigned int i, j, room = ROOM_NONE;\n\n\tpo = pkt_sk(f->arr[idx]);\n\n\tif (try_self) {\n\t\troom = packet_rcv_has_room(po, skb);\n\t\tif (room == ROOM_NORMAL ||\n\t\t    (room == ROOM_LOW && !fanout_flow_is_huge(po, skb)))\n\t\t\treturn idx;\n\t\tpo_skip = po;\n\t}\n\n\ti = j = min_t(int, po->rollover->sock, num - 1);\n\tdo {\n\t\tpo_next = pkt_sk(f->arr[i]);\n\t\tif (po_next != po_skip && !po_next->pressure &&\n\t\t    packet_rcv_has_room(po_next, skb) == ROOM_NORMAL) {\n\t\t\tif (i != j)\n\t\t\t\tpo->rollover->sock = i;\n\t\t\tatomic_long_inc(&po->rollover->num);\n\t\t\tif (room == ROOM_LOW)\n\t\t\t\tatomic_long_inc(&po->rollover->num_huge);\n\t\t\treturn i;\n\t\t}\n\n\t\tif (++i == num)\n\t\t\ti = 0;\n\t} while (i != j);\n\n\tatomic_long_inc(&po->rollover->num_failed);\n\treturn idx;\n}\n\nstatic unsigned int fanout_demux_qm(struct packet_fanout *f,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    unsigned int num)\n{\n\treturn skb_get_queue_mapping(skb) % num;\n}\n\nstatic unsigned int fanout_demux_bpf(struct packet_fanout *f,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     unsigned int num)\n{\n\tstruct bpf_prog *prog;\n\tunsigned int ret = 0;\n\n\trcu_read_lock();\n\tprog = rcu_dereference(f->bpf_prog);\n\tif (prog)\n\t\tret = bpf_prog_run_clear_cb(prog, skb) % num;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic bool fanout_has_flag(struct packet_fanout *f, u16 flag)\n{\n\treturn f->flags & (flag >> 8);\n}\n\nstatic int packet_rcv_fanout(struct sk_buff *skb, struct net_device *dev,\n\t\t\t     struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct packet_fanout *f = pt->af_packet_priv;\n\tunsigned int num = READ_ONCE(f->num_members);\n\tstruct net *net = read_pnet(&f->net);\n\tstruct packet_sock *po;\n\tunsigned int idx;\n\n\tif (!net_eq(dev_net(dev), net) || !num) {\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\tif (fanout_has_flag(f, PACKET_FANOUT_FLAG_DEFRAG)) {\n\t\tskb = ip_check_defrag(net, skb, IP_DEFRAG_AF_PACKET);\n\t\tif (!skb)\n\t\t\treturn 0;\n\t}\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_HASH:\n\tdefault:\n\t\tidx = fanout_demux_hash(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_LB:\n\t\tidx = fanout_demux_lb(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_CPU:\n\t\tidx = fanout_demux_cpu(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_RND:\n\t\tidx = fanout_demux_rnd(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_QM:\n\t\tidx = fanout_demux_qm(f, skb, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tidx = fanout_demux_rollover(f, skb, 0, false, num);\n\t\tbreak;\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tidx = fanout_demux_bpf(f, skb, num);\n\t\tbreak;\n\t}\n\n\tif (fanout_has_flag(f, PACKET_FANOUT_FLAG_ROLLOVER))\n\t\tidx = fanout_demux_rollover(f, skb, idx, true, num);\n\n\tpo = pkt_sk(f->arr[idx]);\n\treturn po->prot_hook.func(skb, dev, &po->prot_hook, orig_dev);\n}\n\nDEFINE_MUTEX(fanout_mutex);\nEXPORT_SYMBOL_GPL(fanout_mutex);\nstatic LIST_HEAD(fanout_list);\nstatic u16 fanout_next_id;\n\nstatic void __fanout_link(struct sock *sk, struct packet_sock *po)\n{\n\tstruct packet_fanout *f = po->fanout;\n\n\tspin_lock(&f->lock);\n\tf->arr[f->num_members] = sk;\n\tsmp_wmb();\n\tf->num_members++;\n\tif (f->num_members == 1)\n\t\tdev_add_pack(&f->prot_hook);\n\tspin_unlock(&f->lock);\n}\n\nstatic void __fanout_unlink(struct sock *sk, struct packet_sock *po)\n{\n\tstruct packet_fanout *f = po->fanout;\n\tint i;\n\n\tspin_lock(&f->lock);\n\tfor (i = 0; i < f->num_members; i++) {\n\t\tif (f->arr[i] == sk)\n\t\t\tbreak;\n\t}\n\tBUG_ON(i >= f->num_members);\n\tf->arr[i] = f->arr[f->num_members - 1];\n\tf->num_members--;\n\tif (f->num_members == 0)\n\t\t__dev_remove_pack(&f->prot_hook);\n\tspin_unlock(&f->lock);\n}\n\nstatic bool match_fanout_group(struct packet_type *ptype, struct sock *sk)\n{\n\tif (sk->sk_family != PF_PACKET)\n\t\treturn false;\n\n\treturn ptype->af_packet_priv == pkt_sk(sk)->fanout;\n}\n\nstatic void fanout_init_data(struct packet_fanout *f)\n{\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_LB:\n\t\tatomic_set(&f->rr_cur, 0);\n\t\tbreak;\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tRCU_INIT_POINTER(f->bpf_prog, NULL);\n\t\tbreak;\n\t}\n}\n\nstatic void __fanout_set_data_bpf(struct packet_fanout *f, struct bpf_prog *new)\n{\n\tstruct bpf_prog *old;\n\n\tspin_lock(&f->lock);\n\told = rcu_dereference_protected(f->bpf_prog, lockdep_is_held(&f->lock));\n\trcu_assign_pointer(f->bpf_prog, new);\n\tspin_unlock(&f->lock);\n\n\tif (old) {\n\t\tsynchronize_net();\n\t\tbpf_prog_destroy(old);\n\t}\n}\n\nstatic int fanout_set_data_cbpf(struct packet_sock *po, char __user *data,\n\t\t\t\tunsigned int len)\n{\n\tstruct bpf_prog *new;\n\tstruct sock_fprog fprog;\n\tint ret;\n\n\tif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\tif (len != sizeof(fprog))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&fprog, data, len))\n\t\treturn -EFAULT;\n\n\tret = bpf_prog_create_from_user(&new, &fprog, NULL, false);\n\tif (ret)\n\t\treturn ret;\n\n\t__fanout_set_data_bpf(po->fanout, new);\n\treturn 0;\n}\n\nstatic int fanout_set_data_ebpf(struct packet_sock *po, char __user *data,\n\t\t\t\tunsigned int len)\n{\n\tstruct bpf_prog *new;\n\tu32 fd;\n\n\tif (sock_flag(&po->sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\tif (len != sizeof(fd))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&fd, data, len))\n\t\treturn -EFAULT;\n\n\tnew = bpf_prog_get_type(fd, BPF_PROG_TYPE_SOCKET_FILTER);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\n\t__fanout_set_data_bpf(po->fanout, new);\n\treturn 0;\n}\n\nstatic int fanout_set_data(struct packet_sock *po, char __user *data,\n\t\t\t   unsigned int len)\n{\n\tswitch (po->fanout->type) {\n\tcase PACKET_FANOUT_CBPF:\n\t\treturn fanout_set_data_cbpf(po, data, len);\n\tcase PACKET_FANOUT_EBPF:\n\t\treturn fanout_set_data_ebpf(po, data, len);\n\tdefault:\n\t\treturn -EINVAL;\n\t};\n}\n\nstatic void fanout_release_data(struct packet_fanout *f)\n{\n\tswitch (f->type) {\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\t__fanout_set_data_bpf(f, NULL);\n\t};\n}\n\nstatic bool __fanout_id_is_free(struct sock *sk, u16 candidate_id)\n{\n\tstruct packet_fanout *f;\n\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == candidate_id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\nstatic bool fanout_find_new_id(struct sock *sk, u16 *new_id)\n{\n\tu16 id = fanout_next_id;\n\n\tdo {\n\t\tif (__fanout_id_is_free(sk, id)) {\n\t\t\t*new_id = id;\n\t\t\tfanout_next_id = id + 1;\n\t\t\treturn true;\n\t\t}\n\n\t\tid++;\n\t} while (id != fanout_next_id);\n\n\treturn false;\n}\n\nstatic int fanout_add(struct sock *sk, u16 id, u16 type_flags)\n{\n\tstruct packet_rollover *rollover = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f, *match;\n\tu8 type = type_flags & 0xff;\n\tu8 flags = type_flags >> 8;\n\tint err;\n\n\tswitch (type) {\n\tcase PACKET_FANOUT_ROLLOVER:\n\t\tif (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)\n\t\t\treturn -EINVAL;\n\tcase PACKET_FANOUT_HASH:\n\tcase PACKET_FANOUT_LB:\n\tcase PACKET_FANOUT_CPU:\n\tcase PACKET_FANOUT_RND:\n\tcase PACKET_FANOUT_QM:\n\tcase PACKET_FANOUT_CBPF:\n\tcase PACKET_FANOUT_EBPF:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&fanout_mutex);\n\n\terr = -EINVAL;\n\tif (!po->running)\n\t\tgoto out;\n\n\terr = -EALREADY;\n\tif (po->fanout)\n\t\tgoto out;\n\n\tif (type == PACKET_FANOUT_ROLLOVER ||\n\t    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {\n\t\terr = -ENOMEM;\n\t\trollover = kzalloc(sizeof(*rollover), GFP_KERNEL);\n\t\tif (!rollover)\n\t\t\tgoto out;\n\t\tatomic_long_set(&rollover->num, 0);\n\t\tatomic_long_set(&rollover->num_huge, 0);\n\t\tatomic_long_set(&rollover->num_failed, 0);\n\t\tpo->rollover = rollover;\n\t}\n\n\tif (type_flags & PACKET_FANOUT_FLAG_UNIQUEID) {\n\t\tif (id != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!fanout_find_new_id(sk, &id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t/* ephemeral flag for the first socket in the group: drop it */\n\t\tflags &= ~(PACKET_FANOUT_FLAG_UNIQUEID >> 8);\n\t}\n\n\tmatch = NULL;\n\tlist_for_each_entry(f, &fanout_list, list) {\n\t\tif (f->id == id &&\n\t\t    read_pnet(&f->net) == sock_net(sk)) {\n\t\t\tmatch = f;\n\t\t\tbreak;\n\t\t}\n\t}\n\terr = -EINVAL;\n\tif (match && match->flags != flags)\n\t\tgoto out;\n\tif (!match) {\n\t\terr = -ENOMEM;\n\t\tmatch = kzalloc(sizeof(*match), GFP_KERNEL);\n\t\tif (!match)\n\t\t\tgoto out;\n\t\twrite_pnet(&match->net, sock_net(sk));\n\t\tmatch->id = id;\n\t\tmatch->type = type;\n\t\tmatch->flags = flags;\n\t\tINIT_LIST_HEAD(&match->list);\n\t\tspin_lock_init(&match->lock);\n\t\trefcount_set(&match->sk_ref, 0);\n\t\tfanout_init_data(match);\n\t\tmatch->prot_hook.type = po->prot_hook.type;\n\t\tmatch->prot_hook.dev = po->prot_hook.dev;\n\t\tmatch->prot_hook.func = packet_rcv_fanout;\n\t\tmatch->prot_hook.af_packet_priv = match;\n\t\tmatch->prot_hook.id_match = match_fanout_group;\n\t\tlist_add(&match->list, &fanout_list);\n\t}\n\terr = -EINVAL;\n\tif (match->type == type &&\n\t    match->prot_hook.type == po->prot_hook.type &&\n\t    match->prot_hook.dev == po->prot_hook.dev) {\n\t\terr = -ENOSPC;\n\t\tif (refcount_read(&match->sk_ref) < PACKET_FANOUT_MAX) {\n\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\tpo->fanout = match;\n\t\t\trefcount_set(&match->sk_ref, refcount_read(&match->sk_ref) + 1);\n\t\t\t__fanout_link(sk, po);\n\t\t\terr = 0;\n\t\t}\n\t}\nout:\n\tif (err && rollover) {\n\t\tkfree(rollover);\n\t\tpo->rollover = NULL;\n\t}\n\tmutex_unlock(&fanout_mutex);\n\treturn err;\n}\n\n/* If pkt_sk(sk)->fanout->sk_ref is zero, this function removes\n * pkt_sk(sk)->fanout from fanout_list and returns pkt_sk(sk)->fanout.\n * It is the responsibility of the caller to call fanout_release_data() and\n * free the returned packet_fanout (after synchronize_net())\n */\nstatic struct packet_fanout *fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (refcount_dec_and_test(&f->sk_ref))\n\t\t\tlist_del(&f->list);\n\t\telse\n\t\t\tf = NULL;\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\treturn f;\n}\n\nstatic bool packet_extra_vlan_len_allowed(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\t/* Earlier code assumed this would be a VLAN pkt, double-check\n\t * this now that we have the actual packet in hand. We can only\n\t * do this check on Ethernet devices.\n\t */\n\tif (unlikely(dev->type != ARPHRD_ETHER))\n\t\treturn false;\n\n\tskb_reset_mac_header(skb);\n\treturn likely(eth_hdr(skb)->h_proto == htons(ETH_P_8021Q));\n}\n\nstatic const struct proto_ops packet_ops;\n\nstatic const struct proto_ops packet_ops_spkt;\n\nstatic int packet_rcv_spkt(struct sk_buff *skb, struct net_device *dev,\n\t\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_pkt *spkt;\n\n\t/*\n\t *\tWhen we registered the protocol we saved the socket in the data\n\t *\tfield for just this event.\n\t */\n\n\tsk = pt->af_packet_priv;\n\n\t/*\n\t *\tYank back the headers [hope the device set this\n\t *\tright or kerboom...]\n\t *\n\t *\tIncoming packets have ll header pulled,\n\t *\tpush it back.\n\t *\n\t *\tFor outgoing ones skb->data == skb_mac_header(skb)\n\t *\tso that this procedure is noop.\n\t */\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto out;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto out;\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (skb == NULL)\n\t\tgoto oom;\n\n\t/* drop any routing info */\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspkt = &PACKET_SKB_CB(skb)->sa.pkt;\n\n\tskb_push(skb, skb->data - skb_mac_header(skb));\n\n\t/*\n\t *\tThe SOCK_PACKET socket receives _all_ frames.\n\t */\n\n\tspkt->spkt_family = dev->type;\n\tstrlcpy(spkt->spkt_device, dev->name, sizeof(spkt->spkt_device));\n\tspkt->spkt_protocol = skb->protocol;\n\n\t/*\n\t *\tCharge the memory to the socket. This is done specifically\n\t *\tto prevent sockets using all the memory up.\n\t */\n\n\tif (sock_queue_rcv_skb(sk, skb) == 0)\n\t\treturn 0;\n\nout:\n\tkfree_skb(skb);\noom:\n\treturn 0;\n}\n\n\n/*\n *\tOutput a raw packet to a device layer. This bypasses all the other\n *\tprotocol layers and you must therefore supply it with a complete frame\n */\n\nstatic int packet_sendmsg_spkt(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tDECLARE_SOCKADDR(struct sockaddr_pkt *, saddr, msg->msg_name);\n\tstruct sk_buff *skb = NULL;\n\tstruct net_device *dev;\n\tstruct sockcm_cookie sockc;\n\t__be16 proto = 0;\n\tint err;\n\tint extra_len = 0;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (saddr) {\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr))\n\t\t\treturn -EINVAL;\n\t\tif (msg->msg_namelen == sizeof(struct sockaddr_pkt))\n\t\t\tproto = saddr->spkt_protocol;\n\t} else\n\t\treturn -ENOTCONN;\t/* SOCK_PACKET must be sent giving an address */\n\n\t/*\n\t *\tFind the device first to size check it\n\t */\n\n\tsaddr->spkt_device[sizeof(saddr->spkt_device) - 1] = 0;\nretry:\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device);\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\t/*\n\t * You may not queue a frame bigger than the mtu. This is the lowest level\n\t * raw protocol and you must do your own fragmentation at this level.\n\t */\n\n\tif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\n\t\tif (!netif_supports_nofcs(dev)) {\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\textra_len = 4; /* We're doing our own CRC */\n\t}\n\n\terr = -EMSGSIZE;\n\tif (len > dev->mtu + dev->hard_header_len + VLAN_HLEN + extra_len)\n\t\tgoto out_unlock;\n\n\tif (!skb) {\n\t\tsize_t reserved = LL_RESERVED_SPACE(dev);\n\t\tint tlen = dev->needed_tailroom;\n\t\tunsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;\n\n\t\trcu_read_unlock();\n\t\tskb = sock_wmalloc(sk, len + reserved + tlen, 0, GFP_KERNEL);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOBUFS;\n\t\t/* FIXME: Save some space for broken drivers that write a hard\n\t\t * header at transmission time by themselves. PPP is the notable\n\t\t * one here. This should really be fixed at the driver level.\n\t\t */\n\t\tskb_reserve(skb, reserved);\n\t\tskb_reset_network_header(skb);\n\n\t\t/* Try to align data part correctly */\n\t\tif (hhlen) {\n\t\t\tskb->data -= hhlen;\n\t\t\tskb->tail -= hhlen;\n\t\t\tif (len < hhlen)\n\t\t\t\tskb_reset_network_header(skb);\n\t\t}\n\t\terr = memcpy_from_msg(skb_put(skb, len), msg, len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto retry;\n\t}\n\n\tif (!dev_validate_header(dev, skb->data, len)) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tif (len > (dev->mtu + dev->hard_header_len + extra_len) &&\n\t    !packet_extra_vlan_len_allowed(dev, skb)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_unlock;\n\t}\n\n\tsockc.tsflags = sk->sk_tsflags;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_unlock;\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tsock_tx_timestamp(sk, sockc.tsflags, &skb_shinfo(skb)->tx_flags);\n\n\tif (unlikely(extra_len == 4))\n\t\tskb->no_fcs = 1;\n\n\tskb_probe_transport_header(skb, 0);\n\n\tdev_queue_xmit(skb);\n\trcu_read_unlock();\n\treturn len;\n\nout_unlock:\n\trcu_read_unlock();\nout_free:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic unsigned int run_filter(struct sk_buff *skb,\n\t\t\t       const struct sock *sk,\n\t\t\t       unsigned int res)\n{\n\tstruct sk_filter *filter;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter != NULL)\n\t\tres = bpf_prog_run_clear_cb(filter->prog, skb);\n\trcu_read_unlock();\n\n\treturn res;\n}\n\nstatic int packet_rcv_vnet(struct msghdr *msg, const struct sk_buff *skb,\n\t\t\t   size_t *len)\n{\n\tstruct virtio_net_hdr vnet_hdr;\n\n\tif (*len < sizeof(vnet_hdr))\n\t\treturn -EINVAL;\n\t*len -= sizeof(vnet_hdr);\n\n\tif (virtio_net_hdr_from_skb(skb, &vnet_hdr, vio_le(), true))\n\t\treturn -EINVAL;\n\n\treturn memcpy_to_msg(msg, (void *)&vnet_hdr, sizeof(vnet_hdr));\n}\n\n/*\n * This function makes lazy skb cloning in hope that most of packets\n * are discarded by BPF.\n *\n * Note tricky part: we DO mangle shared skb! skb->data, skb->len\n * and skb->cb are mangled. It works because (and until) packets\n * falling here are owned by current CPU. Output packets are cloned\n * by dev_queue_xmit_nit(), input packets are processed by net_bh\n * sequencially, so that if we return skb to original state on exit,\n * we will not harm anyone.\n */\n\nstatic int packet_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t      struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_ll *sll;\n\tstruct packet_sock *po;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tbool is_drop_n_account = false;\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tskb->dev = dev;\n\n\tif (dev->header_ops) {\n\t\t/* The device has an explicit notion of ll header,\n\t\t * exported to higher levels.\n\t\t *\n\t\t * Otherwise, the device hides details of its frame\n\t\t * structure, so that corresponding packet head is\n\t\t * never delivered to user.\n\t\t */\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto drop_n_acct;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (nskb == NULL)\n\t\t\tgoto drop_n_acct;\n\n\t\tif (skb_head != skb->data) {\n\t\t\tskb->data = skb_head;\n\t\t\tskb->len = skb_len;\n\t\t}\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tsock_skb_cb_check_size(sizeof(*PACKET_SKB_CB(skb)) + MAX_ADDR_LEN - 8);\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\n\t/* sll->sll_family and sll->sll_protocol are set in packet_recvmsg().\n\t * Use their space for storing the original skb length.\n\t */\n\tPACKET_SKB_CB(skb)->sa.origlen = skb->len;\n\n\tif (pskb_trim(skb, snaplen))\n\t\tgoto drop_n_acct;\n\n\tskb_set_owner_r(skb, sk);\n\tskb->dev = NULL;\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.stats1.tp_packets++;\n\tsock_skb_set_dropcount(sk, skb);\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tsk->sk_data_ready(sk);\n\treturn 0;\n\ndrop_n_acct:\n\tis_drop_n_account = true;\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.stats1.tp_drops++;\n\tatomic_inc(&sk->sk_drops);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (po->stats.stats1.tp_drops)\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tif (do_vnet) {\n\t\tif (virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t\t    vio_le(), true)) {\n\t\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t\tgoto drop_n_account;\n\t\t}\n\t}\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\n\t\tgetnstimeofday(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\t__packet_set_status(po, h.raw, status);\n\t\tsk->sk_data_ready(sk);\n\t} else {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tis_drop_n_account = true;\n\tpo->stats.stats1.tp_drops++;\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}\n\nstatic void tpacket_destruct_skb(struct sk_buff *skb)\n{\n\tstruct packet_sock *po = pkt_sk(skb->sk);\n\n\tif (likely(po->tx_ring.pg_vec)) {\n\t\tvoid *ph;\n\t\t__u32 ts;\n\n\t\tph = skb_shinfo(skb)->destructor_arg;\n\t\tpacket_dec_pending(&po->tx_ring);\n\n\t\tts = __packet_set_timestamp(po, ph, skb);\n\t\t__packet_set_status(po, ph, TP_STATUS_AVAILABLE | ts);\n\t}\n\n\tsock_wfree(skb);\n}\n\nstatic void tpacket_set_protocol(const struct net_device *dev,\n\t\t\t\t struct sk_buff *skb)\n{\n\tif (dev->type == ARPHRD_ETHER) {\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = eth_hdr(skb)->h_proto;\n\t}\n}\n\nstatic int __packet_snd_vnet_parse(struct virtio_net_hdr *vnet_hdr, size_t len)\n{\n\tif ((vnet_hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t    (__virtio16_to_cpu(vio_le(), vnet_hdr->csum_start) +\n\t     __virtio16_to_cpu(vio_le(), vnet_hdr->csum_offset) + 2 >\n\t      __virtio16_to_cpu(vio_le(), vnet_hdr->hdr_len)))\n\t\tvnet_hdr->hdr_len = __cpu_to_virtio16(vio_le(),\n\t\t\t __virtio16_to_cpu(vio_le(), vnet_hdr->csum_start) +\n\t\t\t__virtio16_to_cpu(vio_le(), vnet_hdr->csum_offset) + 2);\n\n\tif (__virtio16_to_cpu(vio_le(), vnet_hdr->hdr_len) > len)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int packet_snd_vnet_parse(struct msghdr *msg, size_t *len,\n\t\t\t\t struct virtio_net_hdr *vnet_hdr)\n{\n\tif (*len < sizeof(*vnet_hdr))\n\t\treturn -EINVAL;\n\t*len -= sizeof(*vnet_hdr);\n\n\tif (!copy_from_iter_full(vnet_hdr, sizeof(*vnet_hdr), &msg->msg_iter))\n\t\treturn -EFAULT;\n\n\treturn __packet_snd_vnet_parse(vnet_hdr, *len);\n}\n\nstatic int tpacket_fill_skb(struct packet_sock *po, struct sk_buff *skb,\n\t\tvoid *frame, struct net_device *dev, void *data, int tp_len,\n\t\t__be16 proto, unsigned char *addr, int hlen, int copylen,\n\t\tconst struct sockcm_cookie *sockc)\n{\n\tunion tpacket_uhdr ph;\n\tint to_write, offset, len, nr_frags, len_max;\n\tstruct socket *sock = po->sk.sk_socket;\n\tstruct page *page;\n\tint err;\n\n\tph.raw = frame;\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = po->sk.sk_priority;\n\tskb->mark = po->sk.sk_mark;\n\tsock_tx_timestamp(&po->sk, sockc->tsflags, &skb_shinfo(skb)->tx_flags);\n\tskb_shinfo(skb)->destructor_arg = ph.raw;\n\n\tskb_reserve(skb, hlen);\n\tskb_reset_network_header(skb);\n\n\tto_write = tp_len;\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\terr = dev_hard_header(skb, dev, ntohs(proto), addr,\n\t\t\t\tNULL, tp_len);\n\t\tif (unlikely(err < 0))\n\t\t\treturn -EINVAL;\n\t} else if (copylen) {\n\t\tint hdrlen = min_t(int, copylen, tp_len);\n\n\t\tskb_push(skb, dev->hard_header_len);\n\t\tskb_put(skb, copylen - dev->hard_header_len);\n\t\terr = skb_store_bits(skb, 0, data, hdrlen);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t\tif (!dev_validate_header(dev, skb->data, hdrlen))\n\t\t\treturn -EINVAL;\n\t\tif (!skb->protocol)\n\t\t\ttpacket_set_protocol(dev, skb);\n\n\t\tdata += hdrlen;\n\t\tto_write -= hdrlen;\n\t}\n\n\toffset = offset_in_page(data);\n\tlen_max = PAGE_SIZE - offset;\n\tlen = ((to_write > len_max) ? len_max : to_write);\n\n\tskb->data_len = to_write;\n\tskb->len += to_write;\n\tskb->truesize += to_write;\n\trefcount_add(to_write, &po->sk.sk_wmem_alloc);\n\n\twhile (likely(to_write)) {\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\tpr_err(\"Packet exceed the number of skb frags(%lu)\\n\",\n\t\t\t       MAX_SKB_FRAGS);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tpage = pgv_to_page(data);\n\t\tdata += len;\n\t\tflush_dcache_page(page);\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, nr_frags, page, offset, len);\n\t\tto_write -= len;\n\t\toffset = 0;\n\t\tlen_max = PAGE_SIZE;\n\t\tlen = ((to_write > len_max) ? len_max : to_write);\n\t}\n\n\tskb_probe_transport_header(skb, 0);\n\n\treturn tp_len;\n}\n\nstatic int tpacket_parse_header(struct packet_sock *po, void *frame,\n\t\t\t\tint size_max, void **data)\n{\n\tunion tpacket_uhdr ph;\n\tint tp_len, off;\n\n\tph.raw = frame;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V3:\n\t\tif (ph.h3->tp_next_offset != 0) {\n\t\t\tpr_warn_once(\"variable sized slot not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttp_len = ph.h3->tp_len;\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\ttp_len = ph.h2->tp_len;\n\t\tbreak;\n\tdefault:\n\t\ttp_len = ph.h1->tp_len;\n\t\tbreak;\n\t}\n\tif (unlikely(tp_len > size_max)) {\n\t\tpr_err(\"packet size is too long (%d > %d)\\n\", tp_len, size_max);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (unlikely(po->tp_tx_has_off)) {\n\t\tint off_min, off_max;\n\n\t\toff_min = po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\t\toff_max = po->tx_ring.frame_size - tp_len;\n\t\tif (po->sk.sk_type == SOCK_DGRAM) {\n\t\t\tswitch (po->tp_version) {\n\t\t\tcase TPACKET_V3:\n\t\t\t\toff = ph.h3->tp_net;\n\t\t\t\tbreak;\n\t\t\tcase TPACKET_V2:\n\t\t\t\toff = ph.h2->tp_net;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\toff = ph.h1->tp_net;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tswitch (po->tp_version) {\n\t\t\tcase TPACKET_V3:\n\t\t\t\toff = ph.h3->tp_mac;\n\t\t\t\tbreak;\n\t\t\tcase TPACKET_V2:\n\t\t\t\toff = ph.h2->tp_mac;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\toff = ph.h1->tp_mac;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (unlikely((off < off_min) || (off_max < off)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\toff = po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\t}\n\n\t*data = frame + off;\n\treturn tp_len;\n}\n\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg)\n{\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\tstruct virtio_net_hdr *vnet_hdr = NULL;\n\tstruct sockcm_cookie sockc;\n\t__be16 proto;\n\tint err, reserve = 0;\n\tvoid *ph;\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\n\tbool need_wait = !(msg->msg_flags & MSG_DONTWAIT);\n\tint tp_len, size_max;\n\tunsigned char *addr;\n\tvoid *data;\n\tint len_sum = 0;\n\tint status = TP_STATUS_AVAILABLE;\n\tint hlen, tlen, copylen = 0;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\tif (likely(saddr == NULL)) {\n\t\tdev\t= packet_cached_dev_get(po);\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen\n\t\t\t\t\t+ offsetof(struct sockaddr_ll,\n\t\t\t\t\t\tsll_addr)))\n\t\t\tgoto out;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t\tdev = dev_get_by_index(sock_net(&po->sk), saddr->sll_ifindex);\n\t}\n\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out;\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_put;\n\n\tsockc.tsflags = po->sk.sk_tsflags;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(&po->sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_put;\n\t}\n\n\tif (po->sk.sk_socket->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\tsize_max = po->tx_ring.frame_size\n\t\t- (po->tp_hdrlen - sizeof(struct sockaddr_ll));\n\n\tif ((size_max > dev->mtu + reserve + VLAN_HLEN) && !po->has_vnet_hdr)\n\t\tsize_max = dev->mtu + reserve + VLAN_HLEN;\n\n\tdo {\n\t\tph = packet_current_frame(po, &po->tx_ring,\n\t\t\t\t\t  TP_STATUS_SEND_REQUEST);\n\t\tif (unlikely(ph == NULL)) {\n\t\t\tif (need_wait && need_resched())\n\t\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = NULL;\n\t\ttp_len = tpacket_parse_header(po, ph, size_max, &data);\n\t\tif (tp_len < 0)\n\t\t\tgoto tpacket_error;\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\thlen = LL_RESERVED_SPACE(dev);\n\t\ttlen = dev->needed_tailroom;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tvnet_hdr = data;\n\t\t\tdata += sizeof(*vnet_hdr);\n\t\t\ttp_len -= sizeof(*vnet_hdr);\n\t\t\tif (tp_len < 0 ||\n\t\t\t    __packet_snd_vnet_parse(vnet_hdr, tp_len)) {\n\t\t\t\ttp_len = -EINVAL;\n\t\t\t\tgoto tpacket_error;\n\t\t\t}\n\t\t\tcopylen = __virtio16_to_cpu(vio_le(),\n\t\t\t\t\t\t    vnet_hdr->hdr_len);\n\t\t}\n\t\tcopylen = max_t(int, copylen, dev->hard_header_len);\n\t\tskb = sock_alloc_send_skb(&po->sk,\n\t\t\t\thlen + tlen + sizeof(struct sockaddr_ll) +\n\t\t\t\t(copylen - dev->hard_header_len),\n\t\t\t\t!need_wait, &err);\n\n\t\tif (unlikely(skb == NULL)) {\n\t\t\t/* we assume the socket was initially writeable ... */\n\t\t\tif (likely(len_sum > 0))\n\t\t\t\terr = len_sum;\n\t\t\tgoto out_status;\n\t\t}\n\t\ttp_len = tpacket_fill_skb(po, skb, ph, dev, data, tp_len, proto,\n\t\t\t\t\t  addr, hlen, copylen, &sockc);\n\t\tif (likely(tp_len >= 0) &&\n\t\t    tp_len > dev->mtu + reserve &&\n\t\t    !po->has_vnet_hdr &&\n\t\t    !packet_extra_vlan_len_allowed(dev, skb))\n\t\t\ttp_len = -EMSGSIZE;\n\n\t\tif (unlikely(tp_len < 0)) {\ntpacket_error:\n\t\t\tif (po->tp_loss) {\n\t\t\t\t__packet_set_status(po, ph,\n\t\t\t\t\t\tTP_STATUS_AVAILABLE);\n\t\t\t\tpacket_increment_head(&po->tx_ring);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tstatus = TP_STATUS_WRONG_FORMAT;\n\t\t\t\terr = tp_len;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t}\n\n\t\tif (po->has_vnet_hdr && virtio_net_hdr_to_skb(skb, vnet_hdr,\n\t\t\t\t\t\t\t      vio_le())) {\n\t\t\ttp_len = -EINVAL;\n\t\t\tgoto tpacket_error;\n\t\t}\n\n\t\tskb->destructor = tpacket_destruct_skb;\n\t\t__packet_set_status(po, ph, TP_STATUS_SENDING);\n\t\tpacket_inc_pending(&po->tx_ring);\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\terr = po->xmit(skb);\n\t\tif (unlikely(err > 0)) {\n\t\t\terr = net_xmit_errno(err);\n\t\t\tif (err && __packet_get_status(po, ph) ==\n\t\t\t\t   TP_STATUS_AVAILABLE) {\n\t\t\t\t/* skb was destructed already */\n\t\t\t\tskb = NULL;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t\t/*\n\t\t\t * skb was dropped but not destructed yet;\n\t\t\t * let's treat it like congestion or err < 0\n\t\t\t */\n\t\t\terr = 0;\n\t\t}\n\t\tpacket_increment_head(&po->tx_ring);\n\t\tlen_sum += tp_len;\n\t} while (likely((ph != NULL) ||\n\t\t/* Note: packet_read_pending() might be slow if we have\n\t\t * to call it as it's per_cpu variable, but in fast-path\n\t\t * we already short-circuit the loop with the first\n\t\t * condition, and luckily don't have to go that path\n\t\t * anyway.\n\t\t */\n\t\t (need_wait && packet_read_pending(&po->tx_ring))));\n\n\terr = len_sum;\n\tgoto out_put;\n\nout_status:\n\t__packet_set_status(po, ph, status);\n\tkfree_skb(skb);\nout_put:\n\tdev_put(dev);\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic struct sk_buff *packet_alloc_skb(struct sock *sk, size_t prepad,\n\t\t\t\t        size_t reserve, size_t len,\n\t\t\t\t        size_t linear, int noblock,\n\t\t\t\t        int *err)\n{\n\tstruct sk_buff *skb;\n\n\t/* Under a page?  Don't bother with paged skb. */\n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   err, 0);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, reserve);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic int packet_snd(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, saddr, msg->msg_name);\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tunsigned char *addr;\n\tint err, reserve = 0;\n\tstruct sockcm_cookie sockc;\n\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\tint offset = 0;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint hlen, tlen, linear;\n\tint extra_len = 0;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (likely(saddr == NULL)) {\n\t\tdev\t= packet_cached_dev_get(po);\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))\n\t\t\tgoto out;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t\tdev = dev_get_by_index(sock_net(sk), saddr->sll_ifindex);\n\t}\n\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out_unlock;\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_unlock;\n\n\tsockc.tsflags = sk->sk_tsflags;\n\tsockc.mark = sk->sk_mark;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err))\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (sock->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\tif (po->has_vnet_hdr) {\n\t\terr = packet_snd_vnet_parse(msg, &len, &vnet_hdr);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(sock_flag(sk, SOCK_NOFCS))) {\n\t\tif (!netif_supports_nofcs(dev)) {\n\t\t\terr = -EPROTONOSUPPORT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\textra_len = 4; /* We're doing our own CRC */\n\t}\n\n\terr = -EMSGSIZE;\n\tif (!vnet_hdr.gso_type &&\n\t    (len > dev->mtu + reserve + VLAN_HLEN + extra_len))\n\t\tgoto out_unlock;\n\n\terr = -ENOBUFS;\n\thlen = LL_RESERVED_SPACE(dev);\n\ttlen = dev->needed_tailroom;\n\tlinear = __virtio16_to_cpu(vio_le(), vnet_hdr.hdr_len);\n\tlinear = max(linear, min_t(int, len, dev->hard_header_len));\n\tskb = packet_alloc_skb(sk, hlen + tlen, hlen, len, linear,\n\t\t\t       msg->msg_flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out_unlock;\n\n\tskb_set_network_header(skb, reserve);\n\n\terr = -EINVAL;\n\tif (sock->type == SOCK_DGRAM) {\n\t\toffset = dev_hard_header(skb, dev, ntohs(proto), addr, NULL, len);\n\t\tif (unlikely(offset < 0))\n\t\t\tgoto out_free;\n\t}\n\n\t/* Returns -EFAULT on error */\n\terr = skb_copy_datagram_from_iter(skb, offset, &msg->msg_iter, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type == SOCK_RAW &&\n\t    !dev_validate_header(dev, skb->data, len)) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tsock_tx_timestamp(sk, sockc.tsflags, &skb_shinfo(skb)->tx_flags);\n\n\tif (!vnet_hdr.gso_type && (len > dev->mtu + reserve + extra_len) &&\n\t    !packet_extra_vlan_len_allowed(dev, skb)) {\n\t\terr = -EMSGSIZE;\n\t\tgoto out_free;\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sockc.mark;\n\n\tif (po->has_vnet_hdr) {\n\t\terr = virtio_net_hdr_to_skb(skb, &vnet_hdr, vio_le());\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tlen += sizeof(vnet_hdr);\n\t}\n\n\tskb_probe_transport_header(skb, reserve);\n\n\tif (unlikely(extra_len == 4))\n\t\tskb->no_fcs = 1;\n\n\terr = po->xmit(skb);\n\tif (err > 0 && (err = net_xmit_errno(err)) != 0)\n\t\tgoto out_unlock;\n\n\tdev_put(dev);\n\n\treturn len;\n\nout_free:\n\tkfree_skb(skb);\nout_unlock:\n\tif (dev)\n\t\tdev_put(dev);\nout:\n\treturn err;\n}\n\nstatic int packet_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\n\tif (po->tx_ring.pg_vec)\n\t\treturn tpacket_snd(po, msg);\n\telse\n\t\treturn packet_snd(sock, msg, len);\n}\n\n/*\n *\tClose a PACKET socket. This is fairly simple. We immediately go\n *\tto 'closed' state and remove our protocol entry in the device list.\n */\n\nstatic int packet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po;\n\tstruct packet_fanout *f;\n\tstruct net *net;\n\tunion tpacket_req_u req_u;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tpo = pkt_sk(sk);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_del_node_init_rcu(sk);\n\tmutex_unlock(&net->packet.sklist_lock);\n\n\tpreempt_disable();\n\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\tpreempt_enable();\n\n\tspin_lock(&po->bind_lock);\n\tunregister_prot_hook(sk, false);\n\tpacket_cached_dev_reset(po);\n\n\tif (po->prot_hook.dev) {\n\t\tdev_put(po->prot_hook.dev);\n\t\tpo->prot_hook.dev = NULL;\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tpacket_flush_mclist(sk);\n\n\tif (po->rx_ring.pg_vec) {\n\t\tmemset(&req_u, 0, sizeof(req_u));\n\t\tpacket_set_ring(sk, &req_u, 1, 0);\n\t}\n\n\tif (po->tx_ring.pg_vec) {\n\t\tmemset(&req_u, 0, sizeof(req_u));\n\t\tpacket_set_ring(sk, &req_u, 1, 1);\n\t}\n\n\tf = fanout_release(sk);\n\n\tsynchronize_net();\n\n\tif (f) {\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\t/*\n\t *\tNow the socket is dead. No more input will appear.\n\t */\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\t/* Purge queues */\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tpacket_free_pending(po);\n\tsk_refcnt_debug_release(sk);\n\n\tsock_put(sk);\n\treturn 0;\n}\n\n/*\n *\tAttach a packet hook.\n */\n\nstatic int packet_do_bind(struct sock *sk, const char *name, int ifindex,\n\t\t\t  __be16 proto)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct net_device *dev_curr;\n\t__be16 proto_curr;\n\tbool need_rehook;\n\tstruct net_device *dev = NULL;\n\tint ret = 0;\n\tbool unlisted = false;\n\n\tif (po->fanout)\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tspin_lock(&po->bind_lock);\n\trcu_read_lock();\n\n\tif (name) {\n\t\tdev = dev_get_by_name_rcu(sock_net(sk), name);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (ifindex) {\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), ifindex);\n\t\tif (!dev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (dev)\n\t\tdev_hold(dev);\n\n\tproto_curr = po->prot_hook.type;\n\tdev_curr = po->prot_hook.dev;\n\n\tneed_rehook = proto_curr != proto || dev_curr != dev;\n\n\tif (need_rehook) {\n\t\tif (po->running) {\n\t\t\trcu_read_unlock();\n\t\t\t__unregister_prot_hook(sk, true);\n\t\t\trcu_read_lock();\n\t\t\tdev_curr = po->prot_hook.dev;\n\t\t\tif (dev)\n\t\t\t\tunlisted = !dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t\t\t dev->ifindex);\n\t\t}\n\n\t\tpo->num = proto;\n\t\tpo->prot_hook.type = proto;\n\n\t\tif (unlikely(unlisted)) {\n\t\t\tdev_put(dev);\n\t\t\tpo->prot_hook.dev = NULL;\n\t\t\tpo->ifindex = -1;\n\t\t\tpacket_cached_dev_reset(po);\n\t\t} else {\n\t\t\tpo->prot_hook.dev = dev;\n\t\t\tpo->ifindex = dev ? dev->ifindex : 0;\n\t\t\tpacket_cached_dev_assign(po, dev);\n\t\t}\n\t}\n\tif (dev_curr)\n\t\tdev_put(dev_curr);\n\n\tif (proto == 0 || !need_rehook)\n\t\tgoto out_unlock;\n\n\tif (!unlisted && (!dev || (dev->flags & IFF_UP))) {\n\t\tregister_prot_hook(sk);\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn ret;\n}\n\n/*\n *\tBind a packet socket to a device\n */\n\nstatic int packet_bind_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tchar name[sizeof(uaddr->sa_data) + 1];\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len != sizeof(struct sockaddr))\n\t\treturn -EINVAL;\n\t/* uaddr->sa_data comes from the userspace, it's not guaranteed to be\n\t * zero-terminated.\n\t */\n\tmemcpy(name, uaddr->sa_data, sizeof(uaddr->sa_data));\n\tname[sizeof(uaddr->sa_data)] = 0;\n\n\treturn packet_do_bind(sk, name, 0, pkt_sk(sk)->num);\n}\n\nstatic int packet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_ll *sll = (struct sockaddr_ll *)uaddr;\n\tstruct sock *sk = sock->sk;\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len < sizeof(struct sockaddr_ll))\n\t\treturn -EINVAL;\n\tif (sll->sll_family != AF_PACKET)\n\t\treturn -EINVAL;\n\n\treturn packet_do_bind(sk, NULL, sll->sll_ifindex,\n\t\t\t      sll->sll_protocol ? : pkt_sk(sk)->num);\n}\n\nstatic struct proto packet_proto = {\n\t.name\t  = \"PACKET\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct packet_sock),\n};\n\n/*\n *\tCreate a packet of type SOCK_PACKET.\n */\n\nstatic int packet_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t int kern)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\t__be16 proto = (__force __be16)protocol; /* weird, but documented */\n\tint err;\n\n\tif (!ns_capable(net->user_ns, CAP_NET_RAW))\n\t\treturn -EPERM;\n\tif (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW &&\n\t    sock->type != SOCK_PACKET)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto, kern);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\tsock->ops = &packet_ops;\n\tif (sock->type == SOCK_PACKET)\n\t\tsock->ops = &packet_ops_spkt;\n\n\tsock_init_data(sock, sk);\n\n\tpo = pkt_sk(sk);\n\tsk->sk_family = PF_PACKET;\n\tpo->num = proto;\n\tpo->xmit = dev_queue_xmit;\n\n\terr = packet_alloc_pending(po);\n\tif (err)\n\t\tgoto out2;\n\n\tpacket_cached_dev_reset(po);\n\n\tsk->sk_destruct = packet_sock_destruct;\n\tsk_refcnt_debug_inc(sk);\n\n\t/*\n\t *\tAttach a protocol block\n\t */\n\n\tspin_lock_init(&po->bind_lock);\n\tmutex_init(&po->pg_vec_lock);\n\tpo->rollover = NULL;\n\tpo->prot_hook.func = packet_rcv;\n\n\tif (sock->type == SOCK_PACKET)\n\t\tpo->prot_hook.func = packet_rcv_spkt;\n\n\tpo->prot_hook.af_packet_priv = sk;\n\n\tif (proto) {\n\t\tpo->prot_hook.type = proto;\n\t\tregister_prot_hook(sk);\n\t}\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_add_node_rcu(sk, &net->packet.sklist);\n\tmutex_unlock(&net->packet.sklist_lock);\n\n\tpreempt_disable();\n\tsock_prot_inuse_add(net, &packet_proto, 1);\n\tpreempt_enable();\n\n\treturn 0;\nout2:\n\tsk_free(sk);\nout:\n\treturn err;\n}\n\n/*\n *\tPull a packet from our receive queue and hand it to the user.\n *\tIf necessary we block.\n */\n\nstatic int packet_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t  int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tint vnet_hdr_len = 0;\n\tunsigned int origlen = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = sock_recv_errqueue(sk, msg, len,\n\t\t\t\t\t SOL_PACKET, PACKET_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tif (pkt_sk(sk)->pressure)\n\t\tpacket_rcv_has_room(pkt_sk(sk), NULL);\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\terr = packet_rcv_vnet(msg, skb, &len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tvnet_hdr_len = sizeof(struct virtio_net_hdr);\n\t}\n\n\t/* You lose any data beyond the buffer you gave. If it worries\n\t * a user program they can ask the device for its MTU\n\t * anyway.\n\t */\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock->type != SOCK_PACKET) {\n\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t/* Original length was stored in sockaddr_ll fields */\n\t\toriglen = PACKET_SKB_CB(skb)->sa.origlen;\n\t\tsll->sll_family = AF_PACKET;\n\t\tsll->sll_protocol = skb->protocol;\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name) {\n\t\t/* If the address length field is there to be filled\n\t\t * in, we fill it in now.\n\t\t */\n\t\tif (sock->type == SOCK_PACKET) {\n\t\t\t__sockaddr_check_size(sizeof(struct sockaddr_pkt));\n\t\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\t\t} else {\n\t\t\tstruct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;\n\n\t\t\tmsg->msg_namelen = sll->sll_halen +\n\t\t\t\toffsetof(struct sockaddr_ll, sll_addr);\n\t\t}\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n\t\t       msg->msg_namelen);\n\t}\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t\t  skb_csum_unnecessary(skb)))\n\t\t\taux.tp_status |= TP_STATUS_CSUM_VALID;\n\n\t\taux.tp_len = origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\taux.tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t\taux.tp_vlan_tpid = 0;\n\t\t}\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}\n\nstatic int packet_getname_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk\t= sock->sk;\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tuaddr->sa_family = AF_PACKET;\n\tmemset(uaddr->sa_data, 0, sizeof(uaddr->sa_data));\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), pkt_sk(sk)->ifindex);\n\tif (dev)\n\t\tstrlcpy(uaddr->sa_data, dev->name, sizeof(uaddr->sa_data));\n\trcu_read_unlock();\n\t*uaddr_len = sizeof(*uaddr);\n\n\treturn 0;\n}\n\nstatic int packet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, sll, uaddr);\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_ifindex = po->ifindex;\n\tsll->sll_protocol = po->num;\n\tsll->sll_pkttype = 0;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), po->ifindex);\n\tif (dev) {\n\t\tsll->sll_hatype = dev->type;\n\t\tsll->sll_halen = dev->addr_len;\n\t\tmemcpy(sll->sll_addr, dev->dev_addr, dev->addr_len);\n\t} else {\n\t\tsll->sll_hatype = 0;\t/* Bad: we have no ARPHRD_UNSPEC */\n\t\tsll->sll_halen = 0;\n\t}\n\trcu_read_unlock();\n\t*uaddr_len = offsetof(struct sockaddr_ll, sll_addr) + sll->sll_halen;\n\n\treturn 0;\n}\n\nstatic int packet_dev_mc(struct net_device *dev, struct packet_mclist *i,\n\t\t\t int what)\n{\n\tswitch (i->type) {\n\tcase PACKET_MR_MULTICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_mc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_mc_del(dev, i->addr);\n\t\tbreak;\n\tcase PACKET_MR_PROMISC:\n\t\treturn dev_set_promiscuity(dev, what);\n\tcase PACKET_MR_ALLMULTI:\n\t\treturn dev_set_allmulti(dev, what);\n\tcase PACKET_MR_UNICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_uc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_uc_del(dev, i->addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void packet_dev_mclist_delete(struct net_device *dev,\n\t\t\t\t     struct packet_mclist **mlp)\n{\n\tstruct packet_mclist *ml;\n\n\twhile ((ml = *mlp) != NULL) {\n\t\tif (ml->ifindex == dev->ifindex) {\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t*mlp = ml->next;\n\t\t\tkfree(ml);\n\t\t} else\n\t\t\tmlp = &ml->next;\n\t}\n}\n\nstatic int packet_mc_add(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml, *i;\n\tstruct net_device *dev;\n\tint err;\n\n\trtnl_lock();\n\n\terr = -ENODEV;\n\tdev = __dev_get_by_index(sock_net(sk), mreq->mr_ifindex);\n\tif (!dev)\n\t\tgoto done;\n\n\terr = -EINVAL;\n\tif (mreq->mr_alen > dev->addr_len)\n\t\tgoto done;\n\n\terr = -ENOBUFS;\n\ti = kmalloc(sizeof(*i), GFP_KERNEL);\n\tif (i == NULL)\n\t\tgoto done;\n\n\terr = 0;\n\tfor (ml = po->mclist; ml; ml = ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tml->count++;\n\t\t\t/* Free the new element ... */\n\t\t\tkfree(i);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\ti->type = mreq->mr_type;\n\ti->ifindex = mreq->mr_ifindex;\n\ti->alen = mreq->mr_alen;\n\tmemcpy(i->addr, mreq->mr_address, i->alen);\n\tmemset(i->addr + i->alen, 0, sizeof(i->addr) - i->alen);\n\ti->count = 1;\n\ti->next = po->mclist;\n\tpo->mclist = i;\n\terr = packet_dev_mc(dev, i, 1);\n\tif (err) {\n\t\tpo->mclist = i->next;\n\t\tkfree(i);\n\t}\n\ndone:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int packet_mc_drop(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_mclist *ml, **mlp;\n\n\trtnl_lock();\n\n\tfor (mlp = &pkt_sk(sk)->mclist; (ml = *mlp) != NULL; mlp = &ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tif (--ml->count == 0) {\n\t\t\t\tstruct net_device *dev;\n\t\t\t\t*mlp = ml->next;\n\t\t\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\t\t\tif (dev)\n\t\t\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t\tkfree(ml);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trtnl_unlock();\n\treturn 0;\n}\n\nstatic void packet_flush_mclist(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml;\n\n\tif (!po->mclist)\n\t\treturn;\n\n\trtnl_lock();\n\twhile ((ml = po->mclist) != NULL) {\n\t\tstruct net_device *dev;\n\n\t\tpo->mclist = ml->next;\n\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\tif (dev != NULL)\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\tkfree(ml);\n\t}\n\trtnl_unlock();\n}\n\nstatic int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tif (val > INT_MAX)\n\t\t\treturn -EINVAL;\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_reserve = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int packet_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint len;\n\tint val, lv = sizeof(val);\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tvoid *data = &val;\n\tunion tpacket_stats_u st;\n\tstruct tpacket_rollover_stats rstats;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase PACKET_STATISTICS:\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tmemcpy(&st, &po->stats, sizeof(st));\n\t\tmemset(&po->stats, 0, sizeof(po->stats));\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\n\t\tif (po->tp_version == TPACKET_V3) {\n\t\t\tlv = sizeof(struct tpacket_stats_v3);\n\t\t\tst.stats3.tp_packets += st.stats3.tp_drops;\n\t\t\tdata = &st.stats3;\n\t\t} else {\n\t\t\tlv = sizeof(struct tpacket_stats);\n\t\t\tst.stats1.tp_packets += st.stats1.tp_drops;\n\t\t\tdata = &st.stats1;\n\t\t}\n\n\t\tbreak;\n\tcase PACKET_AUXDATA:\n\t\tval = po->auxdata;\n\t\tbreak;\n\tcase PACKET_ORIGDEV:\n\t\tval = po->origdev;\n\t\tbreak;\n\tcase PACKET_VNET_HDR:\n\t\tval = po->has_vnet_hdr;\n\t\tbreak;\n\tcase PACKET_VERSION:\n\t\tval = po->tp_version;\n\t\tbreak;\n\tcase PACKET_HDRLEN:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, len))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\t\tval = sizeof(struct tpacket_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tval = sizeof(struct tpacket2_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tval = sizeof(struct tpacket3_hdr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase PACKET_RESERVE:\n\t\tval = po->tp_reserve;\n\t\tbreak;\n\tcase PACKET_LOSS:\n\t\tval = po->tp_loss;\n\t\tbreak;\n\tcase PACKET_TIMESTAMP:\n\t\tval = po->tp_tstamp;\n\t\tbreak;\n\tcase PACKET_FANOUT:\n\t\tval = (po->fanout ?\n\t\t       ((u32)po->fanout->id |\n\t\t\t((u32)po->fanout->type << 16) |\n\t\t\t((u32)po->fanout->flags << 24)) :\n\t\t       0);\n\t\tbreak;\n\tcase PACKET_ROLLOVER_STATS:\n\t\tif (!po->rollover)\n\t\t\treturn -EINVAL;\n\t\trstats.tp_all = atomic_long_read(&po->rollover->num);\n\t\trstats.tp_huge = atomic_long_read(&po->rollover->num_huge);\n\t\trstats.tp_failed = atomic_long_read(&po->rollover->num_failed);\n\t\tdata = &rstats;\n\t\tlv = sizeof(rstats);\n\t\tbreak;\n\tcase PACKET_TX_HAS_OFF:\n\t\tval = po->tp_tx_has_off;\n\t\tbreak;\n\tcase PACKET_QDISC_BYPASS:\n\t\tval = packet_use_direct_xmit(po);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, data, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n\n#ifdef CONFIG_COMPAT\nstatic int compat_packet_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct packet_sock *po = pkt_sk(sock->sk);\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optname == PACKET_FANOUT_DATA &&\n\t    po->fanout && po->fanout->type == PACKET_FANOUT_CBPF) {\n\t\toptval = (char __user *)get_compat_bpf_fprog(optval);\n\t\tif (!optval)\n\t\t\treturn -EFAULT;\n\t\toptlen = sizeof(struct sock_fprog);\n\t}\n\n\treturn packet_setsockopt(sock, level, optname, optval, optlen);\n}\n#endif\n\nstatic int packet_notifier(struct notifier_block *this,\n\t\t\t   unsigned long msg, void *ptr)\n{\n\tstruct sock *sk;\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, &net->packet.sklist) {\n\t\tstruct packet_sock *po = pkt_sk(sk);\n\n\t\tswitch (msg) {\n\t\tcase NETDEV_UNREGISTER:\n\t\t\tif (po->mclist)\n\t\t\t\tpacket_dev_mclist_delete(dev, &po->mclist);\n\t\t\t/* fallthrough */\n\n\t\tcase NETDEV_DOWN:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->running) {\n\t\t\t\t\t__unregister_prot_hook(sk, false);\n\t\t\t\t\tsk->sk_err = ENETDOWN;\n\t\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t\tsk->sk_error_report(sk);\n\t\t\t\t}\n\t\t\t\tif (msg == NETDEV_UNREGISTER) {\n\t\t\t\t\tpacket_cached_dev_reset(po);\n\t\t\t\t\tpo->ifindex = -1;\n\t\t\t\t\tif (po->prot_hook.dev)\n\t\t\t\t\t\tdev_put(po->prot_hook.dev);\n\t\t\t\t\tpo->prot_hook.dev = NULL;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NETDEV_UP:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->num)\n\t\t\t\t\tregister_prot_hook(sk);\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NOTIFY_DONE;\n}\n\n\nstatic int packet_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCGSTAMP:\n\t\treturn sock_get_timestamp(sk, (struct timeval __user *)arg);\n\tcase SIOCGSTAMPNS:\n\t\treturn sock_get_timestampns(sk, (struct timespec __user *)arg);\n\n#ifdef CONFIG_INET\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFFLAGS:\n\t\treturn inet_dgram_ops.ioctl(sock, cmd, arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\treturn 0;\n}\n\nstatic unsigned int packet_poll(struct file *file, struct socket *sock,\n\t\t\t\tpoll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\n\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\tif (po->rx_ring.pg_vec) {\n\t\tif (!packet_previous_rx_frame(po, &po->rx_ring,\n\t\t\tTP_STATUS_KERNEL))\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\t}\n\tif (po->pressure && __packet_rcv_has_room(po, NULL) == ROOM_NORMAL)\n\t\tpo->pressure = 0;\n\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\tspin_lock_bh(&sk->sk_write_queue.lock);\n\tif (po->tx_ring.pg_vec) {\n\t\tif (packet_current_frame(po, &po->tx_ring, TP_STATUS_AVAILABLE))\n\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_write_queue.lock);\n\treturn mask;\n}\n\n\n/* Dirty? Well, I still did not learn better way to account\n * for user mmaps.\n */\n\nstatic void packet_mm_open(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_inc(&pkt_sk(sk)->mapped);\n}\n\nstatic void packet_mm_close(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_dec(&pkt_sk(sk)->mapped);\n}\n\nstatic const struct vm_operations_struct packet_mmap_ops = {\n\t.open\t=\tpacket_mm_open,\n\t.close\t=\tpacket_mm_close,\n};\n\nstatic void free_pg_vec(struct pgv *pg_vec, unsigned int order,\n\t\t\tunsigned int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (likely(pg_vec[i].buffer)) {\n\t\t\tif (is_vmalloc_addr(pg_vec[i].buffer))\n\t\t\t\tvfree(pg_vec[i].buffer);\n\t\t\telse\n\t\t\t\tfree_pages((unsigned long)pg_vec[i].buffer,\n\t\t\t\t\t   order);\n\t\t\tpg_vec[i].buffer = NULL;\n\t\t}\n\t}\n\tkfree(pg_vec);\n}\n\nstatic char *alloc_one_pg_vec_page(unsigned long order)\n{\n\tchar *buffer;\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_COMP |\n\t\t\t  __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY;\n\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/* __get_free_pages failed, fall back to vmalloc */\n\tbuffer = vzalloc((1 << order) * PAGE_SIZE);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/* vmalloc failed, lets dig into swap here */\n\tgfp_flags &= ~__GFP_NORETRY;\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/* complete and utter failure */\n\treturn NULL;\n}\n\nstatic struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)\n{\n\tunsigned int block_nr = req->tp_block_nr;\n\tstruct pgv *pg_vec;\n\tint i;\n\n\tpg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL);\n\tif (unlikely(!pg_vec))\n\t\tgoto out;\n\n\tfor (i = 0; i < block_nr; i++) {\n\t\tpg_vec[i].buffer = alloc_one_pg_vec_page(order);\n\t\tif (unlikely(!pg_vec[i].buffer))\n\t\t\tgoto out_free_pgvec;\n\t}\n\nout:\n\treturn pg_vec;\n\nout_free_pgvec:\n\tfree_pg_vec(pg_vec, order, block_nr);\n\tpg_vec = NULL;\n\tgoto out;\n}\n\nstatic int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    req->tp_block_size <=\n\t\t\t  BLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_block_size > UINT_MAX / req->tp_block_nr))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t\t/* Block transmit is not supported yet */\n\t\t\tif (!tx_ring) {\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\t} else {\n\t\t\t\tstruct tpacket_req3 *req3 = &req_u->req3;\n\n\t\t\t\tif (req3->tp_retire_blk_tov ||\n\t\t\t\t    req3->tp_sizeof_priv ||\n\t\t\t\t    req3->tp_feature_req_word) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (pg_vec && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic int packet_mmap(struct file *file, struct socket *sock,\n\t\tstruct vm_area_struct *vma)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned long size, expected_size;\n\tstruct packet_ring_buffer *rb;\n\tunsigned long start;\n\tint err = -EINVAL;\n\tint i;\n\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\texpected_size = 0;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec) {\n\t\t\texpected_size += rb->pg_vec_len\n\t\t\t\t\t\t* rb->pg_vec_pages\n\t\t\t\t\t\t* PAGE_SIZE;\n\t\t}\n\t}\n\n\tif (expected_size == 0)\n\t\tgoto out;\n\n\tsize = vma->vm_end - vma->vm_start;\n\tif (size != expected_size)\n\t\tgoto out;\n\n\tstart = vma->vm_start;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec == NULL)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < rb->pg_vec_len; i++) {\n\t\t\tstruct page *page;\n\t\t\tvoid *kaddr = rb->pg_vec[i].buffer;\n\t\t\tint pg_num;\n\n\t\t\tfor (pg_num = 0; pg_num < rb->pg_vec_pages; pg_num++) {\n\t\t\t\tpage = pgv_to_page(kaddr);\n\t\t\t\terr = vm_insert_page(vma, start, page);\n\t\t\t\tif (unlikely(err))\n\t\t\t\t\tgoto out;\n\t\t\t\tstart += PAGE_SIZE;\n\t\t\t\tkaddr += PAGE_SIZE;\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_inc(&po->mapped);\n\tvma->vm_ops = &packet_mmap_ops;\n\terr = 0;\n\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic const struct proto_ops packet_ops_spkt = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind_spkt,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname_spkt,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tpacket_sendmsg_spkt,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct proto_ops packet_ops = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname,\n\t.poll =\t\tpacket_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tpacket_setsockopt,\n\t.getsockopt =\tpacket_getsockopt,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_packet_setsockopt,\n#endif\n\t.sendmsg =\tpacket_sendmsg,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tpacket_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct net_proto_family packet_family_ops = {\n\t.family =\tPF_PACKET,\n\t.create =\tpacket_create,\n\t.owner\t=\tTHIS_MODULE,\n};\n\nstatic struct notifier_block packet_netdev_notifier = {\n\t.notifier_call =\tpacket_notifier,\n};\n\n#ifdef CONFIG_PROC_FS\n\nstatic void *packet_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\n\trcu_read_lock();\n\treturn seq_hlist_start_head_rcu(&net->packet.sklist, *pos);\n}\n\nstatic void *packet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net *net = seq_file_net(seq);\n\treturn seq_hlist_next_rcu(v, &net->packet.sklist, pos);\n}\n\nstatic void packet_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int packet_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"sk       RefCnt Type Proto  Iface R Rmem   User   Inode\\n\");\n\telse {\n\t\tstruct sock *s = sk_entry(v);\n\t\tconst struct packet_sock *po = pkt_sk(s);\n\n\t\tseq_printf(seq,\n\t\t\t   \"%pK %-6d %-4d %04x   %-5d %1d %-6u %-6u %-6lu\\n\",\n\t\t\t   s,\n\t\t\t   refcount_read(&s->sk_refcnt),\n\t\t\t   s->sk_type,\n\t\t\t   ntohs(po->num),\n\t\t\t   po->ifindex,\n\t\t\t   po->running,\n\t\t\t   atomic_read(&s->sk_rmem_alloc),\n\t\t\t   from_kuid_munged(seq_user_ns(seq), sock_i_uid(s)),\n\t\t\t   sock_i_ino(s));\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations packet_seq_ops = {\n\t.start\t= packet_seq_start,\n\t.next\t= packet_seq_next,\n\t.stop\t= packet_seq_stop,\n\t.show\t= packet_seq_show,\n};\n\nstatic int packet_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &packet_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations packet_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= packet_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nstatic int __net_init packet_net_init(struct net *net)\n{\n\tmutex_init(&net->packet.sklist_lock);\n\tINIT_HLIST_HEAD(&net->packet.sklist);\n\n\tif (!proc_create(\"packet\", 0, net->proc_net, &packet_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit packet_net_exit(struct net *net)\n{\n\tremove_proc_entry(\"packet\", net->proc_net);\n}\n\nstatic struct pernet_operations packet_net_ops = {\n\t.init = packet_net_init,\n\t.exit = packet_net_exit,\n};\n\n\nstatic void __exit packet_exit(void)\n{\n\tunregister_netdevice_notifier(&packet_netdev_notifier);\n\tunregister_pernet_subsys(&packet_net_ops);\n\tsock_unregister(PF_PACKET);\n\tproto_unregister(&packet_proto);\n}\n\nstatic int __init packet_init(void)\n{\n\tint rc = proto_register(&packet_proto, 0);\n\n\tif (rc != 0)\n\t\tgoto out;\n\n\tsock_register(&packet_family_ops);\n\tregister_pernet_subsys(&packet_net_ops);\n\tregister_netdevice_notifier(&packet_netdev_notifier);\nout:\n\treturn rc;\n}\n\nmodule_init(packet_init);\nmodule_exit(packet_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_PACKET);\n"], "filenames": ["net/packet/af_packet.c"], "buggy_code_start_loc": [2193], "buggy_code_end_loc": [2302], "fixing_code_start_loc": [2194], "fixing_code_end_loc": [2308], "type": "CWE-119", "message": "The tpacket_rcv function in net/packet/af_packet.c in the Linux kernel before 4.13 mishandles vnet headers, which might allow local users to cause a denial of service (buffer overflow, and disk and memory corruption) or possibly have unspecified other impact via crafted system calls.", "other": {"cve": {"id": "CVE-2017-14497", "sourceIdentifier": "cve@mitre.org", "published": "2017-09-15T18:29:00.180", "lastModified": "2023-01-19T15:48:11.540", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The tpacket_rcv function in net/packet/af_packet.c in the Linux kernel before 4.13 mishandles vnet headers, which might allow local users to cause a denial of service (buffer overflow, and disk and memory corruption) or possibly have unspecified other impact via crafted system calls."}, {"lang": "es", "value": "La funci\u00f3n tpacket_rcv en net/packet/af_packet.c en el kernel de Linux en versiones anteriores a la 4.13 no gestiona correctamente cabeceras vnet, lo que podr\u00eda permitir que usuarios locales provoquen una denegaci\u00f3n de servicio (desbordamiento de b\u00fafer y corrupci\u00f3n de disco y de memoria) o, posiblemente, provoquen otro impacto sin especificar mediante llamadas del sistema manipuladas."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.6", "versionEndExcluding": "4.9.51", "matchCriteriaId": "00AB00A3-1183-4070-BF4D-E3354C6D7DB9"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.12.14", "matchCriteriaId": "0758FD06-3B98-48B9-9A66-49CA15173FC5"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=edbd58be15a957f6a760c4a514cd475217eb97fd", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://seclists.org/oss-sec/2017/q3/476", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.debian.org/security/2017/dsa-3981", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/100871", "source": "cve@mitre.org", "tags": ["VDB Entry", "Third Party Advisory"]}, {"url": "http://www.securitytracker.com/id/1039371", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1040106", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1492593", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/edbd58be15a957f6a760c4a514cd475217eb97fd", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://marc.info/?l=linux-kernel&m=150394500728906&w=2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://marc.info/?t=150394517700001&r=1&w=2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2018-01-01", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/edbd58be15a957f6a760c4a514cd475217eb97fd"}}